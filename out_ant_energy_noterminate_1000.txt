energy_cvt_1000_no_terminateant_seed_1111
is energy measures =  True
feet contact, is_energy_measures =  True
<brax.envs.ant.Ant object at 0x7fa2433a79d0>
feet contact and energy
obs_shape
(87,)
action_shape
(8,)
vec env: 
<JaxToTorchWrapper<OrderEnforcing<VectorGymWrapper<brax_custom-ant-v0>>>>
wandb: Currently logged in as: anishapv (qdrl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/icaros/Documents/PPGADev/wandb/run-20230716_203113-7rzmr7ml
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run energy_cvt_1000_no_terminateant_seed_1111
wandb: ‚≠êÔ∏è View project at https://wandb.ai/qdrl/PPGA
wandb: üöÄ View run at https://wandb.ai/qdrl/PPGA/runs/7rzmr7ml
[36m[2023-07-16 20:31:17,416][257371] Environment ant, action_dim=8, obs_dim=87[0m
bounds: 
[(0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.0, 8.0)]
using cvt archive
no kmeans
[36m[2023-07-16 20:31:20,902][257371] Created Scheduler for cma_maega with an archive learning rate of 0.1, and add mode batch, using solution dim 28816 and archive dims [10, 10, 10, 10, 10]. Min threshold is -500.0. Restart rule is no_improvement[0m
[36m[2023-07-16 20:31:38,637][257371] train() took 13.74 seconds to complete[0m
[36m[2023-07-16 20:31:38,637][257371] FPS: 279428.33[0m
[36m[2023-07-16 20:31:42,709][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:31:42,709][257371] Reward + Measures: [[-1.09223743  0.19019434  0.190283    0.18992266  0.19008565  4.12809515]][0m
[37m[1m[2023-07-16 20:31:42,710][257371] Max Reward on eval: -1.0922374332776623[0m
[37m[1m[2023-07-16 20:31:42,710][257371] Min Reward on eval: -1.0922374332776623[0m
[37m[1m[2023-07-16 20:31:42,710][257371] Mean Reward across all agents: -1.0922374332776623[0m
[37m[1m[2023-07-16 20:31:42,710][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:31:48,206][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:31:48,207][257371] Reward + Measures: [[-32.32459957   0.17         0.1657       0.14740001   0.16060001
    4.47504663]
 [ 59.98550554   0.17740001   0.17549999   0.20030001   0.18880001
    4.41325092]
 [ -0.4589016    0.2105       0.20539999   0.22300003   0.19320001
    4.35402536]
 ...
 [ 31.70576495   0.1814       0.1883       0.1831       0.18160002
    4.25515938]
 [ -1.42528477   0.148        0.1585       0.14320001   0.16080001
    4.30354452]
 [ 67.41181046   0.1716       0.17400001   0.1736       0.16980001
    4.45597696]][0m
[37m[1m[2023-07-16 20:31:48,207][257371] Max Reward on eval: 152.04082151167094[0m
[37m[1m[2023-07-16 20:31:48,207][257371] Min Reward on eval: -78.06522632949054[0m
[37m[1m[2023-07-16 20:31:48,208][257371] Mean Reward across all agents: 8.517803536027039[0m
[37m[1m[2023-07-16 20:31:48,208][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:31:48,228][257371] mean_value=493.38426807150313, max_value=652.040821511671[0m
[37m[1m[2023-07-16 20:31:48,262][257371] New mean coefficients: [[ 3.125786   -0.6696669  -0.9543489  -1.9973943  -1.7994578  -0.86615634]][0m
[37m[1m[2023-07-16 20:31:48,263][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:31:56,726][257371] train() took 8.46 seconds to complete[0m
[36m[2023-07-16 20:31:56,726][257371] FPS: 453834.03[0m
[36m[2023-07-16 20:31:56,728][257371] itr=0, itrs=2000, Progress: 0.00%[0m
[36m[2023-07-16 20:32:08,285][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-16 20:32:08,285][257371] FPS: 332685.34[0m
[36m[2023-07-16 20:32:12,545][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:32:12,546][257371] Reward + Measures: [[147.38898422   0.19903865   0.20520133   0.192499     0.19681801
    3.67647648]][0m
[37m[1m[2023-07-16 20:32:12,546][257371] Max Reward on eval: 147.38898422216437[0m
[37m[1m[2023-07-16 20:32:12,546][257371] Min Reward on eval: 147.38898422216437[0m
[37m[1m[2023-07-16 20:32:12,547][257371] Mean Reward across all agents: 147.38898422216437[0m
[37m[1m[2023-07-16 20:32:12,547][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:32:17,531][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:32:17,531][257371] Reward + Measures: [[ 79.5614328    0.1727       0.18600002   0.15699999   0.16580001
    4.13712978]
 [112.69424249   0.09990001   0.12         0.11960001   0.10280001
    4.50765848]
 [139.22898336   0.15149999   0.1452       0.16780001   0.11979999
    4.8292799 ]
 ...
 [120.46097248   0.1188       0.12719999   0.1221       0.1069
    4.30197716]
 [ 26.05857138   0.14259999   0.11620001   0.1276       0.11919999
    4.41173649]
 [110.36985681   0.1219       0.1274       0.1213       0.1362
    4.32495832]][0m
[37m[1m[2023-07-16 20:32:17,532][257371] Max Reward on eval: 333.78631300148555[0m
[37m[1m[2023-07-16 20:32:17,532][257371] Min Reward on eval: -26.594382008723915[0m
[37m[1m[2023-07-16 20:32:17,532][257371] Mean Reward across all agents: 115.30491068950923[0m
[37m[1m[2023-07-16 20:32:17,532][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:32:17,543][257371] mean_value=193.0843181488683, max_value=758.9839281703811[0m
[37m[1m[2023-07-16 20:32:17,546][257371] New mean coefficients: [[ 1.8010997  -0.73897827 -0.10800403 -1.36501    -1.5101334  -0.87118524]][0m
[37m[1m[2023-07-16 20:32:17,547][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:32:26,488][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-16 20:32:26,489][257371] FPS: 429553.01[0m
[36m[2023-07-16 20:32:26,491][257371] itr=1, itrs=2000, Progress: 0.05%[0m
[36m[2023-07-16 20:32:38,282][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-16 20:32:38,282][257371] FPS: 326259.67[0m
[36m[2023-07-16 20:32:42,541][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:32:42,541][257371] Reward + Measures: [[328.20788849   0.19401233   0.21176533   0.18635799   0.189658
    3.10705686]][0m
[37m[1m[2023-07-16 20:32:42,541][257371] Max Reward on eval: 328.20788849234543[0m
[37m[1m[2023-07-16 20:32:42,541][257371] Min Reward on eval: 328.20788849234543[0m
[37m[1m[2023-07-16 20:32:42,542][257371] Mean Reward across all agents: 328.20788849234543[0m
[37m[1m[2023-07-16 20:32:42,542][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:32:47,658][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:32:47,658][257371] Reward + Measures: [[298.69838338   0.23940001   0.25460002   0.19600001   0.223
    3.1260457 ]
 [155.99120426   0.14920001   0.18889999   0.1664       0.139
    3.83613753]
 [369.01349949   0.18270002   0.1648       0.16230001   0.13740002
    3.61009955]
 ...
 [268.91866206   0.2326       0.17900001   0.20509999   0.16059999
    3.32368851]
 [195.68271397   0.14240001   0.1788       0.1718       0.17209999
    3.5140965 ]
 [203.47720577   0.24750002   0.27449998   0.22880001   0.2304
    3.19446111]][0m
[37m[1m[2023-07-16 20:32:47,658][257371] Max Reward on eval: 428.0488128615543[0m
[37m[1m[2023-07-16 20:32:47,659][257371] Min Reward on eval: -55.54475606289925[0m
[37m[1m[2023-07-16 20:32:47,659][257371] Mean Reward across all agents: 230.58765162929544[0m
[37m[1m[2023-07-16 20:32:47,659][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:32:47,669][257371] mean_value=456.73057618637995, max_value=841.4795250233766[0m
[37m[1m[2023-07-16 20:32:47,672][257371] New mean coefficients: [[ 1.6840613  -0.07548583  0.09482104 -1.1478946  -0.85937095 -0.88776433]][0m
[37m[1m[2023-07-16 20:32:47,673][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:32:56,758][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 20:32:56,758][257371] FPS: 422763.85[0m
[36m[2023-07-16 20:32:56,760][257371] itr=2, itrs=2000, Progress: 0.10%[0m
[36m[2023-07-16 20:33:08,414][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-16 20:33:08,414][257371] FPS: 330231.53[0m
[36m[2023-07-16 20:33:12,725][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:33:12,726][257371] Reward + Measures: [[508.35711469   0.18970731   0.21463665   0.17971565   0.18190901
    2.5373764 ]][0m
[37m[1m[2023-07-16 20:33:12,726][257371] Max Reward on eval: 508.35711469484534[0m
[37m[1m[2023-07-16 20:33:12,726][257371] Min Reward on eval: 508.35711469484534[0m
[37m[1m[2023-07-16 20:33:12,726][257371] Mean Reward across all agents: 508.35711469484534[0m
[37m[1m[2023-07-16 20:33:12,727][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:33:17,752][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:33:17,763][257371] Reward + Measures: [[416.23826223   0.25740001   0.26700002   0.28210002   0.17990001
    2.6945436 ]
 [285.41101694   0.11659999   0.17040001   0.10969999   0.1508
    2.74436641]
 [278.39105605   0.22130001   0.26879999   0.1433       0.2375
    3.17484212]
 ...
 [217.37002485   0.19759999   0.1904       0.1432       0.18520002
    2.68687224]
 [544.12280144   0.2146       0.25240001   0.23799999   0.23140001
    2.67394209]
 [378.60381894   0.20810001   0.25759998   0.2027       0.2665
    2.84821606]][0m
[37m[1m[2023-07-16 20:33:17,763][257371] Max Reward on eval: 623.6900958588813[0m
[37m[1m[2023-07-16 20:33:17,763][257371] Min Reward on eval: 102.33491923483089[0m
[37m[1m[2023-07-16 20:33:17,764][257371] Mean Reward across all agents: 389.9482116847795[0m
[37m[1m[2023-07-16 20:33:17,764][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:33:17,775][257371] mean_value=653.5395139266452, max_value=1107.9975853135809[0m
[37m[1m[2023-07-16 20:33:17,778][257371] New mean coefficients: [[ 2.1854844  -0.11248298  0.43206352 -0.4804927  -0.26099157 -1.140174  ]][0m
[37m[1m[2023-07-16 20:33:17,779][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:33:26,786][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 20:33:26,786][257371] FPS: 426413.97[0m
[36m[2023-07-16 20:33:26,789][257371] itr=3, itrs=2000, Progress: 0.15%[0m
[36m[2023-07-16 20:33:38,631][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-16 20:33:38,631][257371] FPS: 324948.68[0m
[36m[2023-07-16 20:33:42,956][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:33:42,956][257371] Reward + Measures: [[767.4799434    0.18752401   0.22287866   0.17851865   0.16761333
    2.07427382]][0m
[37m[1m[2023-07-16 20:33:42,956][257371] Max Reward on eval: 767.4799433956882[0m
[37m[1m[2023-07-16 20:33:42,957][257371] Min Reward on eval: 767.4799433956882[0m
[37m[1m[2023-07-16 20:33:42,957][257371] Mean Reward across all agents: 767.4799433956882[0m
[37m[1m[2023-07-16 20:33:42,957][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:33:47,942][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:33:47,943][257371] Reward + Measures: [[687.70705438   0.25310001   0.2342       0.2436       0.1997
    2.11568522]
 [676.87540818   0.2357       0.22329998   0.1927       0.1885
    2.29571009]
 [599.1797199    0.19250001   0.20680001   0.20030001   0.1829
    2.42139888]
 ...
 [724.17227933   0.23029999   0.28370002   0.21110001   0.23459999
    2.04910398]
 [706.41559123   0.18889999   0.18980001   0.18279999   0.15730001
    2.30136371]
 [360.37605664   0.23840001   0.29720002   0.19679999   0.27490002
    2.26105499]][0m
[37m[1m[2023-07-16 20:33:47,943][257371] Max Reward on eval: 981.2824478163384[0m
[37m[1m[2023-07-16 20:33:47,943][257371] Min Reward on eval: 237.02029415806754[0m
[37m[1m[2023-07-16 20:33:47,944][257371] Mean Reward across all agents: 588.3525076685648[0m
[37m[1m[2023-07-16 20:33:47,944][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:33:47,955][257371] mean_value=876.0286981463289, max_value=1481.2824478163384[0m
[37m[1m[2023-07-16 20:33:47,957][257371] New mean coefficients: [[ 2.5170498  -0.09549224  0.9369341   0.07847792 -0.14536399 -0.53386825]][0m
[37m[1m[2023-07-16 20:33:47,958][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:33:56,983][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 20:33:56,983][257371] FPS: 425590.60[0m
[36m[2023-07-16 20:33:56,985][257371] itr=4, itrs=2000, Progress: 0.20%[0m
[36m[2023-07-16 20:34:08,686][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-16 20:34:08,686][257371] FPS: 328936.93[0m
[36m[2023-07-16 20:34:12,946][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:34:12,946][257371] Reward + Measures: [[1069.2172981     0.17918468    0.233054      0.175099      0.14868566
     1.91062951]][0m
[37m[1m[2023-07-16 20:34:12,946][257371] Max Reward on eval: 1069.2172981002918[0m
[37m[1m[2023-07-16 20:34:12,946][257371] Min Reward on eval: 1069.2172981002918[0m
[37m[1m[2023-07-16 20:34:12,947][257371] Mean Reward across all agents: 1069.2172981002918[0m
[37m[1m[2023-07-16 20:34:12,947][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:34:17,939][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:34:17,939][257371] Reward + Measures: [[459.90327073   0.1567       0.29170001   0.1683       0.198
    1.86475205]
 [941.04772518   0.18780001   0.26930001   0.2333       0.15890001
    2.03069258]
 [760.33722499   0.26489997   0.25400001   0.20019999   0.21949999
    2.41131568]
 ...
 [652.96443554   0.2253       0.2131       0.21660002   0.1505
    2.09588504]
 [377.76583527   0.13060001   0.171        0.10570001   0.16059999
    2.05759287]
 [278.0477295    0.13         0.1895       0.11589999   0.1684
    2.31546807]][0m
[37m[1m[2023-07-16 20:34:17,940][257371] Max Reward on eval: 1302.5618743971222[0m
[37m[1m[2023-07-16 20:34:17,940][257371] Min Reward on eval: 268.9579877892509[0m
[37m[1m[2023-07-16 20:34:17,940][257371] Mean Reward across all agents: 830.2537021266976[0m
[37m[1m[2023-07-16 20:34:17,940][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:34:17,947][257371] mean_value=203.08201997496434, max_value=1187.1843643287198[0m
[37m[1m[2023-07-16 20:34:17,956][257371] New mean coefficients: [[ 1.4917458   0.5584342   0.6579567   0.36110005 -0.04736184  0.20052534]][0m
[37m[1m[2023-07-16 20:34:17,957][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:34:26,967][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 20:34:26,967][257371] FPS: 426255.37[0m
[36m[2023-07-16 20:34:26,969][257371] itr=5, itrs=2000, Progress: 0.25%[0m
[36m[2023-07-16 20:34:38,790][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-16 20:34:38,791][257371] FPS: 325451.01[0m
[36m[2023-07-16 20:34:43,054][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:34:43,055][257371] Reward + Measures: [[1449.6819268     0.18839766    0.25776231    0.191184      0.141912
     1.92839825]][0m
[37m[1m[2023-07-16 20:34:43,055][257371] Max Reward on eval: 1449.6819267966766[0m
[37m[1m[2023-07-16 20:34:43,055][257371] Min Reward on eval: 1449.6819267966766[0m
[37m[1m[2023-07-16 20:34:43,055][257371] Mean Reward across all agents: 1449.6819267966766[0m
[37m[1m[2023-07-16 20:34:43,056][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:34:48,215][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:34:48,221][257371] Reward + Measures: [[1307.71524813    0.24299999    0.35350001    0.2465        0.19380002
     2.08316708]
 [1392.82859798    0.22520001    0.2656        0.19319999    0.16239999
     1.95318604]
 [1084.45574569    0.21610001    0.37380001    0.2705        0.19319999
     2.07424808]
 ...
 [1182.644043      0.23729999    0.31640002    0.2027        0.1743
     2.15157962]
 [ 649.42334984    0.2263        0.3055        0.25600001    0.20909999
     2.04097152]
 [1274.32476138    0.178         0.23559999    0.2027        0.15019999
     2.08119726]][0m
[37m[1m[2023-07-16 20:34:48,221][257371] Max Reward on eval: 1612.9788055628537[0m
[37m[1m[2023-07-16 20:34:48,221][257371] Min Reward on eval: 290.91882260134446[0m
[37m[1m[2023-07-16 20:34:48,222][257371] Mean Reward across all agents: 1128.7018221518529[0m
[37m[1m[2023-07-16 20:34:48,222][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:34:48,228][257371] mean_value=217.91470492626976, max_value=1416.0446861479804[0m
[37m[1m[2023-07-16 20:34:48,231][257371] New mean coefficients: [[ 0.8636135   0.44379276  1.0844352   0.3915      0.10843084 -0.07606432]][0m
[37m[1m[2023-07-16 20:34:48,232][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:34:57,325][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-16 20:34:57,325][257371] FPS: 422391.63[0m
[36m[2023-07-16 20:34:57,327][257371] itr=6, itrs=2000, Progress: 0.30%[0m
[36m[2023-07-16 20:35:09,157][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-16 20:35:09,157][257371] FPS: 325328.77[0m
[36m[2023-07-16 20:35:13,515][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:35:13,515][257371] Reward + Measures: [[1888.12966837    0.20259666    0.29291633    0.21520768    0.13362634
     1.91642678]][0m
[37m[1m[2023-07-16 20:35:13,516][257371] Max Reward on eval: 1888.129668370491[0m
[37m[1m[2023-07-16 20:35:13,516][257371] Min Reward on eval: 1888.129668370491[0m
[37m[1m[2023-07-16 20:35:13,516][257371] Mean Reward across all agents: 1888.129668370491[0m
[37m[1m[2023-07-16 20:35:13,516][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:35:18,499][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:35:18,499][257371] Reward + Measures: [[1696.23631287    0.19150002    0.31579998    0.1935        0.16040002
     2.09380507]
 [1112.53742794    0.25400001    0.37129998    0.2158        0.2106
     1.87426662]
 [1751.2595482     0.20810001    0.2818        0.20469999    0.1538
     2.03284812]
 ...
 [1167.23630434    0.22149999    0.31759998    0.23120001    0.14219999
     1.80383039]
 [1831.97500607    0.2016        0.28580001    0.23239999    0.1332
     2.10692453]
 [ 663.02753353    0.17459999    0.2086        0.183         0.1393
     2.26206088]][0m
[37m[1m[2023-07-16 20:35:18,499][257371] Max Reward on eval: 2197.9439239712433[0m
[37m[1m[2023-07-16 20:35:18,500][257371] Min Reward on eval: 663.0275335261133[0m
[37m[1m[2023-07-16 20:35:18,500][257371] Mean Reward across all agents: 1471.7133117529822[0m
[37m[1m[2023-07-16 20:35:18,500][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:35:18,507][257371] mean_value=270.40782039000203, max_value=1537.1613004791789[0m
[37m[1m[2023-07-16 20:35:18,510][257371] New mean coefficients: [[0.81078076 0.27915847 1.6798508  0.04746231 0.67527324 0.06022833]][0m
[37m[1m[2023-07-16 20:35:18,511][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:35:27,503][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-16 20:35:27,503][257371] FPS: 427115.76[0m
[36m[2023-07-16 20:35:27,506][257371] itr=7, itrs=2000, Progress: 0.35%[0m
[36m[2023-07-16 20:35:39,208][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-16 20:35:39,208][257371] FPS: 328809.02[0m
[36m[2023-07-16 20:35:43,626][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:35:43,631][257371] Reward + Measures: [[2369.48605884    0.20735599    0.32522202    0.22186565    0.134257
     1.97453558]][0m
[37m[1m[2023-07-16 20:35:43,632][257371] Max Reward on eval: 2369.4860588385104[0m
[37m[1m[2023-07-16 20:35:43,632][257371] Min Reward on eval: 2369.4860588385104[0m
[37m[1m[2023-07-16 20:35:43,632][257371] Mean Reward across all agents: 2369.4860588385104[0m
[37m[1m[2023-07-16 20:35:43,632][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:35:48,693][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:35:48,699][257371] Reward + Measures: [[2280.29949954    0.20810001    0.3716        0.24389999    0.1435
     2.24848557]
 [1986.48773194    0.18450001    0.2868        0.19470003    0.12539999
     1.9055239 ]
 [1938.94598393    0.2163        0.38970003    0.24870001    0.20580001
     2.2643373 ]
 ...
 [2092.29188536    0.19590001    0.34280002    0.27340001    0.1434
     2.01504564]
 [1113.74079893    0.19220002    0.41120002    0.2388        0.2349
     1.92623067]
 [1270.99895479    0.20990001    0.39230004    0.2362        0.24439998
     2.43948221]][0m
[37m[1m[2023-07-16 20:35:48,700][257371] Max Reward on eval: 2572.8595886270514[0m
[37m[1m[2023-07-16 20:35:48,700][257371] Min Reward on eval: 978.3107032642234[0m
[37m[1m[2023-07-16 20:35:48,700][257371] Mean Reward across all agents: 1822.6322458494244[0m
[37m[1m[2023-07-16 20:35:48,700][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:35:48,706][257371] mean_value=247.68821743574924, max_value=1927.5159301978304[0m
[37m[1m[2023-07-16 20:35:48,709][257371] New mean coefficients: [[0.9356661  0.17959668 1.805344   0.18156488 0.5125905  0.2296725 ]][0m
[37m[1m[2023-07-16 20:35:48,710][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:35:57,866][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-16 20:35:57,866][257371] FPS: 419467.46[0m
[36m[2023-07-16 20:35:57,868][257371] itr=8, itrs=2000, Progress: 0.40%[0m
[36m[2023-07-16 20:36:09,636][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-16 20:36:09,636][257371] FPS: 327030.09[0m
[36m[2023-07-16 20:36:13,979][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:36:13,979][257371] Reward + Measures: [[2809.2274025     0.20696566    0.32456532    0.21857335    0.11778099
     2.00531578]][0m
[37m[1m[2023-07-16 20:36:13,979][257371] Max Reward on eval: 2809.2274025009037[0m
[37m[1m[2023-07-16 20:36:13,980][257371] Min Reward on eval: 2809.2274025009037[0m
[37m[1m[2023-07-16 20:36:13,980][257371] Mean Reward across all agents: 2809.2274025009037[0m
[37m[1m[2023-07-16 20:36:13,980][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:36:19,109][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:36:19,114][257371] Reward + Measures: [[2130.16416928    0.19960001    0.29589999    0.2113        0.13
     2.2979269 ]
 [1909.71923828    0.20390001    0.28739998    0.20940001    0.13959999
     2.26212764]
 [1473.20320893    0.18830001    0.30270001    0.2086        0.1473
     1.99469793]
 ...
 [1377.38409042    0.22010003    0.29390001    0.24099998    0.12
     1.82789636]
 [1686.45351406    0.25600001    0.33500001    0.25650001    0.1919
     2.21789718]
 [2374.15971373    0.22149999    0.35340002    0.25289997    0.11719999
     1.99242651]][0m
[37m[1m[2023-07-16 20:36:19,115][257371] Max Reward on eval: 3047.8683777115775[0m
[37m[1m[2023-07-16 20:36:19,115][257371] Min Reward on eval: 1018.2162341664546[0m
[37m[1m[2023-07-16 20:36:19,115][257371] Mean Reward across all agents: 2102.801902200491[0m
[37m[1m[2023-07-16 20:36:19,116][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:36:19,120][257371] mean_value=83.57397964536204, max_value=1067.1406870365322[0m
[37m[1m[2023-07-16 20:36:19,123][257371] New mean coefficients: [[ 0.9759476   0.24033967  0.84672797 -0.17332883  0.4989492   1.1465111 ]][0m
[37m[1m[2023-07-16 20:36:19,124][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:36:28,118][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-16 20:36:28,118][257371] FPS: 427015.81[0m
[36m[2023-07-16 20:36:28,121][257371] itr=9, itrs=2000, Progress: 0.45%[0m
[36m[2023-07-16 20:36:39,694][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-16 20:36:39,695][257371] FPS: 332410.77[0m
[36m[2023-07-16 20:36:44,012][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:36:44,012][257371] Reward + Measures: [[3105.73981908    0.20597966    0.30536401    0.21638066    0.11203466
     2.22121406]][0m
[37m[1m[2023-07-16 20:36:44,012][257371] Max Reward on eval: 3105.7398190765343[0m
[37m[1m[2023-07-16 20:36:44,013][257371] Min Reward on eval: 3105.7398190765343[0m
[37m[1m[2023-07-16 20:36:44,013][257371] Mean Reward across all agents: 3105.7398190765343[0m
[37m[1m[2023-07-16 20:36:44,013][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:36:49,068][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:36:49,068][257371] Reward + Measures: [[2554.09739692    0.19780003    0.31440005    0.22239999    0.14229999
     2.27450871]
 [2006.6329193     0.17980002    0.29620001    0.1944        0.10950001
     2.32147717]
 [2549.79423519    0.23910001    0.31800005    0.23629999    0.13280001
     2.42359614]
 ...
 [2039.3990936     0.1945        0.25760004    0.1962        0.12950002
     2.39447093]
 [2266.49823759    0.18180001    0.31039998    0.21780001    0.1115
     2.22174191]
 [1663.37566283    0.1891        0.30430001    0.184         0.1286
     1.95941389]][0m
[37m[1m[2023-07-16 20:36:49,069][257371] Max Reward on eval: 3240.8678589747287[0m
[37m[1m[2023-07-16 20:36:49,069][257371] Min Reward on eval: 936.2537193357945[0m
[37m[1m[2023-07-16 20:36:49,069][257371] Mean Reward across all agents: 2289.380975863769[0m
[37m[1m[2023-07-16 20:36:49,069][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:36:49,074][257371] mean_value=5.60598442348363, max_value=1520.6684104033436[0m
[37m[1m[2023-07-16 20:36:49,077][257371] New mean coefficients: [[0.17984319 0.22257197 0.20945281 0.10948239 0.75936115 2.38098   ]][0m
[37m[1m[2023-07-16 20:36:49,078][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:36:58,089][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 20:36:58,089][257371] FPS: 426198.87[0m
[36m[2023-07-16 20:36:58,092][257371] itr=10, itrs=2000, Progress: 0.50%[0m
[36m[2023-07-16 20:39:00,933][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-16 20:39:01,082][257371] FPS: 324288.82[0m
[36m[2023-07-16 20:39:05,437][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:39:05,438][257371] Reward + Measures: [[3064.99212515    0.21786867    0.31631133    0.223827      0.13599633
     2.78032136]][0m
[37m[1m[2023-07-16 20:39:05,438][257371] Max Reward on eval: 3064.992125146543[0m
[37m[1m[2023-07-16 20:39:05,438][257371] Min Reward on eval: 3064.992125146543[0m
[37m[1m[2023-07-16 20:39:05,439][257371] Mean Reward across all agents: 3064.992125146543[0m
[37m[1m[2023-07-16 20:39:05,439][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:39:10,458][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:39:10,459][257371] Reward + Measures: [[2870.11303707    0.2412        0.34629998    0.2062        0.1476
     2.86339164]
 [2565.62564847    0.2167        0.32450002    0.2234        0.1506
     2.72813582]
 [1809.05844119    0.17730001    0.24850002    0.18490002    0.1542
     2.49614501]
 ...
 [2332.23102196    0.19900002    0.29169998    0.1876        0.1429
     2.72164345]
 [3065.04428105    0.222         0.33820003    0.2386        0.15769999
     2.86677146]
 [2726.39152529    0.21440001    0.36719999    0.22979999    0.1548
     2.72303128]][0m
[37m[1m[2023-07-16 20:39:10,459][257371] Max Reward on eval: 3243.7220764011377[0m
[37m[1m[2023-07-16 20:39:10,459][257371] Min Reward on eval: 1210.3911228621378[0m
[37m[1m[2023-07-16 20:39:10,460][257371] Mean Reward across all agents: 2324.7339948502763[0m
[37m[1m[2023-07-16 20:39:10,460][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:39:10,467][257371] mean_value=1221.6883991932639, max_value=2623.208332800484[0m
[37m[1m[2023-07-16 20:39:10,469][257371] New mean coefficients: [[ 0.17362958  0.12189226  0.66980684 -0.21406816  0.04016751  1.1839871 ]][0m
[37m[1m[2023-07-16 20:39:10,470][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:39:19,169][257371] train() took 8.70 seconds to complete[0m
[36m[2023-07-16 20:39:19,169][257371] FPS: 441513.77[0m
[36m[2023-07-16 20:39:19,172][257371] itr=11, itrs=2000, Progress: 0.55%[0m
[36m[2023-07-16 20:39:31,020][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-16 20:39:31,025][257371] FPS: 324719.71[0m
[36m[2023-07-16 20:39:35,290][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:39:35,291][257371] Reward + Measures: [[3108.35721585    0.22340368    0.31641632    0.223526      0.14246033
     3.18758774]][0m
[37m[1m[2023-07-16 20:39:35,291][257371] Max Reward on eval: 3108.357215847772[0m
[37m[1m[2023-07-16 20:39:35,291][257371] Min Reward on eval: 3108.357215847772[0m
[37m[1m[2023-07-16 20:39:35,292][257371] Mean Reward across all agents: 3108.357215847772[0m
[37m[1m[2023-07-16 20:39:35,292][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:39:40,480][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:39:40,481][257371] Reward + Measures: [[2036.45836448    0.23079999    0.31280002    0.21009998    0.182
     2.76185489]
 [2449.94570921    0.1992        0.26289997    0.19690001    0.11930001
     2.86027074]
 [2208.98786923    0.20299999    0.29800004    0.1965        0.15630001
     2.98953676]
 ...
 [2206.21918484    0.20489998    0.2726        0.18190001    0.1408
     2.68960738]
 [2307.49124718    0.199         0.27550003    0.19369999    0.13240001
     2.89138293]
 [2056.30158999    0.24870001    0.37470001    0.2105        0.2027
     2.94254994]][0m
[37m[1m[2023-07-16 20:39:40,481][257371] Max Reward on eval: 3271.3524780010803[0m
[37m[1m[2023-07-16 20:39:40,481][257371] Min Reward on eval: 1343.4428486589343[0m
[37m[1m[2023-07-16 20:39:40,481][257371] Mean Reward across all agents: 2488.5574134867597[0m
[37m[1m[2023-07-16 20:39:40,482][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:39:40,487][257371] mean_value=1350.4783051253592, max_value=3635.7468575794587[0m
[37m[1m[2023-07-16 20:39:40,489][257371] New mean coefficients: [[0.42195007 0.03829569 0.53122926 0.30392957 0.08759104 1.0252656 ]][0m
[37m[1m[2023-07-16 20:39:40,490][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:39:49,641][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-16 20:39:49,642][257371] FPS: 419700.22[0m
[36m[2023-07-16 20:39:49,644][257371] itr=12, itrs=2000, Progress: 0.60%[0m
[36m[2023-07-16 20:40:01,512][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-16 20:40:01,512][257371] FPS: 324152.12[0m
[36m[2023-07-16 20:40:05,762][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:40:05,762][257371] Reward + Measures: [[3271.07882333    0.22046967    0.30488634    0.22161531    0.13105433
     3.47617173]][0m
[37m[1m[2023-07-16 20:40:05,762][257371] Max Reward on eval: 3271.0788233271237[0m
[37m[1m[2023-07-16 20:40:05,763][257371] Min Reward on eval: 3271.0788233271237[0m
[37m[1m[2023-07-16 20:40:05,763][257371] Mean Reward across all agents: 3271.0788233271237[0m
[37m[1m[2023-07-16 20:40:05,763][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:40:10,784][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:40:10,790][257371] Reward + Measures: [[3498.46890257    0.2261        0.31380001    0.2246        0.13990001
     3.50807571]
 [3144.36663815    0.22389999    0.3468        0.22430001    0.14690001
     3.35752678]
 [2445.96633148    0.1815        0.26050001    0.20050001    0.1242
     3.14721441]
 ...
 [2596.94631959    0.1973        0.26230001    0.20780002    0.1375
     3.28266644]
 [2189.03356932    0.1815        0.23390003    0.1644        0.1123
     3.19201827]
 [2533.62052915    0.21079998    0.2613        0.20250002    0.147
     3.39951515]][0m
[37m[1m[2023-07-16 20:40:10,790][257371] Max Reward on eval: 3616.0781859681474[0m
[37m[1m[2023-07-16 20:40:10,791][257371] Min Reward on eval: 1602.9944991665893[0m
[37m[1m[2023-07-16 20:40:10,791][257371] Mean Reward across all agents: 2765.758635295881[0m
[37m[1m[2023-07-16 20:40:10,791][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:40:10,797][257371] mean_value=1217.346722189179, max_value=3738.970303635435[0m
[37m[1m[2023-07-16 20:40:10,800][257371] New mean coefficients: [[ 0.59349716 -0.16992807  0.01920944  0.63133377  0.29994857  0.90876096]][0m
[37m[1m[2023-07-16 20:40:10,801][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:40:19,819][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 20:40:19,819][257371] FPS: 425873.96[0m
[36m[2023-07-16 20:40:19,821][257371] itr=13, itrs=2000, Progress: 0.65%[0m
[36m[2023-07-16 20:40:31,494][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-16 20:40:31,494][257371] FPS: 329595.09[0m
[36m[2023-07-16 20:40:35,841][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:40:35,841][257371] Reward + Measures: [[3498.68545637    0.21320033    0.28305233    0.22062367    0.11582232
     3.66149378]][0m
[37m[1m[2023-07-16 20:40:35,841][257371] Max Reward on eval: 3498.685456369356[0m
[37m[1m[2023-07-16 20:40:35,842][257371] Min Reward on eval: 3498.685456369356[0m
[37m[1m[2023-07-16 20:40:35,842][257371] Mean Reward across all agents: 3498.685456369356[0m
[37m[1m[2023-07-16 20:40:35,842][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:40:40,936][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:40:40,937][257371] Reward + Measures: [[3146.60296628    0.22740002    0.3204        0.228         0.15090001
     3.76194453]
 [3151.11235807    0.22049999    0.28640002    0.20420001    0.14109999
     3.66707993]
 [2417.67206004    0.18540001    0.25300002    0.1833        0.12400001
     3.57395816]
 ...
 [2523.49332809    0.17309999    0.25630003    0.1883        0.1028
     3.35745692]
 [3163.59725952    0.2124        0.32449999    0.22579999    0.14060001
     3.54040575]
 [3504.10070803    0.20460001    0.25240001    0.21930002    0.10390001
     3.67281413]][0m
[37m[1m[2023-07-16 20:40:40,937][257371] Max Reward on eval: 3807.414276187867[0m
[37m[1m[2023-07-16 20:40:40,937][257371] Min Reward on eval: 1807.9671545147896[0m
[37m[1m[2023-07-16 20:40:40,937][257371] Mean Reward across all agents: 2976.954573151297[0m
[37m[1m[2023-07-16 20:40:40,938][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:40:40,942][257371] mean_value=565.9247635484952, max_value=3749.602633496837[0m
[37m[1m[2023-07-16 20:40:40,945][257371] New mean coefficients: [[0.5809549  0.32378164 0.03881884 0.98323333 0.5010444  1.2101114 ]][0m
[37m[1m[2023-07-16 20:40:40,946][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:40:50,010][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-16 20:40:50,010][257371] FPS: 423754.25[0m
[36m[2023-07-16 20:40:50,013][257371] itr=14, itrs=2000, Progress: 0.70%[0m
[36m[2023-07-16 20:41:01,734][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-16 20:41:01,734][257371] FPS: 328258.60[0m
[36m[2023-07-16 20:41:06,106][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:41:06,107][257371] Reward + Measures: [[3628.00513804    0.21283333    0.27699667    0.22050966    0.11412366
     3.89659119]][0m
[37m[1m[2023-07-16 20:41:06,107][257371] Max Reward on eval: 3628.0051380383975[0m
[37m[1m[2023-07-16 20:41:06,107][257371] Min Reward on eval: 3628.0051380383975[0m
[37m[1m[2023-07-16 20:41:06,108][257371] Mean Reward across all agents: 3628.0051380383975[0m
[37m[1m[2023-07-16 20:41:06,108][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:41:10,771][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:41:10,771][257371] Reward + Measures: [[2503.9364281     0.18090001    0.2142        0.184         0.1086
     3.55049515]
 [2894.55298231    0.1804        0.22019999    0.19780001    0.08880001
     3.67648482]
 [3238.67022704    0.19340001    0.2362        0.20879999    0.0972
     3.71773267]
 ...
 [3778.29409791    0.22790001    0.29100001    0.235         0.1151
     3.91993403]
 [3645.98550416    0.22620001    0.28169999    0.22660001    0.1098
     3.96876192]
 [3005.67909231    0.20539999    0.31930003    0.205         0.14039999
     3.84274292]][0m
[37m[1m[2023-07-16 20:41:10,772][257371] Max Reward on eval: 3898.4582214423453[0m
[37m[1m[2023-07-16 20:41:10,772][257371] Min Reward on eval: 1184.4845829437022[0m
[37m[1m[2023-07-16 20:41:10,772][257371] Mean Reward across all agents: 3118.6006728219636[0m
[37m[1m[2023-07-16 20:41:10,772][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:41:10,775][257371] mean_value=248.07250140298817, max_value=3727.6840475428457[0m
[37m[1m[2023-07-16 20:41:10,779][257371] New mean coefficients: [[ 0.6009617  -0.21537063  0.18868557  0.7865202  -0.16161269  0.86086106]][0m
[37m[1m[2023-07-16 20:41:10,780][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:41:19,934][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-16 20:41:19,934][257371] FPS: 419562.79[0m
[36m[2023-07-16 20:41:19,936][257371] itr=15, itrs=2000, Progress: 0.75%[0m
[36m[2023-07-16 20:41:34,837][257371] train() took 14.88 seconds to complete[0m
[36m[2023-07-16 20:41:34,837][257371] FPS: 258143.83[0m
[36m[2023-07-16 20:41:40,128][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:41:40,128][257371] Reward + Measures: [[3824.61318849    0.21162266    0.26775399    0.21959431    0.10303
     4.05800915]][0m
[37m[1m[2023-07-16 20:41:40,128][257371] Max Reward on eval: 3824.613188488011[0m
[37m[1m[2023-07-16 20:41:40,129][257371] Min Reward on eval: 3824.613188488011[0m
[37m[1m[2023-07-16 20:41:40,129][257371] Mean Reward across all agents: 3824.613188488011[0m
[37m[1m[2023-07-16 20:41:40,129][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:41:46,161][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:41:46,162][257371] Reward + Measures: [[3768.49264526    0.22420001    0.28579998    0.229         0.1176
     4.15662384]
 [3616.69647216    0.20580001    0.26620004    0.21359999    0.1084
     3.9384191 ]
 [3222.17032625    0.22119999    0.2802        0.20439999    0.1166
     4.02042532]
 ...
 [3491.73051647    0.2077        0.26990005    0.20589998    0.10030001
     4.02191257]
 [3042.04750062    0.1938        0.2529        0.18009999    0.10500001
     3.80002952]
 [3390.25720215    0.21470001    0.27280003    0.20739999    0.1245
     4.02297926]][0m
[37m[1m[2023-07-16 20:41:46,162][257371] Max Reward on eval: 4146.831481933443[0m
[37m[1m[2023-07-16 20:41:46,162][257371] Min Reward on eval: 1971.240238188114[0m
[37m[1m[2023-07-16 20:41:46,162][257371] Mean Reward across all agents: 3364.4342971018564[0m
[37m[1m[2023-07-16 20:41:46,162][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:41:46,167][257371] mean_value=-97.9805552264632, max_value=652.5410490631193[0m
[37m[1m[2023-07-16 20:41:46,170][257371] New mean coefficients: [[ 0.54823554 -0.03182909 -0.05170515  0.50988805 -0.09630089  0.3398195 ]][0m
[37m[1m[2023-07-16 20:41:46,171][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:41:56,103][257371] train() took 9.93 seconds to complete[0m
[36m[2023-07-16 20:41:56,103][257371] FPS: 386684.80[0m
[36m[2023-07-16 20:41:56,106][257371] itr=16, itrs=2000, Progress: 0.80%[0m
[36m[2023-07-16 20:42:07,958][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-16 20:42:07,958][257371] FPS: 324627.74[0m
[36m[2023-07-16 20:42:12,274][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:42:12,275][257371] Reward + Measures: [[3935.40442037    0.20455033    0.24969533    0.21326067    0.08959333
     4.08568096]][0m
[37m[1m[2023-07-16 20:42:12,275][257371] Max Reward on eval: 3935.404420366332[0m
[37m[1m[2023-07-16 20:42:12,275][257371] Min Reward on eval: 3935.404420366332[0m
[37m[1m[2023-07-16 20:42:12,275][257371] Mean Reward across all agents: 3935.404420366332[0m
[37m[1m[2023-07-16 20:42:12,276][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:42:17,385][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:42:17,386][257371] Reward + Measures: [[3986.78970342    0.23559999    0.28990003    0.22650002    0.1024
     4.11639023]
 [3140.9195023     0.178         0.23729999    0.1793        0.1018
     3.87028241]
 [3616.36581421    0.20200001    0.25320002    0.20970002    0.0969
     4.01609278]
 ...
 [3750.62336735    0.20130001    0.25400001    0.2043        0.0952
     3.99108386]
 [2547.56498718    0.17599998    0.26970002    0.19160001    0.13450001
     3.84328198]
 [2600.37656017    0.161         0.22220002    0.17289999    0.10220001
     3.74389648]][0m
[37m[1m[2023-07-16 20:42:17,386][257371] Max Reward on eval: 4339.995025608689[0m
[37m[1m[2023-07-16 20:42:17,386][257371] Min Reward on eval: 2045.4352750700898[0m
[37m[1m[2023-07-16 20:42:17,386][257371] Mean Reward across all agents: 3371.820005406205[0m
[37m[1m[2023-07-16 20:42:17,387][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:42:17,389][257371] mean_value=-257.42153616414265, max_value=596.9479819204462[0m
[37m[1m[2023-07-16 20:42:17,392][257371] New mean coefficients: [[ 0.9745836   0.242217    0.10953759  0.28081024 -0.2054719  -0.06727701]][0m
[37m[1m[2023-07-16 20:42:17,393][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:42:26,450][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-16 20:42:26,450][257371] FPS: 424066.32[0m
[36m[2023-07-16 20:42:26,453][257371] itr=17, itrs=2000, Progress: 0.85%[0m
[36m[2023-07-16 20:42:38,022][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-16 20:42:38,023][257371] FPS: 332539.05[0m
[36m[2023-07-16 20:42:42,262][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:42:42,263][257371] Reward + Measures: [[4099.70590084    0.19711232    0.23791833    0.20816468    0.08145434
     4.04828501]][0m
[37m[1m[2023-07-16 20:42:42,263][257371] Max Reward on eval: 4099.705900838404[0m
[37m[1m[2023-07-16 20:42:42,263][257371] Min Reward on eval: 4099.705900838404[0m
[37m[1m[2023-07-16 20:42:42,264][257371] Mean Reward across all agents: 4099.705900838404[0m
[37m[1m[2023-07-16 20:42:42,264][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:42:47,296][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:42:47,297][257371] Reward + Measures: [[3878.25525853    0.2062        0.24720001    0.19780003    0.08799999
     4.02680922]
 [3683.48897554    0.1997        0.23540001    0.19389999    0.0803
     3.90798378]
 [3918.58247378    0.2113        0.26340002    0.2253        0.1086
     4.17684698]
 ...
 [3549.10462955    0.19579999    0.25180003    0.20770001    0.1081
     3.97998595]
 [3949.14920041    0.2174        0.27360001    0.2115        0.0951
     4.15066099]
 [4384.84982303    0.21540001    0.25920001    0.2192        0.0844
     4.18058252]][0m
[37m[1m[2023-07-16 20:42:47,297][257371] Max Reward on eval: 4583.624481197912[0m
[37m[1m[2023-07-16 20:42:47,297][257371] Min Reward on eval: 2060.1205062776803[0m
[37m[1m[2023-07-16 20:42:47,297][257371] Mean Reward across all agents: 3668.95920984352[0m
[37m[1m[2023-07-16 20:42:47,298][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:42:47,301][257371] mean_value=-83.68563617178368, max_value=644.2527206139289[0m
[37m[1m[2023-07-16 20:42:47,304][257371] New mean coefficients: [[ 0.8787034  -0.21727982  0.23397666 -0.03623456 -0.15486553 -0.3859649 ]][0m
[37m[1m[2023-07-16 20:42:47,305][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:42:56,323][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 20:42:56,323][257371] FPS: 425899.82[0m
[36m[2023-07-16 20:42:56,325][257371] itr=18, itrs=2000, Progress: 0.90%[0m
[36m[2023-07-16 20:43:07,924][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-16 20:43:07,924][257371] FPS: 331692.75[0m
[36m[2023-07-16 20:43:12,188][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:43:12,188][257371] Reward + Measures: [[4164.23066955    0.18568265    0.22087532    0.19753334    0.07124934
     3.86793184]][0m
[37m[1m[2023-07-16 20:43:12,189][257371] Max Reward on eval: 4164.230669547188[0m
[37m[1m[2023-07-16 20:43:12,189][257371] Min Reward on eval: 4164.230669547188[0m
[37m[1m[2023-07-16 20:43:12,189][257371] Mean Reward across all agents: 4164.230669547188[0m
[37m[1m[2023-07-16 20:43:12,189][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:43:17,185][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:43:17,186][257371] Reward + Measures: [[3926.713501      0.21619999    0.25650001    0.23530002    0.10160001
     4.29617548]
 [3514.03338437    0.175         0.20990001    0.18190001    0.07380001
     3.66124225]
 [2329.02405926    0.15049998    0.18239999    0.1539        0.08
     3.52595186]
 ...
 [3154.63131719    0.16239999    0.19309999    0.17940001    0.07480001
     3.48910642]
 [3759.57138828    0.1864        0.21569999    0.1955        0.0799
     3.81583023]
 [3729.95500182    0.18350001    0.23340002    0.1978        0.07539999
     3.85636258]][0m
[37m[1m[2023-07-16 20:43:17,186][257371] Max Reward on eval: 4714.687438932992[0m
[37m[1m[2023-07-16 20:43:17,186][257371] Min Reward on eval: 1923.6889857848175[0m
[37m[1m[2023-07-16 20:43:17,187][257371] Mean Reward across all agents: 3574.9258067409087[0m
[37m[1m[2023-07-16 20:43:17,187][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:43:17,190][257371] mean_value=-55.832747300773796, max_value=753.1169943235482[0m
[37m[1m[2023-07-16 20:43:17,193][257371] New mean coefficients: [[ 0.2730009   0.19403437  0.39112955  0.10970868 -0.14007333 -1.3669972 ]][0m
[37m[1m[2023-07-16 20:43:17,194][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:43:26,236][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 20:43:26,237][257371] FPS: 424747.90[0m
[36m[2023-07-16 20:43:26,239][257371] itr=19, itrs=2000, Progress: 0.95%[0m
[36m[2023-07-16 20:43:37,931][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-16 20:43:37,931][257371] FPS: 329126.23[0m
[36m[2023-07-16 20:43:42,258][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:43:42,258][257371] Reward + Measures: [[3855.18825531    0.16701899    0.19999133    0.17852232    0.06384367
     3.48955011]][0m
[37m[1m[2023-07-16 20:43:42,258][257371] Max Reward on eval: 3855.188255309773[0m
[37m[1m[2023-07-16 20:43:42,259][257371] Min Reward on eval: 3855.188255309773[0m
[37m[1m[2023-07-16 20:43:42,259][257371] Mean Reward across all agents: 3855.188255309773[0m
[37m[1m[2023-07-16 20:43:42,259][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:43:47,289][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:43:47,290][257371] Reward + Measures: [[4636.53610228    0.20510001    0.2493        0.2201        0.0739
     3.87500548]
 [2763.06472017    0.14210001    0.16860001    0.1567        0.0667
     3.10584784]
 [3154.17128371    0.16440001    0.2088        0.1674        0.078
     3.30473876]
 ...
 [4433.14010621    0.19849999    0.23980001    0.2113        0.0811
     3.74642539]
 [2711.62156866    0.1513        0.19320001    0.1605        0.0758
     3.12582183]
 [4368.96427921    0.1953        0.23710001    0.2098        0.0763
     3.79123688]][0m
[37m[1m[2023-07-16 20:43:47,290][257371] Max Reward on eval: 4641.977722174954[0m
[37m[1m[2023-07-16 20:43:47,290][257371] Min Reward on eval: 1990.7861213949975[0m
[37m[1m[2023-07-16 20:43:47,291][257371] Mean Reward across all agents: 3504.286582817283[0m
[37m[1m[2023-07-16 20:43:47,291][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:43:47,295][257371] mean_value=85.56470774997385, max_value=889.568404561488[0m
[37m[1m[2023-07-16 20:43:47,297][257371] New mean coefficients: [[ 0.44835287  0.25149184  0.19754939 -0.17270328 -0.20640334 -1.4477254 ]][0m
[37m[1m[2023-07-16 20:43:47,298][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:43:56,330][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 20:43:56,330][257371] FPS: 425244.48[0m
[36m[2023-07-16 20:43:56,333][257371] itr=20, itrs=2000, Progress: 1.00%[0m
[36m[2023-07-16 20:45:56,974][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-16 20:45:57,039][257371] FPS: 324803.49[0m
[36m[2023-07-16 20:46:01,420][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:46:01,421][257371] Reward + Measures: [[2262.93443202    0.116487      0.13712132    0.11954067    0.05444466
     2.57853031]][0m
[37m[1m[2023-07-16 20:46:01,421][257371] Max Reward on eval: 2262.9344320203854[0m
[37m[1m[2023-07-16 20:46:01,421][257371] Min Reward on eval: 2262.9344320203854[0m
[37m[1m[2023-07-16 20:46:01,422][257371] Mean Reward across all agents: 2262.9344320203854[0m
[37m[1m[2023-07-16 20:46:01,422][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:46:06,657][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:46:06,657][257371] Reward + Measures: [[1612.50676728    0.10860001    0.131         0.1076        0.0474
     2.52596164]
 [3162.84048653    0.14100002    0.16860001    0.1557        0.0581
     2.88112807]
 [1509.68310929    0.1038        0.1138        0.1099        0.0567
     2.35176063]
 ...
 [2454.68822093    0.13460001    0.156         0.1344        0.0604
     2.76157713]
 [1754.48318867    0.1036        0.1201        0.10780001    0.0513
     2.40539479]
 [1737.07004168    0.1001        0.133         0.1112        0.0575
     2.43342066]][0m
[37m[1m[2023-07-16 20:46:06,657][257371] Max Reward on eval: 4313.43930055704[0m
[37m[1m[2023-07-16 20:46:06,658][257371] Min Reward on eval: 466.67829794033895[0m
[37m[1m[2023-07-16 20:46:06,658][257371] Mean Reward across all agents: 2193.416636426049[0m
[37m[1m[2023-07-16 20:46:06,658][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:46:06,662][257371] mean_value=-265.9537041672255, max_value=1314.0490478473803[0m
[37m[1m[2023-07-16 20:46:06,665][257371] New mean coefficients: [[ 1.0453795   0.39649224 -0.406954    0.40939277 -0.2008034  -0.67721725]][0m
[37m[1m[2023-07-16 20:46:06,666][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:46:15,465][257371] train() took 8.80 seconds to complete[0m
[36m[2023-07-16 20:46:15,465][257371] FPS: 436513.11[0m
[36m[2023-07-16 20:46:15,467][257371] itr=21, itrs=2000, Progress: 1.05%[0m
[36m[2023-07-16 20:46:27,230][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-16 20:46:27,230][257371] FPS: 327173.24[0m
[36m[2023-07-16 20:46:31,511][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:46:31,511][257371] Reward + Measures: [[3014.3528745     0.13367499    0.15950334    0.14079033    0.05327167
     2.71565223]][0m
[37m[1m[2023-07-16 20:46:31,511][257371] Max Reward on eval: 3014.3528744959694[0m
[37m[1m[2023-07-16 20:46:31,511][257371] Min Reward on eval: 3014.3528744959694[0m
[37m[1m[2023-07-16 20:46:31,512][257371] Mean Reward across all agents: 3014.3528744959694[0m
[37m[1m[2023-07-16 20:46:31,512][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:46:36,523][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:46:36,528][257371] Reward + Measures: [[2031.52334216    0.1261        0.15609999    0.13689999    0.0759
     2.7388885 ]
 [3594.67277526    0.15180001    0.1974        0.1666        0.0517
     2.88592029]
 [3698.82987977    0.1635        0.20869999    0.1851        0.0656
     3.20674682]
 ...
 [3570.35991282    0.16270001    0.19509999    0.1741        0.0702
     3.04319572]
 [3029.82389068    0.13959999    0.16080001    0.14430001    0.0584
     2.91393137]
 [2985.30038454    0.1441        0.1767        0.152         0.0755
     2.80803943]][0m
[37m[1m[2023-07-16 20:46:36,529][257371] Max Reward on eval: 4526.814361556782[0m
[37m[1m[2023-07-16 20:46:36,529][257371] Min Reward on eval: 1545.4394035994076[0m
[37m[1m[2023-07-16 20:46:36,529][257371] Mean Reward across all agents: 3114.9110441652633[0m
[37m[1m[2023-07-16 20:46:36,529][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:46:36,532][257371] mean_value=-96.48179978590454, max_value=1223.3485710840014[0m
[37m[1m[2023-07-16 20:46:36,535][257371] New mean coefficients: [[ 1.0452248   0.66778696  0.09393555  0.05245849 -0.36070865 -1.1926183 ]][0m
[37m[1m[2023-07-16 20:46:36,536][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:46:45,543][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 20:46:45,543][257371] FPS: 426411.14[0m
[36m[2023-07-16 20:46:45,545][257371] itr=22, itrs=2000, Progress: 1.10%[0m
[36m[2023-07-16 20:46:57,329][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-16 20:46:57,330][257371] FPS: 326473.20[0m
[36m[2023-07-16 20:47:01,557][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:47:01,563][257371] Reward + Measures: [[3998.72407879    0.15289599    0.18432467    0.16208766    0.04797767
     2.85947919]][0m
[37m[1m[2023-07-16 20:47:01,563][257371] Max Reward on eval: 3998.724078789978[0m
[37m[1m[2023-07-16 20:47:01,563][257371] Min Reward on eval: 3998.724078789978[0m
[37m[1m[2023-07-16 20:47:01,564][257371] Mean Reward across all agents: 3998.724078789978[0m
[37m[1m[2023-07-16 20:47:01,564][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:47:06,627][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:47:06,633][257371] Reward + Measures: [[3320.22535712    0.1392        0.16760001    0.1504        0.0531
     2.68239665]
 [3590.68984985    0.14720002    0.17980002    0.1576        0.0466
     2.82189441]
 [3101.20905687    0.13250001    0.15890001    0.13680001    0.0428
     2.60023093]
 ...
 [2080.05038071    0.1163        0.1336        0.1202        0.053
     2.32158279]
 [4256.76476293    0.1596        0.206         0.17050001    0.042
     3.07369733]
 [3953.29923638    0.15339999    0.18160002    0.1603        0.0476
     2.83934069]][0m
[37m[1m[2023-07-16 20:47:06,633][257371] Max Reward on eval: 5010.110626172437[0m
[37m[1m[2023-07-16 20:47:06,633][257371] Min Reward on eval: 1125.1192054286366[0m
[37m[1m[2023-07-16 20:47:06,633][257371] Mean Reward across all agents: 3115.1336339522504[0m
[37m[1m[2023-07-16 20:47:06,633][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:47:06,636][257371] mean_value=-197.87582566521493, max_value=1072.9914140760143[0m
[37m[1m[2023-07-16 20:47:06,638][257371] New mean coefficients: [[ 1.0121189  0.1384561 -0.3576743 -0.4008508 -0.1408341 -0.5580716]][0m
[37m[1m[2023-07-16 20:47:06,639][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:47:15,724][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 20:47:15,724][257371] FPS: 422743.17[0m
[36m[2023-07-16 20:47:15,727][257371] itr=23, itrs=2000, Progress: 1.15%[0m
[36m[2023-07-16 20:47:27,607][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-16 20:47:27,608][257371] FPS: 323963.65[0m
[36m[2023-07-16 20:47:31,907][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:47:31,908][257371] Reward + Measures: [[4733.69938666    0.16770865    0.19863768    0.178803      0.045233
     3.0381763 ]][0m
[37m[1m[2023-07-16 20:47:31,908][257371] Max Reward on eval: 4733.699386662467[0m
[37m[1m[2023-07-16 20:47:31,908][257371] Min Reward on eval: 4733.699386662467[0m
[37m[1m[2023-07-16 20:47:31,909][257371] Mean Reward across all agents: 4733.699386662467[0m
[37m[1m[2023-07-16 20:47:31,909][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:47:36,939][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:47:36,945][257371] Reward + Measures: [[3434.97240441    0.1432        0.15280001    0.14289999    0.0435
     2.64686275]
 [3860.65349201    0.16429999    0.2189        0.17760001    0.0492
     2.84903765]
 [3491.36126898    0.152         0.18179999    0.15769999    0.0676
     2.88238597]
 ...
 [3542.76016997    0.14740001    0.1846        0.1586        0.0512
     2.84050751]
 [4560.06657414    0.17900001    0.2194        0.1973        0.0631
     3.13135529]
 [2905.45140836    0.1292        0.14780001    0.1329        0.0524
     2.63123584]][0m
[37m[1m[2023-07-16 20:47:36,945][257371] Max Reward on eval: 5538.452728294465[0m
[37m[1m[2023-07-16 20:47:36,945][257371] Min Reward on eval: 1504.536876591295[0m
[37m[1m[2023-07-16 20:47:36,946][257371] Mean Reward across all agents: 3840.229679979396[0m
[37m[1m[2023-07-16 20:47:36,946][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:47:36,949][257371] mean_value=-119.99531089096189, max_value=1430.775707351631[0m
[37m[1m[2023-07-16 20:47:36,952][257371] New mean coefficients: [[ 0.26402378  0.14954592 -0.1002562  -0.07964143 -0.2805798  -1.0938752 ]][0m
[37m[1m[2023-07-16 20:47:36,953][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:47:46,129][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-16 20:47:46,129][257371] FPS: 418558.71[0m
[36m[2023-07-16 20:47:46,132][257371] itr=24, itrs=2000, Progress: 1.20%[0m
[36m[2023-07-16 20:47:57,945][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-16 20:47:57,945][257371] FPS: 325726.25[0m
[36m[2023-07-16 20:48:02,225][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:48:02,225][257371] Reward + Measures: [[4470.04398653    0.15592533    0.18344568    0.16385633    0.04291166
     2.7145009 ]][0m
[37m[1m[2023-07-16 20:48:02,225][257371] Max Reward on eval: 4470.043986532063[0m
[37m[1m[2023-07-16 20:48:02,226][257371] Min Reward on eval: 4470.043986532063[0m
[37m[1m[2023-07-16 20:48:02,226][257371] Mean Reward across all agents: 4470.043986532063[0m
[37m[1m[2023-07-16 20:48:02,226][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:48:07,406][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:48:07,412][257371] Reward + Measures: [[2590.81466676    0.12119999    0.1442        0.12080001    0.057
     2.2766912 ]
 [3988.42483517    0.17330001    0.22059999    0.1877        0.0737
     3.05274653]
 [4544.40542605    0.1567        0.18569998    0.1718        0.0475
     2.7575624 ]
 ...
 [3913.23647309    0.1517        0.1954        0.1653        0.05449999
     2.62963343]
 [4390.59048467    0.17660001    0.20310001    0.17709999    0.0508
     3.00325274]
 [3945.32843011    0.16790001    0.20079999    0.1664        0.0611
     2.58940792]][0m
[37m[1m[2023-07-16 20:48:07,412][257371] Max Reward on eval: 5554.34350584331[0m
[37m[1m[2023-07-16 20:48:07,413][257371] Min Reward on eval: 1134.9146499291062[0m
[37m[1m[2023-07-16 20:48:07,414][257371] Mean Reward across all agents: 3633.3624355782904[0m
[37m[1m[2023-07-16 20:48:07,414][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:48:07,422][257371] mean_value=-193.11948849218948, max_value=2194.902741792116[0m
[37m[1m[2023-07-16 20:48:07,426][257371] New mean coefficients: [[ 0.41711104  0.1361661   0.20097974 -0.3617022  -0.31492004 -0.91087055]][0m
[37m[1m[2023-07-16 20:48:07,428][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:48:16,645][257371] train() took 9.22 seconds to complete[0m
[36m[2023-07-16 20:48:16,646][257371] FPS: 416684.23[0m
[36m[2023-07-16 20:48:16,648][257371] itr=25, itrs=2000, Progress: 1.25%[0m
[36m[2023-07-16 20:48:28,516][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-16 20:48:28,516][257371] FPS: 324290.06[0m
[36m[2023-07-16 20:48:32,926][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:48:32,927][257371] Reward + Measures: [[4286.2282348     0.14437667    0.16611899    0.14779334    0.03761233
     2.4430604 ]][0m
[37m[1m[2023-07-16 20:48:32,927][257371] Max Reward on eval: 4286.228234795659[0m
[37m[1m[2023-07-16 20:48:32,927][257371] Min Reward on eval: 4286.228234795659[0m
[37m[1m[2023-07-16 20:48:32,927][257371] Mean Reward across all agents: 4286.228234795659[0m
[37m[1m[2023-07-16 20:48:32,927][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:48:38,015][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:48:38,016][257371] Reward + Measures: [[3167.82133289    0.141         0.1654        0.139         0.0516
     2.25534034]
 [3753.07050515    0.1735        0.21429999    0.18660001    0.071
     2.70192456]
 [3811.81913754    0.1468        0.16849999    0.154         0.0434
     2.44310284]
 ...
 [4771.35104366    0.1556        0.18800001    0.16779999    0.0345
     2.48297477]
 [2683.72201544    0.1093        0.13849999    0.1163        0.0423
     2.1358695 ]
 [3699.75278475    0.14479999    0.17840001    0.16240001    0.0541
     2.70916915]][0m
[37m[1m[2023-07-16 20:48:38,016][257371] Max Reward on eval: 5555.993011407694[0m
[37m[1m[2023-07-16 20:48:38,016][257371] Min Reward on eval: 1361.0045814783196[0m
[37m[1m[2023-07-16 20:48:38,017][257371] Mean Reward across all agents: 3334.82092557842[0m
[37m[1m[2023-07-16 20:48:38,017][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:48:38,020][257371] mean_value=-262.85213762279506, max_value=2395.5527263479266[0m
[37m[1m[2023-07-16 20:48:38,023][257371] New mean coefficients: [[ 0.53362924  0.44293663  0.23905231  0.1725592  -0.27242905 -1.0885724 ]][0m
[37m[1m[2023-07-16 20:48:38,024][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:48:47,199][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-16 20:48:47,199][257371] FPS: 418574.08[0m
[36m[2023-07-16 20:48:47,202][257371] itr=26, itrs=2000, Progress: 1.30%[0m
[36m[2023-07-16 20:48:59,019][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-16 20:48:59,019][257371] FPS: 325569.09[0m
[36m[2023-07-16 20:49:03,330][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:49:03,330][257371] Reward + Measures: [[4606.75308798    0.14575401    0.166963      0.14944033    0.03380933
     2.34551072]][0m
[37m[1m[2023-07-16 20:49:03,331][257371] Max Reward on eval: 4606.753087982648[0m
[37m[1m[2023-07-16 20:49:03,331][257371] Min Reward on eval: 4606.753087982648[0m
[37m[1m[2023-07-16 20:49:03,331][257371] Mean Reward across all agents: 4606.753087982648[0m
[37m[1m[2023-07-16 20:49:03,331][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:49:08,363][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:49:08,364][257371] Reward + Measures: [[2877.61861034    0.1243        0.14310001    0.1207        0.0496
     2.05609822]
 [4509.8840561     0.17250001    0.21390001    0.18499999    0.0715
     2.7530725 ]
 [4485.0967712     0.15710001    0.1892        0.16509999    0.04830001
     2.54195023]
 ...
 [4544.98925023    0.1494        0.1663        0.16260001    0.0401
     2.3979466 ]
 [3585.02771383    0.13680001    0.1596        0.1357        0.0444
     2.2469151 ]
 [3870.43425749    0.15740001    0.20729999    0.1691        0.0636
     2.65113139]][0m
[37m[1m[2023-07-16 20:49:08,364][257371] Max Reward on eval: 5940.363647459861[0m
[37m[1m[2023-07-16 20:49:08,364][257371] Min Reward on eval: 1284.8420219111024[0m
[37m[1m[2023-07-16 20:49:08,364][257371] Mean Reward across all agents: 3450.084663508508[0m
[37m[1m[2023-07-16 20:49:08,365][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:49:08,367][257371] mean_value=-254.27438724841218, max_value=1757.3184253678146[0m
[37m[1m[2023-07-16 20:49:08,370][257371] New mean coefficients: [[ 0.5209274   0.15272799  0.24176805  0.2666786  -0.11300783 -0.90146077]][0m
[37m[1m[2023-07-16 20:49:08,371][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:49:17,487][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-16 20:49:17,487][257371] FPS: 421304.17[0m
[36m[2023-07-16 20:49:17,489][257371] itr=27, itrs=2000, Progress: 1.35%[0m
[36m[2023-07-16 20:49:29,188][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-16 20:49:29,188][257371] FPS: 328872.12[0m
[36m[2023-07-16 20:49:33,503][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:49:33,509][257371] Reward + Measures: [[5189.12524663    0.15328968    0.172317      0.15741734    0.03031467
     2.33579707]][0m
[37m[1m[2023-07-16 20:49:33,509][257371] Max Reward on eval: 5189.125246628322[0m
[37m[1m[2023-07-16 20:49:33,509][257371] Min Reward on eval: 5189.125246628322[0m
[37m[1m[2023-07-16 20:49:33,509][257371] Mean Reward across all agents: 5189.125246628322[0m
[37m[1m[2023-07-16 20:49:33,510][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:49:38,479][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:49:38,480][257371] Reward + Measures: [[4769.34843453    0.1434        0.17359999    0.1538        0.0303
     2.21800494]
 [4864.7488632     0.1586        0.18170001    0.17040001    0.0374
     2.54853344]
 [4158.00907897    0.14070001    0.17200001    0.15500002    0.0444
     2.27938652]
 ...
 [5103.31433294    0.16080001    0.20030001    0.1741        0.0382
     2.53236318]
 [4620.66458509    0.152         0.17560002    0.1564        0.0412
     2.26731157]
 [4822.85836795    0.15269999    0.18890001    0.1604        0.0339
     2.36495781]][0m
[37m[1m[2023-07-16 20:49:38,480][257371] Max Reward on eval: 6189.200012189988[0m
[37m[1m[2023-07-16 20:49:38,480][257371] Min Reward on eval: 1140.5620384460315[0m
[37m[1m[2023-07-16 20:49:38,480][257371] Mean Reward across all agents: 4084.7465712268613[0m
[37m[1m[2023-07-16 20:49:38,481][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:49:38,484][257371] mean_value=30.5482680843478, max_value=2929.907173122722[0m
[37m[1m[2023-07-16 20:49:38,487][257371] New mean coefficients: [[ 0.24056587 -0.12899333  0.4865831  -0.09403968  0.19001494 -0.79988337]][0m
[37m[1m[2023-07-16 20:49:38,488][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:49:47,531][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 20:49:47,531][257371] FPS: 424698.80[0m
[36m[2023-07-16 20:49:47,534][257371] itr=28, itrs=2000, Progress: 1.40%[0m
[36m[2023-07-16 20:49:59,424][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-16 20:49:59,424][257371] FPS: 323556.95[0m
[36m[2023-07-16 20:50:03,726][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:50:03,726][257371] Reward + Measures: [[5286.41393955    0.15292899    0.165079      0.15535501    0.02900867
     2.1672852 ]][0m
[37m[1m[2023-07-16 20:50:03,726][257371] Max Reward on eval: 5286.413939552015[0m
[37m[1m[2023-07-16 20:50:03,726][257371] Min Reward on eval: 5286.413939552015[0m
[37m[1m[2023-07-16 20:50:03,727][257371] Mean Reward across all agents: 5286.413939552015[0m
[37m[1m[2023-07-16 20:50:03,727][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:50:08,763][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:50:08,769][257371] Reward + Measures: [[5350.74243161    0.17099999    0.21630001    0.1919        0.0451
     2.42627025]
 [5312.41295629    0.1611        0.18969999    0.1714        0.0338
     2.28194022]
 [6317.78778076    0.1779        0.19840001    0.1851        0.0255
     2.64979291]
 ...
 [4595.35680008    0.1516        0.16309999    0.1507        0.0354
     2.1115098 ]
 [4467.23230934    0.16140001    0.19240001    0.1639        0.0453
     2.41311765]
 [4129.4859085     0.1384        0.1559        0.13599999    0.0366
     2.01237798]][0m
[37m[1m[2023-07-16 20:50:08,769][257371] Max Reward on eval: 6386.2235107227[0m
[37m[1m[2023-07-16 20:50:08,769][257371] Min Reward on eval: 1077.9519119149074[0m
[37m[1m[2023-07-16 20:50:08,770][257371] Mean Reward across all agents: 4203.865768335757[0m
[37m[1m[2023-07-16 20:50:08,770][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:50:08,774][257371] mean_value=47.97145245639403, max_value=2578.154314586152[0m
[37m[1m[2023-07-16 20:50:08,776][257371] New mean coefficients: [[ 0.48586637  0.04542704  0.28652    -0.23655528 -0.16612993 -0.9417907 ]][0m
[37m[1m[2023-07-16 20:50:08,777][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:50:17,814][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 20:50:17,814][257371] FPS: 425018.89[0m
[36m[2023-07-16 20:50:17,816][257371] itr=29, itrs=2000, Progress: 1.45%[0m
[36m[2023-07-16 20:50:29,376][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-16 20:50:29,376][257371] FPS: 332825.81[0m
[36m[2023-07-16 20:50:33,672][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:50:33,672][257371] Reward + Measures: [[5367.21897512    0.15205733    0.16218667    0.15341033    0.02828567
     2.06484723]][0m
[37m[1m[2023-07-16 20:50:33,672][257371] Max Reward on eval: 5367.218975118106[0m
[37m[1m[2023-07-16 20:50:33,672][257371] Min Reward on eval: 5367.218975118106[0m
[37m[1m[2023-07-16 20:50:33,673][257371] Mean Reward across all agents: 5367.218975118106[0m
[37m[1m[2023-07-16 20:50:33,673][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:50:38,810][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:50:38,810][257371] Reward + Measures: [[4318.22652051    0.142         0.15000001    0.1296        0.0296
     1.86063409]
 [5255.4238739     0.1628        0.20410001    0.18050002    0.049
     2.27723122]
 [4517.35604851    0.1531        0.17120001    0.1459        0.0429
     2.03764081]
 ...
 [4228.60185242    0.14840001    0.16770001    0.1451        0.0401
     2.01502156]
 [4248.27399636    0.14950001    0.17670001    0.1568        0.0455
     2.1548965 ]
 [5752.76550296    0.18260001    0.23670001    0.19230001    0.0437
     2.53585267]][0m
[37m[1m[2023-07-16 20:50:38,811][257371] Max Reward on eval: 6231.710174539592[0m
[37m[1m[2023-07-16 20:50:38,811][257371] Min Reward on eval: 1025.7235527249984[0m
[37m[1m[2023-07-16 20:50:38,811][257371] Mean Reward across all agents: 3643.8490081640643[0m
[37m[1m[2023-07-16 20:50:38,811][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:50:38,814][257371] mean_value=-520.6091322636986, max_value=2487.9816894177347[0m
[37m[1m[2023-07-16 20:50:38,817][257371] New mean coefficients: [[ 0.72147644 -0.24747558 -0.15714961 -0.09398282 -0.01917107 -1.2244878 ]][0m
[37m[1m[2023-07-16 20:50:38,818][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:50:47,789][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-16 20:50:47,789][257371] FPS: 428096.50[0m
[36m[2023-07-16 20:50:47,792][257371] itr=30, itrs=2000, Progress: 1.50%[0m
[37m[1m[2023-07-16 20:53:02,373][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000010[0m
[36m[2023-07-16 20:53:14,699][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-16 20:53:14,699][257371] FPS: 326753.81[0m
[36m[2023-07-16 20:53:18,871][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:53:18,872][257371] Reward + Measures: [[5935.76018818    0.16031501    0.16638167    0.16442832    0.02474633
     2.14861107]][0m
[37m[1m[2023-07-16 20:53:18,872][257371] Max Reward on eval: 5935.760188177223[0m
[37m[1m[2023-07-16 20:53:18,872][257371] Min Reward on eval: 5935.760188177223[0m
[37m[1m[2023-07-16 20:53:18,872][257371] Mean Reward across all agents: 5935.760188177223[0m
[37m[1m[2023-07-16 20:53:18,873][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:53:23,852][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:53:23,852][257371] Reward + Measures: [[5274.66926573    0.1567        0.15820001    0.1533        0.0276
     2.08137202]
 [5038.28490452    0.1508        0.1613        0.1575        0.0333
     2.12639403]
 [4644.45571893    0.16760002    0.1939        0.17730001    0.0503
     2.33426452]
 ...
 [4849.68562318    0.1811        0.1997        0.1884        0.0484
     2.56032562]
 [4181.5881348     0.15989999    0.21529999    0.18480001    0.0681
     2.14058232]
 [5671.89468381    0.1717        0.1869        0.1781        0.0379
     2.3849721 ]][0m
[37m[1m[2023-07-16 20:53:23,852][257371] Max Reward on eval: 6654.84252927471[0m
[37m[1m[2023-07-16 20:53:23,853][257371] Min Reward on eval: 3111.7036819510163[0m
[37m[1m[2023-07-16 20:53:23,853][257371] Mean Reward across all agents: 5070.950762790162[0m
[37m[1m[2023-07-16 20:53:23,853][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:53:23,856][257371] mean_value=-211.8387501467164, max_value=1642.4978638097955[0m
[37m[1m[2023-07-16 20:53:23,859][257371] New mean coefficients: [[ 0.42565697 -0.27291316 -0.42483965 -0.16115387 -0.03967493 -0.6775197 ]][0m
[37m[1m[2023-07-16 20:53:23,860][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:53:32,670][257371] train() took 8.81 seconds to complete[0m
[36m[2023-07-16 20:53:32,670][257371] FPS: 435930.09[0m
[36m[2023-07-16 20:53:32,672][257371] itr=31, itrs=2000, Progress: 1.55%[0m
[36m[2023-07-16 20:53:44,263][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-16 20:53:44,263][257371] FPS: 331930.50[0m
[36m[2023-07-16 20:53:48,555][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:53:48,555][257371] Reward + Measures: [[6206.80573382    0.16347133    0.16583666    0.16703366    0.02393433
     2.13571   ]][0m
[37m[1m[2023-07-16 20:53:48,555][257371] Max Reward on eval: 6206.805733821739[0m
[37m[1m[2023-07-16 20:53:48,556][257371] Min Reward on eval: 6206.805733821739[0m
[37m[1m[2023-07-16 20:53:48,556][257371] Mean Reward across all agents: 6206.805733821739[0m
[37m[1m[2023-07-16 20:53:48,556][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:53:53,589][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:53:53,590][257371] Reward + Measures: [[5176.93765262    0.18260001    0.2106        0.20220001    0.0585
     2.44035149]
 [6016.49642946    0.16949999    0.1806        0.1723        0.0267
     2.28140521]
 [5674.04113003    0.15640001    0.16159999    0.1569        0.026
     2.10561633]
 ...
 [5059.66140747    0.1732        0.1797        0.17680001    0.0492
     2.25923204]
 [6763.89050292    0.1751        0.17760001    0.184         0.0155
     2.29206443]
 [5344.94638059    0.1611        0.17480001    0.1653        0.0319
     2.12745118]][0m
[37m[1m[2023-07-16 20:53:53,590][257371] Max Reward on eval: 6786.94921878007[0m
[37m[1m[2023-07-16 20:53:53,590][257371] Min Reward on eval: 2373.2130050237292[0m
[37m[1m[2023-07-16 20:53:53,590][257371] Mean Reward across all agents: 5308.589770055806[0m
[37m[1m[2023-07-16 20:53:53,591][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:53:53,593][257371] mean_value=-419.72289910967606, max_value=1065.3445094925391[0m
[37m[1m[2023-07-16 20:53:53,596][257371] New mean coefficients: [[ 0.1576255  -0.17546779 -0.2396486  -0.05052678  0.11316547 -0.620156  ]][0m
[37m[1m[2023-07-16 20:53:53,597][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:54:02,572][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-16 20:54:02,573][257371] FPS: 427903.38[0m
[36m[2023-07-16 20:54:02,575][257371] itr=32, itrs=2000, Progress: 1.60%[0m
[36m[2023-07-16 20:54:14,196][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-16 20:54:14,196][257371] FPS: 331163.20[0m
[36m[2023-07-16 20:54:18,544][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:54:18,544][257371] Reward + Measures: [[6060.70831191    0.15710066    0.15597634    0.15866767    0.02180067
     1.99117124]][0m
[37m[1m[2023-07-16 20:54:18,544][257371] Max Reward on eval: 6060.708311909252[0m
[37m[1m[2023-07-16 20:54:18,544][257371] Min Reward on eval: 6060.708311909252[0m
[37m[1m[2023-07-16 20:54:18,544][257371] Mean Reward across all agents: 6060.708311909252[0m
[37m[1m[2023-07-16 20:54:18,545][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:54:23,650][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:54:23,651][257371] Reward + Measures: [[2209.00154501    0.15550001    0.1988        0.14750001    0.0957
     2.42837501]
 [2394.75531386    0.11229999    0.11910001    0.113         0.0586
     1.61836374]
 [4864.30438234    0.1873        0.2149        0.18970001    0.05470001
     2.52961636]
 ...
 [3636.03617099    0.15659998    0.17999999    0.17300001    0.0635
     2.00784349]
 [5344.12930298    0.19770001    0.2462        0.21890001    0.075
     2.8070817 ]
 [2490.39981458    0.15350001    0.15650001    0.1533        0.0908
     2.20865178]][0m
[37m[1m[2023-07-16 20:54:23,651][257371] Max Reward on eval: 6471.073974554613[0m
[37m[1m[2023-07-16 20:54:23,651][257371] Min Reward on eval: 1605.0694122523068[0m
[37m[1m[2023-07-16 20:54:23,652][257371] Mean Reward across all agents: 3996.5628998278844[0m
[37m[1m[2023-07-16 20:54:23,652][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:54:23,654][257371] mean_value=-1761.490100248502, max_value=1572.4397862742176[0m
[37m[1m[2023-07-16 20:54:23,656][257371] New mean coefficients: [[ 0.28123412 -0.12531026 -0.12522244  0.16179848 -0.04794143 -0.8988414 ]][0m
[37m[1m[2023-07-16 20:54:23,657][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:54:32,829][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-16 20:54:32,829][257371] FPS: 418751.25[0m
[36m[2023-07-16 20:54:32,832][257371] itr=33, itrs=2000, Progress: 1.65%[0m
[36m[2023-07-16 20:54:44,470][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-16 20:54:44,470][257371] FPS: 330648.15[0m
[36m[2023-07-16 20:54:48,726][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:54:48,726][257371] Reward + Measures: [[6283.70635338    0.15732333    0.153064      0.159762      0.020001
     1.92508304]][0m
[37m[1m[2023-07-16 20:54:48,726][257371] Max Reward on eval: 6283.706353377476[0m
[37m[1m[2023-07-16 20:54:48,726][257371] Min Reward on eval: 6283.706353377476[0m
[37m[1m[2023-07-16 20:54:48,727][257371] Mean Reward across all agents: 6283.706353377476[0m
[37m[1m[2023-07-16 20:54:48,727][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:54:53,929][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:54:53,929][257371] Reward + Measures: [[2424.96895603    0.12460001    0.13130002    0.10520001    0.0519
     1.5605036 ]
 [1301.21888925    0.1158        0.1806        0.14910001    0.1186
     2.9452529 ]
 [2552.23921963    0.12080001    0.138         0.11180001    0.0483
     1.59074712]
 ...
 [5890.19445798    0.1551        0.1602        0.1576        0.022
     1.87802434]
 [2281.70085149    0.1337        0.17150001    0.14430001    0.0961
     2.18084669]
 [4199.61772156    0.16589999    0.17390001    0.17040001    0.054
     2.27391791]][0m
[37m[1m[2023-07-16 20:54:53,930][257371] Max Reward on eval: 6854.8231200850805[0m
[37m[1m[2023-07-16 20:54:53,930][257371] Min Reward on eval: 729.8908524567262[0m
[37m[1m[2023-07-16 20:54:53,930][257371] Mean Reward across all agents: 3777.809500849598[0m
[37m[1m[2023-07-16 20:54:53,930][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:54:53,932][257371] mean_value=-2170.625609501849, max_value=1610.4725019546322[0m
[37m[1m[2023-07-16 20:54:53,935][257371] New mean coefficients: [[ 0.1091187  -0.18022737  0.02391095  0.13086241 -0.08201611 -0.74439144]][0m
[37m[1m[2023-07-16 20:54:53,936][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:55:02,948][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 20:55:02,948][257371] FPS: 426152.03[0m
[36m[2023-07-16 20:55:02,951][257371] itr=34, itrs=2000, Progress: 1.70%[0m
[36m[2023-07-16 20:55:14,658][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-16 20:55:14,658][257371] FPS: 328656.00[0m
[36m[2023-07-16 20:55:19,010][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:55:19,010][257371] Reward + Measures: [[2400.9532303     0.111483      0.11915267    0.102202      0.05444634
     1.25251722]][0m
[37m[1m[2023-07-16 20:55:19,010][257371] Max Reward on eval: 2400.9532302976913[0m
[37m[1m[2023-07-16 20:55:19,011][257371] Min Reward on eval: 2400.9532302976913[0m
[37m[1m[2023-07-16 20:55:19,011][257371] Mean Reward across all agents: 2400.9532302976913[0m
[37m[1m[2023-07-16 20:55:19,011][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:55:24,024][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:55:24,025][257371] Reward + Measures: [[1267.42201516    0.16770001    0.22399998    0.14          0.1347
     2.44537592]
 [1632.83523751    0.1076        0.11960001    0.0981        0.0635
     1.24357307]
 [1604.00370408    0.16230001    0.22430001    0.1486        0.15350001
     2.80506158]
 ...
 [1951.75559238    0.123         0.1323        0.1092        0.0665
     1.32838237]
 [ 822.79184627    0.23530002    0.36990002    0.15179999    0.252
     3.08613825]
 [1377.91309355    0.0914        0.101         0.09299999    0.0728
     1.22040546]][0m
[37m[1m[2023-07-16 20:55:24,025][257371] Max Reward on eval: 4516.606994653586[0m
[37m[1m[2023-07-16 20:55:24,026][257371] Min Reward on eval: 479.6274585686857[0m
[37m[1m[2023-07-16 20:55:24,026][257371] Mean Reward across all agents: 1926.9812908569436[0m
[37m[1m[2023-07-16 20:55:24,026][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:55:24,029][257371] mean_value=-2460.053757596926, max_value=2204.2407780055887[0m
[37m[1m[2023-07-16 20:55:24,031][257371] New mean coefficients: [[ 0.10827124 -0.0711695   0.26803753  0.23769955  0.03917651 -0.8864021 ]][0m
[37m[1m[2023-07-16 20:55:24,032][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:55:33,035][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-16 20:55:33,035][257371] FPS: 426610.71[0m
[36m[2023-07-16 20:55:33,038][257371] itr=35, itrs=2000, Progress: 1.75%[0m
[36m[2023-07-16 20:55:44,792][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-16 20:55:44,792][257371] FPS: 327367.58[0m
[36m[2023-07-16 20:55:49,145][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:55:49,145][257371] Reward + Measures: [[2579.54424068    0.17253999    0.27514932    0.22762801    0.13266768
     2.05331111]][0m
[37m[1m[2023-07-16 20:55:49,145][257371] Max Reward on eval: 2579.5442406846973[0m
[37m[1m[2023-07-16 20:55:49,146][257371] Min Reward on eval: 2579.5442406846973[0m
[37m[1m[2023-07-16 20:55:49,146][257371] Mean Reward across all agents: 2579.5442406846973[0m
[37m[1m[2023-07-16 20:55:49,146][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:55:54,219][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:55:54,220][257371] Reward + Measures: [[2465.59774777    0.1487        0.27649999    0.21139999    0.11110001
     1.96875274]
 [2212.38780977    0.14479999    0.27629998    0.23899999    0.1212
     2.00705934]
 [2329.55935094    0.1477        0.26730001    0.22319999    0.1069
     1.98219514]
 ...
 [2539.11384578    0.1955        0.3026        0.24370001    0.15480001
     2.20572376]
 [1696.50861356    0.1229        0.18089999    0.15150002    0.09770001
     1.74909437]
 [2160.07954689    0.14000002    0.2545        0.20030001    0.113
     2.08067966]][0m
[37m[1m[2023-07-16 20:55:54,220][257371] Max Reward on eval: 3178.6478423817084[0m
[37m[1m[2023-07-16 20:55:54,221][257371] Min Reward on eval: 874.8300380514935[0m
[37m[1m[2023-07-16 20:55:54,221][257371] Mean Reward across all agents: 2308.5731929744697[0m
[37m[1m[2023-07-16 20:55:54,221][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:55:54,222][257371] mean_value=-3860.079986229061, max_value=-2960.119178544625[0m
[36m[2023-07-16 20:55:54,225][257371] XNES is restarting with a new solution whose measures are [0.19090001 0.26440001 0.21470001 0.06820001 3.48564386] and objective is 4526.814361556782[0m
[36m[2023-07-16 20:55:54,226][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-16 20:55:54,228][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-16 20:55:54,229][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:56:03,293][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-16 20:56:03,293][257371] FPS: 423717.77[0m
[36m[2023-07-16 20:56:03,295][257371] itr=36, itrs=2000, Progress: 1.80%[0m
[36m[2023-07-16 20:56:15,121][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-16 20:56:15,122][257371] FPS: 325328.01[0m
[36m[2023-07-16 20:56:19,431][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:56:19,437][257371] Reward + Measures: [[2236.58451315    0.13414033    0.1645        0.13723466    0.07958534
     2.66736794]][0m
[37m[1m[2023-07-16 20:56:19,438][257371] Max Reward on eval: 2236.5845131519404[0m
[37m[1m[2023-07-16 20:56:19,438][257371] Min Reward on eval: 2236.5845131519404[0m
[37m[1m[2023-07-16 20:56:19,439][257371] Mean Reward across all agents: 2236.5845131519404[0m
[37m[1m[2023-07-16 20:56:19,440][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:56:24,525][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:56:24,531][257371] Reward + Measures: [[2251.90331272    0.17060001    0.22399998    0.1743        0.1069
     3.12649155]
 [1802.17714687    0.14560001    0.1679        0.1417        0.09550001
     2.69866538]
 [2208.02902222    0.156         0.21100001    0.1691        0.105
     3.01916385]
 ...
 [2647.35646055    0.1708        0.21519999    0.17030001    0.0856
     3.05564475]
 [2312.17240431    0.15580001    0.22350001    0.1725        0.10089999
     3.17598844]
 [2080.16819187    0.16410001    0.21040002    0.1673        0.1099
     3.02764583]][0m
[37m[1m[2023-07-16 20:56:24,532][257371] Max Reward on eval: 3463.642913864227[0m
[37m[1m[2023-07-16 20:56:24,533][257371] Min Reward on eval: 690.5493183020502[0m
[37m[1m[2023-07-16 20:56:24,533][257371] Mean Reward across all agents: 1908.9726298911232[0m
[37m[1m[2023-07-16 20:56:24,534][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:56:24,536][257371] mean_value=-3642.5029792904907, max_value=-327.9862055787489[0m
[36m[2023-07-16 20:56:24,541][257371] XNES is restarting with a new solution whose measures are [0.1787     0.19020002 0.18980001 0.0291     2.35036445] and objective is 6410.703674319759[0m
[36m[2023-07-16 20:56:24,543][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-16 20:56:24,547][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-16 20:56:24,549][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:56:33,668][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-16 20:56:33,668][257371] FPS: 421188.08[0m
[36m[2023-07-16 20:56:33,671][257371] itr=37, itrs=2000, Progress: 1.85%[0m
[36m[2023-07-16 20:56:45,545][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-16 20:56:45,545][257371] FPS: 324111.72[0m
[36m[2023-07-16 20:56:49,908][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:56:49,913][257371] Reward + Measures: [[4471.11927748    0.14390232    0.15255032    0.14218034    0.03745433
     1.95797575]][0m
[37m[1m[2023-07-16 20:56:49,914][257371] Max Reward on eval: 4471.119277481767[0m
[37m[1m[2023-07-16 20:56:49,914][257371] Min Reward on eval: 4471.119277481767[0m
[37m[1m[2023-07-16 20:56:49,914][257371] Mean Reward across all agents: 4471.119277481767[0m
[37m[1m[2023-07-16 20:56:49,915][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:56:55,002][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:56:55,002][257371] Reward + Measures: [[3387.33258818    0.18260001    0.21859999    0.1793        0.0891
     2.62402797]
 [1068.91115949    0.09460001    0.1069        0.0956        0.0674
     1.58603179]
 [1550.43784713    0.1089        0.11650001    0.1063        0.0568
     1.85334325]
 ...
 [1609.57492061    0.09940001    0.11129999    0.102         0.0602
     1.57938051]
 [2771.89728168    0.14039999    0.15710001    0.1391        0.0544
     2.18126917]
 [1619.83993147    0.1178        0.1175        0.09910001    0.04940001
     1.88789785]][0m
[37m[1m[2023-07-16 20:56:55,002][257371] Max Reward on eval: 5062.471893346798[0m
[37m[1m[2023-07-16 20:56:55,003][257371] Min Reward on eval: 721.0729103413411[0m
[37m[1m[2023-07-16 20:56:55,003][257371] Mean Reward across all agents: 1921.4263810279406[0m
[37m[1m[2023-07-16 20:56:55,003][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:56:55,004][257371] mean_value=-4069.3752825135093, max_value=-388.00894540753825[0m
[36m[2023-07-16 20:56:55,007][257371] XNES is restarting with a new solution whose measures are [0.18799999 0.22850001 0.2076     0.0552     3.05969691] and objective is 5224.113098161831[0m
[36m[2023-07-16 20:56:55,008][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-16 20:56:55,010][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-16 20:56:55,011][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:57:04,016][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-16 20:57:04,016][257371] FPS: 426520.30[0m
[36m[2023-07-16 20:57:04,019][257371] itr=38, itrs=2000, Progress: 1.90%[0m
[36m[2023-07-16 20:57:15,631][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-16 20:57:15,631][257371] FPS: 331427.28[0m
[36m[2023-07-16 20:57:19,975][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:57:19,980][257371] Reward + Measures: [[2777.43193698    0.14742933    0.19985934    0.161707      0.06853667
     2.46603799]][0m
[37m[1m[2023-07-16 20:57:19,981][257371] Max Reward on eval: 2777.431936981975[0m
[37m[1m[2023-07-16 20:57:19,981][257371] Min Reward on eval: 2777.431936981975[0m
[37m[1m[2023-07-16 20:57:19,981][257371] Mean Reward across all agents: 2777.431936981975[0m
[37m[1m[2023-07-16 20:57:19,981][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:57:25,161][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:57:25,166][257371] Reward + Measures: [[363.25199067   0.1408       0.13700001   0.1356       0.14329998
    4.3116312 ]
 [212.26895703   0.1684       0.23980001   0.23440002   0.27550003
    4.37025404]
 [100.63106017   0.19759999   0.24819998   0.21159999   0.30740002
    5.1662879 ]
 ...
 [352.82404991   0.12180001   0.1461       0.141        0.11240001
    4.95600843]
 [282.25190686   0.09450001   0.1348       0.13900001   0.10539999
    5.26164293]
 [172.2959769    0.0803       0.1635       0.15480001   0.10650001
    5.36253309]][0m
[37m[1m[2023-07-16 20:57:25,166][257371] Max Reward on eval: 2612.2834320520983[0m
[37m[1m[2023-07-16 20:57:25,167][257371] Min Reward on eval: -73.88252272622194[0m
[37m[1m[2023-07-16 20:57:25,167][257371] Mean Reward across all agents: 227.1433408948264[0m
[37m[1m[2023-07-16 20:57:25,167][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:57:25,179][257371] mean_value=-317.130388172342, max_value=995.8212910026311[0m
[37m[1m[2023-07-16 20:57:25,182][257371] New mean coefficients: [[-0.1446091   0.41431892 -0.45421323 -1.9104707  -2.3969314  -0.29512167]][0m
[37m[1m[2023-07-16 20:57:25,183][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:57:34,160][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-16 20:57:34,160][257371] FPS: 427833.95[0m
[36m[2023-07-16 20:57:34,163][257371] itr=39, itrs=2000, Progress: 1.95%[0m
[36m[2023-07-16 20:57:46,129][257371] train() took 11.94 seconds to complete[0m
[36m[2023-07-16 20:57:46,129][257371] FPS: 321542.59[0m
[36m[2023-07-16 20:57:50,420][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:57:50,426][257371] Reward + Measures: [[911.65270656   0.088448     0.11872833   0.10427632   0.04948333
    1.99333525]][0m
[37m[1m[2023-07-16 20:57:50,426][257371] Max Reward on eval: 911.6527065582235[0m
[37m[1m[2023-07-16 20:57:50,427][257371] Min Reward on eval: 911.6527065582235[0m
[37m[1m[2023-07-16 20:57:50,427][257371] Mean Reward across all agents: 911.6527065582235[0m
[37m[1m[2023-07-16 20:57:50,427][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:57:55,438][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 20:57:55,443][257371] Reward + Measures: [[ 99.57365845   0.0994       0.0875       0.0992       0.0754
    5.64630365]
 [315.38285545   0.21280001   0.34         0.2036       0.3026
    5.34244633]
 [177.91028471   0.11009999   0.11900001   0.1175       0.1127
    5.67994022]
 ...
 [340.6887474    0.16779999   0.1675       0.14619999   0.13000001
    5.01694965]
 [ 48.30824203   0.1135       0.101        0.1196       0.0932
    6.32523346]
 [320.06684851   0.13530001   0.1251       0.1425       0.0824
    4.2426362 ]][0m
[37m[1m[2023-07-16 20:57:55,444][257371] Max Reward on eval: 1897.7332153515424[0m
[37m[1m[2023-07-16 20:57:55,444][257371] Min Reward on eval: -59.89297485919669[0m
[37m[1m[2023-07-16 20:57:55,444][257371] Mean Reward across all agents: 248.28059345498292[0m
[37m[1m[2023-07-16 20:57:55,444][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 20:57:55,452][257371] mean_value=-587.3832980127947, max_value=767.3529048109892[0m
[37m[1m[2023-07-16 20:57:55,454][257371] New mean coefficients: [[ 0.21681854 -1.8664559  -0.19544098 -1.2279984  -1.3882736  -0.09029749]][0m
[37m[1m[2023-07-16 20:57:55,455][257371] Moving the mean solution point...[0m
[36m[2023-07-16 20:58:04,461][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-16 20:58:04,461][257371] FPS: 426490.68[0m
[36m[2023-07-16 20:58:04,463][257371] itr=40, itrs=2000, Progress: 2.00%[0m
[37m[1m[2023-07-16 21:00:19,969][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000020[0m
[36m[2023-07-16 21:00:32,521][257371] train() took 11.95 seconds to complete[0m
[36m[2023-07-16 21:00:32,521][257371] FPS: 321326.11[0m
[36m[2023-07-16 21:00:36,787][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:00:36,793][257371] Reward + Measures: [[836.81050571   0.08549767   0.11774066   0.10180233   0.04866434
    1.93655455]][0m
[37m[1m[2023-07-16 21:00:36,793][257371] Max Reward on eval: 836.8105057129159[0m
[37m[1m[2023-07-16 21:00:36,794][257371] Min Reward on eval: 836.8105057129159[0m
[37m[1m[2023-07-16 21:00:36,794][257371] Mean Reward across all agents: 836.8105057129159[0m
[37m[1m[2023-07-16 21:00:36,794][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:00:41,736][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:00:41,736][257371] Reward + Measures: [[538.98820782   0.1017       0.1212       0.10340001   0.0855
    3.64900565]
 [119.60359445   0.21599999   0.23200002   0.24290001   0.28369999
    6.14953804]
 [300.31279162   0.11589999   0.16960001   0.1151       0.12049999
    4.02125263]
 ...
 [170.54231445   0.138        0.30760002   0.1997       0.28710005
    5.30247355]
 [257.60496617   0.24319999   0.2103       0.21300001   0.22080003
    5.00443411]
 [130.43810251   0.1201       0.11180001   0.10339999   0.1027
    5.1838975 ]][0m
[37m[1m[2023-07-16 21:00:41,736][257371] Max Reward on eval: 1374.0742874229327[0m
[37m[1m[2023-07-16 21:00:41,737][257371] Min Reward on eval: -218.7433037525043[0m
[37m[1m[2023-07-16 21:00:41,737][257371] Mean Reward across all agents: 247.0420295383463[0m
[37m[1m[2023-07-16 21:00:41,737][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:00:41,744][257371] mean_value=-955.22707864348, max_value=991.6934642806649[0m
[37m[1m[2023-07-16 21:00:41,747][257371] New mean coefficients: [[ 0.42894983 -2.1529381   0.83562887 -1.1689584  -0.6078554  -0.0401522 ]][0m
[37m[1m[2023-07-16 21:00:41,747][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:00:50,652][257371] train() took 8.90 seconds to complete[0m
[36m[2023-07-16 21:00:50,653][257371] FPS: 431298.28[0m
[36m[2023-07-16 21:00:50,655][257371] itr=41, itrs=2000, Progress: 2.05%[0m
[36m[2023-07-16 21:01:02,375][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-16 21:01:02,375][257371] FPS: 328306.34[0m
[36m[2023-07-16 21:01:06,633][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:01:06,633][257371] Reward + Measures: [[934.96757884   0.08639167   0.12228633   0.10143033   0.047925
    2.03879142]][0m
[37m[1m[2023-07-16 21:01:06,634][257371] Max Reward on eval: 934.9675788412836[0m
[37m[1m[2023-07-16 21:01:06,634][257371] Min Reward on eval: 934.9675788412836[0m
[37m[1m[2023-07-16 21:01:06,634][257371] Mean Reward across all agents: 934.9675788412836[0m
[37m[1m[2023-07-16 21:01:06,634][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:01:11,698][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:01:11,699][257371] Reward + Measures: [[197.0440879    0.1048       0.16510001   0.15689999   0.11110001
    5.42538404]
 [ 77.78478753   0.1499       0.16080001   0.17920001   0.164
    5.74306631]
 [ 83.76467038   0.13219999   0.12379999   0.10570001   0.0977
    6.60595942]
 ...
 [272.15603349   0.19059999   0.26630002   0.24770001   0.24960001
    5.00353765]
 [201.74374007   0.0964       0.1411       0.1274       0.1124
    5.32920599]
 [ 85.97478143   0.13250001   0.18550001   0.14570001   0.16680001
    5.30961514]][0m
[37m[1m[2023-07-16 21:01:11,699][257371] Max Reward on eval: 1856.1711387753487[0m
[37m[1m[2023-07-16 21:01:11,699][257371] Min Reward on eval: -15.450805798452347[0m
[37m[1m[2023-07-16 21:01:11,699][257371] Mean Reward across all agents: 281.2268635257024[0m
[37m[1m[2023-07-16 21:01:11,700][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:01:11,704][257371] mean_value=-1107.7372426486086, max_value=836.68963066197[0m
[37m[1m[2023-07-16 21:01:11,707][257371] New mean coefficients: [[ 0.46754923 -1.4066483   1.6162608  -2.1515024  -1.1885536   0.41385877]][0m
[37m[1m[2023-07-16 21:01:11,707][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:01:20,803][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-16 21:01:20,803][257371] FPS: 422250.36[0m
[36m[2023-07-16 21:01:20,806][257371] itr=42, itrs=2000, Progress: 2.10%[0m
[36m[2023-07-16 21:01:32,701][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-16 21:01:32,701][257371] FPS: 323580.89[0m
[36m[2023-07-16 21:01:37,022][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:01:37,023][257371] Reward + Measures: [[1069.45852523    0.09150234    0.130541      0.10712066    0.047866
     2.15282512]][0m
[37m[1m[2023-07-16 21:01:37,023][257371] Max Reward on eval: 1069.4585252318827[0m
[37m[1m[2023-07-16 21:01:37,023][257371] Min Reward on eval: 1069.4585252318827[0m
[37m[1m[2023-07-16 21:01:37,023][257371] Mean Reward across all agents: 1069.4585252318827[0m
[37m[1m[2023-07-16 21:01:37,024][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:01:42,210][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:01:42,210][257371] Reward + Measures: [[405.82501502   0.10520001   0.1437       0.12310001   0.0996
    4.22901678]
 [299.28232009   0.1463       0.28940001   0.18700001   0.2395
    4.15382099]
 [971.17738731   0.2455       0.26409999   0.27870002   0.1763
    3.75545955]
 ...
 [108.21320276   0.0611       0.08710001   0.08470001   0.0506
    4.44033718]
 [540.01109409   0.1586       0.1533       0.17709999   0.1012
    3.54272079]
 [312.56006864   0.09940001   0.2606       0.24680002   0.264
    4.84968662]][0m
[37m[1m[2023-07-16 21:01:42,210][257371] Max Reward on eval: 1494.234445646219[0m
[37m[1m[2023-07-16 21:01:42,211][257371] Min Reward on eval: -74.6596680703573[0m
[37m[1m[2023-07-16 21:01:42,211][257371] Mean Reward across all agents: 288.78118531915754[0m
[37m[1m[2023-07-16 21:01:42,211][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:01:42,215][257371] mean_value=-1276.3332509580318, max_value=828.3291263835505[0m
[37m[1m[2023-07-16 21:01:42,218][257371] New mean coefficients: [[ 0.5398689  -1.4150833   1.950422   -2.9602942  -0.29140782  0.5325309 ]][0m
[37m[1m[2023-07-16 21:01:42,219][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:01:51,152][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-16 21:01:51,152][257371] FPS: 429927.64[0m
[36m[2023-07-16 21:01:51,155][257371] itr=43, itrs=2000, Progress: 2.15%[0m
[36m[2023-07-16 21:02:02,794][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-16 21:02:02,794][257371] FPS: 330579.05[0m
[36m[2023-07-16 21:02:07,092][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:02:07,093][257371] Reward + Measures: [[673.918924     0.08231533   0.13064599   0.10813633   0.05905667
    2.28790236]][0m
[37m[1m[2023-07-16 21:02:07,093][257371] Max Reward on eval: 673.9189239955746[0m
[37m[1m[2023-07-16 21:02:07,093][257371] Min Reward on eval: 673.9189239955746[0m
[37m[1m[2023-07-16 21:02:07,093][257371] Mean Reward across all agents: 673.9189239955746[0m
[37m[1m[2023-07-16 21:02:07,094][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:02:12,115][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:02:12,115][257371] Reward + Measures: [[195.23411207   0.12279999   0.1259       0.14040001   0.11500001
    5.57750273]
 [120.95162754   0.1206       0.1132       0.10770001   0.0948
    6.07154226]
 [ 88.73250822   0.11610001   0.22650002   0.13499999   0.2139
    5.9433465 ]
 ...
 [-96.22654745   0.1191       0.13150001   0.07600001   0.09680001
    6.08180523]
 [ 56.17849835   0.1166       0.13270001   0.126        0.12809999
    5.65389776]
 [121.18873353   0.1417       0.148        0.14750001   0.13510001
    5.94780064]][0m
[37m[1m[2023-07-16 21:02:12,115][257371] Max Reward on eval: 1006.3840865961741[0m
[37m[1m[2023-07-16 21:02:12,116][257371] Min Reward on eval: -96.22654745429755[0m
[37m[1m[2023-07-16 21:02:12,116][257371] Mean Reward across all agents: 257.12626271697326[0m
[37m[1m[2023-07-16 21:02:12,116][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:02:12,119][257371] mean_value=-968.9121332124442, max_value=440.53660392849565[0m
[37m[1m[2023-07-16 21:02:12,122][257371] New mean coefficients: [[ 0.38141018 -2.8266993   0.6736498  -3.4664233  -1.9549874   0.8356656 ]][0m
[37m[1m[2023-07-16 21:02:12,123][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:02:21,280][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-16 21:02:21,281][257371] FPS: 419393.97[0m
[36m[2023-07-16 21:02:21,283][257371] itr=44, itrs=2000, Progress: 2.20%[0m
[36m[2023-07-16 21:02:33,196][257371] train() took 11.89 seconds to complete[0m
[36m[2023-07-16 21:02:33,196][257371] FPS: 323021.74[0m
[36m[2023-07-16 21:02:37,539][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:02:37,553][257371] Reward + Measures: [[723.82941465   0.084547     0.13447234   0.11043201   0.058658
    2.36088991]][0m
[37m[1m[2023-07-16 21:02:37,553][257371] Max Reward on eval: 723.8294146539688[0m
[37m[1m[2023-07-16 21:02:37,553][257371] Min Reward on eval: 723.8294146539688[0m
[37m[1m[2023-07-16 21:02:37,554][257371] Mean Reward across all agents: 723.8294146539688[0m
[37m[1m[2023-07-16 21:02:37,554][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:02:42,598][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:02:42,599][257371] Reward + Measures: [[138.02249201   0.25330001   0.20250002   0.23320003   0.18010001
    4.85426474]
 [135.0790229    0.2931       0.36810002   0.1647       0.22860001
    5.56098318]
 [ 44.21627315   0.20840001   0.2225       0.2244       0.14460002
    5.09806871]
 ...
 [292.6464834    0.1701       0.24150001   0.1652       0.17379999
    4.86127472]
 [ 53.36968396   0.0829       0.1031       0.0955       0.0789
    5.91104889]
 [117.61509695   0.1151       0.2253       0.15930001   0.2172
    5.06774664]][0m
[37m[1m[2023-07-16 21:02:42,599][257371] Max Reward on eval: 1385.751679861202[0m
[37m[1m[2023-07-16 21:02:42,599][257371] Min Reward on eval: -75.27589482162148[0m
[37m[1m[2023-07-16 21:02:42,600][257371] Mean Reward across all agents: 223.34914852859757[0m
[37m[1m[2023-07-16 21:02:42,600][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:02:42,606][257371] mean_value=-1146.8337947414025, max_value=764.4704533146694[0m
[37m[1m[2023-07-16 21:02:42,609][257371] New mean coefficients: [[ 0.834607   -3.4427848  -1.7543106  -4.6806087  -0.3757627   0.23932934]][0m
[37m[1m[2023-07-16 21:02:42,610][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:02:51,637][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 21:02:51,637][257371] FPS: 425462.82[0m
[36m[2023-07-16 21:02:51,640][257371] itr=45, itrs=2000, Progress: 2.25%[0m
[36m[2023-07-16 21:03:03,242][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-16 21:03:03,242][257371] FPS: 331641.71[0m
[36m[2023-07-16 21:03:07,462][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:03:07,467][257371] Reward + Measures: [[767.65211802   0.0869       0.134858     0.112608     0.05930967
    2.38666892]][0m
[37m[1m[2023-07-16 21:03:07,468][257371] Max Reward on eval: 767.6521180159457[0m
[37m[1m[2023-07-16 21:03:07,468][257371] Min Reward on eval: 767.6521180159457[0m
[37m[1m[2023-07-16 21:03:07,468][257371] Mean Reward across all agents: 767.6521180159457[0m
[37m[1m[2023-07-16 21:03:07,468][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:03:12,452][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:03:12,452][257371] Reward + Measures: [[ 90.02357985   0.1542       0.16190001   0.14219999   0.1357
    5.52258062]
 [141.79124161   0.08890001   0.1076       0.0787       0.073
    4.32842588]
 [142.98863134   0.1365       0.16759999   0.11210001   0.16360001
    5.86806774]
 ...
 [330.16331416   0.1969       0.20829999   0.1362       0.15750001
    4.58923101]
 [381.61370038   0.19410001   0.25840002   0.20019999   0.20820001
    4.20306778]
 [243.48182343   0.30070001   0.235        0.30350003   0.24440002
    4.23299551]][0m
[37m[1m[2023-07-16 21:03:12,453][257371] Max Reward on eval: 1249.2777938634158[0m
[37m[1m[2023-07-16 21:03:12,453][257371] Min Reward on eval: -52.40302165476605[0m
[37m[1m[2023-07-16 21:03:12,453][257371] Mean Reward across all agents: 214.93019917915697[0m
[37m[1m[2023-07-16 21:03:12,453][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:03:12,455][257371] mean_value=-920.8699604654726, max_value=574.0609443837149[0m
[37m[1m[2023-07-16 21:03:12,457][257371] New mean coefficients: [[ 0.97033525 -5.580232   -3.8142748  -4.3721004  -0.8351991   0.11810143]][0m
[37m[1m[2023-07-16 21:03:12,458][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:03:21,459][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-16 21:03:21,459][257371] FPS: 426710.89[0m
[36m[2023-07-16 21:03:21,461][257371] itr=46, itrs=2000, Progress: 2.30%[0m
[36m[2023-07-16 21:03:33,264][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-16 21:03:33,264][257371] FPS: 326007.29[0m
[36m[2023-07-16 21:03:37,577][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:03:37,582][257371] Reward + Measures: [[771.71335696   0.08601201   0.13442667   0.11161534   0.05776133
    2.39335179]][0m
[37m[1m[2023-07-16 21:03:37,583][257371] Max Reward on eval: 771.7133569627607[0m
[37m[1m[2023-07-16 21:03:37,584][257371] Min Reward on eval: 771.7133569627607[0m
[37m[1m[2023-07-16 21:03:37,585][257371] Mean Reward across all agents: 771.7133569627607[0m
[37m[1m[2023-07-16 21:03:37,585][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:03:42,670][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:03:42,671][257371] Reward + Measures: [[491.81896402   0.15470001   0.2103       0.16870001   0.13860001
    3.78587723]
 [241.4403668    0.2527       0.20899999   0.21030001   0.1952
    4.06687927]
 [978.38581281   0.19309998   0.2888       0.18790002   0.14240001
    3.54323006]
 ...
 [148.12834944   0.13960001   0.1578       0.16150001   0.14389999
    4.54027796]
 [189.48440841   0.20479999   0.21100001   0.1743       0.2052
    4.57174015]
 [256.16190075   0.18980001   0.18020001   0.2016       0.1681
    4.40329075]][0m
[37m[1m[2023-07-16 21:03:42,671][257371] Max Reward on eval: 1380.3377537975787[0m
[37m[1m[2023-07-16 21:03:42,671][257371] Min Reward on eval: 59.852726456336676[0m
[37m[1m[2023-07-16 21:03:42,672][257371] Mean Reward across all agents: 437.1021270386837[0m
[37m[1m[2023-07-16 21:03:42,672][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:03:42,674][257371] mean_value=-3383.248517553675, max_value=955.4729118302464[0m
[37m[1m[2023-07-16 21:03:42,676][257371] New mean coefficients: [[ 1.6091923  -5.9224653  -5.4447584  -4.7761827  -1.1426105  -0.02916374]][0m
[37m[1m[2023-07-16 21:03:42,677][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:03:51,682][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-16 21:03:51,682][257371] FPS: 426518.70[0m
[36m[2023-07-16 21:03:51,684][257371] itr=47, itrs=2000, Progress: 2.35%[0m
[36m[2023-07-16 21:04:03,254][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-16 21:04:03,254][257371] FPS: 332573.57[0m
[36m[2023-07-16 21:04:07,484][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:04:07,490][257371] Reward + Measures: [[831.98544387   0.08879834   0.13765867   0.11388      0.058757
    2.41679549]][0m
[37m[1m[2023-07-16 21:04:07,490][257371] Max Reward on eval: 831.9854438737606[0m
[37m[1m[2023-07-16 21:04:07,490][257371] Min Reward on eval: 831.9854438737606[0m
[37m[1m[2023-07-16 21:04:07,491][257371] Mean Reward across all agents: 831.9854438737606[0m
[37m[1m[2023-07-16 21:04:07,491][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:04:12,606][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:04:12,611][257371] Reward + Measures: [[ 316.83826017    0.15460001    0.2059        0.184         0.1567
     3.77026224]
 [ 285.76116908    0.1436        0.2009        0.1437        0.17310001
     4.08612013]
 [ 139.0885334     0.1804        0.24699998    0.19930001    0.24730001
     4.38002872]
 ...
 [ 193.30726244    0.1596        0.2251        0.15589999    0.17389999
     4.67370319]
 [1077.92346192    0.1542        0.2086        0.17739999    0.1148
     3.00172973]
 [ 112.1907743     0.09749999    0.14040001    0.1151        0.1122
     4.84334326]][0m
[37m[1m[2023-07-16 21:04:12,612][257371] Max Reward on eval: 1077.9234619241208[0m
[37m[1m[2023-07-16 21:04:12,612][257371] Min Reward on eval: 53.509799489364376[0m
[37m[1m[2023-07-16 21:04:12,612][257371] Mean Reward across all agents: 356.630190105931[0m
[37m[1m[2023-07-16 21:04:12,612][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:04:12,614][257371] mean_value=-3627.6663037894596, max_value=855.9794864688068[0m
[37m[1m[2023-07-16 21:04:12,617][257371] New mean coefficients: [[ 1.3310454  -6.6295214  -6.535368   -5.7790103  -1.6719356  -0.24749571]][0m
[37m[1m[2023-07-16 21:04:12,618][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:04:21,624][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-16 21:04:21,624][257371] FPS: 426446.45[0m
[36m[2023-07-16 21:04:21,627][257371] itr=48, itrs=2000, Progress: 2.40%[0m
[36m[2023-07-16 21:04:33,212][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-16 21:04:33,212][257371] FPS: 332161.85[0m
[36m[2023-07-16 21:04:37,489][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:04:37,495][257371] Reward + Measures: [[833.21635033   0.08783567   0.13280334   0.11254399   0.058398
    2.39366055]][0m
[37m[1m[2023-07-16 21:04:37,495][257371] Max Reward on eval: 833.2163503273412[0m
[37m[1m[2023-07-16 21:04:37,496][257371] Min Reward on eval: 833.2163503273412[0m
[37m[1m[2023-07-16 21:04:37,496][257371] Mean Reward across all agents: 833.2163503273412[0m
[37m[1m[2023-07-16 21:04:37,496][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:04:42,582][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:04:42,582][257371] Reward + Measures: [[231.99875043   0.2172       0.24720001   0.17640001   0.22789998
    5.49350882]
 [511.46185576   0.1981       0.21500002   0.20639999   0.16989999
    4.89734125]
 [ 63.10716429   0.0747       0.09530001   0.1028       0.0794
    5.1600008 ]
 ...
 [ 83.07864502   0.1567       0.17480001   0.1656       0.16419999
    5.79114389]
 [539.34038927   0.21970001   0.22739999   0.2297       0.22230001
    4.29892302]
 [252.12704517   0.1096       0.1524       0.15629999   0.1259
    3.60358119]][0m
[37m[1m[2023-07-16 21:04:42,583][257371] Max Reward on eval: 1173.9559765145182[0m
[37m[1m[2023-07-16 21:04:42,583][257371] Min Reward on eval: 34.61242504669353[0m
[37m[1m[2023-07-16 21:04:42,583][257371] Mean Reward across all agents: 300.146424749508[0m
[37m[1m[2023-07-16 21:04:42,583][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:04:42,585][257371] mean_value=-1025.1053212508182, max_value=775.6966217095139[0m
[37m[1m[2023-07-16 21:04:42,588][257371] New mean coefficients: [[ 1.077694   -8.889878   -5.9858475  -4.0669723  -1.0836432  -0.25837457]][0m
[37m[1m[2023-07-16 21:04:42,589][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:04:51,620][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 21:04:51,621][257371] FPS: 425256.45[0m
[36m[2023-07-16 21:04:51,623][257371] itr=49, itrs=2000, Progress: 2.45%[0m
[36m[2023-07-16 21:05:03,349][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-16 21:05:03,349][257371] FPS: 328127.37[0m
[36m[2023-07-16 21:05:07,671][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:05:07,676][257371] Reward + Measures: [[799.9351182    0.08326366   0.12715334   0.10889067   0.05828467
    2.33812141]][0m
[37m[1m[2023-07-16 21:05:07,677][257371] Max Reward on eval: 799.9351181984466[0m
[37m[1m[2023-07-16 21:05:07,677][257371] Min Reward on eval: 799.9351181984466[0m
[37m[1m[2023-07-16 21:05:07,677][257371] Mean Reward across all agents: 799.9351181984466[0m
[37m[1m[2023-07-16 21:05:07,677][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:05:12,707][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:05:12,713][257371] Reward + Measures: [[-26.39547563   0.23080002   0.2386       0.2543       0.08020001
    5.83930349]
 [ 86.10772721   0.0721       0.09610001   0.0926       0.0848
    6.15668058]
 [  6.86152492   0.07790001   0.0707       0.0643       0.0532
    6.47237396]
 ...
 [ 82.19526954   0.15259999   0.1259       0.1358       0.0852
    5.20570755]
 [ 81.36345947   0.079        0.0912       0.0926       0.0763
    6.07126951]
 [ 48.12290187   0.1497       0.11430001   0.14930001   0.0865
    6.07307148]][0m
[37m[1m[2023-07-16 21:05:12,713][257371] Max Reward on eval: 579.4812078069896[0m
[37m[1m[2023-07-16 21:05:12,713][257371] Min Reward on eval: -144.13590747173876[0m
[37m[1m[2023-07-16 21:05:12,714][257371] Mean Reward across all agents: 54.04998907265779[0m
[37m[1m[2023-07-16 21:05:12,714][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:05:12,718][257371] mean_value=-261.21763203865487, max_value=534.4184506933577[0m
[37m[1m[2023-07-16 21:05:12,721][257371] New mean coefficients: [[ 1.046121  -9.665549  -8.881123  -4.3160524 -1.2831094 -1.0459151]][0m
[37m[1m[2023-07-16 21:05:12,722][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:05:21,738][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 21:05:21,738][257371] FPS: 425964.92[0m
[36m[2023-07-16 21:05:21,741][257371] itr=50, itrs=2000, Progress: 2.50%[0m
[37m[1m[2023-07-16 21:07:21,548][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000030[0m
[36m[2023-07-16 21:07:33,966][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-16 21:07:33,966][257371] FPS: 323803.37[0m
[36m[2023-07-16 21:07:38,194][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:07:38,195][257371] Reward + Measures: [[696.42379928   0.076358     0.11608033   0.10318033   0.05750333
    2.27030444]][0m
[37m[1m[2023-07-16 21:07:38,195][257371] Max Reward on eval: 696.4237992762831[0m
[37m[1m[2023-07-16 21:07:38,195][257371] Min Reward on eval: 696.4237992762831[0m
[37m[1m[2023-07-16 21:07:38,195][257371] Mean Reward across all agents: 696.4237992762831[0m
[37m[1m[2023-07-16 21:07:38,196][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:07:43,255][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:07:43,256][257371] Reward + Measures: [[-94.88825998   0.1538       0.1989       0.19240001   0.1804
    5.78059912]
 [-61.54059635   0.17470001   0.2227       0.20650001   0.1732
    5.60728359]
 [ 76.44076724   0.1552       0.18969999   0.17370002   0.15460001
    4.47572803]
 ...
 [ -7.18900377   0.2024       0.22610001   0.25330001   0.1498
    5.27833223]
 [ 94.39882968   0.16119999   0.29710001   0.26210001   0.25729999
    4.67798233]
 [ 88.08591273   0.28749999   0.3457       0.35909998   0.39910001
    5.93054867]][0m
[37m[1m[2023-07-16 21:07:43,256][257371] Max Reward on eval: 855.9197730943561[0m
[37m[1m[2023-07-16 21:07:43,256][257371] Min Reward on eval: -152.50485892556608[0m
[37m[1m[2023-07-16 21:07:43,257][257371] Mean Reward across all agents: 78.11930897638109[0m
[37m[1m[2023-07-16 21:07:43,257][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:07:43,261][257371] mean_value=-621.0919147030947, max_value=594.3830740118399[0m
[37m[1m[2023-07-16 21:07:43,263][257371] New mean coefficients: [[ 0.99718964 -9.118782   -7.241457   -5.001593   -1.997435   -0.5384396 ]][0m
[37m[1m[2023-07-16 21:07:43,264][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:07:52,323][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-16 21:07:52,323][257371] FPS: 423971.08[0m
[36m[2023-07-16 21:07:52,326][257371] itr=51, itrs=2000, Progress: 2.55%[0m
[36m[2023-07-16 21:08:04,183][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-16 21:08:04,183][257371] FPS: 324509.56[0m
[36m[2023-07-16 21:08:08,563][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:08:08,564][257371] Reward + Measures: [[655.11963494   0.07307733   0.110391     0.10045833   0.05683234
    2.23854113]][0m
[37m[1m[2023-07-16 21:08:08,564][257371] Max Reward on eval: 655.1196349442944[0m
[37m[1m[2023-07-16 21:08:08,564][257371] Min Reward on eval: 655.1196349442944[0m
[37m[1m[2023-07-16 21:08:08,564][257371] Mean Reward across all agents: 655.1196349442944[0m
[37m[1m[2023-07-16 21:08:08,565][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:08:13,811][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:08:13,812][257371] Reward + Measures: [[517.61194225   0.1969       0.2287       0.1594       0.182
    3.62287521]
 [760.74704165   0.20780002   0.23109999   0.17209999   0.16410001
    3.12232757]
 [655.14782236   0.10109999   0.1455       0.1204       0.04740001
    2.89681506]
 ...
 [654.8863602    0.10600001   0.16010001   0.11619999   0.0647
    2.97771311]
 [282.84643201   0.2861       0.2694       0.2023       0.273
    3.98571897]
 [560.20715186   0.1159       0.16260001   0.11370001   0.0706
    3.03746963]][0m
[37m[1m[2023-07-16 21:08:13,812][257371] Max Reward on eval: 1146.7630329331382[0m
[37m[1m[2023-07-16 21:08:13,813][257371] Min Reward on eval: 88.74078506199876[0m
[37m[1m[2023-07-16 21:08:13,813][257371] Mean Reward across all agents: 414.45651948842766[0m
[37m[1m[2023-07-16 21:08:13,813][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:08:13,815][257371] mean_value=-3939.7295071278127, max_value=805.6720304777613[0m
[37m[1m[2023-07-16 21:08:13,817][257371] New mean coefficients: [[ 0.9475741 -9.330427  -9.193714  -5.760794  -1.5124229 -1.4049456]][0m
[37m[1m[2023-07-16 21:08:13,818][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:08:22,955][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-16 21:08:22,956][257371] FPS: 420317.00[0m
[36m[2023-07-16 21:08:22,958][257371] itr=52, itrs=2000, Progress: 2.60%[0m
[36m[2023-07-16 21:08:34,865][257371] train() took 11.88 seconds to complete[0m
[36m[2023-07-16 21:08:34,866][257371] FPS: 323132.35[0m
[36m[2023-07-16 21:08:39,162][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:08:39,162][257371] Reward + Measures: [[585.32330544   0.06931433   0.10465633   0.09726332   0.05688534
    2.18456745]][0m
[37m[1m[2023-07-16 21:08:39,162][257371] Max Reward on eval: 585.3233054389634[0m
[37m[1m[2023-07-16 21:08:39,163][257371] Min Reward on eval: 585.3233054389634[0m
[37m[1m[2023-07-16 21:08:39,163][257371] Mean Reward across all agents: 585.3233054389634[0m
[37m[1m[2023-07-16 21:08:39,163][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:08:44,160][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:08:44,160][257371] Reward + Measures: [[185.16607659   0.06940001   0.1035       0.0938       0.0778
    4.90315628]
 [319.63730759   0.22740002   0.22550002   0.3154       0.24029998
    5.47328806]
 [171.06429507   0.08090001   0.1328       0.12         0.09460001
    4.82798719]
 ...
 [145.03083127   0.0949       0.1559       0.12409999   0.1244
    6.27559042]
 [180.01406022   0.0751       0.13410001   0.11720001   0.10150001
    4.79897594]
 [287.25221687   0.1027       0.23029999   0.22620001   0.2484
    5.66551208]][0m
[37m[1m[2023-07-16 21:08:44,161][257371] Max Reward on eval: 638.1154660575091[0m
[37m[1m[2023-07-16 21:08:44,161][257371] Min Reward on eval: 25.650698148831726[0m
[37m[1m[2023-07-16 21:08:44,161][257371] Mean Reward across all agents: 210.78264470960255[0m
[37m[1m[2023-07-16 21:08:44,161][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:08:44,165][257371] mean_value=-189.7237105465186, max_value=521.2427011918641[0m
[37m[1m[2023-07-16 21:08:44,168][257371] New mean coefficients: [[  0.533507    -7.344837   -10.618955    -7.039422    -2.4395711
   -0.60763025]][0m
[37m[1m[2023-07-16 21:08:44,169][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:08:53,245][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 21:08:53,245][257371] FPS: 423188.66[0m
[36m[2023-07-16 21:08:53,247][257371] itr=53, itrs=2000, Progress: 2.65%[0m
[36m[2023-07-16 21:09:04,862][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-16 21:09:04,863][257371] FPS: 331318.96[0m
[36m[2023-07-16 21:09:09,178][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:09:09,179][257371] Reward + Measures: [[526.29225541   0.06556567   0.09978933   0.09434267   0.05600566
    2.15142179]][0m
[37m[1m[2023-07-16 21:09:09,179][257371] Max Reward on eval: 526.2922554055282[0m
[37m[1m[2023-07-16 21:09:09,179][257371] Min Reward on eval: 526.2922554055282[0m
[37m[1m[2023-07-16 21:09:09,179][257371] Mean Reward across all agents: 526.2922554055282[0m
[37m[1m[2023-07-16 21:09:09,179][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:09:14,210][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:09:14,211][257371] Reward + Measures: [[ 45.52682066   0.0626       0.103        0.0935       0.0803
    6.91176748]
 [ 83.44048928   0.06800001   0.113        0.1182       0.0841
    6.39097214]
 [ 73.99716328   0.0678       0.1044       0.1056       0.065
    6.45194006]
 ...
 [ 95.76705059   0.078        0.13080001   0.14310001   0.0862
    6.50891638]
 [ 42.2293001    0.0596       0.0828       0.1085       0.0683
    6.47444248]
 [154.35880009   0.085        0.09760001   0.113        0.1237
    5.91381073]][0m
[37m[1m[2023-07-16 21:09:14,211][257371] Max Reward on eval: 370.8353466647095[0m
[37m[1m[2023-07-16 21:09:14,212][257371] Min Reward on eval: -6.702359998458997[0m
[37m[1m[2023-07-16 21:09:14,212][257371] Mean Reward across all agents: 81.60171922940577[0m
[37m[1m[2023-07-16 21:09:14,212][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:09:14,214][257371] mean_value=-169.87961445818448, max_value=152.64507134343825[0m
[37m[1m[2023-07-16 21:09:14,216][257371] New mean coefficients: [[  0.08094481  -9.6704445  -13.639167    -6.41216     -2.2707496
   -1.0878788 ]][0m
[37m[1m[2023-07-16 21:09:14,217][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:09:23,266][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 21:09:23,267][257371] FPS: 424422.87[0m
[36m[2023-07-16 21:09:23,269][257371] itr=54, itrs=2000, Progress: 2.70%[0m
[36m[2023-07-16 21:09:35,017][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-16 21:09:35,017][257371] FPS: 327524.31[0m
[36m[2023-07-16 21:09:39,344][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:09:39,345][257371] Reward + Measures: [[471.89107546   0.06325333   0.09365834   0.09117901   0.056985
    2.10351491]][0m
[37m[1m[2023-07-16 21:09:39,345][257371] Max Reward on eval: 471.89107545750704[0m
[37m[1m[2023-07-16 21:09:39,345][257371] Min Reward on eval: 471.89107545750704[0m
[37m[1m[2023-07-16 21:09:39,346][257371] Mean Reward across all agents: 471.89107545750704[0m
[37m[1m[2023-07-16 21:09:39,346][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:09:44,397][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:09:44,398][257371] Reward + Measures: [[110.88660282   0.16870001   0.20720001   0.17150001   0.1653
    4.89735079]
 [ 41.01025814   0.0854       0.1163       0.0947       0.085
    5.804883  ]
 [105.66874728   0.14530002   0.17749999   0.16020001   0.15989999
    5.52132702]
 ...
 [132.62318496   0.10740001   0.14460002   0.13520001   0.11979999
    5.81344938]
 [131.51252995   0.0929       0.1415       0.0962       0.0848
    5.83731031]
 [120.66199569   0.15259999   0.1883       0.17770001   0.15189999
    3.99727058]][0m
[37m[1m[2023-07-16 21:09:44,398][257371] Max Reward on eval: 879.9991702672094[0m
[37m[1m[2023-07-16 21:09:44,398][257371] Min Reward on eval: -27.252307531796397[0m
[37m[1m[2023-07-16 21:09:44,399][257371] Mean Reward across all agents: 94.16611342640304[0m
[37m[1m[2023-07-16 21:09:44,399][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:09:44,400][257371] mean_value=-379.8383892894738, max_value=-66.59019409198046[0m
[36m[2023-07-16 21:09:44,402][257371] XNES is restarting with a new solution whose measures are [0.31480002 0.34660003 0.35679999 0.33670002 4.10141134] and objective is 355.97948646880684[0m
[36m[2023-07-16 21:09:44,403][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-16 21:09:44,406][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-16 21:09:44,407][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:09:53,381][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-16 21:09:53,382][257371] FPS: 427937.69[0m
[36m[2023-07-16 21:09:53,384][257371] itr=55, itrs=2000, Progress: 2.75%[0m
[36m[2023-07-16 21:10:05,149][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-16 21:10:05,149][257371] FPS: 327141.41[0m
[36m[2023-07-16 21:10:09,489][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:10:09,489][257371] Reward + Measures: [[304.78928701   0.21753733   0.22899634   0.25343233   0.201731
    4.53992987]][0m
[37m[1m[2023-07-16 21:10:09,489][257371] Max Reward on eval: 304.7892870057936[0m
[37m[1m[2023-07-16 21:10:09,490][257371] Min Reward on eval: 304.7892870057936[0m
[37m[1m[2023-07-16 21:10:09,490][257371] Mean Reward across all agents: 304.7892870057936[0m
[37m[1m[2023-07-16 21:10:09,490][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:10:14,533][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:10:14,534][257371] Reward + Measures: [[370.30421738   0.30310002   0.36650002   0.3576       0.30380002
    5.99651575]
 [ 67.87109062   0.14850001   0.1229       0.14399999   0.09910001
    5.97216511]
 [201.68014919   0.1294       0.13380001   0.16250001   0.13690001
    5.60353088]
 ...
 [134.91826285   0.1411       0.16150001   0.13930002   0.12539999
    5.61127186]
 [299.34219985   0.0903       0.18880001   0.19580001   0.1441
    5.51942396]
 [162.29126363   0.12640001   0.13930002   0.1372       0.125
    5.68865871]][0m
[37m[1m[2023-07-16 21:10:14,534][257371] Max Reward on eval: 397.5547160822898[0m
[37m[1m[2023-07-16 21:10:14,534][257371] Min Reward on eval: -83.75910373763182[0m
[37m[1m[2023-07-16 21:10:14,534][257371] Mean Reward across all agents: 91.03863653381676[0m
[37m[1m[2023-07-16 21:10:14,535][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:10:14,537][257371] mean_value=-134.69752874945257, max_value=643.3229847161058[0m
[37m[1m[2023-07-16 21:10:14,540][257371] New mean coefficients: [[ 0.01246536  1.2318723   0.52181077 -2.2299595  -1.2460241   0.40825891]][0m
[37m[1m[2023-07-16 21:10:14,541][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:10:23,551][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 21:10:23,551][257371] FPS: 426264.19[0m
[36m[2023-07-16 21:10:23,554][257371] itr=56, itrs=2000, Progress: 2.80%[0m
[36m[2023-07-16 21:10:35,393][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-16 21:10:35,393][257371] FPS: 325017.49[0m
[36m[2023-07-16 21:10:39,703][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:10:39,709][257371] Reward + Measures: [[464.16291168   0.19808598   0.25976565   0.23802766   0.196639
    4.39838934]][0m
[37m[1m[2023-07-16 21:10:39,709][257371] Max Reward on eval: 464.1629116776492[0m
[37m[1m[2023-07-16 21:10:39,709][257371] Min Reward on eval: 464.1629116776492[0m
[37m[1m[2023-07-16 21:10:39,710][257371] Mean Reward across all agents: 464.1629116776492[0m
[37m[1m[2023-07-16 21:10:39,710][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:10:44,878][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:10:44,884][257371] Reward + Measures: [[ -2.58525443   0.1991       0.25780001   0.21329999   0.16879998
    6.24385214]
 [127.09546559   0.14830001   0.16610001   0.17690001   0.1682
    5.97831392]
 [ 85.3479149    0.0881       0.1258       0.08920001   0.1006
    6.5590663 ]
 ...
 [124.10868394   0.094        0.1584       0.09940001   0.11390001
    5.87910032]
 [124.14282286   0.14300001   0.1671       0.1441       0.15390001
    6.52326822]
 [273.30960503   0.19310001   0.23629999   0.2325       0.2273
    5.44597578]][0m
[37m[1m[2023-07-16 21:10:44,884][257371] Max Reward on eval: 448.3785729408264[0m
[37m[1m[2023-07-16 21:10:44,884][257371] Min Reward on eval: -53.269949892908336[0m
[37m[1m[2023-07-16 21:10:44,885][257371] Mean Reward across all agents: 113.09354911490705[0m
[37m[1m[2023-07-16 21:10:44,885][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:10:44,887][257371] mean_value=-145.92742483162732, max_value=334.23262320159154[0m
[37m[1m[2023-07-16 21:10:44,889][257371] New mean coefficients: [[ 0.19405572  2.1017022   0.00417304 -2.0563135  -2.5997422  -0.10592198]][0m
[37m[1m[2023-07-16 21:10:44,890][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:10:53,908][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 21:10:53,909][257371] FPS: 425884.59[0m
[36m[2023-07-16 21:10:53,911][257371] itr=57, itrs=2000, Progress: 2.85%[0m
[36m[2023-07-16 21:11:05,674][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-16 21:11:05,674][257371] FPS: 327106.08[0m
[36m[2023-07-16 21:11:09,942][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:11:09,942][257371] Reward + Measures: [[313.99965461   0.21268699   0.22590633   0.23345532   0.19206032
    4.44464731]][0m
[37m[1m[2023-07-16 21:11:09,942][257371] Max Reward on eval: 313.99965461115863[0m
[37m[1m[2023-07-16 21:11:09,943][257371] Min Reward on eval: 313.99965461115863[0m
[37m[1m[2023-07-16 21:11:09,943][257371] Mean Reward across all agents: 313.99965461115863[0m
[37m[1m[2023-07-16 21:11:09,943][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:11:14,961][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:11:14,962][257371] Reward + Measures: [[107.33595381   0.1212       0.13850001   0.13590001   0.16919999
    5.94241953]
 [ 59.771974     0.0997       0.0972       0.1294       0.0988
    6.47195816]
 [107.35585713   0.0994       0.13930002   0.12710001   0.1117
    6.11228991]
 ...
 [ 67.81900843   0.1237       0.1744       0.1604       0.1522
    6.20174122]
 [ 59.78444666   0.11010001   0.11809999   0.12280001   0.1008
    5.62245989]
 [ 81.26243446   0.13520001   0.106        0.12890001   0.08449999
    5.9335804 ]][0m
[37m[1m[2023-07-16 21:11:14,962][257371] Max Reward on eval: 339.99734663376586[0m
[37m[1m[2023-07-16 21:11:14,962][257371] Min Reward on eval: -177.1947790330276[0m
[37m[1m[2023-07-16 21:11:14,963][257371] Mean Reward across all agents: 82.88226001996021[0m
[37m[1m[2023-07-16 21:11:14,963][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:11:14,965][257371] mean_value=-187.40230310980593, max_value=638.4343493792228[0m
[37m[1m[2023-07-16 21:11:14,967][257371] New mean coefficients: [[ 1.0549109   2.0937161  -0.2072849  -2.6470375  -3.4327946  -0.06389196]][0m
[37m[1m[2023-07-16 21:11:14,968][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:11:23,970][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-16 21:11:23,971][257371] FPS: 426647.20[0m
[36m[2023-07-16 21:11:23,973][257371] itr=58, itrs=2000, Progress: 2.90%[0m
[36m[2023-07-16 21:11:35,768][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-16 21:11:35,768][257371] FPS: 326223.09[0m
[36m[2023-07-16 21:11:40,079][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:11:40,079][257371] Reward + Measures: [[278.71726322   0.23288065   0.247777     0.24775366   0.18509732
    4.72790861]][0m
[37m[1m[2023-07-16 21:11:40,080][257371] Max Reward on eval: 278.7172632167842[0m
[37m[1m[2023-07-16 21:11:40,080][257371] Min Reward on eval: 278.7172632167842[0m
[37m[1m[2023-07-16 21:11:40,080][257371] Mean Reward across all agents: 278.7172632167842[0m
[37m[1m[2023-07-16 21:11:40,080][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:11:45,183][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:11:45,184][257371] Reward + Measures: [[155.23950288   0.1463       0.1559       0.16010001   0.12630001
    5.48134089]
 [ 51.10821679   0.1331       0.13500002   0.1569       0.10750001
    5.9180274 ]
 [ 31.97642658   0.1714       0.17560001   0.114        0.1921
    6.51048899]
 ...
 [ 85.07399171   0.14359999   0.15970001   0.13329999   0.133
    6.372087  ]
 [228.88183426   0.1693       0.1541       0.17200001   0.125
    4.99097681]
 [102.27842896   0.1321       0.12830001   0.149        0.10399999
    6.63530207]][0m
[37m[1m[2023-07-16 21:11:45,184][257371] Max Reward on eval: 366.2621288318187[0m
[37m[1m[2023-07-16 21:11:45,184][257371] Min Reward on eval: -106.21124847028405[0m
[37m[1m[2023-07-16 21:11:45,185][257371] Mean Reward across all agents: 86.91575866266732[0m
[37m[1m[2023-07-16 21:11:45,185][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:11:45,188][257371] mean_value=-129.9726384039765, max_value=725.8815705282457[0m
[37m[1m[2023-07-16 21:11:45,191][257371] New mean coefficients: [[-0.7366072   0.63291717 -1.3535173  -2.838406   -2.8130932   0.67471594]][0m
[37m[1m[2023-07-16 21:11:45,192][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:11:54,374][257371] train() took 9.18 seconds to complete[0m
[36m[2023-07-16 21:11:54,374][257371] FPS: 418280.96[0m
[36m[2023-07-16 21:11:54,376][257371] itr=59, itrs=2000, Progress: 2.95%[0m
[36m[2023-07-16 21:12:06,146][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-16 21:12:06,146][257371] FPS: 327019.74[0m
[36m[2023-07-16 21:12:10,411][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:12:10,411][257371] Reward + Measures: [[351.79605054   0.23456067   0.24008532   0.23690034   0.20426734
    4.82463074]][0m
[37m[1m[2023-07-16 21:12:10,412][257371] Max Reward on eval: 351.7960505403819[0m
[37m[1m[2023-07-16 21:12:10,412][257371] Min Reward on eval: 351.7960505403819[0m
[37m[1m[2023-07-16 21:12:10,412][257371] Mean Reward across all agents: 351.7960505403819[0m
[37m[1m[2023-07-16 21:12:10,412][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:12:15,411][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:12:15,412][257371] Reward + Measures: [[ 77.86327257   0.1997       0.20910001   0.23510002   0.21610001
    5.89359522]
 [ 87.44802198   0.13420001   0.17740001   0.16970001   0.15440002
    6.4699297 ]
 [193.08457305   0.25420001   0.20469999   0.25700003   0.20909999
    6.17590666]
 ...
 [-76.31874146   0.28480002   0.2251       0.31869999   0.26589999
    6.1220603 ]
 [ 57.80621018   0.23550001   0.244        0.28650004   0.20550001
    6.03318644]
 [113.46340889   0.15150002   0.19310002   0.19050001   0.12990001
    6.40002394]][0m
[37m[1m[2023-07-16 21:12:15,412][257371] Max Reward on eval: 325.57705021575094[0m
[37m[1m[2023-07-16 21:12:15,412][257371] Min Reward on eval: -164.49397244723514[0m
[37m[1m[2023-07-16 21:12:15,413][257371] Mean Reward across all agents: 100.42418328353645[0m
[37m[1m[2023-07-16 21:12:15,413][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:12:15,417][257371] mean_value=-104.93754208153878, max_value=749.4279670130927[0m
[37m[1m[2023-07-16 21:12:15,419][257371] New mean coefficients: [[ 1.1589758  -1.2805531  -2.9868975  -2.3200936  -2.1840634  -0.15192342]][0m
[37m[1m[2023-07-16 21:12:15,420][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:12:24,464][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 21:12:24,464][257371] FPS: 424681.60[0m
[36m[2023-07-16 21:12:24,466][257371] itr=60, itrs=2000, Progress: 3.00%[0m
[37m[1m[2023-07-16 21:14:21,258][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000040[0m
[36m[2023-07-16 21:14:33,548][257371] train() took 11.90 seconds to complete[0m
[36m[2023-07-16 21:14:33,548][257371] FPS: 322721.54[0m
[36m[2023-07-16 21:14:37,663][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:14:37,669][257371] Reward + Measures: [[360.19039831   0.20863999   0.21003066   0.23356101   0.18656099
    4.92537785]][0m
[37m[1m[2023-07-16 21:14:37,670][257371] Max Reward on eval: 360.190398313735[0m
[37m[1m[2023-07-16 21:14:37,670][257371] Min Reward on eval: 360.190398313735[0m
[37m[1m[2023-07-16 21:14:37,671][257371] Mean Reward across all agents: 360.190398313735[0m
[37m[1m[2023-07-16 21:14:37,672][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:14:42,890][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:14:42,890][257371] Reward + Measures: [[132.105848     0.17010002   0.1693       0.1796       0.17290001
    5.78707266]
 [213.76971816   0.1155       0.1708       0.21299998   0.18359999
    6.83828449]
 [ 23.12112167   0.08010001   0.1          0.08310001   0.0864
    6.43987656]
 ...
 [  6.035897     0.15320002   0.1099       0.16360001   0.10289999
    5.95389509]
 [ 13.94180193   0.14040001   0.14109999   0.1285       0.1831
    6.31355762]
 [ 63.33267524   0.1868       0.1911       0.16010001   0.1838
    6.41640615]][0m
[37m[1m[2023-07-16 21:14:42,891][257371] Max Reward on eval: 399.8732356946915[0m
[37m[1m[2023-07-16 21:14:42,891][257371] Min Reward on eval: -57.87562822904438[0m
[37m[1m[2023-07-16 21:14:42,891][257371] Mean Reward across all agents: 108.91201342117893[0m
[37m[1m[2023-07-16 21:14:42,891][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:14:42,896][257371] mean_value=-77.65893579666586, max_value=722.8954790032283[0m
[37m[1m[2023-07-16 21:14:42,899][257371] New mean coefficients: [[ 3.3843298  -2.5815067  -2.4476557  -0.25365424 -3.0210836  -1.0472319 ]][0m
[37m[1m[2023-07-16 21:14:42,900][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:14:51,940][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 21:14:51,941][257371] FPS: 424837.59[0m
[36m[2023-07-16 21:14:51,943][257371] itr=61, itrs=2000, Progress: 3.05%[0m
[36m[2023-07-16 21:15:03,796][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-16 21:15:03,796][257371] FPS: 324619.74[0m
[36m[2023-07-16 21:15:08,137][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:15:08,138][257371] Reward + Measures: [[345.99333871   0.20930399   0.20517269   0.22968365   0.18355265
    5.02575588]][0m
[37m[1m[2023-07-16 21:15:08,138][257371] Max Reward on eval: 345.9933387087244[0m
[37m[1m[2023-07-16 21:15:08,138][257371] Min Reward on eval: 345.9933387087244[0m
[37m[1m[2023-07-16 21:15:08,138][257371] Mean Reward across all agents: 345.9933387087244[0m
[37m[1m[2023-07-16 21:15:08,139][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:15:13,167][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:15:13,168][257371] Reward + Measures: [[120.08209269   0.13859999   0.19290002   0.1437       0.1463
    6.52120972]
 [ 87.50467469   0.2527       0.2122       0.21250001   0.23700002
    6.54097748]
 [ 39.5703684    0.1182       0.1504       0.11490001   0.11870001
    6.40190649]
 ...
 [ 56.89527719   0.0895       0.0997       0.1133       0.09780001
    5.96705008]
 [  5.82055013   0.103        0.11140001   0.0865       0.0762
    6.47221851]
 [-16.63032905   0.14009999   0.0992       0.1236       0.0908
    6.14783812]][0m
[37m[1m[2023-07-16 21:15:13,168][257371] Max Reward on eval: 328.325723002106[0m
[37m[1m[2023-07-16 21:15:13,168][257371] Min Reward on eval: -38.436505352566016[0m
[37m[1m[2023-07-16 21:15:13,169][257371] Mean Reward across all agents: 94.91157398781405[0m
[37m[1m[2023-07-16 21:15:13,169][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:15:13,173][257371] mean_value=-68.50271738625878, max_value=749.4379934428073[0m
[37m[1m[2023-07-16 21:15:13,176][257371] New mean coefficients: [[ 5.0744987  -3.0908065  -3.7966218   0.27050757 -2.782101   -1.7024918 ]][0m
[37m[1m[2023-07-16 21:15:13,177][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:15:22,248][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 21:15:22,248][257371] FPS: 423386.37[0m
[36m[2023-07-16 21:15:22,251][257371] itr=62, itrs=2000, Progress: 3.10%[0m
[36m[2023-07-16 21:15:34,043][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-16 21:15:34,043][257371] FPS: 326431.12[0m
[36m[2023-07-16 21:15:38,375][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:15:38,375][257371] Reward + Measures: [[283.10833544   0.26664966   0.193216     0.27950963   0.17806901
    5.42743301]][0m
[37m[1m[2023-07-16 21:15:38,375][257371] Max Reward on eval: 283.1083354447509[0m
[37m[1m[2023-07-16 21:15:38,375][257371] Min Reward on eval: 283.1083354447509[0m
[37m[1m[2023-07-16 21:15:38,376][257371] Mean Reward across all agents: 283.1083354447509[0m
[37m[1m[2023-07-16 21:15:38,376][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:15:43,448][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:15:43,448][257371] Reward + Measures: [[ 61.64322936   0.10220001   0.14400001   0.1574       0.1074
    6.1938715 ]
 [294.75871461   0.24860001   0.25530002   0.23220001   0.25530002
    6.01654434]
 [112.30742369   0.12779999   0.1743       0.12729999   0.1409
    6.18926239]
 ...
 [ 67.76556697   0.09339999   0.0946       0.13510001   0.1072
    6.55367756]
 [ 91.20865534   0.11970001   0.14760001   0.1636       0.14060001
    6.34364271]
 [ 58.26139054   0.0983       0.0789       0.1164       0.1072
    6.43471622]][0m
[37m[1m[2023-07-16 21:15:43,449][257371] Max Reward on eval: 362.5222940847278[0m
[37m[1m[2023-07-16 21:15:43,449][257371] Min Reward on eval: -54.13823904930614[0m
[37m[1m[2023-07-16 21:15:43,449][257371] Mean Reward across all agents: 101.53139473599761[0m
[37m[1m[2023-07-16 21:15:43,449][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:15:43,453][257371] mean_value=-126.14432329967472, max_value=695.8057074345649[0m
[37m[1m[2023-07-16 21:15:43,455][257371] New mean coefficients: [[ 3.2010283  0.5062022 -3.318336  -1.2498411 -3.7691264 -1.3457383]][0m
[37m[1m[2023-07-16 21:15:43,456][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:15:52,592][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-16 21:15:52,592][257371] FPS: 420396.69[0m
[36m[2023-07-16 21:15:52,594][257371] itr=63, itrs=2000, Progress: 3.15%[0m
[36m[2023-07-16 21:16:04,363][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-16 21:16:04,363][257371] FPS: 326977.91[0m
[36m[2023-07-16 21:16:08,707][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:16:08,707][257371] Reward + Measures: [[213.91323746   0.232917     0.16064699   0.30019566   0.175304
    5.4725337 ]][0m
[37m[1m[2023-07-16 21:16:08,707][257371] Max Reward on eval: 213.91323745571347[0m
[37m[1m[2023-07-16 21:16:08,708][257371] Min Reward on eval: 213.91323745571347[0m
[37m[1m[2023-07-16 21:16:08,708][257371] Mean Reward across all agents: 213.91323745571347[0m
[37m[1m[2023-07-16 21:16:08,708][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:16:13,703][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:16:13,704][257371] Reward + Measures: [[ 64.63689286   0.11849999   0.11000001   0.12         0.12230001
    5.57235527]
 [ 18.21559721   0.16320001   0.1367       0.1672       0.1001
    6.67726755]
 [165.85443057   0.352        0.23810001   0.35080001   0.1768
    6.38051701]
 ...
 [ 13.93695609   0.20209999   0.17690001   0.27520001   0.1237
    7.09845448]
 [ 64.7709058    0.12639999   0.11369999   0.1032       0.0931
    6.11124706]
 [ 55.43657009   0.2552       0.21900001   0.16199999   0.2165
    6.98216248]][0m
[37m[1m[2023-07-16 21:16:13,704][257371] Max Reward on eval: 388.83656503483655[0m
[37m[1m[2023-07-16 21:16:13,704][257371] Min Reward on eval: -59.400780606642364[0m
[37m[1m[2023-07-16 21:16:13,704][257371] Mean Reward across all agents: 68.04936779617843[0m
[37m[1m[2023-07-16 21:16:13,705][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:16:13,714][257371] mean_value=-85.37103215514037, max_value=770.657100123957[0m
[37m[1m[2023-07-16 21:16:13,717][257371] New mean coefficients: [[ 5.891198    1.2499237  -4.1314626   0.22665131 -2.224489   -1.6551886 ]][0m
[37m[1m[2023-07-16 21:16:13,718][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:16:22,736][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 21:16:22,736][257371] FPS: 425873.94[0m
[36m[2023-07-16 21:16:22,739][257371] itr=64, itrs=2000, Progress: 3.20%[0m
[36m[2023-07-16 21:16:34,546][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-16 21:16:34,546][257371] FPS: 326015.60[0m
[36m[2023-07-16 21:16:38,850][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:16:38,850][257371] Reward + Measures: [[398.67551979   0.20898199   0.17719667   0.25715467   0.15531632
    5.27515221]][0m
[37m[1m[2023-07-16 21:16:38,850][257371] Max Reward on eval: 398.67551978703574[0m
[37m[1m[2023-07-16 21:16:38,851][257371] Min Reward on eval: 398.67551978703574[0m
[37m[1m[2023-07-16 21:16:38,851][257371] Mean Reward across all agents: 398.67551978703574[0m
[37m[1m[2023-07-16 21:16:38,851][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:16:44,082][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:16:44,082][257371] Reward + Measures: [[ 88.89804109   0.20390001   0.19559999   0.23400001   0.1744
    6.28728437]
 [ 26.55355086   0.0838       0.1023       0.10539999   0.1014
    6.46106958]
 [103.63211686   0.1336       0.14590001   0.14049999   0.1366
    6.31569052]
 ...
 [ 18.49312183   0.0952       0.11060001   0.13790001   0.1122
    6.66181278]
 [ 80.0065481    0.13160001   0.1593       0.14740001   0.12730001
    6.14407206]
 [ 16.56789982   0.09850001   0.106        0.1026       0.10409999
    6.8408823 ]][0m
[37m[1m[2023-07-16 21:16:44,082][257371] Max Reward on eval: 406.92541886232794[0m
[37m[1m[2023-07-16 21:16:44,083][257371] Min Reward on eval: -35.44604325490072[0m
[37m[1m[2023-07-16 21:16:44,083][257371] Mean Reward across all agents: 76.05045590244303[0m
[37m[1m[2023-07-16 21:16:44,083][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:16:44,085][257371] mean_value=-157.89754602227399, max_value=657.4354265487393[0m
[37m[1m[2023-07-16 21:16:44,088][257371] New mean coefficients: [[ 4.7103605  0.6495809 -2.6076837 -0.8856813 -1.435544  -1.4352422]][0m
[37m[1m[2023-07-16 21:16:44,089][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:16:53,205][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-16 21:16:53,205][257371] FPS: 421289.99[0m
[36m[2023-07-16 21:16:53,208][257371] itr=65, itrs=2000, Progress: 3.25%[0m
[36m[2023-07-16 21:17:05,156][257371] train() took 11.92 seconds to complete[0m
[36m[2023-07-16 21:17:05,156][257371] FPS: 322122.30[0m
[36m[2023-07-16 21:17:09,468][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:17:09,473][257371] Reward + Measures: [[395.08602626   0.18157268   0.21920133   0.22458199   0.151813
    5.28805208]][0m
[37m[1m[2023-07-16 21:17:09,474][257371] Max Reward on eval: 395.0860262617094[0m
[37m[1m[2023-07-16 21:17:09,474][257371] Min Reward on eval: 395.0860262617094[0m
[37m[1m[2023-07-16 21:17:09,474][257371] Mean Reward across all agents: 395.0860262617094[0m
[37m[1m[2023-07-16 21:17:09,475][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:17:14,581][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:17:14,586][257371] Reward + Measures: [[135.47960183   0.23030002   0.2167       0.1605       0.20750001
    6.09928942]
 [163.78609899   0.2316       0.2199       0.2          0.14049999
    6.30712461]
 [174.63398795   0.1472       0.18520001   0.2024       0.18170002
    7.03812647]
 ...
 [144.32635007   0.15190001   0.1911       0.15120001   0.15619999
    6.11371374]
 [306.41004095   0.15640001   0.1902       0.2429       0.1847
    5.96836948]
 [139.48717621   0.1232       0.17639999   0.16559999   0.14470001
    6.54427958]][0m
[37m[1m[2023-07-16 21:17:14,587][257371] Max Reward on eval: 453.1485996365547[0m
[37m[1m[2023-07-16 21:17:14,587][257371] Min Reward on eval: -14.55218558695633[0m
[37m[1m[2023-07-16 21:17:14,587][257371] Mean Reward across all agents: 184.1357594221425[0m
[37m[1m[2023-07-16 21:17:14,588][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:17:14,590][257371] mean_value=-92.10369035265633, max_value=360.39682103842057[0m
[37m[1m[2023-07-16 21:17:14,593][257371] New mean coefficients: [[ 3.6590571  1.5612221 -0.7017579 -1.3458636 -2.0996635 -1.4191365]][0m
[37m[1m[2023-07-16 21:17:14,594][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:17:23,778][257371] train() took 9.18 seconds to complete[0m
[36m[2023-07-16 21:17:23,779][257371] FPS: 418160.71[0m
[36m[2023-07-16 21:17:23,781][257371] itr=66, itrs=2000, Progress: 3.30%[0m
[36m[2023-07-16 21:17:35,523][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-16 21:17:35,523][257371] FPS: 327723.82[0m
[36m[2023-07-16 21:17:39,836][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:17:39,837][257371] Reward + Measures: [[405.82308367   0.192277     0.22258167   0.23096567   0.15427034
    5.29751253]][0m
[37m[1m[2023-07-16 21:17:39,837][257371] Max Reward on eval: 405.82308367422536[0m
[37m[1m[2023-07-16 21:17:39,837][257371] Min Reward on eval: 405.82308367422536[0m
[37m[1m[2023-07-16 21:17:39,837][257371] Mean Reward across all agents: 405.82308367422536[0m
[37m[1m[2023-07-16 21:17:39,838][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:17:44,881][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:17:44,881][257371] Reward + Measures: [[189.16582866   0.1517       0.17050003   0.17400001   0.1538
    6.16060781]
 [ 80.27159784   0.0738       0.1006       0.0829       0.077
    5.67149401]
 [ 55.67353631   0.14400001   0.14410001   0.1318       0.14380001
    6.46224546]
 ...
 [ 82.95170995   0.12160001   0.12899999   0.1399       0.1243
    6.48060083]
 [169.04922873   0.13259999   0.149        0.17260002   0.15370002
    5.81267071]
 [187.9036479    0.17040001   0.1794       0.211        0.17130001
    6.11451483]][0m
[37m[1m[2023-07-16 21:17:44,881][257371] Max Reward on eval: 454.7258129492402[0m
[37m[1m[2023-07-16 21:17:44,882][257371] Min Reward on eval: -58.4343046550639[0m
[37m[1m[2023-07-16 21:17:44,882][257371] Mean Reward across all agents: 111.06138591416212[0m
[37m[1m[2023-07-16 21:17:44,882][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:17:44,884][257371] mean_value=-192.3568682439225, max_value=209.1642693617996[0m
[37m[1m[2023-07-16 21:17:44,887][257371] New mean coefficients: [[ 1.6772025  2.9908822  1.4509275 -1.061364  -1.9103297 -0.847926 ]][0m
[37m[1m[2023-07-16 21:17:44,888][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:17:54,133][257371] train() took 9.24 seconds to complete[0m
[36m[2023-07-16 21:17:54,133][257371] FPS: 415418.82[0m
[36m[2023-07-16 21:17:54,135][257371] itr=67, itrs=2000, Progress: 3.35%[0m
[36m[2023-07-16 21:18:05,797][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-16 21:18:05,798][257371] FPS: 330069.54[0m
[36m[2023-07-16 21:18:10,054][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:18:10,060][257371] Reward + Measures: [[392.56395431   0.184662     0.22012666   0.22623633   0.15511067
    5.27476835]][0m
[37m[1m[2023-07-16 21:18:10,060][257371] Max Reward on eval: 392.563954307809[0m
[37m[1m[2023-07-16 21:18:10,061][257371] Min Reward on eval: 392.563954307809[0m
[37m[1m[2023-07-16 21:18:10,061][257371] Mean Reward across all agents: 392.563954307809[0m
[37m[1m[2023-07-16 21:18:10,061][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:18:15,053][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:18:15,058][257371] Reward + Measures: [[47.15555546  0.09890001  0.0998      0.10110001  0.0982      5.62754202]
 [45.50566628  0.099       0.11440001  0.10160001  0.11009999  5.92936468]
 [90.8178568   0.0754      0.1074      0.0831      0.0892      6.02191114]
 ...
 [92.37788003  0.11379999  0.1497      0.11709999  0.11670001  5.65458012]
 [86.72618112  0.13770001  0.17029999  0.0895      0.1487      6.04610252]
 [97.62783031  0.16230001  0.18450001  0.15210001  0.1561      6.53076315]][0m
[37m[1m[2023-07-16 21:18:15,059][257371] Max Reward on eval: 454.47675252333283[0m
[37m[1m[2023-07-16 21:18:15,059][257371] Min Reward on eval: -54.572286405041815[0m
[37m[1m[2023-07-16 21:18:15,059][257371] Mean Reward across all agents: 122.10214915993063[0m
[37m[1m[2023-07-16 21:18:15,059][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:18:15,062][257371] mean_value=-194.33169761550897, max_value=553.2102290118446[0m
[37m[1m[2023-07-16 21:18:15,064][257371] New mean coefficients: [[-0.41418982  2.9040275   2.9623194  -2.0762033  -0.23330915 -0.0660491 ]][0m
[37m[1m[2023-07-16 21:18:15,065][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:18:24,087][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 21:18:24,087][257371] FPS: 425717.91[0m
[36m[2023-07-16 21:18:24,090][257371] itr=68, itrs=2000, Progress: 3.40%[0m
[36m[2023-07-16 21:18:36,256][257371] train() took 12.14 seconds to complete[0m
[36m[2023-07-16 21:18:36,257][257371] FPS: 316251.13[0m
[36m[2023-07-16 21:18:40,536][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:18:40,537][257371] Reward + Measures: [[301.65792003   0.15286566   0.18725301   0.18611534   0.13258433
    5.26446295]][0m
[37m[1m[2023-07-16 21:18:40,537][257371] Max Reward on eval: 301.657920033282[0m
[37m[1m[2023-07-16 21:18:40,537][257371] Min Reward on eval: 301.657920033282[0m
[37m[1m[2023-07-16 21:18:40,537][257371] Mean Reward across all agents: 301.657920033282[0m
[37m[1m[2023-07-16 21:18:40,538][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:18:45,614][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:18:45,615][257371] Reward + Measures: [[ 80.86465928   0.21520002   0.29229999   0.30510002   0.18529999
    6.16176128]
 [148.52741954   0.23790002   0.3292       0.31460002   0.34130001
    6.38105488]
 [123.53626671   0.11189999   0.16170001   0.13170001   0.12920001
    5.89032888]
 ...
 [ 16.63268927   0.13469999   0.16779999   0.16419999   0.13310002
    6.77511311]
 [280.74985769   0.20170002   0.22830001   0.2545       0.1693
    6.01042557]
 [ 52.01449212   0.11210001   0.16260001   0.17060001   0.1074
    6.51838064]][0m
[37m[1m[2023-07-16 21:18:45,615][257371] Max Reward on eval: 400.7737245768309[0m
[37m[1m[2023-07-16 21:18:45,615][257371] Min Reward on eval: -77.92007164023816[0m
[37m[1m[2023-07-16 21:18:45,616][257371] Mean Reward across all agents: 131.21143938336465[0m
[37m[1m[2023-07-16 21:18:45,616][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:18:45,618][257371] mean_value=-153.21698952023078, max_value=491.7169443878755[0m
[37m[1m[2023-07-16 21:18:45,621][257371] New mean coefficients: [[-1.6504965   2.2952318   4.441463   -2.9188397  -1.1804252   0.38630205]][0m
[37m[1m[2023-07-16 21:18:45,622][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:18:54,646][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 21:18:54,646][257371] FPS: 425611.46[0m
[36m[2023-07-16 21:18:54,649][257371] itr=69, itrs=2000, Progress: 3.45%[0m
[36m[2023-07-16 21:19:06,282][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-16 21:19:06,282][257371] FPS: 330799.77[0m
[36m[2023-07-16 21:19:10,613][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:19:10,619][257371] Reward + Measures: [[243.64174381   0.163038     0.18660468   0.19685833   0.11764866
    5.36119556]][0m
[37m[1m[2023-07-16 21:19:10,620][257371] Max Reward on eval: 243.6417438107069[0m
[37m[1m[2023-07-16 21:19:10,620][257371] Min Reward on eval: 243.6417438107069[0m
[37m[1m[2023-07-16 21:19:10,621][257371] Mean Reward across all agents: 243.6417438107069[0m
[37m[1m[2023-07-16 21:19:10,622][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:19:15,812][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:19:15,817][257371] Reward + Measures: [[44.52221402  0.05950001  0.0809      0.0847      0.07480001  6.63358068]
 [32.11162805  0.1638      0.12720001  0.19150001  0.1552      6.38119078]
 [ 0.93363757  0.08270001  0.10220001  0.1148      0.0973      6.14583588]
 ...
 [19.28427887  0.15710001  0.1988      0.125       0.16689999  5.9188652 ]
 [33.11874365  0.0603      0.0898      0.0868      0.07690001  6.3553195 ]
 [48.77537573  0.10950001  0.097       0.16250001  0.1084      6.12131453]][0m
[37m[1m[2023-07-16 21:19:15,817][257371] Max Reward on eval: 320.2042868435383[0m
[37m[1m[2023-07-16 21:19:15,818][257371] Min Reward on eval: -62.84197315010242[0m
[37m[1m[2023-07-16 21:19:15,818][257371] Mean Reward across all agents: 63.335034722077786[0m
[37m[1m[2023-07-16 21:19:15,818][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:19:15,820][257371] mean_value=-238.2474676953429, max_value=434.411466603312[0m
[37m[1m[2023-07-16 21:19:15,822][257371] New mean coefficients: [[ 0.07368982  0.6663829   4.4712152  -1.751201   -0.76788497 -0.03937909]][0m
[37m[1m[2023-07-16 21:19:15,823][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:19:24,927][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-16 21:19:24,927][257371] FPS: 421900.90[0m
[36m[2023-07-16 21:19:24,929][257371] itr=70, itrs=2000, Progress: 3.50%[0m
[37m[1m[2023-07-16 21:21:20,545][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000050[0m
[36m[2023-07-16 21:21:32,818][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-16 21:21:32,818][257371] FPS: 326608.54[0m
[36m[2023-07-16 21:21:36,961][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:21:36,961][257371] Reward + Measures: [[245.96267778   0.16718699   0.19160567   0.19829667   0.12289367
    5.36409187]][0m
[37m[1m[2023-07-16 21:21:36,961][257371] Max Reward on eval: 245.9626777752458[0m
[37m[1m[2023-07-16 21:21:36,961][257371] Min Reward on eval: 245.9626777752458[0m
[37m[1m[2023-07-16 21:21:36,962][257371] Mean Reward across all agents: 245.9626777752458[0m
[37m[1m[2023-07-16 21:21:36,962][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:21:41,987][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:21:41,988][257371] Reward + Measures: [[ 79.01154445   0.1515       0.1323       0.18300001   0.1155
    6.69399977]
 [ 96.05582066   0.1463       0.18370001   0.1961       0.10880001
    5.99701262]
 [138.94863066   0.1611       0.1822       0.18629999   0.147
    5.34218359]
 ...
 [164.40090321   0.1494       0.1857       0.20190001   0.14289999
    5.69958019]
 [156.56198311   0.1645       0.13850001   0.16059999   0.13870001
    5.97131729]
 [ 20.04731673   0.1123       0.1213       0.133        0.12390001
    6.42276478]][0m
[37m[1m[2023-07-16 21:21:41,988][257371] Max Reward on eval: 254.30258366465569[0m
[37m[1m[2023-07-16 21:21:41,989][257371] Min Reward on eval: -67.98097035433166[0m
[37m[1m[2023-07-16 21:21:41,989][257371] Mean Reward across all agents: 77.1663268079981[0m
[37m[1m[2023-07-16 21:21:41,989][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:21:41,990][257371] mean_value=-249.96557908319082, max_value=-98.55424475337416[0m
[36m[2023-07-16 21:21:41,992][257371] XNES is restarting with a new solution whose measures are [0.21640001 0.43199998 0.22660001 0.19100001 1.86169875] and objective is 1686.2915878102183[0m
[36m[2023-07-16 21:21:41,993][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-16 21:21:41,996][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-16 21:21:41,997][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:21:50,796][257371] train() took 8.80 seconds to complete[0m
[36m[2023-07-16 21:21:50,796][257371] FPS: 436493.59[0m
[36m[2023-07-16 21:21:50,799][257371] itr=71, itrs=2000, Progress: 3.55%[0m
[36m[2023-07-16 21:22:02,499][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-16 21:22:02,499][257371] FPS: 328907.07[0m
[36m[2023-07-16 21:22:06,822][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:22:06,828][257371] Reward + Measures: [[961.6678693    0.18852767   0.29373497   0.17429101   0.17678466
    2.54437518]][0m
[37m[1m[2023-07-16 21:22:06,828][257371] Max Reward on eval: 961.6678693044151[0m
[37m[1m[2023-07-16 21:22:06,828][257371] Min Reward on eval: 961.6678693044151[0m
[37m[1m[2023-07-16 21:22:06,829][257371] Mean Reward across all agents: 961.6678693044151[0m
[37m[1m[2023-07-16 21:22:06,829][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:22:11,852][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:22:11,858][257371] Reward + Measures: [[758.18328666   0.2149       0.31479999   0.1578       0.21300001
    2.76745772]
 [556.37517548   0.1463       0.20829999   0.16559999   0.154
    2.58790278]
 [626.30801916   0.1543       0.24969999   0.1821       0.1604
    2.66916728]
 ...
 [893.91336345   0.19140001   0.294        0.16010001   0.18560001
    2.59135556]
 [881.40298937   0.18609999   0.31569999   0.1596       0.1944
    2.69073844]
 [644.6990757    0.13250001   0.18260001   0.13530001   0.12639999
    2.74523211]][0m
[37m[1m[2023-07-16 21:22:11,858][257371] Max Reward on eval: 1118.6492653234861[0m
[37m[1m[2023-07-16 21:22:11,858][257371] Min Reward on eval: 170.96216998929157[0m
[37m[1m[2023-07-16 21:22:11,859][257371] Mean Reward across all agents: 587.2308633112314[0m
[37m[1m[2023-07-16 21:22:11,859][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:22:11,860][257371] mean_value=-5158.07729727818, max_value=866.8685083864257[0m
[37m[1m[2023-07-16 21:22:11,863][257371] New mean coefficients: [[ 0.9218279   0.16553819  0.8951864  -2.1882195  -0.82336986  0.76315916]][0m
[37m[1m[2023-07-16 21:22:11,864][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:22:20,920][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 21:22:20,920][257371] FPS: 424090.70[0m
[36m[2023-07-16 21:22:20,923][257371] itr=72, itrs=2000, Progress: 3.60%[0m
[36m[2023-07-16 21:22:32,788][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-16 21:22:32,788][257371] FPS: 324320.25[0m
[36m[2023-07-16 21:22:37,101][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:22:37,101][257371] Reward + Measures: [[1071.5404277     0.15748233    0.233234      0.13957433    0.12993866
     2.78416395]][0m
[37m[1m[2023-07-16 21:22:37,102][257371] Max Reward on eval: 1071.5404276999398[0m
[37m[1m[2023-07-16 21:22:37,102][257371] Min Reward on eval: 1071.5404276999398[0m
[37m[1m[2023-07-16 21:22:37,102][257371] Mean Reward across all agents: 1071.5404276999398[0m
[37m[1m[2023-07-16 21:22:37,102][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:22:42,139][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:22:42,140][257371] Reward + Measures: [[ 811.31305886    0.2309        0.32670003    0.16599999    0.23740001
     2.97824764]
 [ 760.5344906     0.1533        0.23650001    0.1513        0.1469
     2.97562599]
 [ 630.78029438    0.19          0.25150001    0.178         0.18750001
     3.10233569]
 ...
 [1060.85516352    0.15609999    0.22549999    0.1363        0.1229
     2.93772578]
 [ 340.18520786    0.1612        0.18270002    0.1211        0.14610001
     3.28174472]
 [ 898.31544492    0.1602        0.20999999    0.13700001    0.1313
     2.91488338]][0m
[37m[1m[2023-07-16 21:22:42,140][257371] Max Reward on eval: 1556.5791244119405[0m
[37m[1m[2023-07-16 21:22:42,141][257371] Min Reward on eval: 220.19017028007656[0m
[37m[1m[2023-07-16 21:22:42,141][257371] Mean Reward across all agents: 763.5947913079575[0m
[37m[1m[2023-07-16 21:22:42,141][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:22:42,142][257371] mean_value=-4360.712290744036, max_value=-2648.3307741839153[0m
[36m[2023-07-16 21:22:42,145][257371] XNES is restarting with a new solution whose measures are [0.3973     0.35870001 0.24230002 0.33510002 6.50285053] and objective is 277.4470425396692[0m
[36m[2023-07-16 21:22:42,146][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-16 21:22:42,148][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-16 21:22:42,149][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:22:51,258][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-16 21:22:51,258][257371] FPS: 421635.82[0m
[36m[2023-07-16 21:22:51,260][257371] itr=73, itrs=2000, Progress: 3.65%[0m
[36m[2023-07-16 21:23:03,041][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-16 21:23:03,041][257371] FPS: 326647.22[0m
[36m[2023-07-16 21:23:07,267][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:23:07,268][257371] Reward + Measures: [[218.10247772   0.34427997   0.32061264   0.21136867   0.30848998
    6.49959373]][0m
[37m[1m[2023-07-16 21:23:07,268][257371] Max Reward on eval: 218.10247771911455[0m
[37m[1m[2023-07-16 21:23:07,268][257371] Min Reward on eval: 218.10247771911455[0m
[37m[1m[2023-07-16 21:23:07,269][257371] Mean Reward across all agents: 218.10247771911455[0m
[37m[1m[2023-07-16 21:23:07,269][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:23:12,489][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:23:12,490][257371] Reward + Measures: [[ 273.61930855    0.396         0.38340002    0.1265        0.35460001
     7.07110834]
 [  99.05805861    0.20500003    0.20050001    0.21890001    0.1425
     6.08885288]
 [  56.99781123    0.1367        0.171         0.11919999    0.0782
     6.83565664]
 ...
 [ 159.99907111    0.54770011    0.56910002    0.5036        0.1538
     6.56209135]
 [-192.34955082    0.68580002    0.67000002    0.76169997    0.029
     7.212533  ]
 [  66.26426124    0.1851        0.1498        0.21100001    0.14890002
     6.01647711]][0m
[37m[1m[2023-07-16 21:23:12,490][257371] Max Reward on eval: 555.8151993958279[0m
[37m[1m[2023-07-16 21:23:12,491][257371] Min Reward on eval: -215.20351120177656[0m
[37m[1m[2023-07-16 21:23:12,491][257371] Mean Reward across all agents: 118.02121997912191[0m
[37m[1m[2023-07-16 21:23:12,491][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:23:12,501][257371] mean_value=61.22489549324404, max_value=1055.815199395828[0m
[37m[1m[2023-07-16 21:23:12,504][257371] New mean coefficients: [[-1.0025513   0.10437036  0.5633185  -3.6668186  -2.4947724  -0.40401602]][0m
[37m[1m[2023-07-16 21:23:12,505][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:23:21,517][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 21:23:21,518][257371] FPS: 426156.06[0m
[36m[2023-07-16 21:23:21,520][257371] itr=74, itrs=2000, Progress: 3.70%[0m
[36m[2023-07-16 21:23:33,791][257371] train() took 12.25 seconds to complete[0m
[36m[2023-07-16 21:23:33,791][257371] FPS: 313569.04[0m
[36m[2023-07-16 21:23:38,218][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:23:38,218][257371] Reward + Measures: [[232.19355347   0.31646231   0.30925834   0.220525     0.24009199
    6.01734114]][0m
[37m[1m[2023-07-16 21:23:38,219][257371] Max Reward on eval: 232.19355346601156[0m
[37m[1m[2023-07-16 21:23:38,219][257371] Min Reward on eval: 232.19355346601156[0m
[37m[1m[2023-07-16 21:23:38,219][257371] Mean Reward across all agents: 232.19355346601156[0m
[37m[1m[2023-07-16 21:23:38,219][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:23:43,324][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:23:43,331][257371] Reward + Measures: [[-51.40819833   0.50789994   0.5226       0.47670004   0.47779998
    7.16197205]
 [219.69781606   0.27180001   0.2352       0.34190002   0.18400002
    6.28977776]
 [361.55940971   0.77789998   0.76189995   0.75200003   0.72800004
    7.68965483]
 ...
 [ 78.03435446   0.2775       0.26300001   0.27630001   0.24960001
    6.83066416]
 [ 47.01991677   0.19320001   0.14430001   0.2067       0.0766
    7.08316803]
 [400.29520865   0.83770001   0.81350005   0.75559998   0.79180002
    7.44424009]][0m
[37m[1m[2023-07-16 21:23:43,331][257371] Max Reward on eval: 403.4933412264101[0m
[37m[1m[2023-07-16 21:23:43,332][257371] Min Reward on eval: -139.12669728286565[0m
[37m[1m[2023-07-16 21:23:43,332][257371] Mean Reward across all agents: 84.36735110009072[0m
[37m[1m[2023-07-16 21:23:43,333][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:23:43,346][257371] mean_value=1.513354181677302, max_value=903.4933412264102[0m
[37m[1m[2023-07-16 21:23:43,350][257371] New mean coefficients: [[ 0.7550802  1.306062  -0.9167081 -4.4828134 -3.516335  -0.8345473]][0m
[37m[1m[2023-07-16 21:23:43,351][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:23:52,430][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 21:23:52,430][257371] FPS: 423056.01[0m
[36m[2023-07-16 21:23:52,432][257371] itr=75, itrs=2000, Progress: 3.75%[0m
[36m[2023-07-16 21:24:04,182][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-16 21:24:04,182][257371] FPS: 327580.23[0m
[36m[2023-07-16 21:24:08,491][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:24:08,491][257371] Reward + Measures: [[270.24429532   0.27693367   0.25512934   0.21074232   0.159591
    6.04836845]][0m
[37m[1m[2023-07-16 21:24:08,492][257371] Max Reward on eval: 270.24429532393543[0m
[37m[1m[2023-07-16 21:24:08,492][257371] Min Reward on eval: 270.24429532393543[0m
[37m[1m[2023-07-16 21:24:08,492][257371] Mean Reward across all agents: 270.24429532393543[0m
[37m[1m[2023-07-16 21:24:08,492][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:24:13,526][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:24:13,526][257371] Reward + Measures: [[ -71.16357207    0.53439999    0.5413        0.56959999    0.0672
     7.48908091]
 [  10.3071169     0.1586        0.16250001    0.1279        0.12910001
     6.98075485]
 [  11.82192474    0.155         0.1441        0.1732        0.0698
     6.9957819 ]
 ...
 [-110.33699998    0.49469995    0.48970005    0.54440004    0.0676
     7.37946081]
 [  90.06465054    0.1459        0.2024        0.13360001    0.17400001
     5.78346205]
 [  19.07309925    0.23810001    0.24370001    0.28889999    0.062
     6.99454641]][0m
[37m[1m[2023-07-16 21:24:13,527][257371] Max Reward on eval: 376.16913810614494[0m
[37m[1m[2023-07-16 21:24:13,527][257371] Min Reward on eval: -443.94635104686023[0m
[37m[1m[2023-07-16 21:24:13,527][257371] Mean Reward across all agents: 53.31424083579563[0m
[37m[1m[2023-07-16 21:24:13,527][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:24:13,536][257371] mean_value=-25.282735022710828, max_value=764.2185666657984[0m
[37m[1m[2023-07-16 21:24:13,538][257371] New mean coefficients: [[-0.21598423  1.1880746   0.03388166 -6.504942   -3.0303001  -1.2680087 ]][0m
[37m[1m[2023-07-16 21:24:13,539][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:24:22,635][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-16 21:24:22,636][257371] FPS: 422236.33[0m
[36m[2023-07-16 21:24:22,638][257371] itr=76, itrs=2000, Progress: 3.80%[0m
[36m[2023-07-16 21:24:34,519][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-16 21:24:34,519][257371] FPS: 323940.13[0m
[36m[2023-07-16 21:24:38,898][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:24:38,898][257371] Reward + Measures: [[243.75168723   0.26916403   0.24714066   0.21101435   0.149829
    6.05106068]][0m
[37m[1m[2023-07-16 21:24:38,898][257371] Max Reward on eval: 243.75168722870433[0m
[37m[1m[2023-07-16 21:24:38,899][257371] Min Reward on eval: 243.75168722870433[0m
[37m[1m[2023-07-16 21:24:38,899][257371] Mean Reward across all agents: 243.75168722870433[0m
[37m[1m[2023-07-16 21:24:38,899][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:24:44,035][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:24:44,035][257371] Reward + Measures: [[162.50338868   0.1376       0.24030001   0.18959999   0.2667
    6.50870132]
 [125.19584773   0.14819999   0.16429999   0.1696       0.13259999
    6.68938541]
 [ 36.469146     0.11470001   0.1372       0.16740002   0.13929999
    6.93312788]
 ...
 [ 34.98637405   0.1293       0.1462       0.1432       0.14380001
    6.72080851]
 [191.80823374   0.2599       0.23169999   0.1636       0.1584
    6.43713522]
 [ 75.88665457   0.1193       0.12950002   0.1375       0.0952
    6.83455801]][0m
[37m[1m[2023-07-16 21:24:44,035][257371] Max Reward on eval: 299.48603531830014[0m
[37m[1m[2023-07-16 21:24:44,036][257371] Min Reward on eval: -125.62991804610938[0m
[37m[1m[2023-07-16 21:24:44,036][257371] Mean Reward across all agents: 92.59658112419577[0m
[37m[1m[2023-07-16 21:24:44,036][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:24:44,039][257371] mean_value=-98.21000215482144, max_value=745.1235112849623[0m
[37m[1m[2023-07-16 21:24:44,042][257371] New mean coefficients: [[ 0.25655675  2.1843355  -1.2837477  -5.076668   -2.8785105  -1.7202439 ]][0m
[37m[1m[2023-07-16 21:24:44,043][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:24:53,159][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-16 21:24:53,159][257371] FPS: 421307.31[0m
[36m[2023-07-16 21:24:53,162][257371] itr=77, itrs=2000, Progress: 3.85%[0m
[36m[2023-07-16 21:25:04,916][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-16 21:25:04,916][257371] FPS: 327470.14[0m
[36m[2023-07-16 21:25:09,288][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:25:09,289][257371] Reward + Measures: [[236.44497893   0.31086868   0.25882268   0.23548867   0.15333267
    5.7827239 ]][0m
[37m[1m[2023-07-16 21:25:09,289][257371] Max Reward on eval: 236.44497893284776[0m
[37m[1m[2023-07-16 21:25:09,289][257371] Min Reward on eval: 236.44497893284776[0m
[37m[1m[2023-07-16 21:25:09,289][257371] Mean Reward across all agents: 236.44497893284776[0m
[37m[1m[2023-07-16 21:25:09,290][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:25:14,387][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:25:14,387][257371] Reward + Measures: [[ 44.92754539   0.23260002   0.24489999   0.26890001   0.1627
    6.30383062]
 [ 36.89006488   0.12030001   0.1398       0.1647       0.0944
    6.31649017]
 [ -0.97913083   0.1356       0.10420001   0.13070001   0.0888
    6.11220884]
 ...
 [112.07271512   0.28669998   0.29280004   0.32890001   0.1847
    6.59043646]
 [ 30.30056079   0.1684       0.1672       0.25190002   0.09380001
    6.07817411]
 [ 90.95189785   0.454        0.37709999   0.5413       0.112
    6.90905523]][0m
[37m[1m[2023-07-16 21:25:14,387][257371] Max Reward on eval: 439.37919902801514[0m
[37m[1m[2023-07-16 21:25:14,388][257371] Min Reward on eval: -50.76723111369647[0m
[37m[1m[2023-07-16 21:25:14,388][257371] Mean Reward across all agents: 136.4049734923104[0m
[37m[1m[2023-07-16 21:25:14,388][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:25:14,392][257371] mean_value=-90.74269083087235, max_value=837.7164873099141[0m
[37m[1m[2023-07-16 21:25:14,395][257371] New mean coefficients: [[ 1.1107211   1.3790689  -0.40664393 -5.5855517  -1.3984655  -1.9268363 ]][0m
[37m[1m[2023-07-16 21:25:14,396][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:25:23,513][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-16 21:25:23,513][257371] FPS: 421249.19[0m
[36m[2023-07-16 21:25:23,516][257371] itr=78, itrs=2000, Progress: 3.90%[0m
[36m[2023-07-16 21:25:35,203][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-16 21:25:35,204][257371] FPS: 329376.73[0m
[36m[2023-07-16 21:25:39,556][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:25:39,557][257371] Reward + Measures: [[128.67320813   0.21250966   0.19397631   0.16157667   0.1259
    5.48009157]][0m
[37m[1m[2023-07-16 21:25:39,557][257371] Max Reward on eval: 128.67320813440264[0m
[37m[1m[2023-07-16 21:25:39,557][257371] Min Reward on eval: 128.67320813440264[0m
[37m[1m[2023-07-16 21:25:39,558][257371] Mean Reward across all agents: 128.67320813440264[0m
[37m[1m[2023-07-16 21:25:39,558][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:25:44,786][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:25:44,787][257371] Reward + Measures: [[176.56729914   0.2199       0.15720001   0.1981       0.16260003
    6.11418295]
 [ 68.71934556   0.09750001   0.1124       0.1032       0.11140001
    6.22884321]
 [323.41367715   0.54619998   0.45110002   0.54510003   0.1225
    7.4553771 ]
 ...
 [ 59.20899553   0.12240001   0.1237       0.1103       0.08200001
    6.6387248 ]
 [ 65.87817362   0.1038       0.099        0.0919       0.0737
    6.77525425]
 [ 31.76534515   0.0952       0.0909       0.0883       0.07120001
    7.07036734]][0m
[37m[1m[2023-07-16 21:25:44,787][257371] Max Reward on eval: 323.4136771526188[0m
[37m[1m[2023-07-16 21:25:44,787][257371] Min Reward on eval: -37.66086227996275[0m
[37m[1m[2023-07-16 21:25:44,788][257371] Mean Reward across all agents: 87.45795636362355[0m
[37m[1m[2023-07-16 21:25:44,788][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:25:44,790][257371] mean_value=-175.031180877671, max_value=736.5741014238494[0m
[37m[1m[2023-07-16 21:25:44,792][257371] New mean coefficients: [[ 1.5375187  2.6958065 -1.8479123 -6.7219806 -1.1775099 -1.9474291]][0m
[37m[1m[2023-07-16 21:25:44,793][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:25:53,845][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 21:25:53,845][257371] FPS: 424317.76[0m
[36m[2023-07-16 21:25:53,847][257371] itr=79, itrs=2000, Progress: 3.95%[0m
[36m[2023-07-16 21:26:05,456][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-16 21:26:05,456][257371] FPS: 331515.67[0m
[36m[2023-07-16 21:26:09,728][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:26:09,729][257371] Reward + Measures: [[85.10685454  0.23262432  0.18872099  0.20224467  0.11775433  5.67630482]][0m
[37m[1m[2023-07-16 21:26:09,729][257371] Max Reward on eval: 85.10685453879508[0m
[37m[1m[2023-07-16 21:26:09,729][257371] Min Reward on eval: 85.10685453879508[0m
[37m[1m[2023-07-16 21:26:09,730][257371] Mean Reward across all agents: 85.10685453879508[0m
[37m[1m[2023-07-16 21:26:09,730][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:26:14,827][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:26:14,828][257371] Reward + Measures: [[184.50493638   0.23009999   0.2516       0.26109999   0.1763
    6.54792356]
 [122.18409789   0.12260001   0.1165       0.0744       0.0799
    7.06641006]
 [194.37909698   0.2703       0.22350001   0.23220001   0.15020001
    6.05829859]
 ...
 [200.68140545   0.2723       0.26880002   0.33109999   0.1946
    6.90612125]
 [ 47.43743671   0.09600001   0.10179999   0.09030001   0.0831
    6.29387426]
 [ 61.30364371   0.1038       0.0924       0.0733       0.0665
    6.2272253 ]][0m
[37m[1m[2023-07-16 21:26:14,828][257371] Max Reward on eval: 302.09704064999244[0m
[37m[1m[2023-07-16 21:26:14,828][257371] Min Reward on eval: -57.91478162351996[0m
[37m[1m[2023-07-16 21:26:14,828][257371] Mean Reward across all agents: 106.6828038517413[0m
[37m[1m[2023-07-16 21:26:14,829][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:26:14,831][257371] mean_value=-133.9534081988178, max_value=483.31820472788075[0m
[37m[1m[2023-07-16 21:26:14,834][257371] New mean coefficients: [[ 2.289705   5.186103  -1.619983  -7.072602  -2.3394434 -2.0925748]][0m
[37m[1m[2023-07-16 21:26:14,835][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:26:23,954][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-16 21:26:23,954][257371] FPS: 421173.17[0m
[36m[2023-07-16 21:26:23,956][257371] itr=80, itrs=2000, Progress: 4.00%[0m
[37m[1m[2023-07-16 21:28:19,146][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000060[0m
[36m[2023-07-16 21:28:31,532][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-16 21:28:31,533][257371] FPS: 327577.25[0m
[36m[2023-07-16 21:28:35,691][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:28:35,691][257371] Reward + Measures: [[119.79648727   0.21836568   0.17967632   0.20506667   0.122886
    5.61644936]][0m
[37m[1m[2023-07-16 21:28:35,692][257371] Max Reward on eval: 119.79648726715094[0m
[37m[1m[2023-07-16 21:28:35,692][257371] Min Reward on eval: 119.79648726715094[0m
[37m[1m[2023-07-16 21:28:35,692][257371] Mean Reward across all agents: 119.79648726715094[0m
[37m[1m[2023-07-16 21:28:35,692][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:28:40,753][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:28:40,753][257371] Reward + Measures: [[ 62.92887968   0.1078       0.1069       0.0882       0.11140001
    6.2976613 ]
 [103.42154601   0.122        0.12550001   0.1382       0.12810001
    6.15264845]
 [ 73.06222452   0.10810001   0.09550001   0.08890001   0.098
    6.35125589]
 ...
 [138.87638547   0.1692       0.16180001   0.13         0.14830001
    6.38092995]
 [ 55.05838049   0.10700001   0.1105       0.07929999   0.09500001
    6.42815542]
 [ 72.88723461   0.1541       0.1398       0.1714       0.1148
    5.86069059]][0m
[37m[1m[2023-07-16 21:28:40,754][257371] Max Reward on eval: 332.01515955924987[0m
[37m[1m[2023-07-16 21:28:40,754][257371] Min Reward on eval: -7.419131360109896[0m
[37m[1m[2023-07-16 21:28:40,754][257371] Mean Reward across all agents: 115.56473450417182[0m
[37m[1m[2023-07-16 21:28:40,754][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:28:40,756][257371] mean_value=-187.6839423265508, max_value=32.40663388757807[0m
[37m[1m[2023-07-16 21:28:40,758][257371] New mean coefficients: [[ 3.5596652  3.4723568 -1.75806   -6.2228127 -3.330884  -2.0646107]][0m
[37m[1m[2023-07-16 21:28:40,759][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:28:49,630][257371] train() took 8.87 seconds to complete[0m
[36m[2023-07-16 21:28:49,631][257371] FPS: 432934.23[0m
[36m[2023-07-16 21:28:49,633][257371] itr=81, itrs=2000, Progress: 4.05%[0m
[36m[2023-07-16 21:29:01,523][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-16 21:29:01,523][257371] FPS: 323751.75[0m
[36m[2023-07-16 21:29:05,790][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:29:05,790][257371] Reward + Measures: [[120.18299004   0.165751     0.14408933   0.12907167   0.101088
    5.6677804 ]][0m
[37m[1m[2023-07-16 21:29:05,791][257371] Max Reward on eval: 120.18299004380182[0m
[37m[1m[2023-07-16 21:29:05,791][257371] Min Reward on eval: 120.18299004380182[0m
[37m[1m[2023-07-16 21:29:05,791][257371] Mean Reward across all agents: 120.18299004380182[0m
[37m[1m[2023-07-16 21:29:05,791][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:29:10,797][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:29:10,803][257371] Reward + Measures: [[106.63648891   0.10110001   0.0992       0.11580002   0.10320001
    6.21345186]
 [157.77764144   0.1364       0.1513       0.1275       0.10220001
    6.28642988]
 [ 86.87689042   0.10210001   0.1123       0.09170001   0.08790001
    6.18465805]
 ...
 [112.45723614   0.12969999   0.10390001   0.1499       0.1056
    6.37177658]
 [ 80.12959003   0.0991       0.10640001   0.1051       0.0881
    6.28900623]
 [ 37.22164193   0.1037       0.0898       0.07970001   0.0724
    6.56423903]][0m
[37m[1m[2023-07-16 21:29:10,803][257371] Max Reward on eval: 271.8522272145725[0m
[37m[1m[2023-07-16 21:29:10,803][257371] Min Reward on eval: -58.08311148378998[0m
[37m[1m[2023-07-16 21:29:10,803][257371] Mean Reward across all agents: 98.90811955539641[0m
[37m[1m[2023-07-16 21:29:10,804][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:29:10,805][257371] mean_value=-196.06352265692684, max_value=-58.361444499559695[0m
[36m[2023-07-16 21:29:10,807][257371] XNES is restarting with a new solution whose measures are [0.33790001 0.37360001 0.1645     0.28060004 6.87635899] and objective is 161.94682115302422[0m
[36m[2023-07-16 21:29:10,808][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-16 21:29:10,810][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-16 21:29:10,811][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:29:19,737][257371] train() took 8.92 seconds to complete[0m
[36m[2023-07-16 21:29:19,737][257371] FPS: 430271.11[0m
[36m[2023-07-16 21:29:19,740][257371] itr=82, itrs=2000, Progress: 4.10%[0m
[36m[2023-07-16 21:29:31,329][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-16 21:29:31,329][257371] FPS: 332055.38[0m
[36m[2023-07-16 21:29:35,633][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:29:35,634][257371] Reward + Measures: [[80.29859655  0.13010901  0.1459      0.11704166  0.10196299  6.54571915]][0m
[37m[1m[2023-07-16 21:29:35,634][257371] Max Reward on eval: 80.29859654539898[0m
[37m[1m[2023-07-16 21:29:35,634][257371] Min Reward on eval: 80.29859654539898[0m
[37m[1m[2023-07-16 21:29:35,634][257371] Mean Reward across all agents: 80.29859654539898[0m
[37m[1m[2023-07-16 21:29:35,635][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:29:40,811][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:29:40,817][257371] Reward + Measures: [[29.02289606  0.0952      0.0956      0.0842      0.1024      6.69500828]
 [66.73984368  0.1354      0.14840001  0.17580001  0.1274      6.48234177]
 [42.70923249  0.0676      0.086       0.0744      0.07050001  6.78832865]
 ...
 [85.1084529   0.0974      0.1138      0.1132      0.0981      6.06907892]
 [29.64793864  0.0673      0.1051      0.08640001  0.08380001  6.06554556]
 [69.56994678  0.1124      0.12630001  0.1416      0.1068      6.47403097]][0m
[37m[1m[2023-07-16 21:29:40,817][257371] Max Reward on eval: 335.70099202725106[0m
[37m[1m[2023-07-16 21:29:40,818][257371] Min Reward on eval: -96.64638486299664[0m
[37m[1m[2023-07-16 21:29:40,818][257371] Mean Reward across all agents: 53.3377167351845[0m
[37m[1m[2023-07-16 21:29:40,818][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:29:40,820][257371] mean_value=-204.249929810152, max_value=382.4788161621882[0m
[37m[1m[2023-07-16 21:29:40,823][257371] New mean coefficients: [[-0.00265333  0.28159618 -1.1575782   0.39300454 -0.78399557 -0.9619905 ]][0m
[37m[1m[2023-07-16 21:29:40,824][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:29:49,812][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-16 21:29:49,813][257371] FPS: 427276.71[0m
[36m[2023-07-16 21:29:49,815][257371] itr=83, itrs=2000, Progress: 4.15%[0m
[36m[2023-07-16 21:30:01,589][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-16 21:30:01,589][257371] FPS: 326949.94[0m
[36m[2023-07-16 21:30:05,874][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:30:05,880][257371] Reward + Measures: [[75.77533589  0.14246966  0.15739432  0.12672266  0.11294834  6.48309088]][0m
[37m[1m[2023-07-16 21:30:05,880][257371] Max Reward on eval: 75.7753358890142[0m
[37m[1m[2023-07-16 21:30:05,880][257371] Min Reward on eval: 75.7753358890142[0m
[37m[1m[2023-07-16 21:30:05,881][257371] Mean Reward across all agents: 75.7753358890142[0m
[37m[1m[2023-07-16 21:30:05,881][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:30:10,852][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:30:10,853][257371] Reward + Measures: [[  53.5679501     0.28099999    0.28480002    0.1761        0.2757
     6.9131608 ]
 [  86.08719702    0.075         0.0882        0.08720001    0.0768
     6.29311323]
 [-224.25372947    0.68300003    0.6584        0.71830004    0.0126
     7.32269049]
 ...
 [ -29.63193904    0.34490001    0.35119998    0.35350001    0.09960001
     7.17668009]
 [ 223.64449336    0.2895        0.2881        0.13500001    0.3012
     6.99548054]
 [  77.96817277    0.1455        0.1193        0.20710002    0.09350001
     6.32202244]][0m
[37m[1m[2023-07-16 21:30:10,853][257371] Max Reward on eval: 491.710521734599[0m
[37m[1m[2023-07-16 21:30:10,853][257371] Min Reward on eval: -480.0608121907338[0m
[37m[1m[2023-07-16 21:30:10,853][257371] Mean Reward across all agents: 42.114536130915006[0m
[37m[1m[2023-07-16 21:30:10,854][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:30:10,857][257371] mean_value=-156.06421118907082, max_value=991.7105217345991[0m
[37m[1m[2023-07-16 21:30:10,860][257371] New mean coefficients: [[ 0.4159091  2.857579  -2.1948822 -0.5459256 -1.5274968 -0.5724329]][0m
[37m[1m[2023-07-16 21:30:10,861][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:30:19,850][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-16 21:30:19,851][257371] FPS: 427247.68[0m
[36m[2023-07-16 21:30:19,853][257371] itr=84, itrs=2000, Progress: 4.20%[0m
[36m[2023-07-16 21:30:31,544][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-16 21:30:31,544][257371] FPS: 329273.31[0m
[36m[2023-07-16 21:30:35,892][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:30:35,898][257371] Reward + Measures: [[67.6692682   0.12833966  0.145487    0.11766367  0.10409833  6.44402742]][0m
[37m[1m[2023-07-16 21:30:35,898][257371] Max Reward on eval: 67.669268200436[0m
[37m[1m[2023-07-16 21:30:35,899][257371] Min Reward on eval: 67.669268200436[0m
[37m[1m[2023-07-16 21:30:35,899][257371] Mean Reward across all agents: 67.669268200436[0m
[37m[1m[2023-07-16 21:30:35,899][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:30:40,947][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:30:40,953][257371] Reward + Measures: [[ 39.8263972    0.10840001   0.1338       0.14330001   0.105
    7.08372593]
 [ 10.47408619   0.09630001   0.13689999   0.10450001   0.1204
    6.74032593]
 [-46.56908903   0.10480001   0.1102       0.122        0.0718
    6.83410978]
 ...
 [  0.47274214   0.0705       0.10320001   0.09949999   0.0852
    7.10474396]
 [ 30.83471789   0.10049999   0.094        0.11700001   0.0851
    6.69726181]
 [131.24423095   0.41660005   0.26009998   0.39340004   0.2405
    7.400249  ]][0m
[37m[1m[2023-07-16 21:30:40,953][257371] Max Reward on eval: 222.38498201705517[0m
[37m[1m[2023-07-16 21:30:40,954][257371] Min Reward on eval: -343.0880794537719[0m
[37m[1m[2023-07-16 21:30:40,954][257371] Mean Reward across all agents: 26.625127792269478[0m
[37m[1m[2023-07-16 21:30:40,954][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:30:40,956][257371] mean_value=-213.91886727892572, max_value=326.4174174558053[0m
[37m[1m[2023-07-16 21:30:40,959][257371] New mean coefficients: [[-0.4506824  3.0925786 -1.0630705 -1.2472978 -1.1131175 -0.6159712]][0m
[37m[1m[2023-07-16 21:30:40,960][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:30:49,985][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 21:30:49,986][257371] FPS: 425543.23[0m
[36m[2023-07-16 21:30:49,988][257371] itr=85, itrs=2000, Progress: 4.25%[0m
[36m[2023-07-16 21:31:01,618][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-16 21:31:01,619][257371] FPS: 330877.61[0m
[36m[2023-07-16 21:31:05,941][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:31:05,946][257371] Reward + Measures: [[79.05204795  0.14958301  0.173011    0.12074433  0.12327866  6.32800531]][0m
[37m[1m[2023-07-16 21:31:05,947][257371] Max Reward on eval: 79.05204795228302[0m
[37m[1m[2023-07-16 21:31:05,947][257371] Min Reward on eval: 79.05204795228302[0m
[37m[1m[2023-07-16 21:31:05,947][257371] Mean Reward across all agents: 79.05204795228302[0m
[37m[1m[2023-07-16 21:31:05,947][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:31:10,992][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:31:10,998][257371] Reward + Measures: [[  8.81417582   0.071        0.0957       0.0964       0.0918
    6.48899221]
 [  3.59904577   0.23220001   0.26540002   0.0917       0.19970001
    6.50063181]
 [-21.83702993   0.26540002   0.27540001   0.24139999   0.14820001
    7.14472961]
 ...
 [-31.55387843   0.4975       0.43600002   0.45430002   0.1182
    7.05451202]
 [  2.74788444   0.0842       0.09690001   0.10210001   0.0915
    6.72550821]
 [282.73256621   0.50369996   0.51440001   0.0479       0.46100003
    7.06702948]][0m
[37m[1m[2023-07-16 21:31:10,998][257371] Max Reward on eval: 398.8212308164686[0m
[37m[1m[2023-07-16 21:31:10,998][257371] Min Reward on eval: -248.22793140783907[0m
[37m[1m[2023-07-16 21:31:10,999][257371] Mean Reward across all agents: 46.42080408725262[0m
[37m[1m[2023-07-16 21:31:10,999][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:31:11,002][257371] mean_value=-165.37111528453957, max_value=707.9741871106996[0m
[37m[1m[2023-07-16 21:31:11,005][257371] New mean coefficients: [[-1.3071707   1.9052583  -2.3984566  -3.262844   -1.2018714  -0.21701542]][0m
[37m[1m[2023-07-16 21:31:11,006][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:31:20,060][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 21:31:20,061][257371] FPS: 424161.85[0m
[36m[2023-07-16 21:31:20,063][257371] itr=86, itrs=2000, Progress: 4.30%[0m
[36m[2023-07-16 21:31:31,903][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-16 21:31:31,904][257371] FPS: 325006.71[0m
[36m[2023-07-16 21:31:36,213][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:31:36,214][257371] Reward + Measures: [[109.79677483   0.208776     0.21462968   0.14831567   0.15401399
    6.5241313 ]][0m
[37m[1m[2023-07-16 21:31:36,214][257371] Max Reward on eval: 109.79677483487195[0m
[37m[1m[2023-07-16 21:31:36,214][257371] Min Reward on eval: 109.79677483487195[0m
[37m[1m[2023-07-16 21:31:36,215][257371] Mean Reward across all agents: 109.79677483487195[0m
[37m[1m[2023-07-16 21:31:36,215][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:31:41,383][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:31:41,389][257371] Reward + Measures: [[124.67938187   0.245        0.2511       0.17020001   0.1753
    7.1573534 ]
 [ 34.44530949   0.15369999   0.1988       0.0999       0.1753
    7.08365488]
 [123.21081858   0.1244       0.14380001   0.1217       0.12720001
    6.55082703]
 ...
 [178.14314622   0.23179999   0.2613       0.1122       0.21620002
    7.33936787]
 [139.96973942   0.1436       0.16059999   0.09680001   0.14130001
    6.66283894]
 [ -4.02725656   0.31290001   0.31110001   0.0934       0.28200004
    7.12799835]][0m
[37m[1m[2023-07-16 21:31:41,389][257371] Max Reward on eval: 366.3623804394156[0m
[37m[1m[2023-07-16 21:31:41,390][257371] Min Reward on eval: -49.21477172067389[0m
[37m[1m[2023-07-16 21:31:41,390][257371] Mean Reward across all agents: 75.34312149012653[0m
[37m[1m[2023-07-16 21:31:41,390][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:31:41,394][257371] mean_value=-119.06457799084932, max_value=439.6927474544694[0m
[37m[1m[2023-07-16 21:31:41,397][257371] New mean coefficients: [[-2.8958616   1.7616783  -3.7658188  -4.4622035  -0.61611396  0.13153583]][0m
[37m[1m[2023-07-16 21:31:41,398][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:31:50,456][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-16 21:31:50,456][257371] FPS: 423994.08[0m
[36m[2023-07-16 21:31:50,458][257371] itr=87, itrs=2000, Progress: 4.35%[0m
[36m[2023-07-16 21:32:02,211][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-16 21:32:02,211][257371] FPS: 327465.59[0m
[36m[2023-07-16 21:32:06,580][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:32:06,581][257371] Reward + Measures: [[166.43390159   0.27408233   0.28106099   0.14350566   0.221852
    6.6680336 ]][0m
[37m[1m[2023-07-16 21:32:06,581][257371] Max Reward on eval: 166.43390159251322[0m
[37m[1m[2023-07-16 21:32:06,581][257371] Min Reward on eval: 166.43390159251322[0m
[37m[1m[2023-07-16 21:32:06,582][257371] Mean Reward across all agents: 166.43390159251322[0m
[37m[1m[2023-07-16 21:32:06,582][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:32:11,652][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:32:11,657][257371] Reward + Measures: [[ 70.39256317   0.22670002   0.21689999   0.26480001   0.11799999
    7.00686741]
 [ 40.47366571   0.53790003   0.51940006   0.42490003   0.2163
    7.30932474]
 [ 75.77112845   0.13780001   0.08720001   0.1663       0.1402
    7.15451765]
 ...
 [-37.13660934   0.81040001   0.42470002   0.71250004   0.223
    7.46511459]
 [ 36.35008208   0.08460001   0.0929       0.1128       0.10500001
    6.70115232]
 [  7.51664019   0.07660001   0.0732       0.0722       0.0806
    6.80444479]][0m
[37m[1m[2023-07-16 21:32:11,658][257371] Max Reward on eval: 441.25797365568576[0m
[37m[1m[2023-07-16 21:32:11,658][257371] Min Reward on eval: -295.036221998604[0m
[37m[1m[2023-07-16 21:32:11,658][257371] Mean Reward across all agents: 64.0441688121263[0m
[37m[1m[2023-07-16 21:32:11,659][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:32:11,666][257371] mean_value=-56.7063407692234, max_value=941.2579736556858[0m
[37m[1m[2023-07-16 21:32:11,669][257371] New mean coefficients: [[-2.9498336   3.7831988  -3.8281584  -3.1278594  -2.628356    0.11080308]][0m
[37m[1m[2023-07-16 21:32:11,670][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:32:20,801][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-16 21:32:20,802][257371] FPS: 420596.26[0m
[36m[2023-07-16 21:32:20,804][257371] itr=88, itrs=2000, Progress: 4.40%[0m
[36m[2023-07-16 21:32:32,439][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-16 21:32:32,439][257371] FPS: 330863.69[0m
[36m[2023-07-16 21:32:36,717][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:32:36,723][257371] Reward + Measures: [[74.91306807  0.32628968  0.35586229  0.120036    0.27149498  6.78080702]][0m
[37m[1m[2023-07-16 21:32:36,723][257371] Max Reward on eval: 74.91306806534314[0m
[37m[1m[2023-07-16 21:32:36,723][257371] Min Reward on eval: 74.91306806534314[0m
[37m[1m[2023-07-16 21:32:36,724][257371] Mean Reward across all agents: 74.91306806534314[0m
[37m[1m[2023-07-16 21:32:36,724][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:32:41,745][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:32:41,751][257371] Reward + Measures: [[33.11850236  0.0963      0.10340001  0.0841      0.0814      6.62557554]
 [39.09844918  0.086       0.0997      0.0892      0.0801      6.72164011]
 [44.05418637  0.1008      0.1158      0.11370001  0.08660001  6.6461587 ]
 ...
 [32.05794252  0.098       0.10420001  0.11330002  0.0884      6.79607248]
 [17.30554305  0.0976      0.10879999  0.1025      0.08860001  6.46979904]
 [59.87851412  0.0868      0.1186      0.10950001  0.091       6.72813034]][0m
[37m[1m[2023-07-16 21:32:41,751][257371] Max Reward on eval: 349.4176988825202[0m
[37m[1m[2023-07-16 21:32:41,751][257371] Min Reward on eval: -94.90372185315937[0m
[37m[1m[2023-07-16 21:32:41,752][257371] Mean Reward across all agents: 78.94835436574179[0m
[37m[1m[2023-07-16 21:32:41,752][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:32:41,754][257371] mean_value=-142.6501556004085, max_value=818.3694553234242[0m
[37m[1m[2023-07-16 21:32:41,757][257371] New mean coefficients: [[-1.7949351   5.090597   -5.0700226  -1.3642595  -1.4783192   0.02909581]][0m
[37m[1m[2023-07-16 21:32:41,758][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:32:50,734][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-16 21:32:50,734][257371] FPS: 427881.22[0m
[36m[2023-07-16 21:32:50,736][257371] itr=89, itrs=2000, Progress: 4.45%[0m
[36m[2023-07-16 21:33:02,546][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-16 21:33:02,547][257371] FPS: 325846.17[0m
[36m[2023-07-16 21:33:07,023][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:33:07,023][257371] Reward + Measures: [[41.93612891  0.35285932  0.37110499  0.10721967  0.29872331  6.92466211]][0m
[37m[1m[2023-07-16 21:33:07,024][257371] Max Reward on eval: 41.936128908076874[0m
[37m[1m[2023-07-16 21:33:07,024][257371] Min Reward on eval: 41.936128908076874[0m
[37m[1m[2023-07-16 21:33:07,024][257371] Mean Reward across all agents: 41.936128908076874[0m
[37m[1m[2023-07-16 21:33:07,024][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:33:12,113][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:33:12,113][257371] Reward + Measures: [[93.94415534  0.12560001  0.1296      0.1304      0.1243      6.47942495]
 [79.82970178  0.15870002  0.13160001  0.1531      0.1084      6.5754199 ]
 [47.20764437  0.20740001  0.16409999  0.2422      0.18959999  6.87920713]
 ...
 [77.01166765  0.14710002  0.1481      0.1697      0.0913      6.76991749]
 [70.42233242  0.15360001  0.1523      0.1532      0.14210001  6.78497171]
 [36.06871782  0.20030001  0.1753      0.20250002  0.13380001  7.01395416]][0m
[37m[1m[2023-07-16 21:33:12,113][257371] Max Reward on eval: 394.58372475672513[0m
[37m[1m[2023-07-16 21:33:12,114][257371] Min Reward on eval: -47.6524619075004[0m
[37m[1m[2023-07-16 21:33:12,114][257371] Mean Reward across all agents: 78.3203867102516[0m
[37m[1m[2023-07-16 21:33:12,114][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:33:12,116][257371] mean_value=-163.71986955797416, max_value=561.8804420247807[0m
[37m[1m[2023-07-16 21:33:12,119][257371] New mean coefficients: [[-2.1286352   3.6804504  -3.1619577  -0.20217204 -1.5049624  -0.05077004]][0m
[37m[1m[2023-07-16 21:33:12,120][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:33:21,122][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-16 21:33:21,122][257371] FPS: 426628.05[0m
[36m[2023-07-16 21:33:21,125][257371] itr=90, itrs=2000, Progress: 4.50%[0m
[37m[1m[2023-07-16 21:35:16,898][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000070[0m
[36m[2023-07-16 21:35:29,018][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-16 21:35:29,019][257371] FPS: 327929.01[0m
[36m[2023-07-16 21:35:33,284][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:35:33,285][257371] Reward + Measures: [[9.62821222 0.32575932 0.33791199 0.10593034 0.27451032 6.7968111 ]][0m
[37m[1m[2023-07-16 21:35:33,285][257371] Max Reward on eval: 9.628212221804386[0m
[37m[1m[2023-07-16 21:35:33,285][257371] Min Reward on eval: 9.628212221804386[0m
[37m[1m[2023-07-16 21:35:33,286][257371] Mean Reward across all agents: 9.628212221804386[0m
[37m[1m[2023-07-16 21:35:33,286][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:35:38,408][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:35:38,408][257371] Reward + Measures: [[151.74306455   0.57309997   0.54949999   0.2323       0.39250001
    6.95935965]
 [107.06274227   0.16919999   0.16760001   0.1104       0.1279
    6.13766861]
 [ 58.00195093   0.23840001   0.2378       0.094        0.2062
    6.56114435]
 ...
 [207.52347206   0.4382       0.44260001   0.11540001   0.40970001
    7.24007893]
 [-16.89151647   0.0917       0.1023       0.1054       0.1038
    6.14949274]
 [  8.15788601   0.11390001   0.10089999   0.0794       0.0924
    6.84878635]][0m
[37m[1m[2023-07-16 21:35:38,409][257371] Max Reward on eval: 284.3839225169271[0m
[37m[1m[2023-07-16 21:35:38,409][257371] Min Reward on eval: -186.6073047760874[0m
[37m[1m[2023-07-16 21:35:38,409][257371] Mean Reward across all agents: 76.67412036713496[0m
[37m[1m[2023-07-16 21:35:38,409][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:35:38,412][257371] mean_value=-168.38968563734628, max_value=718.708648238182[0m
[37m[1m[2023-07-16 21:35:38,414][257371] New mean coefficients: [[-0.82180893  3.7835855  -1.6086745  -0.02904566 -3.8114524   0.24952817]][0m
[37m[1m[2023-07-16 21:35:38,415][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:35:47,307][257371] train() took 8.89 seconds to complete[0m
[36m[2023-07-16 21:35:47,307][257371] FPS: 431928.10[0m
[36m[2023-07-16 21:35:47,310][257371] itr=91, itrs=2000, Progress: 4.55%[0m
[36m[2023-07-16 21:35:59,270][257371] train() took 11.93 seconds to complete[0m
[36m[2023-07-16 21:35:59,270][257371] FPS: 321748.84[0m
[36m[2023-07-16 21:36:03,557][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:36:03,557][257371] Reward + Measures: [[34.74532922  0.39558634  0.41142097  0.09417034  0.34362033  6.96570683]][0m
[37m[1m[2023-07-16 21:36:03,557][257371] Max Reward on eval: 34.74532921574777[0m
[37m[1m[2023-07-16 21:36:03,557][257371] Min Reward on eval: 34.74532921574777[0m
[37m[1m[2023-07-16 21:36:03,558][257371] Mean Reward across all agents: 34.74532921574777[0m
[37m[1m[2023-07-16 21:36:03,558][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:36:08,559][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:36:08,559][257371] Reward + Measures: [[212.32836757   0.42470002   0.45530006   0.1569       0.3477
    7.23421955]
 [ 33.13256376   0.1294       0.16720001   0.1163       0.1336
    7.0258441 ]
 [ 42.31329556   0.20939998   0.1611       0.21930002   0.17660001
    6.89774704]
 ...
 [ -0.89331867   0.10820001   0.10610002   0.0872       0.0981
    6.54285431]
 [ 33.51306906   0.19410001   0.189        0.2145       0.1973
    7.00693846]
 [  0.59643046   0.1517       0.1416       0.13590001   0.13170001
    6.70020819]][0m
[37m[1m[2023-07-16 21:36:08,559][257371] Max Reward on eval: 359.995819228515[0m
[37m[1m[2023-07-16 21:36:08,560][257371] Min Reward on eval: -190.353606470814[0m
[37m[1m[2023-07-16 21:36:08,560][257371] Mean Reward across all agents: 61.40783454715313[0m
[37m[1m[2023-07-16 21:36:08,560][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:36:08,564][257371] mean_value=-154.6344239828345, max_value=399.8780102602443[0m
[37m[1m[2023-07-16 21:36:08,567][257371] New mean coefficients: [[-1.6843004   4.2037287  -0.32603192  0.23024027 -1.4464982  -0.10935131]][0m
[37m[1m[2023-07-16 21:36:08,567][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:36:17,637][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 21:36:17,638][257371] FPS: 423450.63[0m
[36m[2023-07-16 21:36:17,640][257371] itr=92, itrs=2000, Progress: 4.60%[0m
[36m[2023-07-16 21:36:29,508][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-16 21:36:29,508][257371] FPS: 324375.22[0m
[36m[2023-07-16 21:36:33,803][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:36:33,803][257371] Reward + Measures: [[17.44510474  0.42581967  0.43787599  0.09343734  0.37386337  7.0170722 ]][0m
[37m[1m[2023-07-16 21:36:33,803][257371] Max Reward on eval: 17.445104741365686[0m
[37m[1m[2023-07-16 21:36:33,804][257371] Min Reward on eval: 17.445104741365686[0m
[37m[1m[2023-07-16 21:36:33,804][257371] Mean Reward across all agents: 17.445104741365686[0m
[37m[1m[2023-07-16 21:36:33,804][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:36:38,853][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:36:38,853][257371] Reward + Measures: [[  9.87350348   0.21500002   0.1391       0.2282       0.1525
    6.55132437]
 [210.99974822   0.39550003   0.37560001   0.1485       0.39790002
    6.76646042]
 [146.32951687   0.3809       0.35570002   0.25479999   0.18679999
    7.18974781]
 ...
 [121.88400296   0.20469999   0.1982       0.21340001   0.1214
    7.00543356]
 [425.51852489   0.70310003   0.0242       0.67460006   0.66170001
    7.19642496]
 [201.27634854   0.42300001   0.31470001   0.28060004   0.2811
    7.28214121]][0m
[37m[1m[2023-07-16 21:36:38,854][257371] Max Reward on eval: 509.5772733395919[0m
[37m[1m[2023-07-16 21:36:38,854][257371] Min Reward on eval: -239.11964107658713[0m
[37m[1m[2023-07-16 21:36:38,854][257371] Mean Reward across all agents: 109.83273343743133[0m
[37m[1m[2023-07-16 21:36:38,854][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:36:38,864][257371] mean_value=3.2594742716217904, max_value=978.2380038172007[0m
[37m[1m[2023-07-16 21:36:38,867][257371] New mean coefficients: [[-1.2368921  5.8442974  0.6561371  1.4628875 -2.4906926 -0.1317243]][0m
[37m[1m[2023-07-16 21:36:38,868][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:36:47,950][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 21:36:47,950][257371] FPS: 422893.00[0m
[36m[2023-07-16 21:36:47,952][257371] itr=93, itrs=2000, Progress: 4.65%[0m
[36m[2023-07-16 21:36:59,704][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-16 21:36:59,704][257371] FPS: 327524.68[0m
[36m[2023-07-16 21:37:03,970][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:37:03,971][257371] Reward + Measures: [[23.4456682   0.42972997  0.44021198  0.09311434  0.37737963  7.01949024]][0m
[37m[1m[2023-07-16 21:37:03,971][257371] Max Reward on eval: 23.44566819932566[0m
[37m[1m[2023-07-16 21:37:03,971][257371] Min Reward on eval: 23.44566819932566[0m
[37m[1m[2023-07-16 21:37:03,972][257371] Mean Reward across all agents: 23.44566819932566[0m
[37m[1m[2023-07-16 21:37:03,972][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:37:08,937][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:37:08,937][257371] Reward + Measures: [[-19.87703319   0.2244       0.30010003   0.13810001   0.27150002
    6.53484583]
 [ 78.96735874   0.1418       0.11900001   0.16540001   0.1099
    6.54041815]
 [ 91.86809805   0.15899999   0.16500001   0.18719999   0.14070001
    6.19945097]
 ...
 [273.32951214   0.62269998   0.49429998   0.5007       0.29660001
    7.58552122]
 [ 80.81989202   0.12910001   0.1384       0.14500001   0.11420001
    6.29834604]
 [ 26.50108713   0.15900001   0.18439999   0.15260001   0.15679999
    6.38907766]][0m
[37m[1m[2023-07-16 21:37:08,938][257371] Max Reward on eval: 505.14824134465306[0m
[37m[1m[2023-07-16 21:37:08,938][257371] Min Reward on eval: -94.22710805740208[0m
[37m[1m[2023-07-16 21:37:08,938][257371] Mean Reward across all agents: 75.81834845692399[0m
[37m[1m[2023-07-16 21:37:08,938][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:37:08,942][257371] mean_value=-147.36472007920713, max_value=1005.1482413446531[0m
[37m[1m[2023-07-16 21:37:08,945][257371] New mean coefficients: [[-0.05941594  5.6703243   1.8233329   0.6001584  -3.4890895  -0.28768933]][0m
[37m[1m[2023-07-16 21:37:08,945][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:37:17,993][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 21:37:17,994][257371] FPS: 424472.01[0m
[36m[2023-07-16 21:37:17,996][257371] itr=94, itrs=2000, Progress: 4.70%[0m
[36m[2023-07-16 21:37:29,595][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-16 21:37:29,596][257371] FPS: 331840.03[0m
[36m[2023-07-16 21:37:33,820][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:37:33,820][257371] Reward + Measures: [[-7.23622785  0.29523998  0.29773164  0.11981499  0.25428665  6.82952738]][0m
[37m[1m[2023-07-16 21:37:33,820][257371] Max Reward on eval: -7.236227848500101[0m
[37m[1m[2023-07-16 21:37:33,820][257371] Min Reward on eval: -7.236227848500101[0m
[37m[1m[2023-07-16 21:37:33,821][257371] Mean Reward across all agents: -7.236227848500101[0m
[37m[1m[2023-07-16 21:37:33,821][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:37:38,817][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:37:38,818][257371] Reward + Measures: [[274.796141     0.23330002   0.22090001   0.25030002   0.21500002
    6.11091661]
 [152.36486408   0.1393       0.1468       0.18310001   0.1293
    6.38127756]
 [171.0446105    0.2103       0.23740001   0.1691       0.2069
    7.00477314]
 ...
 [129.33815477   0.1242       0.13770001   0.1652       0.14209999
    6.53291845]
 [ 54.57243937   0.11130001   0.1213       0.1258       0.1129
    6.65911579]
 [ 11.7619631    0.0922       0.1139       0.1024       0.1111
    7.22840595]][0m
[37m[1m[2023-07-16 21:37:38,818][257371] Max Reward on eval: 274.79614099822936[0m
[37m[1m[2023-07-16 21:37:38,818][257371] Min Reward on eval: -61.76215164109599[0m
[37m[1m[2023-07-16 21:37:38,819][257371] Mean Reward across all agents: 74.44060805473393[0m
[37m[1m[2023-07-16 21:37:38,819][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:37:38,820][257371] mean_value=-182.11629368160968, max_value=-1.3735361435151958[0m
[36m[2023-07-16 21:37:38,822][257371] XNES is restarting with a new solution whose measures are [0.62320006 0.71060002 0.1035     0.57530004 7.36400604] and objective is -62.0569658623077[0m
[36m[2023-07-16 21:37:38,823][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-16 21:37:38,825][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-16 21:37:38,826][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:37:47,806][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-16 21:37:47,806][257371] FPS: 427703.99[0m
[36m[2023-07-16 21:37:47,809][257371] itr=95, itrs=2000, Progress: 4.75%[0m
[36m[2023-07-16 21:37:59,430][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-16 21:37:59,430][257371] FPS: 331259.24[0m
[36m[2023-07-16 21:38:03,754][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:38:03,754][257371] Reward + Measures: [[-16.78123514   0.62157732   0.57002366   0.17676233   0.578385
    7.46700191]][0m
[37m[1m[2023-07-16 21:38:03,754][257371] Max Reward on eval: -16.781235136682984[0m
[37m[1m[2023-07-16 21:38:03,755][257371] Min Reward on eval: -16.781235136682984[0m
[37m[1m[2023-07-16 21:38:03,755][257371] Mean Reward across all agents: -16.781235136682984[0m
[37m[1m[2023-07-16 21:38:03,755][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:38:08,904][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:38:08,905][257371] Reward + Measures: [[-81.39002113   0.7202       0.50169998   0.30250001   0.70650005
    7.63187027]
 [ 88.22307321   0.1587       0.21170001   0.0919       0.1767
    6.73701334]
 [ 29.5644163    0.2647       0.26700002   0.24680002   0.233
    6.62524748]
 ...
 [ 21.68600454   0.22839999   0.24600001   0.1134       0.2559
    6.80164671]
 [ 18.33156883   0.26339999   0.24569999   0.20609999   0.3021
    6.84078169]
 [ 21.12518881   0.1134       0.1225       0.1019       0.1103
    6.50747061]][0m
[37m[1m[2023-07-16 21:38:08,905][257371] Max Reward on eval: 308.04384600836784[0m
[37m[1m[2023-07-16 21:38:08,906][257371] Min Reward on eval: -624.7842979397625[0m
[37m[1m[2023-07-16 21:38:08,906][257371] Mean Reward across all agents: 20.87823416947908[0m
[37m[1m[2023-07-16 21:38:08,906][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:38:08,912][257371] mean_value=-101.3231661867666, max_value=808.0438460083678[0m
[37m[1m[2023-07-16 21:38:08,915][257371] New mean coefficients: [[-1.775831    0.73954725 -1.1479561  -1.5247626  -2.5210917  -0.67459166]][0m
[37m[1m[2023-07-16 21:38:08,916][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:38:17,951][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 21:38:17,952][257371] FPS: 425066.71[0m
[36m[2023-07-16 21:38:17,954][257371] itr=96, itrs=2000, Progress: 4.80%[0m
[36m[2023-07-16 21:38:29,869][257371] train() took 11.89 seconds to complete[0m
[36m[2023-07-16 21:38:29,870][257371] FPS: 323072.92[0m
[36m[2023-07-16 21:38:34,212][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:38:34,212][257371] Reward + Measures: [[21.54308029  0.66369259  0.50068766  0.23802333  0.6429413   7.47134113]][0m
[37m[1m[2023-07-16 21:38:34,213][257371] Max Reward on eval: 21.54308028670837[0m
[37m[1m[2023-07-16 21:38:34,213][257371] Min Reward on eval: 21.54308028670837[0m
[37m[1m[2023-07-16 21:38:34,213][257371] Mean Reward across all agents: 21.54308028670837[0m
[37m[1m[2023-07-16 21:38:34,213][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:38:39,249][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:38:39,250][257371] Reward + Measures: [[   4.84130972    0.29699999    0.19940001    0.27330002    0.28909999
     7.29421997]
 [  33.47225344    0.5485        0.47750002    0.1237        0.5291
     7.37555695]
 [   6.59510949    0.74050003    0.60780001    0.25139999    0.76459998
     7.64954615]
 ...
 [ 145.92445174    0.3845        0.42289996    0.148         0.4567
     7.17221403]
 [  24.04613608    0.20739999    0.16470002    0.20439999    0.19749999
     7.08969593]
 [-474.93119048    0.77200001    0.0949        0.71060002    0.77089995
     7.407197  ]][0m
[37m[1m[2023-07-16 21:38:39,250][257371] Max Reward on eval: 357.6899327861145[0m
[37m[1m[2023-07-16 21:38:39,250][257371] Min Reward on eval: -596.5408745226217[0m
[37m[1m[2023-07-16 21:38:39,251][257371] Mean Reward across all agents: 29.91108236360138[0m
[37m[1m[2023-07-16 21:38:39,251][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:38:39,257][257371] mean_value=-122.30231464227434, max_value=782.4877150925807[0m
[37m[1m[2023-07-16 21:38:39,260][257371] New mean coefficients: [[-1.7878743 -0.6126783 -1.7269695 -2.6010678 -1.5250982 -1.4203101]][0m
[37m[1m[2023-07-16 21:38:39,261][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:38:48,419][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-16 21:38:48,419][257371] FPS: 419409.76[0m
[36m[2023-07-16 21:38:48,421][257371] itr=97, itrs=2000, Progress: 4.85%[0m
[36m[2023-07-16 21:39:00,156][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-16 21:39:00,157][257371] FPS: 328011.48[0m
[36m[2023-07-16 21:39:04,498][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:39:04,499][257371] Reward + Measures: [[-14.64532657   0.7388773    0.34040734   0.48881468   0.71265835
    7.67996979]][0m
[37m[1m[2023-07-16 21:39:04,499][257371] Max Reward on eval: -14.645326569464348[0m
[37m[1m[2023-07-16 21:39:04,499][257371] Min Reward on eval: -14.645326569464348[0m
[37m[1m[2023-07-16 21:39:04,499][257371] Mean Reward across all agents: -14.645326569464348[0m
[37m[1m[2023-07-16 21:39:04,500][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:39:09,552][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:39:09,552][257371] Reward + Measures: [[ -23.62706803    0.12180001    0.12470001    0.0975        0.1071
     6.55931234]
 [ -48.7569966     0.0873        0.1258        0.0935        0.11610001
     6.54496002]
 [-228.05336236    0.53670001    0.56309998    0.1743        0.57730001
     7.27779007]
 ...
 [ -17.70089505    0.23560002    0.2254        0.1001        0.1595
     6.89483023]
 [ -21.47356844    0.94239998    0.2385        0.8530001     0.73820001
     7.92435312]
 [  27.32986698    0.1141        0.10880001    0.10399999    0.0986
     6.60692358]][0m
[37m[1m[2023-07-16 21:39:09,552][257371] Max Reward on eval: 438.04802633188666[0m
[37m[1m[2023-07-16 21:39:09,553][257371] Min Reward on eval: -638.0449047110975[0m
[37m[1m[2023-07-16 21:39:09,553][257371] Mean Reward across all agents: 9.857241625995066[0m
[37m[1m[2023-07-16 21:39:09,553][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:39:09,560][257371] mean_value=-135.57294562170097, max_value=938.0480263318866[0m
[37m[1m[2023-07-16 21:39:09,563][257371] New mean coefficients: [[-0.77827835 -0.35399076 -2.060297   -2.4404795  -0.01014435 -1.6302528 ]][0m
[37m[1m[2023-07-16 21:39:09,564][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:39:18,723][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-16 21:39:18,723][257371] FPS: 419311.80[0m
[36m[2023-07-16 21:39:18,726][257371] itr=98, itrs=2000, Progress: 4.90%[0m
[36m[2023-07-16 21:39:30,382][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-16 21:39:30,382][257371] FPS: 330290.07[0m
[36m[2023-07-16 21:39:34,664][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:39:34,664][257371] Reward + Measures: [[34.83725937  0.61511433  0.41737634  0.28041899  0.57525665  7.4951458 ]][0m
[37m[1m[2023-07-16 21:39:34,664][257371] Max Reward on eval: 34.83725937458162[0m
[37m[1m[2023-07-16 21:39:34,665][257371] Min Reward on eval: 34.83725937458162[0m
[37m[1m[2023-07-16 21:39:34,665][257371] Mean Reward across all agents: 34.83725937458162[0m
[37m[1m[2023-07-16 21:39:34,665][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:39:39,683][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:39:39,683][257371] Reward + Measures: [[30.05914961  0.34900001  0.30719998  0.16680001  0.1912      6.8700099 ]
 [76.21958373  0.42810002  0.47389999  0.07120001  0.44919997  7.2261157 ]
 [33.85490722  0.13920002  0.2181      0.1115      0.22860001  7.20370817]
 ...
 [39.05232401  0.06680001  0.08270001  0.06110001  0.0708      6.85484457]
 [53.80391933  0.16960001  0.17460001  0.0947      0.14000002  6.69390106]
 [-2.57361262  0.15020001  0.1743      0.0631      0.15249999  7.06201506]][0m
[37m[1m[2023-07-16 21:39:39,683][257371] Max Reward on eval: 351.5473194762599[0m
[37m[1m[2023-07-16 21:39:39,684][257371] Min Reward on eval: -554.240247738082[0m
[37m[1m[2023-07-16 21:39:39,684][257371] Mean Reward across all agents: 52.28312776219838[0m
[37m[1m[2023-07-16 21:39:39,684][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:39:39,689][257371] mean_value=-146.46074386000555, max_value=645.4320558651393[0m
[37m[1m[2023-07-16 21:39:39,692][257371] New mean coefficients: [[-0.02890122 -1.8535895  -1.0499594  -2.5372112  -1.29649    -1.4598489 ]][0m
[37m[1m[2023-07-16 21:39:39,693][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:39:48,704][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 21:39:48,704][257371] FPS: 426210.97[0m
[36m[2023-07-16 21:39:48,706][257371] itr=99, itrs=2000, Progress: 4.95%[0m
[36m[2023-07-16 21:40:00,349][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-16 21:40:00,349][257371] FPS: 330586.39[0m
[36m[2023-07-16 21:40:04,639][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:40:04,640][257371] Reward + Measures: [[34.93046405  0.61624467  0.46109065  0.232518    0.57776767  7.53450108]][0m
[37m[1m[2023-07-16 21:40:04,640][257371] Max Reward on eval: 34.93046404767539[0m
[37m[1m[2023-07-16 21:40:04,640][257371] Min Reward on eval: 34.93046404767539[0m
[37m[1m[2023-07-16 21:40:04,640][257371] Mean Reward across all agents: 34.93046404767539[0m
[37m[1m[2023-07-16 21:40:04,640][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:40:09,848][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:40:09,849][257371] Reward + Measures: [[  41.43368061    0.42800003    0.42950001    0.0836        0.44159999
     7.26133823]
 [ 126.04935808    0.4779        0.46129999    0.4515        0.47680002
     7.25462723]
 [ 110.56252824    0.1331        0.1344        0.0823        0.09780001
     6.71322346]
 ...
 [  21.26527711    0.50570005    0.4894        0.1708        0.54069996
     7.38474607]
 [ 132.22315763    0.54170007    0.54890001    0.06820001    0.52670002
     7.43491745]
 [-137.24631403    0.68959999    0.52510005    0.25190002    0.68400002
     7.56391287]][0m
[37m[1m[2023-07-16 21:40:09,849][257371] Max Reward on eval: 277.19700377993286[0m
[37m[1m[2023-07-16 21:40:09,849][257371] Min Reward on eval: -387.8716895602644[0m
[37m[1m[2023-07-16 21:40:09,850][257371] Mean Reward across all agents: 40.54976053758812[0m
[37m[1m[2023-07-16 21:40:09,850][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:40:09,857][257371] mean_value=-99.11777269160382, max_value=592.7195983244571[0m
[37m[1m[2023-07-16 21:40:09,859][257371] New mean coefficients: [[-0.11260024 -1.7412664  -0.24266261 -2.8827722  -1.6377172  -1.5820328 ]][0m
[37m[1m[2023-07-16 21:40:09,860][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:40:19,078][257371] train() took 9.22 seconds to complete[0m
[36m[2023-07-16 21:40:19,079][257371] FPS: 416656.66[0m
[36m[2023-07-16 21:40:19,081][257371] itr=100, itrs=2000, Progress: 5.00%[0m
[37m[1m[2023-07-16 21:42:14,810][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000080[0m
[36m[2023-07-16 21:42:27,125][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-16 21:42:27,126][257371] FPS: 323529.60[0m
[36m[2023-07-16 21:42:31,400][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:42:31,400][257371] Reward + Measures: [[37.79293676  0.62573862  0.497475    0.19969633  0.58873093  7.50378513]][0m
[37m[1m[2023-07-16 21:42:31,400][257371] Max Reward on eval: 37.79293676421613[0m
[37m[1m[2023-07-16 21:42:31,401][257371] Min Reward on eval: 37.79293676421613[0m
[37m[1m[2023-07-16 21:42:31,401][257371] Mean Reward across all agents: 37.79293676421613[0m
[37m[1m[2023-07-16 21:42:31,401][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:42:36,414][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:42:36,415][257371] Reward + Measures: [[ 27.68682262   0.51749998   0.24349999   0.40430003   0.4585
    7.42920256]
 [113.41048303   0.42420003   0.16630001   0.36859998   0.43250003
    7.29225159]
 [ 19.99339912   0.0914       0.0852       0.0737       0.0896
    6.93638706]
 ...
 [-13.29449929   0.0987       0.12539999   0.0926       0.12020002
    7.0312171 ]
 [116.67906738   0.20420001   0.21710001   0.1013       0.18270001
    7.03987598]
 [ 49.26487068   0.11240001   0.1168       0.109        0.08880001
    7.03307962]][0m
[37m[1m[2023-07-16 21:42:36,415][257371] Max Reward on eval: 623.7761859696359[0m
[37m[1m[2023-07-16 21:42:36,415][257371] Min Reward on eval: -141.82110501052813[0m
[37m[1m[2023-07-16 21:42:36,415][257371] Mean Reward across all agents: 58.034093932619726[0m
[37m[1m[2023-07-16 21:42:36,416][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:42:36,421][257371] mean_value=-122.0797344442438, max_value=610.4642519229842[0m
[37m[1m[2023-07-16 21:42:36,424][257371] New mean coefficients: [[-1.1603721  -3.1755352  -0.92320347 -1.6296157  -1.576273   -1.0960093 ]][0m
[37m[1m[2023-07-16 21:42:36,425][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:42:45,454][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 21:42:45,455][257371] FPS: 425337.52[0m
[36m[2023-07-16 21:42:45,457][257371] itr=101, itrs=2000, Progress: 5.05%[0m
[36m[2023-07-16 21:42:57,137][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-16 21:42:57,137][257371] FPS: 329524.07[0m
[36m[2023-07-16 21:43:01,433][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:43:01,433][257371] Reward + Measures: [[29.93203819  0.57449996  0.41600698  0.24683201  0.52715838  7.45212364]][0m
[37m[1m[2023-07-16 21:43:01,433][257371] Max Reward on eval: 29.932038192905914[0m
[37m[1m[2023-07-16 21:43:01,434][257371] Min Reward on eval: 29.932038192905914[0m
[37m[1m[2023-07-16 21:43:01,434][257371] Mean Reward across all agents: 29.932038192905914[0m
[37m[1m[2023-07-16 21:43:01,434][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:43:06,441][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:43:06,442][257371] Reward + Measures: [[91.12170841  0.2102      0.24440002  0.13380001  0.1991      6.80815744]
 [32.25810632  0.223       0.20009999  0.20630001  0.1222      6.5167613 ]
 [23.29198044  0.31140003  0.52340001  0.10389999  0.32179999  7.02991486]
 ...
 [87.69304945  0.29350004  0.0922      0.36049998  0.19660001  7.35032225]
 [27.77793557  0.1823      0.2014      0.15409999  0.1718      6.86138487]
 [63.5385342   0.22579999  0.2392      0.1626      0.14820002  6.70125532]][0m
[37m[1m[2023-07-16 21:43:06,442][257371] Max Reward on eval: 362.3932247204008[0m
[37m[1m[2023-07-16 21:43:06,442][257371] Min Reward on eval: -426.9522094130516[0m
[37m[1m[2023-07-16 21:43:06,442][257371] Mean Reward across all agents: 55.98719076112929[0m
[37m[1m[2023-07-16 21:43:06,443][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:43:06,446][257371] mean_value=-163.86189850257594, max_value=555.0735389674549[0m
[37m[1m[2023-07-16 21:43:06,449][257371] New mean coefficients: [[-1.9010348  -1.4238871  -0.23050112 -1.6705916  -0.58916974 -1.1687295 ]][0m
[37m[1m[2023-07-16 21:43:06,450][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:43:15,479][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 21:43:15,479][257371] FPS: 425362.10[0m
[36m[2023-07-16 21:43:15,481][257371] itr=102, itrs=2000, Progress: 5.10%[0m
[36m[2023-07-16 21:43:27,209][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-16 21:43:27,209][257371] FPS: 328196.69[0m
[36m[2023-07-16 21:43:31,603][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:43:31,604][257371] Reward + Measures: [[14.59806597  0.50114799  0.40741569  0.19418034  0.46398568  7.31336975]][0m
[37m[1m[2023-07-16 21:43:31,604][257371] Max Reward on eval: 14.59806597356826[0m
[37m[1m[2023-07-16 21:43:31,604][257371] Min Reward on eval: 14.59806597356826[0m
[37m[1m[2023-07-16 21:43:31,605][257371] Mean Reward across all agents: 14.59806597356826[0m
[37m[1m[2023-07-16 21:43:31,605][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:43:36,765][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:43:36,766][257371] Reward + Measures: [[ 57.6668146    0.29200003   0.2843       0.33849999   0.30410001
    6.87579727]
 [-61.36747775   0.53500003   0.51170003   0.13239999   0.50389999
    7.46641159]
 [-29.47870035   0.2617       0.3186       0.16630001   0.29960001
    6.8290801 ]
 ...
 [ 10.51903216   0.52000004   0.042        0.53719997   0.38970003
    7.36550999]
 [  3.20625512   0.34150001   0.33760002   0.11979999   0.32380003
    7.03636932]
 [ 33.86031057   0.14190002   0.1194       0.12640001   0.1231
    6.88145924]][0m
[37m[1m[2023-07-16 21:43:36,766][257371] Max Reward on eval: 287.2235556172207[0m
[37m[1m[2023-07-16 21:43:36,766][257371] Min Reward on eval: -153.467657941347[0m
[37m[1m[2023-07-16 21:43:36,766][257371] Mean Reward across all agents: 68.01166477269835[0m
[37m[1m[2023-07-16 21:43:36,767][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:43:36,769][257371] mean_value=-156.42157126921433, max_value=390.5948122475239[0m
[37m[1m[2023-07-16 21:43:36,772][257371] New mean coefficients: [[-2.9190576  -1.0279174   0.30287957 -2.0562584  -1.6783137  -1.2071791 ]][0m
[37m[1m[2023-07-16 21:43:36,773][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:43:45,885][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-16 21:43:45,886][257371] FPS: 421463.06[0m
[36m[2023-07-16 21:43:45,888][257371] itr=103, itrs=2000, Progress: 5.15%[0m
[36m[2023-07-16 21:43:57,678][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-16 21:43:57,679][257371] FPS: 326447.64[0m
[36m[2023-07-16 21:44:01,961][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:44:01,962][257371] Reward + Measures: [[9.16801742 0.40725064 0.28113303 0.22833733 0.35982332 7.12045813]][0m
[37m[1m[2023-07-16 21:44:01,962][257371] Max Reward on eval: 9.168017416850232[0m
[37m[1m[2023-07-16 21:44:01,962][257371] Min Reward on eval: 9.168017416850232[0m
[37m[1m[2023-07-16 21:44:01,962][257371] Mean Reward across all agents: 9.168017416850232[0m
[37m[1m[2023-07-16 21:44:01,962][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:44:06,973][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:44:06,974][257371] Reward + Measures: [[24.31080916  0.16190001  0.19530001  0.13869999  0.18260001  6.42392731]
 [43.03940961  0.18629999  0.17760001  0.1142      0.13540001  6.68570089]
 [52.58055996  0.28459999  0.29510003  0.15220001  0.23469999  6.61927032]
 ...
 [94.85096724  0.20289998  0.1997      0.23360001  0.2586      7.0362854 ]
 [12.64676791  0.18640001  0.2103      0.1058      0.1591      6.80404758]
 [83.32789062  0.23480001  0.24850002  0.1041      0.211       7.01018858]][0m
[37m[1m[2023-07-16 21:44:06,974][257371] Max Reward on eval: 336.70305347763934[0m
[37m[1m[2023-07-16 21:44:06,975][257371] Min Reward on eval: -156.11871555704275[0m
[37m[1m[2023-07-16 21:44:06,975][257371] Mean Reward across all agents: 40.44494486051929[0m
[37m[1m[2023-07-16 21:44:06,975][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:44:06,978][257371] mean_value=-187.24680048750923, max_value=436.52477912428486[0m
[37m[1m[2023-07-16 21:44:06,980][257371] New mean coefficients: [[-1.7500293  -2.2061481   1.4438014  -1.1680342  -0.69038755 -1.5397502 ]][0m
[37m[1m[2023-07-16 21:44:06,981][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:44:16,016][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 21:44:16,016][257371] FPS: 425113.49[0m
[36m[2023-07-16 21:44:16,018][257371] itr=104, itrs=2000, Progress: 5.20%[0m
[36m[2023-07-16 21:44:27,591][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-16 21:44:27,592][257371] FPS: 332589.16[0m
[36m[2023-07-16 21:44:31,859][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:44:31,859][257371] Reward + Measures: [[-19.39030535   0.39280835   0.27107567   0.22554      0.35499263
    7.10737276]][0m
[37m[1m[2023-07-16 21:44:31,860][257371] Max Reward on eval: -19.390305353513988[0m
[37m[1m[2023-07-16 21:44:31,860][257371] Min Reward on eval: -19.390305353513988[0m
[37m[1m[2023-07-16 21:44:31,860][257371] Mean Reward across all agents: -19.390305353513988[0m
[37m[1m[2023-07-16 21:44:31,860][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:44:36,930][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:44:36,931][257371] Reward + Measures: [[ 26.06287819   0.21500002   0.18609999   0.1639       0.20310001
    7.03012228]
 [ 46.32565925   0.43689999   0.40599999   0.17380001   0.4269
    7.22845173]
 [ 71.56963135   0.1093       0.1285       0.0939       0.1058
    6.78534317]
 ...
 [ 17.32354772   0.1626       0.13429999   0.13280001   0.122
    6.55051136]
 [261.78059168   0.94700003   0.0174       0.92010003   0.92989999
    7.80951262]
 [105.9414621    0.377        0.1587       0.3213       0.34419999
    7.23884583]][0m
[37m[1m[2023-07-16 21:44:36,931][257371] Max Reward on eval: 261.780591679737[0m
[37m[1m[2023-07-16 21:44:36,931][257371] Min Reward on eval: -138.38876965641975[0m
[37m[1m[2023-07-16 21:44:36,931][257371] Mean Reward across all agents: 34.215284746634595[0m
[37m[1m[2023-07-16 21:44:36,931][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:44:36,933][257371] mean_value=-174.07963924896674, max_value=555.2206627421547[0m
[37m[1m[2023-07-16 21:44:36,935][257371] New mean coefficients: [[-3.432076    0.28218818  1.3700898  -2.6394982  -0.7336912  -1.1242099 ]][0m
[37m[1m[2023-07-16 21:44:36,936][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:44:45,952][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 21:44:45,952][257371] FPS: 425984.29[0m
[36m[2023-07-16 21:44:45,954][257371] itr=105, itrs=2000, Progress: 5.25%[0m
[36m[2023-07-16 21:44:57,604][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-16 21:44:57,605][257371] FPS: 330442.16[0m
[36m[2023-07-16 21:45:01,858][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:45:01,859][257371] Reward + Measures: [[-0.54347729  0.48412564  0.25010997  0.35721034  0.43232068  7.27627182]][0m
[37m[1m[2023-07-16 21:45:01,859][257371] Max Reward on eval: -0.5434772944886038[0m
[37m[1m[2023-07-16 21:45:01,859][257371] Min Reward on eval: -0.5434772944886038[0m
[37m[1m[2023-07-16 21:45:01,859][257371] Mean Reward across all agents: -0.5434772944886038[0m
[37m[1m[2023-07-16 21:45:01,860][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:45:06,875][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:45:06,876][257371] Reward + Measures: [[ 48.54524337   0.33270001   0.09190001   0.34489998   0.34370002
    7.35475922]
 [167.62960079   0.2624       0.2053       0.17019999   0.19420001
    6.84889555]
 [ 36.45655234   0.27209997   0.28930002   0.1445       0.2811
    7.01024866]
 ...
 [ 14.49690335   0.25280002   0.13970001   0.27169999   0.25079998
    7.37691975]
 [ 54.02492156   0.2414       0.22319999   0.1596       0.22860001
    7.17606068]
 [ 68.5251412    0.331        0.35740003   0.10650001   0.33899999
    6.96773529]][0m
[37m[1m[2023-07-16 21:45:06,876][257371] Max Reward on eval: 282.1136243507266[0m
[37m[1m[2023-07-16 21:45:06,876][257371] Min Reward on eval: -424.4197218545014[0m
[37m[1m[2023-07-16 21:45:06,876][257371] Mean Reward across all agents: 44.05903985995331[0m
[37m[1m[2023-07-16 21:45:06,877][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:45:06,880][257371] mean_value=-163.6037665933264, max_value=510.6225229274901[0m
[37m[1m[2023-07-16 21:45:06,883][257371] New mean coefficients: [[-2.5981934  -1.6021355   0.6663616  -3.4950569  -0.79144746 -1.1177524 ]][0m
[37m[1m[2023-07-16 21:45:06,884][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:45:15,932][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 21:45:15,932][257371] FPS: 424473.54[0m
[36m[2023-07-16 21:45:15,934][257371] itr=106, itrs=2000, Progress: 5.30%[0m
[36m[2023-07-16 21:45:27,833][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-16 21:45:27,834][257371] FPS: 323462.26[0m
[36m[2023-07-16 21:45:32,132][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:45:32,133][257371] Reward + Measures: [[73.49645315  0.66500896  0.12031667  0.63501197  0.58416903  7.4525609 ]][0m
[37m[1m[2023-07-16 21:45:32,133][257371] Max Reward on eval: 73.49645314725781[0m
[37m[1m[2023-07-16 21:45:32,133][257371] Min Reward on eval: 73.49645314725781[0m
[37m[1m[2023-07-16 21:45:32,134][257371] Mean Reward across all agents: 73.49645314725781[0m
[37m[1m[2023-07-16 21:45:32,134][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:45:37,181][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:45:37,181][257371] Reward + Measures: [[  3.68203023   0.96620005   0.0047       0.98289996   0.96670002
    7.98242283]
 [121.60056017   0.3549       0.40060002   0.234        0.38890001
    7.19375086]
 [ 14.89169548   0.22579999   0.1742       0.2023       0.1311
    6.69391775]
 ...
 [ 92.08094015   0.19320001   0.17390001   0.1522       0.1622
    6.5193553 ]
 [ 69.21778611   0.75690001   0.18560001   0.68790001   0.7525
    7.75063944]
 [127.46961134   0.8445999    0.10320001   0.80320007   0.83659995
    7.91928959]][0m
[37m[1m[2023-07-16 21:45:37,182][257371] Max Reward on eval: 398.21501732179894[0m
[37m[1m[2023-07-16 21:45:37,182][257371] Min Reward on eval: -432.5489035129547[0m
[37m[1m[2023-07-16 21:45:37,182][257371] Mean Reward across all agents: 73.45088593889358[0m
[37m[1m[2023-07-16 21:45:37,182][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:45:37,188][257371] mean_value=-80.01178733377097, max_value=598.9156074205112[0m
[37m[1m[2023-07-16 21:45:37,191][257371] New mean coefficients: [[-3.6591501 -0.458822   1.2299278 -4.1141853 -1.4229398 -1.2802541]][0m
[37m[1m[2023-07-16 21:45:37,192][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:45:46,292][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-16 21:45:46,292][257371] FPS: 422040.45[0m
[36m[2023-07-16 21:45:46,295][257371] itr=107, itrs=2000, Progress: 5.35%[0m
[36m[2023-07-16 21:45:57,975][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-16 21:45:57,975][257371] FPS: 329628.92[0m
[36m[2023-07-16 21:46:02,239][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:46:02,240][257371] Reward + Measures: [[-12.77345548   0.45386067   0.19805367   0.33236766   0.39554098
    7.18033838]][0m
[37m[1m[2023-07-16 21:46:02,240][257371] Max Reward on eval: -12.7734554827537[0m
[37m[1m[2023-07-16 21:46:02,240][257371] Min Reward on eval: -12.7734554827537[0m
[37m[1m[2023-07-16 21:46:02,240][257371] Mean Reward across all agents: -12.7734554827537[0m
[37m[1m[2023-07-16 21:46:02,241][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:46:07,412][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:46:07,413][257371] Reward + Measures: [[ 41.01339774   0.108        0.12         0.1066       0.10750001
    6.51505756]
 [ 10.32233213   0.08630001   0.0871       0.07130001   0.0815
    6.48890829]
 [202.04620792   0.86879998   0.0407       0.84989995   0.84009999
    7.65151691]
 ...
 [193.36930675   0.4526       0.46440002   0.44499999   0.43689999
    7.01546097]
 [144.87356278   0.28910002   0.30420002   0.32620001   0.31650001
    7.00027084]
 [ 75.28477384   0.1005       0.1201       0.13170001   0.11970001
    6.80922318]][0m
[37m[1m[2023-07-16 21:46:07,413][257371] Max Reward on eval: 326.08296819143[0m
[37m[1m[2023-07-16 21:46:07,413][257371] Min Reward on eval: -119.84255147073418[0m
[37m[1m[2023-07-16 21:46:07,413][257371] Mean Reward across all agents: 80.62295584132085[0m
[37m[1m[2023-07-16 21:46:07,413][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:46:07,416][257371] mean_value=-142.82693350073393, max_value=602.2254238258516[0m
[37m[1m[2023-07-16 21:46:07,419][257371] New mean coefficients: [[-4.9502516   1.1162086   1.0624477  -3.469252   -0.53845227 -1.182084  ]][0m
[37m[1m[2023-07-16 21:46:07,420][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:46:16,421][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-16 21:46:16,421][257371] FPS: 426662.28[0m
[36m[2023-07-16 21:46:16,423][257371] itr=108, itrs=2000, Progress: 5.40%[0m
[36m[2023-07-16 21:46:28,248][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-16 21:46:28,248][257371] FPS: 325501.66[0m
[36m[2023-07-16 21:46:32,544][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:46:32,545][257371] Reward + Measures: [[5.21976142 0.29523501 0.20547765 0.17966166 0.26275098 6.90707731]][0m
[37m[1m[2023-07-16 21:46:32,545][257371] Max Reward on eval: 5.219761421929344[0m
[37m[1m[2023-07-16 21:46:32,545][257371] Min Reward on eval: 5.219761421929344[0m
[37m[1m[2023-07-16 21:46:32,545][257371] Mean Reward across all agents: 5.219761421929344[0m
[37m[1m[2023-07-16 21:46:32,546][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:46:37,577][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:46:37,582][257371] Reward + Measures: [[50.72950711  0.14119999  0.155       0.14830001  0.163       6.67822742]
 [44.53134876  0.14930001  0.10500001  0.14070001  0.1248      6.69914579]
 [23.51048621  0.37029999  0.2122      0.2271      0.36739999  7.29268217]
 ...
 [78.10926803  0.2036      0.20460001  0.198       0.26820001  6.93372202]
 [10.55690492  0.0895      0.0846      0.0822      0.08180001  6.3861661 ]
 [48.80888387  0.15360001  0.14070001  0.1508      0.20200001  6.86002064]][0m
[37m[1m[2023-07-16 21:46:37,583][257371] Max Reward on eval: 412.5499730416865[0m
[37m[1m[2023-07-16 21:46:37,583][257371] Min Reward on eval: -92.15662229463923[0m
[37m[1m[2023-07-16 21:46:37,583][257371] Mean Reward across all agents: 53.10790571670537[0m
[37m[1m[2023-07-16 21:46:37,583][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:46:37,586][257371] mean_value=-196.42348720312935, max_value=540.7781603827971[0m
[37m[1m[2023-07-16 21:46:37,588][257371] New mean coefficients: [[-4.1441293  1.9191344  2.537622  -3.3947933 -0.7438854 -1.2839279]][0m
[37m[1m[2023-07-16 21:46:37,589][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:46:46,584][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-16 21:46:46,584][257371] FPS: 427000.53[0m
[36m[2023-07-16 21:46:46,586][257371] itr=109, itrs=2000, Progress: 5.45%[0m
[36m[2023-07-16 21:46:58,349][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-16 21:46:58,349][257371] FPS: 327315.88[0m
[36m[2023-07-16 21:47:02,679][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:47:02,679][257371] Reward + Measures: [[3.04882008 0.33471999 0.320555   0.10917233 0.32321599 6.9871006 ]][0m
[37m[1m[2023-07-16 21:47:02,679][257371] Max Reward on eval: 3.0488200841480646[0m
[37m[1m[2023-07-16 21:47:02,679][257371] Min Reward on eval: 3.0488200841480646[0m
[37m[1m[2023-07-16 21:47:02,679][257371] Mean Reward across all agents: 3.0488200841480646[0m
[37m[1m[2023-07-16 21:47:02,680][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:47:07,718][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:47:07,718][257371] Reward + Measures: [[  72.49531385    0.40830001    0.54840004    0.1525        0.52740002
     7.41116333]
 [  51.85565476    0.23210001    0.3888        0.114         0.3396
     6.98249197]
 [  24.25107018    0.11610001    0.27340001    0.2014        0.24489999
     6.93677139]
 ...
 [-113.59727685    0.33500001    0.3962        0.0688        0.38070002
     7.09932661]
 [ -75.47471171    0.43060002    0.58000004    0.18440001    0.57620001
     7.41797638]
 [  35.82589329    0.4152        0.40050003    0.086         0.40109998
     7.09397507]][0m
[37m[1m[2023-07-16 21:47:07,719][257371] Max Reward on eval: 200.4199589197524[0m
[37m[1m[2023-07-16 21:47:07,719][257371] Min Reward on eval: -218.5001426958479[0m
[37m[1m[2023-07-16 21:47:07,719][257371] Mean Reward across all agents: 9.333130235298858[0m
[37m[1m[2023-07-16 21:47:07,719][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:47:07,723][257371] mean_value=-181.86767474216498, max_value=596.9783740123362[0m
[37m[1m[2023-07-16 21:47:07,726][257371] New mean coefficients: [[-4.097704    2.0456846   3.6857686  -3.3497725  -0.19232678 -1.3338984 ]][0m
[37m[1m[2023-07-16 21:47:07,727][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:47:16,799][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 21:47:16,799][257371] FPS: 423380.33[0m
[36m[2023-07-16 21:47:16,801][257371] itr=110, itrs=2000, Progress: 5.50%[0m
[37m[1m[2023-07-16 21:49:14,052][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000090[0m
[36m[2023-07-16 21:49:26,501][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-16 21:49:26,502][257371] FPS: 323450.56[0m
[36m[2023-07-16 21:49:30,786][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:49:30,787][257371] Reward + Measures: [[16.37119031  0.41697028  0.41239265  0.09802834  0.41234162  7.15362835]][0m
[37m[1m[2023-07-16 21:49:30,787][257371] Max Reward on eval: 16.371190309051574[0m
[37m[1m[2023-07-16 21:49:30,787][257371] Min Reward on eval: 16.371190309051574[0m
[37m[1m[2023-07-16 21:49:30,787][257371] Mean Reward across all agents: 16.371190309051574[0m
[37m[1m[2023-07-16 21:49:30,788][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:49:36,000][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:49:36,001][257371] Reward + Measures: [[   8.15828402    0.13990001    0.14560001    0.09420001    0.11270001
     6.70754385]
 [  26.25883936    0.6419        0.64040005    0.11189999    0.63270009
     7.5532012 ]
 [ -83.94468926    0.57480001    0.2735        0.4237        0.48199996
     7.54642725]
 ...
 [  48.59020053    0.22379999    0.2723        0.0873        0.20949998
     6.85359049]
 [  -3.12219678    0.2189        0.1673        0.1585        0.17839999
     6.83127165]
 [-423.199749      0.8853001     0.0561        0.83110011    0.85159999
     7.90775156]][0m
[37m[1m[2023-07-16 21:49:36,001][257371] Max Reward on eval: 288.9684991796501[0m
[37m[1m[2023-07-16 21:49:36,002][257371] Min Reward on eval: -603.4896621573716[0m
[37m[1m[2023-07-16 21:49:36,002][257371] Mean Reward across all agents: -53.746426658843326[0m
[37m[1m[2023-07-16 21:49:36,002][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:49:36,004][257371] mean_value=-264.87622050988205, max_value=358.19260050058364[0m
[37m[1m[2023-07-16 21:49:36,006][257371] New mean coefficients: [[-4.008905    2.3814511   3.4061718  -3.7005253   0.30314195 -1.2283143 ]][0m
[37m[1m[2023-07-16 21:49:36,007][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:49:44,986][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-16 21:49:44,986][257371] FPS: 427775.54[0m
[36m[2023-07-16 21:49:44,988][257371] itr=111, itrs=2000, Progress: 5.55%[0m
[36m[2023-07-16 21:49:56,853][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-16 21:49:56,853][257371] FPS: 324392.73[0m
[36m[2023-07-16 21:50:01,232][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:50:01,232][257371] Reward + Measures: [[7.65574768 0.41779834 0.41065097 0.09997067 0.41314366 7.15489531]][0m
[37m[1m[2023-07-16 21:50:01,232][257371] Max Reward on eval: 7.655747682318354[0m
[37m[1m[2023-07-16 21:50:01,233][257371] Min Reward on eval: 7.655747682318354[0m
[37m[1m[2023-07-16 21:50:01,233][257371] Mean Reward across all agents: 7.655747682318354[0m
[37m[1m[2023-07-16 21:50:01,233][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:50:06,278][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:50:06,284][257371] Reward + Measures: [[ 44.21539315   0.14909999   0.16720001   0.0902       0.1489
    6.85801649]
 [ -0.71995396   0.34839997   0.35240003   0.0835       0.36230001
    7.06582975]
 [166.80019711   0.3836       0.39720002   0.114        0.38960001
    7.33278513]
 ...
 [160.33938488   0.26139998   0.26390001   0.1062       0.2299
    6.84104109]
 [ 18.13095546   0.29850003   0.23210001   0.18980001   0.21960001
    6.79673338]
 [ 44.75844315   0.1349       0.15710001   0.125        0.1425
    6.87557983]][0m
[37m[1m[2023-07-16 21:50:06,284][257371] Max Reward on eval: 209.7695708443178[0m
[37m[1m[2023-07-16 21:50:06,284][257371] Min Reward on eval: -53.49113028459251[0m
[37m[1m[2023-07-16 21:50:06,284][257371] Mean Reward across all agents: 76.9125506274005[0m
[37m[1m[2023-07-16 21:50:06,284][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:50:06,286][257371] mean_value=-167.26909943120873, max_value=101.16629199024523[0m
[37m[1m[2023-07-16 21:50:06,288][257371] New mean coefficients: [[-3.9892318  3.1169348  3.4369586 -3.914187  -0.5428405 -1.0745888]][0m
[37m[1m[2023-07-16 21:50:06,289][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:50:15,357][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 21:50:15,358][257371] FPS: 423513.38[0m
[36m[2023-07-16 21:50:15,360][257371] itr=112, itrs=2000, Progress: 5.60%[0m
[36m[2023-07-16 21:50:27,104][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-16 21:50:27,104][257371] FPS: 327770.95[0m
[36m[2023-07-16 21:50:31,411][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:50:31,411][257371] Reward + Measures: [[9.16068404 0.4262093  0.42062435 0.09936267 0.41970232 7.16232729]][0m
[37m[1m[2023-07-16 21:50:31,411][257371] Max Reward on eval: 9.160684042246166[0m
[37m[1m[2023-07-16 21:50:31,412][257371] Min Reward on eval: 9.160684042246166[0m
[37m[1m[2023-07-16 21:50:31,412][257371] Mean Reward across all agents: 9.160684042246166[0m
[37m[1m[2023-07-16 21:50:31,412][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:50:36,466][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:50:36,467][257371] Reward + Measures: [[124.70018671   0.59630007   0.45390001   0.226        0.5826
    7.58847952]
 [  0.96252614   0.26939997   0.24609999   0.1151       0.26359999
    7.3077364 ]
 [ 70.06794076   0.86049998   0.27579999   0.86490005   0.47950003
    7.87975025]
 ...
 [ 89.2091546    0.71819997   0.14230001   0.66650003   0.67300004
    7.70907307]
 [ 22.04570912   0.1829       0.24340001   0.0836       0.2168
    7.01820707]
 [ 48.57666185   0.1524       0.1529       0.1007       0.1749
    7.09529972]][0m
[37m[1m[2023-07-16 21:50:36,467][257371] Max Reward on eval: 321.4901101935655[0m
[37m[1m[2023-07-16 21:50:36,467][257371] Min Reward on eval: -243.56974505512045[0m
[37m[1m[2023-07-16 21:50:36,467][257371] Mean Reward across all agents: 63.4464252856731[0m
[37m[1m[2023-07-16 21:50:36,468][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:50:36,472][257371] mean_value=-98.03429119710707, max_value=530.9206772661954[0m
[37m[1m[2023-07-16 21:50:36,475][257371] New mean coefficients: [[-4.7200694  3.952414   3.7574785 -4.2491746 -0.8686645 -0.9678082]][0m
[37m[1m[2023-07-16 21:50:36,476][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:50:45,516][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 21:50:45,516][257371] FPS: 424859.06[0m
[36m[2023-07-16 21:50:45,518][257371] itr=113, itrs=2000, Progress: 5.65%[0m
[36m[2023-07-16 21:50:57,372][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-16 21:50:57,372][257371] FPS: 324718.69[0m
[36m[2023-07-16 21:51:01,769][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:51:01,769][257371] Reward + Measures: [[7.13225992 0.42174202 0.41689867 0.09869666 0.41727898 7.15841913]][0m
[37m[1m[2023-07-16 21:51:01,770][257371] Max Reward on eval: 7.132259923107046[0m
[37m[1m[2023-07-16 21:51:01,770][257371] Min Reward on eval: 7.132259923107046[0m
[37m[1m[2023-07-16 21:51:01,770][257371] Mean Reward across all agents: 7.132259923107046[0m
[37m[1m[2023-07-16 21:51:01,770][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:51:06,829][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:51:06,834][257371] Reward + Measures: [[ 80.90693788   0.2339       0.29510003   0.07480001   0.2626
    7.04656696]
 [114.2128238    0.2023       0.26320001   0.0868       0.23150001
    6.62943411]
 [132.69695596   0.45169997   0.4955       0.1056       0.45670006
    7.25791311]
 ...
 [-34.23146603   0.44760004   0.4391       0.396        0.49060002
    7.36682224]
 [ 54.16055504   0.1767       0.28100002   0.06600001   0.20780002
    6.94113493]
 [ 57.87648322   0.1925       0.26009998   0.0974       0.22350001
    6.86687613]][0m
[37m[1m[2023-07-16 21:51:06,835][257371] Max Reward on eval: 220.2857537705451[0m
[37m[1m[2023-07-16 21:51:06,835][257371] Min Reward on eval: -178.62846598993056[0m
[37m[1m[2023-07-16 21:51:06,835][257371] Mean Reward across all agents: 43.63306744545759[0m
[37m[1m[2023-07-16 21:51:06,836][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:51:06,838][257371] mean_value=-180.33254565863064, max_value=196.45797400684256[0m
[37m[1m[2023-07-16 21:51:06,840][257371] New mean coefficients: [[-3.6037574  3.7593937  4.4023795 -4.6623282 -0.7391634 -1.4053025]][0m
[37m[1m[2023-07-16 21:51:06,841][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:51:15,962][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-16 21:51:15,962][257371] FPS: 421086.78[0m
[36m[2023-07-16 21:51:15,965][257371] itr=114, itrs=2000, Progress: 5.70%[0m
[36m[2023-07-16 21:51:27,795][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-16 21:51:27,795][257371] FPS: 325360.91[0m
[36m[2023-07-16 21:51:32,195][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:51:32,195][257371] Reward + Measures: [[8.20867848 0.41865098 0.41231802 0.097579   0.41223666 7.14861059]][0m
[37m[1m[2023-07-16 21:51:32,195][257371] Max Reward on eval: 8.208678480577511[0m
[37m[1m[2023-07-16 21:51:32,195][257371] Min Reward on eval: 8.208678480577511[0m
[37m[1m[2023-07-16 21:51:32,196][257371] Mean Reward across all agents: 8.208678480577511[0m
[37m[1m[2023-07-16 21:51:32,196][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:51:37,266][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:51:37,267][257371] Reward + Measures: [[ 76.77486454   0.1514       0.162        0.0882       0.1362
    6.63950968]
 [ 85.92468644   0.1707       0.17950001   0.15510002   0.16140001
    6.61778784]
 [ 18.25747709   0.28470001   0.21339999   0.18099999   0.25870001
    7.49849558]
 ...
 [-10.4266313    0.27009997   0.31479999   0.0857       0.2712
    7.10433054]
 [ 76.02705885   0.0958       0.1033       0.10399999   0.0876
    6.67004156]
 [ 10.14207437   0.09450001   0.077        0.12520002   0.09170001
    6.55965424]][0m
[37m[1m[2023-07-16 21:51:37,267][257371] Max Reward on eval: 157.74367427248507[0m
[37m[1m[2023-07-16 21:51:37,267][257371] Min Reward on eval: -115.04281366039068[0m
[37m[1m[2023-07-16 21:51:37,268][257371] Mean Reward across all agents: 41.98554647465536[0m
[37m[1m[2023-07-16 21:51:37,268][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:51:37,269][257371] mean_value=-201.69407989890735, max_value=133.2687572972223[0m
[37m[1m[2023-07-16 21:51:37,271][257371] New mean coefficients: [[-3.29274    1.6821113  5.638446  -4.3421416 -0.8710221 -1.1154916]][0m
[37m[1m[2023-07-16 21:51:37,272][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:51:46,291][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 21:51:46,292][257371] FPS: 425793.34[0m
[36m[2023-07-16 21:51:46,294][257371] itr=115, itrs=2000, Progress: 5.75%[0m
[36m[2023-07-16 21:51:58,048][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-16 21:51:58,049][257371] FPS: 327460.49[0m
[36m[2023-07-16 21:52:02,351][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:52:02,352][257371] Reward + Measures: [[7.37986503 0.43571967 0.42875335 0.09900033 0.42835864 7.17380381]][0m
[37m[1m[2023-07-16 21:52:02,352][257371] Max Reward on eval: 7.37986503322499[0m
[37m[1m[2023-07-16 21:52:02,352][257371] Min Reward on eval: 7.37986503322499[0m
[37m[1m[2023-07-16 21:52:02,353][257371] Mean Reward across all agents: 7.37986503322499[0m
[37m[1m[2023-07-16 21:52:02,353][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:52:07,597][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:52:07,603][257371] Reward + Measures: [[ 61.52293979   0.1063       0.13689999   0.117        0.111
    7.24009418]
 [ 33.81990368   0.27560002   0.2119       0.2194       0.13
    7.00546598]
 [310.61228458   0.57319999   0.49670002   0.56550002   0.1201
    7.44705343]
 ...
 [ 93.14914197   0.27779999   0.26390001   0.16059999   0.198
    6.97764444]
 [114.21151934   0.29389998   0.1969       0.26820001   0.14410001
    7.13125992]
 [ 28.99677317   0.3265       0.25560001   0.21750002   0.21110001
    7.30508566]][0m
[37m[1m[2023-07-16 21:52:07,603][257371] Max Reward on eval: 324.1303794376669[0m
[37m[1m[2023-07-16 21:52:07,604][257371] Min Reward on eval: -83.6341061707586[0m
[37m[1m[2023-07-16 21:52:07,604][257371] Mean Reward across all agents: 69.19793578359403[0m
[37m[1m[2023-07-16 21:52:07,604][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:52:07,607][257371] mean_value=-132.31477287845186, max_value=434.60934396709195[0m
[37m[1m[2023-07-16 21:52:07,610][257371] New mean coefficients: [[-2.9871294   1.1309391   7.3322315  -3.892226    0.38281262 -1.2859151 ]][0m
[37m[1m[2023-07-16 21:52:07,611][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:52:16,800][257371] train() took 9.19 seconds to complete[0m
[36m[2023-07-16 21:52:16,801][257371] FPS: 417952.45[0m
[36m[2023-07-16 21:52:16,803][257371] itr=116, itrs=2000, Progress: 5.80%[0m
[36m[2023-07-16 21:52:28,538][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-16 21:52:28,538][257371] FPS: 328015.22[0m
[36m[2023-07-16 21:52:32,914][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:52:32,914][257371] Reward + Measures: [[6.86511076 0.43946034 0.43170968 0.09849467 0.43150702 7.17717457]][0m
[37m[1m[2023-07-16 21:52:32,914][257371] Max Reward on eval: 6.865110757557001[0m
[37m[1m[2023-07-16 21:52:32,915][257371] Min Reward on eval: 6.865110757557001[0m
[37m[1m[2023-07-16 21:52:32,915][257371] Mean Reward across all agents: 6.865110757557001[0m
[37m[1m[2023-07-16 21:52:32,915][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:52:37,992][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:52:37,992][257371] Reward + Measures: [[ -0.56244032   0.1094       0.15540001   0.0978       0.18090001
    7.1244173 ]
 [-37.56709709   0.35170001   0.3506       0.1486       0.3849
    7.15233755]
 [ 95.60727269   0.38100001   0.37789997   0.1512       0.3876
    7.16384888]
 ...
 [109.70138329   0.35800001   0.38250002   0.1261       0.4113
    7.19397068]
 [141.75540618   0.53970003   0.46570006   0.34059998   0.35679999
    7.39538431]
 [ 19.58632707   0.54050004   0.41010004   0.22649999   0.53450006
    7.42139769]][0m
[37m[1m[2023-07-16 21:52:37,992][257371] Max Reward on eval: 264.90019221939144[0m
[37m[1m[2023-07-16 21:52:37,993][257371] Min Reward on eval: -147.52867938280104[0m
[37m[1m[2023-07-16 21:52:37,993][257371] Mean Reward across all agents: 81.60598616641124[0m
[37m[1m[2023-07-16 21:52:37,993][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:52:37,996][257371] mean_value=-143.01333751248205, max_value=521.2284960586426[0m
[37m[1m[2023-07-16 21:52:37,999][257371] New mean coefficients: [[-3.648034    0.9809451   7.0906763  -3.9856415   0.58757484 -0.92717296]][0m
[37m[1m[2023-07-16 21:52:38,000][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:52:47,071][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 21:52:47,071][257371] FPS: 423430.81[0m
[36m[2023-07-16 21:52:47,073][257371] itr=117, itrs=2000, Progress: 5.85%[0m
[36m[2023-07-16 21:52:58,773][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-16 21:52:58,773][257371] FPS: 329078.95[0m
[36m[2023-07-16 21:53:03,094][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:53:03,094][257371] Reward + Measures: [[0.07776001 0.45297    0.45024332 0.094539   0.44475833 7.18899059]][0m
[37m[1m[2023-07-16 21:53:03,095][257371] Max Reward on eval: 0.07776000975358087[0m
[37m[1m[2023-07-16 21:53:03,095][257371] Min Reward on eval: 0.07776000975358087[0m
[37m[1m[2023-07-16 21:53:03,095][257371] Mean Reward across all agents: 0.07776000975358087[0m
[37m[1m[2023-07-16 21:53:03,095][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:53:08,155][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:53:08,156][257371] Reward + Measures: [[92.68582989  0.2228      0.28830001  0.0762      0.226       6.93241596]
 [70.84232623  0.1684      0.17110001  0.1076      0.1619      6.65398645]
 [-0.80780545  0.3531      0.33329999  0.105       0.31810004  7.1573863 ]
 ...
 [44.9156393   0.154       0.1908      0.0759      0.1425      6.68160343]
 [74.92031121  0.31600001  0.32960001  0.0911      0.2949      6.82554007]
 [65.76855475  0.36669999  0.38770002  0.1074      0.34260002  7.13431168]][0m
[37m[1m[2023-07-16 21:53:08,156][257371] Max Reward on eval: 226.22012022091076[0m
[37m[1m[2023-07-16 21:53:08,157][257371] Min Reward on eval: -112.78436528258025[0m
[37m[1m[2023-07-16 21:53:08,157][257371] Mean Reward across all agents: 49.19146236727199[0m
[37m[1m[2023-07-16 21:53:08,157][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:53:08,159][257371] mean_value=-190.69858787380332, max_value=238.90591362667743[0m
[37m[1m[2023-07-16 21:53:08,161][257371] New mean coefficients: [[-4.1877427  1.0477734  7.660548  -4.298509   0.8054258 -1.1819378]][0m
[37m[1m[2023-07-16 21:53:08,162][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:53:17,243][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 21:53:17,244][257371] FPS: 422917.00[0m
[36m[2023-07-16 21:53:17,246][257371] itr=118, itrs=2000, Progress: 5.90%[0m
[36m[2023-07-16 21:53:29,029][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-16 21:53:29,029][257371] FPS: 326798.94[0m
[36m[2023-07-16 21:53:33,284][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:53:33,285][257371] Reward + Measures: [[-2.84439024  0.44460335  0.43956932  0.09684866  0.43664932  7.17280149]][0m
[37m[1m[2023-07-16 21:53:33,285][257371] Max Reward on eval: -2.8443902425451135[0m
[37m[1m[2023-07-16 21:53:33,285][257371] Min Reward on eval: -2.8443902425451135[0m
[37m[1m[2023-07-16 21:53:33,285][257371] Mean Reward across all agents: -2.8443902425451135[0m
[37m[1m[2023-07-16 21:53:33,286][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:53:38,295][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:53:38,296][257371] Reward + Measures: [[ 20.20079807   0.1051       0.108        0.1019       0.1099
    6.32673073]
 [ 44.51420062   0.1006       0.13130002   0.0945       0.1221
    6.63874197]
 [ 33.43837879   0.1303       0.20380001   0.103        0.17940001
    6.86119795]
 ...
 [ 10.63373597   0.09860001   0.10159999   0.0982       0.10699999
    6.51209116]
 [-10.75477444   0.2455       0.37680003   0.0898       0.35419998
    7.17423201]
 [-38.51130877   0.10860001   0.1344       0.08440001   0.1147
    6.33863878]][0m
[37m[1m[2023-07-16 21:53:38,296][257371] Max Reward on eval: 106.77659981977195[0m
[37m[1m[2023-07-16 21:53:38,296][257371] Min Reward on eval: -72.07129548052326[0m
[37m[1m[2023-07-16 21:53:38,296][257371] Mean Reward across all agents: 12.63634008527909[0m
[37m[1m[2023-07-16 21:53:38,297][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:53:38,298][257371] mean_value=-246.14047426137336, max_value=-42.590741089409015[0m
[36m[2023-07-16 21:53:38,300][257371] XNES is restarting with a new solution whose measures are [0.71390003 0.7137     0.0288     0.727      7.60470295] and objective is 259.3305557061452[0m
[36m[2023-07-16 21:53:38,301][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-16 21:53:38,303][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-16 21:53:38,304][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:53:47,344][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 21:53:47,345][257371] FPS: 424839.10[0m
[36m[2023-07-16 21:53:47,347][257371] itr=119, itrs=2000, Progress: 5.95%[0m
[36m[2023-07-16 21:53:59,320][257371] train() took 11.94 seconds to complete[0m
[36m[2023-07-16 21:53:59,320][257371] FPS: 321556.60[0m
[36m[2023-07-16 21:54:03,656][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:54:03,656][257371] Reward + Measures: [[351.81224698   0.78187931   0.76595467   0.04779767   0.78890073
    7.53548288]][0m
[37m[1m[2023-07-16 21:54:03,657][257371] Max Reward on eval: 351.81224698010925[0m
[37m[1m[2023-07-16 21:54:03,657][257371] Min Reward on eval: 351.81224698010925[0m
[37m[1m[2023-07-16 21:54:03,657][257371] Mean Reward across all agents: 351.81224698010925[0m
[37m[1m[2023-07-16 21:54:03,657][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:54:08,953][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:54:08,954][257371] Reward + Measures: [[329.74213837   0.88160002   0.88760006   0.0242       0.91600007
    7.48293018]
 [343.58588034   0.86659998   0.85020012   0.0453       0.87639999
    7.56996012]
 [364.73712684   0.86680001   0.86609995   0.035        0.88929999
    7.61457968]
 ...
 [159.0796235    0.70600003   0.69080001   0.0494       0.72140002
    7.46988392]
 [174.4847491    0.70700002   0.6753       0.08459999   0.70789999
    7.40468836]
 [280.40681337   0.71900004   0.71450007   0.0398       0.71570003
    7.46842718]][0m
[37m[1m[2023-07-16 21:54:08,954][257371] Max Reward on eval: 630.5199794746935[0m
[37m[1m[2023-07-16 21:54:08,954][257371] Min Reward on eval: -33.52325865966268[0m
[37m[1m[2023-07-16 21:54:08,954][257371] Mean Reward across all agents: 234.9720117785117[0m
[37m[1m[2023-07-16 21:54:08,955][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:54:08,961][257371] mean_value=156.9709400151077, max_value=616.4391927932691[0m
[37m[1m[2023-07-16 21:54:08,964][257371] New mean coefficients: [[ 0.42736718 -1.015815    0.84961677 -1.343165   -1.0440074  -0.2479612 ]][0m
[37m[1m[2023-07-16 21:54:08,965][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:54:18,046][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 21:54:18,046][257371] FPS: 422929.43[0m
[36m[2023-07-16 21:54:18,048][257371] itr=120, itrs=2000, Progress: 6.00%[0m
[37m[1m[2023-07-16 21:56:17,210][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000100[0m
[36m[2023-07-16 21:56:29,617][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-16 21:56:29,617][257371] FPS: 324560.45[0m
[36m[2023-07-16 21:56:33,802][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:56:33,802][257371] Reward + Measures: [[343.78059887   0.74496758   0.73223764   0.04654066   0.75719261
    7.48010588]][0m
[37m[1m[2023-07-16 21:56:33,802][257371] Max Reward on eval: 343.78059887023164[0m
[37m[1m[2023-07-16 21:56:33,803][257371] Min Reward on eval: 343.78059887023164[0m
[37m[1m[2023-07-16 21:56:33,803][257371] Mean Reward across all agents: 343.78059887023164[0m
[37m[1m[2023-07-16 21:56:33,803][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:56:38,796][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:56:38,797][257371] Reward + Measures: [[261.36234731   0.6559       0.63169998   0.0535       0.64660001
    7.46162748]
 [325.90888114   0.77640003   0.78119993   0.041        0.81729996
    7.41086912]
 [431.73564999   0.79580009   0.78949994   0.0408       0.81189996
    7.49769449]
 ...
 [184.30214613   0.55129999   0.54449999   0.0307       0.55450004
    7.42560959]
 [148.59032823   0.68460006   0.69139999   0.0232       0.70849997
    7.42764425]
 [338.26251389   0.73009998   0.72719997   0.0256       0.7633
    7.41509867]][0m
[37m[1m[2023-07-16 21:56:38,797][257371] Max Reward on eval: 559.2660933673382[0m
[37m[1m[2023-07-16 21:56:38,797][257371] Min Reward on eval: 33.731775940814984[0m
[37m[1m[2023-07-16 21:56:38,798][257371] Mean Reward across all agents: 294.28857951739553[0m
[37m[1m[2023-07-16 21:56:38,798][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:56:38,802][257371] mean_value=13.703551567214138, max_value=259.92350413269247[0m
[37m[1m[2023-07-16 21:56:38,805][257371] New mean coefficients: [[-0.4876698  -0.66305065 -0.06604201 -1.0801218  -1.4698793   0.11365795]][0m
[37m[1m[2023-07-16 21:56:38,806][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:56:47,719][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-16 21:56:47,720][257371] FPS: 430876.45[0m
[36m[2023-07-16 21:56:47,722][257371] itr=121, itrs=2000, Progress: 6.05%[0m
[36m[2023-07-16 21:56:59,603][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-16 21:56:59,603][257371] FPS: 329211.09[0m
[36m[2023-07-16 21:57:03,825][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:57:03,825][257371] Reward + Measures: [[286.29616764   0.71179736   0.70394737   0.049147     0.73024297
    7.44058275]][0m
[37m[1m[2023-07-16 21:57:03,826][257371] Max Reward on eval: 286.29616763549734[0m
[37m[1m[2023-07-16 21:57:03,826][257371] Min Reward on eval: 286.29616763549734[0m
[37m[1m[2023-07-16 21:57:03,826][257371] Mean Reward across all agents: 286.29616763549734[0m
[37m[1m[2023-07-16 21:57:03,827][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:57:08,855][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:57:08,856][257371] Reward + Measures: [[294.08634837   0.77629995   0.74800003   0.0648       0.78200001
    7.51542044]
 [-35.86926635   0.70300001   0.72320002   0.0345       0.7596001
    7.30474329]
 [347.79853938   0.73360002   0.73049998   0.0352       0.74760002
    7.51560593]
 ...
 [250.70556609   0.70600003   0.6965       0.0411       0.74250001
    7.36452341]
 [ 40.92656797   0.59400004   0.58780003   0.0674       0.62800002
    7.23731375]
 [232.94579589   0.64360005   0.61650002   0.0768       0.63610005
    7.39505148]][0m
[37m[1m[2023-07-16 21:57:08,856][257371] Max Reward on eval: 489.0111310236156[0m
[37m[1m[2023-07-16 21:57:08,857][257371] Min Reward on eval: -82.93416695049964[0m
[37m[1m[2023-07-16 21:57:08,857][257371] Mean Reward across all agents: 219.67515177411056[0m
[37m[1m[2023-07-16 21:57:08,857][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:57:08,859][257371] mean_value=-100.67200264050055, max_value=740.2440159484744[0m
[37m[1m[2023-07-16 21:57:08,862][257371] New mean coefficients: [[-0.74000335 -0.01843864  0.07065754 -1.1272538  -0.9857837   0.62905693]][0m
[37m[1m[2023-07-16 21:57:08,863][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:57:17,827][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-16 21:57:17,827][257371] FPS: 428448.27[0m
[36m[2023-07-16 21:57:17,830][257371] itr=122, itrs=2000, Progress: 6.10%[0m
[36m[2023-07-16 21:57:29,634][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-16 21:57:29,634][257371] FPS: 326162.98[0m
[36m[2023-07-16 21:57:34,020][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:57:34,020][257371] Reward + Measures: [[238.3123188    0.70361435   0.69774365   0.051617     0.72510463
    7.4478097 ]][0m
[37m[1m[2023-07-16 21:57:34,020][257371] Max Reward on eval: 238.31231879833462[0m
[37m[1m[2023-07-16 21:57:34,021][257371] Min Reward on eval: 238.31231879833462[0m
[37m[1m[2023-07-16 21:57:34,021][257371] Mean Reward across all agents: 238.31231879833462[0m
[37m[1m[2023-07-16 21:57:34,021][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:57:39,208][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:57:39,209][257371] Reward + Measures: [[ 62.30106018   0.50789994   0.51570004   0.0454       0.53420001
    7.30995798]
 [124.64482612   0.55989999   0.53320003   0.0686       0.5711
    7.37547827]
 [193.63500549   0.55699998   0.55089998   0.0585       0.57499999
    7.33539343]
 ...
 [109.87568695   0.58240002   0.58420002   0.07000001   0.63350004
    7.17282724]
 [290.88401001   0.77710003   0.77329999   0.0389       0.81370002
    7.39020491]
 [189.76555393   0.57249993   0.56209999   0.0561       0.59960002
    7.3743701 ]][0m
[37m[1m[2023-07-16 21:57:39,209][257371] Max Reward on eval: 464.02645868361[0m
[37m[1m[2023-07-16 21:57:39,209][257371] Min Reward on eval: -78.64931335560978[0m
[37m[1m[2023-07-16 21:57:39,209][257371] Mean Reward across all agents: 170.3824652172324[0m
[37m[1m[2023-07-16 21:57:39,210][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:57:39,212][257371] mean_value=-139.2957554570477, max_value=123.49995053360561[0m
[37m[1m[2023-07-16 21:57:39,214][257371] New mean coefficients: [[-1.0578868  -0.07849795  0.10351182 -2.4359117  -1.0346618   1.2642362 ]][0m
[37m[1m[2023-07-16 21:57:39,215][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:57:48,244][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 21:57:48,244][257371] FPS: 425377.31[0m
[36m[2023-07-16 21:57:48,247][257371] itr=123, itrs=2000, Progress: 6.15%[0m
[36m[2023-07-16 21:58:00,057][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-16 21:58:00,057][257371] FPS: 326018.26[0m
[36m[2023-07-16 21:58:04,340][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:58:04,341][257371] Reward + Measures: [[217.02285935   0.6928423    0.68382764   0.05609534   0.71192002
    7.47151661]][0m
[37m[1m[2023-07-16 21:58:04,341][257371] Max Reward on eval: 217.02285935158073[0m
[37m[1m[2023-07-16 21:58:04,341][257371] Min Reward on eval: 217.02285935158073[0m
[37m[1m[2023-07-16 21:58:04,342][257371] Mean Reward across all agents: 217.02285935158073[0m
[37m[1m[2023-07-16 21:58:04,342][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:58:09,376][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:58:09,376][257371] Reward + Measures: [[254.04386138   0.69889998   0.68780005   0.0687       0.70300007
    7.39223099]
 [230.98159073   0.72440004   0.73390001   0.0372       0.76130003
    7.43423033]
 [244.02091735   0.62910002   0.62669998   0.0641       0.65270001
    7.44298124]
 ...
 [151.57314849   0.63450003   0.61729997   0.0592       0.64840001
    7.40222788]
 [114.96437837   0.6117       0.57709998   0.0947       0.62470001
    7.37493896]
 [205.05835869   0.69670004   0.68510002   0.07579999   0.70679998
    7.41182041]][0m
[37m[1m[2023-07-16 21:58:09,377][257371] Max Reward on eval: 375.49601177000443[0m
[37m[1m[2023-07-16 21:58:09,377][257371] Min Reward on eval: -19.215615062881263[0m
[37m[1m[2023-07-16 21:58:09,377][257371] Mean Reward across all agents: 191.27557729034766[0m
[37m[1m[2023-07-16 21:58:09,377][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:58:09,379][257371] mean_value=-178.77259858894257, max_value=-14.796396057202344[0m
[36m[2023-07-16 21:58:09,381][257371] XNES is restarting with a new solution whose measures are [0.42420003 0.1926     0.42919999 0.21700001 7.58414602] and objective is -11.297514153085649[0m
[36m[2023-07-16 21:58:09,382][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-16 21:58:09,384][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-16 21:58:09,385][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:58:18,392][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 21:58:18,392][257371] FPS: 426406.25[0m
[36m[2023-07-16 21:58:18,395][257371] itr=124, itrs=2000, Progress: 6.20%[0m
[36m[2023-07-16 21:58:30,124][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-16 21:58:30,125][257371] FPS: 328166.10[0m
[36m[2023-07-16 21:58:34,479][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:58:34,480][257371] Reward + Measures: [[67.72502721  0.30334267  0.22304532  0.176668    0.28510234  7.42425394]][0m
[37m[1m[2023-07-16 21:58:34,480][257371] Max Reward on eval: 67.72502720977955[0m
[37m[1m[2023-07-16 21:58:34,480][257371] Min Reward on eval: 67.72502720977955[0m
[37m[1m[2023-07-16 21:58:34,480][257371] Mean Reward across all agents: 67.72502720977955[0m
[37m[1m[2023-07-16 21:58:34,481][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:58:39,661][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:58:39,662][257371] Reward + Measures: [[ 38.3578301    0.56850004   0.3321       0.32270002   0.54150003
    7.52193451]
 [ 74.86568302   0.49190003   0.1479       0.40039998   0.45269999
    7.59453917]
 [149.46986326   0.70880002   0.1415       0.63180006   0.64450008
    7.71135855]
 ...
 [-16.35135604   0.1743       0.12120002   0.15349999   0.16730002
    7.3688097 ]
 [ 38.62357106   0.2119       0.07360001   0.22400001   0.2115
    7.36803389]
 [ 51.61187837   0.54699999   0.1763       0.46819997   0.45989999
    7.65113688]][0m
[37m[1m[2023-07-16 21:58:39,662][257371] Max Reward on eval: 408.1618785592262[0m
[37m[1m[2023-07-16 21:58:39,662][257371] Min Reward on eval: -243.46076537296176[0m
[37m[1m[2023-07-16 21:58:39,662][257371] Mean Reward across all agents: 89.58214543830677[0m
[37m[1m[2023-07-16 21:58:39,662][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:58:39,668][257371] mean_value=-65.83914365043825, max_value=765.8293392769526[0m
[37m[1m[2023-07-16 21:58:39,671][257371] New mean coefficients: [[ 0.24364698 -1.7497907  -3.2254367  -1.3828157  -1.3457903  -1.0617354 ]][0m
[37m[1m[2023-07-16 21:58:39,672][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:58:48,813][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-16 21:58:48,814][257371] FPS: 420129.32[0m
[36m[2023-07-16 21:58:48,816][257371] itr=125, itrs=2000, Progress: 6.25%[0m
[36m[2023-07-16 21:59:00,504][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-16 21:59:00,504][257371] FPS: 329333.85[0m
[36m[2023-07-16 21:59:04,816][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:59:04,817][257371] Reward + Measures: [[63.70290956  0.29599866  0.222205    0.170738    0.27783468  7.42364693]][0m
[37m[1m[2023-07-16 21:59:04,817][257371] Max Reward on eval: 63.70290956232672[0m
[37m[1m[2023-07-16 21:59:04,817][257371] Min Reward on eval: 63.70290956232672[0m
[37m[1m[2023-07-16 21:59:04,818][257371] Mean Reward across all agents: 63.70290956232672[0m
[37m[1m[2023-07-16 21:59:04,818][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:59:09,855][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:59:09,860][257371] Reward + Measures: [[ 44.82908448   0.22939999   0.1214       0.21170001   0.23269999
    7.3188448 ]
 [ 72.87385791   0.13249999   0.10619999   0.1432       0.1418
    7.31575727]
 [ 11.6792883    0.09230001   0.0951       0.0823       0.0839
    7.22232962]
 ...
 [ 56.07494784   0.19310001   0.20370002   0.0794       0.1947
    7.24827671]
 [-40.06742559   0.18099998   0.1481       0.1199       0.1848
    7.40385008]
 [ -6.95307264   0.11099999   0.0861       0.11850001   0.10810001
    7.2839303 ]][0m
[37m[1m[2023-07-16 21:59:09,861][257371] Max Reward on eval: 309.6371532078832[0m
[37m[1m[2023-07-16 21:59:09,861][257371] Min Reward on eval: -102.90033807270228[0m
[37m[1m[2023-07-16 21:59:09,861][257371] Mean Reward across all agents: 59.325343575662785[0m
[37m[1m[2023-07-16 21:59:09,862][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:59:09,864][257371] mean_value=-130.01652836370758, max_value=121.74698822741018[0m
[37m[1m[2023-07-16 21:59:09,866][257371] New mean coefficients: [[-0.6637067 -1.4871032 -1.5459666 -1.4756066 -1.2951105 -1.7908543]][0m
[37m[1m[2023-07-16 21:59:09,867][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:59:19,034][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-16 21:59:19,034][257371] FPS: 418998.53[0m
[36m[2023-07-16 21:59:19,036][257371] itr=126, itrs=2000, Progress: 6.30%[0m
[36m[2023-07-16 21:59:30,803][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-16 21:59:30,803][257371] FPS: 327194.45[0m
[36m[2023-07-16 21:59:35,186][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:59:35,186][257371] Reward + Measures: [[52.41792497  0.27768534  0.21118966  0.15927634  0.26161599  7.41316652]][0m
[37m[1m[2023-07-16 21:59:35,186][257371] Max Reward on eval: 52.41792497418245[0m
[37m[1m[2023-07-16 21:59:35,187][257371] Min Reward on eval: 52.41792497418245[0m
[37m[1m[2023-07-16 21:59:35,187][257371] Mean Reward across all agents: 52.41792497418245[0m
[37m[1m[2023-07-16 21:59:35,187][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:59:40,376][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 21:59:40,377][257371] Reward + Measures: [[-235.93535616    0.79870003    0.07870001    0.80900002    0.77580005
     7.6840415 ]
 [ 133.99757923    0.20369999    0.1557        0.1989        0.20999999
     6.97119856]
 [ 118.82743173    0.33520001    0.32010001    0.11170001    0.3242
     7.47347879]
 ...
 [  14.13898       0.62529999    0.38610002    0.3592        0.58450001
     7.48621082]
 [ 136.36834083    0.17350002    0.083         0.1857        0.16010001
     6.92534876]
 [ -45.36267749    0.2339        0.08759999    0.27509999    0.21269999
     7.33633375]][0m
[37m[1m[2023-07-16 21:59:40,377][257371] Max Reward on eval: 257.8626957505941[0m
[37m[1m[2023-07-16 21:59:40,377][257371] Min Reward on eval: -367.7208804900758[0m
[37m[1m[2023-07-16 21:59:40,378][257371] Mean Reward across all agents: 50.70092253863969[0m
[37m[1m[2023-07-16 21:59:40,378][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 21:59:40,380][257371] mean_value=-166.8846166146663, max_value=59.81532884759733[0m
[37m[1m[2023-07-16 21:59:40,383][257371] New mean coefficients: [[ 0.32313555 -1.8566558  -1.9449725  -0.38533843 -0.8608335  -1.2262005 ]][0m
[37m[1m[2023-07-16 21:59:40,384][257371] Moving the mean solution point...[0m
[36m[2023-07-16 21:59:49,479][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-16 21:59:49,480][257371] FPS: 422253.56[0m
[36m[2023-07-16 21:59:49,482][257371] itr=127, itrs=2000, Progress: 6.35%[0m
[36m[2023-07-16 22:00:01,259][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-16 22:00:01,260][257371] FPS: 326841.54[0m
[36m[2023-07-16 22:00:05,559][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:00:05,559][257371] Reward + Measures: [[53.92237076  0.28026265  0.20500101  0.16823934  0.263688    7.41039944]][0m
[37m[1m[2023-07-16 22:00:05,560][257371] Max Reward on eval: 53.92237076039097[0m
[37m[1m[2023-07-16 22:00:05,560][257371] Min Reward on eval: 53.92237076039097[0m
[37m[1m[2023-07-16 22:00:05,560][257371] Mean Reward across all agents: 53.92237076039097[0m
[37m[1m[2023-07-16 22:00:05,560][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:00:10,609][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:00:10,615][257371] Reward + Measures: [[112.46652259   0.396        0.33850002   0.18270002   0.37760001
    7.46969175]
 [ 47.86400402   0.23799999   0.21350001   0.12340001   0.23640001
    7.36209345]
 [220.43612576   0.74350005   0.26630002   0.69050002   0.5036
    7.79503775]
 ...
 [ 30.46240345   0.15359999   0.1318       0.1024       0.16069999
    7.34440327]
 [ 35.35645124   0.0927       0.08840001   0.0837       0.09940001
    7.135849  ]
 [ -5.09326978   0.07669999   0.0702       0.0839       0.09
    7.02357817]][0m
[37m[1m[2023-07-16 22:00:10,615][257371] Max Reward on eval: 286.30929580600935[0m
[37m[1m[2023-07-16 22:00:10,616][257371] Min Reward on eval: -49.64045803435147[0m
[37m[1m[2023-07-16 22:00:10,616][257371] Mean Reward across all agents: 51.20856737172361[0m
[37m[1m[2023-07-16 22:00:10,616][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:00:10,618][257371] mean_value=-151.73997245171952, max_value=153.39705086532382[0m
[37m[1m[2023-07-16 22:00:10,620][257371] New mean coefficients: [[ 0.3653889  -1.2709799   0.49325466 -0.49366665 -0.64438426 -1.5163867 ]][0m
[37m[1m[2023-07-16 22:00:10,621][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:00:19,716][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-16 22:00:19,716][257371] FPS: 422303.19[0m
[36m[2023-07-16 22:00:19,719][257371] itr=128, itrs=2000, Progress: 6.40%[0m
[36m[2023-07-16 22:00:31,447][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-16 22:00:31,447][257371] FPS: 328216.44[0m
[36m[2023-07-16 22:00:35,799][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:00:35,800][257371] Reward + Measures: [[57.88431276  0.27630132  0.206267    0.163302    0.26172167  7.40269279]][0m
[37m[1m[2023-07-16 22:00:35,800][257371] Max Reward on eval: 57.8843127597456[0m
[37m[1m[2023-07-16 22:00:35,800][257371] Min Reward on eval: 57.8843127597456[0m
[37m[1m[2023-07-16 22:00:35,800][257371] Mean Reward across all agents: 57.8843127597456[0m
[37m[1m[2023-07-16 22:00:35,801][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:00:40,819][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:00:40,820][257371] Reward + Measures: [[ -3.57034038   0.1337       0.1132       0.1048       0.1319
    7.38388014]
 [ 21.52345135   0.47259998   0.4375       0.1276       0.43280002
    7.44872236]
 [ 94.88202177   0.27280003   0.28889999   0.08000001   0.28060001
    7.17173481]
 ...
 [133.33977473   0.39920002   0.12719999   0.34030002   0.4048
    7.56805563]
 [ 47.54294201   0.13250001   0.0995       0.1142       0.1199
    7.14960718]
 [248.96429478   0.71340001   0.24510001   0.51429999   0.68950003
    7.73130941]][0m
[37m[1m[2023-07-16 22:00:40,820][257371] Max Reward on eval: 348.40154981166125[0m
[37m[1m[2023-07-16 22:00:40,820][257371] Min Reward on eval: -60.3533179089427[0m
[37m[1m[2023-07-16 22:00:40,821][257371] Mean Reward across all agents: 78.65495054394023[0m
[37m[1m[2023-07-16 22:00:40,821][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:00:40,824][257371] mean_value=-106.39133767668363, max_value=540.5757219352527[0m
[37m[1m[2023-07-16 22:00:40,827][257371] New mean coefficients: [[ 0.5858352  -1.7333236   0.75917435 -2.675983   -0.2043975  -1.1455677 ]][0m
[37m[1m[2023-07-16 22:00:40,828][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:00:49,849][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 22:00:49,850][257371] FPS: 425705.63[0m
[36m[2023-07-16 22:00:49,852][257371] itr=129, itrs=2000, Progress: 6.45%[0m
[36m[2023-07-16 22:01:01,651][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-16 22:01:01,651][257371] FPS: 326251.44[0m
[36m[2023-07-16 22:01:05,985][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:01:05,986][257371] Reward + Measures: [[52.25269345  0.26793033  0.19907701  0.16210666  0.25395396  7.39548349]][0m
[37m[1m[2023-07-16 22:01:05,986][257371] Max Reward on eval: 52.25269344809469[0m
[37m[1m[2023-07-16 22:01:05,986][257371] Min Reward on eval: 52.25269344809469[0m
[37m[1m[2023-07-16 22:01:05,986][257371] Mean Reward across all agents: 52.25269344809469[0m
[37m[1m[2023-07-16 22:01:05,987][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:01:11,025][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:01:11,031][257371] Reward + Measures: [[  3.98165948   0.36670002   0.19160001   0.26139998   0.39050001
    7.51352835]
 [-26.97196175   0.15150002   0.1195       0.1293       0.1693
    7.44327116]
 [ 87.20348613   0.31830001   0.24969999   0.1991       0.2296
    7.34872198]
 ...
 [170.64009143   0.26200002   0.23819999   0.1631       0.2483
    7.1943512 ]
 [ 87.52298307   0.26120004   0.26550001   0.0758       0.27449998
    7.38473225]
 [132.67029066   0.42609999   0.34850001   0.15449999   0.41939998
    7.43413115]][0m
[37m[1m[2023-07-16 22:01:11,031][257371] Max Reward on eval: 265.99892428610474[0m
[37m[1m[2023-07-16 22:01:11,031][257371] Min Reward on eval: -95.42827464304864[0m
[37m[1m[2023-07-16 22:01:11,032][257371] Mean Reward across all agents: 52.76680736112917[0m
[37m[1m[2023-07-16 22:01:11,032][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:01:11,034][257371] mean_value=-128.55614708162335, max_value=360.11880496678185[0m
[37m[1m[2023-07-16 22:01:11,036][257371] New mean coefficients: [[ 1.740035   -1.1887071   1.1001661  -1.9942688   0.28247803 -1.0473328 ]][0m
[37m[1m[2023-07-16 22:01:11,037][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:01:20,082][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 22:01:20,082][257371] FPS: 424650.94[0m
[36m[2023-07-16 22:01:20,084][257371] itr=130, itrs=2000, Progress: 6.50%[0m
[37m[1m[2023-07-16 22:03:19,643][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000110[0m
[36m[2023-07-16 22:03:31,873][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-16 22:03:31,873][257371] FPS: 326084.25[0m
[36m[2023-07-16 22:03:36,136][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:03:36,137][257371] Reward + Measures: [[51.71489371  0.26769701  0.20491932  0.15276065  0.25557566  7.39256096]][0m
[37m[1m[2023-07-16 22:03:36,137][257371] Max Reward on eval: 51.714893713256025[0m
[37m[1m[2023-07-16 22:03:36,137][257371] Min Reward on eval: 51.714893713256025[0m
[37m[1m[2023-07-16 22:03:36,137][257371] Mean Reward across all agents: 51.714893713256025[0m
[37m[1m[2023-07-16 22:03:36,138][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:03:41,315][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:03:41,316][257371] Reward + Measures: [[ 51.41069533   0.1232       0.127        0.0825       0.15610002
    7.28234434]
 [-12.37796294   0.082        0.0644       0.1164       0.0965
    7.56796026]
 [312.47342484   0.69809997   0.65340006   0.1617       0.64460003
    7.54770803]
 ...
 [ 30.50825192   0.105        0.0834       0.1347       0.13329999
    7.47817564]
 [157.28503462   0.35620001   0.28220001   0.14970002   0.37310001
    7.38649511]
 [152.51410121   0.39579996   0.3917       0.0944       0.39040002
    7.39609385]][0m
[37m[1m[2023-07-16 22:03:41,316][257371] Max Reward on eval: 345.02315402246313[0m
[37m[1m[2023-07-16 22:03:41,316][257371] Min Reward on eval: -64.15005529164337[0m
[37m[1m[2023-07-16 22:03:41,316][257371] Mean Reward across all agents: 59.156739493097014[0m
[37m[1m[2023-07-16 22:03:41,317][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:03:41,319][257371] mean_value=-87.6346351087946, max_value=124.73167397993979[0m
[37m[1m[2023-07-16 22:03:41,322][257371] New mean coefficients: [[ 1.9964988 -1.0320078  1.0520755 -2.918216   0.8127888 -1.1146268]][0m
[37m[1m[2023-07-16 22:03:41,323][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:03:50,366][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 22:03:50,366][257371] FPS: 424727.07[0m
[36m[2023-07-16 22:03:50,368][257371] itr=131, itrs=2000, Progress: 6.55%[0m
[36m[2023-07-16 22:04:02,194][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-16 22:04:02,194][257371] FPS: 325496.17[0m
[36m[2023-07-16 22:04:06,482][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:04:06,488][257371] Reward + Measures: [[46.92815436  0.26694432  0.209776    0.14533533  0.25579     7.38872004]][0m
[37m[1m[2023-07-16 22:04:06,488][257371] Max Reward on eval: 46.92815436171846[0m
[37m[1m[2023-07-16 22:04:06,488][257371] Min Reward on eval: 46.92815436171846[0m
[37m[1m[2023-07-16 22:04:06,489][257371] Mean Reward across all agents: 46.92815436171846[0m
[37m[1m[2023-07-16 22:04:06,489][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:04:11,579][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:04:11,580][257371] Reward + Measures: [[-15.33839316   0.2052       0.19340001   0.0979       0.20750001
    7.34211206]
 [251.55514845   0.5115       0.4341       0.1496       0.51660001
    7.45802164]
 [ 93.12031852   0.38030002   0.39710003   0.0764       0.38489997
    7.3261323 ]
 ...
 [ 10.84433854   0.18889999   0.1749       0.0926       0.2067
    7.25072098]
 [114.79423825   0.32870001   0.23479998   0.17840001   0.32100001
    7.43617105]
 [179.47642415   0.38659999   0.117        0.347        0.36579999
    7.50038385]][0m
[37m[1m[2023-07-16 22:04:11,580][257371] Max Reward on eval: 251.55514845270665[0m
[37m[1m[2023-07-16 22:04:11,580][257371] Min Reward on eval: -85.46392550948076[0m
[37m[1m[2023-07-16 22:04:11,580][257371] Mean Reward across all agents: 60.231690318306455[0m
[37m[1m[2023-07-16 22:04:11,581][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:04:11,582][257371] mean_value=-131.4797425281091, max_value=32.358921274825235[0m
[37m[1m[2023-07-16 22:04:11,585][257371] New mean coefficients: [[ 2.1716683   0.38361    -0.00057673 -3.1694717   1.7260425  -1.5136337 ]][0m
[37m[1m[2023-07-16 22:04:11,586][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:04:20,619][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 22:04:20,620][257371] FPS: 425152.75[0m
[36m[2023-07-16 22:04:20,622][257371] itr=132, itrs=2000, Progress: 6.60%[0m
[36m[2023-07-16 22:04:32,323][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-16 22:04:32,323][257371] FPS: 328977.75[0m
[36m[2023-07-16 22:04:36,682][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:04:36,688][257371] Reward + Measures: [[49.69323141  0.26605502  0.209455    0.14459367  0.25504267  7.37717438]][0m
[37m[1m[2023-07-16 22:04:36,688][257371] Max Reward on eval: 49.69323141186576[0m
[37m[1m[2023-07-16 22:04:36,688][257371] Min Reward on eval: 49.69323141186576[0m
[37m[1m[2023-07-16 22:04:36,689][257371] Mean Reward across all agents: 49.69323141186576[0m
[37m[1m[2023-07-16 22:04:36,689][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:04:41,758][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:04:41,763][257371] Reward + Measures: [[ 67.01951618   0.29440001   0.16500001   0.2457       0.26499999
    7.37280226]
 [ -5.64388314   0.147        0.09779999   0.14700001   0.1382
    7.39858341]
 [ 45.62408351   0.41890001   0.39120004   0.0899       0.42260003
    7.38808918]
 ...
 [ 57.21045755   0.21069999   0.20650001   0.0903       0.20920001
    7.29488134]
 [-11.60154776   0.23389998   0.18340001   0.1472       0.22610001
    7.43628311]
 [ 84.73220523   0.35330003   0.26710001   0.16620001   0.3441
    7.41613007]][0m
[37m[1m[2023-07-16 22:04:41,763][257371] Max Reward on eval: 223.78977325772865[0m
[37m[1m[2023-07-16 22:04:41,763][257371] Min Reward on eval: -68.25272004008293[0m
[37m[1m[2023-07-16 22:04:41,763][257371] Mean Reward across all agents: 44.738430049725864[0m
[37m[1m[2023-07-16 22:04:41,764][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:04:41,765][257371] mean_value=-152.5216940806772, max_value=20.37767242257962[0m
[37m[1m[2023-07-16 22:04:41,768][257371] New mean coefficients: [[ 2.990837    0.17789951  0.06576001 -3.1614444   1.2025852  -1.1709551 ]][0m
[37m[1m[2023-07-16 22:04:41,769][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:04:50,879][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-16 22:04:50,879][257371] FPS: 421593.16[0m
[36m[2023-07-16 22:04:50,881][257371] itr=133, itrs=2000, Progress: 6.65%[0m
[36m[2023-07-16 22:05:02,538][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-16 22:05:02,538][257371] FPS: 330261.48[0m
[36m[2023-07-16 22:05:06,825][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:05:06,826][257371] Reward + Measures: [[55.24651228  0.271676    0.220856    0.138073    0.26311931  7.36720181]][0m
[37m[1m[2023-07-16 22:05:06,826][257371] Max Reward on eval: 55.246512282662174[0m
[37m[1m[2023-07-16 22:05:06,826][257371] Min Reward on eval: 55.246512282662174[0m
[37m[1m[2023-07-16 22:05:06,826][257371] Mean Reward across all agents: 55.246512282662174[0m
[37m[1m[2023-07-16 22:05:06,827][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:05:11,811][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:05:11,811][257371] Reward + Measures: [[ 31.62938553   0.31080002   0.29010001   0.11919999   0.32960001
    7.31230116]
 [280.03377984   0.57740003   0.18120001   0.45609999   0.5776
    7.54728699]
 [ 73.29695889   0.45580003   0.3026       0.28490001   0.4384
    7.28225088]
 ...
 [198.15509678   0.50170004   0.0735       0.53040004   0.40949997
    7.53054428]
 [ 52.19886815   0.2471       0.1592       0.17909999   0.24580002
    7.28546238]
 [192.01395777   0.47159997   0.33570001   0.1991       0.45590001
    7.52008057]][0m
[37m[1m[2023-07-16 22:05:11,811][257371] Max Reward on eval: 368.8776835478842[0m
[37m[1m[2023-07-16 22:05:11,812][257371] Min Reward on eval: -37.093084814958274[0m
[37m[1m[2023-07-16 22:05:11,812][257371] Mean Reward across all agents: 98.9368183236166[0m
[37m[1m[2023-07-16 22:05:11,812][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:05:11,814][257371] mean_value=-113.68838137889323, max_value=225.7306597242714[0m
[37m[1m[2023-07-16 22:05:11,817][257371] New mean coefficients: [[ 3.3651621   0.5967711  -1.934946   -2.1782312   1.5243826  -0.52918714]][0m
[37m[1m[2023-07-16 22:05:11,818][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:05:20,812][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-16 22:05:20,813][257371] FPS: 427002.50[0m
[36m[2023-07-16 22:05:20,815][257371] itr=134, itrs=2000, Progress: 6.70%[0m
[36m[2023-07-16 22:05:32,570][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-16 22:05:32,571][257371] FPS: 327536.04[0m
[36m[2023-07-16 22:05:36,920][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:05:36,926][257371] Reward + Measures: [[61.73743677  0.27961734  0.22808132  0.13790333  0.27006331  7.36095285]][0m
[37m[1m[2023-07-16 22:05:36,926][257371] Max Reward on eval: 61.7374367716741[0m
[37m[1m[2023-07-16 22:05:36,926][257371] Min Reward on eval: 61.7374367716741[0m
[37m[1m[2023-07-16 22:05:36,926][257371] Mean Reward across all agents: 61.7374367716741[0m
[37m[1m[2023-07-16 22:05:36,927][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:05:42,126][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:05:42,131][257371] Reward + Measures: [[ 23.67829059   0.43920001   0.1227       0.48740003   0.391
    7.4941597 ]
 [  4.26084652   0.221        0.1125       0.26290002   0.21070002
    7.34715414]
 [ 86.44655311   0.32360002   0.14929999   0.24399999   0.32909998
    7.48595572]
 ...
 [ -2.02507851   0.23120001   0.19990002   0.13039999   0.22309999
    7.48988962]
 [-45.0276261    0.3066       0.27290002   0.1249       0.2419
    7.43659973]
 [ 29.54766296   0.31669998   0.18260001   0.24349999   0.30800003
    7.38409138]][0m
[37m[1m[2023-07-16 22:05:42,132][257371] Max Reward on eval: 298.2798202062957[0m
[37m[1m[2023-07-16 22:05:42,132][257371] Min Reward on eval: -114.52405533082784[0m
[37m[1m[2023-07-16 22:05:42,132][257371] Mean Reward across all agents: 32.20035535429535[0m
[37m[1m[2023-07-16 22:05:42,132][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:05:42,134][257371] mean_value=-159.03949917350977, max_value=191.3631223978096[0m
[37m[1m[2023-07-16 22:05:42,137][257371] New mean coefficients: [[ 3.1544595   0.03758824 -1.280136   -0.97319376  1.8615849  -0.45554936]][0m
[37m[1m[2023-07-16 22:05:42,138][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:05:51,287][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-16 22:05:51,288][257371] FPS: 419762.12[0m
[36m[2023-07-16 22:05:51,290][257371] itr=135, itrs=2000, Progress: 6.75%[0m
[36m[2023-07-16 22:06:03,094][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-16 22:06:03,094][257371] FPS: 326099.38[0m
[36m[2023-07-16 22:06:07,427][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:06:07,427][257371] Reward + Measures: [[65.34327741  0.28763032  0.24026833  0.13338533  0.27950066  7.35398817]][0m
[37m[1m[2023-07-16 22:06:07,428][257371] Max Reward on eval: 65.34327740697955[0m
[37m[1m[2023-07-16 22:06:07,428][257371] Min Reward on eval: 65.34327740697955[0m
[37m[1m[2023-07-16 22:06:07,428][257371] Mean Reward across all agents: 65.34327740697955[0m
[37m[1m[2023-07-16 22:06:07,428][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:06:12,421][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:06:12,422][257371] Reward + Measures: [[ 47.43243679   0.34830001   0.31380001   0.10750001   0.33660001
    7.35112238]
 [ 14.57861471   0.14530002   0.0959       0.13680001   0.1318
    7.22586155]
 [ 14.73334552   0.18040001   0.12080001   0.1719       0.1718
    7.27079773]
 ...
 [ 54.4602009    0.2228       0.18710001   0.13360001   0.2175
    7.38646698]
 [ -6.22682483   0.26410002   0.2318       0.12149999   0.26369998
    7.35942602]
 [121.67826552   0.26120001   0.19250001   0.1978       0.24730003
    7.34629822]][0m
[37m[1m[2023-07-16 22:06:12,422][257371] Max Reward on eval: 208.87905239835382[0m
[37m[1m[2023-07-16 22:06:12,423][257371] Min Reward on eval: -66.89075065944344[0m
[37m[1m[2023-07-16 22:06:12,423][257371] Mean Reward across all agents: 57.57142436424256[0m
[37m[1m[2023-07-16 22:06:12,423][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:06:12,424][257371] mean_value=-142.21314385955108, max_value=11.677751793409783[0m
[37m[1m[2023-07-16 22:06:12,427][257371] New mean coefficients: [[ 2.6576371  -0.01224313 -1.1547301  -2.1071901   0.7120508  -1.0726998 ]][0m
[37m[1m[2023-07-16 22:06:12,428][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:06:21,507][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 22:06:21,507][257371] FPS: 423034.57[0m
[36m[2023-07-16 22:06:21,509][257371] itr=136, itrs=2000, Progress: 6.80%[0m
[36m[2023-07-16 22:06:33,235][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-16 22:06:33,235][257371] FPS: 328271.22[0m
[36m[2023-07-16 22:06:37,574][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:06:37,575][257371] Reward + Measures: [[65.67199127  0.28809065  0.24012667  0.13221501  0.27955166  7.35267305]][0m
[37m[1m[2023-07-16 22:06:37,575][257371] Max Reward on eval: 65.67199126832426[0m
[37m[1m[2023-07-16 22:06:37,575][257371] Min Reward on eval: 65.67199126832426[0m
[37m[1m[2023-07-16 22:06:37,575][257371] Mean Reward across all agents: 65.67199126832426[0m
[37m[1m[2023-07-16 22:06:37,576][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:06:42,658][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:06:42,659][257371] Reward + Measures: [[ 36.0073582    0.2757       0.17030001   0.19700001   0.28600001
    7.44189835]
 [ 15.86317497   0.26420003   0.1494       0.22089998   0.25659999
    7.34291935]
 [ 51.36106753   0.40499997   0.2076       0.34909999   0.33839998
    7.50566864]
 ...
 [-66.46516823   0.3159       0.27220002   0.1785       0.24330001
    7.41849995]
 [ 48.46402002   0.3603       0.32960001   0.1103       0.37530002
    7.35570526]
 [ 24.01943811   0.27800003   0.2318       0.12710001   0.2431
    7.38597107]][0m
[37m[1m[2023-07-16 22:06:42,659][257371] Max Reward on eval: 269.6618351399899[0m
[37m[1m[2023-07-16 22:06:42,659][257371] Min Reward on eval: -79.3903367260471[0m
[37m[1m[2023-07-16 22:06:42,659][257371] Mean Reward across all agents: 61.373608708430936[0m
[37m[1m[2023-07-16 22:06:42,660][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:06:42,661][257371] mean_value=-147.30501329432218, max_value=-18.530966647912237[0m
[36m[2023-07-16 22:06:42,664][257371] XNES is restarting with a new solution whose measures are [0.26770002 0.32100001 0.21540001 0.27080002 2.11550689] and objective is 731.1818046716974[0m
[36m[2023-07-16 22:06:42,665][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-16 22:06:42,667][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-16 22:06:42,668][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:06:51,689][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 22:06:51,689][257371] FPS: 425711.50[0m
[36m[2023-07-16 22:06:51,692][257371] itr=137, itrs=2000, Progress: 6.85%[0m
[36m[2023-07-16 22:07:03,377][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-16 22:07:03,378][257371] FPS: 329395.80[0m
[36m[2023-07-16 22:07:07,734][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:07:07,740][257371] Reward + Measures: [[812.36656157   0.28344199   0.29638302   0.164437     0.22093999
    1.66986549]][0m
[37m[1m[2023-07-16 22:07:07,740][257371] Max Reward on eval: 812.366561567503[0m
[37m[1m[2023-07-16 22:07:07,740][257371] Min Reward on eval: 812.366561567503[0m
[37m[1m[2023-07-16 22:07:07,741][257371] Mean Reward across all agents: 812.366561567503[0m
[37m[1m[2023-07-16 22:07:07,741][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:07:12,790][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:07:12,795][257371] Reward + Measures: [[680.08745384   0.31720001   0.31360003   0.1628       0.23889999
    1.89632857]
 [864.55474086   0.27729997   0.2888       0.22720002   0.2701
    1.7774092 ]
 [668.82410049   0.19670002   0.25940001   0.1989       0.17760001
    1.86642492]
 ...
 [740.36867146   0.23540001   0.26980004   0.13890001   0.18810001
    1.75509644]
 [753.51778029   0.27620003   0.36139998   0.16229999   0.244
    1.89855349]
 [782.49537563   0.2141       0.2199       0.1585       0.16759999
    1.94356883]][0m
[37m[1m[2023-07-16 22:07:12,796][257371] Max Reward on eval: 1083.8009338511504[0m
[37m[1m[2023-07-16 22:07:12,796][257371] Min Reward on eval: 92.05149574773387[0m
[37m[1m[2023-07-16 22:07:12,796][257371] Mean Reward across all agents: 511.2580461978616[0m
[37m[1m[2023-07-16 22:07:12,796][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:07:12,801][257371] mean_value=-3393.9047213531485, max_value=1310.1335163217968[0m
[37m[1m[2023-07-16 22:07:12,804][257371] New mean coefficients: [[-0.9431255   0.18291134 -0.555886   -0.9337059  -0.88103104 -1.6256573 ]][0m
[37m[1m[2023-07-16 22:07:12,805][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:07:21,881][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 22:07:21,882][257371] FPS: 423154.88[0m
[36m[2023-07-16 22:07:21,884][257371] itr=138, itrs=2000, Progress: 6.90%[0m
[36m[2023-07-16 22:07:33,650][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-16 22:07:33,650][257371] FPS: 327150.86[0m
[36m[2023-07-16 22:07:37,980][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:07:37,981][257371] Reward + Measures: [[474.90144219   0.38223699   0.41060099   0.17802267   0.30115768
    1.13958156]][0m
[37m[1m[2023-07-16 22:07:37,981][257371] Max Reward on eval: 474.9014421931575[0m
[37m[1m[2023-07-16 22:07:37,981][257371] Min Reward on eval: 474.9014421931575[0m
[37m[1m[2023-07-16 22:07:37,982][257371] Mean Reward across all agents: 474.9014421931575[0m
[37m[1m[2023-07-16 22:07:37,982][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:07:42,986][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:07:42,989][257371] Reward + Measures: [[203.8621345    0.2559       0.42560002   0.13620001   0.33469999
    1.36895514]
 [367.39055441   0.1961       0.55870003   0.17080002   0.44679999
    1.40264511]
 [357.07926842   0.3418       0.18359999   0.26760003   0.22490001
    1.81490827]
 ...
 [696.01253702   0.31990001   0.25940001   0.19829999   0.1362
    1.39106524]
 [188.33994521   0.4059       0.2726       0.1935       0.19130002
    1.69574034]
 [544.03918169   0.31         0.3177       0.13510001   0.39340001
    1.27717078]][0m
[37m[1m[2023-07-16 22:07:42,989][257371] Max Reward on eval: 960.8531531893648[0m
[37m[1m[2023-07-16 22:07:42,990][257371] Min Reward on eval: -22.57772907977924[0m
[37m[1m[2023-07-16 22:07:42,990][257371] Mean Reward across all agents: 258.74390727736136[0m
[37m[1m[2023-07-16 22:07:42,990][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:07:43,008][257371] mean_value=-55.09752622257059, max_value=1402.491783109028[0m
[37m[1m[2023-07-16 22:07:43,011][257371] New mean coefficients: [[ 1.1111772   0.76300305  0.15893096 -1.0868938  -1.037428   -1.3767669 ]][0m
[37m[1m[2023-07-16 22:07:43,012][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:07:52,033][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 22:07:52,034][257371] FPS: 425741.73[0m
[36m[2023-07-16 22:07:52,036][257371] itr=139, itrs=2000, Progress: 6.95%[0m
[36m[2023-07-16 22:08:03,759][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-16 22:08:03,759][257371] FPS: 328478.50[0m
[36m[2023-07-16 22:08:08,099][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:08:08,099][257371] Reward + Measures: [[933.40718872   0.42900601   0.38463166   0.16356468   0.20051098
    0.92122328]][0m
[37m[1m[2023-07-16 22:08:08,099][257371] Max Reward on eval: 933.4071887158968[0m
[37m[1m[2023-07-16 22:08:08,100][257371] Min Reward on eval: 933.4071887158968[0m
[37m[1m[2023-07-16 22:08:08,100][257371] Mean Reward across all agents: 933.4071887158968[0m
[37m[1m[2023-07-16 22:08:08,100][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:08:13,195][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:08:13,201][257371] Reward + Measures: [[251.99744988   0.70289999   0.34279999   0.45699999   0.20249999
    0.94021368]
 [325.86921309   0.2369       0.66280001   0.13000001   0.37030002
    1.46592069]
 [356.88695715   0.63450003   0.39660001   0.38090003   0.1212
    1.14180458]
 ...
 [216.53991318   0.69580001   0.21049999   0.4659       0.11830001
    1.41671884]
 [296.28047943   0.2947       0.50819999   0.2739       0.43979999
    0.99410868]
 [128.26487397   0.68740004   0.34619999   0.56280005   0.28459999
    0.92990494]][0m
[37m[1m[2023-07-16 22:08:13,202][257371] Max Reward on eval: 1081.160430887132[0m
[37m[1m[2023-07-16 22:08:13,202][257371] Min Reward on eval: 15.133550400845706[0m
[37m[1m[2023-07-16 22:08:13,202][257371] Mean Reward across all agents: 457.57894391917876[0m
[37m[1m[2023-07-16 22:08:13,203][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:08:13,218][257371] mean_value=372.6409764276749, max_value=1497.8126984081696[0m
[37m[1m[2023-07-16 22:08:13,221][257371] New mean coefficients: [[ 1.3304015   0.75929993  0.8228721  -1.0974904  -1.051714   -1.0092615 ]][0m
[37m[1m[2023-07-16 22:08:13,222][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:08:22,258][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 22:08:22,258][257371] FPS: 425058.90[0m
[36m[2023-07-16 22:08:22,260][257371] itr=140, itrs=2000, Progress: 7.00%[0m
[37m[1m[2023-07-16 22:10:23,519][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000120[0m
[36m[2023-07-16 22:10:35,777][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-16 22:10:35,778][257371] FPS: 328341.31[0m
[36m[2023-07-16 22:10:40,033][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:10:40,033][257371] Reward + Measures: [[1512.46649945    0.42206568    0.37396801    0.14241567    0.15325433
     0.82915527]][0m
[37m[1m[2023-07-16 22:10:40,034][257371] Max Reward on eval: 1512.4664994505886[0m
[37m[1m[2023-07-16 22:10:40,034][257371] Min Reward on eval: 1512.4664994505886[0m
[37m[1m[2023-07-16 22:10:40,034][257371] Mean Reward across all agents: 1512.4664994505886[0m
[37m[1m[2023-07-16 22:10:40,035][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:10:45,067][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:10:45,093][257371] Reward + Measures: [[ 644.41452026    0.50600004    0.55220002    0.30320001    0.18459998
     0.85913545]
 [ 919.53014374    0.29320002    0.60100001    0.1512        0.2422
     0.90091783]
 [ 342.25160219    0.44159999    0.48659998    0.27159998    0.28320003
     1.20279217]
 ...
 [ 625.99292752    0.59850001    0.32839999    0.23410001    0.2251
     1.07962584]
 [1124.29715728    0.33319998    0.41690001    0.19859999    0.17389999
     0.85735363]
 [ 301.27615357    0.25319999    0.54730004    0.20460001    0.57050002
     0.93206781]][0m
[37m[1m[2023-07-16 22:10:45,094][257371] Max Reward on eval: 1680.4512633977458[0m
[37m[1m[2023-07-16 22:10:45,094][257371] Min Reward on eval: 103.71282618464902[0m
[37m[1m[2023-07-16 22:10:45,094][257371] Mean Reward across all agents: 801.1769759506396[0m
[37m[1m[2023-07-16 22:10:45,094][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:10:45,107][257371] mean_value=294.4831304749087, max_value=1604.4589767626487[0m
[37m[1m[2023-07-16 22:10:45,110][257371] New mean coefficients: [[ 1.6075202  -0.30283183  0.7928652  -1.2628953  -0.2802055   0.12465072]][0m
[37m[1m[2023-07-16 22:10:45,111][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:10:54,008][257371] train() took 8.90 seconds to complete[0m
[36m[2023-07-16 22:10:54,009][257371] FPS: 431652.10[0m
[36m[2023-07-16 22:10:54,011][257371] itr=141, itrs=2000, Progress: 7.05%[0m
[36m[2023-07-16 22:11:05,858][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-16 22:11:05,859][257371] FPS: 324969.73[0m
[36m[2023-07-16 22:11:10,184][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:11:10,185][257371] Reward + Measures: [[1978.56367107    0.36027297    0.33756697    0.13628067    0.14548133
     0.8718369 ]][0m
[37m[1m[2023-07-16 22:11:10,185][257371] Max Reward on eval: 1978.5636710691304[0m
[37m[1m[2023-07-16 22:11:10,185][257371] Min Reward on eval: 1978.5636710691304[0m
[37m[1m[2023-07-16 22:11:10,185][257371] Mean Reward across all agents: 1978.5636710691304[0m
[37m[1m[2023-07-16 22:11:10,186][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:11:15,439][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:11:15,440][257371] Reward + Measures: [[1596.01866909    0.46140003    0.29440001    0.14780001    0.17639999
     0.87850946]
 [1067.07657622    0.54159999    0.31240001    0.19860001    0.19240001
     0.90264094]
 [1609.71659086    0.373         0.28569999    0.18440001    0.18670002
     1.03370798]
 ...
 [1458.78208541    0.287         0.37489998    0.14560001    0.16940001
     0.87129253]
 [2065.41943354    0.38930002    0.28990003    0.16110002    0.16730002
     1.05182099]
 [ 424.07378578    0.3802        0.41779995    0.3256        0.26410002
     0.96615869]][0m
[37m[1m[2023-07-16 22:11:15,440][257371] Max Reward on eval: 2150.801071163494[0m
[37m[1m[2023-07-16 22:11:15,440][257371] Min Reward on eval: 245.53276061419456[0m
[37m[1m[2023-07-16 22:11:15,441][257371] Mean Reward across all agents: 1169.1005255101118[0m
[37m[1m[2023-07-16 22:11:15,441][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:11:15,449][257371] mean_value=211.31044859851716, max_value=1138.729857285245[0m
[37m[1m[2023-07-16 22:11:15,452][257371] New mean coefficients: [[ 1.3511468  -0.57588726  0.6976762  -0.67274636 -0.12667769  1.6433741 ]][0m
[37m[1m[2023-07-16 22:11:15,453][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:11:24,573][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-16 22:11:24,573][257371] FPS: 421149.30[0m
[36m[2023-07-16 22:11:24,575][257371] itr=142, itrs=2000, Progress: 7.10%[0m
[36m[2023-07-16 22:11:36,298][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-16 22:11:36,298][257371] FPS: 328411.12[0m
[36m[2023-07-16 22:11:40,572][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:11:40,573][257371] Reward + Measures: [[2311.83673621    0.33123234    0.31401366    0.13234834    0.14528799
     0.98383129]][0m
[37m[1m[2023-07-16 22:11:40,573][257371] Max Reward on eval: 2311.836736206363[0m
[37m[1m[2023-07-16 22:11:40,573][257371] Min Reward on eval: 2311.836736206363[0m
[37m[1m[2023-07-16 22:11:40,573][257371] Mean Reward across all agents: 2311.836736206363[0m
[37m[1m[2023-07-16 22:11:40,574][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:11:45,629][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:11:45,630][257371] Reward + Measures: [[1479.67433171    0.31370002    0.41640005    0.18710001    0.21689999
     1.07656169]
 [1721.21730041    0.43080002    0.40969998    0.18209998    0.1657
     1.0319519 ]
 [1104.72583771    0.39019999    0.29010001    0.1513        0.23740001
     1.35256064]
 ...
 [1527.8067169     0.43880001    0.32520002    0.1918        0.2141
     1.0358485 ]
 [1467.49218751    0.34910002    0.40150005    0.149         0.20850001
     1.31556809]
 [ 996.82416914    0.30230004    0.38890001    0.14850001    0.24590002
     1.47306001]][0m
[37m[1m[2023-07-16 22:11:45,630][257371] Max Reward on eval: 2550.4963378675284[0m
[37m[1m[2023-07-16 22:11:45,630][257371] Min Reward on eval: 420.2860708080232[0m
[37m[1m[2023-07-16 22:11:45,631][257371] Mean Reward across all agents: 1621.899817881989[0m
[37m[1m[2023-07-16 22:11:45,631][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:11:45,637][257371] mean_value=280.7162907331885, max_value=1321.028749608967[0m
[37m[1m[2023-07-16 22:11:45,639][257371] New mean coefficients: [[ 1.6108866   0.08816826  0.82011855 -0.31914574  0.24180558  2.7291143 ]][0m
[37m[1m[2023-07-16 22:11:45,640][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:11:54,590][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-16 22:11:54,590][257371] FPS: 429155.81[0m
[36m[2023-07-16 22:11:54,592][257371] itr=143, itrs=2000, Progress: 7.15%[0m
[36m[2023-07-16 22:12:06,346][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-16 22:12:06,346][257371] FPS: 327535.31[0m
[36m[2023-07-16 22:12:10,656][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:12:10,661][257371] Reward + Measures: [[2648.82137684    0.3272143     0.31405699    0.136758      0.14962865
     1.11672354]][0m
[37m[1m[2023-07-16 22:12:10,662][257371] Max Reward on eval: 2648.8213768376645[0m
[37m[1m[2023-07-16 22:12:10,663][257371] Min Reward on eval: 2648.8213768376645[0m
[37m[1m[2023-07-16 22:12:10,663][257371] Mean Reward across all agents: 2648.8213768376645[0m
[37m[1m[2023-07-16 22:12:10,664][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:12:15,698][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:12:15,704][257371] Reward + Measures: [[1628.47737122    0.30280003    0.48520002    0.18279999    0.1655
     1.23861647]
 [2263.78269957    0.30589995    0.29910001    0.11610001    0.16410001
     1.20271981]
 [1238.96039959    0.2617        0.40310001    0.2095        0.15799999
     1.1666559 ]
 ...
 [1604.76603699    0.33910003    0.35619998    0.15889999    0.18870001
     1.35583746]
 [ 832.66214752    0.31100002    0.4382        0.2773        0.24389999
     1.17242336]
 [1925.88356399    0.33129999    0.29449999    0.16720001    0.20369999
     1.4507097 ]][0m
[37m[1m[2023-07-16 22:12:15,705][257371] Max Reward on eval: 2777.3206024777087[0m
[37m[1m[2023-07-16 22:12:15,705][257371] Min Reward on eval: 832.6621475244872[0m
[37m[1m[2023-07-16 22:12:15,705][257371] Mean Reward across all agents: 1760.6795566026153[0m
[37m[1m[2023-07-16 22:12:15,705][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:12:15,710][257371] mean_value=-189.22070388379643, max_value=1295.69521564364[0m
[37m[1m[2023-07-16 22:12:15,713][257371] New mean coefficients: [[ 0.5720829   0.29513568  0.3509168  -0.18799831 -0.00188078  2.2583406 ]][0m
[37m[1m[2023-07-16 22:12:15,714][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:12:24,715][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-16 22:12:24,716][257371] FPS: 426668.79[0m
[36m[2023-07-16 22:12:24,718][257371] itr=144, itrs=2000, Progress: 7.20%[0m
[36m[2023-07-16 22:12:36,382][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-16 22:12:36,382][257371] FPS: 330061.95[0m
[36m[2023-07-16 22:12:40,674][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:12:40,675][257371] Reward + Measures: [[2915.22139467    0.32589397    0.30619067    0.14151634    0.15575267
     1.32151997]][0m
[37m[1m[2023-07-16 22:12:40,675][257371] Max Reward on eval: 2915.2213946749007[0m
[37m[1m[2023-07-16 22:12:40,675][257371] Min Reward on eval: 2915.2213946749007[0m
[37m[1m[2023-07-16 22:12:40,676][257371] Mean Reward across all agents: 2915.2213946749007[0m
[37m[1m[2023-07-16 22:12:40,676][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:12:45,704][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:12:45,705][257371] Reward + Measures: [[2164.16429904    0.2622        0.3206        0.13740002    0.1794
     1.48094749]
 [2236.70808408    0.37380001    0.3373        0.1452        0.17340001
     1.25980663]
 [1937.22123147    0.25729999    0.24029998    0.1248        0.19560002
     1.44630587]
 ...
 [2760.77633664    0.36739999    0.35040003    0.13850001    0.16419999
     1.29051197]
 [1910.04324339    0.26980001    0.24829999    0.1278        0.20640002
     1.40019083]
 [1174.9126587     0.33930001    0.3114        0.2237        0.26910004
     1.67190588]][0m
[37m[1m[2023-07-16 22:12:45,705][257371] Max Reward on eval: 2961.647064222582[0m
[37m[1m[2023-07-16 22:12:45,705][257371] Min Reward on eval: 540.2313843000214[0m
[37m[1m[2023-07-16 22:12:45,706][257371] Mean Reward across all agents: 1920.5050725166368[0m
[37m[1m[2023-07-16 22:12:45,706][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:12:45,710][257371] mean_value=-377.72484929556344, max_value=1678.3554829285304[0m
[37m[1m[2023-07-16 22:12:45,713][257371] New mean coefficients: [[ 0.6235249  -0.05170313  1.0295527  -0.27811468 -0.22014907  0.29900014]][0m
[37m[1m[2023-07-16 22:12:45,714][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:12:54,791][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 22:12:54,792][257371] FPS: 423127.98[0m
[36m[2023-07-16 22:12:54,794][257371] itr=145, itrs=2000, Progress: 7.25%[0m
[36m[2023-07-16 22:13:06,392][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-16 22:13:06,393][257371] FPS: 332025.24[0m
[36m[2023-07-16 22:13:10,682][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:13:10,683][257371] Reward + Measures: [[3346.18415602    0.32957065    0.33888233    0.14027566    0.14579934
     1.35458112]][0m
[37m[1m[2023-07-16 22:13:10,683][257371] Max Reward on eval: 3346.1841560237126[0m
[37m[1m[2023-07-16 22:13:10,683][257371] Min Reward on eval: 3346.1841560237126[0m
[37m[1m[2023-07-16 22:13:10,683][257371] Mean Reward across all agents: 3346.1841560237126[0m
[37m[1m[2023-07-16 22:13:10,684][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:13:15,850][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:13:15,851][257371] Reward + Measures: [[2885.37030031    0.32810003    0.34410003    0.15210001    0.1727
     1.50930214]
 [3056.93931579    0.29480001    0.3141        0.14130001    0.1472
     1.3356179 ]
 [2472.545742      0.32300001    0.32710001    0.14030002    0.1865
     1.43258262]
 ...
 [2254.93289186    0.3021        0.3461        0.16960001    0.16729999
     1.38980472]
 [1947.17893221    0.28560001    0.34120002    0.17379999    0.23190001
     1.33361506]
 [1389.87627793    0.31430003    0.28389999    0.13859999    0.20079999
     1.5155102 ]][0m
[37m[1m[2023-07-16 22:13:15,851][257371] Max Reward on eval: 3419.855957043264[0m
[37m[1m[2023-07-16 22:13:15,851][257371] Min Reward on eval: 1075.6736316177062[0m
[37m[1m[2023-07-16 22:13:15,852][257371] Mean Reward across all agents: 2389.4523819819024[0m
[37m[1m[2023-07-16 22:13:15,852][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:13:15,855][257371] mean_value=-146.61546731778796, max_value=1401.8454648819597[0m
[37m[1m[2023-07-16 22:13:15,858][257371] New mean coefficients: [[ 0.41001067 -0.12885767  0.41850662 -0.13140506 -0.03730403  0.00176924]][0m
[37m[1m[2023-07-16 22:13:15,859][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:13:24,886][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 22:13:24,887][257371] FPS: 425435.53[0m
[36m[2023-07-16 22:13:24,889][257371] itr=146, itrs=2000, Progress: 7.30%[0m
[36m[2023-07-16 22:13:36,651][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-16 22:13:36,651][257371] FPS: 327291.95[0m
[36m[2023-07-16 22:13:40,980][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:13:40,980][257371] Reward + Measures: [[3748.8128827     0.320171      0.34509236    0.14227       0.14367667
     1.37440717]][0m
[37m[1m[2023-07-16 22:13:40,981][257371] Max Reward on eval: 3748.8128827020623[0m
[37m[1m[2023-07-16 22:13:40,981][257371] Min Reward on eval: 3748.8128827020623[0m
[37m[1m[2023-07-16 22:13:40,981][257371] Mean Reward across all agents: 3748.8128827020623[0m
[37m[1m[2023-07-16 22:13:40,981][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:13:46,012][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:13:46,012][257371] Reward + Measures: [[2050.42095567    0.2536        0.2455        0.15820001    0.155
     1.54286265]
 [1787.17054366    0.2784        0.23440002    0.16150001    0.18050002
     1.49140036]
 [2182.12620354    0.26900002    0.37360001    0.1409        0.1522
     1.36743867]
 ...
 [1679.40309141    0.34639999    0.36129999    0.16430001    0.2494
     1.27431226]
 [2397.12034609    0.33970001    0.3475        0.1592        0.1595
     1.48901355]
 [1810.49132919    0.24419999    0.28169999    0.13249999    0.15180001
     1.29480112]][0m
[37m[1m[2023-07-16 22:13:46,012][257371] Max Reward on eval: 3873.966003457853[0m
[37m[1m[2023-07-16 22:13:46,013][257371] Min Reward on eval: 965.8855132987752[0m
[37m[1m[2023-07-16 22:13:46,013][257371] Mean Reward across all agents: 2628.4056047853514[0m
[37m[1m[2023-07-16 22:13:46,013][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:13:46,017][257371] mean_value=-23.262712414597434, max_value=2499.4351645746365[0m
[37m[1m[2023-07-16 22:13:46,020][257371] New mean coefficients: [[ 0.44017     0.55234575  0.9544859  -0.09860451 -0.75423265  0.20958108]][0m
[37m[1m[2023-07-16 22:13:46,021][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:13:55,062][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 22:13:55,062][257371] FPS: 424800.02[0m
[36m[2023-07-16 22:13:55,065][257371] itr=147, itrs=2000, Progress: 7.35%[0m
[36m[2023-07-16 22:14:06,728][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-16 22:14:06,728][257371] FPS: 330191.22[0m
[36m[2023-07-16 22:14:11,003][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:14:11,004][257371] Reward + Measures: [[4110.34966694    0.32044935    0.3587037     0.14182666    0.13621233
     1.40169501]][0m
[37m[1m[2023-07-16 22:14:11,004][257371] Max Reward on eval: 4110.349666940776[0m
[37m[1m[2023-07-16 22:14:11,004][257371] Min Reward on eval: 4110.349666940776[0m
[37m[1m[2023-07-16 22:14:11,005][257371] Mean Reward across all agents: 4110.349666940776[0m
[37m[1m[2023-07-16 22:14:11,005][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:14:16,021][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:14:16,021][257371] Reward + Measures: [[3716.20776363    0.32260001    0.36500001    0.1513        0.1529
     1.36841941]
 [3414.95535277    0.3274        0.3612        0.18780001    0.1497
     1.42226756]
 [3138.20773316    0.3355        0.33510002    0.14860001    0.1517
     1.54846323]
 ...
 [2650.48823928    0.2886        0.2983        0.1567        0.15089999
     1.59204769]
 [3199.41268921    0.34610003    0.37919998    0.1867        0.154
     1.45799565]
 [2287.71233747    0.32100001    0.34829998    0.16159999    0.16550002
     1.49218011]][0m
[37m[1m[2023-07-16 22:14:16,021][257371] Max Reward on eval: 4121.443115275595[0m
[37m[1m[2023-07-16 22:14:16,022][257371] Min Reward on eval: 1274.3057251155842[0m
[37m[1m[2023-07-16 22:14:16,022][257371] Mean Reward across all agents: 2980.1696481657127[0m
[37m[1m[2023-07-16 22:14:16,022][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:14:16,026][257371] mean_value=-131.57457324647763, max_value=2483.2440981600266[0m
[37m[1m[2023-07-16 22:14:16,028][257371] New mean coefficients: [[ 0.28575104  0.36229616  1.3476148  -0.09518296  0.00328797  0.9214452 ]][0m
[37m[1m[2023-07-16 22:14:16,030][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:14:25,069][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 22:14:25,069][257371] FPS: 424878.25[0m
[36m[2023-07-16 22:14:25,072][257371] itr=148, itrs=2000, Progress: 7.40%[0m
[36m[2023-07-16 22:14:36,787][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-16 22:14:36,788][257371] FPS: 328712.81[0m
[36m[2023-07-16 22:14:41,126][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:14:41,126][257371] Reward + Measures: [[4317.68466496    0.30910534    0.38188833    0.147229      0.14133433
     1.55482578]][0m
[37m[1m[2023-07-16 22:14:41,126][257371] Max Reward on eval: 4317.684664956324[0m
[37m[1m[2023-07-16 22:14:41,127][257371] Min Reward on eval: 4317.684664956324[0m
[37m[1m[2023-07-16 22:14:41,127][257371] Mean Reward across all agents: 4317.684664956324[0m
[37m[1m[2023-07-16 22:14:41,127][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:14:46,198][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:14:46,198][257371] Reward + Measures: [[3555.35345459    0.35530001    0.34029999    0.14549999    0.14670001
     1.72446954]
 [3026.64999383    0.29949999    0.3946        0.18880002    0.15350001
     1.4153657 ]
 [3083.71920773    0.26449999    0.39229998    0.15969999    0.15279999
     1.40297759]
 ...
 [2743.1984005     0.28150001    0.35520002    0.1481        0.1698
     1.57672024]
 [4267.3177185     0.32659999    0.40149999    0.14650001    0.14659999
     1.54703104]
 [2821.70936584    0.30330002    0.33270001    0.1742        0.15009999
     1.48775947]][0m
[37m[1m[2023-07-16 22:14:46,199][257371] Max Reward on eval: 4267.31771849723[0m
[37m[1m[2023-07-16 22:14:46,199][257371] Min Reward on eval: 1332.9530315167735[0m
[37m[1m[2023-07-16 22:14:46,199][257371] Mean Reward across all agents: 3102.6165773540256[0m
[37m[1m[2023-07-16 22:14:46,199][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:14:46,204][257371] mean_value=92.2948701701225, max_value=2932.297689749944[0m
[37m[1m[2023-07-16 22:14:46,207][257371] New mean coefficients: [[ 0.44903708  1.4847827   1.0815732  -0.7982282   0.04836563  1.9190817 ]][0m
[37m[1m[2023-07-16 22:14:46,208][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:14:55,285][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 22:14:55,291][257371] FPS: 423093.17[0m
[36m[2023-07-16 22:14:55,293][257371] itr=149, itrs=2000, Progress: 7.45%[0m
[36m[2023-07-16 22:15:07,050][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-16 22:15:07,051][257371] FPS: 327446.05[0m
[36m[2023-07-16 22:15:11,356][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:15:11,357][257371] Reward + Measures: [[4476.35191506    0.30945301    0.37805298    0.14989766    0.146282
     1.75698268]][0m
[37m[1m[2023-07-16 22:15:11,357][257371] Max Reward on eval: 4476.351915058385[0m
[37m[1m[2023-07-16 22:15:11,357][257371] Min Reward on eval: 4476.351915058385[0m
[37m[1m[2023-07-16 22:15:11,357][257371] Mean Reward across all agents: 4476.351915058385[0m
[37m[1m[2023-07-16 22:15:11,358][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:15:16,414][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:15:16,420][257371] Reward + Measures: [[3173.43417361    0.32520005    0.29899999    0.1736        0.1736
     1.62793279]
 [3916.0213928     0.26809999    0.3427        0.14860001    0.13600002
     1.60961473]
 [2187.842659      0.21429999    0.2388        0.15620001    0.1399
     1.5302676 ]
 ...
 [3629.80934901    0.30200002    0.30429998    0.1499        0.1331
     1.53726506]
 [3201.40336607    0.30739999    0.32119998    0.17040001    0.16150001
     1.68103695]
 [2103.41172027    0.34520003    0.40170002    0.1733        0.1839
     1.57986999]][0m
[37m[1m[2023-07-16 22:15:16,420][257371] Max Reward on eval: 4423.035522440448[0m
[37m[1m[2023-07-16 22:15:16,420][257371] Min Reward on eval: 957.3089599730913[0m
[37m[1m[2023-07-16 22:15:16,421][257371] Mean Reward across all agents: 3018.4505894002014[0m
[37m[1m[2023-07-16 22:15:16,421][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:15:16,424][257371] mean_value=-1015.4815985472013, max_value=2347.284783135704[0m
[37m[1m[2023-07-16 22:15:16,426][257371] New mean coefficients: [[ 0.18741873  0.87631756  0.8867408  -1.0111911  -0.03446614  1.4794339 ]][0m
[37m[1m[2023-07-16 22:15:16,428][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:15:25,451][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 22:15:25,451][257371] FPS: 425643.51[0m
[36m[2023-07-16 22:15:25,454][257371] itr=150, itrs=2000, Progress: 7.50%[0m
[37m[1m[2023-07-16 22:17:28,209][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000130[0m
[36m[2023-07-16 22:17:40,453][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-16 22:17:40,453][257371] FPS: 323688.68[0m
[36m[2023-07-16 22:17:44,743][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:17:44,743][257371] Reward + Measures: [[4595.16850937    0.31097099    0.38592565    0.15376434    0.15196399
     2.00181556]][0m
[37m[1m[2023-07-16 22:17:44,744][257371] Max Reward on eval: 4595.168509368526[0m
[37m[1m[2023-07-16 22:17:44,744][257371] Min Reward on eval: 4595.168509368526[0m
[37m[1m[2023-07-16 22:17:44,744][257371] Mean Reward across all agents: 4595.168509368526[0m
[37m[1m[2023-07-16 22:17:44,744][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:17:49,783][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:17:49,884][257371] Reward + Measures: [[3889.1338806     0.28990003    0.4113        0.1576        0.169
     1.92315423]
 [4178.22747799    0.29539999    0.39900002    0.147         0.1451
     1.80562389]
 [3942.77267454    0.28400001    0.38250002    0.1437        0.1436
     1.79381943]
 ...
 [1760.69150068    0.18310001    0.2561        0.15320002    0.15650001
     1.8009752 ]
 [2877.16039659    0.27760002    0.3581        0.17990001    0.18569998
     1.8654387 ]
 [2038.4149857     0.21350002    0.2703        0.16500001    0.17469999
     1.86921561]][0m
[37m[1m[2023-07-16 22:17:49,884][257371] Max Reward on eval: 4610.966461166087[0m
[37m[1m[2023-07-16 22:17:49,885][257371] Min Reward on eval: 1071.632417678804[0m
[37m[1m[2023-07-16 22:17:49,885][257371] Mean Reward across all agents: 3292.4488958298057[0m
[37m[1m[2023-07-16 22:17:49,885][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:17:49,889][257371] mean_value=-2198.9267267269056, max_value=4628.180114765419[0m
[37m[1m[2023-07-16 22:17:49,894][257371] New mean coefficients: [[-0.43928918 -0.12718564  1.8273594  -0.6293339  -0.31560346  0.9032752 ]][0m
[37m[1m[2023-07-16 22:17:49,896][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:17:58,796][257371] train() took 8.90 seconds to complete[0m
[36m[2023-07-16 22:17:58,796][257371] FPS: 431599.19[0m
[36m[2023-07-16 22:17:58,799][257371] itr=151, itrs=2000, Progress: 7.55%[0m
[36m[2023-07-16 22:18:10,643][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-16 22:18:10,643][257371] FPS: 325126.65[0m
[36m[2023-07-16 22:18:14,884][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:18:14,885][257371] Reward + Measures: [[4081.00468948    0.30647498    0.42281601    0.16155434    0.16075501
     2.12043905]][0m
[37m[1m[2023-07-16 22:18:14,885][257371] Max Reward on eval: 4081.0046894756965[0m
[37m[1m[2023-07-16 22:18:14,885][257371] Min Reward on eval: 4081.0046894756965[0m
[37m[1m[2023-07-16 22:18:14,886][257371] Mean Reward across all agents: 4081.0046894756965[0m
[37m[1m[2023-07-16 22:18:14,886][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:18:19,851][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:18:19,852][257371] Reward + Measures: [[3052.46966555    0.24860001    0.36819997    0.17380001    0.1839
     1.9397763 ]
 [3568.97477724    0.32290003    0.40499997    0.1442        0.1504
     1.8975718 ]
 [3587.17251584    0.28690001    0.4066        0.1631        0.17840001
     2.02973342]
 ...
 [3440.99322508    0.28740001    0.43090001    0.17019999    0.16779999
     1.91549206]
 [2803.1252213     0.21510001    0.37630001    0.1628        0.15699999
     1.77890778]
 [4012.38272098    0.31189999    0.41990003    0.15719999    0.1639
     2.13675165]][0m
[37m[1m[2023-07-16 22:18:19,852][257371] Max Reward on eval: 4251.265380911098[0m
[37m[1m[2023-07-16 22:18:19,852][257371] Min Reward on eval: 2050.914903670922[0m
[37m[1m[2023-07-16 22:18:19,852][257371] Mean Reward across all agents: 3402.684941513643[0m
[37m[1m[2023-07-16 22:18:19,853][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:18:19,855][257371] mean_value=-1308.1862101610761, max_value=3876.628701634641[0m
[37m[1m[2023-07-16 22:18:19,857][257371] New mean coefficients: [[ 0.11268684  0.25231117  1.0074908  -0.76182127  0.03044671  1.3707528 ]][0m
[37m[1m[2023-07-16 22:18:19,858][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:18:28,875][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 22:18:28,875][257371] FPS: 425944.74[0m
[36m[2023-07-16 22:18:28,877][257371] itr=152, itrs=2000, Progress: 7.60%[0m
[36m[2023-07-16 22:18:40,499][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-16 22:18:40,499][257371] FPS: 331404.75[0m
[36m[2023-07-16 22:18:44,880][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:18:44,881][257371] Reward + Measures: [[4206.46861727    0.29226333    0.43045703    0.16266499    0.16241167
     2.3269887 ]][0m
[37m[1m[2023-07-16 22:18:44,881][257371] Max Reward on eval: 4206.468617272784[0m
[37m[1m[2023-07-16 22:18:44,881][257371] Min Reward on eval: 4206.468617272784[0m
[37m[1m[2023-07-16 22:18:44,882][257371] Mean Reward across all agents: 4206.468617272784[0m
[37m[1m[2023-07-16 22:18:44,882][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:18:49,949][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:18:49,950][257371] Reward + Measures: [[2541.29727935    0.24609998    0.31560001    0.1531        0.19679999
     1.97949016]
 [3492.52612113    0.26230001    0.37069997    0.15270001    0.1646
     2.17697382]
 [3760.51078794    0.2536        0.36400002    0.1498        0.1515
     2.18777418]
 ...
 [4076.02987669    0.2965        0.41160002    0.1592        0.15809999
     2.32558727]
 [3836.92175287    0.33329996    0.4269        0.15629999    0.1593
     2.35921359]
 [2848.6736717     0.25780001    0.33670002    0.15809999    0.1468
     2.01310515]][0m
[37m[1m[2023-07-16 22:18:49,950][257371] Max Reward on eval: 4390.637908930669[0m
[37m[1m[2023-07-16 22:18:49,950][257371] Min Reward on eval: 943.7604465240613[0m
[37m[1m[2023-07-16 22:18:49,951][257371] Mean Reward across all agents: 3528.1925759801766[0m
[37m[1m[2023-07-16 22:18:49,951][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:18:49,952][257371] mean_value=-2154.1088604601987, max_value=360.0964298670847[0m
[37m[1m[2023-07-16 22:18:49,955][257371] New mean coefficients: [[ 0.48258552  0.46177393  1.425144   -0.59958416 -0.74555564 -0.2212491 ]][0m
[37m[1m[2023-07-16 22:18:49,955][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:18:59,036][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 22:18:59,036][257371] FPS: 422967.40[0m
[36m[2023-07-16 22:18:59,039][257371] itr=153, itrs=2000, Progress: 7.65%[0m
[36m[2023-07-16 22:19:10,762][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-16 22:19:10,763][257371] FPS: 328381.91[0m
[36m[2023-07-16 22:19:15,146][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:19:15,146][257371] Reward + Measures: [[4824.27646737    0.28705367    0.41850597    0.15825599    0.15596633
     2.24948573]][0m
[37m[1m[2023-07-16 22:19:15,146][257371] Max Reward on eval: 4824.276467374082[0m
[37m[1m[2023-07-16 22:19:15,147][257371] Min Reward on eval: 4824.276467374082[0m
[37m[1m[2023-07-16 22:19:15,147][257371] Mean Reward across all agents: 4824.276467374082[0m
[37m[1m[2023-07-16 22:19:15,147][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:19:20,375][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:19:20,375][257371] Reward + Measures: [[4239.63067627    0.28280002    0.45469999    0.15910001    0.15720001
     2.12009192]
 [4609.73269656    0.30599999    0.38390002    0.1644        0.1627
     2.12921071]
 [2488.82464599    0.29500005    0.33570001    0.19360001    0.1701
     2.01109099]
 ...
 [4336.31866457    0.30900002    0.396         0.1679        0.16620001
     2.12199569]
 [2883.89497374    0.3143        0.37369999    0.1771        0.18099999
     2.02921963]
 [3261.07257078    0.32070002    0.37359998    0.1548        0.16850001
     2.04931688]][0m
[37m[1m[2023-07-16 22:19:20,376][257371] Max Reward on eval: 4955.913787855825[0m
[37m[1m[2023-07-16 22:19:20,376][257371] Min Reward on eval: 2055.088497178629[0m
[37m[1m[2023-07-16 22:19:20,376][257371] Mean Reward across all agents: 3758.5534246495426[0m
[37m[1m[2023-07-16 22:19:20,376][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:19:20,379][257371] mean_value=-1672.3601918057655, max_value=733.0344586606393[0m
[37m[1m[2023-07-16 22:19:20,381][257371] New mean coefficients: [[-0.16210166  0.30413586  1.3667078  -0.00365686 -0.4471761  -0.96369493]][0m
[37m[1m[2023-07-16 22:19:20,382][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:19:29,464][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 22:19:29,464][257371] FPS: 422895.28[0m
[36m[2023-07-16 22:19:29,467][257371] itr=154, itrs=2000, Progress: 7.70%[0m
[36m[2023-07-16 22:19:41,584][257371] train() took 12.09 seconds to complete[0m
[36m[2023-07-16 22:19:41,584][257371] FPS: 317734.22[0m
[36m[2023-07-16 22:19:45,908][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:19:45,908][257371] Reward + Measures: [[4821.93251823    0.29137298    0.43903235    0.157864      0.15361933
     2.09134245]][0m
[37m[1m[2023-07-16 22:19:45,909][257371] Max Reward on eval: 4821.932518227756[0m
[37m[1m[2023-07-16 22:19:45,909][257371] Min Reward on eval: 4821.932518227756[0m
[37m[1m[2023-07-16 22:19:45,909][257371] Mean Reward across all agents: 4821.932518227756[0m
[37m[1m[2023-07-16 22:19:45,909][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:19:50,984][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:19:50,985][257371] Reward + Measures: [[4173.39379886    0.28749999    0.44060001    0.14940001    0.14650001
     1.86416662]
 [4246.64965822    0.29449999    0.4413        0.1639        0.1663
     1.99497151]
 [4577.5930786     0.30850002    0.41140005    0.15400001    0.15320002
     2.00358057]
 ...
 [4233.42776485    0.30620003    0.44310004    0.15900001    0.1517
     1.94871807]
 [2606.50740049    0.3594        0.39200002    0.1935        0.19330001
     1.71165466]
 [3313.15835571    0.28          0.38049999    0.17309999    0.19670001
     1.92034817]][0m
[37m[1m[2023-07-16 22:19:50,985][257371] Max Reward on eval: 4827.867675749027[0m
[37m[1m[2023-07-16 22:19:50,985][257371] Min Reward on eval: 1957.3376159254462[0m
[37m[1m[2023-07-16 22:19:50,985][257371] Mean Reward across all agents: 3799.72281190933[0m
[37m[1m[2023-07-16 22:19:50,986][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:19:50,988][257371] mean_value=-1429.9971291171933, max_value=747.4144025376854[0m
[37m[1m[2023-07-16 22:19:50,990][257371] New mean coefficients: [[ 0.4761708   0.5353531   0.85292834  0.2270634   0.11629623 -0.92265517]][0m
[37m[1m[2023-07-16 22:19:50,991][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:20:00,107][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-16 22:20:00,107][257371] FPS: 421334.71[0m
[36m[2023-07-16 22:20:00,110][257371] itr=155, itrs=2000, Progress: 7.75%[0m
[36m[2023-07-16 22:20:12,038][257371] train() took 11.90 seconds to complete[0m
[36m[2023-07-16 22:20:12,039][257371] FPS: 322722.56[0m
[36m[2023-07-16 22:20:16,344][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:20:16,345][257371] Reward + Measures: [[4959.24192673    0.28009033    0.41822535    0.15134433    0.14420934
     1.85860443]][0m
[37m[1m[2023-07-16 22:20:16,345][257371] Max Reward on eval: 4959.241926727304[0m
[37m[1m[2023-07-16 22:20:16,345][257371] Min Reward on eval: 4959.241926727304[0m
[37m[1m[2023-07-16 22:20:16,345][257371] Mean Reward across all agents: 4959.241926727304[0m
[37m[1m[2023-07-16 22:20:16,346][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:20:21,413][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:20:21,419][257371] Reward + Measures: [[4369.43185426    0.31210002    0.44749999    0.1628        0.15899999
     1.87421262]
 [4538.33065795    0.28150001    0.43889999    0.1543        0.13880001
     1.87780464]
 [4250.5098572     0.30749997    0.41060001    0.1416        0.13250001
     1.82552969]
 ...
 [4364.08392335    0.32070002    0.44860002    0.14490001    0.14070001
     1.88812375]
 [3557.17213436    0.30939999    0.43209997    0.15150002    0.15190001
     1.87800181]
 [4468.74316406    0.28770003    0.43650004    0.1688        0.16790001
     1.90985513]][0m
[37m[1m[2023-07-16 22:20:21,419][257371] Max Reward on eval: 5039.791961647768[0m
[37m[1m[2023-07-16 22:20:21,420][257371] Min Reward on eval: 2159.744873028365[0m
[37m[1m[2023-07-16 22:20:21,420][257371] Mean Reward across all agents: 4061.897030143246[0m
[37m[1m[2023-07-16 22:20:21,420][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:20:21,423][257371] mean_value=-745.1072216495843, max_value=876.8151827785064[0m
[37m[1m[2023-07-16 22:20:21,426][257371] New mean coefficients: [[ 0.10742772  0.18168187  0.44966525  0.19027734 -0.17112744 -1.3262948 ]][0m
[37m[1m[2023-07-16 22:20:21,427][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:20:30,502][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 22:20:30,502][257371] FPS: 423202.38[0m
[36m[2023-07-16 22:20:30,505][257371] itr=156, itrs=2000, Progress: 7.80%[0m
[36m[2023-07-16 22:20:42,263][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-16 22:20:42,263][257371] FPS: 327506.25[0m
[36m[2023-07-16 22:20:46,600][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:20:46,601][257371] Reward + Measures: [[4856.29195844    0.277679      0.41726997    0.14855833    0.13839634
     1.57362711]][0m
[37m[1m[2023-07-16 22:20:46,601][257371] Max Reward on eval: 4856.291958440837[0m
[37m[1m[2023-07-16 22:20:46,601][257371] Min Reward on eval: 4856.291958440837[0m
[37m[1m[2023-07-16 22:20:46,602][257371] Mean Reward across all agents: 4856.291958440837[0m
[37m[1m[2023-07-16 22:20:46,602][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:20:51,673][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:20:51,674][257371] Reward + Measures: [[4635.77041626    0.31609997    0.41019997    0.15460001    0.15540001
     1.84013903]
 [3847.81696321    0.26570001    0.39770001    0.163         0.13950001
     1.3934567 ]
 [3972.24063111    0.28940001    0.40009999    0.1794        0.1605
     1.58931172]
 ...
 [4070.49996947    0.315         0.3777        0.17259999    0.15769999
     1.50879121]
 [4326.59027101    0.30080003    0.433         0.15269999    0.14140001
     1.50346911]
 [3904.46276856    0.29340002    0.42789999    0.16849999    0.14210001
     1.39464009]][0m
[37m[1m[2023-07-16 22:20:51,674][257371] Max Reward on eval: 4964.845397950034[0m
[37m[1m[2023-07-16 22:20:51,674][257371] Min Reward on eval: 2198.2013092061507[0m
[37m[1m[2023-07-16 22:20:51,675][257371] Mean Reward across all agents: 3893.642842099086[0m
[37m[1m[2023-07-16 22:20:51,675][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:20:51,678][257371] mean_value=-201.51281020462952, max_value=902.935005033064[0m
[37m[1m[2023-07-16 22:20:51,680][257371] New mean coefficients: [[ 0.14812484  0.6896338   0.19633907  0.06128794 -0.03528544 -1.9726448 ]][0m
[37m[1m[2023-07-16 22:20:51,682][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:21:00,712][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 22:21:00,713][257371] FPS: 425331.87[0m
[36m[2023-07-16 22:21:00,715][257371] itr=157, itrs=2000, Progress: 7.85%[0m
[36m[2023-07-16 22:21:12,303][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-16 22:21:12,304][257371] FPS: 332220.78[0m
[36m[2023-07-16 22:21:16,568][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:21:16,568][257371] Reward + Measures: [[4645.80589513    0.286217      0.4103837     0.14055666    0.12853499
     1.27724957]][0m
[37m[1m[2023-07-16 22:21:16,568][257371] Max Reward on eval: 4645.805895132472[0m
[37m[1m[2023-07-16 22:21:16,568][257371] Min Reward on eval: 4645.805895132472[0m
[37m[1m[2023-07-16 22:21:16,569][257371] Mean Reward across all agents: 4645.805895132472[0m
[37m[1m[2023-07-16 22:21:16,569][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:21:21,740][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:21:21,741][257371] Reward + Measures: [[4095.76626585    0.36649999    0.40499997    0.16240001    0.14289999
     1.30498838]
 [3093.1731262     0.2816        0.42009997    0.1947        0.1551
     1.26831746]
 [4410.47854613    0.26850003    0.41929999    0.15250002    0.13700001
     1.3592236 ]
 ...
 [3765.21416473    0.20490001    0.39790002    0.1485        0.14299999
     1.35621929]
 [2578.9270363     0.32810003    0.3721        0.199         0.15500002
     1.20378745]
 [3881.63706965    0.31309998    0.39970002    0.15630001    0.1463
     1.35974598]][0m
[37m[1m[2023-07-16 22:21:21,741][257371] Max Reward on eval: 4640.921935960138[0m
[37m[1m[2023-07-16 22:21:21,742][257371] Min Reward on eval: 1788.3023452704306[0m
[37m[1m[2023-07-16 22:21:21,742][257371] Mean Reward across all agents: 3629.0717767481924[0m
[37m[1m[2023-07-16 22:21:21,742][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:21:21,745][257371] mean_value=-283.23542001990097, max_value=714.717448317775[0m
[37m[1m[2023-07-16 22:21:21,747][257371] New mean coefficients: [[-0.18097097  0.20983592 -0.23475933 -0.24343106  0.32675496 -0.97278947]][0m
[37m[1m[2023-07-16 22:21:21,748][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:21:30,771][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 22:21:30,771][257371] FPS: 425665.67[0m
[36m[2023-07-16 22:21:30,773][257371] itr=158, itrs=2000, Progress: 7.90%[0m
[36m[2023-07-16 22:21:42,423][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-16 22:21:42,424][257371] FPS: 330516.52[0m
[36m[2023-07-16 22:21:46,803][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:21:46,804][257371] Reward + Measures: [[4196.80178688    0.29958066    0.39930862    0.137061      0.124019
     1.12705266]][0m
[37m[1m[2023-07-16 22:21:46,804][257371] Max Reward on eval: 4196.801786878963[0m
[37m[1m[2023-07-16 22:21:46,804][257371] Min Reward on eval: 4196.801786878963[0m
[37m[1m[2023-07-16 22:21:46,805][257371] Mean Reward across all agents: 4196.801786878963[0m
[37m[1m[2023-07-16 22:21:46,805][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:21:51,899][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:21:51,899][257371] Reward + Measures: [[3464.89157105    0.32000002    0.45360002    0.12490001    0.12990001
     1.19621301]
 [3211.03592682    0.26310003    0.40559998    0.14410001    0.1459
     1.16445947]
 [3276.41857905    0.31759998    0.42969999    0.15750001    0.1184
     1.12095094]
 ...
 [3811.96237179    0.31740004    0.41420004    0.1569        0.1302
     1.22113121]
 [3651.44712832    0.2809        0.38519999    0.1468        0.13240002
     1.13967705]
 [2647.41718296    0.2723        0.29269999    0.1503        0.13350001
     1.22694969]][0m
[37m[1m[2023-07-16 22:21:51,900][257371] Max Reward on eval: 4646.823974600836[0m
[37m[1m[2023-07-16 22:21:51,900][257371] Min Reward on eval: 2275.3291168480414[0m
[37m[1m[2023-07-16 22:21:51,900][257371] Mean Reward across all agents: 3573.3203238231845[0m
[37m[1m[2023-07-16 22:21:51,900][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:21:51,902][257371] mean_value=-635.5118862141048, max_value=689.6321199659706[0m
[37m[1m[2023-07-16 22:21:51,904][257371] New mean coefficients: [[ 0.4857362  -0.16670552 -0.20710413 -0.11263019  0.23019157  0.7055822 ]][0m
[37m[1m[2023-07-16 22:21:51,905][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:22:00,728][257371] train() took 8.82 seconds to complete[0m
[36m[2023-07-16 22:22:00,728][257371] FPS: 435298.67[0m
[36m[2023-07-16 22:22:00,731][257371] itr=159, itrs=2000, Progress: 7.95%[0m
[36m[2023-07-16 22:22:12,917][257371] train() took 12.15 seconds to complete[0m
[36m[2023-07-16 22:22:12,917][257371] FPS: 316062.13[0m
[36m[2023-07-16 22:22:17,182][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:22:17,182][257371] Reward + Measures: [[4528.08980041    0.29322466    0.39310732    0.13858934    0.12676133
     1.19812202]][0m
[37m[1m[2023-07-16 22:22:17,183][257371] Max Reward on eval: 4528.089800414034[0m
[37m[1m[2023-07-16 22:22:17,183][257371] Min Reward on eval: 4528.089800414034[0m
[37m[1m[2023-07-16 22:22:17,183][257371] Mean Reward across all agents: 4528.089800414034[0m
[37m[1m[2023-07-16 22:22:17,183][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:22:22,296][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:22:22,296][257371] Reward + Measures: [[3705.58096314    0.35450003    0.3987        0.14210001    0.14230001
     1.29529178]
 [3710.47140502    0.2852        0.4499        0.1754        0.15390001
     1.22054887]
 [4431.27178954    0.30810001    0.37520003    0.14660001    0.1425
     1.34624374]
 ...
 [4505.71856687    0.32460001    0.42319998    0.14649999    0.13440001
     1.30242944]
 [4359.8170929     0.29480001    0.40319997    0.15699999    0.1477
     1.35067713]
 [4483.3844452     0.29749998    0.37200004    0.14469999    0.13429999
     1.29398501]][0m
[37m[1m[2023-07-16 22:22:22,297][257371] Max Reward on eval: 4735.351470927242[0m
[37m[1m[2023-07-16 22:22:22,297][257371] Min Reward on eval: 2351.3493938860483[0m
[37m[1m[2023-07-16 22:22:22,297][257371] Mean Reward across all agents: 4024.3600567500403[0m
[37m[1m[2023-07-16 22:22:22,297][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:22:22,299][257371] mean_value=-289.76449063473314, max_value=676.8791434902596[0m
[37m[1m[2023-07-16 22:22:22,302][257371] New mean coefficients: [[ 0.5895252   0.12718776 -0.4748246  -0.67968166  0.90469503  1.7510295 ]][0m
[37m[1m[2023-07-16 22:22:22,303][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:22:31,375][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 22:22:31,375][257371] FPS: 423331.33[0m
[36m[2023-07-16 22:22:31,378][257371] itr=160, itrs=2000, Progress: 8.00%[0m
[37m[1m[2023-07-16 22:24:37,989][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000140[0m
[36m[2023-07-16 22:24:50,124][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-16 22:24:50,124][257371] FPS: 332003.45[0m
[36m[2023-07-16 22:24:54,320][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:24:54,320][257371] Reward + Measures: [[4962.08728054    0.29194501    0.37979364    0.140576      0.13145733
     1.30915236]][0m
[37m[1m[2023-07-16 22:24:54,320][257371] Max Reward on eval: 4962.087280540288[0m
[37m[1m[2023-07-16 22:24:54,321][257371] Min Reward on eval: 4962.087280540288[0m
[37m[1m[2023-07-16 22:24:54,321][257371] Mean Reward across all agents: 4962.087280540288[0m
[37m[1m[2023-07-16 22:24:54,321][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:24:59,443][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:24:59,443][257371] Reward + Measures: [[4507.72088623    0.30090001    0.3626        0.1605        0.15550001
     1.4317162 ]
 [5157.31240844    0.29729998    0.3919        0.1471        0.1331
     1.35564005]
 [4771.20867921    0.28599998    0.38249999    0.1381        0.12720001
     1.31159997]
 ...
 [4652.60067749    0.30900002    0.36570001    0.13370001    0.133
     1.34118581]
 [3866.08449554    0.26500002    0.41529998    0.13959999    0.12639999
     1.3396734 ]
 [4300.99633792    0.27509999    0.36300001    0.147         0.14350002
     1.36862886]][0m
[37m[1m[2023-07-16 22:24:59,444][257371] Max Reward on eval: 5157.312408439489[0m
[37m[1m[2023-07-16 22:24:59,444][257371] Min Reward on eval: 2583.1471328736284[0m
[37m[1m[2023-07-16 22:24:59,444][257371] Mean Reward across all agents: 4348.624000382004[0m
[37m[1m[2023-07-16 22:24:59,444][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:24:59,448][257371] mean_value=-111.373420485247, max_value=1156.2967166455305[0m
[37m[1m[2023-07-16 22:24:59,451][257371] New mean coefficients: [[-0.22947562 -0.18289036 -0.41583958 -0.8147745   0.3268515   1.3551962 ]][0m
[37m[1m[2023-07-16 22:24:59,452][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:25:08,263][257371] train() took 8.81 seconds to complete[0m
[36m[2023-07-16 22:25:08,264][257371] FPS: 435858.17[0m
[36m[2023-07-16 22:25:08,266][257371] itr=161, itrs=2000, Progress: 8.05%[0m
[36m[2023-07-16 22:25:20,050][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-16 22:25:20,050][257371] FPS: 326733.77[0m
[36m[2023-07-16 22:25:24,332][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:25:24,332][257371] Reward + Measures: [[5082.06092638    0.298491      0.37770864    0.142308      0.13580434
     1.41852212]][0m
[37m[1m[2023-07-16 22:25:24,332][257371] Max Reward on eval: 5082.060926375306[0m
[37m[1m[2023-07-16 22:25:24,333][257371] Min Reward on eval: 5082.060926375306[0m
[37m[1m[2023-07-16 22:25:24,333][257371] Mean Reward across all agents: 5082.060926375306[0m
[37m[1m[2023-07-16 22:25:24,333][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:25:29,401][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:25:29,407][257371] Reward + Measures: [[3747.15534208    0.2947        0.35549998    0.1504        0.1409
     1.4549284 ]
 [4506.57202148    0.30310002    0.42739996    0.1517        0.14919999
     1.4611882 ]
 [4278.95809934    0.32750002    0.36489996    0.14960001    0.13469999
     1.31590939]
 ...
 [4377.6294861     0.25229999    0.39820001    0.13579999    0.13219999
     1.48276198]
 [2762.28112789    0.26910001    0.35600004    0.16230001    0.13079999
     1.37095964]
 [4363.27743534    0.30210003    0.35640001    0.134         0.12670001
     1.33889163]][0m
[37m[1m[2023-07-16 22:25:29,407][257371] Max Reward on eval: 5139.732299808156[0m
[37m[1m[2023-07-16 22:25:29,408][257371] Min Reward on eval: 2359.362396264216[0m
[37m[1m[2023-07-16 22:25:29,408][257371] Mean Reward across all agents: 4182.431551578844[0m
[37m[1m[2023-07-16 22:25:29,408][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:25:29,410][257371] mean_value=-397.0602023383064, max_value=677.1685947337455[0m
[37m[1m[2023-07-16 22:25:29,413][257371] New mean coefficients: [[-0.23507853  0.24070084 -0.71241176 -0.24946648 -0.14795682  0.601379  ]][0m
[37m[1m[2023-07-16 22:25:29,414][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:25:38,507][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-16 22:25:38,507][257371] FPS: 422378.74[0m
[36m[2023-07-16 22:25:38,510][257371] itr=162, itrs=2000, Progress: 8.10%[0m
[36m[2023-07-16 22:25:50,122][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-16 22:25:50,122][257371] FPS: 331566.52[0m
[36m[2023-07-16 22:25:54,452][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:25:54,452][257371] Reward + Measures: [[4940.46070091    0.31689134    0.37222901    0.14576833    0.138486
     1.50974786]][0m
[37m[1m[2023-07-16 22:25:54,452][257371] Max Reward on eval: 4940.460700912836[0m
[37m[1m[2023-07-16 22:25:54,453][257371] Min Reward on eval: 4940.460700912836[0m
[37m[1m[2023-07-16 22:25:54,453][257371] Mean Reward across all agents: 4940.460700912836[0m
[37m[1m[2023-07-16 22:25:54,453][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:25:59,520][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:25:59,520][257371] Reward + Measures: [[4248.75314332    0.34680003    0.40970001    0.1512        0.13520001
     1.56055319]
 [3459.0578613     0.35690001    0.33880001    0.1708        0.13100001
     1.48527861]
 [4539.67541503    0.32459998    0.39509997    0.1504        0.13950001
     1.40937102]
 ...
 [4585.26470945    0.34600002    0.38030002    0.1437        0.13349999
     1.49416983]
 [3414.91275026    0.26910001    0.4454        0.17230001    0.17650001
     1.56390965]
 [4655.28619385    0.33659998    0.39839998    0.15100001    0.14739999
     1.53304136]][0m
[37m[1m[2023-07-16 22:25:59,521][257371] Max Reward on eval: 5013.12930298273[0m
[37m[1m[2023-07-16 22:25:59,521][257371] Min Reward on eval: 1887.5663680497091[0m
[37m[1m[2023-07-16 22:25:59,521][257371] Mean Reward across all agents: 4195.53407009686[0m
[37m[1m[2023-07-16 22:25:59,521][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:25:59,523][257371] mean_value=-518.2773314546159, max_value=389.8117097996578[0m
[37m[1m[2023-07-16 22:25:59,526][257371] New mean coefficients: [[ 0.645107    0.27124235 -0.3475692   0.10477221 -0.11426646 -0.2022376 ]][0m
[37m[1m[2023-07-16 22:25:59,527][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:26:08,583][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 22:26:08,583][257371] FPS: 424089.31[0m
[36m[2023-07-16 22:26:08,586][257371] itr=163, itrs=2000, Progress: 8.15%[0m
[36m[2023-07-16 22:26:20,293][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-16 22:26:20,293][257371] FPS: 328850.22[0m
[36m[2023-07-16 22:26:24,557][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:26:24,557][257371] Reward + Measures: [[5131.67732273    0.30461699    0.35410798    0.14494601    0.13740566
     1.50419366]][0m
[37m[1m[2023-07-16 22:26:24,558][257371] Max Reward on eval: 5131.677322730979[0m
[37m[1m[2023-07-16 22:26:24,558][257371] Min Reward on eval: 5131.677322730979[0m
[37m[1m[2023-07-16 22:26:24,558][257371] Mean Reward across all agents: 5131.677322730979[0m
[37m[1m[2023-07-16 22:26:24,559][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:26:29,615][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:26:29,621][257371] Reward + Measures: [[4266.80953981    0.33720002    0.3256        0.16060001    0.148
     1.38871729]
 [2730.10499574    0.35090002    0.31909999    0.19559999    0.1758
     1.38103521]
 [4339.33151248    0.29190001    0.4131        0.16500001    0.1675
     1.68450665]
 ...
 [4791.45779418    0.31280002    0.36410004    0.15339999    0.15510002
     1.54633641]
 [4780.6412811     0.28470001    0.34890002    0.1429        0.1383
     1.54640329]
 [4808.33410646    0.32210001    0.3479        0.14500001    0.1432
     1.53689539]][0m
[37m[1m[2023-07-16 22:26:29,621][257371] Max Reward on eval: 5188.078094482655[0m
[37m[1m[2023-07-16 22:26:29,622][257371] Min Reward on eval: 2730.1049957436976[0m
[37m[1m[2023-07-16 22:26:29,622][257371] Mean Reward across all agents: 4588.90477205564[0m
[37m[1m[2023-07-16 22:26:29,622][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:26:29,625][257371] mean_value=-121.71287412056115, max_value=1284.621965681978[0m
[37m[1m[2023-07-16 22:26:29,628][257371] New mean coefficients: [[ 0.36266562 -0.24597248 -0.87996113  0.04506436 -0.145949    0.21936664]][0m
[37m[1m[2023-07-16 22:26:29,629][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:26:38,664][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 22:26:38,664][257371] FPS: 425094.75[0m
[36m[2023-07-16 22:26:38,666][257371] itr=164, itrs=2000, Progress: 8.20%[0m
[36m[2023-07-16 22:26:50,338][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-16 22:26:50,338][257371] FPS: 329950.75[0m
[36m[2023-07-16 22:26:54,671][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:26:54,671][257371] Reward + Measures: [[5441.68301825    0.29063466    0.31384233    0.14530501    0.13992201
     1.57056642]][0m
[37m[1m[2023-07-16 22:26:54,672][257371] Max Reward on eval: 5441.683018252449[0m
[37m[1m[2023-07-16 22:26:54,672][257371] Min Reward on eval: 5441.683018252449[0m
[37m[1m[2023-07-16 22:26:54,672][257371] Mean Reward across all agents: 5441.683018252449[0m
[37m[1m[2023-07-16 22:26:54,672][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:26:59,807][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:26:59,812][257371] Reward + Measures: [[5109.02096561    0.31960002    0.33189997    0.154         0.14459999
     1.57403886]
 [4911.41873171    0.28639999    0.33470002    0.14540002    0.13689999
     1.5860815 ]
 [4629.33605957    0.29190001    0.34170002    0.1364        0.1382
     1.42988276]
 ...
 [5051.91244505    0.29899999    0.30560002    0.1433        0.14049999
     1.56989801]
 [3764.37390519    0.27180001    0.31370002    0.13420001    0.1362
     1.45315826]
 [4902.10250856    0.29300001    0.37630001    0.1523        0.14560001
     1.57879436]][0m
[37m[1m[2023-07-16 22:26:59,812][257371] Max Reward on eval: 5637.048644984461[0m
[37m[1m[2023-07-16 22:26:59,813][257371] Min Reward on eval: 3394.385162388161[0m
[37m[1m[2023-07-16 22:26:59,813][257371] Mean Reward across all agents: 4754.889829941107[0m
[37m[1m[2023-07-16 22:26:59,813][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:26:59,817][257371] mean_value=-41.881660467114806, max_value=1514.0436206694749[0m
[37m[1m[2023-07-16 22:26:59,820][257371] New mean coefficients: [[ 0.15618753  0.13983819 -0.30055267  0.10194033  0.06018317  0.07738262]][0m
[37m[1m[2023-07-16 22:26:59,821][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:27:08,881][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-16 22:27:08,881][257371] FPS: 423899.20[0m
[36m[2023-07-16 22:27:08,884][257371] itr=165, itrs=2000, Progress: 8.25%[0m
[36m[2023-07-16 22:27:20,643][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-16 22:27:20,643][257371] FPS: 327486.59[0m
[36m[2023-07-16 22:27:25,030][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:27:25,031][257371] Reward + Measures: [[5672.99491497    0.29701099    0.29172933    0.14683999    0.14161001
     1.59923911]][0m
[37m[1m[2023-07-16 22:27:25,031][257371] Max Reward on eval: 5672.994914969051[0m
[37m[1m[2023-07-16 22:27:25,031][257371] Min Reward on eval: 5672.994914969051[0m
[37m[1m[2023-07-16 22:27:25,031][257371] Mean Reward across all agents: 5672.994914969051[0m
[37m[1m[2023-07-16 22:27:25,032][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:27:30,079][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:27:30,079][257371] Reward + Measures: [[5370.92657471    0.27400002    0.34639999    0.15010001    0.14960001
     1.61025453]
 [4864.18115235    0.36630002    0.3163        0.1416        0.13810001
     1.46954215]
 [5314.55651855    0.2588        0.3493        0.14230001    0.1418
     1.5541575 ]
 ...
 [5231.85571295    0.303         0.32789999    0.14330001    0.14240001
     1.46185863]
 [5204.90872196    0.3037        0.29609999    0.15460001    0.14220001
     1.6777159 ]
 [5457.24081424    0.32279998    0.3161        0.1569        0.1499
     1.56924474]][0m
[37m[1m[2023-07-16 22:27:30,079][257371] Max Reward on eval: 5789.005615199892[0m
[37m[1m[2023-07-16 22:27:30,080][257371] Min Reward on eval: 3670.6386108290403[0m
[37m[1m[2023-07-16 22:27:30,080][257371] Mean Reward across all agents: 5108.239217198612[0m
[37m[1m[2023-07-16 22:27:30,080][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:27:30,084][257371] mean_value=-93.86588276439322, max_value=1361.1868160592435[0m
[37m[1m[2023-07-16 22:27:30,086][257371] New mean coefficients: [[ 0.32275957  0.16591245 -0.21118498 -0.06826305  0.33577162  0.478207  ]][0m
[37m[1m[2023-07-16 22:27:30,087][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:27:39,140][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 22:27:39,141][257371] FPS: 424246.97[0m
[36m[2023-07-16 22:27:39,143][257371] itr=166, itrs=2000, Progress: 8.30%[0m
[36m[2023-07-16 22:27:50,934][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-16 22:27:50,935][257371] FPS: 326576.46[0m
[36m[2023-07-16 22:27:55,220][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:27:55,220][257371] Reward + Measures: [[5887.24006391    0.29776666    0.28168401    0.154902      0.14909732
     1.71949804]][0m
[37m[1m[2023-07-16 22:27:55,220][257371] Max Reward on eval: 5887.24006391038[0m
[37m[1m[2023-07-16 22:27:55,221][257371] Min Reward on eval: 5887.24006391038[0m
[37m[1m[2023-07-16 22:27:55,221][257371] Mean Reward across all agents: 5887.24006391038[0m
[37m[1m[2023-07-16 22:27:55,221][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:28:00,240][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:28:00,241][257371] Reward + Measures: [[4616.58597946    0.26590002    0.29029998    0.15989999    0.15539999
     1.69629884]
 [5563.1248779     0.30210003    0.3055        0.1595        0.15300001
     1.70876968]
 [5175.85208125    0.28980002    0.27830002    0.1576        0.14920001
     1.7944783 ]
 ...
 [5258.03518673    0.317         0.32139999    0.1578        0.15689999
     1.63165271]
 [4939.19570925    0.34740001    0.34699997    0.16939999    0.17060001
     1.7019161 ]
 [5223.65644071    0.28990003    0.29160002    0.15650001    0.15460001
     1.6832478 ]][0m
[37m[1m[2023-07-16 22:28:00,241][257371] Max Reward on eval: 5969.615966811357[0m
[37m[1m[2023-07-16 22:28:00,241][257371] Min Reward on eval: 3067.2828597909306[0m
[37m[1m[2023-07-16 22:28:00,242][257371] Mean Reward across all agents: 5176.650050296026[0m
[37m[1m[2023-07-16 22:28:00,242][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:28:00,244][257371] mean_value=-382.1050972065508, max_value=1258.822771338746[0m
[37m[1m[2023-07-16 22:28:00,247][257371] New mean coefficients: [[ 0.34566683  0.00982293 -0.08228579 -0.48558572  0.41810727  0.38744664]][0m
[37m[1m[2023-07-16 22:28:00,248][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:28:09,322][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 22:28:09,322][257371] FPS: 423267.85[0m
[36m[2023-07-16 22:28:09,325][257371] itr=167, itrs=2000, Progress: 8.35%[0m
[36m[2023-07-16 22:28:21,217][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-16 22:28:21,218][257371] FPS: 323794.02[0m
[36m[2023-07-16 22:28:25,648][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:28:25,648][257371] Reward + Measures: [[6064.23363033    0.29432067    0.26061666    0.15665434    0.15085134
     1.79579389]][0m
[37m[1m[2023-07-16 22:28:25,648][257371] Max Reward on eval: 6064.2336303330885[0m
[37m[1m[2023-07-16 22:28:25,649][257371] Min Reward on eval: 6064.2336303330885[0m
[37m[1m[2023-07-16 22:28:25,649][257371] Mean Reward across all agents: 6064.2336303330885[0m
[37m[1m[2023-07-16 22:28:25,649][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:28:30,714][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:28:30,714][257371] Reward + Measures: [[5802.96942139    0.33470002    0.26650003    0.1696        0.15810001
     1.87161052]
 [5435.5750656     0.26009998    0.2696        0.1481        0.14049999
     1.70693862]
 [5751.87841803    0.30669999    0.29370001    0.16240001    0.1593
     1.78298116]
 ...
 [5757.69689944    0.33049998    0.2694        0.16470002    0.15629999
     1.8983196 ]
 [5750.59539794    0.28420001    0.29859999    0.1453        0.1446
     1.78913116]
 [5133.29144281    0.29709998    0.29950002    0.15009999    0.15019999
     1.7793684 ]][0m
[37m[1m[2023-07-16 22:28:30,714][257371] Max Reward on eval: 6079.838501006993[0m
[37m[1m[2023-07-16 22:28:30,715][257371] Min Reward on eval: 3304.702987689577[0m
[37m[1m[2023-07-16 22:28:30,715][257371] Mean Reward across all agents: 5384.78805359601[0m
[37m[1m[2023-07-16 22:28:30,715][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:28:30,717][257371] mean_value=-625.9160244988508, max_value=883.6682174820226[0m
[37m[1m[2023-07-16 22:28:30,720][257371] New mean coefficients: [[ 0.24990831  0.38262483  0.5617274  -0.56290245  0.42968455 -0.34415728]][0m
[37m[1m[2023-07-16 22:28:30,721][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:28:39,866][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-16 22:28:39,866][257371] FPS: 419970.14[0m
[36m[2023-07-16 22:28:39,869][257371] itr=168, itrs=2000, Progress: 8.40%[0m
[36m[2023-07-16 22:28:51,764][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-16 22:28:51,764][257371] FPS: 323693.20[0m
[36m[2023-07-16 22:28:56,159][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:28:56,160][257371] Reward + Measures: [[5728.55741038    0.26147431    0.311654      0.142076      0.140202
     1.596331  ]][0m
[37m[1m[2023-07-16 22:28:56,160][257371] Max Reward on eval: 5728.557410380032[0m
[37m[1m[2023-07-16 22:28:56,160][257371] Min Reward on eval: 5728.557410380032[0m
[37m[1m[2023-07-16 22:28:56,161][257371] Mean Reward across all agents: 5728.557410380032[0m
[37m[1m[2023-07-16 22:28:56,161][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:29:01,240][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:29:01,240][257371] Reward + Measures: [[5297.45449827    0.28709999    0.35880002    0.14560001    0.13959999
     1.54026651]
 [5235.4377136     0.271         0.35339999    0.13410001    0.12990001
     1.46755385]
 [4966.46231081    0.28490004    0.35450003    0.13880001    0.12820001
     1.45283151]
 ...
 [4590.97543338    0.31009999    0.33160001    0.1464        0.1437
     1.52824998]
 [4819.49328611    0.26770002    0.3529        0.1559        0.15090001
     1.54055846]
 [5394.17944333    0.28299999    0.33820003    0.1542        0.1504
     1.64144635]][0m
[37m[1m[2023-07-16 22:29:01,240][257371] Max Reward on eval: 5917.802001956152[0m
[37m[1m[2023-07-16 22:29:01,241][257371] Min Reward on eval: 2975.2055434906856[0m
[37m[1m[2023-07-16 22:29:01,241][257371] Mean Reward across all agents: 5124.854861411581[0m
[37m[1m[2023-07-16 22:29:01,241][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:29:01,243][257371] mean_value=-427.73525124211454, max_value=260.23395296100443[0m
[37m[1m[2023-07-16 22:29:01,246][257371] New mean coefficients: [[-0.14122562  0.24722207  0.6199913  -0.805542    0.529554    0.01477304]][0m
[37m[1m[2023-07-16 22:29:01,246][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:29:10,407][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-16 22:29:10,407][257371] FPS: 419284.00[0m
[36m[2023-07-16 22:29:10,409][257371] itr=169, itrs=2000, Progress: 8.45%[0m
[36m[2023-07-16 22:29:22,227][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-16 22:29:22,227][257371] FPS: 325757.50[0m
[36m[2023-07-16 22:29:26,554][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:29:26,554][257371] Reward + Measures: [[5484.6435347     0.28009599    0.35305133    0.13813399    0.13613868
     1.56308818]][0m
[37m[1m[2023-07-16 22:29:26,554][257371] Max Reward on eval: 5484.643534698785[0m
[37m[1m[2023-07-16 22:29:26,554][257371] Min Reward on eval: 5484.643534698785[0m
[37m[1m[2023-07-16 22:29:26,555][257371] Mean Reward across all agents: 5484.643534698785[0m
[37m[1m[2023-07-16 22:29:26,555][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:29:31,670][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:29:31,722][257371] Reward + Measures: [[4607.59582516    0.2568        0.41430002    0.14000002    0.13530003
     1.58324409]
 [4673.39071656    0.26939997    0.39110002    0.1435        0.13830002
     1.620471  ]
 [5597.70880127    0.29190001    0.31890002    0.15030001    0.1487
     1.61585605]
 ...
 [4939.25360105    0.34780002    0.31239998    0.1656        0.1655
     1.62854135]
 [5099.50885009    0.27090001    0.37490001    0.1362        0.1318
     1.5643481 ]
 [5428.58489992    0.26900002    0.34300002    0.13860001    0.13499999
     1.56032908]][0m
[37m[1m[2023-07-16 22:29:31,722][257371] Max Reward on eval: 5765.240295403451[0m
[37m[1m[2023-07-16 22:29:31,722][257371] Min Reward on eval: 2828.045669561252[0m
[37m[1m[2023-07-16 22:29:31,723][257371] Mean Reward across all agents: 5082.070702871975[0m
[37m[1m[2023-07-16 22:29:31,723][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:29:31,724][257371] mean_value=-444.35283314830195, max_value=280.44897391192535[0m
[37m[1m[2023-07-16 22:29:31,727][257371] New mean coefficients: [[-0.6371148   0.06556012  0.31789607 -0.3395301   0.50142145  0.05738959]][0m
[37m[1m[2023-07-16 22:29:31,728][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:29:40,675][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-16 22:29:40,675][257371] FPS: 429289.24[0m
[36m[2023-07-16 22:29:40,677][257371] itr=170, itrs=2000, Progress: 8.50%[0m
[37m[1m[2023-07-16 22:31:48,273][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000150[0m
[36m[2023-07-16 22:32:00,482][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-16 22:32:00,482][257371] FPS: 324953.23[0m
[36m[2023-07-16 22:32:04,720][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:32:04,720][257371] Reward + Measures: [[5110.37823542    0.30136332    0.38971335    0.134854      0.13259301
     1.53774786]][0m
[37m[1m[2023-07-16 22:32:04,720][257371] Max Reward on eval: 5110.378235423759[0m
[37m[1m[2023-07-16 22:32:04,721][257371] Min Reward on eval: 5110.378235423759[0m
[37m[1m[2023-07-16 22:32:04,721][257371] Mean Reward across all agents: 5110.378235423759[0m
[37m[1m[2023-07-16 22:32:04,721][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:32:09,838][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:32:09,839][257371] Reward + Measures: [[4565.02235415    0.26830003    0.35429999    0.1402        0.1402
     1.57728803]
 [4843.95626829    0.33740002    0.38420001    0.1337        0.1354
     1.55399048]
 [5017.7485046     0.2798        0.36859998    0.13150001    0.1319
     1.53990281]
 ...
 [5065.55728152    0.29809999    0.39269999    0.13680001    0.14210001
     1.4490099 ]
 [4969.19775388    0.33479998    0.4039        0.14390001    0.1408
     1.61914694]
 [4645.69964593    0.29969999    0.41490003    0.14479999    0.1487
     1.62536085]][0m
[37m[1m[2023-07-16 22:32:09,839][257371] Max Reward on eval: 5310.353759721201[0m
[37m[1m[2023-07-16 22:32:09,839][257371] Min Reward on eval: 3951.114318838285[0m
[37m[1m[2023-07-16 22:32:09,839][257371] Mean Reward across all agents: 4855.0711828477015[0m
[37m[1m[2023-07-16 22:32:09,840][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:32:09,841][257371] mean_value=-511.069420733477, max_value=-65.3122783448598[0m
[36m[2023-07-16 22:32:09,844][257371] XNES is restarting with a new solution whose measures are [0.72060001 0.2498     0.45889997 0.0931     1.21414113] and objective is 278.22085760263724[0m
[36m[2023-07-16 22:32:09,844][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-16 22:32:09,847][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-16 22:32:09,848][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:32:18,819][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-16 22:32:18,820][257371] FPS: 428074.95[0m
[36m[2023-07-16 22:32:18,822][257371] itr=171, itrs=2000, Progress: 8.55%[0m
[36m[2023-07-16 22:32:30,553][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-16 22:32:30,554][257371] FPS: 328195.01[0m
[36m[2023-07-16 22:32:34,887][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:32:34,888][257371] Reward + Measures: [[584.18384332   0.70943463   0.25977066   0.25636333   0.107345
    0.92270339]][0m
[37m[1m[2023-07-16 22:32:34,888][257371] Max Reward on eval: 584.1838433181462[0m
[37m[1m[2023-07-16 22:32:34,888][257371] Min Reward on eval: 584.1838433181462[0m
[37m[1m[2023-07-16 22:32:34,889][257371] Mean Reward across all agents: 584.1838433181462[0m
[37m[1m[2023-07-16 22:32:34,889][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:32:40,068][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:32:40,069][257371] Reward + Measures: [[272.61285018   0.74800003   0.27919999   0.42829999   0.074
    1.19214594]
 [722.84439467   0.62860006   0.18270001   0.25960001   0.24629998
    1.02397192]
 [558.08252528   0.5966       0.25         0.25690001   0.0816
    1.07195401]
 ...
 [752.89099121   0.55250001   0.21950002   0.19260001   0.1622
    1.07935154]
 [496.73070908   0.61350006   0.25979999   0.21870001   0.0812
    1.1260432 ]
 [954.79499817   0.56109995   0.18380001   0.19649999   0.19780003
    1.28863227]][0m
[37m[1m[2023-07-16 22:32:40,069][257371] Max Reward on eval: 954.7949981686659[0m
[37m[1m[2023-07-16 22:32:40,069][257371] Min Reward on eval: 27.17927083673421[0m
[37m[1m[2023-07-16 22:32:40,070][257371] Mean Reward across all agents: 390.6767886753181[0m
[37m[1m[2023-07-16 22:32:40,070][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:32:40,078][257371] mean_value=-1541.9955290945757, max_value=1050.539886492584[0m
[37m[1m[2023-07-16 22:32:40,081][257371] New mean coefficients: [[-0.36919823  0.55219865 -0.64598894  0.18392766 -1.1729584  -0.711885  ]][0m
[37m[1m[2023-07-16 22:32:40,082][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:32:49,155][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 22:32:49,155][257371] FPS: 423305.79[0m
[36m[2023-07-16 22:32:49,158][257371] itr=172, itrs=2000, Progress: 8.60%[0m
[36m[2023-07-16 22:33:01,009][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-16 22:33:01,010][257371] FPS: 324894.63[0m
[36m[2023-07-16 22:33:05,323][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:33:05,323][257371] Reward + Measures: [[302.17660906   0.81546694   0.27071133   0.35416934   0.06034
    0.75825518]][0m
[37m[1m[2023-07-16 22:33:05,324][257371] Max Reward on eval: 302.1766090630844[0m
[37m[1m[2023-07-16 22:33:05,324][257371] Min Reward on eval: 302.1766090630844[0m
[37m[1m[2023-07-16 22:33:05,324][257371] Mean Reward across all agents: 302.1766090630844[0m
[37m[1m[2023-07-16 22:33:05,324][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:33:10,299][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:33:10,299][257371] Reward + Measures: [[548.01003267   0.59610003   0.25999999   0.22839999   0.1893
    0.97152627]
 [415.49745557   0.74949998   0.21570002   0.39480001   0.16010001
    0.86446381]
 [373.39215467   0.5715       0.27410001   0.37269998   0.34540001
    1.12586057]
 ...
 [510.25908663   0.77220005   0.2332       0.30450001   0.11570001
    0.79268461]
 [405.38558002   0.58850002   0.30990002   0.25110003   0.19059999
    1.22196901]
 [659.99470521   0.71959996   0.19880001   0.24860001   0.16150001
    0.89727479]][0m
[37m[1m[2023-07-16 22:33:10,300][257371] Max Reward on eval: 753.0033864833415[0m
[37m[1m[2023-07-16 22:33:10,300][257371] Min Reward on eval: -14.387912849814166[0m
[37m[1m[2023-07-16 22:33:10,300][257371] Mean Reward across all agents: 264.70591723029816[0m
[37m[1m[2023-07-16 22:33:10,300][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:33:10,310][257371] mean_value=-58.35386405899049, max_value=1117.4140815697028[0m
[37m[1m[2023-07-16 22:33:10,313][257371] New mean coefficients: [[ 0.37216297 -0.19080263 -0.5165076   0.17967445 -1.1669791  -1.3662384 ]][0m
[37m[1m[2023-07-16 22:33:10,314][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:33:19,276][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-16 22:33:19,276][257371] FPS: 428552.51[0m
[36m[2023-07-16 22:33:19,278][257371] itr=173, itrs=2000, Progress: 8.65%[0m
[36m[2023-07-16 22:33:30,888][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-16 22:33:30,888][257371] FPS: 331647.82[0m
[36m[2023-07-16 22:33:35,136][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:33:35,136][257371] Reward + Measures: [[324.11084048   0.87367266   0.28983566   0.41793531   0.025537
    0.51738173]][0m
[37m[1m[2023-07-16 22:33:35,137][257371] Max Reward on eval: 324.110840476409[0m
[37m[1m[2023-07-16 22:33:35,137][257371] Min Reward on eval: 324.110840476409[0m
[37m[1m[2023-07-16 22:33:35,137][257371] Mean Reward across all agents: 324.110840476409[0m
[37m[1m[2023-07-16 22:33:35,137][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:33:40,133][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:33:40,134][257371] Reward + Measures: [[152.96576311   0.81170005   0.35260001   0.41700003   0.0225
    0.50115967]
 [206.5458746    0.78960001   0.44250003   0.56720001   0.0112
    0.99611205]
 [239.10324955   0.8405       0.31819999   0.46380001   0.0259
    0.69298422]
 ...
 [105.50570964   0.8136999    0.34299999   0.51850003   0.0739
    0.64797515]
 [106.64185905   0.71290004   0.22469997   0.67259997   0.22270003
    0.6849637 ]
 [ 90.4781494    0.87379998   0.43339998   0.64289999   0.0299
    0.56256503]][0m
[37m[1m[2023-07-16 22:33:40,134][257371] Max Reward on eval: 712.5168915034271[0m
[37m[1m[2023-07-16 22:33:40,134][257371] Min Reward on eval: 30.15691958339885[0m
[37m[1m[2023-07-16 22:33:40,135][257371] Mean Reward across all agents: 243.41096471082284[0m
[37m[1m[2023-07-16 22:33:40,135][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:33:40,147][257371] mean_value=7.7774521009120505, max_value=945.8184547415934[0m
[37m[1m[2023-07-16 22:33:40,150][257371] New mean coefficients: [[ 1.5766293  -0.16670635 -0.84669346 -0.3928153  -0.43053442 -1.7929313 ]][0m
[37m[1m[2023-07-16 22:33:40,151][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:33:49,156][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-16 22:33:49,156][257371] FPS: 426488.48[0m
[36m[2023-07-16 22:33:49,159][257371] itr=174, itrs=2000, Progress: 8.70%[0m
[36m[2023-07-16 22:34:00,800][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-16 22:34:00,801][257371] FPS: 330757.21[0m
[36m[2023-07-16 22:34:05,121][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:34:05,121][257371] Reward + Measures: [[676.36893385   0.87034458   0.21446301   0.30968568   0.06034367
    0.48788279]][0m
[37m[1m[2023-07-16 22:34:05,121][257371] Max Reward on eval: 676.3689338546483[0m
[37m[1m[2023-07-16 22:34:05,121][257371] Min Reward on eval: 676.3689338546483[0m
[37m[1m[2023-07-16 22:34:05,122][257371] Mean Reward across all agents: 676.3689338546483[0m
[37m[1m[2023-07-16 22:34:05,122][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:34:10,138][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:34:10,138][257371] Reward + Measures: [[154.53808783   0.86790001   0.30060002   0.63969994   0.0647
    0.5187155 ]
 [575.16607667   0.79800004   0.26910001   0.31210002   0.114
    0.55999249]
 [365.21491434   0.77090001   0.25659999   0.30950001   0.1202
    0.69246978]
 ...
 [444.86270713   0.75500005   0.24559999   0.33450001   0.0972
    0.57737571]
 [268.2673092    0.88900006   0.2237       0.58180004   0.0984
    0.4826327 ]
 [396.61840059   0.68390006   0.27000001   0.32300001   0.322
    0.62561291]][0m
[37m[1m[2023-07-16 22:34:10,138][257371] Max Reward on eval: 715.3158797939308[0m
[37m[1m[2023-07-16 22:34:10,139][257371] Min Reward on eval: 82.67827317479532[0m
[37m[1m[2023-07-16 22:34:10,139][257371] Mean Reward across all agents: 413.306013699074[0m
[37m[1m[2023-07-16 22:34:10,139][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:34:10,149][257371] mean_value=80.17171034895664, max_value=886.190460234895[0m
[37m[1m[2023-07-16 22:34:10,152][257371] New mean coefficients: [[ 1.0238619   0.5019524  -0.23975372 -0.62905097  0.53539944 -2.1354008 ]][0m
[37m[1m[2023-07-16 22:34:10,153][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:34:19,248][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-16 22:34:19,249][257371] FPS: 422252.10[0m
[36m[2023-07-16 22:34:19,251][257371] itr=175, itrs=2000, Progress: 8.75%[0m
[36m[2023-07-16 22:34:31,023][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-16 22:34:31,024][257371] FPS: 327101.13[0m
[36m[2023-07-16 22:34:35,405][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:34:35,406][257371] Reward + Measures: [[960.47322337   0.886805     0.222315     0.30475      0.05008234
    0.45285434]][0m
[37m[1m[2023-07-16 22:34:35,406][257371] Max Reward on eval: 960.4732233737647[0m
[37m[1m[2023-07-16 22:34:35,406][257371] Min Reward on eval: 960.4732233737647[0m
[37m[1m[2023-07-16 22:34:35,406][257371] Mean Reward across all agents: 960.4732233737647[0m
[37m[1m[2023-07-16 22:34:35,407][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:34:40,459][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:34:40,460][257371] Reward + Measures: [[265.71093177   0.88900006   0.27579999   0.59869999   0.09670001
    0.50795597]
 [409.50415421   0.86639994   0.30779999   0.43200001   0.0905
    0.50046533]
 [154.05999659   0.82200003   0.29249999   0.76119995   0.0617
    0.60997814]
 ...
 [499.25411989   0.84189999   0.15400001   0.34009999   0.206
    0.53719467]
 [566.09617231   0.86370003   0.32769999   0.47839996   0.0525
    0.50040847]
 [113.14233343   0.7802       0.18870001   0.54579997   0.2
    0.73448414]][0m
[37m[1m[2023-07-16 22:34:40,460][257371] Max Reward on eval: 955.2038193017245[0m
[37m[1m[2023-07-16 22:34:40,460][257371] Min Reward on eval: 63.20849394437391[0m
[37m[1m[2023-07-16 22:34:40,460][257371] Mean Reward across all agents: 452.13876687463744[0m
[37m[1m[2023-07-16 22:34:40,461][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:34:40,469][257371] mean_value=66.55123040736245, max_value=865.8576261417725[0m
[37m[1m[2023-07-16 22:34:40,472][257371] New mean coefficients: [[ 0.95139956  0.38806102 -0.39659202  0.35765624  0.21802571 -1.0377052 ]][0m
[37m[1m[2023-07-16 22:34:40,473][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:34:49,555][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 22:34:49,556][257371] FPS: 422912.61[0m
[36m[2023-07-16 22:34:49,558][257371] itr=176, itrs=2000, Progress: 8.80%[0m
[36m[2023-07-16 22:35:01,228][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-16 22:35:01,229][257371] FPS: 329903.82[0m
[36m[2023-07-16 22:35:05,535][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:35:05,535][257371] Reward + Measures: [[1223.55841372    0.88925505    0.17437398    0.29470533    0.07021933
     0.45231262]][0m
[37m[1m[2023-07-16 22:35:05,535][257371] Max Reward on eval: 1223.5584137213875[0m
[37m[1m[2023-07-16 22:35:05,536][257371] Min Reward on eval: 1223.5584137213875[0m
[37m[1m[2023-07-16 22:35:05,536][257371] Mean Reward across all agents: 1223.5584137213875[0m
[37m[1m[2023-07-16 22:35:05,536][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:35:10,746][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:35:10,747][257371] Reward + Measures: [[281.91762876   0.71049994   0.26730001   0.56709999   0.2362
    0.45888731]
 [550.36127853   0.73959994   0.2287       0.49969998   0.13880001
    0.59077305]
 [ 70.1818298    0.92600006   0.0271       0.74889994   0.72110003
    1.16608238]
 ...
 [ 69.03351819   0.80150002   0.49969998   0.67210001   0.0394
    0.59980774]
 [386.32147886   0.7766       0.20250002   0.34689999   0.35810003
    0.5812363 ]
 [329.39574861   0.60720003   0.30299997   0.25580001   0.36790001
    0.7739048 ]][0m
[37m[1m[2023-07-16 22:35:10,748][257371] Max Reward on eval: 1207.6432342077837[0m
[37m[1m[2023-07-16 22:35:10,748][257371] Min Reward on eval: -2.631218497082591[0m
[37m[1m[2023-07-16 22:35:10,748][257371] Mean Reward across all agents: 627.7876410111994[0m
[37m[1m[2023-07-16 22:35:10,748][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:35:10,760][257371] mean_value=163.415901402485, max_value=1297.845329215212[0m
[37m[1m[2023-07-16 22:35:10,763][257371] New mean coefficients: [[-0.00102192  0.18357335  0.37828523  0.3013194  -0.0389227  -1.2224519 ]][0m
[37m[1m[2023-07-16 22:35:10,763][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:35:19,842][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 22:35:19,842][257371] FPS: 423051.21[0m
[36m[2023-07-16 22:35:19,844][257371] itr=177, itrs=2000, Progress: 8.85%[0m
[36m[2023-07-16 22:35:31,608][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-16 22:35:31,609][257371] FPS: 327275.01[0m
[36m[2023-07-16 22:35:35,978][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:35:35,978][257371] Reward + Measures: [[1063.60050565    0.91446596    0.41745999    0.31289366    0.00828933
     0.37582788]][0m
[37m[1m[2023-07-16 22:35:35,978][257371] Max Reward on eval: 1063.6005056503764[0m
[37m[1m[2023-07-16 22:35:35,979][257371] Min Reward on eval: 1063.6005056503764[0m
[37m[1m[2023-07-16 22:35:35,979][257371] Mean Reward across all agents: 1063.6005056503764[0m
[37m[1m[2023-07-16 22:35:35,979][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:35:41,088][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:35:41,089][257371] Reward + Measures: [[536.29752354   0.90640002   0.42160001   0.32609999   0.0223
    0.42084679]
 [825.57136535   0.8696       0.1972       0.3242       0.0762
    0.45504972]
 [196.4303646    0.79330003   0.41629997   0.59569997   0.11130001
    0.43100587]
 ...
 [466.66934775   0.90620005   0.34999999   0.3222       0.0172
    0.55614823]
 [149.00406362   0.59319997   0.47010002   0.42139998   0.31530005
    0.68063927]
 [520.60441969   0.92300004   0.21250001   0.5262       0.11259999
    0.40073076]][0m
[37m[1m[2023-07-16 22:35:41,089][257371] Max Reward on eval: 1035.371055617556[0m
[37m[1m[2023-07-16 22:35:41,090][257371] Min Reward on eval: 22.652255004178734[0m
[37m[1m[2023-07-16 22:35:41,090][257371] Mean Reward across all agents: 540.1193335287423[0m
[37m[1m[2023-07-16 22:35:41,090][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:35:41,098][257371] mean_value=69.25198104421577, max_value=1314.7160682629794[0m
[37m[1m[2023-07-16 22:35:41,101][257371] New mean coefficients: [[-0.5177186  -0.16871078  1.533834    1.1304191   0.52270305 -0.75312304]][0m
[37m[1m[2023-07-16 22:35:41,102][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:35:50,129][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 22:35:50,130][257371] FPS: 425454.58[0m
[36m[2023-07-16 22:35:50,132][257371] itr=178, itrs=2000, Progress: 8.90%[0m
[36m[2023-07-16 22:36:02,178][257371] train() took 12.01 seconds to complete[0m
[36m[2023-07-16 22:36:02,179][257371] FPS: 319691.85[0m
[36m[2023-07-16 22:36:06,537][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:36:06,537][257371] Reward + Measures: [[451.94881311   0.89080697   0.66777235   0.55562937   0.00911833
    0.32944185]][0m
[37m[1m[2023-07-16 22:36:06,537][257371] Max Reward on eval: 451.9488131112579[0m
[37m[1m[2023-07-16 22:36:06,538][257371] Min Reward on eval: 451.9488131112579[0m
[37m[1m[2023-07-16 22:36:06,538][257371] Mean Reward across all agents: 451.9488131112579[0m
[37m[1m[2023-07-16 22:36:06,538][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:36:11,562][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:36:11,563][257371] Reward + Measures: [[211.28243541   0.65530008   0.47460005   0.52829999   0.26010001
    0.69981641]
 [ 34.51563225   0.92440003   0.81360006   0.90669996   0.81750005
    1.5726229 ]
 [ 31.30669563   0.79720002   0.72130007   0.78350002   0.76789999
    4.04275799]
 ...
 [686.10811232   0.65810007   0.32020003   0.33829999   0.07919999
    0.6342532 ]
 [ 88.30520823   0.83310002   0.57300001   0.74480003   0.75050002
    1.3492887 ]
 [ 44.13414955   0.73860002   0.7881       0.74340004   0.78870004
    4.25941229]][0m
[37m[1m[2023-07-16 22:36:11,563][257371] Max Reward on eval: 919.2373733478598[0m
[37m[1m[2023-07-16 22:36:11,563][257371] Min Reward on eval: -16.976671834569423[0m
[37m[1m[2023-07-16 22:36:11,564][257371] Mean Reward across all agents: 261.19367567488956[0m
[37m[1m[2023-07-16 22:36:11,564][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:36:11,582][257371] mean_value=-74.14067044029157, max_value=1024.6235427809647[0m
[37m[1m[2023-07-16 22:36:11,584][257371] New mean coefficients: [[-0.7521686   0.14360173  1.6821494   1.2491941   1.2207034  -0.4765568 ]][0m
[37m[1m[2023-07-16 22:36:11,585][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:36:20,655][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 22:36:20,655][257371] FPS: 423462.36[0m
[36m[2023-07-16 22:36:20,658][257371] itr=179, itrs=2000, Progress: 8.95%[0m
[36m[2023-07-16 22:36:32,300][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-16 22:36:32,301][257371] FPS: 330752.90[0m
[36m[2023-07-16 22:36:36,724][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:36:36,724][257371] Reward + Measures: [[42.62814034  0.95256007  0.79697067  0.94783765  0.00932167  0.41858771]][0m
[37m[1m[2023-07-16 22:36:36,724][257371] Max Reward on eval: 42.628140338354214[0m
[37m[1m[2023-07-16 22:36:36,725][257371] Min Reward on eval: 42.628140338354214[0m
[37m[1m[2023-07-16 22:36:36,725][257371] Mean Reward across all agents: 42.628140338354214[0m
[37m[1m[2023-07-16 22:36:36,725][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:36:41,762][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:36:41,762][257371] Reward + Measures: [[589.96070481   0.76610005   0.30050001   0.45339999   0.10439999
    0.66041774]
 [100.65122603   0.89309996   0.70600003   0.82870001   0.0495
    0.53323489]
 [ 93.10459352   0.59420002   0.49559999   0.51450002   0.0621
    3.36377192]
 ...
 [167.68147567   0.49340001   0.2771       0.28400001   0.42039999
    1.08196914]
 [ 97.19605465   0.3066       0.28600001   0.24990001   0.24000001
    3.12306023]
 [ 53.19961562   0.35750002   0.26560003   0.32859999   0.1577
    4.50073195]][0m
[37m[1m[2023-07-16 22:36:41,762][257371] Max Reward on eval: 998.133605948952[0m
[37m[1m[2023-07-16 22:36:41,763][257371] Min Reward on eval: -44.545887352153656[0m
[37m[1m[2023-07-16 22:36:41,763][257371] Mean Reward across all agents: 156.84125686151737[0m
[37m[1m[2023-07-16 22:36:41,763][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:36:41,779][257371] mean_value=-77.84892825559301, max_value=818.6489097377146[0m
[37m[1m[2023-07-16 22:36:41,782][257371] New mean coefficients: [[-1.3158683   0.4179712   1.507832    1.3698323   2.3127453  -0.31130356]][0m
[37m[1m[2023-07-16 22:36:41,783][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:36:50,974][257371] train() took 9.19 seconds to complete[0m
[36m[2023-07-16 22:36:50,975][257371] FPS: 417864.43[0m
[36m[2023-07-16 22:36:50,977][257371] itr=180, itrs=2000, Progress: 9.00%[0m
[37m[1m[2023-07-16 22:38:49,350][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000160[0m
[36m[2023-07-16 22:39:01,706][257371] train() took 11.88 seconds to complete[0m
[36m[2023-07-16 22:39:01,706][257371] FPS: 323298.69[0m
[36m[2023-07-16 22:39:06,086][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:39:06,091][257371] Reward + Measures: [[34.31607939  0.98107195  0.8469466   0.9559437   0.01376933  0.40841073]][0m
[37m[1m[2023-07-16 22:39:06,092][257371] Max Reward on eval: 34.316079392613105[0m
[37m[1m[2023-07-16 22:39:06,092][257371] Min Reward on eval: 34.316079392613105[0m
[37m[1m[2023-07-16 22:39:06,092][257371] Mean Reward across all agents: 34.316079392613105[0m
[37m[1m[2023-07-16 22:39:06,093][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:39:11,159][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:39:11,217][257371] Reward + Measures: [[ 44.29789637   0.81920004   0.54409999   0.82679999   0.0872
    3.30083537]
 [ 59.35577344   0.86930001   0.66309994   0.89200002   0.0622
    2.96432686]
 [ 24.93587923   0.87299997   0.3001       0.82110006   0.244
    0.42623296]
 ...
 [-33.109841     0.1164       0.33919999   0.32640001   0.30329999
    4.23596191]
 [124.81123067   0.94329995   0.66430002   0.9497       0.0497
    2.61956477]
 [143.41677383   0.1286       0.53140002   0.34670001   0.46709999
    2.01452279]][0m
[37m[1m[2023-07-16 22:39:11,218][257371] Max Reward on eval: 619.4021301298402[0m
[37m[1m[2023-07-16 22:39:11,218][257371] Min Reward on eval: -55.9301578331273[0m
[37m[1m[2023-07-16 22:39:11,218][257371] Mean Reward across all agents: 123.55558866953693[0m
[37m[1m[2023-07-16 22:39:11,218][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:39:11,240][257371] mean_value=183.6364020463871, max_value=960.2222328071948[0m
[37m[1m[2023-07-16 22:39:11,243][257371] New mean coefficients: [[-1.6788843   0.9732887   1.2937664   1.5098412   2.1535084  -0.62285227]][0m
[37m[1m[2023-07-16 22:39:11,244][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:39:20,294][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 22:39:20,294][257371] FPS: 424388.83[0m
[36m[2023-07-16 22:39:20,296][257371] itr=181, itrs=2000, Progress: 9.05%[0m
[36m[2023-07-16 22:39:32,157][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-16 22:39:32,157][257371] FPS: 324649.27[0m
[36m[2023-07-16 22:39:36,425][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:39:36,426][257371] Reward + Measures: [[31.73649826  0.98957032  0.82767934  0.9566347   0.02085533  0.38666537]][0m
[37m[1m[2023-07-16 22:39:36,426][257371] Max Reward on eval: 31.736498258697537[0m
[37m[1m[2023-07-16 22:39:36,426][257371] Min Reward on eval: 31.736498258697537[0m
[37m[1m[2023-07-16 22:39:36,427][257371] Mean Reward across all agents: 31.736498258697537[0m
[37m[1m[2023-07-16 22:39:36,427][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:39:41,521][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:39:41,521][257371] Reward + Measures: [[  6.87553649   0.24130002   0.26230001   0.28639999   0.23959999
    3.27663016]
 [109.27608801   0.54530001   0.3944       0.53809994   0.13789999
    2.47437286]
 [ 24.78672155   0.1882       0.44580004   0.25150001   0.44480005
    2.20538116]
 ...
 [189.37557025   0.20190001   0.2638       0.2059       0.23840001
    2.7343514 ]
 [147.35766506   0.60320002   0.19890001   0.52290004   0.46310002
    0.43526879]
 [-57.75381796   0.71279997   0.2264       0.65439999   0.1737
    1.22010159]][0m
[37m[1m[2023-07-16 22:39:41,521][257371] Max Reward on eval: 632.4084815826034[0m
[37m[1m[2023-07-16 22:39:41,522][257371] Min Reward on eval: -301.6823549222201[0m
[37m[1m[2023-07-16 22:39:41,522][257371] Mean Reward across all agents: 54.804135846896706[0m
[37m[1m[2023-07-16 22:39:41,522][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:39:41,539][257371] mean_value=-708.9673886487873, max_value=669.1489729892462[0m
[37m[1m[2023-07-16 22:39:41,542][257371] New mean coefficients: [[-0.52941895  0.74647856  1.016542    1.6696353   2.6993046  -0.5863247 ]][0m
[37m[1m[2023-07-16 22:39:41,543][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:39:50,666][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-16 22:39:50,666][257371] FPS: 420971.77[0m
[36m[2023-07-16 22:39:50,668][257371] itr=182, itrs=2000, Progress: 9.10%[0m
[36m[2023-07-16 22:40:02,266][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-16 22:40:02,266][257371] FPS: 332123.74[0m
[36m[2023-07-16 22:40:06,572][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:40:06,573][257371] Reward + Measures: [[26.07563827  0.99153501  0.75121468  0.93474829  0.07824167  0.34405082]][0m
[37m[1m[2023-07-16 22:40:06,573][257371] Max Reward on eval: 26.075638274987412[0m
[37m[1m[2023-07-16 22:40:06,573][257371] Min Reward on eval: 26.075638274987412[0m
[37m[1m[2023-07-16 22:40:06,574][257371] Mean Reward across all agents: 26.075638274987412[0m
[37m[1m[2023-07-16 22:40:06,574][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:40:11,732][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:40:11,737][257371] Reward + Measures: [[ 41.11047078   0.97700006   0.72340006   0.9285       0.0353
    0.74017519]
 [ 10.20741009   0.73590004   0.16580001   0.62490004   0.35020003
    0.80236167]
 [137.37068562   0.86339998   0.15530001   0.76389998   0.32690001
    0.48940182]
 ...
 [ 86.46122056   0.63080007   0.26750001   0.69730002   0.52319998
    0.8344633 ]
 [119.08772182   0.80270004   0.1099       0.82600003   0.35620001
    0.42613816]
 [ 22.65284622   0.57059997   0.21490002   0.55400002   0.26070002
    1.83236814]][0m
[37m[1m[2023-07-16 22:40:11,737][257371] Max Reward on eval: 817.4377136494265[0m
[37m[1m[2023-07-16 22:40:11,738][257371] Min Reward on eval: -118.76563404896297[0m
[37m[1m[2023-07-16 22:40:11,738][257371] Mean Reward across all agents: 92.34927579855952[0m
[37m[1m[2023-07-16 22:40:11,738][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:40:11,752][257371] mean_value=-169.03292147127766, max_value=547.109774108883[0m
[37m[1m[2023-07-16 22:40:11,755][257371] New mean coefficients: [[-0.36423892  0.824135    1.1553417   2.1205068   2.8241625  -0.58337116]][0m
[37m[1m[2023-07-16 22:40:11,756][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:40:20,742][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-16 22:40:20,743][257371] FPS: 427398.87[0m
[36m[2023-07-16 22:40:20,745][257371] itr=183, itrs=2000, Progress: 9.15%[0m
[36m[2023-07-16 22:40:32,320][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-16 22:40:32,320][257371] FPS: 332715.11[0m
[36m[2023-07-16 22:40:36,634][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:40:36,640][257371] Reward + Measures: [[-20.94180468   0.99119365   0.00286733   0.94014239   0.91879928
    0.33434382]][0m
[37m[1m[2023-07-16 22:40:36,640][257371] Max Reward on eval: -20.941804676825928[0m
[37m[1m[2023-07-16 22:40:36,640][257371] Min Reward on eval: -20.941804676825928[0m
[37m[1m[2023-07-16 22:40:36,641][257371] Mean Reward across all agents: -20.941804676825928[0m
[37m[1m[2023-07-16 22:40:36,641][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:40:41,661][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:40:41,662][257371] Reward + Measures: [[-31.55658058   0.37140003   0.22680001   0.33230001   0.3071
    3.83944249]
 [174.44386738   0.87579995   0.86510003   0.75949997   0.1997
    4.28698111]
 [-49.25911092   0.90510005   0.21210001   0.72570002   0.69839996
    0.71648151]
 ...
 [-20.97642416   0.57470006   0.27509999   0.48780003   0.36480004
    2.09896827]
 [-15.36877369   0.99490005   0.0116       0.98489994   0.9587
    0.54009193]
 [-12.1882897    0.58269995   0.28530002   0.40579996   0.44800004
    1.93263209]][0m
[37m[1m[2023-07-16 22:40:41,662][257371] Max Reward on eval: 353.8815269670449[0m
[37m[1m[2023-07-16 22:40:41,662][257371] Min Reward on eval: -169.5141268050298[0m
[37m[1m[2023-07-16 22:40:41,663][257371] Mean Reward across all agents: 14.90586549245[0m
[37m[1m[2023-07-16 22:40:41,663][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:40:41,685][257371] mean_value=108.1500101214301, max_value=677.7953853993863[0m
[37m[1m[2023-07-16 22:40:41,688][257371] New mean coefficients: [[ 1.0427282   0.3505624   0.50758374  1.5778172   2.5590465  -1.4900656 ]][0m
[37m[1m[2023-07-16 22:40:41,689][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:40:50,745][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 22:40:50,745][257371] FPS: 424090.64[0m
[36m[2023-07-16 22:40:50,748][257371] itr=184, itrs=2000, Progress: 9.20%[0m
[36m[2023-07-16 22:41:02,390][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-16 22:41:02,390][257371] FPS: 330821.28[0m
[36m[2023-07-16 22:41:06,728][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:41:06,729][257371] Reward + Measures: [[28.74348924  0.97517067  0.03851667  0.90388632  0.82003301  0.52189183]][0m
[37m[1m[2023-07-16 22:41:06,729][257371] Max Reward on eval: 28.7434892435199[0m
[37m[1m[2023-07-16 22:41:06,729][257371] Min Reward on eval: 28.7434892435199[0m
[37m[1m[2023-07-16 22:41:06,730][257371] Mean Reward across all agents: 28.7434892435199[0m
[37m[1m[2023-07-16 22:41:06,730][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:41:11,834][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:41:11,839][257371] Reward + Measures: [[ -3.66281273   0.92019999   0.51640004   0.89910001   0.7633
    0.88471365]
 [  0.74425629   0.1957       0.61220002   0.40750003   0.67880005
    1.62400913]
 [-26.79818407   0.31300002   0.73859996   0.38350001   0.75100005
    1.2656244 ]
 ...
 [ 13.71510659   0.75200003   0.28480002   0.69159997   0.2649
    0.80171204]
 [  1.46620218   0.67320001   0.38189998   0.7274       0.57180005
    0.6462695 ]
 [142.80622575   0.75150001   0.31160003   0.65240002   0.12
    1.46191251]][0m
[37m[1m[2023-07-16 22:41:11,840][257371] Max Reward on eval: 209.8555927585112[0m
[37m[1m[2023-07-16 22:41:11,840][257371] Min Reward on eval: -260.6111881782999[0m
[37m[1m[2023-07-16 22:41:11,840][257371] Mean Reward across all agents: 18.935745748153238[0m
[37m[1m[2023-07-16 22:41:11,840][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:41:11,861][257371] mean_value=0.19721386413055977, max_value=695.6405735059176[0m
[37m[1m[2023-07-16 22:41:11,864][257371] New mean coefficients: [[ 0.223993    0.43464506 -1.1626805   1.6108093   2.207864   -2.0426083 ]][0m
[37m[1m[2023-07-16 22:41:11,865][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:41:21,016][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-16 22:41:21,016][257371] FPS: 419689.93[0m
[36m[2023-07-16 22:41:21,019][257371] itr=185, itrs=2000, Progress: 9.25%[0m
[36m[2023-07-16 22:41:33,085][257371] train() took 12.03 seconds to complete[0m
[36m[2023-07-16 22:41:33,086][257371] FPS: 319173.70[0m
[36m[2023-07-16 22:41:37,404][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:41:37,405][257371] Reward + Measures: [[-11.96137852   0.97556072   0.8434813    0.97884035   0.97946697
    0.58308291]][0m
[37m[1m[2023-07-16 22:41:37,405][257371] Max Reward on eval: -11.961378524178581[0m
[37m[1m[2023-07-16 22:41:37,405][257371] Min Reward on eval: -11.961378524178581[0m
[37m[1m[2023-07-16 22:41:37,406][257371] Mean Reward across all agents: -11.961378524178581[0m
[37m[1m[2023-07-16 22:41:37,406][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:41:42,449][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:41:42,450][257371] Reward + Measures: [[ 27.66483479   0.62220001   0.25030002   0.5772       0.4454
    1.80237317]
 [ 49.93434143   0.97189999   0.35900003   0.93990004   0.75480002
    0.8213135 ]
 [-25.77172339   0.66049999   0.1464       0.59709996   0.58280003
    1.29532456]
 ...
 [-37.45716572   0.9429       0.1865       0.91949999   0.78619999
    2.15920091]
 [ 83.97018099   0.56899995   0.45989999   0.62279999   0.55859995
    1.19516695]
 [ 11.11524812   0.93610001   0.1038       0.82560009   0.4549
    0.88102496]][0m
[37m[1m[2023-07-16 22:41:42,450][257371] Max Reward on eval: 192.5900650777854[0m
[37m[1m[2023-07-16 22:41:42,450][257371] Min Reward on eval: -100.21828076709062[0m
[37m[1m[2023-07-16 22:41:42,450][257371] Mean Reward across all agents: 29.524054808251908[0m
[37m[1m[2023-07-16 22:41:42,451][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:41:42,465][257371] mean_value=37.65634757006468, max_value=692.5900650777854[0m
[37m[1m[2023-07-16 22:41:42,468][257371] New mean coefficients: [[ 0.34758598  0.5693944  -0.45483243  1.3301976   1.4287889  -2.0478635 ]][0m
[37m[1m[2023-07-16 22:41:42,469][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:41:51,547][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 22:41:51,547][257371] FPS: 423104.99[0m
[36m[2023-07-16 22:41:51,549][257371] itr=186, itrs=2000, Progress: 9.30%[0m
[36m[2023-07-16 22:42:03,599][257371] train() took 12.01 seconds to complete[0m
[36m[2023-07-16 22:42:03,599][257371] FPS: 319685.54[0m
[36m[2023-07-16 22:42:07,941][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:42:07,941][257371] Reward + Measures: [[56.33505364  0.79902291  0.81600332  0.89958268  0.73056096  0.4890703 ]][0m
[37m[1m[2023-07-16 22:42:07,942][257371] Max Reward on eval: 56.33505364082733[0m
[37m[1m[2023-07-16 22:42:07,942][257371] Min Reward on eval: 56.33505364082733[0m
[37m[1m[2023-07-16 22:42:07,942][257371] Mean Reward across all agents: 56.33505364082733[0m
[37m[1m[2023-07-16 22:42:07,942][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:42:13,138][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:42:13,139][257371] Reward + Measures: [[48.95948862  0.6099      0.30130002  0.5467      0.23510002  1.16982722]
 [57.47699457  0.46469998  0.30950001  0.54980004  0.52610004  1.34687364]
 [ 9.92053647  0.89860004  0.2441      0.87029999  0.57960004  0.67621773]
 ...
 [53.71092355  0.83029997  0.26360002  0.68889999  0.1067      0.73475927]
 [16.24784585  0.75069994  0.57650006  0.72670001  0.3585      0.55236572]
 [43.88719019  0.8276      0.3159      0.82129997  0.12820001  0.77088428]][0m
[37m[1m[2023-07-16 22:42:13,139][257371] Max Reward on eval: 219.42607690505685[0m
[37m[1m[2023-07-16 22:42:13,139][257371] Min Reward on eval: -138.98820398342795[0m
[37m[1m[2023-07-16 22:42:13,139][257371] Mean Reward across all agents: 40.76022202764164[0m
[37m[1m[2023-07-16 22:42:13,140][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:42:13,151][257371] mean_value=-300.7335154262446, max_value=617.4612593635917[0m
[37m[1m[2023-07-16 22:42:13,153][257371] New mean coefficients: [[ 0.88588107  0.07312906 -0.326153    1.6191489   1.0030104  -0.3235861 ]][0m
[37m[1m[2023-07-16 22:42:13,154][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:42:22,202][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 22:42:22,203][257371] FPS: 424478.42[0m
[36m[2023-07-16 22:42:22,205][257371] itr=187, itrs=2000, Progress: 9.35%[0m
[36m[2023-07-16 22:42:33,801][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-16 22:42:33,801][257371] FPS: 332183.45[0m
[36m[2023-07-16 22:42:38,083][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:42:38,083][257371] Reward + Measures: [[36.66328097  0.87432599  0.90038997  0.95047629  0.88628     0.39611161]][0m
[37m[1m[2023-07-16 22:42:38,083][257371] Max Reward on eval: 36.66328096582515[0m
[37m[1m[2023-07-16 22:42:38,084][257371] Min Reward on eval: 36.66328096582515[0m
[37m[1m[2023-07-16 22:42:38,084][257371] Mean Reward across all agents: 36.66328096582515[0m
[37m[1m[2023-07-16 22:42:38,084][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:42:43,177][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:42:43,183][257371] Reward + Measures: [[ 38.65281252   0.5571       0.90550005   0.73370004   0.86920005
    0.41672006]
 [ 99.8308127    0.7622       0.46939999   0.83129996   0.29700002
    0.61182433]
 [116.50627943   0.39020002   0.20709999   0.3804       0.1156
    2.95466995]
 ...
 [ 64.82781982   0.70860004   0.30430001   0.69020003   0.2674
    1.30331254]
 [130.51280305   0.60879999   0.33000001   0.60409999   0.1327
    1.40191746]
 [ 68.77750825   0.73299998   0.72570002   0.91020006   0.2559
    0.38235322]][0m
[37m[1m[2023-07-16 22:42:43,183][257371] Max Reward on eval: 275.65321732349696[0m
[37m[1m[2023-07-16 22:42:43,184][257371] Min Reward on eval: -26.54711326188408[0m
[37m[1m[2023-07-16 22:42:43,184][257371] Mean Reward across all agents: 81.07127837322359[0m
[37m[1m[2023-07-16 22:42:43,184][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:42:43,202][257371] mean_value=-7.807593753571539, max_value=625.1652612769045[0m
[37m[1m[2023-07-16 22:42:43,205][257371] New mean coefficients: [[ 1.1081098   0.25836712 -0.81310016  1.9942398   2.3402963  -0.02711281]][0m
[37m[1m[2023-07-16 22:42:43,205][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:42:52,305][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-16 22:42:52,305][257371] FPS: 422089.00[0m
[36m[2023-07-16 22:42:52,307][257371] itr=188, itrs=2000, Progress: 9.40%[0m
[36m[2023-07-16 22:43:04,197][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-16 22:43:04,197][257371] FPS: 324048.03[0m
[36m[2023-07-16 22:43:08,567][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:43:08,567][257371] Reward + Measures: [[35.80544453  0.85390061  0.92515427  0.9613989   0.919909    0.40744001]][0m
[37m[1m[2023-07-16 22:43:08,568][257371] Max Reward on eval: 35.80544452720161[0m
[37m[1m[2023-07-16 22:43:08,568][257371] Min Reward on eval: 35.80544452720161[0m
[37m[1m[2023-07-16 22:43:08,568][257371] Mean Reward across all agents: 35.80544452720161[0m
[37m[1m[2023-07-16 22:43:08,568][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:43:13,612][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:43:13,613][257371] Reward + Measures: [[ 63.51298571   0.74770004   0.67800003   0.87840003   0.7439
    0.52993977]
 [ 92.60816142   0.435        0.4639       0.588        0.38980001
    1.79457891]
 [ 39.6493933    0.42199999   0.95690006   0.94740003   0.9641
    0.43486091]
 ...
 [ 49.66445062   0.78350002   0.58630002   0.89810002   0.55960006
    0.56108505]
 [108.12521031   0.32910001   0.3263       0.44580004   0.24259999
    2.79354525]
 [ 34.53599215   0.67900002   0.40400001   0.67799997   0.2228
    0.72548819]][0m
[37m[1m[2023-07-16 22:43:13,613][257371] Max Reward on eval: 220.03912830913904[0m
[37m[1m[2023-07-16 22:43:13,613][257371] Min Reward on eval: -62.375627991463986[0m
[37m[1m[2023-07-16 22:43:13,613][257371] Mean Reward across all agents: 56.390394048464856[0m
[37m[1m[2023-07-16 22:43:13,614][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:43:13,627][257371] mean_value=-82.89327937872287, max_value=629.1157692640088[0m
[37m[1m[2023-07-16 22:43:13,630][257371] New mean coefficients: [[ 1.7325528  -0.29672837 -0.38772494  2.3584456   2.166296   -1.4029102 ]][0m
[37m[1m[2023-07-16 22:43:13,631][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:43:22,731][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-16 22:43:22,731][257371] FPS: 422060.23[0m
[36m[2023-07-16 22:43:22,733][257371] itr=189, itrs=2000, Progress: 9.45%[0m
[36m[2023-07-16 22:43:34,560][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-16 22:43:34,560][257371] FPS: 325730.45[0m
[36m[2023-07-16 22:43:38,867][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:43:38,867][257371] Reward + Measures: [[40.41888176  0.84027702  0.9531309   0.97720033  0.95724463  0.36097893]][0m
[37m[1m[2023-07-16 22:43:38,867][257371] Max Reward on eval: 40.41888176226019[0m
[37m[1m[2023-07-16 22:43:38,868][257371] Min Reward on eval: 40.41888176226019[0m
[37m[1m[2023-07-16 22:43:38,868][257371] Mean Reward across all agents: 40.41888176226019[0m
[37m[1m[2023-07-16 22:43:38,868][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:43:43,927][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:43:43,928][257371] Reward + Measures: [[-55.06043718   0.67820007   0.43099999   0.6239       0.41799998
    0.68430865]
 [ 10.57422108   0.3098       0.80229998   0.667        0.77710003
    0.69542265]
 [ 92.95936582   0.588        0.3725       0.68360007   0.31899998
    1.04126537]
 ...
 [-18.83642237   0.71289998   0.61210006   0.55960006   0.30219999
    1.05373728]
 [ -6.49249239   0.67880005   0.8387       0.82669991   0.69379997
    0.54706055]
 [-98.99069538   0.60190004   0.34980002   0.52209997   0.35730001
    1.28736103]][0m
[37m[1m[2023-07-16 22:43:43,928][257371] Max Reward on eval: 173.27245042564465[0m
[37m[1m[2023-07-16 22:43:43,928][257371] Min Reward on eval: -332.46533584138376[0m
[37m[1m[2023-07-16 22:43:43,929][257371] Mean Reward across all agents: -21.26900812989373[0m
[37m[1m[2023-07-16 22:43:43,929][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:43:43,938][257371] mean_value=-250.44514828136636, max_value=510.6603118191252[0m
[37m[1m[2023-07-16 22:43:43,941][257371] New mean coefficients: [[ 1.7576185  -0.44875103 -0.15139547  2.1453025   1.0865864  -0.5102868 ]][0m
[37m[1m[2023-07-16 22:43:43,942][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:43:52,951][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 22:43:52,951][257371] FPS: 426314.82[0m
[36m[2023-07-16 22:43:52,953][257371] itr=190, itrs=2000, Progress: 9.50%[0m
[37m[1m[2023-07-16 22:45:59,901][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000170[0m
[36m[2023-07-16 22:46:12,106][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-16 22:46:12,106][257371] FPS: 329990.90[0m
[36m[2023-07-16 22:46:16,369][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:46:16,369][257371] Reward + Measures: [[34.52362382  0.82664406  0.96252441  0.98032629  0.96822637  0.31927878]][0m
[37m[1m[2023-07-16 22:46:16,370][257371] Max Reward on eval: 34.523623824256994[0m
[37m[1m[2023-07-16 22:46:16,370][257371] Min Reward on eval: 34.523623824256994[0m
[37m[1m[2023-07-16 22:46:16,370][257371] Mean Reward across all agents: 34.523623824256994[0m
[37m[1m[2023-07-16 22:46:16,370][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:46:21,356][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:46:21,356][257371] Reward + Measures: [[-31.89862837   0.35600001   0.24660002   0.40009999   0.28889999
    1.57552397]
 [ 34.42117431   0.18180001   0.79420006   0.65630001   0.8344
    0.72190046]
 [181.98265456   0.57890004   0.58050007   0.76179999   0.63160002
    0.81039429]
 ...
 [ 40.79282507   0.85400003   0.26419997   0.84389991   0.2447
    1.19703734]
 [ 36.36672081   0.75699997   0.79479998   0.76800007   0.75670004
    0.56781161]
 [ 28.89808419   0.3619       0.6268       0.46989998   0.66460001
    0.91843206]][0m
[37m[1m[2023-07-16 22:46:21,357][257371] Max Reward on eval: 204.74016284722603[0m
[37m[1m[2023-07-16 22:46:21,357][257371] Min Reward on eval: -98.31804703033995[0m
[37m[1m[2023-07-16 22:46:21,357][257371] Mean Reward across all agents: 44.520464356014756[0m
[37m[1m[2023-07-16 22:46:21,357][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:46:21,371][257371] mean_value=-72.00597933561178, max_value=551.0035679676803[0m
[37m[1m[2023-07-16 22:46:21,374][257371] New mean coefficients: [[ 2.0533333  -1.2706547  -0.256436    1.6823536   0.94063914 -2.0696163 ]][0m
[37m[1m[2023-07-16 22:46:21,374][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:46:30,464][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-16 22:46:30,464][257371] FPS: 422549.50[0m
[36m[2023-07-16 22:46:30,466][257371] itr=191, itrs=2000, Progress: 9.55%[0m
[36m[2023-07-16 22:46:42,217][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-16 22:46:42,218][257371] FPS: 327820.33[0m
[36m[2023-07-16 22:46:46,546][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:46:46,547][257371] Reward + Measures: [[40.50789358  0.47449833  0.96537107  0.97063929  0.97144562  0.33311316]][0m
[37m[1m[2023-07-16 22:46:46,547][257371] Max Reward on eval: 40.507893583369196[0m
[37m[1m[2023-07-16 22:46:46,547][257371] Min Reward on eval: 40.507893583369196[0m
[37m[1m[2023-07-16 22:46:46,547][257371] Mean Reward across all agents: 40.507893583369196[0m
[37m[1m[2023-07-16 22:46:46,548][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:46:51,579][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:46:51,585][257371] Reward + Measures: [[ 66.17349362   0.51100004   0.72170001   0.78800005   0.63790005
    0.56701052]
 [ -3.65376863   0.76060003   0.64829999   0.7913       0.60780001
    2.26317191]
 [ 89.96432921   0.43050003   0.65259999   0.52579999   0.65990001
    0.47983399]
 ...
 [100.4891834    0.55739999   0.7306       0.83890003   0.71000004
    0.40226632]
 [ -8.27737647   0.93190002   0.85519999   0.93470001   0.85439998
    1.36792397]
 [ 66.20480195   0.23460002   0.52260005   0.35240003   0.4621
    1.73356175]][0m
[37m[1m[2023-07-16 22:46:51,585][257371] Max Reward on eval: 295.83663366306575[0m
[37m[1m[2023-07-16 22:46:51,586][257371] Min Reward on eval: -66.56350457484368[0m
[37m[1m[2023-07-16 22:46:51,586][257371] Mean Reward across all agents: 47.51609784301039[0m
[37m[1m[2023-07-16 22:46:51,586][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:46:51,598][257371] mean_value=-238.95258732009913, max_value=479.4043284394914[0m
[37m[1m[2023-07-16 22:46:51,601][257371] New mean coefficients: [[ 2.019967   -2.3639889  -0.6256957   1.6544178   0.17810673 -2.6242654 ]][0m
[37m[1m[2023-07-16 22:46:51,602][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:47:00,674][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 22:47:00,674][257371] FPS: 423344.66[0m
[36m[2023-07-16 22:47:00,676][257371] itr=192, itrs=2000, Progress: 9.60%[0m
[36m[2023-07-16 22:47:12,584][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-16 22:47:12,584][257371] FPS: 323520.60[0m
[36m[2023-07-16 22:47:16,912][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:47:16,912][257371] Reward + Measures: [[35.98527169  0.22506301  0.96945572  0.96668196  0.97542202  0.29782677]][0m
[37m[1m[2023-07-16 22:47:16,912][257371] Max Reward on eval: 35.985271693970475[0m
[37m[1m[2023-07-16 22:47:16,913][257371] Min Reward on eval: 35.985271693970475[0m
[37m[1m[2023-07-16 22:47:16,913][257371] Mean Reward across all agents: 35.985271693970475[0m
[37m[1m[2023-07-16 22:47:16,913][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:47:22,124][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:47:22,125][257371] Reward + Measures: [[-63.65254898   0.50020003   0.68900007   0.52939999   0.3897
    0.7767415 ]
 [ 28.70276768   0.3567       0.38069999   0.37290001   0.44390002
    1.68260276]
 [192.89443396   0.69390005   0.66030002   0.75439996   0.36129999
    1.52767932]
 ...
 [ 33.09147941   0.47390005   0.90360004   0.87290001   0.90080005
    0.28146935]
 [ 25.90874525   0.8283       0.36070001   0.90189999   0.68409997
    0.45857954]
 [-89.63088942   0.528        0.40080005   0.34730002   0.40489998
    1.35191846]][0m
[37m[1m[2023-07-16 22:47:22,125][257371] Max Reward on eval: 213.5706405328354[0m
[37m[1m[2023-07-16 22:47:22,126][257371] Min Reward on eval: -268.05852219422815[0m
[37m[1m[2023-07-16 22:47:22,126][257371] Mean Reward across all agents: 1.1877179550916332[0m
[37m[1m[2023-07-16 22:47:22,126][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:47:22,135][257371] mean_value=-727.2231119546843, max_value=588.5550966186623[0m
[37m[1m[2023-07-16 22:47:22,137][257371] New mean coefficients: [[ 0.66740406 -2.493634   -0.49539632  2.279317   -0.94903976 -2.6324635 ]][0m
[37m[1m[2023-07-16 22:47:22,138][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:47:31,212][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 22:47:31,212][257371] FPS: 423274.78[0m
[36m[2023-07-16 22:47:31,215][257371] itr=193, itrs=2000, Progress: 9.65%[0m
[36m[2023-07-16 22:47:42,866][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-16 22:47:42,867][257371] FPS: 330768.15[0m
[36m[2023-07-16 22:47:47,083][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:47:47,083][257371] Reward + Measures: [[23.91259385  0.071736    0.98266762  0.97348434  0.99062157  0.22394367]][0m
[37m[1m[2023-07-16 22:47:47,083][257371] Max Reward on eval: 23.912593848274376[0m
[37m[1m[2023-07-16 22:47:47,084][257371] Min Reward on eval: 23.912593848274376[0m
[37m[1m[2023-07-16 22:47:47,084][257371] Mean Reward across all agents: 23.912593848274376[0m
[37m[1m[2023-07-16 22:47:47,084][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:47:52,102][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:47:52,102][257371] Reward + Measures: [[ 37.54616258   0.39669999   0.20390001   0.34640002   0.25190002
    2.55342484]
 [ 98.02598172   0.52080005   0.57260007   0.62889999   0.50349998
    0.75025779]
 [160.90083219   0.90130007   0.86180001   0.93430007   0.83770001
    1.04235065]
 ...
 [ 17.03171875   0.5801       0.1305       0.55360001   0.36630002
    1.71297109]
 [256.61724093   0.53740001   0.3145       0.6257       0.31500003
    1.23794925]
 [ 72.10434428   0.74449998   0.52979994   0.77699995   0.7177
    0.47801438]][0m
[37m[1m[2023-07-16 22:47:52,103][257371] Max Reward on eval: 328.9457778692711[0m
[37m[1m[2023-07-16 22:47:52,103][257371] Min Reward on eval: -125.32488533556462[0m
[37m[1m[2023-07-16 22:47:52,103][257371] Mean Reward across all agents: 40.98393218060268[0m
[37m[1m[2023-07-16 22:47:52,103][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:47:52,120][257371] mean_value=-57.99890096481855, max_value=598.3834320193157[0m
[37m[1m[2023-07-16 22:47:52,123][257371] New mean coefficients: [[-0.37329984 -2.5605547  -1.1184804   1.6727793  -0.5109148  -3.3296044 ]][0m
[37m[1m[2023-07-16 22:47:52,124][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:48:01,221][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-16 22:48:01,221][257371] FPS: 422177.42[0m
[36m[2023-07-16 22:48:01,224][257371] itr=194, itrs=2000, Progress: 9.70%[0m
[36m[2023-07-16 22:48:12,980][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-16 22:48:12,980][257371] FPS: 327712.49[0m
[36m[2023-07-16 22:48:17,279][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:48:17,280][257371] Reward + Measures: [[17.18706602  0.04746833  0.98839337  0.97987866  0.99459767  0.18008392]][0m
[37m[1m[2023-07-16 22:48:17,280][257371] Max Reward on eval: 17.187066018296736[0m
[37m[1m[2023-07-16 22:48:17,280][257371] Min Reward on eval: 17.187066018296736[0m
[37m[1m[2023-07-16 22:48:17,281][257371] Mean Reward across all agents: 17.187066018296736[0m
[37m[1m[2023-07-16 22:48:17,281][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:48:22,319][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:48:22,325][257371] Reward + Measures: [[   2.18103502    0.8592        0.79119998    0.81380004    0.86680001
     0.29698727]
 [-104.42541157    0.37369999    0.36230001    0.27309999    0.2703
     2.49897504]
 [ -34.43902977    0.26950002    0.30160001    0.29100001    0.26340002
     2.65867805]
 ...
 [-212.10522418    0.72349995    0.58470005    0.85079998    0.16580001
     4.55406904]
 [  34.267386      0.72710007    0.89840001    0.9005        0.73070002
     0.29534712]
 [  12.29135919    0.98110008    0.95240003    0.98209995    0.9788
     0.21411657]][0m
[37m[1m[2023-07-16 22:48:22,325][257371] Max Reward on eval: 492.4737911645323[0m
[37m[1m[2023-07-16 22:48:22,326][257371] Min Reward on eval: -212.10522418476177[0m
[37m[1m[2023-07-16 22:48:22,326][257371] Mean Reward across all agents: -8.45241164549078[0m
[37m[1m[2023-07-16 22:48:22,326][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:48:22,339][257371] mean_value=-886.2772044923655, max_value=818.3300550494155[0m
[37m[1m[2023-07-16 22:48:22,342][257371] New mean coefficients: [[ 1.7083597  -2.6116042  -1.1547046   1.0563931   0.43364346 -2.8359146 ]][0m
[37m[1m[2023-07-16 22:48:22,343][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:48:31,380][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 22:48:31,381][257371] FPS: 424987.58[0m
[36m[2023-07-16 22:48:31,383][257371] itr=195, itrs=2000, Progress: 9.75%[0m
[36m[2023-07-16 22:48:43,149][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-16 22:48:43,149][257371] FPS: 327495.56[0m
[36m[2023-07-16 22:48:47,456][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:48:47,456][257371] Reward + Measures: [[52.92476134  0.37773135  0.82944232  0.70372003  0.81062132  0.3235465 ]][0m
[37m[1m[2023-07-16 22:48:47,457][257371] Max Reward on eval: 52.92476133749348[0m
[37m[1m[2023-07-16 22:48:47,457][257371] Min Reward on eval: 52.92476133749348[0m
[37m[1m[2023-07-16 22:48:47,457][257371] Mean Reward across all agents: 52.92476133749348[0m
[37m[1m[2023-07-16 22:48:47,458][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:48:52,542][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:48:52,543][257371] Reward + Measures: [[ 29.12253576   0.31029996   0.38970003   0.4145       0.39570001
    1.38346839]
 [ 45.6999318    0.2418       0.39610001   0.27610001   0.36399999
    1.93222141]
 [-28.44624116   0.30780002   0.49530002   0.1247       0.34400001
    3.18107104]
 ...
 [ -0.65523728   0.19840001   0.20799999   0.125        0.19679999
    4.92918777]
 [ 54.68364316   0.23029999   0.51470006   0.46700001   0.4966
    1.59478414]
 [ 26.97249447   0.5607       0.56900007   0.20829999   0.52889997
    1.68605483]][0m
[37m[1m[2023-07-16 22:48:52,543][257371] Max Reward on eval: 226.34723856644706[0m
[37m[1m[2023-07-16 22:48:52,543][257371] Min Reward on eval: -164.97559255778324[0m
[37m[1m[2023-07-16 22:48:52,544][257371] Mean Reward across all agents: 23.963793555554634[0m
[37m[1m[2023-07-16 22:48:52,544][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:48:52,557][257371] mean_value=-643.1778559383783, max_value=638.835263780132[0m
[37m[1m[2023-07-16 22:48:52,560][257371] New mean coefficients: [[ 2.5117722  -2.49392    -0.8627304   0.42208928  0.5022534  -3.3270466 ]][0m
[37m[1m[2023-07-16 22:48:52,561][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:49:01,705][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-16 22:49:01,710][257371] FPS: 420033.06[0m
[36m[2023-07-16 22:49:01,713][257371] itr=196, itrs=2000, Progress: 9.80%[0m
[36m[2023-07-16 22:49:13,630][257371] train() took 11.88 seconds to complete[0m
[36m[2023-07-16 22:49:13,631][257371] FPS: 323292.30[0m
[36m[2023-07-16 22:49:17,896][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:49:17,896][257371] Reward + Measures: [[27.86849412  0.09275966  0.93177098  0.74133104  0.94093508  0.28497478]][0m
[37m[1m[2023-07-16 22:49:17,896][257371] Max Reward on eval: 27.868494117230746[0m
[37m[1m[2023-07-16 22:49:17,897][257371] Min Reward on eval: 27.868494117230746[0m
[37m[1m[2023-07-16 22:49:17,897][257371] Mean Reward across all agents: 27.868494117230746[0m
[37m[1m[2023-07-16 22:49:17,897][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:49:23,018][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:49:23,024][257371] Reward + Measures: [[  3.71227042   0.70429993   0.36110002   0.64429998   0.1186
    1.04594994]
 [ 45.22415647   0.22470002   0.2471       0.28240001   0.2158
    3.74482346]
 [ 42.86399196   0.1621       0.1856       0.23540001   0.1657
    3.7875042 ]
 ...
 [ 25.97102098   0.2978       0.29990003   0.32529998   0.2174
    2.74744391]
 [ 11.94404905   0.35380003   0.53210002   0.52540004   0.58760005
    1.64448583]
 [-63.14246421   0.78750008   0.1961       0.8351       0.26680002
    0.58627617]][0m
[37m[1m[2023-07-16 22:49:23,024][257371] Max Reward on eval: 174.8546159664169[0m
[37m[1m[2023-07-16 22:49:23,025][257371] Min Reward on eval: -297.79015153376383[0m
[37m[1m[2023-07-16 22:49:23,025][257371] Mean Reward across all agents: -36.02898891455897[0m
[37m[1m[2023-07-16 22:49:23,025][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:49:23,029][257371] mean_value=-1208.462126572478, max_value=530.3468235574662[0m
[37m[1m[2023-07-16 22:49:23,032][257371] New mean coefficients: [[ 1.3116118  -1.5194973  -0.5016613   0.47817665  1.0306568  -2.2787943 ]][0m
[37m[1m[2023-07-16 22:49:23,033][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:49:31,974][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-16 22:49:31,979][257371] FPS: 429558.48[0m
[36m[2023-07-16 22:49:31,982][257371] itr=197, itrs=2000, Progress: 9.85%[0m
[36m[2023-07-16 22:49:43,605][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-16 22:49:43,605][257371] FPS: 331564.27[0m
[36m[2023-07-16 22:49:47,909][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:49:47,910][257371] Reward + Measures: [[25.90087962  0.062412    0.95691997  0.76026934  0.96292204  0.26097399]][0m
[37m[1m[2023-07-16 22:49:47,910][257371] Max Reward on eval: 25.900879616815878[0m
[37m[1m[2023-07-16 22:49:47,910][257371] Min Reward on eval: 25.900879616815878[0m
[37m[1m[2023-07-16 22:49:47,910][257371] Mean Reward across all agents: 25.900879616815878[0m
[37m[1m[2023-07-16 22:49:47,911][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:49:52,949][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:49:52,955][257371] Reward + Measures: [[ 30.22767438   0.4542       0.3348       0.56120002   0.38429996
    1.09582841]
 [-33.34963517   0.47790003   0.48909998   0.65080005   0.44279996
    1.26640046]
 [-74.07285376   0.48279998   0.4161       0.4876       0.43400002
    1.54718494]
 ...
 [ 55.57563354   0.51209998   0.51280004   0.56310004   0.57690001
    0.72701454]
 [ -1.31047163   0.67260003   0.24370001   0.63679999   0.48890001
    0.86879712]
 [152.78671264   0.4316       0.2467       0.53960001   0.34969997
    2.18877387]][0m
[37m[1m[2023-07-16 22:49:52,955][257371] Max Reward on eval: 320.5748415440321[0m
[37m[1m[2023-07-16 22:49:52,955][257371] Min Reward on eval: -126.48214322302957[0m
[37m[1m[2023-07-16 22:49:52,956][257371] Mean Reward across all agents: 57.262113679386616[0m
[37m[1m[2023-07-16 22:49:52,956][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:49:52,965][257371] mean_value=-429.67761339161444, max_value=820.5566148519516[0m
[37m[1m[2023-07-16 22:49:52,968][257371] New mean coefficients: [[ 2.2264116  -1.8855381   0.05771971  0.06547943  1.1583139  -1.549253  ]][0m
[37m[1m[2023-07-16 22:49:52,969][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:50:02,077][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-16 22:50:02,078][257371] FPS: 421673.85[0m
[36m[2023-07-16 22:50:02,080][257371] itr=198, itrs=2000, Progress: 9.90%[0m
[36m[2023-07-16 22:50:13,919][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-16 22:50:13,919][257371] FPS: 325446.87[0m
[36m[2023-07-16 22:50:18,287][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:50:18,287][257371] Reward + Measures: [[31.15024978  0.04469734  0.96595603  0.76848304  0.97033131  0.25065365]][0m
[37m[1m[2023-07-16 22:50:18,288][257371] Max Reward on eval: 31.15024977640452[0m
[37m[1m[2023-07-16 22:50:18,288][257371] Min Reward on eval: 31.15024977640452[0m
[37m[1m[2023-07-16 22:50:18,288][257371] Mean Reward across all agents: 31.15024977640452[0m
[37m[1m[2023-07-16 22:50:18,288][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:50:23,317][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:50:23,318][257371] Reward + Measures: [[ 73.02995014   0.3863       0.63239998   0.48359999   0.51010007
    0.80662155]
 [  6.9277595    0.16809998   0.20729999   0.19320001   0.31259999
    2.58863568]
 [-28.50450721   0.1062       0.17220001   0.147        0.19880001
    3.80427098]
 ...
 [-22.94753466   0.25020003   0.77420002   0.58840001   0.82080001
    0.88427651]
 [  0.22306812   0.11849999   0.2674       0.23720001   0.2915
    2.85073471]
 [  8.50931472   0.1929       0.30650002   0.21870001   0.37670001
    3.02072453]][0m
[37m[1m[2023-07-16 22:50:23,318][257371] Max Reward on eval: 379.26733779734934[0m
[37m[1m[2023-07-16 22:50:23,318][257371] Min Reward on eval: -162.17500501323084[0m
[37m[1m[2023-07-16 22:50:23,318][257371] Mean Reward across all agents: -7.500586141962486[0m
[37m[1m[2023-07-16 22:50:23,318][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:50:23,325][257371] mean_value=-1861.4220885233092, max_value=570.700149472761[0m
[37m[1m[2023-07-16 22:50:23,328][257371] New mean coefficients: [[ 0.53605056 -1.2009987   0.86141086  0.5986289   1.3571372  -0.34005713]][0m
[37m[1m[2023-07-16 22:50:23,329][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:50:32,386][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-16 22:50:32,386][257371] FPS: 424055.97[0m
[36m[2023-07-16 22:50:32,389][257371] itr=199, itrs=2000, Progress: 9.95%[0m
[36m[2023-07-16 22:50:44,063][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-16 22:50:44,064][257371] FPS: 330049.40[0m
[36m[2023-07-16 22:50:48,347][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:50:48,348][257371] Reward + Measures: [[42.52495619  0.04188633  0.97265631  0.76013005  0.97541296  0.27504998]][0m
[37m[1m[2023-07-16 22:50:48,348][257371] Max Reward on eval: 42.52495619287779[0m
[37m[1m[2023-07-16 22:50:48,348][257371] Min Reward on eval: 42.52495619287779[0m
[37m[1m[2023-07-16 22:50:48,349][257371] Mean Reward across all agents: 42.52495619287779[0m
[37m[1m[2023-07-16 22:50:48,349][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:50:53,353][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:50:53,354][257371] Reward + Measures: [[-138.45458362    0.11600001    0.88889998    0.75349998    0.82410002
     2.04486942]
 [  37.59440681    0.58760005    0.47869998    0.65210003    0.4797
     0.53424418]
 [ -51.54624126    0.19500001    0.6006        0.4598        0.50080007
     2.23301578]
 ...
 [ 137.98917479    0.51370001    0.3475        0.46170002    0.14050001
     1.80655134]
 [ 100.94808406    0.59850001    0.36670002    0.67530006    0.52960008
     1.00803602]
 [  56.3116779     0.40840003    0.0926        0.54370004    0.3046
     2.20644736]][0m
[37m[1m[2023-07-16 22:50:53,354][257371] Max Reward on eval: 341.23601530985906[0m
[37m[1m[2023-07-16 22:50:53,354][257371] Min Reward on eval: -183.9670648272382[0m
[37m[1m[2023-07-16 22:50:53,355][257371] Mean Reward across all agents: 30.978748348013358[0m
[37m[1m[2023-07-16 22:50:53,355][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:50:53,365][257371] mean_value=-631.7482421826292, max_value=760.0864147079201[0m
[37m[1m[2023-07-16 22:50:53,368][257371] New mean coefficients: [[ 1.3087744  -2.08579    -0.41995203  0.30822772  1.6257743   0.13883308]][0m
[37m[1m[2023-07-16 22:50:53,369][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:51:02,442][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 22:51:02,442][257371] FPS: 423324.28[0m
[36m[2023-07-16 22:51:02,444][257371] itr=200, itrs=2000, Progress: 10.00%[0m
[37m[1m[2023-07-16 22:53:18,005][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000180[0m
[36m[2023-07-16 22:53:30,378][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-16 22:53:30,378][257371] FPS: 323758.01[0m
[36m[2023-07-16 22:53:34,626][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:53:34,627][257371] Reward + Measures: [[54.96023217  0.027581    0.97737664  0.80249202  0.98163962  0.28354326]][0m
[37m[1m[2023-07-16 22:53:34,627][257371] Max Reward on eval: 54.960232171649004[0m
[37m[1m[2023-07-16 22:53:34,627][257371] Min Reward on eval: 54.960232171649004[0m
[37m[1m[2023-07-16 22:53:34,627][257371] Mean Reward across all agents: 54.960232171649004[0m
[37m[1m[2023-07-16 22:53:34,628][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:53:39,668][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:53:39,669][257371] Reward + Measures: [[-28.00655685   0.56479996   0.10259999   0.50050002   0.28819999
    3.84918857]
 [-34.76394267   0.67729998   0.37         0.61940002   0.4206
    1.02814579]
 [-19.79925196   0.1768       0.26280001   0.23120001   0.34550002
    2.59970403]
 ...
 [ 83.63837424   0.76380008   0.33560002   0.59460002   0.1542
    0.92474884]
 [-45.16973726   0.19860001   0.39820001   0.27190003   0.49309999
    1.83646894]
 [-69.30125211   0.48529997   0.42610002   0.42739996   0.4982
    1.59999347]][0m
[37m[1m[2023-07-16 22:53:39,669][257371] Max Reward on eval: 239.6674957672134[0m
[37m[1m[2023-07-16 22:53:39,670][257371] Min Reward on eval: -176.02867890899068[0m
[37m[1m[2023-07-16 22:53:39,670][257371] Mean Reward across all agents: 10.601963597288085[0m
[37m[1m[2023-07-16 22:53:39,670][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:53:39,680][257371] mean_value=-603.5936129075949, max_value=687.3171842210368[0m
[37m[1m[2023-07-16 22:53:39,682][257371] New mean coefficients: [[ 2.0071683  -1.9669893  -0.04265237  0.579512    1.6518495  -1.0926592 ]][0m
[37m[1m[2023-07-16 22:53:39,683][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:53:48,785][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-16 22:53:48,785][257371] FPS: 421970.92[0m
[36m[2023-07-16 22:53:48,788][257371] itr=201, itrs=2000, Progress: 10.05%[0m
[36m[2023-07-16 22:54:00,382][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-16 22:54:00,383][257371] FPS: 332333.99[0m
[36m[2023-07-16 22:54:04,584][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:54:04,584][257371] Reward + Measures: [[63.17517827  0.01648933  0.98325729  0.85158902  0.98747736  0.26291493]][0m
[37m[1m[2023-07-16 22:54:04,584][257371] Max Reward on eval: 63.17517826997589[0m
[37m[1m[2023-07-16 22:54:04,584][257371] Min Reward on eval: 63.17517826997589[0m
[37m[1m[2023-07-16 22:54:04,585][257371] Mean Reward across all agents: 63.17517826997589[0m
[37m[1m[2023-07-16 22:54:04,585][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:54:09,734][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:54:09,734][257371] Reward + Measures: [[ -12.83224156    0.64600003    0.18430001    0.59040004    0.37919998
     1.0761441 ]
 [  91.53040194    0.36049998    0.1751        0.4341        0.23480001
     1.98487973]
 [ 192.99466156    0.3202        0.0995        0.31670001    0.1901
     3.44265985]
 ...
 [  25.03776895    0.7044        0.24489999    0.69670004    0.26920003
     0.96022809]
 [  25.00576373    0.39590001    0.81310004    0.61659998    0.4298
     1.10631442]
 [-104.54391763    0.57319999    0.46029997    0.583         0.38510001
     1.25754297]][0m
[37m[1m[2023-07-16 22:54:09,735][257371] Max Reward on eval: 283.55224325060846[0m
[37m[1m[2023-07-16 22:54:09,735][257371] Min Reward on eval: -275.40268969815224[0m
[37m[1m[2023-07-16 22:54:09,735][257371] Mean Reward across all agents: 36.42680053885131[0m
[37m[1m[2023-07-16 22:54:09,735][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:54:09,746][257371] mean_value=-826.7489932852692, max_value=498.7681310931966[0m
[37m[1m[2023-07-16 22:54:09,749][257371] New mean coefficients: [[ 1.1615975  -1.4789263  -0.01540064  0.22266617  1.8421633  -1.2888348 ]][0m
[37m[1m[2023-07-16 22:54:09,750][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:54:18,858][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-16 22:54:18,858][257371] FPS: 421692.22[0m
[36m[2023-07-16 22:54:18,860][257371] itr=202, itrs=2000, Progress: 10.10%[0m
[36m[2023-07-16 22:54:30,947][257371] train() took 12.04 seconds to complete[0m
[36m[2023-07-16 22:54:30,947][257371] FPS: 318851.09[0m
[36m[2023-07-16 22:54:35,360][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:54:35,360][257371] Reward + Measures: [[48.86925662  0.01633067  0.9548924   0.77365029  0.97145993  0.37599629]][0m
[37m[1m[2023-07-16 22:54:35,361][257371] Max Reward on eval: 48.86925661993605[0m
[37m[1m[2023-07-16 22:54:35,361][257371] Min Reward on eval: 48.86925661993605[0m
[37m[1m[2023-07-16 22:54:35,361][257371] Mean Reward across all agents: 48.86925661993605[0m
[37m[1m[2023-07-16 22:54:35,361][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:54:40,473][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:54:40,474][257371] Reward + Measures: [[37.78103759  0.2142      0.42139998  0.3001      0.43709999  2.34986043]
 [22.02372084  0.30230001  0.28040001  0.29750001  0.33669999  2.81084704]
 [43.31798168  0.23479998  0.32930002  0.21350001  0.37010002  3.56736755]
 ...
 [28.92789888  0.22280002  0.30609998  0.25120002  0.29070002  3.0042026 ]
 [-5.18358913  0.1631      0.17730001  0.15820001  0.23339999  4.42531395]
 [51.64680125  0.27040002  0.80699998  0.72720003  0.93350011  0.57020807]][0m
[37m[1m[2023-07-16 22:54:40,474][257371] Max Reward on eval: 202.41956137532833[0m
[37m[1m[2023-07-16 22:54:40,474][257371] Min Reward on eval: -141.3378281504847[0m
[37m[1m[2023-07-16 22:54:40,474][257371] Mean Reward across all agents: 19.12016423974051[0m
[37m[1m[2023-07-16 22:54:40,475][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:54:40,483][257371] mean_value=-1042.887781614221, max_value=465.92887593083196[0m
[37m[1m[2023-07-16 22:54:40,486][257371] New mean coefficients: [[ 0.9331393  -1.7625244  -0.02600501 -0.12787911  2.0333242  -1.7519543 ]][0m
[37m[1m[2023-07-16 22:54:40,487][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:54:49,592][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-16 22:54:49,592][257371] FPS: 421842.35[0m
[36m[2023-07-16 22:54:49,594][257371] itr=203, itrs=2000, Progress: 10.15%[0m
[36m[2023-07-16 22:55:01,442][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-16 22:55:01,442][257371] FPS: 325247.24[0m
[36m[2023-07-16 22:55:05,684][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:55:05,684][257371] Reward + Measures: [[78.55814511  0.02299333  0.97344238  0.6379723   0.95415664  0.64023077]][0m
[37m[1m[2023-07-16 22:55:05,685][257371] Max Reward on eval: 78.55814510831134[0m
[37m[1m[2023-07-16 22:55:05,685][257371] Min Reward on eval: 78.55814510831134[0m
[37m[1m[2023-07-16 22:55:05,685][257371] Mean Reward across all agents: 78.55814510831134[0m
[37m[1m[2023-07-16 22:55:05,685][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:55:10,736][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:55:10,736][257371] Reward + Measures: [[-134.64521548    0.51609999    0.5165        0.6135        0.38650003
     2.708781  ]
 [ -65.00969717    0.38189998    0.26280001    0.49180004    0.32089999
     2.67651248]
 [  38.12986888    0.43759999    0.54360002    0.62540001    0.46170002
     1.20172977]
 ...
 [-121.91089084    0.32189998    0.45440003    0.44670001    0.42360002
     3.52779818]
 [  15.56175087    0.1822        0.51389998    0.45439997    0.50760001
     2.54937935]
 [   2.00739642    0.36110002    0.4192        0.47919998    0.29620001
     2.6063199 ]][0m
[37m[1m[2023-07-16 22:55:10,736][257371] Max Reward on eval: 240.80005266610533[0m
[37m[1m[2023-07-16 22:55:10,736][257371] Min Reward on eval: -212.40102769117803[0m
[37m[1m[2023-07-16 22:55:10,737][257371] Mean Reward across all agents: 6.873750422487783[0m
[37m[1m[2023-07-16 22:55:10,737][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:55:10,747][257371] mean_value=-1006.9920679853368, max_value=605.7239850287606[0m
[37m[1m[2023-07-16 22:55:10,750][257371] New mean coefficients: [[ 0.8831624  -1.7417176  -0.04006289  0.144072    0.8689364  -1.552511  ]][0m
[37m[1m[2023-07-16 22:55:10,751][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:55:19,795][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 22:55:19,796][257371] FPS: 424635.64[0m
[36m[2023-07-16 22:55:19,798][257371] itr=204, itrs=2000, Progress: 10.20%[0m
[36m[2023-07-16 22:55:31,497][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-16 22:55:31,497][257371] FPS: 329450.45[0m
[36m[2023-07-16 22:55:35,840][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:55:35,841][257371] Reward + Measures: [[175.68312981   0.39240998   0.58293664   0.66259336   0.37061337
    0.61452824]][0m
[37m[1m[2023-07-16 22:55:35,841][257371] Max Reward on eval: 175.68312981475603[0m
[37m[1m[2023-07-16 22:55:35,841][257371] Min Reward on eval: 175.68312981475603[0m
[37m[1m[2023-07-16 22:55:35,841][257371] Mean Reward across all agents: 175.68312981475603[0m
[37m[1m[2023-07-16 22:55:35,842][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:55:40,924][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:55:40,925][257371] Reward + Measures: [[ 63.47799228   0.505        0.50349998   0.68529999   0.57890004
    0.6907627 ]
 [ 38.38797543   0.39590001   0.47060004   0.53890002   0.22790001
    1.23818588]
 [ 69.57095717   0.49630004   0.52090001   0.54089999   0.37200001
    1.6524452 ]
 ...
 [ 25.16785089   0.51780003   0.7306       0.5467       0.64840001
    0.94276661]
 [-14.76586533   0.15470001   0.85690004   0.79550004   0.86240005
    0.92575186]
 [-63.60291996   0.45889997   0.33989999   0.49280006   0.1918
    2.2159822 ]][0m
[37m[1m[2023-07-16 22:55:40,925][257371] Max Reward on eval: 278.6548175850883[0m
[37m[1m[2023-07-16 22:55:40,926][257371] Min Reward on eval: -148.91165678631515[0m
[37m[1m[2023-07-16 22:55:40,926][257371] Mean Reward across all agents: 45.86702174992728[0m
[37m[1m[2023-07-16 22:55:40,926][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:55:40,938][257371] mean_value=-298.74780443935913, max_value=517.7184213603703[0m
[37m[1m[2023-07-16 22:55:40,940][257371] New mean coefficients: [[ 0.40437475 -1.9593322   0.22029977  0.25185865  0.84920996 -1.5025504 ]][0m
[37m[1m[2023-07-16 22:55:40,941][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:55:50,094][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-16 22:55:50,094][257371] FPS: 419617.15[0m
[36m[2023-07-16 22:55:50,096][257371] itr=205, itrs=2000, Progress: 10.25%[0m
[36m[2023-07-16 22:56:01,846][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-16 22:56:01,846][257371] FPS: 327987.18[0m
[36m[2023-07-16 22:56:06,230][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:56:06,230][257371] Reward + Measures: [[177.7551884    0.09925      0.8644973    0.44618797   0.77379769
    0.61203355]][0m
[37m[1m[2023-07-16 22:56:06,231][257371] Max Reward on eval: 177.75518839877932[0m
[37m[1m[2023-07-16 22:56:06,231][257371] Min Reward on eval: 177.75518839877932[0m
[37m[1m[2023-07-16 22:56:06,231][257371] Mean Reward across all agents: 177.75518839877932[0m
[37m[1m[2023-07-16 22:56:06,231][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:56:11,294][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:56:11,295][257371] Reward + Measures: [[192.34465141   0.47259998   0.57100004   0.50060004   0.23409998
    1.06734288]
 [ -0.83070899   0.17110001   0.20180002   0.21430002   0.20299999
    3.51266289]
 [ 74.78760833   0.4289       0.35859999   0.37170002   0.13460001
    1.79713023]
 ...
 [154.90915107   0.45580003   0.76820004   0.8064       0.5133
    2.53167057]
 [  2.50504921   0.13060001   0.20680001   0.1559       0.1393
    4.74047852]
 [ -8.00400918   0.49520001   0.51389998   0.5948       0.39640003
    1.90113163]][0m
[37m[1m[2023-07-16 22:56:11,295][257371] Max Reward on eval: 360.4352474450134[0m
[37m[1m[2023-07-16 22:56:11,295][257371] Min Reward on eval: -145.7601370839402[0m
[37m[1m[2023-07-16 22:56:11,295][257371] Mean Reward across all agents: 59.42826484932975[0m
[37m[1m[2023-07-16 22:56:11,295][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:56:11,313][257371] mean_value=-396.44514877999694, max_value=815.8340187129681[0m
[37m[1m[2023-07-16 22:56:11,316][257371] New mean coefficients: [[ 0.25764504 -1.9590375  -0.35016066  0.02566519  1.6591109  -3.187167  ]][0m
[37m[1m[2023-07-16 22:56:11,317][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:56:20,414][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-16 22:56:20,414][257371] FPS: 422178.24[0m
[36m[2023-07-16 22:56:20,417][257371] itr=206, itrs=2000, Progress: 10.30%[0m
[36m[2023-07-16 22:56:32,170][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-16 22:56:32,170][257371] FPS: 327996.36[0m
[36m[2023-07-16 22:56:36,504][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:56:36,505][257371] Reward + Measures: [[48.26730069  0.03227933  0.86362165  0.5536983   0.85044271  0.58559757]][0m
[37m[1m[2023-07-16 22:56:36,505][257371] Max Reward on eval: 48.26730069400417[0m
[37m[1m[2023-07-16 22:56:36,505][257371] Min Reward on eval: 48.26730069400417[0m
[37m[1m[2023-07-16 22:56:36,505][257371] Mean Reward across all agents: 48.26730069400417[0m
[37m[1m[2023-07-16 22:56:36,505][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:56:41,686][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:56:41,687][257371] Reward + Measures: [[ 40.34756579   0.3716       0.27009997   0.32689998   0.2309
    3.26962137]
 [ 57.79288196   0.52459997   0.42980003   0.59970003   0.4104
    1.71358204]
 [-69.69123888   0.1737       0.23510002   0.19240001   0.19719999
    4.10412931]
 ...
 [ 99.52602997   0.38530001   0.34980002   0.4179       0.2225
    2.94465137]
 [ 70.77815437   0.1969       0.43630001   0.34940001   0.40159997
    3.81826067]
 [152.11626387   0.35279998   0.52910006   0.2089       0.44569999
    2.37335038]][0m
[37m[1m[2023-07-16 22:56:41,687][257371] Max Reward on eval: 264.56157560567374[0m
[37m[1m[2023-07-16 22:56:41,687][257371] Min Reward on eval: -292.1871490271762[0m
[37m[1m[2023-07-16 22:56:41,687][257371] Mean Reward across all agents: 32.521251669215935[0m
[37m[1m[2023-07-16 22:56:41,688][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:56:41,694][257371] mean_value=-1210.3036645171417, max_value=450.4531510605042[0m
[37m[1m[2023-07-16 22:56:41,697][257371] New mean coefficients: [[-0.41242406 -1.575995    0.49518394  0.127974    1.8984264  -3.4750183 ]][0m
[37m[1m[2023-07-16 22:56:41,698][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:56:50,751][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 22:56:50,752][257371] FPS: 424214.52[0m
[36m[2023-07-16 22:56:50,754][257371] itr=207, itrs=2000, Progress: 10.35%[0m
[36m[2023-07-16 22:57:02,617][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-16 22:57:02,617][257371] FPS: 324845.62[0m
[36m[2023-07-16 22:57:07,002][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:57:07,002][257371] Reward + Measures: [[11.86655365  0.48503229  0.68546629  0.59411067  0.65771103  0.40365615]][0m
[37m[1m[2023-07-16 22:57:07,003][257371] Max Reward on eval: 11.8665536539671[0m
[37m[1m[2023-07-16 22:57:07,003][257371] Min Reward on eval: 11.8665536539671[0m
[37m[1m[2023-07-16 22:57:07,003][257371] Mean Reward across all agents: 11.8665536539671[0m
[37m[1m[2023-07-16 22:57:07,003][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:57:12,085][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:57:12,085][257371] Reward + Measures: [[ 99.70217569   0.37630001   0.3224       0.3346       0.27160001
    2.93542647]
 [-66.56997497   0.32730001   0.25800002   0.35660002   0.2375
    2.67182326]
 [ 15.14377177   0.4014       0.29920003   0.44790003   0.29449996
    2.01679206]
 ...
 [-76.67269851   0.15100001   0.81700003   0.648        0.69130003
    0.97752172]
 [ 59.16983216   0.42459998   0.39519998   0.32820001   0.29049999
    2.7454915 ]
 [  5.13642086   0.24440001   0.19499999   0.27040002   0.1426
    2.84333158]][0m
[37m[1m[2023-07-16 22:57:12,085][257371] Max Reward on eval: 198.40652610654942[0m
[37m[1m[2023-07-16 22:57:12,086][257371] Min Reward on eval: -224.71926128529012[0m
[37m[1m[2023-07-16 22:57:12,086][257371] Mean Reward across all agents: 23.99413531750248[0m
[37m[1m[2023-07-16 22:57:12,086][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:57:12,092][257371] mean_value=-1239.3078757973597, max_value=547.8479822061024[0m
[37m[1m[2023-07-16 22:57:12,095][257371] New mean coefficients: [[-0.07331958 -0.49240696  0.31495768 -0.13052064  2.9974704  -3.4630053 ]][0m
[37m[1m[2023-07-16 22:57:12,096][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:57:21,240][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-16 22:57:21,240][257371] FPS: 420010.74[0m
[36m[2023-07-16 22:57:21,243][257371] itr=208, itrs=2000, Progress: 10.40%[0m
[36m[2023-07-16 22:57:33,013][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-16 22:57:33,013][257371] FPS: 327538.98[0m
[36m[2023-07-16 22:57:37,369][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:57:37,369][257371] Reward + Measures: [[0.26310276 0.83297795 0.26465365 0.80981165 0.47577065 0.51475716]][0m
[37m[1m[2023-07-16 22:57:37,370][257371] Max Reward on eval: 0.2631027592072302[0m
[37m[1m[2023-07-16 22:57:37,370][257371] Min Reward on eval: 0.2631027592072302[0m
[37m[1m[2023-07-16 22:57:37,370][257371] Mean Reward across all agents: 0.2631027592072302[0m
[37m[1m[2023-07-16 22:57:37,370][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:57:42,452][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:57:42,452][257371] Reward + Measures: [[ -57.15989881    0.20819998    0.1559        0.1451        0.20009999
     3.76059031]
 [-140.26819413    0.20780002    0.20809999    0.13020001    0.25670001
     5.22787333]
 [  40.42446318    0.23709999    0.23040001    0.18970001    0.27090001
     3.16457033]
 ...
 [  34.85185669    0.3917        0.42379999    0.3836        0.47259998
     2.33206511]
 [  15.0964632     0.48839998    0.4941        0.4409        0.54299998
     1.47014832]
 [  46.85674583    0.58490002    0.2422        0.57739997    0.38930002
     1.6942122 ]][0m
[37m[1m[2023-07-16 22:57:42,452][257371] Max Reward on eval: 133.44351059782784[0m
[37m[1m[2023-07-16 22:57:42,453][257371] Min Reward on eval: -273.163406869024[0m
[37m[1m[2023-07-16 22:57:42,453][257371] Mean Reward across all agents: -17.358025880412995[0m
[37m[1m[2023-07-16 22:57:42,453][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:57:42,456][257371] mean_value=-2609.4063501149094, max_value=433.7515341523169[0m
[37m[1m[2023-07-16 22:57:42,459][257371] New mean coefficients: [[-0.6417061  -0.37530681 -0.07170001  0.03848465  2.1122577  -2.266397  ]][0m
[37m[1m[2023-07-16 22:57:42,460][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:57:51,574][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-16 22:57:51,575][257371] FPS: 421395.08[0m
[36m[2023-07-16 22:57:51,577][257371] itr=209, itrs=2000, Progress: 10.45%[0m
[36m[2023-07-16 22:58:03,328][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-16 22:58:03,328][257371] FPS: 328071.67[0m
[36m[2023-07-16 22:58:07,664][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:58:07,664][257371] Reward + Measures: [[-54.11103277   0.49153599   0.81566697   0.55870932   0.66798502
    0.38679916]][0m
[37m[1m[2023-07-16 22:58:07,665][257371] Max Reward on eval: -54.11103276513501[0m
[37m[1m[2023-07-16 22:58:07,665][257371] Min Reward on eval: -54.11103276513501[0m
[37m[1m[2023-07-16 22:58:07,665][257371] Mean Reward across all agents: -54.11103276513501[0m
[37m[1m[2023-07-16 22:58:07,665][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:58:12,697][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 22:58:12,697][257371] Reward + Measures: [[ 35.27136775   0.71450001   0.69859999   0.71940005   0.0684
    3.07562137]
 [192.79338074   0.228        0.66889995   0.33930001   0.56380004
    2.05425525]
 [102.80006053   0.30880001   0.48930001   0.52160001   0.50580001
    0.86063796]
 ...
 [ 25.80413095   0.47670004   0.68529999   0.61610001   0.50400001
    0.72592449]
 [138.21180101   0.29730001   0.59899998   0.322        0.51460004
    0.9630931 ]
 [155.14733026   0.42820001   0.45809999   0.36429998   0.37450001
    1.27975082]][0m
[37m[1m[2023-07-16 22:58:12,698][257371] Max Reward on eval: 297.8430323276669[0m
[37m[1m[2023-07-16 22:58:12,698][257371] Min Reward on eval: -175.52927114199846[0m
[37m[1m[2023-07-16 22:58:12,698][257371] Mean Reward across all agents: -5.144717646076024[0m
[37m[1m[2023-07-16 22:58:12,698][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 22:58:12,707][257371] mean_value=-629.3743165210518, max_value=622.7148609175813[0m
[37m[1m[2023-07-16 22:58:12,710][257371] New mean coefficients: [[-1.0177041  -0.2928148   0.19031104  0.2827897   2.408872   -2.0946789 ]][0m
[37m[1m[2023-07-16 22:58:12,711][257371] Moving the mean solution point...[0m
[36m[2023-07-16 22:58:21,752][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 22:58:21,752][257371] FPS: 424811.48[0m
[36m[2023-07-16 22:58:21,755][257371] itr=210, itrs=2000, Progress: 10.50%[0m
[37m[1m[2023-07-16 23:00:33,213][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000190[0m
[36m[2023-07-16 23:00:45,704][257371] train() took 11.91 seconds to complete[0m
[36m[2023-07-16 23:00:45,704][257371] FPS: 322339.06[0m
[36m[2023-07-16 23:00:49,976][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:00:49,976][257371] Reward + Measures: [[15.8282875   0.70546395  0.31479165  0.73604369  0.67652935  0.42131034]][0m
[37m[1m[2023-07-16 23:00:49,976][257371] Max Reward on eval: 15.828287497822647[0m
[37m[1m[2023-07-16 23:00:49,976][257371] Min Reward on eval: 15.828287497822647[0m
[37m[1m[2023-07-16 23:00:49,977][257371] Mean Reward across all agents: 15.828287497822647[0m
[37m[1m[2023-07-16 23:00:49,977][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:00:54,997][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:00:55,099][257371] Reward + Measures: [[127.68614008   0.35020003   0.42490003   0.34619999   0.3795
    1.96807921]
 [ 45.69442971   0.3757       0.2538       0.3766       0.33180004
    2.43186665]
 [ 77.25954792   0.75579995   0.24749999   0.64360005   0.26449999
    0.71193951]
 ...
 [ 17.04253747   0.94609994   0.0075       0.77609998   0.75699997
    1.66011739]
 [-18.62985841   0.303        0.34770003   0.26939997   0.39320001
    2.33692813]
 [ 92.7359686    0.39100003   0.51240003   0.3863       0.56150001
    0.77799499]][0m
[37m[1m[2023-07-16 23:00:55,100][257371] Max Reward on eval: 367.23800562135875[0m
[37m[1m[2023-07-16 23:00:55,100][257371] Min Reward on eval: -120.2650115320459[0m
[37m[1m[2023-07-16 23:00:55,101][257371] Mean Reward across all agents: 34.83402865594499[0m
[37m[1m[2023-07-16 23:00:55,101][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:00:55,133][257371] mean_value=-606.8071375235688, max_value=516.8417297772364[0m
[37m[1m[2023-07-16 23:00:55,139][257371] New mean coefficients: [[-0.27377725 -0.15792227  0.47796413  0.08757629  2.0524912  -3.0702095 ]][0m
[37m[1m[2023-07-16 23:00:55,141][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:01:04,051][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-16 23:01:04,051][257371] FPS: 431056.87[0m
[36m[2023-07-16 23:01:04,054][257371] itr=211, itrs=2000, Progress: 10.55%[0m
[36m[2023-07-16 23:01:15,879][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-16 23:01:15,879][257371] FPS: 326057.59[0m
[36m[2023-07-16 23:01:20,225][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:01:20,225][257371] Reward + Measures: [[16.53534534  0.31089965  0.81215495  0.33273032  0.75505406  0.4125559 ]][0m
[37m[1m[2023-07-16 23:01:20,225][257371] Max Reward on eval: 16.535345344328498[0m
[37m[1m[2023-07-16 23:01:20,225][257371] Min Reward on eval: 16.535345344328498[0m
[37m[1m[2023-07-16 23:01:20,226][257371] Mean Reward across all agents: 16.535345344328498[0m
[37m[1m[2023-07-16 23:01:20,226][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:01:25,313][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:01:25,319][257371] Reward + Measures: [[44.91726332  0.20420001  0.28299999  0.24500003  0.2577      3.9360218 ]
 [19.73702469  0.317       0.70419997  0.58640003  0.58789998  0.85209101]
 [ 9.36683904  0.21270001  0.27300003  0.21140002  0.2392      4.01146317]
 ...
 [56.81466199  0.21710001  0.28480002  0.26890001  0.2481      3.88747597]
 [99.45805644  0.22839999  0.67889994  0.41869998  0.68600005  1.23265445]
 [43.14802971  0.27610001  0.69520009  0.49580002  0.65350002  1.12130988]][0m
[37m[1m[2023-07-16 23:01:25,319][257371] Max Reward on eval: 434.26561652515085[0m
[37m[1m[2023-07-16 23:01:25,320][257371] Min Reward on eval: -184.63621709991713[0m
[37m[1m[2023-07-16 23:01:25,320][257371] Mean Reward across all agents: 70.47472552738863[0m
[37m[1m[2023-07-16 23:01:25,320][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:01:25,335][257371] mean_value=-311.1331836948644, max_value=898.3273406403372[0m
[37m[1m[2023-07-16 23:01:25,338][257371] New mean coefficients: [[ 1.1469142   0.74288094  1.392247    0.20666969  2.0140817  -4.363204  ]][0m
[37m[1m[2023-07-16 23:01:25,339][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:01:34,506][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-16 23:01:34,507][257371] FPS: 418942.33[0m
[36m[2023-07-16 23:01:34,509][257371] itr=212, itrs=2000, Progress: 10.60%[0m
[36m[2023-07-16 23:01:46,616][257371] train() took 12.06 seconds to complete[0m
[36m[2023-07-16 23:01:46,617][257371] FPS: 318438.13[0m
[36m[2023-07-16 23:01:51,009][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:01:51,009][257371] Reward + Measures: [[71.72011261  0.06138033  0.931898    0.46948999  0.83500797  0.65622526]][0m
[37m[1m[2023-07-16 23:01:51,009][257371] Max Reward on eval: 71.7201126075758[0m
[37m[1m[2023-07-16 23:01:51,009][257371] Min Reward on eval: 71.7201126075758[0m
[37m[1m[2023-07-16 23:01:51,010][257371] Mean Reward across all agents: 71.7201126075758[0m
[37m[1m[2023-07-16 23:01:51,010][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:01:56,058][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:01:56,058][257371] Reward + Measures: [[ -67.79855014    0.21669999    0.38319999    0.2139        0.3928
     1.89339089]
 [   9.07575765    0.36150002    0.5636        0.2393        0.53730005
     1.8191334 ]
 [-101.15542449    0.29270002    0.35209998    0.23940001    0.26909998
     2.42797947]
 ...
 [  34.62483663    0.32369998    0.58039999    0.2182        0.45409998
     1.55836284]
 [ -39.0900144     0.45210001    0.6322        0.47319999    0.59829998
     0.83204621]
 [ -79.48493522    0.31840003    0.60569996    0.2455        0.56389999
     1.80210304]][0m
[37m[1m[2023-07-16 23:01:56,059][257371] Max Reward on eval: 100.95847889762372[0m
[37m[1m[2023-07-16 23:01:56,059][257371] Min Reward on eval: -180.74143051584832[0m
[37m[1m[2023-07-16 23:01:56,059][257371] Mean Reward across all agents: -32.81331083382408[0m
[37m[1m[2023-07-16 23:01:56,059][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:01:56,067][257371] mean_value=-1663.964849495841, max_value=522.2059849172365[0m
[37m[1m[2023-07-16 23:01:56,070][257371] New mean coefficients: [[ 0.13635957  1.091644    2.0751834   0.6598255   1.6868138  -3.0114806 ]][0m
[37m[1m[2023-07-16 23:01:56,071][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:02:05,054][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-16 23:02:05,055][257371] FPS: 427521.56[0m
[36m[2023-07-16 23:02:05,057][257371] itr=213, itrs=2000, Progress: 10.65%[0m
[36m[2023-07-16 23:02:16,773][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-16 23:02:16,773][257371] FPS: 329096.94[0m
[36m[2023-07-16 23:02:20,998][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:02:20,998][257371] Reward + Measures: [[26.08337969  0.49408403  0.84838998  0.64215398  0.73670197  0.39907601]][0m
[37m[1m[2023-07-16 23:02:20,999][257371] Max Reward on eval: 26.08337968660558[0m
[37m[1m[2023-07-16 23:02:20,999][257371] Min Reward on eval: 26.08337968660558[0m
[37m[1m[2023-07-16 23:02:20,999][257371] Mean Reward across all agents: 26.08337968660558[0m
[37m[1m[2023-07-16 23:02:20,999][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:02:26,041][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:02:26,047][257371] Reward + Measures: [[-45.41229219   0.57010001   0.285        0.45879999   0.38260001
    2.12240028]
 [-96.06816384   0.46029997   0.52789998   0.37400001   0.59750003
    0.78844196]
 [ 16.1172845    0.76830006   0.8028       0.8118       0.78900003
    1.79631317]
 ...
 [-79.34380983   0.49759999   0.62199998   0.61059999   0.36680001
    0.5651859 ]
 [ 22.58577803   0.32119998   0.51389998   0.32840002   0.48899999
    2.84990549]
 [-83.61370602   0.54290003   0.40880004   0.53670001   0.42500001
    1.03925216]][0m
[37m[1m[2023-07-16 23:02:26,047][257371] Max Reward on eval: 226.76039410885423[0m
[37m[1m[2023-07-16 23:02:26,048][257371] Min Reward on eval: -144.40027672136202[0m
[37m[1m[2023-07-16 23:02:26,048][257371] Mean Reward across all agents: 11.424182190532804[0m
[37m[1m[2023-07-16 23:02:26,048][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:02:26,055][257371] mean_value=-481.229565992503, max_value=469.7773260631682[0m
[37m[1m[2023-07-16 23:02:26,058][257371] New mean coefficients: [[ 0.500087    1.9983313   2.220565    0.55922663  1.1891237  -3.454759  ]][0m
[37m[1m[2023-07-16 23:02:26,059][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:02:35,104][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 23:02:35,105][257371] FPS: 424617.37[0m
[36m[2023-07-16 23:02:35,107][257371] itr=214, itrs=2000, Progress: 10.70%[0m
[36m[2023-07-16 23:02:46,724][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-16 23:02:46,724][257371] FPS: 331887.53[0m
[36m[2023-07-16 23:02:51,018][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:02:51,019][257371] Reward + Measures: [[-13.58758297   0.93834829   0.99319935   0.00771933   0.97980791
    0.73096937]][0m
[37m[1m[2023-07-16 23:02:51,019][257371] Max Reward on eval: -13.587582970038827[0m
[37m[1m[2023-07-16 23:02:51,019][257371] Min Reward on eval: -13.587582970038827[0m
[37m[1m[2023-07-16 23:02:51,020][257371] Mean Reward across all agents: -13.587582970038827[0m
[37m[1m[2023-07-16 23:02:51,020][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:02:56,047][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:02:56,047][257371] Reward + Measures: [[  26.14342766    0.48110005    0.26500002    0.46950004    0.2694
     1.61979139]
 [-117.41029233    0.3118        0.2746        0.31010002    0.24200001
     2.5461421 ]
 [ -26.44833704    0.59219998    0.57210004    0.48670003    0.37130001
     1.79758108]
 ...
 [ -62.18766703    0.50050002    0.57020003    0.50860006    0.59369999
     0.65543509]
 [ -97.96083069    0.1842        0.1332        0.1639        0.16510001
     3.78526855]
 [ -10.831839      0.4499        0.45469999    0.3788        0.36030003
     1.55419731]][0m
[37m[1m[2023-07-16 23:02:56,048][257371] Max Reward on eval: 192.20317080098903[0m
[37m[1m[2023-07-16 23:02:56,048][257371] Min Reward on eval: -212.56415748533328[0m
[37m[1m[2023-07-16 23:02:56,048][257371] Mean Reward across all agents: -32.93758225122642[0m
[37m[1m[2023-07-16 23:02:56,048][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:02:56,053][257371] mean_value=-1671.872362335382, max_value=600.5839280940593[0m
[37m[1m[2023-07-16 23:02:56,056][257371] New mean coefficients: [[ 1.486521    0.44387603  1.7140912   0.07528424  0.69584453 -2.720598  ]][0m
[37m[1m[2023-07-16 23:02:56,057][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:03:05,134][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 23:03:05,135][257371] FPS: 423080.59[0m
[36m[2023-07-16 23:03:05,137][257371] itr=215, itrs=2000, Progress: 10.75%[0m
[36m[2023-07-16 23:03:16,906][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-16 23:03:16,906][257371] FPS: 327536.26[0m
[36m[2023-07-16 23:03:21,200][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:03:21,201][257371] Reward + Measures: [[12.36133133  0.40649101  0.798666    0.51324332  0.61990899  0.4351626 ]][0m
[37m[1m[2023-07-16 23:03:21,201][257371] Max Reward on eval: 12.361331328252975[0m
[37m[1m[2023-07-16 23:03:21,201][257371] Min Reward on eval: 12.361331328252975[0m
[37m[1m[2023-07-16 23:03:21,201][257371] Mean Reward across all agents: 12.361331328252975[0m
[37m[1m[2023-07-16 23:03:21,202][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:03:26,417][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:03:26,418][257371] Reward + Measures: [[ 59.42805625   0.5323       0.70649999   0.61140001   0.46120006
    0.76968127]
 [  8.29723998   0.4901       0.51950008   0.5284       0.30840001
    1.67995036]
 [-27.33350375   0.20510001   0.57120001   0.52600002   0.64139998
    1.60667837]
 ...
 [-28.7059441    0.22480002   0.37909999   0.28800002   0.47440001
    1.90682638]
 [ 63.81901221   0.54319996   0.48710003   0.63590002   0.25680003
    1.41218984]
 [ 90.05170968   0.2685       0.27690002   0.31710002   0.3513
    2.31807399]][0m
[37m[1m[2023-07-16 23:03:26,418][257371] Max Reward on eval: 394.55716130724176[0m
[37m[1m[2023-07-16 23:03:26,418][257371] Min Reward on eval: -304.4999876173679[0m
[37m[1m[2023-07-16 23:03:26,418][257371] Mean Reward across all agents: 48.6096646940331[0m
[37m[1m[2023-07-16 23:03:26,419][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:03:26,424][257371] mean_value=-865.594669797933, max_value=557.8295261666178[0m
[37m[1m[2023-07-16 23:03:26,427][257371] New mean coefficients: [[ 1.261188   -0.45943153  1.8018953   0.5391505   0.51661325 -1.1919225 ]][0m
[37m[1m[2023-07-16 23:03:26,428][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:03:35,394][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-16 23:03:35,394][257371] FPS: 428338.92[0m
[36m[2023-07-16 23:03:35,397][257371] itr=216, itrs=2000, Progress: 10.80%[0m
[36m[2023-07-16 23:03:47,042][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-16 23:03:47,042][257371] FPS: 331114.46[0m
[36m[2023-07-16 23:03:51,346][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:03:51,346][257371] Reward + Measures: [[-3.91661713  0.42527664  0.70660871  0.58596766  0.55547732  0.5114339 ]][0m
[37m[1m[2023-07-16 23:03:51,346][257371] Max Reward on eval: -3.916617134808839[0m
[37m[1m[2023-07-16 23:03:51,347][257371] Min Reward on eval: -3.916617134808839[0m
[37m[1m[2023-07-16 23:03:51,347][257371] Mean Reward across all agents: -3.916617134808839[0m
[37m[1m[2023-07-16 23:03:51,347][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:03:56,436][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:03:56,437][257371] Reward + Measures: [[-51.70041121   0.51809996   0.56750005   0.48569998   0.3405
    1.41084194]
 [ 28.55785331   0.52530003   0.51739997   0.5808       0.20580001
    1.04231501]
 [ -5.76652739   0.23190001   0.32559997   0.35960001   0.36989999
    2.52523685]
 ...
 [-12.10920077   0.63220006   0.65889996   0.73989999   0.55150002
    0.46295258]
 [ 70.94496225   0.29299998   0.58149999   0.41360003   0.56019998
    1.43178558]
 [ 19.10625317   0.22870003   0.6024       0.34400001   0.72490007
    0.9989683 ]][0m
[37m[1m[2023-07-16 23:03:56,437][257371] Max Reward on eval: 284.323968918249[0m
[37m[1m[2023-07-16 23:03:56,437][257371] Min Reward on eval: -217.96437456286512[0m
[37m[1m[2023-07-16 23:03:56,438][257371] Mean Reward across all agents: 21.735764302744954[0m
[37m[1m[2023-07-16 23:03:56,438][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:03:56,445][257371] mean_value=-438.8969008662821, max_value=442.78995327788874[0m
[37m[1m[2023-07-16 23:03:56,448][257371] New mean coefficients: [[ 2.0567384  -1.196838    2.4661887   0.8496574   0.21958321 -1.0096576 ]][0m
[37m[1m[2023-07-16 23:03:56,449][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:04:05,518][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 23:04:05,519][257371] FPS: 423477.99[0m
[36m[2023-07-16 23:04:05,521][257371] itr=217, itrs=2000, Progress: 10.85%[0m
[36m[2023-07-16 23:04:17,366][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-16 23:04:17,366][257371] FPS: 325513.64[0m
[36m[2023-07-16 23:04:21,749][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:04:21,750][257371] Reward + Measures: [[54.47619576  0.36848566  0.74116731  0.58859062  0.59851635  0.52541512]][0m
[37m[1m[2023-07-16 23:04:21,750][257371] Max Reward on eval: 54.47619576093385[0m
[37m[1m[2023-07-16 23:04:21,750][257371] Min Reward on eval: 54.47619576093385[0m
[37m[1m[2023-07-16 23:04:21,750][257371] Mean Reward across all agents: 54.47619576093385[0m
[37m[1m[2023-07-16 23:04:21,751][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:04:26,762][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:04:26,763][257371] Reward + Measures: [[ 35.80896839   0.57600003   0.4901       0.65720004   0.1023
    0.82023299]
 [358.20142749   0.26800001   0.54460001   0.39840001   0.31979999
    1.41453683]
 [-43.66959407   0.59810001   0.20460001   0.68480003   0.49080005
    2.12359858]
 ...
 [141.25880214   0.26040003   0.34169999   0.30360001   0.24250002
    1.90189266]
 [  9.59207419   0.23310001   0.80170006   0.47320005   0.67559999
    0.83630008]
 [-24.60290039   0.4447       0.39570001   0.5467       0.31819996
    2.00696683]][0m
[37m[1m[2023-07-16 23:04:26,763][257371] Max Reward on eval: 504.92409893851726[0m
[37m[1m[2023-07-16 23:04:26,763][257371] Min Reward on eval: -131.28934367708862[0m
[37m[1m[2023-07-16 23:04:26,764][257371] Mean Reward across all agents: 42.593662634782[0m
[37m[1m[2023-07-16 23:04:26,764][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:04:26,769][257371] mean_value=-732.8040643124649, max_value=707.0653031040688[0m
[37m[1m[2023-07-16 23:04:26,772][257371] New mean coefficients: [[ 1.737369   -1.2517914   2.8039784   1.167861    0.47030067 -1.0950031 ]][0m
[37m[1m[2023-07-16 23:04:26,773][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:04:35,755][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-16 23:04:35,755][257371] FPS: 427590.62[0m
[36m[2023-07-16 23:04:35,757][257371] itr=218, itrs=2000, Progress: 10.90%[0m
[36m[2023-07-16 23:04:47,439][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-16 23:04:47,439][257371] FPS: 329996.76[0m
[36m[2023-07-16 23:04:51,749][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:04:51,750][257371] Reward + Measures: [[123.62376047   0.32916468   0.79002637   0.62757468   0.64397699
    0.53253222]][0m
[37m[1m[2023-07-16 23:04:51,750][257371] Max Reward on eval: 123.62376047184476[0m
[37m[1m[2023-07-16 23:04:51,750][257371] Min Reward on eval: 123.62376047184476[0m
[37m[1m[2023-07-16 23:04:51,750][257371] Mean Reward across all agents: 123.62376047184476[0m
[37m[1m[2023-07-16 23:04:51,751][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:04:56,834][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:04:56,835][257371] Reward + Measures: [[-94.36091758   0.266        0.62350005   0.38159999   0.62150002
    1.02734542]
 [  5.08222021   0.4835       0.35240003   0.50010008   0.45570001
    1.66476858]
 [ 39.45260958   0.50159997   0.49709997   0.56720001   0.41319999
    0.77225316]
 ...
 [ 62.78294251   0.44530001   0.3369       0.55510002   0.29790002
    1.48960865]
 [ 34.23382708   0.48850003   0.59170002   0.50389999   0.71509999
    1.42242587]
 [102.61637499   0.4901       0.30000001   0.49060002   0.29719999
    1.53586614]][0m
[37m[1m[2023-07-16 23:04:56,835][257371] Max Reward on eval: 298.5507135517895[0m
[37m[1m[2023-07-16 23:04:56,835][257371] Min Reward on eval: -114.20892264242283[0m
[37m[1m[2023-07-16 23:04:56,835][257371] Mean Reward across all agents: 46.406058687814884[0m
[37m[1m[2023-07-16 23:04:56,836][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:04:56,842][257371] mean_value=-378.85164299370695, max_value=405.41662096821364[0m
[37m[1m[2023-07-16 23:04:56,845][257371] New mean coefficients: [[ 1.1707733  -0.92271596  2.63837     0.878119    1.3719606  -1.4922523 ]][0m
[37m[1m[2023-07-16 23:04:56,846][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:05:05,982][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-16 23:05:05,982][257371] FPS: 420412.02[0m
[36m[2023-07-16 23:05:05,984][257371] itr=219, itrs=2000, Progress: 10.95%[0m
[36m[2023-07-16 23:05:17,954][257371] train() took 11.92 seconds to complete[0m
[36m[2023-07-16 23:05:17,954][257371] FPS: 322033.29[0m
[36m[2023-07-16 23:05:22,233][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:05:22,234][257371] Reward + Measures: [[159.77982885   0.30107865   0.84462863   0.66137803   0.70490527
    0.50148761]][0m
[37m[1m[2023-07-16 23:05:22,234][257371] Max Reward on eval: 159.77982884991317[0m
[37m[1m[2023-07-16 23:05:22,234][257371] Min Reward on eval: 159.77982884991317[0m
[37m[1m[2023-07-16 23:05:22,234][257371] Mean Reward across all agents: 159.77982884991317[0m
[37m[1m[2023-07-16 23:05:22,235][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:05:27,405][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:05:27,406][257371] Reward + Measures: [[  35.69948588    0.36830002    0.57030004    0.52630001    0.62419999
     0.90503782]
 [-101.98368979    0.30470002    0.59079999    0.42840001    0.70310009
     0.89244241]
 [ -74.78331861    0.26860002    0.38710004    0.38910004    0.53400004
     1.31353891]
 ...
 [  55.66352065    0.35320002    0.54460001    0.46960002    0.53890002
     1.3251704 ]
 [  29.4680262     0.48340002    0.47890002    0.51069999    0.49950001
     1.22583401]
 [ 115.50851395    0.43889999    0.42770001    0.47639999    0.49110004
     1.87198544]][0m
[37m[1m[2023-07-16 23:05:27,406][257371] Max Reward on eval: 278.2778950067237[0m
[37m[1m[2023-07-16 23:05:27,406][257371] Min Reward on eval: -419.74695397829635[0m
[37m[1m[2023-07-16 23:05:27,407][257371] Mean Reward across all agents: -6.272009428894938[0m
[37m[1m[2023-07-16 23:05:27,407][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:05:27,414][257371] mean_value=-664.1329972261881, max_value=650.4703917367384[0m
[37m[1m[2023-07-16 23:05:27,417][257371] New mean coefficients: [[ 0.7852235  -0.81481653  2.7407427   0.85893273  1.0526465  -0.5297609 ]][0m
[37m[1m[2023-07-16 23:05:27,418][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:05:36,462][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 23:05:36,462][257371] FPS: 424650.59[0m
[36m[2023-07-16 23:05:36,465][257371] itr=220, itrs=2000, Progress: 11.00%[0m
[37m[1m[2023-07-16 23:07:58,577][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000200[0m
[36m[2023-07-16 23:08:10,940][257371] train() took 11.96 seconds to complete[0m
[36m[2023-07-16 23:08:10,940][257371] FPS: 321143.49[0m
[36m[2023-07-16 23:08:15,148][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:08:15,149][257371] Reward + Measures: [[164.72389995   0.41999462   0.72164941   0.61771965   0.57090068
    0.69769263]][0m
[37m[1m[2023-07-16 23:08:15,149][257371] Max Reward on eval: 164.72389994795338[0m
[37m[1m[2023-07-16 23:08:15,149][257371] Min Reward on eval: 164.72389994795338[0m
[37m[1m[2023-07-16 23:08:15,150][257371] Mean Reward across all agents: 164.72389994795338[0m
[37m[1m[2023-07-16 23:08:15,150][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:08:20,075][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:08:20,076][257371] Reward + Measures: [[ 88.52283375   0.46849999   0.3863       0.47580004   0.21869998
    1.96316147]
 [ 91.70534483   0.29060003   0.38440001   0.33109999   0.38189998
    2.57781601]
 [-46.13815691   0.47440004   0.4226       0.44689998   0.1758
    2.24833941]
 ...
 [ 16.4886542    0.33510002   0.63289994   0.52290004   0.47919998
    0.89243239]
 [ 39.56018794   0.52070004   0.47839999   0.63949996   0.42540002
    0.86818331]
 [ 41.0279999    0.3057       0.65859997   0.50040001   0.55440003
    0.99472439]][0m
[37m[1m[2023-07-16 23:08:20,076][257371] Max Reward on eval: 367.9377179071773[0m
[37m[1m[2023-07-16 23:08:20,076][257371] Min Reward on eval: -124.43260817229748[0m
[37m[1m[2023-07-16 23:08:20,076][257371] Mean Reward across all agents: 71.69732323802259[0m
[37m[1m[2023-07-16 23:08:20,077][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:08:20,081][257371] mean_value=-441.4650261539837, max_value=671.0937640945278[0m
[37m[1m[2023-07-16 23:08:20,084][257371] New mean coefficients: [[ 1.9555209  -0.29253948  3.8225455   0.81065625 -0.46443725 -1.4349061 ]][0m
[37m[1m[2023-07-16 23:08:20,085][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:08:28,969][257371] train() took 8.88 seconds to complete[0m
[36m[2023-07-16 23:08:28,969][257371] FPS: 432318.63[0m
[36m[2023-07-16 23:08:28,971][257371] itr=221, itrs=2000, Progress: 11.05%[0m
[36m[2023-07-16 23:08:40,780][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-16 23:08:40,780][257371] FPS: 326426.08[0m
[36m[2023-07-16 23:08:45,130][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:08:45,131][257371] Reward + Measures: [[194.40234872   0.45342997   0.78170466   0.69728029   0.56919867
    0.6430921 ]][0m
[37m[1m[2023-07-16 23:08:45,131][257371] Max Reward on eval: 194.40234872176063[0m
[37m[1m[2023-07-16 23:08:45,131][257371] Min Reward on eval: 194.40234872176063[0m
[37m[1m[2023-07-16 23:08:45,131][257371] Mean Reward across all agents: 194.40234872176063[0m
[37m[1m[2023-07-16 23:08:45,132][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:08:50,152][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:08:50,153][257371] Reward + Measures: [[170.71407891   0.34619999   0.76340002   0.68489999   0.55319995
    0.66111082]
 [-29.28208403   0.42050001   0.4632       0.39879999   0.42030001
    2.18699479]
 [-16.6225058    0.44350001   0.4481       0.44800001   0.41290003
    2.04369903]
 ...
 [ 69.36598432   0.3829       0.26330003   0.40299997   0.2807
    1.21385705]
 [ 25.23063496   0.49759999   0.42419997   0.433        0.38499999
    1.86500287]
 [169.15361996   0.33650002   0.51679999   0.56470007   0.3883
    1.03979897]][0m
[37m[1m[2023-07-16 23:08:50,153][257371] Max Reward on eval: 223.1737666260451[0m
[37m[1m[2023-07-16 23:08:50,153][257371] Min Reward on eval: -177.22317500286735[0m
[37m[1m[2023-07-16 23:08:50,154][257371] Mean Reward across all agents: 38.204000615985265[0m
[37m[1m[2023-07-16 23:08:50,154][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:08:50,158][257371] mean_value=-721.7623103016258, max_value=220.37682656229924[0m
[37m[1m[2023-07-16 23:08:50,161][257371] New mean coefficients: [[ 1.8219105  -0.22650531  3.9624813   1.2941446  -0.44927844 -1.9239166 ]][0m
[37m[1m[2023-07-16 23:08:50,162][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:08:59,292][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-16 23:08:59,292][257371] FPS: 420652.55[0m
[36m[2023-07-16 23:08:59,295][257371] itr=222, itrs=2000, Progress: 11.10%[0m
[36m[2023-07-16 23:09:11,242][257371] train() took 11.90 seconds to complete[0m
[36m[2023-07-16 23:09:11,242][257371] FPS: 322640.35[0m
[36m[2023-07-16 23:09:15,582][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:09:15,583][257371] Reward + Measures: [[215.57442055   0.47712702   0.81402767   0.72072661   0.56792367
    0.62779725]][0m
[37m[1m[2023-07-16 23:09:15,583][257371] Max Reward on eval: 215.57442055011575[0m
[37m[1m[2023-07-16 23:09:15,583][257371] Min Reward on eval: 215.57442055011575[0m
[37m[1m[2023-07-16 23:09:15,583][257371] Mean Reward across all agents: 215.57442055011575[0m
[37m[1m[2023-07-16 23:09:15,584][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:09:20,594][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:09:20,595][257371] Reward + Measures: [[ 239.98809911    0.46670005    0.41850001    0.45340005    0.33019999
     1.510059  ]
 [ 126.8059933     0.29749998    0.33109999    0.28389999    0.32300001
     2.93891168]
 [  59.69641118    0.0907        0.74510002    0.49570003    0.66689998
     1.89382994]
 ...
 [ 139.52256541    0.69559997    0.30160001    0.67449999    0.32610002
     3.67798853]
 [ -24.64043043    0.41330001    0.32070002    0.3267        0.34510002
     2.85676432]
 [-198.26064019    0.36469999    0.47129998    0.20850001    0.42249998
     2.0123651 ]][0m
[37m[1m[2023-07-16 23:09:20,595][257371] Max Reward on eval: 351.88867950914425[0m
[37m[1m[2023-07-16 23:09:20,595][257371] Min Reward on eval: -283.98553204461933[0m
[37m[1m[2023-07-16 23:09:20,595][257371] Mean Reward across all agents: 89.73229996378393[0m
[37m[1m[2023-07-16 23:09:20,595][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:09:20,604][257371] mean_value=-1492.6094825153168, max_value=615.5836842241115[0m
[37m[1m[2023-07-16 23:09:20,607][257371] New mean coefficients: [[ 1.1892822   0.11293468  4.143676    1.4825565  -1.9210209  -1.0984328 ]][0m
[37m[1m[2023-07-16 23:09:20,608][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:09:29,624][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 23:09:29,624][257371] FPS: 426006.40[0m
[36m[2023-07-16 23:09:29,626][257371] itr=223, itrs=2000, Progress: 11.15%[0m
[36m[2023-07-16 23:09:41,283][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-16 23:09:41,284][257371] FPS: 330795.79[0m
[36m[2023-07-16 23:09:45,541][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:09:45,541][257371] Reward + Measures: [[210.82243898   0.50864363   0.84479964   0.75000197   0.56488866
    0.62848562]][0m
[37m[1m[2023-07-16 23:09:45,542][257371] Max Reward on eval: 210.8224389812383[0m
[37m[1m[2023-07-16 23:09:45,542][257371] Min Reward on eval: 210.8224389812383[0m
[37m[1m[2023-07-16 23:09:45,542][257371] Mean Reward across all agents: 210.8224389812383[0m
[37m[1m[2023-07-16 23:09:45,542][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:09:50,676][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:09:50,676][257371] Reward + Measures: [[ 53.16003355   0.47510001   0.45730001   0.49869999   0.4605
    1.71309412]
 [-97.79790034   0.28939998   0.31980002   0.3152       0.32350001
    3.81986547]
 [144.72073412   0.62860006   0.36630002   0.51340002   0.31420001
    1.56752563]
 ...
 [ 44.67305222   0.38480002   0.62790006   0.52450001   0.56409997
    1.15288579]
 [ 11.62310027   0.41350004   0.5115       0.36360002   0.40150005
    2.19120884]
 [-60.54101255   0.3802       0.63749999   0.477        0.4488
    1.52404058]][0m
[37m[1m[2023-07-16 23:09:50,676][257371] Max Reward on eval: 253.45390783231704[0m
[37m[1m[2023-07-16 23:09:50,677][257371] Min Reward on eval: -119.99627870274708[0m
[37m[1m[2023-07-16 23:09:50,677][257371] Mean Reward across all agents: 34.81976422437357[0m
[37m[1m[2023-07-16 23:09:50,677][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:09:50,682][257371] mean_value=-472.35597438065764, max_value=492.0244557459303[0m
[37m[1m[2023-07-16 23:09:50,685][257371] New mean coefficients: [[ 0.68461967  0.19138937  3.6908832   1.4514937  -1.6571603  -1.5027285 ]][0m
[37m[1m[2023-07-16 23:09:50,686][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:09:59,697][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 23:09:59,697][257371] FPS: 426212.96[0m
[36m[2023-07-16 23:09:59,699][257371] itr=224, itrs=2000, Progress: 11.20%[0m
[36m[2023-07-16 23:10:11,556][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-16 23:10:11,557][257371] FPS: 325094.41[0m
[36m[2023-07-16 23:10:15,888][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:10:15,889][257371] Reward + Measures: [[6.36611022 0.88854468 0.94211894 0.933586   0.87031633 0.72211361]][0m
[37m[1m[2023-07-16 23:10:15,889][257371] Max Reward on eval: 6.366110219270338[0m
[37m[1m[2023-07-16 23:10:15,889][257371] Min Reward on eval: 6.366110219270338[0m
[37m[1m[2023-07-16 23:10:15,890][257371] Mean Reward across all agents: 6.366110219270338[0m
[37m[1m[2023-07-16 23:10:15,890][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:10:20,951][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:10:20,951][257371] Reward + Measures: [[ 38.15789471   0.28260002   0.2956       0.3335       0.331
    2.66835904]
 [-30.15252613   0.33000001   0.41740003   0.40689999   0.2793
    1.65996361]
 [-46.13639419   0.25350001   0.09060001   0.14960001   0.235
    2.06421876]
 ...
 [ -2.68452341   0.3777       0.42640001   0.40369996   0.23599999
    1.76165414]
 [ 38.02680764   0.5891       0.53369999   0.67030001   0.0783
    1.062428  ]
 [  3.17150927   0.2895       0.44260001   0.43540001   0.34619999
    1.69491851]][0m
[37m[1m[2023-07-16 23:10:20,952][257371] Max Reward on eval: 254.8176994547015[0m
[37m[1m[2023-07-16 23:10:20,952][257371] Min Reward on eval: -145.5916542717372[0m
[37m[1m[2023-07-16 23:10:20,952][257371] Mean Reward across all agents: 23.063814408672588[0m
[37m[1m[2023-07-16 23:10:20,952][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:10:20,961][257371] mean_value=-1367.6052007092853, max_value=667.3021984454942[0m
[37m[1m[2023-07-16 23:10:20,963][257371] New mean coefficients: [[ 2.8378403  -0.46822125  2.4956317   0.3551197  -0.85553664 -1.8954647 ]][0m
[37m[1m[2023-07-16 23:10:20,964][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:10:30,079][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-16 23:10:30,079][257371] FPS: 421391.62[0m
[36m[2023-07-16 23:10:30,081][257371] itr=225, itrs=2000, Progress: 11.25%[0m
[36m[2023-07-16 23:10:41,738][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-16 23:10:41,738][257371] FPS: 330833.60[0m
[36m[2023-07-16 23:10:45,951][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:10:45,951][257371] Reward + Measures: [[54.60257962  0.76748037  0.76476336  0.75744057  0.14310466  0.55871451]][0m
[37m[1m[2023-07-16 23:10:45,952][257371] Max Reward on eval: 54.602579618487326[0m
[37m[1m[2023-07-16 23:10:45,952][257371] Min Reward on eval: 54.602579618487326[0m
[37m[1m[2023-07-16 23:10:45,952][257371] Mean Reward across all agents: 54.602579618487326[0m
[37m[1m[2023-07-16 23:10:45,952][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:10:50,984][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:10:50,984][257371] Reward + Measures: [[187.43622874   0.46679997   0.50530005   0.46689996   0.42929998
    2.79657435]
 [136.19529821   0.68320006   0.67130005   0.73750001   0.61070007
    2.63999057]
 [ 91.88115584   0.36719999   0.59560007   0.45359999   0.46420002
    1.25993764]
 ...
 [ 56.75910353   0.79319996   0.76920003   0.87860006   0.77130002
    1.15528488]
 [115.2072811    0.40910003   0.90459996   0.90720004   0.93780005
    2.33756304]
 [144.66072559   0.70220006   0.78470004   0.71640003   0.51200002
    1.48680341]][0m
[37m[1m[2023-07-16 23:10:50,984][257371] Max Reward on eval: 356.07468987586907[0m
[37m[1m[2023-07-16 23:10:50,985][257371] Min Reward on eval: -225.40334204500542[0m
[37m[1m[2023-07-16 23:10:50,985][257371] Mean Reward across all agents: 87.65587437104101[0m
[37m[1m[2023-07-16 23:10:50,985][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:10:51,000][257371] mean_value=-585.6956100441257, max_value=624.4097312446083[0m
[37m[1m[2023-07-16 23:10:51,002][257371] New mean coefficients: [[ 4.9212317  -0.72950447  2.2007356  -0.17963028  0.42569965 -2.896644  ]][0m
[37m[1m[2023-07-16 23:10:51,003][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:11:00,051][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 23:11:00,052][257371] FPS: 424472.42[0m
[36m[2023-07-16 23:11:00,054][257371] itr=226, itrs=2000, Progress: 11.30%[0m
[36m[2023-07-16 23:11:11,767][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-16 23:11:11,768][257371] FPS: 329112.92[0m
[36m[2023-07-16 23:11:16,133][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:11:16,133][257371] Reward + Measures: [[128.30568944   0.76757228   0.80169165   0.74315339   0.15260834
    0.55340159]][0m
[37m[1m[2023-07-16 23:11:16,133][257371] Max Reward on eval: 128.3056894399285[0m
[37m[1m[2023-07-16 23:11:16,134][257371] Min Reward on eval: 128.3056894399285[0m
[37m[1m[2023-07-16 23:11:16,134][257371] Mean Reward across all agents: 128.3056894399285[0m
[37m[1m[2023-07-16 23:11:16,134][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:11:21,229][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:11:21,230][257371] Reward + Measures: [[ 80.87785878   0.3105       0.49990001   0.34130001   0.3757
    2.95228553]
 [133.76843596   0.35980001   0.54040003   0.3811       0.41020003
    2.21402907]
 [113.30091053   0.22920001   0.47279999   0.28169999   0.4021
    2.38658118]
 ...
 [ 36.23612968   0.20369999   0.37420002   0.15159999   0.37189999
    3.98538589]
 [131.04846568   0.31259999   0.44300005   0.22470002   0.29200003
    4.13220692]
 [118.47340037   0.37419999   0.56220001   0.2304       0.47770005
    2.98012614]][0m
[37m[1m[2023-07-16 23:11:21,230][257371] Max Reward on eval: 256.0844802260399[0m
[37m[1m[2023-07-16 23:11:21,230][257371] Min Reward on eval: -192.96210252745078[0m
[37m[1m[2023-07-16 23:11:21,231][257371] Mean Reward across all agents: 59.92225988370408[0m
[37m[1m[2023-07-16 23:11:21,231][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:11:21,238][257371] mean_value=-1554.109376930272, max_value=584.1981064073905[0m
[37m[1m[2023-07-16 23:11:21,240][257371] New mean coefficients: [[ 4.4249578  -0.1916126   1.8676175  -0.3257528  -0.26442695 -3.2251105 ]][0m
[37m[1m[2023-07-16 23:11:21,241][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:11:30,291][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 23:11:30,291][257371] FPS: 424397.01[0m
[36m[2023-07-16 23:11:30,294][257371] itr=227, itrs=2000, Progress: 11.35%[0m
[36m[2023-07-16 23:11:42,088][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-16 23:11:42,088][257371] FPS: 326858.03[0m
[36m[2023-07-16 23:11:46,440][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:11:46,441][257371] Reward + Measures: [[199.6770303    0.74078065   0.81412566   0.71224868   0.19796298
    0.56060946]][0m
[37m[1m[2023-07-16 23:11:46,441][257371] Max Reward on eval: 199.6770302987087[0m
[37m[1m[2023-07-16 23:11:46,441][257371] Min Reward on eval: 199.6770302987087[0m
[37m[1m[2023-07-16 23:11:46,441][257371] Mean Reward across all agents: 199.6770302987087[0m
[37m[1m[2023-07-16 23:11:46,442][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:11:51,649][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:11:51,650][257371] Reward + Measures: [[-16.24675118   0.29030001   0.3585       0.27920002   0.25310001
    3.07331395]
 [ 29.69538343   0.28749999   0.41570002   0.31130001   0.2739
    2.78445435]
 [117.98332516   0.67180002   0.58280003   0.66300005   0.1964
    1.64276946]
 ...
 [160.96062758   0.29679999   0.46199998   0.27290002   0.34650001
    2.63977885]
 [ 47.72738542   0.27740002   0.35779998   0.33600003   0.2938
    2.80187845]
 [ 68.59485415   0.34419999   0.62820005   0.19230001   0.52060002
    1.89499402]][0m
[37m[1m[2023-07-16 23:11:51,650][257371] Max Reward on eval: 296.98999022794885[0m
[37m[1m[2023-07-16 23:11:51,650][257371] Min Reward on eval: -200.03137877164409[0m
[37m[1m[2023-07-16 23:11:51,650][257371] Mean Reward across all agents: 28.68435196698425[0m
[37m[1m[2023-07-16 23:11:51,651][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:11:51,656][257371] mean_value=-1266.8475071676228, max_value=243.33673791178427[0m
[37m[1m[2023-07-16 23:11:51,658][257371] New mean coefficients: [[ 3.2827547  -0.07174519  2.2554884   0.38665563 -1.1117897  -3.5684547 ]][0m
[37m[1m[2023-07-16 23:11:51,659][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:12:00,704][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 23:12:00,705][257371] FPS: 424621.47[0m
[36m[2023-07-16 23:12:00,707][257371] itr=228, itrs=2000, Progress: 11.40%[0m
[36m[2023-07-16 23:12:12,517][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-16 23:12:12,518][257371] FPS: 326461.77[0m
[36m[2023-07-16 23:12:16,893][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:12:16,893][257371] Reward + Measures: [[269.58283181   0.72848827   0.81699598   0.68870604   0.15514567
    0.54105264]][0m
[37m[1m[2023-07-16 23:12:16,893][257371] Max Reward on eval: 269.5828318114949[0m
[37m[1m[2023-07-16 23:12:16,894][257371] Min Reward on eval: 269.5828318114949[0m
[37m[1m[2023-07-16 23:12:16,894][257371] Mean Reward across all agents: 269.5828318114949[0m
[37m[1m[2023-07-16 23:12:16,894][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:12:21,914][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:12:21,915][257371] Reward + Measures: [[  38.3380701     0.71490002    0.57910001    0.45930001    0.46050006
     4.26601744]
 [ 212.31483648    0.48530003    0.50569999    0.20050001    0.46829996
     5.58525324]
 [ 583.03551104    0.92180008    0.97060007    0.0621        0.9382
     4.40936232]
 ...
 [  32.00908813    0.26889998    0.37610003    0.29010001    0.2766
     2.50605536]
 [-425.77282616    0.91250002    0.92860001    0.15089999    0.83120006
     5.75253439]
 [ 133.27944411    0.1947        0.19759999    0.18260001    0.11130001
     4.09901142]][0m
[37m[1m[2023-07-16 23:12:21,915][257371] Max Reward on eval: 710.8369141226751[0m
[37m[1m[2023-07-16 23:12:21,915][257371] Min Reward on eval: -557.6294784742407[0m
[37m[1m[2023-07-16 23:12:21,915][257371] Mean Reward across all agents: 136.9492703941759[0m
[37m[1m[2023-07-16 23:12:21,916][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:12:21,931][257371] mean_value=-1328.3408084969565, max_value=1210.8369141226751[0m
[37m[1m[2023-07-16 23:12:21,933][257371] New mean coefficients: [[ 4.6885004  -0.48013195  3.055127    0.31661212  0.6120677  -3.2589378 ]][0m
[37m[1m[2023-07-16 23:12:21,934][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:12:31,009][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 23:12:31,009][257371] FPS: 423230.94[0m
[36m[2023-07-16 23:12:31,012][257371] itr=229, itrs=2000, Progress: 11.45%[0m
[36m[2023-07-16 23:12:42,736][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-16 23:12:42,737][257371] FPS: 328922.82[0m
[36m[2023-07-16 23:12:47,113][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:12:47,114][257371] Reward + Measures: [[323.48599572   0.71162897   0.8274793    0.661466     0.16483699
    0.53799909]][0m
[37m[1m[2023-07-16 23:12:47,114][257371] Max Reward on eval: 323.4859957239665[0m
[37m[1m[2023-07-16 23:12:47,114][257371] Min Reward on eval: 323.4859957239665[0m
[37m[1m[2023-07-16 23:12:47,114][257371] Mean Reward across all agents: 323.4859957239665[0m
[37m[1m[2023-07-16 23:12:47,115][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:12:52,153][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:12:52,153][257371] Reward + Measures: [[ 76.41266209   0.50030005   0.44499999   0.52499998   0.42399999
    1.71467936]
 [  4.72056804   0.66530001   0.65160006   0.72320002   0.64560002
    2.33600044]
 [141.23434824   0.3761       0.43830004   0.43260002   0.35300002
    1.3655647 ]
 ...
 [ 79.59788873   0.51060003   0.66210002   0.63210005   0.63939995
    0.88064587]
 [ 41.79076615   0.56170005   0.82390004   0.62290001   0.76680005
    1.55157006]
 [ -4.2988476    0.24089999   0.26219997   0.26390001   0.23720001
    3.24052119]][0m
[37m[1m[2023-07-16 23:12:52,154][257371] Max Reward on eval: 388.5024490146898[0m
[37m[1m[2023-07-16 23:12:52,154][257371] Min Reward on eval: -93.25987292691715[0m
[37m[1m[2023-07-16 23:12:52,154][257371] Mean Reward across all agents: 75.01653973294344[0m
[37m[1m[2023-07-16 23:12:52,154][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:12:52,164][257371] mean_value=-319.86542033647214, max_value=659.1430398546042[0m
[37m[1m[2023-07-16 23:12:52,167][257371] New mean coefficients: [[ 4.4538693  -1.2605793   2.2899923  -0.45038795  3.0211952  -1.9149374 ]][0m
[37m[1m[2023-07-16 23:12:52,168][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:13:01,251][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 23:13:01,251][257371] FPS: 422835.14[0m
[36m[2023-07-16 23:13:01,253][257371] itr=230, itrs=2000, Progress: 11.50%[0m
[37m[1m[2023-07-16 23:15:21,599][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000210[0m
[36m[2023-07-16 23:15:33,754][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-16 23:15:33,755][257371] FPS: 330883.03[0m
[36m[2023-07-16 23:15:38,042][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:15:38,042][257371] Reward + Measures: [[395.8804229    0.6490823    0.8130576    0.61801732   0.21833801
    0.56187814]][0m
[37m[1m[2023-07-16 23:15:38,042][257371] Max Reward on eval: 395.8804229035592[0m
[37m[1m[2023-07-16 23:15:38,043][257371] Min Reward on eval: 395.8804229035592[0m
[37m[1m[2023-07-16 23:15:38,043][257371] Mean Reward across all agents: 395.8804229035592[0m
[37m[1m[2023-07-16 23:15:38,043][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:15:43,167][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:15:43,167][257371] Reward + Measures: [[ 51.13777954   0.34730002   0.189        0.32979998   0.26019999
    3.56560135]
 [ 60.68248764   0.23         0.29450002   0.31080002   0.27059999
    3.03498292]
 [ -0.79036526   0.18730001   0.15470001   0.27610001   0.23770002
    4.39356947]
 ...
 [-26.56409821   0.18880001   0.52120006   0.4659       0.50950003
    4.61320448]
 [ 26.9888342    0.39630002   0.1866       0.33699998   0.2221
    4.29465199]
 [236.96843143   0.0521       0.81129998   0.68189996   0.79689997
    5.89853239]][0m
[37m[1m[2023-07-16 23:15:43,167][257371] Max Reward on eval: 319.09830670552327[0m
[37m[1m[2023-07-16 23:15:43,168][257371] Min Reward on eval: -95.79764344263822[0m
[37m[1m[2023-07-16 23:15:43,168][257371] Mean Reward across all agents: 25.98334953200822[0m
[37m[1m[2023-07-16 23:15:43,168][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:15:43,176][257371] mean_value=-1129.022946457516, max_value=819.0983067055233[0m
[37m[1m[2023-07-16 23:15:43,179][257371] New mean coefficients: [[ 2.4951987 -1.2925997  1.1459734 -0.7296883  5.1705217 -1.7894245]][0m
[37m[1m[2023-07-16 23:15:43,180][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:15:52,145][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-16 23:15:52,146][257371] FPS: 428412.02[0m
[36m[2023-07-16 23:15:52,148][257371] itr=231, itrs=2000, Progress: 11.55%[0m
[36m[2023-07-16 23:16:04,123][257371] train() took 11.92 seconds to complete[0m
[36m[2023-07-16 23:16:04,123][257371] FPS: 322040.39[0m
[36m[2023-07-16 23:16:08,426][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:16:08,427][257371] Reward + Measures: [[457.80114334   0.52282101   0.77592838   0.57459635   0.34202033
    0.5818941 ]][0m
[37m[1m[2023-07-16 23:16:08,427][257371] Max Reward on eval: 457.8011433374905[0m
[37m[1m[2023-07-16 23:16:08,427][257371] Min Reward on eval: 457.8011433374905[0m
[37m[1m[2023-07-16 23:16:08,427][257371] Mean Reward across all agents: 457.8011433374905[0m
[37m[1m[2023-07-16 23:16:08,428][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:16:13,624][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:16:13,630][257371] Reward + Measures: [[ 59.1066831    0.27700001   0.47019997   0.21260002   0.40130001
    3.21716356]
 [ 74.50953936   0.103        0.2696       0.1332       0.2086
    5.73087645]
 [ 18.67951512   0.1989       0.21610001   0.21140002   0.20510001
    3.37474132]
 ...
 [-21.89790023   0.1173       0.29920003   0.20220001   0.26419997
    2.79382157]
 [ -8.23770776   0.1531       0.22260001   0.1258       0.1875
    4.75839186]
 [  4.52368026   0.21259999   0.36300001   0.33320001   0.36999997
    3.47189188]][0m
[37m[1m[2023-07-16 23:16:13,630][257371] Max Reward on eval: 277.8107563693076[0m
[37m[1m[2023-07-16 23:16:13,630][257371] Min Reward on eval: -128.65773818960878[0m
[37m[1m[2023-07-16 23:16:13,630][257371] Mean Reward across all agents: 2.2047049810504142[0m
[37m[1m[2023-07-16 23:16:13,631][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:16:13,634][257371] mean_value=-1807.6403680098413, max_value=444.8401747658476[0m
[37m[1m[2023-07-16 23:16:13,637][257371] New mean coefficients: [[ 1.8835912  -0.36049432  2.3414931   0.37218022  3.2516265  -1.7919961 ]][0m
[37m[1m[2023-07-16 23:16:13,638][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:16:22,701][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-16 23:16:22,701][257371] FPS: 423754.48[0m
[36m[2023-07-16 23:16:22,704][257371] itr=232, itrs=2000, Progress: 11.60%[0m
[36m[2023-07-16 23:16:34,446][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-16 23:16:34,447][257371] FPS: 328351.69[0m
[36m[2023-07-16 23:16:38,747][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:16:38,747][257371] Reward + Measures: [[441.56347647   0.44270864   0.76526868   0.57020801   0.466207
    0.56578237]][0m
[37m[1m[2023-07-16 23:16:38,747][257371] Max Reward on eval: 441.563476467308[0m
[37m[1m[2023-07-16 23:16:38,748][257371] Min Reward on eval: 441.563476467308[0m
[37m[1m[2023-07-16 23:16:38,748][257371] Mean Reward across all agents: 441.563476467308[0m
[37m[1m[2023-07-16 23:16:38,748][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:16:43,760][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:16:43,761][257371] Reward + Measures: [[  99.11685971    0.47739998    0.23560002    0.5467        0.1265
     2.59529662]
 [-131.27612064    0.479         0.36380002    0.4323        0.29520002
     2.70878839]
 [-119.54791396    0.40759999    0.36919999    0.33829999    0.30369997
     2.72810531]
 ...
 [ -33.93198612    0.60009998    0.3849        0.65150005    0.25919998
     1.30275118]
 [ -87.19417335    0.42039999    0.39440003    0.39310002    0.3554
     2.65082693]
 [ -42.09287004    0.40410003    0.65459996    0.32529998    0.63730001
     1.1269716 ]][0m
[37m[1m[2023-07-16 23:16:43,761][257371] Max Reward on eval: 274.66371726978105[0m
[37m[1m[2023-07-16 23:16:43,761][257371] Min Reward on eval: -385.55570602631195[0m
[37m[1m[2023-07-16 23:16:43,761][257371] Mean Reward across all agents: -29.600139851154594[0m
[37m[1m[2023-07-16 23:16:43,762][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:16:43,766][257371] mean_value=-873.2050431710968, max_value=302.83724266467743[0m
[37m[1m[2023-07-16 23:16:43,768][257371] New mean coefficients: [[ 1.6467471  -0.8208534   2.2405005   0.49959093  2.4903896  -1.9013304 ]][0m
[37m[1m[2023-07-16 23:16:43,769][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:16:52,762][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-16 23:16:52,762][257371] FPS: 427090.43[0m
[36m[2023-07-16 23:16:52,765][257371] itr=233, itrs=2000, Progress: 11.65%[0m
[36m[2023-07-16 23:17:04,818][257371] train() took 12.01 seconds to complete[0m
[36m[2023-07-16 23:17:04,818][257371] FPS: 319836.52[0m
[36m[2023-07-16 23:17:09,153][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:17:09,153][257371] Reward + Measures: [[413.67347282   0.40968701   0.78036702   0.57384402   0.53925192
    0.54094106]][0m
[37m[1m[2023-07-16 23:17:09,153][257371] Max Reward on eval: 413.67347282335925[0m
[37m[1m[2023-07-16 23:17:09,154][257371] Min Reward on eval: 413.67347282335925[0m
[37m[1m[2023-07-16 23:17:09,154][257371] Mean Reward across all agents: 413.67347282335925[0m
[37m[1m[2023-07-16 23:17:09,154][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:17:14,219][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:17:14,220][257371] Reward + Measures: [[ 66.63909592   0.24910001   0.35590002   0.32300001   0.45029998
    2.7975502 ]
 [179.57956646   0.49900004   0.30860001   0.4619       0.30419999
    1.89050889]
 [187.79729081   0.2325       0.73240006   0.45749998   0.67580003
    0.70520616]
 ...
 [ 83.13068719   0.17950001   0.40900001   0.21930002   0.41949996
    2.5881803 ]
 [ 71.61882539   0.32949999   0.2631       0.32750002   0.37
    3.59832263]
 [ 50.1917682    0.52820009   0.45210001   0.58920002   0.39250001
    1.71197224]][0m
[37m[1m[2023-07-16 23:17:14,220][257371] Max Reward on eval: 387.3072261504829[0m
[37m[1m[2023-07-16 23:17:14,220][257371] Min Reward on eval: -55.764770322153346[0m
[37m[1m[2023-07-16 23:17:14,221][257371] Mean Reward across all agents: 96.52637380368665[0m
[37m[1m[2023-07-16 23:17:14,221][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:17:14,225][257371] mean_value=-1150.8248976623054, max_value=349.65812946859273[0m
[37m[1m[2023-07-16 23:17:14,228][257371] New mean coefficients: [[ 2.3431354  -1.0461266   2.1389816   0.18507308  2.0173564  -1.3604163 ]][0m
[37m[1m[2023-07-16 23:17:14,229][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:17:23,303][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 23:17:23,303][257371] FPS: 423264.45[0m
[36m[2023-07-16 23:17:23,306][257371] itr=234, itrs=2000, Progress: 11.70%[0m
[36m[2023-07-16 23:17:35,252][257371] train() took 11.90 seconds to complete[0m
[36m[2023-07-16 23:17:35,253][257371] FPS: 322720.65[0m
[36m[2023-07-16 23:17:39,626][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:17:39,626][257371] Reward + Measures: [[430.54034905   0.37489265   0.81073862   0.54253596   0.59759003
    0.51617336]][0m
[37m[1m[2023-07-16 23:17:39,626][257371] Max Reward on eval: 430.5403490459608[0m
[37m[1m[2023-07-16 23:17:39,627][257371] Min Reward on eval: 430.5403490459608[0m
[37m[1m[2023-07-16 23:17:39,627][257371] Mean Reward across all agents: 430.5403490459608[0m
[37m[1m[2023-07-16 23:17:39,627][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:17:44,686][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:17:44,686][257371] Reward + Measures: [[  69.54589805    0.54710001    0.34239998    0.54750007    0.36890003
     1.31009424]
 [ 187.68330644    0.42619997    0.56779999    0.54030001    0.3479
     1.06414568]
 [  78.46567622    0.5061        0.4901        0.46479997    0.30370003
     1.25217712]
 ...
 [-114.07484651    0.34960005    0.57359999    0.56650007    0.53870004
     0.78184706]
 [ 160.85357949    0.2696        0.3272        0.27329999    0.32179999
     2.70702481]
 [ -28.12011768    0.30280003    0.34040001    0.27700001    0.27690002
     2.00833488]][0m
[37m[1m[2023-07-16 23:17:44,686][257371] Max Reward on eval: 390.81013299617916[0m
[37m[1m[2023-07-16 23:17:44,687][257371] Min Reward on eval: -249.22786473499144[0m
[37m[1m[2023-07-16 23:17:44,687][257371] Mean Reward across all agents: 32.747037600094664[0m
[37m[1m[2023-07-16 23:17:44,687][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:17:44,691][257371] mean_value=-2058.1103180148198, max_value=890.8101329961792[0m
[37m[1m[2023-07-16 23:17:44,694][257371] New mean coefficients: [[-0.44983745 -0.05535352  1.4805782   0.6562836   2.1869736  -1.4960523 ]][0m
[37m[1m[2023-07-16 23:17:44,695][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:17:53,894][257371] train() took 9.20 seconds to complete[0m
[36m[2023-07-16 23:17:53,894][257371] FPS: 417486.61[0m
[36m[2023-07-16 23:17:53,897][257371] itr=235, itrs=2000, Progress: 11.75%[0m
[36m[2023-07-16 23:18:05,635][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-16 23:18:05,635][257371] FPS: 328578.09[0m
[36m[2023-07-16 23:18:10,004][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:18:10,005][257371] Reward + Measures: [[209.7682715    0.26993933   0.860632     0.54534131   0.81469601
    0.65111566]][0m
[37m[1m[2023-07-16 23:18:10,005][257371] Max Reward on eval: 209.76827149737278[0m
[37m[1m[2023-07-16 23:18:10,005][257371] Min Reward on eval: 209.76827149737278[0m
[37m[1m[2023-07-16 23:18:10,005][257371] Mean Reward across all agents: 209.76827149737278[0m
[37m[1m[2023-07-16 23:18:10,006][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:18:15,052][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:18:15,053][257371] Reward + Measures: [[-12.79961899   0.22309999   0.23380001   0.32099998   0.19670001
    3.60114527]
 [ 57.80471662   0.36750004   0.419        0.57459998   0.44080001
    1.95189857]
 [ 84.39133514   0.36129999   0.45609999   0.2825       0.3574
    3.26214218]
 ...
 [ 56.96201572   0.40840003   0.34579998   0.53610003   0.59590006
    3.2183311 ]
 [-21.69062144   0.34040001   0.2737       0.33830002   0.28119999
    2.63670731]
 [ 56.16535879   0.72170001   0.49389997   0.81290007   0.44309998
    0.77101749]][0m
[37m[1m[2023-07-16 23:18:15,053][257371] Max Reward on eval: 344.7061138316989[0m
[37m[1m[2023-07-16 23:18:15,053][257371] Min Reward on eval: -116.69375226041302[0m
[37m[1m[2023-07-16 23:18:15,054][257371] Mean Reward across all agents: 50.227433025014385[0m
[37m[1m[2023-07-16 23:18:15,054][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:18:15,061][257371] mean_value=-1072.817397928306, max_value=561.5054448406794[0m
[37m[1m[2023-07-16 23:18:15,064][257371] New mean coefficients: [[ 0.7285061  -0.33237034  0.78730345  0.10081261  2.7081401  -1.7918148 ]][0m
[37m[1m[2023-07-16 23:18:15,065][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:18:24,143][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-16 23:18:24,144][257371] FPS: 423065.32[0m
[36m[2023-07-16 23:18:24,146][257371] itr=236, itrs=2000, Progress: 11.80%[0m
[36m[2023-07-16 23:18:36,445][257371] train() took 12.25 seconds to complete[0m
[36m[2023-07-16 23:18:36,446][257371] FPS: 313506.88[0m
[36m[2023-07-16 23:18:40,751][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:18:40,751][257371] Reward + Measures: [[15.83180351  0.3904573   0.89321297  0.75268495  0.80575603  0.78179038]][0m
[37m[1m[2023-07-16 23:18:40,752][257371] Max Reward on eval: 15.831803508969776[0m
[37m[1m[2023-07-16 23:18:40,752][257371] Min Reward on eval: 15.831803508969776[0m
[37m[1m[2023-07-16 23:18:40,752][257371] Mean Reward across all agents: 15.831803508969776[0m
[37m[1m[2023-07-16 23:18:40,752][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:18:45,924][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:18:45,925][257371] Reward + Measures: [[-27.08672535   0.41150004   0.61589998   0.53290004   0.47129998
    1.20802295]
 [ 56.33607657   0.206        0.24990001   0.30000001   0.23889999
    2.86214709]
 [172.7094555    0.38970003   0.45649996   0.43500003   0.43430001
    1.2440176 ]
 ...
 [154.84552037   0.24369998   0.23910001   0.33050001   0.24000001
    2.52287912]
 [ 34.73274241   0.46290001   0.52310002   0.3515       0.52030003
    1.09740925]
 [ 89.24183199   0.2139       0.25440001   0.22389999   0.24460001
    3.30810475]][0m
[37m[1m[2023-07-16 23:18:45,926][257371] Max Reward on eval: 269.1245631825179[0m
[37m[1m[2023-07-16 23:18:45,926][257371] Min Reward on eval: -159.29462769224773[0m
[37m[1m[2023-07-16 23:18:45,926][257371] Mean Reward across all agents: 39.512305210121994[0m
[37m[1m[2023-07-16 23:18:45,926][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:18:45,930][257371] mean_value=-1365.420851665497, max_value=563.487112756446[0m
[37m[1m[2023-07-16 23:18:45,933][257371] New mean coefficients: [[-0.36565804 -0.2961752   1.1306008   0.00494945  1.6483544  -1.2544914 ]][0m
[37m[1m[2023-07-16 23:18:45,934][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:18:54,422][257371] train() took 8.49 seconds to complete[0m
[36m[2023-07-16 23:18:54,423][257371] FPS: 452501.09[0m
[36m[2023-07-16 23:18:54,425][257371] itr=237, itrs=2000, Progress: 11.85%[0m
[36m[2023-07-16 23:19:09,327][257371] train() took 14.85 seconds to complete[0m
[36m[2023-07-16 23:19:09,328][257371] FPS: 258571.02[0m
[36m[2023-07-16 23:19:14,846][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:19:14,847][257371] Reward + Measures: [[26.54925368  0.19998333  0.97119093  0.87298161  0.96837133  0.91224998]][0m
[37m[1m[2023-07-16 23:19:14,847][257371] Max Reward on eval: 26.549253683050715[0m
[37m[1m[2023-07-16 23:19:14,847][257371] Min Reward on eval: 26.549253683050715[0m
[37m[1m[2023-07-16 23:19:14,847][257371] Mean Reward across all agents: 26.549253683050715[0m
[37m[1m[2023-07-16 23:19:14,848][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:19:21,642][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:19:21,642][257371] Reward + Measures: [[-99.84004376   0.2155       0.65869999   0.38580003   0.62659997
    1.64636481]
 [ 46.49436216   0.14240001   0.91020006   0.69670004   0.87120003
    1.80541289]
 [-19.41402635   0.6397       0.58260006   0.64120001   0.30299997
    0.74562472]
 ...
 [ 51.50395615   0.12620001   0.37010002   0.33960003   0.34990001
    3.2877984 ]
 [101.62052823   0.2306       0.88679999   0.68190002   0.79430002
    2.979424  ]
 [ 34.39701691   0.66530007   0.37059999   0.68450004   0.16470002
    1.95444453]][0m
[37m[1m[2023-07-16 23:19:21,642][257371] Max Reward on eval: 238.20830604312943[0m
[37m[1m[2023-07-16 23:19:21,643][257371] Min Reward on eval: -111.32550761178136[0m
[37m[1m[2023-07-16 23:19:21,643][257371] Mean Reward across all agents: 39.949718595709015[0m
[37m[1m[2023-07-16 23:19:21,643][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:19:21,652][257371] mean_value=-324.4055055401793, max_value=587.9457724216813[0m
[37m[1m[2023-07-16 23:19:21,655][257371] New mean coefficients: [[ 0.0065991  -1.1064501   0.2898959  -0.31806853  2.8289733  -0.91670287]][0m
[37m[1m[2023-07-16 23:19:21,657][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:19:31,880][257371] train() took 10.22 seconds to complete[0m
[36m[2023-07-16 23:19:31,880][257371] FPS: 375687.68[0m
[36m[2023-07-16 23:19:31,883][257371] itr=238, itrs=2000, Progress: 11.90%[0m
[36m[2023-07-16 23:19:43,773][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-16 23:19:43,773][257371] FPS: 324485.36[0m
[36m[2023-07-16 23:19:48,102][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:19:48,102][257371] Reward + Measures: [[21.16792706  0.05057266  0.9361856   0.65098804  0.93287134  0.5260855 ]][0m
[37m[1m[2023-07-16 23:19:48,102][257371] Max Reward on eval: 21.16792706462647[0m
[37m[1m[2023-07-16 23:19:48,103][257371] Min Reward on eval: 21.16792706462647[0m
[37m[1m[2023-07-16 23:19:48,103][257371] Mean Reward across all agents: 21.16792706462647[0m
[37m[1m[2023-07-16 23:19:48,103][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:19:53,202][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:19:53,203][257371] Reward + Measures: [[27.32542233  0.1454      0.89779997  0.62360001  0.92999995  0.83296317]
 [68.00166224  0.0354      0.91330004  0.69950002  0.93479997  0.97210217]
 [18.79742141  0.5431      0.90850002  0.0599      0.87299997  2.23729181]
 ...
 [31.0952394   0.0547      0.94600004  0.741       0.95860004  0.60408384]
 [75.37650378  0.31340003  0.63759995  0.55299997  0.68120003  1.41584384]
 [ 1.3062539   0.42159995  0.46879998  0.58280003  0.32049999  0.86418384]][0m
[37m[1m[2023-07-16 23:19:53,203][257371] Max Reward on eval: 183.49368623034098[0m
[37m[1m[2023-07-16 23:19:53,203][257371] Min Reward on eval: -215.03816658519207[0m
[37m[1m[2023-07-16 23:19:53,203][257371] Mean Reward across all agents: 14.250321831715725[0m
[37m[1m[2023-07-16 23:19:53,204][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:19:53,211][257371] mean_value=-546.5937113749342, max_value=570.788473355191[0m
[37m[1m[2023-07-16 23:19:53,214][257371] New mean coefficients: [[ 1.7343818  -2.1050153   1.9208939  -0.33640563  2.2989843  -1.3864816 ]][0m
[37m[1m[2023-07-16 23:19:53,215][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:20:02,414][257371] train() took 9.20 seconds to complete[0m
[36m[2023-07-16 23:20:02,414][257371] FPS: 417538.94[0m
[36m[2023-07-16 23:20:02,416][257371] itr=239, itrs=2000, Progress: 11.95%[0m
[36m[2023-07-16 23:20:14,286][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-16 23:20:14,286][257371] FPS: 324849.70[0m
[36m[2023-07-16 23:20:18,639][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:20:18,640][257371] Reward + Measures: [[25.57334503  0.22515799  0.79168463  0.53039235  0.78821337  0.64723581]][0m
[37m[1m[2023-07-16 23:20:18,640][257371] Max Reward on eval: 25.573345025484723[0m
[37m[1m[2023-07-16 23:20:18,640][257371] Min Reward on eval: 25.573345025484723[0m
[37m[1m[2023-07-16 23:20:18,640][257371] Mean Reward across all agents: 25.573345025484723[0m
[37m[1m[2023-07-16 23:20:18,641][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:20:23,668][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:20:23,668][257371] Reward + Measures: [[ 32.88331068   0.32710001   0.51949996   0.51969999   0.48359999
    1.87643719]
 [ -2.1670732    0.23120001   0.1996       0.23480001   0.20079999
    5.06946087]
 [ 72.68235529   0.1696       0.70660001   0.39860001   0.56080002
    0.94634265]
 ...
 [-85.85753848   0.4481       0.4763       0.48839998   0.24650002
    2.37300038]
 [ -9.24547949   0.2447       0.27290002   0.27130002   0.29010001
    2.39162135]
 [ 58.29371326   0.33140001   0.33259997   0.4104       0.39090002
    2.44208908]][0m
[37m[1m[2023-07-16 23:20:23,669][257371] Max Reward on eval: 236.74502849578857[0m
[37m[1m[2023-07-16 23:20:23,669][257371] Min Reward on eval: -99.26955401068554[0m
[37m[1m[2023-07-16 23:20:23,669][257371] Mean Reward across all agents: 59.890704053718196[0m
[37m[1m[2023-07-16 23:20:23,669][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:20:23,675][257371] mean_value=-862.565103416934, max_value=450.0125549822114[0m
[37m[1m[2023-07-16 23:20:23,678][257371] New mean coefficients: [[ 1.6722804 -1.8639163  0.8730103 -1.1621296  3.3546157 -1.406954 ]][0m
[37m[1m[2023-07-16 23:20:23,679][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:20:32,768][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-16 23:20:32,769][257371] FPS: 422533.79[0m
[36m[2023-07-16 23:20:32,771][257371] itr=240, itrs=2000, Progress: 12.00%[0m
[37m[1m[2023-07-16 23:22:51,341][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000220[0m
[36m[2023-07-16 23:23:03,594][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-16 23:23:03,594][257371] FPS: 327053.57[0m
[36m[2023-07-16 23:23:07,835][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:23:07,836][257371] Reward + Measures: [[21.31159265  0.13370766  0.88457793  0.43545866  0.80181903  1.12839603]][0m
[37m[1m[2023-07-16 23:23:07,836][257371] Max Reward on eval: 21.311592649541016[0m
[37m[1m[2023-07-16 23:23:07,836][257371] Min Reward on eval: 21.311592649541016[0m
[37m[1m[2023-07-16 23:23:07,837][257371] Mean Reward across all agents: 21.311592649541016[0m
[37m[1m[2023-07-16 23:23:07,837][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:23:12,728][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:23:12,729][257371] Reward + Measures: [[ 72.78443742   0.31979999   0.2131       0.39610001   0.25749999
    3.08039832]
 [ 57.80720099   0.40190002   0.33930001   0.5011       0.36929998
    1.8702395 ]
 [141.99119594   0.0116       0.95030004   0.78570002   0.90119994
    3.35751081]
 ...
 [ 12.75511646   0.2342       0.44039997   0.3233       0.3457
    3.65804601]
 [ 56.8044557    0.3804       0.73170006   0.14299999   0.64630002
    3.48485231]
 [ 30.77706074   0.49059996   0.48300001   0.52380008   0.3628
    1.94612622]][0m
[37m[1m[2023-07-16 23:23:12,729][257371] Max Reward on eval: 266.19362254804[0m
[37m[1m[2023-07-16 23:23:12,729][257371] Min Reward on eval: -169.38817995623685[0m
[37m[1m[2023-07-16 23:23:12,729][257371] Mean Reward across all agents: 33.23438052494457[0m
[37m[1m[2023-07-16 23:23:12,730][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:23:12,739][257371] mean_value=-396.414906186915, max_value=617.466730868048[0m
[37m[1m[2023-07-16 23:23:12,742][257371] New mean coefficients: [[ 0.7182242 -1.4350144 -0.5138807 -2.8120024  3.738934  -1.307633 ]][0m
[37m[1m[2023-07-16 23:23:12,743][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:23:21,772][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 23:23:21,773][257371] FPS: 425344.33[0m
[36m[2023-07-16 23:23:21,775][257371] itr=241, itrs=2000, Progress: 12.05%[0m
[36m[2023-07-16 23:23:33,525][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-16 23:23:33,525][257371] FPS: 328159.21[0m
[36m[2023-07-16 23:23:37,767][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:23:37,767][257371] Reward + Measures: [[-55.1009682    0.21544001   0.74273467   0.33743635   0.73832893
    0.82582438]][0m
[37m[1m[2023-07-16 23:23:37,768][257371] Max Reward on eval: -55.100968198965965[0m
[37m[1m[2023-07-16 23:23:37,768][257371] Min Reward on eval: -55.100968198965965[0m
[37m[1m[2023-07-16 23:23:37,768][257371] Mean Reward across all agents: -55.100968198965965[0m
[37m[1m[2023-07-16 23:23:37,768][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:23:42,773][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:23:42,778][257371] Reward + Measures: [[  44.9350094     0.41150004    0.47679996    0.33110002    0.33989999
     2.17449355]
 [   4.65804041    0.1833        0.1997        0.17030001    0.2388
     3.1896255 ]
 [ -32.38450622    0.25509998    0.44580004    0.1265        0.3651
     2.98481178]
 ...
 [  31.02508953    0.23179999    0.62979996    0.25040001    0.67759997
     1.06942308]
 [-104.59052374    0.2359        0.48200002    0.4061        0.59060001
     1.00210631]
 [   1.6720831     0.53750002    0.52889997    0.60519999    0.54540008
     0.96139926]][0m
[37m[1m[2023-07-16 23:23:42,778][257371] Max Reward on eval: 303.1581692151725[0m
[37m[1m[2023-07-16 23:23:42,779][257371] Min Reward on eval: -205.7570109520806[0m
[37m[1m[2023-07-16 23:23:42,779][257371] Mean Reward across all agents: 11.092107352179159[0m
[37m[1m[2023-07-16 23:23:42,779][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:23:42,783][257371] mean_value=-2628.7511005165343, max_value=531.0250895259902[0m
[37m[1m[2023-07-16 23:23:42,785][257371] New mean coefficients: [[ 0.26525044 -0.9260247   0.09569287 -1.3190428   3.7065077  -1.2092612 ]][0m
[37m[1m[2023-07-16 23:23:42,786][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:23:51,813][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 23:23:51,814][257371] FPS: 425456.48[0m
[36m[2023-07-16 23:23:51,816][257371] itr=242, itrs=2000, Progress: 12.10%[0m
[36m[2023-07-16 23:24:03,741][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-16 23:24:03,742][257371] FPS: 323410.22[0m
[36m[2023-07-16 23:24:08,030][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:24:08,031][257371] Reward + Measures: [[-54.47973343   0.20019235   0.73623437   0.32065132   0.7714957
    0.80631208]][0m
[37m[1m[2023-07-16 23:24:08,031][257371] Max Reward on eval: -54.47973343076805[0m
[37m[1m[2023-07-16 23:24:08,031][257371] Min Reward on eval: -54.47973343076805[0m
[37m[1m[2023-07-16 23:24:08,032][257371] Mean Reward across all agents: -54.47973343076805[0m
[37m[1m[2023-07-16 23:24:08,032][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:24:13,084][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:24:13,085][257371] Reward + Measures: [[ -5.92967562   0.4975       0.2728       0.41139999   0.32750002
    1.80424428]
 [171.17870289   0.50640005   0.23050003   0.41230002   0.3678
    2.25007677]
 [ 33.95580645   0.5582       0.60200006   0.45160004   0.64060003
    1.60737598]
 ...
 [ 30.53615751   0.27770004   0.67559999   0.39049998   0.78100008
    0.72860813]
 [-20.24681151   0.37849998   0.3838       0.38280001   0.38860002
    2.66969895]
 [138.61406417   0.64380002   0.1663       0.5054       0.49190003
    1.91893005]][0m
[37m[1m[2023-07-16 23:24:13,085][257371] Max Reward on eval: 277.05979442978276[0m
[37m[1m[2023-07-16 23:24:13,085][257371] Min Reward on eval: -287.40462686163374[0m
[37m[1m[2023-07-16 23:24:13,085][257371] Mean Reward across all agents: 21.203822199650702[0m
[37m[1m[2023-07-16 23:24:13,086][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:24:13,092][257371] mean_value=-648.368586430247, max_value=544.8027446827691[0m
[37m[1m[2023-07-16 23:24:13,095][257371] New mean coefficients: [[ 1.0821005 -1.0221815 -0.5712317 -2.7378798  4.1264296 -1.5001839]][0m
[37m[1m[2023-07-16 23:24:13,096][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:24:22,088][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-16 23:24:22,088][257371] FPS: 427120.82[0m
[36m[2023-07-16 23:24:22,090][257371] itr=243, itrs=2000, Progress: 12.15%[0m
[36m[2023-07-16 23:24:34,092][257371] train() took 11.95 seconds to complete[0m
[36m[2023-07-16 23:24:34,093][257371] FPS: 321406.31[0m
[36m[2023-07-16 23:24:38,415][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:24:38,416][257371] Reward + Measures: [[-90.63080137   0.34076667   0.61570001   0.14240932   0.75650299
    0.88722348]][0m
[37m[1m[2023-07-16 23:24:38,416][257371] Max Reward on eval: -90.63080137085029[0m
[37m[1m[2023-07-16 23:24:38,416][257371] Min Reward on eval: -90.63080137085029[0m
[37m[1m[2023-07-16 23:24:38,417][257371] Mean Reward across all agents: -90.63080137085029[0m
[37m[1m[2023-07-16 23:24:38,417][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:24:43,503][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:24:43,508][257371] Reward + Measures: [[ -74.73241309    0.14390002    0.60670006    0.33229998    0.64609998
     1.54966474]
 [-100.64967847    0.44169998    0.51560003    0.39969999    0.6552
     1.15804327]
 [ -30.8115949     0.15540001    0.53299999    0.31110001    0.58600003
     1.75265431]
 ...
 [ -78.87800447    0.19649999    0.66309994    0.2194        0.73790002
     1.11229956]
 [ -33.23139753    0.19610001    0.54339999    0.19780001    0.53479999
     2.01521778]
 [ -35.51499359    0.1833        0.3414        0.21710001    0.39439997
     2.44228363]][0m
[37m[1m[2023-07-16 23:24:43,509][257371] Max Reward on eval: 159.4383689692244[0m
[37m[1m[2023-07-16 23:24:43,509][257371] Min Reward on eval: -204.3305377786979[0m
[37m[1m[2023-07-16 23:24:43,509][257371] Mean Reward across all agents: -44.11282228910669[0m
[37m[1m[2023-07-16 23:24:43,510][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:24:43,515][257371] mean_value=-1221.7814530453954, max_value=531.8584147598997[0m
[37m[1m[2023-07-16 23:24:43,517][257371] New mean coefficients: [[ 0.73852414 -0.6373632  -0.8741395  -1.9040171   4.5663857  -1.0913987 ]][0m
[37m[1m[2023-07-16 23:24:43,518][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:24:52,556][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-16 23:24:52,556][257371] FPS: 424968.64[0m
[36m[2023-07-16 23:24:52,558][257371] itr=244, itrs=2000, Progress: 12.20%[0m
[36m[2023-07-16 23:25:04,561][257371] train() took 11.95 seconds to complete[0m
[36m[2023-07-16 23:25:04,561][257371] FPS: 321312.69[0m
[36m[2023-07-16 23:25:08,906][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:25:08,906][257371] Reward + Measures: [[-105.21450373    0.30910501    0.62623       0.11050867    0.80688769
     0.90413702]][0m
[37m[1m[2023-07-16 23:25:08,907][257371] Max Reward on eval: -105.21450372832808[0m
[37m[1m[2023-07-16 23:25:08,907][257371] Min Reward on eval: -105.21450372832808[0m
[37m[1m[2023-07-16 23:25:08,907][257371] Mean Reward across all agents: -105.21450372832808[0m
[37m[1m[2023-07-16 23:25:08,907][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:25:14,119][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:25:14,120][257371] Reward + Measures: [[ 35.09997359   0.10480001   0.20800002   0.10359999   0.12050001
    5.25765896]
 [-17.96006816   0.26840001   0.65020001   0.19589999   0.77410001
    1.17049563]
 [  8.05653952   0.1857       0.25359997   0.10950001   0.2402
    3.7319057 ]
 ...
 [102.1033052    0.12169999   0.5697       0.14800002   0.76800001
    2.21040702]
 [ 29.40972191   0.14820002   0.48269996   0.16320002   0.60219997
    3.84411788]
 [ 89.92328256   0.42529997   0.38299999   0.39840001   0.34730002
    1.97920501]][0m
[37m[1m[2023-07-16 23:25:14,120][257371] Max Reward on eval: 134.4770377602894[0m
[37m[1m[2023-07-16 23:25:14,120][257371] Min Reward on eval: -197.25956460917368[0m
[37m[1m[2023-07-16 23:25:14,120][257371] Mean Reward across all agents: 7.812265467733417[0m
[37m[1m[2023-07-16 23:25:14,121][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:25:14,126][257371] mean_value=-1353.8786707807499, max_value=508.6648310845206[0m
[37m[1m[2023-07-16 23:25:14,129][257371] New mean coefficients: [[ 1.878012   -0.8727721  -0.36081356 -1.7527848   4.4682956  -1.414957  ]][0m
[37m[1m[2023-07-16 23:25:14,130][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:25:23,193][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-16 23:25:23,194][257371] FPS: 423760.09[0m
[36m[2023-07-16 23:25:23,196][257371] itr=245, itrs=2000, Progress: 12.25%[0m
[36m[2023-07-16 23:25:34,934][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-16 23:25:34,934][257371] FPS: 328503.44[0m
[36m[2023-07-16 23:25:39,335][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:25:39,335][257371] Reward + Measures: [[15.32139754  0.50934464  0.6882866   0.20962031  0.76034898  0.77548242]][0m
[37m[1m[2023-07-16 23:25:39,335][257371] Max Reward on eval: 15.321397544268645[0m
[37m[1m[2023-07-16 23:25:39,336][257371] Min Reward on eval: 15.321397544268645[0m
[37m[1m[2023-07-16 23:25:39,336][257371] Mean Reward across all agents: 15.321397544268645[0m
[37m[1m[2023-07-16 23:25:39,336][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:25:44,444][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:25:44,449][257371] Reward + Measures: [[ 38.69919098   0.4851       0.79290003   0.1031       0.74080002
    1.56083262]
 [-11.3721673    0.27000001   0.46180001   0.35010001   0.45180002
    2.13410163]
 [101.3942914    0.28299999   0.65359998   0.1227       0.6512
    1.28122675]
 ...
 [148.79176523   0.27700004   0.25040001   0.28340003   0.2572
    3.38652802]
 [ 89.47093605   0.41899997   0.65500003   0.29099998   0.47469997
    1.1611315 ]
 [ 41.50397488   0.47419998   0.59940004   0.29299998   0.51730001
    1.07360399]][0m
[37m[1m[2023-07-16 23:25:44,449][257371] Max Reward on eval: 228.06694997102022[0m
[37m[1m[2023-07-16 23:25:44,450][257371] Min Reward on eval: -149.34301058249548[0m
[37m[1m[2023-07-16 23:25:44,450][257371] Mean Reward across all agents: 46.12791942124231[0m
[37m[1m[2023-07-16 23:25:44,450][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:25:44,457][257371] mean_value=-437.63892525666586, max_value=587.0538055658341[0m
[37m[1m[2023-07-16 23:25:44,460][257371] New mean coefficients: [[ 0.56905437 -0.89980257 -0.48117316 -1.5166678   4.4903383  -1.230378  ]][0m
[37m[1m[2023-07-16 23:25:44,461][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:25:53,651][257371] train() took 9.19 seconds to complete[0m
[36m[2023-07-16 23:25:53,651][257371] FPS: 417928.21[0m
[36m[2023-07-16 23:25:53,654][257371] itr=246, itrs=2000, Progress: 12.30%[0m
[36m[2023-07-16 23:26:05,506][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-16 23:26:05,506][257371] FPS: 325436.30[0m
[36m[2023-07-16 23:26:09,875][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:26:09,875][257371] Reward + Measures: [[8.23415381 0.36540601 0.62155503 0.030649   0.77018833 1.14048839]][0m
[37m[1m[2023-07-16 23:26:09,875][257371] Max Reward on eval: 8.234153809633673[0m
[37m[1m[2023-07-16 23:26:09,876][257371] Min Reward on eval: 8.234153809633673[0m
[37m[1m[2023-07-16 23:26:09,876][257371] Mean Reward across all agents: 8.234153809633673[0m
[37m[1m[2023-07-16 23:26:09,876][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:26:14,924][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:26:14,924][257371] Reward + Measures: [[ 56.90479424   0.20479999   0.44050002   0.2369       0.56999999
    2.48755646]
 [ 98.7294351    0.27509999   0.71220005   0.1622       0.79959995
    1.53957558]
 [189.04039478   0.2034       0.73229998   0.13829999   0.72659999
    1.55303109]
 ...
 [173.57723044   0.27070004   0.48450002   0.2024       0.4657
    1.89072311]
 [181.41002654   0.31040001   0.49250004   0.27110001   0.509
    1.71801531]
 [ 67.64796949   0.21470001   0.76809996   0.23340002   0.72530001
    2.3797729 ]][0m
[37m[1m[2023-07-16 23:26:14,925][257371] Max Reward on eval: 259.39674566315955[0m
[37m[1m[2023-07-16 23:26:14,925][257371] Min Reward on eval: -109.06849151048809[0m
[37m[1m[2023-07-16 23:26:14,925][257371] Mean Reward across all agents: 62.57540635230277[0m
[37m[1m[2023-07-16 23:26:14,925][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:26:14,932][257371] mean_value=-848.6662119836164, max_value=441.8556688361031[0m
[37m[1m[2023-07-16 23:26:14,935][257371] New mean coefficients: [[-0.08034444  0.12630486 -1.2676165  -1.9532442   4.7987137  -1.4323558 ]][0m
[37m[1m[2023-07-16 23:26:14,936][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:26:24,032][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-16 23:26:24,032][257371] FPS: 422250.62[0m
[36m[2023-07-16 23:26:24,034][257371] itr=247, itrs=2000, Progress: 12.35%[0m
[36m[2023-07-16 23:26:35,749][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-16 23:26:35,749][257371] FPS: 329214.93[0m
[36m[2023-07-16 23:26:40,011][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:26:40,011][257371] Reward + Measures: [[62.33086     0.37125868  0.6254397   0.028452    0.74556363  1.39641345]][0m
[37m[1m[2023-07-16 23:26:40,012][257371] Max Reward on eval: 62.3308599959446[0m
[37m[1m[2023-07-16 23:26:40,012][257371] Min Reward on eval: 62.3308599959446[0m
[37m[1m[2023-07-16 23:26:40,012][257371] Mean Reward across all agents: 62.3308599959446[0m
[37m[1m[2023-07-16 23:26:40,012][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:26:45,085][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:26:45,086][257371] Reward + Measures: [[ 130.75452868    0.32780001    0.26140001    0.32280001    0.20280002
     3.09902048]
 [  25.95856104    0.16770001    0.64919996    0.3118        0.72070003
     2.43572879]
 [  84.33996595    0.1533        0.57409996    0.17830001    0.58100003
     2.71371841]
 ...
 [ -84.8083451     0.2832        0.53970003    0.0714        0.59960002
     1.83018517]
 [ -67.52306845    0.42770001    0.39180002    0.33629999    0.35180002
     2.36777449]
 [-130.43859288    0.36750004    0.57860005    0.1866        0.6225
     1.17466438]][0m
[37m[1m[2023-07-16 23:26:45,086][257371] Max Reward on eval: 267.8738961233292[0m
[37m[1m[2023-07-16 23:26:45,086][257371] Min Reward on eval: -178.2268791178707[0m
[37m[1m[2023-07-16 23:26:45,087][257371] Mean Reward across all agents: 25.299874355700666[0m
[37m[1m[2023-07-16 23:26:45,087][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:26:45,094][257371] mean_value=-1001.0749138091089, max_value=712.3021674682759[0m
[37m[1m[2023-07-16 23:26:45,096][257371] New mean coefficients: [[ 0.973554  -1.344568  -1.4226518 -2.4280314  4.2085137 -1.309224 ]][0m
[37m[1m[2023-07-16 23:26:45,097][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:26:54,116][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 23:26:54,117][257371] FPS: 425829.43[0m
[36m[2023-07-16 23:26:54,119][257371] itr=248, itrs=2000, Progress: 12.40%[0m
[36m[2023-07-16 23:27:05,882][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-16 23:27:05,882][257371] FPS: 327938.04[0m
[36m[2023-07-16 23:27:10,207][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:27:10,208][257371] Reward + Measures: [[75.56254775  0.22915533  0.61467034  0.094887    0.785191    1.33095527]][0m
[37m[1m[2023-07-16 23:27:10,208][257371] Max Reward on eval: 75.56254774965468[0m
[37m[1m[2023-07-16 23:27:10,208][257371] Min Reward on eval: 75.56254774965468[0m
[37m[1m[2023-07-16 23:27:10,208][257371] Mean Reward across all agents: 75.56254774965468[0m
[37m[1m[2023-07-16 23:27:10,209][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:27:15,383][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:27:15,384][257371] Reward + Measures: [[ -29.17676446    0.368         0.50480002    0.27290004    0.47850004
     2.13466454]
 [-151.33834903    0.249         0.2823        0.2411        0.28470001
     3.62715149]
 [-154.25038209    0.2491        0.38050002    0.20840001    0.39500001
     1.8401444 ]
 ...
 [ -57.46052269    0.30179998    0.45320001    0.28600001    0.45570001
     2.43609715]
 [-128.87042122    0.20540002    0.55269998    0.14749999    0.69090003
     1.68817651]
 [ -44.08525507    0.35269997    0.32570001    0.32960001    0.3822
     2.48154449]][0m
[37m[1m[2023-07-16 23:27:15,384][257371] Max Reward on eval: 358.5620117379352[0m
[37m[1m[2023-07-16 23:27:15,384][257371] Min Reward on eval: -278.4597709206631[0m
[37m[1m[2023-07-16 23:27:15,384][257371] Mean Reward across all agents: 11.405643423155729[0m
[37m[1m[2023-07-16 23:27:15,385][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:27:15,388][257371] mean_value=-2406.2652485857625, max_value=608.1747342396989[0m
[37m[1m[2023-07-16 23:27:15,391][257371] New mean coefficients: [[ 0.9121617  -0.6632924   0.70724773 -1.9638746   3.9019535  -1.1897202 ]][0m
[37m[1m[2023-07-16 23:27:15,392][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:27:24,439][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 23:27:24,439][257371] FPS: 424512.40[0m
[36m[2023-07-16 23:27:24,441][257371] itr=249, itrs=2000, Progress: 12.45%[0m
[36m[2023-07-16 23:27:36,247][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-16 23:27:36,247][257371] FPS: 326631.02[0m
[36m[2023-07-16 23:27:40,584][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:27:40,584][257371] Reward + Measures: [[259.40032481   0.17685767   0.61986566   0.29384667   0.68815297
    1.36621106]][0m
[37m[1m[2023-07-16 23:27:40,584][257371] Max Reward on eval: 259.4003248086669[0m
[37m[1m[2023-07-16 23:27:40,584][257371] Min Reward on eval: 259.4003248086669[0m
[37m[1m[2023-07-16 23:27:40,585][257371] Mean Reward across all agents: 259.4003248086669[0m
[37m[1m[2023-07-16 23:27:40,585][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:27:45,632][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:27:45,632][257371] Reward + Measures: [[-200.64076921    0.19240001    0.42120001    0.29889998    0.52030003
     2.59810948]
 [  -8.18697469    0.21960001    0.42559996    0.1323        0.33090001
     2.96687484]
 [  -6.74496444    0.37450004    0.35570002    0.39739999    0.3653
     2.02976465]
 ...
 [ -18.2096509     0.2712        0.69419998    0.1596        0.73390001
     1.56709921]
 [ 196.17092608    0.19990002    0.36780003    0.27069998    0.3231
     2.91923213]
 [ -29.08267315    0.27159998    0.39160001    0.37760001    0.43250003
     2.97342849]][0m
[37m[1m[2023-07-16 23:27:45,632][257371] Max Reward on eval: 299.9864130087197[0m
[37m[1m[2023-07-16 23:27:45,633][257371] Min Reward on eval: -441.56415179315957[0m
[37m[1m[2023-07-16 23:27:45,633][257371] Mean Reward across all agents: 34.881401668949465[0m
[37m[1m[2023-07-16 23:27:45,633][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:27:45,637][257371] mean_value=-1754.2380522571143, max_value=299.34619140315124[0m
[37m[1m[2023-07-16 23:27:45,639][257371] New mean coefficients: [[ 1.5351665  -0.7793545   0.49149722 -1.7341379   3.685333   -1.3687125 ]][0m
[37m[1m[2023-07-16 23:27:45,640][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:27:54,753][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-16 23:27:54,753][257371] FPS: 421478.80[0m
[36m[2023-07-16 23:27:54,755][257371] itr=250, itrs=2000, Progress: 12.50%[0m
[37m[1m[2023-07-16 23:30:13,157][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000230[0m
[36m[2023-07-16 23:30:25,563][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-16 23:30:25,564][257371] FPS: 324274.33[0m
[36m[2023-07-16 23:30:29,879][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:30:29,879][257371] Reward + Measures: [[53.40165641  0.16359501  0.73405898  0.24529399  0.83448666  1.79137433]][0m
[37m[1m[2023-07-16 23:30:29,879][257371] Max Reward on eval: 53.401656413696166[0m
[37m[1m[2023-07-16 23:30:29,880][257371] Min Reward on eval: 53.401656413696166[0m
[37m[1m[2023-07-16 23:30:29,880][257371] Mean Reward across all agents: 53.401656413696166[0m
[37m[1m[2023-07-16 23:30:29,880][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:30:34,876][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:30:34,877][257371] Reward + Measures: [[-24.50763144   0.33070001   0.31739998   0.3928       0.32690001
    1.8752476 ]
 [  2.27850565   0.34129998   0.44119999   0.42809996   0.5485
    1.66061485]
 [ 98.21288402   0.34699997   0.45040002   0.42680001   0.50159997
    1.72651517]
 ...
 [260.24507354   0.13109998   0.68289995   0.30469999   0.68990004
    2.18456268]
 [145.66075701   0.10930001   0.57389998   0.26270005   0.708
    2.27847433]
 [115.51667898   0.24000001   0.36780003   0.28959998   0.34440002
    3.82908702]][0m
[37m[1m[2023-07-16 23:30:34,877][257371] Max Reward on eval: 358.145230296487[0m
[37m[1m[2023-07-16 23:30:34,877][257371] Min Reward on eval: -143.7075649023056[0m
[37m[1m[2023-07-16 23:30:34,878][257371] Mean Reward across all agents: 86.10240617230063[0m
[37m[1m[2023-07-16 23:30:34,878][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:30:34,885][257371] mean_value=-709.1750359140527, max_value=564.9331147043243[0m
[37m[1m[2023-07-16 23:30:34,888][257371] New mean coefficients: [[ 1.5129541 -0.6121236  0.0469695 -2.4693322  3.4259405 -1.5891389]][0m
[37m[1m[2023-07-16 23:30:34,889][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:30:43,757][257371] train() took 8.87 seconds to complete[0m
[36m[2023-07-16 23:30:43,758][257371] FPS: 433057.62[0m
[36m[2023-07-16 23:30:43,760][257371] itr=251, itrs=2000, Progress: 12.55%[0m
[36m[2023-07-16 23:30:55,630][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-16 23:30:55,631][257371] FPS: 324963.42[0m
[36m[2023-07-16 23:31:00,013][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:31:00,014][257371] Reward + Measures: [[70.20276777  0.087985    0.77117628  0.33792201  0.86969793  0.81071925]][0m
[37m[1m[2023-07-16 23:31:00,014][257371] Max Reward on eval: 70.20276777105146[0m
[37m[1m[2023-07-16 23:31:00,014][257371] Min Reward on eval: 70.20276777105146[0m
[37m[1m[2023-07-16 23:31:00,015][257371] Mean Reward across all agents: 70.20276777105146[0m
[37m[1m[2023-07-16 23:31:00,015][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:31:05,269][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:31:05,270][257371] Reward + Measures: [[109.81036283   0.37189999   0.78850001   0.26070005   0.81029999
    1.55367947]
 [-20.42058869   0.09250001   0.80100006   0.3177       0.78890002
    1.46124923]
 [-82.76419662   0.27160001   0.54290003   0.42180005   0.67500001
    1.05328643]
 ...
 [ 58.15887099   0.13430001   0.23980001   0.22219999   0.27959999
    2.96140432]
 [ 45.03674873   0.0437       0.81680006   0.47549996   0.79020005
    0.85944569]
 [ 75.45814086   0.19840001   0.41         0.273        0.40000001
    3.10825467]][0m
[37m[1m[2023-07-16 23:31:05,270][257371] Max Reward on eval: 334.10031316429377[0m
[37m[1m[2023-07-16 23:31:05,270][257371] Min Reward on eval: -138.02918770667165[0m
[37m[1m[2023-07-16 23:31:05,270][257371] Mean Reward across all agents: 26.608886825456104[0m
[37m[1m[2023-07-16 23:31:05,271][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:31:05,274][257371] mean_value=-530.661497657726, max_value=440.65514808789[0m
[37m[1m[2023-07-16 23:31:05,277][257371] New mean coefficients: [[ 0.6896433  -0.15553173  0.29471976 -1.1350836   3.43662    -1.6958896 ]][0m
[37m[1m[2023-07-16 23:31:05,278][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:31:14,474][257371] train() took 9.19 seconds to complete[0m
[36m[2023-07-16 23:31:14,474][257371] FPS: 417641.29[0m
[36m[2023-07-16 23:31:14,477][257371] itr=252, itrs=2000, Progress: 12.60%[0m
[36m[2023-07-16 23:31:26,182][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-16 23:31:26,182][257371] FPS: 329470.56[0m
[36m[2023-07-16 23:31:30,477][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:31:30,478][257371] Reward + Measures: [[86.64168935  0.07283466  0.78532225  0.37025967  0.89756405  0.78958529]][0m
[37m[1m[2023-07-16 23:31:30,478][257371] Max Reward on eval: 86.64168935282964[0m
[37m[1m[2023-07-16 23:31:30,478][257371] Min Reward on eval: 86.64168935282964[0m
[37m[1m[2023-07-16 23:31:30,478][257371] Mean Reward across all agents: 86.64168935282964[0m
[37m[1m[2023-07-16 23:31:30,479][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:31:35,467][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:31:35,473][257371] Reward + Measures: [[-14.00621458   0.16940001   0.68489999   0.49860001   0.5873
    1.58243775]
 [ 60.44396588   0.1978       0.42300001   0.2836       0.53759998
    2.49399066]
 [128.36518095   0.14310001   0.7301001    0.2956       0.61440003
    1.77060568]
 ...
 [ -8.92453046   0.26760003   0.51640004   0.43090001   0.64239997
    1.04073429]
 [-87.864686     0.14659999   0.80320007   0.34329998   0.61140001
    1.09638143]
 [ -5.89019546   0.0945       0.75980002   0.43280002   0.81840003
    1.05721951]][0m
[37m[1m[2023-07-16 23:31:35,473][257371] Max Reward on eval: 257.99725152365863[0m
[37m[1m[2023-07-16 23:31:35,473][257371] Min Reward on eval: -221.20640466730111[0m
[37m[1m[2023-07-16 23:31:35,474][257371] Mean Reward across all agents: 39.096212653349596[0m
[37m[1m[2023-07-16 23:31:35,474][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:31:35,480][257371] mean_value=-363.51183797789133, max_value=463.9217500739565[0m
[37m[1m[2023-07-16 23:31:35,483][257371] New mean coefficients: [[ 0.41808695 -0.3405044   0.12728761 -1.0035785   3.6337774  -1.5274801 ]][0m
[37m[1m[2023-07-16 23:31:35,484][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:31:44,473][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-16 23:31:44,473][257371] FPS: 427254.94[0m
[36m[2023-07-16 23:31:44,475][257371] itr=253, itrs=2000, Progress: 12.65%[0m
[36m[2023-07-16 23:31:56,456][257371] train() took 11.93 seconds to complete[0m
[36m[2023-07-16 23:31:56,456][257371] FPS: 321867.10[0m
[36m[2023-07-16 23:32:00,770][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:32:00,770][257371] Reward + Measures: [[78.90250061  0.35218433  0.85739934  0.24036434  0.89506674  0.90882552]][0m
[37m[1m[2023-07-16 23:32:00,770][257371] Max Reward on eval: 78.90250060556622[0m
[37m[1m[2023-07-16 23:32:00,771][257371] Min Reward on eval: 78.90250060556622[0m
[37m[1m[2023-07-16 23:32:00,771][257371] Mean Reward across all agents: 78.90250060556622[0m
[37m[1m[2023-07-16 23:32:00,771][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:32:05,795][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:32:05,795][257371] Reward + Measures: [[ 23.0785028    0.30490002   0.69239998   0.33440003   0.69749999
    0.92500252]
 [-21.4638941    0.39810002   0.49559999   0.26799998   0.61259997
    1.39242101]
 [  0.02917032   0.92670006   0.45479998   0.91379994   0.73250002
    0.75783271]
 ...
 [133.86913228   0.21270001   0.55980003   0.28740001   0.39190003
    2.83151364]
 [192.1338272    0.1621       0.40079999   0.2          0.3229
    3.97128844]
 [ 45.19355027   0.33280003   0.66550004   0.1758       0.79299992
    1.09749973]][0m
[37m[1m[2023-07-16 23:32:05,796][257371] Max Reward on eval: 260.24918274078516[0m
[37m[1m[2023-07-16 23:32:05,796][257371] Min Reward on eval: -148.88524911049754[0m
[37m[1m[2023-07-16 23:32:05,796][257371] Mean Reward across all agents: 35.817726442069485[0m
[37m[1m[2023-07-16 23:32:05,796][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:32:05,803][257371] mean_value=-697.827512666159, max_value=511.5324542372115[0m
[37m[1m[2023-07-16 23:32:05,805][257371] New mean coefficients: [[ 0.14515641  0.41261578  0.85907614 -0.8931944   2.8766732  -1.5352441 ]][0m
[37m[1m[2023-07-16 23:32:05,806][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:32:14,913][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-16 23:32:14,913][257371] FPS: 421740.45[0m
[36m[2023-07-16 23:32:14,916][257371] itr=254, itrs=2000, Progress: 12.70%[0m
[36m[2023-07-16 23:32:26,703][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-16 23:32:26,703][257371] FPS: 327176.85[0m
[36m[2023-07-16 23:32:31,012][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:32:31,012][257371] Reward + Measures: [[-30.92135786   0.32381332   0.8285464    0.31029364   0.837569
    0.90824682]][0m
[37m[1m[2023-07-16 23:32:31,013][257371] Max Reward on eval: -30.921357855447337[0m
[37m[1m[2023-07-16 23:32:31,013][257371] Min Reward on eval: -30.921357855447337[0m
[37m[1m[2023-07-16 23:32:31,013][257371] Mean Reward across all agents: -30.921357855447337[0m
[37m[1m[2023-07-16 23:32:31,013][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:32:36,031][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:32:36,031][257371] Reward + Measures: [[  55.69580565    0.131         0.91049999    0.15079999    0.9285
     1.39888561]
 [  -1.30545744    0.29369998    0.70640004    0.27540001    0.65640002
     1.55233109]
 [ -57.10992175    0.46750003    0.37540004    0.39739999    0.35170004
     1.7879082 ]
 ...
 [  20.53224572    0.24319999    0.93129998    0.16600001    0.93899995
     1.04132879]
 [  74.9224173     0.30500004    0.62870002    0.22650002    0.64289999
     2.5692029 ]
 [-111.03957482    0.4894        0.68219995    0.27860001    0.61580002
     1.09232616]][0m
[37m[1m[2023-07-16 23:32:36,032][257371] Max Reward on eval: 141.4004840732552[0m
[37m[1m[2023-07-16 23:32:36,032][257371] Min Reward on eval: -221.41165065222884[0m
[37m[1m[2023-07-16 23:32:36,032][257371] Mean Reward across all agents: -18.643431516851713[0m
[37m[1m[2023-07-16 23:32:36,032][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:32:36,039][257371] mean_value=-315.2711536829098, max_value=398.6915770561224[0m
[37m[1m[2023-07-16 23:32:36,042][257371] New mean coefficients: [[ 1.1063718   0.16593325  0.7144729  -1.8589156   2.8701558  -2.1704187 ]][0m
[37m[1m[2023-07-16 23:32:36,043][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:32:45,095][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 23:32:45,096][257371] FPS: 424262.33[0m
[36m[2023-07-16 23:32:45,098][257371] itr=255, itrs=2000, Progress: 12.75%[0m
[36m[2023-07-16 23:32:56,985][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-16 23:32:56,986][257371] FPS: 324556.26[0m
[36m[2023-07-16 23:33:01,253][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:33:01,253][257371] Reward + Measures: [[17.10616738  0.18616866  0.846825    0.21396767  0.86546761  0.75103825]][0m
[37m[1m[2023-07-16 23:33:01,253][257371] Max Reward on eval: 17.10616737837775[0m
[37m[1m[2023-07-16 23:33:01,254][257371] Min Reward on eval: 17.10616737837775[0m
[37m[1m[2023-07-16 23:33:01,254][257371] Mean Reward across all agents: 17.10616737837775[0m
[37m[1m[2023-07-16 23:33:01,254][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:33:06,275][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:33:06,280][257371] Reward + Measures: [[ 104.25340724    0.25600001    0.55310005    0.17650001    0.52360004
     2.7019155 ]
 [-124.76149585    0.36139998    0.42919999    0.2498        0.51410002
     2.93262482]
 [ -49.92316676    0.37040001    0.54770005    0.27590001    0.61220002
     1.86204469]
 ...
 [  82.02302321    0.38150001    0.49000001    0.35819998    0.51719999
     2.52230263]
 [ 110.8649346     0.25980002    0.65100002    0.2246        0.65169996
     1.44359887]
 [  72.97776865    0.25620002    0.58199996    0.20019999    0.56680006
     2.12660384]][0m
[37m[1m[2023-07-16 23:33:06,281][257371] Max Reward on eval: 419.73546029938154[0m
[37m[1m[2023-07-16 23:33:06,281][257371] Min Reward on eval: -232.15446469755844[0m
[37m[1m[2023-07-16 23:33:06,281][257371] Mean Reward across all agents: 38.07773792943695[0m
[37m[1m[2023-07-16 23:33:06,281][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:33:06,286][257371] mean_value=-905.3702050962629, max_value=377.83049136721127[0m
[37m[1m[2023-07-16 23:33:06,288][257371] New mean coefficients: [[ 0.6873741   0.5849395   1.5080082  -0.60439754  2.1581898  -2.2041936 ]][0m
[37m[1m[2023-07-16 23:33:06,289][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:33:15,311][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-16 23:33:15,311][257371] FPS: 425738.56[0m
[36m[2023-07-16 23:33:15,313][257371] itr=256, itrs=2000, Progress: 12.80%[0m
[36m[2023-07-16 23:33:27,015][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-16 23:33:27,015][257371] FPS: 329571.23[0m
[36m[2023-07-16 23:33:31,443][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:33:31,443][257371] Reward + Measures: [[138.97113656   0.25583401   0.8583104    0.37305802   0.86549437
    0.73929816]][0m
[37m[1m[2023-07-16 23:33:31,443][257371] Max Reward on eval: 138.97113655716026[0m
[37m[1m[2023-07-16 23:33:31,444][257371] Min Reward on eval: 138.97113655716026[0m
[37m[1m[2023-07-16 23:33:31,444][257371] Mean Reward across all agents: 138.97113655716026[0m
[37m[1m[2023-07-16 23:33:31,444][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:33:36,679][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:33:36,684][257371] Reward + Measures: [[-148.14271825    0.28340003    0.35250002    0.26369998    0.35800001
     2.34168029]
 [  52.3233165     0.2586        0.63950002    0.43010002    0.61439997
     2.08614445]
 [ -73.4584383     0.27160001    0.63630003    0.36719999    0.67640001
     1.03475249]
 ...
 [-154.54443143    0.29869998    0.34689999    0.36130005    0.3529
     2.19245934]
 [ -44.6256619     0.22540002    0.58280003    0.2798        0.56010002
     1.06530058]
 [ -92.99633234    0.2483        0.72329998    0.3272        0.68020004
     0.93385535]][0m
[37m[1m[2023-07-16 23:33:36,685][257371] Max Reward on eval: 212.6326193725923[0m
[37m[1m[2023-07-16 23:33:36,685][257371] Min Reward on eval: -224.13130422774702[0m
[37m[1m[2023-07-16 23:33:36,685][257371] Mean Reward across all agents: 2.2701228158441524[0m
[37m[1m[2023-07-16 23:33:36,686][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:33:36,691][257371] mean_value=-982.113799451115, max_value=492.3441575716474[0m
[37m[1m[2023-07-16 23:33:36,694][257371] New mean coefficients: [[-0.01831865  0.41017872  2.1262133   0.8020135   1.8580569  -2.3761635 ]][0m
[37m[1m[2023-07-16 23:33:36,695][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:33:45,840][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-16 23:33:45,840][257371] FPS: 419952.61[0m
[36m[2023-07-16 23:33:45,843][257371] itr=257, itrs=2000, Progress: 12.85%[0m
[36m[2023-07-16 23:33:57,647][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-16 23:33:57,647][257371] FPS: 326694.37[0m
[36m[2023-07-16 23:34:01,964][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:34:01,964][257371] Reward + Measures: [[-11.23002915   0.17310099   0.89357764   0.43820834   0.9098857
    0.73858958]][0m
[37m[1m[2023-07-16 23:34:01,964][257371] Max Reward on eval: -11.230029147739085[0m
[37m[1m[2023-07-16 23:34:01,965][257371] Min Reward on eval: -11.230029147739085[0m
[37m[1m[2023-07-16 23:34:01,965][257371] Mean Reward across all agents: -11.230029147739085[0m
[37m[1m[2023-07-16 23:34:01,965][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:34:06,994][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:34:06,999][257371] Reward + Measures: [[  4.19380902   0.29889998   0.70849997   0.39040002   0.74470001
    2.06950593]
 [ 77.05019568   0.08639999   0.94410002   0.49239999   0.91650009
    1.04798245]
 [ 99.52205722   0.65850002   0.54500002   0.6415       0.16129999
    0.80703145]
 ...
 [-44.91477505   0.26340002   0.49350005   0.3407       0.48690006
    1.97712898]
 [  0.50390276   0.38659999   0.57210004   0.18629999   0.53510004
    1.30826318]
 [-34.08277941   0.3628       0.6674       0.45580003   0.64810002
    1.09852648]][0m
[37m[1m[2023-07-16 23:34:07,000][257371] Max Reward on eval: 243.84991355491803[0m
[37m[1m[2023-07-16 23:34:07,000][257371] Min Reward on eval: -250.74950600854064[0m
[37m[1m[2023-07-16 23:34:07,000][257371] Mean Reward across all agents: 7.892205964709222[0m
[37m[1m[2023-07-16 23:34:07,001][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:34:07,006][257371] mean_value=-382.0483672181361, max_value=558.1477906584739[0m
[37m[1m[2023-07-16 23:34:07,009][257371] New mean coefficients: [[ 0.41062742  0.49600866  0.93690145  0.14331257  2.281724   -2.5786602 ]][0m
[37m[1m[2023-07-16 23:34:07,010][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:34:16,137][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-16 23:34:16,137][257371] FPS: 420827.25[0m
[36m[2023-07-16 23:34:16,139][257371] itr=258, itrs=2000, Progress: 12.90%[0m
[36m[2023-07-16 23:34:28,028][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-16 23:34:28,029][257371] FPS: 324460.03[0m
[36m[2023-07-16 23:34:32,344][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:34:32,345][257371] Reward + Measures: [[-1.1520892   0.29561833  0.88981968  0.38795263  0.85635096  0.65189624]][0m
[37m[1m[2023-07-16 23:34:32,345][257371] Max Reward on eval: -1.1520891984815074[0m
[37m[1m[2023-07-16 23:34:32,345][257371] Min Reward on eval: -1.1520891984815074[0m
[37m[1m[2023-07-16 23:34:32,345][257371] Mean Reward across all agents: -1.1520891984815074[0m
[37m[1m[2023-07-16 23:34:32,346][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:34:37,339][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:34:37,340][257371] Reward + Measures: [[ 18.93750399   0.49379998   0.41459998   0.46340004   0.24300003
    2.81494641]
 [114.17908891   0.42209998   0.34220001   0.42030001   0.19680001
    1.71455789]
 [ 72.07746649   0.44040003   0.65469998   0.234        0.68710005
    1.19148195]
 ...
 [ 26.57679587   0.47310001   0.68330002   0.40449998   0.5783
    1.45611835]
 [ 62.82606043   0.42670003   0.3795       0.36810002   0.2969
    2.8781631 ]
 [  0.29214388   0.41799998   0.26920006   0.38240001   0.30360001
    1.92357945]][0m
[37m[1m[2023-07-16 23:34:37,340][257371] Max Reward on eval: 130.83800218049436[0m
[37m[1m[2023-07-16 23:34:37,341][257371] Min Reward on eval: -110.30913544641808[0m
[37m[1m[2023-07-16 23:34:37,341][257371] Mean Reward across all agents: 17.761441396467205[0m
[37m[1m[2023-07-16 23:34:37,341][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:34:37,346][257371] mean_value=-693.8416586114636, max_value=423.45020593100566[0m
[37m[1m[2023-07-16 23:34:37,349][257371] New mean coefficients: [[ 1.2701547  -0.21827081  0.73921424 -0.53380364  1.6318684  -2.1034775 ]][0m
[37m[1m[2023-07-16 23:34:37,350][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:34:46,358][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 23:34:46,359][257371] FPS: 426333.83[0m
[36m[2023-07-16 23:34:46,361][257371] itr=259, itrs=2000, Progress: 12.95%[0m
[36m[2023-07-16 23:34:58,364][257371] train() took 11.95 seconds to complete[0m
[36m[2023-07-16 23:34:58,365][257371] FPS: 321274.46[0m
[36m[2023-07-16 23:35:02,706][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:35:02,706][257371] Reward + Measures: [[-56.49910996   0.44077599   0.81748599   0.54584795   0.80935097
    0.50131577]][0m
[37m[1m[2023-07-16 23:35:02,706][257371] Max Reward on eval: -56.499109958548374[0m
[37m[1m[2023-07-16 23:35:02,707][257371] Min Reward on eval: -56.499109958548374[0m
[37m[1m[2023-07-16 23:35:02,707][257371] Mean Reward across all agents: -56.499109958548374[0m
[37m[1m[2023-07-16 23:35:02,707][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:35:07,747][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:35:07,747][257371] Reward + Measures: [[ -85.09481257    0.38779998    0.55439997    0.35330001    0.685
     1.56876206]
 [  61.88480512    0.30539998    0.46669999    0.3378        0.58960003
     1.40336382]
 [  15.3928974     0.34349999    0.65900004    0.39089999    0.70280004
     0.93376428]
 ...
 [  36.42453603    0.37180001    0.73629999    0.30879998    0.72259998
     0.78234476]
 [-145.96158807    0.3299        0.24609999    0.3351        0.35010001
     2.53750491]
 [   6.22068527    0.58530003    0.77229995    0.22989999    0.75000006
     1.77564907]][0m
[37m[1m[2023-07-16 23:35:07,748][257371] Max Reward on eval: 172.5584993342869[0m
[37m[1m[2023-07-16 23:35:07,748][257371] Min Reward on eval: -216.9974869167898[0m
[37m[1m[2023-07-16 23:35:07,748][257371] Mean Reward across all agents: -34.31643957950116[0m
[37m[1m[2023-07-16 23:35:07,748][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:35:07,751][257371] mean_value=-1011.0227049552622, max_value=494.8003645446748[0m
[37m[1m[2023-07-16 23:35:07,754][257371] New mean coefficients: [[ 0.69283247 -0.15565094  1.6173959  -0.02988559  1.0368302  -1.2134502 ]][0m
[37m[1m[2023-07-16 23:35:07,755][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:35:16,935][257371] train() took 9.18 seconds to complete[0m
[36m[2023-07-16 23:35:16,935][257371] FPS: 418374.20[0m
[36m[2023-07-16 23:35:16,937][257371] itr=260, itrs=2000, Progress: 13.00%[0m
[37m[1m[2023-07-16 23:37:48,178][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000240[0m
[36m[2023-07-16 23:38:00,688][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-16 23:38:00,688][257371] FPS: 324527.16[0m
[36m[2023-07-16 23:38:05,016][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:38:05,017][257371] Reward + Measures: [[-45.68576622   0.19193201   0.95880026   0.26163533   0.924101
    0.66221702]][0m
[37m[1m[2023-07-16 23:38:05,017][257371] Max Reward on eval: -45.685766223273326[0m
[37m[1m[2023-07-16 23:38:05,017][257371] Min Reward on eval: -45.685766223273326[0m
[37m[1m[2023-07-16 23:38:05,017][257371] Mean Reward across all agents: -45.685766223273326[0m
[37m[1m[2023-07-16 23:38:05,018][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:38:09,954][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:38:09,954][257371] Reward + Measures: [[  22.00496971    0.54629999    0.81150001    0.0916        0.82970011
     1.33377731]
 [  22.86443538    0.2131        0.79559994    0.32210001    0.74650002
     1.00948942]
 [  -9.58834638    0.4007        0.86970007    0.1556        0.89600003
     0.78356832]
 ...
 [-110.65980877    0.32300001    0.41260001    0.41890001    0.3416
     1.86407268]
 [   8.47951527    0.14210001    0.8301        0.37909999    0.75889999
     1.065539  ]
 [ -56.95336892    0.46429998    0.72049999    0.17800002    0.73690003
     0.96290249]][0m
[37m[1m[2023-07-16 23:38:09,955][257371] Max Reward on eval: 340.8433531373739[0m
[37m[1m[2023-07-16 23:38:09,955][257371] Min Reward on eval: -230.95086381062865[0m
[37m[1m[2023-07-16 23:38:09,955][257371] Mean Reward across all agents: 7.891007018210305[0m
[37m[1m[2023-07-16 23:38:09,955][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:38:09,960][257371] mean_value=-785.6481852447604, max_value=326.6604586127645[0m
[37m[1m[2023-07-16 23:38:09,963][257371] New mean coefficients: [[-0.1885038  0.6645012  1.8508307 -0.0899512  0.8233174 -1.5422095]][0m
[37m[1m[2023-07-16 23:38:09,963][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:38:18,844][257371] train() took 8.88 seconds to complete[0m
[36m[2023-07-16 23:38:18,844][257371] FPS: 432510.43[0m
[36m[2023-07-16 23:38:18,846][257371] itr=261, itrs=2000, Progress: 13.05%[0m
[36m[2023-07-16 23:38:30,454][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-16 23:38:30,455][257371] FPS: 332237.15[0m
[36m[2023-07-16 23:38:34,665][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:38:34,665][257371] Reward + Measures: [[74.92472947  0.28352267  0.90503931  0.22118632  0.9031893   0.74954629]][0m
[37m[1m[2023-07-16 23:38:34,665][257371] Max Reward on eval: 74.9247294743678[0m
[37m[1m[2023-07-16 23:38:34,666][257371] Min Reward on eval: 74.9247294743678[0m
[37m[1m[2023-07-16 23:38:34,666][257371] Mean Reward across all agents: 74.9247294743678[0m
[37m[1m[2023-07-16 23:38:34,666][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:38:39,080][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:38:39,080][257371] Reward + Measures: [[ 23.92552424   0.39359999   0.63279998   0.53109998   0.54089999
    1.26971996]
 [  4.00954238   0.35670003   0.82380003   0.27729997   0.74560004
    1.47186553]
 [ 20.15846833   0.22839999   0.32259998   0.3046       0.29860002
    3.41889644]
 ...
 [ 64.59123923   0.2859       0.37360001   0.30140004   0.32830003
    2.84080696]
 [ 71.49621986   0.30650002   0.55100006   0.266        0.52559996
    1.99501073]
 [-20.75085736   0.354        0.47550002   0.479        0.50920004
    1.29001915]][0m
[37m[1m[2023-07-16 23:38:39,081][257371] Max Reward on eval: 369.8626861887518[0m
[37m[1m[2023-07-16 23:38:39,081][257371] Min Reward on eval: -96.46792794578941[0m
[37m[1m[2023-07-16 23:38:39,081][257371] Mean Reward across all agents: 27.07904317907864[0m
[37m[1m[2023-07-16 23:38:39,081][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:38:39,095][257371] mean_value=-650.5047835594348, max_value=470.747962747795[0m
[37m[1m[2023-07-16 23:38:39,097][257371] New mean coefficients: [[-1.4172912   1.0755639   2.116829    1.3784777   0.6863101  -0.38412488]][0m
[37m[1m[2023-07-16 23:38:39,098][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:38:48,379][257371] train() took 9.28 seconds to complete[0m
[36m[2023-07-16 23:38:48,379][257371] FPS: 413835.25[0m
[36m[2023-07-16 23:38:48,381][257371] itr=262, itrs=2000, Progress: 13.10%[0m
[36m[2023-07-16 23:39:03,826][257371] train() took 15.39 seconds to complete[0m
[36m[2023-07-16 23:39:03,827][257371] FPS: 249477.04[0m
[36m[2023-07-16 23:39:09,270][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:39:09,271][257371] Reward + Measures: [[17.0447921   0.40999031  0.97476131  0.417806    0.97220165  1.83648026]][0m
[37m[1m[2023-07-16 23:39:09,271][257371] Max Reward on eval: 17.044792104363996[0m
[37m[1m[2023-07-16 23:39:09,271][257371] Min Reward on eval: 17.044792104363996[0m
[37m[1m[2023-07-16 23:39:09,271][257371] Mean Reward across all agents: 17.044792104363996[0m
[37m[1m[2023-07-16 23:39:09,271][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:39:15,085][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:39:15,086][257371] Reward + Measures: [[-37.84160614   0.40019998   0.40270001   0.40570003   0.45170003
    3.35467792]
 [ 52.51478492   0.21370001   0.20479999   0.22329998   0.21920002
    3.22164154]
 [ 41.53626153   0.33500001   0.47589999   0.42499995   0.45860001
    2.50671411]
 ...
 [-10.3371517    0.26340002   0.49779996   0.43340001   0.4666
    1.97880018]
 [108.1891208    0.31130001   0.67120004   0.63430005   0.6031
    1.04137385]
 [101.55733775   0.12210001   0.85060006   0.39970002   0.85060006
    2.45997643]][0m
[37m[1m[2023-07-16 23:39:15,086][257371] Max Reward on eval: 358.4975433518295[0m
[37m[1m[2023-07-16 23:39:15,086][257371] Min Reward on eval: -287.4054927970748[0m
[37m[1m[2023-07-16 23:39:15,086][257371] Mean Reward across all agents: 54.917225852021915[0m
[37m[1m[2023-07-16 23:39:15,086][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:39:15,095][257371] mean_value=-691.4225584178782, max_value=572.7073616869748[0m
[37m[1m[2023-07-16 23:39:15,099][257371] New mean coefficients: [[-0.59967864  0.8876622   2.7361808   1.0369381   0.35405657 -0.6088043 ]][0m
[37m[1m[2023-07-16 23:39:15,100][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:39:24,528][257371] train() took 9.43 seconds to complete[0m
[36m[2023-07-16 23:39:24,528][257371] FPS: 407402.88[0m
[36m[2023-07-16 23:39:24,530][257371] itr=263, itrs=2000, Progress: 13.15%[0m
[36m[2023-07-16 23:39:36,363][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-16 23:39:36,364][257371] FPS: 325912.36[0m
[36m[2023-07-16 23:39:40,686][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:39:40,686][257371] Reward + Measures: [[-79.21752166   0.21308233   0.92826396   0.64767933   0.87312597
    0.69916868]][0m
[37m[1m[2023-07-16 23:39:40,687][257371] Max Reward on eval: -79.21752165882356[0m
[37m[1m[2023-07-16 23:39:40,687][257371] Min Reward on eval: -79.21752165882356[0m
[37m[1m[2023-07-16 23:39:40,687][257371] Mean Reward across all agents: -79.21752165882356[0m
[37m[1m[2023-07-16 23:39:40,687][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:39:45,865][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:39:45,866][257371] Reward + Measures: [[495.01389317   0.25839999   0.34380004   0.37149999   0.25620002
    2.49546933]
 [-32.08892883   0.68610001   0.7726       0.7384001    0.68360007
    0.93381035]
 [ 92.93543649   0.3836       0.4269       0.54890001   0.32300001
    1.33541238]
 ...
 [ 53.63946439   0.44850001   0.5794       0.65220004   0.4032
    0.97078294]
 [245.19263985   0.3204       0.3712       0.3721       0.27359998
    2.26678872]
 [ 48.01722753   0.39669999   0.40190002   0.44679999   0.26030001
    2.1185174 ]][0m
[37m[1m[2023-07-16 23:39:45,866][257371] Max Reward on eval: 503.3764266998507[0m
[37m[1m[2023-07-16 23:39:45,866][257371] Min Reward on eval: -142.34048557961358[0m
[37m[1m[2023-07-16 23:39:45,866][257371] Mean Reward across all agents: 55.25152239201272[0m
[37m[1m[2023-07-16 23:39:45,867][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:39:45,871][257371] mean_value=-660.1718114973643, max_value=673.254120183714[0m
[37m[1m[2023-07-16 23:39:45,873][257371] New mean coefficients: [[-1.3471686   0.889266    2.9829183   1.426676   -0.05896702 -0.24121961]][0m
[37m[1m[2023-07-16 23:39:45,874][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:39:54,938][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-16 23:39:54,938][257371] FPS: 423742.94[0m
[36m[2023-07-16 23:39:54,940][257371] itr=264, itrs=2000, Progress: 13.20%[0m
[36m[2023-07-16 23:40:06,722][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-16 23:40:06,722][257371] FPS: 327341.15[0m
[36m[2023-07-16 23:40:11,073][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:40:11,074][257371] Reward + Measures: [[-76.0360711    0.28234634   0.93409735   0.67302465   0.87227505
    0.66801816]][0m
[37m[1m[2023-07-16 23:40:11,074][257371] Max Reward on eval: -76.03607110172804[0m
[37m[1m[2023-07-16 23:40:11,074][257371] Min Reward on eval: -76.03607110172804[0m
[37m[1m[2023-07-16 23:40:11,075][257371] Mean Reward across all agents: -76.03607110172804[0m
[37m[1m[2023-07-16 23:40:11,075][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:40:16,104][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:40:16,105][257371] Reward + Measures: [[ 41.49860129   0.3838       0.51260006   0.34420002   0.55759996
    1.5276041 ]
 [-21.88636953   0.4296       0.63300002   0.75430006   0.48660001
    0.84367579]
 [-47.54082116   0.47529998   0.43290001   0.5025       0.40310001
    2.04010558]
 ...
 [-33.9833406    0.41459998   0.73920006   0.6591       0.69590002
    0.74042588]
 [121.86068089   0.26280001   0.3242       0.26789999   0.30039999
    2.19740057]
 [ 84.41370976   0.30950001   0.4375       0.45979998   0.40009999
    1.65360951]][0m
[37m[1m[2023-07-16 23:40:16,105][257371] Max Reward on eval: 153.10069367749384[0m
[37m[1m[2023-07-16 23:40:16,105][257371] Min Reward on eval: -153.051706615102[0m
[37m[1m[2023-07-16 23:40:16,106][257371] Mean Reward across all agents: 4.984241110206995[0m
[37m[1m[2023-07-16 23:40:16,106][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:40:16,110][257371] mean_value=-499.6264366779262, max_value=241.1446378047506[0m
[37m[1m[2023-07-16 23:40:16,113][257371] New mean coefficients: [[-1.0079687   0.8989374   2.7501178   0.8934059  -0.01722519 -0.72494537]][0m
[37m[1m[2023-07-16 23:40:16,114][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:40:25,103][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-16 23:40:25,104][257371] FPS: 427231.22[0m
[36m[2023-07-16 23:40:25,106][257371] itr=265, itrs=2000, Progress: 13.25%[0m
[36m[2023-07-16 23:40:36,927][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-16 23:40:36,928][257371] FPS: 326355.98[0m
[36m[2023-07-16 23:40:41,224][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:40:41,224][257371] Reward + Measures: [[11.86789549  0.81139159  0.94441032  0.57408935  0.91871369  0.70836264]][0m
[37m[1m[2023-07-16 23:40:41,224][257371] Max Reward on eval: 11.867895488776107[0m
[37m[1m[2023-07-16 23:40:41,225][257371] Min Reward on eval: 11.867895488776107[0m
[37m[1m[2023-07-16 23:40:41,225][257371] Mean Reward across all agents: 11.867895488776107[0m
[37m[1m[2023-07-16 23:40:41,225][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:40:46,286][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:40:46,287][257371] Reward + Measures: [[ 24.07948258   0.50229996   0.34800002   0.59510005   0.29300001
    2.64153171]
 [-10.13599514   0.22329998   0.33870003   0.23         0.2656
    2.06968927]
 [ 51.95340608   0.34910002   0.68790001   0.49629998   0.48020002
    1.78388596]
 ...
 [  2.70811636   0.57660002   0.87159997   0.37970001   0.82709998
    1.18197846]
 [-37.89971261   0.5478       0.78839999   0.64740002   0.87080002
    0.75292313]
 [ 49.88791535   0.47880003   0.616        0.6735       0.26799998
    1.66256392]][0m
[37m[1m[2023-07-16 23:40:46,287][257371] Max Reward on eval: 249.42273713722824[0m
[37m[1m[2023-07-16 23:40:46,287][257371] Min Reward on eval: -138.83945466857404[0m
[37m[1m[2023-07-16 23:40:46,288][257371] Mean Reward across all agents: 12.863225640410679[0m
[37m[1m[2023-07-16 23:40:46,288][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:40:46,299][257371] mean_value=-185.09692162288403, max_value=481.5358205006644[0m
[37m[1m[2023-07-16 23:40:46,302][257371] New mean coefficients: [[-0.7234257   0.71297824  2.4613018   0.76405644  0.6050152  -1.3938551 ]][0m
[37m[1m[2023-07-16 23:40:46,302][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:40:55,329][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 23:40:55,330][257371] FPS: 425467.72[0m
[36m[2023-07-16 23:40:55,332][257371] itr=266, itrs=2000, Progress: 13.30%[0m
[36m[2023-07-16 23:41:07,029][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-16 23:41:07,029][257371] FPS: 329846.37[0m
[36m[2023-07-16 23:41:11,318][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:41:11,319][257371] Reward + Measures: [[-25.6999773    0.76559961   0.98246294   0.80716139   0.98922068
    1.63036168]][0m
[37m[1m[2023-07-16 23:41:11,319][257371] Max Reward on eval: -25.699977300062418[0m
[37m[1m[2023-07-16 23:41:11,319][257371] Min Reward on eval: -25.699977300062418[0m
[37m[1m[2023-07-16 23:41:11,319][257371] Mean Reward across all agents: -25.699977300062418[0m
[37m[1m[2023-07-16 23:41:11,320][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:41:16,414][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:41:16,415][257371] Reward + Measures: [[ 34.55480533   0.6656       0.33150002   0.60550004   0.44910002
    1.80250299]
 [ 52.78924071   0.79820001   0.54659998   0.72710001   0.60659999
    1.16225851]
 [-31.89741504   0.64930004   0.97679996   0.6882       0.97279996
    1.19493687]
 ...
 [ -9.72291808   0.79969996   0.98349994   0.86309999   0.98390001
    1.24306738]
 [-24.57496923   0.4522       0.9321       0.2297       0.96090001
    1.03747773]
 [-31.45118443   0.69660002   0.76210004   0.4138       0.78610003
    1.02833927]][0m
[37m[1m[2023-07-16 23:41:16,415][257371] Max Reward on eval: 123.58021879568696[0m
[37m[1m[2023-07-16 23:41:16,415][257371] Min Reward on eval: -257.4288806784898[0m
[37m[1m[2023-07-16 23:41:16,416][257371] Mean Reward across all agents: -6.517256615426712[0m
[37m[1m[2023-07-16 23:41:16,416][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:41:16,429][257371] mean_value=-56.2866744326677, max_value=427.3714785473868[0m
[37m[1m[2023-07-16 23:41:16,432][257371] New mean coefficients: [[-0.6927706   1.4967415   2.2800453   0.6240976   0.33593088 -1.0748135 ]][0m
[37m[1m[2023-07-16 23:41:16,433][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:41:25,440][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 23:41:25,441][257371] FPS: 426387.95[0m
[36m[2023-07-16 23:41:25,443][257371] itr=267, itrs=2000, Progress: 13.35%[0m
[36m[2023-07-16 23:41:37,154][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-16 23:41:37,154][257371] FPS: 329383.83[0m
[36m[2023-07-16 23:41:41,527][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:41:41,527][257371] Reward + Measures: [[14.68417875  0.92558497  0.7757827   0.94077736  0.89941001  0.51405549]][0m
[37m[1m[2023-07-16 23:41:41,528][257371] Max Reward on eval: 14.68417874770697[0m
[37m[1m[2023-07-16 23:41:41,528][257371] Min Reward on eval: 14.68417874770697[0m
[37m[1m[2023-07-16 23:41:41,528][257371] Mean Reward across all agents: 14.68417874770697[0m
[37m[1m[2023-07-16 23:41:41,528][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:41:46,623][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:41:46,624][257371] Reward + Measures: [[206.00521992   0.4488       0.18249999   0.44610006   0.19820002
    2.24946737]
 [-15.73274598   0.60689998   0.16250001   0.64989996   0.50369996
    1.78826547]
 [-73.01706332   0.46759996   0.45019999   0.41230002   0.46419999
    3.33295608]
 ...
 [-10.33678651   0.6753       0.87029999   0.83049995   0.82880002
    1.15668786]
 [-45.54137337   0.39129999   0.48289999   0.34720001   0.50150001
    3.36201739]
 [-11.85958084   0.52329999   0.5169       0.53839999   0.51210004
    1.99590576]][0m
[37m[1m[2023-07-16 23:41:46,624][257371] Max Reward on eval: 308.0479524116032[0m
[37m[1m[2023-07-16 23:41:46,625][257371] Min Reward on eval: -217.4931735710241[0m
[37m[1m[2023-07-16 23:41:46,625][257371] Mean Reward across all agents: 4.521275823355148[0m
[37m[1m[2023-07-16 23:41:46,625][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:41:46,638][257371] mean_value=-192.6048518814915, max_value=547.0171159308869[0m
[37m[1m[2023-07-16 23:41:46,641][257371] New mean coefficients: [[-1.1331925  1.6228255  1.729181   0.6462905  1.54894   -1.2161707]][0m
[37m[1m[2023-07-16 23:41:46,642][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:41:55,796][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-16 23:41:55,796][257371] FPS: 419551.37[0m
[36m[2023-07-16 23:41:55,799][257371] itr=268, itrs=2000, Progress: 13.40%[0m
[36m[2023-07-16 23:42:07,758][257371] train() took 11.91 seconds to complete[0m
[36m[2023-07-16 23:42:07,758][257371] FPS: 322533.85[0m
[36m[2023-07-16 23:42:12,058][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:42:12,059][257371] Reward + Measures: [[14.91637844  0.98994464  0.9301253   0.98122531  0.95595998  0.50328994]][0m
[37m[1m[2023-07-16 23:42:12,059][257371] Max Reward on eval: 14.916378443327655[0m
[37m[1m[2023-07-16 23:42:12,059][257371] Min Reward on eval: 14.916378443327655[0m
[37m[1m[2023-07-16 23:42:12,060][257371] Mean Reward across all agents: 14.916378443327655[0m
[37m[1m[2023-07-16 23:42:12,060][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:42:17,280][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:42:17,280][257371] Reward + Measures: [[112.1668818    0.0289       0.9188       0.46580002   0.89930004
    2.46139312]
 [112.024233     0.46599999   0.8592       0.7112       0.68900001
    1.17910254]
 [ 70.31309628   0.345        0.6972       0.28280002   0.53310007
    2.859061  ]
 ...
 [103.60230969   0.50410002   0.83039993   0.06820001   0.74940002
    3.76651812]
 [ 22.30047815   0.67259997   0.91289997   0.54080003   0.88729995
    1.1750654 ]
 [ 19.20210064   0.29299998   0.59149998   0.31620002   0.52299994
    2.84957862]][0m
[37m[1m[2023-07-16 23:42:17,280][257371] Max Reward on eval: 150.2530694327783[0m
[37m[1m[2023-07-16 23:42:17,281][257371] Min Reward on eval: -146.20914943912067[0m
[37m[1m[2023-07-16 23:42:17,281][257371] Mean Reward across all agents: 40.79543859513587[0m
[37m[1m[2023-07-16 23:42:17,281][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:42:17,291][257371] mean_value=-257.1106306815019, max_value=522.8294373521582[0m
[37m[1m[2023-07-16 23:42:17,294][257371] New mean coefficients: [[-2.7824485   2.278176    1.7275159   1.5741963   2.5149236  -0.70429623]][0m
[37m[1m[2023-07-16 23:42:17,295][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:42:26,381][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-16 23:42:26,382][257371] FPS: 422667.40[0m
[36m[2023-07-16 23:42:26,384][257371] itr=269, itrs=2000, Progress: 13.45%[0m
[36m[2023-07-16 23:42:38,041][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-16 23:42:38,041][257371] FPS: 330917.85[0m
[36m[2023-07-16 23:42:42,395][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:42:42,395][257371] Reward + Measures: [[34.74696737  0.71818638  0.92824268  0.84380597  0.85385799  0.61968881]][0m
[37m[1m[2023-07-16 23:42:42,395][257371] Max Reward on eval: 34.746967365632166[0m
[37m[1m[2023-07-16 23:42:42,396][257371] Min Reward on eval: 34.746967365632166[0m
[37m[1m[2023-07-16 23:42:42,396][257371] Mean Reward across all agents: 34.746967365632166[0m
[37m[1m[2023-07-16 23:42:42,396][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:42:47,465][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:42:47,465][257371] Reward + Measures: [[ 58.52925753   0.61230004   0.78909999   0.7087       0.66790003
    1.3884058 ]
 [ -6.76111131   0.63029999   0.64580005   0.58399999   0.53330004
    0.92962611]
 [ 10.38150251   0.44409999   0.58579999   0.58660001   0.4621
    1.49607849]
 ...
 [ 22.12734702   0.59540004   0.76770002   0.67979997   0.64270002
    1.14869487]
 [-48.78227616   0.62080002   0.7414       0.71430004   0.63929999
    0.94821256]
 [ 32.01955636   0.69800001   0.70040005   0.65289998   0.662
    0.66427821]][0m
[37m[1m[2023-07-16 23:42:47,465][257371] Max Reward on eval: 142.6897800132283[0m
[37m[1m[2023-07-16 23:42:47,466][257371] Min Reward on eval: -229.98884963570163[0m
[37m[1m[2023-07-16 23:42:47,466][257371] Mean Reward across all agents: -18.32454630192935[0m
[37m[1m[2023-07-16 23:42:47,466][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:42:47,470][257371] mean_value=-286.50822535211194, max_value=92.05191426610054[0m
[37m[1m[2023-07-16 23:42:47,473][257371] New mean coefficients: [[-1.5281328   1.1838449   2.035904    1.03479     1.8507552  -0.34811628]][0m
[37m[1m[2023-07-16 23:42:47,473][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:42:56,530][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 23:42:56,530][257371] FPS: 424080.12[0m
[36m[2023-07-16 23:42:56,533][257371] itr=270, itrs=2000, Progress: 13.50%[0m
[37m[1m[2023-07-16 23:45:27,957][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000250[0m
[36m[2023-07-16 23:45:40,355][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-16 23:45:40,355][257371] FPS: 327493.07[0m
[36m[2023-07-16 23:45:44,689][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:45:44,690][257371] Reward + Measures: [[16.74153634  0.78537464  0.96220499  0.91047901  0.85415739  0.87983119]][0m
[37m[1m[2023-07-16 23:45:44,690][257371] Max Reward on eval: 16.741536337332665[0m
[37m[1m[2023-07-16 23:45:44,690][257371] Min Reward on eval: 16.741536337332665[0m
[37m[1m[2023-07-16 23:45:44,690][257371] Mean Reward across all agents: 16.741536337332665[0m
[37m[1m[2023-07-16 23:45:44,690][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:45:49,602][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:45:49,603][257371] Reward + Measures: [[ -2.73375769   0.22860001   0.15460001   0.2431       0.17730002
    5.0071702 ]
 [  4.97540713   0.73569995   0.86840004   0.77470005   0.67919999
    0.67576426]
 [ -1.64177638   0.60729998   0.22919999   0.60080004   0.52310002
    2.94700408]
 ...
 [-17.72226917   0.0953       0.07970001   0.1245       0.1525
    5.42312336]
 [ 77.74096726   0.38940001   0.22870003   0.39400002   0.34129998
    4.03420973]
 [ 91.01669363   0.62440008   0.5898       0.57510006   0.51190001
    1.35839164]][0m
[37m[1m[2023-07-16 23:45:49,603][257371] Max Reward on eval: 324.1832221637713[0m
[37m[1m[2023-07-16 23:45:49,603][257371] Min Reward on eval: -101.56070386637002[0m
[37m[1m[2023-07-16 23:45:49,604][257371] Mean Reward across all agents: 12.03578443522226[0m
[37m[1m[2023-07-16 23:45:49,604][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:45:49,613][257371] mean_value=-282.26598552154667, max_value=494.02903540479383[0m
[37m[1m[2023-07-16 23:45:49,616][257371] New mean coefficients: [[-0.908615   1.0212674  1.6551018  0.535934   3.2324765 -0.5038046]][0m
[37m[1m[2023-07-16 23:45:49,616][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:45:58,629][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-16 23:45:58,630][257371] FPS: 426134.00[0m
[36m[2023-07-16 23:45:58,632][257371] itr=271, itrs=2000, Progress: 13.55%[0m
[36m[2023-07-16 23:46:10,354][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-16 23:46:10,354][257371] FPS: 329113.89[0m
[36m[2023-07-16 23:46:14,681][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:46:14,681][257371] Reward + Measures: [[29.21466136  0.7505877   0.971044    0.93016964  0.89764035  0.97409725]][0m
[37m[1m[2023-07-16 23:46:14,681][257371] Max Reward on eval: 29.214661364614003[0m
[37m[1m[2023-07-16 23:46:14,682][257371] Min Reward on eval: 29.214661364614003[0m
[37m[1m[2023-07-16 23:46:14,682][257371] Mean Reward across all agents: 29.214661364614003[0m
[37m[1m[2023-07-16 23:46:14,682][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:46:19,919][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:46:19,920][257371] Reward + Measures: [[ -54.70069871    0.1705        0.63450003    0.21140002    0.63770002
     4.22421789]
 [-100.00770106    0.2737        0.36429998    0.2297        0.43450004
     4.03066635]
 [  81.20228172    0.72550005    0.29670003    0.90070003    0.83290005
     2.951159  ]
 ...
 [ -86.41079062    0.21210001    0.4032        0.20739999    0.45289999
     5.33538055]
 [  94.46512931    0.35660002    0.41680002    0.26700002    0.44860002
     3.58827662]
 [  49.52756019    0.20829999    0.6573        0.2271        0.56029999
     2.58045936]][0m
[37m[1m[2023-07-16 23:46:19,920][257371] Max Reward on eval: 219.40949972681702[0m
[37m[1m[2023-07-16 23:46:19,920][257371] Min Reward on eval: -288.52959469407796[0m
[37m[1m[2023-07-16 23:46:19,921][257371] Mean Reward across all agents: -19.394970524954285[0m
[37m[1m[2023-07-16 23:46:19,921][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:46:19,929][257371] mean_value=-525.8454380073066, max_value=485.1455694708508[0m
[37m[1m[2023-07-16 23:46:19,932][257371] New mean coefficients: [[-0.54261357  1.1849115   0.50458336 -0.2985534   3.3676486  -0.28197885]][0m
[37m[1m[2023-07-16 23:46:19,933][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:46:28,993][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-16 23:46:28,994][257371] FPS: 423908.50[0m
[36m[2023-07-16 23:46:28,996][257371] itr=272, itrs=2000, Progress: 13.60%[0m
[36m[2023-07-16 23:46:40,746][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-16 23:46:40,747][257371] FPS: 328385.17[0m
[36m[2023-07-16 23:46:45,165][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:46:45,165][257371] Reward + Measures: [[-19.74658192   0.565175     0.8995676    0.714454     0.93073028
    1.47227108]][0m
[37m[1m[2023-07-16 23:46:45,166][257371] Max Reward on eval: -19.746581915614662[0m
[37m[1m[2023-07-16 23:46:45,166][257371] Min Reward on eval: -19.746581915614662[0m
[37m[1m[2023-07-16 23:46:45,166][257371] Mean Reward across all agents: -19.746581915614662[0m
[37m[1m[2023-07-16 23:46:45,166][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:46:49,793][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:46:49,794][257371] Reward + Measures: [[ -54.51198652    0.96480006    0.2203        0.96049994    0.68120003
     1.90977383]
 [-129.70041357    0.1013        0.54600006    0.57359999    0.57170004
     3.12750769]
 [  58.7872484     0.2807        0.29249999    0.28509998    0.31420001
     3.29542708]
 ...
 [ -95.31466018    0.02          0.87409991    0.59900004    0.89099997
     5.47391272]
 [   8.8247765     0.91530001    0.77880001    0.89860004    0.866
     1.11545122]
 [ -74.10275647    0.033         0.90130007    0.9012        0.91580003
     3.03261662]][0m
[37m[1m[2023-07-16 23:46:49,794][257371] Max Reward on eval: 211.31971501335502[0m
[37m[1m[2023-07-16 23:46:49,794][257371] Min Reward on eval: -291.7698326024227[0m
[37m[1m[2023-07-16 23:46:49,795][257371] Mean Reward across all agents: -25.27238111951608[0m
[37m[1m[2023-07-16 23:46:49,795][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:46:49,805][257371] mean_value=-665.6008550346173, max_value=565.4196599045769[0m
[37m[1m[2023-07-16 23:46:49,808][257371] New mean coefficients: [[ 1.0015311  0.9336162  1.5694978 -0.7727057  1.6380616 -0.4911477]][0m
[37m[1m[2023-07-16 23:46:49,809][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:46:58,345][257371] train() took 8.53 seconds to complete[0m
[36m[2023-07-16 23:46:58,345][257371] FPS: 449934.34[0m
[36m[2023-07-16 23:46:58,347][257371] itr=273, itrs=2000, Progress: 13.65%[0m
[36m[2023-07-16 23:47:10,166][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-16 23:47:10,166][257371] FPS: 326472.39[0m
[36m[2023-07-16 23:47:14,491][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:47:14,491][257371] Reward + Measures: [[-3.60598335  0.71494335  0.85772967  0.71027434  0.92045993  1.39370799]][0m
[37m[1m[2023-07-16 23:47:14,492][257371] Max Reward on eval: -3.60598334586014[0m
[37m[1m[2023-07-16 23:47:14,492][257371] Min Reward on eval: -3.60598334586014[0m
[37m[1m[2023-07-16 23:47:14,492][257371] Mean Reward across all agents: -3.60598334586014[0m
[37m[1m[2023-07-16 23:47:14,493][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:47:19,323][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:47:19,324][257371] Reward + Measures: [[-99.00646551   0.42390004   0.4118       0.42570001   0.52650005
    2.54034948]
 [ 50.70146009   0.18949999   0.24070001   0.2096       0.23710001
    4.37933302]
 [ 12.09725202   0.1875       0.28140002   0.24580002   0.25869998
    2.89733577]
 ...
 [-23.5782849    0.2138       0.40009999   0.30739999   0.33700001
    2.42723012]
 [-40.32774466   0.1156       0.56329995   0.44549999   0.59689999
    1.35690844]
 [ 31.87490704   0.81350005   0.23310001   0.82509995   0.9228
    2.99538946]][0m
[37m[1m[2023-07-16 23:47:19,324][257371] Max Reward on eval: 338.3604717239738[0m
[37m[1m[2023-07-16 23:47:19,325][257371] Min Reward on eval: -579.9356198169291[0m
[37m[1m[2023-07-16 23:47:19,325][257371] Mean Reward across all agents: -36.28392308424929[0m
[37m[1m[2023-07-16 23:47:19,325][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:47:19,332][257371] mean_value=-1480.5899694683806, max_value=606.2591619491577[0m
[37m[1m[2023-07-16 23:47:19,335][257371] New mean coefficients: [[ 1.4376652   0.55625975 -0.6325238  -2.0803704   1.4023081  -0.25939038]][0m
[37m[1m[2023-07-16 23:47:19,336][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:47:28,372][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 23:47:28,372][257371] FPS: 425071.73[0m
[36m[2023-07-16 23:47:28,391][257371] itr=274, itrs=2000, Progress: 13.70%[0m
[36m[2023-07-16 23:47:43,397][257371] train() took 14.87 seconds to complete[0m
[36m[2023-07-16 23:47:43,397][257371] FPS: 258285.50[0m
[36m[2023-07-16 23:47:48,856][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:47:48,856][257371] Reward + Measures: [[-1.15408041  0.73641366  0.915878    0.46897167  0.91481769  1.92076266]][0m
[37m[1m[2023-07-16 23:47:48,857][257371] Max Reward on eval: -1.1540804115080585[0m
[37m[1m[2023-07-16 23:47:48,857][257371] Min Reward on eval: -1.1540804115080585[0m
[37m[1m[2023-07-16 23:47:48,857][257371] Mean Reward across all agents: -1.1540804115080585[0m
[37m[1m[2023-07-16 23:47:48,857][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:47:54,796][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:47:54,796][257371] Reward + Measures: [[  25.33041077    0.1184        0.64050001    0.52059996    0.64350003
     2.87941074]
 [ 408.23829364    0.0466        0.84610003    0.66090006    0.81850004
     5.75017405]
 [ -27.69101367    0.28999999    0.82709998    0.70780003    0.62140006
     1.89774549]
 ...
 [ 142.87527272    0.45210001    0.63560003    0.30640003    0.66810006
     2.39943624]
 [-124.16191956    0.0267        0.9368        0.77969998    0.95150006
     2.19514656]
 [ 146.7965373     0.1851        0.41280004    0.30289999    0.41480002
     5.1594758 ]][0m
[37m[1m[2023-07-16 23:47:54,797][257371] Max Reward on eval: 611.5752325108275[0m
[37m[1m[2023-07-16 23:47:54,797][257371] Min Reward on eval: -201.52908205501734[0m
[37m[1m[2023-07-16 23:47:54,797][257371] Mean Reward across all agents: 48.05230974623947[0m
[37m[1m[2023-07-16 23:47:54,797][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:47:54,809][257371] mean_value=-253.33196727323153, max_value=1111.5752325108274[0m
[37m[1m[2023-07-16 23:47:54,812][257371] New mean coefficients: [[ 2.5577114  -0.07690227 -2.329076   -3.105945    0.7477509   0.01195741]][0m
[37m[1m[2023-07-16 23:47:54,813][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:48:04,621][257371] train() took 9.81 seconds to complete[0m
[36m[2023-07-16 23:48:04,622][257371] FPS: 391560.94[0m
[36m[2023-07-16 23:48:04,624][257371] itr=275, itrs=2000, Progress: 13.75%[0m
[36m[2023-07-16 23:48:16,408][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-16 23:48:16,408][257371] FPS: 327471.78[0m
[36m[2023-07-16 23:48:20,748][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:48:20,749][257371] Reward + Measures: [[-1.17434512  0.55641633  0.76376969  0.70434892  0.72701722  1.47895885]][0m
[37m[1m[2023-07-16 23:48:20,749][257371] Max Reward on eval: -1.1743451221643093[0m
[37m[1m[2023-07-16 23:48:20,749][257371] Min Reward on eval: -1.1743451221643093[0m
[37m[1m[2023-07-16 23:48:20,750][257371] Mean Reward across all agents: -1.1743451221643093[0m
[37m[1m[2023-07-16 23:48:20,750][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:48:26,006][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:48:26,007][257371] Reward + Measures: [[  2.58280234   0.24879999   0.23140001   0.20390001   0.21280001
    3.27495813]
 [ 20.52541894   0.10039999   0.08150001   0.0943       0.1148
    4.7922554 ]
 [ -8.91554898   0.27019998   0.42910001   0.39680001   0.45969996
    3.09533691]
 ...
 [-36.0367939    0.112        0.11720001   0.10740001   0.096
    5.15339994]
 [ 75.21710918   0.26530001   0.25579998   0.22739999   0.2096
    3.31587911]
 [-98.5520637    0.5          0.75369996   0.79459995   0.57120007
    2.91064   ]][0m
[37m[1m[2023-07-16 23:48:26,007][257371] Max Reward on eval: 283.2825603344245[0m
[37m[1m[2023-07-16 23:48:26,007][257371] Min Reward on eval: -268.9590709194308[0m
[37m[1m[2023-07-16 23:48:26,007][257371] Mean Reward across all agents: 40.52429437408892[0m
[37m[1m[2023-07-16 23:48:26,008][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:48:26,012][257371] mean_value=-1948.307303435223, max_value=556.3348788032308[0m
[37m[1m[2023-07-16 23:48:26,015][257371] New mean coefficients: [[ 0.52070403  0.6321463  -0.6857171  -1.2963464   1.2150968   0.24018152]][0m
[37m[1m[2023-07-16 23:48:26,016][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:48:35,085][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 23:48:35,085][257371] FPS: 423487.92[0m
[36m[2023-07-16 23:48:35,087][257371] itr=276, itrs=2000, Progress: 13.80%[0m
[36m[2023-07-16 23:48:46,718][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-16 23:48:46,718][257371] FPS: 331719.39[0m
[36m[2023-07-16 23:48:51,060][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:48:51,061][257371] Reward + Measures: [[-48.43523575   0.701473     0.92239666   0.22758934   0.86585593
    2.49826217]][0m
[37m[1m[2023-07-16 23:48:51,061][257371] Max Reward on eval: -48.435235750873204[0m
[37m[1m[2023-07-16 23:48:51,061][257371] Min Reward on eval: -48.435235750873204[0m
[37m[1m[2023-07-16 23:48:51,062][257371] Mean Reward across all agents: -48.435235750873204[0m
[37m[1m[2023-07-16 23:48:51,062][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:48:56,108][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:48:56,108][257371] Reward + Measures: [[ -27.68265306    0.94440001    0.95780003    0.84530002    0.97010005
     4.49983215]
 [ -71.645863      0.54300004    0.4278        0.58120006    0.35820001
     2.41675687]
 [  -6.84802412    0.85789996    0.74980003    0.7432        0.88190001
     4.56763887]
 ...
 [  -7.01615827    0.2545        0.6024        0.31690001    0.50239998
     2.27870274]
 [-103.45166518    0.28879997    0.25299999    0.3258        0.18810001
     4.40220308]
 [  80.96080853    0.63660002    0.75379997    0.20410001    0.65329999
     2.65598059]][0m
[37m[1m[2023-07-16 23:48:56,109][257371] Max Reward on eval: 367.63918208265676[0m
[37m[1m[2023-07-16 23:48:56,109][257371] Min Reward on eval: -251.52487989999355[0m
[37m[1m[2023-07-16 23:48:56,109][257371] Mean Reward across all agents: 4.4884411590841875[0m
[37m[1m[2023-07-16 23:48:56,109][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:48:56,118][257371] mean_value=-385.16843803468896, max_value=650.735169562107[0m
[37m[1m[2023-07-16 23:48:56,120][257371] New mean coefficients: [[ 2.1944256   0.29000902  0.03629184 -2.5114808  -0.591486    0.17067327]][0m
[37m[1m[2023-07-16 23:48:56,121][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:49:05,151][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-16 23:49:05,151][257371] FPS: 425356.73[0m
[36m[2023-07-16 23:49:05,153][257371] itr=277, itrs=2000, Progress: 13.85%[0m
[36m[2023-07-16 23:49:16,773][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-16 23:49:16,773][257371] FPS: 332007.34[0m
[36m[2023-07-16 23:49:21,155][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:49:21,155][257371] Reward + Measures: [[-3.75763122  0.9052403   0.89669228  0.89431572  0.93079835  1.3746711 ]][0m
[37m[1m[2023-07-16 23:49:21,155][257371] Max Reward on eval: -3.757631217711547[0m
[37m[1m[2023-07-16 23:49:21,156][257371] Min Reward on eval: -3.757631217711547[0m
[37m[1m[2023-07-16 23:49:21,156][257371] Mean Reward across all agents: -3.757631217711547[0m
[37m[1m[2023-07-16 23:49:21,156][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:49:26,289][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:49:26,295][257371] Reward + Measures: [[ -37.50097988    0.25049999    0.46070001    0.31000003    0.509
     2.39706993]
 [  44.36139605    0.53829998    0.85640001    0.32329997    0.90780002
     3.54294252]
 [ -54.26847372    0.47839999    0.42319998    0.5104        0.37930003
     2.87978625]
 ...
 [  13.92374064    0.23989999    0.33070001    0.23280001    0.32280001
     3.31774688]
 [  -8.19403763    0.2823        0.43990001    0.23490003    0.39050001
     2.61813593]
 [-273.48082922    0.98320007    0.0116        0.97100002    0.86699992
     3.08353591]][0m
[37m[1m[2023-07-16 23:49:26,295][257371] Max Reward on eval: 213.36219264790415[0m
[37m[1m[2023-07-16 23:49:26,296][257371] Min Reward on eval: -301.23317107735204[0m
[37m[1m[2023-07-16 23:49:26,296][257371] Mean Reward across all agents: 4.122877905180863[0m
[37m[1m[2023-07-16 23:49:26,296][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:49:26,307][257371] mean_value=-813.8106862305624, max_value=601.2580062850611[0m
[37m[1m[2023-07-16 23:49:26,310][257371] New mean coefficients: [[ 1.1839339   0.28637204  0.36510175 -1.5582278  -1.7508152  -0.05133741]][0m
[37m[1m[2023-07-16 23:49:26,311][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:49:35,449][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-16 23:49:35,449][257371] FPS: 420302.51[0m
[36m[2023-07-16 23:49:35,452][257371] itr=278, itrs=2000, Progress: 13.90%[0m
[36m[2023-07-16 23:49:47,362][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-16 23:49:47,362][257371] FPS: 323940.36[0m
[36m[2023-07-16 23:49:51,807][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:49:51,807][257371] Reward + Measures: [[-26.14262643   0.62994933   0.44322169   0.37530103   0.70681661
    1.57767248]][0m
[37m[1m[2023-07-16 23:49:51,807][257371] Max Reward on eval: -26.142626425534527[0m
[37m[1m[2023-07-16 23:49:51,808][257371] Min Reward on eval: -26.142626425534527[0m
[37m[1m[2023-07-16 23:49:51,808][257371] Mean Reward across all agents: -26.142626425534527[0m
[37m[1m[2023-07-16 23:49:51,808][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:49:56,902][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:49:56,903][257371] Reward + Measures: [[175.05691793   0.352        0.51170003   0.33620003   0.36559999
    1.71615815]
 [ 14.74579208   0.31549999   0.40370002   0.37729999   0.4515
    2.44790244]
 [ 75.92804339   0.38980004   0.38709998   0.39630002   0.50580001
    3.92848134]
 ...
 [  6.65776649   0.27310002   0.39579999   0.3405       0.32699999
    2.62379384]
 [ -5.7015296    0.4179       0.45310003   0.48480001   0.3998
    1.7913357 ]
 [ 41.68724796   0.36309999   0.53680003   0.47950003   0.40260002
    1.46626461]][0m
[37m[1m[2023-07-16 23:49:56,903][257371] Max Reward on eval: 248.31095897629856[0m
[37m[1m[2023-07-16 23:49:56,903][257371] Min Reward on eval: -549.921052949829[0m
[37m[1m[2023-07-16 23:49:56,903][257371] Mean Reward across all agents: -12.309579348764158[0m
[37m[1m[2023-07-16 23:49:56,904][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:49:56,909][257371] mean_value=-1184.303056342995, max_value=597.9772611675784[0m
[37m[1m[2023-07-16 23:49:56,912][257371] New mean coefficients: [[-0.6284114   0.63350415 -0.3411442  -0.5023489   0.6802182   0.11737387]][0m
[37m[1m[2023-07-16 23:49:56,913][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:50:05,986][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-16 23:50:05,987][257371] FPS: 423309.17[0m
[36m[2023-07-16 23:50:05,989][257371] itr=279, itrs=2000, Progress: 13.95%[0m
[36m[2023-07-16 23:50:17,949][257371] train() took 11.90 seconds to complete[0m
[36m[2023-07-16 23:50:17,949][257371] FPS: 322633.50[0m
[36m[2023-07-16 23:50:22,288][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:50:22,288][257371] Reward + Measures: [[-35.15640426   0.64237368   0.45349666   0.36131701   0.72041893
    1.59944177]][0m
[37m[1m[2023-07-16 23:50:22,288][257371] Max Reward on eval: -35.156404255510154[0m
[37m[1m[2023-07-16 23:50:22,289][257371] Min Reward on eval: -35.156404255510154[0m
[37m[1m[2023-07-16 23:50:22,289][257371] Mean Reward across all agents: -35.156404255510154[0m
[37m[1m[2023-07-16 23:50:22,289][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:50:27,378][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:50:27,379][257371] Reward + Measures: [[15.09070789  0.21209998  0.70359999  0.4488      0.62309998  1.76305807]
 [71.43722486  0.198       0.49459997  0.34450001  0.49260002  2.28228426]
 [ 2.19870911  0.18660001  0.35339999  0.31259999  0.34900001  2.87994742]
 ...
 [-8.41223375  0.2703      0.41300002  0.3439      0.56350005  2.34848499]
 [29.79522698  0.34730002  0.45070001  0.36330003  0.52670002  3.87074161]
 [12.80999804  0.45479998  0.71079999  0.38049999  0.73369998  1.89837611]][0m
[37m[1m[2023-07-16 23:50:27,379][257371] Max Reward on eval: 204.4034614680335[0m
[37m[1m[2023-07-16 23:50:27,379][257371] Min Reward on eval: -284.90637969542297[0m
[37m[1m[2023-07-16 23:50:27,380][257371] Mean Reward across all agents: 22.236096029438045[0m
[37m[1m[2023-07-16 23:50:27,380][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:50:27,385][257371] mean_value=-1306.0313839652565, max_value=461.16373451455985[0m
[37m[1m[2023-07-16 23:50:27,388][257371] New mean coefficients: [[-1.3873281   0.6853515   1.4363751   0.36439025  0.76233876  0.07434102]][0m
[37m[1m[2023-07-16 23:50:27,389][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:50:36,509][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-16 23:50:36,510][257371] FPS: 421103.76[0m
[36m[2023-07-16 23:50:36,512][257371] itr=280, itrs=2000, Progress: 14.00%[0m
[37m[1m[2023-07-16 23:53:10,612][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000260[0m
[36m[2023-07-16 23:53:23,061][257371] train() took 11.89 seconds to complete[0m
[36m[2023-07-16 23:53:23,062][257371] FPS: 322862.96[0m
[36m[2023-07-16 23:53:27,323][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:53:27,323][257371] Reward + Measures: [[-75.92514358   0.69778597   0.41265333   0.44808969   0.75957632
    1.7983197 ]][0m
[37m[1m[2023-07-16 23:53:27,323][257371] Max Reward on eval: -75.92514358071715[0m
[37m[1m[2023-07-16 23:53:27,323][257371] Min Reward on eval: -75.92514358071715[0m
[37m[1m[2023-07-16 23:53:27,324][257371] Mean Reward across all agents: -75.92514358071715[0m
[37m[1m[2023-07-16 23:53:27,324][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:53:32,286][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:53:32,286][257371] Reward + Measures: [[-243.9780147     0.74490005    0.13699999    0.79710001    0.79960001
     4.06679153]
 [ -18.63227249    0.47          0.46130005    0.36050001    0.43600002
     1.91257286]
 [  75.02001796    0.55309999    0.55500001    0.26719999    0.73760003
     2.31687236]
 ...
 [ -20.18622158    0.25819999    0.37410003    0.30559999    0.3461
     2.78287673]
 [ -60.70021031    0.3804        0.52899998    0.4718        0.42679998
     2.07996249]
 [  54.52244802    0.39159998    0.26180002    0.32080004    0.29850003
     2.95915127]][0m
[37m[1m[2023-07-16 23:53:32,286][257371] Max Reward on eval: 205.37602879465558[0m
[37m[1m[2023-07-16 23:53:32,287][257371] Min Reward on eval: -329.9371857911348[0m
[37m[1m[2023-07-16 23:53:32,287][257371] Mean Reward across all agents: -17.902019418511976[0m
[37m[1m[2023-07-16 23:53:32,287][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:53:32,292][257371] mean_value=-1088.5206922197142, max_value=591.3378267213702[0m
[37m[1m[2023-07-16 23:53:32,295][257371] New mean coefficients: [[-2.2967286   0.6720962   0.91014355  1.2585647   0.7682005   0.21176115]][0m
[37m[1m[2023-07-16 23:53:32,296][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:53:41,208][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-16 23:53:41,208][257371] FPS: 430946.84[0m
[36m[2023-07-16 23:53:41,211][257371] itr=281, itrs=2000, Progress: 14.05%[0m
[36m[2023-07-16 23:53:53,136][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-16 23:53:53,136][257371] FPS: 323557.83[0m
[36m[2023-07-16 23:53:57,418][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:53:57,418][257371] Reward + Measures: [[-147.77038195    0.77059734    0.33289567    0.60070562    0.80290139
     2.06645703]][0m
[37m[1m[2023-07-16 23:53:57,418][257371] Max Reward on eval: -147.7703819505533[0m
[37m[1m[2023-07-16 23:53:57,419][257371] Min Reward on eval: -147.7703819505533[0m
[37m[1m[2023-07-16 23:53:57,419][257371] Mean Reward across all agents: -147.7703819505533[0m
[37m[1m[2023-07-16 23:53:57,419][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:54:02,613][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:54:02,614][257371] Reward + Measures: [[ 74.76928064   0.27090001   0.45019999   0.3495       0.3319
    2.96354628]
 [ 71.43844534   0.35999998   0.47659999   0.37740001   0.5399
    2.00005102]
 [-44.71492911   0.21110001   0.58420002   0.26999998   0.61919999
    3.51412821]
 ...
 [366.7745161    0.98499995   0.0032       0.98859996   0.96000004
    5.08836794]
 [ 77.75990409   0.37490001   0.64099997   0.37470004   0.6311
    1.50641847]
 [  7.63061107   0.33270001   0.32220003   0.29060003   0.3256
    4.14952421]][0m
[37m[1m[2023-07-16 23:54:02,614][257371] Max Reward on eval: 377.8213252699003[0m
[37m[1m[2023-07-16 23:54:02,614][257371] Min Reward on eval: -750.130344403023[0m
[37m[1m[2023-07-16 23:54:02,614][257371] Mean Reward across all agents: -5.24683680517974[0m
[37m[1m[2023-07-16 23:54:02,615][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:54:02,626][257371] mean_value=-403.4108263056068, max_value=811.7030468256771[0m
[37m[1m[2023-07-16 23:54:02,629][257371] New mean coefficients: [[-2.7280567   0.97484374 -0.9819878   0.44892073  2.1281347   0.17037629]][0m
[37m[1m[2023-07-16 23:54:02,630][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:54:11,678][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-16 23:54:11,678][257371] FPS: 424471.08[0m
[36m[2023-07-16 23:54:11,680][257371] itr=282, itrs=2000, Progress: 14.10%[0m
[36m[2023-07-16 23:54:23,432][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-16 23:54:23,432][257371] FPS: 328431.85[0m
[36m[2023-07-16 23:54:27,745][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:54:27,746][257371] Reward + Measures: [[-285.1043414     0.89426267    0.147549      0.81951368    0.90461034
     2.58234739]][0m
[37m[1m[2023-07-16 23:54:27,746][257371] Max Reward on eval: -285.10434140092246[0m
[37m[1m[2023-07-16 23:54:27,746][257371] Min Reward on eval: -285.10434140092246[0m
[37m[1m[2023-07-16 23:54:27,747][257371] Mean Reward across all agents: -285.10434140092246[0m
[37m[1m[2023-07-16 23:54:27,747][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:54:32,814][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:54:32,815][257371] Reward + Measures: [[-25.71182349   0.50099999   0.56990004   0.51589996   0.54330003
    2.02309871]
 [ 11.81855759   0.1464       0.18240002   0.14490001   0.17560001
    4.44152164]
 [ 55.69295985   0.1628       0.19600001   0.1718       0.1969
    3.92438197]
 ...
 [-10.78895784   0.29530001   0.4217       0.2895       0.4285
    3.7009263 ]
 [195.8609066    0.2397       0.3479       0.3522       0.41510001
    3.53052974]
 [124.08942298   0.2273       0.32060003   0.22640002   0.308
    3.78693843]][0m
[37m[1m[2023-07-16 23:54:32,815][257371] Max Reward on eval: 249.40739942453803[0m
[37m[1m[2023-07-16 23:54:32,815][257371] Min Reward on eval: -594.1280212458223[0m
[37m[1m[2023-07-16 23:54:32,816][257371] Mean Reward across all agents: 21.805628431852437[0m
[37m[1m[2023-07-16 23:54:32,816][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:54:32,819][257371] mean_value=-1821.1704015930836, max_value=414.4813740856759[0m
[37m[1m[2023-07-16 23:54:32,822][257371] New mean coefficients: [[-2.4778125   1.4087689   0.13709468  0.5357332   0.42840278  0.12385145]][0m
[37m[1m[2023-07-16 23:54:32,823][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:54:41,921][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-16 23:54:41,921][257371] FPS: 422127.72[0m
[36m[2023-07-16 23:54:41,924][257371] itr=283, itrs=2000, Progress: 14.15%[0m
[36m[2023-07-16 23:54:53,676][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-16 23:54:53,677][257371] FPS: 328301.16[0m
[36m[2023-07-16 23:54:57,915][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:54:57,916][257371] Reward + Measures: [[-209.75344041    0.97850096    0.54660434    0.96332127    0.98103565
     4.05622959]][0m
[37m[1m[2023-07-16 23:54:57,916][257371] Max Reward on eval: -209.75344041208092[0m
[37m[1m[2023-07-16 23:54:57,916][257371] Min Reward on eval: -209.75344041208092[0m
[37m[1m[2023-07-16 23:54:57,917][257371] Mean Reward across all agents: -209.75344041208092[0m
[37m[1m[2023-07-16 23:54:57,917][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:55:02,942][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:55:02,942][257371] Reward + Measures: [[ 18.68548841   0.41610003   0.4131       0.49759999   0.32140002
    2.54277778]
 [120.17540592   0.47         0.56160003   0.64580005   0.36850002
    2.18273163]
 [ -7.46331179   0.42670003   0.1796       0.2895       0.32800001
    2.87462997]
 ...
 [-28.98460891   0.31010002   0.62740004   0.32730001   0.72839993
    1.67469943]
 [ 23.96667394   0.38159999   0.26879999   0.41170001   0.25780001
    3.74533892]
 [ -9.53287835   0.51300001   0.58569998   0.58569998   0.5801
    1.5921303 ]][0m
[37m[1m[2023-07-16 23:55:02,943][257371] Max Reward on eval: 307.80133010074496[0m
[37m[1m[2023-07-16 23:55:02,943][257371] Min Reward on eval: -431.8546199690551[0m
[37m[1m[2023-07-16 23:55:02,943][257371] Mean Reward across all agents: -17.07862322192553[0m
[37m[1m[2023-07-16 23:55:02,943][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:55:02,950][257371] mean_value=-498.64685888499577, max_value=691.9704305008271[0m
[37m[1m[2023-07-16 23:55:02,953][257371] New mean coefficients: [[-2.400498    1.2188853   0.266214    1.388322   -0.6911788   0.16216329]][0m
[37m[1m[2023-07-16 23:55:02,954][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:55:12,135][257371] train() took 9.18 seconds to complete[0m
[36m[2023-07-16 23:55:12,136][257371] FPS: 418316.98[0m
[36m[2023-07-16 23:55:12,138][257371] itr=284, itrs=2000, Progress: 14.20%[0m
[36m[2023-07-16 23:55:24,022][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-16 23:55:24,022][257371] FPS: 324758.31[0m
[36m[2023-07-16 23:55:28,304][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:55:28,304][257371] Reward + Measures: [[-151.80723413    0.98342365    0.93206072    0.96960235    0.98554701
     3.47435808]][0m
[37m[1m[2023-07-16 23:55:28,305][257371] Max Reward on eval: -151.80723413388554[0m
[37m[1m[2023-07-16 23:55:28,305][257371] Min Reward on eval: -151.80723413388554[0m
[37m[1m[2023-07-16 23:55:28,305][257371] Mean Reward across all agents: -151.80723413388554[0m
[37m[1m[2023-07-16 23:55:28,305][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:55:33,292][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:55:33,293][257371] Reward + Measures: [[ -46.21010636    0.3942        0.46310002    0.2192        0.62079996
     3.06663656]
 [-525.74008177    0.96590006    0.0124        0.95249999    0.94210005
     5.98215628]
 [  33.84505706    0.41429996    0.47250006    0.84940004    0.84800005
     5.15013504]
 ...
 [ -78.72946515    0.2016        0.41409999    0.43039998    0.50199997
     3.30685616]
 [ -10.16224063    0.44660002    0.32950002    0.53540003    0.43779999
     3.42703485]
 [-138.76546141    0.48850003    0.77609998    0.83150005    0.82800001
     4.24862051]][0m
[37m[1m[2023-07-16 23:55:33,293][257371] Max Reward on eval: 212.9334405599162[0m
[37m[1m[2023-07-16 23:55:33,294][257371] Min Reward on eval: -653.6199913065881[0m
[37m[1m[2023-07-16 23:55:33,294][257371] Mean Reward across all agents: -69.09218074599181[0m
[37m[1m[2023-07-16 23:55:33,294][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:55:33,305][257371] mean_value=-650.8437717054634, max_value=613.0358516390454[0m
[37m[1m[2023-07-16 23:55:33,308][257371] New mean coefficients: [[-2.066952    0.5091149  -1.403348    1.2889118  -1.059909    0.60613275]][0m
[37m[1m[2023-07-16 23:55:33,309][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:55:42,412][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-16 23:55:42,412][257371] FPS: 421913.88[0m
[36m[2023-07-16 23:55:42,414][257371] itr=285, itrs=2000, Progress: 14.25%[0m
[36m[2023-07-16 23:55:54,248][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-16 23:55:54,248][257371] FPS: 326052.58[0m
[36m[2023-07-16 23:55:58,538][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:55:58,538][257371] Reward + Measures: [[-407.27000648    0.74770033    0.24315967    0.92915535    0.92447072
     4.87015963]][0m
[37m[1m[2023-07-16 23:55:58,539][257371] Max Reward on eval: -407.27000648238527[0m
[37m[1m[2023-07-16 23:55:58,539][257371] Min Reward on eval: -407.27000648238527[0m
[37m[1m[2023-07-16 23:55:58,539][257371] Mean Reward across all agents: -407.27000648238527[0m
[37m[1m[2023-07-16 23:55:58,539][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:56:03,540][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:56:03,541][257371] Reward + Measures: [[113.73900051   0.1621       0.18210001   0.1917       0.16680001
    4.21733856]
 [-38.56969066   0.33450001   0.31010002   0.3231       0.26070002
    5.04982376]
 [112.27422079   0.33500001   0.44640002   0.50410002   0.37340003
    2.79977727]
 ...
 [ 89.09313252   0.36289999   0.48600003   0.31420001   0.42810002
    2.20818925]
 [-43.07266022   0.1332       0.74139994   0.74660003   0.66610003
    6.52723169]
 [-21.84260017   0.44080001   0.25229999   0.3409       0.36920002
    2.50974441]][0m
[37m[1m[2023-07-16 23:56:03,541][257371] Max Reward on eval: 290.35014069806783[0m
[37m[1m[2023-07-16 23:56:03,541][257371] Min Reward on eval: -630.8099470104556[0m
[37m[1m[2023-07-16 23:56:03,541][257371] Mean Reward across all agents: -16.304356081988185[0m
[37m[1m[2023-07-16 23:56:03,542][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:56:03,552][257371] mean_value=-941.1656520612983, max_value=617.671141147893[0m
[37m[1m[2023-07-16 23:56:03,554][257371] New mean coefficients: [[-0.37946272 -0.13186729 -2.0079713  -0.16342568 -2.1221726   0.4308625 ]][0m
[37m[1m[2023-07-16 23:56:03,556][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:56:12,543][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-16 23:56:12,543][257371] FPS: 427356.77[0m
[36m[2023-07-16 23:56:12,545][257371] itr=286, itrs=2000, Progress: 14.30%[0m
[36m[2023-07-16 23:56:24,214][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-16 23:56:24,215][257371] FPS: 330669.04[0m
[36m[2023-07-16 23:56:28,547][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:56:28,547][257371] Reward + Measures: [[-443.2993057     0.82589597    0.16524066    0.93754035    0.93226928
     4.88073635]][0m
[37m[1m[2023-07-16 23:56:28,548][257371] Max Reward on eval: -443.29930570222524[0m
[37m[1m[2023-07-16 23:56:28,548][257371] Min Reward on eval: -443.29930570222524[0m
[37m[1m[2023-07-16 23:56:28,548][257371] Mean Reward across all agents: -443.29930570222524[0m
[37m[1m[2023-07-16 23:56:28,548][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:56:33,830][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:56:33,831][257371] Reward + Measures: [[-84.21632285   0.12770002   0.67610002   0.69259995   0.69600004
    4.80888414]
 [-29.65712252   0.16450001   0.16790001   0.18249999   0.16190001
    4.48789167]
 [ 16.22068851   0.31720001   0.3427       0.38140002   0.38170001
    3.8076508 ]
 ...
 [  3.06749342   0.5273       0.52570003   0.5521       0.32429999
    2.20379162]
 [-52.86349558   0.52770001   0.52670002   0.49079999   0.3475
    2.17411017]
 [ 27.64015146   0.0922       0.66219997   0.56330007   0.60170001
    5.56533146]][0m
[37m[1m[2023-07-16 23:56:33,831][257371] Max Reward on eval: 292.19713395051656[0m
[37m[1m[2023-07-16 23:56:33,831][257371] Min Reward on eval: -636.6979484698735[0m
[37m[1m[2023-07-16 23:56:33,831][257371] Mean Reward across all agents: 2.50328843054288[0m
[37m[1m[2023-07-16 23:56:33,832][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:56:33,841][257371] mean_value=-662.6841426175143, max_value=698.9475652367436[0m
[37m[1m[2023-07-16 23:56:33,844][257371] New mean coefficients: [[ 0.60047    -0.10447295 -2.964653   -1.1785009  -0.6392015   0.08776635]][0m
[37m[1m[2023-07-16 23:56:33,845][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:56:42,992][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-16 23:56:42,992][257371] FPS: 419886.85[0m
[36m[2023-07-16 23:56:42,995][257371] itr=287, itrs=2000, Progress: 14.35%[0m
[36m[2023-07-16 23:56:55,054][257371] train() took 12.00 seconds to complete[0m
[36m[2023-07-16 23:56:55,054][257371] FPS: 319994.90[0m
[36m[2023-07-16 23:56:59,406][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:56:59,406][257371] Reward + Measures: [[-250.01208585    0.58295196    0.40601832    0.91247296    0.91666299
     4.72118664]][0m
[37m[1m[2023-07-16 23:56:59,406][257371] Max Reward on eval: -250.01208585310957[0m
[37m[1m[2023-07-16 23:56:59,407][257371] Min Reward on eval: -250.01208585310957[0m
[37m[1m[2023-07-16 23:56:59,407][257371] Mean Reward across all agents: -250.01208585310957[0m
[37m[1m[2023-07-16 23:56:59,407][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:57:04,440][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:57:04,441][257371] Reward + Measures: [[ -40.56918486    0.30019999    0.4745        0.3211        0.44630003
     3.02867579]
 [  39.31842542    0.2976        0.5873        0.41400003    0.55940002
     2.86376095]
 [-115.22641035    0.29589999    0.35010001    0.2633        0.4251
     3.67514277]
 ...
 [-207.0812082     0.28239998    0.71160001    0.73269999    0.79089993
     3.50025344]
 [ -75.53603669    0.27940002    0.4226        0.2436        0.40369996
     3.85889602]
 [  46.21068489    0.32350001    0.37149999    0.28830001    0.38799998
     3.37864876]][0m
[37m[1m[2023-07-16 23:57:04,441][257371] Max Reward on eval: 142.959248569794[0m
[37m[1m[2023-07-16 23:57:04,441][257371] Min Reward on eval: -506.03835295978934[0m
[37m[1m[2023-07-16 23:57:04,441][257371] Mean Reward across all agents: -25.36777959960402[0m
[37m[1m[2023-07-16 23:57:04,442][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:57:04,448][257371] mean_value=-989.0399363136494, max_value=498.760150834173[0m
[37m[1m[2023-07-16 23:57:04,451][257371] New mean coefficients: [[ 0.8566915  -0.4999414  -1.3846618  -1.3622837  -0.21755764  0.00694111]][0m
[37m[1m[2023-07-16 23:57:04,452][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:57:13,515][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-16 23:57:13,515][257371] FPS: 423771.74[0m
[36m[2023-07-16 23:57:13,518][257371] itr=288, itrs=2000, Progress: 14.40%[0m
[36m[2023-07-16 23:57:25,784][257371] train() took 12.21 seconds to complete[0m
[36m[2023-07-16 23:57:25,784][257371] FPS: 314574.98[0m
[36m[2023-07-16 23:57:30,077][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:57:30,078][257371] Reward + Measures: [[-474.77820158    0.89650893    0.09432733    0.94858497    0.94146198
     4.48803186]][0m
[37m[1m[2023-07-16 23:57:30,078][257371] Max Reward on eval: -474.77820158006125[0m
[37m[1m[2023-07-16 23:57:30,078][257371] Min Reward on eval: -474.77820158006125[0m
[37m[1m[2023-07-16 23:57:30,079][257371] Mean Reward across all agents: -474.77820158006125[0m
[37m[1m[2023-07-16 23:57:30,079][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:57:34,994][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:57:34,995][257371] Reward + Measures: [[-12.69888047   0.38409999   0.3691       0.4224       0.35419998
    2.93423247]
 [ 17.13952468   0.50500005   0.53830004   0.33630002   0.55759996
    3.22968841]
 [-28.35055833   0.1964       0.29159999   0.23480001   0.30199999
    4.73226309]
 ...
 [-50.81875795   0.46469998   0.42180005   0.42870003   0.37830001
    2.15349078]
 [  0.13259413   0.96090001   0.0162       0.93040001   0.53710002
    4.74277258]
 [118.58429862   0.60570002   0.55700004   0.55690002   0.58380002
    4.45461702]][0m
[37m[1m[2023-07-16 23:57:34,995][257371] Max Reward on eval: 328.68975926907734[0m
[37m[1m[2023-07-16 23:57:34,995][257371] Min Reward on eval: -596.9341278054286[0m
[37m[1m[2023-07-16 23:57:34,995][257371] Mean Reward across all agents: -10.670286954191349[0m
[37m[1m[2023-07-16 23:57:34,996][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:57:35,005][257371] mean_value=-919.9835317031265, max_value=805.1686751735397[0m
[37m[1m[2023-07-16 23:57:35,007][257371] New mean coefficients: [[ 2.0008233  -0.35576323 -0.1518923  -1.7766532  -1.5545576   0.01751947]][0m
[37m[1m[2023-07-16 23:57:35,008][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:57:44,751][257371] train() took 9.74 seconds to complete[0m
[36m[2023-07-16 23:57:44,751][257371] FPS: 394221.71[0m
[36m[2023-07-16 23:57:44,753][257371] itr=289, itrs=2000, Progress: 14.45%[0m
[36m[2023-07-16 23:58:00,646][257371] train() took 15.83 seconds to complete[0m
[36m[2023-07-16 23:58:00,647][257371] FPS: 242587.83[0m
[36m[2023-07-16 23:58:06,212][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:58:06,212][257371] Reward + Measures: [[-588.08719975    0.93765432    0.05245       0.96039933    0.95128196
     5.26586056]][0m
[37m[1m[2023-07-16 23:58:06,213][257371] Max Reward on eval: -588.087199751506[0m
[37m[1m[2023-07-16 23:58:06,213][257371] Min Reward on eval: -588.087199751506[0m
[37m[1m[2023-07-16 23:58:06,213][257371] Mean Reward across all agents: -588.087199751506[0m
[37m[1m[2023-07-16 23:58:06,213][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:58:13,556][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-16 23:58:13,557][257371] Reward + Measures: [[ -9.27758497   0.24590002   0.33170003   0.32539999   0.3344
    4.04663086]
 [-29.67942467   0.1883       0.24340001   0.25030002   0.24430001
    4.32140303]
 [-75.30089434   0.24160002   0.52390003   0.5891       0.46700001
    4.15909004]
 ...
 [ -0.79147043   0.0982       0.09350001   0.1155       0.08460001
    5.70451927]
 [ 31.81497001   0.12119999   0.1085       0.14660001   0.1239
    5.27754545]
 [-29.7880681    0.34040001   0.54360002   0.59799999   0.38650003
    5.83970118]][0m
[37m[1m[2023-07-16 23:58:13,557][257371] Max Reward on eval: 277.5794692019466[0m
[37m[1m[2023-07-16 23:58:13,557][257371] Min Reward on eval: -857.1725616501644[0m
[37m[1m[2023-07-16 23:58:13,558][257371] Mean Reward across all agents: -26.61931962490921[0m
[37m[1m[2023-07-16 23:58:13,558][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-16 23:58:13,568][257371] mean_value=-853.8486667729704, max_value=652.4491452202853[0m
[37m[1m[2023-07-16 23:58:13,572][257371] New mean coefficients: [[ 0.8496351   0.7023878   0.318264   -0.3124684  -0.9198937  -0.13429195]][0m
[37m[1m[2023-07-16 23:58:13,573][257371] Moving the mean solution point...[0m
[36m[2023-07-16 23:58:24,369][257371] train() took 10.79 seconds to complete[0m
[36m[2023-07-16 23:58:24,370][257371] FPS: 355735.29[0m
[36m[2023-07-16 23:58:24,372][257371] itr=290, itrs=2000, Progress: 14.50%[0m
[37m[1m[2023-07-17 00:01:11,914][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000270[0m
[36m[2023-07-17 00:01:24,490][257371] train() took 12.01 seconds to complete[0m
[36m[2023-07-17 00:01:24,491][257371] FPS: 319720.23[0m
[36m[2023-07-17 00:01:28,809][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:01:28,810][257371] Reward + Measures: [[-461.21043885    0.93437302    0.07328833    0.89927733    0.86150229
     4.80224276]][0m
[37m[1m[2023-07-17 00:01:28,810][257371] Max Reward on eval: -461.2104388473472[0m
[37m[1m[2023-07-17 00:01:28,810][257371] Min Reward on eval: -461.2104388473472[0m
[37m[1m[2023-07-17 00:01:28,811][257371] Mean Reward across all agents: -461.2104388473472[0m
[37m[1m[2023-07-17 00:01:28,811][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:01:33,763][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:01:33,764][257371] Reward + Measures: [[-600.52816199    0.98280001    0.0092        0.97419995    0.95760006
     6.90513945]
 [  47.55121777    0.43669996    0.44610006    0.4316        0.43459997
     6.33531523]
 [  78.41919157    0.3917        0.77270001    0.72280008    0.77019995
     3.17540622]
 ...
 [ -11.75951624    0.30050001    0.3414        0.2651        0.34690005
     3.94515681]
 [ -22.1312316     0.41960001    0.4508        0.42659998    0.51220006
     2.51215529]
 [-289.08779097    0.61230004    0.15250002    0.60649997    0.63760006
     5.27881384]][0m
[37m[1m[2023-07-17 00:01:33,764][257371] Max Reward on eval: 274.9446716221049[0m
[37m[1m[2023-07-17 00:01:33,764][257371] Min Reward on eval: -600.5281619850546[0m
[37m[1m[2023-07-17 00:01:33,764][257371] Mean Reward across all agents: -20.516665185407696[0m
[37m[1m[2023-07-17 00:01:33,765][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:01:33,774][257371] mean_value=-281.8249279173052, max_value=564.9938484388658[0m
[37m[1m[2023-07-17 00:01:33,776][257371] New mean coefficients: [[ 1.3690703   0.5677804   0.19658378 -0.5278494  -2.3543768  -0.15473126]][0m
[37m[1m[2023-07-17 00:01:33,777][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:01:42,628][257371] train() took 8.85 seconds to complete[0m
[36m[2023-07-17 00:01:42,628][257371] FPS: 433940.89[0m
[36m[2023-07-17 00:01:42,631][257371] itr=291, itrs=2000, Progress: 14.55%[0m
[36m[2023-07-17 00:01:54,565][257371] train() took 11.88 seconds to complete[0m
[36m[2023-07-17 00:01:54,565][257371] FPS: 323341.04[0m
[36m[2023-07-17 00:01:58,663][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:01:58,663][257371] Reward + Measures: [[-373.93722917    0.90995097    0.11161233    0.86332923    0.76944333
     4.44078779]][0m
[37m[1m[2023-07-17 00:01:58,664][257371] Max Reward on eval: -373.9372291690591[0m
[37m[1m[2023-07-17 00:01:58,664][257371] Min Reward on eval: -373.9372291690591[0m
[37m[1m[2023-07-17 00:01:58,664][257371] Mean Reward across all agents: -373.9372291690591[0m
[37m[1m[2023-07-17 00:01:58,664][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:02:03,583][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:02:03,584][257371] Reward + Measures: [[-283.48063762    0.72449994    0.1617        0.6825        0.62480003
     7.23241425]
 [-564.93230317    0.98200005    0.0123        0.96420002    0.96379995
     6.86601257]
 [ -53.03696984    0.81809998    0.1142        0.71320003    0.63800001
     4.08973598]
 ...
 [ -88.34147282    0.32660004    0.29010001    0.2184        0.28140002
     3.82280803]
 [ -13.64865794    0.8071        0.14250001    0.74260002    0.73879999
     5.45241547]
 [-299.5591414     0.72200006    0.1328        0.73330003    0.61930007
     7.53359318]][0m
[37m[1m[2023-07-17 00:02:03,584][257371] Max Reward on eval: 198.76653724499047[0m
[37m[1m[2023-07-17 00:02:03,584][257371] Min Reward on eval: -803.8967399674933[0m
[37m[1m[2023-07-17 00:02:03,584][257371] Mean Reward across all agents: -215.62921402845478[0m
[37m[1m[2023-07-17 00:02:03,585][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:02:03,590][257371] mean_value=-968.5221686264682, max_value=698.7665372449904[0m
[37m[1m[2023-07-17 00:02:03,593][257371] New mean coefficients: [[ 1.5472739   0.32840052  0.6607341  -0.56438774 -2.293741    0.00079112]][0m
[37m[1m[2023-07-17 00:02:03,594][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:02:12,451][257371] train() took 8.86 seconds to complete[0m
[36m[2023-07-17 00:02:12,452][257371] FPS: 433617.86[0m
[36m[2023-07-17 00:02:12,454][257371] itr=292, itrs=2000, Progress: 14.60%[0m
[36m[2023-07-17 00:02:24,497][257371] train() took 11.98 seconds to complete[0m
[36m[2023-07-17 00:02:24,498][257371] FPS: 320488.59[0m
[36m[2023-07-17 00:02:28,687][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:02:28,687][257371] Reward + Measures: [[-226.96778706    0.85180336    0.17148665    0.74652439    0.64490163
     4.07264042]][0m
[37m[1m[2023-07-17 00:02:28,687][257371] Max Reward on eval: -226.96778706411396[0m
[37m[1m[2023-07-17 00:02:28,688][257371] Min Reward on eval: -226.96778706411396[0m
[37m[1m[2023-07-17 00:02:28,688][257371] Mean Reward across all agents: -226.96778706411396[0m
[37m[1m[2023-07-17 00:02:28,688][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:02:33,684][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:02:33,684][257371] Reward + Measures: [[-74.06547164   0.36480004   0.37060001   0.44549999   0.47480002
    3.80306792]
 [ 72.88053909   0.26199999   0.3908       0.46419999   0.48669997
    4.94998503]
 [ -2.74732323   0.51739997   0.31990001   0.37890002   0.51120001
    3.42531848]
 ...
 [128.41824042   0.79839998   0.18350001   0.67539996   0.63989997
    3.86592078]
 [-42.48262485   0.73730004   0.58670002   0.62770003   0.7112
    2.84904385]
 [-13.87982941   0.71629995   0.28169999   0.62639999   0.58820003
    3.92796135]][0m
[37m[1m[2023-07-17 00:02:33,685][257371] Max Reward on eval: 164.5239922877401[0m
[37m[1m[2023-07-17 00:02:33,685][257371] Min Reward on eval: -706.1539687821642[0m
[37m[1m[2023-07-17 00:02:33,685][257371] Mean Reward across all agents: -99.2788664344486[0m
[37m[1m[2023-07-17 00:02:33,685][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:02:33,694][257371] mean_value=-299.53156832375004, max_value=425.78935078520584[0m
[37m[1m[2023-07-17 00:02:33,697][257371] New mean coefficients: [[ 0.9148678   0.6102493  -0.5892957  -0.27708024 -2.2799528   0.51667976]][0m
[37m[1m[2023-07-17 00:02:33,698][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:02:42,764][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 00:02:42,764][257371] FPS: 423612.56[0m
[36m[2023-07-17 00:02:42,767][257371] itr=293, itrs=2000, Progress: 14.65%[0m
[36m[2023-07-17 00:02:54,558][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 00:02:54,558][257371] FPS: 327285.22[0m
[36m[2023-07-17 00:02:58,899][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:02:58,899][257371] Reward + Measures: [[-140.93703091    0.82114166    0.19743265    0.69094563    0.59398866
     3.88631511]][0m
[37m[1m[2023-07-17 00:02:58,900][257371] Max Reward on eval: -140.9370309140541[0m
[37m[1m[2023-07-17 00:02:58,900][257371] Min Reward on eval: -140.9370309140541[0m
[37m[1m[2023-07-17 00:02:58,900][257371] Mean Reward across all agents: -140.9370309140541[0m
[37m[1m[2023-07-17 00:02:58,900][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:03:04,099][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:03:04,100][257371] Reward + Measures: [[-292.11768286    0.98210001    0.0131        0.96289998    0.94570011
     6.69955826]
 [   0.01984528    0.49720001    0.5941        0.46849999    0.55769998
     3.28396726]
 [  18.29046498    0.56420004    0.51920003    0.46160004    0.53400004
     3.02557611]
 ...
 [-195.80012322    0.84810001    0.17099999    0.76610005    0.63160002
     4.17602301]
 [  -4.83831494    0.69840002    0.71680003    0.56409997    0.71689999
     3.57153487]
 [-410.28767772    0.97980005    0.0584        0.9745        0.88770002
     6.17594099]][0m
[37m[1m[2023-07-17 00:03:04,100][257371] Max Reward on eval: 372.9310283546336[0m
[37m[1m[2023-07-17 00:03:04,100][257371] Min Reward on eval: -672.6626625239849[0m
[37m[1m[2023-07-17 00:03:04,100][257371] Mean Reward across all agents: -41.14733246581696[0m
[37m[1m[2023-07-17 00:03:04,101][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:03:04,116][257371] mean_value=-300.78102655585013, max_value=788.3134946608916[0m
[37m[1m[2023-07-17 00:03:04,119][257371] New mean coefficients: [[ 0.3770855   1.5172861  -2.0524774  -0.0139617  -1.553519    0.55841243]][0m
[37m[1m[2023-07-17 00:03:04,119][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:03:13,111][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 00:03:13,112][257371] FPS: 427119.80[0m
[36m[2023-07-17 00:03:13,114][257371] itr=294, itrs=2000, Progress: 14.70%[0m
[36m[2023-07-17 00:03:24,849][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 00:03:24,850][257371] FPS: 328861.47[0m
[36m[2023-07-17 00:03:29,175][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:03:29,176][257371] Reward + Measures: [[-133.45398182    0.87139869    0.14232868    0.73514825    0.5967077
     4.07158232]][0m
[37m[1m[2023-07-17 00:03:29,176][257371] Max Reward on eval: -133.4539818203545[0m
[37m[1m[2023-07-17 00:03:29,176][257371] Min Reward on eval: -133.4539818203545[0m
[37m[1m[2023-07-17 00:03:29,176][257371] Mean Reward across all agents: -133.4539818203545[0m
[37m[1m[2023-07-17 00:03:29,177][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:03:34,251][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:03:34,251][257371] Reward + Measures: [[ 66.87762646   0.22619998   0.26810002   0.30270001   0.29010001
    4.66384411]
 [ -1.72470883   0.54070008   0.14299999   0.42410001   0.39120004
    3.75040245]
 [ 69.5633704    0.24220002   0.2414       0.2746       0.2472
    4.5229907 ]
 ...
 [157.03439513   0.39030001   0.47080001   0.45790002   0.38859999
    2.77789664]
 [ 38.90623967   0.5636       0.38769999   0.5388       0.37199998
    3.17047572]
 [ 13.93358484   0.3134       0.29860002   0.35209998   0.38049999
    4.04078627]][0m
[37m[1m[2023-07-17 00:03:34,252][257371] Max Reward on eval: 183.45716986664337[0m
[37m[1m[2023-07-17 00:03:34,252][257371] Min Reward on eval: -590.4705657955259[0m
[37m[1m[2023-07-17 00:03:34,252][257371] Mean Reward across all agents: -3.1901081831910703[0m
[37m[1m[2023-07-17 00:03:34,252][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:03:34,256][257371] mean_value=-769.094198725219, max_value=282.59773563555257[0m
[37m[1m[2023-07-17 00:03:34,259][257371] New mean coefficients: [[-0.5998998   1.5472556  -0.8400134   0.05171359 -1.7848742   0.6832694 ]][0m
[37m[1m[2023-07-17 00:03:34,260][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:03:43,376][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 00:03:43,377][257371] FPS: 421295.83[0m
[36m[2023-07-17 00:03:43,379][257371] itr=295, itrs=2000, Progress: 14.75%[0m
[36m[2023-07-17 00:03:55,193][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 00:03:55,194][257371] FPS: 326696.69[0m
[36m[2023-07-17 00:03:59,475][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:03:59,476][257371] Reward + Measures: [[-733.12375895    0.97503662    0.01152733    0.97558802    0.93603057
     6.43346882]][0m
[37m[1m[2023-07-17 00:03:59,476][257371] Max Reward on eval: -733.1237589506675[0m
[37m[1m[2023-07-17 00:03:59,476][257371] Min Reward on eval: -733.1237589506675[0m
[37m[1m[2023-07-17 00:03:59,477][257371] Mean Reward across all agents: -733.1237589506675[0m
[37m[1m[2023-07-17 00:03:59,477][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:04:04,481][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:04:04,481][257371] Reward + Measures: [[-104.63498969    0.22180001    0.6552        0.31170002    0.73820001
     3.31045771]
 [ -34.57870644    0.18800001    0.479         0.2823        0.51739997
     3.80954218]
 [  -8.4082024     0.3574        0.36480001    0.33320001    0.3888
     2.77725577]
 ...
 [  36.15413355    0.08639999    0.069         0.0867        0.10820001
     5.23672724]
 [-446.51458743    0.81780005    0.0825        0.89779997    0.88309997
     6.53467274]
 [-614.56176377    0.9860999     0.0067        0.97589999    0.9684
     7.89635563]][0m
[37m[1m[2023-07-17 00:04:04,482][257371] Max Reward on eval: 579.6637191580609[0m
[37m[1m[2023-07-17 00:04:04,482][257371] Min Reward on eval: -912.1472244282195[0m
[37m[1m[2023-07-17 00:04:04,482][257371] Mean Reward across all agents: -61.960489694076244[0m
[37m[1m[2023-07-17 00:04:04,482][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:04:04,489][257371] mean_value=-1162.4191892215122, max_value=784.750212652835[0m
[37m[1m[2023-07-17 00:04:04,492][257371] New mean coefficients: [[-2.5548325   2.285203   -0.2558511   0.99970925 -0.5952363   0.42165533]][0m
[37m[1m[2023-07-17 00:04:04,493][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:04:13,493][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 00:04:13,493][257371] FPS: 426731.08[0m
[36m[2023-07-17 00:04:13,496][257371] itr=296, itrs=2000, Progress: 14.80%[0m
[36m[2023-07-17 00:04:25,223][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 00:04:25,223][257371] FPS: 329185.47[0m
[36m[2023-07-17 00:04:29,630][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:04:29,635][257371] Reward + Measures: [[-794.18540716    0.98021132    0.012473      0.97381836    0.94502568
     6.43474197]][0m
[37m[1m[2023-07-17 00:04:29,635][257371] Max Reward on eval: -794.1854071590958[0m
[37m[1m[2023-07-17 00:04:29,635][257371] Min Reward on eval: -794.1854071590958[0m
[37m[1m[2023-07-17 00:04:29,636][257371] Mean Reward across all agents: -794.1854071590958[0m
[37m[1m[2023-07-17 00:04:29,636][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:04:34,673][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:04:34,680][257371] Reward + Measures: [[  69.3020892     0.34950003    0.39610001    0.31150001    0.44460002
     4.80864954]
 [ -82.97086623    0.20710002    0.73830003    0.5571        0.74880004
     5.36646509]
 [ -40.65976939    0.14549999    0.10039999    0.1156        0.17739999
     4.74388647]
 ...
 [  27.05056473    0.29220003    0.41620001    0.29840001    0.47439995
     3.19811559]
 [-187.57131767    0.514         0.27340004    0.39500004    0.43520004
     3.61796379]
 [  56.70105715    0.30880004    0.20910001    0.18840002    0.2861
     3.44131923]][0m
[37m[1m[2023-07-17 00:04:34,680][257371] Max Reward on eval: 356.2593121753074[0m
[37m[1m[2023-07-17 00:04:34,681][257371] Min Reward on eval: -789.5006232334301[0m
[37m[1m[2023-07-17 00:04:34,682][257371] Mean Reward across all agents: -38.01057931174204[0m
[37m[1m[2023-07-17 00:04:34,682][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:04:34,697][257371] mean_value=-693.9412339367203, max_value=642.7378746164611[0m
[37m[1m[2023-07-17 00:04:34,701][257371] New mean coefficients: [[-2.8929055   2.772911    0.9634256   0.9649123   0.21322763  0.02294815]][0m
[37m[1m[2023-07-17 00:04:34,702][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:04:43,827][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 00:04:43,828][257371] FPS: 420887.31[0m
[36m[2023-07-17 00:04:43,830][257371] itr=297, itrs=2000, Progress: 14.85%[0m
[36m[2023-07-17 00:04:55,649][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 00:04:55,649][257371] FPS: 326539.25[0m
[36m[2023-07-17 00:05:00,017][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:05:00,022][257371] Reward + Measures: [[-821.21256408    0.98513901    0.00885433    0.97716737    0.95702928
     6.51697969]][0m
[37m[1m[2023-07-17 00:05:00,023][257371] Max Reward on eval: -821.2125640825213[0m
[37m[1m[2023-07-17 00:05:00,023][257371] Min Reward on eval: -821.2125640825213[0m
[37m[1m[2023-07-17 00:05:00,023][257371] Mean Reward across all agents: -821.2125640825213[0m
[37m[1m[2023-07-17 00:05:00,023][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:05:05,047][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:05:05,053][257371] Reward + Measures: [[ -52.12296471    0.33489999    0.4391        0.50559998    0.39550003
     3.22427058]
 [  33.80437561    0.46779999    0.38820001    0.45119998    0.31000003
     3.74124312]
 [-252.9841585     0.90670007    0.07930001    0.86949998    0.83000004
     5.15244293]
 ...
 [-534.90891262    0.8549        0.0905        0.95960009    0.89480001
     6.40884352]
 [ -95.53354644    0.99020004    0.0039        0.98149997    0.96789998
     6.31139803]
 [-680.55775832    0.99430001    0.0038        0.97530001    0.9637
     7.79686213]][0m
[37m[1m[2023-07-17 00:05:05,053][257371] Max Reward on eval: 654.6549549207091[0m
[37m[1m[2023-07-17 00:05:05,054][257371] Min Reward on eval: -858.8969116538763[0m
[37m[1m[2023-07-17 00:05:05,054][257371] Mean Reward across all agents: -216.10316260376715[0m
[37m[1m[2023-07-17 00:05:05,054][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:05:05,066][257371] mean_value=-345.34811917651706, max_value=784.9928315477958[0m
[37m[1m[2023-07-17 00:05:05,069][257371] New mean coefficients: [[-4.6821027   3.4551954   1.0469606   2.4892848   0.39828366  0.32653984]][0m
[37m[1m[2023-07-17 00:05:05,070][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:05:14,112][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 00:05:14,113][257371] FPS: 424742.02[0m
[36m[2023-07-17 00:05:14,115][257371] itr=298, itrs=2000, Progress: 14.90%[0m
[36m[2023-07-17 00:05:26,030][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-17 00:05:26,030][257371] FPS: 323994.03[0m
[36m[2023-07-17 00:05:29,784][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:05:29,784][257371] Reward + Measures: [[-613.58274492    0.94788533    0.02108933    0.93839437    0.92601097
     6.41731787]][0m
[37m[1m[2023-07-17 00:05:29,784][257371] Max Reward on eval: -613.5827449238303[0m
[37m[1m[2023-07-17 00:05:29,785][257371] Min Reward on eval: -613.5827449238303[0m
[37m[1m[2023-07-17 00:05:29,785][257371] Mean Reward across all agents: -613.5827449238303[0m
[37m[1m[2023-07-17 00:05:29,785][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:05:34,989][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:05:34,990][257371] Reward + Measures: [[-166.48033373    0.25070003    0.12150001    0.32650003    0.30770001
     6.50748777]
 [ -19.99237401    0.35890001    0.49199995    0.40459999    0.5693
     5.40360403]
 [-159.9619713     0.8707        0.04280001    0.86320001    0.87989998
     6.41204977]
 ...
 [ -24.36426635    0.26040003    0.1821        0.2287        0.2586
     4.70885992]
 [  16.22148533    0.2502        0.22190002    0.26820001    0.30379999
     4.55380964]
 [ -20.94983713    0.12820001    0.14610001    0.14460002    0.15320002
     6.04976797]][0m
[37m[1m[2023-07-17 00:05:34,990][257371] Max Reward on eval: 322.0661049087532[0m
[37m[1m[2023-07-17 00:05:34,991][257371] Min Reward on eval: -596.5132255524862[0m
[37m[1m[2023-07-17 00:05:34,991][257371] Mean Reward across all agents: -31.979774546891296[0m
[37m[1m[2023-07-17 00:05:34,991][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:05:35,004][257371] mean_value=-290.81025371428257, max_value=678.38409284316[0m
[37m[1m[2023-07-17 00:05:35,006][257371] New mean coefficients: [[-5.5181556   4.1651826  -0.42683256  2.8150494   2.4004612   0.29594308]][0m
[37m[1m[2023-07-17 00:05:35,007][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:05:44,074][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 00:05:44,074][257371] FPS: 423619.50[0m
[36m[2023-07-17 00:05:44,076][257371] itr=299, itrs=2000, Progress: 14.95%[0m
[36m[2023-07-17 00:05:56,147][257371] train() took 12.01 seconds to complete[0m
[36m[2023-07-17 00:05:56,148][257371] FPS: 319793.28[0m
[36m[2023-07-17 00:06:00,101][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:06:00,106][257371] Reward + Measures: [[-641.87915387    0.97074699    0.01101967    0.96280336    0.95046002
     6.49627256]][0m
[37m[1m[2023-07-17 00:06:00,107][257371] Max Reward on eval: -641.8791538689543[0m
[37m[1m[2023-07-17 00:06:00,107][257371] Min Reward on eval: -641.8791538689543[0m
[37m[1m[2023-07-17 00:06:00,107][257371] Mean Reward across all agents: -641.8791538689543[0m
[37m[1m[2023-07-17 00:06:00,107][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:06:04,710][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:06:04,710][257371] Reward + Measures: [[-47.26511442   0.2033       0.15900001   0.176        0.14430001
    4.29175377]
 [ 39.27109869   0.41729999   0.44119999   0.39569998   0.45559999
    3.26613975]
 [ 72.77936806   0.57459998   0.40079999   0.48639998   0.40130001
    3.96513867]
 ...
 [ 64.10794732   0.24889998   0.24770001   0.2877       0.26659998
    3.47623181]
 [  7.54450434   0.198        0.1838       0.20440002   0.1646
    4.22919941]
 [-58.42723707   0.35590002   0.29840001   0.31220001   0.27220002
    3.87209892]][0m
[37m[1m[2023-07-17 00:06:04,710][257371] Max Reward on eval: 213.75316002831096[0m
[37m[1m[2023-07-17 00:06:04,711][257371] Min Reward on eval: -147.09930889680982[0m
[37m[1m[2023-07-17 00:06:04,711][257371] Mean Reward across all agents: 30.322354777816358[0m
[37m[1m[2023-07-17 00:06:04,711][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:06:04,715][257371] mean_value=-1774.882115635622, max_value=216.94443879654148[0m
[37m[1m[2023-07-17 00:06:04,717][257371] New mean coefficients: [[-3.1696346   3.564984   -1.5298225   1.9405231   1.5240937  -0.31514418]][0m
[37m[1m[2023-07-17 00:06:04,718][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:06:13,668][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 00:06:13,668][257371] FPS: 429166.12[0m
[36m[2023-07-17 00:06:13,671][257371] itr=300, itrs=2000, Progress: 15.00%[0m
[37m[1m[2023-07-17 00:08:52,894][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000280[0m
[36m[2023-07-17 00:09:07,679][257371] train() took 14.16 seconds to complete[0m
[36m[2023-07-17 00:09:07,680][257371] FPS: 271214.35[0m
[36m[2023-07-17 00:09:11,957][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:09:11,958][257371] Reward + Measures: [[-653.9899777     0.97662163    0.00796767    0.96893603    0.95804197
     6.57406521]][0m
[37m[1m[2023-07-17 00:09:11,958][257371] Max Reward on eval: -653.9899777034324[0m
[37m[1m[2023-07-17 00:09:11,958][257371] Min Reward on eval: -653.9899777034324[0m
[37m[1m[2023-07-17 00:09:11,958][257371] Mean Reward across all agents: -653.9899777034324[0m
[37m[1m[2023-07-17 00:09:11,958][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:09:16,965][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:09:16,966][257371] Reward + Measures: [[-111.13628812    0.1425        0.68760002    0.62849998    0.66839999
     5.30598402]
 [ -11.05557869    0.4041        0.29180002    0.30640003    0.2978
     3.90094829]
 [ -51.35906939    0.1541        0.43610001    0.435         0.40050003
     5.94235659]
 ...
 [   5.71137631    0.537         0.2105        0.39850003    0.31940001
     3.83516765]
 [  56.14065879    0.22830001    0.35679999    0.30490002    0.32479998
     4.63592863]
 [   8.5765877     0.15980001    0.23190001    0.24540003    0.1646
     6.13226795]][0m
[37m[1m[2023-07-17 00:09:16,966][257371] Max Reward on eval: 356.5086221562233[0m
[37m[1m[2023-07-17 00:09:16,966][257371] Min Reward on eval: -742.9829177954234[0m
[37m[1m[2023-07-17 00:09:16,966][257371] Mean Reward across all agents: -11.588808629097883[0m
[37m[1m[2023-07-17 00:09:16,967][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:09:16,974][257371] mean_value=-679.1652999962727, max_value=756.9790960907603[0m
[37m[1m[2023-07-17 00:09:16,977][257371] New mean coefficients: [[-3.9803333  3.5088663 -3.2128575  2.040954   2.358223  -0.1911144]][0m
[37m[1m[2023-07-17 00:09:16,978][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:09:25,943][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 00:09:25,944][257371] FPS: 428406.67[0m
[36m[2023-07-17 00:09:25,946][257371] itr=301, itrs=2000, Progress: 15.05%[0m
[36m[2023-07-17 00:09:37,746][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 00:09:37,746][257371] FPS: 327217.11[0m
[36m[2023-07-17 00:09:42,031][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:09:42,031][257371] Reward + Measures: [[-674.38733835    0.97522295    0.008439      0.96825397    0.95741761
     6.58597898]][0m
[37m[1m[2023-07-17 00:09:42,031][257371] Max Reward on eval: -674.387338351666[0m
[37m[1m[2023-07-17 00:09:42,032][257371] Min Reward on eval: -674.387338351666[0m
[37m[1m[2023-07-17 00:09:42,032][257371] Mean Reward across all agents: -674.387338351666[0m
[37m[1m[2023-07-17 00:09:42,032][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:09:47,032][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:09:47,032][257371] Reward + Measures: [[ 18.04241279   0.3766       0.44679999   0.36300001   0.42269999
    5.28454828]
 [-48.38240898   0.32399997   0.3752       0.37080002   0.3161
    4.43959713]
 [ -7.28015798   0.37170002   0.43000004   0.3513       0.3998
    5.17015219]
 ...
 [-68.10621705   0.61260003   0.198        0.55320001   0.34650001
    4.72470236]
 [ 25.8016681    0.42640001   0.44689998   0.41859999   0.433
    4.28523588]
 [-18.78728251   0.44949999   0.48499998   0.43140003   0.44639999
    3.85227633]][0m
[37m[1m[2023-07-17 00:09:47,032][257371] Max Reward on eval: 342.25482655148954[0m
[37m[1m[2023-07-17 00:09:47,033][257371] Min Reward on eval: -811.8337860225699[0m
[37m[1m[2023-07-17 00:09:47,033][257371] Mean Reward across all agents: -16.423915511877702[0m
[37m[1m[2023-07-17 00:09:47,033][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:09:47,037][257371] mean_value=-572.1573474624724, max_value=563.6710414605216[0m
[37m[1m[2023-07-17 00:09:47,039][257371] New mean coefficients: [[-2.922995    3.2280126  -3.8197217   1.284788    0.71216273  0.18809357]][0m
[37m[1m[2023-07-17 00:09:47,040][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:09:56,018][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 00:09:56,018][257371] FPS: 427825.30[0m
[36m[2023-07-17 00:09:56,020][257371] itr=302, itrs=2000, Progress: 15.10%[0m
[36m[2023-07-17 00:10:07,700][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 00:10:07,701][257371] FPS: 330457.50[0m
[36m[2023-07-17 00:10:12,042][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:10:12,042][257371] Reward + Measures: [[-688.19767556    0.97426194    0.007429      0.96788597    0.95693231
     6.64983368]][0m
[37m[1m[2023-07-17 00:10:12,043][257371] Max Reward on eval: -688.1976755557843[0m
[37m[1m[2023-07-17 00:10:12,043][257371] Min Reward on eval: -688.1976755557843[0m
[37m[1m[2023-07-17 00:10:12,043][257371] Mean Reward across all agents: -688.1976755557843[0m
[37m[1m[2023-07-17 00:10:12,043][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:10:17,130][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:10:17,130][257371] Reward + Measures: [[ -6.31409046   0.33790001   0.76249999   0.52340001   0.72200006
    5.66781759]
 [390.55923268   0.80759996   0.0271       0.82499999   0.55839998
    6.89451981]
 [239.86540639   0.65819997   0.177        0.64140004   0.52490002
    5.80049419]
 ...
 [-10.85817711   0.49340001   0.41850001   0.43850002   0.41820002
    4.86478186]
 [ 23.78034454   0.54649991   0.56370002   0.51450008   0.49759999
    5.43986988]
 [181.1091006    0.74989998   0.11570001   0.72589999   0.55579996
    5.44203901]][0m
[37m[1m[2023-07-17 00:10:17,130][257371] Max Reward on eval: 476.3872223364189[0m
[37m[1m[2023-07-17 00:10:17,131][257371] Min Reward on eval: -464.9793081155978[0m
[37m[1m[2023-07-17 00:10:17,131][257371] Mean Reward across all agents: 1.3980235097402278[0m
[37m[1m[2023-07-17 00:10:17,131][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:10:17,144][257371] mean_value=88.01896536521826, max_value=765.0302470219042[0m
[37m[1m[2023-07-17 00:10:17,147][257371] New mean coefficients: [[-4.203023    3.8151853  -4.294962    1.6234615   0.8215332   0.10087891]][0m
[37m[1m[2023-07-17 00:10:17,149][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:10:26,312][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-17 00:10:26,313][257371] FPS: 419128.05[0m
[36m[2023-07-17 00:10:26,315][257371] itr=303, itrs=2000, Progress: 15.15%[0m
[36m[2023-07-17 00:10:38,204][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 00:10:38,204][257371] FPS: 324642.64[0m
[36m[2023-07-17 00:10:42,574][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:10:42,579][257371] Reward + Measures: [[-711.03759681    0.98130965    0.006296      0.97469807    0.96503133
     6.70856094]][0m
[37m[1m[2023-07-17 00:10:42,580][257371] Max Reward on eval: -711.0375968119421[0m
[37m[1m[2023-07-17 00:10:42,581][257371] Min Reward on eval: -711.0375968119421[0m
[37m[1m[2023-07-17 00:10:42,581][257371] Mean Reward across all agents: -711.0375968119421[0m
[37m[1m[2023-07-17 00:10:42,582][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:10:47,630][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:10:47,635][257371] Reward + Measures: [[ 48.92139252   0.2119       0.17290001   0.1375       0.18120001
    4.74263859]
 [ 58.10963785   0.27579999   0.22500001   0.21370001   0.23870002
    3.97399187]
 [-42.12176074   0.46070004   0.30340001   0.33360001   0.3669
    3.40361977]
 ...
 [-45.24270513   0.44300005   0.31479999   0.3407       0.40359998
    3.30060005]
 [144.74114347   0.229        0.4368       0.4628       0.49270001
    4.82398558]
 [ 16.13769387   0.1997       0.14400001   0.1214       0.15460001
    4.50230408]][0m
[37m[1m[2023-07-17 00:10:47,636][257371] Max Reward on eval: 144.74114347088616[0m
[37m[1m[2023-07-17 00:10:47,636][257371] Min Reward on eval: -730.1844825834036[0m
[37m[1m[2023-07-17 00:10:47,636][257371] Mean Reward across all agents: -53.63281370184029[0m
[37m[1m[2023-07-17 00:10:47,637][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:10:47,639][257371] mean_value=-1709.1211565817014, max_value=351.83547043429826[0m
[37m[1m[2023-07-17 00:10:47,641][257371] New mean coefficients: [[-3.4082642   3.2135596  -2.9055014   1.275959    0.18214524  0.025656  ]][0m
[37m[1m[2023-07-17 00:10:47,643][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:10:56,660][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 00:10:56,660][257371] FPS: 425928.99[0m
[36m[2023-07-17 00:10:56,662][257371] itr=304, itrs=2000, Progress: 15.20%[0m
[36m[2023-07-17 00:11:08,508][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 00:11:08,508][257371] FPS: 325846.89[0m
[36m[2023-07-17 00:11:12,855][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:11:12,861][257371] Reward + Measures: [[-715.00092457    0.97918165    0.006511      0.9730376     0.96345228
     6.71376944]][0m
[37m[1m[2023-07-17 00:11:12,861][257371] Max Reward on eval: -715.0009245746242[0m
[37m[1m[2023-07-17 00:11:12,861][257371] Min Reward on eval: -715.0009245746242[0m
[37m[1m[2023-07-17 00:11:12,861][257371] Mean Reward across all agents: -715.0009245746242[0m
[37m[1m[2023-07-17 00:11:12,862][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:11:17,916][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:11:17,922][257371] Reward + Measures: [[-120.29014823    0.54769999    0.16370001    0.50779998    0.40419999
     3.91817522]
 [-120.40576885    0.13329999    0.78870004    0.5194        0.82349998
     6.07812738]
 [   3.27124179    0.30160001    0.34490001    0.22119999    0.36989999
     3.35109949]
 ...
 [-249.33400346    0.2172        0.8021        0.56239998    0.70020002
     6.29090929]
 [   8.00711885    0.27610001    0.28819999    0.20650001    0.33230001
     3.853724  ]
 [-132.74075131    0.308         0.52809995    0.48090002    0.67019999
     5.35479689]][0m
[37m[1m[2023-07-17 00:11:17,922][257371] Max Reward on eval: 197.63527750242503[0m
[37m[1m[2023-07-17 00:11:17,923][257371] Min Reward on eval: -320.9239196622046[0m
[37m[1m[2023-07-17 00:11:17,923][257371] Mean Reward across all agents: -25.747731411241947[0m
[37m[1m[2023-07-17 00:11:17,923][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:11:17,930][257371] mean_value=-766.0818863674426, max_value=676.8103981328895[0m
[37m[1m[2023-07-17 00:11:17,933][257371] New mean coefficients: [[-3.2469325   3.1994884   0.06015754  1.6417902  -0.3522361  -0.4102785 ]][0m
[37m[1m[2023-07-17 00:11:17,934][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:11:27,033][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 00:11:27,033][257371] FPS: 422124.32[0m
[36m[2023-07-17 00:11:27,035][257371] itr=305, itrs=2000, Progress: 15.25%[0m
[36m[2023-07-17 00:11:38,645][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 00:11:38,645][257371] FPS: 332526.41[0m
[36m[2023-07-17 00:11:42,937][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:11:42,942][257371] Reward + Measures: [[-710.33489324    0.98174298    0.00661533    0.97448796    0.96468335
     6.68861103]][0m
[37m[1m[2023-07-17 00:11:42,943][257371] Max Reward on eval: -710.334893238809[0m
[37m[1m[2023-07-17 00:11:42,943][257371] Min Reward on eval: -710.334893238809[0m
[37m[1m[2023-07-17 00:11:42,943][257371] Mean Reward across all agents: -710.334893238809[0m
[37m[1m[2023-07-17 00:11:42,943][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:11:48,120][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:11:48,135][257371] Reward + Measures: [[-234.62757491    0.39980003    0.31410003    0.36220002    0.37380001
     4.95494699]
 [  34.20962076    0.66830003    0.36520001    0.59890002    0.68160003
     3.25081944]
 [ 199.17790379    0.28749999    0.29749998    0.29090002    0.2854
     2.7447412 ]
 ...
 [-121.87607151    0.32950002    0.32640001    0.29679999    0.37220001
     4.0332036 ]
 [ -41.01337679    0.42300001    0.2676        0.35859999    0.37820002
     2.50347924]
 [ -87.23948951    0.27739999    0.29050002    0.2854        0.31150001
     3.6295166 ]][0m
[37m[1m[2023-07-17 00:11:48,136][257371] Max Reward on eval: 251.91644039414822[0m
[37m[1m[2023-07-17 00:11:48,136][257371] Min Reward on eval: -842.234527585865[0m
[37m[1m[2023-07-17 00:11:48,136][257371] Mean Reward across all agents: -107.00819985039801[0m
[37m[1m[2023-07-17 00:11:48,137][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:11:48,141][257371] mean_value=-976.8059148007222, max_value=462.24793932745104[0m
[37m[1m[2023-07-17 00:11:48,144][257371] New mean coefficients: [[-2.2057076   3.009492    0.01871238  0.46719956 -0.46687806 -0.45294234]][0m
[37m[1m[2023-07-17 00:11:48,145][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:11:57,168][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 00:11:57,168][257371] FPS: 425673.75[0m
[36m[2023-07-17 00:11:57,170][257371] itr=306, itrs=2000, Progress: 15.30%[0m
[36m[2023-07-17 00:12:08,838][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 00:12:08,838][257371] FPS: 330906.56[0m
[36m[2023-07-17 00:12:13,114][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:12:13,114][257371] Reward + Measures: [[-707.26334431    0.97846836    0.00716833    0.97176129    0.96110731
     6.68978977]][0m
[37m[1m[2023-07-17 00:12:13,114][257371] Max Reward on eval: -707.2633443143958[0m
[37m[1m[2023-07-17 00:12:13,115][257371] Min Reward on eval: -707.2633443143958[0m
[37m[1m[2023-07-17 00:12:13,115][257371] Mean Reward across all agents: -707.2633443143958[0m
[37m[1m[2023-07-17 00:12:13,115][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:12:18,185][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:12:18,190][257371] Reward + Measures: [[ -90.29516122    0.4082        0.3369        0.37380001    0.40830001
     3.19539857]
 [ -50.10768287    0.30410001    0.56420004    0.44960004    0.71730006
     4.35299397]
 [  12.69480999    0.2017        0.20250002    0.17440002    0.2066
     4.15235376]
 ...
 [ -66.06330921    0.26530001    0.48400003    0.40400001    0.49729997
     4.43770933]
 [ -92.16496876    0.24980001    0.22149999    0.21430002    0.2342
     3.77592969]
 [-542.65660955    0.70859998    0.17150001    0.84930003    0.86650002
     6.44941473]][0m
[37m[1m[2023-07-17 00:12:18,191][257371] Max Reward on eval: 71.09032521746121[0m
[37m[1m[2023-07-17 00:12:18,191][257371] Min Reward on eval: -703.2410049431958[0m
[37m[1m[2023-07-17 00:12:18,191][257371] Mean Reward across all agents: -76.10263400493211[0m
[37m[1m[2023-07-17 00:12:18,192][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:12:18,193][257371] mean_value=-1306.1904903048, max_value=66.35043782204842[0m
[37m[1m[2023-07-17 00:12:18,196][257371] New mean coefficients: [[-1.9103804   2.6038637   1.2839983   0.257357   -1.0140092  -0.41701692]][0m
[37m[1m[2023-07-17 00:12:18,197][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:12:27,319][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 00:12:27,319][257371] FPS: 421069.46[0m
[36m[2023-07-17 00:12:27,321][257371] itr=307, itrs=2000, Progress: 15.35%[0m
[36m[2023-07-17 00:12:39,160][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 00:12:39,161][257371] FPS: 326052.11[0m
[36m[2023-07-17 00:12:43,433][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:12:43,434][257371] Reward + Measures: [[-710.57657699    0.98052198    0.00722767    0.97367299    0.96350461
     6.71804333]][0m
[37m[1m[2023-07-17 00:12:43,434][257371] Max Reward on eval: -710.5765769899156[0m
[37m[1m[2023-07-17 00:12:43,434][257371] Min Reward on eval: -710.5765769899156[0m
[37m[1m[2023-07-17 00:12:43,434][257371] Mean Reward across all agents: -710.5765769899156[0m
[37m[1m[2023-07-17 00:12:43,435][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:12:48,454][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:12:48,460][257371] Reward + Measures: [[ -52.51689292    0.59199995    0.23150001    0.53759998    0.36489999
     4.60788298]
 [-767.86331176    0.98070002    0.008         0.9709        0.95850003
     6.8380723 ]
 [  64.45864543    0.1322        0.15619999    0.19419999    0.16830002
     4.13736629]
 ...
 [ -15.40060174    0.48990002    0.29800001    0.47270003    0.36399999
     4.53587484]
 [  12.59593254    0.1406        0.12310001    0.1798        0.1611
     4.04094839]
 [-301.58966256    0.75270003    0.0904        0.73470002    0.68410003
     4.99956512]][0m
[37m[1m[2023-07-17 00:12:48,461][257371] Max Reward on eval: 95.16079312358052[0m
[37m[1m[2023-07-17 00:12:48,461][257371] Min Reward on eval: -845.5359267999186[0m
[37m[1m[2023-07-17 00:12:48,462][257371] Mean Reward across all agents: -196.60088979085222[0m
[37m[1m[2023-07-17 00:12:48,462][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:12:48,468][257371] mean_value=-975.3891626032784, max_value=141.15652186899095[0m
[37m[1m[2023-07-17 00:12:48,473][257371] New mean coefficients: [[-1.8178712   2.732541   -0.602708   -0.23255134  0.12550986 -0.8330002 ]][0m
[37m[1m[2023-07-17 00:12:48,474][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:12:57,479][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 00:12:57,479][257371] FPS: 426555.63[0m
[36m[2023-07-17 00:12:57,481][257371] itr=308, itrs=2000, Progress: 15.40%[0m
[36m[2023-07-17 00:13:09,296][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 00:13:09,296][257371] FPS: 326802.93[0m
[36m[2023-07-17 00:13:13,633][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:13:13,639][257371] Reward + Measures: [[-698.02755263    0.98058134    0.00747667    0.97332925    0.96324104
     6.70660067]][0m
[37m[1m[2023-07-17 00:13:13,639][257371] Max Reward on eval: -698.027552631918[0m
[37m[1m[2023-07-17 00:13:13,639][257371] Min Reward on eval: -698.027552631918[0m
[37m[1m[2023-07-17 00:13:13,639][257371] Mean Reward across all agents: -698.027552631918[0m
[37m[1m[2023-07-17 00:13:13,640][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:13:18,718][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:13:18,724][257371] Reward + Measures: [[-781.39403914    0.97539997    0.0103        0.96530002    0.95669997
     7.35306025]
 [-915.50804899    0.99130005    0.0043        0.98210001    0.97510004
     7.65518045]
 [-124.37057017    0.59410006    0.15939999    0.53290004    0.37830001
     4.01495075]
 ...
 [-115.12785894    0.47610003    0.35540003    0.51679999    0.53469998
     3.94919705]
 [-504.30386255    0.85710001    0.0247        0.83740008    0.82709998
     7.24047995]
 [-736.69986533    0.89639997    0.0143        0.91049999    0.88450003
     7.09625483]][0m
[37m[1m[2023-07-17 00:13:18,724][257371] Max Reward on eval: 164.91720387265087[0m
[37m[1m[2023-07-17 00:13:18,724][257371] Min Reward on eval: -926.3285293991212[0m
[37m[1m[2023-07-17 00:13:18,725][257371] Mean Reward across all agents: -436.84378337839064[0m
[37m[1m[2023-07-17 00:13:18,725][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:13:18,728][257371] mean_value=-582.9367974353443, max_value=256.1250833356177[0m
[37m[1m[2023-07-17 00:13:18,730][257371] New mean coefficients: [[-2.1446128   2.6285353   1.7503332   0.37780905 -0.61754215 -0.60888726]][0m
[37m[1m[2023-07-17 00:13:18,731][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:13:27,799][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 00:13:27,799][257371] FPS: 423571.71[0m
[36m[2023-07-17 00:13:27,802][257371] itr=309, itrs=2000, Progress: 15.45%[0m
[36m[2023-07-17 00:13:39,757][257371] train() took 11.89 seconds to complete[0m
[36m[2023-07-17 00:13:39,758][257371] FPS: 322893.57[0m
[36m[2023-07-17 00:13:44,102][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:13:44,103][257371] Reward + Measures: [[-709.87770421    0.9770816     0.00822933    0.96983171    0.95933264
     6.69341183]][0m
[37m[1m[2023-07-17 00:13:44,103][257371] Max Reward on eval: -709.877704206338[0m
[37m[1m[2023-07-17 00:13:44,103][257371] Min Reward on eval: -709.877704206338[0m
[37m[1m[2023-07-17 00:13:44,103][257371] Mean Reward across all agents: -709.877704206338[0m
[37m[1m[2023-07-17 00:13:44,104][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:13:49,205][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:13:49,205][257371] Reward + Measures: [[-113.38378099    0.35330001    0.28919998    0.26770002    0.2956
     4.40493536]
 [ 152.02185631    0.50419998    0.52149999    0.29370001    0.54049999
     4.54728031]
 [  66.14797752    0.49610004    0.82520002    0.32810003    0.83940011
     6.20726538]
 ...
 [ -61.02338274    0.36910003    0.27509999    0.29190001    0.2483
     4.63566399]
 [ -97.17987775    0.51320004    0.31300002    0.50489998    0.42430001
     4.28656912]
 [ -49.98688985    0.30149999    0.31060001    0.2913        0.31960002
     4.48643017]][0m
[37m[1m[2023-07-17 00:13:49,206][257371] Max Reward on eval: 379.94931501145476[0m
[37m[1m[2023-07-17 00:13:49,206][257371] Min Reward on eval: -795.0344924814533[0m
[37m[1m[2023-07-17 00:13:49,206][257371] Mean Reward across all agents: 10.354305334691777[0m
[37m[1m[2023-07-17 00:13:49,206][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:13:49,215][257371] mean_value=-97.7413687821171, max_value=855.5617935445159[0m
[37m[1m[2023-07-17 00:13:49,218][257371] New mean coefficients: [[-1.3041915   2.17729     4.1606174   0.27216786 -1.5422974  -0.64915335]][0m
[37m[1m[2023-07-17 00:13:49,219][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:13:58,402][257371] train() took 9.18 seconds to complete[0m
[36m[2023-07-17 00:13:58,403][257371] FPS: 418238.51[0m
[36m[2023-07-17 00:13:58,405][257371] itr=310, itrs=2000, Progress: 15.50%[0m
[37m[1m[2023-07-17 00:16:35,070][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000290[0m
[36m[2023-07-17 00:16:47,383][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 00:16:47,383][257371] FPS: 327594.86[0m
[36m[2023-07-17 00:16:51,668][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:16:51,668][257371] Reward + Measures: [[-692.34381329    0.97615135    0.01020733    0.96798795    0.9573766
     6.61403131]][0m
[37m[1m[2023-07-17 00:16:51,668][257371] Max Reward on eval: -692.3438132894851[0m
[37m[1m[2023-07-17 00:16:51,669][257371] Min Reward on eval: -692.3438132894851[0m
[37m[1m[2023-07-17 00:16:51,669][257371] Mean Reward across all agents: -692.3438132894851[0m
[37m[1m[2023-07-17 00:16:51,669][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:16:56,582][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:16:56,583][257371] Reward + Measures: [[  20.64643728    0.10640001    0.64309996    0.67259997    0.66250002
     6.6553607 ]
 [ -37.24781967    0.1288        0.4086        0.47329998    0.44549999
     6.46677017]
 [ -34.78885295    0.12100001    0.43030006    0.47129998    0.47259998
     6.43574905]
 ...
 [ -79.09596655    0.2361        0.65219998    0.66420001    0.5273
     6.6737566 ]
 [ -88.21575229    0.25710002    0.54530001    0.59189999    0.48160002
     6.19954157]
 [-143.60938448    0.33219999    0.42020002    0.36660001    0.2631
     6.66449833]][0m
[37m[1m[2023-07-17 00:16:56,583][257371] Max Reward on eval: 127.35730124455877[0m
[37m[1m[2023-07-17 00:16:56,583][257371] Min Reward on eval: -780.4171523845755[0m
[37m[1m[2023-07-17 00:16:56,583][257371] Mean Reward across all agents: -80.79748202739644[0m
[37m[1m[2023-07-17 00:16:56,584][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:16:56,591][257371] mean_value=-162.62315979090644, max_value=468.94267696598547[0m
[37m[1m[2023-07-17 00:16:56,593][257371] New mean coefficients: [[-1.0274544   2.177367    4.402503    0.22981015 -1.3306316  -0.5247822 ]][0m
[37m[1m[2023-07-17 00:16:56,594][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:17:05,532][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 00:17:05,532][257371] FPS: 429733.71[0m
[36m[2023-07-17 00:17:05,534][257371] itr=311, itrs=2000, Progress: 15.55%[0m
[36m[2023-07-17 00:17:17,171][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 00:17:17,171][257371] FPS: 331744.72[0m
[36m[2023-07-17 00:17:21,429][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:17:21,435][257371] Reward + Measures: [[-684.52691841    0.96710634    0.01354167    0.95819032    0.94698828
     6.51519918]][0m
[37m[1m[2023-07-17 00:17:21,435][257371] Max Reward on eval: -684.5269184110659[0m
[37m[1m[2023-07-17 00:17:21,435][257371] Min Reward on eval: -684.5269184110659[0m
[37m[1m[2023-07-17 00:17:21,436][257371] Mean Reward across all agents: -684.5269184110659[0m
[37m[1m[2023-07-17 00:17:21,436][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:17:26,450][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:17:26,455][257371] Reward + Measures: [[-308.74603618    0.7191        0.153         0.69539994    0.69519997
     5.75993872]
 [-598.52424419    0.85769999    0.0408        0.85109997    0.81849998
     6.00265837]
 [-639.2060051     0.96390003    0.0261        0.96149999    0.95520002
     6.07058096]
 ...
 [ -27.78518715    0.39739999    0.25049999    0.30760002    0.33890003
     3.0200336 ]
 [-328.21385285    0.75559998    0.1214        0.72720003    0.71060002
     5.02973938]
 [ -34.33358975    0.41770002    0.26859999    0.3529        0.38150001
     3.22750258]][0m
[37m[1m[2023-07-17 00:17:26,456][257371] Max Reward on eval: 87.36848782179877[0m
[37m[1m[2023-07-17 00:17:26,456][257371] Min Reward on eval: -728.9812278883298[0m
[37m[1m[2023-07-17 00:17:26,456][257371] Mean Reward across all agents: -211.07064314832684[0m
[37m[1m[2023-07-17 00:17:26,457][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:17:26,459][257371] mean_value=-585.931033516878, max_value=331.11009377080916[0m
[37m[1m[2023-07-17 00:17:26,462][257371] New mean coefficients: [[-1.7892693   2.0701222   4.141998    0.660298   -1.1454349  -0.50074804]][0m
[37m[1m[2023-07-17 00:17:26,462][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:17:35,486][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 00:17:35,486][257371] FPS: 425631.01[0m
[36m[2023-07-17 00:17:35,489][257371] itr=312, itrs=2000, Progress: 15.60%[0m
[36m[2023-07-17 00:17:47,250][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 00:17:47,250][257371] FPS: 328260.66[0m
[36m[2023-07-17 00:17:51,583][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:17:51,583][257371] Reward + Measures: [[-670.92005476    0.95912534    0.017422      0.94965798    0.93834829
     6.42312813]][0m
[37m[1m[2023-07-17 00:17:51,584][257371] Max Reward on eval: -670.9200547610943[0m
[37m[1m[2023-07-17 00:17:51,584][257371] Min Reward on eval: -670.9200547610943[0m
[37m[1m[2023-07-17 00:17:51,584][257371] Mean Reward across all agents: -670.9200547610943[0m
[37m[1m[2023-07-17 00:17:51,584][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:17:56,648][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:17:56,648][257371] Reward + Measures: [[-588.4464903     0.87790006    0.0176        0.884         0.85570002
     6.25346851]
 [-709.92379378    0.98079997    0.0096        0.96989995    0.96060008
     6.50419235]
 [-715.68744659    0.97410005    0.0156        0.95730001    0.94680005
     6.36084747]
 ...
 [-677.2504616     0.94709998    0.0404        0.92070001    0.91720003
     6.02843571]
 [-600.46248244    0.97939998    0.009         0.96399993    0.95359993
     6.12578344]
 [-582.10125427    0.80330002    0.0157        0.80809993    0.79390001
     6.01480198]][0m
[37m[1m[2023-07-17 00:17:56,649][257371] Max Reward on eval: 38.879065385926516[0m
[37m[1m[2023-07-17 00:17:56,649][257371] Min Reward on eval: -754.5087280136534[0m
[37m[1m[2023-07-17 00:17:56,649][257371] Mean Reward across all agents: -561.4569581585382[0m
[37m[1m[2023-07-17 00:17:56,649][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:17:56,651][257371] mean_value=-660.2123232915067, max_value=160.99322120625266[0m
[37m[1m[2023-07-17 00:17:56,653][257371] New mean coefficients: [[-2.653569    1.8391793   8.482751    1.5645884  -1.8477302  -0.46431595]][0m
[37m[1m[2023-07-17 00:17:56,654][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:18:05,831][257371] train() took 9.18 seconds to complete[0m
[36m[2023-07-17 00:18:05,831][257371] FPS: 418527.38[0m
[36m[2023-07-17 00:18:05,834][257371] itr=313, itrs=2000, Progress: 15.65%[0m
[36m[2023-07-17 00:18:17,693][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 00:18:17,693][257371] FPS: 325511.96[0m
[36m[2023-07-17 00:18:22,048][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:18:22,049][257371] Reward + Measures: [[-656.18252148    0.94786197    0.02436633    0.93684196    0.92693734
     6.27977705]][0m
[37m[1m[2023-07-17 00:18:22,049][257371] Max Reward on eval: -656.1825214755228[0m
[37m[1m[2023-07-17 00:18:22,049][257371] Min Reward on eval: -656.1825214755228[0m
[37m[1m[2023-07-17 00:18:22,049][257371] Mean Reward across all agents: -656.1825214755228[0m
[37m[1m[2023-07-17 00:18:22,050][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:18:27,098][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:18:27,099][257371] Reward + Measures: [[-346.54528712    0.9393        0.0381        0.9163        0.90259999
     6.27764177]
 [-571.69367981    0.95920002    0.0242        0.94999999    0.94200003
     6.19336414]
 [ -31.6979058     0.89639997    0.0189        0.88010007    0.72860003
     5.24110794]
 ...
 [-380.83832168    0.9601        0.0275        0.94029999    0.92430001
     5.8063798 ]
 [-250.27394806    0.98460007    0.0084        0.97660011    0.9582001
     6.30959272]
 [-124.53692008    0.90440005    0.0537        0.87879992    0.81709999
     5.43471336]][0m
[37m[1m[2023-07-17 00:18:27,099][257371] Max Reward on eval: 110.97344411169179[0m
[37m[1m[2023-07-17 00:18:27,099][257371] Min Reward on eval: -760.9128989997786[0m
[37m[1m[2023-07-17 00:18:27,100][257371] Mean Reward across all agents: -306.0957862388391[0m
[37m[1m[2023-07-17 00:18:27,100][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:18:27,103][257371] mean_value=-357.6563088309259, max_value=394.5273879951497[0m
[37m[1m[2023-07-17 00:18:27,106][257371] New mean coefficients: [[-3.0469615   1.6985513  12.911848    2.5894952  -1.5206739  -0.31471917]][0m
[37m[1m[2023-07-17 00:18:27,107][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:18:36,211][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 00:18:36,212][257371] FPS: 421841.59[0m
[36m[2023-07-17 00:18:36,214][257371] itr=314, itrs=2000, Progress: 15.70%[0m
[36m[2023-07-17 00:18:47,958][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 00:18:47,959][257371] FPS: 328800.49[0m
[36m[2023-07-17 00:18:52,288][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:18:52,288][257371] Reward + Measures: [[-649.34070837    0.93975925    0.029259      0.92761463    0.9186157
     6.22300053]][0m
[37m[1m[2023-07-17 00:18:52,288][257371] Max Reward on eval: -649.3407083730856[0m
[37m[1m[2023-07-17 00:18:52,289][257371] Min Reward on eval: -649.3407083730856[0m
[37m[1m[2023-07-17 00:18:52,289][257371] Mean Reward across all agents: -649.3407083730856[0m
[37m[1m[2023-07-17 00:18:52,289][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:18:57,390][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:18:57,391][257371] Reward + Measures: [[ 98.66512803   0.19129999   0.22590001   0.15010001   0.1621
    5.80731201]
 [249.3702424    0.58899999   0.56389999   0.55150002   0.49720001
    5.23080587]
 [ 92.5272208    0.34900001   0.50500005   0.25830001   0.4492
    4.65620804]
 ...
 [ 32.0307133    0.18569998   0.19560002   0.1452       0.13380001
    5.60893393]
 [ 30.70382891   0.13190001   0.13800001   0.114        0.1019
    6.30140638]
 [ 64.34846205   0.23360001   0.29270002   0.1635       0.21250001
    5.87661886]][0m
[37m[1m[2023-07-17 00:18:57,391][257371] Max Reward on eval: 249.37024240475148[0m
[37m[1m[2023-07-17 00:18:57,391][257371] Min Reward on eval: -46.12211964651942[0m
[37m[1m[2023-07-17 00:18:57,391][257371] Mean Reward across all agents: 58.17490705715839[0m
[37m[1m[2023-07-17 00:18:57,392][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:18:57,395][257371] mean_value=-208.72760955130556, max_value=707.2154831606895[0m
[37m[1m[2023-07-17 00:18:57,398][257371] New mean coefficients: [[-2.1621528   1.3932538   9.625797    1.1487942  -1.085283    0.01101908]][0m
[37m[1m[2023-07-17 00:18:57,399][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:19:06,567][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-17 00:19:06,567][257371] FPS: 418919.51[0m
[36m[2023-07-17 00:19:06,569][257371] itr=315, itrs=2000, Progress: 15.75%[0m
[36m[2023-07-17 00:19:18,378][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 00:19:18,378][257371] FPS: 327014.95[0m
[36m[2023-07-17 00:19:22,649][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:19:22,650][257371] Reward + Measures: [[-622.59772784    0.921462      0.03855133    0.9095763     0.90270734
     6.16001177]][0m
[37m[1m[2023-07-17 00:19:22,650][257371] Max Reward on eval: -622.5977278362507[0m
[37m[1m[2023-07-17 00:19:22,650][257371] Min Reward on eval: -622.5977278362507[0m
[37m[1m[2023-07-17 00:19:22,651][257371] Mean Reward across all agents: -622.5977278362507[0m
[37m[1m[2023-07-17 00:19:22,651][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:19:27,690][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:19:27,695][257371] Reward + Measures: [[-569.26727481    0.83050007    0.1243        0.83429998    0.8915
     5.53445864]
 [-385.11879629    0.6074        0.30739999    0.68040001    0.86800003
     5.0012393 ]
 [-477.44593886    0.72910005    0.1469        0.72509998    0.79970002
     5.49170685]
 ...
 [-428.08536947    0.65469998    0.21030001    0.6735        0.78580004
     4.96238232]
 [-151.81322482    0.34660003    0.48310003    0.34510002    0.59850001
     3.73701978]
 [  30.81464711    0.2705        0.3461        0.19360001    0.36520001
     3.22198939]][0m
[37m[1m[2023-07-17 00:19:27,696][257371] Max Reward on eval: 30.814647110272197[0m
[37m[1m[2023-07-17 00:19:27,696][257371] Min Reward on eval: -766.8341674627969[0m
[37m[1m[2023-07-17 00:19:27,696][257371] Mean Reward across all agents: -353.69874821716445[0m
[37m[1m[2023-07-17 00:19:27,697][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:19:27,703][257371] mean_value=-413.70621356289263, max_value=172.39559653372123[0m
[37m[1m[2023-07-17 00:19:27,706][257371] New mean coefficients: [[-1.535835    1.7978499  10.956925    0.83843577 -1.8085403  -0.3951198 ]][0m
[37m[1m[2023-07-17 00:19:27,706][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:19:36,762][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 00:19:36,762][257371] FPS: 424131.84[0m
[36m[2023-07-17 00:19:36,765][257371] itr=316, itrs=2000, Progress: 15.80%[0m
[36m[2023-07-17 00:19:48,408][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 00:19:48,409][257371] FPS: 331677.18[0m
[36m[2023-07-17 00:19:52,808][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:19:52,808][257371] Reward + Measures: [[-584.28269938    0.90272439    0.05090266    0.88876367    0.8850379
     6.02293158]][0m
[37m[1m[2023-07-17 00:19:52,808][257371] Max Reward on eval: -584.2826993822848[0m
[37m[1m[2023-07-17 00:19:52,809][257371] Min Reward on eval: -584.2826993822848[0m
[37m[1m[2023-07-17 00:19:52,809][257371] Mean Reward across all agents: -584.2826993822848[0m
[37m[1m[2023-07-17 00:19:52,809][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:19:58,089][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:19:58,090][257371] Reward + Measures: [[-362.60999648    0.62000006    0.29140002    0.71489996    0.83070004
     5.16225767]
 [   1.64483314    0.61930001    0.28079998    0.63480002    0.71899998
     4.16390514]
 [-597.71081163    0.87239999    0.1117        0.88559991    0.92220002
     5.90365171]
 ...
 [ -29.39111604    0.2483        0.55119997    0.354         0.55629998
     2.78981352]
 [-268.29363693    0.57720006    0.2956        0.6268        0.74590009
     4.51207638]
 [-197.57952223    0.36069998    0.5984        0.50260001    0.87070006
     4.13591528]][0m
[37m[1m[2023-07-17 00:19:58,090][257371] Max Reward on eval: 81.14890403021127[0m
[37m[1m[2023-07-17 00:19:58,090][257371] Min Reward on eval: -717.9118690580129[0m
[37m[1m[2023-07-17 00:19:58,091][257371] Mean Reward across all agents: -243.30684906717553[0m
[37m[1m[2023-07-17 00:19:58,091][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:19:58,096][257371] mean_value=-262.95918415313054, max_value=279.04494960040927[0m
[37m[1m[2023-07-17 00:19:58,099][257371] New mean coefficients: [[-0.9991939   1.4896767  12.557812    0.5850605  -1.3903738  -0.76753795]][0m
[37m[1m[2023-07-17 00:19:58,100][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:20:07,235][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 00:20:07,235][257371] FPS: 420435.62[0m
[36m[2023-07-17 00:20:07,237][257371] itr=317, itrs=2000, Progress: 15.85%[0m
[36m[2023-07-17 00:20:19,200][257371] train() took 11.90 seconds to complete[0m
[36m[2023-07-17 00:20:19,200][257371] FPS: 322786.74[0m
[36m[2023-07-17 00:20:23,475][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:20:23,475][257371] Reward + Measures: [[-510.4028087     0.84679461    0.08285266    0.83203465    0.84032762
     5.78043222]][0m
[37m[1m[2023-07-17 00:20:23,476][257371] Max Reward on eval: -510.40280870016693[0m
[37m[1m[2023-07-17 00:20:23,476][257371] Min Reward on eval: -510.40280870016693[0m
[37m[1m[2023-07-17 00:20:23,476][257371] Mean Reward across all agents: -510.40280870016693[0m
[37m[1m[2023-07-17 00:20:23,476][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:20:28,515][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:20:28,567][257371] Reward + Measures: [[  17.52230584    0.2445        0.41610003    0.3019        0.42740002
     3.03480005]
 [   4.41653888    0.22050002    0.373         0.25700003    0.3829
     2.9844501 ]
 [  43.24259323    0.40240002    0.2534        0.45680004    0.40769997
     3.56792688]
 ...
 [-291.45265943    0.64239997    0.18600002    0.63999999    0.69999999
     4.60854435]
 [  13.88056543    0.34          0.3946        0.31030002    0.41360003
     3.01689219]
 [ -44.20460885    0.42180005    0.28739998    0.49980003    0.57269996
     4.38645124]][0m
[37m[1m[2023-07-17 00:20:28,567][257371] Max Reward on eval: 73.97212144760415[0m
[37m[1m[2023-07-17 00:20:28,567][257371] Min Reward on eval: -533.1982384175062[0m
[37m[1m[2023-07-17 00:20:28,568][257371] Mean Reward across all agents: -41.211475686606455[0m
[37m[1m[2023-07-17 00:20:28,568][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:20:28,570][257371] mean_value=-893.5786526711797, max_value=445.70651498492805[0m
[37m[1m[2023-07-17 00:20:28,573][257371] New mean coefficients: [[-0.12344557  1.0588089  10.439595   -0.26688135 -0.76689917 -1.0906771 ]][0m
[37m[1m[2023-07-17 00:20:28,574][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:20:37,734][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-17 00:20:37,734][257371] FPS: 419286.26[0m
[36m[2023-07-17 00:20:37,737][257371] itr=318, itrs=2000, Progress: 15.90%[0m
[36m[2023-07-17 00:20:49,569][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 00:20:49,569][257371] FPS: 326267.49[0m
[36m[2023-07-17 00:20:53,879][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:20:53,880][257371] Reward + Measures: [[-439.7273237     0.77063304    0.12743634    0.75710064    0.780007
     5.45102119]][0m
[37m[1m[2023-07-17 00:20:53,880][257371] Max Reward on eval: -439.72732370301475[0m
[37m[1m[2023-07-17 00:20:53,880][257371] Min Reward on eval: -439.72732370301475[0m
[37m[1m[2023-07-17 00:20:53,880][257371] Mean Reward across all agents: -439.72732370301475[0m
[37m[1m[2023-07-17 00:20:53,881][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:20:58,943][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:20:58,944][257371] Reward + Measures: [[-186.14463787    0.63819999    0.3231        0.91429996    0.93710005
     7.08727884]
 [  -7.94415187    0.29809999    0.479         0.72080004    0.7001
     6.48860645]
 [-397.29055688    0.72030002    0.1208        0.74700004    0.75730008
     6.26029158]
 ...
 [-261.54042579    0.51700002    0.35320002    0.77270001    0.82859993
     6.80680037]
 [-213.50823366    0.25209999    0.65440005    0.81010002    0.91049999
     6.65885544]
 [ -84.55417854    0.55620003    0.28620002    0.77950007    0.7985
     6.7342186 ]][0m
[37m[1m[2023-07-17 00:20:58,944][257371] Max Reward on eval: 497.8448528939392[0m
[37m[1m[2023-07-17 00:20:58,944][257371] Min Reward on eval: -609.4110183629207[0m
[37m[1m[2023-07-17 00:20:58,944][257371] Mean Reward across all agents: -119.43033521884941[0m
[37m[1m[2023-07-17 00:20:58,944][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:20:58,957][257371] mean_value=-14.920046426314002, max_value=852.5349478533492[0m
[37m[1m[2023-07-17 00:20:58,959][257371] New mean coefficients: [[ 1.2421815  0.8000469 11.184934  -0.9969129 -1.9896092 -1.525197 ]][0m
[37m[1m[2023-07-17 00:20:58,960][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:21:08,097][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 00:21:08,097][257371] FPS: 420367.62[0m
[36m[2023-07-17 00:21:08,100][257371] itr=319, itrs=2000, Progress: 15.95%[0m
[36m[2023-07-17 00:21:19,911][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 00:21:19,911][257371] FPS: 326983.71[0m
[36m[2023-07-17 00:21:24,268][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:21:24,268][257371] Reward + Measures: [[-373.11478476    0.70016128    0.17335299    0.68421364    0.72312969
     5.11773634]][0m
[37m[1m[2023-07-17 00:21:24,268][257371] Max Reward on eval: -373.114784762448[0m
[37m[1m[2023-07-17 00:21:24,269][257371] Min Reward on eval: -373.114784762448[0m
[37m[1m[2023-07-17 00:21:24,269][257371] Mean Reward across all agents: -373.114784762448[0m
[37m[1m[2023-07-17 00:21:24,269][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:21:29,389][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:21:29,390][257371] Reward + Measures: [[-37.72260615   0.14830001   0.67699999   0.52030003   0.75529999
    3.58758545]
 [-10.20959187   0.17700002   0.59799999   0.46690002   0.65399998
    3.50375175]
 [-94.18544162   0.45210001   0.23840001   0.45380002   0.44449997
    4.11380386]
 ...
 [  2.40748686   0.24879999   0.54100001   0.39980003   0.61950004
    3.3667717 ]
 [-26.72735255   0.1917       0.6559       0.4492       0.699
    3.33207631]
 [  1.64380283   0.33750001   0.40700004   0.34950003   0.4285
    3.306427  ]][0m
[37m[1m[2023-07-17 00:21:29,390][257371] Max Reward on eval: 92.17583730719052[0m
[37m[1m[2023-07-17 00:21:29,390][257371] Min Reward on eval: -460.8294959451538[0m
[37m[1m[2023-07-17 00:21:29,391][257371] Mean Reward across all agents: -25.424983961052078[0m
[37m[1m[2023-07-17 00:21:29,391][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:21:29,398][257371] mean_value=-66.55302215179421, max_value=419.94719939945503[0m
[37m[1m[2023-07-17 00:21:29,401][257371] New mean coefficients: [[ 1.2507327  0.9785802 11.658201  -1.0775502 -2.2996724 -1.528703 ]][0m
[37m[1m[2023-07-17 00:21:29,402][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:21:38,564][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-17 00:21:38,565][257371] FPS: 419173.38[0m
[36m[2023-07-17 00:21:38,567][257371] itr=320, itrs=2000, Progress: 16.00%[0m
[37m[1m[2023-07-17 00:24:20,677][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000300[0m
[36m[2023-07-17 00:24:33,008][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 00:24:33,008][257371] FPS: 326037.88[0m
[36m[2023-07-17 00:24:37,127][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:24:37,128][257371] Reward + Measures: [[-236.34008422    0.54840565    0.27847567    0.53514296    0.61836267
     4.49756718]][0m
[37m[1m[2023-07-17 00:24:37,128][257371] Max Reward on eval: -236.34008421600504[0m
[37m[1m[2023-07-17 00:24:37,128][257371] Min Reward on eval: -236.34008421600504[0m
[37m[1m[2023-07-17 00:24:37,128][257371] Mean Reward across all agents: -236.34008421600504[0m
[37m[1m[2023-07-17 00:24:37,129][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:24:42,067][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:24:42,067][257371] Reward + Measures: [[ 113.33266923    0.20100001    0.5643        0.49720001    0.51340002
     4.49133539]
 [-198.7096901     0.63599998    0.24780002    0.66479999    0.70340002
     5.02213144]
 [   6.92728869    0.67659998    0.2402        0.76020002    0.80880004
     5.33845615]
 ...
 [ -26.88553715    0.33020002    0.46799999    0.47539997    0.50940001
     3.63411117]
 [ -43.6037354     0.36410001    0.46199998    0.43940002    0.56290001
     3.87211299]
 [ -17.1604567     0.1551        0.40489998    0.33790001    0.43340001
     3.62989116]][0m
[37m[1m[2023-07-17 00:24:42,068][257371] Max Reward on eval: 208.87960242952687[0m
[37m[1m[2023-07-17 00:24:42,068][257371] Min Reward on eval: -543.3627784385578[0m
[37m[1m[2023-07-17 00:24:42,068][257371] Mean Reward across all agents: -17.389840184897636[0m
[37m[1m[2023-07-17 00:24:42,068][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:24:42,074][257371] mean_value=-403.3032442714062, max_value=578.9993495620508[0m
[37m[1m[2023-07-17 00:24:42,076][257371] New mean coefficients: [[ 1.5326076  0.6536591 11.002941  -1.3784747 -2.2129042 -1.7173873]][0m
[37m[1m[2023-07-17 00:24:42,077][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:24:51,140][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 00:24:51,140][257371] FPS: 423796.20[0m
[36m[2023-07-17 00:24:51,142][257371] itr=321, itrs=2000, Progress: 16.05%[0m
[36m[2023-07-17 00:25:03,009][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 00:25:03,009][257371] FPS: 325401.03[0m
[36m[2023-07-17 00:25:07,243][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:25:07,244][257371] Reward + Measures: [[-91.72873787   0.39111766   0.39544898   0.38147399   0.52096295
    3.86220765]][0m
[37m[1m[2023-07-17 00:25:07,244][257371] Max Reward on eval: -91.72873787022257[0m
[37m[1m[2023-07-17 00:25:07,244][257371] Min Reward on eval: -91.72873787022257[0m
[37m[1m[2023-07-17 00:25:07,244][257371] Mean Reward across all agents: -91.72873787022257[0m
[37m[1m[2023-07-17 00:25:07,245][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:25:12,513][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:25:12,519][257371] Reward + Measures: [[-92.66387967   0.21659999   0.32100001   0.244        0.31659999
    4.53268003]
 [-41.79158949   0.27789998   0.40640002   0.35250002   0.39290002
    4.15526533]
 [ 54.65447957   0.51539999   0.52270001   0.53940004   0.52560002
    4.0192976 ]
 ...
 [ 54.2040248    0.442        0.51370001   0.49110004   0.49829999
    3.91173816]
 [-52.40158244   0.18910001   0.27470002   0.22260001   0.24290001
    4.70278502]
 [-98.59658005   0.26480001   0.47980005   0.43739995   0.47409996
    4.32241821]][0m
[37m[1m[2023-07-17 00:25:12,519][257371] Max Reward on eval: 123.9664304137521[0m
[37m[1m[2023-07-17 00:25:12,519][257371] Min Reward on eval: -138.53076489625965[0m
[37m[1m[2023-07-17 00:25:12,520][257371] Mean Reward across all agents: -11.190419274832912[0m
[37m[1m[2023-07-17 00:25:12,520][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:25:12,522][257371] mean_value=-245.38913803153545, max_value=254.07632059136705[0m
[37m[1m[2023-07-17 00:25:12,524][257371] New mean coefficients: [[ 0.7898335   0.80860305  8.454301   -1.8027396  -1.6631548  -1.324316  ]][0m
[37m[1m[2023-07-17 00:25:12,525][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:25:21,661][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 00:25:21,661][257371] FPS: 420428.79[0m
[36m[2023-07-17 00:25:21,663][257371] itr=322, itrs=2000, Progress: 16.10%[0m
[36m[2023-07-17 00:25:33,630][257371] train() took 11.90 seconds to complete[0m
[36m[2023-07-17 00:25:33,630][257371] FPS: 322570.00[0m
[36m[2023-07-17 00:25:38,015][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:25:38,016][257371] Reward + Measures: [[-47.59071885   0.34047168   0.44222102   0.3344973    0.50526768
    3.64000511]][0m
[37m[1m[2023-07-17 00:25:38,016][257371] Max Reward on eval: -47.59071884511524[0m
[37m[1m[2023-07-17 00:25:38,016][257371] Min Reward on eval: -47.59071884511524[0m
[37m[1m[2023-07-17 00:25:38,016][257371] Mean Reward across all agents: -47.59071884511524[0m
[37m[1m[2023-07-17 00:25:38,017][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:25:43,089][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:25:43,090][257371] Reward + Measures: [[ -25.63468489    0.40650001    0.30829999    0.51200002    0.55019999
     3.75912642]
 [-276.87883278    0.74250001    0.1987        0.8075        0.83570004
     5.46164274]
 [ -75.96941616    0.40710002    0.3935        0.55790001    0.58999997
     3.83974314]
 ...
 [ -34.69477946    0.2947        0.36950001    0.36939999    0.39019999
     3.04671168]
 [-106.5385566     0.42570001    0.40170002    0.58700001    0.64810002
     4.1397624 ]
 [-304.574762      0.66309994    0.18910001    0.70730001    0.74999994
     5.4238081 ]][0m
[37m[1m[2023-07-17 00:25:43,090][257371] Max Reward on eval: 80.07671143654734[0m
[37m[1m[2023-07-17 00:25:43,090][257371] Min Reward on eval: -777.4714355455246[0m
[37m[1m[2023-07-17 00:25:43,090][257371] Mean Reward across all agents: -224.42987224213167[0m
[37m[1m[2023-07-17 00:25:43,091][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:25:43,094][257371] mean_value=-252.83495000984502, max_value=279.72212980595697[0m
[37m[1m[2023-07-17 00:25:43,096][257371] New mean coefficients: [[ 0.6184481   0.32322523  9.913123   -1.4713144  -2.4035199  -1.1078146 ]][0m
[37m[1m[2023-07-17 00:25:43,097][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:25:52,197][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 00:25:52,197][257371] FPS: 422081.42[0m
[36m[2023-07-17 00:25:52,199][257371] itr=323, itrs=2000, Progress: 16.15%[0m
[36m[2023-07-17 00:26:04,087][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 00:26:04,087][257371] FPS: 324742.17[0m
[36m[2023-07-17 00:26:08,384][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:26:08,390][257371] Reward + Measures: [[-36.69959854   0.31749934   0.47383967   0.31956765   0.51967597
    3.55637789]][0m
[37m[1m[2023-07-17 00:26:08,390][257371] Max Reward on eval: -36.69959853956347[0m
[37m[1m[2023-07-17 00:26:08,390][257371] Min Reward on eval: -36.69959853956347[0m
[37m[1m[2023-07-17 00:26:08,390][257371] Mean Reward across all agents: -36.69959853956347[0m
[37m[1m[2023-07-17 00:26:08,391][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:26:13,377][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:26:13,378][257371] Reward + Measures: [[-118.90763126    0.45500001    0.48230001    0.4901        0.49980003
     6.41599369]
 [ -64.52875937    0.50050002    0.50400001    0.542         0.5449
     6.47369719]
 [ -87.61813831    0.40459999    0.40510002    0.43340001    0.45120001
     6.80082798]
 ...
 [ -39.02190167    0.40019998    0.40079999    0.4375        0.4454
     7.07077551]
 [-134.11841753    0.171         0.1814        0.197         0.22760001
     5.15823317]
 [ -60.44517228    0.49910003    0.5104        0.50220007    0.48849997
     6.95432138]][0m
[37m[1m[2023-07-17 00:26:13,378][257371] Max Reward on eval: 55.51440282762051[0m
[37m[1m[2023-07-17 00:26:13,378][257371] Min Reward on eval: -175.85405347356573[0m
[37m[1m[2023-07-17 00:26:13,379][257371] Mean Reward across all agents: -57.495860303848445[0m
[37m[1m[2023-07-17 00:26:13,379][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:26:13,383][257371] mean_value=-111.86205321459803, max_value=521.6145201336034[0m
[37m[1m[2023-07-17 00:26:13,386][257371] New mean coefficients: [[ 0.86682785  0.14903826  8.281684   -2.0851893  -1.3192742  -1.1410317 ]][0m
[37m[1m[2023-07-17 00:26:13,392][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:26:22,434][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 00:26:22,434][257371] FPS: 424760.45[0m
[36m[2023-07-17 00:26:22,436][257371] itr=324, itrs=2000, Progress: 16.20%[0m
[36m[2023-07-17 00:26:34,201][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 00:26:34,201][257371] FPS: 328285.98[0m
[36m[2023-07-17 00:26:38,510][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:26:38,515][257371] Reward + Measures: [[-21.81515756   0.28951633   0.51317936   0.31312466   0.55097598
    3.54215503]][0m
[37m[1m[2023-07-17 00:26:38,516][257371] Max Reward on eval: -21.8151575560942[0m
[37m[1m[2023-07-17 00:26:38,516][257371] Min Reward on eval: -21.8151575560942[0m
[37m[1m[2023-07-17 00:26:38,516][257371] Mean Reward across all agents: -21.8151575560942[0m
[37m[1m[2023-07-17 00:26:38,516][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:26:43,526][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:26:43,532][257371] Reward + Measures: [[117.9935665    0.42430001   0.89770001   0.80690002   0.85680002
    5.7672925 ]
 [311.32580471   0.0622       0.82819998   0.76420003   0.79619998
    5.43701649]
 [515.59757995   0.0588       0.8768       0.82579994   0.85669994
    6.57373953]
 ...
 [437.55615233   0.07210001   0.86879998   0.81030005   0.8502
    6.24009371]
 [108.99556339   0.18640001   0.65189999   0.3888       0.57700002
    3.54296756]
 [ 78.06775839   0.20829999   0.41429996   0.27520001   0.39270002
    5.06689167]][0m
[37m[1m[2023-07-17 00:26:43,532][257371] Max Reward on eval: 685.8410033749417[0m
[37m[1m[2023-07-17 00:26:43,532][257371] Min Reward on eval: -160.17718909517862[0m
[37m[1m[2023-07-17 00:26:43,533][257371] Mean Reward across all agents: 251.4479050279147[0m
[37m[1m[2023-07-17 00:26:43,533][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:26:43,540][257371] mean_value=217.84019370851317, max_value=845.2162782778491[0m
[37m[1m[2023-07-17 00:26:43,543][257371] New mean coefficients: [[ 1.2835691  -0.16288117 11.715712   -1.6491783  -2.2151778  -1.1613197 ]][0m
[37m[1m[2023-07-17 00:26:43,544][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:26:52,531][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 00:26:52,531][257371] FPS: 427366.79[0m
[36m[2023-07-17 00:26:52,533][257371] itr=325, itrs=2000, Progress: 16.25%[0m
[36m[2023-07-17 00:27:04,182][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 00:27:04,183][257371] FPS: 331488.24[0m
[36m[2023-07-17 00:27:08,492][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:27:08,493][257371] Reward + Measures: [[-11.5908705    0.27458      0.54428399   0.30712834   0.57433599
    3.46223831]][0m
[37m[1m[2023-07-17 00:27:08,493][257371] Max Reward on eval: -11.590870499616157[0m
[37m[1m[2023-07-17 00:27:08,493][257371] Min Reward on eval: -11.590870499616157[0m
[37m[1m[2023-07-17 00:27:08,494][257371] Mean Reward across all agents: -11.590870499616157[0m
[37m[1m[2023-07-17 00:27:08,494][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:27:13,522][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:27:13,523][257371] Reward + Measures: [[-20.32498912   0.21200001   0.28849998   0.233        0.25149998
    5.08639288]
 [  9.70973769   0.16610001   0.24489999   0.19290002   0.2096
    5.15410757]
 [ -2.66083261   0.1357       0.16540001   0.13380001   0.15030001
    4.69102716]
 ...
 [-20.73308541   0.27740002   0.33680001   0.29029998   0.3536
    3.80980039]
 [ 65.04831153   0.0999       0.0965       0.0911       0.0988
    6.0319705 ]
 [ 25.93342109   0.12590002   0.1504       0.13770001   0.1428
    5.46173048]][0m
[37m[1m[2023-07-17 00:27:13,523][257371] Max Reward on eval: 202.92434504386037[0m
[37m[1m[2023-07-17 00:27:13,524][257371] Min Reward on eval: -102.5463126141578[0m
[37m[1m[2023-07-17 00:27:13,524][257371] Mean Reward across all agents: 30.700229582673945[0m
[37m[1m[2023-07-17 00:27:13,524][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:27:13,526][257371] mean_value=-591.1023884699483, max_value=143.16222571388968[0m
[37m[1m[2023-07-17 00:27:13,528][257371] New mean coefficients: [[ 0.9122133  -0.01820804  8.850043   -1.8550295  -1.6307583  -0.9731575 ]][0m
[37m[1m[2023-07-17 00:27:13,529][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:27:22,559][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 00:27:22,559][257371] FPS: 425343.49[0m
[36m[2023-07-17 00:27:22,561][257371] itr=326, itrs=2000, Progress: 16.30%[0m
[36m[2023-07-17 00:27:34,150][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 00:27:34,150][257371] FPS: 333156.08[0m
[36m[2023-07-17 00:27:38,492][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:27:38,493][257371] Reward + Measures: [[0.65675472 0.25776133 0.571235   0.30019465 0.59771931 3.43097854]][0m
[37m[1m[2023-07-17 00:27:38,493][257371] Max Reward on eval: 0.6567547182481115[0m
[37m[1m[2023-07-17 00:27:38,493][257371] Min Reward on eval: 0.6567547182481115[0m
[37m[1m[2023-07-17 00:27:38,494][257371] Mean Reward across all agents: 0.6567547182481115[0m
[37m[1m[2023-07-17 00:27:38,494][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:27:43,666][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:27:43,667][257371] Reward + Measures: [[-43.48186256   0.45040002   0.58350003   0.50580001   0.58289999
    3.63411379]
 [-33.21911621   0.10120001   0.72860003   0.52670002   0.80150002
    4.5280509 ]
 [ 24.51024668   0.35549998   0.4172       0.36700001   0.37199998
    3.13050818]
 ...
 [-28.00173006   0.1231       0.69480002   0.45850006   0.77340001
    4.24297476]
 [ 34.06391172   0.32080004   0.34260002   0.2911       0.29629999
    3.33818555]
 [ 36.02534751   0.48279998   0.68090004   0.57339996   0.71079999
    3.79846048]][0m
[37m[1m[2023-07-17 00:27:43,667][257371] Max Reward on eval: 162.90895554032176[0m
[37m[1m[2023-07-17 00:27:43,668][257371] Min Reward on eval: -210.60750433981883[0m
[37m[1m[2023-07-17 00:27:43,668][257371] Mean Reward across all agents: 16.913999938942755[0m
[37m[1m[2023-07-17 00:27:43,668][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:27:43,674][257371] mean_value=-223.23849384245622, max_value=271.05227605905105[0m
[37m[1m[2023-07-17 00:27:43,677][257371] New mean coefficients: [[ 1.2963715  -0.12205212  8.850507   -2.440307   -0.6742668  -0.9245054 ]][0m
[37m[1m[2023-07-17 00:27:43,678][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:27:52,712][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 00:27:52,712][257371] FPS: 425158.51[0m
[36m[2023-07-17 00:27:52,714][257371] itr=327, itrs=2000, Progress: 16.35%[0m
[36m[2023-07-17 00:28:04,615][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-17 00:28:04,615][257371] FPS: 324407.73[0m
[36m[2023-07-17 00:28:08,959][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:28:08,959][257371] Reward + Measures: [[9.7145979  0.24013667 0.60199529 0.30196333 0.62456828 3.3906126 ]][0m
[37m[1m[2023-07-17 00:28:08,959][257371] Max Reward on eval: 9.714597902134823[0m
[37m[1m[2023-07-17 00:28:08,960][257371] Min Reward on eval: 9.714597902134823[0m
[37m[1m[2023-07-17 00:28:08,960][257371] Mean Reward across all agents: 9.714597902134823[0m
[37m[1m[2023-07-17 00:28:08,960][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:28:14,057][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:28:14,057][257371] Reward + Measures: [[  1.22818538   0.33460003   0.34670001   0.38069996   0.38769999
    4.66583538]
 [ 74.32611385   0.3098       0.3021       0.29710001   0.30410001
    5.56530237]
 [-50.38482363   0.24630001   0.2721       0.23000002   0.26330003
    6.84895086]
 ...
 [ 30.33447497   0.34110001   0.31510001   0.35209998   0.37380001
    5.07515335]
 [-60.62532333   0.2737       0.2597       0.35950002   0.35239998
    6.32423687]
 [  0.55806092   0.33070001   0.33920002   0.30739999   0.35330006
    6.71920633]][0m
[37m[1m[2023-07-17 00:28:14,057][257371] Max Reward on eval: 162.35813356041908[0m
[37m[1m[2023-07-17 00:28:14,058][257371] Min Reward on eval: -126.29825972486287[0m
[37m[1m[2023-07-17 00:28:14,058][257371] Mean Reward across all agents: 13.313481740673094[0m
[37m[1m[2023-07-17 00:28:14,058][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:28:14,061][257371] mean_value=-143.50697700985845, max_value=160.557156491917[0m
[37m[1m[2023-07-17 00:28:14,064][257371] New mean coefficients: [[ 2.2984009  -0.19702736  7.298401   -3.3614335  -0.58306277 -1.0361847 ]][0m
[37m[1m[2023-07-17 00:28:14,065][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:28:23,155][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 00:28:23,156][257371] FPS: 422483.98[0m
[36m[2023-07-17 00:28:23,158][257371] itr=328, itrs=2000, Progress: 16.40%[0m
[36m[2023-07-17 00:28:34,944][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 00:28:34,944][257371] FPS: 327588.53[0m
[36m[2023-07-17 00:28:39,314][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:28:39,314][257371] Reward + Measures: [[22.95583147  0.21946533  0.63909066  0.29908267  0.65870196  3.39406705]][0m
[37m[1m[2023-07-17 00:28:39,314][257371] Max Reward on eval: 22.95583147466646[0m
[37m[1m[2023-07-17 00:28:39,315][257371] Min Reward on eval: 22.95583147466646[0m
[37m[1m[2023-07-17 00:28:39,315][257371] Mean Reward across all agents: 22.95583147466646[0m
[37m[1m[2023-07-17 00:28:39,315][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:28:44,450][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:28:44,451][257371] Reward + Measures: [[ 38.16614982   0.21510001   0.64070004   0.25260001   0.69630003
    4.45441914]
 [-79.13475937   0.2431       0.57249999   0.32600003   0.6347
    5.02064371]
 [-13.69296201   0.17200001   0.54120004   0.35940003   0.63309997
    4.57298422]
 ...
 [-53.95272036   0.161        0.60620004   0.28070003   0.65329999
    4.8748827 ]
 [-21.38211608   0.41170001   0.42500001   0.602        0.70770001
    5.69431019]
 [ 14.49110685   0.2696       0.50840002   0.31279999   0.52290004
    4.38019657]][0m
[37m[1m[2023-07-17 00:28:44,451][257371] Max Reward on eval: 108.01440839543939[0m
[37m[1m[2023-07-17 00:28:44,451][257371] Min Reward on eval: -123.86686870770646[0m
[37m[1m[2023-07-17 00:28:44,452][257371] Mean Reward across all agents: -19.128112269486937[0m
[37m[1m[2023-07-17 00:28:44,452][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:28:44,458][257371] mean_value=-68.753620597877, max_value=368.88235272906167[0m
[37m[1m[2023-07-17 00:28:44,461][257371] New mean coefficients: [[ 2.5673316  -0.25482363  8.017259   -3.4269545   0.48659468 -1.4551568 ]][0m
[37m[1m[2023-07-17 00:28:44,462][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:28:53,619][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-17 00:28:53,619][257371] FPS: 419427.15[0m
[36m[2023-07-17 00:28:53,621][257371] itr=329, itrs=2000, Progress: 16.45%[0m
[36m[2023-07-17 00:29:05,391][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 00:29:05,391][257371] FPS: 328021.72[0m
[36m[2023-07-17 00:29:09,774][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:29:09,775][257371] Reward + Measures: [[29.70584688  0.20384666  0.66249067  0.29881534  0.67961931  3.36285543]][0m
[37m[1m[2023-07-17 00:29:09,775][257371] Max Reward on eval: 29.70584687918081[0m
[37m[1m[2023-07-17 00:29:09,775][257371] Min Reward on eval: 29.70584687918081[0m
[37m[1m[2023-07-17 00:29:09,776][257371] Mean Reward across all agents: 29.70584687918081[0m
[37m[1m[2023-07-17 00:29:09,776][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:29:14,767][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:29:14,768][257371] Reward + Measures: [[10.38160882  0.29089999  0.6024      0.33930001  0.59569997  3.2744081 ]
 [59.75048803  0.24130002  0.56380004  0.31070003  0.53180003  3.40134883]
 [18.44306956  0.26530001  0.60549998  0.30720001  0.59630007  3.35204697]
 ...
 [12.63794041  0.27420002  0.52950001  0.29569998  0.5086      3.22606087]
 [28.60927079  0.27599999  0.73169994  0.36149999  0.70770007  3.34271622]
 [67.78307084  0.26460001  0.56870002  0.32410002  0.54530001  3.19649363]][0m
[37m[1m[2023-07-17 00:29:14,768][257371] Max Reward on eval: 78.90991996229859[0m
[37m[1m[2023-07-17 00:29:14,768][257371] Min Reward on eval: -56.63852056071628[0m
[37m[1m[2023-07-17 00:29:14,768][257371] Mean Reward across all agents: 18.10564551692686[0m
[37m[1m[2023-07-17 00:29:14,769][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:29:14,772][257371] mean_value=-124.02270493983531, max_value=79.00846473306761[0m
[37m[1m[2023-07-17 00:29:14,774][257371] New mean coefficients: [[ 1.8024784 -0.6117676  8.84092   -1.9746041  1.3415139 -1.3308706]][0m
[37m[1m[2023-07-17 00:29:14,780][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:29:23,860][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 00:29:23,860][257371] FPS: 423001.42[0m
[36m[2023-07-17 00:29:23,862][257371] itr=330, itrs=2000, Progress: 16.50%[0m
[37m[1m[2023-07-17 00:31:57,673][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000310[0m
[36m[2023-07-17 00:32:10,114][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 00:32:10,114][257371] FPS: 324928.91[0m
[36m[2023-07-17 00:32:13,998][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:32:13,998][257371] Reward + Measures: [[38.43772886  0.17599133  0.710751    0.30068934  0.72785628  3.37501884]][0m
[37m[1m[2023-07-17 00:32:13,998][257371] Max Reward on eval: 38.43772886100934[0m
[37m[1m[2023-07-17 00:32:13,999][257371] Min Reward on eval: 38.43772886100934[0m
[37m[1m[2023-07-17 00:32:13,999][257371] Mean Reward across all agents: 38.43772886100934[0m
[37m[1m[2023-07-17 00:32:13,999][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:32:18,906][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:32:18,907][257371] Reward + Measures: [[125.57691668   0.58510005   0.29679999   0.59429997   0.25120002
    3.98901629]
 [ 71.80210826   0.47880003   0.2782       0.49980003   0.22379999
    4.48636055]
 [ 60.69067929   0.33860001   0.2471       0.32870004   0.2105
    5.4969449 ]
 ...
 [ 97.12230048   0.45319995   0.31300002   0.48449999   0.21820001
    4.66281509]
 [ 67.55418089   0.6243       0.37860003   0.60390002   0.35780001
    4.21886778]
 [111.0377148    0.51130003   0.31470001   0.50640005   0.26070002
    4.6332283 ]][0m
[37m[1m[2023-07-17 00:32:18,907][257371] Max Reward on eval: 166.14195629397872[0m
[37m[1m[2023-07-17 00:32:18,908][257371] Min Reward on eval: -17.611571006756275[0m
[37m[1m[2023-07-17 00:32:18,908][257371] Mean Reward across all agents: 74.29202353286257[0m
[37m[1m[2023-07-17 00:32:18,908][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:32:18,917][257371] mean_value=48.3304677086785, max_value=409.5009645551562[0m
[37m[1m[2023-07-17 00:32:18,920][257371] New mean coefficients: [[ 2.6628015 -0.6638224  6.1851053 -2.8965626  1.6085712 -1.1237752]][0m
[37m[1m[2023-07-17 00:32:18,921][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:32:27,863][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 00:32:27,863][257371] FPS: 429528.35[0m
[36m[2023-07-17 00:32:27,866][257371] itr=331, itrs=2000, Progress: 16.55%[0m
[36m[2023-07-17 00:32:39,518][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 00:32:39,519][257371] FPS: 331445.45[0m
[36m[2023-07-17 00:32:43,783][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:32:43,783][257371] Reward + Measures: [[40.87495283  0.14690301  0.76080239  0.29848233  0.77738434  3.41739917]][0m
[37m[1m[2023-07-17 00:32:43,783][257371] Max Reward on eval: 40.87495282834254[0m
[37m[1m[2023-07-17 00:32:43,784][257371] Min Reward on eval: 40.87495282834254[0m
[37m[1m[2023-07-17 00:32:43,784][257371] Mean Reward across all agents: 40.87495282834254[0m
[37m[1m[2023-07-17 00:32:43,784][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:32:48,750][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:32:48,755][257371] Reward + Measures: [[-92.08573909   0.19160001   0.77739996   0.29879999   0.70950001
    3.16339803]
 [-22.15991973   0.10710001   0.85400003   0.39249998   0.86180001
    4.35045385]
 [-77.3007136    0.12029999   0.85699999   0.34460002   0.86370003
    4.26584625]
 ...
 [ 48.71277246   0.1286       0.82719994   0.32070002   0.82100004
    3.57313538]
 [  8.49490342   0.0737       0.917        0.29309997   0.91930002
    3.7954576 ]
 [-53.09020736   0.1116       0.83759993   0.45380002   0.84089994
    4.73559332]][0m
[37m[1m[2023-07-17 00:32:48,756][257371] Max Reward on eval: 175.25153445284815[0m
[37m[1m[2023-07-17 00:32:48,756][257371] Min Reward on eval: -345.11655430076644[0m
[37m[1m[2023-07-17 00:32:48,756][257371] Mean Reward across all agents: -24.029939215907696[0m
[37m[1m[2023-07-17 00:32:48,756][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:32:48,765][257371] mean_value=-60.03263111862045, max_value=558.0487679502583[0m
[37m[1m[2023-07-17 00:32:48,767][257371] New mean coefficients: [[ 1.5143056 -0.003847   4.513387  -2.5153227  2.2331395 -0.8609848]][0m
[37m[1m[2023-07-17 00:32:48,768][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:32:57,925][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-17 00:32:57,926][257371] FPS: 419425.10[0m
[36m[2023-07-17 00:32:57,928][257371] itr=332, itrs=2000, Progress: 16.60%[0m
[36m[2023-07-17 00:33:09,675][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 00:33:09,676][257371] FPS: 328719.58[0m
[36m[2023-07-17 00:33:14,009][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:33:14,009][257371] Reward + Measures: [[36.24892846  0.11652867  0.81030273  0.28676599  0.82616192  3.51042223]][0m
[37m[1m[2023-07-17 00:33:14,010][257371] Max Reward on eval: 36.248928455555365[0m
[37m[1m[2023-07-17 00:33:14,010][257371] Min Reward on eval: 36.248928455555365[0m
[37m[1m[2023-07-17 00:33:14,010][257371] Mean Reward across all agents: 36.248928455555365[0m
[37m[1m[2023-07-17 00:33:14,010][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:33:18,991][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:33:18,997][257371] Reward + Measures: [[ 11.43001742   0.20970002   0.458        0.59039992   0.56770003
    7.1222167 ]
 [ 79.15709968   0.27059999   0.37610003   0.27959999   0.3001
    3.22713447]
 [122.44263547   0.30630001   0.36770001   0.30449998   0.3452
    3.12099624]
 ...
 [137.77908471   0.2744       0.62700003   0.84689999   0.83210003
    7.37230444]
 [ -3.13523073   0.11869999   0.58950007   0.56890005   0.57230002
    5.71391296]
 [-12.61375141   0.32729998   0.4501       0.70700002   0.67449999
    7.04255152]][0m
[37m[1m[2023-07-17 00:33:18,997][257371] Max Reward on eval: 295.1875572282821[0m
[37m[1m[2023-07-17 00:33:18,998][257371] Min Reward on eval: -154.7442608645186[0m
[37m[1m[2023-07-17 00:33:18,998][257371] Mean Reward across all agents: 38.79804633702564[0m
[37m[1m[2023-07-17 00:33:18,998][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:33:19,006][257371] mean_value=-311.356486057838, max_value=569.3426007062196[0m
[37m[1m[2023-07-17 00:33:19,008][257371] New mean coefficients: [[ 1.6446561  -0.10897398  7.225333   -2.208795    1.9435642  -1.1487743 ]][0m
[37m[1m[2023-07-17 00:33:19,009][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:33:28,072][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 00:33:28,072][257371] FPS: 423786.07[0m
[36m[2023-07-17 00:33:28,074][257371] itr=333, itrs=2000, Progress: 16.65%[0m
[36m[2023-07-17 00:33:39,726][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 00:33:39,726][257371] FPS: 331485.20[0m
[36m[2023-07-17 00:33:44,078][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:33:44,079][257371] Reward + Measures: [[31.25320136  0.09734701  0.85312563  0.27743566  0.86907363  3.55268836]][0m
[37m[1m[2023-07-17 00:33:44,079][257371] Max Reward on eval: 31.25320135961443[0m
[37m[1m[2023-07-17 00:33:44,079][257371] Min Reward on eval: 31.25320135961443[0m
[37m[1m[2023-07-17 00:33:44,080][257371] Mean Reward across all agents: 31.25320135961443[0m
[37m[1m[2023-07-17 00:33:44,080][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:33:49,073][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:33:49,074][257371] Reward + Measures: [[ -90.99825902    0.33450001    0.40170002    0.3019        0.43249997
     4.03167677]
 [-123.2398303     0.3134        0.46150002    0.2845        0.47550002
     3.36726427]
 [  55.66393504    0.51780003    0.62480003    0.51560003    0.65950006
     3.62862515]
 ...
 [  67.75323792    0.46949998    0.25799999    0.37050003    0.5061
     3.92952466]
 [  32.88739313    0.2138        0.34709999    0.2191        0.3813
     2.86231923]
 [ -72.13405225    0.0549        0.8828001     0.79840004    0.89559996
     4.86817265]][0m
[37m[1m[2023-07-17 00:33:49,074][257371] Max Reward on eval: 149.83938123732804[0m
[37m[1m[2023-07-17 00:33:49,074][257371] Min Reward on eval: -160.22084625326096[0m
[37m[1m[2023-07-17 00:33:49,074][257371] Mean Reward across all agents: 8.198190316277353[0m
[37m[1m[2023-07-17 00:33:49,075][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:33:49,079][257371] mean_value=-357.80110516103224, max_value=547.1702456104861[0m
[37m[1m[2023-07-17 00:33:49,081][257371] New mean coefficients: [[ 1.0434037  -0.08611175  5.379394   -2.2799816   3.4438534  -1.1999469 ]][0m
[37m[1m[2023-07-17 00:33:49,082][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:33:58,179][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 00:33:58,179][257371] FPS: 422199.30[0m
[36m[2023-07-17 00:33:58,182][257371] itr=334, itrs=2000, Progress: 16.70%[0m
[36m[2023-07-17 00:34:10,028][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 00:34:10,029][257371] FPS: 326007.69[0m
[36m[2023-07-17 00:34:14,451][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:34:14,452][257371] Reward + Measures: [[34.26603664  0.08564933  0.87144399  0.28058898  0.88692832  3.57175183]][0m
[37m[1m[2023-07-17 00:34:14,452][257371] Max Reward on eval: 34.26603663597273[0m
[37m[1m[2023-07-17 00:34:14,452][257371] Min Reward on eval: 34.26603663597273[0m
[37m[1m[2023-07-17 00:34:14,453][257371] Mean Reward across all agents: 34.26603663597273[0m
[37m[1m[2023-07-17 00:34:14,453][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:34:19,489][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:34:19,489][257371] Reward + Measures: [[ 15.07331091   0.16550002   0.70450002   0.30469999   0.7608
    2.76804876]
 [ -5.76126478   0.09500001   0.80159998   0.4276       0.83059996
    3.73852611]
 [ 14.07983825   0.16220002   0.70699996   0.35960001   0.73140001
    3.1279552 ]
 ...
 [-39.1169794    0.23450001   0.6286       0.32980001   0.68739998
    2.94755864]
 [ 41.05061699   0.2509       0.65630001   0.2929       0.66400003
    3.3914392 ]
 [ 18.34466022   0.1362       0.77130002   0.31080002   0.82660002
    3.06548047]][0m
[37m[1m[2023-07-17 00:34:19,490][257371] Max Reward on eval: 106.62263462971896[0m
[37m[1m[2023-07-17 00:34:19,490][257371] Min Reward on eval: -73.22879015970975[0m
[37m[1m[2023-07-17 00:34:19,490][257371] Mean Reward across all agents: 12.732535875899494[0m
[37m[1m[2023-07-17 00:34:19,490][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:34:19,494][257371] mean_value=-86.59347367391898, max_value=169.39890012759176[0m
[37m[1m[2023-07-17 00:34:19,496][257371] New mean coefficients: [[ 1.6462241  -0.57468504  3.1261818  -3.0178003   2.515037   -1.0303731 ]][0m
[37m[1m[2023-07-17 00:34:19,497][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:34:28,597][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 00:34:28,598][257371] FPS: 422060.89[0m
[36m[2023-07-17 00:34:28,600][257371] itr=335, itrs=2000, Progress: 16.75%[0m
[36m[2023-07-17 00:34:40,363][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 00:34:40,364][257371] FPS: 328276.17[0m
[36m[2023-07-17 00:34:44,725][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:34:44,726][257371] Reward + Measures: [[38.44261104  0.08071867  0.882007    0.27662766  0.89757764  3.53948665]][0m
[37m[1m[2023-07-17 00:34:44,726][257371] Max Reward on eval: 38.442611038749476[0m
[37m[1m[2023-07-17 00:34:44,726][257371] Min Reward on eval: 38.442611038749476[0m
[37m[1m[2023-07-17 00:34:44,726][257371] Mean Reward across all agents: 38.442611038749476[0m
[37m[1m[2023-07-17 00:34:44,727][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:34:49,994][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:34:49,995][257371] Reward + Measures: [[  0.52408302   0.30160001   0.47230002   0.27110001   0.40629998
    3.21080565]
 [ 12.76643501   0.30570003   0.51630002   0.25960001   0.42969999
    3.64649439]
 [-64.21053498   0.2203       0.30939999   0.24710003   0.38639998
    4.01244879]
 ...
 [-36.09030614   0.2199       0.61220002   0.2411       0.63130003
    3.31534171]
 [-28.59074462   0.11570001   0.89290011   0.19789998   0.8901
    3.83801889]
 [ 14.0619634    0.3008       0.25100002   0.27910003   0.26030001
    3.60134673]][0m
[37m[1m[2023-07-17 00:34:49,995][257371] Max Reward on eval: 147.21125506907703[0m
[37m[1m[2023-07-17 00:34:49,995][257371] Min Reward on eval: -237.6269131044857[0m
[37m[1m[2023-07-17 00:34:49,995][257371] Mean Reward across all agents: -7.527565011558821[0m
[37m[1m[2023-07-17 00:34:49,995][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:34:49,999][257371] mean_value=-432.44901501046957, max_value=438.9776826060936[0m
[37m[1m[2023-07-17 00:34:50,002][257371] New mean coefficients: [[ 2.1136184 -0.7722632  1.7132992 -2.8199573  1.5896525 -0.6081823]][0m
[37m[1m[2023-07-17 00:34:50,003][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:34:59,247][257371] train() took 9.24 seconds to complete[0m
[36m[2023-07-17 00:34:59,248][257371] FPS: 415456.90[0m
[36m[2023-07-17 00:34:59,250][257371] itr=336, itrs=2000, Progress: 16.80%[0m
[36m[2023-07-17 00:35:10,982][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 00:35:10,983][257371] FPS: 329211.16[0m
[36m[2023-07-17 00:35:15,393][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:35:15,394][257371] Reward + Measures: [[41.17923151  0.07213733  0.90008903  0.26684332  0.91462898  3.52243686]][0m
[37m[1m[2023-07-17 00:35:15,394][257371] Max Reward on eval: 41.17923150503902[0m
[37m[1m[2023-07-17 00:35:15,394][257371] Min Reward on eval: 41.17923150503902[0m
[37m[1m[2023-07-17 00:35:15,395][257371] Mean Reward across all agents: 41.17923150503902[0m
[37m[1m[2023-07-17 00:35:15,395][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:35:20,489][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:35:20,490][257371] Reward + Measures: [[100.42757031   0.32510003   0.46779999   0.40049997   0.42630002
    2.84718156]
 [ 72.15245976   0.26550001   0.59079999   0.3362       0.46849999
    3.03580236]
 [ 52.37498563   0.67860001   0.1231       0.69320005   0.63490003
    4.80902624]
 ...
 [ 12.41017575   0.3721       0.30939999   0.41159996   0.37459999
    4.23848391]
 [ 53.99644419   0.25470001   0.45199999   0.2951       0.36690003
    2.82514405]
 [ 22.38187037   0.17950001   0.80580008   0.56619996   0.85430002
    4.07754135]][0m
[37m[1m[2023-07-17 00:35:20,490][257371] Max Reward on eval: 200.74095461064718[0m
[37m[1m[2023-07-17 00:35:20,490][257371] Min Reward on eval: -39.138681900501254[0m
[37m[1m[2023-07-17 00:35:20,490][257371] Mean Reward across all agents: 45.4472263254201[0m
[37m[1m[2023-07-17 00:35:20,491][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:35:20,499][257371] mean_value=-151.4523986270688, max_value=502.05809700957496[0m
[37m[1m[2023-07-17 00:35:20,502][257371] New mean coefficients: [[ 1.9383632  -0.41333035  1.7588875  -2.6882603  -0.1642797  -0.16057432]][0m
[37m[1m[2023-07-17 00:35:20,503][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:35:29,600][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 00:35:29,600][257371] FPS: 422205.92[0m
[36m[2023-07-17 00:35:29,602][257371] itr=337, itrs=2000, Progress: 16.85%[0m
[36m[2023-07-17 00:35:41,471][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 00:35:41,471][257371] FPS: 325315.82[0m
[36m[2023-07-17 00:35:45,878][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:35:45,879][257371] Reward + Measures: [[44.99161193  0.06768467  0.90828162  0.25981265  0.92094833  3.51355672]][0m
[37m[1m[2023-07-17 00:35:45,879][257371] Max Reward on eval: 44.991611925143616[0m
[37m[1m[2023-07-17 00:35:45,879][257371] Min Reward on eval: 44.991611925143616[0m
[37m[1m[2023-07-17 00:35:45,879][257371] Mean Reward across all agents: 44.991611925143616[0m
[37m[1m[2023-07-17 00:35:45,880][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:35:50,948][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:35:50,948][257371] Reward + Measures: [[ 68.59402933   0.23029999   0.38030002   0.345        0.25960001
    3.73571014]
 [280.81764123   0.2181       0.33110002   0.29010001   0.19029999
    4.00390005]
 [ 11.19504534   0.0508       0.83359998   0.42810002   0.85459995
    3.57058787]
 ...
 [  2.84893164   0.21590002   0.65650004   0.266        0.65589994
    3.82379222]
 [111.37318384   0.2888       0.61910003   0.45690003   0.42640001
    2.8962152 ]
 [203.99070081   0.23710001   0.3389       0.31130001   0.19530001
    4.20744801]][0m
[37m[1m[2023-07-17 00:35:50,949][257371] Max Reward on eval: 357.3715603448451[0m
[37m[1m[2023-07-17 00:35:50,949][257371] Min Reward on eval: -415.1932062558364[0m
[37m[1m[2023-07-17 00:35:50,949][257371] Mean Reward across all agents: 65.95462839558891[0m
[37m[1m[2023-07-17 00:35:50,949][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:35:50,956][257371] mean_value=-529.5900509120243, max_value=393.24018676678355[0m
[37m[1m[2023-07-17 00:35:50,959][257371] New mean coefficients: [[ 1.5600193   0.2028068   0.95075834 -2.3104818  -0.2672909   0.08862796]][0m
[37m[1m[2023-07-17 00:35:50,960][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:36:00,049][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 00:36:00,049][257371] FPS: 422601.35[0m
[36m[2023-07-17 00:36:00,051][257371] itr=338, itrs=2000, Progress: 16.90%[0m
[36m[2023-07-17 00:36:11,756][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 00:36:11,756][257371] FPS: 329882.32[0m
[36m[2023-07-17 00:36:16,083][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:36:16,084][257371] Reward + Measures: [[60.21424243  0.06964766  0.90266633  0.25641364  0.91243601  3.46440625]][0m
[37m[1m[2023-07-17 00:36:16,084][257371] Max Reward on eval: 60.2142424251074[0m
[37m[1m[2023-07-17 00:36:16,085][257371] Min Reward on eval: 60.2142424251074[0m
[37m[1m[2023-07-17 00:36:16,085][257371] Mean Reward across all agents: 60.2142424251074[0m
[37m[1m[2023-07-17 00:36:16,085][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:36:21,120][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:36:21,120][257371] Reward + Measures: [[-13.94702496   0.21780001   0.493        0.52539998   0.60250008
    5.92940903]
 [ 24.14331227   0.21069999   0.5327       0.43950006   0.4993
    6.61624146]
 [-56.78231944   0.625        0.69100004   0.75370002   0.72500002
    4.48943758]
 ...
 [-29.81643223   0.24259999   0.81710005   0.76710004   0.83059996
    4.78261518]
 [-17.39478027   0.26230001   0.6462       0.58410001   0.64300007
    4.04667044]
 [ 75.20277928   0.39219999   0.40920001   0.47160003   0.32389998
    3.1600492 ]][0m
[37m[1m[2023-07-17 00:36:21,120][257371] Max Reward on eval: 279.98578932359817[0m
[37m[1m[2023-07-17 00:36:21,121][257371] Min Reward on eval: -416.5635500444914[0m
[37m[1m[2023-07-17 00:36:21,121][257371] Mean Reward across all agents: -4.0439136763498915[0m
[37m[1m[2023-07-17 00:36:21,121][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:36:21,128][257371] mean_value=-345.0961185759934, max_value=440.294029716102[0m
[37m[1m[2023-07-17 00:36:21,131][257371] New mean coefficients: [[ 1.1692668   0.12298098  0.4774806  -2.2505395  -0.7247887   0.30137053]][0m
[37m[1m[2023-07-17 00:36:21,132][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:36:30,143][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 00:36:30,149][257371] FPS: 426215.51[0m
[36m[2023-07-17 00:36:30,151][257371] itr=339, itrs=2000, Progress: 16.95%[0m
[36m[2023-07-17 00:36:41,929][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 00:36:41,929][257371] FPS: 327797.26[0m
[36m[2023-07-17 00:36:46,152][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:36:46,153][257371] Reward + Measures: [[66.91134388  0.07994066  0.89624095  0.24256568  0.90154868  3.44202352]][0m
[37m[1m[2023-07-17 00:36:46,153][257371] Max Reward on eval: 66.91134388426056[0m
[37m[1m[2023-07-17 00:36:46,153][257371] Min Reward on eval: 66.91134388426056[0m
[37m[1m[2023-07-17 00:36:46,153][257371] Mean Reward across all agents: 66.91134388426056[0m
[37m[1m[2023-07-17 00:36:46,154][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:36:51,167][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:36:51,167][257371] Reward + Measures: [[-26.81099866   0.1477       0.73340005   0.26930001   0.70170003
    3.02124667]
 [-34.09084376   0.26180002   0.61380005   0.32140002   0.6401
    3.34117246]
 [ -1.29508518   0.38770005   0.51910001   0.41810003   0.53470004
    3.99120188]
 ...
 [ 37.68801597   0.205        0.64359999   0.67220002   0.76700002
    5.18063211]
 [ 76.51570251   0.26229998   0.69449997   0.84020007   0.86020005
    5.83039474]
 [-10.56679707   0.059        0.75850004   0.67749995   0.80019999
    4.64292479]][0m
[37m[1m[2023-07-17 00:36:51,168][257371] Max Reward on eval: 456.81812144014987[0m
[37m[1m[2023-07-17 00:36:51,168][257371] Min Reward on eval: -451.0486002007034[0m
[37m[1m[2023-07-17 00:36:51,168][257371] Mean Reward across all agents: 0.2544843857989861[0m
[37m[1m[2023-07-17 00:36:51,168][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:36:51,177][257371] mean_value=-140.64712209632705, max_value=607.5344939741105[0m
[37m[1m[2023-07-17 00:36:51,180][257371] New mean coefficients: [[ 1.2886115  -0.34365806  3.2557337  -1.7288938   0.00710374 -0.154199  ]][0m
[37m[1m[2023-07-17 00:36:51,181][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:37:00,188][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 00:37:00,188][257371] FPS: 426445.99[0m
[36m[2023-07-17 00:37:00,190][257371] itr=340, itrs=2000, Progress: 17.00%[0m
[37m[1m[2023-07-17 00:39:38,775][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000320[0m
[36m[2023-07-17 00:39:51,207][257371] train() took 11.89 seconds to complete[0m
[36m[2023-07-17 00:39:51,207][257371] FPS: 322958.45[0m
[36m[2023-07-17 00:39:55,363][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:39:55,363][257371] Reward + Measures: [[60.43133064  0.0678      0.91320866  0.24189532  0.91819561  3.51570153]][0m
[37m[1m[2023-07-17 00:39:55,363][257371] Max Reward on eval: 60.43133063587865[0m
[37m[1m[2023-07-17 00:39:55,364][257371] Min Reward on eval: 60.43133063587865[0m
[37m[1m[2023-07-17 00:39:55,364][257371] Mean Reward across all agents: 60.43133063587865[0m
[37m[1m[2023-07-17 00:39:55,364][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:40:00,335][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:40:00,335][257371] Reward + Measures: [[  13.59377651    0.43070003    0.30309999    0.5158        0.52500004
     3.6353631 ]
 [-161.32331954    0.46949998    0.48370001    0.17440002    0.47849998
     4.94371796]
 [  -7.44938771    0.169         0.52290004    0.43810001    0.51569998
     4.07071161]
 ...
 [ -31.97464037    0.0703        0.90690005    0.3574        0.92819995
     3.96804881]
 [  13.18411774    0.3184        0.34300002    0.29660001    0.44370005
     3.13748813]
 [  44.04221535    0.10450001    0.85690004    0.20999999    0.87390006
     4.44588423]][0m
[37m[1m[2023-07-17 00:40:00,336][257371] Max Reward on eval: 100.82614857736044[0m
[37m[1m[2023-07-17 00:40:00,336][257371] Min Reward on eval: -161.32331953570247[0m
[37m[1m[2023-07-17 00:40:00,336][257371] Mean Reward across all agents: 0.5128008682619166[0m
[37m[1m[2023-07-17 00:40:00,336][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:40:00,340][257371] mean_value=-506.2991325234164, max_value=401.30956927968674[0m
[37m[1m[2023-07-17 00:40:00,343][257371] New mean coefficients: [[ 1.4430552  -0.25046605  1.5494823  -2.0368385   0.3643639   0.13900909]][0m
[37m[1m[2023-07-17 00:40:00,343][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:40:09,455][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 00:40:09,455][257371] FPS: 421509.12[0m
[36m[2023-07-17 00:40:09,458][257371] itr=341, itrs=2000, Progress: 17.05%[0m
[36m[2023-07-17 00:40:21,311][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 00:40:21,311][257371] FPS: 325845.59[0m
[36m[2023-07-17 00:40:25,579][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:40:25,579][257371] Reward + Measures: [[56.3499853   0.06911567  0.91823834  0.23156834  0.92375332  3.55656409]][0m
[37m[1m[2023-07-17 00:40:25,580][257371] Max Reward on eval: 56.34998530068564[0m
[37m[1m[2023-07-17 00:40:25,580][257371] Min Reward on eval: 56.34998530068564[0m
[37m[1m[2023-07-17 00:40:25,580][257371] Mean Reward across all agents: 56.34998530068564[0m
[37m[1m[2023-07-17 00:40:25,580][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:40:30,616][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:40:30,616][257371] Reward + Measures: [[  87.50490061    0.29580003    0.49209997    0.29340002    0.41420004
     3.85599589]
 [-239.68050263    0.47119999    0.45569998    0.7331        0.83280003
     5.7097373 ]
 [-285.87597655    0.79700005    0.14670001    0.77890009    0.6189
     6.20876408]
 ...
 [ -56.10018283    0.26440001    0.2552        0.26299998    0.28010002
     4.64208841]
 [  35.28031562    0.5402        0.44640002    0.60260004    0.43800002
     2.98410106]
 [-202.04727143    0.75169998    0.2132        0.74240005    0.54459995
     5.11934137]][0m
[37m[1m[2023-07-17 00:40:30,616][257371] Max Reward on eval: 223.14254658641295[0m
[37m[1m[2023-07-17 00:40:30,617][257371] Min Reward on eval: -672.902292236127[0m
[37m[1m[2023-07-17 00:40:30,617][257371] Mean Reward across all agents: -84.254688806249[0m
[37m[1m[2023-07-17 00:40:30,617][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:40:30,623][257371] mean_value=-173.86298163220872, max_value=428.61115259100336[0m
[37m[1m[2023-07-17 00:40:30,626][257371] New mean coefficients: [[ 1.1560462  -0.24437213  0.48366117 -2.2165158   0.7564407  -0.01441118]][0m
[37m[1m[2023-07-17 00:40:30,627][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:40:39,679][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 00:40:39,680][257371] FPS: 424290.93[0m
[36m[2023-07-17 00:40:39,682][257371] itr=342, itrs=2000, Progress: 17.10%[0m
[36m[2023-07-17 00:40:51,553][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 00:40:51,553][257371] FPS: 325241.21[0m
[36m[2023-07-17 00:40:55,879][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:40:55,879][257371] Reward + Measures: [[60.35454231  0.07730167  0.91426796  0.21998933  0.91820037  3.53183651]][0m
[37m[1m[2023-07-17 00:40:55,880][257371] Max Reward on eval: 60.35454230841477[0m
[37m[1m[2023-07-17 00:40:55,880][257371] Min Reward on eval: 60.35454230841477[0m
[37m[1m[2023-07-17 00:40:55,880][257371] Mean Reward across all agents: 60.35454230841477[0m
[37m[1m[2023-07-17 00:40:55,880][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:41:00,926][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:41:00,926][257371] Reward + Measures: [[537.86038586   0.86849993   0.52520007   0.89120001   0.14290002
    5.65966797]
 [ 77.06225895   0.18430001   0.65880007   0.24500003   0.65829998
    3.1884129 ]
 [ 60.89604474   0.0735       0.85410005   0.47740003   0.83099997
    3.74696207]
 ...
 [ 69.11594416   0.2165       0.20100001   0.18380001   0.21949999
    4.44521856]
 [ 77.14206112   0.44010001   0.52450001   0.54590005   0.53979999
    2.75111175]
 [494.54052561   0.79549998   0.51770002   0.80079997   0.15720001
    5.66162586]][0m
[37m[1m[2023-07-17 00:41:00,926][257371] Max Reward on eval: 660.0924377086573[0m
[37m[1m[2023-07-17 00:41:00,927][257371] Min Reward on eval: -269.9025655452162[0m
[37m[1m[2023-07-17 00:41:00,927][257371] Mean Reward across all agents: 172.6343730192757[0m
[37m[1m[2023-07-17 00:41:00,927][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:41:00,940][257371] mean_value=179.62539489412777, max_value=1019.7928658645019[0m
[37m[1m[2023-07-17 00:41:00,942][257371] New mean coefficients: [[ 2.9420938  -0.31776693  0.7293346  -2.8164208   0.32095957 -0.3852375 ]][0m
[37m[1m[2023-07-17 00:41:00,943][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:41:10,077][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 00:41:10,077][257371] FPS: 420507.40[0m
[36m[2023-07-17 00:41:10,080][257371] itr=343, itrs=2000, Progress: 17.15%[0m
[36m[2023-07-17 00:41:22,058][257371] train() took 11.91 seconds to complete[0m
[36m[2023-07-17 00:41:22,058][257371] FPS: 322429.05[0m
[36m[2023-07-17 00:41:26,458][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:41:26,458][257371] Reward + Measures: [[78.57608169  0.08312     0.90889466  0.21676765  0.90809602  3.44016719]][0m
[37m[1m[2023-07-17 00:41:26,458][257371] Max Reward on eval: 78.5760816851644[0m
[37m[1m[2023-07-17 00:41:26,458][257371] Min Reward on eval: 78.5760816851644[0m
[37m[1m[2023-07-17 00:41:26,459][257371] Mean Reward across all agents: 78.5760816851644[0m
[37m[1m[2023-07-17 00:41:26,459][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:41:31,523][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:41:31,524][257371] Reward + Measures: [[103.95947836   0.38620001   0.3414       0.4276       0.46379995
    4.93791819]
 [115.04721165   0.16199999   0.78570002   0.25289997   0.7816
    3.22095609]
 [ -5.52155423   0.1365       0.77329999   0.3908       0.86619997
    4.20426512]
 ...
 [ 70.07117083   0.2115       0.5808       0.49740002   0.57499999
    4.96760988]
 [ 34.89033533   0.27860001   0.42230001   0.39760002   0.40120003
    5.59024286]
 [159.62700559   0.23840001   0.66240001   0.34920001   0.6232
    3.04602599]][0m
[37m[1m[2023-07-17 00:41:31,524][257371] Max Reward on eval: 340.12705324646083[0m
[37m[1m[2023-07-17 00:41:31,525][257371] Min Reward on eval: -78.70803513862192[0m
[37m[1m[2023-07-17 00:41:31,525][257371] Mean Reward across all agents: 91.8555513880528[0m
[37m[1m[2023-07-17 00:41:31,525][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:41:31,533][257371] mean_value=-116.89116479598849, max_value=552.8637332153506[0m
[37m[1m[2023-07-17 00:41:31,535][257371] New mean coefficients: [[ 2.29959    -0.2655254   1.8741276  -1.9306662   0.7419287   0.00056455]][0m
[37m[1m[2023-07-17 00:41:31,536][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:41:40,650][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 00:41:40,651][257371] FPS: 421399.04[0m
[36m[2023-07-17 00:41:40,653][257371] itr=344, itrs=2000, Progress: 17.20%[0m
[36m[2023-07-17 00:41:52,327][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 00:41:52,327][257371] FPS: 330800.52[0m
[36m[2023-07-17 00:41:56,579][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:41:56,580][257371] Reward + Measures: [[93.43392881  0.08310267  0.9075734   0.21871932  0.90359062  3.42154217]][0m
[37m[1m[2023-07-17 00:41:56,580][257371] Max Reward on eval: 93.43392881412271[0m
[37m[1m[2023-07-17 00:41:56,580][257371] Min Reward on eval: 93.43392881412271[0m
[37m[1m[2023-07-17 00:41:56,581][257371] Mean Reward across all agents: 93.43392881412271[0m
[37m[1m[2023-07-17 00:41:56,581][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:42:01,728][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:42:01,728][257371] Reward + Measures: [[-206.33984663    0.27719998    0.37830001    0.28779998    0.42209998
     4.96813679]
 [ -77.64987806    0.2112        0.46229997    0.3409        0.48410007
     3.85394287]
 [-134.1579301     0.27280003    0.38850001    0.2906        0.435
     5.20310593]
 ...
 [ -20.72273038    0.177         0.69730002    0.22850001    0.74150002
     3.90376449]
 [ -51.12414559    0.28049999    0.45010003    0.27649999    0.51109999
     5.29517651]
 [ -57.62691879    0.2766        0.36049995    0.32230002    0.37100002
     4.95558691]][0m
[37m[1m[2023-07-17 00:42:01,729][257371] Max Reward on eval: 217.53908873917536[0m
[37m[1m[2023-07-17 00:42:01,729][257371] Min Reward on eval: -283.8893475282006[0m
[37m[1m[2023-07-17 00:42:01,729][257371] Mean Reward across all agents: -12.154939482427261[0m
[37m[1m[2023-07-17 00:42:01,729][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:42:01,735][257371] mean_value=-182.52643067410497, max_value=448.0142377432564[0m
[37m[1m[2023-07-17 00:42:01,738][257371] New mean coefficients: [[ 1.2238317   0.04304072 -0.63021874 -1.5887175   2.4882703   0.08564818]][0m
[37m[1m[2023-07-17 00:42:01,739][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:42:10,783][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 00:42:10,783][257371] FPS: 424651.71[0m
[36m[2023-07-17 00:42:10,786][257371] itr=345, itrs=2000, Progress: 17.25%[0m
[36m[2023-07-17 00:42:22,641][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 00:42:22,641][257371] FPS: 325698.53[0m
[36m[2023-07-17 00:42:26,985][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:42:26,985][257371] Reward + Measures: [[104.89858118   0.09150233   0.90243638   0.21557666   0.90029168
    3.36244726]][0m
[37m[1m[2023-07-17 00:42:26,986][257371] Max Reward on eval: 104.8985811755241[0m
[37m[1m[2023-07-17 00:42:26,986][257371] Min Reward on eval: 104.8985811755241[0m
[37m[1m[2023-07-17 00:42:26,986][257371] Mean Reward across all agents: 104.8985811755241[0m
[37m[1m[2023-07-17 00:42:26,986][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:42:32,070][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:42:32,071][257371] Reward + Measures: [[ 49.19827794   0.14420001   0.76850003   0.2016       0.77419996
    4.17060661]
 [ 30.02863239   0.13090001   0.70460004   0.2911       0.68979996
    3.97228932]
 [-83.766807     0.35549998   0.44980001   0.5377       0.59749997
    4.26300812]
 ...
 [-29.06598831   0.18730001   0.6857       0.65540004   0.79840004
    4.6844058 ]
 [217.83242515   0.40689999   0.49770004   0.50450003   0.52539998
    3.31228137]
 [ 54.26333723   0.1429       0.74510002   0.20869999   0.7317
    4.33577347]][0m
[37m[1m[2023-07-17 00:42:32,071][257371] Max Reward on eval: 318.68949796338563[0m
[37m[1m[2023-07-17 00:42:32,071][257371] Min Reward on eval: -351.4797306765337[0m
[37m[1m[2023-07-17 00:42:32,071][257371] Mean Reward across all agents: 8.71773266108593[0m
[37m[1m[2023-07-17 00:42:32,072][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:42:32,077][257371] mean_value=-193.57043372311247, max_value=414.587855281113[0m
[37m[1m[2023-07-17 00:42:32,079][257371] New mean coefficients: [[ 2.099956    0.04698899 -0.13346547 -2.1421568   0.33049655 -0.01778574]][0m
[37m[1m[2023-07-17 00:42:32,080][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:42:41,244][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-17 00:42:41,244][257371] FPS: 419136.59[0m
[36m[2023-07-17 00:42:41,246][257371] itr=346, itrs=2000, Progress: 17.30%[0m
[36m[2023-07-17 00:42:53,218][257371] train() took 11.91 seconds to complete[0m
[36m[2023-07-17 00:42:53,218][257371] FPS: 322507.67[0m
[36m[2023-07-17 00:42:57,527][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:42:57,528][257371] Reward + Measures: [[115.67290699   0.09118634   0.90175325   0.213837     0.89455795
    3.34824324]][0m
[37m[1m[2023-07-17 00:42:57,528][257371] Max Reward on eval: 115.67290698830585[0m
[37m[1m[2023-07-17 00:42:57,528][257371] Min Reward on eval: 115.67290698830585[0m
[37m[1m[2023-07-17 00:42:57,528][257371] Mean Reward across all agents: 115.67290698830585[0m
[37m[1m[2023-07-17 00:42:57,529][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:43:02,549][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:43:02,549][257371] Reward + Measures: [[  -6.42249173    0.37920001    0.69100004    0.49200001    0.82690001
     5.03856325]
 [  -2.4869946     0.13020001    0.90679997    0.24459998    0.91949999
     5.09389877]
 [-591.55721472    0.9181        0.0217        0.89910001    0.88160002
     5.99234152]
 ...
 [ -46.36567889    0.26360002    0.87819999    0.66240001    0.88319999
     5.03492117]
 [-452.14893128    0.82299995    0.23369999    0.89849997    0.88060009
     5.6930995 ]
 [  26.30571027    0.1107        0.91079998    0.2027        0.91469997
     4.30079126]][0m
[37m[1m[2023-07-17 00:43:02,550][257371] Max Reward on eval: 251.18891568905673[0m
[37m[1m[2023-07-17 00:43:02,550][257371] Min Reward on eval: -801.7491149848327[0m
[37m[1m[2023-07-17 00:43:02,550][257371] Mean Reward across all agents: -59.87283973109433[0m
[37m[1m[2023-07-17 00:43:02,551][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:43:02,558][257371] mean_value=-157.78507123895534, max_value=536.8610187152202[0m
[37m[1m[2023-07-17 00:43:02,561][257371] New mean coefficients: [[ 1.3708917   0.42074648 -1.8007426  -2.0586903   0.73967916 -0.16034856]][0m
[37m[1m[2023-07-17 00:43:02,562][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:43:11,606][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 00:43:11,607][257371] FPS: 424657.58[0m
[36m[2023-07-17 00:43:11,609][257371] itr=347, itrs=2000, Progress: 17.35%[0m
[36m[2023-07-17 00:43:23,362][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 00:43:23,362][257371] FPS: 328605.18[0m
[36m[2023-07-17 00:43:27,710][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:43:27,710][257371] Reward + Measures: [[132.4009914    0.10470833   0.88162631   0.21626766   0.87499464
    3.28690886]][0m
[37m[1m[2023-07-17 00:43:27,710][257371] Max Reward on eval: 132.40099140311335[0m
[37m[1m[2023-07-17 00:43:27,711][257371] Min Reward on eval: 132.40099140311335[0m
[37m[1m[2023-07-17 00:43:27,711][257371] Mean Reward across all agents: 132.40099140311335[0m
[37m[1m[2023-07-17 00:43:27,711][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:43:32,761][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:43:32,762][257371] Reward + Measures: [[ 57.81614565   0.0702       0.89060003   0.2599       0.91309994
    3.51156545]
 [ 60.26753203   0.0478       0.91020006   0.29150003   0.92189997
    3.67847514]
 [ 61.08851342   0.21330002   0.54330009   0.28849998   0.4989
    3.48671126]
 ...
 [169.76888276   0.20009999   0.80680007   0.28590003   0.78759998
    3.25268674]
 [108.58388996   0.09120001   0.91000003   0.2026       0.90700006
    3.35321975]
 [ 97.71145242   0.24130002   0.72259998   0.3527       0.70830005
    3.25779033]][0m
[37m[1m[2023-07-17 00:43:32,762][257371] Max Reward on eval: 184.04502488337457[0m
[37m[1m[2023-07-17 00:43:32,762][257371] Min Reward on eval: -123.11150357499719[0m
[37m[1m[2023-07-17 00:43:32,762][257371] Mean Reward across all agents: 47.345719011634365[0m
[37m[1m[2023-07-17 00:43:32,763][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:43:32,766][257371] mean_value=-122.45417984414594, max_value=68.74440254782134[0m
[37m[1m[2023-07-17 00:43:32,769][257371] New mean coefficients: [[ 0.893713    0.50993955 -0.37639868 -1.7726792   0.71245295 -0.19704536]][0m
[37m[1m[2023-07-17 00:43:32,770][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:43:41,893][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 00:43:41,893][257371] FPS: 420995.37[0m
[36m[2023-07-17 00:43:41,895][257371] itr=348, itrs=2000, Progress: 17.40%[0m
[36m[2023-07-17 00:43:53,663][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 00:43:53,663][257371] FPS: 328157.60[0m
[36m[2023-07-17 00:43:57,996][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:43:57,997][257371] Reward + Measures: [[140.08620199   0.10501367   0.87679631   0.21465568   0.8708154
    3.24418855]][0m
[37m[1m[2023-07-17 00:43:57,997][257371] Max Reward on eval: 140.08620198618675[0m
[37m[1m[2023-07-17 00:43:57,997][257371] Min Reward on eval: 140.08620198618675[0m
[37m[1m[2023-07-17 00:43:57,998][257371] Mean Reward across all agents: 140.08620198618675[0m
[37m[1m[2023-07-17 00:43:57,998][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:44:03,107][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:44:03,108][257371] Reward + Measures: [[ -25.30673442    0.2066        0.65219992    0.1973        0.66910005
     3.87438369]
 [  50.61161414    0.20290001    0.212         0.22620001    0.1032
     5.42410898]
 [-142.61911437    0.68850005    0.21300001    0.62099999    0.58520001
     5.62861013]
 ...
 [ 151.85050023    0.3008        0.31119999    0.33960003    0.1048
     4.89168119]
 [-424.66200404    0.77740002    0.0553        0.7773        0.74400002
     5.94216585]
 [-496.34542375    0.68520004    0.2705        0.88469994    0.89910001
     6.2335515 ]][0m
[37m[1m[2023-07-17 00:44:03,108][257371] Max Reward on eval: 337.49523829128594[0m
[37m[1m[2023-07-17 00:44:03,108][257371] Min Reward on eval: -901.963645913452[0m
[37m[1m[2023-07-17 00:44:03,108][257371] Mean Reward across all agents: -97.97393485927833[0m
[37m[1m[2023-07-17 00:44:03,108][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:44:03,114][257371] mean_value=-334.2781781529975, max_value=528.0116683528062[0m
[37m[1m[2023-07-17 00:44:03,117][257371] New mean coefficients: [[ 1.3845717   0.27152038 -0.11640611 -2.0717156  -0.40464097 -0.30183795]][0m
[37m[1m[2023-07-17 00:44:03,118][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:44:12,285][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-17 00:44:12,285][257371] FPS: 418957.00[0m
[36m[2023-07-17 00:44:12,287][257371] itr=349, itrs=2000, Progress: 17.45%[0m
[36m[2023-07-17 00:44:24,268][257371] train() took 11.91 seconds to complete[0m
[36m[2023-07-17 00:44:24,268][257371] FPS: 322300.64[0m
[36m[2023-07-17 00:44:28,581][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:44:28,581][257371] Reward + Measures: [[160.07082294   0.115338     0.85877699   0.21728666   0.85171759
    3.17871094]][0m
[37m[1m[2023-07-17 00:44:28,582][257371] Max Reward on eval: 160.07082294239433[0m
[37m[1m[2023-07-17 00:44:28,582][257371] Min Reward on eval: 160.07082294239433[0m
[37m[1m[2023-07-17 00:44:28,582][257371] Mean Reward across all agents: 160.07082294239433[0m
[37m[1m[2023-07-17 00:44:28,582][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:44:33,844][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:44:33,845][257371] Reward + Measures: [[-57.82704041   0.31570002   0.45559999   0.34400001   0.48450002
    4.37392569]
 [110.88991307   0.1165       0.87560004   0.21180001   0.86709994
    3.00799489]
 [-32.53035878   0.47499999   0.59079999   0.37         0.61549997
    5.56934547]
 ...
 [-23.80835433   0.09240001   0.94900006   0.1811       0.9691
    4.6152854 ]
 [ 32.40168385   0.18499999   0.22420001   0.21730001   0.23720001
    4.00648737]
 [ 15.28673014   0.19069998   0.3558       0.35600001   0.35119998
    3.16707587]][0m
[37m[1m[2023-07-17 00:44:33,845][257371] Max Reward on eval: 179.7690391458571[0m
[37m[1m[2023-07-17 00:44:33,845][257371] Min Reward on eval: -422.60665133725854[0m
[37m[1m[2023-07-17 00:44:33,845][257371] Mean Reward across all agents: -0.8671822757336679[0m
[37m[1m[2023-07-17 00:44:33,845][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:44:33,849][257371] mean_value=-998.2194020706004, max_value=227.47093039848903[0m
[37m[1m[2023-07-17 00:44:33,851][257371] New mean coefficients: [[-0.52688575  0.34659982 -1.1798507  -1.4204043   0.46971565  0.02749524]][0m
[37m[1m[2023-07-17 00:44:33,852][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:44:42,989][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 00:44:42,989][257371] FPS: 420373.69[0m
[36m[2023-07-17 00:44:42,991][257371] itr=350, itrs=2000, Progress: 17.50%[0m
[37m[1m[2023-07-17 00:47:30,226][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000330[0m
[36m[2023-07-17 00:47:42,612][257371] train() took 11.89 seconds to complete[0m
[36m[2023-07-17 00:47:42,612][257371] FPS: 322999.72[0m
[36m[2023-07-17 00:47:46,792][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:47:46,792][257371] Reward + Measures: [[147.64881841   0.11781999   0.85277736   0.21521468   0.84751099
    3.16479158]][0m
[37m[1m[2023-07-17 00:47:46,792][257371] Max Reward on eval: 147.64881841307727[0m
[37m[1m[2023-07-17 00:47:46,792][257371] Min Reward on eval: 147.64881841307727[0m
[37m[1m[2023-07-17 00:47:46,793][257371] Mean Reward across all agents: 147.64881841307727[0m
[37m[1m[2023-07-17 00:47:46,793][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:47:51,848][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:47:51,848][257371] Reward + Measures: [[31.1252781   0.1175      0.69310004  0.39610001  0.7209      3.75081897]
 [28.59997902  0.17260002  0.76659995  0.3062      0.76290005  3.8716619 ]
 [-3.02407371  0.09810001  0.67659998  0.48199996  0.75400001  4.47777176]
 ...
 [70.2111088   0.19320001  0.60780001  0.28099999  0.57750005  3.57229352]
 [63.17384077  0.23480001  0.56699997  0.47940001  0.74110001  4.74569273]
 [88.61532236  0.16940001  0.69949996  0.26789999  0.71410006  3.18768382]][0m
[37m[1m[2023-07-17 00:47:51,848][257371] Max Reward on eval: 177.53609371613712[0m
[37m[1m[2023-07-17 00:47:51,849][257371] Min Reward on eval: -289.09098623879254[0m
[37m[1m[2023-07-17 00:47:51,849][257371] Mean Reward across all agents: 22.654368722939562[0m
[37m[1m[2023-07-17 00:47:51,849][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:47:51,855][257371] mean_value=-226.87945151598998, max_value=427.2695305127795[0m
[37m[1m[2023-07-17 00:47:51,858][257371] New mean coefficients: [[-1.777923   0.5848961 -3.3959074 -1.0195513  1.0164793  0.2857071]][0m
[37m[1m[2023-07-17 00:47:51,858][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:48:01,009][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-17 00:48:01,009][257371] FPS: 419723.44[0m
[36m[2023-07-17 00:48:01,011][257371] itr=351, itrs=2000, Progress: 17.55%[0m
[36m[2023-07-17 00:48:12,825][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 00:48:12,825][257371] FPS: 326870.16[0m
[36m[2023-07-17 00:48:17,128][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:48:17,129][257371] Reward + Measures: [[139.94078759   0.13157032   0.8281284    0.22228402   0.82834429
    3.17556739]][0m
[37m[1m[2023-07-17 00:48:17,129][257371] Max Reward on eval: 139.94078758950562[0m
[37m[1m[2023-07-17 00:48:17,129][257371] Min Reward on eval: 139.94078758950562[0m
[37m[1m[2023-07-17 00:48:17,130][257371] Mean Reward across all agents: 139.94078758950562[0m
[37m[1m[2023-07-17 00:48:17,130][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:48:22,099][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:48:22,100][257371] Reward + Measures: [[-43.00154597   0.20580001   0.18560001   0.22360002   0.26789999
    5.19390678]
 [473.57736206   0.90410006   0.64829999   0.92510003   0.07270001
    6.90261078]
 [344.39631076   0.89270002   0.63840002   0.9188       0.0788
    6.88048553]
 ...
 [ 25.14930709   0.0948       0.0902       0.10179999   0.0844
    4.6262908 ]
 [-33.6264841    0.61589998   0.29949999   0.70600003   0.38260001
    5.54450369]
 [-27.29701352   0.1426       0.1257       0.20039999   0.2141
    4.45925283]][0m
[37m[1m[2023-07-17 00:48:22,100][257371] Max Reward on eval: 519.85128973932[0m
[37m[1m[2023-07-17 00:48:22,100][257371] Min Reward on eval: -66.20197451924905[0m
[37m[1m[2023-07-17 00:48:22,100][257371] Mean Reward across all agents: 46.58966180471341[0m
[37m[1m[2023-07-17 00:48:22,100][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:48:22,105][257371] mean_value=-1826.3472680739194, max_value=724.7226244976783[0m
[37m[1m[2023-07-17 00:48:22,107][257371] New mean coefficients: [[-1.202984    0.34630886  1.1096506  -0.6860768   0.33368492  0.49033627]][0m
[37m[1m[2023-07-17 00:48:22,108][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:48:31,155][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 00:48:31,155][257371] FPS: 424522.87[0m
[36m[2023-07-17 00:48:31,158][257371] itr=352, itrs=2000, Progress: 17.60%[0m
[36m[2023-07-17 00:48:43,084][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-17 00:48:43,084][257371] FPS: 323757.16[0m
[36m[2023-07-17 00:48:47,505][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:48:47,511][257371] Reward + Measures: [[124.14606712   0.117058     0.85924298   0.21323232   0.85923427
    3.23752284]][0m
[37m[1m[2023-07-17 00:48:47,511][257371] Max Reward on eval: 124.14606711804345[0m
[37m[1m[2023-07-17 00:48:47,511][257371] Min Reward on eval: 124.14606711804345[0m
[37m[1m[2023-07-17 00:48:47,511][257371] Mean Reward across all agents: 124.14606711804345[0m
[37m[1m[2023-07-17 00:48:47,512][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:48:52,499][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:48:52,500][257371] Reward + Measures: [[ 98.55546282   0.31510004   0.44220001   0.37630004   0.46790004
    3.19801283]
 [ 55.73415349   0.1251       0.77340001   0.25270003   0.80060005
    3.43512511]
 [ 95.23226418   0.1287       0.81420004   0.25430003   0.83350003
    3.37270164]
 ...
 [ 79.02914338   0.2462       0.65560001   0.29980001   0.64170009
    3.02401209]
 [-38.3538229    0.22549999   0.69129997   0.28959998   0.69450003
    3.53353667]
 [174.21634486   0.3118       0.3328       0.3547       0.3407
    3.3927002 ]][0m
[37m[1m[2023-07-17 00:48:52,500][257371] Max Reward on eval: 203.2534064926207[0m
[37m[1m[2023-07-17 00:48:52,500][257371] Min Reward on eval: -90.45718952196185[0m
[37m[1m[2023-07-17 00:48:52,501][257371] Mean Reward across all agents: 76.56682034705273[0m
[37m[1m[2023-07-17 00:48:52,501][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:48:52,503][257371] mean_value=-590.4444052744843, max_value=156.34670348675087[0m
[37m[1m[2023-07-17 00:48:52,506][257371] New mean coefficients: [[ 0.41741157  0.30075273  1.3449198  -1.3397634  -0.5687214   0.11178878]][0m
[37m[1m[2023-07-17 00:48:52,507][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:49:01,550][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 00:49:01,551][257371] FPS: 424694.29[0m
[36m[2023-07-17 00:49:01,553][257371] itr=353, itrs=2000, Progress: 17.65%[0m
[36m[2023-07-17 00:49:13,181][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 00:49:13,182][257371] FPS: 332138.35[0m
[36m[2023-07-17 00:49:17,448][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:49:17,448][257371] Reward + Measures: [[109.76163808   0.10720666   0.88342172   0.20118801   0.88348234
    3.33873367]][0m
[37m[1m[2023-07-17 00:49:17,448][257371] Max Reward on eval: 109.76163808071244[0m
[37m[1m[2023-07-17 00:49:17,448][257371] Min Reward on eval: 109.76163808071244[0m
[37m[1m[2023-07-17 00:49:17,449][257371] Mean Reward across all agents: 109.76163808071244[0m
[37m[1m[2023-07-17 00:49:17,449][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:49:22,464][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:49:22,465][257371] Reward + Measures: [[ 99.19751358   0.16659999   0.7101       0.43780002   0.72140002
    2.97748303]
 [ 28.84171836   0.14220001   0.75100005   0.21879999   0.75430006
    3.34205103]
 [ 69.24005155   0.23760001   0.55799997   0.2633       0.55720001
    5.26479053]
 ...
 [ 34.08483933   0.1486       0.76380008   0.21830001   0.74909997
    3.4680326 ]
 [-10.88723969   0.38659999   0.48950002   0.78029996   0.81000006
    7.24523878]
 [  2.37334442   0.44619998   0.68580002   0.58140004   0.87480003
    7.21443653]][0m
[37m[1m[2023-07-17 00:49:22,465][257371] Max Reward on eval: 262.3306140769273[0m
[37m[1m[2023-07-17 00:49:22,466][257371] Min Reward on eval: -323.6822466675192[0m
[37m[1m[2023-07-17 00:49:22,466][257371] Mean Reward across all agents: 42.38132921035274[0m
[37m[1m[2023-07-17 00:49:22,466][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:49:22,475][257371] mean_value=-58.710368373649445, max_value=607.5811069569353[0m
[37m[1m[2023-07-17 00:49:22,478][257371] New mean coefficients: [[ 0.82731515  0.18970771  2.2943063  -1.2148687  -0.6665373  -0.00274668]][0m
[37m[1m[2023-07-17 00:49:22,479][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:49:31,481][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 00:49:31,481][257371] FPS: 426678.75[0m
[36m[2023-07-17 00:49:31,483][257371] itr=354, itrs=2000, Progress: 17.70%[0m
[36m[2023-07-17 00:49:43,122][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 00:49:43,123][257371] FPS: 331755.75[0m
[36m[2023-07-17 00:49:47,419][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:49:47,420][257371] Reward + Measures: [[109.6031557    0.10354599   0.89145339   0.19550565   0.88827032
    3.34709859]][0m
[37m[1m[2023-07-17 00:49:47,420][257371] Max Reward on eval: 109.60315569856473[0m
[37m[1m[2023-07-17 00:49:47,420][257371] Min Reward on eval: 109.60315569856473[0m
[37m[1m[2023-07-17 00:49:47,421][257371] Mean Reward across all agents: 109.60315569856473[0m
[37m[1m[2023-07-17 00:49:47,421][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:49:52,454][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:49:52,455][257371] Reward + Measures: [[ -25.85963042    0.12719999    0.92360002    0.66070002    0.84750003
     6.85884333]
 [  65.08738619    0.1732        0.6613        0.22740002    0.71750003
     3.73093033]
 [-131.50105382    0.0659        0.8775        0.56940001    0.87140006
     6.60751438]
 ...
 [ 109.27220691    0.0245        0.9601        0.67339993    0.95690006
     7.86413574]
 [-108.87128997    0.39050001    0.54319996    0.53330004    0.49440002
     3.46789098]
 [ -37.27786573    0.23150001    0.28459999    0.27560002    0.26880002
     3.69600224]][0m
[37m[1m[2023-07-17 00:49:52,455][257371] Max Reward on eval: 230.80257986639626[0m
[37m[1m[2023-07-17 00:49:52,455][257371] Min Reward on eval: -251.38691619597375[0m
[37m[1m[2023-07-17 00:49:52,455][257371] Mean Reward across all agents: -7.470768251552647[0m
[37m[1m[2023-07-17 00:49:52,456][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:49:52,463][257371] mean_value=-110.98571414249461, max_value=593.359369702451[0m
[37m[1m[2023-07-17 00:49:52,466][257371] New mean coefficients: [[ 0.85929716  0.15829594  4.3206487  -0.84018934 -1.0578415  -0.05012031]][0m
[37m[1m[2023-07-17 00:49:52,467][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:50:01,512][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 00:50:01,512][257371] FPS: 424625.35[0m
[36m[2023-07-17 00:50:01,514][257371] itr=355, itrs=2000, Progress: 17.75%[0m
[36m[2023-07-17 00:50:13,242][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 00:50:13,242][257371] FPS: 329243.23[0m
[36m[2023-07-17 00:50:17,613][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:50:17,613][257371] Reward + Measures: [[103.1770465    0.09698      0.9056564    0.19272265   0.90113932
    3.36041141]][0m
[37m[1m[2023-07-17 00:50:17,613][257371] Max Reward on eval: 103.17704650248905[0m
[37m[1m[2023-07-17 00:50:17,614][257371] Min Reward on eval: 103.17704650248905[0m
[37m[1m[2023-07-17 00:50:17,614][257371] Mean Reward across all agents: 103.17704650248905[0m
[37m[1m[2023-07-17 00:50:17,614][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:50:22,856][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:50:22,857][257371] Reward + Measures: [[130.9795952    0.28269997   0.54819995   0.3809       0.48659998
    4.32079554]
 [-19.8333157    0.21870001   0.55700004   0.2737       0.59450001
    3.79648066]
 [  4.66271476   0.26990002   0.54879999   0.37740001   0.4113
    3.92049646]
 ...
 [ -6.80814831   0.37630004   0.43039998   0.40540001   0.30489999
    4.86312914]
 [-42.56889058   0.20039999   0.78760004   0.33320004   0.67259997
    4.50565481]
 [106.40385078   0.26719999   0.60859996   0.32979998   0.57569999
    3.92520905]][0m
[37m[1m[2023-07-17 00:50:22,857][257371] Max Reward on eval: 263.55247543090957[0m
[37m[1m[2023-07-17 00:50:22,857][257371] Min Reward on eval: -169.62886471413077[0m
[37m[1m[2023-07-17 00:50:22,857][257371] Mean Reward across all agents: 33.648196813478066[0m
[37m[1m[2023-07-17 00:50:22,858][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:50:22,863][257371] mean_value=-153.3782015051902, max_value=533.2750673526527[0m
[37m[1m[2023-07-17 00:50:22,866][257371] New mean coefficients: [[-0.2425865   0.17713682  4.3002415  -0.11553752  1.2594044  -0.26641446]][0m
[37m[1m[2023-07-17 00:50:22,867][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:50:31,886][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 00:50:31,886][257371] FPS: 425843.85[0m
[36m[2023-07-17 00:50:31,889][257371] itr=356, itrs=2000, Progress: 17.80%[0m
[36m[2023-07-17 00:50:43,518][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 00:50:43,519][257371] FPS: 332102.10[0m
[36m[2023-07-17 00:50:47,806][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:50:47,807][257371] Reward + Measures: [[111.76969026   0.09901033   0.90672362   0.19378066   0.90076733
    3.27032423]][0m
[37m[1m[2023-07-17 00:50:47,807][257371] Max Reward on eval: 111.76969026497521[0m
[37m[1m[2023-07-17 00:50:47,807][257371] Min Reward on eval: 111.76969026497521[0m
[37m[1m[2023-07-17 00:50:47,808][257371] Mean Reward across all agents: 111.76969026497521[0m
[37m[1m[2023-07-17 00:50:47,808][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:50:52,830][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:50:52,831][257371] Reward + Measures: [[-20.87763085   0.23120001   0.66769999   0.22790001   0.68309999
    4.38532686]
 [284.32537368   0.44300005   0.37139997   0.51900005   0.3242
    3.77061319]
 [ -1.42710216   0.18079999   0.70370001   0.27980003   0.60280001
    3.00230408]
 ...
 [ 82.02369694   0.1725       0.82080001   0.25349998   0.77039999
    3.57803988]
 [-25.8375439    0.1552       0.18590002   0.162        0.20819998
    4.36892176]
 [ 66.38796936   0.23220001   0.52530003   0.17650001   0.51910001
    5.07737303]][0m
[37m[1m[2023-07-17 00:50:52,831][257371] Max Reward on eval: 305.81833841931075[0m
[37m[1m[2023-07-17 00:50:52,831][257371] Min Reward on eval: -102.13396789217367[0m
[37m[1m[2023-07-17 00:50:52,832][257371] Mean Reward across all agents: 13.562231121357906[0m
[37m[1m[2023-07-17 00:50:52,832][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:50:52,835][257371] mean_value=-619.331601461331, max_value=616.2706110071532[0m
[37m[1m[2023-07-17 00:50:52,837][257371] New mean coefficients: [[-0.4622466   0.03980999  1.2270565  -0.29039067  2.2084103  -0.52275985]][0m
[37m[1m[2023-07-17 00:50:52,838][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:51:01,869][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 00:51:01,869][257371] FPS: 425288.31[0m
[36m[2023-07-17 00:51:01,872][257371] itr=357, itrs=2000, Progress: 17.85%[0m
[36m[2023-07-17 00:51:13,546][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 00:51:13,546][257371] FPS: 330815.87[0m
[36m[2023-07-17 00:51:17,788][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:51:17,789][257371] Reward + Measures: [[103.55123735   0.09007832   0.91865104   0.19251134   0.91301596
    3.24881172]][0m
[37m[1m[2023-07-17 00:51:17,789][257371] Max Reward on eval: 103.55123735251685[0m
[37m[1m[2023-07-17 00:51:17,789][257371] Min Reward on eval: 103.55123735251685[0m
[37m[1m[2023-07-17 00:51:17,789][257371] Mean Reward across all agents: 103.55123735251685[0m
[37m[1m[2023-07-17 00:51:17,790][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:51:22,821][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:51:22,821][257371] Reward + Measures: [[ -20.27618259    0.18230002    0.6552        0.23660003    0.61770004
     3.38581157]
 [   6.745317      0.21850002    0.63020003    0.26409999    0.56850004
     2.81881452]
 [-104.57087904    0.3382        0.56809998    0.25120002    0.52850002
     3.80497813]
 ...
 [ -13.51767124    0.45239997    0.45890003    0.46510002    0.41700003
     5.8885169 ]
 [   9.76700939    0.19929999    0.58610004    0.34639999    0.66610003
     3.9290669 ]
 [   0.66703364    0.13579999    0.19759999    0.2604        0.25110003
     5.58124256]][0m
[37m[1m[2023-07-17 00:51:22,822][257371] Max Reward on eval: 482.8671626872383[0m
[37m[1m[2023-07-17 00:51:22,822][257371] Min Reward on eval: -776.7398185677827[0m
[37m[1m[2023-07-17 00:51:22,822][257371] Mean Reward across all agents: 3.0738195406586564[0m
[37m[1m[2023-07-17 00:51:22,822][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:51:22,828][257371] mean_value=-164.0801094668096, max_value=480.4573858883716[0m
[37m[1m[2023-07-17 00:51:22,831][257371] New mean coefficients: [[-1.4280913  -0.02707133  1.9623702   0.22459668  3.3590894  -0.8890659 ]][0m
[37m[1m[2023-07-17 00:51:22,832][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:51:31,924][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 00:51:31,925][257371] FPS: 422380.31[0m
[36m[2023-07-17 00:51:31,927][257371] itr=358, itrs=2000, Progress: 17.90%[0m
[36m[2023-07-17 00:51:43,626][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 00:51:43,626][257371] FPS: 330137.78[0m
[36m[2023-07-17 00:51:47,944][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:51:47,944][257371] Reward + Measures: [[99.3228077   0.078355    0.92849493  0.19988932  0.92777735  3.22618413]][0m
[37m[1m[2023-07-17 00:51:47,944][257371] Max Reward on eval: 99.3228076957856[0m
[37m[1m[2023-07-17 00:51:47,944][257371] Min Reward on eval: 99.3228076957856[0m
[37m[1m[2023-07-17 00:51:47,945][257371] Mean Reward across all agents: 99.3228076957856[0m
[37m[1m[2023-07-17 00:51:47,945][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:51:52,966][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:51:52,966][257371] Reward + Measures: [[127.56264605   0.22319999   0.4677       0.25310001   0.41859999
    3.50928044]
 [136.12387375   0.22570001   0.53439999   0.26340002   0.49230003
    3.25139618]
 [137.82110692   0.1604       0.80030006   0.26080003   0.80669993
    3.28564453]
 ...
 [106.15550898   0.16880001   0.58410001   0.23899999   0.55110002
    3.7571876 ]
 [ 52.87042683   0.12590002   0.77360004   0.19939999   0.73330003
    3.50718546]
 [ 10.9002294    0.23539999   0.87939996   0.3858       0.8551001
    3.18150258]][0m
[37m[1m[2023-07-17 00:51:52,967][257371] Max Reward on eval: 255.14340258659794[0m
[37m[1m[2023-07-17 00:51:52,967][257371] Min Reward on eval: -203.6225518866442[0m
[37m[1m[2023-07-17 00:51:52,967][257371] Mean Reward across all agents: 72.949028412645[0m
[37m[1m[2023-07-17 00:51:52,967][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:51:52,971][257371] mean_value=-764.766973432206, max_value=263.7223246131428[0m
[37m[1m[2023-07-17 00:51:52,974][257371] New mean coefficients: [[-1.4886391   0.12537234  0.4860177   0.003232    2.806207   -0.6533264 ]][0m
[37m[1m[2023-07-17 00:51:52,975][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:52:01,965][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 00:52:01,965][257371] FPS: 427240.44[0m
[36m[2023-07-17 00:52:01,967][257371] itr=359, itrs=2000, Progress: 17.95%[0m
[36m[2023-07-17 00:52:13,640][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 00:52:13,640][257371] FPS: 330974.49[0m
[36m[2023-07-17 00:52:17,916][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:52:17,916][257371] Reward + Measures: [[92.42082033  0.07324933  0.93531263  0.20178768  0.93622267  3.1722064 ]][0m
[37m[1m[2023-07-17 00:52:17,916][257371] Max Reward on eval: 92.42082032907918[0m
[37m[1m[2023-07-17 00:52:17,917][257371] Min Reward on eval: 92.42082032907918[0m
[37m[1m[2023-07-17 00:52:17,917][257371] Mean Reward across all agents: 92.42082032907918[0m
[37m[1m[2023-07-17 00:52:17,917][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:52:22,994][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:52:22,994][257371] Reward + Measures: [[-25.26206725   0.32880002   0.51249999   0.28920001   0.46520001
    4.10485792]
 [156.6939509    0.1857       0.63980001   0.43859997   0.67249995
    2.92621303]
 [ 30.00491554   0.29429999   0.26100001   0.32450002   0.25480002
    5.31649446]
 ...
 [210.81832734   0.36170003   0.32860002   0.36059999   0.41510001
    3.75094271]
 [118.29457763   0.435        0.2304       0.3671       0.34769997
    4.36165762]
 [116.10071208   0.44870004   0.50450003   0.45410004   0.52410001
    3.53349376]][0m
[37m[1m[2023-07-17 00:52:22,994][257371] Max Reward on eval: 287.25800179985816[0m
[37m[1m[2023-07-17 00:52:22,995][257371] Min Reward on eval: -137.02091096583754[0m
[37m[1m[2023-07-17 00:52:22,995][257371] Mean Reward across all agents: 89.2616045781744[0m
[37m[1m[2023-07-17 00:52:22,995][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:52:23,002][257371] mean_value=-99.76983258096907, max_value=247.5527320754858[0m
[37m[1m[2023-07-17 00:52:23,005][257371] New mean coefficients: [[-0.9357488   0.08241741  0.54236025 -0.22426961  3.4774885  -0.89871514]][0m
[37m[1m[2023-07-17 00:52:23,006][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:52:32,062][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 00:52:32,063][257371] FPS: 424114.16[0m
[36m[2023-07-17 00:52:32,065][257371] itr=360, itrs=2000, Progress: 18.00%[0m
[37m[1m[2023-07-17 00:55:12,443][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000340[0m
[36m[2023-07-17 00:55:24,967][257371] train() took 11.88 seconds to complete[0m
[36m[2023-07-17 00:55:24,968][257371] FPS: 323132.36[0m
[36m[2023-07-17 00:55:29,144][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:55:29,145][257371] Reward + Measures: [[89.71553838  0.064339    0.94191903  0.210177    0.94376767  3.0904634 ]][0m
[37m[1m[2023-07-17 00:55:29,145][257371] Max Reward on eval: 89.71553838367467[0m
[37m[1m[2023-07-17 00:55:29,145][257371] Min Reward on eval: 89.71553838367467[0m
[37m[1m[2023-07-17 00:55:29,145][257371] Mean Reward across all agents: 89.71553838367467[0m
[37m[1m[2023-07-17 00:55:29,146][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:55:34,093][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:55:34,094][257371] Reward + Measures: [[  31.97313006    0.21180001    0.48750001    0.28220001    0.50290006
     5.61052752]
 [ -92.13741184    0.18799999    0.49869999    0.43470001    0.54619998
     5.14046621]
 [-135.30823325    0.2357        0.30880001    0.3127        0.2949
     3.32777953]
 ...
 [   6.19983957    0.18190001    0.2764        0.24800001    0.23179999
     2.79868007]
 [ -74.57992938    0.24170001    0.24989998    0.23410001    0.29659998
     4.43383551]
 [-352.6781664     0.65100002    0.29789999    0.87309998    0.90079993
     6.64956045]][0m
[37m[1m[2023-07-17 00:55:34,094][257371] Max Reward on eval: 164.11169720524921[0m
[37m[1m[2023-07-17 00:55:34,094][257371] Min Reward on eval: -352.6781663968228[0m
[37m[1m[2023-07-17 00:55:34,095][257371] Mean Reward across all agents: -27.88904577551948[0m
[37m[1m[2023-07-17 00:55:34,095][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:55:34,100][257371] mean_value=-673.8885072596821, max_value=463.31000299798416[0m
[37m[1m[2023-07-17 00:55:34,103][257371] New mean coefficients: [[-1.0086533  0.3114731  4.1515155  0.515383   2.629878  -0.5351292]][0m
[37m[1m[2023-07-17 00:55:34,104][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:55:43,246][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 00:55:43,246][257371] FPS: 420135.42[0m
[36m[2023-07-17 00:55:43,248][257371] itr=361, itrs=2000, Progress: 18.05%[0m
[36m[2023-07-17 00:55:55,076][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 00:55:55,076][257371] FPS: 326487.99[0m
[36m[2023-07-17 00:55:59,400][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:55:59,400][257371] Reward + Measures: [[86.04459906  0.064585    0.94571996  0.20826136  0.94925398  3.07273293]][0m
[37m[1m[2023-07-17 00:55:59,400][257371] Max Reward on eval: 86.044599055723[0m
[37m[1m[2023-07-17 00:55:59,400][257371] Min Reward on eval: 86.044599055723[0m
[37m[1m[2023-07-17 00:55:59,401][257371] Mean Reward across all agents: 86.044599055723[0m
[37m[1m[2023-07-17 00:55:59,401][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:56:04,445][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:56:04,445][257371] Reward + Measures: [[   7.05033045    0.0965        0.82849997    0.17900001    0.87989998
     4.09157562]
 [ -12.60900814    0.1485        0.69670004    0.5948        0.71439999
     4.77002525]
 [  70.54352157    0.5837        0.44010001    0.54209995    0.36619997
     6.55010223]
 ...
 [ -25.37711939    0.29130003    0.52899998    0.30350003    0.57420003
     4.24897909]
 [-328.20007396    0.54339999    0.40640002    0.84800005    0.92540008
     6.41055298]
 [-359.37385036    0.4736        0.5115        0.94810003    0.96180004
     7.42289734]][0m
[37m[1m[2023-07-17 00:56:04,445][257371] Max Reward on eval: 234.60862519349902[0m
[37m[1m[2023-07-17 00:56:04,446][257371] Min Reward on eval: -470.5440855693072[0m
[37m[1m[2023-07-17 00:56:04,446][257371] Mean Reward across all agents: -15.443183189942348[0m
[37m[1m[2023-07-17 00:56:04,446][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:56:04,450][257371] mean_value=-176.94317566668866, max_value=334.9315236866445[0m
[37m[1m[2023-07-17 00:56:04,453][257371] New mean coefficients: [[-1.5855803   0.65322924  5.7732024   0.9806911   2.8218336  -0.50286245]][0m
[37m[1m[2023-07-17 00:56:04,454][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:56:13,505][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 00:56:13,505][257371] FPS: 424336.27[0m
[36m[2023-07-17 00:56:13,507][257371] itr=362, itrs=2000, Progress: 18.10%[0m
[36m[2023-07-17 00:56:25,326][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 00:56:25,326][257371] FPS: 326791.08[0m
[36m[2023-07-17 00:56:29,739][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:56:29,745][257371] Reward + Measures: [[82.7635432   0.06708033  0.95098966  0.20294     0.95362496  3.01466799]][0m
[37m[1m[2023-07-17 00:56:29,745][257371] Max Reward on eval: 82.76354320203633[0m
[37m[1m[2023-07-17 00:56:29,745][257371] Min Reward on eval: 82.76354320203633[0m
[37m[1m[2023-07-17 00:56:29,745][257371] Mean Reward across all agents: 82.76354320203633[0m
[37m[1m[2023-07-17 00:56:29,746][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:56:34,796][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:56:34,802][257371] Reward + Measures: [[  26.15537248    0.30340001    0.5729        0.47310001    0.47729999
     2.56566739]
 [  26.86255916    0.2534        0.38540003    0.29590002    0.32999998
     3.36370921]
 [ -29.6094714     0.0907        0.90910006    0.26659998    0.89569998
     2.93792558]
 ...
 [  58.83099264    0.2447        0.34130001    0.24720001    0.28509998
     3.94997454]
 [-103.7201829     0.1367        0.76770002    0.40279999    0.68489999
     2.47790885]
 [  11.19942875    0.29099998    0.59560001    0.49329996    0.51370001
     2.67422438]][0m
[37m[1m[2023-07-17 00:56:34,802][257371] Max Reward on eval: 182.0540675342083[0m
[37m[1m[2023-07-17 00:56:34,802][257371] Min Reward on eval: -145.1495962075889[0m
[37m[1m[2023-07-17 00:56:34,803][257371] Mean Reward across all agents: 3.901624032808215[0m
[37m[1m[2023-07-17 00:56:34,803][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:56:34,805][257371] mean_value=-844.4657238998814, max_value=9.40842798642062[0m
[37m[1m[2023-07-17 00:56:34,808][257371] New mean coefficients: [[-1.9295883   0.8175747   3.490695    1.034424    2.8351576  -0.41078094]][0m
[37m[1m[2023-07-17 00:56:34,809][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:56:43,988][257371] train() took 9.18 seconds to complete[0m
[36m[2023-07-17 00:56:43,988][257371] FPS: 418412.46[0m
[36m[2023-07-17 00:56:43,990][257371] itr=363, itrs=2000, Progress: 18.15%[0m
[36m[2023-07-17 00:56:55,794][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 00:56:55,794][257371] FPS: 327150.74[0m
[36m[2023-07-17 00:57:00,081][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:57:00,082][257371] Reward + Measures: [[71.9944195   0.05944866  0.95952165  0.20925134  0.96293402  2.9789722 ]][0m
[37m[1m[2023-07-17 00:57:00,082][257371] Max Reward on eval: 71.99441950243337[0m
[37m[1m[2023-07-17 00:57:00,082][257371] Min Reward on eval: 71.99441950243337[0m
[37m[1m[2023-07-17 00:57:00,082][257371] Mean Reward across all agents: 71.99441950243337[0m
[37m[1m[2023-07-17 00:57:00,083][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:57:05,093][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:57:05,099][257371] Reward + Measures: [[90.69536878  0.18780001  0.73900002  0.27830002  0.7216      2.89139986]
 [-5.63223088  0.26570001  0.56080002  0.30200002  0.56120002  2.98710108]
 [-7.10693774  0.2102      0.19310001  0.24130002  0.23800002  4.21033144]
 ...
 [26.34677681  0.15820001  0.1578      0.1707      0.1761      4.82140017]
 [64.91561382  0.15810001  0.84759998  0.23210001  0.86619997  2.98702025]
 [11.29312042  0.18390001  0.28130004  0.21619999  0.2942      3.43359756]][0m
[37m[1m[2023-07-17 00:57:05,099][257371] Max Reward on eval: 145.8356418910902[0m
[37m[1m[2023-07-17 00:57:05,099][257371] Min Reward on eval: -228.00939063122496[0m
[37m[1m[2023-07-17 00:57:05,100][257371] Mean Reward across all agents: 17.729587729065038[0m
[37m[1m[2023-07-17 00:57:05,100][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:57:05,102][257371] mean_value=-766.4125347888273, max_value=62.96016967626917[0m
[37m[1m[2023-07-17 00:57:05,104][257371] New mean coefficients: [[-1.2295892   0.64779437  1.0305724   0.6531463   0.9780861  -0.12294385]][0m
[37m[1m[2023-07-17 00:57:05,105][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:57:14,201][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 00:57:14,201][257371] FPS: 422265.28[0m
[36m[2023-07-17 00:57:14,203][257371] itr=364, itrs=2000, Progress: 18.20%[0m
[36m[2023-07-17 00:57:25,900][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 00:57:25,901][257371] FPS: 330256.22[0m
[36m[2023-07-17 00:57:30,183][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:57:30,183][257371] Reward + Measures: [[63.72674716  0.058199    0.96214169  0.208692    0.96616966  3.01571608]][0m
[37m[1m[2023-07-17 00:57:30,184][257371] Max Reward on eval: 63.726747162536434[0m
[37m[1m[2023-07-17 00:57:30,184][257371] Min Reward on eval: 63.726747162536434[0m
[37m[1m[2023-07-17 00:57:30,184][257371] Mean Reward across all agents: 63.726747162536434[0m
[37m[1m[2023-07-17 00:57:30,184][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:57:35,243][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:57:35,248][257371] Reward + Measures: [[  -8.27385948    0.81729996    0.74239999    0.80599993    0.77450001
     4.39090443]
 [ -13.15023639    0.20130001    0.87110007    0.26300001    0.92329997
     4.20011854]
 [   6.63140229    0.68769997    0.70900005    0.66369998    0.75330007
     4.84447002]
 ...
 [-121.72425292    0.25219998    0.65109998    0.73899996    0.81779999
     6.2473464 ]
 [  17.8641192     0.20740001    0.7238        0.31080002    0.75740004
     4.59204102]
 [  40.92641498    0.19940002    0.79579997    0.24779999    0.79980004
     4.27058983]][0m
[37m[1m[2023-07-17 00:57:35,249][257371] Max Reward on eval: 133.18618489289656[0m
[37m[1m[2023-07-17 00:57:35,249][257371] Min Reward on eval: -190.01699379207565[0m
[37m[1m[2023-07-17 00:57:35,249][257371] Mean Reward across all agents: -21.966122970946962[0m
[37m[1m[2023-07-17 00:57:35,250][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:57:35,254][257371] mean_value=-134.36356159745105, max_value=344.38929758487967[0m
[37m[1m[2023-07-17 00:57:35,257][257371] New mean coefficients: [[-1.9307263   0.717086   -1.9388978   0.64525604  1.3758391   0.0476675 ]][0m
[37m[1m[2023-07-17 00:57:35,258][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:57:44,352][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 00:57:44,352][257371] FPS: 422368.36[0m
[36m[2023-07-17 00:57:44,354][257371] itr=365, itrs=2000, Progress: 18.25%[0m
[36m[2023-07-17 00:57:56,225][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 00:57:56,225][257371] FPS: 325423.91[0m
[36m[2023-07-17 00:58:00,478][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:58:00,478][257371] Reward + Measures: [[55.14624789  0.05905767  0.96063006  0.20877266  0.96647555  3.03698587]][0m
[37m[1m[2023-07-17 00:58:00,479][257371] Max Reward on eval: 55.146247892870754[0m
[37m[1m[2023-07-17 00:58:00,479][257371] Min Reward on eval: 55.146247892870754[0m
[37m[1m[2023-07-17 00:58:00,479][257371] Mean Reward across all agents: 55.146247892870754[0m
[37m[1m[2023-07-17 00:58:00,479][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:58:05,478][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:58:05,483][257371] Reward + Measures: [[  47.1511786     0.25470001    0.51090002    0.24229999    0.49400002
     3.9895165 ]
 [  12.25470337    0.0742        0.82580006    0.24880002    0.92650002
     4.337749  ]
 [  -4.72292095    0.1201        0.75240004    0.32159999    0.8721
     5.42752647]
 ...
 [-101.21444613    0.27530003    0.40599996    0.15100001    0.3651
     4.07065344]
 [ -43.84028346    0.361         0.30840001    0.34419999    0.35870001
     5.30178022]
 [ -73.71455605    0.31750003    0.47320005    0.2034        0.4461
     2.9375093 ]][0m
[37m[1m[2023-07-17 00:58:05,484][257371] Max Reward on eval: 254.86170482020825[0m
[37m[1m[2023-07-17 00:58:05,484][257371] Min Reward on eval: -256.37504670908675[0m
[37m[1m[2023-07-17 00:58:05,484][257371] Mean Reward across all agents: -9.023578104763905[0m
[37m[1m[2023-07-17 00:58:05,484][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:58:05,489][257371] mean_value=-357.37707840432245, max_value=570.3411879854277[0m
[37m[1m[2023-07-17 00:58:05,492][257371] New mean coefficients: [[-1.1233505   0.37765986 -3.8633065   0.12878174  3.3694937  -0.43216285]][0m
[37m[1m[2023-07-17 00:58:05,492][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:58:14,544][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 00:58:14,545][257371] FPS: 424300.15[0m
[36m[2023-07-17 00:58:14,547][257371] itr=366, itrs=2000, Progress: 18.30%[0m
[36m[2023-07-17 00:58:26,474][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-17 00:58:26,474][257371] FPS: 323865.66[0m
[36m[2023-07-17 00:58:30,771][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:58:30,777][257371] Reward + Measures: [[58.53392255  0.06889167  0.95263362  0.20301965  0.96129298  2.93238139]][0m
[37m[1m[2023-07-17 00:58:30,777][257371] Max Reward on eval: 58.533922554545[0m
[37m[1m[2023-07-17 00:58:30,777][257371] Min Reward on eval: 58.533922554545[0m
[37m[1m[2023-07-17 00:58:30,778][257371] Mean Reward across all agents: 58.533922554545[0m
[37m[1m[2023-07-17 00:58:30,778][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:58:36,019][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:58:36,025][257371] Reward + Measures: [[ -67.95976118    0.3355        0.43880001    0.33119997    0.52319998
     3.83707595]
 [  38.14763138    0.1811        0.64530003    0.31780002    0.6225
     2.73432493]
 [   7.78641158    0.1964        0.2462        0.21300001    0.22230001
     4.76221561]
 ...
 [-120.78732138    0.83389997    0.23630002    0.82660002    0.61149997
     4.86683702]
 [  81.32387353    0.1392        0.86849993    0.27710003    0.89099997
     3.05809546]
 [-100.84793799    0.60939997    0.0778        0.58880007    0.59750003
     6.20339775]][0m
[37m[1m[2023-07-17 00:58:36,025][257371] Max Reward on eval: 140.15954734124244[0m
[37m[1m[2023-07-17 00:58:36,025][257371] Min Reward on eval: -285.9328474976122[0m
[37m[1m[2023-07-17 00:58:36,026][257371] Mean Reward across all agents: -28.197255385501702[0m
[37m[1m[2023-07-17 00:58:36,026][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:58:36,029][257371] mean_value=-341.36917952497515, max_value=332.71571842374874[0m
[37m[1m[2023-07-17 00:58:36,031][257371] New mean coefficients: [[-1.1666799   0.49334872 -0.86288357  0.52070403  3.9797487  -0.31836182]][0m
[37m[1m[2023-07-17 00:58:36,032][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:58:45,080][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 00:58:45,080][257371] FPS: 424502.45[0m
[36m[2023-07-17 00:58:45,083][257371] itr=367, itrs=2000, Progress: 18.35%[0m
[36m[2023-07-17 00:58:56,919][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 00:58:56,920][257371] FPS: 326353.43[0m
[36m[2023-07-17 00:59:01,305][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:59:01,306][257371] Reward + Measures: [[62.72522486  0.066966    0.95000827  0.210234    0.96136403  2.8890233 ]][0m
[37m[1m[2023-07-17 00:59:01,306][257371] Max Reward on eval: 62.72522486391372[0m
[37m[1m[2023-07-17 00:59:01,306][257371] Min Reward on eval: 62.72522486391372[0m
[37m[1m[2023-07-17 00:59:01,306][257371] Mean Reward across all agents: 62.72522486391372[0m
[37m[1m[2023-07-17 00:59:01,307][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:59:06,363][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:59:06,363][257371] Reward + Measures: [[-28.68765187   0.097        0.89899999   0.21330002   0.91310006
    4.52549314]
 [158.21007423   0.2448       0.4161       0.35979998   0.39590001
    2.90318465]
 [-13.66024123   0.30669999   0.4614       0.34309998   0.47610003
    2.80792022]
 ...
 [-26.24241924   0.18719999   0.77159995   0.19910002   0.80180007
    4.26913786]
 [-11.10192903   0.17620002   0.71460003   0.2264       0.74040002
    4.89227819]
 [ 45.48305297   0.2457       0.6512       0.273        0.64239997
    2.95670962]][0m
[37m[1m[2023-07-17 00:59:06,364][257371] Max Reward on eval: 202.3153820162639[0m
[37m[1m[2023-07-17 00:59:06,364][257371] Min Reward on eval: -133.4546017713845[0m
[37m[1m[2023-07-17 00:59:06,364][257371] Mean Reward across all agents: 54.999615976109226[0m
[37m[1m[2023-07-17 00:59:06,364][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:59:06,367][257371] mean_value=-522.0873702720605, max_value=600.9005067590624[0m
[37m[1m[2023-07-17 00:59:06,370][257371] New mean coefficients: [[-0.6773491  -0.04619372  3.2013998   0.9499725   3.2926836  -0.2724298 ]][0m
[37m[1m[2023-07-17 00:59:06,371][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:59:15,470][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 00:59:15,471][257371] FPS: 422067.71[0m
[36m[2023-07-17 00:59:15,473][257371] itr=368, itrs=2000, Progress: 18.40%[0m
[36m[2023-07-17 00:59:27,578][257371] train() took 12.03 seconds to complete[0m
[36m[2023-07-17 00:59:27,578][257371] FPS: 319092.82[0m
[36m[2023-07-17 00:59:31,897][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:59:31,898][257371] Reward + Measures: [[61.49660372  0.05849266  0.95552301  0.21843198  0.96639729  2.88836026]][0m
[37m[1m[2023-07-17 00:59:31,898][257371] Max Reward on eval: 61.49660372245471[0m
[37m[1m[2023-07-17 00:59:31,898][257371] Min Reward on eval: 61.49660372245471[0m
[37m[1m[2023-07-17 00:59:31,898][257371] Mean Reward across all agents: 61.49660372245471[0m
[37m[1m[2023-07-17 00:59:31,899][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:59:36,904][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 00:59:36,910][257371] Reward + Measures: [[ 88.24204103   0.31940001   0.36820003   0.30219999   0.3874
    4.3111124 ]
 [-38.28691985   0.0478       0.8775       0.59360003   0.77150005
    3.30474257]
 [ 70.85428715   0.0532       0.94909996   0.25740001   0.95619994
    3.01904774]
 ...
 [ 63.67879868   0.0296       0.838        0.54820007   0.61990005
    3.04326487]
 [ 58.46575117   0.0397       0.82779998   0.63150001   0.70410007
    2.94156575]
 [ 49.37794921   0.0368       0.94779998   0.37970001   0.96950001
    3.4430263 ]][0m
[37m[1m[2023-07-17 00:59:36,910][257371] Max Reward on eval: 134.88831852450966[0m
[37m[1m[2023-07-17 00:59:36,910][257371] Min Reward on eval: -106.41158391088247[0m
[37m[1m[2023-07-17 00:59:36,911][257371] Mean Reward across all agents: 51.447563613612125[0m
[37m[1m[2023-07-17 00:59:36,911][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 00:59:36,915][257371] mean_value=-157.94284289583624, max_value=177.52266845631502[0m
[37m[1m[2023-07-17 00:59:36,917][257371] New mean coefficients: [[-1.1852117  0.070063   4.5467587  1.169255   3.7158506 -0.5448803]][0m
[37m[1m[2023-07-17 00:59:36,918][257371] Moving the mean solution point...[0m
[36m[2023-07-17 00:59:45,980][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 00:59:45,980][257371] FPS: 423846.61[0m
[36m[2023-07-17 00:59:45,982][257371] itr=369, itrs=2000, Progress: 18.45%[0m
[36m[2023-07-17 00:59:57,652][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 00:59:57,653][257371] FPS: 330974.35[0m
[36m[2023-07-17 01:00:01,975][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:00:01,976][257371] Reward + Measures: [[60.63326064  0.05421167  0.96057695  0.22425334  0.97104371  2.84673071]][0m
[37m[1m[2023-07-17 01:00:01,976][257371] Max Reward on eval: 60.633260644421156[0m
[37m[1m[2023-07-17 01:00:01,976][257371] Min Reward on eval: 60.633260644421156[0m
[37m[1m[2023-07-17 01:00:01,977][257371] Mean Reward across all agents: 60.633260644421156[0m
[37m[1m[2023-07-17 01:00:01,977][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:00:06,991][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:00:06,997][257371] Reward + Measures: [[-24.871955     0.25650001   0.26830003   0.20549999   0.25
    4.18961859]
 [184.1716681    0.33629999   0.38850003   0.27190003   0.35249999
    3.73594737]
 [ 34.3660444    0.37650004   0.35280001   0.34330001   0.35159999
    2.7425344 ]
 ...
 [ 36.46518517   0.35829997   0.43290001   0.2911       0.40709996
    3.02634788]
 [ 54.14587789   0.41459998   0.37760001   0.39749998   0.36719999
    2.93335032]
 [-16.54840308   0.30150002   0.28570002   0.285        0.25709999
    4.32495451]][0m
[37m[1m[2023-07-17 01:00:06,997][257371] Max Reward on eval: 225.65031626557465[0m
[37m[1m[2023-07-17 01:00:06,998][257371] Min Reward on eval: -618.9300480239093[0m
[37m[1m[2023-07-17 01:00:06,998][257371] Mean Reward across all agents: 20.002454493211136[0m
[37m[1m[2023-07-17 01:00:06,998][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:00:07,002][257371] mean_value=-837.9008860142723, max_value=480.9563431357916[0m
[37m[1m[2023-07-17 01:00:07,005][257371] New mean coefficients: [[-1.2402487   0.11473191  3.321867    1.3151706   3.0391     -0.03530949]][0m
[37m[1m[2023-07-17 01:00:07,006][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:00:16,021][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 01:00:16,022][257371] FPS: 426012.11[0m
[36m[2023-07-17 01:00:16,024][257371] itr=370, itrs=2000, Progress: 18.50%[0m
[37m[1m[2023-07-17 01:03:03,866][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000350[0m
[36m[2023-07-17 01:03:16,215][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 01:03:16,215][257371] FPS: 328266.86[0m
[36m[2023-07-17 01:03:20,442][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:03:20,443][257371] Reward + Measures: [[54.07386927  0.04970033  0.96668732  0.22831202  0.97666925  2.84010553]][0m
[37m[1m[2023-07-17 01:03:20,443][257371] Max Reward on eval: 54.07386927424684[0m
[37m[1m[2023-07-17 01:03:20,443][257371] Min Reward on eval: 54.07386927424684[0m
[37m[1m[2023-07-17 01:03:20,443][257371] Mean Reward across all agents: 54.07386927424684[0m
[37m[1m[2023-07-17 01:03:20,444][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:03:25,346][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:03:25,347][257371] Reward + Measures: [[134.27431298   0.44510004   0.75510001   0.54890007   0.76899999
    3.99891663]
 [ 29.68941451   0.26300001   0.61600006   0.3168       0.69160002
    3.3350246 ]
 [  4.1701691    0.4578       0.27020001   0.40309998   0.36570001
    3.63428187]
 ...
 [ 36.840897     0.17889999   0.62200004   0.26439998   0.6275
    3.29596949]
 [  9.91484022   0.2595       0.69399995   0.32670003   0.75040001
    3.54302764]
 [ -2.55646572   0.0997       0.1019       0.1305       0.1027
    5.33363438]][0m
[37m[1m[2023-07-17 01:03:25,347][257371] Max Reward on eval: 447.73087185919286[0m
[37m[1m[2023-07-17 01:03:25,347][257371] Min Reward on eval: -214.60738495644182[0m
[37m[1m[2023-07-17 01:03:25,348][257371] Mean Reward across all agents: 31.688830026658714[0m
[37m[1m[2023-07-17 01:03:25,348][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:03:25,353][257371] mean_value=-368.2309711394643, max_value=304.4954400290702[0m
[37m[1m[2023-07-17 01:03:25,355][257371] New mean coefficients: [[-0.7009632  -0.21662262  5.1694164   0.5329361   2.6686442   0.1021257 ]][0m
[37m[1m[2023-07-17 01:03:25,356][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:03:34,418][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 01:03:34,418][257371] FPS: 423840.52[0m
[36m[2023-07-17 01:03:34,421][257371] itr=371, itrs=2000, Progress: 18.55%[0m
[36m[2023-07-17 01:03:46,153][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 01:03:46,153][257371] FPS: 329287.49[0m
[36m[2023-07-17 01:03:50,432][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:03:50,432][257371] Reward + Measures: [[55.10541804  0.04436     0.96835226  0.23896867  0.97863531  2.876724  ]][0m
[37m[1m[2023-07-17 01:03:50,432][257371] Max Reward on eval: 55.105418040866944[0m
[37m[1m[2023-07-17 01:03:50,433][257371] Min Reward on eval: 55.105418040866944[0m
[37m[1m[2023-07-17 01:03:50,433][257371] Mean Reward across all agents: 55.105418040866944[0m
[37m[1m[2023-07-17 01:03:50,433][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:03:55,448][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:03:55,449][257371] Reward + Measures: [[ 43.04800037   0.1936       0.79809999   0.29710001   0.81860012
    2.5288074 ]
 [-85.86802482   0.30100003   0.61739999   0.33960003   0.68119997
    2.71965718]
 [ 55.48120989   0.26719999   0.39879999   0.34979996   0.41289997
    3.82537127]
 ...
 [-32.20052074   0.3238       0.34319997   0.31990001   0.40799999
    3.23684359]
 [136.60023497   0.59600002   0.465        0.66640002   0.41190001
    3.60263062]
 [  5.03778642   0.2543       0.25730002   0.26729998   0.2579
    4.98144054]][0m
[37m[1m[2023-07-17 01:03:55,449][257371] Max Reward on eval: 507.4171523883939[0m
[37m[1m[2023-07-17 01:03:55,449][257371] Min Reward on eval: -710.2468705166132[0m
[37m[1m[2023-07-17 01:03:55,449][257371] Mean Reward across all agents: 42.07800988767314[0m
[37m[1m[2023-07-17 01:03:55,450][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:03:55,455][257371] mean_value=-387.234024719591, max_value=905.8245253111329[0m
[37m[1m[2023-07-17 01:03:55,458][257371] New mean coefficients: [[-0.91421986 -0.36423182  1.815047    0.4044271   5.190239   -0.63716495]][0m
[37m[1m[2023-07-17 01:03:55,459][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:04:04,530][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 01:04:04,531][257371] FPS: 423375.45[0m
[36m[2023-07-17 01:04:04,533][257371] itr=372, itrs=2000, Progress: 18.60%[0m
[36m[2023-07-17 01:04:16,268][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 01:04:16,269][257371] FPS: 329088.81[0m
[36m[2023-07-17 01:04:20,550][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:04:20,551][257371] Reward + Measures: [[55.19239167  0.041759    0.97052068  0.24537666  0.9804076   2.81090617]][0m
[37m[1m[2023-07-17 01:04:20,551][257371] Max Reward on eval: 55.19239166858028[0m
[37m[1m[2023-07-17 01:04:20,551][257371] Min Reward on eval: 55.19239166858028[0m
[37m[1m[2023-07-17 01:04:20,552][257371] Mean Reward across all agents: 55.19239166858028[0m
[37m[1m[2023-07-17 01:04:20,552][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:04:25,593][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:04:25,593][257371] Reward + Measures: [[ 42.67255645   0.0616       0.94029999   0.24240001   0.95230001
    3.46895003]
 [123.9222145    0.1666       0.70850009   0.28380001   0.74360001
    2.60685968]
 [ 24.29367443   0.22390001   0.53640002   0.2498       0.54689997
    4.6924715 ]
 ...
 [112.20691595   0.36840007   0.51060003   0.39900002   0.44740006
    2.64455533]
 [-23.75312405   0.0596       0.94660008   0.23729999   0.96470004
    4.90256596]
 [  8.89209136   0.05620001   0.95339996   0.25149998   0.97600001
    4.0214777 ]][0m
[37m[1m[2023-07-17 01:04:25,593][257371] Max Reward on eval: 259.36625863346273[0m
[37m[1m[2023-07-17 01:04:25,594][257371] Min Reward on eval: -97.49260806757957[0m
[37m[1m[2023-07-17 01:04:25,594][257371] Mean Reward across all agents: 37.94454759667378[0m
[37m[1m[2023-07-17 01:04:25,594][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:04:25,597][257371] mean_value=-208.01636244521364, max_value=213.92150194600254[0m
[37m[1m[2023-07-17 01:04:25,600][257371] New mean coefficients: [[-1.0585207  -0.17308815  1.6937197  -0.08534378  4.916894   -0.70503676]][0m
[37m[1m[2023-07-17 01:04:25,601][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:04:34,705][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 01:04:34,705][257371] FPS: 421859.58[0m
[36m[2023-07-17 01:04:34,707][257371] itr=373, itrs=2000, Progress: 18.65%[0m
[36m[2023-07-17 01:04:46,746][257371] train() took 11.97 seconds to complete[0m
[36m[2023-07-17 01:04:46,747][257371] FPS: 320757.56[0m
[36m[2023-07-17 01:04:51,019][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:04:51,019][257371] Reward + Measures: [[49.92346809  0.039515    0.97383773  0.24547799  0.98321402  2.73525071]][0m
[37m[1m[2023-07-17 01:04:51,019][257371] Max Reward on eval: 49.92346809446115[0m
[37m[1m[2023-07-17 01:04:51,020][257371] Min Reward on eval: 49.92346809446115[0m
[37m[1m[2023-07-17 01:04:51,020][257371] Mean Reward across all agents: 49.92346809446115[0m
[37m[1m[2023-07-17 01:04:51,020][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:04:56,003][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:04:56,004][257371] Reward + Measures: [[-171.8742559     0.69390005    0.2445        0.69559997    0.72509998
     4.79545975]
 [-390.22493177    0.66839999    0.28200001    0.78410006    0.85459995
     5.37949228]
 [   2.53736212    0.1002        0.92299998    0.1921        0.90439999
     3.42576289]
 ...
 [  40.54853679    0.0332        0.9224        0.33870003    0.94910002
     2.67440844]
 [-201.48643451    0.38639998    0.3001        0.27620003    0.30320001
     4.46270609]
 [ -92.97521748    0.35949999    0.2323        0.2728        0.2411
     4.73589802]][0m
[37m[1m[2023-07-17 01:04:56,004][257371] Max Reward on eval: 293.84036443829535[0m
[37m[1m[2023-07-17 01:04:56,004][257371] Min Reward on eval: -622.7980785215972[0m
[37m[1m[2023-07-17 01:04:56,004][257371] Mean Reward across all agents: -61.76889095508063[0m
[37m[1m[2023-07-17 01:04:56,005][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:04:56,008][257371] mean_value=-302.09588779276635, max_value=263.7177334459976[0m
[37m[1m[2023-07-17 01:04:56,010][257371] New mean coefficients: [[-1.6328788  -0.01051091 -1.0137249  -0.24331893  6.030534   -0.7775696 ]][0m
[37m[1m[2023-07-17 01:04:56,011][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:05:05,068][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 01:05:05,068][257371] FPS: 424067.20[0m
[36m[2023-07-17 01:05:05,071][257371] itr=374, itrs=2000, Progress: 18.70%[0m
[36m[2023-07-17 01:05:16,937][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 01:05:16,937][257371] FPS: 325453.91[0m
[36m[2023-07-17 01:05:21,352][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:05:21,352][257371] Reward + Measures: [[46.38503184  0.04718     0.97157532  0.23547201  0.98272634  2.61491203]][0m
[37m[1m[2023-07-17 01:05:21,352][257371] Max Reward on eval: 46.385031838143746[0m
[37m[1m[2023-07-17 01:05:21,353][257371] Min Reward on eval: 46.385031838143746[0m
[37m[1m[2023-07-17 01:05:21,353][257371] Mean Reward across all agents: 46.385031838143746[0m
[37m[1m[2023-07-17 01:05:21,353][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:05:26,452][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:05:26,453][257371] Reward + Measures: [[ -0.09557378   0.1939       0.7101       0.28839999   0.74119997
    2.86310625]
 [ 84.38678124   0.41510001   0.40830001   0.45140001   0.38280001
    3.94176173]
 [114.83164267   0.33470002   0.50889999   0.38140002   0.47949997
    3.5037446 ]
 ...
 [ 18.82300249   0.28739998   0.31150001   0.27670002   0.2498
    4.22437668]
 [ 30.11011124   0.49390003   0.56030005   0.52969998   0.55179995
    3.55940676]
 [ 60.43121236   0.2631       0.38620004   0.35699999   0.37020001
    4.65605688]][0m
[37m[1m[2023-07-17 01:05:26,453][257371] Max Reward on eval: 177.4171266540885[0m
[37m[1m[2023-07-17 01:05:26,454][257371] Min Reward on eval: -140.04473864920436[0m
[37m[1m[2023-07-17 01:05:26,454][257371] Mean Reward across all agents: 21.979360207508332[0m
[37m[1m[2023-07-17 01:05:26,454][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:05:26,457][257371] mean_value=-422.53276837348164, max_value=353.41917082460384[0m
[37m[1m[2023-07-17 01:05:26,460][257371] New mean coefficients: [[-1.3839164  -0.29308832 -1.1009631  -0.34835368  5.8392344  -0.84695065]][0m
[37m[1m[2023-07-17 01:05:26,461][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:05:35,568][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 01:05:35,568][257371] FPS: 421717.27[0m
[36m[2023-07-17 01:05:35,571][257371] itr=375, itrs=2000, Progress: 18.75%[0m
[36m[2023-07-17 01:05:47,224][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 01:05:47,224][257371] FPS: 331551.64[0m
[36m[2023-07-17 01:05:51,479][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:05:51,484][257371] Reward + Measures: [[41.03920291  0.04883     0.97088605  0.23515934  0.98311931  2.47523832]][0m
[37m[1m[2023-07-17 01:05:51,484][257371] Max Reward on eval: 41.039202909873204[0m
[37m[1m[2023-07-17 01:05:51,485][257371] Min Reward on eval: 41.039202909873204[0m
[37m[1m[2023-07-17 01:05:51,485][257371] Mean Reward across all agents: 41.039202909873204[0m
[37m[1m[2023-07-17 01:05:51,485][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:05:56,488][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:05:56,493][257371] Reward + Measures: [[-106.10673288    0.37569997    0.47760001    0.46880004    0.48090002
     4.44511032]
 [  -9.32872242    0.15650001    0.54049999    0.39769998    0.57010001
     4.26934814]
 [ -45.12719294    0.19320001    0.65170002    0.26750001    0.65619999
     4.66092062]
 ...
 [-129.98360014    0.0506        0.80579996    0.68470001    0.79609996
     5.37118292]
 [  29.10897889    0.1175        0.79430002    0.212         0.79630005
     5.49462414]
 [  49.31600167    0.0445        0.86110002    0.73869997    0.83859998
     5.55325794]][0m
[37m[1m[2023-07-17 01:05:56,494][257371] Max Reward on eval: 217.4280350526795[0m
[37m[1m[2023-07-17 01:05:56,494][257371] Min Reward on eval: -310.4497845478938[0m
[37m[1m[2023-07-17 01:05:56,494][257371] Mean Reward across all agents: -21.272916069572723[0m
[37m[1m[2023-07-17 01:05:56,494][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:05:56,498][257371] mean_value=-611.6249837220731, max_value=236.9514770204512[0m
[37m[1m[2023-07-17 01:05:56,500][257371] New mean coefficients: [[-1.6989429  -0.2791753   1.4463804   0.17384261  4.9225583  -0.6962387 ]][0m
[37m[1m[2023-07-17 01:05:56,501][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:06:05,554][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 01:06:05,554][257371] FPS: 424277.85[0m
[36m[2023-07-17 01:06:05,556][257371] itr=376, itrs=2000, Progress: 18.80%[0m
[36m[2023-07-17 01:06:17,610][257371] train() took 11.99 seconds to complete[0m
[36m[2023-07-17 01:06:17,610][257371] FPS: 320341.29[0m
[36m[2023-07-17 01:06:21,988][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:06:21,989][257371] Reward + Measures: [[37.21852427  0.05180767  0.96845961  0.240684    0.98284596  2.39212275]][0m
[37m[1m[2023-07-17 01:06:21,989][257371] Max Reward on eval: 37.21852426981248[0m
[37m[1m[2023-07-17 01:06:21,989][257371] Min Reward on eval: 37.21852426981248[0m
[37m[1m[2023-07-17 01:06:21,989][257371] Mean Reward across all agents: 37.21852426981248[0m
[37m[1m[2023-07-17 01:06:21,990][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:06:27,215][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:06:27,215][257371] Reward + Measures: [[ 45.06644961   0.1001       0.83090001   0.26449999   0.88840002
    2.573349  ]
 [112.40501361   0.2297       0.35690001   0.26069999   0.31959999
    3.92141461]
 [ 59.19256972   0.11880001   0.89470005   0.35620001   0.93099993
    2.41666889]
 ...
 [104.85988333   0.0786       0.85089999   0.32780001   0.90760005
    2.36778569]
 [ 67.04549582   0.13270001   0.7353       0.2368       0.74929994
    3.08819079]
 [ 31.51505456   0.1427       0.8287999    0.33630002   0.87639999
    2.70035148]][0m
[37m[1m[2023-07-17 01:06:27,215][257371] Max Reward on eval: 165.3891079764813[0m
[37m[1m[2023-07-17 01:06:27,216][257371] Min Reward on eval: -326.98038102374414[0m
[37m[1m[2023-07-17 01:06:27,216][257371] Mean Reward across all agents: 20.241602360023954[0m
[37m[1m[2023-07-17 01:06:27,216][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:06:27,221][257371] mean_value=-175.86616962046347, max_value=456.0881335504558[0m
[37m[1m[2023-07-17 01:06:27,224][257371] New mean coefficients: [[-1.9679487  -0.32229185 -0.6017356  -0.05431487  4.582958   -0.69991755]][0m
[37m[1m[2023-07-17 01:06:27,225][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:06:36,321][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 01:06:36,321][257371] FPS: 422259.97[0m
[36m[2023-07-17 01:06:36,323][257371] itr=377, itrs=2000, Progress: 18.85%[0m
[36m[2023-07-17 01:06:48,040][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 01:06:48,041][257371] FPS: 329609.53[0m
[36m[2023-07-17 01:06:52,297][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:06:52,297][257371] Reward + Measures: [[27.14376719  0.05356833  0.96638364  0.23985201  0.98343927  2.34277534]][0m
[37m[1m[2023-07-17 01:06:52,298][257371] Max Reward on eval: 27.1437671853091[0m
[37m[1m[2023-07-17 01:06:52,298][257371] Min Reward on eval: 27.1437671853091[0m
[37m[1m[2023-07-17 01:06:52,298][257371] Mean Reward across all agents: 27.1437671853091[0m
[37m[1m[2023-07-17 01:06:52,298][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:06:57,288][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:06:57,288][257371] Reward + Measures: [[ 24.92140366   0.26519999   0.84440005   0.25550002   0.84380001
    3.67580199]
 [  8.80413115   0.32600001   0.69169998   0.36880001   0.76049995
    3.47124863]
 [  9.23206394   0.44179997   0.62079996   0.46000001   0.73340005
    3.63022995]
 ...
 [ 65.59227965   0.0972       0.93460006   0.20930003   0.96310008
    2.87350464]
 [-24.35164111   0.0921       0.73119998   0.3536       0.81479996
    3.40842223]
 [ 30.39078846   0.26820001   0.58860004   0.292        0.67329997
    3.61668706]][0m
[37m[1m[2023-07-17 01:06:57,288][257371] Max Reward on eval: 516.7835764869117[0m
[37m[1m[2023-07-17 01:06:57,289][257371] Min Reward on eval: -79.07727299090475[0m
[37m[1m[2023-07-17 01:06:57,289][257371] Mean Reward across all agents: 24.91786729961764[0m
[37m[1m[2023-07-17 01:06:57,289][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:06:57,294][257371] mean_value=-86.09631816725947, max_value=483.44432094065183[0m
[37m[1m[2023-07-17 01:06:57,296][257371] New mean coefficients: [[-2.7200751  -0.07991979 -2.4086168  -0.22833967  3.9211369  -1.0929023 ]][0m
[37m[1m[2023-07-17 01:06:57,297][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:07:06,359][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 01:07:06,359][257371] FPS: 423867.26[0m
[36m[2023-07-17 01:07:06,361][257371] itr=378, itrs=2000, Progress: 18.90%[0m
[36m[2023-07-17 01:07:18,173][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 01:07:18,173][257371] FPS: 327003.59[0m
[36m[2023-07-17 01:07:22,496][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:07:22,496][257371] Reward + Measures: [[19.38624712  0.06815733  0.95438999  0.23447265  0.97645199  2.2435627 ]][0m
[37m[1m[2023-07-17 01:07:22,496][257371] Max Reward on eval: 19.38624711930603[0m
[37m[1m[2023-07-17 01:07:22,497][257371] Min Reward on eval: 19.38624711930603[0m
[37m[1m[2023-07-17 01:07:22,497][257371] Mean Reward across all agents: 19.38624711930603[0m
[37m[1m[2023-07-17 01:07:22,497][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:07:27,478][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:07:27,479][257371] Reward + Measures: [[ 46.41306903   0.0539       0.79800004   0.29980001   0.87589997
    2.65754056]
 [ 83.00054811   0.07380001   0.80340004   0.4639       0.84069997
    3.85716987]
 [140.99841149   0.31990001   0.41260001   0.32269999   0.3725
    6.67050505]
 ...
 [-27.9590951    0.17060001   0.76420003   0.2369       0.7754001
    4.40520859]
 [ 22.12553064   0.24679999   0.57870007   0.2561       0.56099999
    5.35812521]
 [ 15.3978496    0.3389       0.2332       0.2418       0.2613
    3.51149178]][0m
[37m[1m[2023-07-17 01:07:27,479][257371] Max Reward on eval: 356.63499834239485[0m
[37m[1m[2023-07-17 01:07:27,479][257371] Min Reward on eval: -300.7284047062509[0m
[37m[1m[2023-07-17 01:07:27,480][257371] Mean Reward across all agents: 41.3321308600652[0m
[37m[1m[2023-07-17 01:07:27,480][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:07:27,484][257371] mean_value=-169.47931105242998, max_value=482.00462827011944[0m
[37m[1m[2023-07-17 01:07:27,487][257371] New mean coefficients: [[-3.4301732  -0.1297967  -2.883653    0.06825224  5.109888   -1.194226  ]][0m
[37m[1m[2023-07-17 01:07:27,488][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:07:36,525][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 01:07:36,526][257371] FPS: 424951.02[0m
[36m[2023-07-17 01:07:36,528][257371] itr=379, itrs=2000, Progress: 18.95%[0m
[36m[2023-07-17 01:07:48,353][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 01:07:48,353][257371] FPS: 326622.35[0m
[36m[2023-07-17 01:07:52,642][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:07:52,642][257371] Reward + Measures: [[3.12168179 0.102502   0.91227293 0.23232934 0.95136929 2.10333633]][0m
[37m[1m[2023-07-17 01:07:52,643][257371] Max Reward on eval: 3.1216817878548464[0m
[37m[1m[2023-07-17 01:07:52,643][257371] Min Reward on eval: 3.1216817878548464[0m
[37m[1m[2023-07-17 01:07:52,643][257371] Mean Reward across all agents: 3.1216817878548464[0m
[37m[1m[2023-07-17 01:07:52,643][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:07:57,642][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:07:57,642][257371] Reward + Measures: [[-76.9573391    0.20200001   0.44830003   0.3132       0.50690001
    4.84832239]
 [ 24.10628867   0.67049998   0.52389997   0.74949998   0.4883
    3.16510558]
 [ 55.11908978   0.1692       0.8563       0.20760003   0.7877
    2.87812972]
 ...
 [138.19098562   0.42140004   0.55440003   0.47510001   0.51920003
    2.20798635]
 [ 97.57010028   0.2719       0.70069999   0.30669999   0.57540005
    2.73765063]
 [  8.48534377   0.1156       0.94390005   0.1841       0.9285
    2.60485816]][0m
[37m[1m[2023-07-17 01:07:57,643][257371] Max Reward on eval: 278.8563129711896[0m
[37m[1m[2023-07-17 01:07:57,643][257371] Min Reward on eval: -131.3993117980659[0m
[37m[1m[2023-07-17 01:07:57,643][257371] Mean Reward across all agents: 43.69841622672804[0m
[37m[1m[2023-07-17 01:07:57,643][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:07:57,647][257371] mean_value=-515.8650191021568, max_value=320.61541094053206[0m
[37m[1m[2023-07-17 01:07:57,650][257371] New mean coefficients: [[-4.1938972   0.02216424 -0.7852974   1.0300152   5.506059   -1.4206942 ]][0m
[37m[1m[2023-07-17 01:07:57,651][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:08:06,728][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 01:08:06,728][257371] FPS: 423125.26[0m
[36m[2023-07-17 01:08:06,730][257371] itr=380, itrs=2000, Progress: 19.00%[0m
[37m[1m[2023-07-17 01:10:48,637][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000360[0m
[36m[2023-07-17 01:11:00,779][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 01:11:00,779][257371] FPS: 332218.14[0m
[36m[2023-07-17 01:11:04,877][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:11:04,878][257371] Reward + Measures: [[-5.9150688   0.10560567  0.91402102  0.24126765  0.95663595  2.0141108 ]][0m
[37m[1m[2023-07-17 01:11:04,878][257371] Max Reward on eval: -5.915068802801099[0m
[37m[1m[2023-07-17 01:11:04,878][257371] Min Reward on eval: -5.915068802801099[0m
[37m[1m[2023-07-17 01:11:04,879][257371] Mean Reward across all agents: -5.915068802801099[0m
[37m[1m[2023-07-17 01:11:04,879][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:11:09,782][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:11:09,782][257371] Reward + Measures: [[-59.15586737   0.25929999   0.43700001   0.1833       0.41560003
    3.25301433]
 [ 11.42458055   0.15290001   0.81240004   0.26390001   0.83380002
    2.78058314]
 [131.69323797   0.50550002   0.71719998   0.1348       0.63050002
    3.88140678]
 ...
 [ 35.10054763   0.42480001   0.40110001   0.3258       0.3687
    2.13544607]
 [  7.97257711   0.23029999   0.22199999   0.19220001   0.207
    3.65666747]
 [ 19.29037962   0.2221       0.30880004   0.1133       0.28620002
    4.66387701]][0m
[37m[1m[2023-07-17 01:11:09,782][257371] Max Reward on eval: 273.46042399288854[0m
[37m[1m[2023-07-17 01:11:09,783][257371] Min Reward on eval: -261.26877212768886[0m
[37m[1m[2023-07-17 01:11:09,783][257371] Mean Reward across all agents: 18.015432968093567[0m
[37m[1m[2023-07-17 01:11:09,783][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:11:09,788][257371] mean_value=-1267.264834770913, max_value=587.8916850369424[0m
[37m[1m[2023-07-17 01:11:09,790][257371] New mean coefficients: [[-3.103671   -0.20086478 -4.042781   -0.55477715  4.270719   -1.9265511 ]][0m
[37m[1m[2023-07-17 01:11:09,791][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:11:18,902][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 01:11:18,902][257371] FPS: 421559.61[0m
[36m[2023-07-17 01:11:18,905][257371] itr=381, itrs=2000, Progress: 19.05%[0m
[36m[2023-07-17 01:11:30,638][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 01:11:30,639][257371] FPS: 329183.74[0m
[36m[2023-07-17 01:11:34,953][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:11:34,953][257371] Reward + Measures: [[-12.70902811   0.120562     0.89664406   0.23421232   0.94715899
    1.95995212]][0m
[37m[1m[2023-07-17 01:11:34,953][257371] Max Reward on eval: -12.70902810561102[0m
[37m[1m[2023-07-17 01:11:34,954][257371] Min Reward on eval: -12.70902810561102[0m
[37m[1m[2023-07-17 01:11:34,954][257371] Mean Reward across all agents: -12.70902810561102[0m
[37m[1m[2023-07-17 01:11:34,954][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:11:40,000][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:11:40,001][257371] Reward + Measures: [[ 59.8458886    0.33070001   0.43959999   0.28140002   0.4021
    2.90372729]
 [ 34.4934686    0.26910001   0.3256       0.2397       0.30640003
    4.98812294]
 [-15.17969424   0.16150002   0.18260001   0.14470001   0.17470001
    5.60518026]
 ...
 [ 36.03523471   0.46900001   0.36500001   0.52810001   0.72920001
    3.76146126]
 [ -8.89946021   0.16779999   0.82790005   0.20750001   0.85050005
    3.00775385]
 [-16.09905619   0.1476       0.87729996   0.26220003   0.89139998
    2.56805086]][0m
[37m[1m[2023-07-17 01:11:40,001][257371] Max Reward on eval: 111.9977201518137[0m
[37m[1m[2023-07-17 01:11:40,001][257371] Min Reward on eval: -91.76419377618004[0m
[37m[1m[2023-07-17 01:11:40,001][257371] Mean Reward across all agents: 18.680950339787884[0m
[37m[1m[2023-07-17 01:11:40,002][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:11:40,004][257371] mean_value=-424.27657390243314, max_value=144.69848319352798[0m
[37m[1m[2023-07-17 01:11:40,006][257371] New mean coefficients: [[-3.6572776  0.4358142 -1.2301912 -0.1833984  4.2072396 -1.9774476]][0m
[37m[1m[2023-07-17 01:11:40,007][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:11:49,119][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 01:11:49,119][257371] FPS: 421515.61[0m
[36m[2023-07-17 01:11:49,121][257371] itr=382, itrs=2000, Progress: 19.10%[0m
[36m[2023-07-17 01:12:01,255][257371] train() took 12.06 seconds to complete[0m
[36m[2023-07-17 01:12:01,255][257371] FPS: 318299.96[0m
[36m[2023-07-17 01:12:05,603][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:12:05,604][257371] Reward + Measures: [[-22.58693127   0.14690767   0.86783558   0.23765299   0.93613434
    1.89692736]][0m
[37m[1m[2023-07-17 01:12:05,604][257371] Max Reward on eval: -22.58693126552551[0m
[37m[1m[2023-07-17 01:12:05,604][257371] Min Reward on eval: -22.58693126552551[0m
[37m[1m[2023-07-17 01:12:05,605][257371] Mean Reward across all agents: -22.58693126552551[0m
[37m[1m[2023-07-17 01:12:05,605][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:12:10,649][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:12:10,650][257371] Reward + Measures: [[-23.30235596   0.2323       0.63050002   0.33989999   0.76840001
    3.48125625]
 [ -3.52546288   0.0502       0.94110006   0.51330006   0.94510001
    4.74668741]
 [  2.76661429   0.17560001   0.50840002   0.39220002   0.51270002
    5.02346134]
 ...
 [-12.13966072   0.1935       0.63479996   0.20580001   0.68830001
    3.63320279]
 [ 18.1733931    0.12990001   0.83349991   0.29480001   0.8854
    2.89361596]
 [-38.97266887   0.18449999   0.4549       0.34379998   0.47150001
    5.54928732]][0m
[37m[1m[2023-07-17 01:12:10,650][257371] Max Reward on eval: 262.0952816221863[0m
[37m[1m[2023-07-17 01:12:10,650][257371] Min Reward on eval: -158.64723634142427[0m
[37m[1m[2023-07-17 01:12:10,651][257371] Mean Reward across all agents: 9.151064092577789[0m
[37m[1m[2023-07-17 01:12:10,651][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:12:10,655][257371] mean_value=-175.08608763665532, max_value=261.35866057929223[0m
[37m[1m[2023-07-17 01:12:10,658][257371] New mean coefficients: [[-4.1528325   0.5927043   0.07477832  0.66425407  3.9476204  -1.7047545 ]][0m
[37m[1m[2023-07-17 01:12:10,659][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:12:19,665][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 01:12:19,665][257371] FPS: 426450.70[0m
[36m[2023-07-17 01:12:19,668][257371] itr=383, itrs=2000, Progress: 19.15%[0m
[36m[2023-07-17 01:12:31,303][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 01:12:31,303][257371] FPS: 332065.55[0m
[36m[2023-07-17 01:12:35,612][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:12:35,613][257371] Reward + Measures: [[-38.18245871   0.169901     0.85561132   0.2448       0.93131536
    1.80416572]][0m
[37m[1m[2023-07-17 01:12:35,613][257371] Max Reward on eval: -38.182458710988605[0m
[37m[1m[2023-07-17 01:12:35,613][257371] Min Reward on eval: -38.182458710988605[0m
[37m[1m[2023-07-17 01:12:35,614][257371] Mean Reward across all agents: -38.182458710988605[0m
[37m[1m[2023-07-17 01:12:35,614][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:12:40,582][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:12:40,582][257371] Reward + Measures: [[-88.92556297   0.33400002   0.44630003   0.3635       0.4646
    4.71853971]
 [ 12.71026227   0.29659998   0.51300001   0.30629998   0.57620001
    3.80729032]
 [-41.88108901   0.2687       0.51319999   0.3247       0.51560003
    4.30386734]
 ...
 [-43.78566145   0.22230001   0.50879997   0.27449998   0.52270001
    4.39884901]
 [-20.70490885   0.3524       0.4192       0.3698       0.43509999
    4.43162537]
 [ 43.634965     0.25130001   0.54610002   0.25170001   0.45169997
    5.1410346 ]][0m
[37m[1m[2023-07-17 01:12:40,583][257371] Max Reward on eval: 150.25796153247356[0m
[37m[1m[2023-07-17 01:12:40,583][257371] Min Reward on eval: -173.78518197294323[0m
[37m[1m[2023-07-17 01:12:40,583][257371] Mean Reward across all agents: -22.64763826642748[0m
[37m[1m[2023-07-17 01:12:40,583][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:12:40,586][257371] mean_value=-170.50186422748632, max_value=154.83800716353164[0m
[37m[1m[2023-07-17 01:12:40,588][257371] New mean coefficients: [[-3.5489194   0.05003971 -2.9665277  -0.3109312   3.744549   -1.5189197 ]][0m
[37m[1m[2023-07-17 01:12:40,589][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:12:49,670][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 01:12:49,670][257371] FPS: 422933.18[0m
[36m[2023-07-17 01:12:49,673][257371] itr=384, itrs=2000, Progress: 19.20%[0m
[36m[2023-07-17 01:13:01,436][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 01:13:01,437][257371] FPS: 328317.04[0m
[36m[2023-07-17 01:13:05,823][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:13:05,823][257371] Reward + Measures: [[-57.60082246   0.18657      0.83641863   0.24518168   0.92616129
    1.77452826]][0m
[37m[1m[2023-07-17 01:13:05,824][257371] Max Reward on eval: -57.600822460414705[0m
[37m[1m[2023-07-17 01:13:05,824][257371] Min Reward on eval: -57.600822460414705[0m
[37m[1m[2023-07-17 01:13:05,824][257371] Mean Reward across all agents: -57.600822460414705[0m
[37m[1m[2023-07-17 01:13:05,824][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:13:10,902][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:13:10,903][257371] Reward + Measures: [[-10.77372356   0.0721       0.82499999   0.27250001   0.91329998
    2.08060265]
 [-14.46513407   0.20990001   0.22949998   0.26640001   0.21870001
    3.73127365]
 [-32.62474542   0.18910001   0.77929991   0.21560001   0.77100009
    2.76686549]
 ...
 [  7.9408538    0.1005       0.81709999   0.22000001   0.91619998
    2.62487245]
 [ 11.39515488   0.07880001   0.84749997   0.21959999   0.92859995
    2.67845726]
 [-53.09014655   0.1701       0.77160001   0.2414       0.85219997
    2.17850089]][0m
[37m[1m[2023-07-17 01:13:10,903][257371] Max Reward on eval: 92.89504507444799[0m
[37m[1m[2023-07-17 01:13:10,903][257371] Min Reward on eval: -108.61732576135546[0m
[37m[1m[2023-07-17 01:13:10,903][257371] Mean Reward across all agents: -19.166935134565332[0m
[37m[1m[2023-07-17 01:13:10,904][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:13:10,906][257371] mean_value=-355.67709401020323, max_value=294.8741887954511[0m
[37m[1m[2023-07-17 01:13:10,908][257371] New mean coefficients: [[-3.8023171  0.7269498 -1.2134818 -0.3745385  3.689996  -1.1646397]][0m
[37m[1m[2023-07-17 01:13:10,909][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:13:20,027][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 01:13:20,027][257371] FPS: 421252.29[0m
[36m[2023-07-17 01:13:20,029][257371] itr=385, itrs=2000, Progress: 19.25%[0m
[36m[2023-07-17 01:13:31,719][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 01:13:31,720][257371] FPS: 330500.34[0m
[36m[2023-07-17 01:13:36,121][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:13:36,122][257371] Reward + Measures: [[-78.30489085   0.20149833   0.83029002   0.24714966   0.92741501
    1.70171094]][0m
[37m[1m[2023-07-17 01:13:36,122][257371] Max Reward on eval: -78.30489084723432[0m
[37m[1m[2023-07-17 01:13:36,122][257371] Min Reward on eval: -78.30489084723432[0m
[37m[1m[2023-07-17 01:13:36,123][257371] Mean Reward across all agents: -78.30489084723432[0m
[37m[1m[2023-07-17 01:13:36,123][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:13:41,371][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:13:41,377][257371] Reward + Measures: [[-163.98112581    0.15630001    0.56759995    0.30240002    0.7105
     3.66960835]
 [-175.66639257    0.3409        0.61090004    0.32680002    0.69240004
     2.65545249]
 [-123.83112356    0.32829997    0.66279995    0.2823        0.72490001
     3.03836179]
 ...
 [ -90.24996664    0.18170001    0.74840003    0.27080002    0.83950007
     3.74773383]
 [ -48.61628313    0.39000002    0.54259998    0.49390003    0.55670005
     4.34357929]
 [ -16.33050111    0.33740002    0.49049997    0.41249999    0.76980001
     4.76810932]][0m
[37m[1m[2023-07-17 01:13:41,377][257371] Max Reward on eval: 346.7612880975008[0m
[37m[1m[2023-07-17 01:13:41,377][257371] Min Reward on eval: -247.19543409794568[0m
[37m[1m[2023-07-17 01:13:41,378][257371] Mean Reward across all agents: -60.459974152629414[0m
[37m[1m[2023-07-17 01:13:41,378][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:13:41,382][257371] mean_value=-246.201037391607, max_value=375.5705526108225[0m
[37m[1m[2023-07-17 01:13:41,384][257371] New mean coefficients: [[-3.661601    0.44348377 -3.0511918  -1.9445696   3.4296575  -1.0839628 ]][0m
[37m[1m[2023-07-17 01:13:41,385][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:13:50,442][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 01:13:50,442][257371] FPS: 424081.37[0m
[36m[2023-07-17 01:13:50,444][257371] itr=386, itrs=2000, Progress: 19.30%[0m
[36m[2023-07-17 01:14:02,190][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 01:14:02,191][257371] FPS: 328803.96[0m
[36m[2023-07-17 01:14:06,507][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:14:06,507][257371] Reward + Measures: [[-109.80487727    0.21802433    0.80076396    0.24117231    0.91339827
     1.67695534]][0m
[37m[1m[2023-07-17 01:14:06,507][257371] Max Reward on eval: -109.80487726912276[0m
[37m[1m[2023-07-17 01:14:06,508][257371] Min Reward on eval: -109.80487726912276[0m
[37m[1m[2023-07-17 01:14:06,508][257371] Mean Reward across all agents: -109.80487726912276[0m
[37m[1m[2023-07-17 01:14:06,508][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:14:11,540][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:14:11,541][257371] Reward + Measures: [[  18.78952759    0.45019999    0.43460003    0.48899999    0.48709998
     3.21705222]
 [-108.314714      0.7899        0.1788        0.80690002    0.85580009
     4.66232538]
 [-156.43222991    0.86209995    0.12100001    0.84420007    0.85000002
     4.80896044]
 ...
 [  15.88351567    0.1362        0.6789        0.39769998    0.73690003
     5.1168685 ]
 [ -31.94803788    0.34370002    0.52170002    0.52209997    0.68040001
     4.55184174]
 [ -93.37537524    0.3159        0.34400001    0.34560001    0.31470001
     3.33542252]][0m
[37m[1m[2023-07-17 01:14:11,541][257371] Max Reward on eval: 142.13668441260234[0m
[37m[1m[2023-07-17 01:14:11,541][257371] Min Reward on eval: -380.66240976415577[0m
[37m[1m[2023-07-17 01:14:11,542][257371] Mean Reward across all agents: -28.97207410318148[0m
[37m[1m[2023-07-17 01:14:11,542][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:14:11,544][257371] mean_value=-338.45074796625227, max_value=114.99047580408069[0m
[37m[1m[2023-07-17 01:14:11,547][257371] New mean coefficients: [[-3.0527813  0.1655724 -2.6411076 -1.4978728  4.132284  -1.236311 ]][0m
[37m[1m[2023-07-17 01:14:11,548][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:14:20,596][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 01:14:20,597][257371] FPS: 424457.82[0m
[36m[2023-07-17 01:14:20,599][257371] itr=387, itrs=2000, Progress: 19.35%[0m
[36m[2023-07-17 01:14:32,655][257371] train() took 11.98 seconds to complete[0m
[36m[2023-07-17 01:14:32,655][257371] FPS: 320466.49[0m
[36m[2023-07-17 01:14:37,035][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:14:37,035][257371] Reward + Measures: [[-136.71604842    0.23662134    0.77420467    0.23935233    0.90574741
     1.6464752 ]][0m
[37m[1m[2023-07-17 01:14:37,035][257371] Max Reward on eval: -136.7160484202021[0m
[37m[1m[2023-07-17 01:14:37,036][257371] Min Reward on eval: -136.7160484202021[0m
[37m[1m[2023-07-17 01:14:37,036][257371] Mean Reward across all agents: -136.7160484202021[0m
[37m[1m[2023-07-17 01:14:37,036][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:14:42,088][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:14:42,089][257371] Reward + Measures: [[ -53.85577594    0.77800006    0.38530001    0.73830003    0.41409999
     4.97850943]
 [  77.00978405    0.81970006    0.52530003    0.80699998    0.13240001
     5.93637705]
 [-116.60523223    0.65710002    0.5413        0.59020007    0.63609999
     3.71378136]
 ...
 [ -16.60758785    0.54080003    0.5165        0.52460003    0.46680003
     3.47782898]
 [  63.33278306    0.6469        0.43660003    0.64089996    0.1753
     4.90863752]
 [ -62.47138144    0.20149998    0.73410004    0.57910001    0.89209998
     3.64972687]][0m
[37m[1m[2023-07-17 01:14:42,089][257371] Max Reward on eval: 223.9317093530204[0m
[37m[1m[2023-07-17 01:14:42,089][257371] Min Reward on eval: -337.55160763785244[0m
[37m[1m[2023-07-17 01:14:42,090][257371] Mean Reward across all agents: -50.85950033961888[0m
[37m[1m[2023-07-17 01:14:42,090][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:14:42,095][257371] mean_value=-221.38093075412365, max_value=257.1762149038758[0m
[37m[1m[2023-07-17 01:14:42,097][257371] New mean coefficients: [[-2.2579699   0.23058507 -3.620768   -2.4181871   4.1323957  -1.2149148 ]][0m
[37m[1m[2023-07-17 01:14:42,098][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:14:51,197][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 01:14:51,197][257371] FPS: 422127.91[0m
[36m[2023-07-17 01:14:51,199][257371] itr=388, itrs=2000, Progress: 19.40%[0m
[36m[2023-07-17 01:15:02,958][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 01:15:02,958][257371] FPS: 328450.01[0m
[36m[2023-07-17 01:15:07,390][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:15:07,390][257371] Reward + Measures: [[-157.84412212    0.24995467    0.75117767    0.23919034    0.90013307
     1.60420012]][0m
[37m[1m[2023-07-17 01:15:07,391][257371] Max Reward on eval: -157.84412212236415[0m
[37m[1m[2023-07-17 01:15:07,391][257371] Min Reward on eval: -157.84412212236415[0m
[37m[1m[2023-07-17 01:15:07,391][257371] Mean Reward across all agents: -157.84412212236415[0m
[37m[1m[2023-07-17 01:15:07,391][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:15:12,460][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:15:12,461][257371] Reward + Measures: [[-31.64951007   0.18410002   0.5941       0.24290001   0.7026
    3.3498292 ]
 [ 43.41269424   0.34299999   0.49380001   0.27489999   0.42899999
    2.8796196 ]
 [-96.76689244   0.15439999   0.76710004   0.24890001   0.90340006
    2.53605914]
 ...
 [-22.00918833   0.2211       0.60820001   0.2441       0.69320005
    3.59474421]
 [-20.17998346   0.28029999   0.43670002   0.25009999   0.49169999
    2.64416337]
 [-66.69349402   0.34510002   0.56809998   0.32480001   0.59429997
    3.16577506]][0m
[37m[1m[2023-07-17 01:15:12,461][257371] Max Reward on eval: 493.47917604632676[0m
[37m[1m[2023-07-17 01:15:12,461][257371] Min Reward on eval: -228.05008033076302[0m
[37m[1m[2023-07-17 01:15:12,462][257371] Mean Reward across all agents: -26.84790103353077[0m
[37m[1m[2023-07-17 01:15:12,462][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:15:12,466][257371] mean_value=-216.87227087172704, max_value=364.39402473925315[0m
[37m[1m[2023-07-17 01:15:12,468][257371] New mean coefficients: [[-2.7889144   1.0321417  -2.0613203  -2.2149749   3.5003426  -0.69054866]][0m
[37m[1m[2023-07-17 01:15:12,469][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:15:21,696][257371] train() took 9.22 seconds to complete[0m
[36m[2023-07-17 01:15:21,696][257371] FPS: 416277.46[0m
[36m[2023-07-17 01:15:21,698][257371] itr=389, itrs=2000, Progress: 19.45%[0m
[36m[2023-07-17 01:15:33,579][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 01:15:33,579][257371] FPS: 325120.68[0m
[36m[2023-07-17 01:15:37,942][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:15:37,943][257371] Reward + Measures: [[-189.63227271    0.26251134    0.72272003    0.23428902    0.8875494
     1.59227681]][0m
[37m[1m[2023-07-17 01:15:37,943][257371] Max Reward on eval: -189.63227271234112[0m
[37m[1m[2023-07-17 01:15:37,943][257371] Min Reward on eval: -189.63227271234112[0m
[37m[1m[2023-07-17 01:15:37,944][257371] Mean Reward across all agents: -189.63227271234112[0m
[37m[1m[2023-07-17 01:15:37,944][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:15:42,985][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:15:43,038][257371] Reward + Measures: [[-51.0908775    0.1802       0.51070005   0.31100002   0.53400004
    2.89967847]
 [ 11.75150439   0.31729999   0.41619998   0.25230002   0.38359997
    3.6871469 ]
 [ 87.85943996   0.21399999   0.36630002   0.29169998   0.35010001
    3.4592526 ]
 ...
 [ 74.96389244   0.24810003   0.4224       0.30710003   0.44249997
    2.76298428]
 [ 64.22159997   0.24080001   0.31449997   0.24879999   0.26630002
    3.57520151]
 [238.30043516   0.57319999   0.2385       0.56949997   0.634
    4.03250837]][0m
[37m[1m[2023-07-17 01:15:43,038][257371] Max Reward on eval: 267.6551887872629[0m
[37m[1m[2023-07-17 01:15:43,038][257371] Min Reward on eval: -215.94188607851976[0m
[37m[1m[2023-07-17 01:15:43,039][257371] Mean Reward across all agents: 21.31861481768722[0m
[37m[1m[2023-07-17 01:15:43,039][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:15:43,041][257371] mean_value=-796.9909344013126, max_value=149.00690310441928[0m
[37m[1m[2023-07-17 01:15:43,044][257371] New mean coefficients: [[-1.5124924  0.7377427 -1.1929414 -2.2859385  2.9766803 -0.9259578]][0m
[37m[1m[2023-07-17 01:15:43,045][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:15:52,150][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 01:15:52,150][257371] FPS: 421811.60[0m
[36m[2023-07-17 01:15:52,152][257371] itr=390, itrs=2000, Progress: 19.50%[0m
[37m[1m[2023-07-17 01:18:41,995][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000370[0m
[36m[2023-07-17 01:18:54,272][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 01:18:54,272][257371] FPS: 324845.87[0m
[36m[2023-07-17 01:18:58,610][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:18:58,611][257371] Reward + Measures: [[-198.79095405    0.24883132    0.73890525    0.21539234    0.90603834
     1.53187561]][0m
[37m[1m[2023-07-17 01:18:58,611][257371] Max Reward on eval: -198.79095404913483[0m
[37m[1m[2023-07-17 01:18:58,611][257371] Min Reward on eval: -198.79095404913483[0m
[37m[1m[2023-07-17 01:18:58,612][257371] Mean Reward across all agents: -198.79095404913483[0m
[37m[1m[2023-07-17 01:18:58,612][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:19:03,451][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:19:03,452][257371] Reward + Measures: [[ -10.51420089    0.41070005    0.71660006    0.58660001    0.46440002
     4.85876036]
 [ -25.46746845    0.54189998    0.56089997    0.51430005    0.51719999
     3.56053734]
 [ 209.724494      0.78920001    0.52850002    0.86940002    0.85650009
     4.79077721]
 ...
 [ -33.79349739    0.25580001    0.49970004    0.29159999    0.52740002
     3.88207817]
 [ -44.54793714    0.51560003    0.50450003    0.48280001    0.45730001
     4.28175735]
 [-122.42868469    0.15720001    0.81040001    0.81749994    0.81910008
     5.18659449]][0m
[37m[1m[2023-07-17 01:19:03,452][257371] Max Reward on eval: 280.0342758517712[0m
[37m[1m[2023-07-17 01:19:03,452][257371] Min Reward on eval: -415.32459922193084[0m
[37m[1m[2023-07-17 01:19:03,452][257371] Mean Reward across all agents: -33.41525707134733[0m
[37m[1m[2023-07-17 01:19:03,453][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:19:03,459][257371] mean_value=-169.1266524079871, max_value=438.52266632134945[0m
[37m[1m[2023-07-17 01:19:03,462][257371] New mean coefficients: [[-2.0547163   0.6562729  -2.2962618  -2.5252292   2.8399491  -0.67650145]][0m
[37m[1m[2023-07-17 01:19:03,463][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:19:12,444][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 01:19:12,444][257371] FPS: 427653.55[0m
[36m[2023-07-17 01:19:12,447][257371] itr=391, itrs=2000, Progress: 19.55%[0m
[36m[2023-07-17 01:19:24,628][257371] train() took 12.11 seconds to complete[0m
[36m[2023-07-17 01:19:24,628][257371] FPS: 317063.99[0m
[36m[2023-07-17 01:19:29,013][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:19:29,013][257371] Reward + Measures: [[-232.23520466    0.25041133    0.71607095    0.20397733    0.89171159
     1.52929008]][0m
[37m[1m[2023-07-17 01:19:29,013][257371] Max Reward on eval: -232.23520466397966[0m
[37m[1m[2023-07-17 01:19:29,014][257371] Min Reward on eval: -232.23520466397966[0m
[37m[1m[2023-07-17 01:19:29,014][257371] Mean Reward across all agents: -232.23520466397966[0m
[37m[1m[2023-07-17 01:19:29,014][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:19:34,086][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:19:34,087][257371] Reward + Measures: [[-60.68756064   0.25130001   0.6965       0.25900003   0.80219996
    4.36165857]
 [-21.72362409   0.3788       0.48780003   0.32550001   0.583
    3.6173234 ]
 [ 36.69196058   0.22310002   0.27719998   0.2221       0.28430003
    4.38011169]
 ...
 [ 26.79031971   0.294        0.39260003   0.30500001   0.42919999
    3.89823389]
 [-68.54165815   0.176        0.86090004   0.2228       0.92469996
    3.53858876]
 [-44.26155411   0.27579999   0.68149996   0.24620001   0.72609997
    3.44304252]][0m
[37m[1m[2023-07-17 01:19:34,087][257371] Max Reward on eval: 80.5510560868308[0m
[37m[1m[2023-07-17 01:19:34,087][257371] Min Reward on eval: -205.76207542400806[0m
[37m[1m[2023-07-17 01:19:34,088][257371] Mean Reward across all agents: -47.06624970811454[0m
[37m[1m[2023-07-17 01:19:34,088][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:19:34,090][257371] mean_value=-182.6621220291024, max_value=47.3166975019935[0m
[37m[1m[2023-07-17 01:19:34,093][257371] New mean coefficients: [[-2.655577    1.3866773  -2.6272123  -2.8909695   3.4132016  -0.64976645]][0m
[37m[1m[2023-07-17 01:19:34,094][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:19:43,217][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 01:19:43,218][257371] FPS: 420954.28[0m
[36m[2023-07-17 01:19:43,220][257371] itr=392, itrs=2000, Progress: 19.60%[0m
[36m[2023-07-17 01:19:54,986][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 01:19:54,986][257371] FPS: 328399.06[0m
[36m[2023-07-17 01:19:59,290][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:19:59,291][257371] Reward + Measures: [[-258.43388707    0.28136933    0.68283534    0.20670068    0.87993902
     1.51855695]][0m
[37m[1m[2023-07-17 01:19:59,291][257371] Max Reward on eval: -258.4338870658346[0m
[37m[1m[2023-07-17 01:19:59,291][257371] Min Reward on eval: -258.4338870658346[0m
[37m[1m[2023-07-17 01:19:59,291][257371] Mean Reward across all agents: -258.4338870658346[0m
[37m[1m[2023-07-17 01:19:59,292][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:20:04,298][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:20:04,298][257371] Reward + Measures: [[ 36.26821421   0.37390003   0.565        0.39120001   0.64219999
    3.0378809 ]
 [  8.09805866   0.41339999   0.51980001   0.43010002   0.53509998
    2.40121627]
 [  7.35103483   0.33449998   0.32169998   0.33500001   0.30640003
    5.23917389]
 ...
 [ 20.72527416   0.41580001   0.45640001   0.42550001   0.48570004
    2.8557713 ]
 [-40.54527052   0.17300001   0.75270003   0.19660001   0.8544001
    3.50476837]
 [-20.10224544   0.23         0.26890001   0.29550001   0.32969999
    3.22379613]][0m
[37m[1m[2023-07-17 01:20:04,299][257371] Max Reward on eval: 239.20225527975708[0m
[37m[1m[2023-07-17 01:20:04,299][257371] Min Reward on eval: -160.74968860317023[0m
[37m[1m[2023-07-17 01:20:04,299][257371] Mean Reward across all agents: 2.9501420861348104[0m
[37m[1m[2023-07-17 01:20:04,299][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:20:04,302][257371] mean_value=-577.4609253069021, max_value=269.2500498001952[0m
[37m[1m[2023-07-17 01:20:04,304][257371] New mean coefficients: [[-1.9937699   1.4079179  -2.1755676  -2.92683     4.4089103  -0.64157414]][0m
[37m[1m[2023-07-17 01:20:04,305][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:20:13,385][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 01:20:13,385][257371] FPS: 422982.32[0m
[36m[2023-07-17 01:20:13,387][257371] itr=393, itrs=2000, Progress: 19.65%[0m
[36m[2023-07-17 01:20:25,237][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 01:20:25,237][257371] FPS: 325944.98[0m
[36m[2023-07-17 01:20:29,474][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:20:29,475][257371] Reward + Measures: [[-281.65356084    0.29844767    0.66486597    0.20642699    0.87475032
     1.51966035]][0m
[37m[1m[2023-07-17 01:20:29,475][257371] Max Reward on eval: -281.6535608448172[0m
[37m[1m[2023-07-17 01:20:29,475][257371] Min Reward on eval: -281.6535608448172[0m
[37m[1m[2023-07-17 01:20:29,475][257371] Mean Reward across all agents: -281.6535608448172[0m
[37m[1m[2023-07-17 01:20:29,475][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:20:34,578][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:20:34,579][257371] Reward + Measures: [[-55.43382165   0.20560001   0.2026       0.24750002   0.20510001
    3.57676768]
 [ -1.26954745   0.15550001   0.1787       0.20270002   0.1499
    3.9419682 ]
 [-25.67224281   0.21360002   0.53350002   0.53220004   0.56610006
    2.40343595]
 ...
 [-20.9275222    0.2335       0.66350001   0.27680001   0.73299998
    3.24095035]
 [ 56.29555132   0.21269999   0.54040003   0.24889998   0.53619999
    2.55263066]
 [ 43.28242371   0.55910003   0.40330002   0.84539998   0.88640004
    3.31369257]][0m
[37m[1m[2023-07-17 01:20:34,579][257371] Max Reward on eval: 366.6709294423461[0m
[37m[1m[2023-07-17 01:20:34,580][257371] Min Reward on eval: -498.6056117827073[0m
[37m[1m[2023-07-17 01:20:34,580][257371] Mean Reward across all agents: -29.16491698721651[0m
[37m[1m[2023-07-17 01:20:34,580][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:20:34,585][257371] mean_value=-817.6113206065635, max_value=381.14731320902337[0m
[37m[1m[2023-07-17 01:20:34,588][257371] New mean coefficients: [[-1.9143894  1.6820675 -1.0137892 -2.6798825  3.8136098 -0.5079607]][0m
[37m[1m[2023-07-17 01:20:34,589][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:20:43,654][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 01:20:43,654][257371] FPS: 423679.62[0m
[36m[2023-07-17 01:20:43,656][257371] itr=394, itrs=2000, Progress: 19.70%[0m
[36m[2023-07-17 01:20:55,475][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 01:20:55,475][257371] FPS: 326799.09[0m
[36m[2023-07-17 01:20:59,823][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:20:59,824][257371] Reward + Measures: [[-288.3565023     0.30374932    0.67120367    0.20017667    0.88675064
     1.48751545]][0m
[37m[1m[2023-07-17 01:20:59,824][257371] Max Reward on eval: -288.3565023023209[0m
[37m[1m[2023-07-17 01:20:59,824][257371] Min Reward on eval: -288.3565023023209[0m
[37m[1m[2023-07-17 01:20:59,824][257371] Mean Reward across all agents: -288.3565023023209[0m
[37m[1m[2023-07-17 01:20:59,825][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:21:05,102][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:21:05,103][257371] Reward + Measures: [[  2.78479555   0.09640001   0.63510007   0.46760002   0.62290001
    4.01006126]
 [-23.04244272   0.21550003   0.20020001   0.27659997   0.24820001
    3.476336  ]
 [196.35069784   0.2429       0.49250004   0.2383       0.49379998
    5.09080982]
 ...
 [-39.71517632   0.1102       0.72960001   0.5625       0.7949
    5.50116587]
 [ -6.99592809   0.34060001   0.41339999   0.34990004   0.412
    2.46076655]
 [149.67995549   0.16859999   0.54030001   0.36920002   0.4842
    3.02940059]][0m
[37m[1m[2023-07-17 01:21:05,103][257371] Max Reward on eval: 346.6205424653366[0m
[37m[1m[2023-07-17 01:21:05,104][257371] Min Reward on eval: -389.08765795342623[0m
[37m[1m[2023-07-17 01:21:05,104][257371] Mean Reward across all agents: 16.42350079896419[0m
[37m[1m[2023-07-17 01:21:05,104][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:21:05,108][257371] mean_value=-593.037303661973, max_value=279.3651888440763[0m
[37m[1m[2023-07-17 01:21:05,110][257371] New mean coefficients: [[-2.965963    2.5419154   2.1037745  -1.6390984   2.9779708  -0.07771602]][0m
[37m[1m[2023-07-17 01:21:05,111][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:21:14,213][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 01:21:14,213][257371] FPS: 421982.38[0m
[36m[2023-07-17 01:21:14,216][257371] itr=395, itrs=2000, Progress: 19.75%[0m
[36m[2023-07-17 01:21:26,092][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 01:21:26,093][257371] FPS: 325232.20[0m
[36m[2023-07-17 01:21:30,400][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:21:30,401][257371] Reward + Measures: [[-285.22494289    0.31172267    0.68772298    0.19831869    0.89735562
     1.4866147 ]][0m
[37m[1m[2023-07-17 01:21:30,401][257371] Max Reward on eval: -285.2249428850404[0m
[37m[1m[2023-07-17 01:21:30,401][257371] Min Reward on eval: -285.2249428850404[0m
[37m[1m[2023-07-17 01:21:30,402][257371] Mean Reward across all agents: -285.2249428850404[0m
[37m[1m[2023-07-17 01:21:30,402][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:21:35,456][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:21:35,457][257371] Reward + Measures: [[ -89.94831677    0.24800001    0.3901        0.1356        0.44490004
     4.41811323]
 [ -90.16315889    0.28440002    0.46380001    0.24130002    0.52530003
     3.39529777]
 [ -37.36865101    0.21089999    0.17280002    0.1725        0.23270002
     4.43207169]
 ...
 [   0.35828783    0.28620002    0.3502        0.14529999    0.44390002
     3.97172093]
 [-150.74277693    0.30470002    0.34489998    0.10160001    0.45549998
     3.50309634]
 [ -92.17982811    0.33849999    0.352         0.26930001    0.3897
     3.45148253]][0m
[37m[1m[2023-07-17 01:21:35,457][257371] Max Reward on eval: 207.06962201818823[0m
[37m[1m[2023-07-17 01:21:35,457][257371] Min Reward on eval: -236.47604958591984[0m
[37m[1m[2023-07-17 01:21:35,457][257371] Mean Reward across all agents: -25.372279775166945[0m
[37m[1m[2023-07-17 01:21:35,458][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:21:35,460][257371] mean_value=-1214.6917901897816, max_value=120.24553548701785[0m
[37m[1m[2023-07-17 01:21:35,463][257371] New mean coefficients: [[-3.1036716   3.1759095   3.7894554  -0.45697165  2.1287446  -0.06091622]][0m
[37m[1m[2023-07-17 01:21:35,464][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:21:44,561][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 01:21:44,561][257371] FPS: 422171.93[0m
[36m[2023-07-17 01:21:44,563][257371] itr=396, itrs=2000, Progress: 19.80%[0m
[36m[2023-07-17 01:21:56,395][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 01:21:56,395][257371] FPS: 326466.74[0m
[36m[2023-07-17 01:22:00,679][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:22:00,679][257371] Reward + Measures: [[-295.03356429    0.30961701    0.71416372    0.19299234    0.90813231
     1.46807516]][0m
[37m[1m[2023-07-17 01:22:00,679][257371] Max Reward on eval: -295.03356428956715[0m
[37m[1m[2023-07-17 01:22:00,680][257371] Min Reward on eval: -295.03356428956715[0m
[37m[1m[2023-07-17 01:22:00,680][257371] Mean Reward across all agents: -295.03356428956715[0m
[37m[1m[2023-07-17 01:22:00,680][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:22:05,725][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:22:05,726][257371] Reward + Measures: [[-24.6444191    0.36919999   0.35139999   0.37100002   0.53310007
    3.9829464 ]
 [-36.66303851   0.31639999   0.38139999   0.25300002   0.51450002
    3.34023285]
 [-24.62383855   0.13609999   0.12349999   0.11919999   0.1877
    4.35368729]
 ...
 [  5.36525514   0.2237       0.09649999   0.17990001   0.1622
    4.73875523]
 [-14.13745762   0.13680001   0.0655       0.0892       0.1005
    4.82235909]
 [-61.0409984    0.26900002   0.45650002   0.31729999   0.4797
    3.5030098 ]][0m
[37m[1m[2023-07-17 01:22:05,726][257371] Max Reward on eval: 299.0869026146829[0m
[37m[1m[2023-07-17 01:22:05,726][257371] Min Reward on eval: -373.548425231874[0m
[37m[1m[2023-07-17 01:22:05,726][257371] Mean Reward across all agents: -30.75119138306533[0m
[37m[1m[2023-07-17 01:22:05,727][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:22:05,730][257371] mean_value=-1340.5133691505769, max_value=479.0517757354316[0m
[37m[1m[2023-07-17 01:22:05,732][257371] New mean coefficients: [[-2.2146094   2.9108412   2.174334   -0.47534132  2.07939     0.03072222]][0m
[37m[1m[2023-07-17 01:22:05,733][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:22:14,792][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 01:22:14,792][257371] FPS: 423983.32[0m
[36m[2023-07-17 01:22:14,794][257371] itr=397, itrs=2000, Progress: 19.85%[0m
[36m[2023-07-17 01:22:26,542][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 01:22:26,542][257371] FPS: 328777.91[0m
[36m[2023-07-17 01:22:30,905][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:22:30,905][257371] Reward + Measures: [[-298.88579783    0.33808801    0.72638297    0.20169164    0.91465169
     1.45577443]][0m
[37m[1m[2023-07-17 01:22:30,906][257371] Max Reward on eval: -298.8857978301055[0m
[37m[1m[2023-07-17 01:22:30,906][257371] Min Reward on eval: -298.8857978301055[0m
[37m[1m[2023-07-17 01:22:30,906][257371] Mean Reward across all agents: -298.8857978301055[0m
[37m[1m[2023-07-17 01:22:30,906][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:22:35,933][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:22:35,934][257371] Reward + Measures: [[ -56.06553796    0.3001        0.43640003    0.26950002    0.39270002
     3.13537049]
 [ -76.19916252    0.51500005    0.33860001    0.62940001    0.625
     5.39991236]
 [ -16.12131408    0.14740001    0.56260002    0.36710003    0.53750002
     3.34956288]
 ...
 [-157.47890867    0.52750003    0.30640003    0.62230003    0.62050003
     5.32435942]
 [-787.24966813    0.98330003    0.006         0.97559994    0.96920007
     6.90131378]
 [ -53.29228017    0.0265        0.85680002    0.63300002    0.92019999
     4.12263346]][0m
[37m[1m[2023-07-17 01:22:35,934][257371] Max Reward on eval: 636.4006805103272[0m
[37m[1m[2023-07-17 01:22:35,935][257371] Min Reward on eval: -920.9625320498366[0m
[37m[1m[2023-07-17 01:22:35,935][257371] Mean Reward across all agents: -150.51264000655092[0m
[37m[1m[2023-07-17 01:22:35,935][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:22:35,938][257371] mean_value=-445.49542489798023, max_value=532.456531950391[0m
[37m[1m[2023-07-17 01:22:35,940][257371] New mean coefficients: [[-2.4975708   3.0877035   3.6931164   0.22393346  2.029128    0.36090022]][0m
[37m[1m[2023-07-17 01:22:35,941][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:22:45,044][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 01:22:45,045][257371] FPS: 421898.31[0m
[36m[2023-07-17 01:22:45,047][257371] itr=398, itrs=2000, Progress: 19.90%[0m
[36m[2023-07-17 01:22:56,854][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 01:22:56,854][257371] FPS: 327269.70[0m
[36m[2023-07-17 01:23:01,189][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:23:01,190][257371] Reward + Measures: [[-286.7618442     0.33349901    0.75686067    0.19745301    0.92998731
     1.44872558]][0m
[37m[1m[2023-07-17 01:23:01,190][257371] Max Reward on eval: -286.7618441958449[0m
[37m[1m[2023-07-17 01:23:01,190][257371] Min Reward on eval: -286.7618441958449[0m
[37m[1m[2023-07-17 01:23:01,190][257371] Mean Reward across all agents: -286.7618441958449[0m
[37m[1m[2023-07-17 01:23:01,191][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:23:06,290][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:23:06,290][257371] Reward + Measures: [[-25.07207692   0.28         0.43490002   0.25079998   0.4535
    2.95620251]
 [ 88.29357573   0.2325       0.2263       0.25209999   0.18520001
    3.14912677]
 [ 72.59285021   0.1833       0.17349999   0.17660001   0.1564
    3.40759158]
 ...
 [ 25.63771183   0.5413       0.41319999   0.50159997   0.3441
    1.9654144 ]
 [  5.28788372   0.33650002   0.29060003   0.36210003   0.24720001
    4.78792429]
 [-19.27193341   0.3867       0.64310002   0.41549999   0.72420007
    1.66229713]][0m
[37m[1m[2023-07-17 01:23:06,291][257371] Max Reward on eval: 255.52507680691778[0m
[37m[1m[2023-07-17 01:23:06,291][257371] Min Reward on eval: -250.71913146630396[0m
[37m[1m[2023-07-17 01:23:06,291][257371] Mean Reward across all agents: 52.2167438163126[0m
[37m[1m[2023-07-17 01:23:06,291][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:23:06,293][257371] mean_value=-1785.357843700682, max_value=200.08180858886865[0m
[37m[1m[2023-07-17 01:23:06,295][257371] New mean coefficients: [[-2.289297    2.665379    3.283238   -0.4082225   1.1881936   0.38145974]][0m
[37m[1m[2023-07-17 01:23:06,296][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:23:15,388][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 01:23:15,389][257371] FPS: 422428.51[0m
[36m[2023-07-17 01:23:15,391][257371] itr=399, itrs=2000, Progress: 19.95%[0m
[36m[2023-07-17 01:23:27,280][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 01:23:27,280][257371] FPS: 324949.84[0m
[36m[2023-07-17 01:23:31,618][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:23:31,618][257371] Reward + Measures: [[-285.40491898    0.34216601    0.78448939    0.19668502    0.93649697
     1.46625614]][0m
[37m[1m[2023-07-17 01:23:31,619][257371] Max Reward on eval: -285.40491898233535[0m
[37m[1m[2023-07-17 01:23:31,619][257371] Min Reward on eval: -285.40491898233535[0m
[37m[1m[2023-07-17 01:23:31,619][257371] Mean Reward across all agents: -285.40491898233535[0m
[37m[1m[2023-07-17 01:23:31,619][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:23:36,830][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:23:36,835][257371] Reward + Measures: [[-197.13920402    0.19520001    0.5029        0.2872        0.71610004
     4.77905035]
 [ -60.25987084    0.25550002    0.57870001    0.28850001    0.62820005
     5.7470026 ]
 [-103.85547184    0.2617        0.59020007    0.27250001    0.69880003
     5.35595322]
 ...
 [  24.73223693    0.4075        0.33200002    0.4296        0.30739999
     3.12406969]
 [   0.80485993    0.3075        0.50579995    0.2793        0.55779999
     4.62288809]
 [-249.34015538    0.3062        0.56440002    0.23239999    0.69629997
     4.36088943]][0m
[37m[1m[2023-07-17 01:23:36,835][257371] Max Reward on eval: 272.00509550375864[0m
[37m[1m[2023-07-17 01:23:36,836][257371] Min Reward on eval: -316.1850643146783[0m
[37m[1m[2023-07-17 01:23:36,836][257371] Mean Reward across all agents: -30.83183283260993[0m
[37m[1m[2023-07-17 01:23:36,836][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:23:36,841][257371] mean_value=-188.23395119954736, max_value=336.87764754461625[0m
[37m[1m[2023-07-17 01:23:36,843][257371] New mean coefficients: [[-2.776054    3.0856607   1.9957422  -0.87504303  0.25727665  0.7222686 ]][0m
[37m[1m[2023-07-17 01:23:36,844][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:23:45,952][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 01:23:45,952][257371] FPS: 421708.94[0m
[36m[2023-07-17 01:23:45,954][257371] itr=400, itrs=2000, Progress: 20.00%[0m
[37m[1m[2023-07-17 01:26:30,874][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000380[0m
[36m[2023-07-17 01:26:43,158][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 01:26:43,159][257371] FPS: 327481.76[0m
[36m[2023-07-17 01:26:47,333][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:26:47,334][257371] Reward + Measures: [[-332.06944354    0.36491999    0.75987464    0.20123933    0.91360635
     1.48336995]][0m
[37m[1m[2023-07-17 01:26:47,334][257371] Max Reward on eval: -332.0694435377638[0m
[37m[1m[2023-07-17 01:26:47,334][257371] Min Reward on eval: -332.0694435377638[0m
[37m[1m[2023-07-17 01:26:47,335][257371] Mean Reward across all agents: -332.0694435377638[0m
[37m[1m[2023-07-17 01:26:47,335][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:26:52,213][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:26:52,213][257371] Reward + Measures: [[ -46.30701757    0.1724        0.46150002    0.25079998    0.36310002
     4.25323868]
 [-208.08876374    0.1062        0.77680004    0.4513        0.77200001
     4.74635077]
 [  23.46624506    0.35860002    0.46750003    0.38410002    0.47569999
     4.58198833]
 ...
 [  63.32592812    0.12779999    0.3558        0.20369999    0.25270003
     5.16586018]
 [  82.29505177    0.0625        0.87730008    0.52590007    0.8768
     5.38225698]
 [  29.22147989    0.29170004    0.59429997    0.33999997    0.65109998
     2.5226078 ]][0m
[37m[1m[2023-07-17 01:26:52,214][257371] Max Reward on eval: 377.1072931472212[0m
[37m[1m[2023-07-17 01:26:52,214][257371] Min Reward on eval: -419.23978138319217[0m
[37m[1m[2023-07-17 01:26:52,214][257371] Mean Reward across all agents: 0.6782844938338372[0m
[37m[1m[2023-07-17 01:26:52,214][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:26:52,218][257371] mean_value=-339.14608440860155, max_value=709.8430706423283[0m
[37m[1m[2023-07-17 01:26:52,221][257371] New mean coefficients: [[-2.1139028  3.2412512  1.8108755 -1.1003937 -0.3716321  0.5389275]][0m
[37m[1m[2023-07-17 01:26:52,222][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:27:01,155][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 01:27:01,155][257371] FPS: 429935.99[0m
[36m[2023-07-17 01:27:01,158][257371] itr=401, itrs=2000, Progress: 20.05%[0m
[36m[2023-07-17 01:27:12,816][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 01:27:12,816][257371] FPS: 331354.56[0m
[36m[2023-07-17 01:27:17,167][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:27:17,167][257371] Reward + Measures: [[-375.92213361    0.39432731    0.73122329    0.20787533    0.88904798
     1.51509631]][0m
[37m[1m[2023-07-17 01:27:17,167][257371] Max Reward on eval: -375.92213361208644[0m
[37m[1m[2023-07-17 01:27:17,168][257371] Min Reward on eval: -375.92213361208644[0m
[37m[1m[2023-07-17 01:27:17,168][257371] Mean Reward across all agents: -375.92213361208644[0m
[37m[1m[2023-07-17 01:27:17,168][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:27:22,236][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:27:22,237][257371] Reward + Measures: [[ -9.36254907   0.42459998   0.37200001   0.37190002   0.33289999
    2.12737989]
 [147.69564291   0.28910002   0.4567       0.27939999   0.4262
    3.83408284]
 [115.4191325    0.29000002   0.4093       0.3057       0.34019998
    3.00523186]
 ...
 [177.28902387   0.37620002   0.49590001   0.35740003   0.4842
    3.01688552]
 [ 47.0576717    0.1252       0.73030001   0.2976       0.74300003
    4.69256592]
 [ 39.03131818   0.27640003   0.34299999   0.27380002   0.36310002
    3.31845474]][0m
[37m[1m[2023-07-17 01:27:22,237][257371] Max Reward on eval: 257.75961681334303[0m
[37m[1m[2023-07-17 01:27:22,237][257371] Min Reward on eval: -416.7341423265636[0m
[37m[1m[2023-07-17 01:27:22,238][257371] Mean Reward across all agents: 13.527970849831279[0m
[37m[1m[2023-07-17 01:27:22,238][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:27:22,240][257371] mean_value=-881.7269513938999, max_value=146.47242987493433[0m
[37m[1m[2023-07-17 01:27:22,243][257371] New mean coefficients: [[-2.5088878   3.7552605   5.356779    0.26645482 -0.80993754  0.43303925]][0m
[37m[1m[2023-07-17 01:27:22,243][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:27:31,434][257371] train() took 9.19 seconds to complete[0m
[36m[2023-07-17 01:27:31,434][257371] FPS: 417891.47[0m
[36m[2023-07-17 01:27:31,437][257371] itr=402, itrs=2000, Progress: 20.10%[0m
[36m[2023-07-17 01:27:43,285][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 01:27:43,286][257371] FPS: 326077.05[0m
[36m[2023-07-17 01:27:47,686][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:27:47,686][257371] Reward + Measures: [[-387.32884275    0.40704995    0.73462498    0.21171266    0.88351238
     1.53853989]][0m
[37m[1m[2023-07-17 01:27:47,687][257371] Max Reward on eval: -387.32884275383566[0m
[37m[1m[2023-07-17 01:27:47,687][257371] Min Reward on eval: -387.32884275383566[0m
[37m[1m[2023-07-17 01:27:47,687][257371] Mean Reward across all agents: -387.32884275383566[0m
[37m[1m[2023-07-17 01:27:47,687][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:27:52,786][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:27:52,787][257371] Reward + Measures: [[  97.40686796    0.55770004    0.60560006    0.54339999    0.639
     3.91587996]
 [ 170.78679181    0.68709999    0.5079        0.73400003    0.7256
     4.4312439 ]
 [ -69.71357243    0.22839999    0.63829994    0.30170003    0.70489997
     3.90144658]
 ...
 [-191.81431868    0.69399995    0.37070003    0.50520003    0.52069998
     3.21195459]
 [  46.1785433     0.44390002    0.47639999    0.20280002    0.48810002
     3.47922182]
 [  -3.38407731    0.1098        0.8969        0.23459999    0.90049994
     3.2165246 ]][0m
[37m[1m[2023-07-17 01:27:52,787][257371] Max Reward on eval: 218.04145909398795[0m
[37m[1m[2023-07-17 01:27:52,787][257371] Min Reward on eval: -193.6042089626193[0m
[37m[1m[2023-07-17 01:27:52,787][257371] Mean Reward across all agents: 17.73451898109277[0m
[37m[1m[2023-07-17 01:27:52,788][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:27:52,794][257371] mean_value=-127.95715738155364, max_value=366.82379721358876[0m
[37m[1m[2023-07-17 01:27:52,797][257371] New mean coefficients: [[-3.394334    3.4894662   4.5217476  -0.3680886  -0.35762542  0.2923419 ]][0m
[37m[1m[2023-07-17 01:27:52,798][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:28:01,866][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 01:28:01,866][257371] FPS: 423534.10[0m
[36m[2023-07-17 01:28:01,868][257371] itr=403, itrs=2000, Progress: 20.15%[0m
[36m[2023-07-17 01:28:13,735][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 01:28:13,735][257371] FPS: 325642.98[0m
[36m[2023-07-17 01:28:18,146][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:28:18,146][257371] Reward + Measures: [[-396.2744259     0.40438566    0.74777305    0.20920467    0.88462931
     1.53985035]][0m
[37m[1m[2023-07-17 01:28:18,146][257371] Max Reward on eval: -396.27442589551396[0m
[37m[1m[2023-07-17 01:28:18,147][257371] Min Reward on eval: -396.27442589551396[0m
[37m[1m[2023-07-17 01:28:18,147][257371] Mean Reward across all agents: -396.27442589551396[0m
[37m[1m[2023-07-17 01:28:18,147][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:28:23,376][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:28:23,377][257371] Reward + Measures: [[ 51.26133486   0.14659999   0.09940001   0.1277       0.1043
    5.26411009]
 [-60.38566823   0.31389999   0.2958       0.2441       0.30430001
    3.6468842 ]
 [100.32316108   0.21350001   0.1743       0.1983       0.16250001
    4.16539097]
 ...
 [-54.51780119   0.1814       0.18560003   0.1763       0.17920001
    4.21074438]
 [-68.6725825    0.41510001   0.30280003   0.35300002   0.44299999
    3.38283396]
 [-34.68545111   0.26019999   0.51100004   0.2177       0.47950003
    3.25555205]][0m
[37m[1m[2023-07-17 01:28:23,377][257371] Max Reward on eval: 266.4128675924614[0m
[37m[1m[2023-07-17 01:28:23,377][257371] Min Reward on eval: -430.79470914555714[0m
[37m[1m[2023-07-17 01:28:23,377][257371] Mean Reward across all agents: 0.8282301816935845[0m
[37m[1m[2023-07-17 01:28:23,378][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:28:23,379][257371] mean_value=-1778.9727883090586, max_value=17.736144647715932[0m
[37m[1m[2023-07-17 01:28:23,382][257371] New mean coefficients: [[-2.2426333   2.729136    3.7964761  -1.1882546  -0.8939649   0.12242499]][0m
[37m[1m[2023-07-17 01:28:23,383][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:28:32,518][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 01:28:32,518][257371] FPS: 420445.74[0m
[36m[2023-07-17 01:28:32,520][257371] itr=404, itrs=2000, Progress: 20.20%[0m
[36m[2023-07-17 01:28:44,370][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 01:28:44,371][257371] FPS: 326040.72[0m
[36m[2023-07-17 01:28:48,747][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:28:48,748][257371] Reward + Measures: [[-421.58350391    0.42555997    0.73843563    0.21329768    0.86956966
     1.54577947]][0m
[37m[1m[2023-07-17 01:28:48,748][257371] Max Reward on eval: -421.58350390806515[0m
[37m[1m[2023-07-17 01:28:48,748][257371] Min Reward on eval: -421.58350390806515[0m
[37m[1m[2023-07-17 01:28:48,749][257371] Mean Reward across all agents: -421.58350390806515[0m
[37m[1m[2023-07-17 01:28:48,749][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:28:53,769][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:28:53,769][257371] Reward + Measures: [[-113.20791835    0.25980002    0.27550003    0.2287        0.27900001
     4.31763458]
 [-203.48120309    0.34940001    0.27129999    0.25550002    0.39719999
     3.11360002]
 [-103.01720155    0.30410001    0.20650001    0.25979999    0.26519999
     4.06210184]
 ...
 [ -74.12236611    0.19700001    0.26210001    0.2069        0.2472
     4.49625254]
 [-144.24075934    0.38840005    0.47059998    0.28759998    0.5535
     3.21532822]
 [-155.61463309    0.48649999    0.28770003    0.42570001    0.3391
     4.61861181]][0m
[37m[1m[2023-07-17 01:28:53,769][257371] Max Reward on eval: 122.12719243085012[0m
[37m[1m[2023-07-17 01:28:53,770][257371] Min Reward on eval: -310.90926648080347[0m
[37m[1m[2023-07-17 01:28:53,770][257371] Mean Reward across all agents: -92.14779580473653[0m
[37m[1m[2023-07-17 01:28:53,770][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:28:53,772][257371] mean_value=-1748.5295782717833, max_value=222.67340772352026[0m
[37m[1m[2023-07-17 01:28:53,775][257371] New mean coefficients: [[-1.5059675   2.0703192   0.7424052  -2.0548918  -1.3531239  -0.21765092]][0m
[37m[1m[2023-07-17 01:28:53,775][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:29:02,847][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 01:29:02,848][257371] FPS: 423355.12[0m
[36m[2023-07-17 01:29:02,850][257371] itr=405, itrs=2000, Progress: 20.25%[0m
[36m[2023-07-17 01:29:14,611][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 01:29:14,611][257371] FPS: 328425.47[0m
[36m[2023-07-17 01:29:18,933][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:29:18,938][257371] Reward + Measures: [[-458.26883902    0.44505033    0.71141505    0.21715832    0.84137028
     1.52567744]][0m
[37m[1m[2023-07-17 01:29:18,939][257371] Max Reward on eval: -458.26883902181794[0m
[37m[1m[2023-07-17 01:29:18,939][257371] Min Reward on eval: -458.26883902181794[0m
[37m[1m[2023-07-17 01:29:18,939][257371] Mean Reward across all agents: -458.26883902181794[0m
[37m[1m[2023-07-17 01:29:18,940][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:29:23,995][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:29:24,001][257371] Reward + Measures: [[  15.26770135    0.35100001    0.39930001    0.31850001    0.43970004
     6.08576679]
 [  29.43503965    0.2089        0.60739994    0.21870001    0.59140003
     4.92558098]
 [-105.39754628    0.31909999    0.51050001    0.30580002    0.51840001
     5.76794767]
 ...
 [ -22.22731142    0.48260003    0.5248        0.3831        0.4842
     5.76328897]
 [-122.23093797    0.32960001    0.38820001    0.28760001    0.30090001
     3.55611682]
 [-222.66487263    0.28890002    0.51139998    0.22970001    0.49850002
     2.8354404 ]][0m
[37m[1m[2023-07-17 01:29:24,001][257371] Max Reward on eval: 103.49399483897723[0m
[37m[1m[2023-07-17 01:29:24,002][257371] Min Reward on eval: -507.19339753491806[0m
[37m[1m[2023-07-17 01:29:24,002][257371] Mean Reward across all agents: -93.98333797770898[0m
[37m[1m[2023-07-17 01:29:24,002][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:29:24,005][257371] mean_value=-284.3136720612838, max_value=128.2013800671419[0m
[37m[1m[2023-07-17 01:29:24,007][257371] New mean coefficients: [[-2.4065776   2.924737    0.52408874 -1.6171217  -1.4829046  -0.27436477]][0m
[37m[1m[2023-07-17 01:29:24,008][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:29:33,099][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 01:29:33,099][257371] FPS: 422473.43[0m
[36m[2023-07-17 01:29:33,102][257371] itr=406, itrs=2000, Progress: 20.30%[0m
[36m[2023-07-17 01:29:44,803][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 01:29:44,803][257371] FPS: 330254.71[0m
[36m[2023-07-17 01:29:49,157][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:29:49,158][257371] Reward + Measures: [[-380.62478448    0.62840968    0.380844      0.46574435    0.61638433
     3.19107103]][0m
[37m[1m[2023-07-17 01:29:49,158][257371] Max Reward on eval: -380.62478447525706[0m
[37m[1m[2023-07-17 01:29:49,158][257371] Min Reward on eval: -380.62478447525706[0m
[37m[1m[2023-07-17 01:29:49,158][257371] Mean Reward across all agents: -380.62478447525706[0m
[37m[1m[2023-07-17 01:29:49,159][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:29:54,201][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:29:54,201][257371] Reward + Measures: [[-51.78620431   0.51810002   0.32380003   0.51640004   0.3303
    4.71778631]
 [-58.99965698   0.0872       0.07359999   0.0781       0.09680001
    5.39029741]
 [-74.25916931   0.30829999   0.51430005   0.3538       0.48100004
    4.19391775]
 ...
 [-60.56550452   0.59170002   0.57429999   0.51540005   0.5715
    3.57115936]
 [ -2.76779599   0.19340001   0.59370005   0.23360001   0.63519996
    4.90482616]
 [-14.88876374   0.82539999   0.47150001   0.81619996   0.1824
    5.81488323]][0m
[37m[1m[2023-07-17 01:29:54,202][257371] Max Reward on eval: 189.46311468383064[0m
[37m[1m[2023-07-17 01:29:54,202][257371] Min Reward on eval: -484.8820161902928[0m
[37m[1m[2023-07-17 01:29:54,202][257371] Mean Reward across all agents: -40.34446334623651[0m
[37m[1m[2023-07-17 01:29:54,202][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:29:54,206][257371] mean_value=-503.49843198239034, max_value=508.8249100450845[0m
[37m[1m[2023-07-17 01:29:54,209][257371] New mean coefficients: [[-2.9007857   3.5161657   3.3686361  -0.19817042 -0.5240717  -0.41170615]][0m
[37m[1m[2023-07-17 01:29:54,210][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:30:03,336][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 01:30:03,336][257371] FPS: 420861.80[0m
[36m[2023-07-17 01:30:03,338][257371] itr=407, itrs=2000, Progress: 20.35%[0m
[36m[2023-07-17 01:30:15,172][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 01:30:15,172][257371] FPS: 326516.88[0m
[36m[2023-07-17 01:30:19,436][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:30:19,437][257371] Reward + Measures: [[-348.09883858    0.56656301    0.44593567    0.37331137    0.54951602
     2.83025217]][0m
[37m[1m[2023-07-17 01:30:19,437][257371] Max Reward on eval: -348.0988385849138[0m
[37m[1m[2023-07-17 01:30:19,437][257371] Min Reward on eval: -348.0988385849138[0m
[37m[1m[2023-07-17 01:30:19,437][257371] Mean Reward across all agents: -348.0988385849138[0m
[37m[1m[2023-07-17 01:30:19,438][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:30:24,504][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:30:24,504][257371] Reward + Measures: [[  20.72716165    0.8391        0.1026        0.87090009    0.8563
     5.80951834]
 [  36.66356255    0.15949999    0.19600001    0.19149999    0.18990001
     5.43944502]
 [ 310.89987566    0.75409997    0.58830005    0.70290005    0.14380001
     5.27957153]
 ...
 [ 178.7565863     0.69890004    0.68900007    0.69400001    0.1092
     6.07466698]
 [-230.10297682    0.45479998    0.35209998    0.37679997    0.39369997
     4.48173189]
 [ -35.21935704    0.38699999    0.35690001    0.26609999    0.38710001
     4.58945799]][0m
[37m[1m[2023-07-17 01:30:24,504][257371] Max Reward on eval: 493.38458823915573[0m
[37m[1m[2023-07-17 01:30:24,505][257371] Min Reward on eval: -377.497548543755[0m
[37m[1m[2023-07-17 01:30:24,505][257371] Mean Reward across all agents: 61.49242940418313[0m
[37m[1m[2023-07-17 01:30:24,505][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:30:24,513][257371] mean_value=-329.7359577143922, max_value=909.0686073634773[0m
[37m[1m[2023-07-17 01:30:24,516][257371] New mean coefficients: [[-2.4964762   3.8110144   5.0603776  -0.02590916 -2.1645293  -0.48840392]][0m
[37m[1m[2023-07-17 01:30:24,517][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:30:33,545][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 01:30:33,545][257371] FPS: 425407.64[0m
[36m[2023-07-17 01:30:33,547][257371] itr=408, itrs=2000, Progress: 20.40%[0m
[36m[2023-07-17 01:30:45,476][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-17 01:30:45,476][257371] FPS: 323834.22[0m
[36m[2023-07-17 01:30:49,871][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:30:49,876][257371] Reward + Measures: [[-396.89208177    0.57966131    0.55204767    0.27658969    0.49657199
     2.24323916]][0m
[37m[1m[2023-07-17 01:30:49,877][257371] Max Reward on eval: -396.89208177162357[0m
[37m[1m[2023-07-17 01:30:49,877][257371] Min Reward on eval: -396.89208177162357[0m
[37m[1m[2023-07-17 01:30:49,877][257371] Mean Reward across all agents: -396.89208177162357[0m
[37m[1m[2023-07-17 01:30:49,878][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:30:55,141][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:30:55,141][257371] Reward + Measures: [[-95.95560873   0.41619998   0.25240001   0.32480001   0.1793
    3.61271286]
 [ -1.80881123   0.23980001   0.25799999   0.20379999   0.31659999
    4.52708769]
 [-90.14977628   0.24450003   0.25059998   0.18710001   0.29309997
    3.70906615]
 ...
 [ 13.85793841   0.20939998   0.27989998   0.17449999   0.33839998
    4.670403  ]
 [-29.67979635   0.29460001   0.27540001   0.1763       0.2638
    4.24645329]
 [ 32.18429148   0.20479999   0.2362       0.1813       0.2192
    4.48326445]][0m
[37m[1m[2023-07-17 01:30:55,141][257371] Max Reward on eval: 82.64403855465352[0m
[37m[1m[2023-07-17 01:30:55,142][257371] Min Reward on eval: -419.0130423918366[0m
[37m[1m[2023-07-17 01:30:55,142][257371] Mean Reward across all agents: -46.181735569934645[0m
[37m[1m[2023-07-17 01:30:55,142][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:30:55,144][257371] mean_value=-841.8002429069847, max_value=284.92384460272945[0m
[37m[1m[2023-07-17 01:30:55,147][257371] New mean coefficients: [[-1.8887904   3.5480015   5.44467    -0.47237822 -2.0965972  -0.6511426 ]][0m
[37m[1m[2023-07-17 01:30:55,148][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:31:04,298][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-17 01:31:04,298][257371] FPS: 419733.92[0m
[36m[2023-07-17 01:31:04,301][257371] itr=409, itrs=2000, Progress: 20.45%[0m
[36m[2023-07-17 01:31:16,152][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 01:31:16,152][257371] FPS: 325906.54[0m
[36m[2023-07-17 01:31:20,417][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:31:20,417][257371] Reward + Measures: [[-411.78224705    0.58221006    0.56577301    0.26424032    0.50388068
     2.20205593]][0m
[37m[1m[2023-07-17 01:31:20,418][257371] Max Reward on eval: -411.78224704636153[0m
[37m[1m[2023-07-17 01:31:20,418][257371] Min Reward on eval: -411.78224704636153[0m
[37m[1m[2023-07-17 01:31:20,418][257371] Mean Reward across all agents: -411.78224704636153[0m
[37m[1m[2023-07-17 01:31:20,418][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:31:25,410][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:31:25,411][257371] Reward + Measures: [[  49.53549235    0.14989999    0.2845        0.26999998    0.40019998
     3.4419899 ]
 [-205.59717751    0.45120001    0.53389996    0.3053        0.4797
     2.63875198]
 [-314.20423509    0.63260001    0.29130003    0.45190001    0.55160004
     3.63003778]
 ...
 [ -61.22207924    0.18889999    0.1186        0.16140001    0.1833
     5.70155764]
 [-110.59634034    0.28989998    0.34729999    0.24060002    0.33970001
     4.51129627]
 [-275.29954718    0.4131        0.39660001    0.28310001    0.41599998
     2.44236183]][0m
[37m[1m[2023-07-17 01:31:25,411][257371] Max Reward on eval: 177.97902158370707[0m
[37m[1m[2023-07-17 01:31:25,411][257371] Min Reward on eval: -868.2571258646203[0m
[37m[1m[2023-07-17 01:31:25,412][257371] Mean Reward across all agents: -134.8254637022784[0m
[37m[1m[2023-07-17 01:31:25,412][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:31:25,414][257371] mean_value=-870.0914595867856, max_value=57.576700927673755[0m
[37m[1m[2023-07-17 01:31:25,417][257371] New mean coefficients: [[-2.0257897   3.9165218   4.2106524  -0.50625706 -2.2444274  -0.36608118]][0m
[37m[1m[2023-07-17 01:31:25,418][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:31:34,432][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 01:31:34,432][257371] FPS: 426075.76[0m
[36m[2023-07-17 01:31:34,434][257371] itr=410, itrs=2000, Progress: 20.50%[0m
[37m[1m[2023-07-17 01:34:26,529][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000390[0m
[36m[2023-07-17 01:34:38,730][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 01:34:38,730][257371] FPS: 330207.93[0m
[36m[2023-07-17 01:34:42,928][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:34:42,928][257371] Reward + Measures: [[-427.49976994    0.59068161    0.57431364    0.25519234    0.50718504
     2.20882511]][0m
[37m[1m[2023-07-17 01:34:42,928][257371] Max Reward on eval: -427.49976994002776[0m
[37m[1m[2023-07-17 01:34:42,929][257371] Min Reward on eval: -427.49976994002776[0m
[37m[1m[2023-07-17 01:34:42,929][257371] Mean Reward across all agents: -427.49976994002776[0m
[37m[1m[2023-07-17 01:34:42,929][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:34:47,833][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:34:47,834][257371] Reward + Measures: [[ 315.40357017    0.2881        0.38649997    0.28000003    0.2956
     3.85265231]
 [   7.17600188    0.2904        0.41669998    0.36400002    0.47270003
     5.0312562 ]
 [ -35.43129264    0.21269999    0.28500003    0.155         0.26929998
     4.08744287]
 ...
 [ -80.05312422    0.27920002    0.4858        0.28990003    0.49079999
     4.14084435]
 [-140.75153685    0.3933        0.39199999    0.24080001    0.38560003
     2.89579749]
 [  26.90760099    0.30559999    0.45240003    0.24100001    0.42459998
     3.36985779]][0m
[37m[1m[2023-07-17 01:34:47,834][257371] Max Reward on eval: 498.1275959050399[0m
[37m[1m[2023-07-17 01:34:47,835][257371] Min Reward on eval: -396.44362068250774[0m
[37m[1m[2023-07-17 01:34:47,835][257371] Mean Reward across all agents: 49.86006034772575[0m
[37m[1m[2023-07-17 01:34:47,835][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:34:47,838][257371] mean_value=-813.2841183941144, max_value=320.27987206421017[0m
[37m[1m[2023-07-17 01:34:47,840][257371] New mean coefficients: [[-2.631094    4.2892003   6.732394   -0.06000257 -1.7130358  -0.4013942 ]][0m
[37m[1m[2023-07-17 01:34:47,841][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:34:56,845][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 01:34:56,845][257371] FPS: 426575.18[0m
[36m[2023-07-17 01:34:56,847][257371] itr=411, itrs=2000, Progress: 20.55%[0m
[36m[2023-07-17 01:35:08,607][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 01:35:08,607][257371] FPS: 328588.73[0m
[36m[2023-07-17 01:35:12,938][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:35:12,938][257371] Reward + Measures: [[-438.49325108    0.59947437    0.5894323     0.24897367    0.50971901
     2.19713736]][0m
[37m[1m[2023-07-17 01:35:12,939][257371] Max Reward on eval: -438.49325107975864[0m
[37m[1m[2023-07-17 01:35:12,939][257371] Min Reward on eval: -438.49325107975864[0m
[37m[1m[2023-07-17 01:35:12,939][257371] Mean Reward across all agents: -438.49325107975864[0m
[37m[1m[2023-07-17 01:35:12,939][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:35:17,905][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:35:17,911][257371] Reward + Measures: [[-61.41305606   0.39500001   0.47810003   0.40570003   0.50740004
    3.24743915]
 [-65.46306304   0.1164       0.22589998   0.17569999   0.22919999
    5.67597961]
 [ 73.12458864   0.32359999   0.57600003   0.3989       0.68730003
    4.45714521]
 ...
 [ 10.17571091   0.0955       0.33300003   0.30710003   0.2852
    6.53076172]
 [ 78.95454618   0.12609999   0.26719999   0.2053       0.2474
    5.96939278]
 [-51.88101102   0.26620004   0.331        0.30669999   0.3204
    3.9701364 ]][0m
[37m[1m[2023-07-17 01:35:17,911][257371] Max Reward on eval: 368.00992243923247[0m
[37m[1m[2023-07-17 01:35:17,912][257371] Min Reward on eval: -182.47758723674343[0m
[37m[1m[2023-07-17 01:35:17,912][257371] Mean Reward across all agents: 25.02743851672108[0m
[37m[1m[2023-07-17 01:35:17,912][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:35:17,915][257371] mean_value=-429.1943208469035, max_value=417.76313839590694[0m
[37m[1m[2023-07-17 01:35:17,918][257371] New mean coefficients: [[-2.69014     4.0091763   6.9528065  -0.15396985 -0.06280661 -0.95226055]][0m
[37m[1m[2023-07-17 01:35:17,919][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:35:27,024][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 01:35:27,024][257371] FPS: 421833.00[0m
[36m[2023-07-17 01:35:27,026][257371] itr=412, itrs=2000, Progress: 20.60%[0m
[36m[2023-07-17 01:35:38,737][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 01:35:38,737][257371] FPS: 329997.69[0m
[36m[2023-07-17 01:35:43,030][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:35:43,031][257371] Reward + Measures: [[-449.43905798    0.60502636    0.60367334    0.247907      0.51702166
     2.18150806]][0m
[37m[1m[2023-07-17 01:35:43,031][257371] Max Reward on eval: -449.4390579831003[0m
[37m[1m[2023-07-17 01:35:43,031][257371] Min Reward on eval: -449.4390579831003[0m
[37m[1m[2023-07-17 01:35:43,031][257371] Mean Reward across all agents: -449.4390579831003[0m
[37m[1m[2023-07-17 01:35:43,032][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:35:48,246][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:35:48,247][257371] Reward + Measures: [[ 44.05911572   0.30110002   0.2773       0.31990001   0.19350001
    4.02062941]
 [-12.32575362   0.26069999   0.24609999   0.24510001   0.1556
    4.82242966]
 [ 17.08371439   0.22519998   0.2859       0.26210001   0.2208
    4.13995123]
 ...
 [ 24.29075384   0.175        0.2095       0.2066       0.17620002
    4.38860846]
 [ 65.42659137   0.20369999   0.24749999   0.19030002   0.20320001
    3.99308014]
 [ -3.1742729    0.1159       0.11800001   0.13259999   0.1036
    6.0686717 ]][0m
[37m[1m[2023-07-17 01:35:48,247][257371] Max Reward on eval: 108.70777942622081[0m
[37m[1m[2023-07-17 01:35:48,247][257371] Min Reward on eval: -366.81052018897606[0m
[37m[1m[2023-07-17 01:35:48,248][257371] Mean Reward across all agents: -36.24045358072604[0m
[37m[1m[2023-07-17 01:35:48,248][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:35:48,249][257371] mean_value=-1732.4581631988221, max_value=72.20581386252385[0m
[37m[1m[2023-07-17 01:35:48,252][257371] New mean coefficients: [[-2.2901108   3.7944813   5.5115356  -0.84835243 -0.9964707  -0.8771367 ]][0m
[37m[1m[2023-07-17 01:35:48,253][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:35:57,266][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 01:35:57,266][257371] FPS: 426117.58[0m
[36m[2023-07-17 01:35:57,269][257371] itr=413, itrs=2000, Progress: 20.65%[0m
[36m[2023-07-17 01:36:08,960][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 01:36:08,960][257371] FPS: 330514.24[0m
[36m[2023-07-17 01:36:13,307][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:36:13,308][257371] Reward + Measures: [[-457.32459536    0.60708266    0.61483598    0.24592233    0.51628703
     2.17712069]][0m
[37m[1m[2023-07-17 01:36:13,308][257371] Max Reward on eval: -457.3245953582037[0m
[37m[1m[2023-07-17 01:36:13,308][257371] Min Reward on eval: -457.3245953582037[0m
[37m[1m[2023-07-17 01:36:13,309][257371] Mean Reward across all agents: -457.3245953582037[0m
[37m[1m[2023-07-17 01:36:13,309][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:36:18,385][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:36:18,390][257371] Reward + Measures: [[-164.46418812    0.51910001    0.46000001    0.42480001    0.42570001
     3.51108813]
 [ -71.58657572    0.51160002    0.38850001    0.4594        0.36660001
     4.83615828]
 [-110.93729831    0.41249999    0.33960003    0.40489998    0.34920001
     4.01534939]
 ...
 [-119.34440355    0.48789999    0.30309999    0.45249996    0.30500004
     3.03275251]
 [ -50.19901096    0.46370003    0.40780002    0.3978        0.35270002
     3.79068923]
 [-101.12031461    0.44670001    0.42380005    0.41240001    0.41759998
     3.63627124]][0m
[37m[1m[2023-07-17 01:36:18,390][257371] Max Reward on eval: 95.4648667048663[0m
[37m[1m[2023-07-17 01:36:18,391][257371] Min Reward on eval: -520.2825526937959[0m
[37m[1m[2023-07-17 01:36:18,391][257371] Mean Reward across all agents: -85.70940660542982[0m
[37m[1m[2023-07-17 01:36:18,391][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:36:18,393][257371] mean_value=-731.9187043446727, max_value=404.9718561630987[0m
[37m[1m[2023-07-17 01:36:18,396][257371] New mean coefficients: [[-2.9975262  3.9057128  5.224844  -1.4046825  0.7865863 -1.0119803]][0m
[37m[1m[2023-07-17 01:36:18,397][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:36:27,475][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 01:36:27,475][257371] FPS: 423094.76[0m
[36m[2023-07-17 01:36:27,477][257371] itr=414, itrs=2000, Progress: 20.70%[0m
[36m[2023-07-17 01:36:39,366][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 01:36:39,366][257371] FPS: 324962.05[0m
[36m[2023-07-17 01:36:43,765][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:36:43,766][257371] Reward + Measures: [[-455.72776659    0.61232567    0.63218766    0.23907934    0.52455598
     2.19945836]][0m
[37m[1m[2023-07-17 01:36:43,766][257371] Max Reward on eval: -455.727766592983[0m
[37m[1m[2023-07-17 01:36:43,766][257371] Min Reward on eval: -455.727766592983[0m
[37m[1m[2023-07-17 01:36:43,766][257371] Mean Reward across all agents: -455.727766592983[0m
[37m[1m[2023-07-17 01:36:43,767][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:36:48,875][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:36:48,876][257371] Reward + Measures: [[   9.54825963    0.28330001    0.58750004    0.39180002    0.54290003
     2.88983846]
 [ 159.26593732    0.2942        0.48079997    0.30939999    0.39310002
     3.29000926]
 [-113.58314734    0.30470002    0.35790002    0.25550002    0.3256
     3.28728104]
 ...
 [-169.84098863    0.46760002    0.5284        0.34040001    0.55019999
     2.26865458]
 [ -31.47350442    0.34010002    0.3845        0.27169999    0.4127
     3.58683705]
 [ 237.46971605    0.29790002    0.56730002    0.47090003    0.44080001
     3.68023682]][0m
[37m[1m[2023-07-17 01:36:48,876][257371] Max Reward on eval: 317.1249103434384[0m
[37m[1m[2023-07-17 01:36:48,876][257371] Min Reward on eval: -292.28330230927094[0m
[37m[1m[2023-07-17 01:36:48,877][257371] Mean Reward across all agents: -12.293963072876592[0m
[37m[1m[2023-07-17 01:36:48,877][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:36:48,879][257371] mean_value=-1207.2431016346904, max_value=158.88979412645003[0m
[37m[1m[2023-07-17 01:36:48,881][257371] New mean coefficients: [[-2.6089797  3.4140327  4.520258  -2.1134238  2.412335  -1.7837005]][0m
[37m[1m[2023-07-17 01:36:48,882][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:36:58,011][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 01:36:58,012][257371] FPS: 420708.40[0m
[36m[2023-07-17 01:36:58,014][257371] itr=415, itrs=2000, Progress: 20.75%[0m
[36m[2023-07-17 01:37:09,918][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 01:37:09,918][257371] FPS: 324478.06[0m
[36m[2023-07-17 01:37:14,312][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:37:14,312][257371] Reward + Measures: [[-457.43282606    0.62775367    0.64911497    0.23183198    0.53073436
     2.16059399]][0m
[37m[1m[2023-07-17 01:37:14,312][257371] Max Reward on eval: -457.4328260644901[0m
[37m[1m[2023-07-17 01:37:14,312][257371] Min Reward on eval: -457.4328260644901[0m
[37m[1m[2023-07-17 01:37:14,313][257371] Mean Reward across all agents: -457.4328260644901[0m
[37m[1m[2023-07-17 01:37:14,313][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:37:19,410][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:37:19,416][257371] Reward + Measures: [[-53.93226807   0.4955       0.5697       0.40630004   0.55320001
    2.71923232]
 [-53.08113136   0.28400001   0.55510002   0.25530002   0.53520006
    3.27751207]
 [-77.9979817    0.1971       0.35720003   0.14890002   0.32500002
    5.14452171]
 ...
 [-73.80285001   0.2027       0.48460004   0.24749999   0.38530001
    4.15903378]
 [-39.94390634   0.2296       0.45740005   0.25110003   0.40720001
    4.15492916]
 [ 85.73189061   0.27940002   0.45439997   0.33319998   0.45580003
    3.15373349]][0m
[37m[1m[2023-07-17 01:37:19,416][257371] Max Reward on eval: 199.4259996470064[0m
[37m[1m[2023-07-17 01:37:19,416][257371] Min Reward on eval: -385.2355213612318[0m
[37m[1m[2023-07-17 01:37:19,416][257371] Mean Reward across all agents: -19.25993573293[0m
[37m[1m[2023-07-17 01:37:19,417][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:37:19,419][257371] mean_value=-474.7204300955638, max_value=67.16601899193125[0m
[37m[1m[2023-07-17 01:37:19,421][257371] New mean coefficients: [[-4.2298517  3.8952081  3.802343  -1.4626348  3.6058905 -1.2088511]][0m
[37m[1m[2023-07-17 01:37:19,422][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:37:28,671][257371] train() took 9.25 seconds to complete[0m
[36m[2023-07-17 01:37:28,671][257371] FPS: 415250.29[0m
[36m[2023-07-17 01:37:28,674][257371] itr=416, itrs=2000, Progress: 20.80%[0m
[36m[2023-07-17 01:37:40,569][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 01:37:40,569][257371] FPS: 324726.13[0m
[36m[2023-07-17 01:37:44,985][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:37:44,985][257371] Reward + Measures: [[-463.56622148    0.63560665    0.66556531    0.22389334    0.54424632
     2.14336467]][0m
[37m[1m[2023-07-17 01:37:44,985][257371] Max Reward on eval: -463.566221478933[0m
[37m[1m[2023-07-17 01:37:44,986][257371] Min Reward on eval: -463.566221478933[0m
[37m[1m[2023-07-17 01:37:44,986][257371] Mean Reward across all agents: -463.566221478933[0m
[37m[1m[2023-07-17 01:37:44,986][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:37:50,054][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:37:50,055][257371] Reward + Measures: [[  -7.26853804    0.27330002    0.19430001    0.23720001    0.22319999
     4.68267393]
 [-160.48560074    0.25640002    0.52770001    0.47569999    0.58210003
     3.78822112]
 [  12.17659531    0.5467        0.28060001    0.42770001    0.34450001
     3.66945505]
 ...
 [ 114.1749481     0.22409999    0.1592        0.1892        0.17999999
     4.87439013]
 [ -85.58315275    0.18639998    0.31810001    0.3163        0.36870003
     5.24284983]
 [ -70.55180904    0.2402        0.5729        0.44860002    0.59780008
     3.77573204]][0m
[37m[1m[2023-07-17 01:37:50,055][257371] Max Reward on eval: 200.49030863652007[0m
[37m[1m[2023-07-17 01:37:50,055][257371] Min Reward on eval: -312.26288020173376[0m
[37m[1m[2023-07-17 01:37:50,056][257371] Mean Reward across all agents: -52.806707110536166[0m
[37m[1m[2023-07-17 01:37:50,056][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:37:50,058][257371] mean_value=-1355.8621391142542, max_value=42.74796434724988[0m
[37m[1m[2023-07-17 01:37:50,060][257371] New mean coefficients: [[-3.8621364   3.9523616   2.1007612  -1.4668787   3.048616   -0.81195366]][0m
[37m[1m[2023-07-17 01:37:50,061][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:37:59,121][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 01:37:59,122][257371] FPS: 423900.66[0m
[36m[2023-07-17 01:37:59,124][257371] itr=417, itrs=2000, Progress: 20.85%[0m
[36m[2023-07-17 01:38:11,025][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 01:38:11,025][257371] FPS: 324621.55[0m
[36m[2023-07-17 01:38:15,454][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:38:15,454][257371] Reward + Measures: [[-472.87973843    0.64226097    0.67684996    0.21486033    0.55989766
     2.11222577]][0m
[37m[1m[2023-07-17 01:38:15,455][257371] Max Reward on eval: -472.87973843235716[0m
[37m[1m[2023-07-17 01:38:15,455][257371] Min Reward on eval: -472.87973843235716[0m
[37m[1m[2023-07-17 01:38:15,455][257371] Mean Reward across all agents: -472.87973843235716[0m
[37m[1m[2023-07-17 01:38:15,455][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:38:20,714][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:38:20,715][257371] Reward + Measures: [[ -87.943563      0.56120008    0.48359999    0.5061        0.44829997
     2.99255967]
 [-131.32649386    0.37339997    0.3691        0.34980002    0.3662
     3.92404795]
 [  -2.97185014    0.34560001    0.40340003    0.329         0.36019999
     4.6983676 ]
 ...
 [ -69.24791381    0.2617        0.31810001    0.3179        0.31800002
     4.30549383]
 [ -13.91651673    0.43350002    0.45559999    0.42309999    0.42340001
     4.4742074 ]
 [ -98.53437708    0.3892        0.5007        0.34660003    0.40349999
     3.51190114]][0m
[37m[1m[2023-07-17 01:38:20,715][257371] Max Reward on eval: 108.14176420634612[0m
[37m[1m[2023-07-17 01:38:20,716][257371] Min Reward on eval: -451.94853021558373[0m
[37m[1m[2023-07-17 01:38:20,716][257371] Mean Reward across all agents: -113.92426580719047[0m
[37m[1m[2023-07-17 01:38:20,716][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:38:20,718][257371] mean_value=-705.2040762530517, max_value=137.01765792157516[0m
[37m[1m[2023-07-17 01:38:20,720][257371] New mean coefficients: [[-3.0320196  3.6507938  0.7778437 -3.0995107  2.3244772 -1.1055489]][0m
[37m[1m[2023-07-17 01:38:20,721][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:38:29,816][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 01:38:29,817][257371] FPS: 422278.50[0m
[36m[2023-07-17 01:38:29,819][257371] itr=418, itrs=2000, Progress: 20.90%[0m
[36m[2023-07-17 01:38:41,640][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 01:38:41,641][257371] FPS: 326742.72[0m
[36m[2023-07-17 01:38:46,003][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:38:46,003][257371] Reward + Measures: [[-496.20911762    0.64279103    0.68417108    0.20501299    0.57244337
     2.08479953]][0m
[37m[1m[2023-07-17 01:38:46,003][257371] Max Reward on eval: -496.2091176239336[0m
[37m[1m[2023-07-17 01:38:46,004][257371] Min Reward on eval: -496.2091176239336[0m
[37m[1m[2023-07-17 01:38:46,004][257371] Mean Reward across all agents: -496.2091176239336[0m
[37m[1m[2023-07-17 01:38:46,004][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:38:51,052][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:38:51,053][257371] Reward + Measures: [[-128.67254685    0.2624        0.2464        0.25290003    0.2816
     3.8074677 ]
 [ -21.37345615    0.41429996    0.66170001    0.39570004    0.67720002
     4.41211843]
 [ -55.33720301    0.30580002    0.31470001    0.29210001    0.32149997
     3.72331429]
 ...
 [ -95.89786484    0.35960004    0.36789998    0.35970002    0.44600001
     4.13433743]
 [ -52.30557526    0.56090003    0.53109998    0.4646        0.55200005
     4.7467742 ]
 [ -19.09555608    0.3944        0.3858        0.35179996    0.37010002
     2.93019032]][0m
[37m[1m[2023-07-17 01:38:51,053][257371] Max Reward on eval: 190.26445964220912[0m
[37m[1m[2023-07-17 01:38:51,053][257371] Min Reward on eval: -333.46566200014206[0m
[37m[1m[2023-07-17 01:38:51,054][257371] Mean Reward across all agents: -28.973662334929383[0m
[37m[1m[2023-07-17 01:38:51,054][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:38:51,056][257371] mean_value=-1261.7631281794586, max_value=112.86170137418515[0m
[37m[1m[2023-07-17 01:38:51,058][257371] New mean coefficients: [[-3.8999557  3.5173826 -1.4119394 -2.6020055  2.516632  -0.7586428]][0m
[37m[1m[2023-07-17 01:38:51,059][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:39:00,175][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 01:39:00,175][257371] FPS: 421340.76[0m
[36m[2023-07-17 01:39:00,177][257371] itr=419, itrs=2000, Progress: 20.95%[0m
[36m[2023-07-17 01:39:12,055][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 01:39:12,055][257371] FPS: 325248.06[0m
[36m[2023-07-17 01:39:16,428][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:39:16,428][257371] Reward + Measures: [[-532.24401777    0.64900267    0.67057699    0.19966133    0.56978929
     2.0739913 ]][0m
[37m[1m[2023-07-17 01:39:16,429][257371] Max Reward on eval: -532.2440177717215[0m
[37m[1m[2023-07-17 01:39:16,429][257371] Min Reward on eval: -532.2440177717215[0m
[37m[1m[2023-07-17 01:39:16,429][257371] Mean Reward across all agents: -532.2440177717215[0m
[37m[1m[2023-07-17 01:39:16,430][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:39:21,550][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:39:21,550][257371] Reward + Measures: [[ -86.73484317    0.37930003    0.45310003    0.57690001    0.49650002
     3.65194392]
 [ -99.18493092    0.3251        0.44239998    0.54809999    0.41140005
     3.9131906 ]
 [ -70.60501989    0.252         0.4632        0.55229998    0.49610001
     4.55200195]
 ...
 [-145.38870487    0.63199997    0.1481        0.66600001    0.55369997
     4.7678771 ]
 [ -48.20179331    0.48160002    0.22160001    0.43400002    0.3788
     3.2580533 ]
 [-129.19018791    0.27779999    0.33330002    0.28200004    0.35590002
     2.97313428]][0m
[37m[1m[2023-07-17 01:39:21,551][257371] Max Reward on eval: 362.503181449417[0m
[37m[1m[2023-07-17 01:39:21,551][257371] Min Reward on eval: -186.84316826602443[0m
[37m[1m[2023-07-17 01:39:21,551][257371] Mean Reward across all agents: 30.365440710577193[0m
[37m[1m[2023-07-17 01:39:21,551][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:39:21,556][257371] mean_value=-583.1178742000736, max_value=320.6344622387347[0m
[37m[1m[2023-07-17 01:39:21,558][257371] New mean coefficients: [[-5.0896816   3.9864085   0.3285451  -1.6950932   2.5291739  -0.00613081]][0m
[37m[1m[2023-07-17 01:39:21,559][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:39:30,643][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 01:39:30,643][257371] FPS: 422801.85[0m
[36m[2023-07-17 01:39:30,646][257371] itr=420, itrs=2000, Progress: 21.00%[0m
[37m[1m[2023-07-17 01:42:17,031][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000400[0m
[36m[2023-07-17 01:42:29,519][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 01:42:29,519][257371] FPS: 328360.05[0m
[36m[2023-07-17 01:42:33,795][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:42:33,796][257371] Reward + Measures: [[-562.47488961    0.65296996    0.65599632    0.20571932    0.56024063
     2.07895875]][0m
[37m[1m[2023-07-17 01:42:33,796][257371] Max Reward on eval: -562.4748896098332[0m
[37m[1m[2023-07-17 01:42:33,796][257371] Min Reward on eval: -562.4748896098332[0m
[37m[1m[2023-07-17 01:42:33,796][257371] Mean Reward across all agents: -562.4748896098332[0m
[37m[1m[2023-07-17 01:42:33,797][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:42:38,746][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:42:38,746][257371] Reward + Measures: [[-172.9566815     0.57499999    0.4395        0.47          0.4903
     3.96539497]
 [-118.88996382    0.38779998    0.33720002    0.42430001    0.39109999
     3.794312  ]
 [ 221.19802582    0.73560005    0.22360002    0.69660002    0.61630005
     4.57860994]
 ...
 [-139.31312       0.49589998    0.45560002    0.3813        0.45770001
     4.16476393]
 [ -48.12177661    0.22409999    0.2613        0.26499999    0.25940001
     4.03310061]
 [ -80.26271473    0.2895        0.26699999    0.33829999    0.30360001
     4.22183084]][0m
[37m[1m[2023-07-17 01:42:38,746][257371] Max Reward on eval: 326.8597961430438[0m
[37m[1m[2023-07-17 01:42:38,747][257371] Min Reward on eval: -432.7611531917006[0m
[37m[1m[2023-07-17 01:42:38,747][257371] Mean Reward across all agents: -88.66839207030908[0m
[37m[1m[2023-07-17 01:42:38,747][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:42:38,749][257371] mean_value=-621.250533795689, max_value=393.7327341477039[0m
[37m[1m[2023-07-17 01:42:38,752][257371] New mean coefficients: [[-6.030435    4.295025    0.6332066  -1.0546263   2.1033027   0.40551245]][0m
[37m[1m[2023-07-17 01:42:38,753][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:42:47,892][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 01:42:47,893][257371] FPS: 420218.18[0m
[36m[2023-07-17 01:42:47,895][257371] itr=421, itrs=2000, Progress: 21.05%[0m
[36m[2023-07-17 01:42:59,891][257371] train() took 11.92 seconds to complete[0m
[36m[2023-07-17 01:42:59,891][257371] FPS: 322103.52[0m
[36m[2023-07-17 01:43:04,158][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:43:04,158][257371] Reward + Measures: [[-609.69772939    0.64770061    0.6367166     0.21249367    0.54649603
     2.08936596]][0m
[37m[1m[2023-07-17 01:43:04,158][257371] Max Reward on eval: -609.6977293898154[0m
[37m[1m[2023-07-17 01:43:04,158][257371] Min Reward on eval: -609.6977293898154[0m
[37m[1m[2023-07-17 01:43:04,159][257371] Mean Reward across all agents: -609.6977293898154[0m
[37m[1m[2023-07-17 01:43:04,159][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:43:09,332][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:43:09,338][257371] Reward + Measures: [[ -28.76134318    0.56269997    0.33520001    0.4059        0.33510002
     3.14863253]
 [-387.12047574    0.43729997    0.2694        0.29960001    0.31949997
     3.96496201]
 [  35.75519918    0.53640002    0.37530002    0.46709999    0.34660003
     3.06425953]
 ...
 [ -18.63785347    0.53299999    0.32160002    0.38699999    0.34040001
     3.76886821]
 [ -87.53583713    0.26859999    0.3091        0.17930001    0.34580001
     4.12891197]
 [-225.40071106    0.30219999    0.27680001    0.21539998    0.3179
     4.58162403]][0m
[37m[1m[2023-07-17 01:43:09,338][257371] Max Reward on eval: 93.7532983297715[0m
[37m[1m[2023-07-17 01:43:09,339][257371] Min Reward on eval: -459.1220025897026[0m
[37m[1m[2023-07-17 01:43:09,339][257371] Mean Reward across all agents: -99.07990648945682[0m
[37m[1m[2023-07-17 01:43:09,339][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:43:09,341][257371] mean_value=-898.8188415319323, max_value=111.786879602526[0m
[37m[1m[2023-07-17 01:43:09,344][257371] New mean coefficients: [[-5.9135437   3.8771377   1.9397615  -0.8256013   3.6709452   0.03961605]][0m
[37m[1m[2023-07-17 01:43:09,345][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:43:18,317][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 01:43:18,317][257371] FPS: 428086.98[0m
[36m[2023-07-17 01:43:18,319][257371] itr=422, itrs=2000, Progress: 21.10%[0m
[36m[2023-07-17 01:43:30,026][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 01:43:30,026][257371] FPS: 330092.15[0m
[36m[2023-07-17 01:43:34,300][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:43:34,301][257371] Reward + Measures: [[-656.74584085    0.64387631    0.63059264    0.20895399    0.55108064
     2.0818646 ]][0m
[37m[1m[2023-07-17 01:43:34,301][257371] Max Reward on eval: -656.7458408483312[0m
[37m[1m[2023-07-17 01:43:34,301][257371] Min Reward on eval: -656.7458408483312[0m
[37m[1m[2023-07-17 01:43:34,301][257371] Mean Reward across all agents: -656.7458408483312[0m
[37m[1m[2023-07-17 01:43:34,302][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:43:39,329][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:43:39,330][257371] Reward + Measures: [[-84.80737541   0.12150001   0.16650002   0.1696       0.17930001
    5.00955868]
 [ 41.46495512   0.2174       0.227        0.24870001   0.20399998
    3.76442194]
 [-99.91985108   0.2969       0.34369999   0.3285       0.30630001
    2.77326083]
 ...
 [ 13.54893837   0.21200001   0.2237       0.2376       0.2066
    3.9356091 ]
 [-38.92829412   0.19710001   0.20550001   0.19860001   0.22379999
    3.99773288]
 [ 17.99014289   0.26500002   0.26999998   0.2899       0.21350001
    3.67024302]][0m
[37m[1m[2023-07-17 01:43:39,330][257371] Max Reward on eval: 91.26279230061918[0m
[37m[1m[2023-07-17 01:43:39,330][257371] Min Reward on eval: -225.54807226918638[0m
[37m[1m[2023-07-17 01:43:39,331][257371] Mean Reward across all agents: -44.33515268504426[0m
[37m[1m[2023-07-17 01:43:39,331][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:43:39,332][257371] mean_value=-2876.6357999218135, max_value=-75.20072035833408[0m
[36m[2023-07-17 01:43:39,335][257371] XNES is restarting with a new solution whose measures are [0.84350008 0.82889998 0.0513     0.85799998 7.52081919] and objective is 464.02645868361[0m
[36m[2023-07-17 01:43:39,336][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 01:43:39,338][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 01:43:39,339][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:43:48,395][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 01:43:48,395][257371] FPS: 424096.06[0m
[36m[2023-07-17 01:43:48,397][257371] itr=423, itrs=2000, Progress: 21.15%[0m
[36m[2023-07-17 01:44:00,044][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 01:44:00,044][257371] FPS: 331685.37[0m
[36m[2023-07-17 01:44:04,326][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:44:04,326][257371] Reward + Measures: [[14.14250885  0.68526804  0.70168328  0.04410533  0.73348093  7.38380051]][0m
[37m[1m[2023-07-17 01:44:04,326][257371] Max Reward on eval: 14.142508852239455[0m
[37m[1m[2023-07-17 01:44:04,327][257371] Min Reward on eval: 14.142508852239455[0m
[37m[1m[2023-07-17 01:44:04,327][257371] Mean Reward across all agents: 14.142508852239455[0m
[37m[1m[2023-07-17 01:44:04,327][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:44:09,313][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:44:09,313][257371] Reward + Measures: [[ 89.02626758   0.73349994   0.75189996   0.0232       0.77890009
    7.48647404]
 [ 52.6332604    0.57460004   0.61350006   0.0493       0.66399997
    7.14964914]
 [  5.56792967   0.46650001   0.50060004   0.07620001   0.54469997
    7.08959532]
 ...
 [-61.58199955   0.54560006   0.55800003   0.0418       0.59719998
    7.23697042]
 [ 34.89989843   0.62690002   0.66420001   0.0473       0.71539998
    7.19150257]
 [-95.43149449   0.48980004   0.52360004   0.0564       0.56020004
    7.17648077]][0m
[37m[1m[2023-07-17 01:44:09,314][257371] Max Reward on eval: 312.9719845909625[0m
[37m[1m[2023-07-17 01:44:09,314][257371] Min Reward on eval: -95.43149448614568[0m
[37m[1m[2023-07-17 01:44:09,314][257371] Mean Reward across all agents: 22.52348992495419[0m
[37m[1m[2023-07-17 01:44:09,314][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:44:09,316][257371] mean_value=-275.33538797640966, max_value=209.72684951233447[0m
[37m[1m[2023-07-17 01:44:09,318][257371] New mean coefficients: [[ 0.773851   -0.98127836 -0.02272248 -2.2907827  -0.938923   -1.1833239 ]][0m
[37m[1m[2023-07-17 01:44:09,319][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:44:18,373][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 01:44:18,373][257371] FPS: 424195.51[0m
[36m[2023-07-17 01:44:18,376][257371] itr=424, itrs=2000, Progress: 21.20%[0m
[36m[2023-07-17 01:44:30,110][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 01:44:30,110][257371] FPS: 329175.50[0m
[36m[2023-07-17 01:44:34,382][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:44:34,383][257371] Reward + Measures: [[4.851531   0.513192   0.55479437 0.05354866 0.5984413  7.12777424]][0m
[37m[1m[2023-07-17 01:44:34,383][257371] Max Reward on eval: 4.851531001980087[0m
[37m[1m[2023-07-17 01:44:34,383][257371] Min Reward on eval: 4.851531001980087[0m
[37m[1m[2023-07-17 01:44:34,383][257371] Mean Reward across all agents: 4.851531001980087[0m
[37m[1m[2023-07-17 01:44:34,384][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:44:39,440][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:44:39,440][257371] Reward + Measures: [[ -8.66281525   0.60560006   0.63810003   0.04720001   0.6947
    7.14787912]
 [107.11715603   0.4093       0.48200002   0.0779       0.53140002
    6.9179635 ]
 [ 70.40267497   0.40120003   0.43319997   0.0719       0.49569997
    6.94807291]
 ...
 [ 48.42838686   0.50450003   0.53949994   0.0551       0.61610001
    6.9729538 ]
 [-64.5678671    0.65639997   0.67059994   0.0271       0.704
    7.4580493 ]
 [-27.45438472   0.4729       0.50940001   0.0478       0.56480002
    7.0759201 ]][0m
[37m[1m[2023-07-17 01:44:39,441][257371] Max Reward on eval: 150.1441269164905[0m
[37m[1m[2023-07-17 01:44:39,441][257371] Min Reward on eval: -115.96355878785252[0m
[37m[1m[2023-07-17 01:44:39,441][257371] Mean Reward across all agents: 18.334424310158493[0m
[37m[1m[2023-07-17 01:44:39,441][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:44:39,443][257371] mean_value=-261.8197412668208, max_value=193.19478632724486[0m
[37m[1m[2023-07-17 01:44:39,445][257371] New mean coefficients: [[ 1.057971   -0.41643292  0.10425623 -1.878227   -1.8133302  -1.5083933 ]][0m
[37m[1m[2023-07-17 01:44:39,446][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:44:48,483][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 01:44:48,483][257371] FPS: 424998.60[0m
[36m[2023-07-17 01:44:48,485][257371] itr=425, itrs=2000, Progress: 21.25%[0m
[36m[2023-07-17 01:45:00,152][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 01:45:00,152][257371] FPS: 331138.88[0m
[36m[2023-07-17 01:45:04,453][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:45:04,454][257371] Reward + Measures: [[26.50634803  0.40532699  0.44533032  0.06427966  0.49175802  6.95893764]][0m
[37m[1m[2023-07-17 01:45:04,454][257371] Max Reward on eval: 26.506348027998445[0m
[37m[1m[2023-07-17 01:45:04,454][257371] Min Reward on eval: 26.506348027998445[0m
[37m[1m[2023-07-17 01:45:04,455][257371] Mean Reward across all agents: 26.506348027998445[0m
[37m[1m[2023-07-17 01:45:04,455][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:45:09,624][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:45:09,624][257371] Reward + Measures: [[ 28.13982528   0.35690001   0.39180002   0.08780001   0.42460003
    6.80858183]
 [ 57.3736081    0.35909998   0.42039996   0.0497       0.45239997
    6.91330194]
 [-34.19061757   0.3231       0.35500002   0.0794       0.39820004
    6.8927393 ]
 ...
 [ 65.33644344   0.39669999   0.44420001   0.0874       0.48470002
    6.90055227]
 [ 47.28373298   0.40890002   0.41400003   0.0502       0.48610002
    7.07448959]
 [ 36.15628148   0.34549999   0.40620002   0.0502       0.43490002
    6.93080235]][0m
[37m[1m[2023-07-17 01:45:09,625][257371] Max Reward on eval: 129.98164630625396[0m
[37m[1m[2023-07-17 01:45:09,625][257371] Min Reward on eval: -74.35603999681771[0m
[37m[1m[2023-07-17 01:45:09,625][257371] Mean Reward across all agents: 37.98164567491175[0m
[37m[1m[2023-07-17 01:45:09,625][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:45:09,627][257371] mean_value=-240.82945012134394, max_value=-66.03016981884171[0m
[36m[2023-07-17 01:45:09,629][257371] XNES is restarting with a new solution whose measures are [0.66050005 0.49719998 0.67039996 0.43969998 3.82381248] and objective is 88.04668596160482[0m
[36m[2023-07-17 01:45:09,630][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 01:45:09,632][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 01:45:09,633][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:45:18,859][257371] train() took 9.22 seconds to complete[0m
[36m[2023-07-17 01:45:18,859][257371] FPS: 416299.51[0m
[36m[2023-07-17 01:45:18,862][257371] itr=426, itrs=2000, Progress: 21.30%[0m
[36m[2023-07-17 01:45:30,721][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 01:45:30,722][257371] FPS: 325795.56[0m
[36m[2023-07-17 01:45:35,056][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:45:35,056][257371] Reward + Measures: [[42.81429694  0.54791266  0.44000995  0.546395    0.34263232  3.3846283 ]][0m
[37m[1m[2023-07-17 01:45:35,057][257371] Max Reward on eval: 42.814296936797625[0m
[37m[1m[2023-07-17 01:45:35,057][257371] Min Reward on eval: 42.814296936797625[0m
[37m[1m[2023-07-17 01:45:35,057][257371] Mean Reward across all agents: 42.814296936797625[0m
[37m[1m[2023-07-17 01:45:35,058][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:45:40,297][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:45:40,298][257371] Reward + Measures: [[128.90337299   0.24620001   0.40060002   0.30940002   0.31210002
    4.09663773]
 [  9.45200378   0.42810002   0.41859999   0.352        0.33559999
    5.13120794]
 [ 28.42857195   0.22810002   0.15580001   0.2536       0.1741
    4.9862957 ]
 ...
 [ 28.49598863   0.46440002   0.2904       0.30000001   0.42379999
    5.1843133 ]
 [-69.66918093   0.28840002   0.24080001   0.28060001   0.2597
    4.48998499]
 [ 59.86762444   0.72890007   0.70419997   0.74200004   0.82610005
    6.22036314]][0m
[37m[1m[2023-07-17 01:45:40,298][257371] Max Reward on eval: 224.84596061799675[0m
[37m[1m[2023-07-17 01:45:40,298][257371] Min Reward on eval: -225.41801070868968[0m
[37m[1m[2023-07-17 01:45:40,299][257371] Mean Reward across all agents: -15.447160474489946[0m
[37m[1m[2023-07-17 01:45:40,299][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:45:40,302][257371] mean_value=-318.2301739895405, max_value=375.3812502753659[0m
[37m[1m[2023-07-17 01:45:40,305][257371] New mean coefficients: [[ 1.6352578 -1.090267  -1.7208767 -1.3816779 -1.8189945 -0.5716357]][0m
[37m[1m[2023-07-17 01:45:40,306][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:45:49,345][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 01:45:49,345][257371] FPS: 424903.95[0m
[36m[2023-07-17 01:45:49,347][257371] itr=427, itrs=2000, Progress: 21.35%[0m
[36m[2023-07-17 01:46:01,150][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 01:46:01,150][257371] FPS: 327316.40[0m
[36m[2023-07-17 01:46:05,505][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:46:05,505][257371] Reward + Measures: [[-14.93400089   0.56259364   0.40195736   0.57426703   0.27915233
    4.02900743]][0m
[37m[1m[2023-07-17 01:46:05,505][257371] Max Reward on eval: -14.934000885778689[0m
[37m[1m[2023-07-17 01:46:05,506][257371] Min Reward on eval: -14.934000885778689[0m
[37m[1m[2023-07-17 01:46:05,506][257371] Mean Reward across all agents: -14.934000885778689[0m
[37m[1m[2023-07-17 01:46:05,506][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:46:10,593][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:46:10,594][257371] Reward + Measures: [[ 62.7144719    0.1219       0.14140001   0.20159999   0.13259999
    5.77216053]
 [-40.35245102   0.09240001   0.23120001   0.17490001   0.22119999
    5.52578068]
 [ 24.42769975   0.17659999   0.1418       0.13699999   0.1741
    6.15306425]
 ...
 [ -2.58712872   0.155        0.16540001   0.1322       0.13420001
    5.80234098]
 [ 50.26527382   0.27579999   0.359        0.3127       0.2076
    5.96032572]
 [-63.34675699   0.1247       0.25940001   0.2089       0.19429998
    5.39652205]][0m
[37m[1m[2023-07-17 01:46:10,594][257371] Max Reward on eval: 148.4083225639537[0m
[37m[1m[2023-07-17 01:46:10,594][257371] Min Reward on eval: -112.55369328921661[0m
[37m[1m[2023-07-17 01:46:10,594][257371] Mean Reward across all agents: 6.508891887940786[0m
[37m[1m[2023-07-17 01:46:10,595][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:46:10,597][257371] mean_value=-550.2113026758012, max_value=116.08923364795672[0m
[37m[1m[2023-07-17 01:46:10,599][257371] New mean coefficients: [[ 1.9253412   0.62962985 -0.5459603  -0.7085534  -1.0307893  -0.5761118 ]][0m
[37m[1m[2023-07-17 01:46:10,600][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:46:19,643][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 01:46:19,648][257371] FPS: 424748.10[0m
[36m[2023-07-17 01:46:19,651][257371] itr=428, itrs=2000, Progress: 21.40%[0m
[36m[2023-07-17 01:46:31,384][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 01:46:31,384][257371] FPS: 329363.31[0m
[36m[2023-07-17 01:46:35,701][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:46:35,702][257371] Reward + Measures: [[-38.77866646   0.34793434   0.45606363   0.336328     0.38575798
    4.10135317]][0m
[37m[1m[2023-07-17 01:46:35,702][257371] Max Reward on eval: -38.77866645525966[0m
[37m[1m[2023-07-17 01:46:35,702][257371] Min Reward on eval: -38.77866645525966[0m
[37m[1m[2023-07-17 01:46:35,702][257371] Mean Reward across all agents: -38.77866645525966[0m
[37m[1m[2023-07-17 01:46:35,702][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:46:40,700][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:46:40,701][257371] Reward + Measures: [[-100.46963895    0.5176        0.32279998    0.50150001    0.27250001
     5.69635725]
 [  24.12945178    0.1578        0.212         0.1839        0.17690001
     4.78943491]
 [  -7.60391726    0.13500001    0.30220002    0.24520002    0.28190002
     5.1582818 ]
 ...
 [  -1.06340464    0.1793        0.48129997    0.26719999    0.45310003
     4.98931694]
 [  12.29689436    0.17349999    0.35349998    0.2586        0.3001
     5.09234333]
 [ -45.65029375    0.33839998    0.3382        0.37150002    0.29240003
     5.44048023]][0m
[37m[1m[2023-07-17 01:46:40,701][257371] Max Reward on eval: 139.38120440378262[0m
[37m[1m[2023-07-17 01:46:40,701][257371] Min Reward on eval: -184.19095947961324[0m
[37m[1m[2023-07-17 01:46:40,702][257371] Mean Reward across all agents: 5.058981001583298[0m
[37m[1m[2023-07-17 01:46:40,702][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:46:40,704][257371] mean_value=-548.2713751174335, max_value=80.77543550189696[0m
[37m[1m[2023-07-17 01:46:40,707][257371] New mean coefficients: [[ 1.113641    2.2436109  -1.7460315  -0.7214153   0.16451347 -0.5534384 ]][0m
[37m[1m[2023-07-17 01:46:40,708][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:46:49,660][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 01:46:49,660][257371] FPS: 429020.60[0m
[36m[2023-07-17 01:46:49,663][257371] itr=429, itrs=2000, Progress: 21.45%[0m
[36m[2023-07-17 01:47:01,475][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 01:47:01,475][257371] FPS: 327134.30[0m
[36m[2023-07-17 01:47:05,816][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:47:05,816][257371] Reward + Measures: [[24.0176457   0.32074866  0.49645498  0.30411401  0.43813732  4.4307785 ]][0m
[37m[1m[2023-07-17 01:47:05,817][257371] Max Reward on eval: 24.017645702857717[0m
[37m[1m[2023-07-17 01:47:05,817][257371] Min Reward on eval: 24.017645702857717[0m
[37m[1m[2023-07-17 01:47:05,817][257371] Mean Reward across all agents: 24.017645702857717[0m
[37m[1m[2023-07-17 01:47:05,817][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:47:10,851][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:47:10,852][257371] Reward + Measures: [[ 41.44623603   0.1917       0.19660001   0.176        0.18700001
    5.3243289 ]
 [ 46.64923243   0.33430001   0.35950002   0.34609997   0.43129998
    5.55230951]
 [ 73.77377416   0.1236       0.17059998   0.15109999   0.1496
    5.86610651]
 ...
 [-97.53963252   0.2563       0.27900001   0.29130003   0.2481
    5.23558903]
 [ 51.36132914   0.31040001   0.38789999   0.301        0.37810001
    5.14749908]
 [ 82.42737475   0.33900002   0.23940001   0.33759999   0.29249999
    5.22248459]][0m
[37m[1m[2023-07-17 01:47:10,852][257371] Max Reward on eval: 225.08329300591723[0m
[37m[1m[2023-07-17 01:47:10,852][257371] Min Reward on eval: -111.90422490015627[0m
[37m[1m[2023-07-17 01:47:10,853][257371] Mean Reward across all agents: 37.48845924006811[0m
[37m[1m[2023-07-17 01:47:10,853][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:47:10,856][257371] mean_value=-195.37375212198103, max_value=166.80887105425703[0m
[37m[1m[2023-07-17 01:47:10,858][257371] New mean coefficients: [[ 2.2782412   2.1299343  -2.8092241  -0.93384624  0.80335987 -0.8525255 ]][0m
[37m[1m[2023-07-17 01:47:10,859][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:47:20,022][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-17 01:47:20,022][257371] FPS: 419161.56[0m
[36m[2023-07-17 01:47:20,025][257371] itr=430, itrs=2000, Progress: 21.50%[0m
[37m[1m[2023-07-17 01:50:02,726][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000410[0m
[36m[2023-07-17 01:50:15,094][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-17 01:50:15,095][257371] FPS: 324455.34[0m
[36m[2023-07-17 01:50:19,273][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:50:19,274][257371] Reward + Measures: [[3.95507377 0.36901131 0.41477066 0.28291965 0.33267933 4.20715904]][0m
[37m[1m[2023-07-17 01:50:19,274][257371] Max Reward on eval: 3.95507377277012[0m
[37m[1m[2023-07-17 01:50:19,274][257371] Min Reward on eval: 3.95507377277012[0m
[37m[1m[2023-07-17 01:50:19,275][257371] Mean Reward across all agents: 3.95507377277012[0m
[37m[1m[2023-07-17 01:50:19,275][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:50:24,181][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:50:24,181][257371] Reward + Measures: [[ 34.56725801   0.26910001   0.33379999   0.24840002   0.36540002
    4.79869366]
 [ 77.28324296   0.1608       0.18859999   0.17379999   0.18350001
    5.34897947]
 [-12.47330274   0.22760001   0.31360003   0.2552       0.23660003
    5.73626232]
 ...
 [168.65742965   0.2536       0.34010002   0.32230002   0.2471
    4.76442432]
 [ 53.79377362   0.3062       0.3195       0.29729998   0.3163
    5.15653467]
 [-89.71933811   0.30159998   0.4061       0.20479999   0.40510002
    5.6638751 ]][0m
[37m[1m[2023-07-17 01:50:24,181][257371] Max Reward on eval: 207.965449308604[0m
[37m[1m[2023-07-17 01:50:24,182][257371] Min Reward on eval: -238.5558939523995[0m
[37m[1m[2023-07-17 01:50:24,182][257371] Mean Reward across all agents: 32.111363058762876[0m
[37m[1m[2023-07-17 01:50:24,182][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:50:24,185][257371] mean_value=-297.4869815705299, max_value=358.07200278122787[0m
[37m[1m[2023-07-17 01:50:24,187][257371] New mean coefficients: [[ 0.83782446  1.9066118  -2.3657787  -0.23426473  1.4093208  -0.5812684 ]][0m
[37m[1m[2023-07-17 01:50:24,188][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:50:33,152][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 01:50:33,152][257371] FPS: 428472.53[0m
[36m[2023-07-17 01:50:33,154][257371] itr=431, itrs=2000, Progress: 21.55%[0m
[36m[2023-07-17 01:50:44,880][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 01:50:44,881][257371] FPS: 329580.18[0m
[36m[2023-07-17 01:50:49,236][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:50:49,236][257371] Reward + Measures: [[23.44808317  0.34412199  0.36361864  0.30769035  0.30427366  4.19763613]][0m
[37m[1m[2023-07-17 01:50:49,236][257371] Max Reward on eval: 23.44808316540714[0m
[37m[1m[2023-07-17 01:50:49,237][257371] Min Reward on eval: 23.44808316540714[0m
[37m[1m[2023-07-17 01:50:49,237][257371] Mean Reward across all agents: 23.44808316540714[0m
[37m[1m[2023-07-17 01:50:49,237][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:50:54,275][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:50:54,275][257371] Reward + Measures: [[73.05333641  0.1797      0.25180003  0.1838      0.22510003  5.02175283]
 [18.50881001  0.23740001  0.27210003  0.2472      0.26069999  5.11194468]
 [33.51442862  0.0693      0.0987      0.0987      0.071       5.76175261]
 ...
 [46.10041884  0.0865      0.11060001  0.0996      0.087       5.61379814]
 [33.26393055  0.0984      0.12639999  0.11260001  0.0843      5.60972214]
 [89.4121282   0.0969      0.10030001  0.13509999  0.08450001  6.40986872]][0m
[37m[1m[2023-07-17 01:50:54,275][257371] Max Reward on eval: 243.10782348643988[0m
[37m[1m[2023-07-17 01:50:54,276][257371] Min Reward on eval: -120.94214223691961[0m
[37m[1m[2023-07-17 01:50:54,276][257371] Mean Reward across all agents: 55.176737429429025[0m
[37m[1m[2023-07-17 01:50:54,276][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:50:54,279][257371] mean_value=-309.6684936162655, max_value=317.99834906398405[0m
[37m[1m[2023-07-17 01:50:54,281][257371] New mean coefficients: [[ 1.8346742   1.3946443  -1.9954864  -0.43564475  0.56488425 -0.71559227]][0m
[37m[1m[2023-07-17 01:50:54,282][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:51:03,319][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 01:51:03,319][257371] FPS: 425005.46[0m
[36m[2023-07-17 01:51:03,322][257371] itr=432, itrs=2000, Progress: 21.60%[0m
[36m[2023-07-17 01:51:15,017][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 01:51:15,017][257371] FPS: 330440.88[0m
[36m[2023-07-17 01:51:19,322][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:51:19,322][257371] Reward + Measures: [[54.68894188  0.30342469  0.37383464  0.3035      0.299602    4.40867615]][0m
[37m[1m[2023-07-17 01:51:19,323][257371] Max Reward on eval: 54.6889418783132[0m
[37m[1m[2023-07-17 01:51:19,323][257371] Min Reward on eval: 54.6889418783132[0m
[37m[1m[2023-07-17 01:51:19,323][257371] Mean Reward across all agents: 54.6889418783132[0m
[37m[1m[2023-07-17 01:51:19,323][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:51:24,317][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:51:24,318][257371] Reward + Measures: [[ 103.19078134    0.26080003    0.3048        0.24249998    0.23660003
     5.72282743]
 [-229.61465956    0.96680003    0.96469992    0.17580001    0.80870008
     7.56169748]
 [  42.87368256    0.193         0.3154        0.2402        0.301
     4.93268299]
 ...
 [ 129.52081305    0.31710002    0.38810003    0.30610001    0.34980002
     5.69300461]
 [ -11.27551931    0.2167        0.22750001    0.2009        0.21640001
     5.45615578]
 [  25.37359984    0.2429        0.3055        0.3274        0.28150001
     4.53846502]][0m
[37m[1m[2023-07-17 01:51:24,318][257371] Max Reward on eval: 505.0688867431134[0m
[37m[1m[2023-07-17 01:51:24,318][257371] Min Reward on eval: -410.40098384115845[0m
[37m[1m[2023-07-17 01:51:24,319][257371] Mean Reward across all agents: 25.932835351450144[0m
[37m[1m[2023-07-17 01:51:24,319][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:51:24,324][257371] mean_value=-207.9120844995954, max_value=512.4786498773843[0m
[37m[1m[2023-07-17 01:51:24,326][257371] New mean coefficients: [[ 2.177353    0.53450286 -1.9354172  -1.7255868   0.7166436  -0.7890114 ]][0m
[37m[1m[2023-07-17 01:51:24,327][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:51:33,312][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 01:51:33,312][257371] FPS: 427485.17[0m
[36m[2023-07-17 01:51:33,314][257371] itr=433, itrs=2000, Progress: 21.65%[0m
[36m[2023-07-17 01:51:44,967][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 01:51:44,967][257371] FPS: 331522.47[0m
[36m[2023-07-17 01:51:49,304][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:51:49,304][257371] Reward + Measures: [[57.75131737  0.29982665  0.36404166  0.30123734  0.28781399  4.39493608]][0m
[37m[1m[2023-07-17 01:51:49,304][257371] Max Reward on eval: 57.75131736892698[0m
[37m[1m[2023-07-17 01:51:49,305][257371] Min Reward on eval: 57.75131736892698[0m
[37m[1m[2023-07-17 01:51:49,305][257371] Mean Reward across all agents: 57.75131736892698[0m
[37m[1m[2023-07-17 01:51:49,305][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:51:54,323][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:51:54,324][257371] Reward + Measures: [[  26.72373943    0.39119998    0.32010004    0.39860001    0.2095
     5.53568459]
 [ -36.47342844    0.30990002    0.32440001    0.26069999    0.32750002
     5.60581923]
 [ -53.03991628    0.17260002    0.16489999    0.16940001    0.1531
     5.44775152]
 ...
 [ -66.725483      0.40179998    0.3053        0.35670003    0.25549999
     5.15956974]
 [-107.68850157    0.19270001    0.14189999    0.20469999    0.1754
     5.32980442]
 [   4.81207735    0.18770002    0.19320001    0.21900001    0.17589998
     5.24804401]][0m
[37m[1m[2023-07-17 01:51:54,324][257371] Max Reward on eval: 291.1868133579381[0m
[37m[1m[2023-07-17 01:51:54,324][257371] Min Reward on eval: -165.19900388792158[0m
[37m[1m[2023-07-17 01:51:54,324][257371] Mean Reward across all agents: 0.44275476861401636[0m
[37m[1m[2023-07-17 01:51:54,325][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:51:54,327][257371] mean_value=-278.6252766428256, max_value=350.75166932122926[0m
[37m[1m[2023-07-17 01:51:54,330][257371] New mean coefficients: [[ 1.782585    1.3881649  -1.0697714  -2.6607015  -0.741979   -0.39165875]][0m
[37m[1m[2023-07-17 01:51:54,330][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:52:03,371][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 01:52:03,371][257371] FPS: 424834.01[0m
[36m[2023-07-17 01:52:03,374][257371] itr=434, itrs=2000, Progress: 21.70%[0m
[36m[2023-07-17 01:52:15,101][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 01:52:15,101][257371] FPS: 329435.14[0m
[36m[2023-07-17 01:52:19,476][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:52:19,476][257371] Reward + Measures: [[61.50253852  0.29970899  0.36756697  0.29954365  0.29188266  4.3919282 ]][0m
[37m[1m[2023-07-17 01:52:19,476][257371] Max Reward on eval: 61.502538520773875[0m
[37m[1m[2023-07-17 01:52:19,477][257371] Min Reward on eval: 61.502538520773875[0m
[37m[1m[2023-07-17 01:52:19,477][257371] Mean Reward across all agents: 61.502538520773875[0m
[37m[1m[2023-07-17 01:52:19,477][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:52:24,536][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:52:24,537][257371] Reward + Measures: [[ 41.30310354   0.16759999   0.2251       0.17749999   0.21280001
    5.08613348]
 [ 39.19227972   0.2041       0.22230001   0.22409999   0.16770001
    5.03200865]
 [ 31.70005205   0.3188       0.34339997   0.30500001   0.3486
    5.31742239]
 ...
 [-13.97886702   0.18789999   0.20560002   0.18020001   0.20719998
    5.65105677]
 [ 56.3279164    0.34260002   0.36849999   0.23210001   0.3233
    5.3751874 ]
 [-27.43793557   0.26570001   0.3132       0.25079998   0.32160002
    5.90134239]][0m
[37m[1m[2023-07-17 01:52:24,537][257371] Max Reward on eval: 211.6961703080684[0m
[37m[1m[2023-07-17 01:52:24,537][257371] Min Reward on eval: -151.59019366055728[0m
[37m[1m[2023-07-17 01:52:24,537][257371] Mean Reward across all agents: 18.31804097213678[0m
[37m[1m[2023-07-17 01:52:24,538][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:52:24,539][257371] mean_value=-340.4379147795268, max_value=110.42392222987738[0m
[37m[1m[2023-07-17 01:52:24,542][257371] New mean coefficients: [[ 0.8192636   0.42841733  0.89247465 -2.577267    0.12368441 -0.79391634]][0m
[37m[1m[2023-07-17 01:52:24,543][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:52:33,637][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 01:52:33,637][257371] FPS: 422312.84[0m
[36m[2023-07-17 01:52:33,639][257371] itr=435, itrs=2000, Progress: 21.75%[0m
[36m[2023-07-17 01:52:45,472][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 01:52:45,473][257371] FPS: 326531.27[0m
[36m[2023-07-17 01:52:49,873][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:52:49,873][257371] Reward + Measures: [[71.86457208  0.29545832  0.36548665  0.29694399  0.288232    4.35369444]][0m
[37m[1m[2023-07-17 01:52:49,873][257371] Max Reward on eval: 71.86457207695763[0m
[37m[1m[2023-07-17 01:52:49,874][257371] Min Reward on eval: 71.86457207695763[0m
[37m[1m[2023-07-17 01:52:49,874][257371] Mean Reward across all agents: 71.86457207695763[0m
[37m[1m[2023-07-17 01:52:49,874][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:52:54,904][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:52:54,905][257371] Reward + Measures: [[-18.62531987   0.26480004   0.26070002   0.26009998   0.20650001
    5.02711916]
 [-13.82606238   0.23080002   0.25429997   0.22040001   0.20650001
    5.48742914]
 [-17.6937906    0.16919999   0.18310001   0.19170001   0.1595
    5.13235044]
 ...
 [-13.61677583   0.13300002   0.21269999   0.1619       0.15540001
    4.73925829]
 [-18.35652625   0.1268       0.12229999   0.14060001   0.11369999
    5.92860842]
 [-27.63470446   0.1409       0.17400001   0.153        0.10389999
    5.61367846]][0m
[37m[1m[2023-07-17 01:52:54,905][257371] Max Reward on eval: 229.16254046615214[0m
[37m[1m[2023-07-17 01:52:54,905][257371] Min Reward on eval: -156.7210412618064[0m
[37m[1m[2023-07-17 01:52:54,906][257371] Mean Reward across all agents: 19.432672265808126[0m
[37m[1m[2023-07-17 01:52:54,906][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:52:54,907][257371] mean_value=-430.0317945433561, max_value=88.0014433570506[0m
[37m[1m[2023-07-17 01:52:54,910][257371] New mean coefficients: [[ 1.1401259  -0.587013    1.1983724  -1.0843009   0.6354775  -0.89531595]][0m
[37m[1m[2023-07-17 01:52:54,911][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:53:04,016][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 01:53:04,016][257371] FPS: 421827.96[0m
[36m[2023-07-17 01:53:04,018][257371] itr=436, itrs=2000, Progress: 21.80%[0m
[36m[2023-07-17 01:53:15,920][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 01:53:15,920][257371] FPS: 324625.95[0m
[36m[2023-07-17 01:53:20,325][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:53:20,326][257371] Reward + Measures: [[73.32372756  0.29705033  0.36805436  0.29815367  0.28974366  4.34502125]][0m
[37m[1m[2023-07-17 01:53:20,326][257371] Max Reward on eval: 73.32372755745429[0m
[37m[1m[2023-07-17 01:53:20,326][257371] Min Reward on eval: 73.32372755745429[0m
[37m[1m[2023-07-17 01:53:20,327][257371] Mean Reward across all agents: 73.32372755745429[0m
[37m[1m[2023-07-17 01:53:20,327][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:53:25,558][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:53:25,559][257371] Reward + Measures: [[152.27850772   0.26680002   0.32320005   0.31649998   0.317
    5.18106985]
 [ -0.7667942    0.77719998   0.67989999   0.30949998   0.53470004
    6.87092447]
 [-38.69528997   0.15940002   0.1753       0.19050001   0.14500001
    4.81483555]
 ...
 [ 87.47491097   0.25600001   0.2667       0.3055       0.2369
    4.75704384]
 [-28.89931761   0.35450003   0.21780001   0.35400003   0.2388
    5.38274479]
 [ 24.88472627   0.15710001   0.18489999   0.16410001   0.1459
    4.75250196]][0m
[37m[1m[2023-07-17 01:53:25,559][257371] Max Reward on eval: 322.25878522880373[0m
[37m[1m[2023-07-17 01:53:25,560][257371] Min Reward on eval: -234.0227212842554[0m
[37m[1m[2023-07-17 01:53:25,560][257371] Mean Reward across all agents: 37.757011011366416[0m
[37m[1m[2023-07-17 01:53:25,560][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:53:25,564][257371] mean_value=-340.4397749419533, max_value=621.4545633028262[0m
[37m[1m[2023-07-17 01:53:25,566][257371] New mean coefficients: [[ 1.8109343  -1.6441113   2.4313018  -0.37806344  1.8206043  -0.842285  ]][0m
[37m[1m[2023-07-17 01:53:25,567][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:53:34,678][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 01:53:34,678][257371] FPS: 421581.49[0m
[36m[2023-07-17 01:53:34,680][257371] itr=437, itrs=2000, Progress: 21.85%[0m
[36m[2023-07-17 01:53:46,471][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 01:53:46,471][257371] FPS: 327733.90[0m
[36m[2023-07-17 01:53:50,830][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:53:50,831][257371] Reward + Measures: [[75.71062718  0.29634532  0.36817065  0.29939932  0.29021502  4.35161638]][0m
[37m[1m[2023-07-17 01:53:50,831][257371] Max Reward on eval: 75.71062717925943[0m
[37m[1m[2023-07-17 01:53:50,831][257371] Min Reward on eval: 75.71062717925943[0m
[37m[1m[2023-07-17 01:53:50,832][257371] Mean Reward across all agents: 75.71062717925943[0m
[37m[1m[2023-07-17 01:53:50,832][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:53:55,897][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:53:55,898][257371] Reward + Measures: [[ 64.88026684   0.2494       0.27509999   0.24609999   0.1839
    4.93766165]
 [ -0.77574835   0.09819999   0.16250001   0.14730002   0.1169
    4.63892078]
 [-36.36322524   0.0944       0.1603       0.11800001   0.1191
    4.82577848]
 ...
 [ 10.00565588   0.0746       0.07700001   0.0824       0.0669
    5.33054781]
 [113.16047509   0.15540001   0.21159999   0.19400001   0.13160001
    4.39967823]
 [ 56.45141169   0.1691       0.2023       0.1925       0.1429
    5.04934788]][0m
[37m[1m[2023-07-17 01:53:55,898][257371] Max Reward on eval: 156.59501317963003[0m
[37m[1m[2023-07-17 01:53:55,898][257371] Min Reward on eval: -119.21322615370154[0m
[37m[1m[2023-07-17 01:53:55,899][257371] Mean Reward across all agents: 9.44826337136558[0m
[37m[1m[2023-07-17 01:53:55,899][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:53:55,900][257371] mean_value=-562.0989617782686, max_value=11.9570231224576[0m
[37m[1m[2023-07-17 01:53:55,903][257371] New mean coefficients: [[ 0.13759267 -0.59796596  0.67130303 -1.5736122   1.6259098  -0.966892  ]][0m
[37m[1m[2023-07-17 01:53:55,904][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:54:05,020][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 01:54:05,021][257371] FPS: 421283.41[0m
[36m[2023-07-17 01:54:05,023][257371] itr=438, itrs=2000, Progress: 21.90%[0m
[36m[2023-07-17 01:54:16,955][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-17 01:54:16,955][257371] FPS: 323773.99[0m
[36m[2023-07-17 01:54:21,399][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:54:21,400][257371] Reward + Measures: [[82.87470045  0.29255834  0.36617032  0.29418933  0.28949466  4.3406992 ]][0m
[37m[1m[2023-07-17 01:54:21,400][257371] Max Reward on eval: 82.8747004536025[0m
[37m[1m[2023-07-17 01:54:21,400][257371] Min Reward on eval: 82.8747004536025[0m
[37m[1m[2023-07-17 01:54:21,401][257371] Mean Reward across all agents: 82.8747004536025[0m
[37m[1m[2023-07-17 01:54:21,401][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:54:26,462][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:54:26,463][257371] Reward + Measures: [[146.60732532   0.26720002   0.36929998   0.29860002   0.25440001
    5.26174736]
 [234.89801028   0.26390001   0.30599999   0.29319999   0.19579999
    5.1271143 ]
 [-38.95995143   0.19240001   0.19329999   0.22270003   0.1534
    5.43655825]
 ...
 [133.71881623   0.19780001   0.3073       0.26500002   0.18880001
    4.16163254]
 [-31.40417549   0.35189998   0.32169998   0.36630002   0.29230002
    4.36406279]
 [-25.20599593   0.21500002   0.2388       0.25139999   0.29240003
    5.41178465]][0m
[37m[1m[2023-07-17 01:54:26,463][257371] Max Reward on eval: 234.8980102780275[0m
[37m[1m[2023-07-17 01:54:26,463][257371] Min Reward on eval: -159.8734265267849[0m
[37m[1m[2023-07-17 01:54:26,463][257371] Mean Reward across all agents: 26.40440063788222[0m
[37m[1m[2023-07-17 01:54:26,463][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:54:26,465][257371] mean_value=-617.0686305549783, max_value=296.71049148083034[0m
[37m[1m[2023-07-17 01:54:26,468][257371] New mean coefficients: [[ 0.45622215 -0.695186    0.74566317 -0.07767582 -0.12584269 -1.1599389 ]][0m
[37m[1m[2023-07-17 01:54:26,469][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:54:35,571][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 01:54:35,571][257371] FPS: 421950.09[0m
[36m[2023-07-17 01:54:35,574][257371] itr=439, itrs=2000, Progress: 21.95%[0m
[36m[2023-07-17 01:54:47,305][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 01:54:47,305][257371] FPS: 329416.45[0m
[36m[2023-07-17 01:54:51,667][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:54:51,673][257371] Reward + Measures: [[81.16753162  0.29711536  0.37235332  0.29685065  0.29506433  4.33017635]][0m
[37m[1m[2023-07-17 01:54:51,673][257371] Max Reward on eval: 81.16753161772212[0m
[37m[1m[2023-07-17 01:54:51,673][257371] Min Reward on eval: 81.16753161772212[0m
[37m[1m[2023-07-17 01:54:51,674][257371] Mean Reward across all agents: 81.16753161772212[0m
[37m[1m[2023-07-17 01:54:51,674][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:54:56,739][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:54:56,745][257371] Reward + Measures: [[112.46906264   0.2287       0.29050002   0.26869997   0.2167
    4.5901103 ]
 [ 60.33759503   0.30219999   0.2129       0.3299       0.2189
    4.73072577]
 [ 94.32716616   0.17130001   0.1102       0.21619999   0.1481
    5.1956563 ]
 ...
 [ 90.27000198   0.26590002   0.36480001   0.26199999   0.26120001
    5.57844114]
 [ 91.58462738   0.16859999   0.20510001   0.20630002   0.1542
    5.29539585]
 [135.4176302    0.3565       0.3987       0.33829999   0.28920001
    5.21621323]][0m
[37m[1m[2023-07-17 01:54:56,745][257371] Max Reward on eval: 184.0668998586014[0m
[37m[1m[2023-07-17 01:54:56,745][257371] Min Reward on eval: -156.34026066791267[0m
[37m[1m[2023-07-17 01:54:56,746][257371] Mean Reward across all agents: 27.749502543086958[0m
[37m[1m[2023-07-17 01:54:56,746][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:54:56,748][257371] mean_value=-314.4442388927722, max_value=526.8016397390891[0m
[37m[1m[2023-07-17 01:54:56,750][257371] New mean coefficients: [[ 0.584987    0.0648787   0.6773112   0.10259363  0.59823257 -1.7234484 ]][0m
[37m[1m[2023-07-17 01:54:56,751][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:55:05,894][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 01:55:05,894][257371] FPS: 420074.00[0m
[36m[2023-07-17 01:55:05,897][257371] itr=440, itrs=2000, Progress: 22.00%[0m
[37m[1m[2023-07-17 01:57:57,022][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000420[0m
[36m[2023-07-17 01:58:09,078][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 01:58:09,078][257371] FPS: 332097.20[0m
[36m[2023-07-17 01:58:13,353][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:58:13,354][257371] Reward + Measures: [[77.37336526  0.29214266  0.37127566  0.29175532  0.29697999  4.30635881]][0m
[37m[1m[2023-07-17 01:58:13,354][257371] Max Reward on eval: 77.37336526254782[0m
[37m[1m[2023-07-17 01:58:13,354][257371] Min Reward on eval: 77.37336526254782[0m
[37m[1m[2023-07-17 01:58:13,354][257371] Mean Reward across all agents: 77.37336526254782[0m
[37m[1m[2023-07-17 01:58:13,354][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:58:18,229][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:58:18,229][257371] Reward + Measures: [[  9.32170514   0.3001       0.30280003   0.30050001   0.27399999
    4.92830229]
 [ 48.13116787   0.25150001   0.29080001   0.27379999   0.20379999
    4.92993355]
 [-10.6447525    0.26010001   0.32229999   0.28960001   0.26680002
    4.80600691]
 ...
 [-34.0568811    0.12949999   0.8757       0.2703       0.87169993
    7.07355452]
 [ 45.4484729    0.289        0.32190001   0.27110001   0.2678
    4.8571043 ]
 [ 28.99099346   0.3109       0.35520002   0.35310003   0.32870001
    5.31602383]][0m
[37m[1m[2023-07-17 01:58:18,230][257371] Max Reward on eval: 150.58119631409645[0m
[37m[1m[2023-07-17 01:58:18,230][257371] Min Reward on eval: -173.4725027091801[0m
[37m[1m[2023-07-17 01:58:18,230][257371] Mean Reward across all agents: 27.32030273739379[0m
[37m[1m[2023-07-17 01:58:18,230][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:58:18,233][257371] mean_value=-228.80552097668664, max_value=414.96471435820683[0m
[37m[1m[2023-07-17 01:58:18,235][257371] New mean coefficients: [[-0.0369674  -0.61556804 -0.013771   -0.01391154 -0.9698532  -1.648839  ]][0m
[37m[1m[2023-07-17 01:58:18,236][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:58:27,140][257371] train() took 8.90 seconds to complete[0m
[36m[2023-07-17 01:58:27,140][257371] FPS: 431351.90[0m
[36m[2023-07-17 01:58:27,142][257371] itr=441, itrs=2000, Progress: 22.05%[0m
[36m[2023-07-17 01:58:38,855][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 01:58:38,856][257371] FPS: 329762.33[0m
[36m[2023-07-17 01:58:43,084][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:58:43,084][257371] Reward + Measures: [[77.38322384  0.29250735  0.37805733  0.29010567  0.30236465  4.2709012 ]][0m
[37m[1m[2023-07-17 01:58:43,085][257371] Max Reward on eval: 77.38322384488313[0m
[37m[1m[2023-07-17 01:58:43,085][257371] Min Reward on eval: 77.38322384488313[0m
[37m[1m[2023-07-17 01:58:43,085][257371] Mean Reward across all agents: 77.38322384488313[0m
[37m[1m[2023-07-17 01:58:43,086][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:58:48,044][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:58:48,045][257371] Reward + Measures: [[-42.05906594   0.13740002   0.2244       0.1912       0.1558
    5.47616291]
 [ 44.88924953   0.2076       0.3642       0.26929998   0.30130002
    4.40732527]
 [ 84.2185259    0.23370002   0.36330003   0.24820001   0.29350001
    3.99550939]
 ...
 [  9.36808508   0.27870002   0.28760001   0.29569998   0.2282
    4.34890556]
 [  4.36452402   0.20920001   0.25190002   0.2189       0.17690001
    5.04978085]
 [-35.44548539   0.40900001   0.40869999   0.0914       0.46370003
    6.43699789]][0m
[37m[1m[2023-07-17 01:58:48,045][257371] Max Reward on eval: 417.8903838641942[0m
[37m[1m[2023-07-17 01:58:48,045][257371] Min Reward on eval: -168.53357648290694[0m
[37m[1m[2023-07-17 01:58:48,046][257371] Mean Reward across all agents: 17.632315087459155[0m
[37m[1m[2023-07-17 01:58:48,046][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:58:48,048][257371] mean_value=-330.90756741895154, max_value=736.0559369878399[0m
[37m[1m[2023-07-17 01:58:48,051][257371] New mean coefficients: [[-0.29507568 -0.2901795   1.1810215  -1.5291239   1.3279178  -1.3283727 ]][0m
[37m[1m[2023-07-17 01:58:48,052][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:58:57,042][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 01:58:57,042][257371] FPS: 427199.78[0m
[36m[2023-07-17 01:58:57,044][257371] itr=442, itrs=2000, Progress: 22.10%[0m
[36m[2023-07-17 01:59:08,921][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 01:59:08,921][257371] FPS: 325277.62[0m
[36m[2023-07-17 01:59:13,226][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:59:13,226][257371] Reward + Measures: [[76.89072966  0.28985134  0.375211    0.28734899  0.30076766  4.25969076]][0m
[37m[1m[2023-07-17 01:59:13,226][257371] Max Reward on eval: 76.89072965642981[0m
[37m[1m[2023-07-17 01:59:13,227][257371] Min Reward on eval: 76.89072965642981[0m
[37m[1m[2023-07-17 01:59:13,227][257371] Mean Reward across all agents: 76.89072965642981[0m
[37m[1m[2023-07-17 01:59:13,227][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:59:18,215][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:59:18,215][257371] Reward + Measures: [[100.13095044   0.35310003   0.38360003   0.31460002   0.26250002
    5.26380014]
 [164.49999046   0.33919999   0.30109999   0.34580001   0.27420002
    5.08795691]
 [-54.99841067   0.26359999   0.2304       0.2185       0.27320001
    5.94376373]
 ...
 [ 39.62791498   0.1701       0.20630001   0.1591       0.1715
    4.92726851]
 [  9.65508849   0.16129999   0.16770001   0.1912       0.14910001
    5.30975294]
 [ -4.49226463   0.23000002   0.25409999   0.24890001   0.2086
    5.5210247 ]][0m
[37m[1m[2023-07-17 01:59:18,215][257371] Max Reward on eval: 179.56369734369218[0m
[37m[1m[2023-07-17 01:59:18,216][257371] Min Reward on eval: -112.76137503646314[0m
[37m[1m[2023-07-17 01:59:18,216][257371] Mean Reward across all agents: 30.83317136817579[0m
[37m[1m[2023-07-17 01:59:18,216][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:59:18,218][257371] mean_value=-300.44028554226827, max_value=283.3847870596524[0m
[37m[1m[2023-07-17 01:59:18,220][257371] New mean coefficients: [[-0.39813876  0.17368275 -1.0248265  -1.6933157   1.1151313  -2.21361   ]][0m
[37m[1m[2023-07-17 01:59:18,221][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:59:27,196][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 01:59:27,196][257371] FPS: 427956.91[0m
[36m[2023-07-17 01:59:27,198][257371] itr=443, itrs=2000, Progress: 22.15%[0m
[36m[2023-07-17 01:59:39,056][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 01:59:39,056][257371] FPS: 325774.24[0m
[36m[2023-07-17 01:59:43,360][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:59:43,361][257371] Reward + Measures: [[78.70509556  0.28886232  0.37799129  0.28866634  0.30162799  4.24556494]][0m
[37m[1m[2023-07-17 01:59:43,361][257371] Max Reward on eval: 78.70509556306736[0m
[37m[1m[2023-07-17 01:59:43,361][257371] Min Reward on eval: 78.70509556306736[0m
[37m[1m[2023-07-17 01:59:43,362][257371] Mean Reward across all agents: 78.70509556306736[0m
[37m[1m[2023-07-17 01:59:43,362][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:59:48,303][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 01:59:48,304][257371] Reward + Measures: [[-108.11316694    0.28920001    0.3339        0.24530001    0.37129998
     4.77715445]
 [  -7.68728937    0.23410001    0.25659999    0.25439999    0.1953
     5.68660498]
 [ -34.39436772    0.2705        0.34129998    0.25890002    0.33189997
     4.41027975]
 ...
 [ -17.01865048    0.31730005    0.31580001    0.34150001    0.3242
     4.27741861]
 [ 112.70605854    0.4786        0.45220003    0.19060002    0.4612
     7.00625467]
 [ -55.70782844    0.27190003    0.3299        0.2361        0.30179998
     5.41488934]][0m
[37m[1m[2023-07-17 01:59:48,304][257371] Max Reward on eval: 171.0526048924774[0m
[37m[1m[2023-07-17 01:59:48,304][257371] Min Reward on eval: -268.5183124726289[0m
[37m[1m[2023-07-17 01:59:48,304][257371] Mean Reward across all agents: 7.1437523857573435[0m
[37m[1m[2023-07-17 01:59:48,305][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 01:59:48,307][257371] mean_value=-249.29324587102684, max_value=504.02097006009535[0m
[37m[1m[2023-07-17 01:59:48,310][257371] New mean coefficients: [[ 1.7478566  -0.21767879 -1.7026775   0.17345166  0.79290104 -2.6748724 ]][0m
[37m[1m[2023-07-17 01:59:48,311][257371] Moving the mean solution point...[0m
[36m[2023-07-17 01:59:57,361][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 01:59:57,361][257371] FPS: 424364.51[0m
[36m[2023-07-17 01:59:57,364][257371] itr=444, itrs=2000, Progress: 22.20%[0m
[36m[2023-07-17 02:00:09,201][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 02:00:09,201][257371] FPS: 326470.88[0m
[36m[2023-07-17 02:00:13,549][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:00:13,549][257371] Reward + Measures: [[81.44877749  0.288311    0.376342    0.28932834  0.300603    4.22988749]][0m
[37m[1m[2023-07-17 02:00:13,549][257371] Max Reward on eval: 81.44877748752805[0m
[37m[1m[2023-07-17 02:00:13,550][257371] Min Reward on eval: 81.44877748752805[0m
[37m[1m[2023-07-17 02:00:13,550][257371] Mean Reward across all agents: 81.44877748752805[0m
[37m[1m[2023-07-17 02:00:13,550][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:00:18,730][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:00:18,731][257371] Reward + Measures: [[ 94.67226094   0.18569998   0.28049999   0.22239999   0.2077
    4.86332798]
 [-28.37981084   0.35910001   0.35949999   0.36539999   0.24060002
    5.06854582]
 [ -9.68923705   0.31070003   0.2881       0.3001       0.2656
    4.74505758]
 ...
 [ 38.10056901   0.30450001   0.14040001   0.36290002   0.25780001
    5.84973097]
 [100.21199502   0.1454       0.23910001   0.18710001   0.1567
    4.92934752]
 [ 16.43313963   0.27740002   0.2462       0.29969999   0.28260002
    5.2437501 ]][0m
[37m[1m[2023-07-17 02:00:18,731][257371] Max Reward on eval: 155.4088111743331[0m
[37m[1m[2023-07-17 02:00:18,731][257371] Min Reward on eval: -147.16813033260405[0m
[37m[1m[2023-07-17 02:00:18,732][257371] Mean Reward across all agents: -3.837074250996549[0m
[37m[1m[2023-07-17 02:00:18,732][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:00:18,733][257371] mean_value=-298.6528956357068, max_value=49.787711356852924[0m
[37m[1m[2023-07-17 02:00:18,736][257371] New mean coefficients: [[ 1.4385922   0.15615022 -1.5293341  -0.33412343  1.4305687  -2.8931031 ]][0m
[37m[1m[2023-07-17 02:00:18,737][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:00:27,723][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 02:00:27,723][257371] FPS: 427418.54[0m
[36m[2023-07-17 02:00:27,725][257371] itr=445, itrs=2000, Progress: 22.25%[0m
[36m[2023-07-17 02:00:39,460][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 02:00:39,460][257371] FPS: 329347.94[0m
[36m[2023-07-17 02:00:43,758][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:00:43,759][257371] Reward + Measures: [[79.41776713  0.28596133  0.37538803  0.28590769  0.30109799  4.20303059]][0m
[37m[1m[2023-07-17 02:00:43,759][257371] Max Reward on eval: 79.41776713031476[0m
[37m[1m[2023-07-17 02:00:43,759][257371] Min Reward on eval: 79.41776713031476[0m
[37m[1m[2023-07-17 02:00:43,760][257371] Mean Reward across all agents: 79.41776713031476[0m
[37m[1m[2023-07-17 02:00:43,760][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:00:48,723][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:00:48,724][257371] Reward + Measures: [[-40.61988368   0.37059999   0.32120001   0.42740002   0.26340002
    5.12155008]
 [  3.03129976   0.26289999   0.45050001   0.1605       0.4513
    5.86900663]
 [-67.40249804   0.24510001   0.23420003   0.22589998   0.25620002
    5.95439863]
 ...
 [  8.68318502   0.29060003   0.20940001   0.34710002   0.21400002
    5.50377417]
 [  2.39142135   0.17540002   0.37400001   0.2438       0.32729998
    4.46903753]
 [-94.6042929    0.22950001   0.43870002   0.15620001   0.44150004
    6.10716963]][0m
[37m[1m[2023-07-17 02:00:48,724][257371] Max Reward on eval: 183.6215457579121[0m
[37m[1m[2023-07-17 02:00:48,724][257371] Min Reward on eval: -195.73583035767078[0m
[37m[1m[2023-07-17 02:00:48,725][257371] Mean Reward across all agents: 7.634690978173865[0m
[37m[1m[2023-07-17 02:00:48,725][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:00:48,726][257371] mean_value=-412.0378127122399, max_value=-2.30952297924253[0m
[36m[2023-07-17 02:00:48,728][257371] XNES is restarting with a new solution whose measures are [0.82499999 0.1426     0.84489995 0.3822     2.78669143] and objective is -28.9647532296367[0m
[36m[2023-07-17 02:00:48,729][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 02:00:48,731][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 02:00:48,732][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:00:57,717][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 02:00:57,717][257371] FPS: 427468.18[0m
[36m[2023-07-17 02:00:57,719][257371] itr=446, itrs=2000, Progress: 22.30%[0m
[36m[2023-07-17 02:01:09,465][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 02:01:09,465][257371] FPS: 329077.55[0m
[36m[2023-07-17 02:01:13,702][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:01:13,702][257371] Reward + Measures: [[-32.16423491   0.48977065   0.26811901   0.47755998   0.27052167
    2.32850122]][0m
[37m[1m[2023-07-17 02:01:13,702][257371] Max Reward on eval: -32.164234905909474[0m
[37m[1m[2023-07-17 02:01:13,703][257371] Min Reward on eval: -32.164234905909474[0m
[37m[1m[2023-07-17 02:01:13,703][257371] Mean Reward across all agents: -32.164234905909474[0m
[37m[1m[2023-07-17 02:01:13,703][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:01:18,684][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:01:18,685][257371] Reward + Measures: [[   7.50729213    0.7572        0.23889999    0.65099996    0.61390001
     4.325418  ]
 [-309.8322268     0.2096        0.22689998    0.2264        0.39089999
     3.82536054]
 [ -17.28569209    0.15090001    0.15720001    0.1365        0.15869999
     5.12428808]
 ...
 [  55.18865612    0.32280001    0.36969998    0.1188        0.44530001
     3.51748395]
 [  69.91223466    0.2861        0.25639999    0.15640001    0.27060002
     3.28451395]
 [ -10.05194693    0.26140001    0.24780002    0.2306        0.35930002
     3.44423747]][0m
[37m[1m[2023-07-17 02:01:18,685][257371] Max Reward on eval: 324.05090797021984[0m
[37m[1m[2023-07-17 02:01:18,686][257371] Min Reward on eval: -400.94075303152204[0m
[37m[1m[2023-07-17 02:01:18,686][257371] Mean Reward across all agents: -30.48461925816903[0m
[37m[1m[2023-07-17 02:01:18,686][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:01:18,689][257371] mean_value=-1504.4142189010981, max_value=279.44476115025753[0m
[37m[1m[2023-07-17 02:01:18,692][257371] New mean coefficients: [[ 0.48052004  0.09509403  1.537302   -1.1762887  -0.54199827 -0.41931775]][0m
[37m[1m[2023-07-17 02:01:18,693][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:01:27,691][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 02:01:27,691][257371] FPS: 426830.61[0m
[36m[2023-07-17 02:01:27,693][257371] itr=447, itrs=2000, Progress: 22.35%[0m
[36m[2023-07-17 02:01:39,472][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 02:01:39,472][257371] FPS: 327984.93[0m
[36m[2023-07-17 02:01:43,814][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:01:43,815][257371] Reward + Measures: [[-8.51003682  0.56812865  0.35462865  0.54074371  0.39345133  2.24603081]][0m
[37m[1m[2023-07-17 02:01:43,815][257371] Max Reward on eval: -8.510036819749084[0m
[37m[1m[2023-07-17 02:01:43,815][257371] Min Reward on eval: -8.510036819749084[0m
[37m[1m[2023-07-17 02:01:43,815][257371] Mean Reward across all agents: -8.510036819749084[0m
[37m[1m[2023-07-17 02:01:43,816][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:01:48,813][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:01:48,814][257371] Reward + Measures: [[ -59.38537614    0.44399998    0.37090001    0.21000002    0.50190002
     4.88660526]
 [  52.23761653    0.15820001    0.81569999    0.50150001    0.87110007
     6.12434149]
 [ -18.93731441    0.3141        0.19430001    0.30309999    0.27650002
     4.46108103]
 ...
 [ -21.05542356    0.54680002    0.41230002    0.52369994    0.40759999
     3.53894114]
 [ -83.12580538    0.20240001    0.19410001    0.13430001    0.21830001
     5.48724127]
 [-172.20852425    0.12539999    0.54580003    0.23450001    0.67519999
     4.42470551]][0m
[37m[1m[2023-07-17 02:01:48,814][257371] Max Reward on eval: 247.58649017475545[0m
[37m[1m[2023-07-17 02:01:48,814][257371] Min Reward on eval: -534.1466827496886[0m
[37m[1m[2023-07-17 02:01:48,815][257371] Mean Reward across all agents: -25.25013401096058[0m
[37m[1m[2023-07-17 02:01:48,815][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:01:48,818][257371] mean_value=-572.8887328050182, max_value=287.8904732236639[0m
[37m[1m[2023-07-17 02:01:48,821][257371] New mean coefficients: [[ 0.62691677 -0.4784273   2.706771   -1.5954287  -0.35918623 -0.6236118 ]][0m
[37m[1m[2023-07-17 02:01:48,822][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:01:57,940][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 02:01:57,940][257371] FPS: 421247.83[0m
[36m[2023-07-17 02:01:57,942][257371] itr=448, itrs=2000, Progress: 22.40%[0m
[36m[2023-07-17 02:02:09,685][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 02:02:09,685][257371] FPS: 329079.50[0m
[36m[2023-07-17 02:02:13,985][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:02:13,985][257371] Reward + Measures: [[7.20930394 0.39417464 0.57037604 0.36985034 0.60681629 2.5316298 ]][0m
[37m[1m[2023-07-17 02:02:13,985][257371] Max Reward on eval: 7.209303935740958[0m
[37m[1m[2023-07-17 02:02:13,986][257371] Min Reward on eval: 7.209303935740958[0m
[37m[1m[2023-07-17 02:02:13,986][257371] Mean Reward across all agents: 7.209303935740958[0m
[37m[1m[2023-07-17 02:02:13,986][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:02:18,990][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:02:18,996][257371] Reward + Measures: [[-284.31426503    0.2378        0.34240001    0.47240001    0.54210001
     6.79032612]
 [ -35.96099606    0.13020001    0.182         0.14820002    0.17209999
     5.76656675]
 [  62.54784404    0.30630001    0.25799999    0.25790003    0.27270001
     4.48000002]
 ...
 [-163.11180936    0.35850003    0.1806        0.39720002    0.29679999
     5.40922165]
 [ -29.75106858    0.21510001    0.19690001    0.17309999    0.21949999
     4.63232946]
 [  17.5681795     0.0983        0.12330001    0.0971        0.1349
     5.52353048]][0m
[37m[1m[2023-07-17 02:02:18,997][257371] Max Reward on eval: 337.29295800291004[0m
[37m[1m[2023-07-17 02:02:18,998][257371] Min Reward on eval: -375.69510652944444[0m
[37m[1m[2023-07-17 02:02:18,998][257371] Mean Reward across all agents: -7.284424125919312[0m
[37m[1m[2023-07-17 02:02:18,999][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:02:19,006][257371] mean_value=-696.1129950906619, max_value=615.4405059606349[0m
[37m[1m[2023-07-17 02:02:19,010][257371] New mean coefficients: [[-0.62809     0.75012684  1.7312481  -2.259405    0.73548585 -0.8136271 ]][0m
[37m[1m[2023-07-17 02:02:19,012][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:02:28,081][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 02:02:28,081][257371] FPS: 423520.58[0m
[36m[2023-07-17 02:02:28,083][257371] itr=449, itrs=2000, Progress: 22.45%[0m
[36m[2023-07-17 02:02:39,744][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 02:02:39,744][257371] FPS: 331317.64[0m
[36m[2023-07-17 02:02:44,065][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:02:44,066][257371] Reward + Measures: [[-103.51971852    0.45263031    0.66632831    0.51357937    0.47851935
     2.5923655 ]][0m
[37m[1m[2023-07-17 02:02:44,066][257371] Max Reward on eval: -103.51971851990271[0m
[37m[1m[2023-07-17 02:02:44,066][257371] Min Reward on eval: -103.51971851990271[0m
[37m[1m[2023-07-17 02:02:44,067][257371] Mean Reward across all agents: -103.51971851990271[0m
[37m[1m[2023-07-17 02:02:44,067][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:02:49,256][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:02:49,256][257371] Reward + Measures: [[-15.09756352   0.2131       0.4269       0.27139997   0.45630002
    4.67458296]
 [ -8.39046725   0.25680003   0.32979998   0.19750001   0.36540002
    4.03221989]
 [ 21.46030957   0.10290001   0.1305       0.082        0.12049999
    5.2109189 ]
 ...
 [ 38.60336814   0.1508       0.14130001   0.0965       0.1497
    5.16298914]
 [ 16.12389519   0.42390004   0.33080003   0.33969998   0.35819998
    2.54149842]
 [-53.67441324   0.1715       0.22570001   0.19270001   0.22260001
    3.9932363 ]][0m
[37m[1m[2023-07-17 02:02:49,256][257371] Max Reward on eval: 287.7748541844543[0m
[37m[1m[2023-07-17 02:02:49,257][257371] Min Reward on eval: -473.15423391172664[0m
[37m[1m[2023-07-17 02:02:49,257][257371] Mean Reward across all agents: -7.790575472052054[0m
[37m[1m[2023-07-17 02:02:49,257][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:02:49,263][257371] mean_value=-1665.0695585557435, max_value=572.1136343307786[0m
[37m[1m[2023-07-17 02:02:49,265][257371] New mean coefficients: [[ 1.2200084   0.9036768   1.498997   -1.1802851  -0.19975436 -0.87161696]][0m
[37m[1m[2023-07-17 02:02:49,266][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:02:58,295][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 02:02:58,295][257371] FPS: 425383.79[0m
[36m[2023-07-17 02:02:58,298][257371] itr=450, itrs=2000, Progress: 22.50%[0m
[37m[1m[2023-07-17 02:05:43,939][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000430[0m
[36m[2023-07-17 02:05:56,323][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 02:05:56,323][257371] FPS: 326474.97[0m
[36m[2023-07-17 02:06:00,662][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:06:00,663][257371] Reward + Measures: [[-60.57801774   0.21231732   0.6327343    0.33998135   0.78262532
    3.27813339]][0m
[37m[1m[2023-07-17 02:06:00,663][257371] Max Reward on eval: -60.57801773758213[0m
[37m[1m[2023-07-17 02:06:00,663][257371] Min Reward on eval: -60.57801773758213[0m
[37m[1m[2023-07-17 02:06:00,664][257371] Mean Reward across all agents: -60.57801773758213[0m
[37m[1m[2023-07-17 02:06:00,664][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:06:05,581][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:06:05,587][257371] Reward + Measures: [[ -37.3487107     0.32360002    0.40499997    0.34090003    0.47620001
     3.71364021]
 [ -28.62203695    0.28290004    0.39030001    0.22239999    0.4147
     5.3273983 ]
 [-366.94909713    0.0482        0.68959999    0.68160003    0.71310002
     6.18168736]
 ...
 [  37.02205015    0.336         0.47510001    0.33500001    0.44010001
     2.88349843]
 [-180.3532572     0.06339999    0.65560001    0.6153        0.70350003
     5.2796874 ]
 [  -3.03082278    0.54699999    0.57460004    0.61560005    0.60729998
     5.24332476]][0m
[37m[1m[2023-07-17 02:06:05,587][257371] Max Reward on eval: 354.8772854777053[0m
[37m[1m[2023-07-17 02:06:05,587][257371] Min Reward on eval: -534.7414829672082[0m
[37m[1m[2023-07-17 02:06:05,588][257371] Mean Reward across all agents: -20.274518018258558[0m
[37m[1m[2023-07-17 02:06:05,588][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:06:05,594][257371] mean_value=-295.80162165565724, max_value=548.058351706451[0m
[37m[1m[2023-07-17 02:06:05,597][257371] New mean coefficients: [[ 0.4777938  -0.55050045  2.6203947  -0.36586392 -0.7079143  -0.60561514]][0m
[37m[1m[2023-07-17 02:06:05,598][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:06:14,561][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 02:06:14,561][257371] FPS: 428501.06[0m
[36m[2023-07-17 02:06:14,563][257371] itr=451, itrs=2000, Progress: 22.55%[0m
[36m[2023-07-17 02:06:26,134][257371] train() took 11.50 seconds to complete[0m
[36m[2023-07-17 02:06:26,135][257371] FPS: 333925.33[0m
[36m[2023-07-17 02:06:30,372][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:06:30,372][257371] Reward + Measures: [[-81.01135075   0.11295133   0.70488369   0.43420434   0.86753529
    3.14488482]][0m
[37m[1m[2023-07-17 02:06:30,373][257371] Max Reward on eval: -81.01135075013437[0m
[37m[1m[2023-07-17 02:06:30,373][257371] Min Reward on eval: -81.01135075013437[0m
[37m[1m[2023-07-17 02:06:30,373][257371] Mean Reward across all agents: -81.01135075013437[0m
[37m[1m[2023-07-17 02:06:30,373][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:06:35,326][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:06:35,326][257371] Reward + Measures: [[-23.89140388   0.0938       0.11210001   0.1036       0.1017
    4.46908998]
 [-76.92644515   0.2005       0.21960001   0.19710001   0.27169999
    4.70718288]
 [-88.58878      0.48660001   0.39000002   0.49589998   0.32739997
    6.48748636]
 ...
 [-70.40888995   0.0914       0.66509998   0.667        0.68620002
    6.31269121]
 [ 77.25764293   0.3391       0.31290004   0.285        0.296
    5.04653311]
 [-10.16307882   0.23109999   0.2172       0.23260002   0.21760002
    5.77788258]][0m
[37m[1m[2023-07-17 02:06:35,326][257371] Max Reward on eval: 163.05663681877778[0m
[37m[1m[2023-07-17 02:06:35,327][257371] Min Reward on eval: -303.9602601975668[0m
[37m[1m[2023-07-17 02:06:35,327][257371] Mean Reward across all agents: -22.05893477417941[0m
[37m[1m[2023-07-17 02:06:35,327][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:06:35,329][257371] mean_value=-1372.6709573148983, max_value=307.773206826796[0m
[37m[1m[2023-07-17 02:06:35,331][257371] New mean coefficients: [[ 1.6170319   0.9353059   1.4280459   0.26416677 -0.74034655 -0.5167701 ]][0m
[37m[1m[2023-07-17 02:06:35,332][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:06:44,289][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 02:06:44,289][257371] FPS: 428795.44[0m
[36m[2023-07-17 02:06:44,291][257371] itr=452, itrs=2000, Progress: 22.60%[0m
[36m[2023-07-17 02:06:55,909][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 02:06:55,909][257371] FPS: 332578.03[0m
[36m[2023-07-17 02:07:00,208][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:07:00,208][257371] Reward + Measures: [[-327.35137612    0.017511      0.93981534    0.88610172    0.95387435
     4.12847662]][0m
[37m[1m[2023-07-17 02:07:00,208][257371] Max Reward on eval: -327.35137611683956[0m
[37m[1m[2023-07-17 02:07:00,209][257371] Min Reward on eval: -327.35137611683956[0m
[37m[1m[2023-07-17 02:07:00,209][257371] Mean Reward across all agents: -327.35137611683956[0m
[37m[1m[2023-07-17 02:07:00,209][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:07:05,183][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:07:05,183][257371] Reward + Measures: [[  49.977464      0.22690001    0.24560001    0.2316        0.27690002
     5.53570938]
 [  65.86272584    0.2872        0.57539999    0.1857        0.63239998
     3.00317073]
 [-130.22076386    0.12390001    0.1074        0.15760002    0.1821
     6.54996824]
 ...
 [  35.85990297    0.1583        0.2622        0.1718        0.2841
     4.28638554]
 [  53.01563558    0.2089        0.32810003    0.25890002    0.35879999
     6.24538898]
 [ -28.84753756    0.0077        0.9558        0.94960004    0.96999997
     6.39432001]][0m
[37m[1m[2023-07-17 02:07:05,183][257371] Max Reward on eval: 342.11949448455124[0m
[37m[1m[2023-07-17 02:07:05,184][257371] Min Reward on eval: -513.5707426208537[0m
[37m[1m[2023-07-17 02:07:05,184][257371] Mean Reward across all agents: -25.137988131369234[0m
[37m[1m[2023-07-17 02:07:05,184][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:07:05,188][257371] mean_value=-773.7009668524694, max_value=595.4854631419221[0m
[37m[1m[2023-07-17 02:07:05,190][257371] New mean coefficients: [[ 1.1167415   0.4372303   1.8279593   1.6800148  -0.8977045  -0.39077687]][0m
[37m[1m[2023-07-17 02:07:05,191][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:07:14,210][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 02:07:14,211][257371] FPS: 425843.01[0m
[36m[2023-07-17 02:07:14,213][257371] itr=453, itrs=2000, Progress: 22.65%[0m
[36m[2023-07-17 02:07:26,174][257371] train() took 11.89 seconds to complete[0m
[36m[2023-07-17 02:07:26,174][257371] FPS: 323011.60[0m
[36m[2023-07-17 02:07:30,520][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:07:30,520][257371] Reward + Measures: [[-7.3038764   0.31628767  0.65343732  0.39713833  0.70921123  3.92447186]][0m
[37m[1m[2023-07-17 02:07:30,520][257371] Max Reward on eval: -7.303876396372235[0m
[37m[1m[2023-07-17 02:07:30,521][257371] Min Reward on eval: -7.303876396372235[0m
[37m[1m[2023-07-17 02:07:30,521][257371] Mean Reward across all agents: -7.303876396372235[0m
[37m[1m[2023-07-17 02:07:30,521][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:07:35,580][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:07:35,580][257371] Reward + Measures: [[25.3652762   0.27220002  0.42130002  0.3057      0.45830002  3.79259944]
 [50.85529898  0.23310001  0.3409      0.34460002  0.37830001  5.51325369]
 [16.79961082  0.33310005  0.50080001  0.31220001  0.66820002  4.1114192 ]
 ...
 [36.41525882  0.16589999  0.25060001  0.16399999  0.22690001  4.94548321]
 [72.07188388  0.2577      0.42969999  0.3053      0.40809998  4.55765438]
 [27.10604407  0.1323      0.1199      0.16240001  0.17160001  6.15542269]][0m
[37m[1m[2023-07-17 02:07:35,580][257371] Max Reward on eval: 420.4436932059936[0m
[37m[1m[2023-07-17 02:07:35,581][257371] Min Reward on eval: -593.4040246042772[0m
[37m[1m[2023-07-17 02:07:35,581][257371] Mean Reward across all agents: -1.1593311924378709[0m
[37m[1m[2023-07-17 02:07:35,581][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:07:35,584][257371] mean_value=-360.47719857024646, max_value=637.009904413506[0m
[37m[1m[2023-07-17 02:07:35,587][257371] New mean coefficients: [[ 1.1404775   0.6004548   0.83045137  1.6285614  -1.015829   -0.34763354]][0m
[37m[1m[2023-07-17 02:07:35,588][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:07:44,657][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 02:07:44,657][257371] FPS: 423495.75[0m
[36m[2023-07-17 02:07:44,660][257371] itr=454, itrs=2000, Progress: 22.70%[0m
[36m[2023-07-17 02:07:56,412][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 02:07:56,412][257371] FPS: 328713.99[0m
[36m[2023-07-17 02:08:00,864][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:08:00,864][257371] Reward + Measures: [[9.64700452 0.32478267 0.61512965 0.36785367 0.64970231 4.373734  ]][0m
[37m[1m[2023-07-17 02:08:00,864][257371] Max Reward on eval: 9.647004521280392[0m
[37m[1m[2023-07-17 02:08:00,865][257371] Min Reward on eval: 9.647004521280392[0m
[37m[1m[2023-07-17 02:08:00,865][257371] Mean Reward across all agents: 9.647004521280392[0m
[37m[1m[2023-07-17 02:08:00,865][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:08:06,094][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:08:06,095][257371] Reward + Measures: [[100.82753356   0.91040003   0.9163       0.9084       0.9332
    6.62521744]
 [ 21.65091349   0.86139995   0.89919996   0.86010009   0.89390004
    5.96152449]
 [ 56.7756243    0.36300001   0.38370004   0.39250001   0.3935
    4.06841326]
 ...
 [ 63.49801602   0.61940002   0.59129995   0.5776       0.63700002
    6.34379911]
 [-23.4035351    0.2005       0.38529998   0.2739       0.41540003
    4.82983065]
 [ -7.84254228   0.17290001   0.20370002   0.1248       0.18630001
    4.95943403]][0m
[37m[1m[2023-07-17 02:08:06,095][257371] Max Reward on eval: 335.8487239412963[0m
[37m[1m[2023-07-17 02:08:06,095][257371] Min Reward on eval: -424.0530815288424[0m
[37m[1m[2023-07-17 02:08:06,096][257371] Mean Reward across all agents: 13.61871916950009[0m
[37m[1m[2023-07-17 02:08:06,096][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:08:06,103][257371] mean_value=-161.46261194630438, max_value=553.0528059227777[0m
[37m[1m[2023-07-17 02:08:06,105][257371] New mean coefficients: [[ 1.6587424  -0.80407894  1.1464893   1.3113748  -1.9046314  -0.5785502 ]][0m
[37m[1m[2023-07-17 02:08:06,106][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:08:15,176][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 02:08:15,176][257371] FPS: 423483.95[0m
[36m[2023-07-17 02:08:15,178][257371] itr=455, itrs=2000, Progress: 22.75%[0m
[36m[2023-07-17 02:08:26,985][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 02:08:26,986][257371] FPS: 327190.94[0m
[36m[2023-07-17 02:08:31,288][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:08:31,289][257371] Reward + Measures: [[13.40036166  0.31074366  0.60557503  0.35966367  0.64102501  4.31395483]][0m
[37m[1m[2023-07-17 02:08:31,289][257371] Max Reward on eval: 13.400361657061516[0m
[37m[1m[2023-07-17 02:08:31,289][257371] Min Reward on eval: 13.400361657061516[0m
[37m[1m[2023-07-17 02:08:31,290][257371] Mean Reward across all agents: 13.400361657061516[0m
[37m[1m[2023-07-17 02:08:31,290][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:08:36,226][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:08:36,226][257371] Reward + Measures: [[  4.87185781   0.1165       0.25750002   0.21030001   0.3213
    4.58618164]
 [ 43.97202412   0.1346       0.14140001   0.1236       0.19350001
    4.91250849]
 [ 31.32907155   0.23440002   0.30700001   0.1622       0.36160001
    5.44757318]
 ...
 [-41.64096798   0.6455       0.66549999   0.60050005   0.70710003
    5.96829176]
 [-73.32443919   0.33860001   0.55660003   0.35859999   0.51990002
    5.28191662]
 [ -2.57841251   0.16069999   0.15910001   0.1569       0.21589999
    5.07277441]][0m
[37m[1m[2023-07-17 02:08:36,226][257371] Max Reward on eval: 386.8069099901244[0m
[37m[1m[2023-07-17 02:08:36,227][257371] Min Reward on eval: -523.0992605168373[0m
[37m[1m[2023-07-17 02:08:36,227][257371] Mean Reward across all agents: -3.9024331126838607[0m
[37m[1m[2023-07-17 02:08:36,227][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:08:36,231][257371] mean_value=-663.5288305045673, max_value=379.4760890287187[0m
[37m[1m[2023-07-17 02:08:36,233][257371] New mean coefficients: [[ 0.57072115 -1.9489326   1.4239312   1.3311123  -1.8412251  -0.42059952]][0m
[37m[1m[2023-07-17 02:08:36,234][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:08:45,236][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 02:08:45,237][257371] FPS: 426637.17[0m
[36m[2023-07-17 02:08:45,239][257371] itr=456, itrs=2000, Progress: 22.80%[0m
[36m[2023-07-17 02:08:56,843][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 02:08:56,843][257371] FPS: 333009.24[0m
[36m[2023-07-17 02:09:01,226][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:09:01,226][257371] Reward + Measures: [[-0.25545663  0.32066467  0.55979264  0.37783167  0.60943532  4.09458447]][0m
[37m[1m[2023-07-17 02:09:01,227][257371] Max Reward on eval: -0.255456633710089[0m
[37m[1m[2023-07-17 02:09:01,227][257371] Min Reward on eval: -0.255456633710089[0m
[37m[1m[2023-07-17 02:09:01,227][257371] Mean Reward across all agents: -0.255456633710089[0m
[37m[1m[2023-07-17 02:09:01,227][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:09:06,245][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:09:06,245][257371] Reward + Measures: [[-7.53850417  0.2326      0.56630003  0.71179998  0.75220001  6.74876165]
 [81.66959694  0.17999999  0.2079      0.2631      0.2404      4.75256824]
 [45.82882494  0.25289997  0.4296      0.1693      0.48639998  5.71184969]
 ...
 [43.11501642  0.23609999  0.37939999  0.2247      0.42160001  4.28126192]
 [21.69937234  0.35010001  0.41960001  0.1019      0.61270005  5.59429312]
 [11.20600461  0.11740001  0.17980002  0.1141      0.16950001  4.54085493]][0m
[37m[1m[2023-07-17 02:09:06,245][257371] Max Reward on eval: 677.8612937870552[0m
[37m[1m[2023-07-17 02:09:06,246][257371] Min Reward on eval: -641.954109166516[0m
[37m[1m[2023-07-17 02:09:06,246][257371] Mean Reward across all agents: -9.616180064214875[0m
[37m[1m[2023-07-17 02:09:06,246][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:09:06,251][257371] mean_value=-463.0231171952634, max_value=907.9978324015159[0m
[37m[1m[2023-07-17 02:09:06,254][257371] New mean coefficients: [[ 0.13126516 -1.9591104   1.1742585   3.016303   -0.81042135  0.14406967]][0m
[37m[1m[2023-07-17 02:09:06,255][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:09:15,306][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 02:09:15,306][257371] FPS: 424352.18[0m
[36m[2023-07-17 02:09:15,309][257371] itr=457, itrs=2000, Progress: 22.85%[0m
[36m[2023-07-17 02:09:27,056][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 02:09:27,056][257371] FPS: 328907.29[0m
[36m[2023-07-17 02:09:31,400][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:09:31,401][257371] Reward + Measures: [[-221.39072956    0.05342666    0.83558005    0.7691747     0.86845195
     4.56341267]][0m
[37m[1m[2023-07-17 02:09:31,401][257371] Max Reward on eval: -221.39072956431067[0m
[37m[1m[2023-07-17 02:09:31,401][257371] Min Reward on eval: -221.39072956431067[0m
[37m[1m[2023-07-17 02:09:31,402][257371] Mean Reward across all agents: -221.39072956431067[0m
[37m[1m[2023-07-17 02:09:31,402][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:09:36,438][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:09:36,438][257371] Reward + Measures: [[-75.75263372   0.1715       0.4686       0.35839999   0.48559999
    3.71979189]
 [-15.00390171   0.28569999   0.39309999   0.29190001   0.4111
    5.7954073 ]
 [-73.08935332   0.28050002   0.48810002   0.24319999   0.48190004
    5.88150358]
 ...
 [111.63893535   0.1788       0.20809999   0.14670001   0.22550002
    6.58808613]
 [-30.58019932   0.2904       0.49119997   0.33919999   0.53109998
    5.36481237]
 [  6.42710373   0.1653       0.20310001   0.18970001   0.26340002
    4.63435173]][0m
[37m[1m[2023-07-17 02:09:36,439][257371] Max Reward on eval: 278.43330615162847[0m
[37m[1m[2023-07-17 02:09:36,439][257371] Min Reward on eval: -487.79507876280695[0m
[37m[1m[2023-07-17 02:09:36,439][257371] Mean Reward across all agents: -22.047256112603062[0m
[37m[1m[2023-07-17 02:09:36,439][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:09:36,443][257371] mean_value=-200.36302775851797, max_value=585.2447989801643[0m
[37m[1m[2023-07-17 02:09:36,446][257371] New mean coefficients: [[ 0.3293906  -2.0276563   1.0277562   3.3666399  -0.85029966  0.08185129]][0m
[37m[1m[2023-07-17 02:09:36,447][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:09:45,494][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 02:09:45,495][257371] FPS: 424513.10[0m
[36m[2023-07-17 02:09:45,497][257371] itr=458, itrs=2000, Progress: 22.90%[0m
[36m[2023-07-17 02:09:57,574][257371] train() took 12.00 seconds to complete[0m
[36m[2023-07-17 02:09:57,575][257371] FPS: 319927.68[0m
[36m[2023-07-17 02:10:01,873][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:10:01,873][257371] Reward + Measures: [[-23.34218743   0.32729301   0.47862932   0.36299565   0.52446198
    3.98689222]][0m
[37m[1m[2023-07-17 02:10:01,873][257371] Max Reward on eval: -23.342187434125208[0m
[37m[1m[2023-07-17 02:10:01,874][257371] Min Reward on eval: -23.342187434125208[0m
[37m[1m[2023-07-17 02:10:01,874][257371] Mean Reward across all agents: -23.342187434125208[0m
[37m[1m[2023-07-17 02:10:01,874][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:10:06,851][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:10:06,851][257371] Reward + Measures: [[119.8387206    0.36460003   0.47409996   0.1945       0.36760002
    5.81378365]
 [ 19.55115076   0.3705       0.3556       0.24199998   0.41139999
    5.44804096]
 [ -3.6288565    0.28389999   0.28850004   0.26620001   0.24439998
    4.65144205]
 ...
 [116.31764744   0.47629997   0.7177       0.46339998   0.7349
    5.50178194]
 [-80.78508367   0.29100004   0.59700006   0.25229999   0.61900002
    5.47463036]
 [-29.08322447   0.32650003   0.34800002   0.2705       0.2823
    5.79229689]][0m
[37m[1m[2023-07-17 02:10:06,851][257371] Max Reward on eval: 324.76424144413323[0m
[37m[1m[2023-07-17 02:10:06,852][257371] Min Reward on eval: -310.0589385058731[0m
[37m[1m[2023-07-17 02:10:06,852][257371] Mean Reward across all agents: 3.9353449523678865[0m
[37m[1m[2023-07-17 02:10:06,852][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:10:06,858][257371] mean_value=-238.154724439349, max_value=686.9901619868315[0m
[37m[1m[2023-07-17 02:10:06,861][257371] New mean coefficients: [[ 0.7608164 -2.253783   0.7216381  2.2662551 -2.8397093 -0.6440151]][0m
[37m[1m[2023-07-17 02:10:06,862][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:10:15,854][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 02:10:15,854][257371] FPS: 427092.26[0m
[36m[2023-07-17 02:10:15,857][257371] itr=459, itrs=2000, Progress: 22.95%[0m
[36m[2023-07-17 02:10:27,505][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 02:10:27,505][257371] FPS: 331698.61[0m
[36m[2023-07-17 02:10:31,829][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:10:31,830][257371] Reward + Measures: [[-60.37638789   0.25762564   0.56380731   0.35356799   0.54265434
    4.24189663]][0m
[37m[1m[2023-07-17 02:10:31,830][257371] Max Reward on eval: -60.37638789046963[0m
[37m[1m[2023-07-17 02:10:31,830][257371] Min Reward on eval: -60.37638789046963[0m
[37m[1m[2023-07-17 02:10:31,830][257371] Mean Reward across all agents: -60.37638789046963[0m
[37m[1m[2023-07-17 02:10:31,831][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:10:37,066][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:10:37,067][257371] Reward + Measures: [[ 55.08832044   0.2067       0.50780004   0.27670002   0.41850001
    4.24961042]
 [-43.72435528   0.1556       0.47420001   0.21789999   0.37450001
    4.99915075]
 [ 31.46827492   0.442        0.48609996   0.1944       0.45879999
    5.1693244 ]
 ...
 [ 63.01346824   0.25760001   0.41319999   0.25799999   0.35499999
    4.44771051]
 [ 53.5875444    0.17580001   0.33410004   0.21260002   0.29780003
    4.40183973]
 [ 59.92270841   0.21529999   0.65149999   0.3143       0.5226
    4.65537882]][0m
[37m[1m[2023-07-17 02:10:37,067][257371] Max Reward on eval: 257.72252462832256[0m
[37m[1m[2023-07-17 02:10:37,067][257371] Min Reward on eval: -296.120114839077[0m
[37m[1m[2023-07-17 02:10:37,067][257371] Mean Reward across all agents: -0.5851224122968371[0m
[37m[1m[2023-07-17 02:10:37,068][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:10:37,071][257371] mean_value=-528.665555249386, max_value=507.6397646751813[0m
[37m[1m[2023-07-17 02:10:37,074][257371] New mean coefficients: [[ 0.09135669 -2.0546892   1.202966    3.238803   -1.4476427  -0.6214123 ]][0m
[37m[1m[2023-07-17 02:10:37,075][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:10:46,164][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 02:10:46,164][257371] FPS: 422564.73[0m
[36m[2023-07-17 02:10:46,167][257371] itr=460, itrs=2000, Progress: 23.00%[0m
[37m[1m[2023-07-17 02:13:42,214][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000440[0m
[36m[2023-07-17 02:13:54,347][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 02:13:54,347][257371] FPS: 330453.09[0m
[36m[2023-07-17 02:13:58,486][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:13:58,486][257371] Reward + Measures: [[-63.77480482   0.25364867   0.57022995   0.37261465   0.55091298
    4.21786499]][0m
[37m[1m[2023-07-17 02:13:58,486][257371] Max Reward on eval: -63.77480482475153[0m
[37m[1m[2023-07-17 02:13:58,487][257371] Min Reward on eval: -63.77480482475153[0m
[37m[1m[2023-07-17 02:13:58,487][257371] Mean Reward across all agents: -63.77480482475153[0m
[37m[1m[2023-07-17 02:13:58,487][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:14:03,361][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:14:03,361][257371] Reward + Measures: [[  49.68105943    0.25100002    0.21610001    0.14760001    0.21250001
     5.38787174]
 [ -19.98102552    0.0664        0.73480004    0.44790003    0.74250001
     5.19201422]
 [-157.04232895    0.29600003    0.3272        0.44020006    0.51389998
     5.36597013]
 ...
 [ -50.08350768    0.2139        0.3204        0.2089        0.255
     5.78282309]
 [   3.30276405    0.18900001    0.13460001    0.13239999    0.16430001
     5.72028828]
 [ -24.50489202    0.26279998    0.43249997    0.27139997    0.46059999
     5.19465685]][0m
[37m[1m[2023-07-17 02:14:03,362][257371] Max Reward on eval: 231.41523603368552[0m
[37m[1m[2023-07-17 02:14:03,362][257371] Min Reward on eval: -400.772183387354[0m
[37m[1m[2023-07-17 02:14:03,362][257371] Mean Reward across all agents: -38.069569349142064[0m
[37m[1m[2023-07-17 02:14:03,362][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:14:03,365][257371] mean_value=-291.2623332198648, max_value=370.91154959211127[0m
[37m[1m[2023-07-17 02:14:03,368][257371] New mean coefficients: [[ 0.09759305 -0.85359013  1.0935512   3.8414173  -0.12112319 -0.770815  ]][0m
[37m[1m[2023-07-17 02:14:03,369][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:14:12,328][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 02:14:12,328][257371] FPS: 428681.99[0m
[36m[2023-07-17 02:14:12,331][257371] itr=461, itrs=2000, Progress: 23.05%[0m
[36m[2023-07-17 02:14:24,026][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 02:14:24,027][257371] FPS: 330339.50[0m
[36m[2023-07-17 02:14:28,320][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:14:28,321][257371] Reward + Measures: [[-687.47450316    0.02125867    0.9588846     0.93813425    0.95823729
     6.09443569]][0m
[37m[1m[2023-07-17 02:14:28,321][257371] Max Reward on eval: -687.4745031641057[0m
[37m[1m[2023-07-17 02:14:28,321][257371] Min Reward on eval: -687.4745031641057[0m
[37m[1m[2023-07-17 02:14:28,321][257371] Mean Reward across all agents: -687.4745031641057[0m
[37m[1m[2023-07-17 02:14:28,321][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:14:33,329][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:14:33,330][257371] Reward + Measures: [[  47.62355255    0.18009999    0.22870003    0.1674        0.2177
     5.6289196 ]
 [ -86.01071547    0.1645        0.73520005    0.64899999    0.75980002
     6.58487034]
 [-448.9801884     0.031         0.90009993    0.83669996    0.90450001
     6.32839918]
 ...
 [ -64.85779467    0.1435        0.28780001    0.23169999    0.35780001
     4.45094442]
 [ -24.19783472    0.0409        0.55179995    0.52950001    0.59630007
     6.78249884]
 [ -17.90231596    0.3409        0.2929        0.3497        0.3779
     5.18071651]][0m
[37m[1m[2023-07-17 02:14:33,330][257371] Max Reward on eval: 422.8817893203348[0m
[37m[1m[2023-07-17 02:14:33,330][257371] Min Reward on eval: -486.21428729593754[0m
[37m[1m[2023-07-17 02:14:33,331][257371] Mean Reward across all agents: -11.086304305640155[0m
[37m[1m[2023-07-17 02:14:33,331][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:14:33,335][257371] mean_value=-248.98336348484415, max_value=603.6177645057462[0m
[37m[1m[2023-07-17 02:14:33,337][257371] New mean coefficients: [[ 0.74337935 -1.4466796   1.5876926   3.5314906  -0.31913558 -1.0212649 ]][0m
[37m[1m[2023-07-17 02:14:33,338][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:14:42,367][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 02:14:42,368][257371] FPS: 425373.11[0m
[36m[2023-07-17 02:14:42,370][257371] itr=462, itrs=2000, Progress: 23.10%[0m
[36m[2023-07-17 02:14:54,174][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 02:14:54,175][257371] FPS: 327361.58[0m
[36m[2023-07-17 02:14:58,428][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:14:58,428][257371] Reward + Measures: [[-245.06414672    0.04086567    0.830006      0.79599899    0.86081868
     4.35495186]][0m
[37m[1m[2023-07-17 02:14:58,428][257371] Max Reward on eval: -245.06414672190843[0m
[37m[1m[2023-07-17 02:14:58,428][257371] Min Reward on eval: -245.06414672190843[0m
[37m[1m[2023-07-17 02:14:58,429][257371] Mean Reward across all agents: -245.06414672190843[0m
[37m[1m[2023-07-17 02:14:58,429][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:15:03,470][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:15:03,471][257371] Reward + Measures: [[ -51.7392533     0.2084        0.43439999    0.31799999    0.45170003
     3.89637876]
 [ -37.31594765    0.23699999    0.34559998    0.24300002    0.36320001
     4.32716322]
 [  -1.51240315    0.25170001    0.13490002    0.27560002    0.24240001
     5.19928694]
 ...
 [-203.91648577    0.90150005    0.88290006    0.90740007    0.90430003
     6.10911036]
 [ -57.39489071    0.28449997    0.2278        0.28270003    0.20369999
     4.92586708]
 [ -10.82650919    0.2386        0.37360001    0.27110001    0.27680001
     4.8329339 ]][0m
[37m[1m[2023-07-17 02:15:03,471][257371] Max Reward on eval: 256.2252983650193[0m
[37m[1m[2023-07-17 02:15:03,471][257371] Min Reward on eval: -493.53756998712197[0m
[37m[1m[2023-07-17 02:15:03,472][257371] Mean Reward across all agents: -42.23585589172375[0m
[37m[1m[2023-07-17 02:15:03,472][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:15:03,475][257371] mean_value=-451.6124991112002, max_value=141.3889629657225[0m
[37m[1m[2023-07-17 02:15:03,477][257371] New mean coefficients: [[ 0.13963568 -2.090982    0.9782983   2.7957606  -0.5119892  -1.4252982 ]][0m
[37m[1m[2023-07-17 02:15:03,478][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:15:12,426][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 02:15:12,426][257371] FPS: 429247.95[0m
[36m[2023-07-17 02:15:12,428][257371] itr=463, itrs=2000, Progress: 23.15%[0m
[36m[2023-07-17 02:15:24,130][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 02:15:24,130][257371] FPS: 330269.37[0m
[36m[2023-07-17 02:15:28,513][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:15:28,518][257371] Reward + Measures: [[-194.23984159    0.03530833    0.84479696    0.84001368    0.88745326
     3.91808057]][0m
[37m[1m[2023-07-17 02:15:28,518][257371] Max Reward on eval: -194.23984158545406[0m
[37m[1m[2023-07-17 02:15:28,519][257371] Min Reward on eval: -194.23984158545406[0m
[37m[1m[2023-07-17 02:15:28,519][257371] Mean Reward across all agents: -194.23984158545406[0m
[37m[1m[2023-07-17 02:15:28,519][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:15:33,737][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:15:33,743][257371] Reward + Measures: [[-99.45696941   0.07140001   0.78509998   0.74480003   0.79939997
    4.88858032]
 [ 16.94604196   0.33860001   0.3619       0.37290001   0.35349998
    4.36548519]
 [ 10.58396036   0.28720003   0.26410002   0.29550001   0.37560001
    4.66715717]
 ...
 [ 39.22741463   0.30829999   0.32889998   0.27430001   0.38420001
    5.22466135]
 [108.79556462   0.26920006   0.26180002   0.2658       0.28120002
    6.16697454]
 [ 75.90666299   0.1793       0.16620001   0.13790001   0.1745
    5.24793386]][0m
[37m[1m[2023-07-17 02:15:33,743][257371] Max Reward on eval: 357.1587152339518[0m
[37m[1m[2023-07-17 02:15:33,744][257371] Min Reward on eval: -403.8284950407222[0m
[37m[1m[2023-07-17 02:15:33,744][257371] Mean Reward across all agents: 29.394910647340094[0m
[37m[1m[2023-07-17 02:15:33,744][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:15:33,747][257371] mean_value=-549.0890575360688, max_value=392.287222085343[0m
[37m[1m[2023-07-17 02:15:33,750][257371] New mean coefficients: [[ 1.5546937 -1.3217232  0.5888944  2.337292  -1.3686802 -1.5549469]][0m
[37m[1m[2023-07-17 02:15:33,751][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:15:42,890][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 02:15:42,890][257371] FPS: 420239.96[0m
[36m[2023-07-17 02:15:42,892][257371] itr=464, itrs=2000, Progress: 23.20%[0m
[36m[2023-07-17 02:15:54,710][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 02:15:54,711][257371] FPS: 326996.81[0m
[36m[2023-07-17 02:15:58,984][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:15:58,984][257371] Reward + Measures: [[-197.34039975    0.043867      0.83719337    0.82276464    0.86851734
     3.85306549]][0m
[37m[1m[2023-07-17 02:15:58,985][257371] Max Reward on eval: -197.34039974859294[0m
[37m[1m[2023-07-17 02:15:58,985][257371] Min Reward on eval: -197.34039974859294[0m
[37m[1m[2023-07-17 02:15:58,985][257371] Mean Reward across all agents: -197.34039974859294[0m
[37m[1m[2023-07-17 02:15:58,985][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:16:04,006][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:16:04,013][257371] Reward + Measures: [[-278.77554847    0.80419999    0.71199995    0.81090003    0.13940001
     6.30471516]
 [ -40.35970352    0.40060002    0.41869998    0.46890002    0.51640004
     4.79106522]
 [  30.12787435    0.22360002    0.39780003    0.27169999    0.43069997
     4.54019403]
 ...
 [ -85.29170894    0.22939999    0.244         0.31860003    0.28140002
     4.83352804]
 [  34.65811873    0.43880001    0.45390001    0.42300001    0.40300003
     7.19820547]
 [  -4.53013303    0.5363        0.54689997    0.60860002    0.55140001
     5.65011978]][0m
[37m[1m[2023-07-17 02:16:04,013][257371] Max Reward on eval: 450.94322777353227[0m
[37m[1m[2023-07-17 02:16:04,013][257371] Min Reward on eval: -505.1276741050184[0m
[37m[1m[2023-07-17 02:16:04,014][257371] Mean Reward across all agents: 8.959760929127205[0m
[37m[1m[2023-07-17 02:16:04,014][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:16:04,018][257371] mean_value=-558.7415645115593, max_value=425.2909570646069[0m
[37m[1m[2023-07-17 02:16:04,020][257371] New mean coefficients: [[ 0.7389926  -0.6791338   1.5142696   1.7668432  -0.08575344 -1.433264  ]][0m
[37m[1m[2023-07-17 02:16:04,021][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:16:13,193][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-17 02:16:13,193][257371] FPS: 418771.54[0m
[36m[2023-07-17 02:16:13,196][257371] itr=465, itrs=2000, Progress: 23.25%[0m
[36m[2023-07-17 02:16:25,010][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 02:16:25,010][257371] FPS: 327065.47[0m
[36m[2023-07-17 02:16:29,241][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:16:29,241][257371] Reward + Measures: [[35.52883636  0.37918568  0.37597233  0.39624968  0.51123065  3.19663548]][0m
[37m[1m[2023-07-17 02:16:29,241][257371] Max Reward on eval: 35.52883635554048[0m
[37m[1m[2023-07-17 02:16:29,242][257371] Min Reward on eval: 35.52883635554048[0m
[37m[1m[2023-07-17 02:16:29,242][257371] Mean Reward across all agents: 35.52883635554048[0m
[37m[1m[2023-07-17 02:16:29,242][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:16:34,200][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:16:34,200][257371] Reward + Measures: [[21.39188681  0.30949998  0.40470001  0.32189998  0.37719998  4.30547714]
 [20.72609195  0.24489999  0.52590001  0.49019995  0.62970001  4.38158894]
 [79.17950388  0.35320002  0.36839998  0.32260004  0.38240001  5.14290762]
 ...
 [99.96174526  0.21700001  0.55120003  0.2667      0.5255      5.05438709]
 [44.77948861  0.29519999  0.28        0.2599      0.38229999  3.77869606]
 [69.59964066  0.52869999  0.1989      0.50800002  0.25670001  4.40672493]][0m
[37m[1m[2023-07-17 02:16:34,201][257371] Max Reward on eval: 198.32963466867804[0m
[37m[1m[2023-07-17 02:16:34,201][257371] Min Reward on eval: -497.8120560802519[0m
[37m[1m[2023-07-17 02:16:34,201][257371] Mean Reward across all agents: 20.783024735704487[0m
[37m[1m[2023-07-17 02:16:34,201][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:16:34,204][257371] mean_value=-501.53942398851206, max_value=428.4143694670965[0m
[37m[1m[2023-07-17 02:16:34,206][257371] New mean coefficients: [[ 1.7449154 -0.9725127  1.3172867  2.3960102 -0.5879363 -1.3163005]][0m
[37m[1m[2023-07-17 02:16:34,207][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:16:43,214][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 02:16:43,215][257371] FPS: 426417.36[0m
[36m[2023-07-17 02:16:43,217][257371] itr=466, itrs=2000, Progress: 23.30%[0m
[36m[2023-07-17 02:16:54,882][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 02:16:54,882][257371] FPS: 331204.17[0m
[36m[2023-07-17 02:16:59,148][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:16:59,149][257371] Reward + Measures: [[41.96775343  0.38343564  0.37195101  0.40296629  0.50986367  3.15973496]][0m
[37m[1m[2023-07-17 02:16:59,149][257371] Max Reward on eval: 41.9677534346545[0m
[37m[1m[2023-07-17 02:16:59,149][257371] Min Reward on eval: 41.9677534346545[0m
[37m[1m[2023-07-17 02:16:59,149][257371] Mean Reward across all agents: 41.9677534346545[0m
[37m[1m[2023-07-17 02:16:59,150][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:17:04,147][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:17:04,148][257371] Reward + Measures: [[254.08205228   0.331        0.45510003   0.37789997   0.53590006
    4.2736249 ]
 [ 81.54392575   0.14710002   0.17040001   0.19580001   0.19600001
    4.70543814]
 [-49.92040232   0.1013       0.1089       0.11979999   0.112
    5.81265974]
 ...
 [-11.38756892   0.27519998   0.35600004   0.41710001   0.46210003
    3.86228156]
 [ 80.89913316   0.164        0.1416       0.2242       0.21399999
    5.81298065]
 [ -8.6436217    0.27040002   0.45440003   0.45030004   0.6257
    4.55206871]][0m
[37m[1m[2023-07-17 02:17:04,148][257371] Max Reward on eval: 428.8000145232305[0m
[37m[1m[2023-07-17 02:17:04,148][257371] Min Reward on eval: -243.178007279709[0m
[37m[1m[2023-07-17 02:17:04,148][257371] Mean Reward across all agents: 59.83616142210128[0m
[37m[1m[2023-07-17 02:17:04,149][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:17:04,155][257371] mean_value=-276.54348756080725, max_value=455.5755114140956[0m
[37m[1m[2023-07-17 02:17:04,157][257371] New mean coefficients: [[ 2.79603     0.45853198  1.8629448   2.3936765   0.03664362 -0.79800344]][0m
[37m[1m[2023-07-17 02:17:04,158][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:17:13,180][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 02:17:13,181][257371] FPS: 425695.32[0m
[36m[2023-07-17 02:17:13,183][257371] itr=467, itrs=2000, Progress: 23.35%[0m
[36m[2023-07-17 02:17:24,921][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 02:17:24,921][257371] FPS: 329162.74[0m
[36m[2023-07-17 02:17:29,176][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:17:29,176][257371] Reward + Measures: [[52.00893952  0.39343968  0.35799229  0.41189766  0.50854963  3.12442446]][0m
[37m[1m[2023-07-17 02:17:29,176][257371] Max Reward on eval: 52.00893951763589[0m
[37m[1m[2023-07-17 02:17:29,177][257371] Min Reward on eval: 52.00893951763589[0m
[37m[1m[2023-07-17 02:17:29,177][257371] Mean Reward across all agents: 52.00893951763589[0m
[37m[1m[2023-07-17 02:17:29,177][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:17:34,227][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:17:34,233][257371] Reward + Measures: [[ -12.51561098    0.1163        0.28580001    0.13090001    0.34400001
     5.81528521]
 [ -48.68756704    0.2474        0.51209998    0.183         0.50959998
     5.32826567]
 [-158.13166189    0.13850001    0.25299999    0.25620002    0.31729999
     5.03327131]
 ...
 [  34.77089047    0.0804        0.138         0.1045        0.1586
     5.62437916]
 [-161.98520646    0.07230001    0.625         0.58880001    0.62510002
     4.77830982]
 [ -32.06621072    0.19590001    0.1682        0.27710003    0.1874
     5.62522078]][0m
[37m[1m[2023-07-17 02:17:34,234][257371] Max Reward on eval: 145.23763621114193[0m
[37m[1m[2023-07-17 02:17:34,234][257371] Min Reward on eval: -446.2590747123584[0m
[37m[1m[2023-07-17 02:17:34,234][257371] Mean Reward across all agents: -49.42776912976854[0m
[37m[1m[2023-07-17 02:17:34,234][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:17:34,237][257371] mean_value=-334.0937639743441, max_value=38.42515744305375[0m
[37m[1m[2023-07-17 02:17:34,239][257371] New mean coefficients: [[ 1.6230214   0.16087013  2.5580144   1.759416    0.55057    -0.71057093]][0m
[37m[1m[2023-07-17 02:17:34,240][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:17:43,357][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 02:17:43,357][257371] FPS: 421294.66[0m
[36m[2023-07-17 02:17:43,359][257371] itr=468, itrs=2000, Progress: 23.40%[0m
[36m[2023-07-17 02:17:55,225][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 02:17:55,225][257371] FPS: 325721.21[0m
[36m[2023-07-17 02:17:59,590][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:17:59,591][257371] Reward + Measures: [[51.82497729  0.39361131  0.3758457   0.42446566  0.52403402  3.09930921]][0m
[37m[1m[2023-07-17 02:17:59,591][257371] Max Reward on eval: 51.82497729107567[0m
[37m[1m[2023-07-17 02:17:59,591][257371] Min Reward on eval: 51.82497729107567[0m
[37m[1m[2023-07-17 02:17:59,592][257371] Mean Reward across all agents: 51.82497729107567[0m
[37m[1m[2023-07-17 02:17:59,592][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:18:04,776][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:18:04,782][257371] Reward + Measures: [[-20.4215786    0.16739999   0.2581       0.26019999   0.26210001
    5.42217588]
 [-53.65160578   0.23780003   0.22130001   0.21429999   0.2638
    5.64711142]
 [ 82.97514187   0.13609999   0.17230001   0.16590001   0.1982
    4.50887823]
 ...
 [  7.97806797   0.15880001   0.27780002   0.1911       0.3317
    3.98073554]
 [ 46.91640427   0.31810001   0.1964       0.32430002   0.29330003
    4.4343977 ]
 [104.76718865   0.19219999   0.22750001   0.2089       0.33459997
    4.18059683]][0m
[37m[1m[2023-07-17 02:18:04,782][257371] Max Reward on eval: 384.9138776862994[0m
[37m[1m[2023-07-17 02:18:04,783][257371] Min Reward on eval: -664.0093345623463[0m
[37m[1m[2023-07-17 02:18:04,783][257371] Mean Reward across all agents: -4.951195926662768[0m
[37m[1m[2023-07-17 02:18:04,783][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:18:04,786][257371] mean_value=-526.402921457556, max_value=543.2690710321069[0m
[37m[1m[2023-07-17 02:18:04,788][257371] New mean coefficients: [[ 2.4646342  -0.19332165  1.8432946   1.6230451  -0.8259609  -0.6546688 ]][0m
[37m[1m[2023-07-17 02:18:04,789][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:18:13,803][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 02:18:13,803][257371] FPS: 426112.74[0m
[36m[2023-07-17 02:18:13,805][257371] itr=469, itrs=2000, Progress: 23.45%[0m
[36m[2023-07-17 02:18:25,885][257371] train() took 12.01 seconds to complete[0m
[36m[2023-07-17 02:18:25,886][257371] FPS: 319788.53[0m
[36m[2023-07-17 02:18:30,209][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:18:30,214][257371] Reward + Measures: [[54.87315504  0.39559099  0.39085868  0.43718269  0.53450096  3.08671737]][0m
[37m[1m[2023-07-17 02:18:30,215][257371] Max Reward on eval: 54.873155041943384[0m
[37m[1m[2023-07-17 02:18:30,215][257371] Min Reward on eval: 54.873155041943384[0m
[37m[1m[2023-07-17 02:18:30,215][257371] Mean Reward across all agents: 54.873155041943384[0m
[37m[1m[2023-07-17 02:18:30,216][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:18:35,215][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:18:35,220][257371] Reward + Measures: [[-623.78012468    0.006         0.96530002    0.94929999    0.96490002
     5.93637609]
 [ 129.4512873     0.35170004    0.34150001    0.40570003    0.45180002
     3.53938603]
 [  60.09402449    0.44679999    0.32790002    0.51789999    0.34169999
     4.88278151]
 ...
 [ -14.42349626    0.3689        0.35080001    0.38359997    0.2843
     4.58622313]
 [  39.15743777    0.38850001    0.34600002    0.4025        0.34299999
     4.83474588]
 [  57.31712975    0.36669999    0.39390001    0.44210002    0.5399
     3.25067902]][0m
[37m[1m[2023-07-17 02:18:35,221][257371] Max Reward on eval: 261.3389094475657[0m
[37m[1m[2023-07-17 02:18:35,221][257371] Min Reward on eval: -785.9359588951804[0m
[37m[1m[2023-07-17 02:18:35,221][257371] Mean Reward across all agents: -10.677398984398065[0m
[37m[1m[2023-07-17 02:18:35,221][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:18:35,224][257371] mean_value=-564.4256899667406, max_value=224.03115316692427[0m
[37m[1m[2023-07-17 02:18:35,227][257371] New mean coefficients: [[ 2.7785432  -0.38719568  1.1904085   1.7642863  -1.2105432  -0.6535372 ]][0m
[37m[1m[2023-07-17 02:18:35,228][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:18:44,302][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 02:18:44,303][257371] FPS: 423230.38[0m
[36m[2023-07-17 02:18:44,305][257371] itr=470, itrs=2000, Progress: 23.50%[0m
[37m[1m[2023-07-17 02:21:36,353][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000450[0m
[36m[2023-07-17 02:21:48,606][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 02:21:48,606][257371] FPS: 326733.54[0m
[36m[2023-07-17 02:21:52,958][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:21:52,959][257371] Reward + Measures: [[58.52820301  0.41471532  0.39784065  0.47116566  0.55500233  3.06274486]][0m
[37m[1m[2023-07-17 02:21:52,959][257371] Max Reward on eval: 58.5282030133551[0m
[37m[1m[2023-07-17 02:21:52,959][257371] Min Reward on eval: 58.5282030133551[0m
[37m[1m[2023-07-17 02:21:52,960][257371] Mean Reward across all agents: 58.5282030133551[0m
[37m[1m[2023-07-17 02:21:52,960][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:21:57,887][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:21:57,887][257371] Reward + Measures: [[228.14401855   0.60659999   0.09670001   0.63780004   0.61510003
    4.86716509]
 [ 47.84281426   0.2746       0.48330003   0.28240001   0.45960003
    4.71802425]
 [-73.52558289   0.76589996   0.18570001   0.76050001   0.44160005
    5.16922808]
 ...
 [107.79210448   0.51949996   0.2438       0.55400002   0.3096
    5.88724136]
 [ 33.83476541   0.78870004   0.62639999   0.81820005   0.82690001
    6.32820845]
 [ -9.338211     0.82509995   0.6807       0.77850002   0.77109998
    5.36702681]][0m
[37m[1m[2023-07-17 02:21:57,887][257371] Max Reward on eval: 234.09295558100567[0m
[37m[1m[2023-07-17 02:21:57,888][257371] Min Reward on eval: -232.2427931137383[0m
[37m[1m[2023-07-17 02:21:57,888][257371] Mean Reward across all agents: 49.39384130268942[0m
[37m[1m[2023-07-17 02:21:57,888][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:21:57,893][257371] mean_value=-285.3624025747416, max_value=371.927323674357[0m
[37m[1m[2023-07-17 02:21:57,896][257371] New mean coefficients: [[ 3.9101617  -1.2803681   2.0113435   2.119361   -1.2805157   0.37045532]][0m
[37m[1m[2023-07-17 02:21:57,897][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:22:06,818][257371] train() took 8.92 seconds to complete[0m
[36m[2023-07-17 02:22:06,819][257371] FPS: 430495.62[0m
[36m[2023-07-17 02:22:06,821][257371] itr=471, itrs=2000, Progress: 23.55%[0m
[36m[2023-07-17 02:22:18,643][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 02:22:18,643][257371] FPS: 332763.73[0m
[36m[2023-07-17 02:22:22,880][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:22:22,880][257371] Reward + Measures: [[77.71312354  0.44842666  0.38602501  0.50534564  0.56957299  3.05510616]][0m
[37m[1m[2023-07-17 02:22:22,880][257371] Max Reward on eval: 77.71312354228316[0m
[37m[1m[2023-07-17 02:22:22,881][257371] Min Reward on eval: 77.71312354228316[0m
[37m[1m[2023-07-17 02:22:22,881][257371] Mean Reward across all agents: 77.71312354228316[0m
[37m[1m[2023-07-17 02:22:22,881][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:22:27,848][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:22:27,849][257371] Reward + Measures: [[  6.40995133   0.22580002   0.23210001   0.15790001   0.21470001
    3.81218719]
 [-13.51297786   0.1928       0.257        0.1514       0.23779999
    4.85345793]
 [-19.62878458   0.20159999   0.47550002   0.271        0.50990003
    3.74222732]
 ...
 [ -8.04365064   0.19770001   0.27490002   0.13850001   0.26190001
    4.15914869]
 [-43.96961989   0.3946       0.4815       0.53610009   0.52320004
    5.06308508]
 [ 34.93661405   0.27589998   0.34080002   0.28130001   0.34720001
    4.3085351 ]][0m
[37m[1m[2023-07-17 02:22:27,849][257371] Max Reward on eval: 227.28562072934582[0m
[37m[1m[2023-07-17 02:22:27,849][257371] Min Reward on eval: -151.13596955128014[0m
[37m[1m[2023-07-17 02:22:27,850][257371] Mean Reward across all agents: 6.591609790716195[0m
[37m[1m[2023-07-17 02:22:27,850][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:22:27,852][257371] mean_value=-927.1024542287388, max_value=319.9541933062544[0m
[37m[1m[2023-07-17 02:22:27,854][257371] New mean coefficients: [[ 1.2336853  -0.99444866  1.6443186   0.23993468 -1.8196942  -0.5105913 ]][0m
[37m[1m[2023-07-17 02:22:27,855][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:22:36,854][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 02:22:36,855][257371] FPS: 426781.91[0m
[36m[2023-07-17 02:22:36,857][257371] itr=472, itrs=2000, Progress: 23.60%[0m
[36m[2023-07-17 02:22:48,670][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 02:22:48,671][257371] FPS: 327106.38[0m
[36m[2023-07-17 02:22:52,991][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:22:52,991][257371] Reward + Measures: [[79.51560083  0.427625    0.39738297  0.49273631  0.56331694  3.04670906]][0m
[37m[1m[2023-07-17 02:22:52,992][257371] Max Reward on eval: 79.51560082740862[0m
[37m[1m[2023-07-17 02:22:52,992][257371] Min Reward on eval: 79.51560082740862[0m
[37m[1m[2023-07-17 02:22:52,992][257371] Mean Reward across all agents: 79.51560082740862[0m
[37m[1m[2023-07-17 02:22:52,992][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:22:58,002][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:22:58,003][257371] Reward + Measures: [[ 13.22554913   0.28310001   0.58810002   0.52650005   0.63480008
    5.98902464]
 [-21.50264763   0.15000001   0.33180004   0.28729996   0.3874
    6.56239271]
 [ 10.40920853   0.16510001   0.19090001   0.1148       0.21350001
    5.07489157]
 ...
 [ 38.42544164   0.25980002   0.32290003   0.2638       0.31799999
    4.28360271]
 [ 48.3359009    0.26470003   0.30800003   0.25219998   0.28560001
    4.14849997]
 [ 96.68914687   0.20250002   0.29029998   0.23899999   0.3827
    4.28475237]][0m
[37m[1m[2023-07-17 02:22:58,003][257371] Max Reward on eval: 323.1240449219942[0m
[37m[1m[2023-07-17 02:22:58,003][257371] Min Reward on eval: -750.8240585335298[0m
[37m[1m[2023-07-17 02:22:58,004][257371] Mean Reward across all agents: -12.761634263080591[0m
[37m[1m[2023-07-17 02:22:58,004][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:22:58,006][257371] mean_value=-685.3571567073294, max_value=405.94376306207835[0m
[37m[1m[2023-07-17 02:22:58,009][257371] New mean coefficients: [[ 1.2816291 -0.1611926  2.1574302  0.6259885 -0.6823733 -0.5171933]][0m
[37m[1m[2023-07-17 02:22:58,010][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:23:06,965][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 02:23:06,965][257371] FPS: 428878.47[0m
[36m[2023-07-17 02:23:06,968][257371] itr=473, itrs=2000, Progress: 23.65%[0m
[36m[2023-07-17 02:23:18,894][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-17 02:23:18,894][257371] FPS: 324027.49[0m
[36m[2023-07-17 02:23:23,156][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:23:23,157][257371] Reward + Measures: [[73.11562417  0.43068635  0.43157634  0.51665199  0.58521038  3.04811907]][0m
[37m[1m[2023-07-17 02:23:23,157][257371] Max Reward on eval: 73.1156241687927[0m
[37m[1m[2023-07-17 02:23:23,157][257371] Min Reward on eval: 73.1156241687927[0m
[37m[1m[2023-07-17 02:23:23,158][257371] Mean Reward across all agents: 73.1156241687927[0m
[37m[1m[2023-07-17 02:23:23,158][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:23:28,118][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:23:28,119][257371] Reward + Measures: [[-246.31241797    0.6613        0.5693        0.75959998    0.40770003
     4.67670393]
 [  42.44900261    0.29180002    0.30510002    0.31689999    0.34699997
     3.94000053]
 [  98.20028689    0.42509994    0.28639999    0.43280002    0.4777
     3.0391531 ]
 ...
 [  84.98029087    0.32609999    0.45970002    0.44439998    0.5341
     3.66369367]
 [  -4.64589869    0.30399999    0.35700002    0.35620004    0.53260005
     3.65035748]
 [ -32.33837717    0.20539999    0.21370001    0.20990001    0.2509
     4.54645824]][0m
[37m[1m[2023-07-17 02:23:28,119][257371] Max Reward on eval: 136.3680629153736[0m
[37m[1m[2023-07-17 02:23:28,119][257371] Min Reward on eval: -421.48994636535645[0m
[37m[1m[2023-07-17 02:23:28,120][257371] Mean Reward across all agents: 2.0345713888708405[0m
[37m[1m[2023-07-17 02:23:28,120][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:23:28,123][257371] mean_value=-413.9598037449968, max_value=229.8001260773118[0m
[37m[1m[2023-07-17 02:23:28,125][257371] New mean coefficients: [[ 1.6782168   0.0224265   1.5003922   0.01217073 -0.44223666 -0.05828196]][0m
[37m[1m[2023-07-17 02:23:28,126][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:23:37,099][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 02:23:37,100][257371] FPS: 428021.35[0m
[36m[2023-07-17 02:23:37,102][257371] itr=474, itrs=2000, Progress: 23.70%[0m
[36m[2023-07-17 02:23:48,952][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 02:23:48,952][257371] FPS: 326026.15[0m
[36m[2023-07-17 02:23:53,316][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:23:53,317][257371] Reward + Measures: [[81.99338781  0.46248201  0.44248599  0.55324864  0.60798967  3.0341363 ]][0m
[37m[1m[2023-07-17 02:23:53,317][257371] Max Reward on eval: 81.99338781182914[0m
[37m[1m[2023-07-17 02:23:53,317][257371] Min Reward on eval: 81.99338781182914[0m
[37m[1m[2023-07-17 02:23:53,318][257371] Mean Reward across all agents: 81.99338781182914[0m
[37m[1m[2023-07-17 02:23:53,318][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:23:58,307][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:23:58,307][257371] Reward + Measures: [[116.69539644   0.31590003   0.30000001   0.39850003   0.4233
    4.3913703 ]
 [126.35483295   0.2843       0.29200003   0.31909999   0.3035
    4.82467747]
 [ 81.87385464   0.47800002   0.46289998   0.5327       0.54759997
    4.28987455]
 ...
 [ 77.95561432   0.60140002   0.115        0.68050003   0.60879999
    4.73497009]
 [ 44.18237504   0.31900001   0.56830001   0.1023       0.50739998
    4.2559042 ]
 [ 59.27749587   0.36689997   0.48260003   0.1849       0.44509998
    4.81589699]][0m
[37m[1m[2023-07-17 02:23:58,307][257371] Max Reward on eval: 629.6747246030718[0m
[37m[1m[2023-07-17 02:23:58,308][257371] Min Reward on eval: -423.5605912551284[0m
[37m[1m[2023-07-17 02:23:58,308][257371] Mean Reward across all agents: 30.808300296766284[0m
[37m[1m[2023-07-17 02:23:58,308][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:23:58,312][257371] mean_value=-500.47418176379443, max_value=469.17219772611895[0m
[37m[1m[2023-07-17 02:23:58,315][257371] New mean coefficients: [[ 0.9130072  -0.18454103  1.018415   -1.5867133  -1.4731885   0.17261866]][0m
[37m[1m[2023-07-17 02:23:58,316][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:24:07,302][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 02:24:07,302][257371] FPS: 427388.31[0m
[36m[2023-07-17 02:24:07,305][257371] itr=475, itrs=2000, Progress: 23.75%[0m
[36m[2023-07-17 02:24:18,930][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 02:24:18,930][257371] FPS: 332412.58[0m
[36m[2023-07-17 02:24:23,172][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:24:23,177][257371] Reward + Measures: [[85.83435949  0.43027502  0.42448035  0.51940167  0.57997328  3.06299615]][0m
[37m[1m[2023-07-17 02:24:23,177][257371] Max Reward on eval: 85.83435949044029[0m
[37m[1m[2023-07-17 02:24:23,178][257371] Min Reward on eval: 85.83435949044029[0m
[37m[1m[2023-07-17 02:24:23,178][257371] Mean Reward across all agents: 85.83435949044029[0m
[37m[1m[2023-07-17 02:24:23,178][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:24:28,126][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:24:28,127][257371] Reward + Measures: [[ 31.88849227   0.2349       0.09499999   0.2552       0.15390001
    5.06679916]
 [-37.57626372   0.2899       0.42599997   0.33370003   0.30590001
    3.53423476]
 [  9.15862703   0.33000001   0.50910002   0.48230001   0.6135
    3.35271239]
 ...
 [ 70.8528374    0.34379998   0.419        0.33590004   0.40380001
    4.5537734 ]
 [ 15.9733817    0.28579998   0.32410002   0.3441       0.373
    4.62464142]
 [ 67.40873337   0.33250004   0.46940002   0.3353       0.48059997
    4.10062695]][0m
[37m[1m[2023-07-17 02:24:28,127][257371] Max Reward on eval: 333.7113113295287[0m
[37m[1m[2023-07-17 02:24:28,128][257371] Min Reward on eval: -719.8440704172477[0m
[37m[1m[2023-07-17 02:24:28,128][257371] Mean Reward across all agents: 30.376213575258056[0m
[37m[1m[2023-07-17 02:24:28,128][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:24:28,131][257371] mean_value=-399.11445771392056, max_value=551.346300244749[0m
[37m[1m[2023-07-17 02:24:28,134][257371] New mean coefficients: [[ 0.86616427 -0.5634658   0.8570877  -0.09332502 -0.3201182   0.6050018 ]][0m
[37m[1m[2023-07-17 02:24:28,135][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:24:37,130][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 02:24:37,130][257371] FPS: 426999.38[0m
[36m[2023-07-17 02:24:37,132][257371] itr=476, itrs=2000, Progress: 23.80%[0m
[36m[2023-07-17 02:24:48,779][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 02:24:48,779][257371] FPS: 331737.53[0m
[36m[2023-07-17 02:24:52,993][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:24:52,994][257371] Reward + Measures: [[96.19336697  0.41402331  0.39685196  0.48578832  0.54665399  3.07989454]][0m
[37m[1m[2023-07-17 02:24:52,994][257371] Max Reward on eval: 96.19336697348577[0m
[37m[1m[2023-07-17 02:24:52,994][257371] Min Reward on eval: 96.19336697348577[0m
[37m[1m[2023-07-17 02:24:52,994][257371] Mean Reward across all agents: 96.19336697348577[0m
[37m[1m[2023-07-17 02:24:52,995][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:24:58,161][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:24:58,162][257371] Reward + Measures: [[  59.89161744    0.3522        0.33450001    0.33090001    0.43080002
     3.51453209]
 [-314.98690558    0.80610007    0.0366        0.82270002    0.80910009
     4.97832251]
 [  12.55213498    0.27560002    0.20729999    0.29980001    0.2649
     4.19253588]
 ...
 [  66.93043676    0.3601        0.34510002    0.38820001    0.45000002
     3.48354793]
 [  40.97006288    0.24740003    0.1453        0.2744        0.21109998
     4.14222193]
 [  14.03086974    0.24150001    0.18960001    0.26199999    0.2297
     4.18774891]][0m
[37m[1m[2023-07-17 02:24:58,162][257371] Max Reward on eval: 247.2446250972338[0m
[37m[1m[2023-07-17 02:24:58,162][257371] Min Reward on eval: -314.9869055842049[0m
[37m[1m[2023-07-17 02:24:58,162][257371] Mean Reward across all agents: 26.47538982910948[0m
[37m[1m[2023-07-17 02:24:58,163][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:24:58,165][257371] mean_value=-614.7339209255354, max_value=222.59860840718284[0m
[37m[1m[2023-07-17 02:24:58,168][257371] New mean coefficients: [[ 0.9388203  -0.5215649   0.37068686 -0.04312602 -0.2111251   0.99540627]][0m
[37m[1m[2023-07-17 02:24:58,169][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:25:07,254][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 02:25:07,254][257371] FPS: 422727.04[0m
[36m[2023-07-17 02:25:07,257][257371] itr=477, itrs=2000, Progress: 23.85%[0m
[36m[2023-07-17 02:25:19,001][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 02:25:19,002][257371] FPS: 328959.01[0m
[36m[2023-07-17 02:25:23,310][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:25:23,311][257371] Reward + Measures: [[109.71117008   0.41398534   0.38103366   0.48121199   0.53670895
    3.09901404]][0m
[37m[1m[2023-07-17 02:25:23,311][257371] Max Reward on eval: 109.71117008476708[0m
[37m[1m[2023-07-17 02:25:23,311][257371] Min Reward on eval: 109.71117008476708[0m
[37m[1m[2023-07-17 02:25:23,312][257371] Mean Reward across all agents: 109.71117008476708[0m
[37m[1m[2023-07-17 02:25:23,312][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:25:28,394][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:25:28,400][257371] Reward + Measures: [[ 29.13933795   0.4743       0.14039999   0.5054       0.41689998
    3.99376225]
 [ 40.96907053   0.49119997   0.28589997   0.52380008   0.53259999
    4.25886679]
 [-13.59245039   0.4815       0.14740001   0.53549999   0.48769999
    5.6051836 ]
 ...
 [145.1289643    0.49470001   0.1141       0.56400007   0.47679996
    4.61852789]
 [ -5.25140161   0.29049999   0.43019995   0.2273       0.49000001
    4.61213779]
 [ 99.20123488   0.39930001   0.36950001   0.42880002   0.49349999
    3.14218259]][0m
[37m[1m[2023-07-17 02:25:28,400][257371] Max Reward on eval: 247.1188780703116[0m
[37m[1m[2023-07-17 02:25:28,400][257371] Min Reward on eval: -450.56025312514976[0m
[37m[1m[2023-07-17 02:25:28,401][257371] Mean Reward across all agents: -8.795139209661231[0m
[37m[1m[2023-07-17 02:25:28,401][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:25:28,403][257371] mean_value=-447.2750200833181, max_value=146.46334245992352[0m
[37m[1m[2023-07-17 02:25:28,406][257371] New mean coefficients: [[ 0.67897654 -0.32540545  0.8283839  -0.20480067  0.14262569  0.7230441 ]][0m
[37m[1m[2023-07-17 02:25:28,407][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:25:37,482][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 02:25:37,483][257371] FPS: 423197.53[0m
[36m[2023-07-17 02:25:37,485][257371] itr=478, itrs=2000, Progress: 23.90%[0m
[36m[2023-07-17 02:25:49,388][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 02:25:49,388][257371] FPS: 324665.52[0m
[36m[2023-07-17 02:25:53,761][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:25:53,761][257371] Reward + Measures: [[109.42341822   0.40916532   0.39937466   0.48583698   0.54694998
    3.14152789]][0m
[37m[1m[2023-07-17 02:25:53,762][257371] Max Reward on eval: 109.42341822087597[0m
[37m[1m[2023-07-17 02:25:53,762][257371] Min Reward on eval: 109.42341822087597[0m
[37m[1m[2023-07-17 02:25:53,762][257371] Mean Reward across all agents: 109.42341822087597[0m
[37m[1m[2023-07-17 02:25:53,762][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:25:58,768][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:25:58,768][257371] Reward + Measures: [[ 30.21786383   0.1807       0.10380001   0.20100002   0.18859999
    5.18852282]
 [-64.3366997    0.40560004   0.10450001   0.43560004   0.41690001
    5.15053797]
 [ 68.46271925   0.28080001   0.44980001   0.33800003   0.50040001
    3.64736104]
 ...
 [ 36.68187755   0.3326       0.4587       0.31169999   0.42089996
    4.5214653 ]
 [-36.50648714   0.6669001    0.08669999   0.685        0.67400002
    4.93903923]
 [-68.74714516   0.24510001   0.18099999   0.27410001   0.22020002
    3.76980519]][0m
[37m[1m[2023-07-17 02:25:58,768][257371] Max Reward on eval: 303.0704269467853[0m
[37m[1m[2023-07-17 02:25:58,769][257371] Min Reward on eval: -466.22871376015246[0m
[37m[1m[2023-07-17 02:25:58,769][257371] Mean Reward across all agents: 30.76694729898295[0m
[37m[1m[2023-07-17 02:25:58,769][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:25:58,773][257371] mean_value=-657.0877611697606, max_value=268.05151772129585[0m
[37m[1m[2023-07-17 02:25:58,775][257371] New mean coefficients: [[ 1.0830281  -0.95653117  0.812059   -0.10657855 -0.9857266   0.8587128 ]][0m
[37m[1m[2023-07-17 02:25:58,776][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:26:07,840][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 02:26:07,841][257371] FPS: 423732.23[0m
[36m[2023-07-17 02:26:07,843][257371] itr=479, itrs=2000, Progress: 23.95%[0m
[36m[2023-07-17 02:26:19,567][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 02:26:19,567][257371] FPS: 329568.61[0m
[36m[2023-07-17 02:26:23,836][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:26:23,837][257371] Reward + Measures: [[109.61732302   0.38724732   0.4085353    0.46970198   0.54154468
    3.17801404]][0m
[37m[1m[2023-07-17 02:26:23,837][257371] Max Reward on eval: 109.6173230213415[0m
[37m[1m[2023-07-17 02:26:23,837][257371] Min Reward on eval: 109.6173230213415[0m
[37m[1m[2023-07-17 02:26:23,838][257371] Mean Reward across all agents: 109.6173230213415[0m
[37m[1m[2023-07-17 02:26:23,838][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:26:28,821][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:26:28,821][257371] Reward + Measures: [[-541.67607064    0.25820002    0.68020004    0.83549994    0.89899999
     5.66424561]
 [  27.19053804    0.29980001    0.35200003    0.3387        0.39250001
     3.00974321]
 [ -74.97789981    0.1574        0.61510003    0.50220007    0.64320004
     4.78308964]
 ...
 [ -28.65592259    0.3506        0.24600001    0.42309999    0.31479999
     4.11665869]
 [  22.16448413    0.26879999    0.55070001    0.39369997    0.63330001
     3.92557883]
 [  43.72279408    0.1318        0.1543        0.17350002    0.16070001
     4.25964689]][0m
[37m[1m[2023-07-17 02:26:28,821][257371] Max Reward on eval: 204.18451115472126[0m
[37m[1m[2023-07-17 02:26:28,822][257371] Min Reward on eval: -586.1945800600806[0m
[37m[1m[2023-07-17 02:26:28,822][257371] Mean Reward across all agents: -35.2070182290471[0m
[37m[1m[2023-07-17 02:26:28,822][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:26:28,824][257371] mean_value=-562.9433421831216, max_value=201.38061098975186[0m
[37m[1m[2023-07-17 02:26:28,827][257371] New mean coefficients: [[ 1.2508755   0.16597772  0.7918557  -0.6269801   0.01679087  1.2180212 ]][0m
[37m[1m[2023-07-17 02:26:28,828][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:26:37,833][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 02:26:37,833][257371] FPS: 426486.16[0m
[36m[2023-07-17 02:26:37,836][257371] itr=480, itrs=2000, Progress: 24.00%[0m
[37m[1m[2023-07-17 02:29:25,770][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000460[0m
[36m[2023-07-17 02:29:38,029][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 02:29:38,029][257371] FPS: 326757.76[0m
[36m[2023-07-17 02:29:42,217][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:29:42,217][257371] Reward + Measures: [[103.14308384   0.37484032   0.4224017    0.46152201   0.54453003
    3.20427656]][0m
[37m[1m[2023-07-17 02:29:42,218][257371] Max Reward on eval: 103.14308384438768[0m
[37m[1m[2023-07-17 02:29:42,218][257371] Min Reward on eval: 103.14308384438768[0m
[37m[1m[2023-07-17 02:29:42,218][257371] Mean Reward across all agents: 103.14308384438768[0m
[37m[1m[2023-07-17 02:29:42,219][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:29:47,150][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:29:47,150][257371] Reward + Measures: [[  30.54124549    0.65100002    0.54820001    0.7001        0.69580001
     4.97880411]
 [ -19.16443338    0.23379998    0.24190001    0.20039999    0.28150001
     4.74248219]
 [-135.60583302    0.53459996    0.50410002    0.51230001    0.58779997
     5.70241451]
 ...
 [  88.56615764    0.30200002    0.4233        0.34810001    0.4743
     3.63186336]
 [  33.22079164    0.2278        0.34660003    0.27239999    0.36730003
     4.4808259 ]
 [   0.94439266    0.28490001    0.45479998    0.33119997    0.50300002
     4.46236324]][0m
[37m[1m[2023-07-17 02:29:47,151][257371] Max Reward on eval: 396.8085908979177[0m
[37m[1m[2023-07-17 02:29:47,151][257371] Min Reward on eval: -272.009580653999[0m
[37m[1m[2023-07-17 02:29:47,151][257371] Mean Reward across all agents: 19.855913170727067[0m
[37m[1m[2023-07-17 02:29:47,151][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:29:47,154][257371] mean_value=-558.0871748852728, max_value=205.5849245501583[0m
[37m[1m[2023-07-17 02:29:47,157][257371] New mean coefficients: [[-0.60433304 -0.40053904  1.3732042  -1.1794088  -0.9851408   0.2007817 ]][0m
[37m[1m[2023-07-17 02:29:47,158][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:29:56,256][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 02:29:56,257][257371] FPS: 422131.90[0m
[36m[2023-07-17 02:29:56,259][257371] itr=481, itrs=2000, Progress: 24.05%[0m
[36m[2023-07-17 02:30:08,066][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 02:30:08,067][257371] FPS: 327232.96[0m
[36m[2023-07-17 02:30:12,304][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:30:12,305][257371] Reward + Measures: [[83.58489485  0.367349    0.46442166  0.48507568  0.57364267  3.22137618]][0m
[37m[1m[2023-07-17 02:30:12,305][257371] Max Reward on eval: 83.58489484972183[0m
[37m[1m[2023-07-17 02:30:12,305][257371] Min Reward on eval: 83.58489484972183[0m
[37m[1m[2023-07-17 02:30:12,305][257371] Mean Reward across all agents: 83.58489484972183[0m
[37m[1m[2023-07-17 02:30:12,306][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:30:17,327][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:30:17,333][257371] Reward + Measures: [[-259.55275203    0.671         0.59569997    0.61930001    0.0584
     6.45566511]
 [  93.10452322    0.44710001    0.28400001    0.51109999    0.56240004
     3.8850162 ]
 [ 154.81911659    0.29310003    0.3443        0.27719998    0.36539999
     4.74509239]
 ...
 [  57.432512      0.27019998    0.1008        0.28410003    0.244
     4.49944925]
 [   9.24235304    0.14659999    0.10840001    0.21540001    0.1709
     5.54345036]
 [  63.38782206    0.43860003    0.06550001    0.47399998    0.44309998
     5.19361639]][0m
[37m[1m[2023-07-17 02:30:17,333][257371] Max Reward on eval: 295.80789300240576[0m
[37m[1m[2023-07-17 02:30:17,334][257371] Min Reward on eval: -287.12113769613205[0m
[37m[1m[2023-07-17 02:30:17,334][257371] Mean Reward across all agents: 20.141580801847162[0m
[37m[1m[2023-07-17 02:30:17,334][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:30:17,338][257371] mean_value=-268.94678417701346, max_value=283.92936859008967[0m
[37m[1m[2023-07-17 02:30:17,341][257371] New mean coefficients: [[-0.09539765 -0.37816975  1.7173269  -1.232939   -0.89310503  0.29129   ]][0m
[37m[1m[2023-07-17 02:30:17,342][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:30:26,440][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 02:30:26,440][257371] FPS: 422151.26[0m
[36m[2023-07-17 02:30:26,442][257371] itr=482, itrs=2000, Progress: 24.10%[0m
[36m[2023-07-17 02:30:38,217][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 02:30:38,218][257371] FPS: 328248.44[0m
[36m[2023-07-17 02:30:42,547][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:30:42,547][257371] Reward + Measures: [[63.3826344   0.33387834  0.50586098  0.47408536  0.58577305  3.26931047]][0m
[37m[1m[2023-07-17 02:30:42,547][257371] Max Reward on eval: 63.38263439529451[0m
[37m[1m[2023-07-17 02:30:42,548][257371] Min Reward on eval: 63.38263439529451[0m
[37m[1m[2023-07-17 02:30:42,548][257371] Mean Reward across all agents: 63.38263439529451[0m
[37m[1m[2023-07-17 02:30:42,548][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:30:47,588][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:30:47,594][257371] Reward + Measures: [[ 28.68152425   0.3565       0.29850003   0.37619996   0.39210001
    3.98724103]
 [114.60461521   0.31080002   0.28369999   0.4039       0.35029998
    3.52554965]
 [ 76.95405263   0.25220004   0.26140001   0.26800001   0.32440001
    3.86035705]
 ...
 [-26.47974282   0.25980002   0.31830004   0.27130002   0.36279997
    4.65445518]
 [ 15.26152928   0.56709999   0.59020007   0.58330005   0.61870003
    4.02777767]
 [-71.88409901   0.31110001   0.32160002   0.32670003   0.35120001
    3.55732584]][0m
[37m[1m[2023-07-17 02:30:47,594][257371] Max Reward on eval: 206.4837486750912[0m
[37m[1m[2023-07-17 02:30:47,595][257371] Min Reward on eval: -339.791127187456[0m
[37m[1m[2023-07-17 02:30:47,595][257371] Mean Reward across all agents: 37.33244407227109[0m
[37m[1m[2023-07-17 02:30:47,595][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:30:47,597][257371] mean_value=-781.6388293392993, max_value=65.45931257372028[0m
[37m[1m[2023-07-17 02:30:47,600][257371] New mean coefficients: [[-0.8835297  -1.0865823   2.238901   -0.7433891  -1.1277114   0.23095062]][0m
[37m[1m[2023-07-17 02:30:47,601][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:30:56,731][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 02:30:56,731][257371] FPS: 420646.16[0m
[36m[2023-07-17 02:30:56,734][257371] itr=483, itrs=2000, Progress: 24.15%[0m
[36m[2023-07-17 02:31:08,474][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 02:31:08,474][257371] FPS: 329101.47[0m
[36m[2023-07-17 02:31:12,698][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:31:12,698][257371] Reward + Measures: [[-15.04386703   0.22212265   0.67875397   0.5657807    0.71164966
    3.36528325]][0m
[37m[1m[2023-07-17 02:31:12,698][257371] Max Reward on eval: -15.043867034186105[0m
[37m[1m[2023-07-17 02:31:12,699][257371] Min Reward on eval: -15.043867034186105[0m
[37m[1m[2023-07-17 02:31:12,699][257371] Mean Reward across all agents: -15.043867034186105[0m
[37m[1m[2023-07-17 02:31:12,699][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:31:17,656][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:31:17,656][257371] Reward + Measures: [[  13.13161766    0.11680001    0.19560002    0.12260001    0.19740002
     4.92478704]
 [ 150.6696711     0.29889998    0.50520003    0.35330001    0.53710002
     4.26466656]
 [ -30.11903248    0.25919998    0.30160001    0.26820001    0.38680002
     4.40529871]
 ...
 [ -18.85397546    0.2119        0.4752        0.32960001    0.52559996
     4.2102952 ]
 [ -18.85158079    0.26280001    0.44770002    0.2992        0.5521
     4.36650181]
 [-184.43805961    0.0646        0.8215        0.79579991    0.84750003
     5.40500736]][0m
[37m[1m[2023-07-17 02:31:17,657][257371] Max Reward on eval: 227.65306088253857[0m
[37m[1m[2023-07-17 02:31:17,657][257371] Min Reward on eval: -513.9806499361991[0m
[37m[1m[2023-07-17 02:31:17,657][257371] Mean Reward across all agents: -8.308268667800585[0m
[37m[1m[2023-07-17 02:31:17,658][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:31:17,660][257371] mean_value=-425.6993138820429, max_value=101.28465931553484[0m
[37m[1m[2023-07-17 02:31:17,662][257371] New mean coefficients: [[ 0.9141297  -0.64294994  1.3477378  -0.4875668  -0.60258436  0.5028815 ]][0m
[37m[1m[2023-07-17 02:31:17,663][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:31:26,686][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 02:31:26,686][257371] FPS: 425674.92[0m
[36m[2023-07-17 02:31:26,688][257371] itr=484, itrs=2000, Progress: 24.20%[0m
[36m[2023-07-17 02:31:38,503][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 02:31:38,503][257371] FPS: 327059.03[0m
[36m[2023-07-17 02:31:42,871][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:31:42,872][257371] Reward + Measures: [[-280.06519957    0.02772667    0.93717206    0.84986001    0.92398632
     3.97667766]][0m
[37m[1m[2023-07-17 02:31:42,872][257371] Max Reward on eval: -280.06519956538403[0m
[37m[1m[2023-07-17 02:31:42,872][257371] Min Reward on eval: -280.06519956538403[0m
[37m[1m[2023-07-17 02:31:42,872][257371] Mean Reward across all agents: -280.06519956538403[0m
[37m[1m[2023-07-17 02:31:42,873][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:31:48,066][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:31:48,066][257371] Reward + Measures: [[ -36.13670786    0.1696        0.67830002    0.46040002    0.68559998
     3.93788004]
 [-219.12833812    0.0697        0.73359996    0.63300002    0.81370002
     5.06918335]
 [-522.14350126    0.031         0.92280006    0.84710008    0.92160004
     5.66523409]
 ...
 [-110.98275188    0.0729        0.75660002    0.62699997    0.78330004
     4.33594465]
 [-137.09569398    0.0875        0.77040005    0.65259999    0.80779999
     5.00622511]
 [ -85.9808194     0.13860001    0.59890002    0.43039998    0.62239999
     4.85603189]][0m
[37m[1m[2023-07-17 02:31:48,067][257371] Max Reward on eval: 252.19671584311874[0m
[37m[1m[2023-07-17 02:31:48,067][257371] Min Reward on eval: -825.9080276617781[0m
[37m[1m[2023-07-17 02:31:48,067][257371] Mean Reward across all agents: -170.290737517949[0m
[37m[1m[2023-07-17 02:31:48,067][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:31:48,069][257371] mean_value=-390.00536380279215, max_value=134.39669579761588[0m
[37m[1m[2023-07-17 02:31:48,072][257371] New mean coefficients: [[-1.1421893  -1.1035789   2.7016926  -0.51514244 -0.83674693  0.1588448 ]][0m
[37m[1m[2023-07-17 02:31:48,073][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:31:57,052][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 02:31:57,053][257371] FPS: 427720.15[0m
[36m[2023-07-17 02:31:57,055][257371] itr=485, itrs=2000, Progress: 24.25%[0m
[36m[2023-07-17 02:32:09,051][257371] train() took 11.92 seconds to complete[0m
[36m[2023-07-17 02:32:09,052][257371] FPS: 322159.35[0m
[36m[2023-07-17 02:32:13,373][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:32:13,373][257371] Reward + Measures: [[-293.84469957    0.02690967    0.93896824    0.85329068    0.92601043
     3.98172975]][0m
[37m[1m[2023-07-17 02:32:13,374][257371] Max Reward on eval: -293.8446995696951[0m
[37m[1m[2023-07-17 02:32:13,374][257371] Min Reward on eval: -293.8446995696951[0m
[37m[1m[2023-07-17 02:32:13,374][257371] Mean Reward across all agents: -293.8446995696951[0m
[37m[1m[2023-07-17 02:32:13,374][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:32:18,418][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:32:18,419][257371] Reward + Measures: [[-140.3337789     0.3152        0.89600003    0.82919997    0.90070003
     5.55204153]
 [ -68.43011588    0.1793        0.11080001    0.17400001    0.15220001
     6.08139467]
 [ 161.6690354     0.12740001    0.09780001    0.15980001    0.1091
     5.793612  ]
 ...
 [  -2.76956511    0.41540003    0.1842        0.41620001    0.32079998
     4.33803654]
 [ 134.97538612    0.1629        0.14119999    0.21430002    0.15100001
     5.14392614]
 [ 220.7532365     0.23300003    0.2227        0.30780002    0.23650001
     4.35465717]][0m
[37m[1m[2023-07-17 02:32:18,419][257371] Max Reward on eval: 322.3344630673528[0m
[37m[1m[2023-07-17 02:32:18,419][257371] Min Reward on eval: -459.0668449290097[0m
[37m[1m[2023-07-17 02:32:18,420][257371] Mean Reward across all agents: 53.60527496739049[0m
[37m[1m[2023-07-17 02:32:18,420][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:32:18,423][257371] mean_value=-264.1080111258446, max_value=251.3640059718218[0m
[37m[1m[2023-07-17 02:32:18,426][257371] New mean coefficients: [[-1.0587575  -0.7352252   1.8517613  -0.75589573 -1.6434559  -0.60568535]][0m
[37m[1m[2023-07-17 02:32:18,427][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:32:27,486][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 02:32:27,486][257371] FPS: 423985.04[0m
[36m[2023-07-17 02:32:27,488][257371] itr=486, itrs=2000, Progress: 24.30%[0m
[36m[2023-07-17 02:32:39,268][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 02:32:39,269][257371] FPS: 328094.52[0m
[36m[2023-07-17 02:32:43,590][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:32:43,591][257371] Reward + Measures: [[-285.61872591    0.02857567    0.93857634    0.84812236    0.92510194
     3.94517827]][0m
[37m[1m[2023-07-17 02:32:43,591][257371] Max Reward on eval: -285.6187259101894[0m
[37m[1m[2023-07-17 02:32:43,591][257371] Min Reward on eval: -285.6187259101894[0m
[37m[1m[2023-07-17 02:32:43,591][257371] Mean Reward across all agents: -285.6187259101894[0m
[37m[1m[2023-07-17 02:32:43,592][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:32:48,587][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:32:48,587][257371] Reward + Measures: [[-177.29878074    0.33159998    0.76110005    0.68800002    0.7949
     4.61041212]
 [-280.09038926    0.0554        0.84700006    0.73140001    0.86119998
     4.70128965]
 [-365.62971683    0.0319        0.89249992    0.77759999    0.89700001
     4.89365101]
 ...
 [-385.50161554    0.0168        0.93400002    0.88920003    0.9386
     6.12032938]
 [-532.35871505    0.0166        0.94480002    0.90039998    0.9465
     6.39257812]
 [-404.56596791    0.0349        0.80480003    0.75460005    0.81669998
     5.92346859]][0m
[37m[1m[2023-07-17 02:32:48,588][257371] Max Reward on eval: 198.9887743450701[0m
[37m[1m[2023-07-17 02:32:48,588][257371] Min Reward on eval: -532.3587150476873[0m
[37m[1m[2023-07-17 02:32:48,588][257371] Mean Reward across all agents: -160.00367263011177[0m
[37m[1m[2023-07-17 02:32:48,588][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:32:48,590][257371] mean_value=-326.0855090577173, max_value=240.5762491115133[0m
[37m[1m[2023-07-17 02:32:48,592][257371] New mean coefficients: [[-1.159123    1.4039168   1.5992587  -0.91090894 -0.7190403  -0.15636465]][0m
[37m[1m[2023-07-17 02:32:48,593][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:32:57,580][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 02:32:57,580][257371] FPS: 427398.03[0m
[36m[2023-07-17 02:32:57,582][257371] itr=487, itrs=2000, Progress: 24.35%[0m
[36m[2023-07-17 02:33:09,567][257371] train() took 11.91 seconds to complete[0m
[36m[2023-07-17 02:33:09,567][257371] FPS: 322354.31[0m
[36m[2023-07-17 02:33:13,915][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:33:13,915][257371] Reward + Measures: [[-289.48386324    0.03162833    0.93180567    0.82302129    0.918082
     3.9166882 ]][0m
[37m[1m[2023-07-17 02:33:13,915][257371] Max Reward on eval: -289.48386324257507[0m
[37m[1m[2023-07-17 02:33:13,915][257371] Min Reward on eval: -289.48386324257507[0m
[37m[1m[2023-07-17 02:33:13,916][257371] Mean Reward across all agents: -289.48386324257507[0m
[37m[1m[2023-07-17 02:33:13,916][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:33:18,946][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:33:18,947][257371] Reward + Measures: [[-362.77052712    0.0195        0.91659993    0.89559996    0.91180003
     4.34287548]
 [-464.50542639    0.0559        0.88550007    0.76190007    0.88959998
     4.79981709]
 [  29.49691074    0.49460003    0.24730001    0.48580003    0.51340002
     4.98155451]
 ...
 [ -12.32269155    0.38710001    0.32460001    0.3867        0.44750005
     4.46382236]
 [   3.51952555    0.43319997    0.85020012    0.75949997    0.8337
     3.67190909]
 [ -29.25925825    0.41710001    0.38540003    0.4368        0.54630005
     4.65907049]][0m
[37m[1m[2023-07-17 02:33:18,947][257371] Max Reward on eval: 370.0297546572983[0m
[37m[1m[2023-07-17 02:33:18,947][257371] Min Reward on eval: -794.4994811935351[0m
[37m[1m[2023-07-17 02:33:18,948][257371] Mean Reward across all agents: -108.64469219317516[0m
[37m[1m[2023-07-17 02:33:18,948][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:33:18,951][257371] mean_value=-234.1621116676196, max_value=402.7215180295054[0m
[37m[1m[2023-07-17 02:33:18,954][257371] New mean coefficients: [[-1.8805419   0.29521084  1.0340843  -1.2968913  -0.89491457 -0.11643927]][0m
[37m[1m[2023-07-17 02:33:18,955][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:33:28,033][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 02:33:28,033][257371] FPS: 423057.15[0m
[36m[2023-07-17 02:33:28,036][257371] itr=488, itrs=2000, Progress: 24.40%[0m
[36m[2023-07-17 02:33:39,746][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 02:33:39,746][257371] FPS: 330002.32[0m
[36m[2023-07-17 02:33:44,008][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:33:44,008][257371] Reward + Measures: [[-291.34895606    0.037542      0.91891307    0.79056233    0.90579271
     3.90040231]][0m
[37m[1m[2023-07-17 02:33:44,008][257371] Max Reward on eval: -291.34895606475675[0m
[37m[1m[2023-07-17 02:33:44,009][257371] Min Reward on eval: -291.34895606475675[0m
[37m[1m[2023-07-17 02:33:44,009][257371] Mean Reward across all agents: -291.34895606475675[0m
[37m[1m[2023-07-17 02:33:44,009][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:33:48,983][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:33:48,983][257371] Reward + Measures: [[ -96.86038808    0.32930002    0.43899998    0.49820003    0.5036
     3.85907984]
 [-114.91530739    0.12030001    0.68730003    0.51660001    0.66169995
     3.78121996]
 [-261.1001653     0.0801        0.81010002    0.68530005    0.80190003
     3.86998367]
 ...
 [ -26.29594037    0.1134        0.49759999    0.35000002    0.52710003
     3.85318685]
 [-522.32285401    0.0196        0.96060002    0.89440006    0.9483
     4.39480209]
 [ -56.87708706    0.14839999    0.5406        0.29520002    0.54080003
     4.24198866]][0m
[37m[1m[2023-07-17 02:33:48,984][257371] Max Reward on eval: 249.7864337220788[0m
[37m[1m[2023-07-17 02:33:48,984][257371] Min Reward on eval: -522.3228540096432[0m
[37m[1m[2023-07-17 02:33:48,984][257371] Mean Reward across all agents: -138.00436021982665[0m
[37m[1m[2023-07-17 02:33:48,984][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:33:48,987][257371] mean_value=-227.2465973500145, max_value=143.56086815917453[0m
[37m[1m[2023-07-17 02:33:48,989][257371] New mean coefficients: [[-0.7376746   1.2834412   2.167675   -2.2467728  -0.46358734 -0.14148657]][0m
[37m[1m[2023-07-17 02:33:48,990][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:33:57,979][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 02:33:57,980][257371] FPS: 427245.90[0m
[36m[2023-07-17 02:33:57,982][257371] itr=489, itrs=2000, Progress: 24.45%[0m
[36m[2023-07-17 02:34:09,608][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 02:34:09,608][257371] FPS: 332473.66[0m
[36m[2023-07-17 02:34:13,948][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:34:13,948][257371] Reward + Measures: [[-276.71210391    0.04925567    0.89086801    0.72049731    0.87991363
     3.85144591]][0m
[37m[1m[2023-07-17 02:34:13,949][257371] Max Reward on eval: -276.7121039050597[0m
[37m[1m[2023-07-17 02:34:13,949][257371] Min Reward on eval: -276.7121039050597[0m
[37m[1m[2023-07-17 02:34:13,949][257371] Mean Reward across all agents: -276.7121039050597[0m
[37m[1m[2023-07-17 02:34:13,949][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:34:19,178][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:34:19,179][257371] Reward + Measures: [[-164.26620199    0.0622        0.82420009    0.78650004    0.83129996
     4.87149143]
 [  19.66084636    0.29609999    0.42460003    0.33239996    0.42290002
     4.42870092]
 [-347.75193068    0.0401        0.93190002    0.85100001    0.92250007
     3.98811698]
 ...
 [-131.55228329    0.10439999    0.75520003    0.7051        0.75430006
     3.63340688]
 [  90.59085054    0.30969998    0.43829998    0.32119998    0.41750002
     4.73090076]
 [-434.89069939    0.0254        0.963         0.90310001    0.95120001
     3.95927167]][0m
[37m[1m[2023-07-17 02:34:19,179][257371] Max Reward on eval: 131.03744761645794[0m
[37m[1m[2023-07-17 02:34:19,179][257371] Min Reward on eval: -832.9933014303446[0m
[37m[1m[2023-07-17 02:34:19,179][257371] Mean Reward across all agents: -172.8824622299512[0m
[37m[1m[2023-07-17 02:34:19,180][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:34:19,182][257371] mean_value=-313.4201239706544, max_value=154.94980547238498[0m
[37m[1m[2023-07-17 02:34:19,184][257371] New mean coefficients: [[-0.28070718  1.7164838   2.1408355  -2.2571416  -0.57028526  0.52888936]][0m
[37m[1m[2023-07-17 02:34:19,185][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:34:28,302][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 02:34:28,302][257371] FPS: 421281.74[0m
[36m[2023-07-17 02:34:28,304][257371] itr=490, itrs=2000, Progress: 24.50%[0m
[37m[1m[2023-07-17 02:37:12,019][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000470[0m
[36m[2023-07-17 02:37:24,453][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 02:37:24,453][257371] FPS: 325955.85[0m
[36m[2023-07-17 02:37:28,728][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:37:28,729][257371] Reward + Measures: [[-256.35475792    0.053583      0.87804466    0.68526697    0.86481804
     3.8774035 ]][0m
[37m[1m[2023-07-17 02:37:28,729][257371] Max Reward on eval: -256.3547579203915[0m
[37m[1m[2023-07-17 02:37:28,729][257371] Min Reward on eval: -256.3547579203915[0m
[37m[1m[2023-07-17 02:37:28,729][257371] Mean Reward across all agents: -256.3547579203915[0m
[37m[1m[2023-07-17 02:37:28,730][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:37:33,727][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:37:33,728][257371] Reward + Measures: [[-213.16929515    0.0664        0.83680004    0.6566        0.81739998
     3.89812636]
 [-154.04059255    0.0782        0.85269994    0.68380004    0.85249996
     3.59132957]
 [-249.51397204    0.0629        0.85979998    0.70590001    0.85970002
     3.67667079]
 ...
 [-362.99419971    0.0303        0.93430007    0.74290007    0.91230005
     4.24609423]
 [-246.2390113     0.0601        0.86170006    0.70710003    0.84330004
     3.82777095]
 [-210.08516278    0.06720001    0.85780001    0.64040005    0.84360009
     3.9085052 ]][0m
[37m[1m[2023-07-17 02:37:33,728][257371] Max Reward on eval: -41.82993236072362[0m
[37m[1m[2023-07-17 02:37:33,728][257371] Min Reward on eval: -533.7963509794324[0m
[37m[1m[2023-07-17 02:37:33,729][257371] Mean Reward across all agents: -258.3397523010791[0m
[37m[1m[2023-07-17 02:37:33,729][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:37:33,730][257371] mean_value=-298.4708223556121, max_value=-110.99253187625337[0m
[36m[2023-07-17 02:37:33,732][257371] XNES is restarting with a new solution whose measures are [0.68479997 0.81980002 0.71029997 0.32589999 1.15831685] and objective is 77.71331928921863[0m
[36m[2023-07-17 02:37:33,733][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 02:37:33,735][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 02:37:33,736][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:37:42,726][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 02:37:42,726][257371] FPS: 427223.67[0m
[36m[2023-07-17 02:37:42,729][257371] itr=491, itrs=2000, Progress: 24.55%[0m
[36m[2023-07-17 02:37:54,452][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 02:37:54,452][257371] FPS: 329695.77[0m
[36m[2023-07-17 02:37:58,735][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:37:58,736][257371] Reward + Measures: [[83.66053445  0.88314599  0.79269338  0.75272769  0.04160133  2.00286722]][0m
[37m[1m[2023-07-17 02:37:58,736][257371] Max Reward on eval: 83.6605344490605[0m
[37m[1m[2023-07-17 02:37:58,736][257371] Min Reward on eval: 83.6605344490605[0m
[37m[1m[2023-07-17 02:37:58,737][257371] Mean Reward across all agents: 83.6605344490605[0m
[37m[1m[2023-07-17 02:37:58,737][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:38:03,749][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:38:03,750][257371] Reward + Measures: [[496.20823717   0.9169001    0.89020008   0.87510008   0.0327
    3.10165286]
 [-86.32619427   0.39159998   0.30920002   0.4025       0.31889999
    3.7702477 ]
 [ 58.64957917   0.38690001   0.333        0.3256       0.30570003
    4.24118185]
 ...
 [ 18.67979451   0.61840004   0.57800001   0.58000004   0.49230003
    3.73487258]
 [-92.47169681   0.43309999   0.44140002   0.3691       0.43799996
    2.99824405]
 [126.29310412   0.42699996   0.30640003   0.41079998   0.41319999
    3.45307994]][0m
[37m[1m[2023-07-17 02:38:03,750][257371] Max Reward on eval: 602.8163474963978[0m
[37m[1m[2023-07-17 02:38:03,750][257371] Min Reward on eval: -362.59601899432016[0m
[37m[1m[2023-07-17 02:38:03,751][257371] Mean Reward across all agents: 13.510616799993837[0m
[37m[1m[2023-07-17 02:38:03,751][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:38:03,756][257371] mean_value=-706.6066338404636, max_value=1045.8276612386107[0m
[37m[1m[2023-07-17 02:38:03,758][257371] New mean coefficients: [[ 0.48797056  1.207758   -0.24218243 -0.9100015  -1.0772045  -0.05434   ]][0m
[37m[1m[2023-07-17 02:38:03,759][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:38:12,765][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 02:38:12,766][257371] FPS: 426461.24[0m
[36m[2023-07-17 02:38:12,768][257371] itr=492, itrs=2000, Progress: 24.60%[0m
[36m[2023-07-17 02:38:24,479][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 02:38:24,479][257371] FPS: 330014.72[0m
[36m[2023-07-17 02:38:28,862][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:38:28,862][257371] Reward + Measures: [[119.53096182   0.85430729   0.81922132   0.73420399   0.044377
    1.98564482]][0m
[37m[1m[2023-07-17 02:38:28,863][257371] Max Reward on eval: 119.53096181604995[0m
[37m[1m[2023-07-17 02:38:28,863][257371] Min Reward on eval: 119.53096181604995[0m
[37m[1m[2023-07-17 02:38:28,863][257371] Mean Reward across all agents: 119.53096181604995[0m
[37m[1m[2023-07-17 02:38:28,863][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:38:34,039][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:38:34,039][257371] Reward + Measures: [[  17.31192252    0.3035        0.2667        0.27090001    0.1062
     6.21876526]
 [ -85.0462256     0.3822        0.41630003    0.27419999    0.3858
     4.7047224 ]
 [ -49.14400671    0.20470002    0.1904        0.16820002    0.18020001
     5.49746084]
 ...
 [-113.21072393    0.28910002    0.28479999    0.25770003    0.19450001
     3.77279353]
 [ 101.41067444    0.37639999    0.42230001    0.25400004    0.2299
     6.14481544]
 [  92.17486004    0.21870001    0.88430005    0.3281        0.89840001
     6.13608551]][0m
[37m[1m[2023-07-17 02:38:34,040][257371] Max Reward on eval: 618.4690971475095[0m
[37m[1m[2023-07-17 02:38:34,040][257371] Min Reward on eval: -490.7775898066349[0m
[37m[1m[2023-07-17 02:38:34,040][257371] Mean Reward across all agents: -6.40947256679503[0m
[37m[1m[2023-07-17 02:38:34,040][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:38:34,044][257371] mean_value=-1055.6168090064293, max_value=644.9541709879411[0m
[37m[1m[2023-07-17 02:38:34,046][257371] New mean coefficients: [[ 0.46311095  2.1723392   1.1892385  -1.5647776   0.47877085  0.00402337]][0m
[37m[1m[2023-07-17 02:38:34,047][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:38:43,063][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 02:38:43,063][257371] FPS: 425983.42[0m
[36m[2023-07-17 02:38:43,066][257371] itr=493, itrs=2000, Progress: 24.65%[0m
[36m[2023-07-17 02:38:54,738][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 02:38:54,738][257371] FPS: 331151.23[0m
[36m[2023-07-17 02:38:59,092][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:38:59,092][257371] Reward + Measures: [[32.90048682  0.68383074  0.63418734  0.38806802  0.20774731  2.42032528]][0m
[37m[1m[2023-07-17 02:38:59,092][257371] Max Reward on eval: 32.90048681772657[0m
[37m[1m[2023-07-17 02:38:59,093][257371] Min Reward on eval: 32.90048681772657[0m
[37m[1m[2023-07-17 02:38:59,093][257371] Mean Reward across all agents: 32.90048681772657[0m
[37m[1m[2023-07-17 02:38:59,093][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:39:04,114][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:39:04,114][257371] Reward + Measures: [[ 96.59402601   0.20810001   0.21170001   0.17029999   0.23150001
    3.23490691]
 [-12.11256234   0.29819998   0.31470001   0.29130003   0.31080002
    4.05612803]
 [-22.97952564   0.13630001   0.1309       0.13020001   0.1416
    3.34744811]
 ...
 [114.56854558   0.23460002   0.20619999   0.18170001   0.29350001
    3.07839346]
 [  0.65762913   0.2185       0.41890001   0.3572       0.35200003
    3.66117406]
 [ 29.19362967   0.17389999   0.16220002   0.15210001   0.17830001
    3.45306897]][0m
[37m[1m[2023-07-17 02:39:04,115][257371] Max Reward on eval: 504.92867219643665[0m
[37m[1m[2023-07-17 02:39:04,115][257371] Min Reward on eval: -159.97722720094026[0m
[37m[1m[2023-07-17 02:39:04,115][257371] Mean Reward across all agents: 21.470065207010606[0m
[37m[1m[2023-07-17 02:39:04,115][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:39:04,119][257371] mean_value=-2271.0833874933865, max_value=732.3561553187203[0m
[37m[1m[2023-07-17 02:39:04,122][257371] New mean coefficients: [[ 0.17624605 -0.24030018 -0.35128987 -0.7966706  -0.2018596   0.23225555]][0m
[37m[1m[2023-07-17 02:39:04,123][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:39:13,170][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 02:39:13,170][257371] FPS: 424515.49[0m
[36m[2023-07-17 02:39:13,173][257371] itr=494, itrs=2000, Progress: 24.70%[0m
[36m[2023-07-17 02:39:24,831][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 02:39:24,831][257371] FPS: 331485.15[0m
[36m[2023-07-17 02:39:29,159][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:39:29,160][257371] Reward + Measures: [[6.61217594 0.78785861 0.67327362 0.70072263 0.07117633 3.26865911]][0m
[37m[1m[2023-07-17 02:39:29,160][257371] Max Reward on eval: 6.612175943065726[0m
[37m[1m[2023-07-17 02:39:29,160][257371] Min Reward on eval: 6.612175943065726[0m
[37m[1m[2023-07-17 02:39:29,161][257371] Mean Reward across all agents: 6.612175943065726[0m
[37m[1m[2023-07-17 02:39:29,161][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:39:34,245][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:39:34,246][257371] Reward + Measures: [[  -1.94812478    0.20960002    0.51470006    0.32370001    0.4698
     3.19511724]
 [  62.93128492    0.85219997    0.57600003    0.86069995    0.67870003
     3.09458709]
 [  20.75828974    0.24949999    0.25530002    0.16580001    0.18090001
     6.10901403]
 ...
 [ -16.88126442    0.41580001    0.2339        0.35380003    0.25910002
     5.55809593]
 [-151.18343253    0.33720002    0.38780004    0.36840001    0.2278
     3.23360896]
 [  82.82809845    0.56030005    0.45310003    0.46290001    0.14250001
     4.11183882]][0m
[37m[1m[2023-07-17 02:39:34,246][257371] Max Reward on eval: 714.448104868643[0m
[37m[1m[2023-07-17 02:39:34,246][257371] Min Reward on eval: -373.10033748075364[0m
[37m[1m[2023-07-17 02:39:34,247][257371] Mean Reward across all agents: 25.156436688390386[0m
[37m[1m[2023-07-17 02:39:34,247][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:39:34,253][257371] mean_value=-909.85863137353, max_value=761.5709185074659[0m
[37m[1m[2023-07-17 02:39:34,256][257371] New mean coefficients: [[-0.5570387  -0.6865921  -0.7152841  -1.3148811   1.8709902   0.54879296]][0m
[37m[1m[2023-07-17 02:39:34,257][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:39:43,374][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 02:39:43,374][257371] FPS: 421273.56[0m
[36m[2023-07-17 02:39:43,376][257371] itr=495, itrs=2000, Progress: 24.75%[0m
[36m[2023-07-17 02:39:54,990][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 02:39:54,990][257371] FPS: 332866.44[0m
[36m[2023-07-17 02:39:59,349][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:39:59,349][257371] Reward + Measures: [[-0.09201711  0.72294658  0.62245363  0.65865535  0.08883266  3.44448686]][0m
[37m[1m[2023-07-17 02:39:59,350][257371] Max Reward on eval: -0.09201711218450025[0m
[37m[1m[2023-07-17 02:39:59,350][257371] Min Reward on eval: -0.09201711218450025[0m
[37m[1m[2023-07-17 02:39:59,350][257371] Mean Reward across all agents: -0.09201711218450025[0m
[37m[1m[2023-07-17 02:39:59,350][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:40:04,426][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:40:04,487][257371] Reward + Measures: [[-25.16660905   0.32650003   0.20549999   0.2811       0.34369999
    2.58595061]
 [107.65513969   0.39019999   0.34509999   0.37630001   0.28560001
    2.67471218]
 [-37.6777296    0.72480005   0.63880002   0.58840001   0.25680003
    4.81313038]
 ...
 [-34.74831919   0.29270002   0.25230002   0.32350001   0.28929999
    3.39952469]
 [ -9.01211686   0.88060009   0.8768999    0.79500002   0.50040001
    3.84512401]
 [120.55002977   0.26230001   0.29350001   0.34619999   0.22930001
    3.35554576]][0m
[37m[1m[2023-07-17 02:40:04,487][257371] Max Reward on eval: 776.0635948226787[0m
[37m[1m[2023-07-17 02:40:04,487][257371] Min Reward on eval: -224.3790578968823[0m
[37m[1m[2023-07-17 02:40:04,488][257371] Mean Reward across all agents: 22.746437141607895[0m
[37m[1m[2023-07-17 02:40:04,488][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:40:04,491][257371] mean_value=-1811.222762486062, max_value=783.4396189126486[0m
[37m[1m[2023-07-17 02:40:04,494][257371] New mean coefficients: [[ 0.2349047   0.39805305  0.08991498 -0.79053193  1.2215135   0.25531337]][0m
[37m[1m[2023-07-17 02:40:04,495][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:40:13,628][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 02:40:13,629][257371] FPS: 420510.02[0m
[36m[2023-07-17 02:40:13,631][257371] itr=496, itrs=2000, Progress: 24.80%[0m
[36m[2023-07-17 02:40:25,523][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 02:40:25,524][257371] FPS: 325031.57[0m
[36m[2023-07-17 02:40:29,750][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:40:29,751][257371] Reward + Measures: [[148.78738834   0.61637735   0.55931634   0.43970266   0.099118
    4.23459435]][0m
[37m[1m[2023-07-17 02:40:29,751][257371] Max Reward on eval: 148.78738834286673[0m
[37m[1m[2023-07-17 02:40:29,751][257371] Min Reward on eval: 148.78738834286673[0m
[37m[1m[2023-07-17 02:40:29,751][257371] Mean Reward across all agents: 148.78738834286673[0m
[37m[1m[2023-07-17 02:40:29,752][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:40:34,719][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:40:34,719][257371] Reward + Measures: [[ -18.4422898     0.19130002    0.18900001    0.1409        0.20449999
     3.3176713 ]
 [ -14.74568844    0.1094        0.1073        0.0868        0.0733
     4.08563852]
 [ -71.71535431    0.30790001    0.42449999    0.25400001    0.4289
     6.2691884 ]
 ...
 [ -88.97267161    0.3152        0.38280001    0.34820002    0.30040002
     5.89658737]
 [ -41.24321075    0.2146        0.454         0.21160002    0.48850003
     4.36026716]
 [-256.52453665    0.85130006    0.82240003    0.83420002    0.0422
     6.80869818]][0m
[37m[1m[2023-07-17 02:40:34,719][257371] Max Reward on eval: 838.9174194382504[0m
[37m[1m[2023-07-17 02:40:34,720][257371] Min Reward on eval: -407.438500393834[0m
[37m[1m[2023-07-17 02:40:34,720][257371] Mean Reward across all agents: 15.033403987355966[0m
[37m[1m[2023-07-17 02:40:34,720][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:40:34,726][257371] mean_value=-1048.6067888494192, max_value=718.9329985825761[0m
[37m[1m[2023-07-17 02:40:34,729][257371] New mean coefficients: [[ 1.4876115   0.29645717  1.6106174  -1.0653713   0.975029    0.42229843]][0m
[37m[1m[2023-07-17 02:40:34,730][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:40:43,728][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 02:40:43,734][257371] FPS: 426803.63[0m
[36m[2023-07-17 02:40:43,736][257371] itr=497, itrs=2000, Progress: 24.85%[0m
[36m[2023-07-17 02:40:55,578][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 02:40:55,578][257371] FPS: 326405.56[0m
[36m[2023-07-17 02:40:59,880][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:40:59,880][257371] Reward + Measures: [[157.89913392   0.62945932   0.57487428   0.46168238   0.10352133
    4.38906622]][0m
[37m[1m[2023-07-17 02:40:59,880][257371] Max Reward on eval: 157.89913392490755[0m
[37m[1m[2023-07-17 02:40:59,881][257371] Min Reward on eval: 157.89913392490755[0m
[37m[1m[2023-07-17 02:40:59,881][257371] Mean Reward across all agents: 157.89913392490755[0m
[37m[1m[2023-07-17 02:40:59,881][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:41:05,064][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:41:05,065][257371] Reward + Measures: [[  3.23958317   0.20030001   0.199        0.1596       0.19230001
    4.75601768]
 [  4.89838258   0.49519998   0.55970001   0.0697       0.51770002
    5.00125837]
 [173.46072785   0.67309999   0.65590006   0.6006       0.04720001
    6.12486267]
 ...
 [-66.84928168   0.1265       0.1584       0.1284       0.1462
    3.99127364]
 [-16.24773072   0.55540001   0.52920002   0.2129       0.30930001
    6.97368383]
 [-25.22523235   0.33260003   0.29970002   0.37729999   0.205
    5.65817595]][0m
[37m[1m[2023-07-17 02:41:05,065][257371] Max Reward on eval: 430.0162477493286[0m
[37m[1m[2023-07-17 02:41:05,065][257371] Min Reward on eval: -353.15001388564707[0m
[37m[1m[2023-07-17 02:41:05,065][257371] Mean Reward across all agents: -8.315387076578045[0m
[37m[1m[2023-07-17 02:41:05,065][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:41:05,070][257371] mean_value=-1048.0100280224767, max_value=617.3315719103068[0m
[37m[1m[2023-07-17 02:41:05,072][257371] New mean coefficients: [[ 1.3000226  -0.89794743  1.8068206  -0.79712176  0.71723926 -0.22838295]][0m
[37m[1m[2023-07-17 02:41:05,073][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:41:14,118][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 02:41:14,119][257371] FPS: 424593.99[0m
[36m[2023-07-17 02:41:14,121][257371] itr=498, itrs=2000, Progress: 24.90%[0m
[36m[2023-07-17 02:41:25,748][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 02:41:25,748][257371] FPS: 332374.38[0m
[36m[2023-07-17 02:41:30,047][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:41:30,048][257371] Reward + Measures: [[12.95575091  0.58530998  0.56302297  0.38384432  0.12323567  3.97171831]][0m
[37m[1m[2023-07-17 02:41:30,048][257371] Max Reward on eval: 12.955750914073608[0m
[37m[1m[2023-07-17 02:41:30,048][257371] Min Reward on eval: 12.955750914073608[0m
[37m[1m[2023-07-17 02:41:30,049][257371] Mean Reward across all agents: 12.955750914073608[0m
[37m[1m[2023-07-17 02:41:30,049][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:41:35,001][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:41:35,002][257371] Reward + Measures: [[ -18.72605018    0.22690001    0.18139999    0.23080002    0.13
     4.78896856]
 [ -66.05995213    0.2737        0.27320001    0.14750001    0.2744
     3.62546706]
 [ -88.65576789    0.21760002    0.177         0.1636        0.1472
     4.71998119]
 ...
 [ -34.70969103    0.22610001    0.1815        0.20630002    0.0817
     4.65372944]
 [-113.05105565    0.38760003    0.32660002    0.2818        0.22320001
     3.80735254]
 [-479.55726625    0.96800005    0.99180001    0.0056        0.98519993
     7.26963758]][0m
[37m[1m[2023-07-17 02:41:35,002][257371] Max Reward on eval: 263.0435971982777[0m
[37m[1m[2023-07-17 02:41:35,003][257371] Min Reward on eval: -479.5572662511142[0m
[37m[1m[2023-07-17 02:41:35,003][257371] Mean Reward across all agents: -32.943687476898326[0m
[37m[1m[2023-07-17 02:41:35,003][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:41:35,007][257371] mean_value=-1321.30165986536, max_value=606.4204394183122[0m
[37m[1m[2023-07-17 02:41:35,010][257371] New mean coefficients: [[ 0.92470855 -0.9305945   0.7676469   1.2272613  -0.03587198  0.4444095 ]][0m
[37m[1m[2023-07-17 02:41:35,011][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:41:44,043][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 02:41:44,043][257371] FPS: 425219.15[0m
[36m[2023-07-17 02:41:44,045][257371] itr=499, itrs=2000, Progress: 24.95%[0m
[36m[2023-07-17 02:41:55,793][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 02:41:55,793][257371] FPS: 328924.37[0m
[36m[2023-07-17 02:42:00,078][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:42:00,079][257371] Reward + Measures: [[40.58024411  0.60287631  0.57564235  0.40491769  0.11403667  4.1046586 ]][0m
[37m[1m[2023-07-17 02:42:00,079][257371] Max Reward on eval: 40.58024410500519[0m
[37m[1m[2023-07-17 02:42:00,079][257371] Min Reward on eval: 40.58024410500519[0m
[37m[1m[2023-07-17 02:42:00,079][257371] Mean Reward across all agents: 40.58024410500519[0m
[37m[1m[2023-07-17 02:42:00,080][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:42:05,108][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:42:05,109][257371] Reward + Measures: [[ -8.01600304   0.16540001   0.30230001   0.1645       0.15269999
    2.52923369]
 [ 21.19778453   0.30180001   0.2388       0.23570001   0.1199
    2.87183309]
 [-37.61377521   0.1195       0.1121       0.10999999   0.0877
    4.28364182]
 ...
 [-20.2059089    0.13579999   0.15800001   0.12660001   0.13090001
    4.21493769]
 [-57.64857226   0.25819999   0.22790001   0.23220001   0.19149999
    3.67841029]
 [-53.5497582    0.1103       0.1122       0.1181       0.1024
    5.50024366]][0m
[37m[1m[2023-07-17 02:42:05,109][257371] Max Reward on eval: 686.9464721541851[0m
[37m[1m[2023-07-17 02:42:05,109][257371] Min Reward on eval: -308.6960289159324[0m
[37m[1m[2023-07-17 02:42:05,110][257371] Mean Reward across all agents: -2.7263752743910197[0m
[37m[1m[2023-07-17 02:42:05,110][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:42:05,114][257371] mean_value=-2234.797625864686, max_value=423.1003024315398[0m
[37m[1m[2023-07-17 02:42:05,117][257371] New mean coefficients: [[-0.13848776  0.33683175 -0.8676529   0.41258967 -1.0989801   0.6542797 ]][0m
[37m[1m[2023-07-17 02:42:05,118][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:42:14,203][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 02:42:14,203][257371] FPS: 422762.92[0m
[36m[2023-07-17 02:42:14,205][257371] itr=500, itrs=2000, Progress: 25.00%[0m
[37m[1m[2023-07-17 02:45:09,977][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000480[0m
[36m[2023-07-17 02:45:22,247][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 02:45:22,247][257371] FPS: 327293.00[0m
[36m[2023-07-17 02:45:26,376][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:45:26,376][257371] Reward + Measures: [[62.84909163  0.41287899  0.36948031  0.34429166  0.08322266  3.32866478]][0m
[37m[1m[2023-07-17 02:45:26,376][257371] Max Reward on eval: 62.84909163357171[0m
[37m[1m[2023-07-17 02:45:26,376][257371] Min Reward on eval: 62.84909163357171[0m
[37m[1m[2023-07-17 02:45:26,377][257371] Mean Reward across all agents: 62.84909163357171[0m
[37m[1m[2023-07-17 02:45:26,377][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:45:31,535][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:45:31,536][257371] Reward + Measures: [[432.96944878   0.88210005   0.87010002   0.85859996   0.0069
    6.28462172]
 [ 17.69829738   0.80120003   0.75850004   0.66170001   0.0643
    5.12763453]
 [-80.63548319   0.19240001   0.1591       0.14920001   0.22140001
    3.53957295]
 ...
 [-95.88793757   0.2066       0.20850001   0.22850001   0.21229999
    4.84033012]
 [ 30.40446471   0.22550002   0.25009999   0.16309999   0.29929999
    5.48380899]
 [-51.21694782   0.27540001   0.23099999   0.22199999   0.2428
    4.49971247]][0m
[37m[1m[2023-07-17 02:45:31,536][257371] Max Reward on eval: 442.0307769961655[0m
[37m[1m[2023-07-17 02:45:31,536][257371] Min Reward on eval: -481.42258643237875[0m
[37m[1m[2023-07-17 02:45:31,537][257371] Mean Reward across all agents: 14.965660531984286[0m
[37m[1m[2023-07-17 02:45:31,537][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:45:31,543][257371] mean_value=-998.1807189747062, max_value=583.083606450444[0m
[37m[1m[2023-07-17 02:45:31,546][257371] New mean coefficients: [[ 0.38977653  1.6415439  -1.4821136   0.24671513  0.75956774  0.6001107 ]][0m
[37m[1m[2023-07-17 02:45:31,547][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:45:40,458][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-17 02:45:40,459][257371] FPS: 431003.32[0m
[36m[2023-07-17 02:45:40,461][257371] itr=501, itrs=2000, Progress: 25.05%[0m
[36m[2023-07-17 02:45:52,025][257371] train() took 11.49 seconds to complete[0m
[36m[2023-07-17 02:45:52,025][257371] FPS: 334179.58[0m
[36m[2023-07-17 02:45:56,239][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:45:56,239][257371] Reward + Measures: [[74.78412238  0.41498569  0.37187666  0.35238701  0.08297566  3.39614058]][0m
[37m[1m[2023-07-17 02:45:56,239][257371] Max Reward on eval: 74.78412237785327[0m
[37m[1m[2023-07-17 02:45:56,239][257371] Min Reward on eval: 74.78412237785327[0m
[37m[1m[2023-07-17 02:45:56,240][257371] Mean Reward across all agents: 74.78412237785327[0m
[37m[1m[2023-07-17 02:45:56,240][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:46:01,169][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:46:01,170][257371] Reward + Measures: [[ 40.34468239   0.27959999   0.28569999   0.2271       0.30790001
    3.76413512]
 [-20.68127879   0.21000002   0.16589999   0.23169999   0.25079998
    3.85753894]
 [ 22.74520552   0.3247       0.2369       0.28480002   0.1649
    4.50790405]
 ...
 [145.02291384   0.50999999   0.28290001   0.44899997   0.17060001
    3.56545758]
 [  4.90905982   0.4542       0.92959994   0.42840001   0.8804
    4.38308716]
 [ 20.88843496   0.11570001   0.1195       0.134        0.11989999
    4.83640385]][0m
[37m[1m[2023-07-17 02:46:01,170][257371] Max Reward on eval: 568.3600540278479[0m
[37m[1m[2023-07-17 02:46:01,170][257371] Min Reward on eval: -475.2127065737965[0m
[37m[1m[2023-07-17 02:46:01,170][257371] Mean Reward across all agents: -4.344957687721818[0m
[37m[1m[2023-07-17 02:46:01,170][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:46:01,175][257371] mean_value=-1339.775712570028, max_value=599.8001727221039[0m
[37m[1m[2023-07-17 02:46:01,178][257371] New mean coefficients: [[-0.5666898   1.8505749  -1.6947472   0.85108113  0.07786071  0.48352966]][0m
[37m[1m[2023-07-17 02:46:01,179][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:46:10,092][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-17 02:46:10,092][257371] FPS: 430906.88[0m
[36m[2023-07-17 02:46:10,094][257371] itr=502, itrs=2000, Progress: 25.10%[0m
[36m[2023-07-17 02:46:21,812][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 02:46:21,812][257371] FPS: 329833.62[0m
[36m[2023-07-17 02:46:26,223][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:46:26,224][257371] Reward + Measures: [[89.46013287  0.45680997  0.41502202  0.39537966  0.08301233  3.54727769]][0m
[37m[1m[2023-07-17 02:46:26,224][257371] Max Reward on eval: 89.46013286742975[0m
[37m[1m[2023-07-17 02:46:26,224][257371] Min Reward on eval: 89.46013286742975[0m
[37m[1m[2023-07-17 02:46:26,225][257371] Mean Reward across all agents: 89.46013286742975[0m
[37m[1m[2023-07-17 02:46:26,225][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:46:31,246][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:46:31,247][257371] Reward + Measures: [[-13.20235676   0.43850002   0.32950005   0.37789997   0.2458
    3.80416727]
 [  8.93365378   0.36390001   0.2142       0.31149998   0.25830001
    4.16497087]
 [-83.84813876   0.29089999   0.2402       0.20750001   0.15619999
    3.85377765]
 ...
 [ 22.70016085   0.30710003   0.29790002   0.26139998   0.20700002
    3.69510388]
 [ 68.4818642    0.60729998   0.49910003   0.44720003   0.25540003
    3.91298652]
 [-49.19855178   0.24129999   0.32230002   0.2802       0.28579998
    3.16586733]][0m
[37m[1m[2023-07-17 02:46:31,247][257371] Max Reward on eval: 342.92737291691594[0m
[37m[1m[2023-07-17 02:46:31,248][257371] Min Reward on eval: -238.21401785981143[0m
[37m[1m[2023-07-17 02:46:31,248][257371] Mean Reward across all agents: 13.09712433461222[0m
[37m[1m[2023-07-17 02:46:31,248][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:46:31,252][257371] mean_value=-1187.7519713234155, max_value=505.5052225400474[0m
[37m[1m[2023-07-17 02:46:31,255][257371] New mean coefficients: [[-0.35537827  2.073685   -1.9384996   1.5259147   0.7838392   0.3863489 ]][0m
[37m[1m[2023-07-17 02:46:31,256][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:46:40,272][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 02:46:40,273][257371] FPS: 425980.46[0m
[36m[2023-07-17 02:46:40,275][257371] itr=503, itrs=2000, Progress: 25.15%[0m
[36m[2023-07-17 02:46:51,971][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 02:46:51,971][257371] FPS: 330510.76[0m
[36m[2023-07-17 02:46:56,214][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:46:56,214][257371] Reward + Measures: [[91.12626095  0.450203    0.40483963  0.39033064  0.08574034  3.56537914]][0m
[37m[1m[2023-07-17 02:46:56,214][257371] Max Reward on eval: 91.12626094969127[0m
[37m[1m[2023-07-17 02:46:56,215][257371] Min Reward on eval: 91.12626094969127[0m
[37m[1m[2023-07-17 02:46:56,215][257371] Mean Reward across all agents: 91.12626094969127[0m
[37m[1m[2023-07-17 02:46:56,215][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:47:01,234][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:47:01,235][257371] Reward + Measures: [[ -39.05387079    0.21030001    0.24870001    0.1816        0.24629998
     4.27408457]
 [-129.36198844    0.73650002    0.65380001    0.70519996    0.11850001
     5.35172892]
 [ 111.54687457    0.35750002    0.46310002    0.27830002    0.34369999
     4.13241911]
 ...
 [-471.85985067    0.81059998    0.70849997    0.76280004    0.0741
     5.87688017]
 [-179.67512136    0.46750003    0.3901        0.44169998    0.06170001
     6.1463871 ]
 [-112.69850143    0.66060001    0.79869998    0.1037        0.70710003
     5.83362722]][0m
[37m[1m[2023-07-17 02:47:01,235][257371] Max Reward on eval: 437.22854517158123[0m
[37m[1m[2023-07-17 02:47:01,235][257371] Min Reward on eval: -471.8598506723065[0m
[37m[1m[2023-07-17 02:47:01,235][257371] Mean Reward across all agents: -24.62642105554984[0m
[37m[1m[2023-07-17 02:47:01,236][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:47:01,240][257371] mean_value=-855.653865395339, max_value=372.10394610880456[0m
[37m[1m[2023-07-17 02:47:01,243][257371] New mean coefficients: [[-0.32628086  2.6164136  -1.4561191   0.85862386  1.7814602   0.8033983 ]][0m
[37m[1m[2023-07-17 02:47:01,244][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:47:10,277][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 02:47:10,278][257371] FPS: 425155.90[0m
[36m[2023-07-17 02:47:10,280][257371] itr=504, itrs=2000, Progress: 25.20%[0m
[36m[2023-07-17 02:47:21,956][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 02:47:21,956][257371] FPS: 331113.50[0m
[36m[2023-07-17 02:47:26,164][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:47:26,164][257371] Reward + Measures: [[107.0683542    0.49233031   0.44809136   0.42696667   0.07918267
    3.64727139]][0m
[37m[1m[2023-07-17 02:47:26,164][257371] Max Reward on eval: 107.06835420171085[0m
[37m[1m[2023-07-17 02:47:26,164][257371] Min Reward on eval: 107.06835420171085[0m
[37m[1m[2023-07-17 02:47:26,165][257371] Mean Reward across all agents: 107.06835420171085[0m
[37m[1m[2023-07-17 02:47:26,165][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:47:31,190][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:47:31,191][257371] Reward + Measures: [[-21.9439249    0.3651       0.30420002   0.27669999   0.20469999
    3.08101583]
 [-16.61387288   0.39920002   0.38420001   0.48600003   0.2737
    2.85429502]
 [-81.32623412   0.19690001   0.1973       0.1856       0.1674
    4.05468178]
 ...
 [-67.07300628   0.28659999   0.2705       0.3163       0.24919999
    3.15690923]
 [120.15249776   0.75819999   0.88420004   0.55950004   0.36890003
    4.2989707 ]
 [-93.5808292    0.42460003   0.3585       0.43409997   0.30840001
    2.64171338]][0m
[37m[1m[2023-07-17 02:47:31,191][257371] Max Reward on eval: 399.3748245296971[0m
[37m[1m[2023-07-17 02:47:31,191][257371] Min Reward on eval: -177.64379619294778[0m
[37m[1m[2023-07-17 02:47:31,192][257371] Mean Reward across all agents: -3.189271898678523[0m
[37m[1m[2023-07-17 02:47:31,192][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:47:31,196][257371] mean_value=-1575.1470678720739, max_value=524.9654815279106[0m
[37m[1m[2023-07-17 02:47:31,199][257371] New mean coefficients: [[-0.5625708  0.7073209 -1.7951547 -0.7871858  2.0871458  1.0272276]][0m
[37m[1m[2023-07-17 02:47:31,200][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:47:40,278][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 02:47:40,279][257371] FPS: 423080.96[0m
[36m[2023-07-17 02:47:40,281][257371] itr=505, itrs=2000, Progress: 25.25%[0m
[36m[2023-07-17 02:47:52,091][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 02:47:52,091][257371] FPS: 327206.12[0m
[36m[2023-07-17 02:47:56,429][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:47:56,430][257371] Reward + Measures: [[103.13339366   0.47812796   0.43694499   0.42043832   0.08122733
    3.68226099]][0m
[37m[1m[2023-07-17 02:47:56,430][257371] Max Reward on eval: 103.13339365686623[0m
[37m[1m[2023-07-17 02:47:56,430][257371] Min Reward on eval: 103.13339365686623[0m
[37m[1m[2023-07-17 02:47:56,430][257371] Mean Reward across all agents: 103.13339365686623[0m
[37m[1m[2023-07-17 02:47:56,431][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:48:01,462][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:48:01,462][257371] Reward + Measures: [[-66.85291726   0.19160001   0.25410002   0.21980003   0.2208
    3.01018214]
 [ 29.09483031   0.11720001   0.10340001   0.1274       0.1072
    5.08819485]
 [-34.26824819   0.18499999   0.22160001   0.17850001   0.20729999
    3.12537956]
 ...
 [-98.92541815   0.68220001   0.33500001   0.56339997   0.23460002
    3.83930063]
 [-78.91537402   0.2376       0.29460001   0.23800002   0.22230001
    2.60206485]
 [ -0.78599003   0.14289999   0.21640001   0.16420001   0.1595
    3.94772959]][0m
[37m[1m[2023-07-17 02:48:01,463][257371] Max Reward on eval: 467.0608749477891[0m
[37m[1m[2023-07-17 02:48:01,463][257371] Min Reward on eval: -221.31777306920267[0m
[37m[1m[2023-07-17 02:48:01,463][257371] Mean Reward across all agents: -14.592626715384597[0m
[37m[1m[2023-07-17 02:48:01,463][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:48:01,466][257371] mean_value=-1956.1670737476682, max_value=471.24187135578836[0m
[37m[1m[2023-07-17 02:48:01,469][257371] New mean coefficients: [[ 0.57887363 -0.02230573 -1.9853303   0.24846911  0.08487678  0.4745462 ]][0m
[37m[1m[2023-07-17 02:48:01,470][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:48:10,578][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 02:48:10,584][257371] FPS: 421665.67[0m
[36m[2023-07-17 02:48:10,590][257371] itr=506, itrs=2000, Progress: 25.30%[0m
[36m[2023-07-17 02:48:22,445][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 02:48:22,446][257371] FPS: 326275.39[0m
[36m[2023-07-17 02:48:26,761][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:48:26,761][257371] Reward + Measures: [[96.3670127   0.46656731  0.42369503  0.40938166  0.08373133  3.67957473]][0m
[37m[1m[2023-07-17 02:48:26,761][257371] Max Reward on eval: 96.36701269520924[0m
[37m[1m[2023-07-17 02:48:26,762][257371] Min Reward on eval: 96.36701269520924[0m
[37m[1m[2023-07-17 02:48:26,762][257371] Mean Reward across all agents: 96.36701269520924[0m
[37m[1m[2023-07-17 02:48:26,762][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:48:32,030][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:48:32,031][257371] Reward + Measures: [[-53.21968561   0.24229999   0.2005       0.2119       0.2131
    4.5815444 ]
 [ -9.13299459   0.51380002   0.48210001   0.45730001   0.23150001
    3.17255139]
 [-19.98627218   0.64490002   0.62470001   0.48699996   0.2559
    3.90246463]
 ...
 [  1.66858556   0.3335       0.31280002   0.27900001   0.3082
    2.56819773]
 [-60.86479908   0.2895       0.29779997   0.28739998   0.23309998
    3.39565277]
 [348.13021758   0.8064       0.80849999   0.75130004   0.64790004
    5.66967392]][0m
[37m[1m[2023-07-17 02:48:32,032][257371] Max Reward on eval: 348.1302175845951[0m
[37m[1m[2023-07-17 02:48:32,032][257371] Min Reward on eval: -226.4175062293187[0m
[37m[1m[2023-07-17 02:48:32,032][257371] Mean Reward across all agents: -0.1537554517571103[0m
[37m[1m[2023-07-17 02:48:32,032][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:48:32,038][257371] mean_value=-1100.349294759801, max_value=626.5649753477921[0m
[37m[1m[2023-07-17 02:48:32,040][257371] New mean coefficients: [[ 0.7220607  -0.41767985 -1.1931672  -0.7792916   2.3233078  -0.0145382 ]][0m
[37m[1m[2023-07-17 02:48:32,041][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:48:41,097][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 02:48:41,098][257371] FPS: 424107.00[0m
[36m[2023-07-17 02:48:41,100][257371] itr=507, itrs=2000, Progress: 25.35%[0m
[36m[2023-07-17 02:48:52,724][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 02:48:52,724][257371] FPS: 332465.91[0m
[36m[2023-07-17 02:48:56,966][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:48:56,971][257371] Reward + Measures: [[93.51358836  0.46287668  0.41495597  0.40235198  0.08558     3.67624354]][0m
[37m[1m[2023-07-17 02:48:56,972][257371] Max Reward on eval: 93.51358836034953[0m
[37m[1m[2023-07-17 02:48:56,972][257371] Min Reward on eval: 93.51358836034953[0m
[37m[1m[2023-07-17 02:48:56,972][257371] Mean Reward across all agents: 93.51358836034953[0m
[37m[1m[2023-07-17 02:48:56,973][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:49:01,938][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:49:01,939][257371] Reward + Measures: [[ 16.61044109   0.244        0.22409999   0.20939998   0.18550001
    3.83033419]
 [ 26.2868343    0.354        0.21519999   0.26950002   0.2217
    3.40571666]
 [116.7021408    0.60630006   0.39900002   0.48540002   0.0898
    4.18815231]
 ...
 [ 34.76977557   0.5977       0.47150001   0.40260002   0.08620001
    4.51076269]
 [-51.11288646   0.5262       0.66099995   0.44159999   0.38750002
    4.08941936]
 [-55.23329904   0.23650001   0.3414       0.22160001   0.23550001
    4.02725077]][0m
[37m[1m[2023-07-17 02:49:01,939][257371] Max Reward on eval: 631.2043113546912[0m
[37m[1m[2023-07-17 02:49:01,939][257371] Min Reward on eval: -203.44653658049646[0m
[37m[1m[2023-07-17 02:49:01,940][257371] Mean Reward across all agents: 9.183311178134948[0m
[37m[1m[2023-07-17 02:49:01,940][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:49:01,944][257371] mean_value=-1613.7929553171236, max_value=754.8733922010404[0m
[37m[1m[2023-07-17 02:49:01,946][257371] New mean coefficients: [[ 0.4983697  -2.322475   -0.74127364 -1.3601098   1.624961    0.5185274 ]][0m
[37m[1m[2023-07-17 02:49:01,947][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:49:11,039][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 02:49:11,039][257371] FPS: 422462.08[0m
[36m[2023-07-17 02:49:11,041][257371] itr=508, itrs=2000, Progress: 25.40%[0m
[36m[2023-07-17 02:49:22,951][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 02:49:22,951][257371] FPS: 324588.48[0m
[36m[2023-07-17 02:49:27,297][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:49:27,297][257371] Reward + Measures: [[77.027671    0.43592432  0.39148867  0.38059968  0.08910033  3.58846831]][0m
[37m[1m[2023-07-17 02:49:27,297][257371] Max Reward on eval: 77.02767100308564[0m
[37m[1m[2023-07-17 02:49:27,298][257371] Min Reward on eval: 77.02767100308564[0m
[37m[1m[2023-07-17 02:49:27,298][257371] Mean Reward across all agents: 77.02767100308564[0m
[37m[1m[2023-07-17 02:49:27,298][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:49:32,285][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:49:32,285][257371] Reward + Measures: [[ -5.02691247   0.2476       0.25810003   0.18440001   0.20130001
    4.18431568]
 [ 70.5234139    0.5618       0.35360003   0.53490001   0.56399995
    5.43308592]
 [ 53.05380063   0.37920004   0.4824       0.35820001   0.50430006
    2.86759114]
 ...
 [ 39.08115883   0.68300003   0.57450002   0.55489999   0.0273
    4.85672426]
 [ 41.23139259   0.2615       0.51380008   0.31310001   0.46510002
    4.5630846 ]
 [-19.97366689   0.23120001   0.24969999   0.17490001   0.22780001
    4.33113098]][0m
[37m[1m[2023-07-17 02:49:32,286][257371] Max Reward on eval: 128.89494801205583[0m
[37m[1m[2023-07-17 02:49:32,286][257371] Min Reward on eval: -188.71087793405167[0m
[37m[1m[2023-07-17 02:49:32,286][257371] Mean Reward across all agents: -10.637017024940597[0m
[37m[1m[2023-07-17 02:49:32,286][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:49:32,289][257371] mean_value=-1442.796053109197, max_value=239.13027369681532[0m
[37m[1m[2023-07-17 02:49:32,292][257371] New mean coefficients: [[ 1.9417816  -2.0144708  -0.42626148 -1.0583001   0.9596095   0.2314269 ]][0m
[37m[1m[2023-07-17 02:49:32,293][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:49:41,296][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 02:49:41,296][257371] FPS: 426623.20[0m
[36m[2023-07-17 02:49:41,298][257371] itr=509, itrs=2000, Progress: 25.45%[0m
[36m[2023-07-17 02:49:52,919][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 02:49:52,919][257371] FPS: 332612.57[0m
[36m[2023-07-17 02:49:57,170][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:49:57,171][257371] Reward + Measures: [[67.5890573   0.39835432  0.35907471  0.34657365  0.09076433  3.5810554 ]][0m
[37m[1m[2023-07-17 02:49:57,171][257371] Max Reward on eval: 67.58905729786636[0m
[37m[1m[2023-07-17 02:49:57,171][257371] Min Reward on eval: 67.58905729786636[0m
[37m[1m[2023-07-17 02:49:57,172][257371] Mean Reward across all agents: 67.58905729786636[0m
[37m[1m[2023-07-17 02:49:57,172][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:50:02,188][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:50:02,188][257371] Reward + Measures: [[-104.46749566    0.28040001    0.27360001    0.19579999    0.19690001
     4.15339518]
 [ -47.61434138    0.45820004    0.2534        0.32860002    0.1231
     4.47625017]
 [  41.11636111    0.21729998    0.2254        0.1602        0.0541
     4.4569211 ]
 ...
 [  23.91084093    0.57249999    0.52460003    0.50089997    0.0621
     4.70660686]
 [ -25.96273669    0.30760002    0.3091        0.24879999    0.26620001
     4.08863926]
 [ -38.06741495    0.25079998    0.1921        0.1849        0.1373
     4.04813766]][0m
[37m[1m[2023-07-17 02:50:02,189][257371] Max Reward on eval: 465.0892265820876[0m
[37m[1m[2023-07-17 02:50:02,189][257371] Min Reward on eval: -211.68281645798123[0m
[37m[1m[2023-07-17 02:50:02,189][257371] Mean Reward across all agents: -5.4840935702859515[0m
[37m[1m[2023-07-17 02:50:02,189][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:50:02,192][257371] mean_value=-1340.577079509295, max_value=372.8880877915106[0m
[37m[1m[2023-07-17 02:50:02,195][257371] New mean coefficients: [[ 1.235447   -2.715283   -0.97555    -1.0835439   0.05739766  0.28727007]][0m
[37m[1m[2023-07-17 02:50:02,196][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:50:11,290][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 02:50:11,291][257371] FPS: 422292.73[0m
[36m[2023-07-17 02:50:11,293][257371] itr=510, itrs=2000, Progress: 25.50%[0m
[37m[1m[2023-07-17 02:53:04,508][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000490[0m
[36m[2023-07-17 02:53:16,786][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 02:53:16,787][257371] FPS: 329850.77[0m
[36m[2023-07-17 02:53:20,988][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:53:20,989][257371] Reward + Measures: [[56.6821088   0.36507064  0.32672831  0.32081899  0.09378932  3.56024075]][0m
[37m[1m[2023-07-17 02:53:20,989][257371] Max Reward on eval: 56.682108795764904[0m
[37m[1m[2023-07-17 02:53:20,989][257371] Min Reward on eval: 56.682108795764904[0m
[37m[1m[2023-07-17 02:53:20,989][257371] Mean Reward across all agents: 56.682108795764904[0m
[37m[1m[2023-07-17 02:53:20,990][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:53:25,916][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:53:25,917][257371] Reward + Measures: [[-33.80859334   0.43190002   0.52090001   0.30830002   0.25710002
    3.90504694]
 [-64.81820724   0.2613       0.38120002   0.22670002   0.31550002
    3.64906931]
 [-87.36197174   0.303        0.44949999   0.21859999   0.37130001
    4.3102293 ]
 ...
 [ 22.38490606   0.3247       0.31959999   0.1876       0.19749999
    4.27078724]
 [ 71.23498652   0.41589999   0.20680001   0.34619999   0.23009999
    3.92956233]
 [-61.60365018   0.27770001   0.37650001   0.1971       0.37550002
    3.86480021]][0m
[37m[1m[2023-07-17 02:53:25,917][257371] Max Reward on eval: 385.962080089096[0m
[37m[1m[2023-07-17 02:53:25,917][257371] Min Reward on eval: -194.9180613335222[0m
[37m[1m[2023-07-17 02:53:25,918][257371] Mean Reward across all agents: -10.479027880643319[0m
[37m[1m[2023-07-17 02:53:25,918][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:53:25,922][257371] mean_value=-1290.7024652523216, max_value=505.7254211830159[0m
[37m[1m[2023-07-17 02:53:25,924][257371] New mean coefficients: [[ 1.5906265 -1.2502952 -2.1461072  1.0188028  0.2525558  0.7329569]][0m
[37m[1m[2023-07-17 02:53:25,925][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:53:34,950][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 02:53:34,950][257371] FPS: 425603.85[0m
[36m[2023-07-17 02:53:34,952][257371] itr=511, itrs=2000, Progress: 25.55%[0m
[36m[2023-07-17 02:53:46,549][257371] train() took 11.52 seconds to complete[0m
[36m[2023-07-17 02:53:46,549][257371] FPS: 333262.79[0m
[36m[2023-07-17 02:53:50,803][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:53:50,804][257371] Reward + Measures: [[50.29993229  0.33733234  0.30134764  0.30093867  0.09937133  3.54228902]][0m
[37m[1m[2023-07-17 02:53:50,804][257371] Max Reward on eval: 50.29993229067057[0m
[37m[1m[2023-07-17 02:53:50,804][257371] Min Reward on eval: 50.29993229067057[0m
[37m[1m[2023-07-17 02:53:50,804][257371] Mean Reward across all agents: 50.29993229067057[0m
[37m[1m[2023-07-17 02:53:50,805][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:53:55,709][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:53:55,710][257371] Reward + Measures: [[ -0.2843333    0.22690001   0.23080002   0.17200001   0.17480002
    4.05491447]
 [-17.73562623   0.21430002   0.28390002   0.18979999   0.23049998
    4.52907705]
 [-74.80838339   0.38690001   0.37760001   0.24440001   0.25999999
    3.17746639]
 ...
 [107.80343888   0.35230002   0.36040002   0.1925       0.39430001
    3.62094569]
 [-92.20851916   0.35679999   0.31259999   0.26150003   0.1961
    3.69782424]
 [-47.59790453   0.48249999   0.47390005   0.34489998   0.50319999
    3.32384086]][0m
[37m[1m[2023-07-17 02:53:55,710][257371] Max Reward on eval: 412.86362360594796[0m
[37m[1m[2023-07-17 02:53:55,710][257371] Min Reward on eval: -547.0252292011166[0m
[37m[1m[2023-07-17 02:53:55,711][257371] Mean Reward across all agents: 14.450779780190311[0m
[37m[1m[2023-07-17 02:53:55,711][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:53:55,715][257371] mean_value=-1833.6067896913771, max_value=547.3656607709294[0m
[37m[1m[2023-07-17 02:53:55,717][257371] New mean coefficients: [[ 1.3605154  -0.45783144 -1.3449051   2.1963158   0.07651217  1.1325332 ]][0m
[37m[1m[2023-07-17 02:53:55,718][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:54:04,735][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 02:54:04,735][257371] FPS: 425943.64[0m
[36m[2023-07-17 02:54:04,738][257371] itr=512, itrs=2000, Progress: 25.60%[0m
[36m[2023-07-17 02:54:16,548][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 02:54:16,549][257371] FPS: 327324.50[0m
[36m[2023-07-17 02:54:20,863][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:54:20,869][257371] Reward + Measures: [[56.05229444  0.33442     0.30192932  0.30252066  0.09998133  3.63222265]][0m
[37m[1m[2023-07-17 02:54:20,869][257371] Max Reward on eval: 56.052294437890716[0m
[37m[1m[2023-07-17 02:54:20,869][257371] Min Reward on eval: 56.052294437890716[0m
[37m[1m[2023-07-17 02:54:20,869][257371] Mean Reward across all agents: 56.052294437890716[0m
[37m[1m[2023-07-17 02:54:20,870][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:54:25,827][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:54:25,827][257371] Reward + Measures: [[ -20.99178837    0.35420001    0.33390003    0.40109998    0.32640001
     2.78388119]
 [ 107.60570696    0.28459999    0.377         0.24100001    0.34110004
     3.04940653]
 [-126.39092073    0.50080001    0.48499998    0.46129999    0.0675
     5.43756342]
 ...
 [   9.48224303    0.40939999    0.34630001    0.33980003    0.32680002
     2.78714085]
 [ -74.06508525    0.36489999    0.26570001    0.28389999    0.1194
     3.27993178]
 [-137.97585559    0.80779999    0.78330004    0.77540004    0.0259
     6.23060942]][0m
[37m[1m[2023-07-17 02:54:25,827][257371] Max Reward on eval: 336.46816723279187[0m
[37m[1m[2023-07-17 02:54:25,828][257371] Min Reward on eval: -291.0022725831717[0m
[37m[1m[2023-07-17 02:54:25,828][257371] Mean Reward across all agents: 13.10843563305067[0m
[37m[1m[2023-07-17 02:54:25,828][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:54:25,832][257371] mean_value=-1264.7778717566187, max_value=378.4828747667234[0m
[37m[1m[2023-07-17 02:54:25,834][257371] New mean coefficients: [[ 0.31395078 -0.90747166 -2.2809708   2.1866837  -0.12731612  1.2795726 ]][0m
[37m[1m[2023-07-17 02:54:25,835][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:54:34,787][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 02:54:34,793][257371] FPS: 429038.93[0m
[36m[2023-07-17 02:54:34,796][257371] itr=513, itrs=2000, Progress: 25.65%[0m
[36m[2023-07-17 02:54:46,645][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 02:54:46,645][257371] FPS: 326207.45[0m
[36m[2023-07-17 02:54:50,884][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:54:50,890][257371] Reward + Measures: [[65.977746    0.36536467  0.32937798  0.33024633  0.09917467  3.68367767]][0m
[37m[1m[2023-07-17 02:54:50,890][257371] Max Reward on eval: 65.97774600220737[0m
[37m[1m[2023-07-17 02:54:50,891][257371] Min Reward on eval: 65.97774600220737[0m
[37m[1m[2023-07-17 02:54:50,891][257371] Mean Reward across all agents: 65.97774600220737[0m
[37m[1m[2023-07-17 02:54:50,891][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:54:56,092][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:54:56,098][257371] Reward + Measures: [[ -27.16321243    0.27940002    0.29429999    0.28990003    0.25999999
     4.79781008]
 [ -16.0735317     0.51940006    0.35629997    0.34510002    0.16730002
     4.00590754]
 [   8.06796039    0.39429998    0.39450002    0.33220002    0.30630001
     4.13286304]
 ...
 [  -1.40883945    0.29100001    0.35770002    0.19170001    0.38690001
     2.76275635]
 [-146.21161081    0.40260002    0.46230003    0.18860002    0.39580002
     2.87560987]
 [  14.07543497    0.37710002    0.44120002    0.26710004    0.29850003
     3.09373331]][0m
[37m[1m[2023-07-17 02:54:56,098][257371] Max Reward on eval: 239.9292152597569[0m
[37m[1m[2023-07-17 02:54:56,098][257371] Min Reward on eval: -198.81385185625405[0m
[37m[1m[2023-07-17 02:54:56,099][257371] Mean Reward across all agents: -17.53412710632859[0m
[37m[1m[2023-07-17 02:54:56,099][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:54:56,101][257371] mean_value=-999.3655367021596, max_value=264.73331025447357[0m
[37m[1m[2023-07-17 02:54:56,104][257371] New mean coefficients: [[-0.5067265  -0.5307614  -1.8033727   1.2554992   0.39470786  1.155253  ]][0m
[37m[1m[2023-07-17 02:54:56,105][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:55:05,089][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 02:55:05,089][257371] FPS: 427500.14[0m
[36m[2023-07-17 02:55:05,091][257371] itr=514, itrs=2000, Progress: 25.70%[0m
[36m[2023-07-17 02:55:16,697][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 02:55:16,698][257371] FPS: 333023.97[0m
[36m[2023-07-17 02:55:20,985][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:55:20,985][257371] Reward + Measures: [[64.22307305  0.35891935  0.32477334  0.32876399  0.09722266  3.72877026]][0m
[37m[1m[2023-07-17 02:55:20,985][257371] Max Reward on eval: 64.22307304603201[0m
[37m[1m[2023-07-17 02:55:20,986][257371] Min Reward on eval: 64.22307304603201[0m
[37m[1m[2023-07-17 02:55:20,986][257371] Mean Reward across all agents: 64.22307304603201[0m
[37m[1m[2023-07-17 02:55:20,986][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:55:25,921][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:55:25,921][257371] Reward + Measures: [[-21.77445312   0.47300002   0.36349997   0.25569999   0.2069
    3.69505477]
 [ 45.14365672   0.67479998   0.67000002   0.62700003   0.0496
    5.46274757]
 [ 51.79967885   0.56480002   0.50250006   0.51049995   0.0912
    4.59514713]
 ...
 [ 38.99511319   0.51889998   0.53149998   0.42980003   0.39120001
    2.9444046 ]
 [-73.15632391   0.23269999   0.18529999   0.20829999   0.12330001
    4.35662079]
 [-19.33882786   0.52090001   0.37509999   0.2563       0.1231
    3.64847946]][0m
[37m[1m[2023-07-17 02:55:25,922][257371] Max Reward on eval: 248.09195949120914[0m
[37m[1m[2023-07-17 02:55:25,922][257371] Min Reward on eval: -406.53821373141835[0m
[37m[1m[2023-07-17 02:55:25,922][257371] Mean Reward across all agents: -11.061006367110666[0m
[37m[1m[2023-07-17 02:55:25,922][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:55:25,927][257371] mean_value=-754.5566095376904, max_value=594.0228481898084[0m
[37m[1m[2023-07-17 02:55:25,929][257371] New mean coefficients: [[-0.61600983 -0.93516827 -0.88315696  0.12711215  0.79944754  1.1442684 ]][0m
[37m[1m[2023-07-17 02:55:25,930][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:55:34,879][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 02:55:34,884][257371] FPS: 429174.55[0m
[36m[2023-07-17 02:55:34,887][257371] itr=515, itrs=2000, Progress: 25.75%[0m
[36m[2023-07-17 02:55:46,583][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 02:55:46,583][257371] FPS: 330556.40[0m
[36m[2023-07-17 02:55:50,759][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:55:50,759][257371] Reward + Measures: [[60.70624104  0.36257035  0.32902965  0.33340302  0.10013933  3.7910254 ]][0m
[37m[1m[2023-07-17 02:55:50,760][257371] Max Reward on eval: 60.70624104180661[0m
[37m[1m[2023-07-17 02:55:50,760][257371] Min Reward on eval: 60.70624104180661[0m
[37m[1m[2023-07-17 02:55:50,760][257371] Mean Reward across all agents: 60.70624104180661[0m
[37m[1m[2023-07-17 02:55:50,760][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:55:55,734][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:55:55,735][257371] Reward + Measures: [[ 126.09576813    0.50929999    0.51969999    0.42540002    0.09230001
     6.35725403]
 [  -2.18200921    0.66729993    0.62639999    0.5535        0.66060001
     5.0899806 ]
 [-103.39538317    0.3529        0.38390002    0.37210003    0.1962
     5.08623362]
 ...
 [ -61.62725125    0.38650003    0.33039999    0.34299999    0.2352
     3.35723114]
 [-112.62030209    0.35550001    0.3238        0.35830003    0.0814
     6.82943344]
 [  32.15388479    0.35680002    0.2933        0.25060001    0.1444
     2.73061872]][0m
[37m[1m[2023-07-17 02:55:55,735][257371] Max Reward on eval: 346.60525245335884[0m
[37m[1m[2023-07-17 02:55:55,735][257371] Min Reward on eval: -345.25404752851466[0m
[37m[1m[2023-07-17 02:55:55,735][257371] Mean Reward across all agents: 12.042479786526231[0m
[37m[1m[2023-07-17 02:55:55,736][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:55:55,740][257371] mean_value=-971.6837404713274, max_value=474.0172771557051[0m
[37m[1m[2023-07-17 02:55:55,743][257371] New mean coefficients: [[-0.79818714 -0.911705   -1.8183839   0.5276896  -0.67482364  1.8962181 ]][0m
[37m[1m[2023-07-17 02:55:55,744][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:56:04,724][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 02:56:04,725][257371] FPS: 427665.47[0m
[36m[2023-07-17 02:56:04,727][257371] itr=516, itrs=2000, Progress: 25.80%[0m
[36m[2023-07-17 02:56:16,975][257371] train() took 12.17 seconds to complete[0m
[36m[2023-07-17 02:56:16,975][257371] FPS: 315600.76[0m
[36m[2023-07-17 02:56:21,333][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:56:21,334][257371] Reward + Measures: [[59.32175556  0.34967533  0.32017499  0.32761332  0.10234733  3.83591986]][0m
[37m[1m[2023-07-17 02:56:21,334][257371] Max Reward on eval: 59.321755557358436[0m
[37m[1m[2023-07-17 02:56:21,334][257371] Min Reward on eval: 59.321755557358436[0m
[37m[1m[2023-07-17 02:56:21,335][257371] Mean Reward across all agents: 59.321755557358436[0m
[37m[1m[2023-07-17 02:56:21,335][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:56:26,358][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:56:26,363][257371] Reward + Measures: [[ 21.91515798   0.5686       0.58640003   0.52680004   0.19829999
    4.94855261]
 [ 34.65142278   0.53680003   0.55430001   0.42009997   0.2445
    5.02636623]
 [181.83554096   0.63880002   0.61690003   0.59170002   0.0733
    4.8833518 ]
 ...
 [  0.74621785   0.64169997   0.69639999   0.6814       0.0715
    4.48256826]
 [-39.85774658   0.20030001   0.24089999   0.24000001   0.24300002
    3.51668739]
 [ -9.90443892   0.19380002   0.17029999   0.14310001   0.0734
    4.76850939]][0m
[37m[1m[2023-07-17 02:56:26,364][257371] Max Reward on eval: 704.6465454183519[0m
[37m[1m[2023-07-17 02:56:26,364][257371] Min Reward on eval: -308.1434812431224[0m
[37m[1m[2023-07-17 02:56:26,364][257371] Mean Reward across all agents: 8.217269445886167[0m
[37m[1m[2023-07-17 02:56:26,364][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:56:26,370][257371] mean_value=-614.8990638614574, max_value=633.4688738540747[0m
[37m[1m[2023-07-17 02:56:26,373][257371] New mean coefficients: [[-0.14670229 -0.518208   -2.4433854  -0.11224759  0.18582296  1.7825934 ]][0m
[37m[1m[2023-07-17 02:56:26,374][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:56:35,468][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 02:56:35,469][257371] FPS: 422304.05[0m
[36m[2023-07-17 02:56:35,471][257371] itr=517, itrs=2000, Progress: 25.85%[0m
[36m[2023-07-17 02:56:47,526][257371] train() took 11.98 seconds to complete[0m
[36m[2023-07-17 02:56:47,526][257371] FPS: 320649.30[0m
[36m[2023-07-17 02:56:51,730][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:56:51,736][257371] Reward + Measures: [[59.60900041  0.34502003  0.31581098  0.32896     0.10854234  3.92750192]][0m
[37m[1m[2023-07-17 02:56:51,736][257371] Max Reward on eval: 59.60900040875924[0m
[37m[1m[2023-07-17 02:56:51,736][257371] Min Reward on eval: 59.60900040875924[0m
[37m[1m[2023-07-17 02:56:51,736][257371] Mean Reward across all agents: 59.60900040875924[0m
[37m[1m[2023-07-17 02:56:51,737][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:56:56,659][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:56:56,660][257371] Reward + Measures: [[-240.12987616    0.48659998    0.38600001    0.36820003    0.2105
     3.23269081]
 [  18.96807105    0.3698        0.28299999    0.44969997    0.32909998
     4.17496443]
 [ -21.49902291    0.17840001    0.1684        0.1524        0.1541
     4.10744047]
 ...
 [  -7.9424599     0.2502        0.2395        0.2175        0.20109999
     3.15854812]
 [ 195.04550633    0.7306        0.69950002    0.68410003    0.07
     5.73699188]
 [  33.70763561    0.4073        0.34169999    0.278         0.1547
     4.46918488]][0m
[37m[1m[2023-07-17 02:56:56,660][257371] Max Reward on eval: 389.3675441865809[0m
[37m[1m[2023-07-17 02:56:56,660][257371] Min Reward on eval: -240.12987615950405[0m
[37m[1m[2023-07-17 02:56:56,660][257371] Mean Reward across all agents: 50.371517605491555[0m
[37m[1m[2023-07-17 02:56:56,661][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:56:56,665][257371] mean_value=-1525.174771523891, max_value=432.8050522898089[0m
[37m[1m[2023-07-17 02:56:56,667][257371] New mean coefficients: [[-0.16226727 -1.9284497  -1.5983355  -2.3688521   0.33862323  2.2482424 ]][0m
[37m[1m[2023-07-17 02:56:56,668][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:57:05,650][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 02:57:05,651][257371] FPS: 427592.63[0m
[36m[2023-07-17 02:57:05,653][257371] itr=518, itrs=2000, Progress: 25.90%[0m
[36m[2023-07-17 02:57:17,341][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 02:57:17,341][257371] FPS: 330769.51[0m
[36m[2023-07-17 02:57:21,710][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:57:21,716][257371] Reward + Measures: [[57.05282568  0.34481266  0.31528634  0.33089533  0.11261734  4.03570747]][0m
[37m[1m[2023-07-17 02:57:21,716][257371] Max Reward on eval: 57.05282567654488[0m
[37m[1m[2023-07-17 02:57:21,716][257371] Min Reward on eval: 57.05282567654488[0m
[37m[1m[2023-07-17 02:57:21,717][257371] Mean Reward across all agents: 57.05282567654488[0m
[37m[1m[2023-07-17 02:57:21,717][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:57:26,762][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:57:26,767][257371] Reward + Measures: [[ 16.15760389   0.19919999   0.1495       0.09500001   0.1839
    5.49540091]
 [  5.78222229   0.1657       0.1286       0.0732       0.1331
    5.53355169]
 [  4.41800615   0.18189999   0.11370001   0.0741       0.10680001
    5.21896315]
 ...
 [  3.18897124   0.28400001   0.19950001   0.1904       0.1168
    5.4438715 ]
 [-55.78946335   0.20649998   0.2306       0.19310002   0.1531
    4.56436253]
 [  3.40780114   0.1671       0.11680001   0.0826       0.12710002
    4.06146097]][0m
[37m[1m[2023-07-17 02:57:26,768][257371] Max Reward on eval: 398.79522128026[0m
[37m[1m[2023-07-17 02:57:26,768][257371] Min Reward on eval: -200.35419934820385[0m
[37m[1m[2023-07-17 02:57:26,768][257371] Mean Reward across all agents: 27.129827040460835[0m
[37m[1m[2023-07-17 02:57:26,768][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:57:26,771][257371] mean_value=-1189.70981981484, max_value=259.125841822424[0m
[37m[1m[2023-07-17 02:57:26,773][257371] New mean coefficients: [[ 0.40966874 -1.5016112  -1.6010944  -0.452505   -0.30278385  3.0373378 ]][0m
[37m[1m[2023-07-17 02:57:26,774][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:57:35,943][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-17 02:57:35,944][257371] FPS: 418880.43[0m
[36m[2023-07-17 02:57:35,946][257371] itr=519, itrs=2000, Progress: 25.95%[0m
[36m[2023-07-17 02:57:47,732][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 02:57:47,733][257371] FPS: 327981.02[0m
[36m[2023-07-17 02:57:51,932][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:57:51,938][257371] Reward + Measures: [[52.76668907  0.33414099  0.30718502  0.32504767  0.11302867  4.01625443]][0m
[37m[1m[2023-07-17 02:57:51,938][257371] Max Reward on eval: 52.76668907158222[0m
[37m[1m[2023-07-17 02:57:51,938][257371] Min Reward on eval: 52.76668907158222[0m
[37m[1m[2023-07-17 02:57:51,938][257371] Mean Reward across all agents: 52.76668907158222[0m
[37m[1m[2023-07-17 02:57:51,939][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:57:57,137][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 02:57:57,143][257371] Reward + Measures: [[-63.01928758   0.2069       0.1998       0.1982       0.1869
    3.45727134]
 [ 25.11393833   0.37269998   0.28580001   0.28930002   0.1247
    4.22494173]
 [-73.52187519   0.33270001   0.27379999   0.27069998   0.2194
    4.16343451]
 ...
 [-21.35323976   0.1717       0.14839999   0.14240001   0.0999
    4.2935791 ]
 [-30.79534825   0.21850002   0.2076       0.16129999   0.1232
    3.39705634]
 [-50.75267313   0.322        0.27789998   0.20910001   0.0691
    5.35860205]][0m
[37m[1m[2023-07-17 02:57:57,143][257371] Max Reward on eval: 242.7342866827734[0m
[37m[1m[2023-07-17 02:57:57,143][257371] Min Reward on eval: -428.7855176931713[0m
[37m[1m[2023-07-17 02:57:57,144][257371] Mean Reward across all agents: -28.971690139358834[0m
[37m[1m[2023-07-17 02:57:57,144][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 02:57:57,146][257371] mean_value=-1996.8581833805097, max_value=156.38790521482593[0m
[37m[1m[2023-07-17 02:57:57,148][257371] New mean coefficients: [[ 0.03119412 -0.74204725 -1.5957912   0.32375258 -0.29681677  4.0797167 ]][0m
[37m[1m[2023-07-17 02:57:57,149][257371] Moving the mean solution point...[0m
[36m[2023-07-17 02:58:06,154][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 02:58:06,154][257371] FPS: 426509.07[0m
[36m[2023-07-17 02:58:06,156][257371] itr=520, itrs=2000, Progress: 26.00%[0m
[37m[1m[2023-07-17 03:00:56,621][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000500[0m
[36m[2023-07-17 03:01:08,663][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 03:01:08,663][257371] FPS: 333028.53[0m
[36m[2023-07-17 03:01:12,912][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:01:12,913][257371] Reward + Measures: [[52.31531806  0.32107499  0.29512167  0.31483436  0.11476666  4.08755302]][0m
[37m[1m[2023-07-17 03:01:12,913][257371] Max Reward on eval: 52.315318060734306[0m
[37m[1m[2023-07-17 03:01:12,913][257371] Min Reward on eval: 52.315318060734306[0m
[37m[1m[2023-07-17 03:01:12,914][257371] Mean Reward across all agents: 52.315318060734306[0m
[37m[1m[2023-07-17 03:01:12,914][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:01:17,940][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:01:17,940][257371] Reward + Measures: [[ -37.09725022    0.77940005    0.69839996    0.70240003    0.0361
     4.97423744]
 [-110.32452681    0.62410003    0.58810002    0.56980002    0.0663
     5.01624632]
 [ -16.7452333     0.67199999    0.62080002    0.58380002    0.0765
     4.5443306 ]
 ...
 [   2.60330466    0.1761        0.1752        0.14830001    0.1104
     2.88301826]
 [  69.70153146    0.34619999    0.36499998    0.2502        0.32880002
     3.1646409 ]
 [ -34.39521815    0.3066        0.24530001    0.30250001    0.15100001
     4.33516216]][0m
[37m[1m[2023-07-17 03:01:17,941][257371] Max Reward on eval: 134.31020448514028[0m
[37m[1m[2023-07-17 03:01:17,941][257371] Min Reward on eval: -163.71855445710244[0m
[37m[1m[2023-07-17 03:01:17,941][257371] Mean Reward across all agents: -12.316558544900017[0m
[37m[1m[2023-07-17 03:01:17,941][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:01:17,943][257371] mean_value=-1327.05204469784, max_value=314.8113630813393[0m
[37m[1m[2023-07-17 03:01:17,946][257371] New mean coefficients: [[ 0.2884054  -0.64774156 -2.266319    0.19264032 -0.21712056  4.6315823 ]][0m
[37m[1m[2023-07-17 03:01:17,947][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:01:26,862][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-17 03:01:26,862][257371] FPS: 430797.49[0m
[36m[2023-07-17 03:01:26,865][257371] itr=521, itrs=2000, Progress: 26.05%[0m
[36m[2023-07-17 03:01:39,062][257371] train() took 12.12 seconds to complete[0m
[36m[2023-07-17 03:01:39,063][257371] FPS: 316813.18[0m
[36m[2023-07-17 03:01:43,354][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:01:43,354][257371] Reward + Measures: [[52.33744803  0.32094899  0.29739499  0.31344733  0.11253866  4.12866783]][0m
[37m[1m[2023-07-17 03:01:43,355][257371] Max Reward on eval: 52.33744802602208[0m
[37m[1m[2023-07-17 03:01:43,355][257371] Min Reward on eval: 52.33744802602208[0m
[37m[1m[2023-07-17 03:01:43,355][257371] Mean Reward across all agents: 52.33744802602208[0m
[37m[1m[2023-07-17 03:01:43,355][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:01:48,591][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:01:48,591][257371] Reward + Measures: [[-34.92626037   0.16329999   0.13780001   0.1312       0.0634
    4.40881252]
 [134.5407204    0.50730002   0.4508       0.40839997   0.0603
    5.29077148]
 [ 26.99700812   0.33390003   0.31330004   0.27429998   0.0558
    4.00148535]
 ...
 [ 33.48511779   0.2868       0.22760001   0.2378       0.10910001
    4.76811647]
 [  0.82603419   0.44190001   0.43580005   0.3856       0.1165
    5.30533361]
 [108.93377947   0.47340003   0.4219       0.3547       0.0901
    4.68825912]][0m
[37m[1m[2023-07-17 03:01:48,592][257371] Max Reward on eval: 331.27385703949255[0m
[37m[1m[2023-07-17 03:01:48,592][257371] Min Reward on eval: -116.61183928842657[0m
[37m[1m[2023-07-17 03:01:48,592][257371] Mean Reward across all agents: 19.195066888135702[0m
[37m[1m[2023-07-17 03:01:48,592][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:01:48,595][257371] mean_value=-971.5824354362977, max_value=283.1754088755678[0m
[37m[1m[2023-07-17 03:01:48,598][257371] New mean coefficients: [[ 0.7892077 -1.4691927 -1.8826513  0.7017156 -1.2408614  5.5786366]][0m
[37m[1m[2023-07-17 03:01:48,599][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:01:57,705][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 03:01:57,705][257371] FPS: 421785.29[0m
[36m[2023-07-17 03:01:57,707][257371] itr=522, itrs=2000, Progress: 26.10%[0m
[36m[2023-07-17 03:02:09,514][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 03:02:09,514][257371] FPS: 327494.15[0m
[36m[2023-07-17 03:02:13,765][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:02:13,765][257371] Reward + Measures: [[55.67603913  0.32971233  0.30378032  0.31877735  0.11297066  4.18536234]][0m
[37m[1m[2023-07-17 03:02:13,765][257371] Max Reward on eval: 55.67603913212019[0m
[37m[1m[2023-07-17 03:02:13,766][257371] Min Reward on eval: 55.67603913212019[0m
[37m[1m[2023-07-17 03:02:13,766][257371] Mean Reward across all agents: 55.67603913212019[0m
[37m[1m[2023-07-17 03:02:13,766][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:02:18,824][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:02:18,825][257371] Reward + Measures: [[  40.33583701    0.4443        0.3477        0.2854        0.23
     3.38540888]
 [  85.6178559     0.28599998    0.23699999    0.1534        0.23630002
     3.48383951]
 [ -25.85417173    0.23010002    0.21470001    0.16010001    0.1745
     4.15975475]
 ...
 [  31.63225571    0.1564        0.12980001    0.0958        0.0915
     3.65746546]
 [ -79.00840732    0.156         0.1471        0.0995        0.1305
     4.09952116]
 [-113.156157      0.56200004    0.6268        0.23119998    0.52210003
     3.76874352]][0m
[37m[1m[2023-07-17 03:02:18,825][257371] Max Reward on eval: 176.82939648437315[0m
[37m[1m[2023-07-17 03:02:18,825][257371] Min Reward on eval: -286.4830336254323[0m
[37m[1m[2023-07-17 03:02:18,825][257371] Mean Reward across all agents: -33.029810875381614[0m
[37m[1m[2023-07-17 03:02:18,826][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:02:18,828][257371] mean_value=-2454.944564000952, max_value=374.99866551383667[0m
[37m[1m[2023-07-17 03:02:18,830][257371] New mean coefficients: [[ 1.1717715  -1.0125108  -0.05746663  1.3455453  -1.1370664   5.9589734 ]][0m
[37m[1m[2023-07-17 03:02:18,831][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:02:27,779][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 03:02:27,780][257371] FPS: 429227.36[0m
[36m[2023-07-17 03:02:27,782][257371] itr=523, itrs=2000, Progress: 26.15%[0m
[36m[2023-07-17 03:02:39,427][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 03:02:39,427][257371] FPS: 331912.57[0m
[36m[2023-07-17 03:02:43,736][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:02:43,736][257371] Reward + Measures: [[55.41229132  0.33211765  0.30935967  0.3222      0.114709    4.25274801]][0m
[37m[1m[2023-07-17 03:02:43,736][257371] Max Reward on eval: 55.41229131727031[0m
[37m[1m[2023-07-17 03:02:43,737][257371] Min Reward on eval: 55.41229131727031[0m
[37m[1m[2023-07-17 03:02:43,737][257371] Mean Reward across all agents: 55.41229131727031[0m
[37m[1m[2023-07-17 03:02:43,737][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:02:48,738][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:02:48,739][257371] Reward + Measures: [[129.515522     0.57090002   0.50030005   0.47580001   0.0544
    3.9104259 ]
 [-35.10221348   0.43420002   0.48100001   0.37400001   0.0907
    3.27657318]
 [ 79.02162524   0.50270003   0.4601       0.43529996   0.0519
    4.30602694]
 ...
 [-47.71800327   0.29809999   0.28299999   0.27360001   0.0969
    4.11241388]
 [ 53.68315666   0.5072       0.47399998   0.40299997   0.13080001
    3.35478973]
 [125.18559904   0.43730003   0.42410001   0.38480002   0.0458
    3.95945215]][0m
[37m[1m[2023-07-17 03:02:48,739][257371] Max Reward on eval: 279.2867045360152[0m
[37m[1m[2023-07-17 03:02:48,739][257371] Min Reward on eval: -206.61503119845875[0m
[37m[1m[2023-07-17 03:02:48,740][257371] Mean Reward across all agents: 36.25240658953073[0m
[37m[1m[2023-07-17 03:02:48,740][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:02:48,743][257371] mean_value=-760.9932202764805, max_value=651.6699380826205[0m
[37m[1m[2023-07-17 03:02:48,746][257371] New mean coefficients: [[ 1.8860903  -1.3455331   0.66200745  1.8702226  -0.7737917   7.5283804 ]][0m
[37m[1m[2023-07-17 03:02:48,747][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:02:57,838][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 03:02:57,838][257371] FPS: 422463.66[0m
[36m[2023-07-17 03:02:57,841][257371] itr=524, itrs=2000, Progress: 26.20%[0m
[36m[2023-07-17 03:03:09,594][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 03:03:09,594][257371] FPS: 328983.49[0m
[36m[2023-07-17 03:03:13,923][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:03:13,924][257371] Reward + Measures: [[55.18867989  0.32484668  0.30146134  0.31891999  0.11568334  4.27997208]][0m
[37m[1m[2023-07-17 03:03:13,924][257371] Max Reward on eval: 55.188679887094615[0m
[37m[1m[2023-07-17 03:03:13,924][257371] Min Reward on eval: 55.188679887094615[0m
[37m[1m[2023-07-17 03:03:13,924][257371] Mean Reward across all agents: 55.188679887094615[0m
[37m[1m[2023-07-17 03:03:13,925][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:03:18,942][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:03:18,942][257371] Reward + Measures: [[150.20916559   0.55610007   0.43420002   0.47499999   0.0609
    4.66388035]
 [219.78506064   0.73170006   0.66500008   0.62250006   0.0297
    5.16749287]
 [183.05720172   0.40120003   0.36140004   0.34029999   0.0499
    5.06967831]
 ...
 [100.61293408   0.73470002   0.65580004   0.66049999   0.10320001
    4.71732044]
 [-43.15580799   0.20630001   0.1707       0.14049999   0.064
    3.84555292]
 [ 83.8582656    0.5104       0.44730002   0.38750002   0.0663
    3.82448363]][0m
[37m[1m[2023-07-17 03:03:18,943][257371] Max Reward on eval: 530.5201111059636[0m
[37m[1m[2023-07-17 03:03:18,943][257371] Min Reward on eval: -206.36665297324072[0m
[37m[1m[2023-07-17 03:03:18,943][257371] Mean Reward across all agents: 40.86868798991143[0m
[37m[1m[2023-07-17 03:03:18,944][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:03:18,948][257371] mean_value=-924.7924434698172, max_value=328.305892744516[0m
[37m[1m[2023-07-17 03:03:18,951][257371] New mean coefficients: [[ 2.200502   -1.3850582   0.43371612  2.1557312  -2.4809396   6.126555  ]][0m
[37m[1m[2023-07-17 03:03:18,951][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:03:27,957][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 03:03:27,957][257371] FPS: 426505.93[0m
[36m[2023-07-17 03:03:27,959][257371] itr=525, itrs=2000, Progress: 26.25%[0m
[36m[2023-07-17 03:03:39,615][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 03:03:39,615][257371] FPS: 331700.98[0m
[36m[2023-07-17 03:03:43,873][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:03:43,874][257371] Reward + Measures: [[59.49973563  0.33913031  0.31661364  0.32706499  0.11003033  4.41321707]][0m
[37m[1m[2023-07-17 03:03:43,874][257371] Max Reward on eval: 59.49973563300384[0m
[37m[1m[2023-07-17 03:03:43,874][257371] Min Reward on eval: 59.49973563300384[0m
[37m[1m[2023-07-17 03:03:43,874][257371] Mean Reward across all agents: 59.49973563300384[0m
[37m[1m[2023-07-17 03:03:43,875][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:03:48,839][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:03:48,840][257371] Reward + Measures: [[ 55.35235452   0.4815       0.45110002   0.45310003   0.0612
    5.36614227]
 [-33.99835959   0.56540006   0.35780001   0.41010004   0.13350001
    5.49022627]
 [-24.2100567    0.38790002   0.33919999   0.33130002   0.0531
    5.06881857]
 ...
 [207.23554785   0.43210003   0.44390002   0.3872       0.0403
    5.73031092]
 [-22.04343263   0.38570002   0.33540002   0.34         0.1693
    2.96858287]
 [-61.58680552   0.2667       0.22459999   0.17200002   0.1194
    4.16697645]][0m
[37m[1m[2023-07-17 03:03:48,840][257371] Max Reward on eval: 378.94719697441906[0m
[37m[1m[2023-07-17 03:03:48,840][257371] Min Reward on eval: -149.73726744346786[0m
[37m[1m[2023-07-17 03:03:48,840][257371] Mean Reward across all agents: 38.80693848868695[0m
[37m[1m[2023-07-17 03:03:48,841][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:03:48,844][257371] mean_value=-674.9146255038198, max_value=366.94030739361756[0m
[37m[1m[2023-07-17 03:03:48,846][257371] New mean coefficients: [[ 3.2378368  -0.682363   -0.46814245  1.6475499  -2.5411627   6.933812  ]][0m
[37m[1m[2023-07-17 03:03:48,847][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:03:57,877][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 03:03:57,877][257371] FPS: 425350.05[0m
[36m[2023-07-17 03:03:57,880][257371] itr=526, itrs=2000, Progress: 26.30%[0m
[36m[2023-07-17 03:04:09,586][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 03:04:09,586][257371] FPS: 330228.91[0m
[36m[2023-07-17 03:04:13,898][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:04:13,898][257371] Reward + Measures: [[60.72788094  0.34998664  0.32937533  0.33217368  0.10652899  4.46731424]][0m
[37m[1m[2023-07-17 03:04:13,899][257371] Max Reward on eval: 60.72788094035763[0m
[37m[1m[2023-07-17 03:04:13,899][257371] Min Reward on eval: 60.72788094035763[0m
[37m[1m[2023-07-17 03:04:13,899][257371] Mean Reward across all agents: 60.72788094035763[0m
[37m[1m[2023-07-17 03:04:13,900][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:04:19,168][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:04:19,174][257371] Reward + Measures: [[ 39.96300181   0.51280004   0.50209999   0.40190002   0.28660002
    4.18543959]
 [ 19.48280351   0.20290001   0.20479999   0.11540001   0.16470002
    3.05559778]
 [-20.83174073   0.12420001   0.1184       0.1497       0.10810001
    3.56722951]
 ...
 [ 36.64577637   0.40950003   0.5614       0.35370001   0.49020001
    5.00261688]
 [-58.59595618   0.54819995   0.56670004   0.2457       0.39070001
    4.88990164]
 [ 64.35986828   0.58400005   0.6214       0.39320001   0.37910002
    4.70715761]][0m
[37m[1m[2023-07-17 03:04:19,174][257371] Max Reward on eval: 191.92565775942057[0m
[37m[1m[2023-07-17 03:04:19,174][257371] Min Reward on eval: -220.41428804530295[0m
[37m[1m[2023-07-17 03:04:19,175][257371] Mean Reward across all agents: -20.64779656972035[0m
[37m[1m[2023-07-17 03:04:19,175][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:04:19,178][257371] mean_value=-1953.3281073068501, max_value=353.9339958311422[0m
[37m[1m[2023-07-17 03:04:19,181][257371] New mean coefficients: [[ 3.531657    0.14579678  0.914326    1.1775495  -1.7373766   7.399235  ]][0m
[37m[1m[2023-07-17 03:04:19,182][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:04:28,336][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-17 03:04:28,337][257371] FPS: 419556.51[0m
[36m[2023-07-17 03:04:28,339][257371] itr=527, itrs=2000, Progress: 26.35%[0m
[36m[2023-07-17 03:04:40,031][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 03:04:40,032][257371] FPS: 330588.15[0m
[36m[2023-07-17 03:04:44,235][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:04:44,236][257371] Reward + Measures: [[58.49516443  0.33784699  0.31921101  0.32221332  0.10528033  4.47010231]][0m
[37m[1m[2023-07-17 03:04:44,236][257371] Max Reward on eval: 58.49516442508575[0m
[37m[1m[2023-07-17 03:04:44,236][257371] Min Reward on eval: 58.49516442508575[0m
[37m[1m[2023-07-17 03:04:44,237][257371] Mean Reward across all agents: 58.49516442508575[0m
[37m[1m[2023-07-17 03:04:44,237][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:04:49,212][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:04:49,212][257371] Reward + Measures: [[ 53.7100107    0.20840001   0.204        0.21460001   0.0622
    4.05745935]
 [  3.20395032   0.22420001   0.1901       0.21879999   0.1481
    4.10841465]
 [-48.59722286   0.28010002   0.22860003   0.2362       0.0596
    4.40181971]
 ...
 [-24.01135565   0.30570003   0.30950001   0.29370001   0.1158
    4.1461091 ]
 [115.90421871   0.53739995   0.42840004   0.39439997   0.1037
    3.84038019]
 [131.56140853   0.47750002   0.44380003   0.42930004   0.0439
    4.53919029]][0m
[37m[1m[2023-07-17 03:04:49,213][257371] Max Reward on eval: 342.82860756929733[0m
[37m[1m[2023-07-17 03:04:49,213][257371] Min Reward on eval: -84.69566294597462[0m
[37m[1m[2023-07-17 03:04:49,213][257371] Mean Reward across all agents: 28.085214919021478[0m
[37m[1m[2023-07-17 03:04:49,213][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:04:49,216][257371] mean_value=-1300.2806407664805, max_value=505.9656128353639[0m
[37m[1m[2023-07-17 03:04:49,219][257371] New mean coefficients: [[ 3.7033472  1.3697833  0.998314   0.527     -0.5436722  7.391108 ]][0m
[37m[1m[2023-07-17 03:04:49,220][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:04:58,269][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 03:04:58,270][257371] FPS: 424418.22[0m
[36m[2023-07-17 03:04:58,272][257371] itr=528, itrs=2000, Progress: 26.40%[0m
[36m[2023-07-17 03:05:10,041][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 03:05:10,041][257371] FPS: 328503.00[0m
[36m[2023-07-17 03:05:14,308][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:05:14,309][257371] Reward + Measures: [[59.86567279  0.32659766  0.30812034  0.30848235  0.10865566  4.49433899]][0m
[37m[1m[2023-07-17 03:05:14,309][257371] Max Reward on eval: 59.865672794053175[0m
[37m[1m[2023-07-17 03:05:14,309][257371] Min Reward on eval: 59.865672794053175[0m
[37m[1m[2023-07-17 03:05:14,309][257371] Mean Reward across all agents: 59.865672794053175[0m
[37m[1m[2023-07-17 03:05:14,310][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:05:19,282][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:05:19,283][257371] Reward + Measures: [[ 22.71700352   0.20910001   0.67389995   0.49640003   0.61320007
    3.03171515]
 [ -8.12604334   0.2581       0.38400003   0.32280001   0.33669999
    2.94942069]
 [ 33.9597698    0.39919999   0.51859999   0.41429996   0.354
    2.69016147]
 ...
 [ 71.04202033   0.45740005   0.64660007   0.15580001   0.55100006
    4.58285141]
 [-37.96067798   0.52350008   0.46960002   0.38399997   0.28169999
    2.86227202]
 [ -4.50700884   0.60569996   0.59710002   0.48950002   0.1758
    4.34909964]][0m
[37m[1m[2023-07-17 03:05:19,283][257371] Max Reward on eval: 446.3043838739395[0m
[37m[1m[2023-07-17 03:05:19,283][257371] Min Reward on eval: -123.90601018089801[0m
[37m[1m[2023-07-17 03:05:19,284][257371] Mean Reward across all agents: 44.77711897821025[0m
[37m[1m[2023-07-17 03:05:19,284][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:05:19,288][257371] mean_value=-411.18493839906034, max_value=884.3777103472501[0m
[37m[1m[2023-07-17 03:05:19,291][257371] New mean coefficients: [[3.770721   1.7311444  1.6257517  0.7717078  0.03246796 7.0668373 ]][0m
[37m[1m[2023-07-17 03:05:19,292][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:05:28,234][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 03:05:28,235][257371] FPS: 429504.40[0m
[36m[2023-07-17 03:05:28,237][257371] itr=529, itrs=2000, Progress: 26.45%[0m
[36m[2023-07-17 03:05:39,904][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 03:05:39,905][257371] FPS: 331398.84[0m
[36m[2023-07-17 03:05:44,232][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:05:44,233][257371] Reward + Measures: [[63.82876706  0.32153666  0.30296832  0.30768567  0.109945    4.49559975]][0m
[37m[1m[2023-07-17 03:05:44,233][257371] Max Reward on eval: 63.82876706301037[0m
[37m[1m[2023-07-17 03:05:44,233][257371] Min Reward on eval: 63.82876706301037[0m
[37m[1m[2023-07-17 03:05:44,233][257371] Mean Reward across all agents: 63.82876706301037[0m
[37m[1m[2023-07-17 03:05:44,234][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:05:49,328][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:05:49,333][257371] Reward + Measures: [[ 90.30696253   0.47460005   0.41770002   0.37690002   0.07140001
    4.36494827]
 [ 25.7006626    0.34440002   0.29010001   0.27250001   0.085
    3.80660105]
 [  2.83558723   0.5733       0.46010002   0.48050004   0.12639999
    3.31842017]
 ...
 [139.8098712    0.67409998   0.62120003   0.56059998   0.0625
    4.16579294]
 [ 62.89097705   0.34440002   0.30560002   0.28470001   0.0795
    4.06271505]
 [-42.64248897   0.17029999   0.22289999   0.16589999   0.11670001
    4.52944851]][0m
[37m[1m[2023-07-17 03:05:49,334][257371] Max Reward on eval: 263.12998385472457[0m
[37m[1m[2023-07-17 03:05:49,334][257371] Min Reward on eval: -126.77660705908784[0m
[37m[1m[2023-07-17 03:05:49,334][257371] Mean Reward across all agents: 41.210303961151794[0m
[37m[1m[2023-07-17 03:05:49,335][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:05:49,337][257371] mean_value=-1238.09009995916, max_value=336.4738336793521[0m
[37m[1m[2023-07-17 03:05:49,339][257371] New mean coefficients: [[4.015322   2.3908927  2.241208   1.2950088  0.10266827 7.503868  ]][0m
[37m[1m[2023-07-17 03:05:49,340][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:05:58,458][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 03:05:58,458][257371] FPS: 421220.21[0m
[36m[2023-07-17 03:05:58,460][257371] itr=530, itrs=2000, Progress: 26.50%[0m
[37m[1m[2023-07-17 03:08:47,800][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000510[0m
[36m[2023-07-17 03:08:59,947][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 03:08:59,947][257371] FPS: 330879.83[0m
[36m[2023-07-17 03:09:04,078][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:09:04,079][257371] Reward + Measures: [[64.91957506  0.32270434  0.30386734  0.30605736  0.10787433  4.49940348]][0m
[37m[1m[2023-07-17 03:09:04,079][257371] Max Reward on eval: 64.91957505579565[0m
[37m[1m[2023-07-17 03:09:04,079][257371] Min Reward on eval: 64.91957505579565[0m
[37m[1m[2023-07-17 03:09:04,079][257371] Mean Reward across all agents: 64.91957505579565[0m
[37m[1m[2023-07-17 03:09:04,079][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:09:08,963][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:09:08,963][257371] Reward + Measures: [[ 90.53549717   0.77010006   0.55690002   0.60229999   0.1349
    4.24982405]
 [169.20662448   0.71290004   0.62660003   0.59960002   0.0561
    4.24782085]
 [127.33448623   0.63800001   0.53219998   0.4876       0.1832
    4.00524521]
 ...
 [ 37.61543995   0.50980008   0.48260003   0.43269998   0.28510004
    4.06748819]
 [ 47.62278485   0.59030002   0.52529997   0.42030001   0.25060001
    3.75962377]
 [441.23689461   0.91839999   0.88640004   0.87150002   0.0351
    4.00004816]][0m
[37m[1m[2023-07-17 03:09:08,963][257371] Max Reward on eval: 604.7839850771009[0m
[37m[1m[2023-07-17 03:09:08,964][257371] Min Reward on eval: -270.01568983867764[0m
[37m[1m[2023-07-17 03:09:08,964][257371] Mean Reward across all agents: 226.50386621438122[0m
[37m[1m[2023-07-17 03:09:08,964][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:09:08,970][257371] mean_value=14.241641979930586, max_value=643.1923036274857[0m
[37m[1m[2023-07-17 03:09:08,973][257371] New mean coefficients: [[4.0129056 2.100882  2.4816926 1.3536835 0.5673776 6.895728 ]][0m
[37m[1m[2023-07-17 03:09:08,974][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:09:18,058][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 03:09:18,058][257371] FPS: 422819.51[0m
[36m[2023-07-17 03:09:18,060][257371] itr=531, itrs=2000, Progress: 26.55%[0m
[36m[2023-07-17 03:09:29,621][257371] train() took 11.48 seconds to complete[0m
[36m[2023-07-17 03:09:29,621][257371] FPS: 334381.65[0m
[36m[2023-07-17 03:09:33,836][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:09:33,836][257371] Reward + Measures: [[71.7327962   0.32330501  0.304297    0.30896866  0.10706433  4.56409407]][0m
[37m[1m[2023-07-17 03:09:33,836][257371] Max Reward on eval: 71.73279619648643[0m
[37m[1m[2023-07-17 03:09:33,836][257371] Min Reward on eval: 71.73279619648643[0m
[37m[1m[2023-07-17 03:09:33,837][257371] Mean Reward across all agents: 71.73279619648643[0m
[37m[1m[2023-07-17 03:09:33,837][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:09:38,770][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:09:38,770][257371] Reward + Measures: [[179.54354926   0.52069998   0.4815       0.4698       0.0608
    4.59058428]
 [118.44350814   0.58779997   0.54179996   0.50299996   0.0737
    4.07349253]
 [ 67.86525304   0.42019996   0.37550002   0.36480001   0.055
    3.95893168]
 ...
 [125.3815637    0.55580002   0.50739998   0.49430004   0.0953
    4.80712223]
 [ 45.2268729    0.45880005   0.40170002   0.41300002   0.10470001
    4.5144124 ]
 [ 40.17423664   0.35020003   0.30630001   0.32740003   0.10619999
    4.1574707 ]][0m
[37m[1m[2023-07-17 03:09:38,771][257371] Max Reward on eval: 247.17722893729805[0m
[37m[1m[2023-07-17 03:09:38,771][257371] Min Reward on eval: -62.20266968181822[0m
[37m[1m[2023-07-17 03:09:38,771][257371] Mean Reward across all agents: 63.329415125971394[0m
[37m[1m[2023-07-17 03:09:38,771][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:09:38,773][257371] mean_value=-1407.9518294665938, max_value=73.41756784595385[0m
[37m[1m[2023-07-17 03:09:38,776][257371] New mean coefficients: [[4.1984305 1.7485304 2.3949308 1.6701527 0.6490779 5.495609 ]][0m
[37m[1m[2023-07-17 03:09:38,777][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:09:47,806][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 03:09:47,806][257371] FPS: 425372.57[0m
[36m[2023-07-17 03:09:47,808][257371] itr=532, itrs=2000, Progress: 26.60%[0m
[36m[2023-07-17 03:09:59,627][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 03:09:59,627][257371] FPS: 327148.45[0m
[36m[2023-07-17 03:10:03,942][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:10:03,942][257371] Reward + Measures: [[71.92688694  0.314262    0.29360732  0.29950666  0.114905    4.59036064]][0m
[37m[1m[2023-07-17 03:10:03,942][257371] Max Reward on eval: 71.92688694377975[0m
[37m[1m[2023-07-17 03:10:03,943][257371] Min Reward on eval: 71.92688694377975[0m
[37m[1m[2023-07-17 03:10:03,943][257371] Mean Reward across all agents: 71.92688694377975[0m
[37m[1m[2023-07-17 03:10:03,943][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:10:08,965][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:10:08,966][257371] Reward + Measures: [[ 98.59349532   0.57919997   0.55860001   0.56349999   0.049
    4.41814041]
 [ 68.17485951   0.62830001   0.60000002   0.58929998   0.0574
    4.41291952]
 [ 79.8655173    0.63770002   0.61339998   0.61300004   0.043
    4.47063971]
 ...
 [136.07377261   0.77990001   0.74860001   0.71030003   0.0471
    4.5593462 ]
 [-27.12940293   0.35050002   0.3263       0.30280003   0.10339999
    4.18763399]
 [ 53.84790349   0.3908       0.35980001   0.36810002   0.0667
    3.83235025]][0m
[37m[1m[2023-07-17 03:10:08,966][257371] Max Reward on eval: 241.85190091966652[0m
[37m[1m[2023-07-17 03:10:08,966][257371] Min Reward on eval: -89.63707623276859[0m
[37m[1m[2023-07-17 03:10:08,967][257371] Mean Reward across all agents: 65.95788158235317[0m
[37m[1m[2023-07-17 03:10:08,967][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:10:08,969][257371] mean_value=-894.9884161580909, max_value=184.1570862213227[0m
[37m[1m[2023-07-17 03:10:08,971][257371] New mean coefficients: [[3.894619  1.4481436 2.6486514 1.433071  0.7649393 5.5587225]][0m
[37m[1m[2023-07-17 03:10:08,972][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:10:18,030][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 03:10:18,030][257371] FPS: 424029.47[0m
[36m[2023-07-17 03:10:18,033][257371] itr=533, itrs=2000, Progress: 26.65%[0m
[36m[2023-07-17 03:10:29,735][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 03:10:29,736][257371] FPS: 330379.42[0m
[36m[2023-07-17 03:10:34,006][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:10:34,007][257371] Reward + Measures: [[75.34946666  0.315705    0.295333    0.30430067  0.12114     4.68511009]][0m
[37m[1m[2023-07-17 03:10:34,007][257371] Max Reward on eval: 75.34946665674566[0m
[37m[1m[2023-07-17 03:10:34,007][257371] Min Reward on eval: 75.34946665674566[0m
[37m[1m[2023-07-17 03:10:34,008][257371] Mean Reward across all agents: 75.34946665674566[0m
[37m[1m[2023-07-17 03:10:34,008][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:10:38,946][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:10:38,947][257371] Reward + Measures: [[100.74847888   0.49830005   0.57890004   0.50019997   0.1568
    5.68842316]
 [179.12909381   0.62819999   0.69449997   0.61250001   0.1455
    5.29486656]
 [145.62859141   0.44320002   0.41560003   0.4066       0.0843
    4.43790007]
 ...
 [ 65.22526389   0.2879       0.2744       0.2545       0.09599999
    3.67904782]
 [ 66.64047343   0.34299999   0.33609998   0.26989999   0.0352
    4.15413427]
 [ 71.74882836   0.31290001   0.29230002   0.25350001   0.0599
    3.57038951]][0m
[37m[1m[2023-07-17 03:10:38,947][257371] Max Reward on eval: 319.06743277153[0m
[37m[1m[2023-07-17 03:10:38,947][257371] Min Reward on eval: -127.82725939620286[0m
[37m[1m[2023-07-17 03:10:38,947][257371] Mean Reward across all agents: 75.9306789648791[0m
[37m[1m[2023-07-17 03:10:38,948][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:10:38,951][257371] mean_value=-475.6603051726579, max_value=356.7392156927891[0m
[37m[1m[2023-07-17 03:10:38,953][257371] New mean coefficients: [[3.1067638 1.5712665 2.5236733 1.3392713 0.8541701 4.9594283]][0m
[37m[1m[2023-07-17 03:10:38,954][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:10:48,038][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 03:10:48,038][257371] FPS: 422820.03[0m
[36m[2023-07-17 03:10:48,040][257371] itr=534, itrs=2000, Progress: 26.70%[0m
[36m[2023-07-17 03:10:59,843][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 03:10:59,843][257371] FPS: 327492.89[0m
[36m[2023-07-17 03:11:04,165][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:11:04,166][257371] Reward + Measures: [[77.06780538  0.31999698  0.30095667  0.311167    0.12138434  4.7101326 ]][0m
[37m[1m[2023-07-17 03:11:04,166][257371] Max Reward on eval: 77.06780538482639[0m
[37m[1m[2023-07-17 03:11:04,166][257371] Min Reward on eval: 77.06780538482639[0m
[37m[1m[2023-07-17 03:11:04,166][257371] Mean Reward across all agents: 77.06780538482639[0m
[37m[1m[2023-07-17 03:11:04,167][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:11:09,208][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:11:09,208][257371] Reward + Measures: [[148.14318121   0.58630002   0.5284       0.48539996   0.11009999
    4.69269896]
 [ 36.34895727   0.22620001   0.18870001   0.21089999   0.15000001
    4.35943604]
 [ 67.08487845   0.38840002   0.3452       0.31089997   0.0778
    3.79287124]
 ...
 [207.68176211   0.61700004   0.58160001   0.54680002   0.06569999
    4.44970751]
 [177.84953253   0.61510003   0.56989998   0.53600001   0.0923
    4.66795874]
 [103.16455817   0.58039999   0.53850001   0.47510004   0.0568
    4.17535543]][0m
[37m[1m[2023-07-17 03:11:09,209][257371] Max Reward on eval: 384.47697258125993[0m
[37m[1m[2023-07-17 03:11:09,209][257371] Min Reward on eval: -37.484222017787395[0m
[37m[1m[2023-07-17 03:11:09,209][257371] Mean Reward across all agents: 143.808684288479[0m
[37m[1m[2023-07-17 03:11:09,209][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:11:09,212][257371] mean_value=-237.7204616296532, max_value=258.78088272502885[0m
[37m[1m[2023-07-17 03:11:09,215][257371] New mean coefficients: [[3.1812325 1.6061517 3.522211  1.4332267 1.5179442 4.9260263]][0m
[37m[1m[2023-07-17 03:11:09,216][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:11:18,262][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 03:11:18,263][257371] FPS: 424565.58[0m
[36m[2023-07-17 03:11:18,265][257371] itr=535, itrs=2000, Progress: 26.75%[0m
[36m[2023-07-17 03:11:30,104][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 03:11:30,104][257371] FPS: 326558.35[0m
[36m[2023-07-17 03:11:34,365][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:11:34,366][257371] Reward + Measures: [[83.21412932  0.33912468  0.31812233  0.32522264  0.12007666  4.75134516]][0m
[37m[1m[2023-07-17 03:11:34,366][257371] Max Reward on eval: 83.21412932303961[0m
[37m[1m[2023-07-17 03:11:34,366][257371] Min Reward on eval: 83.21412932303961[0m
[37m[1m[2023-07-17 03:11:34,366][257371] Mean Reward across all agents: 83.21412932303961[0m
[37m[1m[2023-07-17 03:11:34,367][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:11:39,572][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:11:39,573][257371] Reward + Measures: [[-14.05012919   0.48029995   0.40879998   0.33150002   0.19599999
    4.28791285]
 [-15.25083756   0.30949998   0.24700001   0.25840002   0.1301
    4.43704796]
 [ 92.02950006   0.33100006   0.30859998   0.27560002   0.0858
    3.67715144]
 ...
 [133.13047316   0.47869998   0.5147       0.45409998   0.1391
    4.25362301]
 [ 70.66945283   0.3809       0.3479       0.33180001   0.1068
    4.32059336]
 [124.27463797   0.5776       0.52279997   0.46700001   0.0839
    4.48514462]][0m
[37m[1m[2023-07-17 03:11:39,573][257371] Max Reward on eval: 236.33654181286693[0m
[37m[1m[2023-07-17 03:11:39,573][257371] Min Reward on eval: -174.2249555407092[0m
[37m[1m[2023-07-17 03:11:39,573][257371] Mean Reward across all agents: 38.644911339674145[0m
[37m[1m[2023-07-17 03:11:39,574][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:11:39,575][257371] mean_value=-1004.0774215257263, max_value=83.79533407416068[0m
[37m[1m[2023-07-17 03:11:39,578][257371] New mean coefficients: [[3.4692345  1.3552068  3.3992295  0.95751214 1.6531596  4.1098776 ]][0m
[37m[1m[2023-07-17 03:11:39,579][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:11:48,602][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 03:11:48,602][257371] FPS: 425645.62[0m
[36m[2023-07-17 03:11:48,605][257371] itr=536, itrs=2000, Progress: 26.80%[0m
[36m[2023-07-17 03:12:00,757][257371] train() took 12.07 seconds to complete[0m
[36m[2023-07-17 03:12:00,757][257371] FPS: 318117.30[0m
[36m[2023-07-17 03:12:05,096][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:12:05,097][257371] Reward + Measures: [[88.76799568  0.34562832  0.32526666  0.33317566  0.11900967  4.78872538]][0m
[37m[1m[2023-07-17 03:12:05,097][257371] Max Reward on eval: 88.76799568108706[0m
[37m[1m[2023-07-17 03:12:05,097][257371] Min Reward on eval: 88.76799568108706[0m
[37m[1m[2023-07-17 03:12:05,098][257371] Mean Reward across all agents: 88.76799568108706[0m
[37m[1m[2023-07-17 03:12:05,098][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:12:10,131][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:12:10,132][257371] Reward + Measures: [[-74.55449492   0.30860001   0.28209996   0.2          0.1876
    3.41837001]
 [ 91.89077537   0.40360004   0.4104       0.39660001   0.0783
    4.34909296]
 [102.97102362   0.62739998   0.75849998   0.39369997   0.40959999
    3.26184702]
 ...
 [-54.08828295   0.35190001   0.37459999   0.1983       0.28930002
    3.36809921]
 [ 12.04767757   0.29620001   0.26710001   0.22049999   0.07359999
    4.00824451]
 [-23.85375316   0.20820001   0.20079999   0.12710001   0.0962
    3.24506497]][0m
[37m[1m[2023-07-17 03:12:10,132][257371] Max Reward on eval: 233.2699513113126[0m
[37m[1m[2023-07-17 03:12:10,132][257371] Min Reward on eval: -88.07713080011308[0m
[37m[1m[2023-07-17 03:12:10,133][257371] Mean Reward across all agents: 12.320416825836348[0m
[37m[1m[2023-07-17 03:12:10,133][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:12:10,135][257371] mean_value=-2294.256456394107, max_value=277.3543405632554[0m
[37m[1m[2023-07-17 03:12:10,137][257371] New mean coefficients: [[3.3446271  0.08479357 2.4007459  0.04198462 1.7748107  5.125199  ]][0m
[37m[1m[2023-07-17 03:12:10,138][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:12:19,261][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 03:12:19,261][257371] FPS: 421008.80[0m
[36m[2023-07-17 03:12:19,263][257371] itr=537, itrs=2000, Progress: 26.85%[0m
[36m[2023-07-17 03:12:30,934][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 03:12:30,935][257371] FPS: 331291.01[0m
[36m[2023-07-17 03:12:35,189][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:12:35,190][257371] Reward + Measures: [[97.34677436  0.35556334  0.334959    0.34654501  0.122943    4.83298302]][0m
[37m[1m[2023-07-17 03:12:35,190][257371] Max Reward on eval: 97.34677436166163[0m
[37m[1m[2023-07-17 03:12:35,190][257371] Min Reward on eval: 97.34677436166163[0m
[37m[1m[2023-07-17 03:12:35,191][257371] Mean Reward across all agents: 97.34677436166163[0m
[37m[1m[2023-07-17 03:12:35,191][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:12:40,146][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:12:40,146][257371] Reward + Measures: [[143.82474958   0.45559999   0.3906       0.35880002   0.0909
    4.38359547]
 [109.67774323   0.43100005   0.3716       0.33820003   0.08109999
    4.38131905]
 [ 77.72771761   0.32130003   0.30320001   0.27900001   0.07480001
    4.02510214]
 ...
 [-64.9705601    0.1724       0.1327       0.14950001   0.0976
    4.43647051]
 [ 60.75493662   0.63179994   0.48400003   0.45140001   0.0949
    3.95249724]
 [108.07457198   0.50370002   0.4488       0.40009999   0.0601
    4.08064127]][0m
[37m[1m[2023-07-17 03:12:40,146][257371] Max Reward on eval: 293.4187145293225[0m
[37m[1m[2023-07-17 03:12:40,147][257371] Min Reward on eval: -116.93043135707849[0m
[37m[1m[2023-07-17 03:12:40,147][257371] Mean Reward across all agents: 60.80944835883216[0m
[37m[1m[2023-07-17 03:12:40,147][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:12:40,149][257371] mean_value=-1427.6050595226252, max_value=227.32584966851658[0m
[37m[1m[2023-07-17 03:12:40,152][257371] New mean coefficients: [[ 3.587353    0.02600629  2.1177263  -0.9662046   1.5507656   4.8238425 ]][0m
[37m[1m[2023-07-17 03:12:40,153][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:12:49,103][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 03:12:49,103][257371] FPS: 429131.70[0m
[36m[2023-07-17 03:12:49,105][257371] itr=538, itrs=2000, Progress: 26.90%[0m
[36m[2023-07-17 03:13:00,967][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 03:13:00,967][257371] FPS: 325875.11[0m
[36m[2023-07-17 03:13:05,215][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:13:05,216][257371] Reward + Measures: [[112.93438047   0.394941     0.37311134   0.37983468   0.11693333
    4.88011885]][0m
[37m[1m[2023-07-17 03:13:05,216][257371] Max Reward on eval: 112.93438047220738[0m
[37m[1m[2023-07-17 03:13:05,216][257371] Min Reward on eval: 112.93438047220738[0m
[37m[1m[2023-07-17 03:13:05,216][257371] Mean Reward across all agents: 112.93438047220738[0m
[37m[1m[2023-07-17 03:13:05,217][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:13:10,172][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:13:10,172][257371] Reward + Measures: [[ 27.06388767   0.42490003   0.22920001   0.28729999   0.0689
    4.21384811]
 [-11.95101044   0.27869996   0.19770001   0.20940001   0.20539999
    4.96394968]
 [ 49.40201445   0.26910001   0.15550001   0.17590001   0.17750001
    4.25005436]
 ...
 [ 13.02642816   0.15810001   0.11520001   0.09670001   0.12159999
    4.31081533]
 [ 23.61402734   0.27700001   0.1401       0.21019998   0.11620001
    4.09873819]
 [ 42.18950019   0.1869       0.13110001   0.14420001   0.1487
    4.58728886]][0m
[37m[1m[2023-07-17 03:13:10,172][257371] Max Reward on eval: 191.5500314269215[0m
[37m[1m[2023-07-17 03:13:10,173][257371] Min Reward on eval: -109.91889635636471[0m
[37m[1m[2023-07-17 03:13:10,173][257371] Mean Reward across all agents: 31.088328801887165[0m
[37m[1m[2023-07-17 03:13:10,173][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:13:10,176][257371] mean_value=-1472.258037449401, max_value=248.9131787665643[0m
[37m[1m[2023-07-17 03:13:10,178][257371] New mean coefficients: [[ 2.359211    0.11647069  2.3403125  -0.3567254   1.812865    5.9250174 ]][0m
[37m[1m[2023-07-17 03:13:10,179][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:13:19,142][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 03:13:19,142][257371] FPS: 428525.54[0m
[36m[2023-07-17 03:13:19,144][257371] itr=539, itrs=2000, Progress: 26.95%[0m
[36m[2023-07-17 03:13:30,986][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 03:13:30,986][257371] FPS: 326491.68[0m
[36m[2023-07-17 03:13:35,253][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:13:35,253][257371] Reward + Measures: [[114.70127725   0.39356565   0.372857     0.38270399   0.12232733
    4.93552065]][0m
[37m[1m[2023-07-17 03:13:35,254][257371] Max Reward on eval: 114.70127725191608[0m
[37m[1m[2023-07-17 03:13:35,254][257371] Min Reward on eval: 114.70127725191608[0m
[37m[1m[2023-07-17 03:13:35,254][257371] Mean Reward across all agents: 114.70127725191608[0m
[37m[1m[2023-07-17 03:13:35,254][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:13:40,231][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:13:40,231][257371] Reward + Measures: [[278.20885035   0.68480003   0.65780002   0.60390002   0.0627
    4.63282251]
 [ 72.03677111   0.69099998   0.64420003   0.56470001   0.14320001
    4.23110676]
 [ 77.63303031   0.3554       0.384        0.2965       0.1293
    4.23038244]
 ...
 [113.18420628   0.44930002   0.42149997   0.41220003   0.14950001
    4.58153868]
 [265.42317207   0.6965       0.67709994   0.62860006   0.08060001
    4.57523489]
 [ 45.29917585   0.39840001   0.40880004   0.31019998   0.17190002
    3.81942797]][0m
[37m[1m[2023-07-17 03:13:40,232][257371] Max Reward on eval: 373.212821983546[0m
[37m[1m[2023-07-17 03:13:40,232][257371] Min Reward on eval: -143.80000792853534[0m
[37m[1m[2023-07-17 03:13:40,232][257371] Mean Reward across all agents: 65.99177722933237[0m
[37m[1m[2023-07-17 03:13:40,232][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:13:40,236][257371] mean_value=-629.5784295044638, max_value=317.9998827733163[0m
[37m[1m[2023-07-17 03:13:40,238][257371] New mean coefficients: [[ 1.9842974  -0.09441809  2.6933196  -0.51719606  2.1073883   6.0943365 ]][0m
[37m[1m[2023-07-17 03:13:40,239][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:13:49,219][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 03:13:49,219][257371] FPS: 427723.82[0m
[36m[2023-07-17 03:13:49,221][257371] itr=540, itrs=2000, Progress: 27.00%[0m
[37m[1m[2023-07-17 03:16:50,250][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000520[0m
[36m[2023-07-17 03:17:02,402][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 03:17:02,403][257371] FPS: 330292.42[0m
[36m[2023-07-17 03:17:06,556][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:17:06,557][257371] Reward + Measures: [[115.44348634   0.38784733   0.36755028   0.37492937   0.119946
    4.99064255]][0m
[37m[1m[2023-07-17 03:17:06,557][257371] Max Reward on eval: 115.44348634449925[0m
[37m[1m[2023-07-17 03:17:06,557][257371] Min Reward on eval: 115.44348634449925[0m
[37m[1m[2023-07-17 03:17:06,557][257371] Mean Reward across all agents: 115.44348634449925[0m
[37m[1m[2023-07-17 03:17:06,558][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:17:11,531][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:17:11,531][257371] Reward + Measures: [[ 82.31586238   0.235        0.23120001   0.20039999   0.09010001
    4.12574911]
 [-22.32056865   0.2588       0.16800001   0.17480001   0.1084
    4.11003304]
 [156.64246656   0.51210004   0.51130003   0.43870002   0.0782
    4.3518424 ]
 ...
 [ 53.70955758   0.52490002   0.50810003   0.46040002   0.0819
    4.01482868]
 [-35.05207772   0.20930003   0.1679       0.1523       0.1191
    4.33097696]
 [-12.42676655   0.28690001   0.21000002   0.18539999   0.11889999
    4.03115702]][0m
[37m[1m[2023-07-17 03:17:11,531][257371] Max Reward on eval: 206.85452795252203[0m
[37m[1m[2023-07-17 03:17:11,532][257371] Min Reward on eval: -103.91954541010782[0m
[37m[1m[2023-07-17 03:17:11,532][257371] Mean Reward across all agents: 9.185930529648846[0m
[37m[1m[2023-07-17 03:17:11,532][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:17:11,534][257371] mean_value=-1550.121303599875, max_value=33.94894484629526[0m
[37m[1m[2023-07-17 03:17:11,536][257371] New mean coefficients: [[ 2.333297   -0.1451528   2.3789387  -0.13152039  0.27292335  5.6852493 ]][0m
[37m[1m[2023-07-17 03:17:11,537][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:17:20,654][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 03:17:20,655][257371] FPS: 421251.16[0m
[36m[2023-07-17 03:17:20,657][257371] itr=541, itrs=2000, Progress: 27.05%[0m
[36m[2023-07-17 03:17:32,264][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 03:17:32,265][257371] FPS: 333145.83[0m
[36m[2023-07-17 03:17:36,574][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:17:36,574][257371] Reward + Measures: [[126.96049325   0.40370533   0.38239565   0.39133501   0.11707534
    5.04270458]][0m
[37m[1m[2023-07-17 03:17:36,575][257371] Max Reward on eval: 126.96049324910268[0m
[37m[1m[2023-07-17 03:17:36,575][257371] Min Reward on eval: 126.96049324910268[0m
[37m[1m[2023-07-17 03:17:36,575][257371] Mean Reward across all agents: 126.96049324910268[0m
[37m[1m[2023-07-17 03:17:36,575][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:17:41,372][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:17:41,373][257371] Reward + Measures: [[ 53.68138419   0.16420001   0.16080001   0.16800001   0.1112
    4.38854933]
 [-17.84610879   0.30179998   0.45359999   0.20780002   0.41929999
    3.83758664]
 [ 59.58633578   0.27430001   0.32699999   0.123        0.1822
    4.51221943]
 ...
 [131.72097517   0.45570001   0.4357       0.44119999   0.0868
    5.46960926]
 [115.64236754   0.27540001   0.39180002   0.1398       0.20429997
    4.70309019]
 [ 91.16461292   0.4298       0.39790002   0.38890001   0.0599
    4.79682398]][0m
[37m[1m[2023-07-17 03:17:41,373][257371] Max Reward on eval: 343.4132136723027[0m
[37m[1m[2023-07-17 03:17:41,373][257371] Min Reward on eval: -108.62826107209548[0m
[37m[1m[2023-07-17 03:17:41,373][257371] Mean Reward across all agents: 68.45245244662866[0m
[37m[1m[2023-07-17 03:17:41,374][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:17:41,377][257371] mean_value=-818.38442988757, max_value=444.5636322338611[0m
[37m[1m[2023-07-17 03:17:41,379][257371] New mean coefficients: [[ 1.9213943  -0.96334255  2.509624    0.13947004  0.28534034  5.4588523 ]][0m
[37m[1m[2023-07-17 03:17:41,380][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:17:50,018][257371] train() took 8.64 seconds to complete[0m
[36m[2023-07-17 03:17:50,018][257371] FPS: 444641.03[0m
[36m[2023-07-17 03:17:50,021][257371] itr=542, itrs=2000, Progress: 27.10%[0m
[36m[2023-07-17 03:18:02,068][257371] train() took 11.97 seconds to complete[0m
[36m[2023-07-17 03:18:02,068][257371] FPS: 320883.62[0m
[36m[2023-07-17 03:18:06,331][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:18:06,332][257371] Reward + Measures: [[128.74416677   0.40650532   0.38522997   0.3940337    0.11826867
    5.08946753]][0m
[37m[1m[2023-07-17 03:18:06,332][257371] Max Reward on eval: 128.74416676770448[0m
[37m[1m[2023-07-17 03:18:06,332][257371] Min Reward on eval: 128.74416676770448[0m
[37m[1m[2023-07-17 03:18:06,332][257371] Mean Reward across all agents: 128.74416676770448[0m
[37m[1m[2023-07-17 03:18:06,332][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:18:11,306][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:18:11,307][257371] Reward + Measures: [[ 19.32271518   0.20750001   0.17940001   0.17549999   0.08690001
    4.25930071]
 [ 28.5976924    0.1487       0.14740001   0.12260001   0.0656
    5.10253286]
 [ 29.67231181   0.51999998   0.47830001   0.46620002   0.0603
    3.92961311]
 ...
 [127.39443469   0.17760001   0.15350001   0.1371       0.06460001
    4.85545588]
 [ 59.45617247   0.49429998   0.4021       0.35080001   0.0531
    4.3252821 ]
 [106.89202191   0.50139999   0.43619999   0.36360002   0.0512
    4.37447309]][0m
[37m[1m[2023-07-17 03:18:11,307][257371] Max Reward on eval: 311.38474584273064[0m
[37m[1m[2023-07-17 03:18:11,308][257371] Min Reward on eval: -73.44218797200593[0m
[37m[1m[2023-07-17 03:18:11,308][257371] Mean Reward across all agents: 74.18123677537696[0m
[37m[1m[2023-07-17 03:18:11,308][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:18:11,310][257371] mean_value=-519.405071246581, max_value=94.0252316802351[0m
[37m[1m[2023-07-17 03:18:11,312][257371] New mean coefficients: [[ 1.1012017  -0.8812976   2.229304    0.36638057  1.084594    5.861608  ]][0m
[37m[1m[2023-07-17 03:18:11,313][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:18:20,310][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 03:18:20,310][257371] FPS: 426901.83[0m
[36m[2023-07-17 03:18:20,313][257371] itr=543, itrs=2000, Progress: 27.15%[0m
[36m[2023-07-17 03:18:32,108][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 03:18:32,108][257371] FPS: 327794.31[0m
[36m[2023-07-17 03:18:36,431][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:18:36,436][257371] Reward + Measures: [[136.04447702   0.4230113    0.40180933   0.40994799   0.11646599
    5.07063103]][0m
[37m[1m[2023-07-17 03:18:36,437][257371] Max Reward on eval: 136.04447701626185[0m
[37m[1m[2023-07-17 03:18:36,437][257371] Min Reward on eval: 136.04447701626185[0m
[37m[1m[2023-07-17 03:18:36,437][257371] Mean Reward across all agents: 136.04447701626185[0m
[37m[1m[2023-07-17 03:18:36,438][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:18:41,447][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:18:41,453][257371] Reward + Measures: [[146.9864709    0.57750005   0.53380007   0.50380003   0.06710001
    4.37021494]
 [ 81.45469645   0.37040001   0.34909996   0.303        0.0663
    4.15328169]
 [ 93.57750165   0.47980005   0.43610001   0.4278       0.07390001
    4.06889296]
 ...
 [ 62.32412348   0.22589998   0.20910001   0.22640002   0.1053
    4.59819174]
 [125.79114657   0.47709998   0.48539996   0.45370004   0.0711
    4.50471163]
 [163.41508383   0.54820001   0.50529999   0.49650002   0.08000001
    4.79760695]][0m
[37m[1m[2023-07-17 03:18:41,453][257371] Max Reward on eval: 287.1545247086324[0m
[37m[1m[2023-07-17 03:18:41,453][257371] Min Reward on eval: -6.056241456512362[0m
[37m[1m[2023-07-17 03:18:41,454][257371] Mean Reward across all agents: 127.3950911616446[0m
[37m[1m[2023-07-17 03:18:41,454][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:18:41,456][257371] mean_value=-212.72166492488861, max_value=61.66072746960191[0m
[37m[1m[2023-07-17 03:18:41,458][257371] New mean coefficients: [[ 0.99051356 -0.37659943  2.1885998   0.29271245  0.7266129   5.9767947 ]][0m
[37m[1m[2023-07-17 03:18:41,459][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:18:50,447][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 03:18:50,447][257371] FPS: 427329.51[0m
[36m[2023-07-17 03:18:50,449][257371] itr=544, itrs=2000, Progress: 27.20%[0m
[36m[2023-07-17 03:19:02,075][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 03:19:02,076][257371] FPS: 332452.02[0m
[36m[2023-07-17 03:19:06,325][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:19:06,326][257371] Reward + Measures: [[146.55590532   0.43332165   0.41287267   0.42391968   0.11834066
    5.14388371]][0m
[37m[1m[2023-07-17 03:19:06,326][257371] Max Reward on eval: 146.55590532052412[0m
[37m[1m[2023-07-17 03:19:06,326][257371] Min Reward on eval: 146.55590532052412[0m
[37m[1m[2023-07-17 03:19:06,326][257371] Mean Reward across all agents: 146.55590532052412[0m
[37m[1m[2023-07-17 03:19:06,326][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:19:11,332][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:19:11,332][257371] Reward + Measures: [[-42.36334893   0.36409998   0.28210002   0.2349       0.1451
    3.7378099 ]
 [ 62.7116444    0.6494       0.60320002   0.54819995   0.0524
    4.63183784]
 [ 14.39363646   0.33810002   0.2493       0.28099999   0.17690001
    4.31213713]
 ...
 [ 63.34771383   0.67750007   0.60510004   0.54179996   0.1084
    4.3016715 ]
 [ 71.48674451   0.49420005   0.3811       0.34969997   0.1129
    4.20896149]
 [ 15.71009602   0.4664       0.29749998   0.3427       0.20050001
    3.97390985]][0m
[37m[1m[2023-07-17 03:19:11,332][257371] Max Reward on eval: 251.40727235209198[0m
[37m[1m[2023-07-17 03:19:11,333][257371] Min Reward on eval: -161.18485221723094[0m
[37m[1m[2023-07-17 03:19:11,333][257371] Mean Reward across all agents: 20.292574594880357[0m
[37m[1m[2023-07-17 03:19:11,333][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:19:11,336][257371] mean_value=-977.930190562513, max_value=342.8181928100073[0m
[37m[1m[2023-07-17 03:19:11,338][257371] New mean coefficients: [[ 1.341275   -0.13895941  2.7757509  -0.17469573  0.37643766  6.093508  ]][0m
[37m[1m[2023-07-17 03:19:11,339][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:19:20,456][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 03:19:20,457][257371] FPS: 421249.62[0m
[36m[2023-07-17 03:19:20,459][257371] itr=545, itrs=2000, Progress: 27.25%[0m
[36m[2023-07-17 03:19:32,065][257371] train() took 11.52 seconds to complete[0m
[36m[2023-07-17 03:19:32,066][257371] FPS: 333224.45[0m
[36m[2023-07-17 03:19:36,360][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:19:36,360][257371] Reward + Measures: [[158.72225913   0.44407266   0.42354834   0.43129134   0.11531167
    5.23470879]][0m
[37m[1m[2023-07-17 03:19:36,361][257371] Max Reward on eval: 158.72225913483504[0m
[37m[1m[2023-07-17 03:19:36,361][257371] Min Reward on eval: 158.72225913483504[0m
[37m[1m[2023-07-17 03:19:36,361][257371] Mean Reward across all agents: 158.72225913483504[0m
[37m[1m[2023-07-17 03:19:36,361][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:19:41,549][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:19:41,554][257371] Reward + Measures: [[-33.87225307   0.29389998   0.55040002   0.3089       0.4804
    2.78524208]
 [ 38.31548763   0.49939999   0.32659999   0.31860003   0.2023
    3.11536765]
 [-79.76885116   0.36380002   0.34910002   0.28280002   0.38970003
    3.13290095]
 ...
 [-49.96362713   0.31999999   0.4312       0.27860004   0.42200002
    2.9193058 ]
 [-54.66062621   0.35370001   0.42379999   0.31030002   0.398
    2.93949938]
 [121.13886969   0.54479998   0.48610002   0.47229996   0.0785
    4.22005701]][0m
[37m[1m[2023-07-17 03:19:41,554][257371] Max Reward on eval: 207.4295873682946[0m
[37m[1m[2023-07-17 03:19:41,555][257371] Min Reward on eval: -377.8414804806933[0m
[37m[1m[2023-07-17 03:19:41,555][257371] Mean Reward across all agents: -5.26179636375639[0m
[37m[1m[2023-07-17 03:19:41,555][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:19:41,558][257371] mean_value=-846.1886075164609, max_value=282.4308558493671[0m
[37m[1m[2023-07-17 03:19:41,561][257371] New mean coefficients: [[ 1.7279212 -1.0478594  2.3378947  0.5052733  0.287499   6.716247 ]][0m
[37m[1m[2023-07-17 03:19:41,562][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:19:50,527][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 03:19:50,528][257371] FPS: 428405.78[0m
[36m[2023-07-17 03:19:50,530][257371] itr=546, itrs=2000, Progress: 27.30%[0m
[36m[2023-07-17 03:20:02,239][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 03:20:02,239][257371] FPS: 330104.55[0m
[36m[2023-07-17 03:20:06,468][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:20:06,468][257371] Reward + Measures: [[158.97662848   0.43878931   0.41676635   0.42930996   0.11904766
    5.2876358 ]][0m
[37m[1m[2023-07-17 03:20:06,468][257371] Max Reward on eval: 158.97662848426535[0m
[37m[1m[2023-07-17 03:20:06,469][257371] Min Reward on eval: 158.97662848426535[0m
[37m[1m[2023-07-17 03:20:06,469][257371] Mean Reward across all agents: 158.97662848426535[0m
[37m[1m[2023-07-17 03:20:06,469][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:20:11,479][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:20:11,480][257371] Reward + Measures: [[  42.2698083     0.37170002    0.33020002    0.28510004    0.0744
     4.10890388]
 [-200.66444884    0.39140001    0.24790001    0.28640005    0.15100001
     3.61448479]
 [ -79.40102611    0.50260001    0.45559999    0.4436        0.0676
     4.52408838]
 ...
 [ 186.89837655    0.44220001    0.40400001    0.40119997    0.12249999
     4.86077785]
 [   7.31837841    0.28640002    0.22540002    0.20709999    0.0879
     4.65167189]
 [  59.45791387    0.51450002    0.49359998    0.45009995    0.0569
     4.60777044]][0m
[37m[1m[2023-07-17 03:20:11,480][257371] Max Reward on eval: 252.67548963744193[0m
[37m[1m[2023-07-17 03:20:11,480][257371] Min Reward on eval: -249.28163631509523[0m
[37m[1m[2023-07-17 03:20:11,480][257371] Mean Reward across all agents: 17.7129093295929[0m
[37m[1m[2023-07-17 03:20:11,481][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:20:11,483][257371] mean_value=-928.2665815155864, max_value=41.338569830615526[0m
[37m[1m[2023-07-17 03:20:11,485][257371] New mean coefficients: [[ 1.5645411  -0.5475403   1.9714566  -0.19669527  0.7919499   6.240723  ]][0m
[37m[1m[2023-07-17 03:20:11,486][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:20:20,494][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 03:20:20,494][257371] FPS: 426363.98[0m
[36m[2023-07-17 03:20:20,497][257371] itr=547, itrs=2000, Progress: 27.35%[0m
[36m[2023-07-17 03:20:32,374][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 03:20:32,374][257371] FPS: 325421.09[0m
[36m[2023-07-17 03:20:36,624][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:20:36,625][257371] Reward + Measures: [[163.61357416   0.44489363   0.42574197   0.44344333   0.12203833
    5.33173037]][0m
[37m[1m[2023-07-17 03:20:36,625][257371] Max Reward on eval: 163.61357415519208[0m
[37m[1m[2023-07-17 03:20:36,625][257371] Min Reward on eval: 163.61357415519208[0m
[37m[1m[2023-07-17 03:20:36,625][257371] Mean Reward across all agents: 163.61357415519208[0m
[37m[1m[2023-07-17 03:20:36,625][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:20:41,685][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:20:41,691][257371] Reward + Measures: [[ 99.21387161   0.38159999   0.40170002   0.33940002   0.05610001
    4.34616184]
 [130.07240274   0.39669999   0.35550001   0.38159999   0.14000002
    4.76213884]
 [115.9658463    0.47849998   0.44330001   0.40850002   0.1067
    4.61580372]
 ...
 [322.47779109   0.70340008   0.68839997   0.66639996   0.0658
    5.38539743]
 [ 80.36501765   0.2667       0.2561       0.2244       0.04190001
    3.7703259 ]
 [ 78.4559159    0.30289999   0.29089999   0.27790001   0.06390001
    4.27793264]][0m
[37m[1m[2023-07-17 03:20:41,691][257371] Max Reward on eval: 322.4777910931036[0m
[37m[1m[2023-07-17 03:20:41,692][257371] Min Reward on eval: -26.156474555528256[0m
[37m[1m[2023-07-17 03:20:41,692][257371] Mean Reward across all agents: 135.9694732978547[0m
[37m[1m[2023-07-17 03:20:41,692][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:20:41,694][257371] mean_value=-480.2372723800561, max_value=111.62065925896974[0m
[37m[1m[2023-07-17 03:20:41,697][257371] New mean coefficients: [[ 1.3579106  -0.69387233  1.2626386  -0.12242188  0.2982422   6.500281  ]][0m
[37m[1m[2023-07-17 03:20:41,698][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:20:50,719][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 03:20:50,719][257371] FPS: 425736.70[0m
[36m[2023-07-17 03:20:50,721][257371] itr=548, itrs=2000, Progress: 27.40%[0m
[36m[2023-07-17 03:21:02,514][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 03:21:02,514][257371] FPS: 327774.85[0m
[36m[2023-07-17 03:21:06,755][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:21:06,756][257371] Reward + Measures: [[171.6293798    0.455066     0.43538997   0.45274633   0.11716367
    5.39425135]][0m
[37m[1m[2023-07-17 03:21:06,756][257371] Max Reward on eval: 171.62937979895165[0m
[37m[1m[2023-07-17 03:21:06,756][257371] Min Reward on eval: 171.62937979895165[0m
[37m[1m[2023-07-17 03:21:06,756][257371] Mean Reward across all agents: 171.62937979895165[0m
[37m[1m[2023-07-17 03:21:06,757][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:21:11,707][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:21:11,708][257371] Reward + Measures: [[-102.22820524    0.49550006    0.41809997    0.2507        0.3423
     3.77247691]
 [ -84.41797488    0.59850001    0.65760005    0.3028        0.49020001
     3.83466911]
 [  -6.50384209    0.31240001    0.23019998    0.2586        0.20050001
     3.59156036]
 ...
 [   0.15963448    0.25          0.23900001    0.16010001    0.18270001
     4.93753767]
 [  14.10301148    0.45390001    0.49759999    0.38          0.24949999
     3.70057654]
 [ -19.78629225    0.35770002    0.3188        0.27169999    0.16229999
     3.54630923]][0m
[37m[1m[2023-07-17 03:21:11,708][257371] Max Reward on eval: 276.68306042905897[0m
[37m[1m[2023-07-17 03:21:11,708][257371] Min Reward on eval: -288.74964615777134[0m
[37m[1m[2023-07-17 03:21:11,708][257371] Mean Reward across all agents: 12.515365216505677[0m
[37m[1m[2023-07-17 03:21:11,709][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:21:11,710][257371] mean_value=-1075.3504655263216, max_value=263.57264182469555[0m
[37m[1m[2023-07-17 03:21:11,713][257371] New mean coefficients: [[ 1.3163091  -0.91746724  1.3522509  -0.38241154  0.33592582  6.337637  ]][0m
[37m[1m[2023-07-17 03:21:11,714][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:21:20,752][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 03:21:20,752][257371] FPS: 424924.00[0m
[36m[2023-07-17 03:21:20,755][257371] itr=549, itrs=2000, Progress: 27.45%[0m
[36m[2023-07-17 03:21:32,580][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 03:21:32,580][257371] FPS: 326875.26[0m
[36m[2023-07-17 03:21:36,916][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:21:36,917][257371] Reward + Measures: [[195.14376083   0.47813398   0.45981601   0.47719201   0.112781
    5.4470892 ]][0m
[37m[1m[2023-07-17 03:21:36,917][257371] Max Reward on eval: 195.14376083212977[0m
[37m[1m[2023-07-17 03:21:36,917][257371] Min Reward on eval: 195.14376083212977[0m
[37m[1m[2023-07-17 03:21:36,917][257371] Mean Reward across all agents: 195.14376083212977[0m
[37m[1m[2023-07-17 03:21:36,918][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:21:41,952][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:21:41,952][257371] Reward + Measures: [[-52.93494144   0.56520003   0.53050005   0.49909997   0.0931
    4.86613655]
 [369.31214339   0.89410001   0.90210003   0.84729999   0.0635
    5.91299725]
 [  3.93698409   0.32810003   0.32980001   0.32509997   0.0555
    5.49079275]
 ...
 [305.02989531   0.57779998   0.5636       0.51370001   0.0429
    5.56294489]
 [295.08445047   0.76350003   0.77759999   0.71340001   0.0959
    5.10718679]
 [ 32.8383426    0.38440001   0.38870001   0.3132       0.41610003
    3.38885617]][0m
[37m[1m[2023-07-17 03:21:41,952][257371] Max Reward on eval: 445.56622309577654[0m
[37m[1m[2023-07-17 03:21:41,953][257371] Min Reward on eval: -207.84226323959882[0m
[37m[1m[2023-07-17 03:21:41,953][257371] Mean Reward across all agents: 69.12824373191789[0m
[37m[1m[2023-07-17 03:21:41,953][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:21:41,956][257371] mean_value=-307.93399819621885, max_value=287.4467008068045[0m
[37m[1m[2023-07-17 03:21:41,959][257371] New mean coefficients: [[ 0.87178725 -1.451087    1.7682803  -0.47562885  0.5092511   7.557007  ]][0m
[37m[1m[2023-07-17 03:21:41,960][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:21:51,028][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 03:21:51,028][257371] FPS: 423531.59[0m
[36m[2023-07-17 03:21:51,031][257371] itr=550, itrs=2000, Progress: 27.50%[0m
[37m[1m[2023-07-17 03:24:48,703][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000530[0m
[36m[2023-07-17 03:25:00,792][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 03:25:00,792][257371] FPS: 332039.36[0m
[36m[2023-07-17 03:25:04,898][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:25:04,898][257371] Reward + Measures: [[192.86925127   0.45916569   0.44163901   0.46464998   0.11750966
    5.46277189]][0m
[37m[1m[2023-07-17 03:25:04,898][257371] Max Reward on eval: 192.86925126540064[0m
[37m[1m[2023-07-17 03:25:04,899][257371] Min Reward on eval: 192.86925126540064[0m
[37m[1m[2023-07-17 03:25:04,899][257371] Mean Reward across all agents: 192.86925126540064[0m
[37m[1m[2023-07-17 03:25:04,899][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:25:09,770][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:25:09,771][257371] Reward + Measures: [[  2.73931485   0.2739       0.25300002   0.22069998   0.1876
    3.32633901]
 [141.89463298   0.72220004   0.68149996   0.63709998   0.0554
    4.49446726]
 [135.74395824   0.59919995   0.59720004   0.50749999   0.1656
    4.23603439]
 ...
 [-60.41988835   0.50369996   0.43190002   0.36180001   0.198
    4.211133  ]
 [-90.64265334   0.26250002   0.19829999   0.25140002   0.15010001
    3.69348788]
 [-63.19311046   0.48720002   0.41280004   0.33610001   0.1602
    4.45966101]][0m
[37m[1m[2023-07-17 03:25:09,771][257371] Max Reward on eval: 250.8611743323505[0m
[37m[1m[2023-07-17 03:25:09,771][257371] Min Reward on eval: -133.1230545022525[0m
[37m[1m[2023-07-17 03:25:09,772][257371] Mean Reward across all agents: 18.837231617798736[0m
[37m[1m[2023-07-17 03:25:09,772][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:25:09,773][257371] mean_value=-996.0606482426047, max_value=220.29078599135505[0m
[37m[1m[2023-07-17 03:25:09,776][257371] New mean coefficients: [[ 0.8854521  -1.5236502   1.7213036  -0.43366894  1.225993    6.4485693 ]][0m
[37m[1m[2023-07-17 03:25:09,777][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:25:18,699][257371] train() took 8.92 seconds to complete[0m
[36m[2023-07-17 03:25:18,699][257371] FPS: 430463.49[0m
[36m[2023-07-17 03:25:18,701][257371] itr=551, itrs=2000, Progress: 27.55%[0m
[36m[2023-07-17 03:25:30,541][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 03:25:30,541][257371] FPS: 326447.20[0m
[36m[2023-07-17 03:25:34,860][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:25:34,860][257371] Reward + Measures: [[203.82906479   0.47569466   0.45912069   0.47702503   0.107779
    5.50376415]][0m
[37m[1m[2023-07-17 03:25:34,860][257371] Max Reward on eval: 203.82906479050828[0m
[37m[1m[2023-07-17 03:25:34,861][257371] Min Reward on eval: 203.82906479050828[0m
[37m[1m[2023-07-17 03:25:34,861][257371] Mean Reward across all agents: 203.82906479050828[0m
[37m[1m[2023-07-17 03:25:34,861][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:25:39,866][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:25:39,867][257371] Reward + Measures: [[ 158.48541637    0.65679997    0.63849998    0.55290002    0.0755
     4.20660639]
 [  93.62145984    0.7683        0.71600002    0.70950001    0.0566
     5.24459314]
 [-168.67149235    0.5851        0.5467        0.4903        0.13259999
     4.42823172]
 ...
 [ -99.60497949    0.46629998    0.36360002    0.38320002    0.12280001
     4.19383383]
 [  20.1481975     0.52960008    0.53210002    0.41980001    0.17749999
     3.76156735]
 [-171.86079101    0.58140004    0.498         0.48210001    0.0819
     5.0812583 ]][0m
[37m[1m[2023-07-17 03:25:39,867][257371] Max Reward on eval: 500.0363168242155[0m
[37m[1m[2023-07-17 03:25:39,867][257371] Min Reward on eval: -293.9899391738698[0m
[37m[1m[2023-07-17 03:25:39,868][257371] Mean Reward across all agents: 27.358238294553974[0m
[37m[1m[2023-07-17 03:25:39,868][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:25:39,870][257371] mean_value=-482.40400607752747, max_value=303.2132957537286[0m
[37m[1m[2023-07-17 03:25:39,872][257371] New mean coefficients: [[ 0.65105855 -1.2728105   2.103545   -0.832216    1.6154637   6.262303  ]][0m
[37m[1m[2023-07-17 03:25:39,873][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:25:48,905][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 03:25:48,905][257371] FPS: 425245.75[0m
[36m[2023-07-17 03:25:48,908][257371] itr=552, itrs=2000, Progress: 27.60%[0m
[36m[2023-07-17 03:26:00,701][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 03:26:00,702][257371] FPS: 327736.90[0m
[36m[2023-07-17 03:26:05,013][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:26:05,014][257371] Reward + Measures: [[209.34098412   0.48617497   0.46914464   0.49082729   0.11081733
    5.54689455]][0m
[37m[1m[2023-07-17 03:26:05,014][257371] Max Reward on eval: 209.34098411793983[0m
[37m[1m[2023-07-17 03:26:05,014][257371] Min Reward on eval: 209.34098411793983[0m
[37m[1m[2023-07-17 03:26:05,014][257371] Mean Reward across all agents: 209.34098411793983[0m
[37m[1m[2023-07-17 03:26:05,015][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:26:10,056][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:26:10,057][257371] Reward + Measures: [[-18.67420111   0.58750004   0.61379999   0.414        0.23210001
    4.97477436]
 [119.32271864   0.61669999   0.79540008   0.1175       0.70319998
    5.53090286]
 [ -9.47472605   0.60330003   0.60910004   0.41120002   0.31800002
    4.12954998]
 ...
 [-30.4994264    0.20439999   0.1987       0.167        0.0953
    4.62427092]
 [  0.30048869   0.67739999   0.64779997   0.62770003   0.0435
    5.03150177]
 [-56.86491688   0.71610004   0.72649997   0.4535       0.29570001
    5.44340515]][0m
[37m[1m[2023-07-17 03:26:10,057][257371] Max Reward on eval: 474.16586729316043[0m
[37m[1m[2023-07-17 03:26:10,057][257371] Min Reward on eval: -245.57387664699928[0m
[37m[1m[2023-07-17 03:26:10,057][257371] Mean Reward across all agents: 56.48890756456049[0m
[37m[1m[2023-07-17 03:26:10,057][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:26:10,063][257371] mean_value=-408.74223329297104, max_value=512.1177069443463[0m
[37m[1m[2023-07-17 03:26:10,066][257371] New mean coefficients: [[ 0.55779785 -1.4966722   3.1231632  -1.1045108   1.3368043   6.4560223 ]][0m
[37m[1m[2023-07-17 03:26:10,067][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:26:19,134][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 03:26:19,135][257371] FPS: 423588.38[0m
[36m[2023-07-17 03:26:19,137][257371] itr=553, itrs=2000, Progress: 27.65%[0m
[36m[2023-07-17 03:26:31,003][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 03:26:31,004][257371] FPS: 325791.26[0m
[36m[2023-07-17 03:26:35,304][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:26:35,304][257371] Reward + Measures: [[225.63659689   0.51786464   0.50069499   0.52400929   0.10658
    5.65297413]][0m
[37m[1m[2023-07-17 03:26:35,305][257371] Max Reward on eval: 225.6365968916182[0m
[37m[1m[2023-07-17 03:26:35,305][257371] Min Reward on eval: 225.6365968916182[0m
[37m[1m[2023-07-17 03:26:35,305][257371] Mean Reward across all agents: 225.6365968916182[0m
[37m[1m[2023-07-17 03:26:35,306][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:26:40,278][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:26:40,279][257371] Reward + Measures: [[ 85.80133268   0.54409999   0.44780001   0.48780003   0.46889997
    2.94909883]
 [ 50.6795921    0.2933       0.32170001   0.1911       0.19810002
    3.8656075 ]
 [ 32.29720995   0.32300001   0.31030002   0.26229998   0.23480001
    3.34906769]
 ...
 [ 16.76126903   0.19849999   0.20130001   0.221        0.16919999
    3.4863174 ]
 [ 18.40453713   0.42090002   0.3448       0.37040001   0.38719997
    3.08533454]
 [-79.10012529   0.38919997   0.30840001   0.29280004   0.0905
    3.59681106]][0m
[37m[1m[2023-07-17 03:26:40,279][257371] Max Reward on eval: 186.4114846722223[0m
[37m[1m[2023-07-17 03:26:40,279][257371] Min Reward on eval: -229.54447868396528[0m
[37m[1m[2023-07-17 03:26:40,279][257371] Mean Reward across all agents: 18.63386264712853[0m
[37m[1m[2023-07-17 03:26:40,280][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:26:40,281][257371] mean_value=-1560.0322728168644, max_value=323.42423255941617[0m
[37m[1m[2023-07-17 03:26:40,284][257371] New mean coefficients: [[ 1.0325915 -1.3682332  3.2806692 -1.0431132  1.1035322  6.7048416]][0m
[37m[1m[2023-07-17 03:26:40,285][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:26:49,304][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 03:26:49,305][257371] FPS: 425799.23[0m
[36m[2023-07-17 03:26:49,307][257371] itr=554, itrs=2000, Progress: 27.70%[0m
[36m[2023-07-17 03:27:00,995][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 03:27:00,996][257371] FPS: 330739.96[0m
[36m[2023-07-17 03:27:05,307][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:27:05,308][257371] Reward + Measures: [[219.52723173   0.51034534   0.49515066   0.52416664   0.113131
    5.71566153]][0m
[37m[1m[2023-07-17 03:27:05,308][257371] Max Reward on eval: 219.52723172776177[0m
[37m[1m[2023-07-17 03:27:05,308][257371] Min Reward on eval: 219.52723172776177[0m
[37m[1m[2023-07-17 03:27:05,309][257371] Mean Reward across all agents: 219.52723172776177[0m
[37m[1m[2023-07-17 03:27:05,309][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:27:10,500][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:27:10,501][257371] Reward + Measures: [[315.6121578    0.74560004   0.68020004   0.63420004   0.033
    5.6025238 ]
 [348.92998614   0.6821       0.66330004   0.64289999   0.0753
    5.44994974]
 [326.54029085   0.82889998   0.75319999   0.70209998   0.033
    5.81058979]
 ...
 [168.91319318   0.44150001   0.38569999   0.42050001   0.0382
    5.79545832]
 [252.1790595    0.53520006   0.52539998   0.48179999   0.0356
    5.28504944]
 [414.40506315   0.86580002   0.8179       0.7669       0.0197
    6.14459753]][0m
[37m[1m[2023-07-17 03:27:10,501][257371] Max Reward on eval: 558.5947817336768[0m
[37m[1m[2023-07-17 03:27:10,501][257371] Min Reward on eval: -98.79040757324546[0m
[37m[1m[2023-07-17 03:27:10,501][257371] Mean Reward across all agents: 272.44073021350863[0m
[37m[1m[2023-07-17 03:27:10,502][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:27:10,507][257371] mean_value=-35.192370376720184, max_value=453.75758193271474[0m
[37m[1m[2023-07-17 03:27:10,510][257371] New mean coefficients: [[ 1.777072  -1.2676978  3.4305341 -0.987176   1.2823615  6.72026  ]][0m
[37m[1m[2023-07-17 03:27:10,511][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:27:19,536][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 03:27:19,536][257371] FPS: 425556.65[0m
[36m[2023-07-17 03:27:19,538][257371] itr=555, itrs=2000, Progress: 27.75%[0m
[36m[2023-07-17 03:27:31,159][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 03:27:31,159][257371] FPS: 332762.86[0m
[36m[2023-07-17 03:27:35,419][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:27:35,419][257371] Reward + Measures: [[221.34547284   0.51039267   0.49574602   0.51947665   0.10781334
    5.692379  ]][0m
[37m[1m[2023-07-17 03:27:35,420][257371] Max Reward on eval: 221.34547283577814[0m
[37m[1m[2023-07-17 03:27:35,420][257371] Min Reward on eval: 221.34547283577814[0m
[37m[1m[2023-07-17 03:27:35,420][257371] Mean Reward across all agents: 221.34547283577814[0m
[37m[1m[2023-07-17 03:27:35,420][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:27:40,460][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:27:40,461][257371] Reward + Measures: [[-56.00907269   0.18679999   0.2155       0.22520001   0.19330001
    4.14122152]
 [-12.82376849   0.25970003   0.26479998   0.222        0.2825
    3.3247025 ]
 [-13.76818093   0.3432       0.31979999   0.26340002   0.1427
    4.06012487]
 ...
 [-51.54011771   0.24379997   0.24300002   0.17269999   0.21669999
    3.67346048]
 [ 28.94883508   0.5043       0.44949999   0.41319999   0.1111
    4.18454027]
 [-13.13484084   0.3432       0.3012       0.28860003   0.14040001
    3.61418581]][0m
[37m[1m[2023-07-17 03:27:40,461][257371] Max Reward on eval: 439.63394644353536[0m
[37m[1m[2023-07-17 03:27:40,461][257371] Min Reward on eval: -133.56518224813044[0m
[37m[1m[2023-07-17 03:27:40,462][257371] Mean Reward across all agents: 11.506469527017527[0m
[37m[1m[2023-07-17 03:27:40,462][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:27:40,464][257371] mean_value=-2617.8328598334438, max_value=41.57830740798778[0m
[37m[1m[2023-07-17 03:27:40,466][257371] New mean coefficients: [[ 0.9409505  -0.4660694   3.056068   -0.10424292  0.82659113  7.4802403 ]][0m
[37m[1m[2023-07-17 03:27:40,467][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:27:49,592][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 03:27:49,593][257371] FPS: 420866.95[0m
[36m[2023-07-17 03:27:49,595][257371] itr=556, itrs=2000, Progress: 27.80%[0m
[36m[2023-07-17 03:28:01,359][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 03:28:01,359][257371] FPS: 328628.30[0m
[36m[2023-07-17 03:28:05,686][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:28:05,686][257371] Reward + Measures: [[199.81553133   0.5023753    0.48719501   0.52074534   0.11262466
    5.78042841]][0m
[37m[1m[2023-07-17 03:28:05,687][257371] Max Reward on eval: 199.81553132565057[0m
[37m[1m[2023-07-17 03:28:05,687][257371] Min Reward on eval: 199.81553132565057[0m
[37m[1m[2023-07-17 03:28:05,687][257371] Mean Reward across all agents: 199.81553132565057[0m
[37m[1m[2023-07-17 03:28:05,687][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:28:10,673][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:28:10,674][257371] Reward + Measures: [[-65.023906     0.19460002   0.212        0.2177       0.22060001
    5.02828932]
 [-34.7756716    0.48300001   0.38400003   0.37559998   0.27760002
    4.45488691]
 [ -6.12593554   0.57820004   0.49940005   0.51350003   0.0952
    4.35771561]
 ...
 [-57.48033366   0.64239997   0.62459999   0.56620002   0.57529998
    4.68791485]
 [-35.02469419   0.32030001   0.2138       0.27760002   0.1188
    4.64538765]
 [-34.76118527   0.23799999   0.2543       0.1919       0.20280002
    3.72808337]][0m
[37m[1m[2023-07-17 03:28:10,674][257371] Max Reward on eval: 466.38486766470595[0m
[37m[1m[2023-07-17 03:28:10,674][257371] Min Reward on eval: -307.25821230020375[0m
[37m[1m[2023-07-17 03:28:10,675][257371] Mean Reward across all agents: -10.886984178788959[0m
[37m[1m[2023-07-17 03:28:10,675][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:28:10,677][257371] mean_value=-1246.4520262523909, max_value=474.1318796607072[0m
[37m[1m[2023-07-17 03:28:10,679][257371] New mean coefficients: [[ 0.80656016 -1.3904328   2.1589675  -0.12589341 -0.12175578  7.239521  ]][0m
[37m[1m[2023-07-17 03:28:10,680][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:28:19,639][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 03:28:19,639][257371] FPS: 428714.16[0m
[36m[2023-07-17 03:28:19,642][257371] itr=557, itrs=2000, Progress: 27.85%[0m
[36m[2023-07-17 03:28:31,534][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 03:28:31,534][257371] FPS: 325017.01[0m
[36m[2023-07-17 03:28:35,783][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:28:35,788][257371] Reward + Measures: [[194.16527599   0.50287533   0.4893837    0.519014     0.11421199
    5.78838587]][0m
[37m[1m[2023-07-17 03:28:35,789][257371] Max Reward on eval: 194.16527599394752[0m
[37m[1m[2023-07-17 03:28:35,790][257371] Min Reward on eval: 194.16527599394752[0m
[37m[1m[2023-07-17 03:28:35,791][257371] Mean Reward across all agents: 194.16527599394752[0m
[37m[1m[2023-07-17 03:28:35,791][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:28:40,752][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:28:40,753][257371] Reward + Measures: [[ 10.52992286   0.55860007   0.465        0.43800002   0.24819998
    3.49690795]
 [-71.7930586    0.32820001   0.3664       0.35780001   0.25439999
    2.98723412]
 [-73.88550634   0.42050001   0.39999998   0.21160002   0.26949999
    3.74595451]
 ...
 [154.26570034   0.3545       0.47         0.35540003   0.27410004
    2.52324271]
 [-41.42106108   0.26690003   0.38010001   0.30809999   0.30329999
    2.29960942]
 [  0.18232905   0.30960003   0.29530001   0.25190002   0.14900002
    4.45108843]][0m
[37m[1m[2023-07-17 03:28:40,753][257371] Max Reward on eval: 491.1730937716551[0m
[37m[1m[2023-07-17 03:28:40,753][257371] Min Reward on eval: -168.73991297371686[0m
[37m[1m[2023-07-17 03:28:40,753][257371] Mean Reward across all agents: 37.470805904671565[0m
[37m[1m[2023-07-17 03:28:40,753][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:28:40,756][257371] mean_value=-980.8870056080151, max_value=403.23783269038415[0m
[37m[1m[2023-07-17 03:28:40,758][257371] New mean coefficients: [[ 0.57249856 -0.7599428   1.8095319   0.3964324  -0.70969677  6.668289  ]][0m
[37m[1m[2023-07-17 03:28:40,759][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:28:49,766][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 03:28:49,766][257371] FPS: 426418.92[0m
[36m[2023-07-17 03:28:49,768][257371] itr=558, itrs=2000, Progress: 27.90%[0m
[36m[2023-07-17 03:29:01,415][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 03:29:01,415][257371] FPS: 331909.97[0m
[36m[2023-07-17 03:29:05,729][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:29:05,729][257371] Reward + Measures: [[173.3892086    0.49465564   0.48124734   0.51287705   0.11662433
    5.82950497]][0m
[37m[1m[2023-07-17 03:29:05,730][257371] Max Reward on eval: 173.38920859967646[0m
[37m[1m[2023-07-17 03:29:05,730][257371] Min Reward on eval: 173.38920859967646[0m
[37m[1m[2023-07-17 03:29:05,730][257371] Mean Reward across all agents: 173.38920859967646[0m
[37m[1m[2023-07-17 03:29:05,730][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:29:10,747][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:29:10,748][257371] Reward + Measures: [[ -7.83317083   0.32189998   0.32440001   0.33849999   0.0342
    4.69152689]
 [282.5038818    0.52679998   0.5158       0.50800008   0.0554
    4.67121649]
 [ 25.15421804   0.34940001   0.3364       0.24870001   0.14140001
    4.8772788 ]
 ...
 [ 85.48954807   0.24130002   0.23909998   0.2922       0.0948
    5.03340912]
 [ 32.9458206    0.61580002   0.60869998   0.44180003   0.21830001
    5.14215088]
 [-62.87459261   0.46129999   0.57209998   0.32409999   0.3856
    4.36014175]][0m
[37m[1m[2023-07-17 03:29:10,748][257371] Max Reward on eval: 550.6093607174232[0m
[37m[1m[2023-07-17 03:29:10,748][257371] Min Reward on eval: -183.6555832883343[0m
[37m[1m[2023-07-17 03:29:10,748][257371] Mean Reward across all agents: 90.07222543964586[0m
[37m[1m[2023-07-17 03:29:10,749][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:29:10,753][257371] mean_value=-302.13987400639934, max_value=603.0620912584985[0m
[37m[1m[2023-07-17 03:29:10,755][257371] New mean coefficients: [[ 0.9526149  -2.353579    1.4804605   0.27475664  0.1266824   7.507806  ]][0m
[37m[1m[2023-07-17 03:29:10,756][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:29:19,879][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 03:29:19,879][257371] FPS: 421015.79[0m
[36m[2023-07-17 03:29:19,881][257371] itr=559, itrs=2000, Progress: 27.95%[0m
[36m[2023-07-17 03:29:31,758][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 03:29:31,758][257371] FPS: 325471.22[0m
[36m[2023-07-17 03:29:36,046][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:29:36,047][257371] Reward + Measures: [[168.88905984   0.508605     0.49601066   0.52559566   0.10853633
    5.87004471]][0m
[37m[1m[2023-07-17 03:29:36,047][257371] Max Reward on eval: 168.88905984293905[0m
[37m[1m[2023-07-17 03:29:36,047][257371] Min Reward on eval: 168.88905984293905[0m
[37m[1m[2023-07-17 03:29:36,048][257371] Mean Reward across all agents: 168.88905984293905[0m
[37m[1m[2023-07-17 03:29:36,048][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:29:41,072][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:29:41,072][257371] Reward + Measures: [[-13.27043787   0.33659998   0.33530003   0.28830001   0.0874
    4.89625788]
 [-82.48763726   0.28920001   0.37409997   0.13330001   0.37840003
    5.54704142]
 [  9.41099076   0.14119999   0.10210001   0.10420001   0.1171
    4.36793709]
 ...
 [ 44.00664723   0.29410002   0.3039       0.227        0.20869999
    3.11207962]
 [ 56.52328717   0.25800002   0.29179999   0.20079999   0.21209998
    3.12786937]
 [ 43.15994334   0.1109       0.1346       0.09550001   0.0855
    3.93573308]][0m
[37m[1m[2023-07-17 03:29:41,073][257371] Max Reward on eval: 282.04303499590605[0m
[37m[1m[2023-07-17 03:29:41,073][257371] Min Reward on eval: -131.03796429019422[0m
[37m[1m[2023-07-17 03:29:41,073][257371] Mean Reward across all agents: 8.801191818730072[0m
[37m[1m[2023-07-17 03:29:41,073][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:29:41,075][257371] mean_value=-2839.470297741801, max_value=301.50087051377756[0m
[37m[1m[2023-07-17 03:29:41,077][257371] New mean coefficients: [[ 0.44584763 -0.8604233   0.34661043  0.39544085 -0.5059429   7.035934  ]][0m
[37m[1m[2023-07-17 03:29:41,078][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:29:50,146][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 03:29:50,146][257371] FPS: 423543.47[0m
[36m[2023-07-17 03:29:50,148][257371] itr=560, itrs=2000, Progress: 28.00%[0m
[37m[1m[2023-07-17 03:32:47,826][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000540[0m
[36m[2023-07-17 03:33:00,162][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 03:33:00,163][257371] FPS: 327436.97[0m
[36m[2023-07-17 03:33:04,363][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:33:04,363][257371] Reward + Measures: [[154.46822311   0.52971929   0.51632464   0.54565996   0.10578733
    5.94630527]][0m
[37m[1m[2023-07-17 03:33:04,363][257371] Max Reward on eval: 154.46822311146423[0m
[37m[1m[2023-07-17 03:33:04,364][257371] Min Reward on eval: 154.46822311146423[0m
[37m[1m[2023-07-17 03:33:04,364][257371] Mean Reward across all agents: 154.46822311146423[0m
[37m[1m[2023-07-17 03:33:04,364][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:33:09,354][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:33:09,354][257371] Reward + Measures: [[  42.55443769    0.34920001    0.35260001    0.31200001    0.32859999
     3.22094011]
 [   1.07253676    0.37280002    0.47140002    0.3513        0.33790001
     3.30573344]
 [  32.39331097    0.36300001    0.31090003    0.31020004    0.07560001
     4.48316002]
 ...
 [-261.80127259    0.67500001    0.77969998    0.24390002    0.65939999
     4.61107779]
 [ -55.95875166    0.1268        0.09900001    0.0993        0.1005
     5.40235996]
 [  29.1750509     0.71799999    0.6979        0.64289999    0.1348
     4.36391783]][0m
[37m[1m[2023-07-17 03:33:09,355][257371] Max Reward on eval: 589.5937118627131[0m
[37m[1m[2023-07-17 03:33:09,355][257371] Min Reward on eval: -314.68828248549255[0m
[37m[1m[2023-07-17 03:33:09,355][257371] Mean Reward across all agents: 41.169182486639[0m
[37m[1m[2023-07-17 03:33:09,355][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:33:09,361][257371] mean_value=-930.5858481153101, max_value=470.2467690161057[0m
[37m[1m[2023-07-17 03:33:09,364][257371] New mean coefficients: [[ 1.2253642  -1.6163306  -0.6529674   0.42592135 -1.5089102   6.8135014 ]][0m
[37m[1m[2023-07-17 03:33:09,365][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:33:18,355][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 03:33:18,355][257371] FPS: 427224.61[0m
[36m[2023-07-17 03:33:18,357][257371] itr=561, itrs=2000, Progress: 28.05%[0m
[36m[2023-07-17 03:33:29,950][257371] train() took 11.51 seconds to complete[0m
[36m[2023-07-17 03:33:29,951][257371] FPS: 333554.58[0m
[36m[2023-07-17 03:33:34,216][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:33:34,217][257371] Reward + Measures: [[109.73790832   0.50291198   0.48969099   0.52053529   0.11097267
    5.98311567]][0m
[37m[1m[2023-07-17 03:33:34,217][257371] Max Reward on eval: 109.73790831917081[0m
[37m[1m[2023-07-17 03:33:34,217][257371] Min Reward on eval: 109.73790831917081[0m
[37m[1m[2023-07-17 03:33:34,218][257371] Mean Reward across all agents: 109.73790831917081[0m
[37m[1m[2023-07-17 03:33:34,218][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:33:39,199][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:33:39,199][257371] Reward + Measures: [[-30.82690774   0.50980002   0.37220001   0.40680003   0.0929
    4.3334856 ]
 [  9.19055201   0.29350001   0.27959999   0.21269999   0.149
    3.92024732]
 [ 27.92884148   0.3777       0.35080001   0.35749999   0.36610001
    3.19890189]
 ...
 [ -9.22904308   0.3989       0.48240003   0.36859998   0.44679999
    3.10400319]
 [ 33.10933338   0.26120001   0.2559       0.264        0.115
    4.83382511]
 [273.31531929   0.69710004   0.6523       0.61580002   0.0457
    4.29542685]][0m
[37m[1m[2023-07-17 03:33:39,200][257371] Max Reward on eval: 647.2417906273156[0m
[37m[1m[2023-07-17 03:33:39,200][257371] Min Reward on eval: -190.85298931860598[0m
[37m[1m[2023-07-17 03:33:39,200][257371] Mean Reward across all agents: 129.89315295899013[0m
[37m[1m[2023-07-17 03:33:39,200][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:33:39,205][257371] mean_value=-868.116861273584, max_value=528.6071322004616[0m
[37m[1m[2023-07-17 03:33:39,208][257371] New mean coefficients: [[ 1.8996987  -2.4499536   0.35986465  0.10791057 -1.2625867   7.86233   ]][0m
[37m[1m[2023-07-17 03:33:39,209][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:33:48,157][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 03:33:48,157][257371] FPS: 429249.18[0m
[36m[2023-07-17 03:33:48,159][257371] itr=562, itrs=2000, Progress: 28.10%[0m
[36m[2023-07-17 03:33:59,795][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 03:33:59,795][257371] FPS: 332248.68[0m
[36m[2023-07-17 03:34:04,123][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:34:04,123][257371] Reward + Measures: [[89.13235934  0.49979967  0.48630732  0.51802033  0.11208134  5.99606514]][0m
[37m[1m[2023-07-17 03:34:04,124][257371] Max Reward on eval: 89.13235934456613[0m
[37m[1m[2023-07-17 03:34:04,124][257371] Min Reward on eval: 89.13235934456613[0m
[37m[1m[2023-07-17 03:34:04,124][257371] Mean Reward across all agents: 89.13235934456613[0m
[37m[1m[2023-07-17 03:34:04,124][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:34:09,160][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:34:09,161][257371] Reward + Measures: [[ -22.71089123    0.1035        0.1094        0.1181        0.07520001
     4.75296164]
 [  14.38497735    0.16700001    0.2024        0.1312        0.127
     5.52154684]
 [-140.70466838    0.59600008    0.59570003    0.5851        0.0421
     6.10236406]
 ...
 [ -28.0544418     0.47530004    0.51570004    0.37820002    0.18450001
     5.01342583]
 [ -75.64632442    0.67520005    0.67559999    0.66350001    0.0294
     6.29339075]
 [  16.06761733    0.2906        0.32519999    0.20200001    0.176
     4.71099472]][0m
[37m[1m[2023-07-17 03:34:09,161][257371] Max Reward on eval: 357.4271850399673[0m
[37m[1m[2023-07-17 03:34:09,161][257371] Min Reward on eval: -188.38695217357017[0m
[37m[1m[2023-07-17 03:34:09,162][257371] Mean Reward across all agents: -0.11928401814834766[0m
[37m[1m[2023-07-17 03:34:09,162][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:34:09,164][257371] mean_value=-1566.4944293579224, max_value=124.16307761279681[0m
[37m[1m[2023-07-17 03:34:09,166][257371] New mean coefficients: [[ 1.6534493  -1.2657359   0.663535    0.22815502 -0.4416644   7.4292674 ]][0m
[37m[1m[2023-07-17 03:34:09,167][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:34:18,307][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 03:34:18,307][257371] FPS: 420220.09[0m
[36m[2023-07-17 03:34:18,310][257371] itr=563, itrs=2000, Progress: 28.15%[0m
[36m[2023-07-17 03:34:30,042][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 03:34:30,042][257371] FPS: 329564.44[0m
[36m[2023-07-17 03:34:34,331][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:34:34,332][257371] Reward + Measures: [[80.83134354  0.49339798  0.48030201  0.51218098  0.11368099  5.97267914]][0m
[37m[1m[2023-07-17 03:34:34,332][257371] Max Reward on eval: 80.83134353698335[0m
[37m[1m[2023-07-17 03:34:34,332][257371] Min Reward on eval: 80.83134353698335[0m
[37m[1m[2023-07-17 03:34:34,332][257371] Mean Reward across all agents: 80.83134353698335[0m
[37m[1m[2023-07-17 03:34:34,333][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:34:39,571][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:34:39,572][257371] Reward + Measures: [[161.88864434   0.26130003   0.30990002   0.29539999   0.1983
    5.21640635]
 [-32.20710166   0.1882       0.15680002   0.15989999   0.08889999
    3.99251676]
 [ 93.76040954   0.33689997   0.33640003   0.28830001   0.0467
    4.44240713]
 ...
 [ 24.63893222   0.61559999   0.60710001   0.59060001   0.032
    6.48375845]
 [ 21.69089487   0.3188       0.25219998   0.1869       0.1138
    4.49325418]
 [ 42.72812574   0.31470004   0.30560002   0.25240001   0.2
    4.27654314]][0m
[37m[1m[2023-07-17 03:34:39,572][257371] Max Reward on eval: 497.346000189241[0m
[37m[1m[2023-07-17 03:34:39,572][257371] Min Reward on eval: -207.61988232173024[0m
[37m[1m[2023-07-17 03:34:39,573][257371] Mean Reward across all agents: -4.751577666759971[0m
[37m[1m[2023-07-17 03:34:39,573][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:34:39,575][257371] mean_value=-2111.3389835728913, max_value=667.449339733158[0m
[37m[1m[2023-07-17 03:34:39,577][257371] New mean coefficients: [[ 0.68557024 -0.9744415  -0.19397813  0.13186707 -1.1573964   6.936342  ]][0m
[37m[1m[2023-07-17 03:34:39,578][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:34:48,636][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 03:34:48,636][257371] FPS: 424046.32[0m
[36m[2023-07-17 03:34:48,638][257371] itr=564, itrs=2000, Progress: 28.20%[0m
[36m[2023-07-17 03:35:00,592][257371] train() took 11.88 seconds to complete[0m
[36m[2023-07-17 03:35:00,592][257371] FPS: 323335.11[0m
[36m[2023-07-17 03:35:04,925][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:35:04,925][257371] Reward + Measures: [[50.76571879  0.49945468  0.48655567  0.51784968  0.113561    5.99540997]][0m
[37m[1m[2023-07-17 03:35:04,926][257371] Max Reward on eval: 50.76571878862401[0m
[37m[1m[2023-07-17 03:35:04,926][257371] Min Reward on eval: 50.76571878862401[0m
[37m[1m[2023-07-17 03:35:04,926][257371] Mean Reward across all agents: 50.76571878862401[0m
[37m[1m[2023-07-17 03:35:04,926][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:35:09,940][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:35:09,941][257371] Reward + Measures: [[ 207.83731928    0.41299996    0.37050003    0.38699999    0.0754
     4.28853178]
 [  11.08014444    0.55790001    0.52929997    0.50450003    0.11079999
     3.95186234]
 [ 192.52813224    0.81059998    0.79720002    0.6893        0.13520001
     5.20747089]
 ...
 [ 114.35098935    0.94160002    0.92819995    0.90650004    0.0172
     6.26651621]
 [ 114.95147039    0.68400002    0.61710006    0.58490002    0.14830001
     4.81041098]
 [-112.46150996    0.48899999    0.48790008    0.52940005    0.07390001
     4.78296423]][0m
[37m[1m[2023-07-17 03:35:09,941][257371] Max Reward on eval: 580.6980586340418[0m
[37m[1m[2023-07-17 03:35:09,941][257371] Min Reward on eval: -196.99163748119025[0m
[37m[1m[2023-07-17 03:35:09,941][257371] Mean Reward across all agents: 144.87814228199363[0m
[37m[1m[2023-07-17 03:35:09,942][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:35:09,945][257371] mean_value=-581.5046818637791, max_value=745.1943672356475[0m
[37m[1m[2023-07-17 03:35:09,947][257371] New mean coefficients: [[ 0.44550598 -0.4582454  -0.2525445  -0.1351719  -1.670901    7.300146  ]][0m
[37m[1m[2023-07-17 03:35:09,948][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:35:19,004][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 03:35:19,004][257371] FPS: 424128.51[0m
[36m[2023-07-17 03:35:19,006][257371] itr=565, itrs=2000, Progress: 28.25%[0m
[36m[2023-07-17 03:35:30,726][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 03:35:30,726][257371] FPS: 329826.98[0m
[36m[2023-07-17 03:35:35,026][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:35:35,026][257371] Reward + Measures: [[13.26867871  0.49978331  0.48615736  0.52217031  0.11728534  6.04540491]][0m
[37m[1m[2023-07-17 03:35:35,026][257371] Max Reward on eval: 13.268678706852272[0m
[37m[1m[2023-07-17 03:35:35,026][257371] Min Reward on eval: 13.268678706852272[0m
[37m[1m[2023-07-17 03:35:35,027][257371] Mean Reward across all agents: 13.268678706852272[0m
[37m[1m[2023-07-17 03:35:35,027][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:35:39,999][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:35:39,999][257371] Reward + Measures: [[ 39.69395855   0.62219995   0.56459999   0.56209999   0.0492
    5.06782913]
 [-36.16588758   0.2744       0.22580002   0.20050001   0.0995
    4.19699717]
 [ -5.7556236    0.50159997   0.5492       0.35670003   0.23870002
    5.68220711]
 ...
 [-33.82144501   0.23369999   0.2868       0.1955       0.23540001
    4.04426336]
 [-62.07754022   0.33860001   0.31669998   0.25939998   0.257
    4.73583555]
 [-64.2887376    0.26850003   0.23670001   0.23059998   0.13660002
    5.21381664]][0m
[37m[1m[2023-07-17 03:35:39,999][257371] Max Reward on eval: 449.24912307858466[0m
[37m[1m[2023-07-17 03:35:40,000][257371] Min Reward on eval: -159.9375243880786[0m
[37m[1m[2023-07-17 03:35:40,000][257371] Mean Reward across all agents: -12.196670991031919[0m
[37m[1m[2023-07-17 03:35:40,000][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:35:40,002][257371] mean_value=-2109.6276932058704, max_value=125.20689249623214[0m
[37m[1m[2023-07-17 03:35:40,004][257371] New mean coefficients: [[ 0.22856638 -0.06684059  0.20933026  0.16485725 -0.9410048   7.1036563 ]][0m
[37m[1m[2023-07-17 03:35:40,005][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:35:49,003][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 03:35:49,004][257371] FPS: 426830.25[0m
[36m[2023-07-17 03:35:49,006][257371] itr=566, itrs=2000, Progress: 28.30%[0m
[36m[2023-07-17 03:36:00,680][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 03:36:00,680][257371] FPS: 331158.16[0m
[36m[2023-07-17 03:36:04,999][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:36:05,000][257371] Reward + Measures: [[-24.75047237   0.49852267   0.48741633   0.52617502   0.12377399
    6.10349751]][0m
[37m[1m[2023-07-17 03:36:05,000][257371] Max Reward on eval: -24.750472372091956[0m
[37m[1m[2023-07-17 03:36:05,000][257371] Min Reward on eval: -24.750472372091956[0m
[37m[1m[2023-07-17 03:36:05,000][257371] Mean Reward across all agents: -24.750472372091956[0m
[37m[1m[2023-07-17 03:36:05,001][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:36:10,063][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:36:10,063][257371] Reward + Measures: [[ 28.10603669   0.22060001   0.17480002   0.1425       0.14909999
    4.90709114]
 [-65.02310852   0.30039999   0.30320001   0.26570004   0.1222
    5.42126417]
 [-40.55972273   0.31130001   0.30399999   0.2538       0.204
    6.11340332]
 ...
 [-30.93900387   0.52590001   0.51969999   0.48899999   0.0641
    6.14466238]
 [195.02612027   0.51730007   0.48519999   0.45240003   0.0746
    4.67241383]
 [-27.95289122   0.13680001   0.114        0.18550001   0.1736
    5.54927588]][0m
[37m[1m[2023-07-17 03:36:10,063][257371] Max Reward on eval: 486.44886275972823[0m
[37m[1m[2023-07-17 03:36:10,064][257371] Min Reward on eval: -355.5052528725937[0m
[37m[1m[2023-07-17 03:36:10,064][257371] Mean Reward across all agents: -0.34461534367451313[0m
[37m[1m[2023-07-17 03:36:10,064][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:36:10,067][257371] mean_value=-409.9360284805437, max_value=594.8433506374402[0m
[37m[1m[2023-07-17 03:36:10,069][257371] New mean coefficients: [[ 0.6645508  -1.1101613   0.34588304 -0.0443695  -0.38978022  7.0765715 ]][0m
[37m[1m[2023-07-17 03:36:10,070][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:36:19,286][257371] train() took 9.21 seconds to complete[0m
[36m[2023-07-17 03:36:19,286][257371] FPS: 416774.43[0m
[36m[2023-07-17 03:36:19,288][257371] itr=567, itrs=2000, Progress: 28.35%[0m
[36m[2023-07-17 03:36:31,137][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 03:36:31,137][257371] FPS: 326345.94[0m
[36m[2023-07-17 03:36:35,492][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:36:35,493][257371] Reward + Measures: [[-52.91814637   0.50876302   0.49745101   0.53592235   0.12228666
    6.13063812]][0m
[37m[1m[2023-07-17 03:36:35,493][257371] Max Reward on eval: -52.91814637167975[0m
[37m[1m[2023-07-17 03:36:35,493][257371] Min Reward on eval: -52.91814637167975[0m
[37m[1m[2023-07-17 03:36:35,493][257371] Mean Reward across all agents: -52.91814637167975[0m
[37m[1m[2023-07-17 03:36:35,494][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:36:40,468][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:36:40,469][257371] Reward + Measures: [[-88.21542499   0.50839996   0.40470001   0.3409       0.1724
    5.96165609]
 [ 77.57972978   0.51270002   0.45309997   0.42720005   0.1508
    3.55833626]
 [ 16.62162945   0.48839998   0.39429998   0.35350001   0.17920001
    2.67437601]
 ...
 [-97.90278534   0.54400003   0.49950001   0.4966       0.0416
    5.5834136 ]
 [-16.21020232   0.58520001   0.55930001   0.39309999   0.12310001
    3.88250542]
 [ 87.21682567   0.53370005   0.50620002   0.48189998   0.07300001
    5.40664291]][0m
[37m[1m[2023-07-17 03:36:40,469][257371] Max Reward on eval: 529.969177247677[0m
[37m[1m[2023-07-17 03:36:40,470][257371] Min Reward on eval: -141.47837653351016[0m
[37m[1m[2023-07-17 03:36:40,470][257371] Mean Reward across all agents: 30.520718158141257[0m
[37m[1m[2023-07-17 03:36:40,470][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:36:40,472][257371] mean_value=-1499.8321204499337, max_value=276.7872478421875[0m
[37m[1m[2023-07-17 03:36:40,475][257371] New mean coefficients: [[-0.32696402 -0.85163957  0.3165911  -0.09221482 -0.25216043  7.2483068 ]][0m
[37m[1m[2023-07-17 03:36:40,476][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:36:49,472][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 03:36:49,472][257371] FPS: 426926.21[0m
[36m[2023-07-17 03:36:49,474][257371] itr=568, itrs=2000, Progress: 28.40%[0m
[36m[2023-07-17 03:37:01,258][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 03:37:01,258][257371] FPS: 328031.70[0m
[36m[2023-07-17 03:37:05,606][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:37:05,606][257371] Reward + Measures: [[-55.78224584   0.50940436   0.49724665   0.54208237   0.12726633
    6.18463135]][0m
[37m[1m[2023-07-17 03:37:05,606][257371] Max Reward on eval: -55.78224583638898[0m
[37m[1m[2023-07-17 03:37:05,607][257371] Min Reward on eval: -55.78224583638898[0m
[37m[1m[2023-07-17 03:37:05,607][257371] Mean Reward across all agents: -55.78224583638898[0m
[37m[1m[2023-07-17 03:37:05,607][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:37:10,815][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:37:10,816][257371] Reward + Measures: [[ -75.78011215    0.57569999    0.61090004    0.51820004    0.1318
     6.07430315]
 [-141.44149776    0.55489999    0.58160001    0.32640001    0.34200001
     4.87563896]
 [-214.79511355    0.64359999    0.65869999    0.47940001    0.24020003
     5.8233676 ]
 ...
 [ -93.84078958    0.67819995    0.66370004    0.67179996    0.07030001
     6.16680384]
 [ -12.48767341    0.44160005    0.59619999    0.36649999    0.3145
     5.69213629]
 [ -48.92726748    0.40149999    0.3827        0.42249998    0.1459
     5.46127081]][0m
[37m[1m[2023-07-17 03:37:10,816][257371] Max Reward on eval: 376.44382480936474[0m
[37m[1m[2023-07-17 03:37:10,816][257371] Min Reward on eval: -309.2505569210276[0m
[37m[1m[2023-07-17 03:37:10,817][257371] Mean Reward across all agents: -8.886358431492509[0m
[37m[1m[2023-07-17 03:37:10,817][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:37:10,820][257371] mean_value=-399.8271487094105, max_value=231.56344565911678[0m
[37m[1m[2023-07-17 03:37:10,822][257371] New mean coefficients: [[-0.6924168  -1.1183497   0.42860126 -0.25583535 -0.15723334  6.81471   ]][0m
[37m[1m[2023-07-17 03:37:10,823][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:37:19,760][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 03:37:19,761][257371] FPS: 429743.44[0m
[36m[2023-07-17 03:37:19,763][257371] itr=569, itrs=2000, Progress: 28.45%[0m
[36m[2023-07-17 03:37:31,684][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-17 03:37:31,684][257371] FPS: 324277.30[0m
[36m[2023-07-17 03:37:36,002][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:37:36,002][257371] Reward + Measures: [[-61.88338089   0.49458668   0.48315099   0.53248769   0.13681333
    6.20555401]][0m
[37m[1m[2023-07-17 03:37:36,003][257371] Max Reward on eval: -61.88338089079931[0m
[37m[1m[2023-07-17 03:37:36,003][257371] Min Reward on eval: -61.88338089079931[0m
[37m[1m[2023-07-17 03:37:36,003][257371] Mean Reward across all agents: -61.88338089079931[0m
[37m[1m[2023-07-17 03:37:36,003][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:37:40,984][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:37:40,985][257371] Reward + Measures: [[182.66107018   0.42880002   0.42550001   0.40530005   0.08790001
    5.01101065]
 [124.31083973   0.4718       0.4605       0.43000004   0.1023
    5.20605898]
 [ 22.72262628   0.3513       0.36890003   0.3053       0.27480003
    3.83195734]
 ...
 [143.27937654   0.69740003   0.65960002   0.49910003   0.14559999
    5.55448103]
 [140.48672916   0.38189998   0.4289       0.37680003   0.15019999
    4.36695242]
 [ -8.59153342   0.68500006   0.68519998   0.58329999   0.11630001
    5.52291965]][0m
[37m[1m[2023-07-17 03:37:40,985][257371] Max Reward on eval: 645.3616511458531[0m
[37m[1m[2023-07-17 03:37:40,985][257371] Min Reward on eval: -292.9928259543143[0m
[37m[1m[2023-07-17 03:37:40,986][257371] Mean Reward across all agents: 90.97395701336633[0m
[37m[1m[2023-07-17 03:37:40,986][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:37:40,989][257371] mean_value=-1089.9532104469768, max_value=425.0179206638056[0m
[37m[1m[2023-07-17 03:37:40,992][257371] New mean coefficients: [[-0.7749577  -0.3999613   1.3192494  -0.56248367  0.62525046  6.3458266 ]][0m
[37m[1m[2023-07-17 03:37:40,992][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:37:50,021][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 03:37:50,021][257371] FPS: 425418.98[0m
[36m[2023-07-17 03:37:50,023][257371] itr=570, itrs=2000, Progress: 28.50%[0m
[37m[1m[2023-07-17 03:40:45,794][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000550[0m
[36m[2023-07-17 03:40:57,833][257371] train() took 11.52 seconds to complete[0m
[36m[2023-07-17 03:40:57,834][257371] FPS: 333379.89[0m
[36m[2023-07-17 03:41:02,002][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:41:02,003][257371] Reward + Measures: [[-66.62154593   0.51670098   0.50530469   0.54852265   0.12953132
    6.27607393]][0m
[37m[1m[2023-07-17 03:41:02,003][257371] Max Reward on eval: -66.62154593064626[0m
[37m[1m[2023-07-17 03:41:02,003][257371] Min Reward on eval: -66.62154593064626[0m
[37m[1m[2023-07-17 03:41:02,003][257371] Mean Reward across all agents: -66.62154593064626[0m
[37m[1m[2023-07-17 03:41:02,004][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:41:06,952][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:41:06,952][257371] Reward + Measures: [[110.94394112   0.59850001   0.54650003   0.5007       0.14430001
    5.12334156]
 [311.66414163   0.21340001   0.78459996   0.74270004   0.66620004
    5.15236664]
 [ 21.43338165   0.75769997   0.71000004   0.67460001   0.0239
    6.32463837]
 ...
 [-48.94525343   0.58579999   0.56380004   0.4429       0.20020001
    6.15644836]
 [248.98899162   0.61090004   0.58279997   0.57410002   0.06789999
    5.13381338]
 [-55.57900484   0.20739999   0.2422       0.15989999   0.2184
    3.94768   ]][0m
[37m[1m[2023-07-17 03:41:06,953][257371] Max Reward on eval: 461.4408188296482[0m
[37m[1m[2023-07-17 03:41:06,953][257371] Min Reward on eval: -249.8272990710102[0m
[37m[1m[2023-07-17 03:41:06,953][257371] Mean Reward across all agents: 46.946326678181634[0m
[37m[1m[2023-07-17 03:41:06,953][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:41:06,956][257371] mean_value=-517.6942650176351, max_value=292.3095552832225[0m
[37m[1m[2023-07-17 03:41:06,959][257371] New mean coefficients: [[-1.1188564  -0.51074946  1.2938043  -0.30288056  0.5468898   6.085374  ]][0m
[37m[1m[2023-07-17 03:41:06,960][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:41:15,969][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 03:41:15,969][257371] FPS: 426316.22[0m
[36m[2023-07-17 03:41:15,971][257371] itr=571, itrs=2000, Progress: 28.55%[0m
[36m[2023-07-17 03:41:27,667][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 03:41:27,668][257371] FPS: 330498.31[0m
[36m[2023-07-17 03:41:31,913][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:41:31,919][257371] Reward + Measures: [[-57.96736466   0.51446068   0.50237399   0.5485903    0.13420533
    6.28854561]][0m
[37m[1m[2023-07-17 03:41:31,919][257371] Max Reward on eval: -57.96736466054589[0m
[37m[1m[2023-07-17 03:41:31,920][257371] Min Reward on eval: -57.96736466054589[0m
[37m[1m[2023-07-17 03:41:31,920][257371] Mean Reward across all agents: -57.96736466054589[0m
[37m[1m[2023-07-17 03:41:31,920][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:41:36,911][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:41:36,912][257371] Reward + Measures: [[  87.0944108     0.27760002    0.33650002    0.21139999    0.2859
     3.85614705]
 [ 208.98056354    0.67559999    0.66399997    0.66250002    0.07660001
     5.96878433]
 [ -61.48763408    0.35140002    0.359         0.33290002    0.17570001
     3.17357135]
 ...
 [  61.88616184    0.30380002    0.32860002    0.2378        0.19840001
     3.77648926]
 [ -39.44389984    0.2976        0.27649999    0.2271        0.11790001
     4.21981955]
 [-100.27716913    0.1295        0.0988        0.0673        0.1035
     5.48189354]][0m
[37m[1m[2023-07-17 03:41:36,912][257371] Max Reward on eval: 290.81229403633625[0m
[37m[1m[2023-07-17 03:41:36,912][257371] Min Reward on eval: -165.49331137733535[0m
[37m[1m[2023-07-17 03:41:36,913][257371] Mean Reward across all agents: 7.430518010315068[0m
[37m[1m[2023-07-17 03:41:36,918][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:41:36,920][257371] mean_value=-1183.1727078061988, max_value=205.8721229317124[0m
[37m[1m[2023-07-17 03:41:36,923][257371] New mean coefficients: [[-1.1723707  -0.39536744  0.7874032  -0.2512461   0.16565663  6.201392  ]][0m
[37m[1m[2023-07-17 03:41:36,924][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:41:45,966][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 03:41:45,966][257371] FPS: 424761.87[0m
[36m[2023-07-17 03:41:45,969][257371] itr=572, itrs=2000, Progress: 28.60%[0m
[36m[2023-07-17 03:41:58,105][257371] train() took 12.06 seconds to complete[0m
[36m[2023-07-17 03:41:58,105][257371] FPS: 318466.20[0m
[36m[2023-07-17 03:42:02,444][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:42:02,445][257371] Reward + Measures: [[-64.88555963   0.51149333   0.49983403   0.54484999   0.13281366
    6.33467817]][0m
[37m[1m[2023-07-17 03:42:02,445][257371] Max Reward on eval: -64.88555963094016[0m
[37m[1m[2023-07-17 03:42:02,445][257371] Min Reward on eval: -64.88555963094016[0m
[37m[1m[2023-07-17 03:42:02,446][257371] Mean Reward across all agents: -64.88555963094016[0m
[37m[1m[2023-07-17 03:42:02,446][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:42:07,439][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:42:07,439][257371] Reward + Measures: [[   5.62153245    0.5248        0.53649992    0.43990007    0.15730001
     3.96672559]
 [  42.29365564    0.16049999    0.153         0.15120001    0.0684
     5.79794741]
 [-115.08281883    0.75920004    0.74879998    0.74410003    0.0357
     5.42616653]
 ...
 [   5.90791677    0.54050004    0.68810004    0.55340004    0.22739999
     5.49220657]
 [  10.5992788     0.42500001    0.50349998    0.31219998    0.29950002
     3.47927451]
 [  22.16380969    0.43139997    0.43670002    0.31469998    0.26799998
     3.28677034]][0m
[37m[1m[2023-07-17 03:42:07,439][257371] Max Reward on eval: 338.44689681492747[0m
[37m[1m[2023-07-17 03:42:07,440][257371] Min Reward on eval: -151.3491363298148[0m
[37m[1m[2023-07-17 03:42:07,440][257371] Mean Reward across all agents: 7.1265090512084805[0m
[37m[1m[2023-07-17 03:42:07,440][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:42:07,442][257371] mean_value=-645.5515307815431, max_value=234.05324098213617[0m
[37m[1m[2023-07-17 03:42:07,445][257371] New mean coefficients: [[-0.9063635  -0.56391084  0.69490284 -0.33132643  0.22808212  6.81277   ]][0m
[37m[1m[2023-07-17 03:42:07,446][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:42:16,509][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 03:42:16,509][257371] FPS: 423790.01[0m
[36m[2023-07-17 03:42:16,511][257371] itr=573, itrs=2000, Progress: 28.65%[0m
[36m[2023-07-17 03:42:28,135][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 03:42:28,135][257371] FPS: 332582.29[0m
[36m[2023-07-17 03:42:32,460][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:42:32,466][257371] Reward + Measures: [[-61.01891135   0.54460567   0.53412706   0.57490569   0.12433
    6.36951923]][0m
[37m[1m[2023-07-17 03:42:32,466][257371] Max Reward on eval: -61.01891134587508[0m
[37m[1m[2023-07-17 03:42:32,467][257371] Min Reward on eval: -61.01891134587508[0m
[37m[1m[2023-07-17 03:42:32,467][257371] Mean Reward across all agents: -61.01891134587508[0m
[37m[1m[2023-07-17 03:42:32,467][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:42:37,431][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:42:37,431][257371] Reward + Measures: [[ 12.86823654   0.1779       0.16360001   0.21980003   0.10570001
    5.52291679]
 [136.63516292   0.54940003   0.56170005   0.48370001   0.1226
    5.26193857]
 [ 31.99080978   0.38890001   0.42560002   0.3479       0.47
    3.26505899]
 ...
 [152.43430563   0.44890004   0.44510004   0.45500001   0.0605
    5.80843735]
 [ 14.66285706   0.36480001   0.29340002   0.30670002   0.07390001
    4.97338772]
 [ 67.95181273   0.41439995   0.40850002   0.41389999   0.06940001
    4.57785177]][0m
[37m[1m[2023-07-17 03:42:37,432][257371] Max Reward on eval: 479.8769206965342[0m
[37m[1m[2023-07-17 03:42:37,432][257371] Min Reward on eval: -192.84208945985884[0m
[37m[1m[2023-07-17 03:42:37,432][257371] Mean Reward across all agents: 62.3633829374[0m
[37m[1m[2023-07-17 03:42:37,432][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:42:37,435][257371] mean_value=-384.738341507981, max_value=646.2073370164153[0m
[37m[1m[2023-07-17 03:42:37,438][257371] New mean coefficients: [[-0.7930436  -1.0270646  -0.25697    -0.13938977 -0.6622405   7.2957726 ]][0m
[37m[1m[2023-07-17 03:42:37,439][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:42:46,489][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 03:42:46,490][257371] FPS: 424353.88[0m
[36m[2023-07-17 03:42:46,492][257371] itr=574, itrs=2000, Progress: 28.70%[0m
[36m[2023-07-17 03:42:58,287][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 03:42:58,288][257371] FPS: 327789.08[0m
[36m[2023-07-17 03:43:02,368][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:43:02,369][257371] Reward + Measures: [[-62.50424184   0.54423797   0.53349203   0.57014138   0.118631
    6.38115549]][0m
[37m[1m[2023-07-17 03:43:02,369][257371] Max Reward on eval: -62.504241838524294[0m
[37m[1m[2023-07-17 03:43:02,369][257371] Min Reward on eval: -62.504241838524294[0m
[37m[1m[2023-07-17 03:43:02,370][257371] Mean Reward across all agents: -62.504241838524294[0m
[37m[1m[2023-07-17 03:43:02,370][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:43:07,071][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:43:07,071][257371] Reward + Measures: [[  73.49497933    0.45910001    0.41260001    0.38579997    0.0896
     5.34224701]
 [ 115.98829801    0.45500001    0.47750002    0.43610001    0.10519999
     5.16510105]
 [  46.86458454    0.1895        0.17210001    0.1506        0.125
     3.43705869]
 ...
 [-153.52513478    0.69960004    0.6742        0.56669998    0.1469
     5.31988382]
 [ -12.07534498    0.2339        0.23469999    0.20550001    0.17569999
     3.49915552]
 [ 229.48033812    0.56690001    0.53330004    0.50959998    0.0623
     5.41431379]][0m
[37m[1m[2023-07-17 03:43:07,071][257371] Max Reward on eval: 641.7628211903618[0m
[37m[1m[2023-07-17 03:43:07,072][257371] Min Reward on eval: -274.6180467566475[0m
[37m[1m[2023-07-17 03:43:07,072][257371] Mean Reward across all agents: 46.20470354842535[0m
[37m[1m[2023-07-17 03:43:07,072][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:43:07,076][257371] mean_value=-842.7472290567055, max_value=283.21989180480665[0m
[37m[1m[2023-07-17 03:43:07,078][257371] New mean coefficients: [[-1.0636268  -1.0677793   0.06394398  0.6904539  -0.7244166   7.4285398 ]][0m
[37m[1m[2023-07-17 03:43:07,079][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:43:16,254][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-17 03:43:16,254][257371] FPS: 418617.58[0m
[36m[2023-07-17 03:43:16,256][257371] itr=575, itrs=2000, Progress: 28.75%[0m
[36m[2023-07-17 03:43:28,253][257371] train() took 11.92 seconds to complete[0m
[36m[2023-07-17 03:43:28,253][257371] FPS: 322226.62[0m
[36m[2023-07-17 03:43:32,608][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:43:32,609][257371] Reward + Measures: [[-66.00664991   0.54438567   0.53360099   0.57206899   0.11995734
    6.4244895 ]][0m
[37m[1m[2023-07-17 03:43:32,609][257371] Max Reward on eval: -66.00664990888221[0m
[37m[1m[2023-07-17 03:43:32,609][257371] Min Reward on eval: -66.00664990888221[0m
[37m[1m[2023-07-17 03:43:32,609][257371] Mean Reward across all agents: -66.00664990888221[0m
[37m[1m[2023-07-17 03:43:32,610][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:43:37,674][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:43:37,675][257371] Reward + Measures: [[111.72686672   0.23480001   0.23830001   0.21730001   0.0442
    4.25143957]
 [-78.73228486   0.39240003   0.4206       0.14960001   0.36339998
    4.04516506]
 [254.89890154   0.7274       0.73940003   0.59390002   0.1441
    4.81975698]
 ...
 [212.71701526   0.67309999   0.77340001   0.59450001   0.26250002
    4.91162109]
 [ 18.21917391   0.44049999   0.42609999   0.43090001   0.0848
    5.75337982]
 [187.35636572   0.50840002   0.50040001   0.48460004   0.0376
    5.54182673]][0m
[37m[1m[2023-07-17 03:43:37,675][257371] Max Reward on eval: 583.1804170350545[0m
[37m[1m[2023-07-17 03:43:37,676][257371] Min Reward on eval: -184.03689809497445[0m
[37m[1m[2023-07-17 03:43:37,676][257371] Mean Reward across all agents: 74.58431162931427[0m
[37m[1m[2023-07-17 03:43:37,676][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:43:37,678][257371] mean_value=-596.9707749189035, max_value=249.4312409152681[0m
[37m[1m[2023-07-17 03:43:37,681][257371] New mean coefficients: [[-1.2401237  -1.2878345  -0.75781494  0.5279179  -0.44460568  8.194996  ]][0m
[37m[1m[2023-07-17 03:43:37,682][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:43:46,736][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 03:43:46,736][257371] FPS: 424176.41[0m
[36m[2023-07-17 03:43:46,739][257371] itr=576, itrs=2000, Progress: 28.80%[0m
[36m[2023-07-17 03:43:58,653][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 03:43:58,653][257371] FPS: 324498.75[0m
[36m[2023-07-17 03:44:02,837][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:44:02,837][257371] Reward + Measures: [[-69.37450646   0.54664499   0.53543562   0.57370937   0.11817601
    6.45219183]][0m
[37m[1m[2023-07-17 03:44:02,837][257371] Max Reward on eval: -69.37450645738838[0m
[37m[1m[2023-07-17 03:44:02,838][257371] Min Reward on eval: -69.37450645738838[0m
[37m[1m[2023-07-17 03:44:02,838][257371] Mean Reward across all agents: -69.37450645738838[0m
[37m[1m[2023-07-17 03:44:02,838][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:44:07,813][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:44:07,813][257371] Reward + Measures: [[  -0.9490494     0.25799999    0.23170002    0.1964        0.16339999
     3.94131708]
 [-110.6271442     0.36050001    0.33220002    0.31300002    0.21110001
     3.91749167]
 [ -55.70604069    0.65250003    0.47319999    0.49689999    0.21510001
     3.47891617]
 ...
 [ 129.72509384    0.75380003    0.71420002    0.667         0.0665
     5.03365946]
 [  62.4134481     0.47709998    0.54479998    0.42810002    0.13750002
     5.87104416]
 [  18.73081439    0.53619999    0.40510002    0.42770001    0.19430001
     2.98119044]][0m
[37m[1m[2023-07-17 03:44:07,814][257371] Max Reward on eval: 419.34683227390053[0m
[37m[1m[2023-07-17 03:44:07,814][257371] Min Reward on eval: -218.65983153916895[0m
[37m[1m[2023-07-17 03:44:07,814][257371] Mean Reward across all agents: 56.037897343671986[0m
[37m[1m[2023-07-17 03:44:07,814][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:44:07,818][257371] mean_value=-741.9405353619471, max_value=394.62501531583496[0m
[37m[1m[2023-07-17 03:44:07,821][257371] New mean coefficients: [[-1.2205799  -0.99651057 -0.53831017  1.1453023  -0.8092575   8.729759  ]][0m
[37m[1m[2023-07-17 03:44:07,822][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:44:16,822][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 03:44:16,823][257371] FPS: 426721.98[0m
[36m[2023-07-17 03:44:16,825][257371] itr=577, itrs=2000, Progress: 28.85%[0m
[36m[2023-07-17 03:44:28,745][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-17 03:44:28,745][257371] FPS: 324250.33[0m
[36m[2023-07-17 03:44:33,087][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:44:33,088][257371] Reward + Measures: [[-74.29700381   0.53847033   0.52828336   0.57094562   0.12715666
    6.48948908]][0m
[37m[1m[2023-07-17 03:44:33,088][257371] Max Reward on eval: -74.29700381480312[0m
[37m[1m[2023-07-17 03:44:33,088][257371] Min Reward on eval: -74.29700381480312[0m
[37m[1m[2023-07-17 03:44:33,089][257371] Mean Reward across all agents: -74.29700381480312[0m
[37m[1m[2023-07-17 03:44:33,089][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:44:38,295][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:44:38,301][257371] Reward + Measures: [[  37.31711484    0.51719999    0.51990002    0.50129998    0.047
     5.63354969]
 [  59.86692221    0.60400003    0.67080003    0.62320006    0.1322
     5.82429457]
 [  65.78154151    0.74340004    0.727         0.61400002    0.1286
     6.22383261]
 ...
 [-125.64479046    0.7299        0.73229998    0.66549999    0.1122
     5.58731937]
 [ 145.90981137    0.55809999    0.49419999    0.47630006    0.0539
     6.06407976]
 [ 170.91804628    0.77640003    0.8247        0.69850004    0.1736
     6.31576395]][0m
[37m[1m[2023-07-17 03:44:38,301][257371] Max Reward on eval: 437.21716308204924[0m
[37m[1m[2023-07-17 03:44:38,302][257371] Min Reward on eval: -283.610544625856[0m
[37m[1m[2023-07-17 03:44:38,302][257371] Mean Reward across all agents: 61.79005520764454[0m
[37m[1m[2023-07-17 03:44:38,302][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:44:38,307][257371] mean_value=-544.039762266027, max_value=629.5968721482559[0m
[37m[1m[2023-07-17 03:44:38,310][257371] New mean coefficients: [[-1.4750506 -0.8481705 -0.5715089  0.7183107 -0.5751257  8.772207 ]][0m
[37m[1m[2023-07-17 03:44:38,311][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:44:47,409][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 03:44:47,409][257371] FPS: 422126.23[0m
[36m[2023-07-17 03:44:47,412][257371] itr=578, itrs=2000, Progress: 28.90%[0m
[36m[2023-07-17 03:44:59,246][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 03:44:59,246][257371] FPS: 326754.15[0m
[36m[2023-07-17 03:45:03,567][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:45:03,568][257371] Reward + Measures: [[-77.92289388   0.54599231   0.53557765   0.57391733   0.11899133
    6.51529551]][0m
[37m[1m[2023-07-17 03:45:03,568][257371] Max Reward on eval: -77.92289388240191[0m
[37m[1m[2023-07-17 03:45:03,568][257371] Min Reward on eval: -77.92289388240191[0m
[37m[1m[2023-07-17 03:45:03,568][257371] Mean Reward across all agents: -77.92289388240191[0m
[37m[1m[2023-07-17 03:45:03,569][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:45:08,649][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:45:08,649][257371] Reward + Measures: [[-28.95337636   0.4691       0.44120002   0.42659998   0.0551
    5.61499643]
 [279.55627131   0.514        0.77669996   0.62229997   0.32660002
    6.75547934]
 [-47.05988823   0.49439999   0.51750004   0.50190002   0.0405
    5.61544275]
 ...
 [268.38588904   0.2965       0.76630002   0.42149997   0.58930004
    5.88851595]
 [114.95687606   0.40180001   0.396        0.38150001   0.0918
    5.8370676 ]
 [  8.78683324   0.36149999   0.43280002   0.3928       0.1717
    6.03331518]][0m
[37m[1m[2023-07-17 03:45:08,649][257371] Max Reward on eval: 508.20647424161433[0m
[37m[1m[2023-07-17 03:45:08,650][257371] Min Reward on eval: -288.99195386692884[0m
[37m[1m[2023-07-17 03:45:08,650][257371] Mean Reward across all agents: 8.344574040526291[0m
[37m[1m[2023-07-17 03:45:08,650][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:45:08,653][257371] mean_value=-322.456112855728, max_value=677.8808807710922[0m
[37m[1m[2023-07-17 03:45:08,656][257371] New mean coefficients: [[-2.1487994  -0.6697935  -0.06423271  0.91411823 -0.70129895  8.265114  ]][0m
[37m[1m[2023-07-17 03:45:08,657][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:45:17,761][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 03:45:17,761][257371] FPS: 421864.50[0m
[36m[2023-07-17 03:45:17,764][257371] itr=579, itrs=2000, Progress: 28.95%[0m
[36m[2023-07-17 03:45:29,488][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 03:45:29,488][257371] FPS: 329845.51[0m
[36m[2023-07-17 03:45:33,924][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:45:33,924][257371] Reward + Measures: [[-87.14656606   0.55075234   0.54060829   0.58042502   0.11725266
    6.56005764]][0m
[37m[1m[2023-07-17 03:45:33,925][257371] Max Reward on eval: -87.14656606069083[0m
[37m[1m[2023-07-17 03:45:33,925][257371] Min Reward on eval: -87.14656606069083[0m
[37m[1m[2023-07-17 03:45:33,925][257371] Mean Reward across all agents: -87.14656606069083[0m
[37m[1m[2023-07-17 03:45:33,925][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:45:38,988][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:45:38,988][257371] Reward + Measures: [[  4.44425523   0.24740005   0.21110001   0.23450001   0.11270001
    5.44329977]
 [-17.94716632   0.4382       0.3319       0.31389999   0.1724
    3.37463832]
 [280.38418015   0.69020003   0.66680002   0.64729995   0.0638
    5.30980301]
 ...
 [ 36.75362285   0.3272       0.28200004   0.32339999   0.1601
    5.7370162 ]
 [-34.99149501   0.32780001   0.31570002   0.38970003   0.1182
    6.24018049]
 [-12.0310689    0.45110002   0.45989999   0.32539999   0.1742
    3.89225769]][0m
[37m[1m[2023-07-17 03:45:38,989][257371] Max Reward on eval: 525.3957295308821[0m
[37m[1m[2023-07-17 03:45:38,989][257371] Min Reward on eval: -214.94637516215442[0m
[37m[1m[2023-07-17 03:45:38,989][257371] Mean Reward across all agents: 36.62625173026366[0m
[37m[1m[2023-07-17 03:45:38,989][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:45:38,992][257371] mean_value=-854.3869020447254, max_value=574.4399420765009[0m
[37m[1m[2023-07-17 03:45:38,994][257371] New mean coefficients: [[-2.013506   -1.6531366  -0.4233471   0.2739901   0.12308758  8.629831  ]][0m
[37m[1m[2023-07-17 03:45:38,995][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:45:48,068][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 03:45:48,068][257371] FPS: 423328.25[0m
[36m[2023-07-17 03:45:48,070][257371] itr=580, itrs=2000, Progress: 29.00%[0m
[37m[1m[2023-07-17 03:48:42,770][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000560[0m
[36m[2023-07-17 03:48:54,913][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 03:48:54,913][257371] FPS: 333054.63[0m
[36m[2023-07-17 03:48:59,118][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:48:59,118][257371] Reward + Measures: [[-85.44724992   0.5571323    0.54710495   0.58516461   0.11417333
    6.56990862]][0m
[37m[1m[2023-07-17 03:48:59,119][257371] Max Reward on eval: -85.44724991812097[0m
[37m[1m[2023-07-17 03:48:59,119][257371] Min Reward on eval: -85.44724991812097[0m
[37m[1m[2023-07-17 03:48:59,119][257371] Mean Reward across all agents: -85.44724991812097[0m
[37m[1m[2023-07-17 03:48:59,119][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:49:04,026][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:49:04,027][257371] Reward + Measures: [[-103.88265487    0.52890003    0.5194        0.4409        0.1279
     5.29938984]
 [ -23.55419132    0.45469999    0.34830001    0.3563        0.0559
     4.9074173 ]
 [  44.9832935     0.35759997    0.35089996    0.35600001    0.139
     5.11534405]
 ...
 [ -95.62325753    0.32179999    0.55849999    0.21329999    0.53190005
     3.67574859]
 [ 340.88598075    0.713         0.67370003    0.62410003    0.0316
     5.66980553]
 [ 241.77713082    0.50040001    0.49320003    0.49309999    0.0819
     5.65415525]][0m
[37m[1m[2023-07-17 03:49:04,027][257371] Max Reward on eval: 601.6826801101445[0m
[37m[1m[2023-07-17 03:49:04,027][257371] Min Reward on eval: -244.22131976541132[0m
[37m[1m[2023-07-17 03:49:04,027][257371] Mean Reward across all agents: 69.32533116538958[0m
[37m[1m[2023-07-17 03:49:04,028][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:49:04,030][257371] mean_value=-542.1553305685974, max_value=350.7228610950647[0m
[37m[1m[2023-07-17 03:49:04,032][257371] New mean coefficients: [[-2.0309365  -1.6367327  -1.3995082  -0.06315225  0.5197361   8.720589  ]][0m
[37m[1m[2023-07-17 03:49:04,033][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:49:12,941][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-17 03:49:12,941][257371] FPS: 431159.83[0m
[36m[2023-07-17 03:49:12,944][257371] itr=581, itrs=2000, Progress: 29.05%[0m
[36m[2023-07-17 03:49:24,713][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 03:49:24,714][257371] FPS: 328492.05[0m
[36m[2023-07-17 03:49:28,927][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:49:28,932][257371] Reward + Measures: [[-87.81039478   0.55919468   0.54976469   0.58925062   0.11711299
    6.58568048]][0m
[37m[1m[2023-07-17 03:49:28,933][257371] Max Reward on eval: -87.81039478102942[0m
[37m[1m[2023-07-17 03:49:28,933][257371] Min Reward on eval: -87.81039478102942[0m
[37m[1m[2023-07-17 03:49:28,933][257371] Mean Reward across all agents: -87.81039478102942[0m
[37m[1m[2023-07-17 03:49:28,933][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:49:33,871][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:49:33,871][257371] Reward + Measures: [[ 399.57448725    0.71390003    0.83220005    0.67980003    0.22870003
     5.88309669]
 [ -58.379937      0.21739998    0.1621        0.19930001    0.1486
     4.7941103 ]
 [-220.77880817    0.78780001    0.77060002    0.75510007    0.0269
     5.7130022 ]
 ...
 [  66.46207588    0.35160002    0.2412        0.18910001    0.10879999
     4.93945837]
 [  93.31002397    0.59279996    0.62279999    0.20640002    0.50560004
     5.38678074]
 [-157.95528359    0.8283        0.81730002    0.73849994    0.10520001
     5.00913334]][0m
[37m[1m[2023-07-17 03:49:33,872][257371] Max Reward on eval: 489.6593921072781[0m
[37m[1m[2023-07-17 03:49:33,872][257371] Min Reward on eval: -254.88700964041055[0m
[37m[1m[2023-07-17 03:49:33,872][257371] Mean Reward across all agents: 26.446114391476982[0m
[37m[1m[2023-07-17 03:49:33,872][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:49:33,875][257371] mean_value=-662.4108291841368, max_value=411.6248730451046[0m
[37m[1m[2023-07-17 03:49:33,878][257371] New mean coefficients: [[-1.611295   -1.9728206  -0.87153596 -0.08806652  1.0224186   8.699145  ]][0m
[37m[1m[2023-07-17 03:49:33,879][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:49:42,906][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 03:49:42,907][257371] FPS: 425458.68[0m
[36m[2023-07-17 03:49:42,909][257371] itr=582, itrs=2000, Progress: 29.10%[0m
[36m[2023-07-17 03:49:54,946][257371] train() took 11.95 seconds to complete[0m
[36m[2023-07-17 03:49:54,946][257371] FPS: 321206.80[0m
[36m[2023-07-17 03:49:59,254][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:49:59,255][257371] Reward + Measures: [[-90.72947764   0.55841362   0.5500803    0.58821595   0.12025099
    6.62861013]][0m
[37m[1m[2023-07-17 03:49:59,255][257371] Max Reward on eval: -90.72947764178036[0m
[37m[1m[2023-07-17 03:49:59,255][257371] Min Reward on eval: -90.72947764178036[0m
[37m[1m[2023-07-17 03:49:59,255][257371] Mean Reward across all agents: -90.72947764178036[0m
[37m[1m[2023-07-17 03:49:59,256][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:50:04,245][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:50:04,245][257371] Reward + Measures: [[  60.51484754    0.44329998    0.43490002    0.32300001    0.16690001
     5.48720646]
 [ -18.08330026    0.30399999    0.25070003    0.15210001    0.18050002
     4.54233027]
 [  52.50288852    0.70230001    0.67629999    0.66919994    0.0658
     6.39014196]
 ...
 [ 103.51318921    0.92430001    0.89209998    0.78590006    0.11940002
     5.16500282]
 [-123.8486884     0.52469999    0.52380008    0.53220004    0.0312
     5.13099957]
 [  23.77268219    0.84980005    0.83640003    0.82299995    0.0318
     5.70333242]][0m
[37m[1m[2023-07-17 03:50:04,245][257371] Max Reward on eval: 565.7590708721895[0m
[37m[1m[2023-07-17 03:50:04,246][257371] Min Reward on eval: -277.38615799564866[0m
[37m[1m[2023-07-17 03:50:04,246][257371] Mean Reward across all agents: 49.397453367681706[0m
[37m[1m[2023-07-17 03:50:04,246][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:50:04,249][257371] mean_value=-462.87274214903175, max_value=511.8067467357963[0m
[37m[1m[2023-07-17 03:50:04,251][257371] New mean coefficients: [[-2.727039  -2.0122683 -0.8271715 -0.5572288  1.2434607  9.0384865]][0m
[37m[1m[2023-07-17 03:50:04,252][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:50:13,167][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-17 03:50:13,167][257371] FPS: 430812.90[0m
[36m[2023-07-17 03:50:13,169][257371] itr=583, itrs=2000, Progress: 29.15%[0m
[36m[2023-07-17 03:50:24,961][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 03:50:24,962][257371] FPS: 327931.89[0m
[36m[2023-07-17 03:50:29,242][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:50:29,242][257371] Reward + Measures: [[-88.54934052   0.54770672   0.53708333   0.57776535   0.12358533
    6.65263987]][0m
[37m[1m[2023-07-17 03:50:29,242][257371] Max Reward on eval: -88.549340518779[0m
[37m[1m[2023-07-17 03:50:29,243][257371] Min Reward on eval: -88.549340518779[0m
[37m[1m[2023-07-17 03:50:29,243][257371] Mean Reward across all agents: -88.549340518779[0m
[37m[1m[2023-07-17 03:50:29,243][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:50:34,261][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:50:34,267][257371] Reward + Measures: [[ 23.25124435   0.13490002   0.12260001   0.11170001   0.1339
    3.91979909]
 [ 24.57725091   0.31780002   0.2309       0.2626       0.13759999
    4.33040953]
 [-13.02101958   0.45289999   0.3355       0.33510002   0.20120001
    4.9493804 ]
 ...
 [ 68.05470174   0.3073       0.22120002   0.24070001   0.1199
    4.4288702 ]
 [  2.66761606   0.28640002   0.21110001   0.1938       0.18520001
    4.61357641]
 [-36.41896969   0.3414       0.2509       0.26450002   0.1857
    4.8513813 ]][0m
[37m[1m[2023-07-17 03:50:34,267][257371] Max Reward on eval: 285.91049959603697[0m
[37m[1m[2023-07-17 03:50:34,267][257371] Min Reward on eval: -124.46378881947604[0m
[37m[1m[2023-07-17 03:50:34,268][257371] Mean Reward across all agents: 35.38616587311846[0m
[37m[1m[2023-07-17 03:50:34,268][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:50:34,270][257371] mean_value=-1018.93059767032, max_value=231.24681594333873[0m
[37m[1m[2023-07-17 03:50:34,272][257371] New mean coefficients: [[-1.596726   -1.8770674  -0.90286255 -0.2778075   1.767057    9.234077  ]][0m
[37m[1m[2023-07-17 03:50:34,273][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:50:43,309][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 03:50:43,309][257371] FPS: 425046.56[0m
[36m[2023-07-17 03:50:43,312][257371] itr=584, itrs=2000, Progress: 29.20%[0m
[36m[2023-07-17 03:50:54,974][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 03:50:54,975][257371] FPS: 331483.93[0m
[36m[2023-07-17 03:50:59,247][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:50:59,247][257371] Reward + Measures: [[-84.16999362   0.52557701   0.51619136   0.56329966   0.130925
    6.6484766 ]][0m
[37m[1m[2023-07-17 03:50:59,247][257371] Max Reward on eval: -84.16999361837087[0m
[37m[1m[2023-07-17 03:50:59,247][257371] Min Reward on eval: -84.16999361837087[0m
[37m[1m[2023-07-17 03:50:59,248][257371] Mean Reward across all agents: -84.16999361837087[0m
[37m[1m[2023-07-17 03:50:59,248][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:51:04,247][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:51:04,248][257371] Reward + Measures: [[ 224.18441999    0.7058        0.69450003    0.67230004    0.0541
     5.28658199]
 [ -60.35201758    0.58470005    0.57560003    0.59030002    0.0681
     6.22758341]
 [  13.74493045    0.24879999    0.40330002    0.21500002    0.40080005
     4.00539732]
 ...
 [ 136.87359819    0.46880004    0.46000001    0.44940001    0.0591
     4.59962654]
 [ -73.07129906    0.42269999    0.39739999    0.38400003    0.0538
     5.42965364]
 [-150.47289167    0.60039997    0.59450001    0.44309998    0.23220001
     6.20856428]][0m
[37m[1m[2023-07-17 03:51:04,248][257371] Max Reward on eval: 533.5837450234685[0m
[37m[1m[2023-07-17 03:51:04,248][257371] Min Reward on eval: -196.89046993795782[0m
[37m[1m[2023-07-17 03:51:04,249][257371] Mean Reward across all agents: 37.53791310770433[0m
[37m[1m[2023-07-17 03:51:04,249][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:51:04,251][257371] mean_value=-504.60708482204916, max_value=561.6710221825422[0m
[37m[1m[2023-07-17 03:51:04,253][257371] New mean coefficients: [[-1.9688447 -1.9865    -0.8350725 -1.5232748  1.3520131  8.974028 ]][0m
[37m[1m[2023-07-17 03:51:04,254][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:51:13,300][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 03:51:13,300][257371] FPS: 424555.51[0m
[36m[2023-07-17 03:51:13,303][257371] itr=585, itrs=2000, Progress: 29.25%[0m
[36m[2023-07-17 03:51:25,049][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 03:51:25,049][257371] FPS: 329145.72[0m
[36m[2023-07-17 03:51:29,370][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:51:29,371][257371] Reward + Measures: [[-87.54558097   0.54748362   0.53752863   0.58507198   0.12375433
    6.69407558]][0m
[37m[1m[2023-07-17 03:51:29,371][257371] Max Reward on eval: -87.5455809720237[0m
[37m[1m[2023-07-17 03:51:29,371][257371] Min Reward on eval: -87.5455809720237[0m
[37m[1m[2023-07-17 03:51:29,371][257371] Mean Reward across all agents: -87.5455809720237[0m
[37m[1m[2023-07-17 03:51:29,372][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:51:34,431][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:51:34,437][257371] Reward + Measures: [[ 23.00608734   0.77829999   0.79000002   0.72980005   0.06239999
    3.81275058]
 [-13.63965278   0.85360014   0.82950002   0.7938       0.0167
    6.71253681]
 [ 65.49310088   0.6814       0.87319994   0.66180003   0.31259999
    6.9497695 ]
 ...
 [215.94266749   0.75810003   0.71869999   0.69569999   0.0321
    4.76033878]
 [386.1452517    0.81549996   0.75419998   0.76480001   0.0228
    5.63849878]
 [211.4288118    0.56240004   0.54269999   0.51280004   0.0402
    6.22715759]][0m
[37m[1m[2023-07-17 03:51:34,437][257371] Max Reward on eval: 531.3853282947559[0m
[37m[1m[2023-07-17 03:51:34,438][257371] Min Reward on eval: -176.75033943839372[0m
[37m[1m[2023-07-17 03:51:34,438][257371] Mean Reward across all agents: 136.86922658748009[0m
[37m[1m[2023-07-17 03:51:34,438][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:51:34,441][257371] mean_value=-243.0640348096028, max_value=263.44019522220776[0m
[37m[1m[2023-07-17 03:51:34,444][257371] New mean coefficients: [[-2.2788556 -1.8484911 -0.5411897 -1.7426324  1.3305997  8.980353 ]][0m
[37m[1m[2023-07-17 03:51:34,445][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:51:43,604][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-17 03:51:43,605][257371] FPS: 419308.17[0m
[36m[2023-07-17 03:51:43,607][257371] itr=586, itrs=2000, Progress: 29.30%[0m
[36m[2023-07-17 03:51:55,569][257371] train() took 11.88 seconds to complete[0m
[36m[2023-07-17 03:51:55,569][257371] FPS: 323145.90[0m
[36m[2023-07-17 03:51:59,843][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:51:59,844][257371] Reward + Measures: [[-80.7830106    0.53264898   0.52483767   0.57445699   0.13248633
    6.713377  ]][0m
[37m[1m[2023-07-17 03:51:59,844][257371] Max Reward on eval: -80.7830106015684[0m
[37m[1m[2023-07-17 03:51:59,844][257371] Min Reward on eval: -80.7830106015684[0m
[37m[1m[2023-07-17 03:51:59,844][257371] Mean Reward across all agents: -80.7830106015684[0m
[37m[1m[2023-07-17 03:51:59,844][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:52:05,084][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:52:05,089][257371] Reward + Measures: [[   9.16823955    0.56809998    0.73209995    0.1366        0.66080004
     3.61997914]
 [   7.14626458    0.28239998    0.27710003    0.28980002    0.2608
     3.48745704]
 [  -1.64698925    0.35470003    0.45109996    0.26719999    0.45120001
     3.23574233]
 ...
 [-100.43141965    0.43690005    0.48820001    0.1776        0.4664
     3.81362319]
 [  -0.63534118    0.38249999    0.54619998    0.26910001    0.54080003
     3.42571115]
 [ -25.65635251    0.3822        0.58820003    0.25480002    0.52640003
     3.02343774]][0m
[37m[1m[2023-07-17 03:52:05,090][257371] Max Reward on eval: 259.7069756170502[0m
[37m[1m[2023-07-17 03:52:05,090][257371] Min Reward on eval: -358.3509459233319[0m
[37m[1m[2023-07-17 03:52:05,090][257371] Mean Reward across all agents: -12.594808400223156[0m
[37m[1m[2023-07-17 03:52:05,091][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:52:05,093][257371] mean_value=-867.5585695541685, max_value=264.27192839682664[0m
[37m[1m[2023-07-17 03:52:05,095][257371] New mean coefficients: [[-2.5744636  -2.4039345  -0.89978707 -1.8107134   0.81494164  8.9003725 ]][0m
[37m[1m[2023-07-17 03:52:05,096][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:52:14,096][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 03:52:14,096][257371] FPS: 426766.93[0m
[36m[2023-07-17 03:52:14,098][257371] itr=587, itrs=2000, Progress: 29.35%[0m
[36m[2023-07-17 03:52:25,702][257371] train() took 11.52 seconds to complete[0m
[36m[2023-07-17 03:52:25,702][257371] FPS: 333256.70[0m
[36m[2023-07-17 03:52:29,983][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:52:29,983][257371] Reward + Measures: [[-79.7154708    0.54321229   0.53380233   0.58196229   0.12811001
    6.74033928]][0m
[37m[1m[2023-07-17 03:52:29,984][257371] Max Reward on eval: -79.71547079509652[0m
[37m[1m[2023-07-17 03:52:29,984][257371] Min Reward on eval: -79.71547079509652[0m
[37m[1m[2023-07-17 03:52:29,984][257371] Mean Reward across all agents: -79.71547079509652[0m
[37m[1m[2023-07-17 03:52:29,984][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:52:35,002][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:52:35,007][257371] Reward + Measures: [[255.79018448   0.76169997   0.7313       0.71060008   0.0329
    6.06545258]
 [ 86.40702493   0.55779999   0.42000005   0.39429998   0.1552
    4.27127695]
 [ 83.5591937    0.60519999   0.63170004   0.52210003   0.1708
    5.22768354]
 ...
 [  7.93329746   0.24859999   0.15380001   0.15120001   0.1166
    4.80836058]
 [ 11.93248007   0.4179       0.36399999   0.28490004   0.34220001
    4.79460335]
 [ -4.44009798   0.47260004   0.26750001   0.31980002   0.18800001
    4.37173748]][0m
[37m[1m[2023-07-17 03:52:35,008][257371] Max Reward on eval: 398.27826600372794[0m
[37m[1m[2023-07-17 03:52:35,008][257371] Min Reward on eval: -238.3312711740844[0m
[37m[1m[2023-07-17 03:52:35,008][257371] Mean Reward across all agents: 37.45301526237946[0m
[37m[1m[2023-07-17 03:52:35,008][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:52:35,010][257371] mean_value=-436.84560805834553, max_value=769.8974438441917[0m
[37m[1m[2023-07-17 03:52:35,012][257371] New mean coefficients: [[-1.7193725  -2.297417   -1.3394094  -1.92085    -0.13035089  8.677886  ]][0m
[37m[1m[2023-07-17 03:52:35,013][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:52:44,056][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 03:52:44,056][257371] FPS: 424719.60[0m
[36m[2023-07-17 03:52:44,058][257371] itr=588, itrs=2000, Progress: 29.40%[0m
[36m[2023-07-17 03:52:55,861][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 03:52:55,861][257371] FPS: 327493.71[0m
[36m[2023-07-17 03:53:00,181][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:53:00,181][257371] Reward + Measures: [[-73.11753235   0.53794599   0.52976733   0.58033496   0.13012066
    6.77095318]][0m
[37m[1m[2023-07-17 03:53:00,182][257371] Max Reward on eval: -73.1175323450421[0m
[37m[1m[2023-07-17 03:53:00,182][257371] Min Reward on eval: -73.1175323450421[0m
[37m[1m[2023-07-17 03:53:00,182][257371] Mean Reward across all agents: -73.1175323450421[0m
[37m[1m[2023-07-17 03:53:00,183][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:53:05,232][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:53:05,233][257371] Reward + Measures: [[ -84.70207541    0.611         0.54750001    0.48790002    0.10649999
     4.66244888]
 [  20.49002918    0.51840001    0.51789999    0.39790002    0.289
     4.32394648]
 [-142.87614723    0.40920001    0.63239998    0.32899997    0.44499999
     4.78955698]
 ...
 [  51.55842016    0.47020003    0.78730005    0.58850002    0.4553
     5.38735771]
 [-152.75444742    0.54910004    0.7087        0.37929997    0.44030005
     4.6095891 ]
 [ -22.51344518    0.26040003    0.3398        0.1807        0.2527
     3.84899497]][0m
[37m[1m[2023-07-17 03:53:05,233][257371] Max Reward on eval: 360.4610331386328[0m
[37m[1m[2023-07-17 03:53:05,234][257371] Min Reward on eval: -316.95287893312053[0m
[37m[1m[2023-07-17 03:53:05,234][257371] Mean Reward across all agents: -48.24794731971757[0m
[37m[1m[2023-07-17 03:53:05,234][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:53:05,238][257371] mean_value=-475.24161013499463, max_value=273.85983374013813[0m
[37m[1m[2023-07-17 03:53:05,241][257371] New mean coefficients: [[-1.8054821  -1.9949566  -1.9219279  -2.3588517  -0.15626419  8.166393  ]][0m
[37m[1m[2023-07-17 03:53:05,242][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:53:14,293][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 03:53:14,293][257371] FPS: 424350.39[0m
[36m[2023-07-17 03:53:14,295][257371] itr=589, itrs=2000, Progress: 29.45%[0m
[36m[2023-07-17 03:53:26,044][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 03:53:26,044][257371] FPS: 329105.06[0m
[36m[2023-07-17 03:53:30,397][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:53:30,403][257371] Reward + Measures: [[-73.0376803    0.53853798   0.53090501   0.57882202   0.13449332
    6.78940487]][0m
[37m[1m[2023-07-17 03:53:30,403][257371] Max Reward on eval: -73.03768029891802[0m
[37m[1m[2023-07-17 03:53:30,403][257371] Min Reward on eval: -73.03768029891802[0m
[37m[1m[2023-07-17 03:53:30,404][257371] Mean Reward across all agents: -73.03768029891802[0m
[37m[1m[2023-07-17 03:53:30,404][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:53:35,434][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:53:35,440][257371] Reward + Measures: [[-14.1019934    0.368        0.75190008   0.41030002   0.74200004
    4.224617  ]
 [  3.69141743   0.35310003   0.6498       0.1557       0.5438
    4.12093306]
 [ -8.22274325   0.38349998   0.52329999   0.25970003   0.44580004
    4.05085993]
 ...
 [  4.55635486   0.41159996   0.60949993   0.1411       0.55789995
    4.38573408]
 [-14.50704314   0.51810002   0.76160002   0.0921       0.64280003
    5.17537451]
 [ 30.89232217   0.36939999   0.49130002   0.20920001   0.48590001
    3.63582802]][0m
[37m[1m[2023-07-17 03:53:35,440][257371] Max Reward on eval: 405.64060162520036[0m
[37m[1m[2023-07-17 03:53:35,440][257371] Min Reward on eval: -527.2519164392725[0m
[37m[1m[2023-07-17 03:53:35,441][257371] Mean Reward across all agents: -5.751213243810968[0m
[37m[1m[2023-07-17 03:53:35,441][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:53:35,448][257371] mean_value=-268.25915682773024, max_value=424.6345528929682[0m
[37m[1m[2023-07-17 03:53:35,451][257371] New mean coefficients: [[-2.4553125  -1.8407698  -1.623007   -2.5021992  -0.05340804  7.7898903 ]][0m
[37m[1m[2023-07-17 03:53:35,452][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:53:44,604][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-17 03:53:44,605][257371] FPS: 419610.81[0m
[36m[2023-07-17 03:53:44,607][257371] itr=590, itrs=2000, Progress: 29.50%[0m
[37m[1m[2023-07-17 03:56:38,579][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000570[0m
[36m[2023-07-17 03:56:51,073][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 03:56:51,073][257371] FPS: 325581.54[0m
[36m[2023-07-17 03:56:55,351][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:56:55,352][257371] Reward + Measures: [[-71.61081836   0.53035331   0.52145571   0.57583368   0.13355133
    6.80375099]][0m
[37m[1m[2023-07-17 03:56:55,352][257371] Max Reward on eval: -71.6108183649816[0m
[37m[1m[2023-07-17 03:56:55,352][257371] Min Reward on eval: -71.6108183649816[0m
[37m[1m[2023-07-17 03:56:55,352][257371] Mean Reward across all agents: -71.6108183649816[0m
[37m[1m[2023-07-17 03:56:55,353][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:57:00,287][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:57:00,293][257371] Reward + Measures: [[  15.61944321    0.50910002    0.57370007    0.44039997    0.51169997
     4.48026419]
 [ -30.87318004    0.34279999    0.44189999    0.23800002    0.3154
     4.545156  ]
 [   0.91179008    0.22299998    0.22979999    0.139         0.19880001
     3.57856345]
 ...
 [ 118.5952382     0.40549999    0.49360004    0.3813        0.21879999
     5.85289097]
 [  47.77711783    0.33980003    0.42950001    0.29820004    0.40629998
     3.33930969]
 [-175.24795842    0.64230001    0.68500006    0.1443        0.64250004
     4.07073355]][0m
[37m[1m[2023-07-17 03:57:00,294][257371] Max Reward on eval: 387.6530363151338[0m
[37m[1m[2023-07-17 03:57:00,294][257371] Min Reward on eval: -586.1869468690827[0m
[37m[1m[2023-07-17 03:57:00,295][257371] Mean Reward across all agents: -20.01765408239049[0m
[37m[1m[2023-07-17 03:57:00,295][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:57:00,303][257371] mean_value=-793.5964092672924, max_value=317.9746768122442[0m
[37m[1m[2023-07-17 03:57:00,307][257371] New mean coefficients: [[-2.4023874  -2.1467881  -0.8585528  -2.2865736   0.35609043  7.819052  ]][0m
[37m[1m[2023-07-17 03:57:00,308][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:57:09,349][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 03:57:09,349][257371] FPS: 424846.71[0m
[36m[2023-07-17 03:57:09,352][257371] itr=591, itrs=2000, Progress: 29.55%[0m
[36m[2023-07-17 03:57:21,126][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 03:57:21,126][257371] FPS: 328342.20[0m
[36m[2023-07-17 03:57:25,349][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:57:25,355][257371] Reward + Measures: [[-76.56922276   0.55061001   0.5419153    0.5898723    0.13259834
    6.85332727]][0m
[37m[1m[2023-07-17 03:57:25,355][257371] Max Reward on eval: -76.56922275587765[0m
[37m[1m[2023-07-17 03:57:25,355][257371] Min Reward on eval: -76.56922275587765[0m
[37m[1m[2023-07-17 03:57:25,355][257371] Mean Reward across all agents: -76.56922275587765[0m
[37m[1m[2023-07-17 03:57:25,356][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:57:30,258][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:57:30,259][257371] Reward + Measures: [[-100.6169921     0.54970002    0.54949999    0.48969999    0.16790001
     5.61468363]
 [   2.39817356    0.1391        0.13640001    0.11539999    0.1165
     4.54891253]
 [  20.74858837    0.25670001    0.21970001    0.20509999    0.1539
     3.96239924]
 ...
 [  15.45947044    0.52899998    0.56330007    0.1446        0.50330007
     5.19955015]
 [  26.83974199    0.1785        0.16419999    0.1375        0.1657
     4.87550306]
 [ 159.03806863    0.75089997    0.74690002    0.71320003    0.0403
     5.2518754 ]][0m
[37m[1m[2023-07-17 03:57:30,259][257371] Max Reward on eval: 496.47887062276715[0m
[37m[1m[2023-07-17 03:57:30,259][257371] Min Reward on eval: -222.8088708460331[0m
[37m[1m[2023-07-17 03:57:30,259][257371] Mean Reward across all agents: 5.521409237399415[0m
[37m[1m[2023-07-17 03:57:30,260][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:57:30,262][257371] mean_value=-692.3051715672108, max_value=323.4738221019255[0m
[37m[1m[2023-07-17 03:57:30,265][257371] New mean coefficients: [[-1.8830471  -1.7253569  -0.30002826 -2.4122708   0.03068912  7.046876  ]][0m
[37m[1m[2023-07-17 03:57:30,266][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:57:39,210][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 03:57:39,210][257371] FPS: 429400.51[0m
[36m[2023-07-17 03:57:39,212][257371] itr=592, itrs=2000, Progress: 29.60%[0m
[36m[2023-07-17 03:57:50,855][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 03:57:50,855][257371] FPS: 332072.56[0m
[36m[2023-07-17 03:57:55,077][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:57:55,077][257371] Reward + Measures: [[-76.08717437   0.52946103   0.52038127   0.57167804   0.136453
    6.82433319]][0m
[37m[1m[2023-07-17 03:57:55,077][257371] Max Reward on eval: -76.0871743722274[0m
[37m[1m[2023-07-17 03:57:55,078][257371] Min Reward on eval: -76.0871743722274[0m
[37m[1m[2023-07-17 03:57:55,078][257371] Mean Reward across all agents: -76.0871743722274[0m
[37m[1m[2023-07-17 03:57:55,078][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:58:00,024][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:58:00,025][257371] Reward + Measures: [[  96.19144348    0.93979996    0.93949997    0.9156        0.0204
     5.81316948]
 [ 261.27128077    0.72360003    0.68590003    0.61409998    0.0793
     5.55900288]
 [-186.3518753     0.59310001    0.66670001    0.29029998    0.50170004
     4.05425215]
 ...
 [ -72.01386962    0.1785        0.93809998    0.40889999    0.903
     4.19416142]
 [ 123.46082091    0.75439996    0.73990005    0.74110001    0.0515
     5.24559641]
 [ 318.0394907     0.82019997    0.83409995    0.71830004    0.1164
     5.47948694]][0m
[37m[1m[2023-07-17 03:58:00,025][257371] Max Reward on eval: 444.0745678086532[0m
[37m[1m[2023-07-17 03:58:00,025][257371] Min Reward on eval: -465.7325744311791[0m
[37m[1m[2023-07-17 03:58:00,026][257371] Mean Reward across all agents: 68.13866460718644[0m
[37m[1m[2023-07-17 03:58:00,026][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:58:00,031][257371] mean_value=-424.43088546749567, max_value=491.38346952723725[0m
[37m[1m[2023-07-17 03:58:00,034][257371] New mean coefficients: [[-1.5103159  -2.3258662   0.31079203 -1.9705329   0.02932886  6.825068  ]][0m
[37m[1m[2023-07-17 03:58:00,035][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:58:08,977][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 03:58:08,977][257371] FPS: 429511.04[0m
[36m[2023-07-17 03:58:08,979][257371] itr=593, itrs=2000, Progress: 29.65%[0m
[36m[2023-07-17 03:58:20,609][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 03:58:20,609][257371] FPS: 332543.83[0m
[36m[2023-07-17 03:58:24,880][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:58:24,881][257371] Reward + Measures: [[-71.9893824    0.52691966   0.51898265   0.57078469   0.13602634
    6.84911919]][0m
[37m[1m[2023-07-17 03:58:24,881][257371] Max Reward on eval: -71.98938239963073[0m
[37m[1m[2023-07-17 03:58:24,881][257371] Min Reward on eval: -71.98938239963073[0m
[37m[1m[2023-07-17 03:58:24,882][257371] Mean Reward across all agents: -71.98938239963073[0m
[37m[1m[2023-07-17 03:58:24,882][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:58:29,868][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:58:29,869][257371] Reward + Measures: [[-70.21978936   0.57770002   0.62290001   0.48719999   0.1382
    5.50921583]
 [-34.70370593   0.19360001   0.23769999   0.17410001   0.1717
    4.40692949]
 [-55.10913976   0.40740004   0.4224       0.3845       0.3955
    4.79873514]
 ...
 [-95.85096311   0.68839997   0.63679999   0.5722       0.1365
    4.09883356]
 [ -4.39665709   0.24390002   0.24089999   0.15320002   0.1832
    3.4339962 ]
 [-11.52219897   0.15840001   0.1506       0.11669999   0.1207
    3.28002024]][0m
[37m[1m[2023-07-17 03:58:29,869][257371] Max Reward on eval: 185.37749712634832[0m
[37m[1m[2023-07-17 03:58:29,869][257371] Min Reward on eval: -298.732953434065[0m
[37m[1m[2023-07-17 03:58:29,869][257371] Mean Reward across all agents: -50.872077885641026[0m
[37m[1m[2023-07-17 03:58:29,870][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:58:29,871][257371] mean_value=-1794.977914794273, max_value=222.87400466981543[0m
[37m[1m[2023-07-17 03:58:29,874][257371] New mean coefficients: [[-1.0183268  -1.9204665   0.8618     -1.9205033  -0.08039051  6.588951  ]][0m
[37m[1m[2023-07-17 03:58:29,875][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:58:38,877][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 03:58:38,878][257371] FPS: 426602.35[0m
[36m[2023-07-17 03:58:38,880][257371] itr=594, itrs=2000, Progress: 29.70%[0m
[36m[2023-07-17 03:58:50,501][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 03:58:50,501][257371] FPS: 332694.31[0m
[36m[2023-07-17 03:58:54,732][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:58:54,733][257371] Reward + Measures: [[-73.28295911   0.52715266   0.51851535   0.56837898   0.13496634
    6.84881783]][0m
[37m[1m[2023-07-17 03:58:54,733][257371] Max Reward on eval: -73.2829591139437[0m
[37m[1m[2023-07-17 03:58:54,733][257371] Min Reward on eval: -73.2829591139437[0m
[37m[1m[2023-07-17 03:58:54,733][257371] Mean Reward across all agents: -73.2829591139437[0m
[37m[1m[2023-07-17 03:58:54,734][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:58:59,908][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:58:59,909][257371] Reward + Measures: [[-86.83518379   0.1573       0.18810001   0.07970001   0.16620001
    4.39525175]
 [ 21.42634429   0.35140002   0.34109998   0.35609999   0.064
    5.82865286]
 [138.36461083   0.31200001   0.31259999   0.31189999   0.0932
    5.73363066]
 ...
 [-69.49821039   0.34779999   0.34020001   0.33389997   0.0495
    6.72412109]
 [ 78.55451507   0.69410008   0.69200003   0.4962       0.2218
    6.76228046]
 [ 31.59590638   0.52039999   0.48619995   0.50999999   0.0533
    5.78700113]][0m
[37m[1m[2023-07-17 03:58:59,909][257371] Max Reward on eval: 463.5921592184808[0m
[37m[1m[2023-07-17 03:58:59,909][257371] Min Reward on eval: -344.58126875124873[0m
[37m[1m[2023-07-17 03:58:59,909][257371] Mean Reward across all agents: 15.436165075828809[0m
[37m[1m[2023-07-17 03:58:59,910][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:58:59,915][257371] mean_value=-248.7725048237264, max_value=712.289589446148[0m
[37m[1m[2023-07-17 03:58:59,918][257371] New mean coefficients: [[-1.4841197  -2.789156    0.25070882 -1.3770835   0.6819225   7.579396  ]][0m
[37m[1m[2023-07-17 03:58:59,919][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:59:08,948][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 03:59:08,948][257371] FPS: 425379.75[0m
[36m[2023-07-17 03:59:08,950][257371] itr=595, itrs=2000, Progress: 29.75%[0m
[36m[2023-07-17 03:59:20,910][257371] train() took 11.88 seconds to complete[0m
[36m[2023-07-17 03:59:20,910][257371] FPS: 323209.12[0m
[36m[2023-07-17 03:59:25,160][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:59:25,160][257371] Reward + Measures: [[-78.13079293   0.54091495   0.5325973    0.58527261   0.13508201
    6.88985157]][0m
[37m[1m[2023-07-17 03:59:25,160][257371] Max Reward on eval: -78.13079293271721[0m
[37m[1m[2023-07-17 03:59:25,161][257371] Min Reward on eval: -78.13079293271721[0m
[37m[1m[2023-07-17 03:59:25,161][257371] Mean Reward across all agents: -78.13079293271721[0m
[37m[1m[2023-07-17 03:59:25,161][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:59:30,102][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:59:30,102][257371] Reward + Measures: [[-137.7132871     0.52539998    0.55980009    0.41729999    0.27250001
     5.27701807]
 [-132.15489721    0.4443        0.47910005    0.42600003    0.1375
     5.7034421 ]
 [ -19.06623431    0.24150001    0.29239997    0.10569999    0.29680002
     4.2824235 ]
 ...
 [  18.60669168    0.55070001    0.69449997    0.23959999    0.55849999
     5.60282612]
 [ -63.41167498    0.33810002    0.38820001    0.24360001    0.18820001
     5.05278206]
 [ -27.24201263    0.4276        0.48240003    0.33989999    0.18529999
     4.4379859 ]][0m
[37m[1m[2023-07-17 03:59:30,102][257371] Max Reward on eval: 373.04240870340726[0m
[37m[1m[2023-07-17 03:59:30,103][257371] Min Reward on eval: -261.5789229909424[0m
[37m[1m[2023-07-17 03:59:30,103][257371] Mean Reward across all agents: 23.129547932300678[0m
[37m[1m[2023-07-17 03:59:30,103][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 03:59:30,107][257371] mean_value=-876.5406133340654, max_value=303.5282467730137[0m
[37m[1m[2023-07-17 03:59:30,110][257371] New mean coefficients: [[-1.9045651  -1.8018506   0.49133432 -2.1623456  -0.2372486   7.0324373 ]][0m
[37m[1m[2023-07-17 03:59:30,111][257371] Moving the mean solution point...[0m
[36m[2023-07-17 03:59:39,134][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 03:59:39,134][257371] FPS: 425644.27[0m
[36m[2023-07-17 03:59:39,137][257371] itr=596, itrs=2000, Progress: 29.80%[0m
[36m[2023-07-17 03:59:50,786][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 03:59:50,786][257371] FPS: 331923.11[0m
[36m[2023-07-17 03:59:55,045][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 03:59:55,045][257371] Reward + Measures: [[-75.01226536   0.56566435   0.55924332   0.60290599   0.12888733
    6.92765045]][0m
[37m[1m[2023-07-17 03:59:55,045][257371] Max Reward on eval: -75.01226535632969[0m
[37m[1m[2023-07-17 03:59:55,045][257371] Min Reward on eval: -75.01226535632969[0m
[37m[1m[2023-07-17 03:59:55,046][257371] Mean Reward across all agents: -75.01226535632969[0m
[37m[1m[2023-07-17 03:59:55,046][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:00:00,097][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:00:00,108][257371] Reward + Measures: [[-20.33767184   0.4355       0.4743       0.33930001   0.278
    3.29309511]
 [ -6.04442985   0.34120002   0.3811       0.32209998   0.28150001
    3.84866023]
 [101.27458409   0.45829996   0.50099999   0.49810001   0.10829999
    5.64569521]
 ...
 [ 15.24915848   0.29980001   0.30440003   0.25780001   0.3263
    2.92998242]
 [364.69132758   0.53479999   0.61280006   0.1675       0.66670001
    4.23491144]
 [-21.73879715   0.56129998   0.54269999   0.59200001   0.0865
    4.8381958 ]][0m
[37m[1m[2023-07-17 04:00:00,109][257371] Max Reward on eval: 449.29252511332163[0m
[37m[1m[2023-07-17 04:00:00,109][257371] Min Reward on eval: -132.8988186642062[0m
[37m[1m[2023-07-17 04:00:00,109][257371] Mean Reward across all agents: 13.137813781351136[0m
[37m[1m[2023-07-17 04:00:00,109][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:00:00,112][257371] mean_value=-1874.678602293494, max_value=295.30497019183383[0m
[37m[1m[2023-07-17 04:00:00,114][257371] New mean coefficients: [[-2.4128413 -1.043823   0.4157409 -2.6856492  0.7396715  6.2296   ]][0m
[37m[1m[2023-07-17 04:00:00,115][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:00:09,126][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 04:00:09,126][257371] FPS: 426246.47[0m
[36m[2023-07-17 04:00:09,129][257371] itr=597, itrs=2000, Progress: 29.85%[0m
[36m[2023-07-17 04:00:20,882][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 04:00:20,882][257371] FPS: 328974.51[0m
[36m[2023-07-17 04:00:25,208][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:00:25,209][257371] Reward + Measures: [[-80.14881581   0.54182565   0.53317064   0.58308196   0.12973499
    6.88683891]][0m
[37m[1m[2023-07-17 04:00:25,209][257371] Max Reward on eval: -80.14881581387841[0m
[37m[1m[2023-07-17 04:00:25,209][257371] Min Reward on eval: -80.14881581387841[0m
[37m[1m[2023-07-17 04:00:25,210][257371] Mean Reward across all agents: -80.14881581387841[0m
[37m[1m[2023-07-17 04:00:25,210][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:00:30,204][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:00:30,205][257371] Reward + Measures: [[  -8.46008845    0.28640002    0.29729998    0.28149998    0.22670002
     4.21553564]
 [  58.04409036    0.30690002    0.52679998    0.37300003    0.30920002
     5.77364492]
 [  57.87742375    0.37779999    0.47840005    0.30969998    0.37759998
     3.02040672]
 ...
 [ -33.14481424    0.33250001    0.34400001    0.25000003    0.23650001
     4.32136679]
 [-129.96857924    0.49740002    0.63519996    0.19600001    0.61720002
     4.35468674]
 [ -26.00676256    0.30270001    0.3563        0.31459999    0.25600001
     3.51069951]][0m
[37m[1m[2023-07-17 04:00:30,205][257371] Max Reward on eval: 392.19828293339816[0m
[37m[1m[2023-07-17 04:00:30,205][257371] Min Reward on eval: -165.6419764255639[0m
[37m[1m[2023-07-17 04:00:30,206][257371] Mean Reward across all agents: 1.6030161783421983[0m
[37m[1m[2023-07-17 04:00:30,206][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:00:30,208][257371] mean_value=-1557.5479283816787, max_value=272.23325275425145[0m
[37m[1m[2023-07-17 04:00:30,211][257371] New mean coefficients: [[-1.9889221  -0.18995523  0.16842306 -2.253146    1.6554573   6.184534  ]][0m
[37m[1m[2023-07-17 04:00:30,216][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:00:39,374][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-17 04:00:39,374][257371] FPS: 419369.66[0m
[36m[2023-07-17 04:00:39,376][257371] itr=598, itrs=2000, Progress: 29.90%[0m
[36m[2023-07-17 04:00:51,280][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 04:00:51,280][257371] FPS: 324830.33[0m
[36m[2023-07-17 04:00:55,668][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:00:55,668][257371] Reward + Measures: [[-81.81212835   0.55315733   0.54739529   0.5920043    0.13029334
    6.91074944]][0m
[37m[1m[2023-07-17 04:00:55,669][257371] Max Reward on eval: -81.81212834936147[0m
[37m[1m[2023-07-17 04:00:55,669][257371] Min Reward on eval: -81.81212834936147[0m
[37m[1m[2023-07-17 04:00:55,669][257371] Mean Reward across all agents: -81.81212834936147[0m
[37m[1m[2023-07-17 04:00:55,669][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:01:00,708][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:01:00,709][257371] Reward + Measures: [[363.45985876   0.54579997   0.52220005   0.53840005   0.0763
    5.73583937]
 [-14.69769177   0.26799998   0.28430003   0.21700001   0.1732
    3.21087909]
 [ 15.80284727   0.1275       0.11369999   0.1112       0.0926
    3.93071103]
 ...
 [  7.95344564   0.52390003   0.52780002   0.33679998   0.2392
    5.98515272]
 [ 46.124811     0.72479999   0.73320001   0.5212       0.25009999
    5.7964282 ]
 [ 47.80569003   0.67370003   0.69130003   0.57050008   0.1073
    5.93927002]][0m
[37m[1m[2023-07-17 04:01:00,709][257371] Max Reward on eval: 685.5594329972752[0m
[37m[1m[2023-07-17 04:01:00,710][257371] Min Reward on eval: -215.32175444699823[0m
[37m[1m[2023-07-17 04:01:00,710][257371] Mean Reward across all agents: 74.92618413161314[0m
[37m[1m[2023-07-17 04:01:00,710][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:01:00,715][257371] mean_value=-955.7758195671593, max_value=650.9588318970837[0m
[37m[1m[2023-07-17 04:01:00,718][257371] New mean coefficients: [[-2.499339  -1.4421647  1.029989  -1.9915354  1.1265     6.3719144]][0m
[37m[1m[2023-07-17 04:01:00,719][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:01:09,882][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-17 04:01:09,882][257371] FPS: 419158.26[0m
[36m[2023-07-17 04:01:09,884][257371] itr=599, itrs=2000, Progress: 29.95%[0m
[36m[2023-07-17 04:01:21,865][257371] train() took 11.90 seconds to complete[0m
[36m[2023-07-17 04:01:21,865][257371] FPS: 322748.46[0m
[36m[2023-07-17 04:01:26,156][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:01:26,157][257371] Reward + Measures: [[-74.80265597   0.5378263    0.53375262   0.57877797   0.13675566
    6.8991003 ]][0m
[37m[1m[2023-07-17 04:01:26,157][257371] Max Reward on eval: -74.8026559699215[0m
[37m[1m[2023-07-17 04:01:26,157][257371] Min Reward on eval: -74.8026559699215[0m
[37m[1m[2023-07-17 04:01:26,157][257371] Mean Reward across all agents: -74.8026559699215[0m
[37m[1m[2023-07-17 04:01:26,158][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:01:31,441][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:01:31,442][257371] Reward + Measures: [[247.34284017   0.77759999   0.85650009   0.1822       0.79879999
    4.96891499]
 [ 43.28775689   0.3378       0.36750004   0.1286       0.4059
    4.33134317]
 [-10.25318747   0.25619999   0.36789998   0.16849999   0.3493
    4.34219742]
 ...
 [-13.77028304   0.1521       0.16410001   0.1242       0.1629
    4.24285269]
 [ 48.13659295   0.39040002   0.41669998   0.2062       0.40769997
    3.51340461]
 [380.77834704   0.76859999   0.90189999   0.1173       0.86590004
    5.39813614]][0m
[37m[1m[2023-07-17 04:01:31,443][257371] Max Reward on eval: 421.85450550578537[0m
[37m[1m[2023-07-17 04:01:31,443][257371] Min Reward on eval: -271.3552703542635[0m
[37m[1m[2023-07-17 04:01:31,443][257371] Mean Reward across all agents: 50.888358090173874[0m
[37m[1m[2023-07-17 04:01:31,443][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:01:31,448][257371] mean_value=-1067.0319429081371, max_value=351.94261718064956[0m
[37m[1m[2023-07-17 04:01:31,451][257371] New mean coefficients: [[-1.2620996 -1.4250638  1.1776526 -1.6059778  2.5048335  7.2932096]][0m
[37m[1m[2023-07-17 04:01:31,452][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:01:40,525][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 04:01:40,526][257371] FPS: 423274.36[0m
[36m[2023-07-17 04:01:40,528][257371] itr=600, itrs=2000, Progress: 30.00%[0m
[37m[1m[2023-07-17 04:04:33,404][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000580[0m
[36m[2023-07-17 04:04:45,674][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 04:04:45,674][257371] FPS: 330093.08[0m
[36m[2023-07-17 04:04:49,924][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:04:49,925][257371] Reward + Measures: [[-81.06307481   0.55537498   0.54826599   0.59662962   0.139835
    6.94765997]][0m
[37m[1m[2023-07-17 04:04:49,925][257371] Max Reward on eval: -81.06307480907724[0m
[37m[1m[2023-07-17 04:04:49,925][257371] Min Reward on eval: -81.06307480907724[0m
[37m[1m[2023-07-17 04:04:49,925][257371] Mean Reward across all agents: -81.06307480907724[0m
[37m[1m[2023-07-17 04:04:49,926][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:04:54,864][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:04:54,864][257371] Reward + Measures: [[-209.26080971    0.59310001    0.61970001    0.52649999    0.1245
     7.10421848]
 [-165.70228004    0.86510003    0.95469999    0.1754        0.78960007
     6.83121204]
 [  59.50597787    0.82660002    0.69690001    0.70020002    0.0278
     4.76057911]
 ...
 [  87.49198385    0.60140002    0.59440005    0.61940002    0.0545
     6.61411428]
 [-192.27255175    0.61899996    0.75360006    0.52520001    0.3373
     5.96251822]
 [-263.05738367    0.83109999    0.84559995    0.4542        0.42969999
     6.23123789]][0m
[37m[1m[2023-07-17 04:04:54,864][257371] Max Reward on eval: 460.22082524317665[0m
[37m[1m[2023-07-17 04:04:54,865][257371] Min Reward on eval: -580.9974513130495[0m
[37m[1m[2023-07-17 04:04:54,865][257371] Mean Reward across all agents: -5.311356878191699[0m
[37m[1m[2023-07-17 04:04:54,865][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:04:54,871][257371] mean_value=-243.40837364934083, max_value=687.4660183161029[0m
[37m[1m[2023-07-17 04:04:54,873][257371] New mean coefficients: [[-2.2713184 -1.6259649  1.3357882 -1.6700553  2.746584   7.2624946]][0m
[37m[1m[2023-07-17 04:04:54,874][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:05:03,811][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 04:05:03,811][257371] FPS: 429787.89[0m
[36m[2023-07-17 04:05:03,813][257371] itr=601, itrs=2000, Progress: 30.05%[0m
[36m[2023-07-17 04:05:15,707][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 04:05:15,707][257371] FPS: 324976.83[0m
[36m[2023-07-17 04:05:19,983][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:05:19,983][257371] Reward + Measures: [[-89.06839822   0.56767869   0.56056798   0.607086     0.13674934
    6.96631908]][0m
[37m[1m[2023-07-17 04:05:19,983][257371] Max Reward on eval: -89.06839822437925[0m
[37m[1m[2023-07-17 04:05:19,984][257371] Min Reward on eval: -89.06839822437925[0m
[37m[1m[2023-07-17 04:05:19,984][257371] Mean Reward across all agents: -89.06839822437925[0m
[37m[1m[2023-07-17 04:05:19,984][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:05:25,149][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:05:25,150][257371] Reward + Measures: [[-103.9958822     0.4745        0.51470006    0.39340001    0.19770001
     6.00136805]
 [-122.93560841    0.49060002    0.84920007    0.2086        0.77829999
     5.46703815]
 [ -29.54264058    0.51140004    0.49130002    0.49630004    0.07210001
     6.50758696]
 ...
 [ -48.50383909    0.70700002    0.81840003    0.21790002    0.68970001
     6.38419104]
 [-130.94302273    0.35399997    0.66619998    0.0673        0.69990003
     5.93240309]
 [-145.56881581    0.46689996    0.53039998    0.27110001    0.39250001
     3.95262909]][0m
[37m[1m[2023-07-17 04:05:25,150][257371] Max Reward on eval: 157.94413209315388[0m
[37m[1m[2023-07-17 04:05:25,150][257371] Min Reward on eval: -599.9117012288422[0m
[37m[1m[2023-07-17 04:05:25,150][257371] Mean Reward across all agents: -94.55538211086736[0m
[37m[1m[2023-07-17 04:05:25,151][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:05:25,156][257371] mean_value=-260.92716288637394, max_value=348.60951303809884[0m
[37m[1m[2023-07-17 04:05:25,159][257371] New mean coefficients: [[-2.1203666 -2.2914433  0.9823122 -1.5718334  2.7620387  8.242239 ]][0m
[37m[1m[2023-07-17 04:05:25,160][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:05:34,187][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 04:05:34,188][257371] FPS: 425460.86[0m
[36m[2023-07-17 04:05:34,190][257371] itr=602, itrs=2000, Progress: 30.10%[0m
[36m[2023-07-17 04:05:45,866][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 04:05:45,867][257371] FPS: 331235.11[0m
[36m[2023-07-17 04:05:50,098][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:05:50,099][257371] Reward + Measures: [[-84.08083667   0.55647767   0.55482531   0.59934199   0.13723333
    6.96340466]][0m
[37m[1m[2023-07-17 04:05:50,099][257371] Max Reward on eval: -84.08083666632429[0m
[37m[1m[2023-07-17 04:05:50,099][257371] Min Reward on eval: -84.08083666632429[0m
[37m[1m[2023-07-17 04:05:50,100][257371] Mean Reward across all agents: -84.08083666632429[0m
[37m[1m[2023-07-17 04:05:50,100][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:05:55,052][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:05:55,053][257371] Reward + Measures: [[ -6.39605589   0.2172       0.1876       0.2084       0.17050001
    3.97003245]
 [ 34.46362296   0.16430001   0.31490001   0.18870001   0.26980001
    4.69661331]
 [-25.88316625   0.23790002   0.59110004   0.4059       0.56720006
    4.96172094]
 ...
 [ 25.5032942    0.2277       0.23440002   0.1567       0.1803
    4.490448  ]
 [ 25.24528357   0.60110003   0.63280004   0.51310003   0.24029998
    3.37395453]
 [ 26.32387868   0.28340003   0.4619       0.36620003   0.33160001
    4.26892805]][0m
[37m[1m[2023-07-17 04:05:55,053][257371] Max Reward on eval: 211.2172944365535[0m
[37m[1m[2023-07-17 04:05:55,053][257371] Min Reward on eval: -167.50806140145286[0m
[37m[1m[2023-07-17 04:05:55,053][257371] Mean Reward across all agents: 27.14897529645373[0m
[37m[1m[2023-07-17 04:05:55,054][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:05:55,056][257371] mean_value=-1708.7261276236038, max_value=577.1265040893574[0m
[37m[1m[2023-07-17 04:05:55,059][257371] New mean coefficients: [[-1.6580455 -1.5143592  0.6221624 -1.1013952  2.7635329  8.34454  ]][0m
[37m[1m[2023-07-17 04:05:55,060][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:06:04,013][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 04:06:04,013][257371] FPS: 428984.44[0m
[36m[2023-07-17 04:06:04,015][257371] itr=603, itrs=2000, Progress: 30.15%[0m
[36m[2023-07-17 04:06:15,991][257371] train() took 11.89 seconds to complete[0m
[36m[2023-07-17 04:06:15,991][257371] FPS: 322983.95[0m
[36m[2023-07-17 04:06:20,238][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:06:20,239][257371] Reward + Measures: [[-88.45113689   0.55478036   0.55280632   0.60161132   0.141243
    6.98456001]][0m
[37m[1m[2023-07-17 04:06:20,239][257371] Max Reward on eval: -88.45113689098952[0m
[37m[1m[2023-07-17 04:06:20,239][257371] Min Reward on eval: -88.45113689098952[0m
[37m[1m[2023-07-17 04:06:20,239][257371] Mean Reward across all agents: -88.45113689098952[0m
[37m[1m[2023-07-17 04:06:20,240][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:06:25,230][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:06:25,230][257371] Reward + Measures: [[-28.87210448   0.1295       0.0957       0.10420001   0.08050001
    4.58106565]
 [ -5.55700431   0.45110002   0.48010001   0.3937       0.38070002
    3.99601483]
 [-13.2201028    0.21750002   0.21040002   0.1847       0.1708
    4.51989079]
 ...
 [-29.45322472   0.21620002   0.2093       0.1741       0.1847
    4.34481382]
 [  6.03695442   0.22919999   0.22160001   0.1743       0.18810001
    4.59057283]
 [-22.06326544   0.21640001   0.18550001   0.15710001   0.21230002
    4.77857304]][0m
[37m[1m[2023-07-17 04:06:25,231][257371] Max Reward on eval: 426.42183635868133[0m
[37m[1m[2023-07-17 04:06:25,231][257371] Min Reward on eval: -151.66837928332387[0m
[37m[1m[2023-07-17 04:06:25,231][257371] Mean Reward across all agents: 1.7377084813788535[0m
[37m[1m[2023-07-17 04:06:25,231][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:06:25,233][257371] mean_value=-696.6989861078428, max_value=311.7622715780572[0m
[37m[1m[2023-07-17 04:06:25,236][257371] New mean coefficients: [[-0.70819926 -2.5780613   1.2858961  -0.18568861  1.5208591   9.017491  ]][0m
[37m[1m[2023-07-17 04:06:25,237][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:06:34,229][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 04:06:34,230][257371] FPS: 427085.17[0m
[36m[2023-07-17 04:06:34,232][257371] itr=604, itrs=2000, Progress: 30.20%[0m
[36m[2023-07-17 04:06:46,160][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-17 04:06:46,160][257371] FPS: 324215.65[0m
[36m[2023-07-17 04:06:50,428][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:06:50,428][257371] Reward + Measures: [[-77.56141016   0.54499334   0.54257202   0.59354466   0.14789332
    6.98670244]][0m
[37m[1m[2023-07-17 04:06:50,429][257371] Max Reward on eval: -77.56141016111604[0m
[37m[1m[2023-07-17 04:06:50,429][257371] Min Reward on eval: -77.56141016111604[0m
[37m[1m[2023-07-17 04:06:50,429][257371] Mean Reward across all agents: -77.56141016111604[0m
[37m[1m[2023-07-17 04:06:50,430][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:06:55,458][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:06:55,458][257371] Reward + Measures: [[  64.96922754    0.2313        0.20220001    0.19690001    0.20390001
     3.69241333]
 [ -37.36321708    0.35020003    0.32689998    0.29949999    0.0905
     3.83381438]
 [-159.74574016    0.87039995    0.87720007    0.84619999    0.05950001
     6.49610901]
 ...
 [  14.46880889    0.45680004    0.4474        0.26580003    0.30250001
     4.25225163]
 [-125.00751995    0.54960006    0.59209996    0.51910001    0.1204
     5.40535212]
 [-142.60529159    0.77110004    0.7737        0.6142        0.20869999
     5.62710047]][0m
[37m[1m[2023-07-17 04:06:55,459][257371] Max Reward on eval: 243.7102802079171[0m
[37m[1m[2023-07-17 04:06:55,459][257371] Min Reward on eval: -197.3911609513685[0m
[37m[1m[2023-07-17 04:06:55,459][257371] Mean Reward across all agents: -29.117150775916645[0m
[37m[1m[2023-07-17 04:06:55,459][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:06:55,461][257371] mean_value=-1488.5090449249503, max_value=28.993488829272337[0m
[37m[1m[2023-07-17 04:06:55,464][257371] New mean coefficients: [[-1.3626254  -2.2915392   1.1657504  -0.19801351  1.565826    8.859274  ]][0m
[37m[1m[2023-07-17 04:06:55,465][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:07:04,435][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 04:07:04,436][257371] FPS: 428153.77[0m
[36m[2023-07-17 04:07:04,438][257371] itr=605, itrs=2000, Progress: 30.25%[0m
[36m[2023-07-17 04:07:16,488][257371] train() took 11.97 seconds to complete[0m
[36m[2023-07-17 04:07:16,489][257371] FPS: 320868.25[0m
[36m[2023-07-17 04:07:20,753][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:07:20,753][257371] Reward + Measures: [[-82.75764163   0.56951165   0.56753469   0.61118931   0.14170767
    7.03199148]][0m
[37m[1m[2023-07-17 04:07:20,753][257371] Max Reward on eval: -82.75764162729297[0m
[37m[1m[2023-07-17 04:07:20,753][257371] Min Reward on eval: -82.75764162729297[0m
[37m[1m[2023-07-17 04:07:20,754][257371] Mean Reward across all agents: -82.75764162729297[0m
[37m[1m[2023-07-17 04:07:20,754][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:07:25,751][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:07:25,751][257371] Reward + Measures: [[200.46386568   0.41010004   0.81510001   0.54260004   0.5352
    6.58109426]
 [-77.63178086   0.20560001   0.85570002   0.40570003   0.78540003
    5.58636236]
 [  0.13178243   0.48719999   0.46449995   0.44280002   0.13839999
    4.1274066 ]
 ...
 [ 81.51908483   0.66330004   0.76400006   0.71149999   0.1433
    6.09506369]
 [-44.59525007   0.3163       0.31580001   0.3427       0.0477
    4.16164827]
 [307.91033931   0.71689999   0.86180001   0.0716       0.81809998
    4.19751596]][0m
[37m[1m[2023-07-17 04:07:25,752][257371] Max Reward on eval: 509.25206660453694[0m
[37m[1m[2023-07-17 04:07:25,752][257371] Min Reward on eval: -396.2262482601218[0m
[37m[1m[2023-07-17 04:07:25,752][257371] Mean Reward across all agents: 40.464190609466705[0m
[37m[1m[2023-07-17 04:07:25,752][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:07:25,758][257371] mean_value=-1101.3155426562398, max_value=735.1017113514711[0m
[37m[1m[2023-07-17 04:07:25,760][257371] New mean coefficients: [[-1.2581478  -1.7632835   2.0523305  -0.02932489  0.604975    8.603864  ]][0m
[37m[1m[2023-07-17 04:07:25,761][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:07:34,815][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 04:07:34,816][257371] FPS: 424193.87[0m
[36m[2023-07-17 04:07:34,818][257371] itr=606, itrs=2000, Progress: 30.30%[0m
[36m[2023-07-17 04:07:46,488][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 04:07:46,489][257371] FPS: 331408.13[0m
[36m[2023-07-17 04:07:50,765][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:07:50,766][257371] Reward + Measures: [[-83.89682224   0.58519936   0.58040434   0.62570769   0.13043301
    7.05907917]][0m
[37m[1m[2023-07-17 04:07:50,766][257371] Max Reward on eval: -83.8968222389828[0m
[37m[1m[2023-07-17 04:07:50,766][257371] Min Reward on eval: -83.8968222389828[0m
[37m[1m[2023-07-17 04:07:50,766][257371] Mean Reward across all agents: -83.8968222389828[0m
[37m[1m[2023-07-17 04:07:50,767][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:07:56,046][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:07:56,046][257371] Reward + Measures: [[ 50.00929677   0.3637       0.53899997   0.34310004   0.3283
    3.47041321]
 [-40.39742293   0.27739999   0.19850001   0.2323       0.1166
    5.45970917]
 [  9.67285941   0.1868       0.244        0.2142       0.2067
    4.30074549]
 ...
 [-24.73115149   0.25639999   0.28660002   0.22570001   0.26290002
    3.72512817]
 [-62.77955557   0.17839999   0.18889999   0.14839999   0.1481
    4.13850164]
 [ 38.00348523   0.67799997   0.69389999   0.58470005   0.206
    3.99274373]][0m
[37m[1m[2023-07-17 04:07:56,046][257371] Max Reward on eval: 543.4833913002163[0m
[37m[1m[2023-07-17 04:07:56,047][257371] Min Reward on eval: -244.59016322707757[0m
[37m[1m[2023-07-17 04:07:56,047][257371] Mean Reward across all agents: 26.687472722041992[0m
[37m[1m[2023-07-17 04:07:56,047][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:07:56,049][257371] mean_value=-1475.1838231386466, max_value=275.41114650356786[0m
[37m[1m[2023-07-17 04:07:56,052][257371] New mean coefficients: [[-2.2689657  -0.21636033  1.4448442  -0.85801065  0.54899895  7.886917  ]][0m
[37m[1m[2023-07-17 04:07:56,053][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:08:05,069][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 04:08:05,069][257371] FPS: 425968.29[0m
[36m[2023-07-17 04:08:05,072][257371] itr=607, itrs=2000, Progress: 30.35%[0m
[36m[2023-07-17 04:08:16,861][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 04:08:16,861][257371] FPS: 328039.44[0m
[36m[2023-07-17 04:08:21,224][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:08:21,224][257371] Reward + Measures: [[-92.48455169   0.56380397   0.55823964   0.61042166   0.13521633
    7.03576565]][0m
[37m[1m[2023-07-17 04:08:21,224][257371] Max Reward on eval: -92.48455169028172[0m
[37m[1m[2023-07-17 04:08:21,224][257371] Min Reward on eval: -92.48455169028172[0m
[37m[1m[2023-07-17 04:08:21,225][257371] Mean Reward across all agents: -92.48455169028172[0m
[37m[1m[2023-07-17 04:08:21,225][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:08:26,253][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:08:26,253][257371] Reward + Measures: [[ 21.15879983   0.24329999   0.324        0.23109999   0.19860001
    5.37724113]
 [151.94684897   0.17209999   0.6929       0.36849999   0.68400002
    4.02147293]
 [438.7168827    0.1938       0.89729995   0.58289999   0.89469999
    6.17265511]
 ...
 [-80.82100132   0.39199999   0.7044       0.1885       0.68479997
    4.69517708]
 [  0.82879394   0.26719999   0.32910001   0.1842       0.24380003
    3.55591583]
 [302.77752656   0.84769994   0.83319998   0.83429998   0.0166
    4.85248709]][0m
[37m[1m[2023-07-17 04:08:26,254][257371] Max Reward on eval: 459.35422707190736[0m
[37m[1m[2023-07-17 04:08:26,254][257371] Min Reward on eval: -291.78818038357423[0m
[37m[1m[2023-07-17 04:08:26,254][257371] Mean Reward across all agents: 42.82346098338865[0m
[37m[1m[2023-07-17 04:08:26,254][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:08:26,260][257371] mean_value=-429.42550066520005, max_value=438.69308939624966[0m
[37m[1m[2023-07-17 04:08:26,263][257371] New mean coefficients: [[-2.5167515  -0.50899875  2.1923862  -0.5937073  -0.81845737  7.7303286 ]][0m
[37m[1m[2023-07-17 04:08:26,264][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:08:35,314][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 04:08:35,314][257371] FPS: 424406.98[0m
[36m[2023-07-17 04:08:35,316][257371] itr=608, itrs=2000, Progress: 30.40%[0m
[36m[2023-07-17 04:08:47,208][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 04:08:47,208][257371] FPS: 325115.84[0m
[36m[2023-07-17 04:08:51,525][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:08:51,525][257371] Reward + Measures: [[-94.4852273    0.58253831   0.57405132   0.6194877    0.127028
    7.06206226]][0m
[37m[1m[2023-07-17 04:08:51,526][257371] Max Reward on eval: -94.4852273022106[0m
[37m[1m[2023-07-17 04:08:51,526][257371] Min Reward on eval: -94.4852273022106[0m
[37m[1m[2023-07-17 04:08:51,526][257371] Mean Reward across all agents: -94.4852273022106[0m
[37m[1m[2023-07-17 04:08:51,526][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:08:56,565][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:08:56,566][257371] Reward + Measures: [[-27.04207983   0.43179998   0.47820002   0.3547       0.22270003
    4.67349625]
 [ 11.67576004   0.421        0.46590003   0.29809999   0.4456
    3.24325442]
 [-55.45080096   0.43660003   0.39180002   0.3019       0.23120001
    3.66584778]
 ...
 [-46.52865628   0.41020003   0.51109999   0.20490001   0.51980001
    3.36997461]
 [ -6.82911503   0.40869999   0.39489999   0.2349       0.34430003
    3.60823441]
 [100.03519537   0.48330003   0.42539999   0.27739999   0.35620001
    4.45053339]][0m
[37m[1m[2023-07-17 04:08:56,566][257371] Max Reward on eval: 546.3600731175393[0m
[37m[1m[2023-07-17 04:08:56,567][257371] Min Reward on eval: -321.10894523127934[0m
[37m[1m[2023-07-17 04:08:56,567][257371] Mean Reward across all agents: -13.769040703771756[0m
[37m[1m[2023-07-17 04:08:56,567][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:08:56,570][257371] mean_value=-645.505405984532, max_value=195.69637083625258[0m
[37m[1m[2023-07-17 04:08:56,573][257371] New mean coefficients: [[-1.895757   -1.6606575   2.4876478   0.19966918 -1.4113823   8.466567  ]][0m
[37m[1m[2023-07-17 04:08:56,574][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:09:05,630][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 04:09:05,631][257371] FPS: 424071.82[0m
[36m[2023-07-17 04:09:05,633][257371] itr=609, itrs=2000, Progress: 30.45%[0m
[36m[2023-07-17 04:09:17,305][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 04:09:17,305][257371] FPS: 331294.13[0m
[36m[2023-07-17 04:09:21,599][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:09:21,600][257371] Reward + Measures: [[-99.67697203   0.57407796   0.56994927   0.61299139   0.13301399
    7.05375719]][0m
[37m[1m[2023-07-17 04:09:21,600][257371] Max Reward on eval: -99.67697202824765[0m
[37m[1m[2023-07-17 04:09:21,600][257371] Min Reward on eval: -99.67697202824765[0m
[37m[1m[2023-07-17 04:09:21,601][257371] Mean Reward across all agents: -99.67697202824765[0m
[37m[1m[2023-07-17 04:09:21,601][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:09:26,618][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:09:26,619][257371] Reward + Measures: [[-112.22457362    0.42490003    0.81459999    0.3382        0.77959996
     4.79700184]
 [ -83.52375746    0.29460001    0.68349999    0.20440002    0.7033
     4.48322868]
 [-100.09647835    0.6724        0.8599        0.1868        0.77069998
     5.88813782]
 ...
 [  72.99774523    0.20570002    0.27770004    0.18719999    0.29480001
     5.1962924 ]
 [  48.29120808    0.26429999    0.32179999    0.19240001    0.32549998
     4.4678874 ]
 [  47.31086447    0.3382        0.39520001    0.23410001    0.3211
     4.68428183]][0m
[37m[1m[2023-07-17 04:09:26,619][257371] Max Reward on eval: 425.9846221127547[0m
[37m[1m[2023-07-17 04:09:26,620][257371] Min Reward on eval: -412.32734680785796[0m
[37m[1m[2023-07-17 04:09:26,620][257371] Mean Reward across all agents: -32.586939207712206[0m
[37m[1m[2023-07-17 04:09:26,620][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:09:26,625][257371] mean_value=-275.4508465574886, max_value=226.9153341932629[0m
[37m[1m[2023-07-17 04:09:26,628][257371] New mean coefficients: [[-1.7638097  -1.5092201   2.6023545   0.36504245 -1.3641016   8.045713  ]][0m
[37m[1m[2023-07-17 04:09:26,629][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:09:35,624][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 04:09:35,625][257371] FPS: 426945.51[0m
[36m[2023-07-17 04:09:35,627][257371] itr=610, itrs=2000, Progress: 30.50%[0m
[37m[1m[2023-07-17 04:12:29,920][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000590[0m
[36m[2023-07-17 04:12:42,384][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 04:12:42,384][257371] FPS: 328682.96[0m
[36m[2023-07-17 04:12:46,543][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:12:46,543][257371] Reward + Measures: [[-106.77874941    0.58896267    0.58182466    0.62722737    0.12053867
     7.09213686]][0m
[37m[1m[2023-07-17 04:12:46,544][257371] Max Reward on eval: -106.77874941283662[0m
[37m[1m[2023-07-17 04:12:46,544][257371] Min Reward on eval: -106.77874941283662[0m
[37m[1m[2023-07-17 04:12:46,544][257371] Mean Reward across all agents: -106.77874941283662[0m
[37m[1m[2023-07-17 04:12:46,544][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:12:51,566][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:12:51,567][257371] Reward + Measures: [[  59.49075242    0.62580007    0.60160005    0.56519997    0.0313
     6.60386658]
 [ 141.82545446    0.76529998    0.85280001    0.71509999    0.22330001
     6.74725437]
 [ 271.62546105    0.65530002    0.65230006    0.65760005    0.0947
     5.22332239]
 ...
 [  20.19760802    0.7392        0.71579999    0.70469999    0.0354
     5.91347265]
 [-107.0883423     0.59450001    0.58640003    0.56309998    0.0436
     5.36641836]
 [ 265.01512719    0.60829997    0.58859998    0.59750003    0.0531
     6.51711893]][0m
[37m[1m[2023-07-17 04:12:51,567][257371] Max Reward on eval: 639.5653514985927[0m
[37m[1m[2023-07-17 04:12:51,567][257371] Min Reward on eval: -336.6068782692775[0m
[37m[1m[2023-07-17 04:12:51,568][257371] Mean Reward across all agents: 31.68184526638687[0m
[37m[1m[2023-07-17 04:12:51,568][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:12:51,571][257371] mean_value=-1026.7400078042403, max_value=367.49883357363325[0m
[37m[1m[2023-07-17 04:12:51,573][257371] New mean coefficients: [[-1.8434522  -2.0196056   2.9276657  -0.10198525  0.232391    7.7955384 ]][0m
[37m[1m[2023-07-17 04:12:51,574][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:13:00,644][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 04:13:00,644][257371] FPS: 423469.95[0m
[36m[2023-07-17 04:13:00,646][257371] itr=611, itrs=2000, Progress: 30.55%[0m
[36m[2023-07-17 04:13:12,392][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 04:13:12,392][257371] FPS: 329270.05[0m
[36m[2023-07-17 04:13:16,663][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:13:16,663][257371] Reward + Measures: [[-111.47364107    0.58227497    0.57565135    0.61939329    0.126105
     7.09116268]][0m
[37m[1m[2023-07-17 04:13:16,663][257371] Max Reward on eval: -111.47364106576451[0m
[37m[1m[2023-07-17 04:13:16,664][257371] Min Reward on eval: -111.47364106576451[0m
[37m[1m[2023-07-17 04:13:16,664][257371] Mean Reward across all agents: -111.47364106576451[0m
[37m[1m[2023-07-17 04:13:16,664][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:13:21,664][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:13:21,664][257371] Reward + Measures: [[ -69.60266218    0.56940001    0.56950003    0.52950001    0.12230001
     4.90031481]
 [ -12.73796092    0.4025        0.45720002    0.33970001    0.32510003
     4.31369114]
 [ -44.04828248    0.39519998    0.4244        0.29360002    0.2113
     4.34367466]
 ...
 [-460.66456412    0.12680002    0.96189994    0.60659999    0.95740002
     5.75709915]
 [-112.03144022    0.74159998    0.7493        0.71799999    0.0653
     6.0114851 ]
 [-131.19998237    0.15280001    0.85610002    0.45380002    0.85620004
     5.20780134]][0m
[37m[1m[2023-07-17 04:13:21,665][257371] Max Reward on eval: 657.5346859298181[0m
[37m[1m[2023-07-17 04:13:21,665][257371] Min Reward on eval: -474.04798961952326[0m
[37m[1m[2023-07-17 04:13:21,665][257371] Mean Reward across all agents: 11.922008166725028[0m
[37m[1m[2023-07-17 04:13:21,665][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:13:21,669][257371] mean_value=-832.7655402997893, max_value=762.3093360652106[0m
[37m[1m[2023-07-17 04:13:21,671][257371] New mean coefficients: [[-1.5149752  -2.7646604   2.9821527   0.24789774 -0.587737    7.849975  ]][0m
[37m[1m[2023-07-17 04:13:21,672][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:13:30,729][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 04:13:30,729][257371] FPS: 424077.92[0m
[36m[2023-07-17 04:13:30,731][257371] itr=612, itrs=2000, Progress: 30.60%[0m
[36m[2023-07-17 04:13:42,565][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 04:13:42,565][257371] FPS: 326657.26[0m
[36m[2023-07-17 04:13:46,906][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:13:46,907][257371] Reward + Measures: [[-111.1823617     0.58364439    0.57855564    0.6236816     0.12434067
     7.10584259]][0m
[37m[1m[2023-07-17 04:13:46,907][257371] Max Reward on eval: -111.18236169897307[0m
[37m[1m[2023-07-17 04:13:46,907][257371] Min Reward on eval: -111.18236169897307[0m
[37m[1m[2023-07-17 04:13:46,907][257371] Mean Reward across all agents: -111.18236169897307[0m
[37m[1m[2023-07-17 04:13:46,908][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:13:51,880][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:13:51,881][257371] Reward + Measures: [[-62.64830369   0.30600002   0.37560001   0.14460002   0.39250001
    4.905128  ]
 [-28.59165244   0.3874       0.4118       0.26370001   0.29679999
    4.41738892]
 [192.91095686   0.48789999   0.54549998   0.44060001   0.1806
    5.08478642]
 ...
 [-82.39559558   0.55840003   0.81960005   0.18470001   0.68330002
    4.89149094]
 [-95.30887318   0.45120001   0.64840001   0.35490003   0.44360003
    4.99294519]
 [ 37.14817598   0.42020002   0.49239999   0.32659999   0.26359999
    3.66886592]][0m
[37m[1m[2023-07-17 04:13:51,881][257371] Max Reward on eval: 266.28599357996137[0m
[37m[1m[2023-07-17 04:13:51,881][257371] Min Reward on eval: -470.4322233188897[0m
[37m[1m[2023-07-17 04:13:51,881][257371] Mean Reward across all agents: -33.00108239800918[0m
[37m[1m[2023-07-17 04:13:51,882][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:13:51,885][257371] mean_value=-492.97449235935267, max_value=215.11653779966815[0m
[37m[1m[2023-07-17 04:13:51,887][257371] New mean coefficients: [[-1.3309238 -3.8574     3.0789447  0.8205191 -1.7719977  8.377073 ]][0m
[37m[1m[2023-07-17 04:13:51,888][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:14:00,798][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-17 04:14:00,799][257371] FPS: 431041.80[0m
[36m[2023-07-17 04:14:00,801][257371] itr=613, itrs=2000, Progress: 30.65%[0m
[36m[2023-07-17 04:14:12,413][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 04:14:12,413][257371] FPS: 333070.05[0m
[36m[2023-07-17 04:14:16,657][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:14:16,657][257371] Reward + Measures: [[-106.85934433    0.58670896    0.58210069    0.62402833    0.12620799
     7.09863186]][0m
[37m[1m[2023-07-17 04:14:16,658][257371] Max Reward on eval: -106.85934433106846[0m
[37m[1m[2023-07-17 04:14:16,658][257371] Min Reward on eval: -106.85934433106846[0m
[37m[1m[2023-07-17 04:14:16,658][257371] Mean Reward across all agents: -106.85934433106846[0m
[37m[1m[2023-07-17 04:14:16,658][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:14:21,605][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:14:21,606][257371] Reward + Measures: [[ -77.98551751    0.65399998    0.6031        0.53250003    0.1176
     4.06886005]
 [  -2.34529661    0.36810002    0.2624        0.25800002    0.20639999
     4.31987524]
 [  56.326918      0.41929999    0.48450002    0.33430001    0.32530001
     4.26875687]
 ...
 [  18.48383596    0.66769999    0.4858        0.49200001    0.14240001
     5.34867907]
 [   6.82660832    0.56510001    0.42430001    0.42789999    0.1548
     4.62351465]
 [-288.70655823    0.17309999    0.91890001    0.51780003    0.88609999
     4.04698801]][0m
[37m[1m[2023-07-17 04:14:21,606][257371] Max Reward on eval: 468.45760058686136[0m
[37m[1m[2023-07-17 04:14:21,606][257371] Min Reward on eval: -288.70655822865666[0m
[37m[1m[2023-07-17 04:14:21,607][257371] Mean Reward across all agents: 49.298739793505796[0m
[37m[1m[2023-07-17 04:14:21,607][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:14:21,610][257371] mean_value=-336.96787962387765, max_value=582.2857595557253[0m
[37m[1m[2023-07-17 04:14:21,613][257371] New mean coefficients: [[-1.1516528 -4.3692183  3.4214172  1.1950598 -1.9241587  8.422732 ]][0m
[37m[1m[2023-07-17 04:14:21,614][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:14:30,662][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 04:14:30,662][257371] FPS: 424496.40[0m
[36m[2023-07-17 04:14:30,664][257371] itr=614, itrs=2000, Progress: 30.70%[0m
[36m[2023-07-17 04:14:42,352][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 04:14:42,352][257371] FPS: 330937.31[0m
[36m[2023-07-17 04:14:46,699][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:14:46,699][257371] Reward + Measures: [[-110.7060618     0.57889736    0.57303834    0.6171487     0.129132
     7.10353565]][0m
[37m[1m[2023-07-17 04:14:46,700][257371] Max Reward on eval: -110.70606179813451[0m
[37m[1m[2023-07-17 04:14:46,700][257371] Min Reward on eval: -110.70606179813451[0m
[37m[1m[2023-07-17 04:14:46,700][257371] Mean Reward across all agents: -110.70606179813451[0m
[37m[1m[2023-07-17 04:14:46,700][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:14:51,850][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:14:51,851][257371] Reward + Measures: [[ 36.50173046   0.41779995   0.36169997   0.37390003   0.0585
    5.02125549]
 [140.51924753   0.59629995   0.85390007   0.8530001    0.34450004
    5.36903143]
 [144.3831087    0.54899997   0.90189999   0.87630004   0.70660001
    5.54176378]
 ...
 [187.1840198    0.60900003   0.92379999   0.2798       0.79139996
    6.6443696 ]
 [-43.6684       0.6124       0.63740003   0.52790004   0.2608
    5.13704252]
 [-23.63123462   0.76120007   0.7985       0.78240007   0.1084
    5.90382624]][0m
[37m[1m[2023-07-17 04:14:51,851][257371] Max Reward on eval: 473.8770895055961[0m
[37m[1m[2023-07-17 04:14:51,851][257371] Min Reward on eval: -363.25839240206403[0m
[37m[1m[2023-07-17 04:14:51,852][257371] Mean Reward across all agents: 47.33478962585921[0m
[37m[1m[2023-07-17 04:14:51,852][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:14:51,859][257371] mean_value=-235.75145513334888, max_value=435.08702038395734[0m
[37m[1m[2023-07-17 04:14:51,861][257371] New mean coefficients: [[-0.62102914 -4.3028097   3.9920397   1.760553   -1.6056731   7.67807   ]][0m
[37m[1m[2023-07-17 04:14:51,862][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:15:00,917][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 04:15:00,917][257371] FPS: 424156.63[0m
[36m[2023-07-17 04:15:00,920][257371] itr=615, itrs=2000, Progress: 30.75%[0m
[36m[2023-07-17 04:15:12,588][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 04:15:12,588][257371] FPS: 331373.90[0m
[36m[2023-07-17 04:15:16,954][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:15:16,954][257371] Reward + Measures: [[-114.06463974    0.59884971    0.59208864    0.63460332    0.11543734
     7.12675905]][0m
[37m[1m[2023-07-17 04:15:16,954][257371] Max Reward on eval: -114.0646397376925[0m
[37m[1m[2023-07-17 04:15:16,955][257371] Min Reward on eval: -114.0646397376925[0m
[37m[1m[2023-07-17 04:15:16,955][257371] Mean Reward across all agents: -114.0646397376925[0m
[37m[1m[2023-07-17 04:15:16,955][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:15:21,983][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:15:21,983][257371] Reward + Measures: [[ 28.16831093   0.64420003   0.65860003   0.26089999   0.454
    6.43516159]
 [ -4.28553124   0.52759999   0.54039997   0.22660001   0.37310001
    6.61173344]
 [ 54.08115863   0.68339998   0.75660002   0.22360002   0.58459997
    6.52992249]
 ...
 [  5.73903287   0.1376       0.1714       0.13020001   0.0808
    5.64114761]
 [-10.3702566    0.85859996   0.86590004   0.23819999   0.65400004
    6.98694468]
 [  7.24325512   0.50170004   0.51840001   0.368        0.2572
    6.01678753]][0m
[37m[1m[2023-07-17 04:15:21,984][257371] Max Reward on eval: 488.32135244719683[0m
[37m[1m[2023-07-17 04:15:21,984][257371] Min Reward on eval: -340.3251771847252[0m
[37m[1m[2023-07-17 04:15:21,984][257371] Mean Reward across all agents: 14.42750952918362[0m
[37m[1m[2023-07-17 04:15:21,984][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:15:21,990][257371] mean_value=-183.7827143599906, max_value=368.9566333543473[0m
[37m[1m[2023-07-17 04:15:21,993][257371] New mean coefficients: [[-0.6178017 -4.093336   3.4162233  1.9066837 -0.9532817  7.805792 ]][0m
[37m[1m[2023-07-17 04:15:21,994][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:15:31,106][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 04:15:31,107][257371] FPS: 421479.98[0m
[36m[2023-07-17 04:15:31,109][257371] itr=616, itrs=2000, Progress: 30.80%[0m
[36m[2023-07-17 04:15:42,972][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 04:15:42,972][257371] FPS: 325951.92[0m
[36m[2023-07-17 04:15:47,336][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:15:47,336][257371] Reward + Measures: [[-113.01892179    0.61303198    0.60894001    0.64962399    0.11967132
     7.15384769]][0m
[37m[1m[2023-07-17 04:15:47,337][257371] Max Reward on eval: -113.0189217852668[0m
[37m[1m[2023-07-17 04:15:47,337][257371] Min Reward on eval: -113.0189217852668[0m
[37m[1m[2023-07-17 04:15:47,337][257371] Mean Reward across all agents: -113.0189217852668[0m
[37m[1m[2023-07-17 04:15:47,337][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:15:52,305][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:15:52,306][257371] Reward + Measures: [[-13.56067563   0.1567       0.14229999   0.13039999   0.1125
    4.13295317]
 [-52.99483963   0.18529999   0.1851       0.1948       0.1657
    4.31978226]
 [-61.17116185   0.2626       0.289        0.21630001   0.1664
    4.31843424]
 ...
 [-54.2363978    0.19219999   0.23810001   0.21539998   0.2217
    3.68471456]
 [-38.13189567   0.26250002   0.23239999   0.19389999   0.1481
    4.28444529]
 [-84.86893996   0.55019999   0.41540003   0.498        0.0608
    4.6119113 ]][0m
[37m[1m[2023-07-17 04:15:52,306][257371] Max Reward on eval: 568.4507288955152[0m
[37m[1m[2023-07-17 04:15:52,307][257371] Min Reward on eval: -472.2250061701983[0m
[37m[1m[2023-07-17 04:15:52,307][257371] Mean Reward across all agents: -49.351327634479176[0m
[37m[1m[2023-07-17 04:15:52,307][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:15:52,309][257371] mean_value=-2237.2884651815602, max_value=165.25537425701253[0m
[37m[1m[2023-07-17 04:15:52,311][257371] New mean coefficients: [[-0.04115003 -2.5447574   2.8853598   1.681705   -1.115765    7.72114   ]][0m
[37m[1m[2023-07-17 04:15:52,312][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:16:01,322][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 04:16:01,322][257371] FPS: 426285.37[0m
[36m[2023-07-17 04:16:01,324][257371] itr=617, itrs=2000, Progress: 30.85%[0m
[36m[2023-07-17 04:16:13,155][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 04:16:13,156][257371] FPS: 326791.20[0m
[36m[2023-07-17 04:16:17,490][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:16:17,490][257371] Reward + Measures: [[-110.41505407    0.61513764    0.61012703    0.64802766    0.11526465
     7.15063667]][0m
[37m[1m[2023-07-17 04:16:17,490][257371] Max Reward on eval: -110.41505407335893[0m
[37m[1m[2023-07-17 04:16:17,491][257371] Min Reward on eval: -110.41505407335893[0m
[37m[1m[2023-07-17 04:16:17,491][257371] Mean Reward across all agents: -110.41505407335893[0m
[37m[1m[2023-07-17 04:16:17,491][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:16:22,529][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:16:22,530][257371] Reward + Measures: [[ -6.75528596   0.37530002   0.33510002   0.31010002   0.0604
    4.99400568]
 [-25.85856192   0.31439999   0.29490003   0.21010001   0.21229999
    2.91045022]
 [ 37.31508003   0.50229996   0.47839999   0.43860003   0.48300001
    3.98480654]
 ...
 [ 59.74925977   0.48750004   0.45770001   0.40489998   0.45030004
    4.1998992 ]
 [230.30775732   0.67699999   0.6631       0.66860002   0.0579
    5.55051088]
 [ -8.05390508   0.40920001   0.36559999   0.33479998   0.32949999
    3.50127864]][0m
[37m[1m[2023-07-17 04:16:22,530][257371] Max Reward on eval: 488.04321382050404[0m
[37m[1m[2023-07-17 04:16:22,530][257371] Min Reward on eval: -337.3489723302424[0m
[37m[1m[2023-07-17 04:16:22,531][257371] Mean Reward across all agents: -24.77610538847305[0m
[37m[1m[2023-07-17 04:16:22,531][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:16:22,535][257371] mean_value=-483.55193996958656, max_value=431.93441236604286[0m
[37m[1m[2023-07-17 04:16:22,538][257371] New mean coefficients: [[-0.01952819 -2.2299595   2.550735    1.9810135  -0.40171117  7.5387173 ]][0m
[37m[1m[2023-07-17 04:16:22,539][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:16:31,602][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 04:16:31,602][257371] FPS: 423792.94[0m
[36m[2023-07-17 04:16:31,604][257371] itr=618, itrs=2000, Progress: 30.90%[0m
[36m[2023-07-17 04:16:43,317][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 04:16:43,317][257371] FPS: 330265.55[0m
[36m[2023-07-17 04:16:47,617][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:16:47,617][257371] Reward + Measures: [[-111.28137894    0.63330436    0.62661433    0.66001934    0.10756066
     7.18012905]][0m
[37m[1m[2023-07-17 04:16:47,618][257371] Max Reward on eval: -111.28137893550374[0m
[37m[1m[2023-07-17 04:16:47,618][257371] Min Reward on eval: -111.28137893550374[0m
[37m[1m[2023-07-17 04:16:47,618][257371] Mean Reward across all agents: -111.28137893550374[0m
[37m[1m[2023-07-17 04:16:47,618][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:16:52,696][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:16:52,697][257371] Reward + Measures: [[  66.34253836    0.23660003    0.42769995    0.34689999    0.30149999
     4.61050367]
 [ -80.13163764    0.41589999    0.3867        0.39200002    0.0447
     4.89693451]
 [ -22.9542008     0.29780003    0.27740002    0.3003        0.12830001
     4.47274637]
 ...
 [  38.61941864    0.22219999    0.89169997    0.479         0.90509999
     6.08733463]
 [-113.10260035    0.1997        0.88080007    0.58149999    0.86709994
     5.75269032]
 [ -29.9995791     0.29659998    0.31          0.18959999    0.25040001
     3.78666425]][0m
[37m[1m[2023-07-17 04:16:52,697][257371] Max Reward on eval: 365.3341750891879[0m
[37m[1m[2023-07-17 04:16:52,697][257371] Min Reward on eval: -350.2108039919287[0m
[37m[1m[2023-07-17 04:16:52,698][257371] Mean Reward across all agents: -42.14890369380496[0m
[37m[1m[2023-07-17 04:16:52,698][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:16:52,701][257371] mean_value=-572.8201951740986, max_value=258.63956194278467[0m
[37m[1m[2023-07-17 04:16:52,703][257371] New mean coefficients: [[-0.3760938 -3.0004401  2.0623677  1.873116  -0.6835812  7.6081715]][0m
[37m[1m[2023-07-17 04:16:52,704][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:17:01,929][257371] train() took 9.22 seconds to complete[0m
[36m[2023-07-17 04:17:01,930][257371] FPS: 416345.36[0m
[36m[2023-07-17 04:17:01,932][257371] itr=619, itrs=2000, Progress: 30.95%[0m
[36m[2023-07-17 04:17:13,623][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 04:17:13,624][257371] FPS: 330750.42[0m
[36m[2023-07-17 04:17:17,957][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:17:17,957][257371] Reward + Measures: [[-105.69064016    0.62251592    0.61795431    0.65280998    0.11724599
     7.17157936]][0m
[37m[1m[2023-07-17 04:17:17,957][257371] Max Reward on eval: -105.69064015863783[0m
[37m[1m[2023-07-17 04:17:17,957][257371] Min Reward on eval: -105.69064015863783[0m
[37m[1m[2023-07-17 04:17:17,958][257371] Mean Reward across all agents: -105.69064015863783[0m
[37m[1m[2023-07-17 04:17:17,958][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:17:23,189][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:17:23,190][257371] Reward + Measures: [[ 18.77291828   0.31930003   0.29459998   0.34530002   0.25
    4.51259756]
 [-13.59549331   0.33180001   0.31020001   0.2182       0.24170001
    4.62349081]
 [-70.49597774   0.53660005   0.42500001   0.33629999   0.2507
    4.46127844]
 ...
 [287.12314132   0.77500004   0.76910001   0.66170001   0.12840001
    5.97252798]
 [ -9.9387602    0.28920001   0.26300001   0.2696       0.24700001
    4.69199038]
 [ -3.50936757   0.24590002   0.22979999   0.2378       0.20840001
    4.07130527]][0m
[37m[1m[2023-07-17 04:17:23,190][257371] Max Reward on eval: 334.4385499736294[0m
[37m[1m[2023-07-17 04:17:23,190][257371] Min Reward on eval: -252.4144396688789[0m
[37m[1m[2023-07-17 04:17:23,190][257371] Mean Reward across all agents: 3.874927916765335[0m
[37m[1m[2023-07-17 04:17:23,191][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:17:23,193][257371] mean_value=-705.8379808993252, max_value=191.99288648199575[0m
[37m[1m[2023-07-17 04:17:23,196][257371] New mean coefficients: [[-0.83532643 -2.9463782   1.7914457   1.4530448   0.44289726  7.014994  ]][0m
[37m[1m[2023-07-17 04:17:23,197][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:17:32,218][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 04:17:32,219][257371] FPS: 425717.40[0m
[36m[2023-07-17 04:17:32,221][257371] itr=620, itrs=2000, Progress: 31.00%[0m
[37m[1m[2023-07-17 04:20:26,730][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000600[0m
[36m[2023-07-17 04:20:38,899][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 04:20:38,899][257371] FPS: 331134.75[0m
[36m[2023-07-17 04:20:43,167][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:20:43,167][257371] Reward + Measures: [[-107.90966365    0.6229533     0.61786497    0.65609235    0.11597034
     7.16789722]][0m
[37m[1m[2023-07-17 04:20:43,168][257371] Max Reward on eval: -107.90966364601223[0m
[37m[1m[2023-07-17 04:20:43,168][257371] Min Reward on eval: -107.90966364601223[0m
[37m[1m[2023-07-17 04:20:43,168][257371] Mean Reward across all agents: -107.90966364601223[0m
[37m[1m[2023-07-17 04:20:43,168][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:20:48,081][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:20:48,081][257371] Reward + Measures: [[-175.79808041    0.63660002    0.68599999    0.15189999    0.64600003
     3.70558095]
 [ 231.14101687    0.59540004    0.65790004    0.56179994    0.2428
     6.59959793]
 [ -71.5626554     0.42219996    0.70640004    0.30250001    0.63409996
     3.8743813 ]
 ...
 [ 366.09004393    0.546         0.53220004    0.5661        0.0612
     5.5110817 ]
 [ -21.33387827    0.76370001    0.7471        0.76679999    0.0747
     6.73309708]
 [ 275.18057535    0.75839996    0.76660007    0.69749999    0.0686
     4.59264994]][0m
[37m[1m[2023-07-17 04:20:48,081][257371] Max Reward on eval: 803.8731689164415[0m
[37m[1m[2023-07-17 04:20:48,082][257371] Min Reward on eval: -400.5693510503508[0m
[37m[1m[2023-07-17 04:20:48,082][257371] Mean Reward across all agents: 19.970998846086943[0m
[37m[1m[2023-07-17 04:20:48,082][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:20:48,085][257371] mean_value=-993.7665525579085, max_value=371.4371909519606[0m
[37m[1m[2023-07-17 04:20:48,087][257371] New mean coefficients: [[-1.4550692 -1.5618491  2.1833081  1.0547723 -0.5667359  6.2654495]][0m
[37m[1m[2023-07-17 04:20:48,088][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:20:57,165][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 04:20:57,166][257371] FPS: 423100.10[0m
[36m[2023-07-17 04:20:57,168][257371] itr=621, itrs=2000, Progress: 31.05%[0m
[36m[2023-07-17 04:21:08,829][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 04:21:08,829][257371] FPS: 332284.27[0m
[36m[2023-07-17 04:21:13,058][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:21:13,059][257371] Reward + Measures: [[-116.84480354    0.63259768    0.62803131    0.66321069    0.112707
     7.18425751]][0m
[37m[1m[2023-07-17 04:21:13,059][257371] Max Reward on eval: -116.84480353897081[0m
[37m[1m[2023-07-17 04:21:13,059][257371] Min Reward on eval: -116.84480353897081[0m
[37m[1m[2023-07-17 04:21:13,060][257371] Mean Reward across all agents: -116.84480353897081[0m
[37m[1m[2023-07-17 04:21:13,060][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:21:18,199][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:21:18,200][257371] Reward + Measures: [[  16.01255364    0.37869999    0.45000002    0.27500001    0.39480001
     3.44272923]
 [  38.19272136    0.27150002    0.93230003    0.47129998    0.94540006
     6.65291595]
 [-117.94442779    0.30060002    0.38229999    0.12800001    0.45110002
     4.92936802]
 ...
 [ -29.09687348    0.2881        0.25209999    0.23120001    0.15470001
     3.16964984]
 [-171.45265488    0.37989998    0.74139994    0.0896        0.77239996
     5.09230566]
 [ -58.42056908    0.29969999    0.33660004    0.22160001    0.20740001
     3.52459025]][0m
[37m[1m[2023-07-17 04:21:18,200][257371] Max Reward on eval: 705.3054046232253[0m
[37m[1m[2023-07-17 04:21:18,200][257371] Min Reward on eval: -321.96284099435434[0m
[37m[1m[2023-07-17 04:21:18,201][257371] Mean Reward across all agents: 112.27566348193791[0m
[37m[1m[2023-07-17 04:21:18,201][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:21:18,208][257371] mean_value=-489.56736447224006, max_value=709.6305363315904[0m
[37m[1m[2023-07-17 04:21:18,211][257371] New mean coefficients: [[-2.7273755  -1.5970201   1.5776825   0.09561926  1.5581751   5.22043   ]][0m
[37m[1m[2023-07-17 04:21:18,212][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:21:27,158][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 04:21:27,159][257371] FPS: 429314.37[0m
[36m[2023-07-17 04:21:27,161][257371] itr=622, itrs=2000, Progress: 31.10%[0m
[36m[2023-07-17 04:21:38,915][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 04:21:38,916][257371] FPS: 328929.43[0m
[36m[2023-07-17 04:21:43,154][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:21:43,155][257371] Reward + Measures: [[-128.22282512    0.66052133    0.65492767    0.68488294    0.101716
     7.22642136]][0m
[37m[1m[2023-07-17 04:21:43,155][257371] Max Reward on eval: -128.2228251243502[0m
[37m[1m[2023-07-17 04:21:43,155][257371] Min Reward on eval: -128.2228251243502[0m
[37m[1m[2023-07-17 04:21:43,155][257371] Mean Reward across all agents: -128.2228251243502[0m
[37m[1m[2023-07-17 04:21:43,156][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:21:48,168][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:21:48,168][257371] Reward + Measures: [[   2.82800399    0.54720002    0.61569995    0.0734        0.6049
     4.90007973]
 [  -1.98382485    0.6979        0.87940007    0.07910001    0.88449997
     6.45890903]
 [  72.47547646    0.64959997    0.59849995    0.61430001    0.12539999
     6.10626554]
 ...
 [  80.96680897    0.76239997    0.75060004    0.294         0.50789994
     5.9817667 ]
 [-140.42015532    0.49630004    0.47009999    0.3804        0.22049999
     6.00682974]
 [-134.28856956    0.60859996    0.74900001    0.22829998    0.60420007
     6.62112761]][0m
[37m[1m[2023-07-17 04:21:48,168][257371] Max Reward on eval: 521.5381650798023[0m
[37m[1m[2023-07-17 04:21:48,169][257371] Min Reward on eval: -522.9709043499548[0m
[37m[1m[2023-07-17 04:21:48,169][257371] Mean Reward across all agents: -44.091173872451364[0m
[37m[1m[2023-07-17 04:21:48,169][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:21:48,175][257371] mean_value=-330.2968857883092, max_value=583.6301232985554[0m
[37m[1m[2023-07-17 04:21:48,178][257371] New mean coefficients: [[-2.3218539  -1.2567046   2.038832    0.15305537  0.46529615  4.994733  ]][0m
[37m[1m[2023-07-17 04:21:48,179][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:21:57,189][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 04:21:57,189][257371] FPS: 426281.13[0m
[36m[2023-07-17 04:21:57,191][257371] itr=623, itrs=2000, Progress: 31.15%[0m
[36m[2023-07-17 04:22:08,967][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 04:22:08,967][257371] FPS: 328323.23[0m
[36m[2023-07-17 04:22:13,276][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:22:13,276][257371] Reward + Measures: [[-144.96720654    0.66629666    0.663809      0.69587761    0.10164333
     7.24400377]][0m
[37m[1m[2023-07-17 04:22:13,276][257371] Max Reward on eval: -144.9672065360495[0m
[37m[1m[2023-07-17 04:22:13,277][257371] Min Reward on eval: -144.9672065360495[0m
[37m[1m[2023-07-17 04:22:13,277][257371] Mean Reward across all agents: -144.9672065360495[0m
[37m[1m[2023-07-17 04:22:13,277][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:22:18,254][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:22:18,255][257371] Reward + Measures: [[-158.65064776    0.76969999    0.93699998    0.51260006    0.49220005
     6.35542536]
 [  41.86848805    0.45089999    0.87179995    0.50439996    0.64209998
     5.45034456]
 [  58.13144263    0.38460001    0.7816        0.68300003    0.51540005
     4.29981756]
 ...
 [-184.91642761    0.77039999    0.80490011    0.64429998    0.25580001
     6.16068745]
 [  38.81879636    0.33780003    0.69160002    0.2422        0.67430001
     4.28295946]
 [-142.70948385    0.45949998    0.79710001    0.18700001    0.64990002
     3.72461104]][0m
[37m[1m[2023-07-17 04:22:18,255][257371] Max Reward on eval: 338.1507797151804[0m
[37m[1m[2023-07-17 04:22:18,255][257371] Min Reward on eval: -460.1184310592711[0m
[37m[1m[2023-07-17 04:22:18,255][257371] Mean Reward across all agents: -26.056568447789758[0m
[37m[1m[2023-07-17 04:22:18,255][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:22:18,262][257371] mean_value=-381.84223324808096, max_value=568.4131479347467[0m
[37m[1m[2023-07-17 04:22:18,264][257371] New mean coefficients: [[-2.0049057  -2.4499621   1.8254262   0.03789809  0.16821155  5.379465  ]][0m
[37m[1m[2023-07-17 04:22:18,265][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:22:27,257][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 04:22:27,257][257371] FPS: 427128.04[0m
[36m[2023-07-17 04:22:27,259][257371] itr=624, itrs=2000, Progress: 31.20%[0m
[36m[2023-07-17 04:22:39,043][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 04:22:39,043][257371] FPS: 328203.98[0m
[36m[2023-07-17 04:22:43,367][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:22:43,367][257371] Reward + Measures: [[-157.23634542    0.69451869    0.691284      0.71910864    0.090882
     7.27839422]][0m
[37m[1m[2023-07-17 04:22:43,368][257371] Max Reward on eval: -157.2363454151777[0m
[37m[1m[2023-07-17 04:22:43,368][257371] Min Reward on eval: -157.2363454151777[0m
[37m[1m[2023-07-17 04:22:43,368][257371] Mean Reward across all agents: -157.2363454151777[0m
[37m[1m[2023-07-17 04:22:43,368][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:22:48,376][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:22:48,377][257371] Reward + Measures: [[-40.34059803   0.39560002   0.46000001   0.35640001   0.24749999
    3.88375735]
 [-95.83162783   0.41980001   0.36789998   0.1737       0.33310002
    4.74880171]
 [-40.9801941    0.26100001   0.36070001   0.20579998   0.36130002
    3.61605835]
 ...
 [ 65.58773621   0.583        0.70420003   0.1882       0.67699999
    4.19226837]
 [-52.86316571   0.53820002   0.56220001   0.46620002   0.33950001
    3.60354996]
 [  4.9837493    0.54479998   0.66060007   0.3901       0.46479997
    3.4012208 ]][0m
[37m[1m[2023-07-17 04:22:48,377][257371] Max Reward on eval: 475.9570141321048[0m
[37m[1m[2023-07-17 04:22:48,377][257371] Min Reward on eval: -408.8318538250402[0m
[37m[1m[2023-07-17 04:22:48,377][257371] Mean Reward across all agents: -7.056998491970288[0m
[37m[1m[2023-07-17 04:22:48,377][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:22:48,381][257371] mean_value=-879.8923294730938, max_value=385.13434056133474[0m
[37m[1m[2023-07-17 04:22:48,383][257371] New mean coefficients: [[-0.98846734 -1.490171    2.4639285   0.30819172  1.2418483   5.2580137 ]][0m
[37m[1m[2023-07-17 04:22:48,385][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:22:57,357][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 04:22:57,358][257371] FPS: 428046.25[0m
[36m[2023-07-17 04:22:57,360][257371] itr=625, itrs=2000, Progress: 31.25%[0m
[36m[2023-07-17 04:23:09,205][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 04:23:09,206][257371] FPS: 326510.88[0m
[36m[2023-07-17 04:23:13,550][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:23:13,551][257371] Reward + Measures: [[-158.19685541    0.69785768    0.69396371    0.72227895    0.09613767
     7.28794575]][0m
[37m[1m[2023-07-17 04:23:13,551][257371] Max Reward on eval: -158.19685540811082[0m
[37m[1m[2023-07-17 04:23:13,552][257371] Min Reward on eval: -158.19685540811082[0m
[37m[1m[2023-07-17 04:23:13,552][257371] Mean Reward across all agents: -158.19685540811082[0m
[37m[1m[2023-07-17 04:23:13,552][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:23:18,585][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:23:18,586][257371] Reward + Measures: [[ 35.67924277   0.25030002   0.57989997   0.53179997   0.40879998
    5.64687872]
 [129.86961957   0.43779999   0.42480001   0.43610001   0.12530003
    5.59572554]
 [136.4008985    0.63160002   0.88199997   0.72890002   0.3328
    5.59947729]
 ...
 [472.17572931   0.06569999   0.74589998   0.73199999   0.73610008
    5.82089949]
 [153.87643434   0.40439996   0.64340001   0.59330004   0.29350001
    6.1637888 ]
 [566.7902288    0.0283       0.91890001   0.90990001   0.93049997
    6.13979053]][0m
[37m[1m[2023-07-17 04:23:18,586][257371] Max Reward on eval: 566.790228802152[0m
[37m[1m[2023-07-17 04:23:18,586][257371] Min Reward on eval: -329.7000026577851[0m
[37m[1m[2023-07-17 04:23:18,586][257371] Mean Reward across all agents: 54.11211467546098[0m
[37m[1m[2023-07-17 04:23:18,587][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:23:18,591][257371] mean_value=-289.30181193152265, max_value=578.641003190726[0m
[37m[1m[2023-07-17 04:23:18,594][257371] New mean coefficients: [[-1.5551674  -1.0856426   2.153694    0.21341188  1.9633483   5.424935  ]][0m
[37m[1m[2023-07-17 04:23:18,595][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:23:27,680][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 04:23:27,680][257371] FPS: 422773.58[0m
[36m[2023-07-17 04:23:27,682][257371] itr=626, itrs=2000, Progress: 31.30%[0m
[36m[2023-07-17 04:23:39,405][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 04:23:39,406][257371] FPS: 329854.87[0m
[36m[2023-07-17 04:23:43,690][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:23:43,691][257371] Reward + Measures: [[-160.67067604    0.72038597    0.72037768    0.74182534    0.08918867
     7.31626272]][0m
[37m[1m[2023-07-17 04:23:43,691][257371] Max Reward on eval: -160.67067603592562[0m
[37m[1m[2023-07-17 04:23:43,691][257371] Min Reward on eval: -160.67067603592562[0m
[37m[1m[2023-07-17 04:23:43,691][257371] Mean Reward across all agents: -160.67067603592562[0m
[37m[1m[2023-07-17 04:23:43,692][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:23:48,889][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:23:48,889][257371] Reward + Measures: [[ -42.66691445    0.31740001    0.36070004    0.31100002    0.30070001
     5.39894533]
 [ -18.68388594    0.41009998    0.41190001    0.39060003    0.38780001
     5.70423937]
 [  90.83265646    0.29519999    0.38170001    0.20149998    0.33610001
     3.29222918]
 ...
 [-322.71591042    0.56450003    0.94810003    0.33680001    0.84499997
     6.86468458]
 [ -13.32560675    0.31580001    0.41139999    0.30610001    0.36050004
     5.88541651]
 [ -18.77780826    0.1222        0.1201        0.0974        0.0859
     4.53718138]][0m
[37m[1m[2023-07-17 04:23:48,890][257371] Max Reward on eval: 604.333074533986[0m
[37m[1m[2023-07-17 04:23:48,890][257371] Min Reward on eval: -450.6136345505714[0m
[37m[1m[2023-07-17 04:23:48,890][257371] Mean Reward across all agents: 57.43108783811363[0m
[37m[1m[2023-07-17 04:23:48,890][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:23:48,895][257371] mean_value=-849.7382597502439, max_value=800.4091104376862[0m
[37m[1m[2023-07-17 04:23:48,898][257371] New mean coefficients: [[-1.1657948 -1.7061946  1.9837594  1.421052   1.4587733  6.1778803]][0m
[37m[1m[2023-07-17 04:23:48,899][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:23:57,958][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 04:23:57,959][257371] FPS: 423953.61[0m
[36m[2023-07-17 04:23:57,961][257371] itr=627, itrs=2000, Progress: 31.35%[0m
[36m[2023-07-17 04:24:09,819][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 04:24:09,819][257371] FPS: 326094.77[0m
[36m[2023-07-17 04:24:14,095][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:24:14,095][257371] Reward + Measures: [[-153.57182961    0.72041297    0.7244637     0.74052036    0.09599201
     7.32442331]][0m
[37m[1m[2023-07-17 04:24:14,095][257371] Max Reward on eval: -153.57182961419798[0m
[37m[1m[2023-07-17 04:24:14,096][257371] Min Reward on eval: -153.57182961419798[0m
[37m[1m[2023-07-17 04:24:14,096][257371] Mean Reward across all agents: -153.57182961419798[0m
[37m[1m[2023-07-17 04:24:14,096][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:24:19,156][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:24:19,157][257371] Reward + Measures: [[  29.22947388    0.51179999    0.71359998    0.53250003    0.34439999
     6.01375341]
 [-140.78008895    0.59430003    0.57030004    0.46339998    0.2168
     4.09341002]
 [  26.00361634    0.63770002    0.8398        0.3642        0.59840006
     4.81640005]
 ...
 [  -1.66970047    0.3527        0.3973        0.2174        0.2665
     4.43000555]
 [ -45.27150596    0.85600007    0.89429998    0.76259995    0.16419999
     5.4727335 ]
 [-277.74709642    0.70030004    0.90930003    0.09470001    0.93889999
     6.4731369 ]][0m
[37m[1m[2023-07-17 04:24:19,157][257371] Max Reward on eval: 232.93818188440054[0m
[37m[1m[2023-07-17 04:24:19,157][257371] Min Reward on eval: -641.052749626711[0m
[37m[1m[2023-07-17 04:24:19,157][257371] Mean Reward across all agents: -98.45366308597784[0m
[37m[1m[2023-07-17 04:24:19,157][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:24:19,162][257371] mean_value=-477.3426756733488, max_value=369.42001304128866[0m
[37m[1m[2023-07-17 04:24:19,165][257371] New mean coefficients: [[-0.09327233 -1.3663273   2.2163413   2.1799116   1.598544    6.261101  ]][0m
[37m[1m[2023-07-17 04:24:19,166][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:24:28,163][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 04:24:28,163][257371] FPS: 426901.37[0m
[36m[2023-07-17 04:24:28,166][257371] itr=628, itrs=2000, Progress: 31.40%[0m
[36m[2023-07-17 04:24:39,947][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 04:24:39,947][257371] FPS: 328227.50[0m
[36m[2023-07-17 04:24:44,215][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:24:44,215][257371] Reward + Measures: [[-152.93577657    0.72786427    0.73135561    0.74747962    0.09152799
     7.35166216]][0m
[37m[1m[2023-07-17 04:24:44,215][257371] Max Reward on eval: -152.93577657210793[0m
[37m[1m[2023-07-17 04:24:44,216][257371] Min Reward on eval: -152.93577657210793[0m
[37m[1m[2023-07-17 04:24:44,216][257371] Mean Reward across all agents: -152.93577657210793[0m
[37m[1m[2023-07-17 04:24:44,216][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:24:49,234][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:24:49,234][257371] Reward + Measures: [[  52.71603228    0.206         0.3177        0.1463        0.32910001
     3.92823219]
 [  19.77435056    0.32389998    0.36230001    0.28060001    0.25740001
     3.76101375]
 [-168.10570807    0.62159997    0.95240003    0.14320001    0.86580002
     4.82438517]
 ...
 [ -56.6917686     0.27160001    0.29100001    0.23170002    0.17410001
     5.32345581]
 [  50.16668181    0.26349998    0.35639998    0.1415        0.38159999
     4.44764185]
 [ 249.54063606    0.82880002    0.93510002    0.61800003    0.34080002
     5.79195404]][0m
[37m[1m[2023-07-17 04:24:49,235][257371] Max Reward on eval: 407.0533184118569[0m
[37m[1m[2023-07-17 04:24:49,235][257371] Min Reward on eval: -362.3700309020467[0m
[37m[1m[2023-07-17 04:24:49,235][257371] Mean Reward across all agents: -13.992842509781251[0m
[37m[1m[2023-07-17 04:24:49,235][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:24:49,239][257371] mean_value=-953.3847587916226, max_value=696.9175879758448[0m
[37m[1m[2023-07-17 04:24:49,242][257371] New mean coefficients: [[-0.04598942 -1.76491     2.2544997   2.7908235  -0.14076746  6.8992033 ]][0m
[37m[1m[2023-07-17 04:24:49,243][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:24:58,226][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 04:24:58,226][257371] FPS: 427575.54[0m
[36m[2023-07-17 04:24:58,228][257371] itr=629, itrs=2000, Progress: 31.45%[0m
[36m[2023-07-17 04:25:09,867][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 04:25:09,867][257371] FPS: 332279.51[0m
[36m[2023-07-17 04:25:14,116][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:25:14,117][257371] Reward + Measures: [[-145.58990097    0.7293036     0.74014598    0.74995232    0.09829167
     7.35429573]][0m
[37m[1m[2023-07-17 04:25:14,117][257371] Max Reward on eval: -145.58990097410341[0m
[37m[1m[2023-07-17 04:25:14,117][257371] Min Reward on eval: -145.58990097410341[0m
[37m[1m[2023-07-17 04:25:14,118][257371] Mean Reward across all agents: -145.58990097410341[0m
[37m[1m[2023-07-17 04:25:14,118][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:25:19,134][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:25:19,134][257371] Reward + Measures: [[  4.52172627   0.2739       0.18249999   0.1929       0.12630001
    4.82866049]
 [ 13.12147129   0.31019998   0.33059999   0.25999999   0.266
    3.95846629]
 [105.78964721   0.38690001   0.4061       0.12969999   0.37099999
    3.19551754]
 ...
 [  7.96413272   0.59869999   0.84119999   0.26079997   0.63390005
    6.75934458]
 [112.69092007   0.38510001   0.46079999   0.28580001   0.43080002
    3.47272277]
 [ 54.49670473   0.96940005   0.98500007   0.1991       0.79610008
    7.39958668]][0m
[37m[1m[2023-07-17 04:25:19,134][257371] Max Reward on eval: 463.1683788098395[0m
[37m[1m[2023-07-17 04:25:19,135][257371] Min Reward on eval: -420.8480491862865[0m
[37m[1m[2023-07-17 04:25:19,135][257371] Mean Reward across all agents: 30.019596820334748[0m
[37m[1m[2023-07-17 04:25:19,135][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:25:19,138][257371] mean_value=-1290.3346223971728, max_value=177.35928109004251[0m
[37m[1m[2023-07-17 04:25:19,140][257371] New mean coefficients: [[ 0.22886774 -1.941106    2.3807795   2.7366357  -1.2575033   7.0722694 ]][0m
[37m[1m[2023-07-17 04:25:19,141][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:25:28,177][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 04:25:28,177][257371] FPS: 425070.91[0m
[36m[2023-07-17 04:25:28,179][257371] itr=630, itrs=2000, Progress: 31.50%[0m
[37m[1m[2023-07-17 04:28:23,509][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000610[0m
[36m[2023-07-17 04:28:35,663][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 04:28:35,663][257371] FPS: 329304.72[0m
[36m[2023-07-17 04:28:39,938][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:28:39,939][257371] Reward + Measures: [[-150.36683825    0.73019403    0.74200302    0.74618763    0.09999367
     7.35065889]][0m
[37m[1m[2023-07-17 04:28:39,939][257371] Max Reward on eval: -150.36683824985792[0m
[37m[1m[2023-07-17 04:28:39,939][257371] Min Reward on eval: -150.36683824985792[0m
[37m[1m[2023-07-17 04:28:39,939][257371] Mean Reward across all agents: -150.36683824985792[0m
[37m[1m[2023-07-17 04:28:39,940][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:28:44,876][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:28:44,877][257371] Reward + Measures: [[169.36458015   0.66590005   0.71950001   0.5557       0.20109999
    4.34455204]
 [ 27.44410728   0.3687       0.29080003   0.31549999   0.43010002
    4.00828791]
 [-80.86384086   0.55190003   0.78510004   0.47609997   0.52459997
    4.51710224]
 ...
 [ 34.27971285   0.52280003   0.68920004   0.12720001   0.62279999
    4.47475672]
 [-60.21101415   0.49050003   0.47459999   0.46890002   0.456
    4.92528296]
 [ 74.14546733   0.456        0.73119998   0.28620002   0.58840001
    5.11567736]][0m
[37m[1m[2023-07-17 04:28:44,877][257371] Max Reward on eval: 474.04235461079514[0m
[37m[1m[2023-07-17 04:28:44,877][257371] Min Reward on eval: -368.2802015194669[0m
[37m[1m[2023-07-17 04:28:44,877][257371] Mean Reward across all agents: 24.99935597431187[0m
[37m[1m[2023-07-17 04:28:44,878][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:28:44,881][257371] mean_value=-418.23251564228804, max_value=427.9563039626852[0m
[37m[1m[2023-07-17 04:28:44,884][257371] New mean coefficients: [[ 0.8351475 -1.6509248  3.336235   2.4151402 -1.966894   6.516769 ]][0m
[37m[1m[2023-07-17 04:28:44,885][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:28:53,790][257371] train() took 8.90 seconds to complete[0m
[36m[2023-07-17 04:28:53,790][257371] FPS: 431309.84[0m
[36m[2023-07-17 04:28:53,793][257371] itr=631, itrs=2000, Progress: 31.55%[0m
[36m[2023-07-17 04:29:05,646][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 04:29:05,646][257371] FPS: 326288.73[0m
[36m[2023-07-17 04:29:09,864][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:29:09,865][257371] Reward + Measures: [[-142.86576157    0.74310905    0.75807667    0.76378369    0.097791
     7.38589954]][0m
[37m[1m[2023-07-17 04:29:09,865][257371] Max Reward on eval: -142.86576156749967[0m
[37m[1m[2023-07-17 04:29:09,865][257371] Min Reward on eval: -142.86576156749967[0m
[37m[1m[2023-07-17 04:29:09,865][257371] Mean Reward across all agents: -142.86576156749967[0m
[37m[1m[2023-07-17 04:29:09,866][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:29:14,842][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:29:14,842][257371] Reward + Measures: [[195.30550756   0.69099998   0.66720003   0.6839       0.0578
    6.1910243 ]
 [403.08900546   0.68279999   0.71200001   0.65819997   0.66670007
    5.4534812 ]
 [174.17857337   0.7062       0.70909995   0.64240003   0.67430001
    5.2832737 ]
 ...
 [ -9.05130908   0.96119994   0.9321       0.92340004   0.0136
    6.00550222]
 [189.34411184   0.67950004   0.67430001   0.62289995   0.63530004
    5.52653837]
 [  4.85718576   0.83969992   0.82270002   0.79650003   0.0316
    6.8425684 ]][0m
[37m[1m[2023-07-17 04:29:14,843][257371] Max Reward on eval: 625.1949527097865[0m
[37m[1m[2023-07-17 04:29:14,843][257371] Min Reward on eval: -231.06573812570423[0m
[37m[1m[2023-07-17 04:29:14,843][257371] Mean Reward across all agents: 121.63231543570389[0m
[37m[1m[2023-07-17 04:29:14,843][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:29:14,848][257371] mean_value=-229.64104437261093, max_value=471.7586056211498[0m
[37m[1m[2023-07-17 04:29:14,851][257371] New mean coefficients: [[ 1.6814892 -2.6126246  3.6725378  2.8032455 -2.422118   6.9855256]][0m
[37m[1m[2023-07-17 04:29:14,852][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:29:23,922][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 04:29:23,922][257371] FPS: 423466.54[0m
[36m[2023-07-17 04:29:23,924][257371] itr=632, itrs=2000, Progress: 31.60%[0m
[36m[2023-07-17 04:29:35,773][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 04:29:35,773][257371] FPS: 326332.35[0m
[36m[2023-07-17 04:29:40,100][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:29:40,101][257371] Reward + Measures: [[-134.64606903    0.72475737    0.7408753     0.74819458    0.10225333
     7.37027407]][0m
[37m[1m[2023-07-17 04:29:40,101][257371] Max Reward on eval: -134.64606902652307[0m
[37m[1m[2023-07-17 04:29:40,101][257371] Min Reward on eval: -134.64606902652307[0m
[37m[1m[2023-07-17 04:29:40,102][257371] Mean Reward across all agents: -134.64606902652307[0m
[37m[1m[2023-07-17 04:29:40,102][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:29:45,089][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:29:45,090][257371] Reward + Measures: [[ -29.97056591    0.6681        0.68689996    0.3195        0.73009998
     5.67910242]
 [ 374.51619054    0.83169997    0.87509996    0.84219998    0.1148
     5.32133484]
 [  39.91495664    0.1839        0.34450001    0.26519999    0.34960002
     4.8431344 ]
 ...
 [  78.66466852    0.56840003    0.68650001    0.64360005    0.28490004
     5.70085526]
 [ -69.02446408    0.54579997    0.61009997    0.3134        0.63420004
     4.6699338 ]
 [-179.98962907    0.2332        0.92320007    0.62260002    0.92490005
     6.05491018]][0m
[37m[1m[2023-07-17 04:29:45,090][257371] Max Reward on eval: 472.10899164378645[0m
[37m[1m[2023-07-17 04:29:45,090][257371] Min Reward on eval: -561.5842717866414[0m
[37m[1m[2023-07-17 04:29:45,090][257371] Mean Reward across all agents: 11.959705892495734[0m
[37m[1m[2023-07-17 04:29:45,090][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:29:45,096][257371] mean_value=-212.1179003300751, max_value=382.7703904430007[0m
[37m[1m[2023-07-17 04:29:45,098][257371] New mean coefficients: [[ 1.5394566 -2.0220828  3.8717577  2.532842  -1.9292104  6.969207 ]][0m
[37m[1m[2023-07-17 04:29:45,104][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:29:54,047][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 04:29:54,047][257371] FPS: 429448.33[0m
[36m[2023-07-17 04:29:54,050][257371] itr=633, itrs=2000, Progress: 31.65%[0m
[36m[2023-07-17 04:30:05,669][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 04:30:05,669][257371] FPS: 332926.95[0m
[36m[2023-07-17 04:30:09,900][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:30:09,906][257371] Reward + Measures: [[-119.84864588    0.74298859    0.7589007     0.75981236    0.09997667
     7.38799667]][0m
[37m[1m[2023-07-17 04:30:09,906][257371] Max Reward on eval: -119.84864588003683[0m
[37m[1m[2023-07-17 04:30:09,907][257371] Min Reward on eval: -119.84864588003683[0m
[37m[1m[2023-07-17 04:30:09,907][257371] Mean Reward across all agents: -119.84864588003683[0m
[37m[1m[2023-07-17 04:30:09,907][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:30:14,963][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:30:14,968][257371] Reward + Measures: [[ -4.36314826   0.53519994   0.6189       0.19180001   0.46669999
    5.38271904]
 [  2.7305572    0.54070002   0.55730003   0.24299999   0.41850004
    3.84487987]
 [  5.15770375   0.34369999   0.3628       0.24300002   0.17559999
    5.30100489]
 ...
 [-14.36711659   0.41799998   0.4262       0.43200001   0.0394
    5.8330493 ]
 [ 67.83884731   0.57310003   0.55120003   0.5413       0.0783
    5.957654  ]
 [ 19.70860579   0.39650002   0.48210001   0.19660001   0.32259998
    4.18527555]][0m
[37m[1m[2023-07-17 04:30:14,969][257371] Max Reward on eval: 467.2731933396775[0m
[37m[1m[2023-07-17 04:30:14,969][257371] Min Reward on eval: -428.2142701201839[0m
[37m[1m[2023-07-17 04:30:14,969][257371] Mean Reward across all agents: 21.26221200043297[0m
[37m[1m[2023-07-17 04:30:14,970][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:30:14,973][257371] mean_value=-330.4408934369858, max_value=296.0468685751287[0m
[37m[1m[2023-07-17 04:30:14,976][257371] New mean coefficients: [[ 0.86463094 -1.6984639   3.2459142   2.4738023  -1.7410836   7.260435  ]][0m
[37m[1m[2023-07-17 04:30:14,977][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:30:24,043][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 04:30:24,043][257371] FPS: 423629.07[0m
[36m[2023-07-17 04:30:24,045][257371] itr=634, itrs=2000, Progress: 31.70%[0m
[36m[2023-07-17 04:30:35,711][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 04:30:35,711][257371] FPS: 331549.21[0m
[36m[2023-07-17 04:30:39,946][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:30:39,951][257371] Reward + Measures: [[-108.01518901    0.73968399    0.75786895    0.759471      0.10141233
     7.38449621]][0m
[37m[1m[2023-07-17 04:30:39,952][257371] Max Reward on eval: -108.01518900889694[0m
[37m[1m[2023-07-17 04:30:39,952][257371] Min Reward on eval: -108.01518900889694[0m
[37m[1m[2023-07-17 04:30:39,952][257371] Mean Reward across all agents: -108.01518900889694[0m
[37m[1m[2023-07-17 04:30:39,952][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:30:45,158][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:30:45,158][257371] Reward + Measures: [[-122.70303562    0.4962        0.58380002    0.34909999    0.31640002
     6.74415302]
 [ -33.39817391    0.65670007    0.72980005    0.67460006    0.15889999
     5.4464469 ]
 [ -15.4037895     0.52509999    0.67370003    0.37350002    0.44229999
     6.31121588]
 ...
 [-112.78201452    0.64779997    0.85550004    0.32680002    0.67660004
     7.07610798]
 [ -41.8230041     0.68760002    0.72869998    0.42880002    0.76449996
     5.40226364]
 [ -94.25674614    0.4718        0.78969997    0.2899        0.75080007
     5.90331745]][0m
[37m[1m[2023-07-17 04:30:45,159][257371] Max Reward on eval: 337.7023836031556[0m
[37m[1m[2023-07-17 04:30:45,159][257371] Min Reward on eval: -432.5696449105628[0m
[37m[1m[2023-07-17 04:30:45,159][257371] Mean Reward across all agents: -50.48886578794011[0m
[37m[1m[2023-07-17 04:30:45,159][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:30:45,166][257371] mean_value=-244.20097028482076, max_value=419.02434406010553[0m
[37m[1m[2023-07-17 04:30:45,169][257371] New mean coefficients: [[ 0.98365664 -2.3614137   2.7119353   2.9380867  -2.32848     7.938203  ]][0m
[37m[1m[2023-07-17 04:30:45,170][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:30:54,270][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 04:30:54,270][257371] FPS: 422052.29[0m
[36m[2023-07-17 04:30:54,273][257371] itr=635, itrs=2000, Progress: 31.75%[0m
[36m[2023-07-17 04:31:06,054][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 04:31:06,055][257371] FPS: 328266.95[0m
[36m[2023-07-17 04:31:10,321][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:31:10,322][257371] Reward + Measures: [[-100.48611146    0.74536401    0.76756167    0.76640707    0.10350167
     7.3783164 ]][0m
[37m[1m[2023-07-17 04:31:10,322][257371] Max Reward on eval: -100.4861114567189[0m
[37m[1m[2023-07-17 04:31:10,322][257371] Min Reward on eval: -100.4861114567189[0m
[37m[1m[2023-07-17 04:31:10,322][257371] Mean Reward across all agents: -100.4861114567189[0m
[37m[1m[2023-07-17 04:31:10,323][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:31:15,297][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:31:15,298][257371] Reward + Measures: [[-216.8550148     0.25910002    0.74119991    0.58410001    0.63790005
     2.91277599]
 [-434.88370513    0.15470001    0.88360006    0.67640001    0.81470007
     3.20164037]
 [-285.60210848    0.40459999    0.71279997    0.2516        0.69850004
     4.03246784]
 ...
 [-250.60451703    0.27019998    0.74620003    0.48219997    0.67930001
     3.55098271]
 [-140.90584397    0.30399999    0.40120003    0.3951        0.4409
     4.02181482]
 [-101.98748985    0.27659997    0.36690003    0.3594        0.32010001
     4.00090456]][0m
[37m[1m[2023-07-17 04:31:15,298][257371] Max Reward on eval: 334.1962427666411[0m
[37m[1m[2023-07-17 04:31:15,298][257371] Min Reward on eval: -795.7461090266704[0m
[37m[1m[2023-07-17 04:31:15,299][257371] Mean Reward across all agents: -96.76542759960257[0m
[37m[1m[2023-07-17 04:31:15,299][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:31:15,301][257371] mean_value=-715.2889475361563, max_value=213.42294110207342[0m
[37m[1m[2023-07-17 04:31:15,304][257371] New mean coefficients: [[ 0.9168878 -1.7315086  2.1605186  2.9921758 -2.4297318  8.166795 ]][0m
[37m[1m[2023-07-17 04:31:15,305][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:31:24,391][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 04:31:24,391][257371] FPS: 422696.76[0m
[36m[2023-07-17 04:31:24,394][257371] itr=636, itrs=2000, Progress: 31.80%[0m
[36m[2023-07-17 04:31:36,071][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 04:31:36,071][257371] FPS: 331282.44[0m
[36m[2023-07-17 04:31:40,373][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:31:40,379][257371] Reward + Measures: [[-90.47283812   0.75862831   0.77445567   0.77343798   0.09659633
    7.40877295]][0m
[37m[1m[2023-07-17 04:31:40,379][257371] Max Reward on eval: -90.4728381169187[0m
[37m[1m[2023-07-17 04:31:40,379][257371] Min Reward on eval: -90.4728381169187[0m
[37m[1m[2023-07-17 04:31:40,380][257371] Mean Reward across all agents: -90.4728381169187[0m
[37m[1m[2023-07-17 04:31:40,380][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:31:45,358][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:31:45,359][257371] Reward + Measures: [[ 145.57511567    0.70250005    0.66040003    0.65920001    0.0657
     4.50270844]
 [ -93.10759339    0.35670003    0.57310003    0.29879999    0.61460012
     4.13472605]
 [-172.11914906    0.40090004    0.43889999    0.0852        0.42820001
     5.02314997]
 ...
 [-120.69530087    0.49770004    0.53759998    0.1486        0.53060001
     4.53676367]
 [  45.88476362    0.44469997    0.40289998    0.3497        0.28330001
     4.96380186]
 [-153.59527973    0.45809999    0.91259998    0.43970004    0.77400005
     6.23330784]][0m
[37m[1m[2023-07-17 04:31:45,359][257371] Max Reward on eval: 311.8013407167047[0m
[37m[1m[2023-07-17 04:31:45,359][257371] Min Reward on eval: -746.3124732756987[0m
[37m[1m[2023-07-17 04:31:45,359][257371] Mean Reward across all agents: -150.05150049882624[0m
[37m[1m[2023-07-17 04:31:45,360][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:31:45,363][257371] mean_value=-582.5727906557797, max_value=163.76790055082583[0m
[37m[1m[2023-07-17 04:31:45,366][257371] New mean coefficients: [[ 0.25818795 -1.404298    1.8502586   2.2814634  -1.5961154   7.736382  ]][0m
[37m[1m[2023-07-17 04:31:45,367][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:31:54,320][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 04:31:54,320][257371] FPS: 428981.28[0m
[36m[2023-07-17 04:31:54,322][257371] itr=637, itrs=2000, Progress: 31.85%[0m
[36m[2023-07-17 04:32:06,013][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 04:32:06,013][257371] FPS: 330791.45[0m
[36m[2023-07-17 04:32:10,267][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:32:10,267][257371] Reward + Measures: [[-77.31617779   0.768583     0.78437996   0.78632534   0.08580133
    7.4206171 ]][0m
[37m[1m[2023-07-17 04:32:10,268][257371] Max Reward on eval: -77.316177787073[0m
[37m[1m[2023-07-17 04:32:10,268][257371] Min Reward on eval: -77.316177787073[0m
[37m[1m[2023-07-17 04:32:10,268][257371] Mean Reward across all agents: -77.316177787073[0m
[37m[1m[2023-07-17 04:32:10,268][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:32:15,306][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:32:15,312][257371] Reward + Measures: [[347.28821466   0.28670001   0.71100003   0.3527       0.67960006
    6.11287689]
 [319.11286688   0.34920001   0.69480002   0.30219999   0.60680002
    5.7959609 ]
 [-66.06708181   0.29260001   0.46300003   0.34380001   0.48629999
    4.66837931]
 ...
 [ 72.23482613   0.73769999   0.83470005   0.6354       0.26680002
    6.4828372 ]
 [305.71089815   0.43849999   0.79439998   0.27800003   0.76980001
    6.16773319]
 [364.94903473   0.57319999   0.55879998   0.57950002   0.0738
    5.56859827]][0m
[37m[1m[2023-07-17 04:32:15,312][257371] Max Reward on eval: 512.8864698429592[0m
[37m[1m[2023-07-17 04:32:15,313][257371] Min Reward on eval: -425.79642396960406[0m
[37m[1m[2023-07-17 04:32:15,313][257371] Mean Reward across all agents: 65.10041386056652[0m
[37m[1m[2023-07-17 04:32:15,313][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:32:15,319][257371] mean_value=-381.6968079725001, max_value=413.1071155680838[0m
[37m[1m[2023-07-17 04:32:15,322][257371] New mean coefficients: [[ 0.02633736 -2.8583302   2.2384336   3.0079744  -3.480917    7.7918735 ]][0m
[37m[1m[2023-07-17 04:32:15,323][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:32:24,381][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 04:32:24,381][257371] FPS: 424024.94[0m
[36m[2023-07-17 04:32:24,383][257371] itr=638, itrs=2000, Progress: 31.90%[0m
[36m[2023-07-17 04:32:36,118][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 04:32:36,119][257371] FPS: 329531.95[0m
[36m[2023-07-17 04:32:40,400][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:32:40,401][257371] Reward + Measures: [[-70.56009999   0.76840705   0.78927368   0.78382224   0.092535
    7.44017839]][0m
[37m[1m[2023-07-17 04:32:40,401][257371] Max Reward on eval: -70.56009999368456[0m
[37m[1m[2023-07-17 04:32:40,401][257371] Min Reward on eval: -70.56009999368456[0m
[37m[1m[2023-07-17 04:32:40,401][257371] Mean Reward across all agents: -70.56009999368456[0m
[37m[1m[2023-07-17 04:32:40,402][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:32:45,393][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:32:45,394][257371] Reward + Measures: [[-272.72206851    0.71100003    0.81159991    0.0636        0.85770005
     5.3249383 ]
 [-212.40248063    0.56650001    0.82489997    0.186         0.81190008
     5.70882654]
 [  -2.46195985    0.83490002    0.83640003    0.83859998    0.0406
     5.4224534 ]
 ...
 [ -46.76355008    0.56059998    0.73140001    0.45390001    0.45889997
     4.75165081]
 [  73.91402381    0.60900003    0.87360001    0.62599999    0.46650001
     5.33591604]
 [-197.26717184    0.50240004    0.70719999    0.32780004    0.5478
     5.06334829]][0m
[37m[1m[2023-07-17 04:32:45,394][257371] Max Reward on eval: 346.17149259662256[0m
[37m[1m[2023-07-17 04:32:45,395][257371] Min Reward on eval: -429.0449848102406[0m
[37m[1m[2023-07-17 04:32:45,395][257371] Mean Reward across all agents: -62.41111980063659[0m
[37m[1m[2023-07-17 04:32:45,395][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:32:45,400][257371] mean_value=-389.01624168385814, max_value=603.2356416819617[0m
[37m[1m[2023-07-17 04:32:45,454][257371] New mean coefficients: [[ 0.171395  -2.6425116  2.1225762  3.1268957 -2.8451521  7.765102 ]][0m
[37m[1m[2023-07-17 04:32:45,455][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:32:54,501][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 04:32:54,501][257371] FPS: 424590.43[0m
[36m[2023-07-17 04:32:54,503][257371] itr=639, itrs=2000, Progress: 31.95%[0m
[36m[2023-07-17 04:33:06,245][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 04:33:06,245][257371] FPS: 329399.71[0m
[36m[2023-07-17 04:33:10,562][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:33:10,562][257371] Reward + Measures: [[-65.01301426   0.77764761   0.79710865   0.79297835   0.08588333
    7.45987177]][0m
[37m[1m[2023-07-17 04:33:10,563][257371] Max Reward on eval: -65.01301425626687[0m
[37m[1m[2023-07-17 04:33:10,563][257371] Min Reward on eval: -65.01301425626687[0m
[37m[1m[2023-07-17 04:33:10,563][257371] Mean Reward across all agents: -65.01301425626687[0m
[37m[1m[2023-07-17 04:33:10,563][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:33:15,840][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:33:15,892][257371] Reward + Measures: [[-157.92571852    0.73330003    0.84679997    0.1028        0.79800004
     4.30712509]
 [-113.72916539    0.53960001    0.6164        0.0842        0.65939999
     4.72925425]
 [ -98.39054018    0.62650001    0.58890003    0.54939997    0.1934
     4.71781778]
 ...
 [-153.25984241    0.62690002    0.72489995    0.15840001    0.65979999
     4.62238216]
 [ 109.35488871    0.41149998    0.40780002    0.28309998    0.41780001
     4.18245411]
 [-110.09085988    0.3849        0.39320001    0.15809999    0.33420002
     4.52352762]][0m
[37m[1m[2023-07-17 04:33:15,893][257371] Max Reward on eval: 719.8707199189812[0m
[37m[1m[2023-07-17 04:33:15,893][257371] Min Reward on eval: -316.1961493063718[0m
[37m[1m[2023-07-17 04:33:15,893][257371] Mean Reward across all agents: 33.54623588824778[0m
[37m[1m[2023-07-17 04:33:15,893][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:33:15,897][257371] mean_value=-690.6152939990226, max_value=705.5236986738483[0m
[37m[1m[2023-07-17 04:33:15,899][257371] New mean coefficients: [[ 0.33820394 -1.6826382   1.979433    2.7070372  -3.6541417   7.554943  ]][0m
[37m[1m[2023-07-17 04:33:15,900][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:33:25,018][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 04:33:25,018][257371] FPS: 421244.87[0m
[36m[2023-07-17 04:33:25,020][257371] itr=640, itrs=2000, Progress: 32.00%[0m
[37m[1m[2023-07-17 04:36:22,107][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000620[0m
[36m[2023-07-17 04:36:34,385][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 04:36:34,386][257371] FPS: 328421.91[0m
[36m[2023-07-17 04:36:38,710][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:36:38,710][257371] Reward + Measures: [[-66.2559195    0.81405932   0.82512665   0.81817496   0.07040967
    7.4944973 ]][0m
[37m[1m[2023-07-17 04:36:38,710][257371] Max Reward on eval: -66.25591950452191[0m
[37m[1m[2023-07-17 04:36:38,711][257371] Min Reward on eval: -66.25591950452191[0m
[37m[1m[2023-07-17 04:36:38,711][257371] Mean Reward across all agents: -66.25591950452191[0m
[37m[1m[2023-07-17 04:36:38,711][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:36:43,917][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:36:43,917][257371] Reward + Measures: [[137.45682443   0.29130003   0.38850003   0.28600001   0.3955
    3.40046   ]
 [ 65.00549104   0.3197       0.36100003   0.28210002   0.39989996
    3.09874845]
 [114.17954553   0.35370001   0.33790001   0.33600003   0.31710002
    3.17500663]
 ...
 [126.64183042   0.21820001   0.3712       0.20930003   0.36919999
    4.20521832]
 [-79.18240639   0.95459998   0.94600004   0.93580002   0.0159
    5.81973505]
 [184.49503801   0.2431       0.29140002   0.26610002   0.3504
    3.16063857]][0m
[37m[1m[2023-07-17 04:36:43,918][257371] Max Reward on eval: 291.5631623947993[0m
[37m[1m[2023-07-17 04:36:43,918][257371] Min Reward on eval: -368.45986364670097[0m
[37m[1m[2023-07-17 04:36:43,918][257371] Mean Reward across all agents: 40.63013184891393[0m
[37m[1m[2023-07-17 04:36:43,918][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:36:43,920][257371] mean_value=-1403.4358572978083, max_value=51.96312745220156[0m
[37m[1m[2023-07-17 04:36:43,923][257371] New mean coefficients: [[ 0.23546778 -1.5759829   2.4680257   2.4830701  -3.1331038   6.8594003 ]][0m
[37m[1m[2023-07-17 04:36:43,924][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:36:53,005][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 04:36:53,006][257371] FPS: 422920.82[0m
[36m[2023-07-17 04:36:53,008][257371] itr=641, itrs=2000, Progress: 32.05%[0m
[36m[2023-07-17 04:37:04,865][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 04:37:04,866][257371] FPS: 326259.43[0m
[36m[2023-07-17 04:37:09,137][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:37:09,138][257371] Reward + Measures: [[-57.56592878   0.81050259   0.81801569   0.81712437   0.06403067
    7.50793743]][0m
[37m[1m[2023-07-17 04:37:09,138][257371] Max Reward on eval: -57.56592877751938[0m
[37m[1m[2023-07-17 04:37:09,138][257371] Min Reward on eval: -57.56592877751938[0m
[37m[1m[2023-07-17 04:37:09,139][257371] Mean Reward across all agents: -57.56592877751938[0m
[37m[1m[2023-07-17 04:37:09,139][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:37:14,148][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:37:14,149][257371] Reward + Measures: [[  38.41371808    0.33140001    0.82819998    0.62330002    0.86359996
     4.47121429]
 [  86.98012414    0.6868        0.68099999    0.7101        0.0761
     5.8358717 ]
 [ 163.31644863    0.62110001    0.65420002    0.32890001    0.70380002
     4.99545431]
 ...
 [-116.10971027    0.92860001    0.91499996    0.9156        0.35409999
     4.84353209]
 [  -9.65775108    0.17690001    0.81050009    0.61989999    0.81770003
     4.68435049]
 [  70.36856255    0.45179996    0.41949996    0.42279997    0.07309999
     5.05596924]][0m
[37m[1m[2023-07-17 04:37:14,149][257371] Max Reward on eval: 488.2875984216109[0m
[37m[1m[2023-07-17 04:37:14,149][257371] Min Reward on eval: -326.3425608383492[0m
[37m[1m[2023-07-17 04:37:14,149][257371] Mean Reward across all agents: 28.26846391315779[0m
[37m[1m[2023-07-17 04:37:14,149][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:37:14,156][257371] mean_value=-396.43900080540067, max_value=536.68816923383[0m
[37m[1m[2023-07-17 04:37:14,159][257371] New mean coefficients: [[-0.44175774 -2.7715507   2.0512385   2.293454   -3.6874266   7.3778095 ]][0m
[37m[1m[2023-07-17 04:37:14,160][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:37:23,178][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 04:37:23,178][257371] FPS: 425926.96[0m
[36m[2023-07-17 04:37:23,180][257371] itr=642, itrs=2000, Progress: 32.10%[0m
[36m[2023-07-17 04:37:34,907][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 04:37:34,907][257371] FPS: 329876.07[0m
[36m[2023-07-17 04:37:39,189][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:37:39,190][257371] Reward + Measures: [[-51.07615066   0.81794333   0.82215267   0.82105726   0.06077133
    7.51448631]][0m
[37m[1m[2023-07-17 04:37:39,190][257371] Max Reward on eval: -51.076150663357886[0m
[37m[1m[2023-07-17 04:37:39,190][257371] Min Reward on eval: -51.076150663357886[0m
[37m[1m[2023-07-17 04:37:39,190][257371] Mean Reward across all agents: -51.076150663357886[0m
[37m[1m[2023-07-17 04:37:39,191][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:37:44,130][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:37:44,136][257371] Reward + Measures: [[-31.30642786   0.18350001   0.1796       0.1693       0.13590001
    3.60283065]
 [-14.21885707   0.2253       0.1952       0.16760001   0.164
    3.97908378]
 [-35.04873591   0.23349999   0.21530001   0.2191       0.11920001
    4.25605392]
 ...
 [-39.51556748   0.17209999   0.17340001   0.1594       0.11359999
    3.53906226]
 [172.70394664   0.60670006   0.57980001   0.58179998   0.05800001
    4.98985672]
 [433.28987886   0.85539991   0.80930007   0.79700005   0.0231
    5.78872442]][0m
[37m[1m[2023-07-17 04:37:44,136][257371] Max Reward on eval: 730.2941169683821[0m
[37m[1m[2023-07-17 04:37:44,136][257371] Min Reward on eval: -155.60685111228378[0m
[37m[1m[2023-07-17 04:37:44,137][257371] Mean Reward across all agents: 83.68974986884814[0m
[37m[1m[2023-07-17 04:37:44,137][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:37:44,139][257371] mean_value=-1886.620301584946, max_value=242.02194217527847[0m
[37m[1m[2023-07-17 04:37:44,142][257371] New mean coefficients: [[-0.2953532 -2.0962937  1.9802425  2.391773  -1.6934814  7.6585755]][0m
[37m[1m[2023-07-17 04:37:44,143][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:37:53,095][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 04:37:53,096][257371] FPS: 428992.10[0m
[36m[2023-07-17 04:37:53,098][257371] itr=643, itrs=2000, Progress: 32.15%[0m
[36m[2023-07-17 04:38:04,830][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 04:38:04,831][257371] FPS: 329717.05[0m
[36m[2023-07-17 04:38:09,128][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:38:09,129][257371] Reward + Measures: [[-44.35657132   0.8294974    0.83552033   0.83575571   0.06043633
    7.52574015]][0m
[37m[1m[2023-07-17 04:38:09,129][257371] Max Reward on eval: -44.35657131896944[0m
[37m[1m[2023-07-17 04:38:09,129][257371] Min Reward on eval: -44.35657131896944[0m
[37m[1m[2023-07-17 04:38:09,130][257371] Mean Reward across all agents: -44.35657131896944[0m
[37m[1m[2023-07-17 04:38:09,130][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:38:14,144][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:38:14,149][257371] Reward + Measures: [[ 170.93617866    0.64600003    0.83700001    0.63829994    0.31040001
     6.54972219]
 [ -30.21341941    0.69490004    0.6742        0.46430001    0.29720005
     5.0136447 ]
 [ -20.19729494    0.87040007    0.86330003    0.8023001     0.0998
     5.08398533]
 ...
 [  45.60599375    0.81099999    0.78869998    0.69570005    0.11059999
     5.85871029]
 [  78.64442618    0.73130006    0.70340002    0.52759999    0.2086
     4.85458851]
 [-167.10618114    0.76570004    0.96510011    0.32539999    0.74440002
     6.68528891]][0m
[37m[1m[2023-07-17 04:38:14,150][257371] Max Reward on eval: 342.48240279834715[0m
[37m[1m[2023-07-17 04:38:14,150][257371] Min Reward on eval: -503.60704041449355[0m
[37m[1m[2023-07-17 04:38:14,150][257371] Mean Reward across all agents: -4.777380209063628[0m
[37m[1m[2023-07-17 04:38:14,150][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:38:14,155][257371] mean_value=-449.2119005699031, max_value=395.70656282973175[0m
[37m[1m[2023-07-17 04:38:14,158][257371] New mean coefficients: [[-0.06685176 -1.9506259   2.2008169   2.5330849  -0.6161444   7.7199316 ]][0m
[37m[1m[2023-07-17 04:38:14,159][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:38:23,126][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 04:38:23,126][257371] FPS: 428309.74[0m
[36m[2023-07-17 04:38:23,128][257371] itr=644, itrs=2000, Progress: 32.20%[0m
[36m[2023-07-17 04:38:35,085][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-17 04:38:35,086][257371] FPS: 323494.90[0m
[36m[2023-07-17 04:38:39,403][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:38:39,403][257371] Reward + Measures: [[-41.96400989   0.8238973    0.82387835   0.82655036   0.05585633
    7.50364828]][0m
[37m[1m[2023-07-17 04:38:39,404][257371] Max Reward on eval: -41.96400988598284[0m
[37m[1m[2023-07-17 04:38:39,404][257371] Min Reward on eval: -41.96400988598284[0m
[37m[1m[2023-07-17 04:38:39,404][257371] Mean Reward across all agents: -41.96400988598284[0m
[37m[1m[2023-07-17 04:38:39,404][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:38:44,360][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:38:44,361][257371] Reward + Measures: [[ 55.27635327   0.84109992   0.90819997   0.79720002   0.18090001
    5.96823692]
 [ 26.81939449   0.17040001   0.16880001   0.1647       0.1285
    3.97559404]
 [-30.57723852   0.1825       0.17440002   0.16580001   0.14560001
    4.44190693]
 ...
 [ 10.24487693   0.1926       0.18280001   0.19059999   0.08990001
    4.67382288]
 [-78.70765219   0.51190001   0.42249998   0.41759998   0.1178
    4.61436605]
 [ 27.74682374   0.1736       0.19700001   0.1548       0.1742
    4.102633  ]][0m
[37m[1m[2023-07-17 04:38:44,361][257371] Max Reward on eval: 470.55426860526205[0m
[37m[1m[2023-07-17 04:38:44,361][257371] Min Reward on eval: -391.50087950630115[0m
[37m[1m[2023-07-17 04:38:44,361][257371] Mean Reward across all agents: 23.28430358723329[0m
[37m[1m[2023-07-17 04:38:44,362][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:38:44,364][257371] mean_value=-1690.5003112681716, max_value=174.7851559644227[0m
[37m[1m[2023-07-17 04:38:44,367][257371] New mean coefficients: [[-0.67810965 -0.6932665   1.3157067   1.9569676   0.945784    7.657212  ]][0m
[37m[1m[2023-07-17 04:38:44,367][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:38:53,383][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 04:38:53,383][257371] FPS: 426012.33[0m
[36m[2023-07-17 04:38:53,385][257371] itr=645, itrs=2000, Progress: 32.25%[0m
[36m[2023-07-17 04:39:05,155][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 04:39:05,155][257371] FPS: 328563.54[0m
[36m[2023-07-17 04:39:09,491][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:39:09,491][257371] Reward + Measures: [[-41.54561032   0.81062436   0.82195067   0.82038736   0.06813899
    7.49694729]][0m
[37m[1m[2023-07-17 04:39:09,492][257371] Max Reward on eval: -41.54561032030479[0m
[37m[1m[2023-07-17 04:39:09,492][257371] Min Reward on eval: -41.54561032030479[0m
[37m[1m[2023-07-17 04:39:09,492][257371] Mean Reward across all agents: -41.54561032030479[0m
[37m[1m[2023-07-17 04:39:09,492][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:39:14,474][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:39:14,475][257371] Reward + Measures: [[ 203.79882618    0.87769997    0.95269996    0.63560003    0.37709999
     6.34197187]
 [-182.12655532    0.7051        0.79119998    0.60120004    0.23010002
     5.92670298]
 [  74.95929178    0.87140006    0.8599        0.84530002    0.0269
     6.95056391]
 ...
 [ 108.92851446    0.3048        0.7744        0.37369999    0.79890001
     5.07564163]
 [  26.93452082    0.40290004    0.32990003    0.33469999    0.0862
     4.69164562]
 [ 143.11942673    0.66620004    0.72320002    0.58599997    0.17450002
     5.7571106 ]][0m
[37m[1m[2023-07-17 04:39:14,475][257371] Max Reward on eval: 777.3828201225027[0m
[37m[1m[2023-07-17 04:39:14,475][257371] Min Reward on eval: -304.8649418771267[0m
[37m[1m[2023-07-17 04:39:14,475][257371] Mean Reward across all agents: 81.58741151319451[0m
[37m[1m[2023-07-17 04:39:14,476][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:39:14,481][257371] mean_value=-253.16552870166933, max_value=763.8304807937518[0m
[37m[1m[2023-07-17 04:39:14,484][257371] New mean coefficients: [[-0.07505417 -0.53838205  1.1565152   2.0173702   1.1074493   8.734396  ]][0m
[37m[1m[2023-07-17 04:39:14,485][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:39:23,506][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 04:39:23,507][257371] FPS: 425715.61[0m
[36m[2023-07-17 04:39:23,509][257371] itr=646, itrs=2000, Progress: 32.30%[0m
[36m[2023-07-17 04:39:35,329][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 04:39:35,329][257371] FPS: 327280.45[0m
[36m[2023-07-17 04:39:39,697][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:39:39,697][257371] Reward + Measures: [[-47.68693789   0.81130934   0.81507427   0.81696004   0.06021333
    7.48480797]][0m
[37m[1m[2023-07-17 04:39:39,697][257371] Max Reward on eval: -47.68693788956079[0m
[37m[1m[2023-07-17 04:39:39,698][257371] Min Reward on eval: -47.68693788956079[0m
[37m[1m[2023-07-17 04:39:39,698][257371] Mean Reward across all agents: -47.68693788956079[0m
[37m[1m[2023-07-17 04:39:39,698][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:39:44,977][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:39:44,984][257371] Reward + Measures: [[ -6.24329633   0.17029999   0.29780003   0.27920002   0.27590001
    3.7933712 ]
 [-44.301617     0.48680001   0.47         0.48570004   0.0948
    3.66032767]
 [ 17.97705765   0.1275       0.16860001   0.1164       0.19100001
    3.73706222]
 ...
 [-43.49344514   0.38720003   0.34050003   0.30770001   0.1825
    3.67083049]
 [  0.39449037   0.5359       0.57690001   0.1842       0.49280006
    4.57451963]
 [ 61.70970823   0.68510002   0.6728       0.65930003   0.0496
    4.98811293]][0m
[37m[1m[2023-07-17 04:39:44,984][257371] Max Reward on eval: 606.2674140810966[0m
[37m[1m[2023-07-17 04:39:44,985][257371] Min Reward on eval: -245.71849439023063[0m
[37m[1m[2023-07-17 04:39:44,985][257371] Mean Reward across all agents: 60.095232741485376[0m
[37m[1m[2023-07-17 04:39:44,985][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:39:44,987][257371] mean_value=-923.3606521567842, max_value=435.954361182577[0m
[37m[1m[2023-07-17 04:39:44,990][257371] New mean coefficients: [[-0.6645193  -0.41104186  0.67343205  1.6380528   0.6927496   8.958629  ]][0m
[37m[1m[2023-07-17 04:39:44,991][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:39:54,033][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 04:39:54,033][257371] FPS: 424794.26[0m
[36m[2023-07-17 04:39:54,035][257371] itr=647, itrs=2000, Progress: 32.35%[0m
[36m[2023-07-17 04:40:05,752][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 04:40:05,752][257371] FPS: 330064.17[0m
[36m[2023-07-17 04:40:10,067][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:40:10,067][257371] Reward + Measures: [[-44.42992776   0.82328898   0.82634032   0.826572     0.05965733
    7.49259949]][0m
[37m[1m[2023-07-17 04:40:10,068][257371] Max Reward on eval: -44.42992776348531[0m
[37m[1m[2023-07-17 04:40:10,068][257371] Min Reward on eval: -44.42992776348531[0m
[37m[1m[2023-07-17 04:40:10,068][257371] Mean Reward across all agents: -44.42992776348531[0m
[37m[1m[2023-07-17 04:40:10,068][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:40:15,092][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:40:15,093][257371] Reward + Measures: [[  -1.1553005     0.40500003    0.33890003    0.36219999    0.15539999
     4.49865723]
 [ 108.68617907    0.4966        0.47690001    0.52820003    0.05520001
     5.44716549]
 [-102.47708948    0.57650006    0.65370005    0.56649995    0.15530001
     6.16771269]
 ...
 [ 182.38426034    0.90160006    0.92129993    0.5043        0.40950003
     6.38811207]
 [ 287.09273552    0.56750005    0.56110001    0.51419997    0.103
     6.0501256 ]
 [   7.93530166    0.77600002    0.78239995    0.74680001    0.0545
     6.21342087]][0m
[37m[1m[2023-07-17 04:40:15,093][257371] Max Reward on eval: 479.7359561689198[0m
[37m[1m[2023-07-17 04:40:15,093][257371] Min Reward on eval: -336.06054595831085[0m
[37m[1m[2023-07-17 04:40:15,093][257371] Mean Reward across all agents: 35.631795781605014[0m
[37m[1m[2023-07-17 04:40:15,094][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:40:15,098][257371] mean_value=-374.0313701409599, max_value=478.9745718292665[0m
[37m[1m[2023-07-17 04:40:15,101][257371] New mean coefficients: [[-0.9280088 -0.7767057  0.9095628  1.5103116 -0.6905416  8.869808 ]][0m
[37m[1m[2023-07-17 04:40:15,102][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:40:24,147][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 04:40:24,147][257371] FPS: 424588.25[0m
[36m[2023-07-17 04:40:24,150][257371] itr=648, itrs=2000, Progress: 32.40%[0m
[36m[2023-07-17 04:40:35,903][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 04:40:35,903][257371] FPS: 328997.42[0m
[36m[2023-07-17 04:40:40,164][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:40:40,164][257371] Reward + Measures: [[-43.69398379   0.81174427   0.82234132   0.82163501   0.068098
    7.47978115]][0m
[37m[1m[2023-07-17 04:40:40,165][257371] Max Reward on eval: -43.693983794159294[0m
[37m[1m[2023-07-17 04:40:40,165][257371] Min Reward on eval: -43.693983794159294[0m
[37m[1m[2023-07-17 04:40:40,165][257371] Mean Reward across all agents: -43.693983794159294[0m
[37m[1m[2023-07-17 04:40:40,165][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:40:45,144][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:40:45,145][257371] Reward + Measures: [[-124.36588726    0.60620004    0.70139998    0.53890002    0.27519998
     4.36220312]
 [ -16.06241788    0.46160004    0.44169998    0.42420003    0.3152
     3.32838559]
 [  28.80569678    0.45360002    0.47830001    0.2098        0.39790004
     5.24143219]
 ...
 [ -59.66337156    0.39349997    0.45429999    0.3511        0.24160002
     4.38859558]
 [ -30.60845753    0.55800003    0.54070002    0.154         0.47089997
     6.15491247]
 [  -8.53885805    0.6559        0.62419999    0.19579999    0.52850002
     6.10792351]][0m
[37m[1m[2023-07-17 04:40:45,145][257371] Max Reward on eval: 598.4092521566897[0m
[37m[1m[2023-07-17 04:40:45,145][257371] Min Reward on eval: -221.2777550497558[0m
[37m[1m[2023-07-17 04:40:45,145][257371] Mean Reward across all agents: 42.23649119751546[0m
[37m[1m[2023-07-17 04:40:45,146][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:40:45,149][257371] mean_value=-440.4634100929609, max_value=247.40974052117002[0m
[37m[1m[2023-07-17 04:40:45,152][257371] New mean coefficients: [[-1.594609  -2.1587703  0.7608216  1.1848009 -1.9149716  8.751234 ]][0m
[37m[1m[2023-07-17 04:40:45,153][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:40:54,232][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 04:40:54,232][257371] FPS: 423020.69[0m
[36m[2023-07-17 04:40:54,234][257371] itr=649, itrs=2000, Progress: 32.45%[0m
[36m[2023-07-17 04:41:05,914][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 04:41:05,914][257371] FPS: 331203.87[0m
[36m[2023-07-17 04:41:10,253][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:41:10,253][257371] Reward + Measures: [[-57.83582031   0.82207191   0.82560766   0.82700831   0.05784466
    7.48434687]][0m
[37m[1m[2023-07-17 04:41:10,253][257371] Max Reward on eval: -57.83582030893928[0m
[37m[1m[2023-07-17 04:41:10,254][257371] Min Reward on eval: -57.83582030893928[0m
[37m[1m[2023-07-17 04:41:10,254][257371] Mean Reward across all agents: -57.83582030893928[0m
[37m[1m[2023-07-17 04:41:10,254][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:41:15,302][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:41:15,303][257371] Reward + Measures: [[-622.17003249    0.83459997    0.98899996    0.0963        0.99340004
     7.73210096]
 [  29.46664643    0.87760001    0.90290004    0.86550009    0.92409992
     6.30155182]
 [  78.8595724     0.64300007    0.97760004    0.1647        0.97220004
     6.45826578]
 ...
 [ 357.42878437    0.55320001    0.98800004    0.26540002    0.98710006
     6.40372992]
 [-589.38581659    0.87559998    0.98780006    0.06460001    0.98859996
     7.42320108]
 [  32.9001863     0.5316        0.66570008    0.50419998    0.72049999
     4.8307991 ]][0m
[37m[1m[2023-07-17 04:41:15,303][257371] Max Reward on eval: 566.2814483799041[0m
[37m[1m[2023-07-17 04:41:15,303][257371] Min Reward on eval: -742.1358337270794[0m
[37m[1m[2023-07-17 04:41:15,304][257371] Mean Reward across all agents: -68.6962485908341[0m
[37m[1m[2023-07-17 04:41:15,304][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:41:15,311][257371] mean_value=-316.09614186396743, max_value=461.5684843344682[0m
[37m[1m[2023-07-17 04:41:15,314][257371] New mean coefficients: [[-1.673031  -2.3907692  0.8202275  1.346775  -3.2728186  8.693    ]][0m
[37m[1m[2023-07-17 04:41:15,315][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:41:24,462][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-17 04:41:24,463][257371] FPS: 419875.26[0m
[36m[2023-07-17 04:41:24,465][257371] itr=650, itrs=2000, Progress: 32.50%[0m
[37m[1m[2023-07-17 04:44:23,882][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000630[0m
[36m[2023-07-17 04:44:36,242][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 04:44:36,243][257371] FPS: 326643.20[0m
[36m[2023-07-17 04:44:40,519][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:44:40,520][257371] Reward + Measures: [[-62.94172696   0.81569302   0.82220298   0.8228333    0.06353199
    7.47494125]][0m
[37m[1m[2023-07-17 04:44:40,520][257371] Max Reward on eval: -62.94172695888897[0m
[37m[1m[2023-07-17 04:44:40,520][257371] Min Reward on eval: -62.94172695888897[0m
[37m[1m[2023-07-17 04:44:40,520][257371] Mean Reward across all agents: -62.94172695888897[0m
[37m[1m[2023-07-17 04:44:40,521][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:44:45,559][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:44:45,560][257371] Reward + Measures: [[-11.72457291   0.3766       0.58180004   0.34560001   0.39859998
    5.8130579 ]
 [134.08257867   0.71030003   0.90810007   0.52829999   0.53319997
    6.24730921]
 [127.78766347   0.84639996   0.91499996   0.75340003   0.2052
    6.13861513]
 ...
 [-32.35300646   0.52100003   0.57870001   0.39480001   0.55730003
    5.50375748]
 [441.1345816    0.53899997   0.72710001   0.2554       0.60530001
    6.27706718]
 [-43.34189475   0.51659995   0.40309998   0.46010002   0.59820002
    5.82164001]][0m
[37m[1m[2023-07-17 04:44:45,560][257371] Max Reward on eval: 677.7840244691819[0m
[37m[1m[2023-07-17 04:44:45,560][257371] Min Reward on eval: -288.926296219416[0m
[37m[1m[2023-07-17 04:44:45,561][257371] Mean Reward across all agents: 53.68932461013878[0m
[37m[1m[2023-07-17 04:44:45,561][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:44:45,566][257371] mean_value=-297.42000332385066, max_value=687.4964209688314[0m
[37m[1m[2023-07-17 04:44:45,569][257371] New mean coefficients: [[-1.0601709 -3.703207   1.3862193  1.7850828 -4.402273   9.05504  ]][0m
[37m[1m[2023-07-17 04:44:45,570][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:44:54,486][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-17 04:44:54,486][257371] FPS: 430772.97[0m
[36m[2023-07-17 04:44:54,489][257371] itr=651, itrs=2000, Progress: 32.55%[0m
[36m[2023-07-17 04:45:06,192][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 04:45:06,192][257371] FPS: 330480.53[0m
[36m[2023-07-17 04:45:10,437][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:45:10,438][257371] Reward + Measures: [[-70.34706862   0.80979759   0.81596404   0.81644464   0.06656767
    7.49350071]][0m
[37m[1m[2023-07-17 04:45:10,438][257371] Max Reward on eval: -70.3470686167228[0m
[37m[1m[2023-07-17 04:45:10,438][257371] Min Reward on eval: -70.3470686167228[0m
[37m[1m[2023-07-17 04:45:10,438][257371] Mean Reward across all agents: -70.3470686167228[0m
[37m[1m[2023-07-17 04:45:10,439][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:45:15,443][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:45:15,444][257371] Reward + Measures: [[ -88.34432843    0.59060001    0.56669998    0.6051001     0.10950001
     3.83082557]
 [ 324.85419738    0.77380002    0.75650007    0.76319999    0.0332
     5.6678586 ]
 [-130.83241009    0.77100003    0.81400007    0.59400004    0.28220001
     5.19491434]
 ...
 [ -46.20971711    0.82100004    0.82050002    0.74910003    0.1426
     4.60636759]
 [-141.35577533    0.68580002    0.68700004    0.6821        0.1337
     4.04886961]
 [  -9.581616      0.8132        0.88609999    0.2552        0.70270008
     6.13416243]][0m
[37m[1m[2023-07-17 04:45:15,444][257371] Max Reward on eval: 402.7542787004262[0m
[37m[1m[2023-07-17 04:45:15,444][257371] Min Reward on eval: -570.8453941836954[0m
[37m[1m[2023-07-17 04:45:15,445][257371] Mean Reward across all agents: -82.83901690997489[0m
[37m[1m[2023-07-17 04:45:15,445][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:45:15,448][257371] mean_value=-315.8765194503841, max_value=154.7742416884971[0m
[37m[1m[2023-07-17 04:45:15,451][257371] New mean coefficients: [[-1.195297  -3.8897343  0.5883579  1.9778944 -3.6570308  9.454358 ]][0m
[37m[1m[2023-07-17 04:45:15,452][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:45:24,493][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 04:45:24,493][257371] FPS: 424794.90[0m
[36m[2023-07-17 04:45:24,496][257371] itr=652, itrs=2000, Progress: 32.60%[0m
[36m[2023-07-17 04:45:36,142][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 04:45:36,142][257371] FPS: 332076.00[0m
[36m[2023-07-17 04:45:40,468][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:45:40,468][257371] Reward + Measures: [[-70.51648569   0.82033664   0.83664298   0.83150029   0.07034867
    7.51619005]][0m
[37m[1m[2023-07-17 04:45:40,468][257371] Max Reward on eval: -70.51648568578685[0m
[37m[1m[2023-07-17 04:45:40,469][257371] Min Reward on eval: -70.51648568578685[0m
[37m[1m[2023-07-17 04:45:40,469][257371] Mean Reward across all agents: -70.51648568578685[0m
[37m[1m[2023-07-17 04:45:40,469][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:45:45,498][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:45:45,499][257371] Reward + Measures: [[-215.02713678    0.23469999    0.81569999    0.45210004    0.83070004
     4.86752462]
 [ 238.4626322     0.39860001    0.83109999    0.7863        0.4824
     5.90363407]
 [ 203.32934023    0.71740001    0.69550002    0.67600006    0.0375
     5.77541542]
 ...
 [-162.24972558    0.56760001    0.83420002    0.20469999    0.7841
     5.95310163]
 [ -23.84524879    0.1098        0.73560005    0.63150001    0.72290003
     6.85575199]
 [-216.50118965    0.113         0.93710005    0.80010003    0.92149991
     7.43209076]][0m
[37m[1m[2023-07-17 04:45:45,499][257371] Max Reward on eval: 836.9147796850651[0m
[37m[1m[2023-07-17 04:45:45,499][257371] Min Reward on eval: -554.5933380587492[0m
[37m[1m[2023-07-17 04:45:45,499][257371] Mean Reward across all agents: -18.745190181526493[0m
[37m[1m[2023-07-17 04:45:45,500][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:45:45,505][257371] mean_value=-347.89220935223506, max_value=811.9006376851926[0m
[37m[1m[2023-07-17 04:45:45,508][257371] New mean coefficients: [[-1.086791   -2.99166    -0.26367366  1.5338043  -2.5263197   9.914518  ]][0m
[37m[1m[2023-07-17 04:45:45,509][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:45:54,630][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 04:45:54,630][257371] FPS: 421084.60[0m
[36m[2023-07-17 04:45:54,632][257371] itr=653, itrs=2000, Progress: 32.65%[0m
[36m[2023-07-17 04:46:06,522][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 04:46:06,522][257371] FPS: 325309.81[0m
[36m[2023-07-17 04:46:10,794][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:46:10,794][257371] Reward + Measures: [[-84.68060424   0.83326805   0.84124362   0.84029502   0.060049
    7.54170322]][0m
[37m[1m[2023-07-17 04:46:10,794][257371] Max Reward on eval: -84.6806042445062[0m
[37m[1m[2023-07-17 04:46:10,795][257371] Min Reward on eval: -84.6806042445062[0m
[37m[1m[2023-07-17 04:46:10,795][257371] Mean Reward across all agents: -84.6806042445062[0m
[37m[1m[2023-07-17 04:46:10,795][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:46:15,826][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:46:15,826][257371] Reward + Measures: [[-54.10821383   0.36149999   0.43990001   0.2471       0.31979999
    4.81791496]
 [ 13.73773182   0.26360002   0.34209999   0.20750001   0.36590001
    4.26711416]
 [150.99221419   0.84100002   0.78099996   0.741        0.0623
    4.87060022]
 ...
 [118.33149661   0.32020003   0.84720004   0.3761       0.75200003
    6.00668287]
 [ 27.53226699   0.49079999   0.50650001   0.10500001   0.48010001
    5.04443836]
 [-15.31268958   0.354        0.36580002   0.3486       0.3493
    5.0936389 ]][0m
[37m[1m[2023-07-17 04:46:15,827][257371] Max Reward on eval: 521.4222221404314[0m
[37m[1m[2023-07-17 04:46:15,827][257371] Min Reward on eval: -232.99551294813864[0m
[37m[1m[2023-07-17 04:46:15,827][257371] Mean Reward across all agents: 24.109929244610278[0m
[37m[1m[2023-07-17 04:46:15,827][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:46:15,830][257371] mean_value=-586.5615469313574, max_value=508.477845887203[0m
[37m[1m[2023-07-17 04:46:15,833][257371] New mean coefficients: [[-0.5318428 -3.1577086 -0.621032   1.3998275 -0.5138986 10.085896 ]][0m
[37m[1m[2023-07-17 04:46:15,834][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:46:24,941][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 04:46:24,941][257371] FPS: 421733.38[0m
[36m[2023-07-17 04:46:24,943][257371] itr=654, itrs=2000, Progress: 32.70%[0m
[36m[2023-07-17 04:46:36,862][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 04:46:36,862][257371] FPS: 324464.86[0m
[36m[2023-07-17 04:46:41,168][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:46:41,169][257371] Reward + Measures: [[-83.42313738   0.81908935   0.83553106   0.82907701   0.075132
    7.52370167]][0m
[37m[1m[2023-07-17 04:46:41,169][257371] Max Reward on eval: -83.42313737760566[0m
[37m[1m[2023-07-17 04:46:41,169][257371] Min Reward on eval: -83.42313737760566[0m
[37m[1m[2023-07-17 04:46:41,170][257371] Mean Reward across all agents: -83.42313737760566[0m
[37m[1m[2023-07-17 04:46:41,170][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:46:46,179][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:46:46,180][257371] Reward + Measures: [[-308.39439036    0.66140002    0.66250002    0.22040001    0.62129998
     5.42750502]
 [ -61.83636503    0.1802        0.14960001    0.1417        0.1401
     4.00254393]
 [ -68.07098709    0.56810004    0.51420003    0.47409996    0.49239999
     4.46203184]
 ...
 [ 198.43067488    0.81480008    0.79140002    0.77069998    0.0673
     3.95570922]
 [ 173.35619448    0.65110004    0.64770001    0.53430003    0.1683
     4.17778635]
 [ -31.40200682    0.35310003    0.33529997    0.32730001    0.27790001
     4.01967525]][0m
[37m[1m[2023-07-17 04:46:46,180][257371] Max Reward on eval: 569.7038955882192[0m
[37m[1m[2023-07-17 04:46:46,180][257371] Min Reward on eval: -348.30798818790356[0m
[37m[1m[2023-07-17 04:46:46,180][257371] Mean Reward across all agents: 3.5807056362563685[0m
[37m[1m[2023-07-17 04:46:46,181][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:46:46,183][257371] mean_value=-1088.5120328061716, max_value=138.2388941119012[0m
[37m[1m[2023-07-17 04:46:46,185][257371] New mean coefficients: [[-1.6811711  -2.7639477  -1.3170428   1.3402313  -0.22019464 10.35248   ]][0m
[37m[1m[2023-07-17 04:46:46,186][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:46:55,231][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 04:46:55,231][257371] FPS: 424615.61[0m
[36m[2023-07-17 04:46:55,234][257371] itr=655, itrs=2000, Progress: 32.75%[0m
[36m[2023-07-17 04:47:06,932][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 04:47:06,932][257371] FPS: 330635.04[0m
[36m[2023-07-17 04:47:11,278][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:47:11,278][257371] Reward + Measures: [[-83.88194645   0.80687261   0.82508099   0.82002032   0.07544133
    7.49860525]][0m
[37m[1m[2023-07-17 04:47:11,279][257371] Max Reward on eval: -83.88194644800787[0m
[37m[1m[2023-07-17 04:47:11,279][257371] Min Reward on eval: -83.88194644800787[0m
[37m[1m[2023-07-17 04:47:11,279][257371] Mean Reward across all agents: -83.88194644800787[0m
[37m[1m[2023-07-17 04:47:11,279][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:47:16,518][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:47:16,519][257371] Reward + Measures: [[ 42.11937038   0.22290002   0.2649       0.1908       0.2428
    3.35194325]
 [ 33.84826249   0.64280003   0.62400001   0.53170002   0.23410001
    3.96880269]
 [-58.19683117   0.22389999   0.28470001   0.2316       0.28200004
    3.37898564]
 ...
 [ 83.01967215   0.2184       0.4763       0.29749998   0.47760001
    3.47850084]
 [ 21.19104415   0.3062       0.35060003   0.26570001   0.3145
    3.18104863]
 [ 27.82019386   0.31420001   0.3213       0.2832       0.33370003
    3.08678389]][0m
[37m[1m[2023-07-17 04:47:16,519][257371] Max Reward on eval: 538.565810436476[0m
[37m[1m[2023-07-17 04:47:16,519][257371] Min Reward on eval: -363.1454982498661[0m
[37m[1m[2023-07-17 04:47:16,519][257371] Mean Reward across all agents: 26.747074000522503[0m
[37m[1m[2023-07-17 04:47:16,520][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:47:16,522][257371] mean_value=-1784.170217126233, max_value=668.0461871683533[0m
[37m[1m[2023-07-17 04:47:16,524][257371] New mean coefficients: [[-0.6851172 -3.1570306 -1.0823472  1.0178813 -1.1026754 10.719271 ]][0m
[37m[1m[2023-07-17 04:47:16,525][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:47:25,680][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-17 04:47:25,680][257371] FPS: 419547.40[0m
[36m[2023-07-17 04:47:25,682][257371] itr=656, itrs=2000, Progress: 32.80%[0m
[36m[2023-07-17 04:47:37,530][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 04:47:37,530][257371] FPS: 326523.40[0m
[36m[2023-07-17 04:47:41,850][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:47:41,850][257371] Reward + Measures: [[-83.87473283   0.81232333   0.83486867   0.82727033   0.07776767
    7.53071165]][0m
[37m[1m[2023-07-17 04:47:41,851][257371] Max Reward on eval: -83.87473283359141[0m
[37m[1m[2023-07-17 04:47:41,851][257371] Min Reward on eval: -83.87473283359141[0m
[37m[1m[2023-07-17 04:47:41,851][257371] Mean Reward across all agents: -83.87473283359141[0m
[37m[1m[2023-07-17 04:47:41,851][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:47:46,890][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:47:46,890][257371] Reward + Measures: [[286.15755079   0.92539996   0.84229994   0.82859993   0.0175
    4.99804068]
 [ 49.33561815   0.39320001   0.32600003   0.3513       0.06339999
    4.98393965]
 [-29.91778302   0.30360001   0.20480001   0.23980001   0.23699999
    4.77040243]
 ...
 [-74.25438806   0.33110002   0.42989999   0.20639999   0.46250001
    4.20372105]
 [-26.90004255   0.31529999   0.32140002   0.27670002   0.2174
    3.38875055]
 [  8.77600592   0.20250002   0.1969       0.18620001   0.1398
    3.40601707]][0m
[37m[1m[2023-07-17 04:47:46,891][257371] Max Reward on eval: 668.3693924061954[0m
[37m[1m[2023-07-17 04:47:46,891][257371] Min Reward on eval: -721.0953102274798[0m
[37m[1m[2023-07-17 04:47:46,891][257371] Mean Reward across all agents: 10.273200722792783[0m
[37m[1m[2023-07-17 04:47:46,891][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:47:46,894][257371] mean_value=-1392.765389739975, max_value=167.75930478033487[0m
[37m[1m[2023-07-17 04:47:46,896][257371] New mean coefficients: [[-0.419076   -2.146899   -0.86415565  0.6921585   0.42332745 10.425438  ]][0m
[37m[1m[2023-07-17 04:47:46,897][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:47:56,030][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 04:47:56,030][257371] FPS: 420546.02[0m
[36m[2023-07-17 04:47:56,032][257371] itr=657, itrs=2000, Progress: 32.85%[0m
[36m[2023-07-17 04:48:07,760][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 04:48:07,760][257371] FPS: 329902.90[0m
[36m[2023-07-17 04:48:12,062][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:48:12,062][257371] Reward + Measures: [[-81.30460115   0.82425934   0.84386259   0.83636671   0.073405
    7.54344416]][0m
[37m[1m[2023-07-17 04:48:12,062][257371] Max Reward on eval: -81.30460115421238[0m
[37m[1m[2023-07-17 04:48:12,063][257371] Min Reward on eval: -81.30460115421238[0m
[37m[1m[2023-07-17 04:48:12,063][257371] Mean Reward across all agents: -81.30460115421238[0m
[37m[1m[2023-07-17 04:48:12,063][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:48:17,061][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:48:17,062][257371] Reward + Measures: [[   6.2313589     0.26280004    0.24889998    0.21490002    0.19780003
     3.75375175]
 [-346.46382709    0.34039998    0.92329997    0.49600002    0.8531
     5.11864996]
 [ -54.12690866    0.1895        0.1681        0.1684        0.095
     4.46608686]
 ...
 [ 406.27285051    0.74070001    0.73989999    0.65210003    0.1133
     6.04786158]
 [ -62.90008808    0.27570003    0.4034        0.25560001    0.30410001
     3.74765086]
 [-339.45879579    0.69380009    0.64960003    0.6778        0.0411
     5.03513145]][0m
[37m[1m[2023-07-17 04:48:17,062][257371] Max Reward on eval: 609.5849113645032[0m
[37m[1m[2023-07-17 04:48:17,062][257371] Min Reward on eval: -383.77643962837755[0m
[37m[1m[2023-07-17 04:48:17,063][257371] Mean Reward across all agents: -15.030852668950256[0m
[37m[1m[2023-07-17 04:48:17,063][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:48:17,065][257371] mean_value=-1162.896533920988, max_value=211.04100425373534[0m
[37m[1m[2023-07-17 04:48:17,067][257371] New mean coefficients: [[-1.0407337  -1.5551214  -0.72337484  0.05036783 -0.02390796  9.581732  ]][0m
[37m[1m[2023-07-17 04:48:17,068][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:48:26,106][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 04:48:26,106][257371] FPS: 424925.76[0m
[36m[2023-07-17 04:48:26,109][257371] itr=658, itrs=2000, Progress: 32.90%[0m
[36m[2023-07-17 04:48:37,882][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 04:48:37,882][257371] FPS: 328473.21[0m
[36m[2023-07-17 04:48:42,259][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:48:42,260][257371] Reward + Measures: [[-82.76154339   0.8218497    0.8376193    0.83301055   0.068484
    7.54417229]][0m
[37m[1m[2023-07-17 04:48:42,260][257371] Max Reward on eval: -82.76154338691255[0m
[37m[1m[2023-07-17 04:48:42,260][257371] Min Reward on eval: -82.76154338691255[0m
[37m[1m[2023-07-17 04:48:42,261][257371] Mean Reward across all agents: -82.76154338691255[0m
[37m[1m[2023-07-17 04:48:42,261][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:48:47,245][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:48:47,246][257371] Reward + Measures: [[   6.58246437    0.2669        0.26010001    0.2429        0.17870001
     3.91027379]
 [  19.97833484    0.58270001    0.58820003    0.49829999    0.54159999
     4.38879633]
 [-211.75654766    0.5873        0.91420001    0.0454        0.94440001
     5.98135328]
 ...
 [-162.54783532    0.60340005    0.80419999    0.16940001    0.72419995
     4.89735079]
 [-139.8042097     0.91020006    0.9684        0.008         0.97489995
     6.40205717]
 [ -52.01699516    0.8229        0.76789999    0.80190003    0.033
     4.27051401]][0m
[37m[1m[2023-07-17 04:48:47,246][257371] Max Reward on eval: 639.9737157434225[0m
[37m[1m[2023-07-17 04:48:47,246][257371] Min Reward on eval: -505.3239047659561[0m
[37m[1m[2023-07-17 04:48:47,246][257371] Mean Reward across all agents: 10.01712034840363[0m
[37m[1m[2023-07-17 04:48:47,246][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:48:47,250][257371] mean_value=-616.968249772338, max_value=333.3795526871065[0m
[37m[1m[2023-07-17 04:48:47,253][257371] New mean coefficients: [[-1.4505869  -1.0570854  -0.8177463  -0.21641934  0.6721858   9.054075  ]][0m
[37m[1m[2023-07-17 04:48:47,255][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:48:56,277][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 04:48:56,277][257371] FPS: 425701.00[0m
[36m[2023-07-17 04:48:56,279][257371] itr=659, itrs=2000, Progress: 32.95%[0m
[36m[2023-07-17 04:49:07,974][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 04:49:07,975][257371] FPS: 330801.37[0m
[36m[2023-07-17 04:49:12,327][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:49:12,328][257371] Reward + Measures: [[-88.9278871    0.82995433   0.84981865   0.84199655   0.06981733
    7.55899   ]][0m
[37m[1m[2023-07-17 04:49:12,328][257371] Max Reward on eval: -88.92788709688419[0m
[37m[1m[2023-07-17 04:49:12,328][257371] Min Reward on eval: -88.92788709688419[0m
[37m[1m[2023-07-17 04:49:12,328][257371] Mean Reward across all agents: -88.92788709688419[0m
[37m[1m[2023-07-17 04:49:12,329][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:49:17,313][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:49:17,314][257371] Reward + Measures: [[ -66.1529414     0.50739998    0.55690002    0.42869997    0.20060001
     4.43800879]
 [-106.64375069    0.37760004    0.51179999    0.21040002    0.53649998
     4.07856178]
 [  55.3974321     0.50400001    0.58300006    0.2393        0.45429999
     6.08976603]
 ...
 [ 118.48357963    0.78800005    0.88079995    0.64670002    0.31220004
     6.87626886]
 [ -13.46790401    0.93880004    0.98019999    0.0054        0.98359996
     7.14701939]
 [  -8.83446596    0.81759995    0.95200008    0.60030001    0.45630002
     7.0046792 ]][0m
[37m[1m[2023-07-17 04:49:17,314][257371] Max Reward on eval: 502.38084221072495[0m
[37m[1m[2023-07-17 04:49:17,314][257371] Min Reward on eval: -638.0747719026637[0m
[37m[1m[2023-07-17 04:49:17,315][257371] Mean Reward across all agents: 15.96745581102763[0m
[37m[1m[2023-07-17 04:49:17,315][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:49:17,320][257371] mean_value=-396.32719659103515, max_value=579.1878136391379[0m
[37m[1m[2023-07-17 04:49:17,323][257371] New mean coefficients: [[-1.3785917  -1.3178084  -0.565052   -0.6943119   0.44037548  8.609384  ]][0m
[37m[1m[2023-07-17 04:49:17,324][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:49:26,304][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 04:49:26,304][257371] FPS: 427697.90[0m
[36m[2023-07-17 04:49:26,306][257371] itr=660, itrs=2000, Progress: 33.00%[0m
[37m[1m[2023-07-17 04:52:26,749][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000640[0m
[36m[2023-07-17 04:52:38,942][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 04:52:38,942][257371] FPS: 329333.13[0m
[36m[2023-07-17 04:52:43,304][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:52:43,304][257371] Reward + Measures: [[-84.55461988   0.81955338   0.85828692   0.83737296   0.092124
    7.5644393 ]][0m
[37m[1m[2023-07-17 04:52:43,304][257371] Max Reward on eval: -84.55461987720996[0m
[37m[1m[2023-07-17 04:52:43,304][257371] Min Reward on eval: -84.55461987720996[0m
[37m[1m[2023-07-17 04:52:43,305][257371] Mean Reward across all agents: -84.55461987720996[0m
[37m[1m[2023-07-17 04:52:43,305][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:52:48,378][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:52:48,378][257371] Reward + Measures: [[ 99.31583211   0.28389999   0.59350008   0.2369       0.54790002
    5.68212938]
 [213.53204301   0.58400005   0.54460001   0.5474       0.45439997
    5.7610507 ]
 [233.56988414   0.57860005   0.58179998   0.51230001   0.13300002
    5.68199635]
 ...
 [ 19.81524617   0.46149999   0.35260004   0.36829999   0.0783
    4.08444595]
 [ 99.61008773   0.38949999   0.37960002   0.40740004   0.0948
    5.32030582]
 [ 11.49437936   0.19240001   0.1594       0.18020001   0.13510001
    3.95377135]][0m
[37m[1m[2023-07-17 04:52:48,379][257371] Max Reward on eval: 588.417563476041[0m
[37m[1m[2023-07-17 04:52:48,379][257371] Min Reward on eval: -597.7653084240155[0m
[37m[1m[2023-07-17 04:52:48,379][257371] Mean Reward across all agents: 65.9000942319059[0m
[37m[1m[2023-07-17 04:52:48,379][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:52:48,383][257371] mean_value=-975.2704886176551, max_value=308.1970825507271[0m
[37m[1m[2023-07-17 04:52:48,386][257371] New mean coefficients: [[-1.7991804  -1.7265784  -0.20279726 -0.98260903 -1.3493901   8.303845  ]][0m
[37m[1m[2023-07-17 04:52:48,387][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:52:57,474][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 04:52:57,474][257371] FPS: 422670.92[0m
[36m[2023-07-17 04:52:57,477][257371] itr=661, itrs=2000, Progress: 33.05%[0m
[36m[2023-07-17 04:53:09,349][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 04:53:09,349][257371] FPS: 325756.03[0m
[36m[2023-07-17 04:53:13,664][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:53:13,664][257371] Reward + Measures: [[-94.15781009   0.81571501   0.85495168   0.8365187    0.089034
    7.55799294]][0m
[37m[1m[2023-07-17 04:53:13,665][257371] Max Reward on eval: -94.15781008802331[0m
[37m[1m[2023-07-17 04:53:13,665][257371] Min Reward on eval: -94.15781008802331[0m
[37m[1m[2023-07-17 04:53:13,665][257371] Mean Reward across all agents: -94.15781008802331[0m
[37m[1m[2023-07-17 04:53:13,666][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:53:18,638][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:53:18,638][257371] Reward + Measures: [[-117.15389409    0.58210009    0.80369997    0.39320001    0.53380007
     5.19351721]
 [ 349.23711398    0.54720002    0.7561        0.57319999    0.32290003
     5.86736822]
 [ 485.07579469    0.68629998    0.67680001    0.68000001    0.0469
     6.03378153]
 ...
 [ 109.46678544    0.36220002    0.36209998    0.33379999    0.27290002
     3.65595484]
 [   7.05191863    0.31680003    0.23309998    0.24589999    0.1884
     3.35645032]
 [ -63.72037298    0.3838        0.46869999    0.2306        0.32319999
     4.11890459]][0m
[37m[1m[2023-07-17 04:53:18,639][257371] Max Reward on eval: 646.9784469710663[0m
[37m[1m[2023-07-17 04:53:18,639][257371] Min Reward on eval: -577.6071367698954[0m
[37m[1m[2023-07-17 04:53:18,639][257371] Mean Reward across all agents: 115.97287304895436[0m
[37m[1m[2023-07-17 04:53:18,639][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:53:18,644][257371] mean_value=-550.459126702252, max_value=504.8620266588755[0m
[37m[1m[2023-07-17 04:53:18,647][257371] New mean coefficients: [[-2.4276662  -2.331308   -0.49544308 -0.69573855 -1.4961524   8.808637  ]][0m
[37m[1m[2023-07-17 04:53:18,648][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:53:27,735][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 04:53:27,735][257371] FPS: 422678.66[0m
[36m[2023-07-17 04:53:27,737][257371] itr=662, itrs=2000, Progress: 33.10%[0m
[36m[2023-07-17 04:53:39,605][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 04:53:39,606][257371] FPS: 325979.32[0m
[36m[2023-07-17 04:53:43,885][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:53:43,886][257371] Reward + Measures: [[-96.26826076   0.807455     0.86015069   0.82688361   0.11259168
    7.55546331]][0m
[37m[1m[2023-07-17 04:53:43,886][257371] Max Reward on eval: -96.26826075788192[0m
[37m[1m[2023-07-17 04:53:43,886][257371] Min Reward on eval: -96.26826075788192[0m
[37m[1m[2023-07-17 04:53:43,887][257371] Mean Reward across all agents: -96.26826075788192[0m
[37m[1m[2023-07-17 04:53:43,887][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:53:49,077][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:53:49,078][257371] Reward + Measures: [[ 181.78217721    0.66330004    0.6462        0.5539        0.13170001
     5.61713171]
 [  41.78919192    0.33879998    0.64319998    0.62980002    0.4492
     4.28214645]
 [-101.57023902    0.60570002    0.72720003    0.55240005    0.23430002
     4.33074045]
 ...
 [  28.22248139    0.32790002    0.29980001    0.287         0.1337
     2.92117095]
 [  -6.55125865    0.51949996    0.46799999    0.50319999    0.23049998
     4.1683197 ]
 [ -73.81592361    0.50890005    0.86630005    0.5851        0.56040001
     7.40234375]][0m
[37m[1m[2023-07-17 04:53:49,078][257371] Max Reward on eval: 476.17235657945275[0m
[37m[1m[2023-07-17 04:53:49,078][257371] Min Reward on eval: -404.5769644342363[0m
[37m[1m[2023-07-17 04:53:49,078][257371] Mean Reward across all agents: 8.778588871669784[0m
[37m[1m[2023-07-17 04:53:49,079][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:53:49,082][257371] mean_value=-706.8825376857068, max_value=626.1878700055968[0m
[37m[1m[2023-07-17 04:53:49,085][257371] New mean coefficients: [[-2.4344246 -1.5005854 -0.8559984 -0.7577285 -1.3856014  9.660208 ]][0m
[37m[1m[2023-07-17 04:53:49,086][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:53:58,017][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 04:53:58,018][257371] FPS: 430006.35[0m
[36m[2023-07-17 04:53:58,020][257371] itr=663, itrs=2000, Progress: 33.15%[0m
[36m[2023-07-17 04:54:09,613][257371] train() took 11.51 seconds to complete[0m
[36m[2023-07-17 04:54:09,614][257371] FPS: 333738.37[0m
[36m[2023-07-17 04:54:13,902][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:54:13,903][257371] Reward + Measures: [[-106.61432726    0.80428767    0.860048      0.82716095    0.11513299
     7.54630899]][0m
[37m[1m[2023-07-17 04:54:13,903][257371] Max Reward on eval: -106.6143272574258[0m
[37m[1m[2023-07-17 04:54:13,903][257371] Min Reward on eval: -106.6143272574258[0m
[37m[1m[2023-07-17 04:54:13,904][257371] Mean Reward across all agents: -106.6143272574258[0m
[37m[1m[2023-07-17 04:54:13,904][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:54:18,958][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:54:18,958][257371] Reward + Measures: [[ 500.8907738     0.69119996    0.86240005    0.76280004    0.2112
     6.31960249]
 [-237.63421119    0.38929999    0.69580001    0.39700004    0.65679997
     5.10539389]
 [  39.62298847    0.185         0.19840001    0.16690001    0.19240001
     4.70107746]
 ...
 [ -63.573504      0.60080004    0.56919998    0.5794        0.0684
     5.65640831]
 [ 422.04558262    0.69090003    0.65760005    0.67140001    0.048
     5.32125902]
 [ -41.66946271    0.50349998    0.52879995    0.207         0.47010002
     4.21821117]][0m
[37m[1m[2023-07-17 04:54:18,958][257371] Max Reward on eval: 617.1128535468131[0m
[37m[1m[2023-07-17 04:54:18,959][257371] Min Reward on eval: -474.62032582517713[0m
[37m[1m[2023-07-17 04:54:18,959][257371] Mean Reward across all agents: 63.40910779835683[0m
[37m[1m[2023-07-17 04:54:18,959][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:54:18,963][257371] mean_value=-301.2427807410952, max_value=380.72466235881757[0m
[37m[1m[2023-07-17 04:54:18,966][257371] New mean coefficients: [[-2.6350098  -1.1420509  -0.86082435 -1.3120406  -1.1057051   9.216644  ]][0m
[37m[1m[2023-07-17 04:54:18,967][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:54:28,093][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 04:54:28,093][257371] FPS: 420845.59[0m
[36m[2023-07-17 04:54:28,096][257371] itr=664, itrs=2000, Progress: 33.20%[0m
[36m[2023-07-17 04:54:39,859][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 04:54:39,860][257371] FPS: 328812.15[0m
[36m[2023-07-17 04:54:44,124][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:54:44,124][257371] Reward + Measures: [[-113.26448271    0.79819369    0.86740428    0.82843238    0.12765133
     7.55199718]][0m
[37m[1m[2023-07-17 04:54:44,125][257371] Max Reward on eval: -113.26448271377745[0m
[37m[1m[2023-07-17 04:54:44,125][257371] Min Reward on eval: -113.26448271377745[0m
[37m[1m[2023-07-17 04:54:44,125][257371] Mean Reward across all agents: -113.26448271377745[0m
[37m[1m[2023-07-17 04:54:44,125][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:54:49,122][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:54:49,122][257371] Reward + Measures: [[ -33.16398381    0.54650003    0.74349993    0.2802        0.55779999
     4.49928665]
 [ 187.66298057    0.46760002    0.88479996    0.72439998    0.49559999
     6.7387023 ]
 [ 275.04896545    0.0864        0.96900004    0.54159999    0.97510004
     6.37829208]
 ...
 [  72.82066678    0.61570001    0.82860005    0.51780003    0.44949999
     6.72438574]
 [  90.4904232     0.38249999    0.98120004    0.36590001    0.98250008
     6.58426428]
 [-431.17236328    0.95459998    0.98330003    0.            0.99590009
     6.77751017]][0m
[37m[1m[2023-07-17 04:54:49,123][257371] Max Reward on eval: 408.0937218518928[0m
[37m[1m[2023-07-17 04:54:49,123][257371] Min Reward on eval: -629.7320995321497[0m
[37m[1m[2023-07-17 04:54:49,123][257371] Mean Reward across all agents: -80.68321837406359[0m
[37m[1m[2023-07-17 04:54:49,123][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:54:49,128][257371] mean_value=-315.58976700735434, max_value=366.84802981567634[0m
[37m[1m[2023-07-17 04:54:49,131][257371] New mean coefficients: [[-2.511566   -0.4711097  -0.41179603 -1.5476923  -0.51185197  9.147721  ]][0m
[37m[1m[2023-07-17 04:54:49,132][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:54:58,116][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 04:54:58,116][257371] FPS: 427471.08[0m
[36m[2023-07-17 04:54:58,119][257371] itr=665, itrs=2000, Progress: 33.25%[0m
[36m[2023-07-17 04:55:09,772][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 04:55:09,773][257371] FPS: 331906.64[0m
[36m[2023-07-17 04:55:14,039][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:55:14,040][257371] Reward + Measures: [[-114.40377802    0.79242539    0.86574459    0.82822233    0.12717099
     7.55486727]][0m
[37m[1m[2023-07-17 04:55:14,040][257371] Max Reward on eval: -114.40377801968948[0m
[37m[1m[2023-07-17 04:55:14,040][257371] Min Reward on eval: -114.40377801968948[0m
[37m[1m[2023-07-17 04:55:14,041][257371] Mean Reward across all agents: -114.40377801968948[0m
[37m[1m[2023-07-17 04:55:14,041][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:55:19,126][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:55:19,127][257371] Reward + Measures: [[ 73.6208866    0.50200003   0.6656       0.44790003   0.4066
    4.52243137]
 [ 61.74511389   0.45179996   0.73839998   0.73920006   0.36469999
    5.42512226]
 [179.46103277   0.39499998   0.87670004   0.71270001   0.52130002
    6.60133505]
 ...
 [ 56.35554805   0.47390005   0.51270002   0.49450001   0.15370001
    5.0527029 ]
 [195.2587347    0.57090002   0.94480002   0.77600002   0.4068
    6.55806732]
 [267.8625894    0.58210003   0.55809999   0.56510001   0.053
    5.42611456]][0m
[37m[1m[2023-07-17 04:55:19,127][257371] Max Reward on eval: 749.2001201730221[0m
[37m[1m[2023-07-17 04:55:19,127][257371] Min Reward on eval: -227.23004194796084[0m
[37m[1m[2023-07-17 04:55:19,128][257371] Mean Reward across all agents: 112.56517643356531[0m
[37m[1m[2023-07-17 04:55:19,128][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:55:19,132][257371] mean_value=-230.6430229749876, max_value=317.0214905360733[0m
[37m[1m[2023-07-17 04:55:19,135][257371] New mean coefficients: [[-1.7601061   0.5077551  -0.20645069 -1.8522948   0.7583764   9.119116  ]][0m
[37m[1m[2023-07-17 04:55:19,136][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:55:28,239][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 04:55:28,240][257371] FPS: 421912.29[0m
[36m[2023-07-17 04:55:28,242][257371] itr=666, itrs=2000, Progress: 33.30%[0m
[36m[2023-07-17 04:55:40,179][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-17 04:55:40,179][257371] FPS: 324049.02[0m
[36m[2023-07-17 04:55:44,455][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:55:44,456][257371] Reward + Measures: [[-129.30340027    0.79426169    0.85493797    0.82221901    0.11902834
     7.55273581]][0m
[37m[1m[2023-07-17 04:55:44,456][257371] Max Reward on eval: -129.30340027195703[0m
[37m[1m[2023-07-17 04:55:44,456][257371] Min Reward on eval: -129.30340027195703[0m
[37m[1m[2023-07-17 04:55:44,456][257371] Mean Reward across all agents: -129.30340027195703[0m
[37m[1m[2023-07-17 04:55:44,457][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:55:49,410][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:55:49,411][257371] Reward + Measures: [[ -52.93881688    0.53740001    0.55790001    0.43630001    0.23600002
     5.84080982]
 [  49.7579556     0.47389999    0.65160006    0.50319999    0.38710001
     5.74293613]
 [-139.20085443    0.59770006    0.60470003    0.5413        0.1505
     5.47562599]
 ...
 [   9.67092682    0.57160002    0.59740001    0.2332        0.5097
     5.24152946]
 [-319.7420564     0.34129998    0.65729994    0.34190002    0.62
     5.23825359]
 [ -19.41437031    0.3283        0.34979999    0.28420001    0.24700001
     4.4570632 ]][0m
[37m[1m[2023-07-17 04:55:49,411][257371] Max Reward on eval: 424.39188288170845[0m
[37m[1m[2023-07-17 04:55:49,411][257371] Min Reward on eval: -563.2170457871631[0m
[37m[1m[2023-07-17 04:55:49,411][257371] Mean Reward across all agents: -32.22171085785487[0m
[37m[1m[2023-07-17 04:55:49,412][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:55:49,414][257371] mean_value=-357.6498249706056, max_value=74.23373897524456[0m
[37m[1m[2023-07-17 04:55:49,417][257371] New mean coefficients: [[-1.711427   -0.30938107  0.09062867 -1.6971018  -0.53185755  8.628736  ]][0m
[37m[1m[2023-07-17 04:55:49,418][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:55:58,356][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 04:55:58,357][257371] FPS: 429657.02[0m
[36m[2023-07-17 04:55:58,359][257371] itr=667, itrs=2000, Progress: 33.35%[0m
[36m[2023-07-17 04:56:10,344][257371] train() took 11.90 seconds to complete[0m
[36m[2023-07-17 04:56:10,344][257371] FPS: 322783.91[0m
[36m[2023-07-17 04:56:14,690][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:56:14,691][257371] Reward + Measures: [[-128.80828227    0.78594369    0.84850967    0.81687093    0.12213434
     7.55007219]][0m
[37m[1m[2023-07-17 04:56:14,691][257371] Max Reward on eval: -128.80828226789126[0m
[37m[1m[2023-07-17 04:56:14,691][257371] Min Reward on eval: -128.80828226789126[0m
[37m[1m[2023-07-17 04:56:14,692][257371] Mean Reward across all agents: -128.80828226789126[0m
[37m[1m[2023-07-17 04:56:14,692][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:56:19,953][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:56:19,953][257371] Reward + Measures: [[ 646.25152201    0.92080003    0.93670005    0.85369998    0.11359999
     5.29731703]
 [ -18.02067881    0.37380001    0.3969        0.29769999    0.1824
     4.31874132]
 [  90.01458291    0.34689999    0.866         0.3035        0.81689996
     4.12930822]
 ...
 [-147.86588661    0.57419997    0.69259995    0.64230007    0.1805
     6.07299089]
 [-229.59038272    0.50489998    0.52689999    0.1829        0.41249999
     5.5918355 ]
 [ -47.90812049    0.66650003    0.70190001    0.6767        0.11189999
     5.78009319]][0m
[37m[1m[2023-07-17 04:56:19,954][257371] Max Reward on eval: 773.7956161316484[0m
[37m[1m[2023-07-17 04:56:19,954][257371] Min Reward on eval: -537.9160542538273[0m
[37m[1m[2023-07-17 04:56:19,954][257371] Mean Reward across all agents: 61.70243276545003[0m
[37m[1m[2023-07-17 04:56:19,954][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:56:19,957][257371] mean_value=-311.47396590112635, max_value=155.160623044339[0m
[37m[1m[2023-07-17 04:56:19,960][257371] New mean coefficients: [[-2.0015695  -0.33980817  0.02596848 -1.3098654   0.08215618  8.735412  ]][0m
[37m[1m[2023-07-17 04:56:19,961][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:56:29,137][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-17 04:56:29,137][257371] FPS: 418572.05[0m
[36m[2023-07-17 04:56:29,139][257371] itr=668, itrs=2000, Progress: 33.40%[0m
[36m[2023-07-17 04:56:40,889][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 04:56:40,889][257371] FPS: 329312.84[0m
[36m[2023-07-17 04:56:45,187][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:56:45,187][257371] Reward + Measures: [[-138.74104578    0.79466301    0.873281      0.8308593     0.135979
     7.5836277 ]][0m
[37m[1m[2023-07-17 04:56:45,187][257371] Max Reward on eval: -138.74104577716307[0m
[37m[1m[2023-07-17 04:56:45,188][257371] Min Reward on eval: -138.74104577716307[0m
[37m[1m[2023-07-17 04:56:45,188][257371] Mean Reward across all agents: -138.74104577716307[0m
[37m[1m[2023-07-17 04:56:45,188][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:56:50,181][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:56:50,182][257371] Reward + Measures: [[  19.57796732    0.42050001    0.56980002    0.35069999    0.3594
     4.72519255]
 [ 250.42937084    0.40670004    0.88190001    0.5025        0.75670004
     7.38691854]
 [ 160.02110386    0.42969999    0.8962        0.89169997    0.528
     4.64655685]
 ...
 [-159.69079498    0.3617        0.69980001    0.37450001    0.60719997
     5.98224878]
 [ -56.44910173    0.70990002    0.69970006    0.64540005    0.1301
     5.70930672]
 [-197.63363002    0.79070002    0.78119993    0.78869998    0.0454
     6.68362761]][0m
[37m[1m[2023-07-17 04:56:50,182][257371] Max Reward on eval: 415.9794482784346[0m
[37m[1m[2023-07-17 04:56:50,182][257371] Min Reward on eval: -709.5443783084862[0m
[37m[1m[2023-07-17 04:56:50,183][257371] Mean Reward across all agents: -43.963128051381[0m
[37m[1m[2023-07-17 04:56:50,183][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:56:50,186][257371] mean_value=-422.97900610661435, max_value=486.7345989755179[0m
[37m[1m[2023-07-17 04:56:50,188][257371] New mean coefficients: [[-2.1325972  -0.8401548  -0.29173294 -1.4463288  -0.47798997  8.644722  ]][0m
[37m[1m[2023-07-17 04:56:50,189][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:56:59,204][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 04:56:59,205][257371] FPS: 426031.49[0m
[36m[2023-07-17 04:56:59,207][257371] itr=669, itrs=2000, Progress: 33.45%[0m
[36m[2023-07-17 04:57:10,843][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 04:57:10,844][257371] FPS: 332501.37[0m
[36m[2023-07-17 04:57:15,208][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:57:15,209][257371] Reward + Measures: [[-142.32372469    0.78607404    0.88531494    0.82858205    0.15770566
     7.58788538]][0m
[37m[1m[2023-07-17 04:57:15,209][257371] Max Reward on eval: -142.32372469008445[0m
[37m[1m[2023-07-17 04:57:15,209][257371] Min Reward on eval: -142.32372469008445[0m
[37m[1m[2023-07-17 04:57:15,209][257371] Mean Reward across all agents: -142.32372469008445[0m
[37m[1m[2023-07-17 04:57:15,210][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:57:20,250][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 04:57:20,250][257371] Reward + Measures: [[-109.1296295     0.54759997    0.54139996    0.53680003    0.0587
     5.85020924]
 [  39.3059433     0.4619        0.65679997    0.42770001    0.44159999
     6.38051462]
 [ -42.0134022     0.2119        0.1716        0.18709999    0.1295
     5.17269516]
 ...
 [ 272.72778354    0.0509        0.74310005    0.51170003    0.75270003
     6.19670725]
 [  17.32204473    0.39270002    0.80260003    0.3592        0.72990006
     3.81157279]
 [  76.27072648    0.77069998    0.83039999    0.77249998    0.15530001
     4.50614023]][0m
[37m[1m[2023-07-17 04:57:20,251][257371] Max Reward on eval: 577.2732143716887[0m
[37m[1m[2023-07-17 04:57:20,251][257371] Min Reward on eval: -256.1895494321361[0m
[37m[1m[2023-07-17 04:57:20,251][257371] Mean Reward across all agents: 40.70538303349381[0m
[37m[1m[2023-07-17 04:57:20,251][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 04:57:20,256][257371] mean_value=-319.7377597808456, max_value=510.97305376681203[0m
[37m[1m[2023-07-17 04:57:20,259][257371] New mean coefficients: [[-1.6682744  -1.909749   -0.12801458 -1.4509774  -0.67594665  8.363894  ]][0m
[37m[1m[2023-07-17 04:57:20,260][257371] Moving the mean solution point...[0m
[36m[2023-07-17 04:57:29,336][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 04:57:29,337][257371] FPS: 423149.13[0m
[36m[2023-07-17 04:57:29,339][257371] itr=670, itrs=2000, Progress: 33.50%[0m
[37m[1m[2023-07-17 05:00:32,650][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000650[0m
[36m[2023-07-17 05:00:44,944][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 05:00:44,945][257371] FPS: 329282.67[0m
[36m[2023-07-17 05:00:49,312][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:00:49,313][257371] Reward + Measures: [[-143.86403086    0.76596093    0.86919701    0.80945444    0.16773467
     7.56933975]][0m
[37m[1m[2023-07-17 05:00:49,313][257371] Max Reward on eval: -143.86403086260754[0m
[37m[1m[2023-07-17 05:00:49,313][257371] Min Reward on eval: -143.86403086260754[0m
[37m[1m[2023-07-17 05:00:49,314][257371] Mean Reward across all agents: -143.86403086260754[0m
[37m[1m[2023-07-17 05:00:49,314][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:00:54,376][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:00:54,376][257371] Reward + Measures: [[ -86.21977521    0.33199999    0.3141        0.22930001    0.19630001
     4.15442991]
 [-170.6637358     0.94819993    0.94260007    0.93440002    0.0261
     7.20581818]
 [ -40.64892278    0.36880001    0.34769997    0.26890001    0.2421
     4.17849684]
 ...
 [-112.90121073    0.66030002    0.5927        0.49869999    0.1944
     4.1051836 ]
 [ -25.4006786     0.28390002    0.2863        0.2454        0.16630001
     4.27404022]
 [ -90.46094492    0.43649998    0.40980002    0.37240002    0.19240001
     4.19800282]][0m
[37m[1m[2023-07-17 05:00:54,377][257371] Max Reward on eval: 344.1071934878826[0m
[37m[1m[2023-07-17 05:00:54,377][257371] Min Reward on eval: -548.4608964712476[0m
[37m[1m[2023-07-17 05:00:54,377][257371] Mean Reward across all agents: -88.03089863792695[0m
[37m[1m[2023-07-17 05:00:54,377][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:00:54,380][257371] mean_value=-705.1409729957986, max_value=377.092223782291[0m
[37m[1m[2023-07-17 05:00:54,382][257371] New mean coefficients: [[-1.0284058 -1.9113243  0.5634427 -1.0639272 -1.1560695  8.363092 ]][0m
[37m[1m[2023-07-17 05:00:54,383][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:01:03,485][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 05:01:03,485][257371] FPS: 421955.22[0m
[36m[2023-07-17 05:01:03,488][257371] itr=671, itrs=2000, Progress: 33.55%[0m
[36m[2023-07-17 05:01:15,292][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 05:01:15,292][257371] FPS: 327648.31[0m
[36m[2023-07-17 05:01:19,540][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:01:19,541][257371] Reward + Measures: [[-130.81789371    0.72604936    0.89123267    0.79984927    0.23427032
     7.57916117]][0m
[37m[1m[2023-07-17 05:01:19,541][257371] Max Reward on eval: -130.81789371498698[0m
[37m[1m[2023-07-17 05:01:19,541][257371] Min Reward on eval: -130.81789371498698[0m
[37m[1m[2023-07-17 05:01:19,542][257371] Mean Reward across all agents: -130.81789371498698[0m
[37m[1m[2023-07-17 05:01:19,542][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:01:24,548][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:01:24,549][257371] Reward + Measures: [[-283.48132421    0.90570003    0.9752        0.0088        0.98409998
     6.37506962]
 [-103.31197284    0.67049998    0.66909999    0.66320002    0.0572
     6.0314436 ]
 [  23.0544066     0.75760001    0.7719        0.68559998    0.16230001
     5.44191599]
 ...
 [  49.34700396    0.7198        0.74079996    0.50209999    0.26100001
     4.19862509]
 [ 253.86428499    0.30510002    0.84810013    0.51699996    0.77969998
     5.46399927]
 [ -68.84511495    0.509         0.73550004    0.35999998    0.70910001
     3.69222188]][0m
[37m[1m[2023-07-17 05:01:24,549][257371] Max Reward on eval: 448.6697473125532[0m
[37m[1m[2023-07-17 05:01:24,549][257371] Min Reward on eval: -439.5139557970688[0m
[37m[1m[2023-07-17 05:01:24,550][257371] Mean Reward across all agents: -95.41503103242663[0m
[37m[1m[2023-07-17 05:01:24,550][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:01:24,554][257371] mean_value=-350.83993612385495, max_value=490.8691957912902[0m
[37m[1m[2023-07-17 05:01:24,557][257371] New mean coefficients: [[-1.0162752  -1.8211987   0.37736836 -1.1573316  -1.1147012   8.163191  ]][0m
[37m[1m[2023-07-17 05:01:24,557][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:01:33,519][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 05:01:33,519][257371] FPS: 428573.79[0m
[36m[2023-07-17 05:01:33,521][257371] itr=672, itrs=2000, Progress: 33.60%[0m
[36m[2023-07-17 05:01:45,181][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 05:01:45,182][257371] FPS: 331795.54[0m
[36m[2023-07-17 05:01:49,429][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:01:49,435][257371] Reward + Measures: [[-121.96137249    0.69425505    0.87810397    0.76915264    0.26989633
     7.56796598]][0m
[37m[1m[2023-07-17 05:01:49,435][257371] Max Reward on eval: -121.96137248564492[0m
[37m[1m[2023-07-17 05:01:49,436][257371] Min Reward on eval: -121.96137248564492[0m
[37m[1m[2023-07-17 05:01:49,436][257371] Mean Reward across all agents: -121.96137248564492[0m
[37m[1m[2023-07-17 05:01:49,436][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:01:54,400][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:01:54,401][257371] Reward + Measures: [[  97.59777162    0.47329998    0.85729998    0.24799998    0.81669998
     4.45233488]
 [  -4.16861902    0.50570005    0.46000001    0.45730001    0.41929999
     4.79292488]
 [  59.40611227    0.66379994    0.53540003    0.5927        0.42430001
     4.76653528]
 ...
 [-228.59818722    0.45979998    0.59000009    0.13880001    0.52740002
     5.78338194]
 [ 133.04782152    0.38680002    0.89359999    0.53039998    0.69469994
     6.73109293]
 [ -96.35236259    0.72210002    0.83560002    0.58760005    0.31549999
     6.18281221]][0m
[37m[1m[2023-07-17 05:01:54,401][257371] Max Reward on eval: 650.2336197087541[0m
[37m[1m[2023-07-17 05:01:54,401][257371] Min Reward on eval: -430.41162858484313[0m
[37m[1m[2023-07-17 05:01:54,401][257371] Mean Reward across all agents: 20.904645145600707[0m
[37m[1m[2023-07-17 05:01:54,402][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:01:54,407][257371] mean_value=-312.6521979118347, max_value=651.9367857652958[0m
[37m[1m[2023-07-17 05:01:54,410][257371] New mean coefficients: [[-1.9593477  -2.206089   -0.27294675 -1.2416735  -0.5650567   8.183952  ]][0m
[37m[1m[2023-07-17 05:01:54,411][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:02:03,388][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 05:02:03,388][257371] FPS: 427815.60[0m
[36m[2023-07-17 05:02:03,391][257371] itr=673, itrs=2000, Progress: 33.65%[0m
[36m[2023-07-17 05:02:15,029][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 05:02:15,029][257371] FPS: 332438.69[0m
[36m[2023-07-17 05:02:19,359][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:02:19,359][257371] Reward + Measures: [[-106.39124624    0.64884537    0.88955426    0.74958563    0.33322832
     7.56434107]][0m
[37m[1m[2023-07-17 05:02:19,359][257371] Max Reward on eval: -106.39124624252166[0m
[37m[1m[2023-07-17 05:02:19,360][257371] Min Reward on eval: -106.39124624252166[0m
[37m[1m[2023-07-17 05:02:19,360][257371] Mean Reward across all agents: -106.39124624252166[0m
[37m[1m[2023-07-17 05:02:19,360][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:02:24,410][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:02:24,411][257371] Reward + Measures: [[  41.62350704    0.76990002    0.80380005    0.3427        0.49610001
     6.1386652 ]
 [ -70.53533529    0.96700001    0.98730004    0.003         0.99200004
     5.82523346]
 [  26.32956679    0.80680001    0.8082        0.77350003    0.1497
     5.39341974]
 ...
 [ 236.95159054    0.87540001    0.95100003    0.88799995    0.95250005
     6.51073408]
 [  22.62339831    0.33039999    0.7428        0.42139998    0.63010007
     5.43425131]
 [-212.79473348    0.63419998    0.78960007    0.66390002    0.2816
     6.30055571]][0m
[37m[1m[2023-07-17 05:02:24,411][257371] Max Reward on eval: 486.0973012473434[0m
[37m[1m[2023-07-17 05:02:24,411][257371] Min Reward on eval: -575.0266781143844[0m
[37m[1m[2023-07-17 05:02:24,411][257371] Mean Reward across all agents: -38.171238946724266[0m
[37m[1m[2023-07-17 05:02:24,412][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:02:24,415][257371] mean_value=-750.2900086031498, max_value=282.56977604661705[0m
[37m[1m[2023-07-17 05:02:24,418][257371] New mean coefficients: [[-1.6746653 -2.9831567  0.1462102 -1.3545488 -2.0107121  7.7590556]][0m
[37m[1m[2023-07-17 05:02:24,419][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:02:33,539][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 05:02:33,539][257371] FPS: 421142.19[0m
[36m[2023-07-17 05:02:33,542][257371] itr=674, itrs=2000, Progress: 33.70%[0m
[36m[2023-07-17 05:02:45,268][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 05:02:45,268][257371] FPS: 329974.06[0m
[36m[2023-07-17 05:02:49,496][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:02:49,496][257371] Reward + Measures: [[-108.28114061    0.65195435    0.89332533    0.75213265    0.33558235
     7.57428503]][0m
[37m[1m[2023-07-17 05:02:49,496][257371] Max Reward on eval: -108.28114060962565[0m
[37m[1m[2023-07-17 05:02:49,497][257371] Min Reward on eval: -108.28114060962565[0m
[37m[1m[2023-07-17 05:02:49,497][257371] Mean Reward across all agents: -108.28114060962565[0m
[37m[1m[2023-07-17 05:02:49,497][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:02:54,643][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:02:54,644][257371] Reward + Measures: [[-29.10365621   0.24770001   0.26409999   0.19780001   0.204
    5.04648733]
 [-13.67796482   0.1831       0.1671       0.17470001   0.1684
    4.00945807]
 [-29.90702778   0.30610001   0.30180001   0.31040004   0.31720003
    4.69896936]
 ...
 [  5.05976826   0.25229999   0.26500002   0.2174       0.30650002
    4.00989771]
 [  9.8251913    0.44630003   0.44730002   0.40959999   0.41920003
    6.02351999]
 [185.95596077   0.94010001   0.94080001   0.93220007   0.93559998
    6.23223591]][0m
[37m[1m[2023-07-17 05:02:54,644][257371] Max Reward on eval: 619.5147705123294[0m
[37m[1m[2023-07-17 05:02:54,644][257371] Min Reward on eval: -396.16293051615355[0m
[37m[1m[2023-07-17 05:02:54,645][257371] Mean Reward across all agents: -21.268904136085283[0m
[37m[1m[2023-07-17 05:02:54,645][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:02:54,647][257371] mean_value=-883.7107715058185, max_value=261.22661595376564[0m
[37m[1m[2023-07-17 05:02:54,650][257371] New mean coefficients: [[-0.904214  -2.7011087 -0.4422549 -1.411399  -2.9302247  8.633305 ]][0m
[37m[1m[2023-07-17 05:02:54,651][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:03:03,590][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 05:03:03,590][257371] FPS: 429632.77[0m
[36m[2023-07-17 05:03:03,593][257371] itr=675, itrs=2000, Progress: 33.75%[0m
[36m[2023-07-17 05:03:15,196][257371] train() took 11.52 seconds to complete[0m
[36m[2023-07-17 05:03:15,196][257371] FPS: 333395.70[0m
[36m[2023-07-17 05:03:19,455][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:03:19,460][257371] Reward + Measures: [[-103.23702022    0.61365837    0.89423394    0.71737295    0.39651701
     7.56020689]][0m
[37m[1m[2023-07-17 05:03:19,461][257371] Max Reward on eval: -103.23702021932141[0m
[37m[1m[2023-07-17 05:03:19,461][257371] Min Reward on eval: -103.23702021932141[0m
[37m[1m[2023-07-17 05:03:19,461][257371] Mean Reward across all agents: -103.23702021932141[0m
[37m[1m[2023-07-17 05:03:19,461][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:03:24,489][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:03:24,494][257371] Reward + Measures: [[-144.1166399     0.79180002    0.85739994    0.62810004    0.30639997
     6.74528837]
 [   7.63032814    0.3475        0.4082        0.22649999    0.27520001
     3.20069814]
 [ 131.63972548    0.64200002    0.60869998    0.4948        0.16360001
     4.5173068 ]
 ...
 [  -6.91818876    0.51969999    0.57639998    0.46630001    0.23
     4.84205008]
 [  72.71467204    0.56220007    0.55129999    0.49980003    0.09499999
     4.37890482]
 [-112.64925845    0.70090002    0.87530005    0.60210007    0.38570002
     6.51041651]][0m
[37m[1m[2023-07-17 05:03:24,495][257371] Max Reward on eval: 493.66833492405715[0m
[37m[1m[2023-07-17 05:03:24,495][257371] Min Reward on eval: -511.43860242702067[0m
[37m[1m[2023-07-17 05:03:24,495][257371] Mean Reward across all agents: -28.542626166438403[0m
[37m[1m[2023-07-17 05:03:24,496][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:03:24,498][257371] mean_value=-708.1603619031845, max_value=177.95797834841954[0m
[37m[1m[2023-07-17 05:03:24,501][257371] New mean coefficients: [[-0.75948006 -2.434914   -0.1839127  -1.4808377  -1.936836    7.9375677 ]][0m
[37m[1m[2023-07-17 05:03:24,502][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:03:33,509][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 05:03:33,509][257371] FPS: 426416.05[0m
[36m[2023-07-17 05:03:33,511][257371] itr=676, itrs=2000, Progress: 33.80%[0m
[36m[2023-07-17 05:03:45,404][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 05:03:45,404][257371] FPS: 325335.79[0m
[36m[2023-07-17 05:03:49,784][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:03:49,784][257371] Reward + Measures: [[-92.53280873   0.58611971   0.90291399   0.70016301   0.44416967
    7.58354855]][0m
[37m[1m[2023-07-17 05:03:49,784][257371] Max Reward on eval: -92.5328087336109[0m
[37m[1m[2023-07-17 05:03:49,785][257371] Min Reward on eval: -92.5328087336109[0m
[37m[1m[2023-07-17 05:03:49,785][257371] Mean Reward across all agents: -92.5328087336109[0m
[37m[1m[2023-07-17 05:03:49,785][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:03:54,799][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:03:54,799][257371] Reward + Measures: [[-17.90968631   0.26019999   0.51710004   0.4619       0.35970002
    4.71593475]
 [ 10.60859384   0.1585       0.4357       0.35769999   0.40340003
    4.13473749]
 [-30.73395689   0.38910004   0.49470001   0.27369997   0.31670004
    3.83425379]
 ...
 [-60.68834687   0.33440003   0.38000003   0.33469999   0.27980003
    3.83795214]
 [-65.2525984    0.29270002   0.39039999   0.20750001   0.3554
    3.52771759]
 [  7.46910465   0.72170001   0.8132       0.48409995   0.38480002
    5.96480894]][0m
[37m[1m[2023-07-17 05:03:54,799][257371] Max Reward on eval: 277.1600064896047[0m
[37m[1m[2023-07-17 05:03:54,800][257371] Min Reward on eval: -422.3245811840519[0m
[37m[1m[2023-07-17 05:03:54,800][257371] Mean Reward across all agents: -48.278329256566586[0m
[37m[1m[2023-07-17 05:03:54,800][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:03:54,803][257371] mean_value=-858.948421621437, max_value=176.05024112078934[0m
[37m[1m[2023-07-17 05:03:54,806][257371] New mean coefficients: [[-0.18297946 -1.1872667  -0.1267358  -1.7127292  -0.24317098  7.446813  ]][0m
[37m[1m[2023-07-17 05:03:54,806][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:04:03,778][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 05:04:03,778][257371] FPS: 428111.67[0m
[36m[2023-07-17 05:04:03,780][257371] itr=677, itrs=2000, Progress: 33.85%[0m
[36m[2023-07-17 05:04:15,747][257371] train() took 11.88 seconds to complete[0m
[36m[2023-07-17 05:04:15,747][257371] FPS: 323223.91[0m
[36m[2023-07-17 05:04:20,064][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:04:20,070][257371] Reward + Measures: [[-104.80195779    0.60943633    0.89775664    0.71334034    0.41136301
     7.58626938]][0m
[37m[1m[2023-07-17 05:04:20,070][257371] Max Reward on eval: -104.80195778559148[0m
[37m[1m[2023-07-17 05:04:20,070][257371] Min Reward on eval: -104.80195778559148[0m
[37m[1m[2023-07-17 05:04:20,071][257371] Mean Reward across all agents: -104.80195778559148[0m
[37m[1m[2023-07-17 05:04:20,071][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:04:25,112][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:04:25,118][257371] Reward + Measures: [[130.49297551   0.59540004   0.69010001   0.65880007   0.12459999
    6.53228998]
 [ -6.0585633    0.26019999   0.28929999   0.2494       0.27790004
    3.70324564]
 [-31.41857604   0.27419999   0.28659999   0.3039       0.2791
    3.72742534]
 ...
 [-88.34107052   0.2942       0.51359999   0.39820001   0.62519997
    5.33643866]
 [323.25535389   0.5158       0.62650001   0.55170006   0.22839999
    5.49589109]
 [169.52767602   0.18099999   0.68380004   0.68900007   0.58740002
    6.48212528]][0m
[37m[1m[2023-07-17 05:04:25,119][257371] Max Reward on eval: 539.24843411115[0m
[37m[1m[2023-07-17 05:04:25,119][257371] Min Reward on eval: -367.6323308698833[0m
[37m[1m[2023-07-17 05:04:25,120][257371] Mean Reward across all agents: 35.58167153357925[0m
[37m[1m[2023-07-17 05:04:25,121][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:04:25,129][257371] mean_value=-481.06178388969806, max_value=566.072225623452[0m
[37m[1m[2023-07-17 05:04:25,134][257371] New mean coefficients: [[ 1.0790002   0.19313288  0.37710804 -1.7196438   1.073796    7.3862686 ]][0m
[37m[1m[2023-07-17 05:04:25,136][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:04:34,141][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 05:04:34,141][257371] FPS: 426515.69[0m
[36m[2023-07-17 05:04:34,144][257371] itr=678, itrs=2000, Progress: 33.90%[0m
[36m[2023-07-17 05:04:45,804][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 05:04:45,804][257371] FPS: 331791.79[0m
[36m[2023-07-17 05:04:50,060][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:04:50,060][257371] Reward + Measures: [[-84.18065192   0.56984103   0.900406     0.68660402   0.46811929
    7.56929111]][0m
[37m[1m[2023-07-17 05:04:50,060][257371] Max Reward on eval: -84.18065191539004[0m
[37m[1m[2023-07-17 05:04:50,060][257371] Min Reward on eval: -84.18065191539004[0m
[37m[1m[2023-07-17 05:04:50,061][257371] Mean Reward across all agents: -84.18065191539004[0m
[37m[1m[2023-07-17 05:04:50,061][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:04:55,065][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:04:55,066][257371] Reward + Measures: [[ -44.42512007    0.32020003    0.38170001    0.27940002    0.46779999
     3.76539874]
 [-175.63861464    0.75010002    0.0627        0.7457        0.53009999
     5.07589626]
 [  66.52821379    0.53080004    0.69379997    0.66799998    0.21010001
     6.00170851]
 ...
 [-160.8256334     0.76290005    0.76580006    0.68519998    0.1464
     5.54553604]
 [ -51.02545741    0.49309999    0.49629998    0.46799999    0.15910001
     5.13371515]
 [ -48.67849207    0.25780001    0.88239998    0.86420006    0.66180003
     6.59875345]][0m
[37m[1m[2023-07-17 05:04:55,066][257371] Max Reward on eval: 527.8140745415818[0m
[37m[1m[2023-07-17 05:04:55,066][257371] Min Reward on eval: -825.2303542881215[0m
[37m[1m[2023-07-17 05:04:55,067][257371] Mean Reward across all agents: -46.932051439422736[0m
[37m[1m[2023-07-17 05:04:55,067][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:04:55,071][257371] mean_value=-591.7543719467699, max_value=614.3269743928365[0m
[37m[1m[2023-07-17 05:04:55,074][257371] New mean coefficients: [[ 0.9807093   0.28981405  0.3988415  -1.6761719   0.59052575  7.4514184 ]][0m
[37m[1m[2023-07-17 05:04:55,075][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:05:04,073][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 05:05:04,073][257371] FPS: 426849.44[0m
[36m[2023-07-17 05:05:04,076][257371] itr=679, itrs=2000, Progress: 33.95%[0m
[36m[2023-07-17 05:05:15,830][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 05:05:15,831][257371] FPS: 329187.30[0m
[36m[2023-07-17 05:05:20,123][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:05:20,123][257371] Reward + Measures: [[-83.07403242   0.57458603   0.90531671   0.69352961   0.46710932
    7.59074974]][0m
[37m[1m[2023-07-17 05:05:20,123][257371] Max Reward on eval: -83.07403241702598[0m
[37m[1m[2023-07-17 05:05:20,124][257371] Min Reward on eval: -83.07403241702598[0m
[37m[1m[2023-07-17 05:05:20,124][257371] Mean Reward across all agents: -83.07403241702598[0m
[37m[1m[2023-07-17 05:05:20,124][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:05:25,130][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:05:25,136][257371] Reward + Measures: [[ -47.37226747    0.4804        0.80510008    0.16350001    0.75500005
     5.84528303]
 [  -4.63256787    0.2978        0.45919997    0.30239996    0.41330004
     3.91730857]
 [-235.00832621    0.20120001    0.30039999    0.34          0.1839
     3.79853892]
 ...
 [ 122.68481827    0.4073        0.93779993    0.421         0.96690005
     5.73601866]
 [ 297.57173918    0.79320002    0.1219        0.7985        0.86139995
     5.69505072]
 [-416.08937742    0.31029999    0.84350008    0.37939999    0.84650004
     5.90355444]][0m
[37m[1m[2023-07-17 05:05:25,136][257371] Max Reward on eval: 587.289928438235[0m
[37m[1m[2023-07-17 05:05:25,137][257371] Min Reward on eval: -518.5567512642592[0m
[37m[1m[2023-07-17 05:05:25,137][257371] Mean Reward across all agents: -53.101024169565726[0m
[37m[1m[2023-07-17 05:05:25,138][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:05:25,147][257371] mean_value=-432.44047365004775, max_value=591.8846205476672[0m
[37m[1m[2023-07-17 05:05:25,151][257371] New mean coefficients: [[ 1.086986    0.32531944  0.79725647 -1.5029479   0.73272514  7.675569  ]][0m
[37m[1m[2023-07-17 05:05:25,152][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:05:34,220][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 05:05:34,220][257371] FPS: 423579.04[0m
[36m[2023-07-17 05:05:34,222][257371] itr=680, itrs=2000, Progress: 34.00%[0m
[37m[1m[2023-07-17 05:08:39,816][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000660[0m
[36m[2023-07-17 05:08:51,975][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 05:08:51,975][257371] FPS: 332503.46[0m
[36m[2023-07-17 05:08:56,200][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:08:56,200][257371] Reward + Measures: [[-67.26872178   0.55858397   0.90724331   0.67992568   0.49585566
    7.58103514]][0m
[37m[1m[2023-07-17 05:08:56,201][257371] Max Reward on eval: -67.26872177683816[0m
[37m[1m[2023-07-17 05:08:56,201][257371] Min Reward on eval: -67.26872177683816[0m
[37m[1m[2023-07-17 05:08:56,201][257371] Mean Reward across all agents: -67.26872177683816[0m
[37m[1m[2023-07-17 05:08:56,201][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:09:01,161][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:09:01,162][257371] Reward + Measures: [[ -88.09336506    0.52069998    0.73630005    0.35269997    0.50660002
     5.0132184 ]
 [-160.05475135    0.278         0.77260005    0.44689998    0.75819999
     4.98126984]
 [  45.35972448    0.7281        0.89309996    0.71180004    0.30350003
     4.55814075]
 ...
 [ -33.41238058    0.37410003    0.9307        0.66900003    0.67449999
     5.80936813]
 [ -42.53185563    0.31920001    0.71439999    0.43459997    0.72980005
     5.83376312]
 [  -7.01841246    0.1848        0.32750002    0.1925        0.20610002
     4.55490589]][0m
[37m[1m[2023-07-17 05:09:01,162][257371] Max Reward on eval: 665.295534133166[0m
[37m[1m[2023-07-17 05:09:01,162][257371] Min Reward on eval: -468.3686790449545[0m
[37m[1m[2023-07-17 05:09:01,162][257371] Mean Reward across all agents: 10.179827895328996[0m
[37m[1m[2023-07-17 05:09:01,163][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:09:01,166][257371] mean_value=-363.22466913703914, max_value=364.62491327871635[0m
[37m[1m[2023-07-17 05:09:01,168][257371] New mean coefficients: [[ 0.98277277  1.0203084   0.4072175  -1.7685537   0.9728433   7.5224233 ]][0m
[37m[1m[2023-07-17 05:09:01,169][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:09:10,097][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 05:09:10,098][257371] FPS: 430174.82[0m
[36m[2023-07-17 05:09:10,100][257371] itr=681, itrs=2000, Progress: 34.05%[0m
[36m[2023-07-17 05:09:21,907][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 05:09:21,907][257371] FPS: 327572.31[0m
[36m[2023-07-17 05:09:26,194][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:09:26,195][257371] Reward + Measures: [[-58.54569851   0.56025934   0.90828794   0.66990769   0.50540131
    7.58145189]][0m
[37m[1m[2023-07-17 05:09:26,195][257371] Max Reward on eval: -58.54569851049424[0m
[37m[1m[2023-07-17 05:09:26,195][257371] Min Reward on eval: -58.54569851049424[0m
[37m[1m[2023-07-17 05:09:26,195][257371] Mean Reward across all agents: -58.54569851049424[0m
[37m[1m[2023-07-17 05:09:26,196][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:09:31,416][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:09:31,421][257371] Reward + Measures: [[-49.36270114   0.37550002   0.31900001   0.32870004   0.2247
    5.58751822]
 [161.36870958   0.34729999   0.91230005   0.46640006   0.81870002
    6.87182188]
 [-34.79937888   0.5291       0.83350003   0.56140006   0.48569998
    4.2480092 ]
 ...
 [-89.63191207   0.52999997   0.67040002   0.51400006   0.37270001
    5.38717556]
 [ 17.29875316   0.2282       0.4068       0.1894       0.35410002
    3.485394  ]
 [ 81.52906581   0.62190002   0.67480004   0.53380007   0.67920005
    4.79214811]][0m
[37m[1m[2023-07-17 05:09:31,422][257371] Max Reward on eval: 579.1661099999212[0m
[37m[1m[2023-07-17 05:09:31,422][257371] Min Reward on eval: -499.7157463892363[0m
[37m[1m[2023-07-17 05:09:31,422][257371] Mean Reward across all agents: 5.654299166753615[0m
[37m[1m[2023-07-17 05:09:31,422][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:09:31,425][257371] mean_value=-1087.4705089024974, max_value=343.4612800687857[0m
[37m[1m[2023-07-17 05:09:31,428][257371] New mean coefficients: [[ 0.6222286  -0.21364081  0.40209842 -1.2541249  -0.75885296  7.859495  ]][0m
[37m[1m[2023-07-17 05:09:31,428][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:09:40,550][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 05:09:40,551][257371] FPS: 421034.85[0m
[36m[2023-07-17 05:09:40,553][257371] itr=682, itrs=2000, Progress: 34.10%[0m
[36m[2023-07-17 05:09:52,242][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 05:09:52,242][257371] FPS: 330923.89[0m
[36m[2023-07-17 05:09:56,535][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:09:56,536][257371] Reward + Measures: [[-48.16989711   0.54905397   0.91115642   0.67718434   0.51095766
    7.59188128]][0m
[37m[1m[2023-07-17 05:09:56,536][257371] Max Reward on eval: -48.16989711414751[0m
[37m[1m[2023-07-17 05:09:56,536][257371] Min Reward on eval: -48.16989711414751[0m
[37m[1m[2023-07-17 05:09:56,537][257371] Mean Reward across all agents: -48.16989711414751[0m
[37m[1m[2023-07-17 05:09:56,537][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:10:01,482][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:10:01,483][257371] Reward + Measures: [[  19.36838889    0.58880001    0.62260002    0.56739998    0.14210001
     3.88348174]
 [-312.7003714     0.86180001    0.9587        0.81709999    0.2142
     6.01420403]
 [  30.48514939    0.24780002    0.7191        0.67580003    0.57270002
     3.49526334]
 ...
 [ -36.41758432    0.33750001    0.42399999    0.3867        0.3055
     2.62399364]
 [   6.86993547    0.52380002    0.86740011    0.60219997    0.47550002
     6.80358458]
 [ -81.58050709    0.45210001    0.95690006    0.53490001    0.79890007
     7.80445719]][0m
[37m[1m[2023-07-17 05:10:01,483][257371] Max Reward on eval: 462.80833718404176[0m
[37m[1m[2023-07-17 05:10:01,484][257371] Min Reward on eval: -479.1702804113738[0m
[37m[1m[2023-07-17 05:10:01,484][257371] Mean Reward across all agents: -1.1712293263698081[0m
[37m[1m[2023-07-17 05:10:01,484][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:10:01,489][257371] mean_value=-516.2671051969452, max_value=514.2582798932912[0m
[37m[1m[2023-07-17 05:10:01,492][257371] New mean coefficients: [[ 0.29231477 -0.44468206 -0.17638803 -1.4344094  -2.026462    8.143311  ]][0m
[37m[1m[2023-07-17 05:10:01,493][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:10:10,479][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 05:10:10,479][257371] FPS: 427409.99[0m
[36m[2023-07-17 05:10:10,481][257371] itr=683, itrs=2000, Progress: 34.15%[0m
[36m[2023-07-17 05:10:22,394][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 05:10:22,394][257371] FPS: 324767.87[0m
[36m[2023-07-17 05:10:26,708][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:10:26,708][257371] Reward + Measures: [[-60.81951744   0.580167     0.90987599   0.704027     0.46287936
    7.61044407]][0m
[37m[1m[2023-07-17 05:10:26,708][257371] Max Reward on eval: -60.8195174447229[0m
[37m[1m[2023-07-17 05:10:26,709][257371] Min Reward on eval: -60.8195174447229[0m
[37m[1m[2023-07-17 05:10:26,709][257371] Mean Reward across all agents: -60.8195174447229[0m
[37m[1m[2023-07-17 05:10:26,709][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:10:31,708][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:10:31,714][257371] Reward + Measures: [[ 163.50789642    0.11280002    0.98029995    0.60360003    0.9835
     5.96592045]
 [-295.27268889    0.61000007    0.98220009    0.20649998    0.98460007
     6.02362585]
 [ -74.71571125    0.30590001    0.50369996    0.30749997    0.46470004
     4.13462782]
 ...
 [ -29.34668949    0.25130001    0.24790004    0.1781        0.2509
     3.62258601]
 [ 199.16891845    0.51240003    0.86329997    0.85010004    0.3707
     5.35579634]
 [  -1.73180746    0.26079997    0.2368        0.26079997    0.28130001
     4.49816847]][0m
[37m[1m[2023-07-17 05:10:31,714][257371] Max Reward on eval: 517.3127169732004[0m
[37m[1m[2023-07-17 05:10:31,715][257371] Min Reward on eval: -473.2992172882077[0m
[37m[1m[2023-07-17 05:10:31,715][257371] Mean Reward across all agents: 30.990907862808392[0m
[37m[1m[2023-07-17 05:10:31,715][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:10:31,720][257371] mean_value=-474.6865021108763, max_value=316.8243597122826[0m
[37m[1m[2023-07-17 05:10:31,723][257371] New mean coefficients: [[ 0.10373122 -0.73803055 -1.0593591  -1.7345421  -2.7679224   8.645411  ]][0m
[37m[1m[2023-07-17 05:10:31,724][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:10:40,733][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 05:10:40,733][257371] FPS: 426305.13[0m
[36m[2023-07-17 05:10:40,736][257371] itr=684, itrs=2000, Progress: 34.20%[0m
[36m[2023-07-17 05:10:52,681][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-17 05:10:52,682][257371] FPS: 323895.94[0m
[36m[2023-07-17 05:10:57,109][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:10:57,110][257371] Reward + Measures: [[-59.97774409   0.57889068   0.91091794   0.69211704   0.47640434
    7.6220417 ]][0m
[37m[1m[2023-07-17 05:10:57,110][257371] Max Reward on eval: -59.977744093773495[0m
[37m[1m[2023-07-17 05:10:57,110][257371] Min Reward on eval: -59.977744093773495[0m
[37m[1m[2023-07-17 05:10:57,110][257371] Mean Reward across all agents: -59.977744093773495[0m
[37m[1m[2023-07-17 05:10:57,111][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:11:02,173][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:11:02,174][257371] Reward + Measures: [[ 94.22610091   0.35899997   0.93849993   0.21920002   0.93489999
    4.57171679]
 [ -2.8996242    0.368        0.40570003   0.35789999   0.41430002
    3.30979204]
 [103.42116449   0.4237       0.67039996   0.3529       0.66899997
    4.68933201]
 ...
 [-64.74038407   0.55730003   0.73210001   0.2951       0.62020004
    3.3723855 ]
 [ 14.96448038   0.37890002   0.53240007   0.45359999   0.22750001
    6.26358891]
 [-16.7208333    0.51139992   0.57100004   0.3901       0.4522
    3.50359321]][0m
[37m[1m[2023-07-17 05:11:02,174][257371] Max Reward on eval: 531.6680059589446[0m
[37m[1m[2023-07-17 05:11:02,174][257371] Min Reward on eval: -400.7765350423753[0m
[37m[1m[2023-07-17 05:11:02,174][257371] Mean Reward across all agents: 30.65362194173816[0m
[37m[1m[2023-07-17 05:11:02,175][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:11:02,179][257371] mean_value=-253.621332963183, max_value=318.79886112066686[0m
[37m[1m[2023-07-17 05:11:02,182][257371] New mean coefficients: [[ 0.80302364 -0.77257985 -1.392663   -1.357599   -2.6154292   9.194353  ]][0m
[37m[1m[2023-07-17 05:11:02,183][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:11:11,279][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 05:11:11,280][257371] FPS: 422228.44[0m
[36m[2023-07-17 05:11:11,282][257371] itr=685, itrs=2000, Progress: 34.25%[0m
[36m[2023-07-17 05:11:23,007][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 05:11:23,007][257371] FPS: 329964.34[0m
[36m[2023-07-17 05:11:27,261][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:11:27,261][257371] Reward + Measures: [[-32.79101273   0.53133899   0.90927833   0.667786     0.53226233
    7.61846352]][0m
[37m[1m[2023-07-17 05:11:27,262][257371] Max Reward on eval: -32.79101273010628[0m
[37m[1m[2023-07-17 05:11:27,262][257371] Min Reward on eval: -32.79101273010628[0m
[37m[1m[2023-07-17 05:11:27,262][257371] Mean Reward across all agents: -32.79101273010628[0m
[37m[1m[2023-07-17 05:11:27,262][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:11:32,279][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:11:32,280][257371] Reward + Measures: [[ 142.60399931    0.32530001    0.47320005    0.48179999    0.25170001
     4.82239103]
 [ 190.32290649    0.39039999    0.85839999    0.66530001    0.60110003
     6.2435174 ]
 [-264.64058398    0.39210001    0.79699993    0.2739        0.79150003
     5.6981926 ]
 ...
 [ -74.77449345    0.67000002    0.88490009    0.45110002    0.55370003
     5.67291737]
 [  65.87983799    0.62759995    0.71240002    0.43169999    0.31860003
     5.52967453]
 [-120.04183234    0.50500005    0.84889996    0.61560005    0.4849
     6.26279211]][0m
[37m[1m[2023-07-17 05:11:32,280][257371] Max Reward on eval: 453.3128023038618[0m
[37m[1m[2023-07-17 05:11:32,280][257371] Min Reward on eval: -481.00570021537135[0m
[37m[1m[2023-07-17 05:11:32,280][257371] Mean Reward across all agents: -40.130096780052845[0m
[37m[1m[2023-07-17 05:11:32,281][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:11:32,285][257371] mean_value=-315.44178682787265, max_value=476.322100277605[0m
[37m[1m[2023-07-17 05:11:32,288][257371] New mean coefficients: [[ 0.9479823   0.21693635 -1.3119931  -1.4644152  -1.4178487   9.088908  ]][0m
[37m[1m[2023-07-17 05:11:32,288][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:11:41,282][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 05:11:41,282][257371] FPS: 427033.02[0m
[36m[2023-07-17 05:11:41,285][257371] itr=686, itrs=2000, Progress: 34.30%[0m
[36m[2023-07-17 05:11:52,967][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 05:11:52,967][257371] FPS: 331249.02[0m
[36m[2023-07-17 05:11:57,234][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:11:57,234][257371] Reward + Measures: [[-24.71137218   0.51675165   0.89890093   0.64158833   0.55233866
    7.58882475]][0m
[37m[1m[2023-07-17 05:11:57,234][257371] Max Reward on eval: -24.71137217507457[0m
[37m[1m[2023-07-17 05:11:57,235][257371] Min Reward on eval: -24.71137217507457[0m
[37m[1m[2023-07-17 05:11:57,235][257371] Mean Reward across all agents: -24.71137217507457[0m
[37m[1m[2023-07-17 05:11:57,235][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:12:02,455][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:12:02,456][257371] Reward + Measures: [[  -1.74378907    0.46310002    0.37540004    0.40310001    0.35199997
     3.19045496]
 [-554.434165      0.0701        0.68370003    0.31330001    0.6803
     5.19940853]
 [ -10.76820284    0.1419        0.1645        0.13960001    0.14530002
     4.34676981]
 ...
 [ -56.52967443    0.34090003    0.48179999    0.22680001    0.41179997
     4.39470768]
 [-168.08930236    0.3303        0.9314        0.3134        0.92469996
     4.87261152]
 [ -56.9833582     0.46300003    0.48470002    0.51010001    0.21290003
     4.4650836 ]][0m
[37m[1m[2023-07-17 05:12:02,456][257371] Max Reward on eval: 553.3266084130853[0m
[37m[1m[2023-07-17 05:12:02,456][257371] Min Reward on eval: -554.434164999798[0m
[37m[1m[2023-07-17 05:12:02,457][257371] Mean Reward across all agents: -21.380940355099717[0m
[37m[1m[2023-07-17 05:12:02,457][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:12:02,460][257371] mean_value=-902.2775916535547, max_value=179.6787052270326[0m
[37m[1m[2023-07-17 05:12:02,463][257371] New mean coefficients: [[ 1.2598989   1.0070806  -0.8759254  -1.7423999   0.13701344  8.289604  ]][0m
[37m[1m[2023-07-17 05:12:02,464][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:12:11,533][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 05:12:11,534][257371] FPS: 423452.08[0m
[36m[2023-07-17 05:12:11,536][257371] itr=687, itrs=2000, Progress: 34.35%[0m
[36m[2023-07-17 05:12:23,271][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 05:12:23,272][257371] FPS: 329724.69[0m
[36m[2023-07-17 05:12:27,637][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:12:27,638][257371] Reward + Measures: [[-8.01591761  0.48947933  0.90218002  0.63596332  0.58353734  7.60045052]][0m
[37m[1m[2023-07-17 05:12:27,638][257371] Max Reward on eval: -8.015917605183708[0m
[37m[1m[2023-07-17 05:12:27,638][257371] Min Reward on eval: -8.015917605183708[0m
[37m[1m[2023-07-17 05:12:27,638][257371] Mean Reward across all agents: -8.015917605183708[0m
[37m[1m[2023-07-17 05:12:27,639][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:12:32,615][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:12:32,616][257371] Reward + Measures: [[-256.64986353    0.87180007    0.88560003    0.39660001    0.51169997
     5.76111317]
 [-161.59425707    0.55240005    0.64390004    0.31470001    0.4271
     5.44214392]
 [-168.30733423    0.88280004    0.89589995    0.87129992    0.08580001
     5.46850348]
 ...
 [-187.43503176    0.63010001    0.66800004    0.32460001    0.4488
     5.58277845]
 [ 228.04112432    0.28569999    0.2613        0.17660001    0.23220001
     4.73914385]
 [   0.30826286    0.95480007    0.92439997    0.93800002    0.58810002
     5.91421652]][0m
[37m[1m[2023-07-17 05:12:32,616][257371] Max Reward on eval: 590.2795354425907[0m
[37m[1m[2023-07-17 05:12:32,616][257371] Min Reward on eval: -556.6276922114193[0m
[37m[1m[2023-07-17 05:12:32,616][257371] Mean Reward across all agents: 50.164127099968866[0m
[37m[1m[2023-07-17 05:12:32,617][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:12:32,622][257371] mean_value=-353.056729390761, max_value=562.8133363230154[0m
[37m[1m[2023-07-17 05:12:32,625][257371] New mean coefficients: [[ 1.6953636   1.0929805   0.112818   -1.9986336   0.18705766  7.627794  ]][0m
[37m[1m[2023-07-17 05:12:32,626][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:12:41,594][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 05:12:41,595][257371] FPS: 428233.53[0m
[36m[2023-07-17 05:12:41,597][257371] itr=688, itrs=2000, Progress: 34.40%[0m
[36m[2023-07-17 05:12:53,346][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 05:12:53,346][257371] FPS: 329301.61[0m
[36m[2023-07-17 05:12:57,587][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:12:57,587][257371] Reward + Measures: [[-6.73348069  0.48895231  0.90880221  0.64098799  0.58523667  7.62168026]][0m
[37m[1m[2023-07-17 05:12:57,587][257371] Max Reward on eval: -6.733480688242135[0m
[37m[1m[2023-07-17 05:12:57,588][257371] Min Reward on eval: -6.733480688242135[0m
[37m[1m[2023-07-17 05:12:57,588][257371] Mean Reward across all agents: -6.733480688242135[0m
[37m[1m[2023-07-17 05:12:57,588][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:13:02,584][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:13:02,585][257371] Reward + Measures: [[ 154.85151432    0.26519999    0.66300005    0.41210005    0.51769996
     5.84790802]
 [-524.85980985    0.91829997    0.97090006    0.0147        0.98120004
     5.927948  ]
 [   5.24174192    0.50310004    0.87170011    0.5011        0.6024
     6.93810606]
 ...
 [ -53.05177572    0.23530002    0.34580001    0.21710001    0.35609999
     3.70076919]
 [-241.98129085    0.53599995    0.861         0.48460004    0.65350002
     6.95705271]
 [-136.50340799    0.74520004    0.84020007    0.38340002    0.59219998
     5.2785821 ]][0m
[37m[1m[2023-07-17 05:13:02,585][257371] Max Reward on eval: 563.4127540080808[0m
[37m[1m[2023-07-17 05:13:02,585][257371] Min Reward on eval: -724.4051513809711[0m
[37m[1m[2023-07-17 05:13:02,586][257371] Mean Reward across all agents: -82.39205835967095[0m
[37m[1m[2023-07-17 05:13:02,586][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:13:02,590][257371] mean_value=-592.6799199588423, max_value=582.1029473747627[0m
[37m[1m[2023-07-17 05:13:02,593][257371] New mean coefficients: [[ 1.3680246   0.79263717 -0.10231759 -2.156157   -0.6109667   7.391653  ]][0m
[37m[1m[2023-07-17 05:13:02,594][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:13:11,695][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 05:13:11,695][257371] FPS: 422010.86[0m
[36m[2023-07-17 05:13:11,697][257371] itr=689, itrs=2000, Progress: 34.45%[0m
[36m[2023-07-17 05:13:23,471][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 05:13:23,472][257371] FPS: 328567.56[0m
[36m[2023-07-17 05:13:27,796][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:13:27,797][257371] Reward + Measures: [[2.68081955 0.48632601 0.90418434 0.63404197 0.58944899 7.62700033]][0m
[37m[1m[2023-07-17 05:13:27,797][257371] Max Reward on eval: 2.6808195513542743[0m
[37m[1m[2023-07-17 05:13:27,797][257371] Min Reward on eval: 2.6808195513542743[0m
[37m[1m[2023-07-17 05:13:27,798][257371] Mean Reward across all agents: 2.6808195513542743[0m
[37m[1m[2023-07-17 05:13:27,798][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:13:32,833][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:13:32,834][257371] Reward + Measures: [[ 330.77704525    0.1365        0.88980001    0.54220003    0.89920008
     7.20141554]
 [-220.76170443    0.65939999    0.67769998    0.57430005    0.16160001
     6.10283422]
 [-365.2418213     0.65690005    0.95460004    0.31440002    0.8344
     7.31889868]
 ...
 [-247.22387001    0.56090003    0.96149999    0.64090008    0.61479998
     7.36981297]
 [-358.45506859    0.64580005    0.96569997    0.25240001    0.92950004
     7.39415312]
 [  -5.65454265    0.2404        0.68290001    0.63510001    0.53030002
     4.61891937]][0m
[37m[1m[2023-07-17 05:13:32,834][257371] Max Reward on eval: 532.0188341111876[0m
[37m[1m[2023-07-17 05:13:32,834][257371] Min Reward on eval: -555.6101980257779[0m
[37m[1m[2023-07-17 05:13:32,835][257371] Mean Reward across all agents: -39.95012333195563[0m
[37m[1m[2023-07-17 05:13:32,835][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:13:32,839][257371] mean_value=-331.29782763701206, max_value=182.134814978519[0m
[37m[1m[2023-07-17 05:13:32,841][257371] New mean coefficients: [[ 0.6578375   0.3701378  -0.56804425 -1.8045334  -1.0842103   7.9257264 ]][0m
[37m[1m[2023-07-17 05:13:32,842][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:13:41,888][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 05:13:41,889][257371] FPS: 424573.78[0m
[36m[2023-07-17 05:13:41,891][257371] itr=690, itrs=2000, Progress: 34.50%[0m
[37m[1m[2023-07-17 05:16:51,489][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000670[0m
[36m[2023-07-17 05:17:03,740][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 05:17:03,741][257371] FPS: 332556.84[0m
[36m[2023-07-17 05:17:07,999][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:17:08,000][257371] Reward + Measures: [[22.18127953  0.46631932  0.90889531  0.61924398  0.62358302  7.62554502]][0m
[37m[1m[2023-07-17 05:17:08,000][257371] Max Reward on eval: 22.18127952514317[0m
[37m[1m[2023-07-17 05:17:08,000][257371] Min Reward on eval: 22.18127952514317[0m
[37m[1m[2023-07-17 05:17:08,000][257371] Mean Reward across all agents: 22.18127952514317[0m
[37m[1m[2023-07-17 05:17:08,001][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:17:12,994][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:17:12,995][257371] Reward + Measures: [[ 123.89568953    0.23410001    0.84200001    0.54440004    0.71340001
     5.57478714]
 [  47.91428649    0.3145        0.57560003    0.14860001    0.4752
     5.06432056]
 [-103.32546569    0.38800001    0.43790004    0.21429999    0.31870002
     4.17917585]
 ...
 [  94.95641236    0.38270003    0.97010005    0.54860002    0.78780001
     6.9969902 ]
 [   9.00995503    0.47470003    0.58350003    0.2757        0.37709999
     4.74800301]
 [ 165.43671368    0.2879        0.9788        0.65859997    0.78740007
     7.28531408]][0m
[37m[1m[2023-07-17 05:17:12,995][257371] Max Reward on eval: 332.4026007647626[0m
[37m[1m[2023-07-17 05:17:12,995][257371] Min Reward on eval: -342.379686835967[0m
[37m[1m[2023-07-17 05:17:12,996][257371] Mean Reward across all agents: 6.567485071881694[0m
[37m[1m[2023-07-17 05:17:12,996][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:17:12,999][257371] mean_value=-317.47698384487455, max_value=168.87788428236385[0m
[37m[1m[2023-07-17 05:17:13,002][257371] New mean coefficients: [[ 1.3636603   1.0099643  -0.00191778 -1.8933233  -0.6422087   7.704848  ]][0m
[37m[1m[2023-07-17 05:17:13,003][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:17:21,992][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 05:17:21,992][257371] FPS: 427255.12[0m
[36m[2023-07-17 05:17:21,994][257371] itr=691, itrs=2000, Progress: 34.55%[0m
[36m[2023-07-17 05:17:33,705][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 05:17:33,705][257371] FPS: 330319.06[0m
[36m[2023-07-17 05:17:38,025][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:17:38,026][257371] Reward + Measures: [[14.55565793  0.48819804  0.90685463  0.64831567  0.57695562  7.60695171]][0m
[37m[1m[2023-07-17 05:17:38,026][257371] Max Reward on eval: 14.555657934907746[0m
[37m[1m[2023-07-17 05:17:38,026][257371] Min Reward on eval: 14.555657934907746[0m
[37m[1m[2023-07-17 05:17:38,026][257371] Mean Reward across all agents: 14.555657934907746[0m
[37m[1m[2023-07-17 05:17:38,027][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:17:42,999][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:17:42,999][257371] Reward + Measures: [[ 44.47093505   0.39499998   0.95739996   0.39410001   0.87309998
    5.26685429]
 [ 27.51887799   0.37010002   0.97570002   0.49419999   0.88700002
    6.61592436]
 [  4.56506206   0.86640006   0.86300004   0.85999995   0.0423
    6.79609013]
 ...
 [115.81553409   0.50200003   0.98279995   0.5061       0.80009997
    7.10187674]
 [-82.51029683   0.4763       0.98310006   0.35160002   0.9842
    5.98579788]
 [-10.36861783   0.84330004   0.80879992   0.77679998   0.2782
    4.95843267]][0m
[37m[1m[2023-07-17 05:17:42,999][257371] Max Reward on eval: 501.3856811819598[0m
[37m[1m[2023-07-17 05:17:43,000][257371] Min Reward on eval: -475.4069480802864[0m
[37m[1m[2023-07-17 05:17:43,000][257371] Mean Reward across all agents: -28.830334924679153[0m
[37m[1m[2023-07-17 05:17:43,000][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:17:43,007][257371] mean_value=-291.8189210684763, max_value=438.5115844516621[0m
[37m[1m[2023-07-17 05:17:43,010][257371] New mean coefficients: [[ 1.9055793  1.1076581  0.2573082 -1.7635667 -0.4263674  7.5601664]][0m
[37m[1m[2023-07-17 05:17:43,011][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:17:51,962][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 05:17:51,963][257371] FPS: 429039.97[0m
[36m[2023-07-17 05:17:51,965][257371] itr=692, itrs=2000, Progress: 34.60%[0m
[36m[2023-07-17 05:18:03,565][257371] train() took 11.51 seconds to complete[0m
[36m[2023-07-17 05:18:03,565][257371] FPS: 333668.95[0m
[36m[2023-07-17 05:18:08,091][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:18:08,091][257371] Reward + Measures: [[18.26203534  0.49310863  0.90847492  0.66553766  0.55912334  7.62172365]][0m
[37m[1m[2023-07-17 05:18:08,091][257371] Max Reward on eval: 18.262035340322065[0m
[37m[1m[2023-07-17 05:18:08,091][257371] Min Reward on eval: 18.262035340322065[0m
[37m[1m[2023-07-17 05:18:08,092][257371] Mean Reward across all agents: 18.262035340322065[0m
[37m[1m[2023-07-17 05:18:08,092][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:18:13,088][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:18:13,089][257371] Reward + Measures: [[ 170.23220002    0.76010001    0.9325        0.14739999    0.91959995
     4.8885355 ]
 [  14.40949818    0.48629999    0.75300002    0.65220004    0.31829998
     5.58244133]
 [-200.96912384    0.45120001    0.6749        0.1772        0.616
     5.12673426]
 ...
 [  -8.96139112    0.26720002    0.2624        0.171         0.19999999
     3.82346725]
 [-246.12894242    0.86809999    0.86650002    0.78140002    0.1417
     6.59512949]
 [  19.62053518    0.61969995    0.73520005    0.63529998    0.30020002
     5.29306698]][0m
[37m[1m[2023-07-17 05:18:13,089][257371] Max Reward on eval: 512.1652870748192[0m
[37m[1m[2023-07-17 05:18:13,089][257371] Min Reward on eval: -382.10794446468356[0m
[37m[1m[2023-07-17 05:18:13,089][257371] Mean Reward across all agents: 15.820497346138746[0m
[37m[1m[2023-07-17 05:18:13,090][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:18:13,093][257371] mean_value=-652.7941473777761, max_value=307.9823949842763[0m
[37m[1m[2023-07-17 05:18:13,095][257371] New mean coefficients: [[ 1.0879456   0.48437774  0.6553098  -1.6641055  -1.929718    7.0663557 ]][0m
[37m[1m[2023-07-17 05:18:13,096][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:18:22,010][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-17 05:18:22,010][257371] FPS: 430867.23[0m
[36m[2023-07-17 05:18:22,013][257371] itr=693, itrs=2000, Progress: 34.65%[0m
[36m[2023-07-17 05:18:34,017][257371] train() took 11.92 seconds to complete[0m
[36m[2023-07-17 05:18:34,018][257371] FPS: 322187.55[0m
[36m[2023-07-17 05:18:38,355][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:18:38,355][257371] Reward + Measures: [[4.73807055 0.50866497 0.91252959 0.66308969 0.55766827 7.62493086]][0m
[37m[1m[2023-07-17 05:18:38,355][257371] Max Reward on eval: 4.7380705548003705[0m
[37m[1m[2023-07-17 05:18:38,356][257371] Min Reward on eval: 4.7380705548003705[0m
[37m[1m[2023-07-17 05:18:38,356][257371] Mean Reward across all agents: 4.7380705548003705[0m
[37m[1m[2023-07-17 05:18:38,356][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:18:43,398][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:18:43,398][257371] Reward + Measures: [[-112.37682161    0.2586        0.3247        0.1191        0.30710003
     4.91763878]
 [ -71.05672701    0.37139997    0.70649999    0.62620002    0.40990001
     4.35581064]
 [  55.37870745    0.53490007    0.63370001    0.6512        0.17639999
     4.60450268]
 ...
 [-133.38168902    0.52100003    0.77800006    0.76509994    0.36020002
     4.34774303]
 [  42.76821509    0.2228        0.19140001    0.20779999    0.16759999
     3.66137576]
 [   5.93889143    0.16320001    0.34320003    0.252         0.35100001
     4.72101307]][0m
[37m[1m[2023-07-17 05:18:43,398][257371] Max Reward on eval: 448.7792778079398[0m
[37m[1m[2023-07-17 05:18:43,399][257371] Min Reward on eval: -306.6116975972429[0m
[37m[1m[2023-07-17 05:18:43,399][257371] Mean Reward across all agents: 26.04228531418464[0m
[37m[1m[2023-07-17 05:18:43,399][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:18:43,402][257371] mean_value=-921.7418394797525, max_value=548.0408592469885[0m
[37m[1m[2023-07-17 05:18:43,405][257371] New mean coefficients: [[ 0.29729968  0.07771641 -0.3235638  -1.780523   -1.7774806   7.7086267 ]][0m
[37m[1m[2023-07-17 05:18:43,405][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:18:52,498][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 05:18:52,498][257371] FPS: 422396.82[0m
[36m[2023-07-17 05:18:52,501][257371] itr=694, itrs=2000, Progress: 34.70%[0m
[36m[2023-07-17 05:19:04,325][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 05:19:04,326][257371] FPS: 327238.16[0m
[36m[2023-07-17 05:19:08,700][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:19:08,701][257371] Reward + Measures: [[20.57971805  0.47488534  0.91570801  0.64114767  0.60526699  7.61026096]][0m
[37m[1m[2023-07-17 05:19:08,701][257371] Max Reward on eval: 20.57971804951438[0m
[37m[1m[2023-07-17 05:19:08,701][257371] Min Reward on eval: 20.57971804951438[0m
[37m[1m[2023-07-17 05:19:08,702][257371] Mean Reward across all agents: 20.57971804951438[0m
[37m[1m[2023-07-17 05:19:08,702][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:19:13,750][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:19:13,756][257371] Reward + Measures: [[ -40.60481074    0.4779        0.97620004    0.39509997    0.8969
     7.1120224 ]
 [-110.22038556    0.64390004    0.97690004    0.27129999    0.89600003
     6.90230656]
 [-108.10908848    0.70660001    0.96179992    0.45559999    0.69090003
     7.08393049]
 ...
 [  21.15978455    0.40100002    0.84650004    0.50670004    0.62360001
     5.2571125 ]
 [  34.04139375    0.27200001    0.97080004    0.5399        0.88189995
     6.66276932]
 [ 200.88514419    0.42280003    0.97279996    0.36129999    0.98110002
     7.11084747]][0m
[37m[1m[2023-07-17 05:19:13,756][257371] Max Reward on eval: 422.96818924704564[0m
[37m[1m[2023-07-17 05:19:13,756][257371] Min Reward on eval: -452.2217254713411[0m
[37m[1m[2023-07-17 05:19:13,757][257371] Mean Reward across all agents: 60.0081197508087[0m
[37m[1m[2023-07-17 05:19:13,757][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:19:13,761][257371] mean_value=-211.78680185205792, max_value=489.518554679545[0m
[37m[1m[2023-07-17 05:19:13,764][257371] New mean coefficients: [[ 0.5859741   0.39236516 -0.16440903 -1.814185   -1.3822731   7.4793253 ]][0m
[37m[1m[2023-07-17 05:19:13,765][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:19:22,850][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 05:19:22,851][257371] FPS: 422720.84[0m
[36m[2023-07-17 05:19:22,853][257371] itr=695, itrs=2000, Progress: 34.75%[0m
[36m[2023-07-17 05:19:34,607][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 05:19:34,608][257371] FPS: 329177.78[0m
[36m[2023-07-17 05:19:38,854][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:19:38,855][257371] Reward + Measures: [[3.6014428  0.5189873  0.9125337  0.68329197 0.530599   7.62575006]][0m
[37m[1m[2023-07-17 05:19:38,855][257371] Max Reward on eval: 3.601442801272584[0m
[37m[1m[2023-07-17 05:19:38,855][257371] Min Reward on eval: 3.601442801272584[0m
[37m[1m[2023-07-17 05:19:38,855][257371] Mean Reward across all agents: 3.601442801272584[0m
[37m[1m[2023-07-17 05:19:38,856][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:19:43,896][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:19:43,897][257371] Reward + Measures: [[-222.56299686    0.89190006    0.9691        0.0356        0.95109999
     5.37602615]
 [ 262.46753979    0.10910001    0.94020003    0.52830005    0.91609997
     5.1315608 ]
 [  17.80524941    0.46800002    0.40019998    0.3831        0.26209998
     3.0894587 ]
 ...
 [  15.08899156    0.88599998    0.94729996    0.72670001    0.26570004
     7.46217299]
 [-141.28106665    0.44780001    0.97410005    0.4113        0.88689995
     5.39683294]
 [-292.81841089    0.82609999    0.88749999    0.15640001    0.75269997
     5.50084496]][0m
[37m[1m[2023-07-17 05:19:43,897][257371] Max Reward on eval: 471.20275879846884[0m
[37m[1m[2023-07-17 05:19:43,898][257371] Min Reward on eval: -668.7894363068044[0m
[37m[1m[2023-07-17 05:19:43,898][257371] Mean Reward across all agents: 9.760740756481788[0m
[37m[1m[2023-07-17 05:19:43,898][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:19:43,903][257371] mean_value=-287.9760577783458, max_value=280.46465013963666[0m
[37m[1m[2023-07-17 05:19:43,906][257371] New mean coefficients: [[ 1.0611914  -0.11811    -0.13882005 -1.562032   -2.8944025   7.936727  ]][0m
[37m[1m[2023-07-17 05:19:43,907][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:19:52,992][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 05:19:52,993][257371] FPS: 422720.43[0m
[36m[2023-07-17 05:19:52,995][257371] itr=696, itrs=2000, Progress: 34.80%[0m
[36m[2023-07-17 05:20:04,789][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 05:20:04,789][257371] FPS: 328083.68[0m
[36m[2023-07-17 05:20:09,165][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:20:09,165][257371] Reward + Measures: [[-20.09289593   0.58201128   0.90112036   0.69740707   0.45774865
    7.59310913]][0m
[37m[1m[2023-07-17 05:20:09,166][257371] Max Reward on eval: -20.092895927916203[0m
[37m[1m[2023-07-17 05:20:09,166][257371] Min Reward on eval: -20.092895927916203[0m
[37m[1m[2023-07-17 05:20:09,166][257371] Mean Reward across all agents: -20.092895927916203[0m
[37m[1m[2023-07-17 05:20:09,167][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:20:14,196][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:20:14,197][257371] Reward + Measures: [[ -74.8326679     0.64969999    0.52450001    0.5589        0.32399997
     4.56691074]
 [-114.64892502    0.815         0.83740008    0.77599996    0.15680002
     4.63666487]
 [-224.44143968    0.85830003    0.9465        0.80760002    0.20720001
     6.36984253]
 ...
 [ -87.26333566    0.39200002    0.69110006    0.23320003    0.68300003
     5.34303999]
 [-334.35949754    0.83700007    0.76270002    0.80419999    0.06370001
     5.39679337]
 [ -17.41639044    0.70600003    0.88000005    0.80919999    0.1912
     6.67779493]][0m
[37m[1m[2023-07-17 05:20:14,197][257371] Max Reward on eval: 409.18130303462965[0m
[37m[1m[2023-07-17 05:20:14,198][257371] Min Reward on eval: -477.954193114955[0m
[37m[1m[2023-07-17 05:20:14,198][257371] Mean Reward across all agents: -42.5970287715302[0m
[37m[1m[2023-07-17 05:20:14,198][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:20:14,200][257371] mean_value=-865.4252781996631, max_value=239.16637064356465[0m
[37m[1m[2023-07-17 05:20:14,202][257371] New mean coefficients: [[ 0.13981605 -0.6055922   0.18696412 -1.6053271  -2.8874378   6.948995  ]][0m
[37m[1m[2023-07-17 05:20:14,203][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:20:23,274][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 05:20:23,274][257371] FPS: 423438.87[0m
[36m[2023-07-17 05:20:23,276][257371] itr=697, itrs=2000, Progress: 34.85%[0m
[36m[2023-07-17 05:20:35,084][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 05:20:35,085][257371] FPS: 327612.51[0m
[36m[2023-07-17 05:20:39,392][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:20:39,393][257371] Reward + Measures: [[-19.48475342   0.57643032   0.89674771   0.6930353    0.46084732
    7.5943346 ]][0m
[37m[1m[2023-07-17 05:20:39,393][257371] Max Reward on eval: -19.484753419795073[0m
[37m[1m[2023-07-17 05:20:39,393][257371] Min Reward on eval: -19.484753419795073[0m
[37m[1m[2023-07-17 05:20:39,393][257371] Mean Reward across all agents: -19.484753419795073[0m
[37m[1m[2023-07-17 05:20:39,394][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:20:44,636][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:20:44,637][257371] Reward + Measures: [[ -61.63340404    0.44480005    0.45070001    0.3434        0.17480001
     5.86188126]
 [-250.21371298    0.0378        0.95609999    0.84710008    0.95590001
     5.50867796]
 [  41.64728622    0.66930002    0.64679998    0.40879998    0.34640002
     6.31402826]
 ...
 [ -38.91212464    0.38789999    0.41880003    0.3554        0.33379999
     4.18584919]
 [ 161.26314763    0.33689997    0.80290002    0.60879999    0.53360003
     5.98786545]
 [  83.36425397    0.3723        0.96569997    0.57460004    0.76870006
     6.47667456]][0m
[37m[1m[2023-07-17 05:20:44,637][257371] Max Reward on eval: 291.026958424598[0m
[37m[1m[2023-07-17 05:20:44,637][257371] Min Reward on eval: -568.2478656738997[0m
[37m[1m[2023-07-17 05:20:44,638][257371] Mean Reward across all agents: -19.576589273714625[0m
[37m[1m[2023-07-17 05:20:44,638][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:20:44,640][257371] mean_value=-382.0149287272561, max_value=313.08722756806753[0m
[37m[1m[2023-07-17 05:20:44,643][257371] New mean coefficients: [[-0.44935137 -0.79975176  0.10114756 -1.4307792  -3.2893045   7.0210276 ]][0m
[37m[1m[2023-07-17 05:20:44,644][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:20:53,662][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 05:20:53,662][257371] FPS: 425893.08[0m
[36m[2023-07-17 05:20:53,665][257371] itr=698, itrs=2000, Progress: 34.90%[0m
[36m[2023-07-17 05:21:05,381][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 05:21:05,381][257371] FPS: 330178.53[0m
[36m[2023-07-17 05:21:09,756][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:21:09,757][257371] Reward + Measures: [[-28.37500781   0.57730436   0.8808527    0.70312208   0.43102962
    7.58708048]][0m
[37m[1m[2023-07-17 05:21:09,757][257371] Max Reward on eval: -28.375007806887773[0m
[37m[1m[2023-07-17 05:21:09,757][257371] Min Reward on eval: -28.375007806887773[0m
[37m[1m[2023-07-17 05:21:09,757][257371] Mean Reward across all agents: -28.375007806887773[0m
[37m[1m[2023-07-17 05:21:09,758][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:21:14,791][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:21:14,792][257371] Reward + Measures: [[339.33506584   0.1059       0.96340007   0.74130005   0.89139998
    3.8430779 ]
 [323.24857238   0.035        0.96950006   0.61680001   0.97410005
    4.43346596]
 [-84.6234572    0.43779999   0.57120001   0.1019       0.57539999
    5.62178564]
 ...
 [-33.75897688   0.2458       0.62519997   0.21930002   0.63920003
    4.17200375]
 [-37.32095423   0.2227       0.96870005   0.54399997   0.95340008
    5.43961811]
 [  3.29251889   0.15409999   0.30669999   0.18430001   0.2631
    4.33545923]][0m
[37m[1m[2023-07-17 05:21:14,792][257371] Max Reward on eval: 339.43154350621626[0m
[37m[1m[2023-07-17 05:21:14,792][257371] Min Reward on eval: -577.3769111877307[0m
[37m[1m[2023-07-17 05:21:14,792][257371] Mean Reward across all agents: -2.8898131822406246[0m
[37m[1m[2023-07-17 05:21:14,793][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:21:14,797][257371] mean_value=-485.73300602351975, max_value=476.8334692845557[0m
[37m[1m[2023-07-17 05:21:14,800][257371] New mean coefficients: [[-0.17319459 -0.31260827  0.7695054  -1.4340085  -3.1351237   6.6316338 ]][0m
[37m[1m[2023-07-17 05:21:14,801][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:21:23,898][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 05:21:23,898][257371] FPS: 422171.77[0m
[36m[2023-07-17 05:21:23,901][257371] itr=699, itrs=2000, Progress: 34.95%[0m
[36m[2023-07-17 05:21:35,738][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 05:21:35,738][257371] FPS: 326933.09[0m
[36m[2023-07-17 05:21:39,992][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:21:39,993][257371] Reward + Measures: [[-41.05426698   0.59202403   0.89218831   0.70495671   0.43325397
    7.58888292]][0m
[37m[1m[2023-07-17 05:21:39,993][257371] Max Reward on eval: -41.05426698037425[0m
[37m[1m[2023-07-17 05:21:39,993][257371] Min Reward on eval: -41.05426698037425[0m
[37m[1m[2023-07-17 05:21:39,993][257371] Mean Reward across all agents: -41.05426698037425[0m
[37m[1m[2023-07-17 05:21:39,994][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:21:45,022][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:21:45,022][257371] Reward + Measures: [[ 45.11345952   0.51319999   0.52640003   0.31659999   0.24530004
    6.03698969]
 [-40.42735593   0.2237       0.3529       0.18629999   0.32410002
    4.43330574]
 [ 14.72101089   0.6476       0.71669996   0.1446       0.63090003
    6.0909934 ]
 ...
 [-63.87394807   0.74849999   0.95409995   0.66829997   0.40480003
    5.95913363]
 [-58.20548594   0.40990001   0.43449998   0.1115       0.41999999
    4.87629557]
 [ -0.8805116    0.40970001   0.5169       0.2323       0.41430002
    4.29380608]][0m
[37m[1m[2023-07-17 05:21:45,022][257371] Max Reward on eval: 224.06354262726381[0m
[37m[1m[2023-07-17 05:21:45,023][257371] Min Reward on eval: -589.2083568345755[0m
[37m[1m[2023-07-17 05:21:45,023][257371] Mean Reward across all agents: -53.52924503715862[0m
[37m[1m[2023-07-17 05:21:45,023][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:21:45,025][257371] mean_value=-458.89802901767445, max_value=126.3928865406371[0m
[37m[1m[2023-07-17 05:21:45,028][257371] New mean coefficients: [[ 0.0744378  -0.08832879  1.0792434  -1.8539596  -2.7099776   5.8382187 ]][0m
[37m[1m[2023-07-17 05:21:45,029][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:21:54,104][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 05:21:54,105][257371] FPS: 423192.56[0m
[36m[2023-07-17 05:21:54,107][257371] itr=700, itrs=2000, Progress: 35.00%[0m
[37m[1m[2023-07-17 05:25:07,062][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000680[0m
[36m[2023-07-17 05:25:19,321][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 05:25:19,321][257371] FPS: 327615.44[0m
[36m[2023-07-17 05:25:23,610][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:25:23,611][257371] Reward + Measures: [[-38.03015585   0.5610413    0.89251333   0.69249839   0.46805534
    7.5979991 ]][0m
[37m[1m[2023-07-17 05:25:23,611][257371] Max Reward on eval: -38.030155849727684[0m
[37m[1m[2023-07-17 05:25:23,611][257371] Min Reward on eval: -38.030155849727684[0m
[37m[1m[2023-07-17 05:25:23,612][257371] Mean Reward across all agents: -38.030155849727684[0m
[37m[1m[2023-07-17 05:25:23,612][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:25:28,587][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:25:28,593][257371] Reward + Measures: [[-68.78424411   0.55299997   0.56240004   0.3348       0.46409997
    3.53730845]
 [206.70666789   0.60120004   0.95020002   0.65900004   0.50910008
    6.08785963]
 [303.43020489   0.69499999   0.76340002   0.74370009   0.16909999
    4.76519918]
 ...
 [-98.29888534   0.36519998   0.92930001   0.70030004   0.60260004
    5.2118926 ]
 [  9.55814525   0.40650001   0.56040001   0.22919999   0.54210001
    3.04308844]
 [-93.7865517    0.36350003   0.39309999   0.2915       0.27880001
    4.61893272]][0m
[37m[1m[2023-07-17 05:25:28,593][257371] Max Reward on eval: 454.5620078418404[0m
[37m[1m[2023-07-17 05:25:28,594][257371] Min Reward on eval: -429.89801597334446[0m
[37m[1m[2023-07-17 05:25:28,594][257371] Mean Reward across all agents: 56.88450430452044[0m
[37m[1m[2023-07-17 05:25:28,594][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:25:28,600][257371] mean_value=-216.69000363550447, max_value=272.82580542544474[0m
[37m[1m[2023-07-17 05:25:28,603][257371] New mean coefficients: [[-0.3035667  -0.46983087  1.034126   -1.5179921  -3.7014835   6.256136  ]][0m
[37m[1m[2023-07-17 05:25:28,604][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:25:37,716][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 05:25:37,717][257371] FPS: 421471.66[0m
[36m[2023-07-17 05:25:37,719][257371] itr=701, itrs=2000, Progress: 35.05%[0m
[36m[2023-07-17 05:25:49,450][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 05:25:49,450][257371] FPS: 330734.02[0m
[36m[2023-07-17 05:25:53,682][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:25:53,683][257371] Reward + Measures: [[-31.78057247   0.57135832   0.891761     0.70798838   0.44273201
    7.60215616]][0m
[37m[1m[2023-07-17 05:25:53,683][257371] Max Reward on eval: -31.780572473482152[0m
[37m[1m[2023-07-17 05:25:53,683][257371] Min Reward on eval: -31.780572473482152[0m
[37m[1m[2023-07-17 05:25:53,684][257371] Mean Reward across all agents: -31.780572473482152[0m
[37m[1m[2023-07-17 05:25:53,684][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:25:58,605][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:25:58,606][257371] Reward + Measures: [[-104.45741541    0.29969999    0.59149998    0.31200001    0.48580003
     6.50965738]
 [ -58.03188728    0.21790002    0.3935        0.28550002    0.2441
     6.21721554]
 [-121.87085918    0.67410004    0.67510003    0.49040005    0.2174
     6.39779186]
 ...
 [ 354.03140642    0.27160001    0.96789992    0.61070007    0.89820004
     5.68269062]
 [ 200.00231813    0.23480001    0.6189        0.5007        0.43249997
     7.16070271]
 [-187.9764481     0.54680008    0.79020005    0.45880005    0.51719999
     7.57001972]][0m
[37m[1m[2023-07-17 05:25:58,606][257371] Max Reward on eval: 429.00297603718934[0m
[37m[1m[2023-07-17 05:25:58,606][257371] Min Reward on eval: -582.2724819408729[0m
[37m[1m[2023-07-17 05:25:58,607][257371] Mean Reward across all agents: -51.57093979568485[0m
[37m[1m[2023-07-17 05:25:58,607][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:25:58,612][257371] mean_value=-308.46029478281037, max_value=340.0746877495763[0m
[37m[1m[2023-07-17 05:25:58,615][257371] New mean coefficients: [[-0.3650108  -0.7663966   1.5634916  -0.84350723 -4.526712    6.498802  ]][0m
[37m[1m[2023-07-17 05:25:58,615][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:26:07,615][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 05:26:07,616][257371] FPS: 426749.20[0m
[36m[2023-07-17 05:26:07,618][257371] itr=702, itrs=2000, Progress: 35.10%[0m
[36m[2023-07-17 05:26:19,188][257371] train() took 11.48 seconds to complete[0m
[36m[2023-07-17 05:26:19,188][257371] FPS: 334367.96[0m
[36m[2023-07-17 05:26:23,430][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:26:23,430][257371] Reward + Measures: [[-50.14183656   0.542593     0.90899765   0.67690003   0.52104199
    7.64145374]][0m
[37m[1m[2023-07-17 05:26:23,430][257371] Max Reward on eval: -50.14183656194589[0m
[37m[1m[2023-07-17 05:26:23,430][257371] Min Reward on eval: -50.14183656194589[0m
[37m[1m[2023-07-17 05:26:23,431][257371] Mean Reward across all agents: -50.14183656194589[0m
[37m[1m[2023-07-17 05:26:23,431][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:26:28,417][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:26:28,418][257371] Reward + Measures: [[524.84883402   0.84629995   0.82980007   0.81540006   0.0281
    5.38854551]
 [-30.29823772   0.0993       0.10029999   0.10290001   0.0743
    4.49050617]
 [173.40070167   0.51660001   0.51430005   0.50209999   0.0621
    4.73088408]
 ...
 [ 43.62453645   0.65640002   0.56090003   0.56160003   0.38130003
    5.18924952]
 [-12.70669607   0.09470001   0.09640001   0.08540001   0.0763
    4.26665878]
 [-24.15125829   0.39140001   0.38850001   0.35419998   0.29210001
    4.59291983]][0m
[37m[1m[2023-07-17 05:26:28,418][257371] Max Reward on eval: 524.8488340156152[0m
[37m[1m[2023-07-17 05:26:28,418][257371] Min Reward on eval: -481.5005683822557[0m
[37m[1m[2023-07-17 05:26:28,418][257371] Mean Reward across all agents: 0.7429483888343096[0m
[37m[1m[2023-07-17 05:26:28,418][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:26:28,420][257371] mean_value=-1093.2897708136475, max_value=285.6699239856976[0m
[37m[1m[2023-07-17 05:26:28,423][257371] New mean coefficients: [[-0.40965068 -0.3439269   1.238708   -1.0951316  -3.2197053   6.8457284 ]][0m
[37m[1m[2023-07-17 05:26:28,424][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:26:37,405][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 05:26:37,405][257371] FPS: 427654.21[0m
[36m[2023-07-17 05:26:37,407][257371] itr=703, itrs=2000, Progress: 35.15%[0m
[36m[2023-07-17 05:26:49,133][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 05:26:49,133][257371] FPS: 329872.49[0m
[36m[2023-07-17 05:26:53,470][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:26:53,470][257371] Reward + Measures: [[-49.7704385    0.536084     0.89954036   0.6722393    0.51699936
    7.63108206]][0m
[37m[1m[2023-07-17 05:26:53,470][257371] Max Reward on eval: -49.770438495408676[0m
[37m[1m[2023-07-17 05:26:53,471][257371] Min Reward on eval: -49.770438495408676[0m
[37m[1m[2023-07-17 05:26:53,471][257371] Mean Reward across all agents: -49.770438495408676[0m
[37m[1m[2023-07-17 05:26:53,471][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:26:58,503][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:26:58,509][257371] Reward + Measures: [[ -23.76195776    0.6146        0.61430001    0.5316        0.61039996
     5.25028229]
 [ 199.71838852    0.20650001    0.67609996    0.32190001    0.62659997
     5.39304829]
 [ 139.0841678     0.61860001    0.67550004    0.5333001     0.53140002
     5.62670851]
 ...
 [-188.39458379    0.73940003    0.85890007    0.0722        0.80310005
     5.47356796]
 [-192.88893401    0.875         0.86700004    0.7579        0.1164
     6.91071701]
 [-112.53351234    0.54479998    0.57100004    0.50840002    0.54820007
     5.68848372]][0m
[37m[1m[2023-07-17 05:26:58,509][257371] Max Reward on eval: 636.5517005981877[0m
[37m[1m[2023-07-17 05:26:58,509][257371] Min Reward on eval: -492.8714842699468[0m
[37m[1m[2023-07-17 05:26:58,510][257371] Mean Reward across all agents: -0.3006924011293847[0m
[37m[1m[2023-07-17 05:26:58,510][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:26:58,514][257371] mean_value=-242.49033603658944, max_value=467.64528629608054[0m
[37m[1m[2023-07-17 05:26:58,517][257371] New mean coefficients: [[-0.4729192  -0.30930555  1.9411521  -0.66363364 -3.593928    6.8142962 ]][0m
[37m[1m[2023-07-17 05:26:58,518][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:27:07,580][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 05:27:07,580][257371] FPS: 423829.42[0m
[36m[2023-07-17 05:27:07,582][257371] itr=704, itrs=2000, Progress: 35.20%[0m
[36m[2023-07-17 05:27:19,415][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 05:27:19,415][257371] FPS: 326966.23[0m
[36m[2023-07-17 05:27:23,765][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:27:23,766][257371] Reward + Measures: [[-50.17463532   0.56373698   0.90034401   0.69209033   0.47820532
    7.63101435]][0m
[37m[1m[2023-07-17 05:27:23,766][257371] Max Reward on eval: -50.17463531565495[0m
[37m[1m[2023-07-17 05:27:23,766][257371] Min Reward on eval: -50.17463531565495[0m
[37m[1m[2023-07-17 05:27:23,766][257371] Mean Reward across all agents: -50.17463531565495[0m
[37m[1m[2023-07-17 05:27:23,767][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:27:29,024][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:27:29,024][257371] Reward + Measures: [[318.40204144   0.46230003   0.94379997   0.67670006   0.5636
    5.67240238]
 [181.46853829   0.62580001   0.94729996   0.78260005   0.39770001
    6.05465174]
 [314.42086505   0.84790003   0.84829998   0.84069997   0.0638
    5.31493711]
 ...
 [108.29893016   0.80979997   0.91289997   0.58069998   0.8524
    5.84556484]
 [261.7598634    0.46100003   0.85480005   0.62970001   0.53610003
    6.07205629]
 [ 91.58710341   0.64469999   0.83939999   0.6347       0.37729999
    5.61487722]][0m
[37m[1m[2023-07-17 05:27:29,025][257371] Max Reward on eval: 687.5013122566045[0m
[37m[1m[2023-07-17 05:27:29,025][257371] Min Reward on eval: -364.85479165855793[0m
[37m[1m[2023-07-17 05:27:29,025][257371] Mean Reward across all agents: 152.74956083561176[0m
[37m[1m[2023-07-17 05:27:29,025][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:27:29,033][257371] mean_value=-101.66248009076635, max_value=689.1887726714514[0m
[37m[1m[2023-07-17 05:27:29,036][257371] New mean coefficients: [[-0.94569033  0.4445101   2.0428772  -0.8051092  -4.0318093   6.395034  ]][0m
[37m[1m[2023-07-17 05:27:29,037][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:27:38,097][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 05:27:38,102][257371] FPS: 423923.54[0m
[36m[2023-07-17 05:27:38,110][257371] itr=705, itrs=2000, Progress: 35.25%[0m
[36m[2023-07-17 05:27:49,970][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 05:27:49,970][257371] FPS: 326272.92[0m
[36m[2023-07-17 05:27:54,222][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:27:54,222][257371] Reward + Measures: [[-64.0727284    0.60569233   0.89675438   0.70446062   0.43024132
    7.61940241]][0m
[37m[1m[2023-07-17 05:27:54,223][257371] Max Reward on eval: -64.07272839977153[0m
[37m[1m[2023-07-17 05:27:54,223][257371] Min Reward on eval: -64.07272839977153[0m
[37m[1m[2023-07-17 05:27:54,223][257371] Mean Reward across all agents: -64.07272839977153[0m
[37m[1m[2023-07-17 05:27:54,223][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:27:59,277][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:27:59,283][257371] Reward + Measures: [[-27.4984317    0.30329999   0.40310001   0.17460001   0.43720004
    4.59482574]
 [ 40.87810778   0.42389998   0.4052       0.44320002   0.377
    4.01639891]
 [-90.04470583   0.36060002   0.58859998   0.33570001   0.45749998
    6.74964142]
 ...
 [ 55.68361686   0.2436       0.33290002   0.29530001   0.29950002
    4.26301336]
 [117.8964271    0.30310002   0.86980003   0.46250001   0.75389999
    5.71384335]
 [ 55.57979069   0.36380002   0.21180001   0.33290002   0.21080001
    4.24393034]][0m
[37m[1m[2023-07-17 05:27:59,283][257371] Max Reward on eval: 489.26059941388667[0m
[37m[1m[2023-07-17 05:27:59,283][257371] Min Reward on eval: -455.6521291796118[0m
[37m[1m[2023-07-17 05:27:59,284][257371] Mean Reward across all agents: 8.21647022227328[0m
[37m[1m[2023-07-17 05:27:59,284][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:27:59,286][257371] mean_value=-336.0661065511398, max_value=455.49116797149463[0m
[37m[1m[2023-07-17 05:27:59,289][257371] New mean coefficients: [[-1.0880811   0.08575398  1.1862463  -0.5694014  -3.869668    7.213529  ]][0m
[37m[1m[2023-07-17 05:27:59,290][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:28:08,215][257371] train() took 8.92 seconds to complete[0m
[36m[2023-07-17 05:28:08,215][257371] FPS: 430333.04[0m
[36m[2023-07-17 05:28:08,218][257371] itr=706, itrs=2000, Progress: 35.30%[0m
[36m[2023-07-17 05:28:19,964][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 05:28:19,964][257371] FPS: 329331.49[0m
[36m[2023-07-17 05:28:24,181][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:28:24,182][257371] Reward + Measures: [[-71.59912398   0.59547436   0.88934571   0.69954628   0.43086833
    7.61717939]][0m
[37m[1m[2023-07-17 05:28:24,182][257371] Max Reward on eval: -71.5991239762035[0m
[37m[1m[2023-07-17 05:28:24,182][257371] Min Reward on eval: -71.5991239762035[0m
[37m[1m[2023-07-17 05:28:24,183][257371] Mean Reward across all agents: -71.5991239762035[0m
[37m[1m[2023-07-17 05:28:24,183][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:28:29,126][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:28:29,127][257371] Reward + Measures: [[ 16.181438     0.46670005   0.54809999   0.47659999   0.37900001
    3.38333964]
 [227.50988213   0.32280001   0.60410005   0.62130004   0.41700003
    4.73113489]
 [133.40259777   0.69190001   0.76980001   0.52210003   0.4048
    4.52621698]
 ...
 [ 36.79080393   0.40419999   0.57590002   0.44970003   0.38850001
    3.80850577]
 [ 10.6373188    0.3567       0.36829999   0.37210003   0.25800002
    3.64594197]
 [-38.73667354   0.6983       0.69999999   0.40369996   0.3274
    5.19716454]][0m
[37m[1m[2023-07-17 05:28:29,127][257371] Max Reward on eval: 348.58511063605545[0m
[37m[1m[2023-07-17 05:28:29,128][257371] Min Reward on eval: -441.1923866339028[0m
[37m[1m[2023-07-17 05:28:29,128][257371] Mean Reward across all agents: -13.778891041716525[0m
[37m[1m[2023-07-17 05:28:29,128][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:28:29,131][257371] mean_value=-682.6399359627113, max_value=314.3690913417608[0m
[37m[1m[2023-07-17 05:28:29,133][257371] New mean coefficients: [[-1.3262506  -0.3427757   0.5577659  -0.21417764 -3.7208827   7.862884  ]][0m
[37m[1m[2023-07-17 05:28:29,134][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:28:38,149][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 05:28:38,149][257371] FPS: 426032.96[0m
[36m[2023-07-17 05:28:38,152][257371] itr=707, itrs=2000, Progress: 35.35%[0m
[36m[2023-07-17 05:28:49,980][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 05:28:49,980][257371] FPS: 327105.40[0m
[36m[2023-07-17 05:28:54,323][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:28:54,323][257371] Reward + Measures: [[-60.75969054   0.52779102   0.90930867   0.65739202   0.54870534
    7.64196873]][0m
[37m[1m[2023-07-17 05:28:54,324][257371] Max Reward on eval: -60.75969054009145[0m
[37m[1m[2023-07-17 05:28:54,324][257371] Min Reward on eval: -60.75969054009145[0m
[37m[1m[2023-07-17 05:28:54,324][257371] Mean Reward across all agents: -60.75969054009145[0m
[37m[1m[2023-07-17 05:28:54,324][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:28:59,398][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:28:59,399][257371] Reward + Measures: [[-212.98818969    0.74169999    0.84119999    0.78580004    0.16329999
     6.84301138]
 [  89.94767594    0.40200004    0.5783        0.32430002    0.51090002
     4.91849852]
 [-351.37649872    0.86009997    0.96779996    0.43930003    0.60400003
     6.98477316]
 ...
 [  80.16856297    0.3055        0.87580007    0.59710002    0.80509996
     6.49055958]
 [ 111.19446536    0.46700001    0.7335        0.5693        0.3558
     5.79438066]
 [  51.49395909    0.48090002    0.56720001    0.32780004    0.38830003
     5.10905123]][0m
[37m[1m[2023-07-17 05:28:59,399][257371] Max Reward on eval: 466.68737555723635[0m
[37m[1m[2023-07-17 05:28:59,399][257371] Min Reward on eval: -797.8252563508228[0m
[37m[1m[2023-07-17 05:28:59,399][257371] Mean Reward across all agents: -128.1391762681938[0m
[37m[1m[2023-07-17 05:28:59,400][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:28:59,402][257371] mean_value=-665.5062771486129, max_value=160.6953361109915[0m
[37m[1m[2023-07-17 05:28:59,405][257371] New mean coefficients: [[-0.69019985 -0.08950409  0.44521126  0.36193666 -3.0781617   8.728064  ]][0m
[37m[1m[2023-07-17 05:28:59,406][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:29:08,467][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 05:29:08,468][257371] FPS: 423847.97[0m
[36m[2023-07-17 05:29:08,470][257371] itr=708, itrs=2000, Progress: 35.40%[0m
[36m[2023-07-17 05:29:20,339][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 05:29:20,340][257371] FPS: 326017.15[0m
[36m[2023-07-17 05:29:24,644][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:29:24,644][257371] Reward + Measures: [[-67.0462676    0.57471967   0.89523959   0.68768799   0.46559232
    7.63305187]][0m
[37m[1m[2023-07-17 05:29:24,645][257371] Max Reward on eval: -67.04626759687928[0m
[37m[1m[2023-07-17 05:29:24,645][257371] Min Reward on eval: -67.04626759687928[0m
[37m[1m[2023-07-17 05:29:24,645][257371] Mean Reward across all agents: -67.04626759687928[0m
[37m[1m[2023-07-17 05:29:24,645][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:29:29,751][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:29:29,756][257371] Reward + Measures: [[ -47.0549285     0.2987        0.39990002    0.37399998    0.20440002
     4.64296722]
 [  18.86997212    0.57919997    0.73250002    0.46689996    0.39020002
     5.458848  ]
 [ 388.69192363    0.1           0.97550005    0.57250005    0.98219997
     6.01535177]
 ...
 [-140.93507663    0.72139996    0.81050009    0.2165        0.62659997
     4.91942739]
 [ 133.22205002    0.72120011    0.85100001    0.75019997    0.22119999
     6.5220933 ]
 [-171.71231531    0.76289999    0.77220005    0.1401        0.69670004
     5.27831411]][0m
[37m[1m[2023-07-17 05:29:29,757][257371] Max Reward on eval: 615.3057853618636[0m
[37m[1m[2023-07-17 05:29:29,757][257371] Min Reward on eval: -501.3971257437021[0m
[37m[1m[2023-07-17 05:29:29,757][257371] Mean Reward across all agents: 9.587309647598195[0m
[37m[1m[2023-07-17 05:29:29,758][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:29:29,760][257371] mean_value=-591.8693501740756, max_value=184.6864617605274[0m
[37m[1m[2023-07-17 05:29:29,763][257371] New mean coefficients: [[-0.5079714  -0.00113975  1.0741549   0.82499874 -2.9221647   9.039219  ]][0m
[37m[1m[2023-07-17 05:29:29,764][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:29:38,905][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 05:29:38,906][257371] FPS: 420146.37[0m
[36m[2023-07-17 05:29:38,908][257371] itr=709, itrs=2000, Progress: 35.45%[0m
[36m[2023-07-17 05:29:50,673][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 05:29:50,673][257371] FPS: 328838.37[0m
[36m[2023-07-17 05:29:54,968][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:29:54,969][257371] Reward + Measures: [[-65.26919756   0.56553864   0.90207469   0.67016298   0.50094336
    7.65283632]][0m
[37m[1m[2023-07-17 05:29:54,969][257371] Max Reward on eval: -65.26919755803678[0m
[37m[1m[2023-07-17 05:29:54,969][257371] Min Reward on eval: -65.26919755803678[0m
[37m[1m[2023-07-17 05:29:54,970][257371] Mean Reward across all agents: -65.26919755803678[0m
[37m[1m[2023-07-17 05:29:54,970][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:30:00,213][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:30:00,265][257371] Reward + Measures: [[-110.14893452    0.25689998    0.24969999    0.25599998    0.0925
     4.66515112]
 [ -13.37086717    0.18629999    0.20710002    0.19939999    0.19230001
     3.84514427]
 [ -99.1286072     0.2721        0.96439999    0.5018        0.89449996
     5.77335358]
 ...
 [ -19.18697701    0.33790001    0.30480003    0.33320001    0.2951
     4.6101222 ]
 [ -71.96473383    0.56129998    0.74400002    0.228         0.65450001
     4.69551659]
 [  -6.31324166    0.47139999    0.41999999    0.39660001    0.29320002
     5.44030809]][0m
[37m[1m[2023-07-17 05:30:00,266][257371] Max Reward on eval: 600.6979446459561[0m
[37m[1m[2023-07-17 05:30:00,266][257371] Min Reward on eval: -416.42923780251294[0m
[37m[1m[2023-07-17 05:30:00,266][257371] Mean Reward across all agents: -41.96360611762788[0m
[37m[1m[2023-07-17 05:30:00,266][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:30:00,269][257371] mean_value=-1069.8483902079274, max_value=493.5328876283472[0m
[37m[1m[2023-07-17 05:30:00,271][257371] New mean coefficients: [[-1.060317   -1.0029912   0.23081732  0.6940151  -2.8850734   9.04567   ]][0m
[37m[1m[2023-07-17 05:30:00,272][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:30:09,253][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 05:30:09,254][257371] FPS: 427638.30[0m
[36m[2023-07-17 05:30:09,256][257371] itr=710, itrs=2000, Progress: 35.50%[0m
[37m[1m[2023-07-17 05:33:12,410][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000690[0m
[36m[2023-07-17 05:33:24,578][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 05:33:24,578][257371] FPS: 330690.01[0m
[36m[2023-07-17 05:33:28,830][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:33:28,831][257371] Reward + Measures: [[-102.13068331    0.58487928    0.91652465    0.64691669    0.52831036
     7.66777182]][0m
[37m[1m[2023-07-17 05:33:28,831][257371] Max Reward on eval: -102.13068331486299[0m
[37m[1m[2023-07-17 05:33:28,831][257371] Min Reward on eval: -102.13068331486299[0m
[37m[1m[2023-07-17 05:33:28,831][257371] Mean Reward across all agents: -102.13068331486299[0m
[37m[1m[2023-07-17 05:33:28,832][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:33:33,729][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:33:33,729][257371] Reward + Measures: [[-267.20891567    0.62880003    0.73879999    0.2333        0.63650006
     6.39922285]
 [-261.65046011    0.1602        0.61790001    0.40349999    0.59750003
     5.39519501]
 [ -28.10549004    0.56800002    0.7439        0.67799997    0.22550002
     6.16955423]
 ...
 [-267.60955529    0.78310001    0.87470001    0.73200005    0.21630001
     5.92091846]
 [-395.48513757    0.0823        0.73780006    0.50630003    0.72090006
     6.39405155]
 [-729.20416641    0.75509995    0.96429998    0.14600001    0.97409993
     6.73784781]][0m
[37m[1m[2023-07-17 05:33:33,729][257371] Max Reward on eval: 629.3175442101434[0m
[37m[1m[2023-07-17 05:33:33,729][257371] Min Reward on eval: -765.562911982555[0m
[37m[1m[2023-07-17 05:33:33,730][257371] Mean Reward across all agents: -189.72691643562[0m
[37m[1m[2023-07-17 05:33:33,730][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:33:33,732][257371] mean_value=-559.5145855036812, max_value=272.34625899858514[0m
[37m[1m[2023-07-17 05:33:33,735][257371] New mean coefficients: [[-0.53971475 -0.4545021   0.25066218  0.643308   -1.2390256   8.907661  ]][0m
[37m[1m[2023-07-17 05:33:33,736][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:33:42,706][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 05:33:42,707][257371] FPS: 428137.75[0m
[36m[2023-07-17 05:33:42,709][257371] itr=711, itrs=2000, Progress: 35.55%[0m
[36m[2023-07-17 05:33:54,342][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 05:33:54,342][257371] FPS: 332582.57[0m
[36m[2023-07-17 05:33:58,629][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:33:58,635][257371] Reward + Measures: [[-120.62319308    0.595891      0.90614933    0.64350331    0.51196033
     7.66850376]][0m
[37m[1m[2023-07-17 05:33:58,635][257371] Max Reward on eval: -120.62319308213947[0m
[37m[1m[2023-07-17 05:33:58,636][257371] Min Reward on eval: -120.62319308213947[0m
[37m[1m[2023-07-17 05:33:58,636][257371] Mean Reward across all agents: -120.62319308213947[0m
[37m[1m[2023-07-17 05:33:58,636][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:34:03,712][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:34:03,717][257371] Reward + Measures: [[ -17.19606475    0.30719998    0.39799997    0.38110003    0.27020001
     3.67375159]
 [ 336.02252387    0.1531        0.94530004    0.61619997    0.89440006
     6.21608305]
 [ -11.43545315    0.3177        0.80880004    0.0409        0.83030003
     4.55048704]
 ...
 [ -18.56989654    0.39329997    0.55660003    0.24300002    0.44460002
     3.66017985]
 [ -52.14610712    0.368         0.32300001    0.3062        0.1517
     3.43544316]
 [-142.32016587    0.96110004    0.96170008    0.75480002    0.2184
     6.48803949]][0m
[37m[1m[2023-07-17 05:34:03,718][257371] Max Reward on eval: 638.0882492337842[0m
[37m[1m[2023-07-17 05:34:03,718][257371] Min Reward on eval: -539.8588466439396[0m
[37m[1m[2023-07-17 05:34:03,718][257371] Mean Reward across all agents: -20.119061468374582[0m
[37m[1m[2023-07-17 05:34:03,718][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:34:03,722][257371] mean_value=-687.6060777433411, max_value=464.41555211494955[0m
[37m[1m[2023-07-17 05:34:03,725][257371] New mean coefficients: [[-0.794073   -0.5455022  -0.00391531  0.563012   -0.8706875   8.822502  ]][0m
[37m[1m[2023-07-17 05:34:03,726][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:34:12,816][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 05:34:12,817][257371] FPS: 422488.61[0m
[36m[2023-07-17 05:34:12,819][257371] itr=712, itrs=2000, Progress: 35.60%[0m
[36m[2023-07-17 05:34:24,599][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 05:34:24,599][257371] FPS: 328387.36[0m
[36m[2023-07-17 05:34:28,827][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:34:28,827][257371] Reward + Measures: [[-135.03223298    0.57637668    0.91796434    0.60855001    0.57538933
     7.68928623]][0m
[37m[1m[2023-07-17 05:34:28,828][257371] Max Reward on eval: -135.03223298444937[0m
[37m[1m[2023-07-17 05:34:28,828][257371] Min Reward on eval: -135.03223298444937[0m
[37m[1m[2023-07-17 05:34:28,828][257371] Mean Reward across all agents: -135.03223298444937[0m
[37m[1m[2023-07-17 05:34:28,828][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:34:33,823][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:34:33,824][257371] Reward + Measures: [[ 142.42752121    0.54170001    0.56900001    0.57770002    0.1304
     4.96456671]
 [-181.81919502    0.76630002    0.86669999    0.54750007    0.41810003
     6.83517218]
 [ -99.67475409    0.44050002    0.47370002    0.4578        0.13259999
     4.48276472]
 ...
 [ 537.76321747    0.88149995    0.90469998    0.87919998    0.0825
     6.50852823]
 [  33.28638621    0.53649998    0.71200001    0.63660002    0.30759999
     4.59259748]
 [ -25.64445015    0.28889999    0.28819999    0.22810002    0.222
     4.59044218]][0m
[37m[1m[2023-07-17 05:34:33,824][257371] Max Reward on eval: 670.1359403038863[0m
[37m[1m[2023-07-17 05:34:33,824][257371] Min Reward on eval: -418.7048626050353[0m
[37m[1m[2023-07-17 05:34:33,825][257371] Mean Reward across all agents: 45.35827868292097[0m
[37m[1m[2023-07-17 05:34:33,825][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:34:33,828][257371] mean_value=-304.3643864258564, max_value=167.80383320950136[0m
[37m[1m[2023-07-17 05:34:33,830][257371] New mean coefficients: [[-0.02142107 -0.57066786  0.72957224  0.683116   -0.40040365  8.278053  ]][0m
[37m[1m[2023-07-17 05:34:33,831][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:34:42,794][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 05:34:42,794][257371] FPS: 428523.93[0m
[36m[2023-07-17 05:34:42,797][257371] itr=713, itrs=2000, Progress: 35.65%[0m
[36m[2023-07-17 05:34:54,436][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 05:34:54,436][257371] FPS: 332399.74[0m
[36m[2023-07-17 05:34:58,693][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:34:58,698][257371] Reward + Measures: [[-126.42019717    0.57922733    0.90810061    0.6283707     0.54253036
     7.6822052 ]][0m
[37m[1m[2023-07-17 05:34:58,698][257371] Max Reward on eval: -126.42019717178074[0m
[37m[1m[2023-07-17 05:34:58,699][257371] Min Reward on eval: -126.42019717178074[0m
[37m[1m[2023-07-17 05:34:58,699][257371] Mean Reward across all agents: -126.42019717178074[0m
[37m[1m[2023-07-17 05:34:58,699][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:35:03,737][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:35:03,743][257371] Reward + Measures: [[ -83.03051355    0.62250006    0.96399993    0.52820003    0.68610001
     6.25784922]
 [  78.22557732    0.40470004    0.86380005    0.66930002    0.49720001
     6.52858973]
 [-242.2711184     0.58199996    0.98200005    0.2359        0.98479998
     6.91427612]
 ...
 [-389.61251778    0.48820004    0.98430008    0.30809999    0.98680001
     6.12306309]
 [ -13.49349503    0.23310001    0.2572        0.2069        0.23210001
     4.42699242]
 [ 291.75883914    0.20080002    0.97279996    0.68170005    0.78909999
     7.24109793]][0m
[37m[1m[2023-07-17 05:35:03,743][257371] Max Reward on eval: 497.6592178378254[0m
[37m[1m[2023-07-17 05:35:03,743][257371] Min Reward on eval: -557.2264124187641[0m
[37m[1m[2023-07-17 05:35:03,744][257371] Mean Reward across all agents: -55.14124051622588[0m
[37m[1m[2023-07-17 05:35:03,744][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:35:03,746][257371] mean_value=-374.8662708526665, max_value=264.8483835998126[0m
[37m[1m[2023-07-17 05:35:03,749][257371] New mean coefficients: [[ 0.2896953  -0.18808892  0.68324596  0.08823609 -0.18866932  7.876611  ]][0m
[37m[1m[2023-07-17 05:35:03,750][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:35:12,713][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 05:35:12,713][257371] FPS: 428533.61[0m
[36m[2023-07-17 05:35:12,715][257371] itr=714, itrs=2000, Progress: 35.70%[0m
[36m[2023-07-17 05:35:24,481][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 05:35:24,482][257371] FPS: 328753.59[0m
[36m[2023-07-17 05:35:28,837][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:35:28,837][257371] Reward + Measures: [[-135.14457496    0.56305999    0.91568303    0.60749531    0.58565301
     7.70041323]][0m
[37m[1m[2023-07-17 05:35:28,838][257371] Max Reward on eval: -135.14457496149674[0m
[37m[1m[2023-07-17 05:35:28,838][257371] Min Reward on eval: -135.14457496149674[0m
[37m[1m[2023-07-17 05:35:28,838][257371] Mean Reward across all agents: -135.14457496149674[0m
[37m[1m[2023-07-17 05:35:28,838][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:35:33,843][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:35:33,849][257371] Reward + Measures: [[   4.5472299     0.29620001    0.35390002    0.23800002    0.32729998
     4.00166178]
 [   6.38860101    0.32500002    0.28340003    0.29620001    0.2438
     4.47600508]
 [-120.65896488    0.73340005    0.95959997    0.15460001    0.92989999
     6.66731882]
 ...
 [-127.35388472    0.52160001    0.78420001    0.1265        0.78670001
     4.20502472]
 [-108.43368146    0.40480003    0.51160002    0.31490001    0.34470001
     6.12468672]
 [ -59.55794415    0.74870002    0.84980005    0.58050007    0.34990001
     4.91511488]][0m
[37m[1m[2023-07-17 05:35:33,849][257371] Max Reward on eval: 357.85913660358636[0m
[37m[1m[2023-07-17 05:35:33,849][257371] Min Reward on eval: -681.4146652256138[0m
[37m[1m[2023-07-17 05:35:33,850][257371] Mean Reward across all agents: -72.4401582328613[0m
[37m[1m[2023-07-17 05:35:33,850][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:35:33,854][257371] mean_value=-397.38089173702406, max_value=290.4449052018813[0m
[37m[1m[2023-07-17 05:35:33,857][257371] New mean coefficients: [[ 0.11823072 -0.17788342  0.59457785 -0.14526372 -1.0322871   7.3363066 ]][0m
[37m[1m[2023-07-17 05:35:33,858][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:35:42,982][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 05:35:42,982][257371] FPS: 420936.20[0m
[36m[2023-07-17 05:35:42,984][257371] itr=715, itrs=2000, Progress: 35.75%[0m
[36m[2023-07-17 05:35:54,717][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 05:35:54,717][257371] FPS: 329858.66[0m
[36m[2023-07-17 05:35:58,957][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:35:58,958][257371] Reward + Measures: [[-164.08834368    0.55918133    0.925291      0.572891      0.63394731
     7.71545935]][0m
[37m[1m[2023-07-17 05:35:58,958][257371] Max Reward on eval: -164.08834368096717[0m
[37m[1m[2023-07-17 05:35:58,958][257371] Min Reward on eval: -164.08834368096717[0m
[37m[1m[2023-07-17 05:35:58,959][257371] Mean Reward across all agents: -164.08834368096717[0m
[37m[1m[2023-07-17 05:35:58,959][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:36:03,904][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:36:03,904][257371] Reward + Measures: [[-164.98857475    0.77990001    0.96280003    0.49340001    0.5887
     6.38523531]
 [ -12.44171901    0.54789996    0.95610011    0.33420002    0.88860005
     5.47959375]
 [ 120.8938163     0.67469996    0.65630001    0.63819999    0.67309999
     5.45199776]
 ...
 [ 126.63835413    0.6742        0.97220004    0.8707        0.31560001
     6.93674469]
 [-119.92903046    0.72830003    0.93190002    0.48559999    0.54790002
     5.16588879]
 [-169.78645684    0.72270006    0.66460001    0.77780002    0.10820001
     4.58080101]][0m
[37m[1m[2023-07-17 05:36:03,905][257371] Max Reward on eval: 462.45435034800323[0m
[37m[1m[2023-07-17 05:36:03,905][257371] Min Reward on eval: -520.3853416373953[0m
[37m[1m[2023-07-17 05:36:03,905][257371] Mean Reward across all agents: -52.76480531656032[0m
[37m[1m[2023-07-17 05:36:03,905][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:36:03,908][257371] mean_value=-350.4267993559593, max_value=175.0914803167499[0m
[37m[1m[2023-07-17 05:36:03,910][257371] New mean coefficients: [[ 0.02646222  0.19936946  0.87029755 -0.28514296 -0.66655576  6.569972  ]][0m
[37m[1m[2023-07-17 05:36:03,911][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:36:12,918][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 05:36:12,919][257371] FPS: 426414.86[0m
[36m[2023-07-17 05:36:12,921][257371] itr=716, itrs=2000, Progress: 35.80%[0m
[36m[2023-07-17 05:36:25,007][257371] train() took 12.00 seconds to complete[0m
[36m[2023-07-17 05:36:25,007][257371] FPS: 320046.13[0m
[36m[2023-07-17 05:36:29,345][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:36:29,345][257371] Reward + Measures: [[-147.27952791    0.57131499    0.92861962    0.6104787     0.59399569
     7.71592712]][0m
[37m[1m[2023-07-17 05:36:29,346][257371] Max Reward on eval: -147.27952791142621[0m
[37m[1m[2023-07-17 05:36:29,346][257371] Min Reward on eval: -147.27952791142621[0m
[37m[1m[2023-07-17 05:36:29,346][257371] Mean Reward across all agents: -147.27952791142621[0m
[37m[1m[2023-07-17 05:36:29,346][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:36:34,365][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:36:34,366][257371] Reward + Measures: [[-143.91010476    0.76599997    0.87709999    0.63080001    0.34699997
     6.6044879 ]
 [ -46.58240655    0.67870003    0.96260005    0.65200001    0.50150007
     7.01120758]
 [ -90.50559423    0.57849997    0.96970004    0.59740001    0.64339995
     6.71272278]
 ...
 [ 241.0570009     0.27599999    0.97760004    0.57370001    0.88870001
     6.42654037]
 [-143.12115788    0.65460002    0.98110002    0.29170001    0.88340008
     6.88223267]
 [-249.38926743    0.76160002    0.97670001    0.41549999    0.69279999
     6.85871649]][0m
[37m[1m[2023-07-17 05:36:34,366][257371] Max Reward on eval: 525.0964107759297[0m
[37m[1m[2023-07-17 05:36:34,366][257371] Min Reward on eval: -492.3246336117387[0m
[37m[1m[2023-07-17 05:36:34,367][257371] Mean Reward across all agents: -8.6296953295139[0m
[37m[1m[2023-07-17 05:36:34,367][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:36:34,370][257371] mean_value=-298.6962238631295, max_value=201.84123720659818[0m
[37m[1m[2023-07-17 05:36:34,373][257371] New mean coefficients: [[ 0.9953091   0.8019552   1.4526193  -0.02580193 -0.28433913  6.7939878 ]][0m
[37m[1m[2023-07-17 05:36:34,374][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:36:43,412][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 05:36:43,413][257371] FPS: 424913.86[0m
[36m[2023-07-17 05:36:43,415][257371] itr=717, itrs=2000, Progress: 35.85%[0m
[36m[2023-07-17 05:36:55,181][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 05:36:55,181][257371] FPS: 328801.83[0m
[36m[2023-07-17 05:36:59,460][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:36:59,460][257371] Reward + Measures: [[-132.36513796    0.55059302    0.91478562    0.61631632    0.58353263
     7.69759083]][0m
[37m[1m[2023-07-17 05:36:59,461][257371] Max Reward on eval: -132.365137963048[0m
[37m[1m[2023-07-17 05:36:59,461][257371] Min Reward on eval: -132.365137963048[0m
[37m[1m[2023-07-17 05:36:59,461][257371] Mean Reward across all agents: -132.365137963048[0m
[37m[1m[2023-07-17 05:36:59,461][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:37:04,698][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:37:04,699][257371] Reward + Measures: [[  34.34861462    0.48330003    0.6056        0.3626        0.43470001
     2.72825599]
 [-118.88816454    0.43920001    0.3723        0.3849        0.13990001
     3.50431228]
 [ -41.72719573    0.22239999    0.5776        0.48659998    0.51370001
     3.64183474]
 ...
 [-558.21802203    0.43460003    0.85150003    0.35859999    0.86669999
     6.10689306]
 [-455.07350442    0.0986        0.81980002    0.56520003    0.82340002
     5.52663994]
 [ -58.99046575    0.14490001    0.25980002    0.25640002    0.26260003
     4.38638926]][0m
[37m[1m[2023-07-17 05:37:04,699][257371] Max Reward on eval: 222.12441251482815[0m
[37m[1m[2023-07-17 05:37:04,699][257371] Min Reward on eval: -718.766326932027[0m
[37m[1m[2023-07-17 05:37:04,699][257371] Mean Reward across all agents: -158.47048011078175[0m
[37m[1m[2023-07-17 05:37:04,700][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:37:04,703][257371] mean_value=-919.5296584888098, max_value=103.27259845989532[0m
[37m[1m[2023-07-17 05:37:04,705][257371] New mean coefficients: [[ 0.94624573  1.0141438   1.5192113   0.31181452 -1.3122225   7.626438  ]][0m
[37m[1m[2023-07-17 05:37:04,706][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:37:13,706][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 05:37:13,706][257371] FPS: 426752.48[0m
[36m[2023-07-17 05:37:13,709][257371] itr=718, itrs=2000, Progress: 35.90%[0m
[36m[2023-07-17 05:37:25,828][257371] train() took 12.03 seconds to complete[0m
[36m[2023-07-17 05:37:25,829][257371] FPS: 319257.28[0m
[36m[2023-07-17 05:37:30,123][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:37:30,124][257371] Reward + Measures: [[-127.46531038    0.56326765    0.91334063    0.62114066    0.56630266
     7.69232082]][0m
[37m[1m[2023-07-17 05:37:30,124][257371] Max Reward on eval: -127.46531038342839[0m
[37m[1m[2023-07-17 05:37:30,124][257371] Min Reward on eval: -127.46531038342839[0m
[37m[1m[2023-07-17 05:37:30,125][257371] Mean Reward across all agents: -127.46531038342839[0m
[37m[1m[2023-07-17 05:37:30,125][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:37:35,142][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:37:35,148][257371] Reward + Measures: [[   0.38427377    0.69510001    0.68480003    0.6961        0.0319
     7.04455042]
 [-523.90466308    0.57440007    0.9691        0.26079997    0.97189999
     5.45526838]
 [ 109.20155575    0.48909998    0.97200006    0.60260004    0.69550002
     6.92798996]
 ...
 [ -44.1324521     0.72150004    0.82120001    0.66790003    0.21440001
     7.09468222]
 [ -38.88317014    0.2287        0.50669998    0.36629999    0.43109998
     5.19886351]
 [ -95.34850601    0.65140003    0.86710006    0.69679993    0.35710001
     6.79566193]][0m
[37m[1m[2023-07-17 05:37:35,148][257371] Max Reward on eval: 469.5627274194732[0m
[37m[1m[2023-07-17 05:37:35,148][257371] Min Reward on eval: -561.6788406429463[0m
[37m[1m[2023-07-17 05:37:35,149][257371] Mean Reward across all agents: -52.42846299594043[0m
[37m[1m[2023-07-17 05:37:35,149][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:37:35,152][257371] mean_value=-338.09821196332723, max_value=496.8017786906927[0m
[37m[1m[2023-07-17 05:37:35,155][257371] New mean coefficients: [[ 0.89147604  0.5648354   1.2639098   0.05401641 -0.71667635  7.4195204 ]][0m
[37m[1m[2023-07-17 05:37:35,156][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:37:44,167][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 05:37:44,168][257371] FPS: 426209.13[0m
[36m[2023-07-17 05:37:44,170][257371] itr=719, itrs=2000, Progress: 35.95%[0m
[36m[2023-07-17 05:37:56,069][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 05:37:56,069][257371] FPS: 325183.79[0m
[36m[2023-07-17 05:38:00,404][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:38:00,404][257371] Reward + Measures: [[-115.75170984    0.55141431    0.92689234    0.60823363    0.60671663
     7.71571732]][0m
[37m[1m[2023-07-17 05:38:00,404][257371] Max Reward on eval: -115.75170983527853[0m
[37m[1m[2023-07-17 05:38:00,404][257371] Min Reward on eval: -115.75170983527853[0m
[37m[1m[2023-07-17 05:38:00,405][257371] Mean Reward across all agents: -115.75170983527853[0m
[37m[1m[2023-07-17 05:38:00,405][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:38:05,404][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:38:05,405][257371] Reward + Measures: [[-216.02601596    0.84989995    0.79349995    0.83339995    0.0288
     5.74779415]
 [ 114.80965355    0.47280002    0.86720002    0.6257        0.53430003
     5.34010553]
 [-202.37956404    0.74959999    0.85130006    0.77640003    0.16580002
     5.32797384]
 ...
 [ -21.22567326    0.23650001    0.41499996    0.2832        0.37380001
     4.50797796]
 [  41.74724245    0.15030001    0.56610006    0.55990005    0.65580004
     3.85532808]
 [  12.25259494    0.4578        0.97609997    0.5187        0.80100006
     6.78307343]][0m
[37m[1m[2023-07-17 05:38:05,405][257371] Max Reward on eval: 417.0693586357869[0m
[37m[1m[2023-07-17 05:38:05,405][257371] Min Reward on eval: -656.2305374135263[0m
[37m[1m[2023-07-17 05:38:05,405][257371] Mean Reward across all agents: -33.492619847375984[0m
[37m[1m[2023-07-17 05:38:05,406][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:38:05,409][257371] mean_value=-411.32988727491875, max_value=444.562850372185[0m
[37m[1m[2023-07-17 05:38:05,411][257371] New mean coefficients: [[1.418354   1.5219923  1.5919063  0.13080303 0.59075654 7.2895045 ]][0m
[37m[1m[2023-07-17 05:38:05,412][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:38:14,427][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 05:38:14,428][257371] FPS: 426029.15[0m
[36m[2023-07-17 05:38:14,430][257371] itr=720, itrs=2000, Progress: 36.00%[0m
[37m[1m[2023-07-17 05:41:20,614][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000700[0m
[36m[2023-07-17 05:41:32,976][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 05:41:32,977][257371] FPS: 326797.79[0m
[36m[2023-07-17 05:41:37,244][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:41:37,245][257371] Reward + Measures: [[-120.00379086    0.53588963    0.92147535    0.59822202    0.62300104
     7.71366262]][0m
[37m[1m[2023-07-17 05:41:37,245][257371] Max Reward on eval: -120.00379085774604[0m
[37m[1m[2023-07-17 05:41:37,245][257371] Min Reward on eval: -120.00379085774604[0m
[37m[1m[2023-07-17 05:41:37,246][257371] Mean Reward across all agents: -120.00379085774604[0m
[37m[1m[2023-07-17 05:41:37,246][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:41:42,145][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:41:42,162][257371] Reward + Measures: [[  36.70908266    0.41640002    0.80720007    0.40400001    0.72939998
     6.56486654]
 [ -28.10463527    0.4835        0.95240003    0.34450004    0.88729995
     7.0885973 ]
 [  28.57153964    0.74199998    0.96340007    0.82300007    0.3127
     5.63944483]
 ...
 [-236.15430304    0.70410007    0.97500002    0.37440002    0.7899
     6.76428175]
 [ 151.02615071    0.28739998    0.98190004    0.45930001    0.98680001
     7.46822071]
 [ -88.29148279    0.75520003    0.78360003    0.6591        0.15750001
     5.64576435]][0m
[37m[1m[2023-07-17 05:41:42,163][257371] Max Reward on eval: 405.72569059953094[0m
[37m[1m[2023-07-17 05:41:42,163][257371] Min Reward on eval: -546.7653226679191[0m
[37m[1m[2023-07-17 05:41:42,163][257371] Mean Reward across all agents: -19.248598360088042[0m
[37m[1m[2023-07-17 05:41:42,163][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:41:42,166][257371] mean_value=-318.0385431408845, max_value=364.1871854546223[0m
[37m[1m[2023-07-17 05:41:42,169][257371] New mean coefficients: [[ 1.6668737   1.2979648   1.6048969   0.14031927 -0.21641725  7.5081077 ]][0m
[37m[1m[2023-07-17 05:41:42,170][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:41:51,169][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 05:41:51,170][257371] FPS: 426745.85[0m
[36m[2023-07-17 05:41:51,172][257371] itr=721, itrs=2000, Progress: 36.05%[0m
[36m[2023-07-17 05:42:02,879][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 05:42:02,880][257371] FPS: 330457.13[0m
[36m[2023-07-17 05:42:07,120][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:42:07,120][257371] Reward + Measures: [[-152.41488995    0.59503168    0.9398877     0.56835032    0.63249266
     7.74284506]][0m
[37m[1m[2023-07-17 05:42:07,120][257371] Max Reward on eval: -152.41488994902724[0m
[37m[1m[2023-07-17 05:42:07,120][257371] Min Reward on eval: -152.41488994902724[0m
[37m[1m[2023-07-17 05:42:07,121][257371] Mean Reward across all agents: -152.41488994902724[0m
[37m[1m[2023-07-17 05:42:07,121][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:42:12,150][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:42:12,211][257371] Reward + Measures: [[-78.49759719   0.0914       0.86750001   0.83850002   0.85729998
    4.72352362]
 [228.14508648   0.61549997   0.70790005   0.65480006   0.16159999
    4.84024143]
 [-60.94144634   0.68489999   0.77450007   0.71259993   0.1391
    6.55607748]
 ...
 [344.73559085   0.17230001   0.90999997   0.91099995   0.79030001
    4.79522038]
 [ 78.36944276   0.49670005   0.85350001   0.67760003   0.40760002
    6.46013641]
 [257.7991285    0.17330001   0.95489997   0.94990009   0.8075
    4.04367828]][0m
[37m[1m[2023-07-17 05:42:12,211][257371] Max Reward on eval: 500.47430799088323[0m
[37m[1m[2023-07-17 05:42:12,212][257371] Min Reward on eval: -511.32519590854645[0m
[37m[1m[2023-07-17 05:42:12,212][257371] Mean Reward across all agents: 87.22559635792658[0m
[37m[1m[2023-07-17 05:42:12,212][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:42:12,218][257371] mean_value=-166.79452519771115, max_value=711.1461405448615[0m
[37m[1m[2023-07-17 05:42:12,221][257371] New mean coefficients: [[ 2.0809512   1.0225495   1.499183    0.45324302 -1.2294097   8.023903  ]][0m
[37m[1m[2023-07-17 05:42:12,222][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:42:21,305][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 05:42:21,305][257371] FPS: 422850.72[0m
[36m[2023-07-17 05:42:21,308][257371] itr=722, itrs=2000, Progress: 36.10%[0m
[36m[2023-07-17 05:42:33,116][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 05:42:33,116][257371] FPS: 327685.99[0m
[36m[2023-07-17 05:42:37,460][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:42:37,460][257371] Reward + Measures: [[-174.14132178    0.58885032    0.92614532    0.53544968    0.65061969
     7.72621727]][0m
[37m[1m[2023-07-17 05:42:37,461][257371] Max Reward on eval: -174.1413217811281[0m
[37m[1m[2023-07-17 05:42:37,461][257371] Min Reward on eval: -174.1413217811281[0m
[37m[1m[2023-07-17 05:42:37,461][257371] Mean Reward across all agents: -174.1413217811281[0m
[37m[1m[2023-07-17 05:42:37,461][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:42:42,436][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:42:42,437][257371] Reward + Measures: [[565.92622638   0.0117       0.89590007   0.71710008   0.91509992
    6.78993177]
 [288.48672773   0.68380004   0.9709       0.9077999    0.3062
    6.82403421]
 [ 88.22829816   0.35120001   0.97460002   0.34870002   0.98040003
    5.96197605]
 ...
 [-92.26321603   0.89650005   0.92780012   0.79829997   0.14930001
    5.90008497]
 [ 35.56425525   0.73210001   0.82310003   0.70529997   0.21300001
    6.45573187]
 [-20.28142399   0.54450005   0.7888       0.64779997   0.33870003
    5.19395304]][0m
[37m[1m[2023-07-17 05:42:42,437][257371] Max Reward on eval: 693.138242748566[0m
[37m[1m[2023-07-17 05:42:42,437][257371] Min Reward on eval: -529.6742058103904[0m
[37m[1m[2023-07-17 05:42:42,437][257371] Mean Reward across all agents: 45.470371399452205[0m
[37m[1m[2023-07-17 05:42:42,438][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:42:42,442][257371] mean_value=-370.76284923472923, max_value=521.4337461414282[0m
[37m[1m[2023-07-17 05:42:42,445][257371] New mean coefficients: [[ 1.8885472   1.483005    2.2486196   0.27811366 -1.091929    6.992509  ]][0m
[37m[1m[2023-07-17 05:42:42,446][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:42:51,427][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 05:42:51,428][257371] FPS: 427622.53[0m
[36m[2023-07-17 05:42:51,430][257371] itr=723, itrs=2000, Progress: 36.15%[0m
[36m[2023-07-17 05:43:03,219][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 05:43:03,219][257371] FPS: 328267.38[0m
[36m[2023-07-17 05:43:07,440][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:43:07,441][257371] Reward + Measures: [[-194.75597027    0.60366201    0.93452895    0.51620835    0.67276227
     7.74163389]][0m
[37m[1m[2023-07-17 05:43:07,441][257371] Max Reward on eval: -194.7559702654633[0m
[37m[1m[2023-07-17 05:43:07,441][257371] Min Reward on eval: -194.7559702654633[0m
[37m[1m[2023-07-17 05:43:07,442][257371] Mean Reward across all agents: -194.7559702654633[0m
[37m[1m[2023-07-17 05:43:07,442][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:43:12,613][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:43:12,613][257371] Reward + Measures: [[ -25.22694892    0.23279999    0.80610001    0.80120003    0.63670003
     5.28082371]
 [-111.87726593    0.75550002    0.63819999    0.6455        0.7712
     4.79382277]
 [ 245.45628075    0.58390003    0.63239998    0.6279        0.17730001
     5.19833899]
 ...
 [-276.70854504    0.60170001    0.97640002    0.22219999    0.98000002
     6.40730286]
 [ 166.21465423    0.20509999    0.86520004    0.56980002    0.7123
     6.1239152 ]
 [ -56.70011299    0.2304        0.30880001    0.31549999    0.30930001
     4.62069273]][0m
[37m[1m[2023-07-17 05:43:12,614][257371] Max Reward on eval: 589.8131304094568[0m
[37m[1m[2023-07-17 05:43:12,614][257371] Min Reward on eval: -504.62815953549[0m
[37m[1m[2023-07-17 05:43:12,614][257371] Mean Reward across all agents: 13.063644867131291[0m
[37m[1m[2023-07-17 05:43:12,614][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:43:12,617][257371] mean_value=-652.5677145608917, max_value=154.08228930854506[0m
[37m[1m[2023-07-17 05:43:12,620][257371] New mean coefficients: [[ 2.496737    1.228281    2.1581995   0.41995943 -2.024056    7.197508  ]][0m
[37m[1m[2023-07-17 05:43:12,621][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:43:21,580][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 05:43:21,580][257371] FPS: 428717.97[0m
[36m[2023-07-17 05:43:21,582][257371] itr=724, itrs=2000, Progress: 36.20%[0m
[36m[2023-07-17 05:43:33,200][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 05:43:33,200][257371] FPS: 333054.81[0m
[36m[2023-07-17 05:43:37,449][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:43:37,449][257371] Reward + Measures: [[-198.92531902    0.60112596    0.94019628    0.51101398    0.68544561
     7.74954605]][0m
[37m[1m[2023-07-17 05:43:37,449][257371] Max Reward on eval: -198.92531901885414[0m
[37m[1m[2023-07-17 05:43:37,450][257371] Min Reward on eval: -198.92531901885414[0m
[37m[1m[2023-07-17 05:43:37,450][257371] Mean Reward across all agents: -198.92531901885414[0m
[37m[1m[2023-07-17 05:43:37,450][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:43:42,396][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:43:42,397][257371] Reward + Measures: [[ 70.96258373   0.2138       0.23150001   0.2739       0.1945
    4.7383647 ]
 [-58.53336405   0.62359995   0.6512       0.64940006   0.1053
    3.81045389]
 [ -2.77377742   0.46700001   0.5212       0.38369998   0.42290002
    3.93486142]
 ...
 [ 51.00947591   0.29160002   0.43280002   0.48379999   0.28510004
    4.30948114]
 [ 77.07942602   0.45170003   0.82050002   0.80019999   0.40540001
    5.27917337]
 [-26.61731584   0.56879997   0.6771       0.2277       0.6081
    4.84131098]][0m
[37m[1m[2023-07-17 05:43:42,397][257371] Max Reward on eval: 418.4106159389019[0m
[37m[1m[2023-07-17 05:43:42,397][257371] Min Reward on eval: -542.8074245404453[0m
[37m[1m[2023-07-17 05:43:42,398][257371] Mean Reward across all agents: -2.712582772518867[0m
[37m[1m[2023-07-17 05:43:42,398][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:43:42,401][257371] mean_value=-549.0761631767807, max_value=541.629679532988[0m
[37m[1m[2023-07-17 05:43:42,404][257371] New mean coefficients: [[ 2.3850508  1.2549833  2.2566288  0.6386804 -1.3242084  6.98717  ]][0m
[37m[1m[2023-07-17 05:43:42,405][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:43:51,449][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 05:43:51,449][257371] FPS: 424642.75[0m
[36m[2023-07-17 05:43:51,452][257371] itr=725, itrs=2000, Progress: 36.25%[0m
[36m[2023-07-17 05:44:03,176][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 05:44:03,176][257371] FPS: 330010.70[0m
[36m[2023-07-17 05:44:07,504][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:44:07,505][257371] Reward + Measures: [[-214.02260462    0.59685999    0.94936866    0.50833935    0.70550436
     7.77072239]][0m
[37m[1m[2023-07-17 05:44:07,505][257371] Max Reward on eval: -214.0226046184399[0m
[37m[1m[2023-07-17 05:44:07,505][257371] Min Reward on eval: -214.0226046184399[0m
[37m[1m[2023-07-17 05:44:07,505][257371] Mean Reward across all agents: -214.0226046184399[0m
[37m[1m[2023-07-17 05:44:07,506][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:44:12,535][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:44:12,536][257371] Reward + Measures: [[ -87.35581513    0.35320002    0.34889999    0.32710001    0.28929999
     5.49940062]
 [ -10.39845086    0.55940002    0.96390003    0.40920001    0.78859997
     6.62384129]
 [-259.88059762    0.62029999    0.77609998    0.31920001    0.54409999
     5.72961426]
 ...
 [ -55.05202377    0.24510001    0.24600001    0.22410002    0.18179999
     4.24024725]
 [-327.97202347    0.67040002    0.98629999    0.42250004    0.79570001
     6.73961353]
 [-192.59477665    0.43060002    0.98099995    0.47200003    0.8398
     5.93242121]][0m
[37m[1m[2023-07-17 05:44:12,536][257371] Max Reward on eval: 460.5529518107302[0m
[37m[1m[2023-07-17 05:44:12,536][257371] Min Reward on eval: -611.115531934565[0m
[37m[1m[2023-07-17 05:44:12,537][257371] Mean Reward across all agents: -92.40332852460605[0m
[37m[1m[2023-07-17 05:44:12,537][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:44:12,540][257371] mean_value=-511.07369314193784, max_value=137.19909137071824[0m
[37m[1m[2023-07-17 05:44:12,543][257371] New mean coefficients: [[ 2.5842886   0.9523601   2.8306382   0.73836285 -2.160637    7.050209  ]][0m
[37m[1m[2023-07-17 05:44:12,544][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:44:21,599][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 05:44:21,599][257371] FPS: 424150.96[0m
[36m[2023-07-17 05:44:21,601][257371] itr=726, itrs=2000, Progress: 36.30%[0m
[36m[2023-07-17 05:44:33,357][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 05:44:33,357][257371] FPS: 329128.20[0m
[36m[2023-07-17 05:44:37,705][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:44:37,705][257371] Reward + Measures: [[-188.5799198     0.58647168    0.93587142    0.53997701    0.66531301
     7.75580072]][0m
[37m[1m[2023-07-17 05:44:37,706][257371] Max Reward on eval: -188.57991980380984[0m
[37m[1m[2023-07-17 05:44:37,706][257371] Min Reward on eval: -188.57991980380984[0m
[37m[1m[2023-07-17 05:44:37,706][257371] Mean Reward across all agents: -188.57991980380984[0m
[37m[1m[2023-07-17 05:44:37,706][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:44:42,708][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:44:42,709][257371] Reward + Measures: [[ 171.5240493     0.62730002    0.87150002    0.70909995    0.39090002
     6.52528477]
 [-194.81445409    0.56720001    0.96900004    0.33969998    0.88020003
     7.33418131]
 [ 267.3140311     0.27879998    0.89519995    0.52300006    0.88510001
     5.65121984]
 ...
 [ 655.68474957    0.51899999    0.91060001    0.3008        0.82910007
     6.06244135]
 [ 398.41129017    0.36470002    0.9217        0.47490001    0.92080003
     6.06853151]
 [  60.18392442    0.68049997    0.97340012    0.80779999    0.4052
     6.4873004 ]][0m
[37m[1m[2023-07-17 05:44:42,709][257371] Max Reward on eval: 662.128609681502[0m
[37m[1m[2023-07-17 05:44:42,709][257371] Min Reward on eval: -538.9601214361377[0m
[37m[1m[2023-07-17 05:44:42,709][257371] Mean Reward across all agents: 212.30438268778553[0m
[37m[1m[2023-07-17 05:44:42,710][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:44:42,716][257371] mean_value=-127.41310734487033, max_value=563.1073089442593[0m
[37m[1m[2023-07-17 05:44:42,719][257371] New mean coefficients: [[ 3.5214953  1.7701168  3.2853444  1.1343434 -1.4871274  7.6763067]][0m
[37m[1m[2023-07-17 05:44:42,720][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:44:51,756][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 05:44:51,757][257371] FPS: 425020.55[0m
[36m[2023-07-17 05:44:51,759][257371] itr=727, itrs=2000, Progress: 36.35%[0m
[36m[2023-07-17 05:45:03,598][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 05:45:03,599][257371] FPS: 326787.06[0m
[36m[2023-07-17 05:45:07,909][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:45:07,909][257371] Reward + Measures: [[-208.56828585    0.56582731    0.94704163    0.5199073     0.71765834
     7.76967192]][0m
[37m[1m[2023-07-17 05:45:07,909][257371] Max Reward on eval: -208.56828585275147[0m
[37m[1m[2023-07-17 05:45:07,910][257371] Min Reward on eval: -208.56828585275147[0m
[37m[1m[2023-07-17 05:45:07,910][257371] Mean Reward across all agents: -208.56828585275147[0m
[37m[1m[2023-07-17 05:45:07,910][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:45:12,905][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:45:12,906][257371] Reward + Measures: [[  23.12641862    0.27700001    0.97270006    0.54159999    0.97799999
     5.62238455]
 [ 321.93598436    0.32660004    0.72549999    0.36069998    0.66839999
     5.48855305]
 [ 292.72945883    0.46740004    0.94279999    0.47590002    0.87320006
     5.65617895]
 ...
 [  76.60535532    0.57569999    0.64530003    0.38800001    0.64720005
     4.52357578]
 [ 237.87714123    0.227         0.87830001    0.74960005    0.70889997
     5.2256465 ]
 [-258.7494812     0.58680004    0.97620004    0.29010001    0.98260003
     6.45231962]][0m
[37m[1m[2023-07-17 05:45:12,906][257371] Max Reward on eval: 669.4464301779866[0m
[37m[1m[2023-07-17 05:45:12,907][257371] Min Reward on eval: -507.4477920586243[0m
[37m[1m[2023-07-17 05:45:12,907][257371] Mean Reward across all agents: 138.71034428421544[0m
[37m[1m[2023-07-17 05:45:12,907][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:45:12,916][257371] mean_value=-104.38560762721693, max_value=859.7525028835983[0m
[37m[1m[2023-07-17 05:45:12,924][257371] New mean coefficients: [[ 4.2012906  2.0692568  4.239606   1.7894285 -1.1142235  8.014879 ]][0m
[37m[1m[2023-07-17 05:45:12,925][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:45:21,870][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 05:45:21,871][257371] FPS: 429336.11[0m
[36m[2023-07-17 05:45:21,873][257371] itr=728, itrs=2000, Progress: 36.40%[0m
[36m[2023-07-17 05:45:33,700][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 05:45:33,700][257371] FPS: 327110.60[0m
[36m[2023-07-17 05:45:38,086][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:45:38,087][257371] Reward + Measures: [[-214.05772326    0.585329      0.94137597    0.4962683     0.71567529
     7.76544714]][0m
[37m[1m[2023-07-17 05:45:38,087][257371] Max Reward on eval: -214.0577232551695[0m
[37m[1m[2023-07-17 05:45:38,087][257371] Min Reward on eval: -214.0577232551695[0m
[37m[1m[2023-07-17 05:45:38,088][257371] Mean Reward across all agents: -214.0577232551695[0m
[37m[1m[2023-07-17 05:45:38,088][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:45:43,147][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:45:43,148][257371] Reward + Measures: [[  13.37954976    0.51660001    0.70559996    0.58090001    0.34520003
     6.96226645]
 [  16.82772898    0.60909998    0.60340005    0.51700002    0.14670001
     6.82411957]
 [  17.14683068    0.40979996    0.68839997    0.40939999    0.509
     7.04301596]
 ...
 [-154.19607233    0.5916        0.67200005    0.44390002    0.34
     6.56766272]
 [-331.01444481    0.83050007    0.96500009    0.34290001    0.7299
     7.07887411]
 [-105.09319418    0.66570008    0.77440006    0.63709998    0.26989999
     7.05163336]][0m
[37m[1m[2023-07-17 05:45:43,148][257371] Max Reward on eval: 409.88037789426744[0m
[37m[1m[2023-07-17 05:45:43,148][257371] Min Reward on eval: -369.6458910161629[0m
[37m[1m[2023-07-17 05:45:43,149][257371] Mean Reward across all agents: -28.767692408784526[0m
[37m[1m[2023-07-17 05:45:43,149][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:45:43,153][257371] mean_value=-264.3547564031361, max_value=209.76055213778977[0m
[37m[1m[2023-07-17 05:45:43,156][257371] New mean coefficients: [[ 3.798979   1.6872892  4.060908   1.5299184 -1.1927781  7.5610895]][0m
[37m[1m[2023-07-17 05:45:43,157][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:45:52,221][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 05:45:52,221][257371] FPS: 423719.33[0m
[36m[2023-07-17 05:45:52,223][257371] itr=729, itrs=2000, Progress: 36.45%[0m
[36m[2023-07-17 05:46:03,893][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 05:46:03,893][257371] FPS: 331637.48[0m
[36m[2023-07-17 05:46:08,223][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:46:08,223][257371] Reward + Measures: [[-186.60159627    0.59665138    0.93967903    0.52262038    0.68043864
     7.76603889]][0m
[37m[1m[2023-07-17 05:46:08,224][257371] Max Reward on eval: -186.60159627461002[0m
[37m[1m[2023-07-17 05:46:08,224][257371] Min Reward on eval: -186.60159627461002[0m
[37m[1m[2023-07-17 05:46:08,224][257371] Mean Reward across all agents: -186.60159627461002[0m
[37m[1m[2023-07-17 05:46:08,224][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:46:13,414][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:46:13,415][257371] Reward + Measures: [[  11.99813668    0.55950004    0.64970005    0.34300002    0.39780003
     4.86849451]
 [ -63.38379006    0.72910005    0.70700002    0.7051        0.68959999
     4.86564255]
 [ -38.29524901    0.45730001    0.46539998    0.44910002    0.45120001
     3.82359314]
 ...
 [ -59.13779919    0.15900001    0.19230001    0.1618        0.18269999
     4.88361502]
 [ -59.08309267    0.29050002    0.28740001    0.29980001    0.33720002
     4.45615244]
 [-266.41834009    0.84570009    0.73839998    0.80400002    0.11740001
     5.52473593]][0m
[37m[1m[2023-07-17 05:46:13,415][257371] Max Reward on eval: 320.91637235935775[0m
[37m[1m[2023-07-17 05:46:13,415][257371] Min Reward on eval: -322.7760273949243[0m
[37m[1m[2023-07-17 05:46:13,415][257371] Mean Reward across all agents: -30.04009464001157[0m
[37m[1m[2023-07-17 05:46:13,416][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:46:13,417][257371] mean_value=-1846.4121302439762, max_value=49.24635337421631[0m
[37m[1m[2023-07-17 05:46:13,420][257371] New mean coefficients: [[ 3.484431    1.5643141   2.7155418   1.0309911  -0.58880454  7.789399  ]][0m
[37m[1m[2023-07-17 05:46:13,420][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:46:22,446][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 05:46:22,446][257371] FPS: 425523.21[0m
[36m[2023-07-17 05:46:22,449][257371] itr=730, itrs=2000, Progress: 36.50%[0m
[37m[1m[2023-07-17 05:49:32,890][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000710[0m
[36m[2023-07-17 05:49:45,097][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 05:49:45,098][257371] FPS: 329398.66[0m
[36m[2023-07-17 05:49:49,362][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:49:49,362][257371] Reward + Measures: [[-179.24617342    0.57969695    0.94087368    0.52211833    0.69249672
     7.77026939]][0m
[37m[1m[2023-07-17 05:49:49,362][257371] Max Reward on eval: -179.24617341922186[0m
[37m[1m[2023-07-17 05:49:49,363][257371] Min Reward on eval: -179.24617341922186[0m
[37m[1m[2023-07-17 05:49:49,363][257371] Mean Reward across all agents: -179.24617341922186[0m
[37m[1m[2023-07-17 05:49:49,363][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:49:54,395][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:49:54,401][257371] Reward + Measures: [[-41.70223236   0.60770005   0.93050003   0.40629998   0.70900005
    5.74454355]
 [100.78501534   0.45679998   0.97749996   0.44449997   0.8872
    7.01580381]
 [  1.94069386   0.396        0.49670002   0.27369997   0.3522
    5.48972988]
 ...
 [-64.33368087   0.6674       0.97510004   0.49770004   0.69730002
    7.58335829]
 [144.01373381   0.49510002   0.78799999   0.4711       0.514
    5.7784462 ]
 [ 15.23455159   0.28170002   0.47830001   0.24959998   0.4515
    3.97275662]][0m
[37m[1m[2023-07-17 05:49:54,401][257371] Max Reward on eval: 585.8221425794065[0m
[37m[1m[2023-07-17 05:49:54,401][257371] Min Reward on eval: -473.39695742484184[0m
[37m[1m[2023-07-17 05:49:54,402][257371] Mean Reward across all agents: 56.73003950508543[0m
[37m[1m[2023-07-17 05:49:54,402][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:49:54,406][257371] mean_value=-346.19322041618113, max_value=377.8878567754973[0m
[37m[1m[2023-07-17 05:49:54,409][257371] New mean coefficients: [[ 2.7138588  1.0720866  2.2957716  1.2850302 -1.8295841  7.6051445]][0m
[37m[1m[2023-07-17 05:49:54,410][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:50:03,369][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 05:50:03,370][257371] FPS: 428673.72[0m
[36m[2023-07-17 05:50:03,372][257371] itr=731, itrs=2000, Progress: 36.55%[0m
[36m[2023-07-17 05:50:15,199][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 05:50:15,199][257371] FPS: 327148.54[0m
[36m[2023-07-17 05:50:19,492][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:50:19,492][257371] Reward + Measures: [[-218.21983293    0.57850701    0.95428401    0.43451166    0.79621369
     7.79258251]][0m
[37m[1m[2023-07-17 05:50:19,492][257371] Max Reward on eval: -218.21983293380285[0m
[37m[1m[2023-07-17 05:50:19,493][257371] Min Reward on eval: -218.21983293380285[0m
[37m[1m[2023-07-17 05:50:19,493][257371] Mean Reward across all agents: -218.21983293380285[0m
[37m[1m[2023-07-17 05:50:19,493][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:50:24,453][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:50:24,454][257371] Reward + Measures: [[-142.44993973    0.57700002    0.99279994    0.24340001    0.99330008
     6.37915707]
 [ 122.89654657    0.17900001    0.40529999    0.36709997    0.43959999
     3.99743438]
 [-239.41594351    0.54320002    0.94910002    0.33290002    0.90100002
     7.40655613]
 ...
 [ 125.58621193    0.35590002    0.36540002    0.36430004    0.39629999
     4.01690054]
 [ 144.61160922    0.40000001    0.34290001    0.38800001    0.40110001
     3.1752913 ]
 [  70.40492775    0.18719999    0.50680006    0.3461        0.57530004
     4.65459156]][0m
[37m[1m[2023-07-17 05:50:24,454][257371] Max Reward on eval: 574.2736854353919[0m
[37m[1m[2023-07-17 05:50:24,454][257371] Min Reward on eval: -588.2208576274104[0m
[37m[1m[2023-07-17 05:50:24,454][257371] Mean Reward across all agents: 13.55192757299444[0m
[37m[1m[2023-07-17 05:50:24,455][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:50:24,459][257371] mean_value=-278.89279235685495, max_value=576.1282930690329[0m
[37m[1m[2023-07-17 05:50:24,462][257371] New mean coefficients: [[ 3.0443025  1.4332227  3.0541248  1.529586  -1.0956142  7.8458076]][0m
[37m[1m[2023-07-17 05:50:24,463][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:50:33,457][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 05:50:33,457][257371] FPS: 427015.61[0m
[36m[2023-07-17 05:50:33,460][257371] itr=732, itrs=2000, Progress: 36.60%[0m
[36m[2023-07-17 05:50:45,196][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 05:50:45,196][257371] FPS: 329712.09[0m
[36m[2023-07-17 05:50:49,507][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:50:49,508][257371] Reward + Measures: [[-247.97450788    0.60510671    0.965038      0.39724332    0.83027267
     7.80944967]][0m
[37m[1m[2023-07-17 05:50:49,508][257371] Max Reward on eval: -247.97450787593226[0m
[37m[1m[2023-07-17 05:50:49,508][257371] Min Reward on eval: -247.97450787593226[0m
[37m[1m[2023-07-17 05:50:49,509][257371] Mean Reward across all agents: -247.97450787593226[0m
[37m[1m[2023-07-17 05:50:49,509][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:50:54,552][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:50:54,553][257371] Reward + Measures: [[-95.54180721   0.82749999   0.95889997   0.36260003   0.70380002
    6.44456434]
 [-42.47748277   0.48879996   0.77829999   0.5855       0.42050001
    5.82221413]
 [310.38290942   0.39470002   0.67359996   0.23360001   0.67580003
    6.03639841]
 ...
 [434.9552784    0.3682       0.95660001   0.64090008   0.73860008
    6.55046034]
 [-47.73265149   0.23170002   0.1655       0.26840001   0.101
    5.41793013]
 [242.8255856    0.3003       0.87060004   0.48700005   0.86790001
    6.95424271]][0m
[37m[1m[2023-07-17 05:50:54,553][257371] Max Reward on eval: 755.6489667315036[0m
[37m[1m[2023-07-17 05:50:54,553][257371] Min Reward on eval: -801.9123306287453[0m
[37m[1m[2023-07-17 05:50:54,554][257371] Mean Reward across all agents: -4.963545351299244[0m
[37m[1m[2023-07-17 05:50:54,554][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:50:54,558][257371] mean_value=-340.7791924018504, max_value=168.64172272089615[0m
[37m[1m[2023-07-17 05:50:54,561][257371] New mean coefficients: [[ 2.9789588  1.6889176  3.3091826  1.9490414 -1.2693784  7.9059906]][0m
[37m[1m[2023-07-17 05:50:54,562][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:51:03,737][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-17 05:51:03,737][257371] FPS: 418607.66[0m
[36m[2023-07-17 05:51:03,740][257371] itr=733, itrs=2000, Progress: 36.65%[0m
[36m[2023-07-17 05:51:15,574][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 05:51:15,574][257371] FPS: 327005.78[0m
[36m[2023-07-17 05:51:19,851][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:51:19,856][257371] Reward + Measures: [[-257.82315588    0.66520399    0.95840633    0.37465966    0.79933232
     7.79172993]][0m
[37m[1m[2023-07-17 05:51:19,857][257371] Max Reward on eval: -257.8231558792194[0m
[37m[1m[2023-07-17 05:51:19,857][257371] Min Reward on eval: -257.8231558792194[0m
[37m[1m[2023-07-17 05:51:19,857][257371] Mean Reward across all agents: -257.8231558792194[0m
[37m[1m[2023-07-17 05:51:19,858][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:51:24,819][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:51:24,820][257371] Reward + Measures: [[-675.7566261     0.5851        0.89790004    0.25900003    0.90480006
     6.71791935]
 [-671.19377515    0.87790006    0.98390007    0.09170001    0.98979998
     6.84491301]
 [-431.20427464    0.50159997    0.97310001    0.37420002    0.97469997
     7.58132029]
 ...
 [-575.94806194    0.11390001    0.79580003    0.61969995    0.78660005
     6.02604818]
 [-151.11489605    0.6821        0.829         0.52299994    0.43640003
     6.28196573]
 [-776.52772521    0.8912999     0.98359996    0.0682        0.99020004
     7.03584909]][0m
[37m[1m[2023-07-17 05:51:24,820][257371] Max Reward on eval: 531.6107015821151[0m
[37m[1m[2023-07-17 05:51:24,820][257371] Min Reward on eval: -776.5277252138243[0m
[37m[1m[2023-07-17 05:51:24,821][257371] Mean Reward across all agents: -260.73872989435586[0m
[37m[1m[2023-07-17 05:51:24,821][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:51:24,824][257371] mean_value=-616.4034387672252, max_value=149.01089417593045[0m
[37m[1m[2023-07-17 05:51:24,826][257371] New mean coefficients: [[ 3.1423874   1.0575249   2.9798923   2.0270379  -0.79695445  8.316806  ]][0m
[37m[1m[2023-07-17 05:51:24,827][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:51:33,857][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 05:51:33,857][257371] FPS: 425330.58[0m
[36m[2023-07-17 05:51:33,859][257371] itr=734, itrs=2000, Progress: 36.70%[0m
[36m[2023-07-17 05:51:45,610][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 05:51:45,610][257371] FPS: 329355.58[0m
[36m[2023-07-17 05:51:49,912][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:51:49,912][257371] Reward + Measures: [[-169.06532646    0.60276496    0.93887436    0.47659534    0.71995205
     7.76870966]][0m
[37m[1m[2023-07-17 05:51:49,913][257371] Max Reward on eval: -169.0653264619639[0m
[37m[1m[2023-07-17 05:51:49,913][257371] Min Reward on eval: -169.0653264619639[0m
[37m[1m[2023-07-17 05:51:49,913][257371] Mean Reward across all agents: -169.0653264619639[0m
[37m[1m[2023-07-17 05:51:49,913][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:51:54,950][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:51:54,950][257371] Reward + Measures: [[-40.00734949   0.71940005   0.7841       0.71629995   0.36080003
    4.35316277]
 [210.93107984   0.77720004   0.49260002   0.41230002   0.80170006
    4.74521017]
 [152.89740445   0.30899999   0.90790004   0.58910006   0.78109998
    5.12627935]
 ...
 [ 72.08919244   0.34630001   0.51200002   0.3987       0.44730002
    3.49319005]
 [-90.96203594   0.35380003   0.36809999   0.18610001   0.33400002
    3.97990108]
 [ 10.47776318   0.23899999   0.3143       0.16839999   0.30899999
    3.86220551]][0m
[37m[1m[2023-07-17 05:51:54,950][257371] Max Reward on eval: 499.3258934035199[0m
[37m[1m[2023-07-17 05:51:54,951][257371] Min Reward on eval: -459.7522048920393[0m
[37m[1m[2023-07-17 05:51:54,951][257371] Mean Reward across all agents: 47.97981913743444[0m
[37m[1m[2023-07-17 05:51:54,951][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:51:54,957][257371] mean_value=-390.5329037726131, max_value=558.9746482435173[0m
[37m[1m[2023-07-17 05:51:54,960][257371] New mean coefficients: [[ 2.649239   1.4549873  3.4549828  2.0255427 -1.0745263  7.5007505]][0m
[37m[1m[2023-07-17 05:51:54,961][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:52:03,944][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 05:52:03,944][257371] FPS: 427521.21[0m
[36m[2023-07-17 05:52:03,947][257371] itr=735, itrs=2000, Progress: 36.75%[0m
[36m[2023-07-17 05:52:15,888][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-17 05:52:15,889][257371] FPS: 323954.31[0m
[36m[2023-07-17 05:52:20,102][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:52:20,103][257371] Reward + Measures: [[-216.17267501    0.59007633    0.94335431    0.43111628    0.7824977
     7.7782526 ]][0m
[37m[1m[2023-07-17 05:52:20,103][257371] Max Reward on eval: -216.17267501425928[0m
[37m[1m[2023-07-17 05:52:20,103][257371] Min Reward on eval: -216.17267501425928[0m
[37m[1m[2023-07-17 05:52:20,104][257371] Mean Reward across all agents: -216.17267501425928[0m
[37m[1m[2023-07-17 05:52:20,104][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:52:25,141][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:52:25,142][257371] Reward + Measures: [[ 193.48936749    0.84750003    0.991         0.0828        0.98989993
     7.01896763]
 [  54.52477957    0.72570002    0.9594        0.71380001    0.95580006
     6.906991  ]
 [-658.87308502    0.96689999    0.98600006    0.0012        0.98999995
     7.2720027 ]
 ...
 [ 309.06382845    0.68970001    0.986         0.2349        0.99289989
     6.02059841]
 [ 495.93319318    0.66930002    0.97440004    0.2017        0.93670005
     6.88371897]
 [-440.90996458    0.78119993    0.99050009    0.15650001    0.99519998
     7.67491627]][0m
[37m[1m[2023-07-17 05:52:25,142][257371] Max Reward on eval: 495.9331931753084[0m
[37m[1m[2023-07-17 05:52:25,142][257371] Min Reward on eval: -715.3247222821694[0m
[37m[1m[2023-07-17 05:52:25,143][257371] Mean Reward across all agents: -132.46267886223765[0m
[37m[1m[2023-07-17 05:52:25,143][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:52:25,146][257371] mean_value=-416.301629825727, max_value=248.80182015223042[0m
[37m[1m[2023-07-17 05:52:25,149][257371] New mean coefficients: [[ 3.0108511  2.023518   3.969188   1.9780095 -1.6429665  6.7414994]][0m
[37m[1m[2023-07-17 05:52:25,150][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:52:34,229][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 05:52:34,230][257371] FPS: 423015.50[0m
[36m[2023-07-17 05:52:34,232][257371] itr=736, itrs=2000, Progress: 36.80%[0m
[36m[2023-07-17 05:52:46,110][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 05:52:46,110][257371] FPS: 325811.34[0m
[36m[2023-07-17 05:52:50,456][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:52:50,457][257371] Reward + Measures: [[-166.50644256    0.60105032    0.92194468    0.48739535    0.68818128
     7.74974346]][0m
[37m[1m[2023-07-17 05:52:50,457][257371] Max Reward on eval: -166.50644255587446[0m
[37m[1m[2023-07-17 05:52:50,457][257371] Min Reward on eval: -166.50644255587446[0m
[37m[1m[2023-07-17 05:52:50,458][257371] Mean Reward across all agents: -166.50644255587446[0m
[37m[1m[2023-07-17 05:52:50,458][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:52:55,733][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:52:55,733][257371] Reward + Measures: [[ -3.76258592   0.19140001   0.48789999   0.1002       0.51739997
    4.30991268]
 [111.5075963    0.3355       0.4711       0.2638       0.39809999
    4.27986717]
 [ 84.19216703   0.29070002   0.41120002   0.14749999   0.38260004
    4.1884141 ]
 ...
 [ 61.5464111    0.52890009   0.77020001   0.4377       0.75029993
    3.7999413 ]
 [ 23.66893622   0.36139998   0.40920001   0.2888       0.28230003
    4.08861113]
 [ 24.05425342   0.2509       0.37039998   0.177        0.36770001
    4.30962753]][0m
[37m[1m[2023-07-17 05:52:55,734][257371] Max Reward on eval: 296.0586393074133[0m
[37m[1m[2023-07-17 05:52:55,734][257371] Min Reward on eval: -720.5453262612224[0m
[37m[1m[2023-07-17 05:52:55,734][257371] Mean Reward across all agents: -40.981343710828206[0m
[37m[1m[2023-07-17 05:52:55,734][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:52:55,736][257371] mean_value=-645.8553820886543, max_value=86.61476982185732[0m
[37m[1m[2023-07-17 05:52:55,738][257371] New mean coefficients: [[ 3.5910335  3.4817746  3.9806964  1.4466386 -0.5063968  6.488948 ]][0m
[37m[1m[2023-07-17 05:52:55,739][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:53:04,802][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 05:53:04,802][257371] FPS: 423805.46[0m
[36m[2023-07-17 05:53:04,805][257371] itr=737, itrs=2000, Progress: 36.85%[0m
[36m[2023-07-17 05:53:16,818][257371] train() took 11.92 seconds to complete[0m
[36m[2023-07-17 05:53:16,818][257371] FPS: 322127.08[0m
[36m[2023-07-17 05:53:21,115][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:53:21,116][257371] Reward + Measures: [[-194.15217688    0.596286      0.92695141    0.46779931    0.72021401
     7.76632023]][0m
[37m[1m[2023-07-17 05:53:21,116][257371] Max Reward on eval: -194.15217687602245[0m
[37m[1m[2023-07-17 05:53:21,116][257371] Min Reward on eval: -194.15217687602245[0m
[37m[1m[2023-07-17 05:53:21,116][257371] Mean Reward across all agents: -194.15217687602245[0m
[37m[1m[2023-07-17 05:53:21,117][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:53:26,093][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:53:26,093][257371] Reward + Measures: [[ -78.80418052    0.46359998    0.26230001    0.43000004    0.48640004
     5.15236282]
 [  69.44088234    0.58180004    0.87659997    0.52179998    0.60389996
     6.45799589]
 [ -66.47996062    0.70979995    0.95550007    0.61230004    0.58700001
     5.84506845]
 ...
 [-294.84288126    0.70469999    0.12660001    0.713         0.736
     6.02550364]
 [-242.12664328    0.76710004    0.20320001    0.80900002    0.90450001
     6.11759901]
 [  -4.0439045     0.47999999    0.43020001    0.46020004    0.40899998
     4.3663497 ]][0m
[37m[1m[2023-07-17 05:53:26,094][257371] Max Reward on eval: 304.2703561719507[0m
[37m[1m[2023-07-17 05:53:26,094][257371] Min Reward on eval: -444.3590964946896[0m
[37m[1m[2023-07-17 05:53:26,094][257371] Mean Reward across all agents: -122.99230626588769[0m
[37m[1m[2023-07-17 05:53:26,094][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:53:26,097][257371] mean_value=-308.236379013497, max_value=247.17259414790087[0m
[37m[1m[2023-07-17 05:53:26,100][257371] New mean coefficients: [[ 3.5086558  3.2198322  4.607918   2.089812  -0.5528633  6.2606072]][0m
[37m[1m[2023-07-17 05:53:26,101][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:53:35,115][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 05:53:35,115][257371] FPS: 426079.52[0m
[36m[2023-07-17 05:53:35,118][257371] itr=738, itrs=2000, Progress: 36.90%[0m
[36m[2023-07-17 05:53:46,890][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 05:53:46,891][257371] FPS: 328640.98[0m
[36m[2023-07-17 05:53:51,281][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:53:51,282][257371] Reward + Measures: [[-205.44641779    0.58575934    0.93903464    0.44571099    0.76875001
     7.78420305]][0m
[37m[1m[2023-07-17 05:53:51,282][257371] Max Reward on eval: -205.4464177906893[0m
[37m[1m[2023-07-17 05:53:51,282][257371] Min Reward on eval: -205.4464177906893[0m
[37m[1m[2023-07-17 05:53:51,282][257371] Mean Reward across all agents: -205.4464177906893[0m
[37m[1m[2023-07-17 05:53:51,283][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:53:56,286][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:53:56,291][257371] Reward + Measures: [[ 312.70276213    0.32700002    0.85390007    0.53830004    0.61199999
     5.89756107]
 [-311.68607902    0.7622        0.98170006    0.14399999    0.98710006
     7.21563959]
 [ 117.51186466    0.3468        0.93460006    0.59600002    0.75529999
     6.10585594]
 ...
 [ 276.46455863    0.1982        0.98280001    0.66950005    0.89670002
     7.54825449]
 [ -42.21115256    0.36939999    0.97340006    0.41490003    0.98229998
     6.32921505]
 [ 181.53445913    0.29410002    0.91650003    0.38769999    0.92750007
     6.33234119]][0m
[37m[1m[2023-07-17 05:53:56,292][257371] Max Reward on eval: 495.0282936265692[0m
[37m[1m[2023-07-17 05:53:56,292][257371] Min Reward on eval: -650.7536697463132[0m
[37m[1m[2023-07-17 05:53:56,292][257371] Mean Reward across all agents: 65.25998910454719[0m
[37m[1m[2023-07-17 05:53:56,292][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:53:56,295][257371] mean_value=-331.8466938478229, max_value=69.74198531372662[0m
[37m[1m[2023-07-17 05:53:56,298][257371] New mean coefficients: [[ 4.3252373   3.9466448   5.594542    2.1186225  -0.00762391  5.5878124 ]][0m
[37m[1m[2023-07-17 05:53:56,299][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:54:05,332][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 05:54:05,332][257371] FPS: 425158.44[0m
[36m[2023-07-17 05:54:05,335][257371] itr=739, itrs=2000, Progress: 36.95%[0m
[36m[2023-07-17 05:54:16,937][257371] train() took 11.51 seconds to complete[0m
[36m[2023-07-17 05:54:16,938][257371] FPS: 333490.18[0m
[36m[2023-07-17 05:54:21,171][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:54:21,172][257371] Reward + Measures: [[-197.59041746    0.58281332    0.94199997    0.43390965    0.78438801
     7.79298306]][0m
[37m[1m[2023-07-17 05:54:21,172][257371] Max Reward on eval: -197.59041746220404[0m
[37m[1m[2023-07-17 05:54:21,172][257371] Min Reward on eval: -197.59041746220404[0m
[37m[1m[2023-07-17 05:54:21,173][257371] Mean Reward across all agents: -197.59041746220404[0m
[37m[1m[2023-07-17 05:54:21,173][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:54:26,231][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:54:26,231][257371] Reward + Measures: [[ 34.91432672   0.21599999   0.2289       0.20609999   0.27590001
    4.21771383]
 [ 88.47657871   0.27040002   0.31350002   0.26390001   0.37170002
    4.01186371]
 [104.1447275    0.3125       0.3707       0.2762       0.40090004
    4.06464911]
 ...
 [ 88.71885982   0.25579998   0.24120001   0.23150001   0.2793
    4.21982145]
 [115.68912411   0.41799998   0.43109998   0.36200002   0.46599999
    4.08121967]
 [ 32.00849771   0.35590002   0.4481       0.35639998   0.42700002
    5.66739273]][0m
[37m[1m[2023-07-17 05:54:26,231][257371] Max Reward on eval: 339.90935744692104[0m
[37m[1m[2023-07-17 05:54:26,232][257371] Min Reward on eval: -282.64362529013306[0m
[37m[1m[2023-07-17 05:54:26,232][257371] Mean Reward across all agents: 50.95418955479104[0m
[37m[1m[2023-07-17 05:54:26,232][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:54:26,234][257371] mean_value=-1142.8552338032432, max_value=163.50052762678996[0m
[37m[1m[2023-07-17 05:54:26,237][257371] New mean coefficients: [[ 4.5321717  3.4358788  4.923052   2.1140084 -0.8532689  6.5645165]][0m
[37m[1m[2023-07-17 05:54:26,238][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:54:35,306][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 05:54:35,306][257371] FPS: 423552.46[0m
[36m[2023-07-17 05:54:35,308][257371] itr=740, itrs=2000, Progress: 37.00%[0m
[37m[1m[2023-07-17 05:57:49,906][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000720[0m
[36m[2023-07-17 05:58:01,972][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 05:58:01,973][257371] FPS: 331652.60[0m
[36m[2023-07-17 05:58:06,228][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:58:06,228][257371] Reward + Measures: [[-209.61689349    0.62394667    0.94725466    0.433458      0.760952
     7.80225945]][0m
[37m[1m[2023-07-17 05:58:06,229][257371] Max Reward on eval: -209.6168934886891[0m
[37m[1m[2023-07-17 05:58:06,229][257371] Min Reward on eval: -209.6168934886891[0m
[37m[1m[2023-07-17 05:58:06,229][257371] Mean Reward across all agents: -209.6168934886891[0m
[37m[1m[2023-07-17 05:58:06,229][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:58:11,295][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:58:11,296][257371] Reward + Measures: [[ -50.58142573    0.69080007    0.95170003    0.65070003    0.50230002
     6.33401346]
 [ 316.20888211    0.95199996    0.94410002    0.94810003    0.0207
     7.49202442]
 [ 139.38385629    0.90480006    0.89770001    0.68470001    0.25600001
     6.17235041]
 ...
 [ -66.0578047     0.24070001    0.43259999    0.39699998    0.3836
     3.99564624]
 [  67.41014794    0.3515        0.60159999    0.48720002    0.33070001
     6.36447477]
 [-114.22894312    0.86320001    0.86569995    0.79610002    0.14660001
     7.61125946]][0m
[37m[1m[2023-07-17 05:58:11,296][257371] Max Reward on eval: 490.03736971560863[0m
[37m[1m[2023-07-17 05:58:11,296][257371] Min Reward on eval: -285.60098741827534[0m
[37m[1m[2023-07-17 05:58:11,296][257371] Mean Reward across all agents: 31.932527926495375[0m
[37m[1m[2023-07-17 05:58:11,297][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:58:11,300][257371] mean_value=-284.3289027614339, max_value=402.70712209980235[0m
[37m[1m[2023-07-17 05:58:11,303][257371] New mean coefficients: [[4.1412044 3.676864  4.705476  2.0620303 1.1043715 5.6904593]][0m
[37m[1m[2023-07-17 05:58:11,304][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:58:20,234][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 05:58:20,234][257371] FPS: 430114.57[0m
[36m[2023-07-17 05:58:20,236][257371] itr=741, itrs=2000, Progress: 37.05%[0m
[36m[2023-07-17 05:58:32,215][257371] train() took 11.89 seconds to complete[0m
[36m[2023-07-17 05:58:32,216][257371] FPS: 323040.89[0m
[36m[2023-07-17 05:58:36,481][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:58:36,487][257371] Reward + Measures: [[-175.45533019    0.58319736    0.95265639    0.43412402    0.79890931
     7.81394672]][0m
[37m[1m[2023-07-17 05:58:36,487][257371] Max Reward on eval: -175.45533018626443[0m
[37m[1m[2023-07-17 05:58:36,487][257371] Min Reward on eval: -175.45533018626443[0m
[37m[1m[2023-07-17 05:58:36,487][257371] Mean Reward across all agents: -175.45533018626443[0m
[37m[1m[2023-07-17 05:58:36,487][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:58:41,453][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:58:41,454][257371] Reward + Measures: [[  77.57818578    0.0859        0.95819998    0.95080006    0.89670008
     6.48644876]
 [ 222.37511564    0.86969995    0.95650005    0.67250001    0.35840002
     6.33705235]
 [-114.19455531    0.0011        0.98780006    0.98259991    0.99630004
     6.37818718]
 ...
 [  17.80244639    0.28350002    0.90410006    0.87449998    0.65309995
     6.07143736]
 [ 202.68864919    0.68169999    0.72180003    0.69630003    0.09949999
     5.90793514]
 [ 220.17213534    0.6494        0.97500002    0.60649997    0.59729999
     6.33564949]][0m
[37m[1m[2023-07-17 05:58:41,454][257371] Max Reward on eval: 425.0160045482218[0m
[37m[1m[2023-07-17 05:58:41,455][257371] Min Reward on eval: -478.86090943850576[0m
[37m[1m[2023-07-17 05:58:41,455][257371] Mean Reward across all agents: 21.514253530173164[0m
[37m[1m[2023-07-17 05:58:41,455][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:58:41,459][257371] mean_value=-352.8288241644912, max_value=302.9634556697834[0m
[37m[1m[2023-07-17 05:58:41,462][257371] New mean coefficients: [[3.9632359  4.279261   4.456484   2.0259702  0.80956185 5.8338695 ]][0m
[37m[1m[2023-07-17 05:58:41,463][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:58:50,577][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 05:58:50,578][257371] FPS: 421375.63[0m
[36m[2023-07-17 05:58:50,580][257371] itr=742, itrs=2000, Progress: 37.10%[0m
[36m[2023-07-17 05:59:02,428][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 05:59:02,428][257371] FPS: 326569.07[0m
[36m[2023-07-17 05:59:06,757][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:59:06,758][257371] Reward + Measures: [[-157.62683431    0.58676499    0.94722426    0.46784765    0.75536865
     7.80161142]][0m
[37m[1m[2023-07-17 05:59:06,758][257371] Max Reward on eval: -157.62683430799086[0m
[37m[1m[2023-07-17 05:59:06,758][257371] Min Reward on eval: -157.62683430799086[0m
[37m[1m[2023-07-17 05:59:06,758][257371] Mean Reward across all agents: -157.62683430799086[0m
[37m[1m[2023-07-17 05:59:06,759][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:59:11,756][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:59:11,756][257371] Reward + Measures: [[ -95.46592522    0.59779996    0.85890007    0.33679998    0.64580005
     6.82575083]
 [-401.71004487    0.97259998    0.99179995    0.            0.9946
     7.54933643]
 [ -76.33887626    0.55369997    0.83729994    0.11979999    0.81199998
     6.42783499]
 ...
 [-334.72017287    0.87740004    0.991         0.0713        0.99630004
     7.84951019]
 [-350.58721732    0.78640002    0.99160004    0.12809999    0.99449998
     7.77789164]
 [-288.97101018    0.67410004    0.98730004    0.21529999    0.98800004
     7.88089848]][0m
[37m[1m[2023-07-17 05:59:11,756][257371] Max Reward on eval: 83.04144629947841[0m
[37m[1m[2023-07-17 05:59:11,757][257371] Min Reward on eval: -782.0336532761343[0m
[37m[1m[2023-07-17 05:59:11,757][257371] Mean Reward across all agents: -315.3084175122299[0m
[37m[1m[2023-07-17 05:59:11,757][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:59:11,759][257371] mean_value=-695.7826687967288, max_value=92.20091425850865[0m
[37m[1m[2023-07-17 05:59:11,761][257371] New mean coefficients: [[4.8268747  4.7234335  5.1897826  2.7565787  0.40080595 6.4753556 ]][0m
[37m[1m[2023-07-17 05:59:11,762][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:59:20,873][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 05:59:20,873][257371] FPS: 421564.04[0m
[36m[2023-07-17 05:59:20,875][257371] itr=743, itrs=2000, Progress: 37.15%[0m
[36m[2023-07-17 05:59:32,645][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 05:59:32,645][257371] FPS: 328849.35[0m
[36m[2023-07-17 05:59:36,946][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:59:36,946][257371] Reward + Measures: [[-155.51201004    0.58575433    0.95815772    0.43673465    0.79919767
     7.8174634 ]][0m
[37m[1m[2023-07-17 05:59:36,946][257371] Max Reward on eval: -155.5120100421859[0m
[37m[1m[2023-07-17 05:59:36,946][257371] Min Reward on eval: -155.5120100421859[0m
[37m[1m[2023-07-17 05:59:36,947][257371] Mean Reward across all agents: -155.5120100421859[0m
[37m[1m[2023-07-17 05:59:36,947][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:59:41,908][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 05:59:41,908][257371] Reward + Measures: [[201.2886453    0.48460004   0.96859998   0.74169999   0.59529996
    7.37547016]
 [ 70.61859885   0.2428       0.2174       0.29389998   0.1999
    3.59103775]
 [ 18.74178139   0.31580001   0.24660002   0.3154       0.2335
    3.27558899]
 ...
 [-28.87524975   0.28740001   0.22760001   0.29250002   0.21980003
    3.19006538]
 [-52.77871537   0.26180002   0.61390001   0.42120001   0.56470001
    2.70922685]
 [-22.25816413   0.34890002   0.48740003   0.28930002   0.39750001
    3.43866587]][0m
[37m[1m[2023-07-17 05:59:41,908][257371] Max Reward on eval: 341.41230203956366[0m
[37m[1m[2023-07-17 05:59:41,909][257371] Min Reward on eval: -424.0242824390531[0m
[37m[1m[2023-07-17 05:59:41,909][257371] Mean Reward across all agents: -14.9119179961875[0m
[37m[1m[2023-07-17 05:59:41,909][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 05:59:41,911][257371] mean_value=-814.8594584039413, max_value=186.93125606421157[0m
[37m[1m[2023-07-17 05:59:41,914][257371] New mean coefficients: [[4.5305514 5.135043  5.30901   2.5635586 2.2063453 5.728153 ]][0m
[37m[1m[2023-07-17 05:59:41,915][257371] Moving the mean solution point...[0m
[36m[2023-07-17 05:59:50,977][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 05:59:50,977][257371] FPS: 423826.53[0m
[36m[2023-07-17 05:59:50,979][257371] itr=744, itrs=2000, Progress: 37.20%[0m
[36m[2023-07-17 06:00:02,735][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 06:00:02,736][257371] FPS: 329183.78[0m
[36m[2023-07-17 06:00:07,054][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:00:07,060][257371] Reward + Measures: [[-197.04817213    0.6018303     0.96833193    0.38981566    0.84847933
     7.83702517]][0m
[37m[1m[2023-07-17 06:00:07,060][257371] Max Reward on eval: -197.04817213168346[0m
[37m[1m[2023-07-17 06:00:07,060][257371] Min Reward on eval: -197.04817213168346[0m
[37m[1m[2023-07-17 06:00:07,060][257371] Mean Reward across all agents: -197.04817213168346[0m
[37m[1m[2023-07-17 06:00:07,061][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:00:12,310][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:00:12,316][257371] Reward + Measures: [[ 444.93603755    0.17730001    0.7682001     0.44829997    0.74200004
     6.09225607]
 [  47.30408188    0.35859999    0.65950006    0.46420002    0.45560002
     6.2560401 ]
 [-153.29584738    0.47800002    0.61149997    0.54619998    0.2102
     5.4031806 ]
 ...
 [   8.95335873    0.24080001    0.3673        0.23540001    0.29210001
     5.13179159]
 [-103.65933362    0.47170004    0.47939998    0.20539999    0.37410003
     5.43552732]
 [ -85.70107074    0.51599997    0.46370003    0.27739999    0.32890001
     5.50034094]][0m
[37m[1m[2023-07-17 06:00:12,316][257371] Max Reward on eval: 681.4943246488226[0m
[37m[1m[2023-07-17 06:00:12,317][257371] Min Reward on eval: -578.6460456827656[0m
[37m[1m[2023-07-17 06:00:12,317][257371] Mean Reward across all agents: -72.50060271957038[0m
[37m[1m[2023-07-17 06:00:12,317][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:00:12,319][257371] mean_value=-428.89496723253365, max_value=292.56028843680576[0m
[37m[1m[2023-07-17 06:00:12,322][257371] New mean coefficients: [[3.9697454 4.7229176 5.134983  2.5669978 1.1460615 5.584139 ]][0m
[37m[1m[2023-07-17 06:00:12,323][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:00:21,431][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 06:00:21,431][257371] FPS: 421671.73[0m
[36m[2023-07-17 06:00:21,433][257371] itr=745, itrs=2000, Progress: 37.25%[0m
[36m[2023-07-17 06:00:33,240][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 06:00:33,240][257371] FPS: 327701.99[0m
[36m[2023-07-17 06:00:37,565][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:00:37,571][257371] Reward + Measures: [[-183.31774097    0.56042469    0.9630174     0.41697401    0.84806168
     7.83354044]][0m
[37m[1m[2023-07-17 06:00:37,571][257371] Max Reward on eval: -183.31774096555807[0m
[37m[1m[2023-07-17 06:00:37,571][257371] Min Reward on eval: -183.31774096555807[0m
[37m[1m[2023-07-17 06:00:37,572][257371] Mean Reward across all agents: -183.31774096555807[0m
[37m[1m[2023-07-17 06:00:37,572][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:00:42,532][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:00:42,532][257371] Reward + Measures: [[328.44612787   0.48990002   0.89800006   0.70649999   0.4463
    5.50468779]
 [198.34418345   0.75620002   0.67009997   0.704        0.0491
    4.94654131]
 [292.22602654   0.78120005   0.69989997   0.72069997   0.0487
    5.78694534]
 ...
 [166.02765913   0.4691       0.56639999   0.29819998   0.27719998
    5.44642496]
 [143.23813631   0.74849999   0.70620006   0.70360005   0.0697
    4.81390619]
 [271.34295083   0.78189999   0.86780006   0.85000002   0.14070001
    6.32964897]][0m
[37m[1m[2023-07-17 06:00:42,532][257371] Max Reward on eval: 570.5044136058539[0m
[37m[1m[2023-07-17 06:00:42,533][257371] Min Reward on eval: -100.18086624434218[0m
[37m[1m[2023-07-17 06:00:42,533][257371] Mean Reward across all agents: 180.75576657039267[0m
[37m[1m[2023-07-17 06:00:42,533][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:00:42,536][257371] mean_value=-267.3380563971596, max_value=190.893533072418[0m
[37m[1m[2023-07-17 06:00:42,539][257371] New mean coefficients: [[3.7975862  4.5962586  4.815503   2.3027918  0.81737566 5.6394515 ]][0m
[37m[1m[2023-07-17 06:00:42,540][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:00:51,506][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 06:00:51,506][257371] FPS: 428351.94[0m
[36m[2023-07-17 06:00:51,509][257371] itr=746, itrs=2000, Progress: 37.30%[0m
[36m[2023-07-17 06:01:03,217][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 06:01:03,217][257371] FPS: 330611.24[0m
[36m[2023-07-17 06:01:07,464][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:01:07,465][257371] Reward + Measures: [[-291.28497641    0.63744599    0.97863734    0.29981333    0.92786562
     7.86631107]][0m
[37m[1m[2023-07-17 06:01:07,465][257371] Max Reward on eval: -291.2849764052261[0m
[37m[1m[2023-07-17 06:01:07,465][257371] Min Reward on eval: -291.2849764052261[0m
[37m[1m[2023-07-17 06:01:07,466][257371] Mean Reward across all agents: -291.2849764052261[0m
[37m[1m[2023-07-17 06:01:07,466][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:01:12,409][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:01:12,409][257371] Reward + Measures: [[319.61150645   0.40019998   0.69029999   0.2969       0.64490002
    6.12604284]
 [129.67197569   0.47119999   0.29410002   0.28029999   0.40549999
    4.97751474]
 [365.81544094   0.58579999   0.5474       0.13440001   0.5406
    4.9190073 ]
 ...
 [243.06148835   0.51480001   0.38460001   0.25650001   0.46599999
    4.82508278]
 [393.11456201   0.56440002   0.6979       0.20219998   0.59910005
    5.01998234]
 [ 47.42228802   0.4765       0.19129999   0.41150004   0.42420003
    4.84790802]][0m
[37m[1m[2023-07-17 06:01:12,410][257371] Max Reward on eval: 783.1020164223853[0m
[37m[1m[2023-07-17 06:01:12,410][257371] Min Reward on eval: -462.4391671910882[0m
[37m[1m[2023-07-17 06:01:12,410][257371] Mean Reward across all agents: 191.73233954699643[0m
[37m[1m[2023-07-17 06:01:12,410][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:01:12,418][257371] mean_value=-122.2086669476306, max_value=684.3653145947687[0m
[37m[1m[2023-07-17 06:01:12,421][257371] New mean coefficients: [[3.7195432  4.235783   4.188131   1.8804207  0.34307718 5.974308  ]][0m
[37m[1m[2023-07-17 06:01:12,422][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:01:21,453][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 06:01:21,453][257371] FPS: 425271.13[0m
[36m[2023-07-17 06:01:21,455][257371] itr=747, itrs=2000, Progress: 37.35%[0m
[36m[2023-07-17 06:01:33,052][257371] train() took 11.50 seconds to complete[0m
[36m[2023-07-17 06:01:33,052][257371] FPS: 333780.64[0m
[36m[2023-07-17 06:01:37,354][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:01:37,359][257371] Reward + Measures: [[-248.60445998    0.61118764    0.97060537    0.34977132    0.8861627
     7.85260916]][0m
[37m[1m[2023-07-17 06:01:37,360][257371] Max Reward on eval: -248.60445998308245[0m
[37m[1m[2023-07-17 06:01:37,360][257371] Min Reward on eval: -248.60445998308245[0m
[37m[1m[2023-07-17 06:01:37,360][257371] Mean Reward across all agents: -248.60445998308245[0m
[37m[1m[2023-07-17 06:01:37,361][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:01:42,316][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:01:42,316][257371] Reward + Measures: [[-340.47024154    0.40809998    0.60690004    0.17550001    0.59330004
     5.89417219]
 [-715.65876387    0.97670001    0.98619998    0.0035        0.99260008
     7.38403463]
 [-755.29801176    0.97920001    0.98839998    0.0035        0.99410003
     6.90435791]
 ...
 [-287.53319737    0.67870003    0.98390007    0.22129999    0.98579997
     7.80001926]
 [-270.91873768    0.66379994    0.88149995    0.2395        0.81459999
     6.69632959]
 [-359.42663957    0.75860006    0.97150004    0.16110002    0.97970009
     6.80775166]][0m
[37m[1m[2023-07-17 06:01:42,317][257371] Max Reward on eval: 209.34451007470489[0m
[37m[1m[2023-07-17 06:01:42,317][257371] Min Reward on eval: -820.7820701694116[0m
[37m[1m[2023-07-17 06:01:42,317][257371] Mean Reward across all agents: -482.3719315148601[0m
[37m[1m[2023-07-17 06:01:42,317][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:01:42,319][257371] mean_value=-792.977687333203, max_value=157.46265195594933[0m
[37m[1m[2023-07-17 06:01:42,321][257371] New mean coefficients: [[3.1529386 4.3068104 4.248353  1.8638784 2.1291142 5.151344 ]][0m
[37m[1m[2023-07-17 06:01:42,322][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:01:51,279][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 06:01:51,279][257371] FPS: 428825.91[0m
[36m[2023-07-17 06:01:51,281][257371] itr=748, itrs=2000, Progress: 37.40%[0m
[36m[2023-07-17 06:02:03,090][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 06:02:03,091][257371] FPS: 327731.11[0m
[36m[2023-07-17 06:02:07,390][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:02:07,391][257371] Reward + Measures: [[-246.27457772    0.59984165    0.98201656    0.31298131    0.94784766
     7.88036585]][0m
[37m[1m[2023-07-17 06:02:07,391][257371] Max Reward on eval: -246.2745777171198[0m
[37m[1m[2023-07-17 06:02:07,391][257371] Min Reward on eval: -246.2745777171198[0m
[37m[1m[2023-07-17 06:02:07,391][257371] Mean Reward across all agents: -246.2745777171198[0m
[37m[1m[2023-07-17 06:02:07,392][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:02:12,411][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:02:12,411][257371] Reward + Measures: [[359.01519037   0.186        0.96060002   0.57620001   0.95050001
    6.19389486]
 [ 55.41258757   0.48370001   0.85210001   0.78260005   0.50649995
    5.02523851]
 [ 74.82402892   0.3524       0.94230002   0.6038       0.67559999
    6.41984797]
 ...
 [135.23425367   0.75760001   0.94450009   0.46610004   0.94260007
    5.34804487]
 [ 45.21343633   0.66970003   0.354        0.63669997   0.58960003
    5.94015312]
 [-16.82310149   0.2904       0.54640001   0.1529       0.47100002
    4.9004426 ]][0m
[37m[1m[2023-07-17 06:02:12,411][257371] Max Reward on eval: 468.30852126567623[0m
[37m[1m[2023-07-17 06:02:12,412][257371] Min Reward on eval: -465.62148286653684[0m
[37m[1m[2023-07-17 06:02:12,412][257371] Mean Reward across all agents: 74.90572626040736[0m
[37m[1m[2023-07-17 06:02:12,412][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:02:12,417][257371] mean_value=-245.8064328958132, max_value=130.51606223952442[0m
[37m[1m[2023-07-17 06:02:12,419][257371] New mean coefficients: [[4.653606  4.561005  4.875769  2.0889695 1.7512375 5.8112326]][0m
[37m[1m[2023-07-17 06:02:12,420][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:02:21,406][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 06:02:21,406][257371] FPS: 427433.42[0m
[36m[2023-07-17 06:02:21,408][257371] itr=749, itrs=2000, Progress: 37.45%[0m
[36m[2023-07-17 06:02:33,268][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 06:02:33,268][257371] FPS: 326359.79[0m
[36m[2023-07-17 06:02:37,541][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:02:37,541][257371] Reward + Measures: [[-423.25219465    0.75440073    0.98931867    0.16956766    0.98159766
     7.89392424]][0m
[37m[1m[2023-07-17 06:02:37,541][257371] Max Reward on eval: -423.25219464840944[0m
[37m[1m[2023-07-17 06:02:37,541][257371] Min Reward on eval: -423.25219464840944[0m
[37m[1m[2023-07-17 06:02:37,542][257371] Mean Reward across all agents: -423.25219464840944[0m
[37m[1m[2023-07-17 06:02:37,542][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:02:42,779][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:02:42,785][257371] Reward + Measures: [[ -19.63818233    0.34800002    0.29519999    0.33800003    0.1744
     4.37525654]
 [ -38.27386899    0.54860002    0.43959999    0.45249996    0.17739999
     4.32572889]
 [ -52.3182924     0.78739995    0.78439999    0.76500005    0.11129999
     4.74871397]
 ...
 [  81.78993117    0.47319999    0.6322        0.4172        0.32609999
     5.15696335]
 [-150.92170039    0.7281        0.74760002    0.71520001    0.0953
     5.35274458]
 [ -23.82557786    0.34259999    0.26610002    0.28          0.21970001
     4.30585098]][0m
[37m[1m[2023-07-17 06:02:42,785][257371] Max Reward on eval: 306.20577286910265[0m
[37m[1m[2023-07-17 06:02:42,785][257371] Min Reward on eval: -404.49367665573953[0m
[37m[1m[2023-07-17 06:02:42,786][257371] Mean Reward across all agents: 1.0751897638369503[0m
[37m[1m[2023-07-17 06:02:42,786][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:02:42,788][257371] mean_value=-347.36914156907955, max_value=53.52375303746504[0m
[37m[1m[2023-07-17 06:02:42,791][257371] New mean coefficients: [[5.5815125 4.46929   5.3047    2.3321223 1.6791675 6.4948783]][0m
[37m[1m[2023-07-17 06:02:42,791][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:02:51,886][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 06:02:51,887][257371] FPS: 422282.79[0m
[36m[2023-07-17 06:02:51,889][257371] itr=750, itrs=2000, Progress: 37.50%[0m
[37m[1m[2023-07-17 06:05:58,816][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000730[0m
[36m[2023-07-17 06:06:10,949][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 06:06:10,949][257371] FPS: 330729.47[0m
[36m[2023-07-17 06:06:15,269][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:06:15,269][257371] Reward + Measures: [[-414.96996988    0.76486927    0.98758268    0.15417433    0.98659831
     7.89085722]][0m
[37m[1m[2023-07-17 06:06:15,269][257371] Max Reward on eval: -414.96996988216847[0m
[37m[1m[2023-07-17 06:06:15,269][257371] Min Reward on eval: -414.96996988216847[0m
[37m[1m[2023-07-17 06:06:15,270][257371] Mean Reward across all agents: -414.96996988216847[0m
[37m[1m[2023-07-17 06:06:15,270][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:06:20,247][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:06:20,253][257371] Reward + Measures: [[ -93.88460363    0.2976        0.34640002    0.28199998    0.39899999
     4.04561472]
 [-367.77135136    0.6591        0.76270002    0.1122        0.76110005
     4.87424231]
 [ -54.53268129    0.22980002    0.25600001    0.21890001    0.18350001
     4.60933876]
 ...
 [ -95.75123539    0.52289999    0.5151        0.48599997    0.36019999
     4.0728488 ]
 [-147.86138411    0.28439999    0.35050002    0.25320002    0.33570001
     4.8765254 ]
 [-134.78080612    0.31220001    0.39129999    0.29999998    0.40899998
     3.72052836]][0m
[37m[1m[2023-07-17 06:06:20,253][257371] Max Reward on eval: 190.59376714527608[0m
[37m[1m[2023-07-17 06:06:20,254][257371] Min Reward on eval: -609.7038612699137[0m
[37m[1m[2023-07-17 06:06:20,254][257371] Mean Reward across all agents: -62.94625146506953[0m
[37m[1m[2023-07-17 06:06:20,254][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:06:20,256][257371] mean_value=-809.0857954533835, max_value=303.56398746542015[0m
[37m[1m[2023-07-17 06:06:20,259][257371] New mean coefficients: [[5.0792437 4.656116  4.5323825 1.2507255 1.1832621 6.0964146]][0m
[37m[1m[2023-07-17 06:06:20,260][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:06:29,108][257371] train() took 8.85 seconds to complete[0m
[36m[2023-07-17 06:06:29,108][257371] FPS: 434056.00[0m
[36m[2023-07-17 06:06:29,111][257371] itr=751, itrs=2000, Progress: 37.55%[0m
[36m[2023-07-17 06:06:41,040][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-17 06:06:41,040][257371] FPS: 324396.51[0m
[36m[2023-07-17 06:06:45,246][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:06:45,246][257371] Reward + Measures: [[-350.56541131    0.69591224    0.98202699    0.23869434    0.95081067
     7.86861992]][0m
[37m[1m[2023-07-17 06:06:45,246][257371] Max Reward on eval: -350.56541131362087[0m
[37m[1m[2023-07-17 06:06:45,247][257371] Min Reward on eval: -350.56541131362087[0m
[37m[1m[2023-07-17 06:06:45,247][257371] Mean Reward across all agents: -350.56541131362087[0m
[37m[1m[2023-07-17 06:06:45,247][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:06:50,270][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:06:50,276][257371] Reward + Measures: [[ 46.23373271   0.9698       0.97139996   0.96680003   0.96950001
    7.04672861]
 [-74.90854411   0.51169997   0.52809995   0.5169       0.53109998
    6.11196089]
 [148.88629374   0.82800001   0.81120008   0.81399995   0.82300007
    5.68182707]
 ...
 [-27.75960992   0.5007       0.54030001   0.50220007   0.51230001
    6.2589426 ]
 [ -4.23523884   0.79720002   0.70789999   0.77490008   0.7723
    6.20123243]
 [-19.95900335   0.50850004   0.50839996   0.56129998   0.55439997
    5.94111633]][0m
[37m[1m[2023-07-17 06:06:50,276][257371] Max Reward on eval: 196.52705300021915[0m
[37m[1m[2023-07-17 06:06:50,277][257371] Min Reward on eval: -455.10163473375144[0m
[37m[1m[2023-07-17 06:06:50,277][257371] Mean Reward across all agents: -10.593699550591369[0m
[37m[1m[2023-07-17 06:06:50,277][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:06:50,280][257371] mean_value=-179.9706433174545, max_value=346.44877500903146[0m
[37m[1m[2023-07-17 06:06:50,282][257371] New mean coefficients: [[4.6388855 4.3561926 4.8568864 1.8367131 1.6775808 6.048144 ]][0m
[37m[1m[2023-07-17 06:06:50,283][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:06:59,246][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 06:06:59,246][257371] FPS: 428511.89[0m
[36m[2023-07-17 06:06:59,249][257371] itr=752, itrs=2000, Progress: 37.60%[0m
[36m[2023-07-17 06:07:10,982][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 06:07:10,983][257371] FPS: 329763.75[0m
[36m[2023-07-17 06:07:15,305][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:07:15,305][257371] Reward + Measures: [[-326.56608556    0.63427401    0.978764      0.28469801    0.94954294
     7.86060524]][0m
[37m[1m[2023-07-17 06:07:15,305][257371] Max Reward on eval: -326.56608556034433[0m
[37m[1m[2023-07-17 06:07:15,305][257371] Min Reward on eval: -326.56608556034433[0m
[37m[1m[2023-07-17 06:07:15,306][257371] Mean Reward across all agents: -326.56608556034433[0m
[37m[1m[2023-07-17 06:07:15,306][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:07:20,312][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:07:20,323][257371] Reward + Measures: [[ -89.81881549    0.89890003    0.94859999    0.0448        0.93530005
     7.46075439]
 [  20.87637743    0.88700002    0.8872        0.1085        0.80370009
     7.04480457]
 [-262.44265939    0.94709998    0.98509997    0.0023        0.98640007
     6.96110106]
 ...
 [-125.82772923    0.9382        0.98339999    0.0022        0.9867
     6.95089483]
 [-182.01358414    0.92930001    0.97959995    0.006         0.9835
     6.84893656]
 [ -51.39378856    0.44580004    0.9601        0.27399999    0.97780001
     7.10057163]][0m
[37m[1m[2023-07-17 06:07:20,324][257371] Max Reward on eval: 169.62023543473333[0m
[37m[1m[2023-07-17 06:07:20,324][257371] Min Reward on eval: -491.15599822532386[0m
[37m[1m[2023-07-17 06:07:20,324][257371] Mean Reward across all agents: -101.88432646677273[0m
[37m[1m[2023-07-17 06:07:20,325][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:07:20,326][257371] mean_value=-416.54457111191846, max_value=239.90192729861025[0m
[37m[1m[2023-07-17 06:07:20,329][257371] New mean coefficients: [[4.8065925 4.7000155 5.573389  2.0501885 2.6667075 5.235815 ]][0m
[37m[1m[2023-07-17 06:07:20,330][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:07:29,363][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 06:07:29,363][257371] FPS: 425194.76[0m
[36m[2023-07-17 06:07:29,365][257371] itr=753, itrs=2000, Progress: 37.65%[0m
[36m[2023-07-17 06:07:41,322][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-17 06:07:41,322][257371] FPS: 323650.44[0m
[36m[2023-07-17 06:07:45,603][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:07:45,604][257371] Reward + Measures: [[-382.28656336    0.71126902    0.98648298    0.19873734    0.9827376
     7.88046455]][0m
[37m[1m[2023-07-17 06:07:45,604][257371] Max Reward on eval: -382.2865633550701[0m
[37m[1m[2023-07-17 06:07:45,604][257371] Min Reward on eval: -382.2865633550701[0m
[37m[1m[2023-07-17 06:07:45,604][257371] Mean Reward across all agents: -382.2865633550701[0m
[37m[1m[2023-07-17 06:07:45,605][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:07:50,602][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:07:50,608][257371] Reward + Measures: [[-24.86235371   0.27369997   0.22350001   0.21360002   0.2131
    3.21455073]
 [248.22416493   0.36749998   0.82930005   0.27480003   0.85939997
    4.51118469]
 [-47.98391974   0.2773       0.26019999   0.221        0.30270001
    3.6158154 ]
 ...
 [-66.74466021   0.5223       0.49729997   0.40279999   0.23280001
    3.57199478]
 [-41.07941897   0.33600003   0.30990002   0.27729997   0.27360001
    3.49729133]
 [ 12.8764115    0.31529999   0.26839998   0.2467       0.2376
    3.28940082]][0m
[37m[1m[2023-07-17 06:07:50,608][257371] Max Reward on eval: 593.5332641497255[0m
[37m[1m[2023-07-17 06:07:50,609][257371] Min Reward on eval: -381.5303688259795[0m
[37m[1m[2023-07-17 06:07:50,609][257371] Mean Reward across all agents: -19.965852716785374[0m
[37m[1m[2023-07-17 06:07:50,609][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:07:50,611][257371] mean_value=-1629.054755151751, max_value=259.43824338587376[0m
[37m[1m[2023-07-17 06:07:50,614][257371] New mean coefficients: [[4.314555  4.605192  5.013052  1.6088424 3.3062136 4.946681 ]][0m
[37m[1m[2023-07-17 06:07:50,615][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:07:59,575][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 06:07:59,576][257371] FPS: 428616.19[0m
[36m[2023-07-17 06:07:59,578][257371] itr=754, itrs=2000, Progress: 37.70%[0m
[36m[2023-07-17 06:08:11,377][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 06:08:11,377][257371] FPS: 327970.91[0m
[36m[2023-07-17 06:08:15,693][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:08:15,694][257371] Reward + Measures: [[-462.07418545    0.80345005    0.99008334    0.126825      0.98965329
     7.89203358]][0m
[37m[1m[2023-07-17 06:08:15,694][257371] Max Reward on eval: -462.0741854534983[0m
[37m[1m[2023-07-17 06:08:15,694][257371] Min Reward on eval: -462.0741854534983[0m
[37m[1m[2023-07-17 06:08:15,694][257371] Mean Reward across all agents: -462.0741854534983[0m
[37m[1m[2023-07-17 06:08:15,695][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:08:20,712][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:08:20,713][257371] Reward + Measures: [[-54.8230193    0.35710001   0.5697       0.37170002   0.4894
    5.58037519]
 [  6.93366152   0.72250003   0.81199998   0.63730001   0.73940003
    5.25065613]
 [-27.95361029   0.86680001   0.95020002   0.0499       0.9472
    6.71912336]
 ...
 [120.64762356   0.69750005   0.88430005   0.1705       0.87220001
    7.21806479]
 [169.68177272   0.85030001   0.93620008   0.06460001   0.9271
    6.80559778]
 [-15.9193087    0.83710003   0.99080002   0.0665       0.99360001
    6.16228342]][0m
[37m[1m[2023-07-17 06:08:20,713][257371] Max Reward on eval: 381.72523116394876[0m
[37m[1m[2023-07-17 06:08:20,713][257371] Min Reward on eval: -490.07126616872847[0m
[37m[1m[2023-07-17 06:08:20,713][257371] Mean Reward across all agents: -8.933112433905421[0m
[37m[1m[2023-07-17 06:08:20,714][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:08:20,716][257371] mean_value=-314.111767051728, max_value=96.90874732970326[0m
[37m[1m[2023-07-17 06:08:20,719][257371] New mean coefficients: [[5.3773355 4.888109  5.1186967 2.0421376 3.6257632 5.9547515]][0m
[37m[1m[2023-07-17 06:08:20,720][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:08:29,795][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 06:08:29,796][257371] FPS: 423178.72[0m
[36m[2023-07-17 06:08:29,798][257371] itr=755, itrs=2000, Progress: 37.75%[0m
[36m[2023-07-17 06:08:41,578][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 06:08:41,578][257371] FPS: 328531.59[0m
[36m[2023-07-17 06:08:45,914][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:08:45,914][257371] Reward + Measures: [[-457.17494819    0.79542464    0.99027562    0.13238101    0.99109066
     7.89375401]][0m
[37m[1m[2023-07-17 06:08:45,915][257371] Max Reward on eval: -457.1749481915453[0m
[37m[1m[2023-07-17 06:08:45,915][257371] Min Reward on eval: -457.1749481915453[0m
[37m[1m[2023-07-17 06:08:45,915][257371] Mean Reward across all agents: -457.1749481915453[0m
[37m[1m[2023-07-17 06:08:45,916][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:08:50,946][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:08:50,953][257371] Reward + Measures: [[ 61.84467042   0.2888       0.3285       0.28430003   0.3021
    4.42077541]
 [103.03150092   0.28150001   0.33610001   0.28299999   0.35280001
    4.05149937]
 [ 27.56201757   0.82490009   0.6552       0.82840008   0.2651
    6.96242476]
 ...
 [ 19.96656858   0.40559998   0.19680001   0.41929999   0.46479997
    5.41245222]
 [ 14.92596294   0.35610002   0.39840004   0.3089       0.3809
    5.10280752]
 [ 80.05906536   0.1908       0.25049999   0.19920002   0.25819999
    4.78181219]][0m
[37m[1m[2023-07-17 06:08:50,953][257371] Max Reward on eval: 545.9536476191133[0m
[37m[1m[2023-07-17 06:08:50,954][257371] Min Reward on eval: -483.6764130823314[0m
[37m[1m[2023-07-17 06:08:50,955][257371] Mean Reward across all agents: 70.55492870731398[0m
[37m[1m[2023-07-17 06:08:50,955][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:08:50,964][257371] mean_value=-266.8596379574574, max_value=703.7633485595322[0m
[37m[1m[2023-07-17 06:08:50,968][257371] New mean coefficients: [[5.6083508 4.603581  5.386099  2.7254477 1.8259698 6.7870054]][0m
[37m[1m[2023-07-17 06:08:50,970][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:09:00,026][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 06:09:00,026][257371] FPS: 424112.15[0m
[36m[2023-07-17 06:09:00,028][257371] itr=756, itrs=2000, Progress: 37.80%[0m
[36m[2023-07-17 06:09:11,629][257371] train() took 11.51 seconds to complete[0m
[36m[2023-07-17 06:09:11,629][257371] FPS: 333519.94[0m
[36m[2023-07-17 06:09:15,934][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:09:15,934][257371] Reward + Measures: [[-362.94884251    0.70092332    0.98741031    0.204438      0.98727304
     7.89478636]][0m
[37m[1m[2023-07-17 06:09:15,935][257371] Max Reward on eval: -362.94884251085966[0m
[37m[1m[2023-07-17 06:09:15,935][257371] Min Reward on eval: -362.94884251085966[0m
[37m[1m[2023-07-17 06:09:15,935][257371] Mean Reward across all agents: -362.94884251085966[0m
[37m[1m[2023-07-17 06:09:15,935][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:09:20,918][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:09:20,924][257371] Reward + Measures: [[-159.79414886    0.58390003    0.87309998    0.33130002    0.82629997
     7.75338984]
 [-418.69931885    0.52240002    0.67559999    0.23570001    0.6322
     7.4756813 ]
 [-159.04249304    0.22909999    0.72220004    0.49869999    0.71250004
     7.54812574]
 ...
 [-178.09689767    0.5377        0.85039997    0.43920001    0.72359997
     7.7203064 ]
 [-119.82479732    0.75270003    0.82749999    0.4526        0.49869999
     7.76480579]
 [-224.06466024    0.4384        0.86900008    0.50800002    0.76440001
     7.48836088]][0m
[37m[1m[2023-07-17 06:09:20,925][257371] Max Reward on eval: 178.34387504811167[0m
[37m[1m[2023-07-17 06:09:20,925][257371] Min Reward on eval: -695.1159095862415[0m
[37m[1m[2023-07-17 06:09:20,926][257371] Mean Reward across all agents: -230.9851832224342[0m
[37m[1m[2023-07-17 06:09:20,926][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:09:20,938][257371] mean_value=-375.2518036308532, max_value=298.1909500606466[0m
[37m[1m[2023-07-17 06:09:20,942][257371] New mean coefficients: [[6.645441  5.74672   7.0144076 3.8785062 2.7259784 6.553017 ]][0m
[37m[1m[2023-07-17 06:09:20,943][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:09:29,988][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 06:09:29,988][257371] FPS: 424649.37[0m
[36m[2023-07-17 06:09:29,991][257371] itr=757, itrs=2000, Progress: 37.85%[0m
[36m[2023-07-17 06:09:41,849][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 06:09:41,849][257371] FPS: 326270.86[0m
[36m[2023-07-17 06:09:46,249][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:09:46,250][257371] Reward + Measures: [[-314.06884648    0.66394603    0.98662502    0.22997668    0.98684734
     7.90115118]][0m
[37m[1m[2023-07-17 06:09:46,250][257371] Max Reward on eval: -314.0688464821911[0m
[37m[1m[2023-07-17 06:09:46,250][257371] Min Reward on eval: -314.0688464821911[0m
[37m[1m[2023-07-17 06:09:46,251][257371] Mean Reward across all agents: -314.0688464821911[0m
[37m[1m[2023-07-17 06:09:46,251][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:09:51,540][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:09:51,540][257371] Reward + Measures: [[-459.31432248    0.78890002    0.84840006    0.05290001    0.79580003
     6.17607403]
 [  22.53803253    0.51319999    0.59770006    0.52880001    0.53100002
     4.13559675]
 [  31.56949213    0.50030005    0.57690001    0.50230002    0.51770002
     4.37745428]
 ...
 [ -22.06706067    0.21000002    0.31110001    0.10350001    0.22590001
     5.61903763]
 [ -83.93351412    0.3906        0.64740002    0.38260001    0.44189999
     3.91320205]
 [ -48.28989981    0.60689992    0.74739999    0.42700002    0.67870003
     5.59754658]][0m
[37m[1m[2023-07-17 06:09:51,541][257371] Max Reward on eval: 167.39197877440603[0m
[37m[1m[2023-07-17 06:09:51,541][257371] Min Reward on eval: -517.9065454068593[0m
[37m[1m[2023-07-17 06:09:51,541][257371] Mean Reward across all agents: -53.28195285766113[0m
[37m[1m[2023-07-17 06:09:51,541][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:09:51,544][257371] mean_value=-445.2560361306597, max_value=-62.8109836101159[0m
[36m[2023-07-17 06:09:51,547][257371] XNES is restarting with a new solution whose measures are [0.1059     0.96340007 0.74130005 0.89139998 3.8430779 ] and objective is 339.3350658387877[0m
[36m[2023-07-17 06:09:51,548][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 06:09:51,550][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 06:09:51,551][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:10:00,533][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 06:10:00,533][257371] FPS: 427610.72[0m
[36m[2023-07-17 06:10:00,535][257371] itr=758, itrs=2000, Progress: 37.90%[0m
[36m[2023-07-17 06:10:12,406][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 06:10:12,407][257371] FPS: 325954.15[0m
[36m[2023-07-17 06:10:16,810][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:10:16,811][257371] Reward + Measures: [[146.40008035   0.41486865   0.6716423    0.47206396   0.4740127
    5.35824347]][0m
[37m[1m[2023-07-17 06:10:16,811][257371] Max Reward on eval: 146.40008034762022[0m
[37m[1m[2023-07-17 06:10:16,811][257371] Min Reward on eval: 146.40008034762022[0m
[37m[1m[2023-07-17 06:10:16,812][257371] Mean Reward across all agents: 146.40008034762022[0m
[37m[1m[2023-07-17 06:10:16,812][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:10:21,853][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:10:21,854][257371] Reward + Measures: [[-17.37252533   0.2809       0.2854       0.2608       0.28730002
    4.79633665]
 [ -5.50325918   0.87540001   0.87350005   0.86910003   0.87589997
    7.68030882]
 [ 34.87182504   0.20710002   0.2026       0.0997       0.23990002
    5.63962317]
 ...
 [  8.34837992   0.2746       0.60659999   0.14210001   0.63690001
    5.3513093 ]
 [148.0036695    0.29499999   0.35099998   0.1596       0.34900001
    4.47192335]
 [ -6.43778133   0.10649999   0.0984       0.09150001   0.1249
    5.56781769]][0m
[37m[1m[2023-07-17 06:10:21,854][257371] Max Reward on eval: 472.1829035503324[0m
[37m[1m[2023-07-17 06:10:21,855][257371] Min Reward on eval: -254.00812150265557[0m
[37m[1m[2023-07-17 06:10:21,855][257371] Mean Reward across all agents: 15.64597971363681[0m
[37m[1m[2023-07-17 06:10:21,855][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:10:21,857][257371] mean_value=-364.25482693655675, max_value=507.99118669015826[0m
[37m[1m[2023-07-17 06:10:21,860][257371] New mean coefficients: [[ 1.1270312  -0.2970359  -0.18542826 -2.799931   -0.5151378  -0.17140925]][0m
[37m[1m[2023-07-17 06:10:21,861][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:10:31,062][257371] train() took 9.20 seconds to complete[0m
[36m[2023-07-17 06:10:31,063][257371] FPS: 417389.81[0m
[36m[2023-07-17 06:10:31,065][257371] itr=759, itrs=2000, Progress: 37.95%[0m
[36m[2023-07-17 06:10:42,931][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 06:10:42,932][257371] FPS: 326120.29[0m
[36m[2023-07-17 06:10:47,228][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:10:47,229][257371] Reward + Measures: [[-339.66894377    0.64343268    0.66395402    0.06264466    0.64993936
     6.14847183]][0m
[37m[1m[2023-07-17 06:10:47,229][257371] Max Reward on eval: -339.6689437670319[0m
[37m[1m[2023-07-17 06:10:47,229][257371] Min Reward on eval: -339.6689437670319[0m
[37m[1m[2023-07-17 06:10:47,229][257371] Mean Reward across all agents: -339.6689437670319[0m
[37m[1m[2023-07-17 06:10:47,230][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:10:52,226][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:10:52,278][257371] Reward + Measures: [[-32.14846758   0.11610001   0.11130001   0.12609999   0.13249999
    6.08263159]
 [ 28.59034902   0.20729999   0.1548       0.17279999   0.198
    5.2874732 ]
 [ -5.0108942    0.28840002   0.2879       0.28859997   0.31930003
    5.54643202]
 ...
 [-64.26899789   0.20610002   0.2404       0.19340001   0.2493
    5.55445623]
 [-34.32686052   0.14210001   0.1073       0.07919999   0.11130001
    6.56951237]
 [103.33636549   0.78780001   0.8179       0.84299994   0.81770003
    6.74109364]][0m
[37m[1m[2023-07-17 06:10:52,278][257371] Max Reward on eval: 432.68165302611885[0m
[37m[1m[2023-07-17 06:10:52,279][257371] Min Reward on eval: -529.7383194020251[0m
[37m[1m[2023-07-17 06:10:52,279][257371] Mean Reward across all agents: 4.292033806826148[0m
[37m[1m[2023-07-17 06:10:52,279][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:10:52,281][257371] mean_value=-349.00487156236665, max_value=192.17837837189282[0m
[37m[1m[2023-07-17 06:10:52,283][257371] New mean coefficients: [[ 2.3152804  -0.00791845  0.23597774 -1.7269573   0.08531713  0.3850205 ]][0m
[37m[1m[2023-07-17 06:10:52,284][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:11:01,288][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 06:11:01,288][257371] FPS: 426562.63[0m
[36m[2023-07-17 06:11:01,290][257371] itr=760, itrs=2000, Progress: 38.00%[0m
[37m[1m[2023-07-17 06:14:12,289][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000740[0m
[36m[2023-07-17 06:14:24,566][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 06:14:24,566][257371] FPS: 328634.11[0m
[36m[2023-07-17 06:14:28,778][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:14:28,779][257371] Reward + Measures: [[16.68397668  0.17398934  0.20221166  0.13929132  0.18253434  4.9727354 ]][0m
[37m[1m[2023-07-17 06:14:28,779][257371] Max Reward on eval: 16.683976675041794[0m
[37m[1m[2023-07-17 06:14:28,779][257371] Min Reward on eval: 16.683976675041794[0m
[37m[1m[2023-07-17 06:14:28,779][257371] Mean Reward across all agents: 16.683976675041794[0m
[37m[1m[2023-07-17 06:14:28,780][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:14:33,756][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:14:33,757][257371] Reward + Measures: [[143.95767409   0.24569999   0.27340001   0.2271       0.26139998
    5.75620651]
 [-15.79579231   0.23720002   0.2352       0.13450001   0.22120002
    6.60648441]
 [ 44.72411492   0.12         0.1691       0.12640001   0.13630001
    5.66625166]
 ...
 [ 36.30927697   0.1116       0.12719999   0.10290001   0.1473
    5.46143293]
 [ 11.14071857   0.0753       0.0728       0.0647       0.0785
    6.03518677]
 [ 34.7501893    0.1971       0.24630003   0.17130001   0.23330002
    5.7849884 ]][0m
[37m[1m[2023-07-17 06:14:33,757][257371] Max Reward on eval: 289.55981488078834[0m
[37m[1m[2023-07-17 06:14:33,757][257371] Min Reward on eval: -254.44631297998131[0m
[37m[1m[2023-07-17 06:14:33,757][257371] Mean Reward across all agents: 32.77626385929496[0m
[37m[1m[2023-07-17 06:14:33,758][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:14:33,759][257371] mean_value=-279.78073078286826, max_value=197.66077251786535[0m
[37m[1m[2023-07-17 06:14:33,762][257371] New mean coefficients: [[ 2.0478501  -0.7683389   0.09412478 -2.5936804   0.9006541  -0.6480855 ]][0m
[37m[1m[2023-07-17 06:14:33,763][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:14:42,877][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 06:14:42,878][257371] FPS: 421376.43[0m
[36m[2023-07-17 06:14:42,880][257371] itr=761, itrs=2000, Progress: 38.05%[0m
[36m[2023-07-17 06:14:54,674][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 06:14:54,675][257371] FPS: 328069.18[0m
[36m[2023-07-17 06:14:58,970][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:14:58,971][257371] Reward + Measures: [[31.16046703  0.17076501  0.20591967  0.16785166  0.16029632  4.74818468]][0m
[37m[1m[2023-07-17 06:14:58,971][257371] Max Reward on eval: 31.160467031233672[0m
[37m[1m[2023-07-17 06:14:58,971][257371] Min Reward on eval: 31.160467031233672[0m
[37m[1m[2023-07-17 06:14:58,972][257371] Mean Reward across all agents: 31.160467031233672[0m
[37m[1m[2023-07-17 06:14:58,972][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:15:03,939][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:15:03,940][257371] Reward + Measures: [[108.79755116   0.80470002   0.48240003   0.7845       0.24990001
    7.46340656]
 [ 21.24969784   0.13380001   0.1234       0.1099       0.115
    6.00145054]
 [-31.01520922   0.12119999   0.116        0.0988       0.1148
    6.11392832]
 ...
 [-25.17270793   0.20200002   0.24800001   0.22959998   0.19600001
    5.73008347]
 [ 32.30286161   0.259        0.23559999   0.2428       0.2309
    6.00501347]
 [-51.00853768   0.1098       0.11180001   0.13759999   0.14320001
    5.99160719]][0m
[37m[1m[2023-07-17 06:15:03,940][257371] Max Reward on eval: 155.20438480819575[0m
[37m[1m[2023-07-17 06:15:03,940][257371] Min Reward on eval: -122.88231307165697[0m
[37m[1m[2023-07-17 06:15:03,940][257371] Mean Reward across all agents: 10.922824730032584[0m
[37m[1m[2023-07-17 06:15:03,941][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:15:03,942][257371] mean_value=-341.9971039075715, max_value=26.904512528190864[0m
[37m[1m[2023-07-17 06:15:03,944][257371] New mean coefficients: [[ 2.3025684  -1.7853789  -0.36497396 -0.7355839   1.04813     0.01582921]][0m
[37m[1m[2023-07-17 06:15:03,945][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:15:13,014][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 06:15:13,015][257371] FPS: 423486.98[0m
[36m[2023-07-17 06:15:13,017][257371] itr=762, itrs=2000, Progress: 38.10%[0m
[36m[2023-07-17 06:15:24,829][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 06:15:24,829][257371] FPS: 327621.29[0m
[36m[2023-07-17 06:15:29,154][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:15:29,155][257371] Reward + Measures: [[94.31963311  0.16000733  0.19072932  0.17605133  0.14684066  4.83361912]][0m
[37m[1m[2023-07-17 06:15:29,155][257371] Max Reward on eval: 94.31963311107558[0m
[37m[1m[2023-07-17 06:15:29,155][257371] Min Reward on eval: 94.31963311107558[0m
[37m[1m[2023-07-17 06:15:29,156][257371] Mean Reward across all agents: 94.31963311107558[0m
[37m[1m[2023-07-17 06:15:29,156][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:15:34,160][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:15:34,160][257371] Reward + Measures: [[ 171.7954502     0.12459999    0.1549        0.1441        0.14649999
     5.35516596]
 [  53.31936467    0.12400001    0.11850001    0.1145        0.1066
     5.5446558 ]
 [-168.81673386    0.3457        0.3116        0.35959998    0.0668
     6.79523039]
 ...
 [ 107.90776546    0.1237        0.1428        0.148         0.1494
     5.89135599]
 [  38.06291448    0.0803        0.0874        0.07669999    0.06700001
     5.66584635]
 [  14.24010621    0.13070001    0.1177        0.13349999    0.14660001
     6.17034769]][0m
[37m[1m[2023-07-17 06:15:34,161][257371] Max Reward on eval: 171.79545020181686[0m
[37m[1m[2023-07-17 06:15:34,161][257371] Min Reward on eval: -168.81673385892063[0m
[37m[1m[2023-07-17 06:15:34,161][257371] Mean Reward across all agents: 28.37594704500648[0m
[37m[1m[2023-07-17 06:15:34,161][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:15:34,162][257371] mean_value=-323.36781702635284, max_value=-48.427913706214945[0m
[36m[2023-07-17 06:15:34,165][257371] XNES is restarting with a new solution whose measures are [0.92740005 0.86490005 0.93040001 0.92410004 0.27784818] and objective is 35.106754783540964[0m
[36m[2023-07-17 06:15:34,166][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 06:15:34,168][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 06:15:34,169][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:15:43,160][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 06:15:43,161][257371] FPS: 427134.19[0m
[36m[2023-07-17 06:15:43,163][257371] itr=763, itrs=2000, Progress: 38.15%[0m
[36m[2023-07-17 06:15:55,231][257371] train() took 11.98 seconds to complete[0m
[36m[2023-07-17 06:15:55,231][257371] FPS: 320572.28[0m
[36m[2023-07-17 06:15:59,553][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:15:59,553][257371] Reward + Measures: [[103.77658737   0.57232463   0.34094033   0.53454334   0.44595769
    0.67976063]][0m
[37m[1m[2023-07-17 06:15:59,554][257371] Max Reward on eval: 103.77658737446713[0m
[37m[1m[2023-07-17 06:15:59,554][257371] Min Reward on eval: 103.77658737446713[0m
[37m[1m[2023-07-17 06:15:59,554][257371] Mean Reward across all agents: 103.77658737446713[0m
[37m[1m[2023-07-17 06:15:59,555][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:16:04,525][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:16:04,526][257371] Reward + Measures: [[ 66.18782179   0.55150002   0.29490003   0.465        0.39050001
    1.14504457]
 [ 90.77576312   0.64830011   0.29970002   0.66619998   0.46919999
    0.55689424]
 [103.94558391   0.4382       0.38090003   0.46240002   0.2726
    0.92197102]
 ...
 [182.57266041   0.45040002   0.35110003   0.44390002   0.3777
    0.79234296]
 [173.69610974   0.41020003   0.36480004   0.37900004   0.32069999
    1.16140103]
 [152.69273136   0.41159996   0.3231       0.4269       0.3145
    1.17264259]][0m
[37m[1m[2023-07-17 06:16:04,526][257371] Max Reward on eval: 277.1881466127932[0m
[37m[1m[2023-07-17 06:16:04,527][257371] Min Reward on eval: -82.6606843387708[0m
[37m[1m[2023-07-17 06:16:04,527][257371] Mean Reward across all agents: 69.86380465615903[0m
[37m[1m[2023-07-17 06:16:04,527][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:16:04,533][257371] mean_value=-298.43752558892436, max_value=434.4334743531143[0m
[37m[1m[2023-07-17 06:16:04,536][257371] New mean coefficients: [[ 0.07520643 -1.3263885  -0.1025368  -1.840252   -0.39397693 -1.5501062 ]][0m
[37m[1m[2023-07-17 06:16:04,537][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:16:13,603][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 06:16:13,604][257371] FPS: 423598.22[0m
[36m[2023-07-17 06:16:13,606][257371] itr=764, itrs=2000, Progress: 38.20%[0m
[36m[2023-07-17 06:16:25,421][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 06:16:25,421][257371] FPS: 327551.80[0m
[36m[2023-07-17 06:16:29,724][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:16:29,725][257371] Reward + Measures: [[135.19091819   0.42192      0.53798866   0.33921963   0.48900634
    0.57210386]][0m
[37m[1m[2023-07-17 06:16:29,725][257371] Max Reward on eval: 135.1909181912631[0m
[37m[1m[2023-07-17 06:16:29,725][257371] Min Reward on eval: 135.1909181912631[0m
[37m[1m[2023-07-17 06:16:29,725][257371] Mean Reward across all agents: 135.1909181912631[0m
[37m[1m[2023-07-17 06:16:29,726][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:16:34,688][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:16:34,689][257371] Reward + Measures: [[ 63.95885974   0.20039999   0.68260002   0.43490002   0.49530002
    0.94964951]
 [121.71021172   0.21880002   0.65219998   0.40349999   0.64499998
    0.78373981]
 [  4.2263683    0.52839994   0.39660004   0.56400001   0.35080001
    0.65919477]
 ...
 [102.41788485   0.20299999   0.6476       0.39270002   0.56090003
    0.68535334]
 [  8.61562267   0.22070001   0.69670004   0.40470001   0.5923
    0.96624053]
 [106.91246174   0.27630001   0.68119997   0.42770004   0.71080005
    0.65565509]][0m
[37m[1m[2023-07-17 06:16:34,689][257371] Max Reward on eval: 296.1600513550162[0m
[37m[1m[2023-07-17 06:16:34,690][257371] Min Reward on eval: -197.13653382188642[0m
[37m[1m[2023-07-17 06:16:34,690][257371] Mean Reward across all agents: 81.6298942215642[0m
[37m[1m[2023-07-17 06:16:34,690][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:16:34,695][257371] mean_value=-244.3323012647218, max_value=364.1765420788[0m
[37m[1m[2023-07-17 06:16:34,698][257371] New mean coefficients: [[ 0.42529008 -0.84039336  0.09087007 -0.76762176  0.10363546 -2.000025  ]][0m
[37m[1m[2023-07-17 06:16:34,699][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:16:43,779][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 06:16:43,779][257371] FPS: 422972.01[0m
[36m[2023-07-17 06:16:43,781][257371] itr=765, itrs=2000, Progress: 38.25%[0m
[36m[2023-07-17 06:16:55,467][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 06:16:55,468][257371] FPS: 331243.71[0m
[36m[2023-07-17 06:16:59,732][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:16:59,732][257371] Reward + Measures: [[9.59390806 0.45080435 0.89127541 0.077253   0.90233427 0.35091284]][0m
[37m[1m[2023-07-17 06:16:59,733][257371] Max Reward on eval: 9.593908062115128[0m
[37m[1m[2023-07-17 06:16:59,733][257371] Min Reward on eval: 9.593908062115128[0m
[37m[1m[2023-07-17 06:16:59,733][257371] Mean Reward across all agents: 9.593908062115128[0m
[37m[1m[2023-07-17 06:16:59,734][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:17:04,698][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:17:04,699][257371] Reward + Measures: [[ 63.69399595   0.4914       0.75080007   0.35069999   0.76949996
    0.30906627]
 [ 76.65246678   0.07480001   0.78039998   0.44959998   0.70600003
    0.57656848]
 [ 48.87309977   0.38780001   0.84839994   0.1437       0.7658
    0.53589028]
 ...
 [-58.12399104   0.58479995   0.83330005   0.1049       0.80369997
    0.57652807]
 [ 26.39304793   0.5424       0.57959998   0.5248       0.73629999
    0.44279549]
 [ 29.75884667   0.51770002   0.67070001   0.50489998   0.73210001
    0.36722311]][0m
[37m[1m[2023-07-17 06:17:04,699][257371] Max Reward on eval: 198.38099481338867[0m
[37m[1m[2023-07-17 06:17:04,699][257371] Min Reward on eval: -112.23714446304366[0m
[37m[1m[2023-07-17 06:17:04,700][257371] Mean Reward across all agents: 34.75969110643426[0m
[37m[1m[2023-07-17 06:17:04,700][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:17:04,708][257371] mean_value=49.91258199250089, max_value=610.0144100540085[0m
[37m[1m[2023-07-17 06:17:04,711][257371] New mean coefficients: [[-0.00433177 -0.7888186   0.29560754  0.01563698  0.6660224  -2.994432  ]][0m
[37m[1m[2023-07-17 06:17:04,712][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:17:13,746][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 06:17:13,746][257371] FPS: 425125.98[0m
[36m[2023-07-17 06:17:13,748][257371] itr=766, itrs=2000, Progress: 38.30%[0m
[36m[2023-07-17 06:17:25,417][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 06:17:25,418][257371] FPS: 331689.15[0m
[36m[2023-07-17 06:17:29,687][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:17:29,688][257371] Reward + Measures: [[16.97593612  0.09320567  0.96503133  0.37212932  0.96509302  0.22073571]][0m
[37m[1m[2023-07-17 06:17:29,688][257371] Max Reward on eval: 16.975936124726417[0m
[37m[1m[2023-07-17 06:17:29,688][257371] Min Reward on eval: 16.975936124726417[0m
[37m[1m[2023-07-17 06:17:29,689][257371] Mean Reward across all agents: 16.975936124726417[0m
[37m[1m[2023-07-17 06:17:29,689][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:17:34,663][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:17:34,664][257371] Reward + Measures: [[ 34.81428354   0.81160003   0.50209999   0.80730003   0.44830003
    0.31876394]
 [ 32.65801691   0.0621       0.91160005   0.41580001   0.91890001
    0.35949516]
 [ 29.38379032   0.1305       0.86980003   0.34780002   0.82299995
    0.46039382]
 ...
 [ 40.35182729   0.479        0.85009998   0.45360002   0.85660011
    0.36135027]
 [ 27.30202143   0.58610004   0.78719997   0.42899999   0.84730005
    0.24431486]
 [-19.07825374   0.63420004   0.56800002   0.41319999   0.73589998
    0.35758981]][0m
[37m[1m[2023-07-17 06:17:34,664][257371] Max Reward on eval: 103.93067551869899[0m
[37m[1m[2023-07-17 06:17:34,664][257371] Min Reward on eval: -60.25779747888737[0m
[37m[1m[2023-07-17 06:17:34,665][257371] Mean Reward across all agents: 27.55948874793452[0m
[37m[1m[2023-07-17 06:17:34,665][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:17:34,671][257371] mean_value=-12.066099273678143, max_value=531.69306827283[0m
[37m[1m[2023-07-17 06:17:34,674][257371] New mean coefficients: [[ 0.13895088 -0.17984957  1.0424858   0.52000743  0.45638102 -4.258022  ]][0m
[37m[1m[2023-07-17 06:17:34,675][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:17:43,692][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 06:17:43,692][257371] FPS: 425937.79[0m
[36m[2023-07-17 06:17:43,695][257371] itr=767, itrs=2000, Progress: 38.35%[0m
[36m[2023-07-17 06:17:55,588][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 06:17:55,588][257371] FPS: 325420.88[0m
[36m[2023-07-17 06:17:59,980][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:17:59,981][257371] Reward + Measures: [[14.28420161  0.00493267  0.98922801  0.75226873  0.98457533  0.13559268]][0m
[37m[1m[2023-07-17 06:17:59,981][257371] Max Reward on eval: 14.284201613606722[0m
[37m[1m[2023-07-17 06:17:59,981][257371] Min Reward on eval: 14.284201613606722[0m
[37m[1m[2023-07-17 06:17:59,981][257371] Mean Reward across all agents: 14.284201613606722[0m
[37m[1m[2023-07-17 06:17:59,982][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:18:05,003][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:18:05,004][257371] Reward + Measures: [[10.71874707  0.77319998  0.65609998  0.74220002  0.60780001  0.24663177]
 [25.89970005  0.54259998  0.8998      0.35549998  0.88990003  0.18854761]
 [ 6.17439438  0.93500006  0.3213      0.9339      0.4639      0.26188946]
 ...
 [ 4.49372132  0.31119999  0.90070003  0.44619998  0.89959997  0.22835696]
 [-5.19508878  0.80600005  0.86999989  0.52490002  0.90630001  0.18525933]
 [90.10257482  0.24680002  0.85939997  0.56280005  0.80080003  0.39620861]][0m
[37m[1m[2023-07-17 06:18:05,004][257371] Max Reward on eval: 120.8926944661187[0m
[37m[1m[2023-07-17 06:18:05,005][257371] Min Reward on eval: -36.34446933714207[0m
[37m[1m[2023-07-17 06:18:05,005][257371] Mean Reward across all agents: 23.45923522402847[0m
[37m[1m[2023-07-17 06:18:05,005][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:18:05,011][257371] mean_value=-14.624750583100498, max_value=474.7684803607711[0m
[37m[1m[2023-07-17 06:18:05,014][257371] New mean coefficients: [[-0.36634696  0.3644461   0.99330276  0.80623984 -0.11705154 -5.1047945 ]][0m
[37m[1m[2023-07-17 06:18:05,015][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:18:14,155][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 06:18:14,155][257371] FPS: 420216.14[0m
[36m[2023-07-17 06:18:14,157][257371] itr=768, itrs=2000, Progress: 38.40%[0m
[36m[2023-07-17 06:18:26,092][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-17 06:18:26,092][257371] FPS: 324231.44[0m
[36m[2023-07-17 06:18:30,432][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:18:30,438][257371] Reward + Measures: [[5.7931923  0.00427967 0.99591339 0.9624607  0.99308598 0.07649584]][0m
[37m[1m[2023-07-17 06:18:30,438][257371] Max Reward on eval: 5.793192303039599[0m
[37m[1m[2023-07-17 06:18:30,438][257371] Min Reward on eval: 5.793192303039599[0m
[37m[1m[2023-07-17 06:18:30,439][257371] Mean Reward across all agents: 5.793192303039599[0m
[37m[1m[2023-07-17 06:18:30,439][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:18:35,661][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:18:35,662][257371] Reward + Measures: [[16.59740286  0.45120001  0.89360011  0.3159      0.86000007  0.36642191]
 [27.92570636  0.71279997  0.9551      0.43259999  0.90710002  0.21156192]
 [28.65156785  0.42389998  0.78949994  0.86219996  0.8811      0.25628409]
 ...
 [33.67548466  0.60540003  0.70889997  0.83750004  0.80100006  0.25135478]
 [22.29978852  0.0135      0.97130007  0.81090003  0.94209999  0.20303486]
 [52.52192426  0.0197      0.97229999  0.63130003  0.9526      0.28381109]][0m
[37m[1m[2023-07-17 06:18:35,662][257371] Max Reward on eval: 74.82875152314082[0m
[37m[1m[2023-07-17 06:18:35,663][257371] Min Reward on eval: -37.26305104345083[0m
[37m[1m[2023-07-17 06:18:35,663][257371] Mean Reward across all agents: 18.18356068345193[0m
[37m[1m[2023-07-17 06:18:35,663][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:18:35,669][257371] mean_value=-14.536734071383307, max_value=360.9484314588831[0m
[37m[1m[2023-07-17 06:18:35,672][257371] New mean coefficients: [[ 0.08289245 -0.6914405   1.32061     1.4966328  -1.1682863  -5.8473616 ]][0m
[37m[1m[2023-07-17 06:18:35,673][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:18:44,738][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 06:18:44,738][257371] FPS: 423700.61[0m
[36m[2023-07-17 06:18:44,740][257371] itr=769, itrs=2000, Progress: 38.45%[0m
[36m[2023-07-17 06:18:56,441][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 06:18:56,441][257371] FPS: 330787.87[0m
[36m[2023-07-17 06:19:00,788][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:19:00,788][257371] Reward + Measures: [[4.91911249 0.00342333 0.99780101 0.91963232 0.99882662 0.05799136]][0m
[37m[1m[2023-07-17 06:19:00,789][257371] Max Reward on eval: 4.919112487023484[0m
[37m[1m[2023-07-17 06:19:00,789][257371] Min Reward on eval: 4.919112487023484[0m
[37m[1m[2023-07-17 06:19:00,789][257371] Mean Reward across all agents: 4.919112487023484[0m
[37m[1m[2023-07-17 06:19:00,790][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:19:05,849][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:19:05,850][257371] Reward + Measures: [[-42.35598741   0.52039999   0.32720003   0.53979999   0.1242
    2.96536922]
 [  6.89090614   0.59580004   0.35639998   0.55269998   0.1212
    3.41132617]
 [-25.46015165   0.42350003   0.2586       0.4305       0.15479998
    3.90827227]
 ...
 [158.18711667   0.67569995   0.44140002   0.62320006   0.1067
    1.45408535]
 [-34.52715128   0.2332       0.14680001   0.20390001   0.1209
    5.18235302]
 [ 44.91632247   0.44689998   0.67080003   0.72069997   0.3522
    1.95575607]][0m
[37m[1m[2023-07-17 06:19:05,850][257371] Max Reward on eval: 361.80130625652384[0m
[37m[1m[2023-07-17 06:19:05,850][257371] Min Reward on eval: -99.9681191444397[0m
[37m[1m[2023-07-17 06:19:05,851][257371] Mean Reward across all agents: 23.181827006780548[0m
[37m[1m[2023-07-17 06:19:05,851][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:19:05,856][257371] mean_value=-248.75183712479463, max_value=332.07018940442117[0m
[37m[1m[2023-07-17 06:19:05,859][257371] New mean coefficients: [[-0.7237543  -0.16131485  1.4350705   1.555958   -0.32473338 -4.586504  ]][0m
[37m[1m[2023-07-17 06:19:05,860][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:19:15,002][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 06:19:15,003][257371] FPS: 420086.02[0m
[36m[2023-07-17 06:19:15,005][257371] itr=770, itrs=2000, Progress: 38.50%[0m
[37m[1m[2023-07-17 06:22:43,996][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000750[0m
[36m[2023-07-17 06:22:56,294][257371] train() took 11.52 seconds to complete[0m
[36m[2023-07-17 06:22:56,294][257371] FPS: 333387.73[0m
[36m[2023-07-17 06:23:00,553][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:23:00,553][257371] Reward + Measures: [[-7.66973565  0.01428267  0.97446197  0.8472473   0.88997769  0.35457104]][0m
[37m[1m[2023-07-17 06:23:00,554][257371] Max Reward on eval: -7.669735645143944[0m
[37m[1m[2023-07-17 06:23:00,554][257371] Min Reward on eval: -7.669735645143944[0m
[37m[1m[2023-07-17 06:23:00,554][257371] Mean Reward across all agents: -7.669735645143944[0m
[37m[1m[2023-07-17 06:23:00,554][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:23:05,598][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:23:05,659][257371] Reward + Measures: [[232.68340397   0.51700002   0.3407       0.60879999   0.27270004
    1.25311625]
 [321.64687387   0.2735       0.27379999   0.30560002   0.23049998
    3.79164672]
 [ 80.77682077   0.30070001   0.41240001   0.2922       0.38010001
    1.19515216]
 ...
 [183.01917438   0.24110003   0.2079       0.25039998   0.1622
    4.08513784]
 [655.2958727    0.331        0.42340001   0.47580001   0.3628
    1.24338436]
 [ 61.73243467   0.71810001   0.1864       0.7924       0.66549999
    1.41265869]][0m
[37m[1m[2023-07-17 06:23:05,659][257371] Max Reward on eval: 838.2791290193563[0m
[37m[1m[2023-07-17 06:23:05,660][257371] Min Reward on eval: -145.39967299271956[0m
[37m[1m[2023-07-17 06:23:05,660][257371] Mean Reward across all agents: 158.08732824010747[0m
[37m[1m[2023-07-17 06:23:05,660][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:23:05,669][257371] mean_value=-908.9078261884853, max_value=856.7948047611118[0m
[37m[1m[2023-07-17 06:23:05,673][257371] New mean coefficients: [[-1.602526    0.18377966  2.3223596   1.1429577  -0.22503705 -3.678371  ]][0m
[37m[1m[2023-07-17 06:23:05,674][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:23:14,704][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 06:23:14,704][257371] FPS: 425321.88[0m
[36m[2023-07-17 06:23:14,706][257371] itr=771, itrs=2000, Progress: 38.55%[0m
[36m[2023-07-17 06:23:26,521][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 06:23:26,521][257371] FPS: 327521.55[0m
[36m[2023-07-17 06:23:30,714][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:23:30,720][257371] Reward + Measures: [[17.96603048  0.00700267  0.98901463  0.98278004  0.99284035  0.48295331]][0m
[37m[1m[2023-07-17 06:23:30,720][257371] Max Reward on eval: 17.966030481663285[0m
[37m[1m[2023-07-17 06:23:30,721][257371] Min Reward on eval: 17.966030481663285[0m
[37m[1m[2023-07-17 06:23:30,721][257371] Mean Reward across all agents: 17.966030481663285[0m
[37m[1m[2023-07-17 06:23:30,721][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:23:35,722][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:23:35,728][257371] Reward + Measures: [[ 10.09825413   0.18410002   0.55669993   0.41249999   0.64499998
    0.99105471]
 [ -7.07085114   0.55629998   0.41799998   0.49499997   0.45360002
    2.92749262]
 [ 68.69316548   0.35819998   0.56590003   0.4296       0.34420002
    1.73787236]
 ...
 [-15.94721671   0.29700002   0.40939999   0.3326       0.40470001
    2.95350957]
 [-37.64791135   0.18380001   0.27770001   0.18099999   0.3087
    4.20800686]
 [-23.51330428   0.4147       0.6857       0.71780008   0.58640003
    2.70377994]][0m
[37m[1m[2023-07-17 06:23:35,728][257371] Max Reward on eval: 345.33716391040946[0m
[37m[1m[2023-07-17 06:23:35,728][257371] Min Reward on eval: -194.87920614946634[0m
[37m[1m[2023-07-17 06:23:35,728][257371] Mean Reward across all agents: -13.205667407441751[0m
[37m[1m[2023-07-17 06:23:35,729][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:23:35,732][257371] mean_value=-903.6747302068306, max_value=486.46767670642583[0m
[37m[1m[2023-07-17 06:23:35,734][257371] New mean coefficients: [[-1.0395641   1.3491719   2.241293    1.4114614  -0.24285014 -2.0175524 ]][0m
[37m[1m[2023-07-17 06:23:35,735][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:23:44,837][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 06:23:44,837][257371] FPS: 421977.31[0m
[36m[2023-07-17 06:23:44,839][257371] itr=772, itrs=2000, Progress: 38.60%[0m
[36m[2023-07-17 06:23:56,798][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-17 06:23:56,798][257371] FPS: 323538.15[0m
[36m[2023-07-17 06:24:01,141][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:24:01,147][257371] Reward + Measures: [[-56.93748991   0.90634763   0.57227796   0.92612404   0.51916802
    0.23255318]][0m
[37m[1m[2023-07-17 06:24:01,148][257371] Max Reward on eval: -56.93748990648884[0m
[37m[1m[2023-07-17 06:24:01,149][257371] Min Reward on eval: -56.93748990648884[0m
[37m[1m[2023-07-17 06:24:01,149][257371] Mean Reward across all agents: -56.93748990648884[0m
[37m[1m[2023-07-17 06:24:01,150][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:24:06,160][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:24:06,166][257371] Reward + Measures: [[-45.36300069   0.34399998   0.3612       0.39920002   0.35209998
    1.73533332]
 [-54.74962186   0.37220001   0.4887       0.52279997   0.47319999
    0.93930405]
 [-17.35369821   0.77010006   0.79449999   0.7748       0.77930003
    4.63021469]
 ...
 [-72.05611315   0.09810001   0.0734       0.1062       0.0813
    5.12132597]
 [ 24.53339108   0.32930002   0.38440001   0.126        0.4059
    2.95876861]
 [-89.45472033   0.37760001   0.31459999   0.30630001   0.20560001
    2.66437078]][0m
[37m[1m[2023-07-17 06:24:06,166][257371] Max Reward on eval: 169.50566431330518[0m
[37m[1m[2023-07-17 06:24:06,166][257371] Min Reward on eval: -265.6611166258808[0m
[37m[1m[2023-07-17 06:24:06,167][257371] Mean Reward across all agents: -6.83992989425619[0m
[37m[1m[2023-07-17 06:24:06,167][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:24:06,172][257371] mean_value=-629.4656628432415, max_value=342.53326893750443[0m
[37m[1m[2023-07-17 06:24:06,174][257371] New mean coefficients: [[-1.149311    2.1296852   2.4656506   1.6996015  -0.20196034 -1.6135697 ]][0m
[37m[1m[2023-07-17 06:24:06,175][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:24:15,133][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 06:24:15,134][257371] FPS: 428752.38[0m
[36m[2023-07-17 06:24:15,136][257371] itr=773, itrs=2000, Progress: 38.65%[0m
[36m[2023-07-17 06:24:27,099][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-17 06:24:27,099][257371] FPS: 323436.05[0m
[36m[2023-07-17 06:24:31,429][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:24:31,429][257371] Reward + Measures: [[15.63747926  0.84985828  0.97648162  0.98136598  0.58005935  0.67301726]][0m
[37m[1m[2023-07-17 06:24:31,429][257371] Max Reward on eval: 15.637479261298173[0m
[37m[1m[2023-07-17 06:24:31,430][257371] Min Reward on eval: 15.637479261298173[0m
[37m[1m[2023-07-17 06:24:31,430][257371] Mean Reward across all agents: 15.637479261298173[0m
[37m[1m[2023-07-17 06:24:31,430][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:24:36,602][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:24:36,603][257371] Reward + Measures: [[-29.31577014   0.39470002   0.82340002   0.111        0.71970004
    1.91087818]
 [-93.34020025   0.48180005   0.34240004   0.3802       0.20110002
    2.63931918]
 [-22.54165917   0.1638       0.14960001   0.1142       0.14680001
    4.26176929]
 ...
 [ 99.19666121   0.09240001   0.84279996   0.75410002   0.72939998
    2.17476368]
 [-29.98887948   0.52759999   0.40580001   0.31070003   0.5915001
    1.43040943]
 [ 10.18057516   0.23639999   0.37789997   0.15890001   0.38779998
    3.57385826]][0m
[37m[1m[2023-07-17 06:24:36,603][257371] Max Reward on eval: 601.2333526554518[0m
[37m[1m[2023-07-17 06:24:36,603][257371] Min Reward on eval: -233.04545159011613[0m
[37m[1m[2023-07-17 06:24:36,604][257371] Mean Reward across all agents: -3.2945585633607455[0m
[37m[1m[2023-07-17 06:24:36,604][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:24:36,609][257371] mean_value=-824.9115363678924, max_value=594.1031158718183[0m
[37m[1m[2023-07-17 06:24:36,611][257371] New mean coefficients: [[-0.981766   1.0602261  2.2691813  1.6548188 -1.060289  -1.618132 ]][0m
[37m[1m[2023-07-17 06:24:36,612][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:24:45,630][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 06:24:45,630][257371] FPS: 425903.07[0m
[36m[2023-07-17 06:24:45,632][257371] itr=774, itrs=2000, Progress: 38.70%[0m
[36m[2023-07-17 06:24:57,351][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 06:24:57,351][257371] FPS: 330291.91[0m
[36m[2023-07-17 06:25:01,591][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:25:01,596][257371] Reward + Measures: [[-22.86490925   0.45282933   0.64263499   0.82504898   0.61277264
    0.4768872 ]][0m
[37m[1m[2023-07-17 06:25:01,597][257371] Max Reward on eval: -22.86490925074881[0m
[37m[1m[2023-07-17 06:25:01,597][257371] Min Reward on eval: -22.86490925074881[0m
[37m[1m[2023-07-17 06:25:01,597][257371] Mean Reward across all agents: -22.86490925074881[0m
[37m[1m[2023-07-17 06:25:01,598][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:25:06,570][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:25:06,570][257371] Reward + Measures: [[-195.42442774    0.24920002    0.602         0.20130001    0.5226
     0.9227336 ]
 [-103.68308715    0.26339999    0.4014        0.17029999    0.41990003
     1.9689461 ]
 [  84.87274       0.63140005    0.45860001    0.56930006    0.34580001
     1.18667829]
 ...
 [  68.14084295    0.52509999    0.396         0.50830001    0.32839999
     3.06357121]
 [-100.5314629     0.27180001    0.67039996    0.35140002    0.65150005
     0.62425166]
 [-146.68348288    0.21469998    0.26120001    0.14690001    0.35310003
     3.08541226]][0m
[37m[1m[2023-07-17 06:25:06,571][257371] Max Reward on eval: 270.2071705047041[0m
[37m[1m[2023-07-17 06:25:06,571][257371] Min Reward on eval: -245.7008142296225[0m
[37m[1m[2023-07-17 06:25:06,571][257371] Mean Reward across all agents: 2.051903928613909[0m
[37m[1m[2023-07-17 06:25:06,571][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:25:06,577][257371] mean_value=-942.0135764488048, max_value=496.1216934308832[0m
[37m[1m[2023-07-17 06:25:06,580][257371] New mean coefficients: [[ 0.8761543   0.92637163  2.570824    1.9645123  -1.2729208  -1.7067817 ]][0m
[37m[1m[2023-07-17 06:25:06,580][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:25:15,556][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 06:25:15,556][257371] FPS: 427933.27[0m
[36m[2023-07-17 06:25:15,558][257371] itr=775, itrs=2000, Progress: 38.75%[0m
[36m[2023-07-17 06:25:27,468][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 06:25:27,468][257371] FPS: 325037.08[0m
[36m[2023-07-17 06:25:31,764][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:25:31,765][257371] Reward + Measures: [[24.33453662  0.38342032  0.80103236  0.84808761  0.62875497  0.49577612]][0m
[37m[1m[2023-07-17 06:25:31,765][257371] Max Reward on eval: 24.334536622822245[0m
[37m[1m[2023-07-17 06:25:31,765][257371] Min Reward on eval: 24.334536622822245[0m
[37m[1m[2023-07-17 06:25:31,766][257371] Mean Reward across all agents: 24.334536622822245[0m
[37m[1m[2023-07-17 06:25:31,766][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:25:36,803][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:25:36,809][257371] Reward + Measures: [[ 47.7917579    0.38910002   0.30849999   0.345        0.33440003
    3.94234776]
 [246.38930229   0.55120003   0.37649998   0.54699999   0.1858
    3.50282741]
 [ 83.64952346   0.58859998   0.29990003   0.55419999   0.34910002
    1.45858371]
 ...
 [ 85.93209128   0.56189996   0.37989998   0.55230004   0.34059998
    3.42951202]
 [ 80.18450497   0.72469997   0.22910002   0.75560009   0.29319999
    0.99041814]
 [ 24.33647266   0.22290002   0.14210001   0.22540002   0.18850002
    3.35058594]][0m
[37m[1m[2023-07-17 06:25:36,809][257371] Max Reward on eval: 436.05953217968346[0m
[37m[1m[2023-07-17 06:25:36,809][257371] Min Reward on eval: -90.85564921209588[0m
[37m[1m[2023-07-17 06:25:36,810][257371] Mean Reward across all agents: 91.30380325525078[0m
[37m[1m[2023-07-17 06:25:36,810][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:25:36,816][257371] mean_value=-423.03281282697884, max_value=330.40176719007593[0m
[37m[1m[2023-07-17 06:25:36,819][257371] New mean coefficients: [[ 1.1865435  1.222693   1.6888983  2.172627  -1.3862972 -1.334219 ]][0m
[37m[1m[2023-07-17 06:25:36,820][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:25:45,887][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 06:25:45,887][257371] FPS: 423579.79[0m
[36m[2023-07-17 06:25:45,889][257371] itr=776, itrs=2000, Progress: 38.80%[0m
[36m[2023-07-17 06:25:57,479][257371] train() took 11.50 seconds to complete[0m
[36m[2023-07-17 06:25:57,479][257371] FPS: 334056.63[0m
[36m[2023-07-17 06:26:01,765][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:26:01,771][257371] Reward + Measures: [[110.23871922   0.53415799   0.78545529   0.94581234   0.66346669
    0.47192252]][0m
[37m[1m[2023-07-17 06:26:01,771][257371] Max Reward on eval: 110.23871922036439[0m
[37m[1m[2023-07-17 06:26:01,771][257371] Min Reward on eval: 110.23871922036439[0m
[37m[1m[2023-07-17 06:26:01,772][257371] Mean Reward across all agents: 110.23871922036439[0m
[37m[1m[2023-07-17 06:26:01,772][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:26:06,742][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:26:06,743][257371] Reward + Measures: [[ -62.47507185    0.66759998    0.43880007    0.78249997    0.43790004
     0.67833132]
 [  77.35874857    0.3917        0.5176        0.54399997    0.4962
     0.76678598]
 [-128.59853648    0.76839995    0.43199998    0.77850002    0.43649998
     1.42738056]
 ...
 [-332.79424237    0.49320003    0.5808        0.40109998    0.35999998
     0.88243645]
 [ -80.73618765    0.52560002    0.66909999    0.58160007    0.49170002
     0.58955395]
 [-283.95886706    0.50510001    0.43829998    0.3017        0.31990001
     1.09870327]][0m
[37m[1m[2023-07-17 06:26:06,743][257371] Max Reward on eval: 353.9471662535798[0m
[37m[1m[2023-07-17 06:26:06,743][257371] Min Reward on eval: -543.8976554802618[0m
[37m[1m[2023-07-17 06:26:06,743][257371] Mean Reward across all agents: -69.13535721546[0m
[37m[1m[2023-07-17 06:26:06,744][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:26:06,748][257371] mean_value=-860.5376222725711, max_value=99.56877650934288[0m
[37m[1m[2023-07-17 06:26:06,750][257371] New mean coefficients: [[-0.19163883  1.9232962   1.3382682   2.0773623  -1.1929786  -1.3611279 ]][0m
[37m[1m[2023-07-17 06:26:06,751][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:26:15,797][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 06:26:15,797][257371] FPS: 424600.31[0m
[36m[2023-07-17 06:26:15,799][257371] itr=777, itrs=2000, Progress: 38.85%[0m
[36m[2023-07-17 06:26:27,694][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 06:26:27,694][257371] FPS: 325463.04[0m
[36m[2023-07-17 06:26:32,044][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:26:32,044][257371] Reward + Measures: [[121.39726797   0.62407863   0.75225765   0.6205883    0.58280027
    0.50437814]][0m
[37m[1m[2023-07-17 06:26:32,044][257371] Max Reward on eval: 121.39726797454233[0m
[37m[1m[2023-07-17 06:26:32,045][257371] Min Reward on eval: 121.39726797454233[0m
[37m[1m[2023-07-17 06:26:32,045][257371] Mean Reward across all agents: 121.39726797454233[0m
[37m[1m[2023-07-17 06:26:32,045][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:26:37,087][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:26:37,092][257371] Reward + Measures: [[-35.9248114    0.1127       0.745        0.49650002   0.66540003
    2.54047132]
 [ 71.84728405   0.81070006   0.2922       0.79300004   0.1962
    0.68406838]
 [161.48859218   0.35840002   0.7044       0.52329999   0.57420003
    0.88685197]
 ...
 [ 39.67219163   0.56700003   0.45440003   0.60579997   0.56520003
    0.50100702]
 [-78.56341335   0.074        0.76239997   0.43799996   0.70029998
    4.20392561]
 [139.23249247   0.58890003   0.35170001   0.55540001   0.48800001
    0.83620119]][0m
[37m[1m[2023-07-17 06:26:37,093][257371] Max Reward on eval: 473.508870148845[0m
[37m[1m[2023-07-17 06:26:37,093][257371] Min Reward on eval: -346.2926407140214[0m
[37m[1m[2023-07-17 06:26:37,093][257371] Mean Reward across all agents: 52.210211017433124[0m
[37m[1m[2023-07-17 06:26:37,093][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:26:37,099][257371] mean_value=-388.54010413680317, max_value=440.6232736708003[0m
[37m[1m[2023-07-17 06:26:37,102][257371] New mean coefficients: [[ 0.36445326  2.0254292   0.24283385  1.0461371  -1.012665   -0.1807158 ]][0m
[37m[1m[2023-07-17 06:26:37,103][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:26:46,229][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 06:26:46,230][257371] FPS: 420822.63[0m
[36m[2023-07-17 06:26:46,232][257371] itr=778, itrs=2000, Progress: 38.90%[0m
[36m[2023-07-17 06:26:57,936][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 06:26:57,937][257371] FPS: 330731.42[0m
[36m[2023-07-17 06:27:02,208][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:27:02,209][257371] Reward + Measures: [[181.27299088   0.808806     0.49547502   0.74608558   0.46404165
    0.44383463]][0m
[37m[1m[2023-07-17 06:27:02,209][257371] Max Reward on eval: 181.27299088420816[0m
[37m[1m[2023-07-17 06:27:02,209][257371] Min Reward on eval: 181.27299088420816[0m
[37m[1m[2023-07-17 06:27:02,210][257371] Mean Reward across all agents: 181.27299088420816[0m
[37m[1m[2023-07-17 06:27:02,210][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:27:07,245][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:27:07,251][257371] Reward + Measures: [[174.86559754   0.54840004   0.4041       0.51209998   0.0973
    4.14727783]
 [ 32.01885994   0.47220001   0.50349998   0.42430001   0.74010003
    3.29922342]
 [ 43.39310326   0.48330003   0.64230007   0.52249998   0.7094
    0.72705507]
 ...
 [-21.84471789   0.95370007   0.27060002   0.94370002   0.24489999
    0.5895552 ]
 [197.2426329    0.75110006   0.2809       0.79210001   0.34169999
    0.48436108]
 [-51.62845968   0.19960001   0.42669997   0.44729996   0.41580001
    3.12731624]][0m
[37m[1m[2023-07-17 06:27:07,251][257371] Max Reward on eval: 300.58601382002234[0m
[37m[1m[2023-07-17 06:27:07,251][257371] Min Reward on eval: -183.06491661793552[0m
[37m[1m[2023-07-17 06:27:07,251][257371] Mean Reward across all agents: 26.389320246743516[0m
[37m[1m[2023-07-17 06:27:07,252][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:27:07,258][257371] mean_value=-289.2758878045638, max_value=471.8154459364759[0m
[37m[1m[2023-07-17 06:27:07,261][257371] New mean coefficients: [[ 0.71873444  2.2858946  -1.0316364   1.3098855  -0.85805994  0.5317883 ]][0m
[37m[1m[2023-07-17 06:27:07,262][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:27:16,397][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 06:27:16,397][257371] FPS: 420424.36[0m
[36m[2023-07-17 06:27:16,400][257371] itr=779, itrs=2000, Progress: 38.95%[0m
[36m[2023-07-17 06:27:28,288][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 06:27:28,288][257371] FPS: 325651.10[0m
[36m[2023-07-17 06:27:32,556][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:27:32,557][257371] Reward + Measures: [[19.61908555  0.92726231  0.6088413   0.91542071  0.42948696  1.14101887]][0m
[37m[1m[2023-07-17 06:27:32,557][257371] Max Reward on eval: 19.619085548767092[0m
[37m[1m[2023-07-17 06:27:32,557][257371] Min Reward on eval: 19.619085548767092[0m
[37m[1m[2023-07-17 06:27:32,557][257371] Mean Reward across all agents: 19.619085548767092[0m
[37m[1m[2023-07-17 06:27:32,558][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:27:37,812][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:27:37,818][257371] Reward + Measures: [[-24.10551734   0.81140006   0.21830001   0.78130001   0.2491
    1.30303562]
 [214.43167967   0.2696       0.33500001   0.222        0.33790001
    2.68284607]
 [-15.6026785    0.32339999   0.24330001   0.3145       0.15120001
    4.59387207]
 ...
 [-27.00732937   0.58260006   0.2366       0.56160003   0.21440001
    2.44496989]
 [ 31.35997504   0.2184       0.2077       0.16940001   0.20150001
    3.91582227]
 [ 59.68723069   0.7622       0.38330004   0.63450003   0.40560004
    0.92620814]][0m
[37m[1m[2023-07-17 06:27:37,818][257371] Max Reward on eval: 331.24509527664634[0m
[37m[1m[2023-07-17 06:27:37,819][257371] Min Reward on eval: -252.6490909928456[0m
[37m[1m[2023-07-17 06:27:37,819][257371] Mean Reward across all agents: 29.14756883324763[0m
[37m[1m[2023-07-17 06:27:37,819][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:27:37,823][257371] mean_value=-1090.4097208793755, max_value=389.4308999156251[0m
[37m[1m[2023-07-17 06:27:37,826][257371] New mean coefficients: [[-1.0895844   2.126023   -1.4951758   1.4994055  -0.7680775   0.17835173]][0m
[37m[1m[2023-07-17 06:27:37,827][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:27:46,972][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 06:27:46,972][257371] FPS: 419970.72[0m
[36m[2023-07-17 06:27:46,975][257371] itr=780, itrs=2000, Progress: 39.00%[0m
[37m[1m[2023-07-17 06:31:02,337][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000760[0m
[36m[2023-07-17 06:31:14,674][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 06:31:14,674][257371] FPS: 331141.84[0m
[36m[2023-07-17 06:31:18,928][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:31:18,929][257371] Reward + Measures: [[-6.73242098  0.99690801  0.99370199  0.99563068  0.99307501  2.19212699]][0m
[37m[1m[2023-07-17 06:31:18,929][257371] Max Reward on eval: -6.732420984423166[0m
[37m[1m[2023-07-17 06:31:18,929][257371] Min Reward on eval: -6.732420984423166[0m
[37m[1m[2023-07-17 06:31:18,929][257371] Mean Reward across all agents: -6.732420984423166[0m
[37m[1m[2023-07-17 06:31:18,930][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:31:23,886][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:31:23,886][257371] Reward + Measures: [[   0.59340923    0.99360007    0.99360001    0.99300003    0.99340004
     2.27367282]
 [ 189.26881099    0.36120003    0.56960005    0.44370005    0.30610001
     2.57996082]
 [  -5.2351701     0.93400002    0.88910007    0.93450004    0.9073
     1.83891892]
 ...
 [-190.42589663    0.4111        0.51719999    0.22719999    0.38510001
     1.63586462]
 [ -26.13635008    0.68540001    0.16129999    0.69690001    0.43940002
     4.00359583]
 [ -25.22734299    0.60700005    0.5467        0.56750005    0.49670002
     4.02653837]][0m
[37m[1m[2023-07-17 06:31:23,887][257371] Max Reward on eval: 456.9599390124902[0m
[37m[1m[2023-07-17 06:31:23,887][257371] Min Reward on eval: -193.72236537141725[0m
[37m[1m[2023-07-17 06:31:23,887][257371] Mean Reward across all agents: 18.46596707737593[0m
[37m[1m[2023-07-17 06:31:23,888][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:31:23,893][257371] mean_value=-1008.8305873572778, max_value=448.13880082019614[0m
[37m[1m[2023-07-17 06:31:23,896][257371] New mean coefficients: [[ 0.10851347  1.8748838  -0.85640043  1.9669588   0.3615471  -0.16946048]][0m
[37m[1m[2023-07-17 06:31:23,897][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:31:32,893][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 06:31:32,894][257371] FPS: 426918.39[0m
[36m[2023-07-17 06:31:32,896][257371] itr=781, itrs=2000, Progress: 39.05%[0m
[36m[2023-07-17 06:31:44,521][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 06:31:44,522][257371] FPS: 333044.66[0m
[36m[2023-07-17 06:31:48,745][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:31:48,746][257371] Reward + Measures: [[-23.06996614   0.77510625   0.50961298   0.59131169   0.52896869
    0.95338571]][0m
[37m[1m[2023-07-17 06:31:48,746][257371] Max Reward on eval: -23.069966137243824[0m
[37m[1m[2023-07-17 06:31:48,746][257371] Min Reward on eval: -23.069966137243824[0m
[37m[1m[2023-07-17 06:31:48,746][257371] Mean Reward across all agents: -23.069966137243824[0m
[37m[1m[2023-07-17 06:31:48,746][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:31:53,669][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:31:53,670][257371] Reward + Measures: [[  64.16032744    0.39809999    0.75010008    0.52999997    0.61739999
     1.4136399 ]
 [ -13.63605885    0.72720003    0.49990001    0.52209997    0.54660004
     1.4537214 ]
 [-141.93117644    0.2559        0.41739997    0.29120001    0.35810003
     2.96078682]
 ...
 [-237.12048723    0.7274        0.69330001    0.46939999    0.3439
     3.9589684 ]
 [  14.31332208    0.43940002    0.43910003    0.4262        0.48840004
     4.19446516]
 [ -71.35126177    0.22879998    0.75660002    0.40459999    0.58820003
     1.42185426]][0m
[37m[1m[2023-07-17 06:31:53,670][257371] Max Reward on eval: 311.1099784698337[0m
[37m[1m[2023-07-17 06:31:53,671][257371] Min Reward on eval: -349.9548378199339[0m
[37m[1m[2023-07-17 06:31:53,671][257371] Mean Reward across all agents: -33.605276240089424[0m
[37m[1m[2023-07-17 06:31:53,671][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:31:53,674][257371] mean_value=-661.2370756293323, max_value=186.11709921877443[0m
[37m[1m[2023-07-17 06:31:53,677][257371] New mean coefficients: [[ 0.54178756  1.7517369   0.70555824  2.0133548   0.2617251  -0.43932998]][0m
[37m[1m[2023-07-17 06:31:53,678][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:32:02,674][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 06:32:02,674][257371] FPS: 426932.33[0m
[36m[2023-07-17 06:32:02,677][257371] itr=782, itrs=2000, Progress: 39.10%[0m
[36m[2023-07-17 06:32:14,413][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 06:32:14,413][257371] FPS: 329908.24[0m
[36m[2023-07-17 06:32:18,687][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:32:18,687][257371] Reward + Measures: [[37.39607301  0.78634632  0.25268331  0.64929897  0.44632798  0.765652  ]][0m
[37m[1m[2023-07-17 06:32:18,687][257371] Max Reward on eval: 37.396073012998116[0m
[37m[1m[2023-07-17 06:32:18,688][257371] Min Reward on eval: 37.396073012998116[0m
[37m[1m[2023-07-17 06:32:18,688][257371] Mean Reward across all agents: 37.396073012998116[0m
[37m[1m[2023-07-17 06:32:18,688][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:32:23,649][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:32:23,650][257371] Reward + Measures: [[  80.70244026    0.1429        0.73540002    0.42399999    0.67720002
     2.71867228]
 [-101.70175646    0.43709999    0.42659998    0.56170005    0.13810001
     3.65622258]
 [  10.31321621    0.36489999    0.30930001    0.44650003    0.1059
     4.78085947]
 ...
 [-154.68961525    0.28150001    0.3592        0.2172        0.39200002
     2.52091575]
 [  81.01145433    0.40739998    0.41409999    0.35560003    0.3484
     3.01258373]
 [  82.24734188    0.2931        0.2474        0.257         0.2277
     3.65627027]][0m
[37m[1m[2023-07-17 06:32:23,650][257371] Max Reward on eval: 307.8757104406133[0m
[37m[1m[2023-07-17 06:32:23,651][257371] Min Reward on eval: -442.6751060131937[0m
[37m[1m[2023-07-17 06:32:23,651][257371] Mean Reward across all agents: 48.5169297997892[0m
[37m[1m[2023-07-17 06:32:23,651][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:32:23,656][257371] mean_value=-833.3863025311191, max_value=488.59266928426837[0m
[37m[1m[2023-07-17 06:32:23,659][257371] New mean coefficients: [[-0.00916457  0.8614988   0.9603396   1.7819964   1.1227325  -0.7757459 ]][0m
[37m[1m[2023-07-17 06:32:23,660][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:32:32,688][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 06:32:32,688][257371] FPS: 425391.87[0m
[36m[2023-07-17 06:32:32,691][257371] itr=783, itrs=2000, Progress: 39.15%[0m
[36m[2023-07-17 06:32:44,316][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 06:32:44,317][257371] FPS: 332980.03[0m
[36m[2023-07-17 06:32:48,675][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:32:48,675][257371] Reward + Measures: [[8.72939076 0.79903424 0.24164867 0.70682371 0.50634766 0.69660515]][0m
[37m[1m[2023-07-17 06:32:48,675][257371] Max Reward on eval: 8.729390757297224[0m
[37m[1m[2023-07-17 06:32:48,676][257371] Min Reward on eval: 8.729390757297224[0m
[37m[1m[2023-07-17 06:32:48,676][257371] Mean Reward across all agents: 8.729390757297224[0m
[37m[1m[2023-07-17 06:32:48,676][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:32:53,669][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:32:53,675][257371] Reward + Measures: [[  33.90499524    0.53400004    0.2254        0.61420006    0.43840003
     3.06508613]
 [-103.28455372    0.141         0.41149998    0.30249998    0.3752
     2.6074841 ]
 [ 157.10975309    0.20340002    0.42910001    0.27640003    0.37240002
     4.06413317]
 ...
 [  21.84071465    0.46680003    0.78610003    0.66720003    0.72830003
     1.16661704]
 [  73.14810448    0.24310003    0.45180002    0.36059999    0.41859999
     3.46214294]
 [  21.30596049    0.52420002    0.23799999    0.53750002    0.36219999
     4.00858259]][0m
[37m[1m[2023-07-17 06:32:53,675][257371] Max Reward on eval: 325.65232467800377[0m
[37m[1m[2023-07-17 06:32:53,675][257371] Min Reward on eval: -415.99757195822895[0m
[37m[1m[2023-07-17 06:32:53,675][257371] Mean Reward across all agents: 36.08557899774705[0m
[37m[1m[2023-07-17 06:32:53,676][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:32:53,680][257371] mean_value=-646.6530188947032, max_value=370.22677174114585[0m
[37m[1m[2023-07-17 06:32:53,683][257371] New mean coefficients: [[-0.35621178  2.3216276   0.76221514  1.4823356   0.87650937 -1.3129818 ]][0m
[37m[1m[2023-07-17 06:32:53,683][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:33:02,748][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 06:33:02,749][257371] FPS: 423687.02[0m
[36m[2023-07-17 06:33:02,751][257371] itr=784, itrs=2000, Progress: 39.20%[0m
[36m[2023-07-17 06:33:14,395][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 06:33:14,395][257371] FPS: 332377.17[0m
[36m[2023-07-17 06:33:18,622][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:33:18,623][257371] Reward + Measures: [[-28.47675164   0.68785435   0.38512963   0.71242934   0.48564136
    0.52522302]][0m
[37m[1m[2023-07-17 06:33:18,623][257371] Max Reward on eval: -28.476751636332793[0m
[37m[1m[2023-07-17 06:33:18,623][257371] Min Reward on eval: -28.476751636332793[0m
[37m[1m[2023-07-17 06:33:18,624][257371] Mean Reward across all agents: -28.476751636332793[0m
[37m[1m[2023-07-17 06:33:18,624][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:33:23,682][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:33:23,688][257371] Reward + Measures: [[  26.13888514    0.67430001    0.546         0.71209997    0.68959999
     2.34852529]
 [ -82.72862558    0.43220001    0.53329998    0.36500001    0.47909999
     1.24673975]
 [ -40.64803628    0.72329998    0.31220001    0.79500002    0.27260002
     2.06655717]
 ...
 [-104.7664531     0.63310003    0.19530001    0.61610001    0.25400001
     2.06292224]
 [-261.60999867    0.46339998    0.38680002    0.34919998    0.26730001
     2.1734035 ]
 [ -25.54734895    0.57880002    0.18549998    0.50470001    0.1409
     3.20810008]][0m
[37m[1m[2023-07-17 06:33:23,688][257371] Max Reward on eval: 232.95246553048491[0m
[37m[1m[2023-07-17 06:33:23,688][257371] Min Reward on eval: -280.2553215006512[0m
[37m[1m[2023-07-17 06:33:23,688][257371] Mean Reward across all agents: -23.382652214409447[0m
[37m[1m[2023-07-17 06:33:23,689][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:33:23,692][257371] mean_value=-300.0694153813214, max_value=373.539918104361[0m
[37m[1m[2023-07-17 06:33:23,695][257371] New mean coefficients: [[ 0.46861    2.910549   1.8896036  1.1712015  1.0161707 -1.7150087]][0m
[37m[1m[2023-07-17 06:33:23,696][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:33:32,762][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 06:33:32,763][257371] FPS: 423631.20[0m
[36m[2023-07-17 06:33:32,765][257371] itr=785, itrs=2000, Progress: 39.25%[0m
[36m[2023-07-17 06:33:44,631][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 06:33:44,631][257371] FPS: 326154.55[0m
[36m[2023-07-17 06:33:48,976][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:33:48,977][257371] Reward + Measures: [[-30.27327469   0.77374601   0.35411701   0.79075766   0.40465698
    0.45093808]][0m
[37m[1m[2023-07-17 06:33:48,977][257371] Max Reward on eval: -30.273274689331515[0m
[37m[1m[2023-07-17 06:33:48,977][257371] Min Reward on eval: -30.273274689331515[0m
[37m[1m[2023-07-17 06:33:48,978][257371] Mean Reward across all agents: -30.273274689331515[0m
[37m[1m[2023-07-17 06:33:48,978][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:33:54,319][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:33:54,320][257371] Reward + Measures: [[ 16.36978562   0.37900001   0.32780001   0.46040002   0.24249999
    3.57628179]
 [ 45.54199067   0.56569999   0.40089998   0.62269998   0.2375
    1.3428129 ]
 [-16.74384186   0.72869998   0.57179999   0.72609997   0.13329999
    0.86009854]
 ...
 [ 51.02150012   0.51960003   0.57709998   0.60069996   0.29589999
    2.06826448]
 [-37.9190073    0.66259998   0.40030003   0.71439999   0.40899998
    0.98951572]
 [ 69.59315611   0.85220003   0.72369999   0.8448       0.0401
    2.60194898]][0m
[37m[1m[2023-07-17 06:33:54,320][257371] Max Reward on eval: 218.0752824805677[0m
[37m[1m[2023-07-17 06:33:54,321][257371] Min Reward on eval: -156.77980244435602[0m
[37m[1m[2023-07-17 06:33:54,321][257371] Mean Reward across all agents: -4.951077289737382[0m
[37m[1m[2023-07-17 06:33:54,321][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:33:54,325][257371] mean_value=-320.135024458582, max_value=236.07356320397955[0m
[37m[1m[2023-07-17 06:33:54,328][257371] New mean coefficients: [[-0.32005847  2.769805    2.116287    0.8970264   1.8252165  -1.7082472 ]][0m
[37m[1m[2023-07-17 06:33:54,329][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:34:03,413][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 06:34:03,414][257371] FPS: 422773.98[0m
[36m[2023-07-17 06:34:03,416][257371] itr=786, itrs=2000, Progress: 39.30%[0m
[36m[2023-07-17 06:34:15,264][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 06:34:15,264][257371] FPS: 326827.26[0m
[36m[2023-07-17 06:34:19,626][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:34:19,631][257371] Reward + Measures: [[-34.9837487    0.81524503   0.38159603   0.80484164   0.45239297
    0.42655635]][0m
[37m[1m[2023-07-17 06:34:19,632][257371] Max Reward on eval: -34.98374870333677[0m
[37m[1m[2023-07-17 06:34:19,632][257371] Min Reward on eval: -34.98374870333677[0m
[37m[1m[2023-07-17 06:34:19,632][257371] Mean Reward across all agents: -34.98374870333677[0m
[37m[1m[2023-07-17 06:34:19,633][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:34:24,651][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:34:24,657][257371] Reward + Measures: [[-149.18454206    0.52069998    0.49000001    0.60269994    0.19890001
     1.16475534]
 [ -36.5834664     0.86990005    0.48740003    0.92319995    0.25130001
     2.65389037]
 [ -23.91272937    0.76700002    0.74150002    0.91630012    0.23629999
     1.01145494]
 ...
 [ -97.24043754    0.126         0.62660003    0.46779999    0.68049997
     1.90497482]
 [ -42.48750115    0.77869999    0.60910004    0.82910007    0.1646
     1.58204639]
 [-150.32602498    0.58529997    0.41780001    0.67900002    0.29490003
     0.92735225]][0m
[37m[1m[2023-07-17 06:34:24,657][257371] Max Reward on eval: 165.61126027777792[0m
[37m[1m[2023-07-17 06:34:24,658][257371] Min Reward on eval: -240.0412292778492[0m
[37m[1m[2023-07-17 06:34:24,658][257371] Mean Reward across all agents: -41.004982392980054[0m
[37m[1m[2023-07-17 06:34:24,658][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:34:24,664][257371] mean_value=-183.386105044859, max_value=346.6420179152747[0m
[37m[1m[2023-07-17 06:34:24,667][257371] New mean coefficients: [[-0.31727597  2.8314083   2.405872    0.9946886   1.8132135  -1.9778018 ]][0m
[37m[1m[2023-07-17 06:34:24,668][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:34:33,726][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 06:34:33,726][257371] FPS: 423994.69[0m
[36m[2023-07-17 06:34:33,729][257371] itr=787, itrs=2000, Progress: 39.35%[0m
[36m[2023-07-17 06:34:45,380][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 06:34:45,380][257371] FPS: 332255.63[0m
[36m[2023-07-17 06:34:49,741][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:34:49,741][257371] Reward + Measures: [[-41.90222864   0.88371563   0.53372967   0.81908667   0.58941537
    0.39678624]][0m
[37m[1m[2023-07-17 06:34:49,741][257371] Max Reward on eval: -41.90222864471554[0m
[37m[1m[2023-07-17 06:34:49,742][257371] Min Reward on eval: -41.90222864471554[0m
[37m[1m[2023-07-17 06:34:49,742][257371] Mean Reward across all agents: -41.90222864471554[0m
[37m[1m[2023-07-17 06:34:49,742][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:34:54,846][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:34:54,847][257371] Reward + Measures: [[ 83.65938107   0.61149997   0.81690007   0.70130002   0.74760002
    1.52880692]
 [ 86.34344346   0.21100001   0.72770005   0.50220001   0.61620003
    1.16306674]
 [ -4.92708142   0.60180002   0.44910002   0.63810003   0.47199997
    0.70968688]
 ...
 [-68.9812719    0.58090001   0.27330002   0.31439999   0.59110004
    2.63256907]
 [117.32382496   0.4824       0.77759999   0.58330005   0.67530006
    1.63454437]
 [ -2.96888434   0.36220002   0.58459997   0.56580001   0.75019997
    0.63927025]][0m
[37m[1m[2023-07-17 06:34:54,847][257371] Max Reward on eval: 157.58126739393919[0m
[37m[1m[2023-07-17 06:34:54,847][257371] Min Reward on eval: -249.9569839810021[0m
[37m[1m[2023-07-17 06:34:54,848][257371] Mean Reward across all agents: 24.400790515388397[0m
[37m[1m[2023-07-17 06:34:54,848][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:34:54,855][257371] mean_value=-122.81641724218711, max_value=557.3489217889961[0m
[37m[1m[2023-07-17 06:34:54,857][257371] New mean coefficients: [[-0.72129023  1.9194467   2.3963814   0.8370755   1.9446108  -2.063802  ]][0m
[37m[1m[2023-07-17 06:34:54,858][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:35:03,851][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 06:35:03,851][257371] FPS: 427104.21[0m
[36m[2023-07-17 06:35:03,853][257371] itr=788, itrs=2000, Progress: 39.40%[0m
[36m[2023-07-17 06:35:15,502][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 06:35:15,503][257371] FPS: 332382.13[0m
[36m[2023-07-17 06:35:19,831][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:35:19,832][257371] Reward + Measures: [[-39.07351428   0.92501593   0.68449497   0.86920631   0.73126131
    0.37423518]][0m
[37m[1m[2023-07-17 06:35:19,832][257371] Max Reward on eval: -39.07351428460159[0m
[37m[1m[2023-07-17 06:35:19,832][257371] Min Reward on eval: -39.07351428460159[0m
[37m[1m[2023-07-17 06:35:19,832][257371] Mean Reward across all agents: -39.07351428460159[0m
[37m[1m[2023-07-17 06:35:19,833][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:35:24,766][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:35:24,766][257371] Reward + Measures: [[-19.92805709   0.88280004   0.98460001   0.46090004   0.91580009
    0.52002734]
 [-30.76441433   0.31059998   0.81539994   0.223        0.76410002
    0.99240237]
 [  8.69861893   0.54890007   0.69480002   0.42670003   0.72600001
    0.87098807]
 ...
 [-82.37181996   0.68320006   0.74110001   0.59119999   0.80940002
    0.5285567 ]
 [ 33.6117676    0.41840002   0.9073       0.2316       0.90939999
    1.71405065]
 [ 28.71323038   0.65780002   0.69760001   0.66329998   0.83549994
    1.03884709]][0m
[37m[1m[2023-07-17 06:35:24,767][257371] Max Reward on eval: 164.08792302543299[0m
[37m[1m[2023-07-17 06:35:24,767][257371] Min Reward on eval: -120.90141007397324[0m
[37m[1m[2023-07-17 06:35:24,767][257371] Mean Reward across all agents: 16.212907303739495[0m
[37m[1m[2023-07-17 06:35:24,767][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:35:24,773][257371] mean_value=-101.57295424590183, max_value=103.46743636953116[0m
[37m[1m[2023-07-17 06:35:24,776][257371] New mean coefficients: [[-1.6159394  1.5335646  2.7428722  0.9891829  2.1877494 -1.9387898]][0m
[37m[1m[2023-07-17 06:35:24,777][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:35:33,743][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 06:35:33,743][257371] FPS: 428373.24[0m
[36m[2023-07-17 06:35:33,745][257371] itr=789, itrs=2000, Progress: 39.45%[0m
[36m[2023-07-17 06:35:45,412][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 06:35:45,412][257371] FPS: 331926.93[0m
[36m[2023-07-17 06:35:49,765][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:35:49,765][257371] Reward + Measures: [[-26.79966043   0.94286895   0.83367729   0.91388834   0.85097265
    0.37271586]][0m
[37m[1m[2023-07-17 06:35:49,765][257371] Max Reward on eval: -26.799660426514222[0m
[37m[1m[2023-07-17 06:35:49,765][257371] Min Reward on eval: -26.799660426514222[0m
[37m[1m[2023-07-17 06:35:49,766][257371] Mean Reward across all agents: -26.799660426514222[0m
[37m[1m[2023-07-17 06:35:49,766][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:35:54,779][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:35:54,780][257371] Reward + Measures: [[-15.33512763   0.55319995   0.61520004   0.58140004   0.63140005
    0.59839374]
 [-78.11048849   0.31490001   0.48119998   0.26089999   0.44710001
    1.76694858]
 [  2.36119383   0.17459999   0.5248       0.35700002   0.51789999
    1.46198106]
 ...
 [ 17.72941872   0.2836       0.57810003   0.28850001   0.52089995
    1.28020501]
 [ -9.60776084   0.44299999   0.87890005   0.1318       0.8854
    0.66485929]
 [ -1.09226335   0.77220005   0.44150001   0.74469995   0.53179997
    0.49224266]][0m
[37m[1m[2023-07-17 06:35:54,780][257371] Max Reward on eval: 148.98346852362155[0m
[37m[1m[2023-07-17 06:35:54,780][257371] Min Reward on eval: -117.50597384438151[0m
[37m[1m[2023-07-17 06:35:54,780][257371] Mean Reward across all agents: 1.2998471054342644[0m
[37m[1m[2023-07-17 06:35:54,781][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:35:54,784][257371] mean_value=-244.91697901539612, max_value=132.32141781411605[0m
[37m[1m[2023-07-17 06:35:54,786][257371] New mean coefficients: [[-0.3914013  0.661875   3.2137086  1.1189488  2.3375287 -2.030836 ]][0m
[37m[1m[2023-07-17 06:35:54,787][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:36:03,770][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 06:36:03,770][257371] FPS: 427549.00[0m
[36m[2023-07-17 06:36:03,773][257371] itr=790, itrs=2000, Progress: 39.50%[0m
[37m[1m[2023-07-17 06:39:15,747][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000770[0m
[36m[2023-07-17 06:39:28,411][257371] train() took 11.91 seconds to complete[0m
[36m[2023-07-17 06:39:28,411][257371] FPS: 322528.51[0m
[36m[2023-07-17 06:39:32,652][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:39:32,652][257371] Reward + Measures: [[-14.61742758   0.96060234   0.92849624   0.94917238   0.95385903
    0.34906912]][0m
[37m[1m[2023-07-17 06:39:32,652][257371] Max Reward on eval: -14.6174275849363[0m
[37m[1m[2023-07-17 06:39:32,653][257371] Min Reward on eval: -14.6174275849363[0m
[37m[1m[2023-07-17 06:39:32,653][257371] Mean Reward across all agents: -14.6174275849363[0m
[37m[1m[2023-07-17 06:39:32,653][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:39:37,820][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:39:37,821][257371] Reward + Measures: [[-28.20136266   0.11600001   0.86120003   0.54650003   0.8653
    0.60833609]
 [ -5.32290599   0.34249997   0.56559998   0.54399997   0.32449999
    2.58864474]
 [  6.99386089   0.78280002   0.1367       0.82050002   0.7653001
    1.42017436]
 ...
 [  5.81070506   0.32220003   0.68330002   0.56739998   0.53590006
    0.95935249]
 [  8.63205243   0.42000005   0.67570001   0.6311       0.39270002
    1.37578201]
 [131.86900135   0.17990002   0.53849995   0.34309998   0.33870003
    3.25464487]][0m
[37m[1m[2023-07-17 06:39:37,821][257371] Max Reward on eval: 458.0316848922521[0m
[37m[1m[2023-07-17 06:39:37,821][257371] Min Reward on eval: -158.7759298954392[0m
[37m[1m[2023-07-17 06:39:37,822][257371] Mean Reward across all agents: -5.7599905430613445[0m
[37m[1m[2023-07-17 06:39:37,822][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:39:37,826][257371] mean_value=-382.14265750615743, max_value=415.1270171461375[0m
[37m[1m[2023-07-17 06:39:37,835][257371] New mean coefficients: [[-0.04536879  0.5277414   2.3606715   1.3594186   3.2024364  -1.5746512 ]][0m
[37m[1m[2023-07-17 06:39:37,836][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:39:46,858][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 06:39:46,858][257371] FPS: 425700.53[0m
[36m[2023-07-17 06:39:46,860][257371] itr=791, itrs=2000, Progress: 39.55%[0m
[36m[2023-07-17 06:39:58,687][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 06:39:58,687][257371] FPS: 328737.87[0m
[36m[2023-07-17 06:40:02,920][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:40:02,920][257371] Reward + Measures: [[-1.16609367  0.98914963  0.98876429  0.98550236  0.99388868  0.29629922]][0m
[37m[1m[2023-07-17 06:40:02,920][257371] Max Reward on eval: -1.1660936674009135[0m
[37m[1m[2023-07-17 06:40:02,921][257371] Min Reward on eval: -1.1660936674009135[0m
[37m[1m[2023-07-17 06:40:02,921][257371] Mean Reward across all agents: -1.1660936674009135[0m
[37m[1m[2023-07-17 06:40:02,921][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:40:07,873][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:40:07,874][257371] Reward + Measures: [[-58.05970121   0.37020001   0.60430002   0.3281       0.62470001
    1.18167686]
 [-48.67592369   0.0229       0.80730003   0.61680001   0.66100001
    1.6195296 ]
 [-77.09523108   0.6182       0.56160003   0.51539999   0.35689998
    1.45472562]
 ...
 [ 37.2076265    0.41580001   0.62410003   0.63819999   0.44530001
    1.89920509]
 [-28.65255023   0.88970006   0.95410007   0.11390001   0.9465
    0.3935954 ]
 [ 19.53678151   0.2534       0.94880003   0.43630001   0.96020001
    1.30553055]][0m
[37m[1m[2023-07-17 06:40:07,874][257371] Max Reward on eval: 401.5308952391148[0m
[37m[1m[2023-07-17 06:40:07,874][257371] Min Reward on eval: -222.66240691517015[0m
[37m[1m[2023-07-17 06:40:07,874][257371] Mean Reward across all agents: -22.348967529247883[0m
[37m[1m[2023-07-17 06:40:07,875][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:40:07,878][257371] mean_value=-551.877991161223, max_value=521.05655638779[0m
[37m[1m[2023-07-17 06:40:07,881][257371] New mean coefficients: [[-0.60796326  0.18202507  2.270328    1.5337424   3.1475441  -0.519336  ]][0m
[37m[1m[2023-07-17 06:40:07,882][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:40:16,827][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 06:40:16,828][257371] FPS: 429355.12[0m
[36m[2023-07-17 06:40:16,830][257371] itr=792, itrs=2000, Progress: 39.60%[0m
[36m[2023-07-17 06:40:28,561][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 06:40:28,562][257371] FPS: 329980.84[0m
[36m[2023-07-17 06:40:32,844][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:40:32,845][257371] Reward + Measures: [[0.51777042 0.98136997 0.99292368 0.98250926 0.99436897 0.26624992]][0m
[37m[1m[2023-07-17 06:40:32,845][257371] Max Reward on eval: 0.5177704230972049[0m
[37m[1m[2023-07-17 06:40:32,845][257371] Min Reward on eval: 0.5177704230972049[0m
[37m[1m[2023-07-17 06:40:32,845][257371] Mean Reward across all agents: 0.5177704230972049[0m
[37m[1m[2023-07-17 06:40:32,846][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:40:37,862][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:40:37,862][257371] Reward + Measures: [[ -58.96607946    0.51960003    0.33399999    0.49860001    0.38409999
     2.13235879]
 [-104.08237221    0.27920002    0.78840005    0.44150001    0.75590003
     0.62763792]
 [ -21.92723019    0.30400002    0.54879999    0.37090001    0.46040002
     1.09737766]
 ...
 [ -11.973159      0.89659995    0.95459998    0.81989998    0.96200001
     0.54637879]
 [-137.08005598    0.34029999    0.58260006    0.41370001    0.57060003
     1.0138371 ]
 [ -45.6613684     0.51879996    0.81459999    0.53560001    0.72249997
     0.85921186]][0m
[37m[1m[2023-07-17 06:40:37,863][257371] Max Reward on eval: 188.17723272657022[0m
[37m[1m[2023-07-17 06:40:37,863][257371] Min Reward on eval: -287.12331868447365[0m
[37m[1m[2023-07-17 06:40:37,863][257371] Mean Reward across all agents: -9.551719106691575[0m
[37m[1m[2023-07-17 06:40:37,863][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:40:37,868][257371] mean_value=-486.76810584865115, max_value=230.8326053095642[0m
[37m[1m[2023-07-17 06:40:37,870][257371] New mean coefficients: [[-0.0554859  -0.15354651  2.3992662   1.1938362   2.9751234  -1.8259032 ]][0m
[37m[1m[2023-07-17 06:40:37,871][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:40:46,897][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 06:40:46,897][257371] FPS: 425533.67[0m
[36m[2023-07-17 06:40:46,899][257371] itr=793, itrs=2000, Progress: 39.65%[0m
[36m[2023-07-17 06:40:58,468][257371] train() took 11.47 seconds to complete[0m
[36m[2023-07-17 06:40:58,468][257371] FPS: 334776.52[0m
[36m[2023-07-17 06:41:02,768][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:41:02,768][257371] Reward + Measures: [[0.66552135 0.98280466 0.99432468 0.98476934 0.99293131 0.21819197]][0m
[37m[1m[2023-07-17 06:41:02,769][257371] Max Reward on eval: 0.6655213533718973[0m
[37m[1m[2023-07-17 06:41:02,769][257371] Min Reward on eval: 0.6655213533718973[0m
[37m[1m[2023-07-17 06:41:02,769][257371] Mean Reward across all agents: 0.6655213533718973[0m
[37m[1m[2023-07-17 06:41:02,769][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:41:07,727][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:41:07,728][257371] Reward + Measures: [[ 51.61796499   0.4499       0.84170002   0.24819998   0.86630005
    0.82760161]
 [ 52.30125526   0.43850002   0.85079998   0.32870004   0.85659999
    0.85726166]
 [163.32607365   0.55220002   0.3563       0.77529997   0.71520001
    1.91331518]
 ...
 [-47.30327037   0.39179999   0.47450003   0.16689999   0.58880007
    2.51893997]
 [-46.5432364    0.35190001   0.47150001   0.1031       0.56290001
    4.09151077]
 [ -5.85947125   0.31300002   0.32760003   0.24660002   0.38409999
    2.66959548]][0m
[37m[1m[2023-07-17 06:41:07,728][257371] Max Reward on eval: 306.78994182497263[0m
[37m[1m[2023-07-17 06:41:07,728][257371] Min Reward on eval: -173.1307364146225[0m
[37m[1m[2023-07-17 06:41:07,728][257371] Mean Reward across all agents: 29.15190325739093[0m
[37m[1m[2023-07-17 06:41:07,729][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:41:07,735][257371] mean_value=-429.0187461564173, max_value=628.7495241760312[0m
[37m[1m[2023-07-17 06:41:07,737][257371] New mean coefficients: [[ 0.9707783 -1.2207589  2.4505916  0.779282   2.8850892 -1.4113069]][0m
[37m[1m[2023-07-17 06:41:07,738][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:41:16,804][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 06:41:16,805][257371] FPS: 423627.85[0m
[36m[2023-07-17 06:41:16,807][257371] itr=794, itrs=2000, Progress: 39.70%[0m
[36m[2023-07-17 06:41:28,706][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 06:41:28,707][257371] FPS: 325430.44[0m
[36m[2023-07-17 06:41:33,080][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:41:33,080][257371] Reward + Measures: [[1.87609336 0.97472697 0.99545467 0.97725928 0.99603862 0.18045959]][0m
[37m[1m[2023-07-17 06:41:33,081][257371] Max Reward on eval: 1.8760933575898706[0m
[37m[1m[2023-07-17 06:41:33,081][257371] Min Reward on eval: 1.8760933575898706[0m
[37m[1m[2023-07-17 06:41:33,081][257371] Mean Reward across all agents: 1.8760933575898706[0m
[37m[1m[2023-07-17 06:41:33,081][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:41:38,054][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:41:38,055][257371] Reward + Measures: [[-17.70717441   0.69330001   0.5011       0.55440003   0.3867
    0.599819  ]
 [-23.24979973   0.73159999   0.7694       0.70490009   0.63010007
    1.0589999 ]
 [  9.98578355   0.2304       0.7177       0.30130002   0.61580002
    1.04030371]
 ...
 [ 95.51752209   0.21069999   0.75410002   0.24419999   0.61989999
    2.63265991]
 [ 29.41788186   0.396        0.30750003   0.45440003   0.16989999
    2.8732326 ]
 [ 24.79154649   0.24730001   0.93440002   0.54110003   0.87859994
    0.33796149]][0m
[37m[1m[2023-07-17 06:41:38,055][257371] Max Reward on eval: 155.0509469552897[0m
[37m[1m[2023-07-17 06:41:38,055][257371] Min Reward on eval: -281.8363046481274[0m
[37m[1m[2023-07-17 06:41:38,056][257371] Mean Reward across all agents: -0.42213530610022476[0m
[37m[1m[2023-07-17 06:41:38,056][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:41:38,060][257371] mean_value=-429.48325280801265, max_value=92.71023144410475[0m
[37m[1m[2023-07-17 06:41:38,063][257371] New mean coefficients: [[ 0.4761528  -0.23373812  2.5742495   0.9605063   2.6676245  -0.6104363 ]][0m
[37m[1m[2023-07-17 06:41:38,064][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:41:47,036][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 06:41:47,036][257371] FPS: 428050.16[0m
[36m[2023-07-17 06:41:47,038][257371] itr=795, itrs=2000, Progress: 39.75%[0m
[36m[2023-07-17 06:41:58,751][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 06:41:58,752][257371] FPS: 330547.77[0m
[36m[2023-07-17 06:42:03,069][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:42:03,069][257371] Reward + Measures: [[22.49278304  0.5426513   0.952389    0.69653404  0.94673967  0.41026095]][0m
[37m[1m[2023-07-17 06:42:03,069][257371] Max Reward on eval: 22.49278304399588[0m
[37m[1m[2023-07-17 06:42:03,070][257371] Min Reward on eval: 22.49278304399588[0m
[37m[1m[2023-07-17 06:42:03,070][257371] Mean Reward across all agents: 22.49278304399588[0m
[37m[1m[2023-07-17 06:42:03,070][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:42:08,121][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:42:08,122][257371] Reward + Measures: [[ 89.76866968   0.32650003   0.53720003   0.36550003   0.40169999
    1.02133596]
 [131.96619201   0.38340002   0.53520006   0.28449997   0.44239998
    1.17173123]
 [-16.45349734   0.62660003   0.93419999   0.2131       0.68230003
    0.87522119]
 ...
 [-52.60570581   0.33419999   0.44820005   0.2626       0.25780001
    1.95397973]
 [ -6.35341422   0.76430005   0.70020002   0.81910002   0.65560001
    1.44197893]
 [  4.3002477    0.0436       0.92180008   0.53780001   0.90460008
    1.97929537]][0m
[37m[1m[2023-07-17 06:42:08,122][257371] Max Reward on eval: 247.82013227967545[0m
[37m[1m[2023-07-17 06:42:08,122][257371] Min Reward on eval: -277.13873288761823[0m
[37m[1m[2023-07-17 06:42:08,123][257371] Mean Reward across all agents: -19.194051582684107[0m
[37m[1m[2023-07-17 06:42:08,123][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:42:08,128][257371] mean_value=-661.7006906970838, max_value=180.15521456335722[0m
[37m[1m[2023-07-17 06:42:08,131][257371] New mean coefficients: [[0.39305744 0.3879385  2.4298685  0.92033285 2.7064433  0.5132934 ]][0m
[37m[1m[2023-07-17 06:42:08,131][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:42:17,155][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 06:42:17,155][257371] FPS: 425652.32[0m
[36m[2023-07-17 06:42:17,157][257371] itr=796, itrs=2000, Progress: 39.80%[0m
[36m[2023-07-17 06:42:29,030][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 06:42:29,031][257371] FPS: 326123.50[0m
[36m[2023-07-17 06:42:33,389][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:42:33,389][257371] Reward + Measures: [[35.09663823  0.42764431  0.99364263  0.56794369  0.99299461  1.21075094]][0m
[37m[1m[2023-07-17 06:42:33,389][257371] Max Reward on eval: 35.0966382282647[0m
[37m[1m[2023-07-17 06:42:33,390][257371] Min Reward on eval: 35.0966382282647[0m
[37m[1m[2023-07-17 06:42:33,390][257371] Mean Reward across all agents: 35.0966382282647[0m
[37m[1m[2023-07-17 06:42:33,390][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:42:38,663][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:42:38,664][257371] Reward + Measures: [[  -5.61292695    0.9497        0.87529993    0.90130007    0.93620008
     1.72925365]
 [  17.24823072    0.419         0.78890002    0.25750002    0.76539999
     2.51062751]
 [  40.28368267    0.56680006    0.83929998    0.72550005    0.86000007
     1.75958288]
 ...
 [-226.19222429    0.4434        0.2771        0.50950003    0.1626
     2.33397746]
 [   1.76559625    0.75870001    0.95819998    0.76220006    0.92609996
     0.55393618]
 [  29.18512579    0.40570003    0.42519999    0.46259999    0.50850004
     2.43058205]][0m
[37m[1m[2023-07-17 06:42:38,664][257371] Max Reward on eval: 171.01903887866064[0m
[37m[1m[2023-07-17 06:42:38,665][257371] Min Reward on eval: -238.24042890295385[0m
[37m[1m[2023-07-17 06:42:38,665][257371] Mean Reward across all agents: -6.534638803639754[0m
[37m[1m[2023-07-17 06:42:38,665][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:42:38,670][257371] mean_value=-279.38656702309487, max_value=186.5784946909934[0m
[37m[1m[2023-07-17 06:42:38,672][257371] New mean coefficients: [[-0.05339676  0.43582624  2.1451783  -0.30283648  2.4037728   0.8567361 ]][0m
[37m[1m[2023-07-17 06:42:38,673][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:42:47,802][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 06:42:47,803][257371] FPS: 420704.97[0m
[36m[2023-07-17 06:42:47,805][257371] itr=797, itrs=2000, Progress: 39.85%[0m
[36m[2023-07-17 06:42:59,668][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 06:42:59,668][257371] FPS: 326348.66[0m
[36m[2023-07-17 06:43:04,047][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:43:04,048][257371] Reward + Measures: [[-175.86413998    0.05270433    0.99111032    0.90296233    0.98902732
     3.76897264]][0m
[37m[1m[2023-07-17 06:43:04,048][257371] Max Reward on eval: -175.86413997910878[0m
[37m[1m[2023-07-17 06:43:04,048][257371] Min Reward on eval: -175.86413997910878[0m
[37m[1m[2023-07-17 06:43:04,048][257371] Mean Reward across all agents: -175.86413997910878[0m
[37m[1m[2023-07-17 06:43:04,049][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:43:09,080][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:43:09,081][257371] Reward + Measures: [[-85.17790961   0.37779999   0.40550002   0.25130001   0.46960002
    2.95377016]
 [-19.44049753   0.16159999   0.2201       0.1802       0.22129999
    3.85476232]
 [-40.15103129   0.17330001   0.16940001   0.19860001   0.15220001
    3.9012506 ]
 ...
 [ -7.92735698   0.76700002   0.74869996   0.72629994   0.77570003
    4.44456625]
 [ 68.48628306   0.29140002   0.23170002   0.25330001   0.278
    4.28632212]
 [ 73.40547717   0.41589999   0.5          0.37740001   0.60370004
    3.09244204]][0m
[37m[1m[2023-07-17 06:43:09,081][257371] Max Reward on eval: 560.7780532887206[0m
[37m[1m[2023-07-17 06:43:09,081][257371] Min Reward on eval: -250.97579142004253[0m
[37m[1m[2023-07-17 06:43:09,082][257371] Mean Reward across all agents: 2.7241470163437405[0m
[37m[1m[2023-07-17 06:43:09,082][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:43:09,086][257371] mean_value=-677.0227670532252, max_value=457.9274177832783[0m
[37m[1m[2023-07-17 06:43:09,089][257371] New mean coefficients: [[0.39919236 0.26146227 1.917126   0.0438453  1.6598318  1.0510333 ]][0m
[37m[1m[2023-07-17 06:43:09,090][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:43:18,127][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 06:43:18,127][257371] FPS: 424992.34[0m
[36m[2023-07-17 06:43:18,129][257371] itr=798, itrs=2000, Progress: 39.90%[0m
[36m[2023-07-17 06:43:29,878][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 06:43:29,879][257371] FPS: 329485.16[0m
[36m[2023-07-17 06:43:34,226][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:43:34,226][257371] Reward + Measures: [[480.8664267    0.00186      0.99432534   0.98231298   0.98768765
    5.6821022 ]][0m
[37m[1m[2023-07-17 06:43:34,226][257371] Max Reward on eval: 480.86642669896315[0m
[37m[1m[2023-07-17 06:43:34,227][257371] Min Reward on eval: 480.86642669896315[0m
[37m[1m[2023-07-17 06:43:34,227][257371] Mean Reward across all agents: 480.86642669896315[0m
[37m[1m[2023-07-17 06:43:34,227][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:43:39,262][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:43:39,263][257371] Reward + Measures: [[-31.89615955   0.0562       0.90869999   0.7374       0.92080003
    5.68239832]
 [-77.20358324   0.741        0.63490003   0.68109995   0.37579998
    3.31182551]
 [123.90343662   0.0454       0.96450007   0.56049997   0.95540011
    5.80544376]
 ...
 [-20.24158863   0.49580002   0.46300003   0.53460002   0.15460001
    4.75189447]
 [-49.25754592   0.58970004   0.33680001   0.50509995   0.2561
    5.19542456]
 [ 28.06065171   0.2746       0.39210001   0.2105       0.37579998
    3.03077507]][0m
[37m[1m[2023-07-17 06:43:39,263][257371] Max Reward on eval: 859.0127411060035[0m
[37m[1m[2023-07-17 06:43:39,264][257371] Min Reward on eval: -263.8080606541829[0m
[37m[1m[2023-07-17 06:43:39,264][257371] Mean Reward across all agents: 69.99251860802528[0m
[37m[1m[2023-07-17 06:43:39,264][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:43:39,270][257371] mean_value=-617.6617478845993, max_value=409.6669578769182[0m
[37m[1m[2023-07-17 06:43:39,273][257371] New mean coefficients: [[ 0.08838007 -0.4722345   1.1733      0.10427164  1.6782897   1.5125372 ]][0m
[37m[1m[2023-07-17 06:43:39,274][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:43:48,281][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 06:43:48,282][257371] FPS: 426392.03[0m
[36m[2023-07-17 06:43:48,284][257371] itr=799, itrs=2000, Progress: 39.95%[0m
[36m[2023-07-17 06:44:00,042][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 06:44:00,042][257371] FPS: 329304.93[0m
[36m[2023-07-17 06:44:04,300][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:44:04,300][257371] Reward + Measures: [[585.8687469    0.00184967   0.99173862   0.98143798   0.98480535
    7.3656106 ]][0m
[37m[1m[2023-07-17 06:44:04,301][257371] Max Reward on eval: 585.8687468962578[0m
[37m[1m[2023-07-17 06:44:04,301][257371] Min Reward on eval: 585.8687468962578[0m
[37m[1m[2023-07-17 06:44:04,301][257371] Mean Reward across all agents: 585.8687468962578[0m
[37m[1m[2023-07-17 06:44:04,301][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:44:09,311][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:44:09,312][257371] Reward + Measures: [[ 62.05923042   0.40080005   0.5643       0.24590002   0.5363
    3.45446587]
 [ 90.03658093   0.19340001   0.5521       0.52240002   0.59729999
    4.54467249]
 [184.76907687   0.45640001   0.44689998   0.1638       0.52000004
    2.67192793]
 ...
 [-34.37391879   0.0169       0.81280005   0.77850002   0.81630003
    5.42920828]
 [ 79.87622026   0.27330002   0.67500001   0.63180006   0.7306
    4.35353756]
 [ 86.32137526   0.90679997   0.95220006   0.94810003   0.89180005
    5.14040184]][0m
[37m[1m[2023-07-17 06:44:09,312][257371] Max Reward on eval: 793.6019287019968[0m
[37m[1m[2023-07-17 06:44:09,312][257371] Min Reward on eval: -467.17767594615[0m
[37m[1m[2023-07-17 06:44:09,312][257371] Mean Reward across all agents: 66.60316546881666[0m
[37m[1m[2023-07-17 06:44:09,313][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:44:09,318][257371] mean_value=-348.6549845824576, max_value=519.0910944400802[0m
[37m[1m[2023-07-17 06:44:09,321][257371] New mean coefficients: [[-0.05945133 -0.39333352  1.8505092   0.195147    1.386544    1.8132017 ]][0m
[37m[1m[2023-07-17 06:44:09,322][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:44:18,461][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 06:44:18,461][257371] FPS: 420243.64[0m
[36m[2023-07-17 06:44:18,463][257371] itr=800, itrs=2000, Progress: 40.00%[0m
[37m[1m[2023-07-17 06:47:35,457][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000780[0m
[36m[2023-07-17 06:47:48,476][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 06:47:48,476][257371] FPS: 328476.42[0m
[36m[2023-07-17 06:47:52,756][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:47:52,756][257371] Reward + Measures: [[515.75007569   0.00171033   0.99265671   0.981659     0.98519564
    7.21109295]][0m
[37m[1m[2023-07-17 06:47:52,757][257371] Max Reward on eval: 515.7500756894212[0m
[37m[1m[2023-07-17 06:47:52,757][257371] Min Reward on eval: 515.7500756894212[0m
[37m[1m[2023-07-17 06:47:52,757][257371] Mean Reward across all agents: 515.7500756894212[0m
[37m[1m[2023-07-17 06:47:52,758][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:47:57,657][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:47:57,658][257371] Reward + Measures: [[ 197.76464748    0.8502        0.95160002    0.0056        0.97420007
     6.79864264]
 [ -90.4858058     0.30780002    0.82940006    0.78599995    0.56980002
     6.02400398]
 [ 127.24398806    0.0066        0.89540005    0.88660002    0.89850008
     6.50784254]
 ...
 [-113.13424779    0.68340003    0.95069999    0.            0.7676
     3.64227223]
 [ -51.3015547     0.0045        0.99109995    0.87670004    0.96140003
     5.83829451]
 [  48.81273271    0.36859998    0.99469995    0.27380002    0.97360003
     6.12734222]][0m
[37m[1m[2023-07-17 06:47:57,658][257371] Max Reward on eval: 885.1698837351054[0m
[37m[1m[2023-07-17 06:47:57,658][257371] Min Reward on eval: -613.031368297711[0m
[37m[1m[2023-07-17 06:47:57,658][257371] Mean Reward across all agents: 95.94059331419325[0m
[37m[1m[2023-07-17 06:47:57,659][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:47:57,664][257371] mean_value=-281.9388921978946, max_value=579.0463692851208[0m
[37m[1m[2023-07-17 06:47:57,681][257371] New mean coefficients: [[-0.38408738 -0.4016808   1.2577376  -0.01320738  1.630451    1.517895  ]][0m
[37m[1m[2023-07-17 06:47:57,682][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:48:06,720][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 06:48:06,720][257371] FPS: 424935.10[0m
[36m[2023-07-17 06:48:06,722][257371] itr=801, itrs=2000, Progress: 40.05%[0m
[36m[2023-07-17 06:48:18,317][257371] train() took 11.50 seconds to complete[0m
[36m[2023-07-17 06:48:18,317][257371] FPS: 334056.81[0m
[36m[2023-07-17 06:48:22,515][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:48:22,515][257371] Reward + Measures: [[544.65153528   0.026496     0.94686532   0.89447266   0.89587694
    6.62496233]][0m
[37m[1m[2023-07-17 06:48:22,515][257371] Max Reward on eval: 544.6515352811984[0m
[37m[1m[2023-07-17 06:48:22,516][257371] Min Reward on eval: 544.6515352811984[0m
[37m[1m[2023-07-17 06:48:22,516][257371] Mean Reward across all agents: 544.6515352811984[0m
[37m[1m[2023-07-17 06:48:22,516][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:48:27,381][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:48:27,382][257371] Reward + Measures: [[ 33.84752179   0.22409999   0.41020003   0.3299       0.48210001
    4.45513105]
 [-11.5471324    0.28320003   0.67200005   0.2748       0.77529997
    2.60008407]
 [321.17674543   0.19849999   0.79740006   0.98349994   0.98390007
    7.96892691]
 ...
 [-86.23514574   0.2454       0.23400001   0.36930001   0.27560002
    5.65448904]
 [  3.39178429   0.5169       0.7015       0.17540002   0.66149998
    4.64914513]
 [ 44.15955939   0.30039999   0.46890002   0.58170003   0.45199999
    3.96168971]][0m
[37m[1m[2023-07-17 06:48:27,382][257371] Max Reward on eval: 854.1864090174437[0m
[37m[1m[2023-07-17 06:48:27,382][257371] Min Reward on eval: -510.4706573376432[0m
[37m[1m[2023-07-17 06:48:27,382][257371] Mean Reward across all agents: 69.44550901890302[0m
[37m[1m[2023-07-17 06:48:27,383][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:48:27,387][257371] mean_value=-286.17801646687116, max_value=595.84032231613[0m
[37m[1m[2023-07-17 06:48:27,390][257371] New mean coefficients: [[0.04730049 0.49763897 0.5054817  0.39931124 1.5220212  2.0843666 ]][0m
[37m[1m[2023-07-17 06:48:27,391][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:48:36,314][257371] train() took 8.92 seconds to complete[0m
[36m[2023-07-17 06:48:36,314][257371] FPS: 430449.57[0m
[36m[2023-07-17 06:48:36,316][257371] itr=802, itrs=2000, Progress: 40.10%[0m
[36m[2023-07-17 06:48:48,051][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 06:48:48,051][257371] FPS: 329939.09[0m
[36m[2023-07-17 06:48:52,368][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:48:52,368][257371] Reward + Measures: [[559.60764635   0.00149267   0.99638671   0.97871369   0.98451841
    6.70606518]][0m
[37m[1m[2023-07-17 06:48:52,369][257371] Max Reward on eval: 559.6076463530811[0m
[37m[1m[2023-07-17 06:48:52,369][257371] Min Reward on eval: 559.6076463530811[0m
[37m[1m[2023-07-17 06:48:52,369][257371] Mean Reward across all agents: 559.6076463530811[0m
[37m[1m[2023-07-17 06:48:52,369][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:48:57,387][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:48:57,388][257371] Reward + Measures: [[155.719316     0.5133       0.77520001   0.57260001   0.52110004
    4.58723307]
 [ 36.76482537   0.13330001   0.16060001   0.21139999   0.14210001
    3.25254989]
 [-24.38547263   0.8160001    0.81750005   0.80509996   0.74360007
    4.55007505]
 ...
 [ 73.16556762   0.0037       0.99050009   0.884        0.94700003
    7.89880371]
 [ 86.48019428   0.41070005   0.47639999   0.27200001   0.46289998
    3.26656532]
 [ 47.78230242   0.82050002   0.86970007   0.63689995   0.80499995
    4.72693682]][0m
[37m[1m[2023-07-17 06:48:57,388][257371] Max Reward on eval: 723.7248406298459[0m
[37m[1m[2023-07-17 06:48:57,388][257371] Min Reward on eval: -350.75204084906727[0m
[37m[1m[2023-07-17 06:48:57,389][257371] Mean Reward across all agents: 86.25707834587956[0m
[37m[1m[2023-07-17 06:48:57,389][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:48:57,395][257371] mean_value=-389.2794683362112, max_value=517.3228131486487[0m
[37m[1m[2023-07-17 06:48:57,398][257371] New mean coefficients: [[-0.06185084  0.5498453   1.1458112   0.908093    1.4123948   1.9467459 ]][0m
[37m[1m[2023-07-17 06:48:57,399][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:49:06,516][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 06:49:06,516][257371] FPS: 421257.51[0m
[36m[2023-07-17 06:49:06,519][257371] itr=803, itrs=2000, Progress: 40.15%[0m
[36m[2023-07-17 06:49:18,147][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 06:49:18,148][257371] FPS: 333068.08[0m
[36m[2023-07-17 06:49:22,339][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:49:22,339][257371] Reward + Measures: [[36.78441431  0.002377    0.99132663  0.97026098  0.97805941  6.81925631]][0m
[37m[1m[2023-07-17 06:49:22,340][257371] Max Reward on eval: 36.784414309123704[0m
[37m[1m[2023-07-17 06:49:22,340][257371] Min Reward on eval: 36.784414309123704[0m
[37m[1m[2023-07-17 06:49:22,340][257371] Mean Reward across all agents: 36.784414309123704[0m
[37m[1m[2023-07-17 06:49:22,340][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:49:27,504][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:49:27,505][257371] Reward + Measures: [[105.30877046   0.20240001   0.42560005   0.3303       0.44759998
    3.8505466 ]
 [ 63.18915413   0.0018       0.99650002   0.97600001   0.98430008
    5.36475706]
 [143.5994725    0.98920006   0.98409998   0.97749996   0.97170001
    5.19206953]
 ...
 [161.75273606   0.66100001   0.59149998   0.6965       0.14570001
    4.04221869]
 [139.05161431   0.2352       0.2879       0.36609998   0.26540002
    3.20161819]
 [ 80.3803155    0.38669997   0.44080001   0.60979998   0.34399998
    2.10743499]][0m
[37m[1m[2023-07-17 06:49:27,505][257371] Max Reward on eval: 816.1375465399585[0m
[37m[1m[2023-07-17 06:49:27,505][257371] Min Reward on eval: -371.97100826129315[0m
[37m[1m[2023-07-17 06:49:27,505][257371] Mean Reward across all agents: 98.15981278969865[0m
[37m[1m[2023-07-17 06:49:27,506][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:49:27,511][257371] mean_value=-443.7627357648145, max_value=528.5056664323055[0m
[37m[1m[2023-07-17 06:49:27,514][257371] New mean coefficients: [[-0.35173097  0.5304187   0.99163365 -0.00518698  1.5367571   1.9527702 ]][0m
[37m[1m[2023-07-17 06:49:27,515][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:49:36,494][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 06:49:36,494][257371] FPS: 427724.89[0m
[36m[2023-07-17 06:49:36,497][257371] itr=804, itrs=2000, Progress: 40.20%[0m
[36m[2023-07-17 06:49:48,145][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 06:49:48,145][257371] FPS: 332517.53[0m
[36m[2023-07-17 06:49:52,384][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:49:52,384][257371] Reward + Measures: [[568.09171664   0.00155167   0.99482733   0.97821295   0.98408395
    6.77577257]][0m
[37m[1m[2023-07-17 06:49:52,384][257371] Max Reward on eval: 568.0917166439167[0m
[37m[1m[2023-07-17 06:49:52,385][257371] Min Reward on eval: 568.0917166439167[0m
[37m[1m[2023-07-17 06:49:52,385][257371] Mean Reward across all agents: 568.0917166439167[0m
[37m[1m[2023-07-17 06:49:52,385][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:49:57,437][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:49:57,438][257371] Reward + Measures: [[218.46052027   0.0425       0.8865       0.79179996   0.86790001
    4.87931585]
 [ 80.65542739   0.0096       0.96789998   0.85500002   0.95380002
    3.94852066]
 [ 74.13597689   0.83840001   0.27260002   0.87900001   0.46480003
    3.83819938]
 ...
 [ 39.10513411   0.15370001   0.5122       0.2746       0.5291
    2.22395015]
 [-59.82432509   0.83460009   0.90700006   0.80919993   0.86110002
    5.64079428]
 [-99.62118555   0.47420001   0.7295       0.1804       0.7335
    4.37159872]][0m
[37m[1m[2023-07-17 06:49:57,438][257371] Max Reward on eval: 825.7834956645966[0m
[37m[1m[2023-07-17 06:49:57,438][257371] Min Reward on eval: -686.4935646105557[0m
[37m[1m[2023-07-17 06:49:57,439][257371] Mean Reward across all agents: 86.86333930345943[0m
[37m[1m[2023-07-17 06:49:57,439][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:49:57,444][257371] mean_value=-372.82332197671406, max_value=417.78120619797994[0m
[37m[1m[2023-07-17 06:49:57,447][257371] New mean coefficients: [[-0.30804384  0.42091864  0.4606393  -0.15017354  1.4842653   1.9922501 ]][0m
[37m[1m[2023-07-17 06:49:57,448][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:50:06,406][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 06:50:06,407][257371] FPS: 428701.79[0m
[36m[2023-07-17 06:50:06,409][257371] itr=805, itrs=2000, Progress: 40.25%[0m
[36m[2023-07-17 06:50:18,150][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 06:50:18,151][257371] FPS: 329738.15[0m
[36m[2023-07-17 06:50:22,481][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:50:22,481][257371] Reward + Measures: [[-38.63505497   0.00731267   0.99189889   0.9674173    0.98622465
    6.04543495]][0m
[37m[1m[2023-07-17 06:50:22,481][257371] Max Reward on eval: -38.635054972864175[0m
[37m[1m[2023-07-17 06:50:22,482][257371] Min Reward on eval: -38.635054972864175[0m
[37m[1m[2023-07-17 06:50:22,482][257371] Mean Reward across all agents: -38.635054972864175[0m
[37m[1m[2023-07-17 06:50:22,482][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:50:27,521][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:50:27,521][257371] Reward + Measures: [[220.04472365   0.1117       0.74949998   0.48010001   0.76980001
    4.31126738]
 [101.65843679   0.45800003   0.50550002   0.55759996   0.69869995
    2.51514816]
 [ -5.21254635   0.62059999   0.82699996   0.3809       0.67679995
    5.65915966]
 ...
 [435.35717394   0.0159       0.90869999   0.87290001   0.89239997
    6.80458403]
 [ 97.13834521   0.53540003   0.51890004   0.41340002   0.53000003
    5.52753067]
 [257.44640236   0.122        0.38840002   0.35339999   0.40830001
    6.14048958]][0m
[37m[1m[2023-07-17 06:50:27,521][257371] Max Reward on eval: 878.0487518673763[0m
[37m[1m[2023-07-17 06:50:27,522][257371] Min Reward on eval: -532.3605833535082[0m
[37m[1m[2023-07-17 06:50:27,522][257371] Mean Reward across all agents: 109.87542446152113[0m
[37m[1m[2023-07-17 06:50:27,522][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:50:27,528][257371] mean_value=-391.8973981688585, max_value=339.45499277932424[0m
[37m[1m[2023-07-17 06:50:27,531][257371] New mean coefficients: [[ 0.09622785 -0.7394063  -0.2053281  -0.9963241   1.4360622   1.7527227 ]][0m
[37m[1m[2023-07-17 06:50:27,532][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:50:36,581][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 06:50:36,587][257371] FPS: 424400.01[0m
[36m[2023-07-17 06:50:36,590][257371] itr=806, itrs=2000, Progress: 40.30%[0m
[36m[2023-07-17 06:50:48,440][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 06:50:48,440][257371] FPS: 326671.56[0m
[36m[2023-07-17 06:50:52,767][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:50:52,772][257371] Reward + Measures: [[212.24652618   0.00500767   0.9878906    0.958552     0.97811693
    6.82539701]][0m
[37m[1m[2023-07-17 06:50:52,773][257371] Max Reward on eval: 212.24652618353713[0m
[37m[1m[2023-07-17 06:50:52,773][257371] Min Reward on eval: 212.24652618353713[0m
[37m[1m[2023-07-17 06:50:52,773][257371] Mean Reward across all agents: 212.24652618353713[0m
[37m[1m[2023-07-17 06:50:52,773][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:50:57,721][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:50:57,722][257371] Reward + Measures: [[-84.89702213   0.54830003   0.76800001   0.0606       0.6516
    4.04229784]
 [803.20345305   0.0024       0.99480003   0.98180002   0.98759997
    7.19051075]
 [798.49753569   0.0032       0.99650002   0.92859995   0.97729999
    6.97625732]
 ...
 [  2.22271311   0.83789998   0.94630003   0.0028       0.8847
    3.39047027]
 [379.96557902   0.0019       0.98899996   0.98159999   0.98149997
    7.9224062 ]
 [607.98642731   0.0021       0.98859996   0.97620004   0.97799999
    7.61683798]][0m
[37m[1m[2023-07-17 06:50:57,722][257371] Max Reward on eval: 935.6561050296762[0m
[37m[1m[2023-07-17 06:50:57,722][257371] Min Reward on eval: -344.557933808316[0m
[37m[1m[2023-07-17 06:50:57,722][257371] Mean Reward across all agents: 173.51153677003322[0m
[37m[1m[2023-07-17 06:50:57,723][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:50:57,728][257371] mean_value=-371.06338358670473, max_value=418.01841003649645[0m
[37m[1m[2023-07-17 06:50:57,731][257371] New mean coefficients: [[-0.321962   -0.35689524 -1.3664366  -1.5966858   1.2544677   1.8932159 ]][0m
[37m[1m[2023-07-17 06:50:57,731][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:51:06,726][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 06:51:06,727][257371] FPS: 426983.29[0m
[36m[2023-07-17 06:51:06,729][257371] itr=807, itrs=2000, Progress: 40.35%[0m
[36m[2023-07-17 06:51:18,606][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 06:51:18,606][257371] FPS: 325938.84[0m
[36m[2023-07-17 06:51:22,938][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:51:22,939][257371] Reward + Measures: [[-47.48960085   0.83103174   0.98713595   0.00484633   0.96946502
    4.94223118]][0m
[37m[1m[2023-07-17 06:51:22,939][257371] Max Reward on eval: -47.48960084844419[0m
[37m[1m[2023-07-17 06:51:22,939][257371] Min Reward on eval: -47.48960084844419[0m
[37m[1m[2023-07-17 06:51:22,940][257371] Mean Reward across all agents: -47.48960084844419[0m
[37m[1m[2023-07-17 06:51:22,940][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:51:27,922][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:51:27,923][257371] Reward + Measures: [[-265.91984844    0.48989996    0.91110003    0.28459999    0.94169998
     4.90699148]
 [ 203.83677983    0.89559996    0.97480005    0.0042        0.98709995
     6.93320322]
 [  80.97953249    0.38989997    0.25030002    0.33460003    0.22189999
     3.87306476]
 ...
 [  35.05497798    0.52770001    0.3752        0.47550002    0.43270001
     3.67392659]
 [  39.02778888    0.93440002    0.9874        0.0015        0.98559999
     5.82972336]
 [ 114.10630703    0.73690003    0.83660001    0.48669997    0.92379999
     4.35249567]][0m
[37m[1m[2023-07-17 06:51:27,923][257371] Max Reward on eval: 804.4072799668065[0m
[37m[1m[2023-07-17 06:51:27,923][257371] Min Reward on eval: -658.3863640127704[0m
[37m[1m[2023-07-17 06:51:27,924][257371] Mean Reward across all agents: 23.104547308875905[0m
[37m[1m[2023-07-17 06:51:27,924][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:51:27,928][257371] mean_value=-424.72662708533295, max_value=363.9036808363968[0m
[37m[1m[2023-07-17 06:51:27,931][257371] New mean coefficients: [[-0.65681165 -0.8138944  -1.0325496  -1.2197978   0.6001323   1.837064  ]][0m
[37m[1m[2023-07-17 06:51:27,932][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:51:36,969][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 06:51:36,974][257371] FPS: 424990.75[0m
[36m[2023-07-17 06:51:36,977][257371] itr=808, itrs=2000, Progress: 40.40%[0m
[36m[2023-07-17 06:51:48,945][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-17 06:51:48,945][257371] FPS: 323488.39[0m
[36m[2023-07-17 06:51:53,160][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:51:53,165][257371] Reward + Measures: [[-49.67582863   0.8512007    0.89811832   0.07956367   0.93690902
    5.99860287]][0m
[37m[1m[2023-07-17 06:51:53,166][257371] Max Reward on eval: -49.67582862933853[0m
[37m[1m[2023-07-17 06:51:53,166][257371] Min Reward on eval: -49.67582862933853[0m
[37m[1m[2023-07-17 06:51:53,166][257371] Mean Reward across all agents: -49.67582862933853[0m
[37m[1m[2023-07-17 06:51:53,166][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:51:58,395][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:51:58,401][257371] Reward + Measures: [[142.94466186   0.3335       0.86849993   0.2362       0.84989995
    6.36639023]
 [143.88631774   0.41570002   0.87550002   0.1071       0.82530004
    6.35226965]
 [-78.74340788   0.1576       0.8682       0.7924       0.91540003
    6.19589043]
 ...
 [-45.30654239   0.63779998   0.65929997   0.61800003   0.65259999
    4.33100891]
 [-20.50780106   0.51020002   0.82010001   0.19389999   0.71990001
    5.69710493]
 [ -8.56036562   0.76050007   0.81280005   0.1166       0.87460005
    5.88066292]][0m
[37m[1m[2023-07-17 06:51:58,401][257371] Max Reward on eval: 855.9442901591771[0m
[37m[1m[2023-07-17 06:51:58,401][257371] Min Reward on eval: -646.3983878996223[0m
[37m[1m[2023-07-17 06:51:58,401][257371] Mean Reward across all agents: 107.81307951816177[0m
[37m[1m[2023-07-17 06:51:58,402][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:51:58,407][257371] mean_value=-292.4851212117009, max_value=306.3549817934083[0m
[37m[1m[2023-07-17 06:51:58,409][257371] New mean coefficients: [[-0.7482026  -0.98125315 -1.6016357  -1.8183093   0.48680517  1.8148241 ]][0m
[37m[1m[2023-07-17 06:51:58,410][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:52:07,366][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 06:52:07,366][257371] FPS: 428867.28[0m
[36m[2023-07-17 06:52:07,368][257371] itr=809, itrs=2000, Progress: 40.45%[0m
[36m[2023-07-17 06:52:19,008][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 06:52:19,008][257371] FPS: 332752.59[0m
[36m[2023-07-17 06:52:23,246][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:52:23,246][257371] Reward + Measures: [[23.80497973  0.87118036  0.96185702  0.02561667  0.96018535  6.50454569]][0m
[37m[1m[2023-07-17 06:52:23,246][257371] Max Reward on eval: 23.804979732631065[0m
[37m[1m[2023-07-17 06:52:23,247][257371] Min Reward on eval: 23.804979732631065[0m
[37m[1m[2023-07-17 06:52:23,247][257371] Mean Reward across all agents: 23.804979732631065[0m
[37m[1m[2023-07-17 06:52:23,247][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:52:28,316][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:52:28,322][257371] Reward + Measures: [[-27.7188775    0.33049998   0.33540002   0.33060002   0.16229999
    4.32843161]
 [-17.26622492   0.61840004   0.54250002   0.56199998   0.083
    5.58749342]
 [ 74.60166638   0.1894       0.60049999   0.1139       0.50240004
    4.90161324]
 ...
 [  6.44868515   0.88119996   0.9485001    0.0197       0.95120001
    7.4896636 ]
 [173.48588378   0.27920002   0.21920002   0.22510003   0.1902
    4.20031595]
 [ 48.29986109   0.2168       0.21689999   0.2062       0.18980001
    4.16160536]][0m
[37m[1m[2023-07-17 06:52:28,322][257371] Max Reward on eval: 704.9120920056943[0m
[37m[1m[2023-07-17 06:52:28,322][257371] Min Reward on eval: -218.62108991974964[0m
[37m[1m[2023-07-17 06:52:28,323][257371] Mean Reward across all agents: 91.13724009728232[0m
[37m[1m[2023-07-17 06:52:28,323][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:52:28,326][257371] mean_value=-889.8392928552082, max_value=134.76718953896864[0m
[37m[1m[2023-07-17 06:52:28,329][257371] New mean coefficients: [[-0.55093133 -0.9841594  -1.6861482  -2.1053815  -0.04851785  1.4681122 ]][0m
[37m[1m[2023-07-17 06:52:28,330][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:52:37,496][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-17 06:52:37,497][257371] FPS: 418984.26[0m
[36m[2023-07-17 06:52:37,499][257371] itr=810, itrs=2000, Progress: 40.50%[0m
[37m[1m[2023-07-17 06:55:51,957][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000790[0m
[36m[2023-07-17 06:56:04,041][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 06:56:04,042][257371] FPS: 329688.18[0m
[36m[2023-07-17 06:56:08,281][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:56:08,282][257371] Reward + Measures: [[-97.59320622   0.88414007   0.98098105   0.00650467   0.97292668
    7.6336112 ]][0m
[37m[1m[2023-07-17 06:56:08,282][257371] Max Reward on eval: -97.59320621955023[0m
[37m[1m[2023-07-17 06:56:08,282][257371] Min Reward on eval: -97.59320621955023[0m
[37m[1m[2023-07-17 06:56:08,283][257371] Mean Reward across all agents: -97.59320621955023[0m
[37m[1m[2023-07-17 06:56:08,283][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:56:13,229][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:56:13,229][257371] Reward + Measures: [[-206.46056902    0.2393        0.93269998    0.50560004    0.90480006
     6.77753448]
 [ -62.65728543    0.88899994    0.9716        0.0077        0.96700001
     7.77399826]
 [ 174.96041085    0.42680001    0.95810002    0.20190001    0.90890008
     7.04833841]
 ...
 [ -82.24166498    0.88660002    0.98210001    0.003         0.9769001
     7.96117258]
 [  67.83928777    0.33239999    0.55949992    0.38460001    0.51870006
     3.98109055]
 [ -51.82175339    0.90550005    0.98600006    0.0019        0.98220009
     7.74320698]][0m
[37m[1m[2023-07-17 06:56:13,229][257371] Max Reward on eval: 791.0975189018529[0m
[37m[1m[2023-07-17 06:56:13,230][257371] Min Reward on eval: -357.7475662288023[0m
[37m[1m[2023-07-17 06:56:13,230][257371] Mean Reward across all agents: 18.016411701726785[0m
[37m[1m[2023-07-17 06:56:13,230][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:56:13,234][257371] mean_value=-411.42040699607327, max_value=319.08398033097075[0m
[37m[1m[2023-07-17 06:56:13,242][257371] New mean coefficients: [[-0.8036825  -1.2654066  -1.4606811  -2.1670876   0.04356031  1.5444403 ]][0m
[37m[1m[2023-07-17 06:56:13,243][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:56:22,297][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 06:56:22,298][257371] FPS: 424205.59[0m
[36m[2023-07-17 06:56:22,300][257371] itr=811, itrs=2000, Progress: 40.55%[0m
[36m[2023-07-17 06:56:34,079][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 06:56:34,080][257371] FPS: 328738.14[0m
[36m[2023-07-17 06:56:38,376][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:56:38,377][257371] Reward + Measures: [[-54.48285475   0.89478594   0.98402637   0.004035     0.9764064
    7.58142853]][0m
[37m[1m[2023-07-17 06:56:38,377][257371] Max Reward on eval: -54.48285474999967[0m
[37m[1m[2023-07-17 06:56:38,377][257371] Min Reward on eval: -54.48285474999967[0m
[37m[1m[2023-07-17 06:56:38,378][257371] Mean Reward across all agents: -54.48285474999967[0m
[37m[1m[2023-07-17 06:56:38,378][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:56:43,319][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:56:43,320][257371] Reward + Measures: [[ -46.86169004    0.89860004    0.9842        0.0023        0.97780001
     7.97931528]
 [-115.08186616    0.89350003    0.98450005    0.0041        0.97620004
     7.66758966]
 [   6.52504128    0.27950001    0.41829997    0.2811        0.2983
     4.05385923]
 ...
 [ -65.55317276    0.90220004    0.98629999    0.0026        0.97970003
     7.971416  ]
 [ -60.1625604     0.90630001    0.98909998    0.0024        0.98320001
     7.36142683]
 [-142.41682626    0.89489996    0.98769999    0.002         0.98229998
     7.57018375]][0m
[37m[1m[2023-07-17 06:56:43,320][257371] Max Reward on eval: 682.2675514177885[0m
[37m[1m[2023-07-17 06:56:43,320][257371] Min Reward on eval: -377.33712290208786[0m
[37m[1m[2023-07-17 06:56:43,320][257371] Mean Reward across all agents: 9.523053794431984[0m
[37m[1m[2023-07-17 06:56:43,321][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:56:43,324][257371] mean_value=-574.4702752881141, max_value=359.8110160386955[0m
[37m[1m[2023-07-17 06:56:43,326][257371] New mean coefficients: [[-0.9442258  -1.755589   -1.760455   -2.1316538   0.18076514  1.3449928 ]][0m
[37m[1m[2023-07-17 06:56:43,327][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:56:52,255][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 06:56:52,255][257371] FPS: 430208.88[0m
[36m[2023-07-17 06:56:52,257][257371] itr=812, itrs=2000, Progress: 40.60%[0m
[36m[2023-07-17 06:57:03,847][257371] train() took 11.50 seconds to complete[0m
[36m[2023-07-17 06:57:03,847][257371] FPS: 334056.63[0m
[36m[2023-07-17 06:57:08,136][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:57:08,136][257371] Reward + Measures: [[-85.78658036   0.89602292   0.98404372   0.00293233   0.97708333
    7.94776487]][0m
[37m[1m[2023-07-17 06:57:08,137][257371] Max Reward on eval: -85.78658035683213[0m
[37m[1m[2023-07-17 06:57:08,137][257371] Min Reward on eval: -85.78658035683213[0m
[37m[1m[2023-07-17 06:57:08,137][257371] Mean Reward across all agents: -85.78658035683213[0m
[37m[1m[2023-07-17 06:57:08,138][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:57:13,308][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:57:13,309][257371] Reward + Measures: [[-24.73931764   0.26760003   0.2726       0.27400002   0.2254
    5.40778399]
 [393.96298028   0.0028       0.95170003   0.94230002   0.96880001
    6.60479879]
 [ 67.71254827   0.47870001   0.58860004   0.15009999   0.6067
    6.07286215]
 ...
 [ 82.5651035    0.56890005   0.56980002   0.64579999   0.07120001
    6.22139215]
 [-33.25289624   0.87560004   0.99040002   0.0002       0.98089999
    6.20241547]
 [ 30.70705481   0.5474       0.98470002   0.15479998   0.96849996
    5.44742441]][0m
[37m[1m[2023-07-17 06:57:13,309][257371] Max Reward on eval: 773.387271898007[0m
[37m[1m[2023-07-17 06:57:13,309][257371] Min Reward on eval: -659.5578422378749[0m
[37m[1m[2023-07-17 06:57:13,310][257371] Mean Reward across all agents: 50.31155200978952[0m
[37m[1m[2023-07-17 06:57:13,310][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:57:13,314][257371] mean_value=-352.64501984520507, max_value=361.8705192787522[0m
[37m[1m[2023-07-17 06:57:13,317][257371] New mean coefficients: [[-1.4519527 -2.0790498 -2.154484  -2.3841617 -0.1100278  1.2538211]][0m
[37m[1m[2023-07-17 06:57:13,318][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:57:22,324][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 06:57:22,324][257371] FPS: 426473.45[0m
[36m[2023-07-17 06:57:22,326][257371] itr=813, itrs=2000, Progress: 40.65%[0m
[36m[2023-07-17 06:57:34,088][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 06:57:34,088][257371] FPS: 329168.48[0m
[36m[2023-07-17 06:57:38,310][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:57:38,311][257371] Reward + Measures: [[-43.06752284   0.91601992   0.98473537   0.00249367   0.97969335
    7.65702772]][0m
[37m[1m[2023-07-17 06:57:38,311][257371] Max Reward on eval: -43.06752284139298[0m
[37m[1m[2023-07-17 06:57:38,311][257371] Min Reward on eval: -43.06752284139298[0m
[37m[1m[2023-07-17 06:57:38,312][257371] Mean Reward across all agents: -43.06752284139298[0m
[37m[1m[2023-07-17 06:57:38,312][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:57:43,277][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:57:43,277][257371] Reward + Measures: [[-10.87522697   0.85809994   0.83649999   0.1151       0.90100002
    6.86529922]
 [154.16990472   0.5977       0.60769999   0.58250004   0.60789996
    5.86011887]
 [-49.58722888   0.89639997   0.98740005   0.002        0.98170006
    7.68432713]
 ...
 [-41.63075733   0.78050005   0.79370004   0.0217       0.82320005
    6.60231352]
 [  6.52402734   0.85810006   0.97500002   0.0042       0.9637
    7.78466797]
 [216.08152152   0.82849997   0.84829998   0.08710001   0.82359999
    7.6087203 ]][0m
[37m[1m[2023-07-17 06:57:43,277][257371] Max Reward on eval: 756.1846008354798[0m
[37m[1m[2023-07-17 06:57:43,278][257371] Min Reward on eval: -499.57878496777266[0m
[37m[1m[2023-07-17 06:57:43,278][257371] Mean Reward across all agents: 28.159006086122464[0m
[37m[1m[2023-07-17 06:57:43,278][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:57:43,282][257371] mean_value=-313.89176768300075, max_value=468.96512761371207[0m
[37m[1m[2023-07-17 06:57:43,284][257371] New mean coefficients: [[-1.562493   -2.449568   -2.0017178  -2.5090137  -0.09307346  1.2012091 ]][0m
[37m[1m[2023-07-17 06:57:43,285][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:57:52,298][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 06:57:52,299][257371] FPS: 426135.89[0m
[36m[2023-07-17 06:57:52,301][257371] itr=814, itrs=2000, Progress: 40.70%[0m
[36m[2023-07-17 06:58:04,078][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 06:58:04,079][257371] FPS: 328737.04[0m
[36m[2023-07-17 06:58:08,415][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:58:08,416][257371] Reward + Measures: [[-116.9093916     0.89769691    0.98318034    0.00334767    0.97771835
     7.93641472]][0m
[37m[1m[2023-07-17 06:58:08,416][257371] Max Reward on eval: -116.9093915961998[0m
[37m[1m[2023-07-17 06:58:08,416][257371] Min Reward on eval: -116.9093915961998[0m
[37m[1m[2023-07-17 06:58:08,417][257371] Mean Reward across all agents: -116.9093915961998[0m
[37m[1m[2023-07-17 06:58:08,417][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:58:13,401][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:58:13,402][257371] Reward + Measures: [[ 39.00501405   0.33610001   0.29980001   0.1426       0.23969999
    6.59451675]
 [-35.00609114   0.91549999   0.91549999   0.87769997   0.88000005
    6.39370632]
 [-27.84256068   0.89130002   0.3048       0.81370002   0.51999998
    6.54753494]
 ...
 [ 73.89025497   0.96449995   0.96649998   0.94160002   0.93629998
    7.69918537]
 [  3.66487989   0.83869994   0.81910002   0.7931       0.78520006
    7.07607365]
 [ 59.84733652   0.67729998   0.67690003   0.61440009   0.60179996
    7.12063742]][0m
[37m[1m[2023-07-17 06:58:13,402][257371] Max Reward on eval: 493.2551119148731[0m
[37m[1m[2023-07-17 06:58:13,402][257371] Min Reward on eval: -231.31891580224038[0m
[37m[1m[2023-07-17 06:58:13,402][257371] Mean Reward across all agents: 38.61791833894327[0m
[37m[1m[2023-07-17 06:58:13,403][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:58:13,407][257371] mean_value=-236.59213873528876, max_value=555.7387760380575[0m
[37m[1m[2023-07-17 06:58:13,409][257371] New mean coefficients: [[-1.3998834  -1.3218426  -3.2192366  -4.0086045   0.14737105  1.3713472 ]][0m
[37m[1m[2023-07-17 06:58:13,410][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:58:22,453][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 06:58:22,454][257371] FPS: 424708.30[0m
[36m[2023-07-17 06:58:22,456][257371] itr=815, itrs=2000, Progress: 40.75%[0m
[36m[2023-07-17 06:58:34,177][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 06:58:34,177][257371] FPS: 330387.69[0m
[36m[2023-07-17 06:58:38,370][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:58:38,371][257371] Reward + Measures: [[-86.6663228    0.9003613    0.9839583    0.00291267   0.97835129
    7.91057539]][0m
[37m[1m[2023-07-17 06:58:38,371][257371] Max Reward on eval: -86.66632279827033[0m
[37m[1m[2023-07-17 06:58:38,371][257371] Min Reward on eval: -86.66632279827033[0m
[37m[1m[2023-07-17 06:58:38,372][257371] Mean Reward across all agents: -86.66632279827033[0m
[37m[1m[2023-07-17 06:58:38,372][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:58:43,378][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:58:43,379][257371] Reward + Measures: [[  -4.21799917    0.5377        0.53009999    0.50749999    0.50929999
     6.31396437]
 [   0.92047549    0.50389999    0.47569999    0.51809996    0.49780002
     6.21415949]
 [ -30.91718481    0.66470003    0.65510005    0.15530001    0.68959999
     5.90157795]
 ...
 [-135.06822349    0.89790004    0.98600006    0.0029        0.98140001
     7.99057722]
 [ -26.21964106    0.58530003    0.53459996    0.53909999    0.57160002
     5.72342873]
 [ 243.53736188    0.80980009    0.90450001    0.10420001    0.87889999
     6.2473712 ]][0m
[37m[1m[2023-07-17 06:58:43,379][257371] Max Reward on eval: 468.103178973496[0m
[37m[1m[2023-07-17 06:58:43,379][257371] Min Reward on eval: -615.3056506697088[0m
[37m[1m[2023-07-17 06:58:43,379][257371] Mean Reward across all agents: -23.09562308002917[0m
[37m[1m[2023-07-17 06:58:43,380][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:58:43,382][257371] mean_value=-371.9692215176206, max_value=324.6616660822068[0m
[37m[1m[2023-07-17 06:58:43,385][257371] New mean coefficients: [[-1.6083716 -1.4483346 -4.155279  -4.215625  -0.5937113  1.0902634]][0m
[37m[1m[2023-07-17 06:58:43,386][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:58:52,428][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 06:58:52,428][257371] FPS: 424762.87[0m
[36m[2023-07-17 06:58:52,430][257371] itr=816, itrs=2000, Progress: 40.80%[0m
[36m[2023-07-17 06:59:04,180][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 06:59:04,181][257371] FPS: 329517.53[0m
[36m[2023-07-17 06:59:08,470][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:59:08,470][257371] Reward + Measures: [[-79.63803186   0.89184171   0.96875399   0.01646967   0.97254831
    7.96173811]][0m
[37m[1m[2023-07-17 06:59:08,471][257371] Max Reward on eval: -79.63803186307986[0m
[37m[1m[2023-07-17 06:59:08,471][257371] Min Reward on eval: -79.63803186307986[0m
[37m[1m[2023-07-17 06:59:08,471][257371] Mean Reward across all agents: -79.63803186307986[0m
[37m[1m[2023-07-17 06:59:08,471][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:59:13,534][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:59:13,535][257371] Reward + Measures: [[ -83.4240665     0.89029998    0.98559999    0.0022        0.97819996
     7.98361349]
 [-106.57029726    0.75170004    0.87820005    0.2271        0.87460005
     6.29632902]
 [ -27.95158577    0.21769999    0.21440001    0.0654        0.20439999
     6.83075571]
 ...
 [  71.55401654    0.0627        0.91350001    0.82050002    0.96149999
     7.43354034]
 [  96.15261798    0.0926        0.85620004    0.77069998    0.91000003
     7.02001429]
 [  41.78826881    0.5108        0.6954        0.45539999    0.64070004
     5.5331912 ]][0m
[37m[1m[2023-07-17 06:59:13,535][257371] Max Reward on eval: 872.1547317539341[0m
[37m[1m[2023-07-17 06:59:13,535][257371] Min Reward on eval: -285.41165828555825[0m
[37m[1m[2023-07-17 06:59:13,535][257371] Mean Reward across all agents: 124.05794786651663[0m
[37m[1m[2023-07-17 06:59:13,536][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:59:13,540][257371] mean_value=-364.111893071834, max_value=202.99759318781147[0m
[37m[1m[2023-07-17 06:59:13,542][257371] New mean coefficients: [[-2.5814571  -1.7596217  -3.5405147  -4.442248    0.13447559  1.0608431 ]][0m
[37m[1m[2023-07-17 06:59:13,543][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:59:22,605][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 06:59:22,610][257371] FPS: 423833.39[0m
[36m[2023-07-17 06:59:22,613][257371] itr=817, itrs=2000, Progress: 40.85%[0m
[36m[2023-07-17 06:59:34,349][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 06:59:34,349][257371] FPS: 329970.09[0m
[36m[2023-07-17 06:59:38,654][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:59:38,654][257371] Reward + Measures: [[-65.67175477   0.8835817    0.93634665   0.043804     0.96215934
    7.95426178]][0m
[37m[1m[2023-07-17 06:59:38,655][257371] Max Reward on eval: -65.67175476699825[0m
[37m[1m[2023-07-17 06:59:38,655][257371] Min Reward on eval: -65.67175476699825[0m
[37m[1m[2023-07-17 06:59:38,655][257371] Mean Reward across all agents: -65.67175476699825[0m
[37m[1m[2023-07-17 06:59:38,655][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:59:43,673][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 06:59:43,674][257371] Reward + Measures: [[ -5.68381311   0.33309999   0.35069999   0.21570002   0.25500003
    4.65138578]
 [ -5.74825861   0.66020006   0.50560004   0.39780003   0.25149998
    7.39780569]
 [-16.65231225   0.72260004   0.74190003   0.74690002   0.73800009
    6.1590538 ]
 ...
 [ 70.3566686    0.64610004   0.58899999   0.34189999   0.70209998
    6.0121994 ]
 [-74.31446491   0.8351       0.65750003   0.27789998   0.86240005
    6.47011518]
 [-27.29082952   0.63959998   0.36250001   0.29840001   0.16680001
    7.59883261]][0m
[37m[1m[2023-07-17 06:59:43,674][257371] Max Reward on eval: 449.206041313475[0m
[37m[1m[2023-07-17 06:59:43,674][257371] Min Reward on eval: -457.4699242361821[0m
[37m[1m[2023-07-17 06:59:43,674][257371] Mean Reward across all agents: 33.999070411979794[0m
[37m[1m[2023-07-17 06:59:43,675][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 06:59:43,680][257371] mean_value=-276.3114585078244, max_value=607.8870222232072[0m
[37m[1m[2023-07-17 06:59:43,682][257371] New mean coefficients: [[-2.1440904  -2.7099516  -3.5934443  -2.7002504   0.19825143  1.3326616 ]][0m
[37m[1m[2023-07-17 06:59:43,683][257371] Moving the mean solution point...[0m
[36m[2023-07-17 06:59:52,620][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 06:59:52,620][257371] FPS: 429790.59[0m
[36m[2023-07-17 06:59:52,622][257371] itr=818, itrs=2000, Progress: 40.90%[0m
[36m[2023-07-17 07:00:04,420][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 07:00:04,421][257371] FPS: 328257.36[0m
[36m[2023-07-17 07:00:08,738][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:00:08,739][257371] Reward + Measures: [[-78.63357037   0.88723934   0.94324666   0.03869267   0.96571535
    7.96074343]][0m
[37m[1m[2023-07-17 07:00:08,739][257371] Max Reward on eval: -78.63357036822933[0m
[37m[1m[2023-07-17 07:00:08,739][257371] Min Reward on eval: -78.63357036822933[0m
[37m[1m[2023-07-17 07:00:08,740][257371] Mean Reward across all agents: -78.63357036822933[0m
[37m[1m[2023-07-17 07:00:08,740][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:00:14,003][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:00:14,004][257371] Reward + Measures: [[ 75.18127515   0.8847       0.86709994   0.87510008   0.87460005
    7.88540745]
 [186.00336791   0.83090001   0.84079999   0.1752       0.78619999
    7.86975241]
 [290.57657531   0.86470002   0.88970006   0.0481       0.87639999
    7.83007288]
 ...
 [ 32.08169829   0.66850007   0.5952       0.35420001   0.36359999
    7.70418787]
 [  9.49721236   0.84130001   0.81999999   0.83719999   0.83809996
    7.73066282]
 [194.43705009   0.94130003   0.9738       0.0036       0.98450005
    7.52293158]][0m
[37m[1m[2023-07-17 07:00:14,004][257371] Max Reward on eval: 860.6593856535852[0m
[37m[1m[2023-07-17 07:00:14,004][257371] Min Reward on eval: -259.72029973305763[0m
[37m[1m[2023-07-17 07:00:14,004][257371] Mean Reward across all agents: 54.70117844572349[0m
[37m[1m[2023-07-17 07:00:14,005][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:00:14,009][257371] mean_value=-225.50510605236684, max_value=364.85976504657697[0m
[37m[1m[2023-07-17 07:00:14,011][257371] New mean coefficients: [[-1.9140263  -2.640908   -4.20509    -2.6598024   0.02036698  1.0893872 ]][0m
[37m[1m[2023-07-17 07:00:14,012][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:00:22,987][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 07:00:22,987][257371] FPS: 427963.31[0m
[36m[2023-07-17 07:00:22,989][257371] itr=819, itrs=2000, Progress: 40.95%[0m
[36m[2023-07-17 07:00:34,757][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 07:00:34,757][257371] FPS: 329027.36[0m
[36m[2023-07-17 07:00:39,037][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:00:39,038][257371] Reward + Measures: [[-141.49572948    0.91585797    0.85696769    0.11962566    0.95183635
     7.19586849]][0m
[37m[1m[2023-07-17 07:00:39,038][257371] Max Reward on eval: -141.49572947862796[0m
[37m[1m[2023-07-17 07:00:39,038][257371] Min Reward on eval: -141.49572947862796[0m
[37m[1m[2023-07-17 07:00:39,038][257371] Mean Reward across all agents: -141.49572947862796[0m
[37m[1m[2023-07-17 07:00:39,039][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:00:44,039][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:00:44,040][257371] Reward + Measures: [[169.75208302   0.79609996   0.7985       0.28099999   0.64189994
    7.58429289]
 [-58.04654503   0.72189999   0.64179999   0.51289999   0.39790002
    7.46874619]
 [125.70516536   0.50099999   0.5377       0.38439998   0.4752
    7.34041548]
 ...
 [ 42.88269139   0.55600005   0.46809998   0.46990004   0.44860002
    7.88874149]
 [277.0168495    0.78619999   0.93190002   0.014        0.96450007
    7.75696182]
 [326.80309728   0.91400003   0.9443       0.0303       0.94559997
    7.08090448]][0m
[37m[1m[2023-07-17 07:00:44,040][257371] Max Reward on eval: 586.3663797348738[0m
[37m[1m[2023-07-17 07:00:44,040][257371] Min Reward on eval: -217.95720507502557[0m
[37m[1m[2023-07-17 07:00:44,040][257371] Mean Reward across all agents: 118.49284417609832[0m
[37m[1m[2023-07-17 07:00:44,041][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:00:44,046][257371] mean_value=-169.319413350654, max_value=307.1591726183224[0m
[37m[1m[2023-07-17 07:00:44,049][257371] New mean coefficients: [[-1.8384957 -2.6303093 -5.1238647 -2.9718945 -0.7180273  1.5089856]][0m
[37m[1m[2023-07-17 07:00:44,050][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:00:53,089][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 07:00:53,089][257371] FPS: 424910.38[0m
[36m[2023-07-17 07:00:53,091][257371] itr=820, itrs=2000, Progress: 41.00%[0m
[37m[1m[2023-07-17 07:04:14,790][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000800[0m
[36m[2023-07-17 07:04:26,984][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 07:04:26,985][257371] FPS: 330606.36[0m
[36m[2023-07-17 07:04:31,192][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:04:31,192][257371] Reward + Measures: [[-119.06522186    0.90710831    0.84348267    0.13054667    0.94657195
     7.23587084]][0m
[37m[1m[2023-07-17 07:04:31,193][257371] Max Reward on eval: -119.06522185975173[0m
[37m[1m[2023-07-17 07:04:31,193][257371] Min Reward on eval: -119.06522185975173[0m
[37m[1m[2023-07-17 07:04:31,193][257371] Mean Reward across all agents: -119.06522185975173[0m
[37m[1m[2023-07-17 07:04:31,193][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:04:36,187][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:04:36,188][257371] Reward + Measures: [[189.49566481   0.9188       0.92700005   0.0618       0.93790001
    7.04519653]
 [251.22634407   0.90310001   0.96600002   0.0088       0.98089999
    7.99991465]
 [-58.01132347   0.86669999   0.88190001   0.0861       0.93879998
    7.69145536]
 ...
 [-58.20061824   0.88990003   0.77540004   0.45000002   0.53310007
    7.2354846 ]
 [130.51632236   0.87630004   0.96299994   0.0067       0.97930002
    7.95953131]
 [287.89526439   0.90249997   0.96499997   0.0045       0.98049992
    7.90886688]][0m
[37m[1m[2023-07-17 07:04:36,188][257371] Max Reward on eval: 627.1758041739464[0m
[37m[1m[2023-07-17 07:04:36,188][257371] Min Reward on eval: -612.0919265925884[0m
[37m[1m[2023-07-17 07:04:36,189][257371] Mean Reward across all agents: 127.51374837119727[0m
[37m[1m[2023-07-17 07:04:36,189][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:04:36,194][257371] mean_value=-207.58083508074827, max_value=650.6926710616983[0m
[37m[1m[2023-07-17 07:04:36,197][257371] New mean coefficients: [[-1.9940569 -2.3148289 -5.769669  -2.220182  -1.1564906  1.5344603]][0m
[37m[1m[2023-07-17 07:04:36,198][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:04:45,105][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-17 07:04:45,105][257371] FPS: 431177.54[0m
[36m[2023-07-17 07:04:45,108][257371] itr=821, itrs=2000, Progress: 41.05%[0m
[36m[2023-07-17 07:04:56,708][257371] train() took 11.50 seconds to complete[0m
[36m[2023-07-17 07:04:56,709][257371] FPS: 333879.14[0m
[36m[2023-07-17 07:05:00,979][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:05:00,980][257371] Reward + Measures: [[-122.84777876    0.90796632    0.83797199    0.13639733    0.94730067
     7.2417407 ]][0m
[37m[1m[2023-07-17 07:05:00,980][257371] Max Reward on eval: -122.84777876404696[0m
[37m[1m[2023-07-17 07:05:00,980][257371] Min Reward on eval: -122.84777876404696[0m
[37m[1m[2023-07-17 07:05:00,980][257371] Mean Reward across all agents: -122.84777876404696[0m
[37m[1m[2023-07-17 07:05:00,981][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:05:05,893][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:05:05,899][257371] Reward + Measures: [[-31.48119749   0.90059996   0.88770002   0.0958       0.97040004
    7.86370087]
 [ 37.5535572    0.86879998   0.80419999   0.16070001   0.9393
    7.89709187]
 [ 29.26196624   0.9012       0.53310001   0.42940003   0.93020004
    7.25784302]
 ...
 [ 48.07948062   0.89219999   0.84670001   0.2577       0.72930002
    6.21376038]
 [ 55.32631751   0.88479996   0.60410005   0.3567       0.93060011
    7.87412024]
 [ 50.12235968   0.65889996   0.2146       0.57019997   0.65719998
    6.308074  ]][0m
[37m[1m[2023-07-17 07:05:05,899][257371] Max Reward on eval: 740.453899396956[0m
[37m[1m[2023-07-17 07:05:05,900][257371] Min Reward on eval: -481.4515719294548[0m
[37m[1m[2023-07-17 07:05:05,900][257371] Mean Reward across all agents: 47.55442630546497[0m
[37m[1m[2023-07-17 07:05:05,900][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:05:05,907][257371] mean_value=-241.4612230118023, max_value=537.7865143084898[0m
[37m[1m[2023-07-17 07:05:05,909][257371] New mean coefficients: [[-2.2169895 -2.2637193 -6.660023  -2.5454268 -0.854071   1.573905 ]][0m
[37m[1m[2023-07-17 07:05:05,910][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:05:14,888][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 07:05:14,889][257371] FPS: 427781.19[0m
[36m[2023-07-17 07:05:14,891][257371] itr=822, itrs=2000, Progress: 41.10%[0m
[36m[2023-07-17 07:05:26,574][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 07:05:26,574][257371] FPS: 331501.68[0m
[36m[2023-07-17 07:05:30,825][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:05:30,826][257371] Reward + Measures: [[-117.93267043    0.91026193    0.87580597    0.10087433    0.953888
     7.300138  ]][0m
[37m[1m[2023-07-17 07:05:30,826][257371] Max Reward on eval: -117.9326704270355[0m
[37m[1m[2023-07-17 07:05:30,826][257371] Min Reward on eval: -117.9326704270355[0m
[37m[1m[2023-07-17 07:05:30,826][257371] Mean Reward across all agents: -117.9326704270355[0m
[37m[1m[2023-07-17 07:05:30,827][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:05:35,809][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:05:35,810][257371] Reward + Measures: [[ 29.35509915   0.74310005   0.65240002   0.25110003   0.84610003
    7.69168329]
 [196.33609227   0.72580004   0.94320005   0.1596       0.92019999
    7.38609409]
 [-27.44865846   0.87840003   0.84849995   0.1276       0.95060009
    7.95887613]
 ...
 [ 44.06725567   0.80579996   0.68909997   0.23720001   0.87509996
    7.85523558]
 [ 99.7561504    0.88510001   0.96060008   0.0075       0.9799
    7.65012693]
 [-29.67730477   0.8714       0.71859998   0.25139999   0.93730003
    7.96449518]][0m
[37m[1m[2023-07-17 07:05:35,810][257371] Max Reward on eval: 546.9507446197792[0m
[37m[1m[2023-07-17 07:05:35,810][257371] Min Reward on eval: -179.60984611958264[0m
[37m[1m[2023-07-17 07:05:35,810][257371] Mean Reward across all agents: 49.646264549444304[0m
[37m[1m[2023-07-17 07:05:35,810][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:05:35,813][257371] mean_value=-334.5954732015653, max_value=504.57871514875944[0m
[37m[1m[2023-07-17 07:05:35,816][257371] New mean coefficients: [[-1.8157091  -2.7048101  -6.5720887  -3.0085356  -0.26227206  1.9433571 ]][0m
[37m[1m[2023-07-17 07:05:35,817][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:05:44,770][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 07:05:44,770][257371] FPS: 428979.31[0m
[36m[2023-07-17 07:05:44,772][257371] itr=823, itrs=2000, Progress: 41.15%[0m
[36m[2023-07-17 07:05:56,557][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 07:05:56,557][257371] FPS: 328657.35[0m
[36m[2023-07-17 07:06:00,782][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:06:00,783][257371] Reward + Measures: [[-117.38465817    0.90891266    0.87623435    0.10081666    0.95361829
     7.31224489]][0m
[37m[1m[2023-07-17 07:06:00,783][257371] Max Reward on eval: -117.3846581749892[0m
[37m[1m[2023-07-17 07:06:00,783][257371] Min Reward on eval: -117.3846581749892[0m
[37m[1m[2023-07-17 07:06:00,783][257371] Mean Reward across all agents: -117.3846581749892[0m
[37m[1m[2023-07-17 07:06:00,784][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:06:05,934][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:06:05,939][257371] Reward + Measures: [[ -77.22078789    0.8872        0.9846999     0.0031        0.97179997
     7.99347639]
 [ -69.70503968    0.92150003    0.69259995    0.30050001    0.98000002
     7.9739604 ]
 [ -60.738725      0.83849996    0.8872        0.75080007    0.8373
     6.96759796]
 ...
 [ -59.96545367    0.88840008    0.9842        0.0044        0.97349995
     7.98694754]
 [ -36.97643607    0.62420005    0.6807        0.56129998    0.66290003
     5.6918931 ]
 [-141.13014054    0.89880002    0.93920004    0.046         0.97310001
     7.96456003]][0m
[37m[1m[2023-07-17 07:06:05,940][257371] Max Reward on eval: 605.5692844317294[0m
[37m[1m[2023-07-17 07:06:05,940][257371] Min Reward on eval: -315.55623053668535[0m
[37m[1m[2023-07-17 07:06:05,940][257371] Mean Reward across all agents: -65.77668024327295[0m
[37m[1m[2023-07-17 07:06:05,940][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:06:05,942][257371] mean_value=-484.8800667229812, max_value=358.08039114741564[0m
[37m[1m[2023-07-17 07:06:05,945][257371] New mean coefficients: [[-2.332644  -3.8932142 -6.865342  -1.3052077 -0.8749941  2.01391  ]][0m
[37m[1m[2023-07-17 07:06:05,946][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:06:14,911][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 07:06:14,911][257371] FPS: 428398.74[0m
[36m[2023-07-17 07:06:14,914][257371] itr=824, itrs=2000, Progress: 41.20%[0m
[36m[2023-07-17 07:06:26,787][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 07:06:26,787][257371] FPS: 326162.65[0m
[36m[2023-07-17 07:06:31,026][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:06:31,026][257371] Reward + Measures: [[-109.83314088    0.90839404    0.88565427    0.09151634    0.95583397
     7.34969378]][0m
[37m[1m[2023-07-17 07:06:31,027][257371] Max Reward on eval: -109.83314087808266[0m
[37m[1m[2023-07-17 07:06:31,027][257371] Min Reward on eval: -109.83314087808266[0m
[37m[1m[2023-07-17 07:06:31,027][257371] Mean Reward across all agents: -109.83314087808266[0m
[37m[1m[2023-07-17 07:06:31,027][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:06:35,979][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:06:35,980][257371] Reward + Measures: [[ 432.36581994    0.80760002    0.86339998    0.0283        0.86770004
     7.85408497]
 [  -0.84598658    0.10030001    0.0979        0.0481        0.0693
     6.78959751]
 [  71.85446049    0.47890005    0.54689997    0.21399999    0.4526
     6.38337517]
 ...
 [-110.96475648    0.9181        0.95179999    0.032         0.9776001
     7.33072376]
 [ -46.04239421    0.22640002    0.1259        0.2148        0.1921
     6.74524689]
 [-125.16854261    0.90450001    0.7608        0.21379998    0.94989997
     7.20423126]][0m
[37m[1m[2023-07-17 07:06:35,980][257371] Max Reward on eval: 453.6944694265723[0m
[37m[1m[2023-07-17 07:06:35,980][257371] Min Reward on eval: -336.6409130289918[0m
[37m[1m[2023-07-17 07:06:35,980][257371] Mean Reward across all agents: 0.7589623391154944[0m
[37m[1m[2023-07-17 07:06:35,981][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:06:35,984][257371] mean_value=-384.28024508054125, max_value=343.6350283480126[0m
[37m[1m[2023-07-17 07:06:35,987][257371] New mean coefficients: [[-0.8857367  -4.3262625  -7.4351687  -2.0151618  -0.02190989  2.1450448 ]][0m
[37m[1m[2023-07-17 07:06:35,988][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:06:44,921][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 07:06:44,921][257371] FPS: 429923.17[0m
[36m[2023-07-17 07:06:44,923][257371] itr=825, itrs=2000, Progress: 41.25%[0m
[36m[2023-07-17 07:06:56,683][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 07:06:56,683][257371] FPS: 329292.34[0m
[36m[2023-07-17 07:07:00,934][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:07:00,935][257371] Reward + Measures: [[-107.67777895    0.90855628    0.89448935    0.08386667    0.95807135
     7.37782097]][0m
[37m[1m[2023-07-17 07:07:00,935][257371] Max Reward on eval: -107.67777895078586[0m
[37m[1m[2023-07-17 07:07:00,935][257371] Min Reward on eval: -107.67777895078586[0m
[37m[1m[2023-07-17 07:07:00,936][257371] Mean Reward across all agents: -107.67777895078586[0m
[37m[1m[2023-07-17 07:07:00,936][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:07:05,935][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:07:05,940][257371] Reward + Measures: [[  25.90065052    0.1346        0.1322        0.10210001    0.11080001
     5.52261448]
 [  73.24102795    0.44949999    0.65039998    0.211         0.62790006
     6.10798502]
 [-109.83626867    0.47200003    0.54350007    0.26540002    0.3432
     6.10325336]
 ...
 [ -38.32253209    0.3599        0.26530001    0.1961        0.35089999
     6.56202078]
 [  -6.08419301    0.1639        0.21659999    0.1567        0.20710002
     5.50993729]
 [  80.94146704    0.47510004    0.43859997    0.15339999    0.36620003
     6.83247995]][0m
[37m[1m[2023-07-17 07:07:05,941][257371] Max Reward on eval: 655.0815105273854[0m
[37m[1m[2023-07-17 07:07:05,941][257371] Min Reward on eval: -310.05955415247007[0m
[37m[1m[2023-07-17 07:07:05,941][257371] Mean Reward across all agents: 61.60779975236576[0m
[37m[1m[2023-07-17 07:07:05,941][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:07:05,946][257371] mean_value=-213.58308674159386, max_value=337.7503564304413[0m
[37m[1m[2023-07-17 07:07:05,949][257371] New mean coefficients: [[-0.5343773 -3.8368602 -7.266622  -2.2270863  0.0926358  2.0863445]][0m
[37m[1m[2023-07-17 07:07:05,950][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:07:14,979][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 07:07:14,979][257371] FPS: 425374.69[0m
[36m[2023-07-17 07:07:14,981][257371] itr=826, itrs=2000, Progress: 41.30%[0m
[36m[2023-07-17 07:07:27,083][257371] train() took 12.00 seconds to complete[0m
[36m[2023-07-17 07:07:27,083][257371] FPS: 320003.57[0m
[36m[2023-07-17 07:07:31,367][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:07:31,367][257371] Reward + Measures: [[-110.13770611    0.90760767    0.90626162    0.07287233    0.96035963
     7.42508984]][0m
[37m[1m[2023-07-17 07:07:31,367][257371] Max Reward on eval: -110.13770611267225[0m
[37m[1m[2023-07-17 07:07:31,368][257371] Min Reward on eval: -110.13770611267225[0m
[37m[1m[2023-07-17 07:07:31,368][257371] Mean Reward across all agents: -110.13770611267225[0m
[37m[1m[2023-07-17 07:07:31,368][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:07:36,362][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:07:36,363][257371] Reward + Measures: [[  86.17779522    0.70320004    0.51259995    0.38100001    0.3048
     7.8595643 ]
 [   2.87677055    0.824         0.4382        0.65510005    0.28490001
     6.20073652]
 [ -70.568022      0.53390002    0.32789999    0.43520004    0.0722
     7.5609498 ]
 ...
 [ 116.00204785    0.80590004    0.6124        0.27540001    0.58919996
     7.01844645]
 [ 221.42859803    0.84889996    0.69709998    0.40430003    0.56680006
     7.39413166]
 [-226.98652658    0.80380005    0.52710003    0.713         0.0692
     7.89237976]][0m
[37m[1m[2023-07-17 07:07:36,363][257371] Max Reward on eval: 470.8156471259892[0m
[37m[1m[2023-07-17 07:07:36,363][257371] Min Reward on eval: -388.3704586137086[0m
[37m[1m[2023-07-17 07:07:36,363][257371] Mean Reward across all agents: 28.572176086588932[0m
[37m[1m[2023-07-17 07:07:36,364][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:07:36,372][257371] mean_value=-126.79021360541257, max_value=485.71971176721854[0m
[37m[1m[2023-07-17 07:07:36,381][257371] New mean coefficients: [[-0.50094485 -2.8890498  -6.774073   -4.068163   -0.02779736  2.1161358 ]][0m
[37m[1m[2023-07-17 07:07:36,382][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:07:45,417][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 07:07:45,417][257371] FPS: 425071.12[0m
[36m[2023-07-17 07:07:45,420][257371] itr=827, itrs=2000, Progress: 41.35%[0m
[36m[2023-07-17 07:07:57,267][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 07:07:57,267][257371] FPS: 326918.53[0m
[36m[2023-07-17 07:08:01,540][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:08:01,540][257371] Reward + Measures: [[-109.79970706    0.90699363    0.90754801    0.0717        0.96081036
     7.44094992]][0m
[37m[1m[2023-07-17 07:08:01,540][257371] Max Reward on eval: -109.79970706423306[0m
[37m[1m[2023-07-17 07:08:01,541][257371] Min Reward on eval: -109.79970706423306[0m
[37m[1m[2023-07-17 07:08:01,541][257371] Mean Reward across all agents: -109.79970706423306[0m
[37m[1m[2023-07-17 07:08:01,541][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:08:06,546][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:08:06,547][257371] Reward + Measures: [[-103.84572696    0.90950006    0.77630001    0.21269999    0.92679995
     6.9029398 ]
 [ -97.74083821    0.90739995    0.81120008    0.17880002    0.9716
     7.8826189 ]
 [ 184.72158824    0.63870001    0.67410004    0.13950001    0.72190005
     6.99970245]
 ...
 [-141.3060899     0.90850002    0.92110008    0.88789999    0.94139999
     7.39562082]
 [-112.77248355    0.90719998    0.90869999    0.8908        0.89229995
     7.57927084]
 [-171.43057728    0.86420006    0.89949989    0.81140006    0.93040001
     7.34010315]][0m
[37m[1m[2023-07-17 07:08:06,547][257371] Max Reward on eval: 486.1457993898308[0m
[37m[1m[2023-07-17 07:08:06,547][257371] Min Reward on eval: -366.44597054473127[0m
[37m[1m[2023-07-17 07:08:06,548][257371] Mean Reward across all agents: -50.12916459091793[0m
[37m[1m[2023-07-17 07:08:06,548][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:08:06,551][257371] mean_value=-286.53479438379856, max_value=291.0552518456474[0m
[37m[1m[2023-07-17 07:08:06,554][257371] New mean coefficients: [[-0.03482914 -2.9891272  -7.78941    -4.5553746  -0.07515541  1.9818349 ]][0m
[37m[1m[2023-07-17 07:08:06,555][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:08:15,570][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 07:08:15,570][257371] FPS: 426037.99[0m
[36m[2023-07-17 07:08:15,572][257371] itr=828, itrs=2000, Progress: 41.40%[0m
[36m[2023-07-17 07:08:27,406][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 07:08:27,406][257371] FPS: 327326.69[0m
[36m[2023-07-17 07:08:31,763][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:08:31,763][257371] Reward + Measures: [[-114.65111253    0.90659636    0.91491503    0.06535033    0.96348697
     7.48370218]][0m
[37m[1m[2023-07-17 07:08:31,764][257371] Max Reward on eval: -114.65111253254898[0m
[37m[1m[2023-07-17 07:08:31,764][257371] Min Reward on eval: -114.65111253254898[0m
[37m[1m[2023-07-17 07:08:31,764][257371] Mean Reward across all agents: -114.65111253254898[0m
[37m[1m[2023-07-17 07:08:31,764][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:08:37,033][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:08:37,033][257371] Reward + Measures: [[ 49.13600205   0.12620001   0.13420001   0.13450001   0.08889999
    5.5544157 ]
 [ -1.17145002   0.26200002   0.29060003   0.1005       0.28910002
    5.99720812]
 [ 35.23273898   0.0658       0.10109999   0.09190001   0.07829999
    5.96320677]
 ...
 [ 64.40890952   0.10930001   0.18810001   0.0877       0.12330001
    6.48725843]
 [-12.99286221   0.36690003   0.2836       0.2436       0.37350002
    6.44785309]
 [ 38.68653555   0.0614       0.09950001   0.0947       0.0741
    6.65068674]][0m
[37m[1m[2023-07-17 07:08:37,033][257371] Max Reward on eval: 320.40343095511196[0m
[37m[1m[2023-07-17 07:08:37,034][257371] Min Reward on eval: -257.8089459404349[0m
[37m[1m[2023-07-17 07:08:37,034][257371] Mean Reward across all agents: 43.438407578421504[0m
[37m[1m[2023-07-17 07:08:37,034][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:08:37,036][257371] mean_value=-246.38653491720245, max_value=44.483518733625004[0m
[37m[1m[2023-07-17 07:08:37,038][257371] New mean coefficients: [[ 0.06990305 -3.2945929  -6.39795    -5.22828    -0.19403596  1.8191216 ]][0m
[37m[1m[2023-07-17 07:08:37,039][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:08:46,033][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 07:08:46,033][257371] FPS: 427039.97[0m
[36m[2023-07-17 07:08:46,036][257371] itr=829, itrs=2000, Progress: 41.45%[0m
[36m[2023-07-17 07:08:57,779][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 07:08:57,779][257371] FPS: 329866.24[0m
[36m[2023-07-17 07:09:02,164][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:09:02,165][257371] Reward + Measures: [[-121.16768375    0.90267706    0.90039796    0.07812833    0.95863694
     7.48151016]][0m
[37m[1m[2023-07-17 07:09:02,165][257371] Max Reward on eval: -121.16768375288629[0m
[37m[1m[2023-07-17 07:09:02,165][257371] Min Reward on eval: -121.16768375288629[0m
[37m[1m[2023-07-17 07:09:02,165][257371] Mean Reward across all agents: -121.16768375288629[0m
[37m[1m[2023-07-17 07:09:02,166][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:09:07,242][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:09:07,243][257371] Reward + Measures: [[  11.91217956    0.56580001    0.37450001    0.57500005    0.20639999
     5.97043371]
 [ 160.74633624    0.82480001    0.7507        0.27500001    0.6397
     7.83641195]
 [-102.99904619    0.65230006    0.57630002    0.54470003    0.2498
     7.19361877]
 ...
 [-303.0016322     0.75349998    0.51749998    0.71200001    0.1214
     6.96212387]
 [-287.8382592     0.77850002    0.4549        0.74470001    0.15760002
     6.89879751]
 [   6.42812199    0.122         0.1232        0.1398        0.0914
     6.30067587]][0m
[37m[1m[2023-07-17 07:09:07,243][257371] Max Reward on eval: 284.5727348035201[0m
[37m[1m[2023-07-17 07:09:07,244][257371] Min Reward on eval: -648.2152709953486[0m
[37m[1m[2023-07-17 07:09:07,244][257371] Mean Reward across all agents: -26.48713907218872[0m
[37m[1m[2023-07-17 07:09:07,244][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:09:07,248][257371] mean_value=-276.5426876983385, max_value=292.0896753984392[0m
[37m[1m[2023-07-17 07:09:07,251][257371] New mean coefficients: [[ 0.38730004 -2.5710697  -5.9206233  -5.0605907   0.602327    2.077877  ]][0m
[37m[1m[2023-07-17 07:09:07,252][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:09:16,320][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 07:09:16,320][257371] FPS: 423525.42[0m
[36m[2023-07-17 07:09:16,323][257371] itr=830, itrs=2000, Progress: 41.50%[0m
[37m[1m[2023-07-17 07:12:31,279][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000810[0m
[36m[2023-07-17 07:12:43,713][257371] train() took 11.93 seconds to complete[0m
[36m[2023-07-17 07:12:43,713][257371] FPS: 321907.56[0m
[36m[2023-07-17 07:12:47,889][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:12:47,895][257371] Reward + Measures: [[-125.71966066    0.90245998    0.92149395    0.05838266    0.96277964
     7.54935884]][0m
[37m[1m[2023-07-17 07:12:47,896][257371] Max Reward on eval: -125.71966066156106[0m
[37m[1m[2023-07-17 07:12:47,897][257371] Min Reward on eval: -125.71966066156106[0m
[37m[1m[2023-07-17 07:12:47,897][257371] Mean Reward across all agents: -125.71966066156106[0m
[37m[1m[2023-07-17 07:12:47,898][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:12:52,832][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:12:52,832][257371] Reward + Measures: [[152.15921833   0.17520002   0.54860002   0.33249998   0.45970002
    5.43037605]
 [207.69715651   0.56280005   0.59980005   0.60860008   0.57090002
    6.61052036]
 [ 84.25113236   0.28029999   0.54409999   0.1811       0.38439998
    5.66476345]
 ...
 [ 16.0563945    0.35949999   0.2098       0.2712       0.29300001
    7.17679214]
 [376.69638798   0.22739999   0.64920002   0.29050002   0.57260001
    6.57484818]
 [-33.38622779   0.25480002   0.1318       0.15269999   0.0923
    6.87336349]][0m
[37m[1m[2023-07-17 07:12:52,833][257371] Max Reward on eval: 657.157178383926[0m
[37m[1m[2023-07-17 07:12:52,833][257371] Min Reward on eval: -466.51820942908523[0m
[37m[1m[2023-07-17 07:12:52,833][257371] Mean Reward across all agents: 121.71402299773588[0m
[37m[1m[2023-07-17 07:12:52,833][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:12:52,839][257371] mean_value=-151.76103284840084, max_value=964.610297174193[0m
[37m[1m[2023-07-17 07:12:52,842][257371] New mean coefficients: [[ 0.10549775 -3.2144969  -5.679034   -5.68583     1.2252458   2.3841548 ]][0m
[37m[1m[2023-07-17 07:12:52,843][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:13:01,717][257371] train() took 8.87 seconds to complete[0m
[36m[2023-07-17 07:13:01,718][257371] FPS: 432765.96[0m
[36m[2023-07-17 07:13:01,720][257371] itr=831, itrs=2000, Progress: 41.55%[0m
[36m[2023-07-17 07:13:13,630][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 07:13:13,630][257371] FPS: 325108.86[0m
[36m[2023-07-17 07:13:17,908][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:13:17,909][257371] Reward + Measures: [[-122.77114454    0.90154034    0.92630661    0.05415933    0.96443999
     7.58153009]][0m
[37m[1m[2023-07-17 07:13:17,909][257371] Max Reward on eval: -122.77114454201775[0m
[37m[1m[2023-07-17 07:13:17,909][257371] Min Reward on eval: -122.77114454201775[0m
[37m[1m[2023-07-17 07:13:17,910][257371] Mean Reward across all agents: -122.77114454201775[0m
[37m[1m[2023-07-17 07:13:17,910][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:13:23,193][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:13:23,193][257371] Reward + Measures: [[194.63651656   0.97589999   0.96130002   0.96690005   0.97640002
    7.92420959]
 [ 69.85780427   0.94740003   0.93210012   0.93940002   0.94769996
    7.38455677]
 [-32.87460801   0.75949997   0.72370005   0.71640003   0.72430003
    6.64967871]
 ...
 [129.18198709   0.96130002   0.94330007   0.94929999   0.96340001
    7.97254324]
 [ 85.91693329   0.82609999   0.79040003   0.7252       0.80050004
    7.28783655]
 [ 73.15298401   0.72130001   0.63190001   0.60640001   0.61260003
    7.65773153]][0m
[37m[1m[2023-07-17 07:13:23,193][257371] Max Reward on eval: 273.64836309980603[0m
[37m[1m[2023-07-17 07:13:23,194][257371] Min Reward on eval: -171.10130260214208[0m
[37m[1m[2023-07-17 07:13:23,194][257371] Mean Reward across all agents: 96.80286562113301[0m
[37m[1m[2023-07-17 07:13:23,194][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:13:23,199][257371] mean_value=7.6159720541145965, max_value=570.1117593044805[0m
[37m[1m[2023-07-17 07:13:23,202][257371] New mean coefficients: [[ 0.47591394 -2.9190803  -6.958607   -6.5398006   1.3168947   2.389094  ]][0m
[37m[1m[2023-07-17 07:13:23,203][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:13:32,366][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-17 07:13:32,367][257371] FPS: 419146.57[0m
[36m[2023-07-17 07:13:32,369][257371] itr=832, itrs=2000, Progress: 41.60%[0m
[36m[2023-07-17 07:13:44,275][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 07:13:44,275][257371] FPS: 325370.36[0m
[36m[2023-07-17 07:13:48,607][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:13:48,607][257371] Reward + Measures: [[-119.65515347    0.89897329    0.93508267    0.04585733    0.96602833
     7.65742159]][0m
[37m[1m[2023-07-17 07:13:48,608][257371] Max Reward on eval: -119.65515347478409[0m
[37m[1m[2023-07-17 07:13:48,608][257371] Min Reward on eval: -119.65515347478409[0m
[37m[1m[2023-07-17 07:13:48,608][257371] Mean Reward across all agents: -119.65515347478409[0m
[37m[1m[2023-07-17 07:13:48,608][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:13:53,594][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:13:53,594][257371] Reward + Measures: [[ 46.44691235   0.58249998   0.57960004   0.14670001   0.58929998
    6.83523798]
 [-62.98669225   0.45960003   0.29899999   0.18800001   0.44190001
    7.57797098]
 [-19.33701018   0.23459999   0.30689999   0.0915       0.2703
    7.01943207]
 ...
 [ 10.68001933   0.25409999   0.3145       0.15289998   0.26089999
    7.0112114 ]
 [-40.21502542   0.40400001   0.49460003   0.40770003   0.26050001
    6.38871908]
 [ 67.58922076   0.2906       0.26890001   0.23920003   0.3407
    7.05233335]][0m
[37m[1m[2023-07-17 07:13:53,595][257371] Max Reward on eval: 684.0062828190624[0m
[37m[1m[2023-07-17 07:13:53,595][257371] Min Reward on eval: -417.5580265704542[0m
[37m[1m[2023-07-17 07:13:53,595][257371] Mean Reward across all agents: 34.75286823469003[0m
[37m[1m[2023-07-17 07:13:53,595][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:13:53,598][257371] mean_value=-208.10035698590013, max_value=239.60307239550502[0m
[37m[1m[2023-07-17 07:13:53,601][257371] New mean coefficients: [[-0.4548182 -2.9451263 -6.7219005 -5.665002   0.7686018  2.226157 ]][0m
[37m[1m[2023-07-17 07:13:53,602][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:14:02,626][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 07:14:02,627][257371] FPS: 425586.01[0m
[36m[2023-07-17 07:14:02,629][257371] itr=833, itrs=2000, Progress: 41.65%[0m
[36m[2023-07-17 07:14:14,373][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 07:14:14,374][257371] FPS: 329819.65[0m
[36m[2023-07-17 07:14:18,750][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:14:18,751][257371] Reward + Measures: [[-93.2043571    0.88675731   0.91017395   0.064667     0.95677733
    7.72668695]][0m
[37m[1m[2023-07-17 07:14:18,751][257371] Max Reward on eval: -93.2043570980203[0m
[37m[1m[2023-07-17 07:14:18,751][257371] Min Reward on eval: -93.2043570980203[0m
[37m[1m[2023-07-17 07:14:18,751][257371] Mean Reward across all agents: -93.2043570980203[0m
[37m[1m[2023-07-17 07:14:18,752][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:14:23,789][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:14:23,790][257371] Reward + Measures: [[ 459.87256045    0.79549998    0.78730005    0.0399        0.78870004
     7.75452805]
 [-137.53177543    0.31389999    0.39850003    0.06820001    0.39790002
     6.27122593]
 [ 196.52737707    0.96359998    0.96690005    0.0126        0.97209996
     7.98425388]
 ...
 [  12.78428143    0.8136        0.81760007    0.0437        0.81999999
     7.8718524 ]
 [ 399.99125866    0.7748        0.79360002    0.0912        0.7899
     7.64060593]
 [  72.20387029    0.65730006    0.80629998    0.10100001    0.83850002
     7.81494093]][0m
[37m[1m[2023-07-17 07:14:23,790][257371] Max Reward on eval: 609.4212570140138[0m
[37m[1m[2023-07-17 07:14:23,790][257371] Min Reward on eval: -581.6742897065357[0m
[37m[1m[2023-07-17 07:14:23,791][257371] Mean Reward across all agents: 168.01997417340115[0m
[37m[1m[2023-07-17 07:14:23,791][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:14:23,793][257371] mean_value=-213.00087287649114, max_value=119.437858464567[0m
[37m[1m[2023-07-17 07:14:23,796][257371] New mean coefficients: [[-0.84293365 -2.6656792  -6.0365124  -5.026349    1.3134096   2.3244176 ]][0m
[37m[1m[2023-07-17 07:14:23,797][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:14:32,952][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-17 07:14:32,953][257371] FPS: 419492.55[0m
[36m[2023-07-17 07:14:32,955][257371] itr=834, itrs=2000, Progress: 41.70%[0m
[36m[2023-07-17 07:14:44,749][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 07:14:44,750][257371] FPS: 328300.47[0m
[36m[2023-07-17 07:14:49,077][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:14:49,077][257371] Reward + Measures: [[-83.36024557   0.87994766   0.8820467    0.088125     0.94838327
    7.74358606]][0m
[37m[1m[2023-07-17 07:14:49,078][257371] Max Reward on eval: -83.36024557286113[0m
[37m[1m[2023-07-17 07:14:49,078][257371] Min Reward on eval: -83.36024557286113[0m
[37m[1m[2023-07-17 07:14:49,078][257371] Mean Reward across all agents: -83.36024557286113[0m
[37m[1m[2023-07-17 07:14:49,078][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:14:54,072][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:14:54,073][257371] Reward + Measures: [[27.8745301   0.0599      0.0926      0.0975      0.0707      4.28655672]
 [64.07884175  0.0803      0.1303      0.1286      0.11080001  5.74391842]
 [22.61883117  0.06040001  0.08760001  0.0801      0.07470001  5.38840199]
 ...
 [23.31488325  0.1146      0.1426      0.0872      0.09910001  6.40246534]
 [55.71402205  0.0518      0.072       0.08279999  0.06330001  4.8880744 ]
 [45.41174282  0.0801      0.1046      0.0892      0.0841      5.6303401 ]][0m
[37m[1m[2023-07-17 07:14:54,073][257371] Max Reward on eval: 208.61653113421053[0m
[37m[1m[2023-07-17 07:14:54,073][257371] Min Reward on eval: -100.27866564858705[0m
[37m[1m[2023-07-17 07:14:54,074][257371] Mean Reward across all agents: 47.3737272250311[0m
[37m[1m[2023-07-17 07:14:54,074][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:14:54,075][257371] mean_value=-396.57526463377525, max_value=-12.798330415579244[0m
[36m[2023-07-17 07:14:54,083][257371] XNES is restarting with a new solution whose measures are [0.79940003 0.8118     0.79450005 0.1135     4.50113535] and objective is 454.5620078418404[0m
[36m[2023-07-17 07:14:54,085][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 07:14:54,087][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 07:14:54,088][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:15:03,099][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 07:15:03,099][257371] FPS: 426209.08[0m
[36m[2023-07-17 07:15:03,101][257371] itr=835, itrs=2000, Progress: 41.75%[0m
[36m[2023-07-17 07:15:14,949][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 07:15:14,949][257371] FPS: 326849.73[0m
[36m[2023-07-17 07:15:19,214][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:15:19,214][257371] Reward + Measures: [[40.95926996  0.45875302  0.4132193   0.44017464  0.21195066  4.1569109 ]][0m
[37m[1m[2023-07-17 07:15:19,214][257371] Max Reward on eval: 40.9592699563765[0m
[37m[1m[2023-07-17 07:15:19,215][257371] Min Reward on eval: 40.9592699563765[0m
[37m[1m[2023-07-17 07:15:19,215][257371] Mean Reward across all agents: 40.9592699563765[0m
[37m[1m[2023-07-17 07:15:19,215][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:15:24,210][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:15:24,210][257371] Reward + Measures: [[136.43785841   0.40060002   0.37849998   0.3434       0.223
    4.62815428]
 [-48.35540425   0.25939998   0.27519998   0.28350002   0.21010001
    5.33613348]
 [  1.09260396   0.2802       0.26350001   0.2854       0.1639
    5.99999189]
 ...
 [ 49.74584949   0.20539999   0.17920001   0.17920002   0.15539999
    4.74579859]
 [-59.68817671   0.34670001   0.28050002   0.29050002   0.13810001
    4.75432158]
 [ 81.99585254   0.24849999   0.2277       0.2299       0.16590001
    5.40773296]][0m
[37m[1m[2023-07-17 07:15:24,211][257371] Max Reward on eval: 456.1428219914436[0m
[37m[1m[2023-07-17 07:15:24,211][257371] Min Reward on eval: -393.2376060613431[0m
[37m[1m[2023-07-17 07:15:24,211][257371] Mean Reward across all agents: 13.813318510031735[0m
[37m[1m[2023-07-17 07:15:24,211][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:15:24,213][257371] mean_value=-374.15983850465784, max_value=416.73715192476243[0m
[37m[1m[2023-07-17 07:15:24,215][257371] New mean coefficients: [[ 0.1831327   1.1350152  -0.8122362   0.01883566 -1.3970398  -0.72046274]][0m
[37m[1m[2023-07-17 07:15:24,216][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:15:33,165][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 07:15:33,166][257371] FPS: 429173.75[0m
[36m[2023-07-17 07:15:33,168][257371] itr=836, itrs=2000, Progress: 41.80%[0m
[36m[2023-07-17 07:15:44,828][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 07:15:44,828][257371] FPS: 332233.47[0m
[36m[2023-07-17 07:15:49,108][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:15:49,108][257371] Reward + Measures: [[28.09992311  0.57773334  0.50569701  0.5153327   0.12286001  3.98423696]][0m
[37m[1m[2023-07-17 07:15:49,108][257371] Max Reward on eval: 28.099923111644575[0m
[37m[1m[2023-07-17 07:15:49,109][257371] Min Reward on eval: 28.099923111644575[0m
[37m[1m[2023-07-17 07:15:49,109][257371] Mean Reward across all agents: 28.099923111644575[0m
[37m[1m[2023-07-17 07:15:49,109][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:15:54,099][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:15:54,105][257371] Reward + Measures: [[-201.61184312    0.39739999    0.48850003    0.24730001    0.3565
     4.58759785]
 [ -30.13684359    0.13950001    0.1446        0.123         0.1285
     5.75073862]
 [  10.00891356    0.1944        0.2297        0.14920001    0.1671
     5.93646288]
 ...
 [ 157.85285175    0.44169998    0.43780002    0.1407        0.38210002
     5.19624138]
 [ -25.32972386    0.2115        0.18070002    0.19589999    0.15810001
     5.90856123]
 [ -16.69934658    0.25910002    0.23629999    0.19330001    0.19729999
     5.32156897]][0m
[37m[1m[2023-07-17 07:15:54,105][257371] Max Reward on eval: 271.7444891540334[0m
[37m[1m[2023-07-17 07:15:54,106][257371] Min Reward on eval: -222.7065119839739[0m
[37m[1m[2023-07-17 07:15:54,106][257371] Mean Reward across all agents: 21.47022001834036[0m
[37m[1m[2023-07-17 07:15:54,106][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:15:54,108][257371] mean_value=-407.9576116441708, max_value=122.2748980463136[0m
[37m[1m[2023-07-17 07:15:54,110][257371] New mean coefficients: [[ 0.4205289   0.91364515 -1.3360155   0.57828856 -1.8224292  -0.91567165]][0m
[37m[1m[2023-07-17 07:15:54,111][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:16:03,109][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 07:16:03,110][257371] FPS: 426843.73[0m
[36m[2023-07-17 07:16:03,112][257371] itr=837, itrs=2000, Progress: 41.85%[0m
[36m[2023-07-17 07:16:14,820][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 07:16:14,820][257371] FPS: 330741.28[0m
[36m[2023-07-17 07:16:19,158][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:16:19,158][257371] Reward + Measures: [[-3.1408586   0.46720564  0.36936563  0.39408132  0.12673099  3.75543642]][0m
[37m[1m[2023-07-17 07:16:19,158][257371] Max Reward on eval: -3.140858600565077[0m
[37m[1m[2023-07-17 07:16:19,159][257371] Min Reward on eval: -3.140858600565077[0m
[37m[1m[2023-07-17 07:16:19,159][257371] Mean Reward across all agents: -3.140858600565077[0m
[37m[1m[2023-07-17 07:16:19,159][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:16:24,448][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:16:24,449][257371] Reward + Measures: [[-65.091495     0.27069998   0.2674       0.1754       0.2016
    5.46307135]
 [-51.52927253   0.24340001   0.25530002   0.28260002   0.1542
    6.01096296]
 [-20.51549501   0.1354       0.13150001   0.13880001   0.1164
    5.75329351]
 ...
 [-23.60575789   0.1675       0.17660002   0.1202       0.13430001
    5.69963408]
 [ 28.75763783   0.20369999   0.17990001   0.18790001   0.1868
    5.14718962]
 [-29.25809468   0.19000001   0.20180002   0.148        0.19239999
    5.83935165]][0m
[37m[1m[2023-07-17 07:16:24,449][257371] Max Reward on eval: 212.74349757283926[0m
[37m[1m[2023-07-17 07:16:24,449][257371] Min Reward on eval: -197.01746344119312[0m
[37m[1m[2023-07-17 07:16:24,450][257371] Mean Reward across all agents: -11.491695555443497[0m
[37m[1m[2023-07-17 07:16:24,450][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:16:24,451][257371] mean_value=-554.8335455934945, max_value=-83.63233188826007[0m
[36m[2023-07-17 07:16:24,453][257371] XNES is restarting with a new solution whose measures are [0.71219999 0.58719999 0.55159998 0.55400002 7.88492441] and objective is 119.07248284872621[0m
[36m[2023-07-17 07:16:24,454][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 07:16:24,457][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 07:16:24,457][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:16:33,614][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-17 07:16:33,614][257371] FPS: 419456.68[0m
[36m[2023-07-17 07:16:33,616][257371] itr=838, itrs=2000, Progress: 41.90%[0m
[36m[2023-07-17 07:16:45,440][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 07:16:45,440][257371] FPS: 327665.27[0m
[36m[2023-07-17 07:16:49,748][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:16:49,749][257371] Reward + Measures: [[164.07405019   0.85565537   0.80403203   0.79678434   0.80461305
    7.84226608]][0m
[37m[1m[2023-07-17 07:16:49,749][257371] Max Reward on eval: 164.07405018517744[0m
[37m[1m[2023-07-17 07:16:49,749][257371] Min Reward on eval: 164.07405018517744[0m
[37m[1m[2023-07-17 07:16:49,749][257371] Mean Reward across all agents: 164.07405018517744[0m
[37m[1m[2023-07-17 07:16:49,750][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:16:54,785][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:16:54,785][257371] Reward + Measures: [[117.85521877   0.87650007   0.80289996   0.78899997   0.79460001
    7.9346118 ]
 [104.23862787   0.20560001   0.27309999   0.22320001   0.22740002
    5.35563517]
 [ 79.4614266    0.51820004   0.50620002   0.51190001   0.52039999
    6.90807867]
 ...
 [ 21.37599364   0.37360001   0.3924       0.37750003   0.37190002
    5.85845518]
 [-59.61796111   0.94779998   0.94300002   0.94360012   0.94629997
    6.84408569]
 [  6.56523328   0.17979999   0.1842       0.22490001   0.15680002
    5.0981493 ]][0m
[37m[1m[2023-07-17 07:16:54,786][257371] Max Reward on eval: 242.7541217789054[0m
[37m[1m[2023-07-17 07:16:54,786][257371] Min Reward on eval: -370.8326115154661[0m
[37m[1m[2023-07-17 07:16:54,786][257371] Mean Reward across all agents: 22.495517759854835[0m
[37m[1m[2023-07-17 07:16:54,786][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:16:54,789][257371] mean_value=-195.26452618912683, max_value=307.5160010192815[0m
[37m[1m[2023-07-17 07:16:54,791][257371] New mean coefficients: [[-1.4048176   0.40894675  0.04862791  0.14037073 -1.4840348  -0.26474285]][0m
[37m[1m[2023-07-17 07:16:54,792][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:17:03,801][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 07:17:03,802][257371] FPS: 426305.28[0m
[36m[2023-07-17 07:17:03,804][257371] itr=839, itrs=2000, Progress: 41.95%[0m
[36m[2023-07-17 07:17:15,686][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 07:17:15,686][257371] FPS: 325966.69[0m
[36m[2023-07-17 07:17:20,041][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:17:20,041][257371] Reward + Measures: [[20.88462087  0.74867195  0.68738198  0.69327736  0.70778435  7.4361515 ]][0m
[37m[1m[2023-07-17 07:17:20,041][257371] Max Reward on eval: 20.88462086633914[0m
[37m[1m[2023-07-17 07:17:20,042][257371] Min Reward on eval: 20.88462086633914[0m
[37m[1m[2023-07-17 07:17:20,042][257371] Mean Reward across all agents: 20.88462086633914[0m
[37m[1m[2023-07-17 07:17:20,042][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:17:25,171][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:17:25,171][257371] Reward + Measures: [[ 34.74148669   0.33379999   0.34349999   0.3691       0.36810002
    6.68709517]
 [110.234077     0.51379997   0.39900002   0.37949997   0.3942
    7.75274372]
 [ 76.71303475   0.1112       0.0878       0.1027       0.09940001
    6.79755497]
 ...
 [ 29.19974181   0.73789996   0.74590009   0.65280002   0.66329998
    7.43900681]
 [ 53.13036107   0.19750002   0.2132       0.19490001   0.20650001
    6.22627401]
 [-26.39790587   0.34940001   0.29800001   0.27950001   0.29570001
    6.96148777]][0m
[37m[1m[2023-07-17 07:17:25,172][257371] Max Reward on eval: 655.5488829930313[0m
[37m[1m[2023-07-17 07:17:25,172][257371] Min Reward on eval: -222.65558483097703[0m
[37m[1m[2023-07-17 07:17:25,172][257371] Mean Reward across all agents: 64.32980680213285[0m
[37m[1m[2023-07-17 07:17:25,172][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:17:25,177][257371] mean_value=-171.90388238050227, max_value=657.5240493115365[0m
[37m[1m[2023-07-17 07:17:25,179][257371] New mean coefficients: [[-1.7043222   0.42664382  0.21741956  0.94422823 -1.9653717   0.41247112]][0m
[37m[1m[2023-07-17 07:17:25,180][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:17:34,275][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 07:17:34,275][257371] FPS: 422324.29[0m
[36m[2023-07-17 07:17:34,277][257371] itr=840, itrs=2000, Progress: 42.00%[0m
[37m[1m[2023-07-17 07:20:48,106][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000820[0m
[36m[2023-07-17 07:21:00,624][257371] train() took 11.96 seconds to complete[0m
[36m[2023-07-17 07:21:00,625][257371] FPS: 321093.05[0m
[36m[2023-07-17 07:21:04,996][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:21:04,997][257371] Reward + Measures: [[19.46734758  0.81410658  0.77735966  0.70409763  0.77173662  6.99744511]][0m
[37m[1m[2023-07-17 07:21:04,997][257371] Max Reward on eval: 19.467347583536903[0m
[37m[1m[2023-07-17 07:21:04,997][257371] Min Reward on eval: 19.467347583536903[0m
[37m[1m[2023-07-17 07:21:04,997][257371] Mean Reward across all agents: 19.467347583536903[0m
[37m[1m[2023-07-17 07:21:04,998][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:21:09,990][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:21:09,991][257371] Reward + Measures: [[ -91.01393199    0.97640002    0.96340007    0.97069997    0.97619992
     7.83044434]
 [  78.62460193    0.48880002    0.43710002    0.42649999    0.43670002
     7.60649967]
 [  50.71518588    0.91540003    0.88899994    0.90090007    0.9127
     7.68230391]
 ...
 [ -50.8791052     0.47489998    0.32030001    0.2476        0.49320003
     6.3905282 ]
 [ -67.75587409    0.88149995    0.80289996    0.77680004    0.80670005
     6.99460983]
 [-145.5235602     0.86380005    0.80140001    0.81010002    0.81150001
     7.67140198]][0m
[37m[1m[2023-07-17 07:21:09,991][257371] Max Reward on eval: 319.6174602939747[0m
[37m[1m[2023-07-17 07:21:09,992][257371] Min Reward on eval: -618.4310368062928[0m
[37m[1m[2023-07-17 07:21:09,992][257371] Mean Reward across all agents: 28.23680192597973[0m
[37m[1m[2023-07-17 07:21:09,992][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:21:09,994][257371] mean_value=-185.24844955187817, max_value=135.2510572392897[0m
[37m[1m[2023-07-17 07:21:09,997][257371] New mean coefficients: [[-1.8722767   0.91469526  2.2695823   0.34228146 -1.6547494   0.53119415]][0m
[37m[1m[2023-07-17 07:21:09,998][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:21:18,956][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 07:21:18,956][257371] FPS: 428748.15[0m
[36m[2023-07-17 07:21:18,959][257371] itr=841, itrs=2000, Progress: 42.05%[0m
[36m[2023-07-17 07:21:30,841][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 07:21:30,842][257371] FPS: 325932.80[0m
[36m[2023-07-17 07:21:35,189][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:21:35,190][257371] Reward + Measures: [[-34.46476257   0.81777561   0.75141567   0.76669765   0.77870464
    7.48884773]][0m
[37m[1m[2023-07-17 07:21:35,190][257371] Max Reward on eval: -34.46476256570208[0m
[37m[1m[2023-07-17 07:21:35,190][257371] Min Reward on eval: -34.46476256570208[0m
[37m[1m[2023-07-17 07:21:35,191][257371] Mean Reward across all agents: -34.46476256570208[0m
[37m[1m[2023-07-17 07:21:35,191][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:21:40,368][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:21:40,369][257371] Reward + Measures: [[ 36.41330069   0.88620007   0.79820001   0.78299999   0.79510009
    7.99033499]
 [ 96.23016266   0.86259997   0.79879999   0.83510011   0.81980002
    7.46340036]
 [ 42.14848997   0.17830001   0.15460001   0.1655       0.1139
    6.59466696]
 ...
 [ 14.75724794   0.21180001   0.20799999   0.1268       0.1965
    6.65837193]
 [108.97370722   0.93760008   0.8865       0.88020003   0.88589996
    7.99823904]
 [ 14.52182735   0.1469       0.12310001   0.14039999   0.14320002
    6.18440771]][0m
[37m[1m[2023-07-17 07:21:40,369][257371] Max Reward on eval: 422.1621286060661[0m
[37m[1m[2023-07-17 07:21:40,369][257371] Min Reward on eval: -432.3772934045643[0m
[37m[1m[2023-07-17 07:21:40,370][257371] Mean Reward across all agents: 57.86174236628951[0m
[37m[1m[2023-07-17 07:21:40,370][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:21:40,373][257371] mean_value=-152.15291845605572, max_value=457.7275359884301[0m
[37m[1m[2023-07-17 07:21:40,375][257371] New mean coefficients: [[-2.1116583   0.06335413  2.012414    1.0301263  -2.4578793   0.4816228 ]][0m
[37m[1m[2023-07-17 07:21:40,376][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:21:49,362][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 07:21:49,363][257371] FPS: 427402.43[0m
[36m[2023-07-17 07:21:49,365][257371] itr=842, itrs=2000, Progress: 42.10%[0m
[36m[2023-07-17 07:22:01,039][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 07:22:01,040][257371] FPS: 331827.13[0m
[36m[2023-07-17 07:22:05,297][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:22:05,297][257371] Reward + Measures: [[14.56094126  0.86178792  0.82106996  0.81157106  0.8112976   7.22929144]][0m
[37m[1m[2023-07-17 07:22:05,297][257371] Max Reward on eval: 14.560941258935374[0m
[37m[1m[2023-07-17 07:22:05,298][257371] Min Reward on eval: 14.560941258935374[0m
[37m[1m[2023-07-17 07:22:05,298][257371] Mean Reward across all agents: 14.560941258935374[0m
[37m[1m[2023-07-17 07:22:05,298][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:22:10,260][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:22:10,261][257371] Reward + Measures: [[ 67.68155217   0.7295       0.75799996   0.60120004   0.64950001
    7.55812168]
 [ 50.34860051   0.81280005   0.79790002   0.76640004   0.76480001
    7.53566837]
 [ -5.85544969   0.1521       0.0844       0.0663       0.064
    7.31544828]
 ...
 [-33.2699897    0.19840001   0.229        0.1621       0.23210001
    5.95351076]
 [  9.4311973    0.1146       0.11130001   0.0851       0.0961
    6.98715067]
 [ -8.17108156   0.80680007   0.79689997   0.60320002   0.77910006
    7.09801054]][0m
[37m[1m[2023-07-17 07:22:10,261][257371] Max Reward on eval: 359.1588287457824[0m
[37m[1m[2023-07-17 07:22:10,262][257371] Min Reward on eval: -263.07757853902876[0m
[37m[1m[2023-07-17 07:22:10,262][257371] Mean Reward across all agents: 39.139802280396665[0m
[37m[1m[2023-07-17 07:22:10,262][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:22:10,267][257371] mean_value=-223.9015655529046, max_value=470.56485183555634[0m
[37m[1m[2023-07-17 07:22:10,269][257371] New mean coefficients: [[-2.733711   -0.15010972  3.6103492   0.99995106 -2.4441452   0.1451779 ]][0m
[37m[1m[2023-07-17 07:22:10,270][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:22:19,243][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 07:22:19,243][257371] FPS: 428047.62[0m
[36m[2023-07-17 07:22:19,245][257371] itr=843, itrs=2000, Progress: 42.15%[0m
[36m[2023-07-17 07:22:31,044][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 07:22:31,044][257371] FPS: 328224.61[0m
[36m[2023-07-17 07:22:35,287][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:22:35,288][257371] Reward + Measures: [[49.09897364  0.76235634  0.75937939  0.66292596  0.70237207  7.14628839]][0m
[37m[1m[2023-07-17 07:22:35,288][257371] Max Reward on eval: 49.098973641578105[0m
[37m[1m[2023-07-17 07:22:35,288][257371] Min Reward on eval: 49.098973641578105[0m
[37m[1m[2023-07-17 07:22:35,289][257371] Mean Reward across all agents: 49.098973641578105[0m
[37m[1m[2023-07-17 07:22:35,289][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:22:40,264][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:22:40,265][257371] Reward + Measures: [[-84.29348646   0.2263       0.32800004   0.26710001   0.36849999
    6.99104071]
 [356.67053224   0.0508       0.5808       0.53249997   0.55440003
    7.05686283]
 [126.3621259    0.49600002   0.41679999   0.16190003   0.45019999
    7.56544638]
 ...
 [ 74.63727608   0.0702       0.0993       0.0966       0.078
    6.62084198]
 [ 75.59904576   0.92460006   0.87729996   0.8563       0.8901
    7.53354406]
 [ 72.48713685   0.93740004   0.93370003   0.8962       0.8858
    7.85984278]][0m
[37m[1m[2023-07-17 07:22:40,265][257371] Max Reward on eval: 574.5490169588477[0m
[37m[1m[2023-07-17 07:22:40,265][257371] Min Reward on eval: -337.00553656890986[0m
[37m[1m[2023-07-17 07:22:40,265][257371] Mean Reward across all agents: 42.479167499309455[0m
[37m[1m[2023-07-17 07:22:40,266][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:22:40,269][257371] mean_value=-189.5740915596047, max_value=325.0941598869217[0m
[37m[1m[2023-07-17 07:22:40,271][257371] New mean coefficients: [[-3.1766043   0.40820986  4.314922    0.76445895 -2.9199367  -0.29907784]][0m
[37m[1m[2023-07-17 07:22:40,272][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:22:49,194][257371] train() took 8.92 seconds to complete[0m
[36m[2023-07-17 07:22:49,194][257371] FPS: 430502.66[0m
[36m[2023-07-17 07:22:49,196][257371] itr=844, itrs=2000, Progress: 42.20%[0m
[36m[2023-07-17 07:23:00,865][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 07:23:00,865][257371] FPS: 331952.37[0m
[36m[2023-07-17 07:23:05,253][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:23:05,254][257371] Reward + Measures: [[70.14989272  0.88107902  0.89893502  0.7556206   0.79679036  7.48355246]][0m
[37m[1m[2023-07-17 07:23:05,254][257371] Max Reward on eval: 70.14989272291777[0m
[37m[1m[2023-07-17 07:23:05,254][257371] Min Reward on eval: 70.14989272291777[0m
[37m[1m[2023-07-17 07:23:05,255][257371] Mean Reward across all agents: 70.14989272291777[0m
[37m[1m[2023-07-17 07:23:05,255][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:23:10,310][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:23:10,311][257371] Reward + Measures: [[ -74.05788977    0.21570002    0.20060001    0.0801        0.2194
     7.26663065]
 [ 181.52060689    0.33500004    0.7306        0.0601        0.66480005
     6.87999058]
 [ -22.25766027    0.1856        0.2987        0.1312        0.3021
     7.1898818 ]
 ...
 [-122.01714235    0.30329999    0.30700001    0.13780001    0.228
     7.26815414]
 [  23.14379068    0.17029999    0.16409999    0.1865        0.21030001
     6.32470036]
 [  48.5984307     0.2877        0.34430003    0.1024        0.3188
     7.19742155]][0m
[37m[1m[2023-07-17 07:23:10,311][257371] Max Reward on eval: 658.7364821457304[0m
[37m[1m[2023-07-17 07:23:10,311][257371] Min Reward on eval: -301.49479105416685[0m
[37m[1m[2023-07-17 07:23:10,312][257371] Mean Reward across all agents: 32.28994224510489[0m
[37m[1m[2023-07-17 07:23:10,312][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:23:10,316][257371] mean_value=-201.40465007703278, max_value=686.6412933592965[0m
[37m[1m[2023-07-17 07:23:10,318][257371] New mean coefficients: [[-4.2109118   0.06537193  3.1879516   1.9856629  -3.0422015  -0.4395006 ]][0m
[37m[1m[2023-07-17 07:23:10,319][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:23:19,480][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-17 07:23:19,481][257371] FPS: 419249.92[0m
[36m[2023-07-17 07:23:19,483][257371] itr=845, itrs=2000, Progress: 42.25%[0m
[36m[2023-07-17 07:23:31,091][257371] train() took 11.51 seconds to complete[0m
[36m[2023-07-17 07:23:31,092][257371] FPS: 333706.31[0m
[36m[2023-07-17 07:23:35,312][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:23:35,313][257371] Reward + Measures: [[68.54242949  0.87683803  0.89467365  0.73923135  0.79413533  7.26619387]][0m
[37m[1m[2023-07-17 07:23:35,313][257371] Max Reward on eval: 68.54242948890479[0m
[37m[1m[2023-07-17 07:23:35,313][257371] Min Reward on eval: 68.54242948890479[0m
[37m[1m[2023-07-17 07:23:35,314][257371] Mean Reward across all agents: 68.54242948890479[0m
[37m[1m[2023-07-17 07:23:35,314][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:23:40,316][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:23:40,316][257371] Reward + Measures: [[ 37.77592373   0.97040004   0.95620006   0.96240008   0.96829998
    7.98495817]
 [ 69.60417717   0.5686       0.51080006   0.49790001   0.49600002
    7.62306738]
 [147.17181778   0.87600005   0.82870001   0.72460002   0.84170002
    7.60920334]
 ...
 [ -1.08073903   0.37180001   0.37460002   0.24010001   0.24250002
    7.67699194]
 [ 28.79367231   0.88290006   0.88050002   0.83820003   0.84200001
    7.92579794]
 [ 44.22396587   0.53319997   0.2915       0.21839999   0.23099999
    7.95956182]][0m
[37m[1m[2023-07-17 07:23:40,317][257371] Max Reward on eval: 285.4003763125278[0m
[37m[1m[2023-07-17 07:23:40,317][257371] Min Reward on eval: -159.5629377182573[0m
[37m[1m[2023-07-17 07:23:40,317][257371] Mean Reward across all agents: 32.38801705974263[0m
[37m[1m[2023-07-17 07:23:40,317][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:23:40,319][257371] mean_value=-144.16348108734508, max_value=536.9654588276436[0m
[37m[1m[2023-07-17 07:23:40,322][257371] New mean coefficients: [[-4.2783256  -0.5292135   3.3050652   2.5979707  -2.9470935  -0.72978926]][0m
[37m[1m[2023-07-17 07:23:40,323][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:23:49,370][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 07:23:49,371][257371] FPS: 424513.60[0m
[36m[2023-07-17 07:23:49,373][257371] itr=846, itrs=2000, Progress: 42.30%[0m
[36m[2023-07-17 07:24:01,183][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 07:24:01,183][257371] FPS: 327947.29[0m
[36m[2023-07-17 07:24:05,500][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:24:05,501][257371] Reward + Measures: [[-63.85261746   0.91567868   0.92289901   0.88293636   0.884691
    7.44959354]][0m
[37m[1m[2023-07-17 07:24:05,501][257371] Max Reward on eval: -63.85261745771686[0m
[37m[1m[2023-07-17 07:24:05,501][257371] Min Reward on eval: -63.85261745771686[0m
[37m[1m[2023-07-17 07:24:05,501][257371] Mean Reward across all agents: -63.85261745771686[0m
[37m[1m[2023-07-17 07:24:05,502][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:24:10,804][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:24:10,810][257371] Reward + Measures: [[ 50.93983769   0.65760005   0.63630003   0.59610003   0.57820004
    6.89496374]
 [-91.93614811   0.30950001   0.45319995   0.11799999   0.34530002
    6.29944563]
 [ 24.51557602   0.71789998   0.78680009   0.68510002   0.71989995
    7.29578543]
 ...
 [  6.0665043    0.4481       0.52730006   0.36539999   0.45670006
    7.10277939]
 [-33.22179765   0.1948       0.3565       0.1294       0.31029999
    6.30460453]
 [  5.23697275   0.87239999   0.96329993   0.89740002   0.91440004
    7.38175917]][0m
[37m[1m[2023-07-17 07:24:10,810][257371] Max Reward on eval: 443.6454396066489[0m
[37m[1m[2023-07-17 07:24:10,811][257371] Min Reward on eval: -252.4556121806614[0m
[37m[1m[2023-07-17 07:24:10,811][257371] Mean Reward across all agents: -5.839598804108693[0m
[37m[1m[2023-07-17 07:24:10,811][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:24:10,813][257371] mean_value=-231.07376090846554, max_value=69.84579832426573[0m
[37m[1m[2023-07-17 07:24:10,816][257371] New mean coefficients: [[-3.968576    0.29778528  2.572557    2.393581   -3.0324538  -0.62028295]][0m
[37m[1m[2023-07-17 07:24:10,817][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:24:19,891][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 07:24:19,891][257371] FPS: 423268.63[0m
[36m[2023-07-17 07:24:19,893][257371] itr=847, itrs=2000, Progress: 42.35%[0m
[36m[2023-07-17 07:24:31,665][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 07:24:31,665][257371] FPS: 329093.92[0m
[36m[2023-07-17 07:24:35,927][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:24:35,927][257371] Reward + Measures: [[17.1072462   0.91664535  0.92191535  0.90323567  0.90134829  6.65207005]][0m
[37m[1m[2023-07-17 07:24:35,927][257371] Max Reward on eval: 17.1072462006633[0m
[37m[1m[2023-07-17 07:24:35,928][257371] Min Reward on eval: 17.1072462006633[0m
[37m[1m[2023-07-17 07:24:35,928][257371] Mean Reward across all agents: 17.1072462006633[0m
[37m[1m[2023-07-17 07:24:35,928][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:24:40,944][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:24:40,945][257371] Reward + Measures: [[ -18.17141006    0.87239999    0.78959996    0.75130004    0.72349995
     7.47194815]
 [   1.95453502    0.26949999    0.23339999    0.20550001    0.24879999
     6.90612793]
 [ -94.17567908    0.2057        0.317         0.3256        0.2139
     5.74529505]
 ...
 [-195.86937525    0.73500001    0.8743        0.007         0.72329998
     7.42918158]
 [ -74.42575909    0.46499997    0.4526        0.24389999    0.4508
     7.02617598]
 [ -48.69323536    0.098         0.55610001    0.38330001    0.39810002
     6.42320347]][0m
[37m[1m[2023-07-17 07:24:40,945][257371] Max Reward on eval: 367.515785260126[0m
[37m[1m[2023-07-17 07:24:40,945][257371] Min Reward on eval: -303.7661457387032[0m
[37m[1m[2023-07-17 07:24:40,946][257371] Mean Reward across all agents: -11.618402582129367[0m
[37m[1m[2023-07-17 07:24:40,946][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:24:40,950][257371] mean_value=-257.88389896557936, max_value=72.87903572695187[0m
[37m[1m[2023-07-17 07:24:40,952][257371] New mean coefficients: [[-3.2124898   0.22868377  2.9965792   1.463668   -1.9855413  -0.50534683]][0m
[37m[1m[2023-07-17 07:24:40,953][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:24:50,045][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 07:24:50,045][257371] FPS: 422444.77[0m
[36m[2023-07-17 07:24:50,047][257371] itr=848, itrs=2000, Progress: 42.40%[0m
[36m[2023-07-17 07:25:01,823][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 07:25:01,823][257371] FPS: 328901.62[0m
[36m[2023-07-17 07:25:06,113][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:25:06,113][257371] Reward + Measures: [[-40.8015646    0.95474428   0.9544186    0.91680598   0.91649032
    7.81437731]][0m
[37m[1m[2023-07-17 07:25:06,113][257371] Max Reward on eval: -40.801564598897784[0m
[37m[1m[2023-07-17 07:25:06,114][257371] Min Reward on eval: -40.801564598897784[0m
[37m[1m[2023-07-17 07:25:06,114][257371] Mean Reward across all agents: -40.801564598897784[0m
[37m[1m[2023-07-17 07:25:06,114][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:25:11,058][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:25:11,059][257371] Reward + Measures: [[100.28506795   0.0628       0.68380004   0.58710003   0.65880007
    5.98581648]
 [ 33.36461791   0.87390006   0.93330002   0.91539997   0.91040003
    7.32925034]
 [ 32.28658942   0.67750001   0.78200001   0.72609997   0.74760002
    7.58705473]
 ...
 [-24.9930025    0.23279999   0.2886       0.22839999   0.25219998
    6.65577936]
 [137.11506083   0.963        0.9733001    0.97620004   0.97030002
    7.88472891]
 [ 85.66397968   0.82429999   0.8617       0.81459999   0.81140006
    7.85039854]][0m
[37m[1m[2023-07-17 07:25:11,059][257371] Max Reward on eval: 579.0452098878101[0m
[37m[1m[2023-07-17 07:25:11,059][257371] Min Reward on eval: -513.3827934317757[0m
[37m[1m[2023-07-17 07:25:11,059][257371] Mean Reward across all agents: 25.53834807952994[0m
[37m[1m[2023-07-17 07:25:11,060][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:25:11,064][257371] mean_value=-198.00353102412976, max_value=704.080312035326[0m
[37m[1m[2023-07-17 07:25:11,066][257371] New mean coefficients: [[-3.883954  -0.6896506  2.7118232  2.1412246 -2.6439352 -0.6772215]][0m
[37m[1m[2023-07-17 07:25:11,067][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:25:20,068][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 07:25:20,069][257371] FPS: 426698.44[0m
[36m[2023-07-17 07:25:20,071][257371] itr=849, itrs=2000, Progress: 42.45%[0m
[36m[2023-07-17 07:25:31,834][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 07:25:31,834][257371] FPS: 329287.83[0m
[36m[2023-07-17 07:25:36,177][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:25:36,177][257371] Reward + Measures: [[3.61913771 0.91708595 0.9164257  0.87965071 0.87960863 6.89201832]][0m
[37m[1m[2023-07-17 07:25:36,178][257371] Max Reward on eval: 3.619137707301454[0m
[37m[1m[2023-07-17 07:25:36,178][257371] Min Reward on eval: 3.619137707301454[0m
[37m[1m[2023-07-17 07:25:36,178][257371] Mean Reward across all agents: 3.619137707301454[0m
[37m[1m[2023-07-17 07:25:36,178][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:25:41,161][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:25:41,162][257371] Reward + Measures: [[-37.1803112    0.96049994   0.95970005   0.93190002   0.92989999
    7.55063963]
 [ 13.81157361   0.41         0.45520002   0.40770003   0.42280003
    7.30754995]
 [ 38.24969672   0.1833       0.37440002   0.31009999   0.29650003
    6.07848787]
 ...
 [252.45272777   0.39089999   0.49780002   0.1717       0.4993
    6.78025198]
 [-17.32558612   0.71160001   0.68800002   0.67740005   0.68399996
    6.8976264 ]
 [ -0.75005345   0.95760006   0.96939993   0.9716       0.96589994
    6.74136591]][0m
[37m[1m[2023-07-17 07:25:41,162][257371] Max Reward on eval: 252.4527277675923[0m
[37m[1m[2023-07-17 07:25:41,162][257371] Min Reward on eval: -122.60924151903018[0m
[37m[1m[2023-07-17 07:25:41,162][257371] Mean Reward across all agents: 15.414173116180349[0m
[37m[1m[2023-07-17 07:25:41,163][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:25:41,165][257371] mean_value=-198.5663572114469, max_value=104.8754553571992[0m
[37m[1m[2023-07-17 07:25:41,167][257371] New mean coefficients: [[-3.689547  -1.2412734  2.946422   1.7035332 -2.7122993 -0.5510446]][0m
[37m[1m[2023-07-17 07:25:41,168][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:25:50,158][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 07:25:50,158][257371] FPS: 427208.18[0m
[36m[2023-07-17 07:25:50,161][257371] itr=850, itrs=2000, Progress: 42.50%[0m
[37m[1m[2023-07-17 07:29:12,249][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000830[0m
[36m[2023-07-17 07:29:24,474][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 07:29:24,475][257371] FPS: 329720.73[0m
[36m[2023-07-17 07:29:28,823][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:29:28,824][257371] Reward + Measures: [[2.29734697 0.92505836 0.92378366 0.8874467  0.887326   6.83863068]][0m
[37m[1m[2023-07-17 07:29:28,824][257371] Max Reward on eval: 2.297346969471274[0m
[37m[1m[2023-07-17 07:29:28,824][257371] Min Reward on eval: 2.297346969471274[0m
[37m[1m[2023-07-17 07:29:28,824][257371] Mean Reward across all agents: 2.297346969471274[0m
[37m[1m[2023-07-17 07:29:28,825][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:29:33,754][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:29:33,755][257371] Reward + Measures: [[460.75032378   0.84200001   0.8872       0.0405       0.87720007
    7.60917997]
 [ 36.51015151   0.46250001   0.40480003   0.35910001   0.45590001
    6.83601236]
 [ 11.18571506   0.9332       0.96560001   0.90990001   0.93600005
    6.6014657 ]
 ...
 [-57.31890579   0.3283       0.3242       0.1468       0.3351
    6.35548639]
 [ 42.10266664   0.23769999   0.26300001   0.2067       0.28169999
    6.02009344]
 [  2.66947814   0.28080001   0.17519999   0.25580001   0.2141
    5.5091548 ]][0m
[37m[1m[2023-07-17 07:29:33,755][257371] Max Reward on eval: 460.7503237827681[0m
[37m[1m[2023-07-17 07:29:33,755][257371] Min Reward on eval: -561.0904808248627[0m
[37m[1m[2023-07-17 07:29:33,756][257371] Mean Reward across all agents: 3.931095709790534[0m
[37m[1m[2023-07-17 07:29:33,756][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:29:33,758][257371] mean_value=-245.42520016746013, max_value=163.42640482530336[0m
[37m[1m[2023-07-17 07:29:33,761][257371] New mean coefficients: [[-4.298818   -0.76904285  1.3018641   2.495462   -1.8891402  -0.54433215]][0m
[37m[1m[2023-07-17 07:29:33,762][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:29:42,800][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 07:29:42,800][257371] FPS: 424921.99[0m
[36m[2023-07-17 07:29:42,803][257371] itr=851, itrs=2000, Progress: 42.55%[0m
[36m[2023-07-17 07:29:54,535][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 07:29:54,536][257371] FPS: 330151.61[0m
[36m[2023-07-17 07:29:58,900][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:29:58,901][257371] Reward + Measures: [[-69.91871899   0.93112135   0.92666298   0.90307802   0.90172136
    7.2484622 ]][0m
[37m[1m[2023-07-17 07:29:58,901][257371] Max Reward on eval: -69.91871899256378[0m
[37m[1m[2023-07-17 07:29:58,901][257371] Min Reward on eval: -69.91871899256378[0m
[37m[1m[2023-07-17 07:29:58,902][257371] Mean Reward across all agents: -69.91871899256378[0m
[37m[1m[2023-07-17 07:29:58,902][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:30:03,945][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:30:03,946][257371] Reward + Measures: [[-17.30811952   0.3673       0.50060004   0.36209998   0.46539998
    5.66065359]
 [-23.56977027   0.20280002   0.184        0.1653       0.1582
    6.66970825]
 [ 98.79606866   0.34819999   0.4348       0.189        0.33759999
    6.92851257]
 ...
 [396.55172515   0.0865       0.60050005   0.61750001   0.61980003
    6.95144796]
 [-20.17520555   0.76060003   0.70820004   0.70099992   0.70050001
    5.91439342]
 [ 50.7186762    0.142        0.4842       0.23629999   0.40810004
    6.08423996]][0m
[37m[1m[2023-07-17 07:30:03,946][257371] Max Reward on eval: 396.55172514803706[0m
[37m[1m[2023-07-17 07:30:03,946][257371] Min Reward on eval: -191.1433105662465[0m
[37m[1m[2023-07-17 07:30:03,947][257371] Mean Reward across all agents: 29.067898461792012[0m
[37m[1m[2023-07-17 07:30:03,947][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:30:03,950][257371] mean_value=-211.30214172029343, max_value=299.6577882071284[0m
[37m[1m[2023-07-17 07:30:03,953][257371] New mean coefficients: [[-4.238067   -0.6362915   0.52742875  3.7427697  -1.806254   -0.09482875]][0m
[37m[1m[2023-07-17 07:30:03,954][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:30:13,000][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 07:30:13,000][257371] FPS: 424557.62[0m
[36m[2023-07-17 07:30:13,003][257371] itr=852, itrs=2000, Progress: 42.60%[0m
[36m[2023-07-17 07:30:24,804][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 07:30:24,804][257371] FPS: 328156.29[0m
[36m[2023-07-17 07:30:29,016][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:30:29,016][257371] Reward + Measures: [[22.07287885  0.94645399  0.94563574  0.87606031  0.89790201  6.6933651 ]][0m
[37m[1m[2023-07-17 07:30:29,017][257371] Max Reward on eval: 22.072878845966343[0m
[37m[1m[2023-07-17 07:30:29,017][257371] Min Reward on eval: 22.072878845966343[0m
[37m[1m[2023-07-17 07:30:29,017][257371] Mean Reward across all agents: 22.072878845966343[0m
[37m[1m[2023-07-17 07:30:29,017][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:30:34,005][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:30:34,011][257371] Reward + Measures: [[-29.11076418   0.37620002   0.37190002   0.39640003   0.37099999
    7.45872259]
 [-15.38754499   0.16080001   0.1734       0.1049       0.16150001
    6.19519043]
 [ 15.43368936   0.2951       0.25709999   0.27680001   0.24089999
    7.12091064]
 ...
 [-83.92224833   0.25460002   0.23190002   0.23850003   0.25299999
    6.46500492]
 [ 16.13525995   0.77860004   0.76800007   0.77990001   0.76570004
    7.75146866]
 [ 36.63804442   0.46339998   0.55080003   0.4887       0.54089999
    7.08824253]][0m
[37m[1m[2023-07-17 07:30:34,011][257371] Max Reward on eval: 236.12013564892112[0m
[37m[1m[2023-07-17 07:30:34,012][257371] Min Reward on eval: -422.9124677480198[0m
[37m[1m[2023-07-17 07:30:34,012][257371] Mean Reward across all agents: 3.4895309990698693[0m
[37m[1m[2023-07-17 07:30:34,012][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:30:34,014][257371] mean_value=-239.07821426353712, max_value=179.5876378487352[0m
[37m[1m[2023-07-17 07:30:34,017][257371] New mean coefficients: [[-2.3613858  -0.7415564   1.746109    3.1010656  -2.6296334  -0.00111219]][0m
[37m[1m[2023-07-17 07:30:34,018][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:30:42,952][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 07:30:42,952][257371] FPS: 429892.75[0m
[36m[2023-07-17 07:30:42,954][257371] itr=853, itrs=2000, Progress: 42.65%[0m
[36m[2023-07-17 07:30:54,564][257371] train() took 11.51 seconds to complete[0m
[36m[2023-07-17 07:30:54,564][257371] FPS: 333738.89[0m
[36m[2023-07-17 07:30:58,755][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:30:58,756][257371] Reward + Measures: [[61.06585045  0.93027329  0.95580596  0.75396597  0.88409323  6.43044376]][0m
[37m[1m[2023-07-17 07:30:58,756][257371] Max Reward on eval: 61.06585045037312[0m
[37m[1m[2023-07-17 07:30:58,756][257371] Min Reward on eval: 61.06585045037312[0m
[37m[1m[2023-07-17 07:30:58,756][257371] Mean Reward across all agents: 61.06585045037312[0m
[37m[1m[2023-07-17 07:30:58,757][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:31:03,977][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:31:03,978][257371] Reward + Measures: [[-32.29517623   0.07870001   0.07099999   0.0868       0.06769999
    6.91424036]
 [ 15.51582576   0.1499       0.1517       0.10749999   0.0893
    6.18760443]
 [-38.72021629   0.1374       0.1221       0.0918       0.0909
    6.54219007]
 ...
 [-50.72727596   0.14470001   0.09420001   0.0539       0.0474
    6.67625761]
 [-71.7309136    0.12800001   0.0823       0.0887       0.075
    6.65565443]
 [-29.86558413   0.22720002   0.1797       0.2053       0.16049999
    6.67546082]][0m
[37m[1m[2023-07-17 07:31:03,978][257371] Max Reward on eval: 439.8873653545976[0m
[37m[1m[2023-07-17 07:31:03,978][257371] Min Reward on eval: -123.78458537627012[0m
[37m[1m[2023-07-17 07:31:03,979][257371] Mean Reward across all agents: -0.7890001218894465[0m
[37m[1m[2023-07-17 07:31:03,979][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:31:03,981][257371] mean_value=-272.78516891295243, max_value=208.76180463102207[0m
[37m[1m[2023-07-17 07:31:03,983][257371] New mean coefficients: [[-1.0283496   0.05532479  1.2910151   1.5905229  -1.5685048  -0.5143781 ]][0m
[37m[1m[2023-07-17 07:31:03,984][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:31:13,033][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 07:31:13,033][257371] FPS: 424446.22[0m
[36m[2023-07-17 07:31:13,035][257371] itr=854, itrs=2000, Progress: 42.70%[0m
[36m[2023-07-17 07:31:24,843][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 07:31:24,843][257371] FPS: 328034.54[0m
[36m[2023-07-17 07:31:29,177][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:31:29,178][257371] Reward + Measures: [[96.63558961  0.55511302  0.78320605  0.61149466  0.77339435  6.73835516]][0m
[37m[1m[2023-07-17 07:31:29,178][257371] Max Reward on eval: 96.63558961344789[0m
[37m[1m[2023-07-17 07:31:29,178][257371] Min Reward on eval: 96.63558961344789[0m
[37m[1m[2023-07-17 07:31:29,179][257371] Mean Reward across all agents: 96.63558961344789[0m
[37m[1m[2023-07-17 07:31:29,179][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:31:34,232][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:31:34,233][257371] Reward + Measures: [[ 62.55157028   0.7992       0.85010004   0.83640003   0.81930012
    7.42016602]
 [ 32.88965956   0.87109995   0.89910001   0.91390002   0.88129997
    7.58548689]
 [-18.98497223   0.69990003   0.69529992   0.69990003   0.70630002
    7.47945023]
 ...
 [ -1.25757425   0.97139996   0.97169989   0.97220004   0.97270006
    7.9757843 ]
 [-33.27135347   0.76899999   0.36849999   0.71649998   0.3497
    7.16844511]
 [424.7216816    0.87799996   0.91869992   0.0209       0.90789998
    7.781425  ]][0m
[37m[1m[2023-07-17 07:31:34,233][257371] Max Reward on eval: 696.3624424677342[0m
[37m[1m[2023-07-17 07:31:34,233][257371] Min Reward on eval: -180.04704979136585[0m
[37m[1m[2023-07-17 07:31:34,233][257371] Mean Reward across all agents: 24.91606118370703[0m
[37m[1m[2023-07-17 07:31:34,234][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:31:34,237][257371] mean_value=-189.95158302777816, max_value=369.5809272967465[0m
[37m[1m[2023-07-17 07:31:34,239][257371] New mean coefficients: [[-0.83639944 -0.4321364   1.9909959   1.672808   -2.1693707  -0.7681594 ]][0m
[37m[1m[2023-07-17 07:31:34,240][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:31:43,299][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 07:31:43,299][257371] FPS: 423960.48[0m
[36m[2023-07-17 07:31:43,301][257371] itr=855, itrs=2000, Progress: 42.75%[0m
[36m[2023-07-17 07:31:55,228][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 07:31:55,228][257371] FPS: 324682.27[0m
[36m[2023-07-17 07:31:59,568][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:31:59,568][257371] Reward + Measures: [[99.33091918  0.58027965  0.80467838  0.64428562  0.78732729  6.73030519]][0m
[37m[1m[2023-07-17 07:31:59,569][257371] Max Reward on eval: 99.33091918378098[0m
[37m[1m[2023-07-17 07:31:59,569][257371] Min Reward on eval: 99.33091918378098[0m
[37m[1m[2023-07-17 07:31:59,569][257371] Mean Reward across all agents: 99.33091918378098[0m
[37m[1m[2023-07-17 07:31:59,570][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:32:04,604][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:32:04,604][257371] Reward + Measures: [[351.40935801   0.79330003   0.96189994   0.0064       0.96999997
    7.47322035]
 [ 74.09289764   0.58020008   0.77749997   0.11729999   0.66399997
    6.45777082]
 [-12.26426692   0.52229995   0.39910001   0.4621       0.49830005
    6.24870396]
 ...
 [ 30.71911149   0.64109999   0.59840006   0.67980003   0.67380005
    7.03004932]
 [-11.23469987   0.38180003   0.43760005   0.1426       0.38910002
    6.32991314]
 [-18.35375814   0.12890001   0.16159999   0.127        0.13780001
    5.93497086]][0m
[37m[1m[2023-07-17 07:32:04,605][257371] Max Reward on eval: 466.9227774752304[0m
[37m[1m[2023-07-17 07:32:04,605][257371] Min Reward on eval: -252.37597276566083[0m
[37m[1m[2023-07-17 07:32:04,605][257371] Mean Reward across all agents: 21.933730297790063[0m
[37m[1m[2023-07-17 07:32:04,605][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:32:04,608][257371] mean_value=-226.57732457161424, max_value=216.7459078819674[0m
[37m[1m[2023-07-17 07:32:04,611][257371] New mean coefficients: [[-0.05967224  0.19547114  2.7131186   0.93130493 -2.2414932  -0.75227916]][0m
[37m[1m[2023-07-17 07:32:04,612][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:32:13,693][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 07:32:13,693][257371] FPS: 422914.91[0m
[36m[2023-07-17 07:32:13,696][257371] itr=856, itrs=2000, Progress: 42.80%[0m
[36m[2023-07-17 07:32:25,632][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 07:32:25,632][257371] FPS: 324550.73[0m
[36m[2023-07-17 07:32:29,975][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:32:29,975][257371] Reward + Measures: [[38.83968567  0.97101063  0.97753996  0.96496832  0.96720475  6.29622698]][0m
[37m[1m[2023-07-17 07:32:29,975][257371] Max Reward on eval: 38.83968567489112[0m
[37m[1m[2023-07-17 07:32:29,976][257371] Min Reward on eval: 38.83968567489112[0m
[37m[1m[2023-07-17 07:32:29,976][257371] Mean Reward across all agents: 38.83968567489112[0m
[37m[1m[2023-07-17 07:32:29,976][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:32:35,045][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:32:35,046][257371] Reward + Measures: [[229.63378677   0.36849996   0.4594       0.37030002   0.17370002
    6.91512156]
 [122.79994342   0.41069999   0.60159999   0.57170004   0.29320002
    7.28473806]
 [  0.29982398   0.1103       0.12230001   0.12630001   0.12030001
    6.14028549]
 ...
 [  5.65817009   0.5596       0.3845       0.46699998   0.49969998
    6.69688511]
 [ 68.42692737   0.52790004   0.63679999   0.53010005   0.20310001
    7.43864584]
 [ 39.65897514   0.84880012   0.88559991   0.86070007   0.85129994
    7.39190245]][0m
[37m[1m[2023-07-17 07:32:35,046][257371] Max Reward on eval: 536.753343565017[0m
[37m[1m[2023-07-17 07:32:35,046][257371] Min Reward on eval: -163.25688471437897[0m
[37m[1m[2023-07-17 07:32:35,046][257371] Mean Reward across all agents: 39.054369202870596[0m
[37m[1m[2023-07-17 07:32:35,047][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:32:35,050][257371] mean_value=-194.24683048755782, max_value=548.8854736807291[0m
[37m[1m[2023-07-17 07:32:35,053][257371] New mean coefficients: [[ 1.1488649   0.08046807  4.1742463   0.04850727 -2.5609386  -0.5383377 ]][0m
[37m[1m[2023-07-17 07:32:35,054][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:32:44,118][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 07:32:44,118][257371] FPS: 423730.61[0m
[36m[2023-07-17 07:32:44,120][257371] itr=857, itrs=2000, Progress: 42.85%[0m
[36m[2023-07-17 07:32:56,024][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 07:32:56,025][257371] FPS: 325306.53[0m
[36m[2023-07-17 07:33:00,311][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:33:00,312][257371] Reward + Measures: [[76.65545706  0.67255402  0.79688996  0.35282764  0.71277636  6.92992115]][0m
[37m[1m[2023-07-17 07:33:00,312][257371] Max Reward on eval: 76.65545705974598[0m
[37m[1m[2023-07-17 07:33:00,312][257371] Min Reward on eval: 76.65545705974598[0m
[37m[1m[2023-07-17 07:33:00,313][257371] Mean Reward across all agents: 76.65545705974598[0m
[37m[1m[2023-07-17 07:33:00,313][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:33:05,303][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:33:05,304][257371] Reward + Measures: [[ -25.14889022    0.98620003    0.98660004    0.95109999    0.95080006
     7.93273115]
 [ 166.06640935    0.66790003    0.74879998    0.07270001    0.70889997
     6.97787714]
 [  54.47187828    0.40720001    0.634         0.15110002    0.54139996
     7.12316847]
 ...
 [ -30.92882487    0.34470001    0.21529999    0.20130001    0.21510001
     7.3499527 ]
 [  14.36959228    0.1005        0.0717        0.089         0.051
     6.65969324]
 [-173.08241441    0.1917        0.23609999    0.17830001    0.14560001
     5.64304495]][0m
[37m[1m[2023-07-17 07:33:05,304][257371] Max Reward on eval: 756.4639511007815[0m
[37m[1m[2023-07-17 07:33:05,304][257371] Min Reward on eval: -173.08241441249848[0m
[37m[1m[2023-07-17 07:33:05,305][257371] Mean Reward across all agents: 28.872877807222807[0m
[37m[1m[2023-07-17 07:33:05,305][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:33:05,307][257371] mean_value=-221.3974196691819, max_value=271.45204556838127[0m
[37m[1m[2023-07-17 07:33:05,310][257371] New mean coefficients: [[ 0.20445156 -0.4585889   3.660577    0.0157539  -1.8901689  -0.18929058]][0m
[37m[1m[2023-07-17 07:33:05,311][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:33:14,292][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 07:33:14,292][257371] FPS: 427648.15[0m
[36m[2023-07-17 07:33:14,294][257371] itr=858, itrs=2000, Progress: 42.90%[0m
[36m[2023-07-17 07:33:26,108][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 07:33:26,108][257371] FPS: 327934.15[0m
[36m[2023-07-17 07:33:30,419][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:33:30,424][257371] Reward + Measures: [[45.18300836  0.9527033   0.97581267  0.95449793  0.96170098  6.27168989]][0m
[37m[1m[2023-07-17 07:33:30,425][257371] Max Reward on eval: 45.183008360245466[0m
[37m[1m[2023-07-17 07:33:30,425][257371] Min Reward on eval: 45.183008360245466[0m
[37m[1m[2023-07-17 07:33:30,425][257371] Mean Reward across all agents: 45.183008360245466[0m
[37m[1m[2023-07-17 07:33:30,425][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:33:35,485][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:33:35,491][257371] Reward + Measures: [[ 87.827873     0.70029998   0.82550001   0.1294       0.78420001
    7.41518116]
 [  2.94769642   0.67180002   0.68970001   0.60519999   0.56830001
    7.06177282]
 [ 45.8445882    0.23150001   0.1885       0.22389999   0.19389999
    6.3697381 ]
 ...
 [ 63.68292193   0.74050003   0.71820003   0.69760001   0.6796
    7.14536524]
 [ 58.19269322   0.31740001   0.22649999   0.32020003   0.2217
    6.66708231]
 [195.31703066   0.71190006   0.89150012   0.19560002   0.80390006
    6.5384078 ]][0m
[37m[1m[2023-07-17 07:33:35,491][257371] Max Reward on eval: 338.365688315779[0m
[37m[1m[2023-07-17 07:33:35,492][257371] Min Reward on eval: -124.99316554339603[0m
[37m[1m[2023-07-17 07:33:35,492][257371] Mean Reward across all agents: 46.46888604373971[0m
[37m[1m[2023-07-17 07:33:35,492][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:33:35,496][257371] mean_value=-176.50006495484087, max_value=342.32989402693437[0m
[37m[1m[2023-07-17 07:33:35,499][257371] New mean coefficients: [[-0.33573747  0.5883511   4.8070703  -0.00633809 -1.5256636  -0.14213529]][0m
[37m[1m[2023-07-17 07:33:35,500][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:33:44,555][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 07:33:44,555][257371] FPS: 424165.96[0m
[36m[2023-07-17 07:33:44,557][257371] itr=859, itrs=2000, Progress: 42.95%[0m
[36m[2023-07-17 07:33:56,249][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 07:33:56,249][257371] FPS: 331260.98[0m
[36m[2023-07-17 07:34:00,502][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:34:00,502][257371] Reward + Measures: [[47.62935027  0.956635    0.97808635  0.95988137  0.96433967  6.25562906]][0m
[37m[1m[2023-07-17 07:34:00,502][257371] Max Reward on eval: 47.629350272390866[0m
[37m[1m[2023-07-17 07:34:00,503][257371] Min Reward on eval: 47.629350272390866[0m
[37m[1m[2023-07-17 07:34:00,503][257371] Mean Reward across all agents: 47.629350272390866[0m
[37m[1m[2023-07-17 07:34:00,503][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:34:05,718][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:34:05,725][257371] Reward + Measures: [[ -5.30900182   0.73079997   0.74770004   0.122        0.73089999
    7.65891886]
 [ 33.01220994   0.75590008   0.77850002   0.09370001   0.81260008
    7.77555227]
 [ 80.401464     0.43330002   0.6006       0.47860003   0.55549997
    7.31201935]
 ...
 [281.47293078   0.12049999   0.57720006   0.4127       0.44530001
    7.40722752]
 [213.03707713   0.21519999   0.52859998   0.46599999   0.50459999
    7.15000486]
 [617.63014698   0.13590001   0.82160008   0.68510002   0.78810006
    7.14968586]][0m
[37m[1m[2023-07-17 07:34:05,725][257371] Max Reward on eval: 726.3464241022244[0m
[37m[1m[2023-07-17 07:34:05,725][257371] Min Reward on eval: -142.18582910113037[0m
[37m[1m[2023-07-17 07:34:05,725][257371] Mean Reward across all agents: 67.90706510333932[0m
[37m[1m[2023-07-17 07:34:05,726][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:34:05,730][257371] mean_value=-231.34003598578605, max_value=238.3137677377809[0m
[37m[1m[2023-07-17 07:34:05,732][257371] New mean coefficients: [[-0.41991073  2.0693455   4.904846    0.13980815 -1.4056604  -0.51521266]][0m
[37m[1m[2023-07-17 07:34:05,733][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:34:14,679][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 07:34:14,679][257371] FPS: 429353.31[0m
[36m[2023-07-17 07:34:14,681][257371] itr=860, itrs=2000, Progress: 43.00%[0m
[37m[1m[2023-07-17 07:37:46,525][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000840[0m
[36m[2023-07-17 07:37:58,863][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 07:37:58,863][257371] FPS: 325859.39[0m
[36m[2023-07-17 07:38:03,105][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:38:03,106][257371] Reward + Measures: [[51.99570992  0.97135597  0.9821924   0.9719494   0.97314328  6.2019906 ]][0m
[37m[1m[2023-07-17 07:38:03,106][257371] Max Reward on eval: 51.99570991757129[0m
[37m[1m[2023-07-17 07:38:03,106][257371] Min Reward on eval: 51.99570991757129[0m
[37m[1m[2023-07-17 07:38:03,107][257371] Mean Reward across all agents: 51.99570991757129[0m
[37m[1m[2023-07-17 07:38:03,107][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:38:08,074][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:38:08,075][257371] Reward + Measures: [[-282.01809459    0.611         0.58390003    0.63959998    0.1222
     7.22503519]
 [ -26.09432616    0.18320002    0.20019999    0.17170002    0.2368
     6.11896372]
 [   8.76289194    0.88190001    0.91050005    0.91680002    0.88510001
     7.18507719]
 ...
 [  40.37272984    0.39879999    0.34630004    0.2631        0.3371
     6.62989044]
 [ -20.03695345    0.67609996    0.73100001    0.6924001     0.67320001
     7.08879328]
 [  27.35727063    0.79149997    0.79809999    0.80190003    0.79160005
     6.99922037]][0m
[37m[1m[2023-07-17 07:38:08,075][257371] Max Reward on eval: 420.2289892510511[0m
[37m[1m[2023-07-17 07:38:08,075][257371] Min Reward on eval: -299.9910415621474[0m
[37m[1m[2023-07-17 07:38:08,075][257371] Mean Reward across all agents: 14.788056810830515[0m
[37m[1m[2023-07-17 07:38:08,075][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:38:08,078][257371] mean_value=-236.37302615961724, max_value=154.45457585740698[0m
[37m[1m[2023-07-17 07:38:08,080][257371] New mean coefficients: [[-0.10438445  0.93797255  4.1055083  -0.11355194 -0.5604728  -0.19172451]][0m
[37m[1m[2023-07-17 07:38:08,081][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:38:17,067][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 07:38:17,067][257371] FPS: 427424.92[0m
[36m[2023-07-17 07:38:17,069][257371] itr=861, itrs=2000, Progress: 43.05%[0m
[36m[2023-07-17 07:38:28,722][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 07:38:28,722][257371] FPS: 332410.08[0m
[36m[2023-07-17 07:38:32,949][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:38:32,950][257371] Reward + Measures: [[15.74992925  0.69432634  0.73265296  0.78289062  0.70668638  6.1420598 ]][0m
[37m[1m[2023-07-17 07:38:32,950][257371] Max Reward on eval: 15.749929251836331[0m
[37m[1m[2023-07-17 07:38:32,950][257371] Min Reward on eval: 15.749929251836331[0m
[37m[1m[2023-07-17 07:38:32,950][257371] Mean Reward across all agents: 15.749929251836331[0m
[37m[1m[2023-07-17 07:38:32,951][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:38:37,956][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:38:37,957][257371] Reward + Measures: [[20.91556627  0.90599996  0.88940001  0.80550003  0.86590004  6.1470809 ]
 [-0.15196483  0.20939998  0.27929997  0.24660002  0.13950001  6.41864872]
 [ 7.20292584  0.421       0.50650001  0.47049999  0.49159995  6.05667591]
 ...
 [99.69259811  0.59400004  0.79619998  0.67570001  0.76679999  7.16688013]
 [44.04435554  0.36919999  0.49519998  0.44839999  0.24529998  6.39429951]
 [27.40157569  0.72849995  0.78979999  0.75719994  0.77330005  6.56017828]][0m
[37m[1m[2023-07-17 07:38:37,957][257371] Max Reward on eval: 439.5018119590357[0m
[37m[1m[2023-07-17 07:38:37,957][257371] Min Reward on eval: -76.11681736577302[0m
[37m[1m[2023-07-17 07:38:37,957][257371] Mean Reward across all agents: 50.36229226748312[0m
[37m[1m[2023-07-17 07:38:37,958][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:38:37,960][257371] mean_value=-195.04097521422008, max_value=172.28821633052866[0m
[37m[1m[2023-07-17 07:38:37,963][257371] New mean coefficients: [[-0.01779754  1.1047344   4.0269594  -0.89626944 -0.38499403  0.16164714]][0m
[37m[1m[2023-07-17 07:38:37,964][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:38:46,979][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 07:38:46,979][257371] FPS: 426034.62[0m
[36m[2023-07-17 07:38:46,981][257371] itr=862, itrs=2000, Progress: 43.10%[0m
[36m[2023-07-17 07:38:58,814][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 07:38:58,814][257371] FPS: 327273.52[0m
[36m[2023-07-17 07:39:03,093][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:39:03,094][257371] Reward + Measures: [[39.25354878  0.73259735  0.78668702  0.811149    0.75354034  6.09560394]][0m
[37m[1m[2023-07-17 07:39:03,094][257371] Max Reward on eval: 39.25354877933606[0m
[37m[1m[2023-07-17 07:39:03,094][257371] Min Reward on eval: 39.25354877933606[0m
[37m[1m[2023-07-17 07:39:03,095][257371] Mean Reward across all agents: 39.25354877933606[0m
[37m[1m[2023-07-17 07:39:03,095][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:39:08,102][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:39:08,102][257371] Reward + Measures: [[116.44043445   0.47209999   0.71470004   0.57410002   0.70880008
    6.41896009]
 [ 22.3610357    0.63260001   0.54390001   0.52970004   0.54960006
    7.02663422]
 [ 10.5927701    0.30440003   0.4296       0.32650003   0.34410003
    6.60062933]
 ...
 [-20.27286484   0.133        0.1207       0.12080001   0.1311
    5.81191969]
 [171.59532795   0.30000001   0.4571       0.50340003   0.5255
    7.54857969]
 [ 33.74791966   0.31729999   0.35390002   0.32879999   0.38980001
    5.99718332]][0m
[37m[1m[2023-07-17 07:39:08,103][257371] Max Reward on eval: 658.0494220125489[0m
[37m[1m[2023-07-17 07:39:08,103][257371] Min Reward on eval: -120.4293947102502[0m
[37m[1m[2023-07-17 07:39:08,103][257371] Mean Reward across all agents: 36.11484710300087[0m
[37m[1m[2023-07-17 07:39:08,103][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:39:08,107][257371] mean_value=-199.49410028443913, max_value=294.30466213851827[0m
[37m[1m[2023-07-17 07:39:08,110][257371] New mean coefficients: [[-2.171112   -0.10886121  3.5508175  -0.35082543 -0.7341827   0.04131989]][0m
[37m[1m[2023-07-17 07:39:08,110][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:39:17,217][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 07:39:17,217][257371] FPS: 421773.36[0m
[36m[2023-07-17 07:39:17,219][257371] itr=863, itrs=2000, Progress: 43.15%[0m
[36m[2023-07-17 07:39:28,817][257371] train() took 11.50 seconds to complete[0m
[36m[2023-07-17 07:39:28,817][257371] FPS: 333907.93[0m
[36m[2023-07-17 07:39:33,081][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:39:33,082][257371] Reward + Measures: [[63.0848453   0.76520467  0.84425169  0.83074093  0.80400497  6.04428911]][0m
[37m[1m[2023-07-17 07:39:33,082][257371] Max Reward on eval: 63.08484530209102[0m
[37m[1m[2023-07-17 07:39:33,082][257371] Min Reward on eval: 63.08484530209102[0m
[37m[1m[2023-07-17 07:39:33,082][257371] Mean Reward across all agents: 63.08484530209102[0m
[37m[1m[2023-07-17 07:39:33,083][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:39:38,297][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:39:38,297][257371] Reward + Measures: [[61.03275645  0.78610003  0.86439991  0.81470007  0.82000011  6.11631346]
 [25.26626526  0.56230003  0.57550001  0.53870004  0.59070003  6.08868361]
 [35.16421128  0.79879999  0.84720004  0.84870005  0.81809998  6.33485603]
 ...
 [53.21105019  0.74620003  0.80879992  0.80830002  0.79279995  6.23929977]
 [41.57648455  0.83850002  0.89850008  0.87360001  0.85799998  6.53963041]
 [21.61800386  0.84899998  0.90669996  0.87629998  0.85599995  6.3731389 ]][0m
[37m[1m[2023-07-17 07:39:38,298][257371] Max Reward on eval: 94.42837524992646[0m
[37m[1m[2023-07-17 07:39:38,298][257371] Min Reward on eval: -56.549774487689135[0m
[37m[1m[2023-07-17 07:39:38,298][257371] Mean Reward across all agents: 50.94113301596008[0m
[37m[1m[2023-07-17 07:39:38,298][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:39:38,300][257371] mean_value=-196.38336123955742, max_value=-28.856404758253568[0m
[36m[2023-07-17 07:39:38,303][257371] XNES is restarting with a new solution whose measures are [0.14600001 0.75759995 0.25040004 0.8167001  2.88161898] and objective is 217.50195883419366[0m
[36m[2023-07-17 07:39:38,304][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 07:39:38,306][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 07:39:38,307][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:39:47,344][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 07:39:47,345][257371] FPS: 424972.32[0m
[36m[2023-07-17 07:39:47,347][257371] itr=864, itrs=2000, Progress: 43.20%[0m
[36m[2023-07-17 07:39:59,159][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 07:39:59,159][257371] FPS: 327874.07[0m
[36m[2023-07-17 07:40:03,508][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:40:03,508][257371] Reward + Measures: [[254.77708477   0.177852     0.68866861   0.21912301   0.74986666
    2.76163411]][0m
[37m[1m[2023-07-17 07:40:03,508][257371] Max Reward on eval: 254.77708476830645[0m
[37m[1m[2023-07-17 07:40:03,509][257371] Min Reward on eval: 254.77708476830645[0m
[37m[1m[2023-07-17 07:40:03,509][257371] Mean Reward across all agents: 254.77708476830645[0m
[37m[1m[2023-07-17 07:40:03,509][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:40:08,494][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:40:08,495][257371] Reward + Measures: [[ -1.99356308   0.24560001   0.5704       0.30890003   0.70780003
    3.58847427]
 [191.89281344   0.33180004   0.26100001   0.333        0.2112
    2.9577651 ]
 [258.94139862   0.40530005   0.41680002   0.42069998   0.38189998
    3.10191488]
 ...
 [104.79421746   0.35859999   0.458        0.30369997   0.42840001
    2.79669499]
 [109.26213437   0.2723       0.59299999   0.21890001   0.57209998
    3.0956583 ]
 [ 93.84892806   0.32410002   0.30060002   0.36230001   0.24920002
    3.1031599 ]][0m
[37m[1m[2023-07-17 07:40:08,495][257371] Max Reward on eval: 407.1870460316539[0m
[37m[1m[2023-07-17 07:40:08,495][257371] Min Reward on eval: -273.48709295806475[0m
[37m[1m[2023-07-17 07:40:08,496][257371] Mean Reward across all agents: 51.3696342406894[0m
[37m[1m[2023-07-17 07:40:08,496][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:40:08,498][257371] mean_value=-847.2018582455821, max_value=198.41497407289847[0m
[37m[1m[2023-07-17 07:40:08,501][257371] New mean coefficients: [[ 1.061617   -0.51010156 -1.3764877  -0.6318276  -1.5509385  -0.3917923 ]][0m
[37m[1m[2023-07-17 07:40:08,502][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:40:17,422][257371] train() took 8.92 seconds to complete[0m
[36m[2023-07-17 07:40:17,423][257371] FPS: 430549.88[0m
[36m[2023-07-17 07:40:17,425][257371] itr=865, itrs=2000, Progress: 43.25%[0m
[36m[2023-07-17 07:40:29,172][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 07:40:29,172][257371] FPS: 329730.57[0m
[36m[2023-07-17 07:40:33,429][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:40:33,430][257371] Reward + Measures: [[253.39718093   0.19086333   0.65518063   0.227001     0.71328932
    2.77710342]][0m
[37m[1m[2023-07-17 07:40:33,430][257371] Max Reward on eval: 253.3971809324928[0m
[37m[1m[2023-07-17 07:40:33,430][257371] Min Reward on eval: 253.3971809324928[0m
[37m[1m[2023-07-17 07:40:33,431][257371] Mean Reward across all agents: 253.3971809324928[0m
[37m[1m[2023-07-17 07:40:33,431][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:40:38,438][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:40:38,439][257371] Reward + Measures: [[ 126.0070717     0.2139        0.384         0.3055        0.32730001
     4.53767347]
 [ -87.12581305    0.30090001    0.50670004    0.30179998    0.53380007
     4.06530285]
 [-129.80977833    0.22070001    0.31030002    0.28749999    0.38870001
     4.4707756 ]
 ...
 [ -40.65302905    0.27219999    0.32520002    0.36530003    0.42810002
     3.95501256]
 [  73.28386014    0.13170001    0.41510001    0.32479998    0.37940001
     5.1785388 ]
 [ -46.05671671    0.10030001    0.78079998    0.44319996    0.815
     5.60750151]][0m
[37m[1m[2023-07-17 07:40:38,439][257371] Max Reward on eval: 339.6065807260573[0m
[37m[1m[2023-07-17 07:40:38,440][257371] Min Reward on eval: -184.28344390802084[0m
[37m[1m[2023-07-17 07:40:38,440][257371] Mean Reward across all agents: 27.31586373426849[0m
[37m[1m[2023-07-17 07:40:38,440][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:40:38,442][257371] mean_value=-552.5778884309849, max_value=175.33388964634975[0m
[37m[1m[2023-07-17 07:40:38,445][257371] New mean coefficients: [[ 1.0263449   0.85128164 -0.49212682  0.5236136  -0.681977   -0.6162214 ]][0m
[37m[1m[2023-07-17 07:40:38,446][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:40:47,439][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 07:40:47,439][257371] FPS: 427084.00[0m
[36m[2023-07-17 07:40:47,441][257371] itr=866, itrs=2000, Progress: 43.30%[0m
[36m[2023-07-17 07:40:59,173][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 07:40:59,173][257371] FPS: 330143.56[0m
[36m[2023-07-17 07:41:03,488][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:41:03,493][257371] Reward + Measures: [[240.30188215   0.20500167   0.62951535   0.23295064   0.68623739
    2.76100326]][0m
[37m[1m[2023-07-17 07:41:03,494][257371] Max Reward on eval: 240.30188214523463[0m
[37m[1m[2023-07-17 07:41:03,494][257371] Min Reward on eval: 240.30188214523463[0m
[37m[1m[2023-07-17 07:41:03,494][257371] Mean Reward across all agents: 240.30188214523463[0m
[37m[1m[2023-07-17 07:41:03,494][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:41:08,470][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:41:08,471][257371] Reward + Measures: [[  22.78310587    0.41760001    0.48930001    0.47390005    0.50340003
     3.66492891]
 [  86.86655808    0.1229        0.17800002    0.1621        0.20670001
     4.89201975]
 [-131.17408015    0.21049999    0.56080002    0.30590001    0.51330006
     4.01372433]
 ...
 [-196.23166176    0.39820001    0.43630001    0.36590001    0.39579999
     4.29455471]
 [  59.48402184    0.5625        0.56739998    0.58200002    0.54120004
     3.28858066]
 [ -90.38462211    0.30710003    0.49720001    0.4646        0.42130002
     3.5459969 ]][0m
[37m[1m[2023-07-17 07:41:08,471][257371] Max Reward on eval: 281.82458783183245[0m
[37m[1m[2023-07-17 07:41:08,471][257371] Min Reward on eval: -211.23690105229616[0m
[37m[1m[2023-07-17 07:41:08,472][257371] Mean Reward across all agents: 28.079492541699164[0m
[37m[1m[2023-07-17 07:41:08,472][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:41:08,474][257371] mean_value=-341.7952331635196, max_value=160.2948082167045[0m
[37m[1m[2023-07-17 07:41:08,477][257371] New mean coefficients: [[ 0.91337806 -0.43302214 -0.09708408  0.776701   -0.12867326 -0.43378192]][0m
[37m[1m[2023-07-17 07:41:08,478][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:41:17,473][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 07:41:17,473][257371] FPS: 426957.95[0m
[36m[2023-07-17 07:41:17,476][257371] itr=867, itrs=2000, Progress: 43.35%[0m
[36m[2023-07-17 07:41:29,210][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 07:41:29,210][257371] FPS: 330091.00[0m
[36m[2023-07-17 07:41:33,489][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:41:33,490][257371] Reward + Measures: [[248.75570222   0.19819067   0.63573736   0.23374666   0.69255739
    2.74583006]][0m
[37m[1m[2023-07-17 07:41:33,490][257371] Max Reward on eval: 248.75570222265105[0m
[37m[1m[2023-07-17 07:41:33,490][257371] Min Reward on eval: 248.75570222265105[0m
[37m[1m[2023-07-17 07:41:33,490][257371] Mean Reward across all agents: 248.75570222265105[0m
[37m[1m[2023-07-17 07:41:33,491][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:41:38,465][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:41:38,471][257371] Reward + Measures: [[ 94.01163004   0.21500002   0.25509998   0.22290002   0.21680002
    3.4727025 ]
 [-34.61662296   0.68040001   0.3256       0.70370001   0.24759999
    5.03218269]
 [ 41.68497844   0.6365       0.1895       0.59580004   0.58420002
    4.6513772 ]
 ...
 [ -2.42169764   0.46689996   0.4217       0.40400001   0.50780004
    2.79014945]
 [ 25.69157725   0.31230003   0.33559999   0.34760001   0.30239999
    3.78949046]
 [173.25143437   0.70340008   0.39050001   0.7712       0.34059998
    4.78656721]][0m
[37m[1m[2023-07-17 07:41:38,471][257371] Max Reward on eval: 362.76368309110404[0m
[37m[1m[2023-07-17 07:41:38,472][257371] Min Reward on eval: -142.8338194454089[0m
[37m[1m[2023-07-17 07:41:38,472][257371] Mean Reward across all agents: 51.05768232505847[0m
[37m[1m[2023-07-17 07:41:38,472][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:41:38,475][257371] mean_value=-682.9108489719147, max_value=353.3928687166938[0m
[37m[1m[2023-07-17 07:41:38,478][257371] New mean coefficients: [[ 1.2223674  -0.8610835   0.44740942 -0.384246    0.28583613 -0.54244274]][0m
[37m[1m[2023-07-17 07:41:38,479][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:41:47,444][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 07:41:47,445][257371] FPS: 428389.23[0m
[36m[2023-07-17 07:41:47,447][257371] itr=868, itrs=2000, Progress: 43.40%[0m
[36m[2023-07-17 07:41:59,400][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-17 07:41:59,401][257371] FPS: 324054.64[0m
[36m[2023-07-17 07:42:03,735][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:42:03,735][257371] Reward + Measures: [[273.78051215   0.19368333   0.64160764   0.23021866   0.69872427
    2.71989202]][0m
[37m[1m[2023-07-17 07:42:03,735][257371] Max Reward on eval: 273.78051215200213[0m
[37m[1m[2023-07-17 07:42:03,736][257371] Min Reward on eval: 273.78051215200213[0m
[37m[1m[2023-07-17 07:42:03,736][257371] Mean Reward across all agents: 273.78051215200213[0m
[37m[1m[2023-07-17 07:42:03,736][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:42:08,703][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:42:08,709][257371] Reward + Measures: [[-100.73283144    0.6329        0.2726        0.65360004    0.68790001
     3.70345545]
 [  -2.21289272    0.32150003    0.46940002    0.1635        0.5589
     2.77538991]
 [  26.57411276    0.56120002    0.34600002    0.51640004    0.47779998
     3.96480107]
 ...
 [  96.76671556    0.37549999    0.42270002    0.36490002    0.50590008
     3.23309445]
 [  57.34970941    0.45629999    0.3687        0.43689999    0.55120003
     3.74129844]
 [   9.53438252    0.33309999    0.414         0.2572        0.44829997
     2.91196299]][0m
[37m[1m[2023-07-17 07:42:08,709][257371] Max Reward on eval: 281.37480779998003[0m
[37m[1m[2023-07-17 07:42:08,709][257371] Min Reward on eval: -549.9433317116112[0m
[37m[1m[2023-07-17 07:42:08,710][257371] Mean Reward across all agents: 33.03404249275049[0m
[37m[1m[2023-07-17 07:42:08,710][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:42:08,712][257371] mean_value=-547.8991903488615, max_value=124.74099110450538[0m
[37m[1m[2023-07-17 07:42:08,715][257371] New mean coefficients: [[ 1.1980296  -1.147536   -0.02561784  1.1014626   0.09184611 -0.04052901]][0m
[37m[1m[2023-07-17 07:42:08,716][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:42:17,664][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 07:42:17,664][257371] FPS: 429209.58[0m
[36m[2023-07-17 07:42:17,667][257371] itr=869, itrs=2000, Progress: 43.45%[0m
[36m[2023-07-17 07:42:29,515][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 07:42:29,515][257371] FPS: 326990.21[0m
[36m[2023-07-17 07:42:33,879][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:42:33,879][257371] Reward + Measures: [[311.2682369    0.19151033   0.63971233   0.23350333   0.69371963
    2.70589304]][0m
[37m[1m[2023-07-17 07:42:33,879][257371] Max Reward on eval: 311.2682369034183[0m
[37m[1m[2023-07-17 07:42:33,880][257371] Min Reward on eval: 311.2682369034183[0m
[37m[1m[2023-07-17 07:42:33,880][257371] Mean Reward across all agents: 311.2682369034183[0m
[37m[1m[2023-07-17 07:42:33,880][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:42:39,171][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:42:39,172][257371] Reward + Measures: [[-30.73602002   0.0775       0.51209998   0.31220001   0.55290002
    3.5766983 ]
 [225.9064431    0.42609999   0.46289998   0.4743       0.46160004
    2.90632129]
 [ 95.1143964    0.52750003   0.3019       0.4772       0.38579997
    4.19691706]
 ...
 [ 46.31872907   0.39979997   0.34959999   0.48330003   0.35209998
    3.50113535]
 [ -9.46116353   0.19230001   0.57679999   0.24030001   0.65609998
    3.5999372 ]
 [-74.90367865   0.22090001   0.40850002   0.2132       0.45510003
    4.54454994]][0m
[37m[1m[2023-07-17 07:42:39,172][257371] Max Reward on eval: 371.1371383793652[0m
[37m[1m[2023-07-17 07:42:39,172][257371] Min Reward on eval: -138.94522349415348[0m
[37m[1m[2023-07-17 07:42:39,172][257371] Mean Reward across all agents: 59.21877826017815[0m
[37m[1m[2023-07-17 07:42:39,173][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:42:39,176][257371] mean_value=-276.9090172903693, max_value=197.3018581022839[0m
[37m[1m[2023-07-17 07:42:39,179][257371] New mean coefficients: [[ 0.6602921  -0.99003667 -0.38099325  1.2478875  -0.14342093 -0.7087598 ]][0m
[37m[1m[2023-07-17 07:42:39,180][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:42:48,331][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-17 07:42:48,331][257371] FPS: 419688.03[0m
[36m[2023-07-17 07:42:48,334][257371] itr=870, itrs=2000, Progress: 43.50%[0m
[37m[1m[2023-07-17 07:46:00,524][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000850[0m
[36m[2023-07-17 07:46:12,972][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 07:46:12,973][257371] FPS: 328273.49[0m
[36m[2023-07-17 07:46:17,119][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:46:17,119][257371] Reward + Measures: [[304.91819249   0.19345532   0.63036066   0.24014166   0.68554497
    2.69361091]][0m
[37m[1m[2023-07-17 07:46:17,119][257371] Max Reward on eval: 304.9181924872999[0m
[37m[1m[2023-07-17 07:46:17,119][257371] Min Reward on eval: 304.9181924872999[0m
[37m[1m[2023-07-17 07:46:17,120][257371] Mean Reward across all agents: 304.9181924872999[0m
[37m[1m[2023-07-17 07:46:17,120][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:46:22,018][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:46:22,018][257371] Reward + Measures: [[ 40.27502486   0.46250001   0.43760005   0.4549       0.3946
    3.87932897]
 [ 60.88952237   0.22479999   0.31590003   0.2244       0.39569998
    2.96350098]
 [-52.19983085   0.29089999   0.32150003   0.41300002   0.414
    4.17819452]
 ...
 [121.11944915   0.20200001   0.5679       0.2638       0.60300004
    3.55607986]
 [-85.76629702   0.26990002   0.3247       0.33950001   0.3626
    4.31469917]
 [  5.11819383   0.37260005   0.31819999   0.31999999   0.32750002
    3.13038278]][0m
[37m[1m[2023-07-17 07:46:22,018][257371] Max Reward on eval: 294.9379491686821[0m
[37m[1m[2023-07-17 07:46:22,019][257371] Min Reward on eval: -115.0157492721919[0m
[37m[1m[2023-07-17 07:46:22,019][257371] Mean Reward across all agents: 67.61386322848021[0m
[37m[1m[2023-07-17 07:46:22,019][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:46:22,021][257371] mean_value=-725.8660242857686, max_value=193.39842500469092[0m
[37m[1m[2023-07-17 07:46:22,024][257371] New mean coefficients: [[ 0.94390905  0.27977186 -0.34720606  0.6436961   0.15459922 -0.78908134]][0m
[37m[1m[2023-07-17 07:46:22,025][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:46:31,004][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 07:46:31,004][257371] FPS: 427754.95[0m
[36m[2023-07-17 07:46:31,006][257371] itr=871, itrs=2000, Progress: 43.55%[0m
[36m[2023-07-17 07:46:42,834][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 07:46:42,834][257371] FPS: 327434.78[0m
[36m[2023-07-17 07:46:47,057][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:46:47,063][257371] Reward + Measures: [[297.61920627   0.19669867   0.62879103   0.24442667   0.68654966
    2.64848447]][0m
[37m[1m[2023-07-17 07:46:47,063][257371] Max Reward on eval: 297.61920627142604[0m
[37m[1m[2023-07-17 07:46:47,063][257371] Min Reward on eval: 297.61920627142604[0m
[37m[1m[2023-07-17 07:46:47,064][257371] Mean Reward across all agents: 297.61920627142604[0m
[37m[1m[2023-07-17 07:46:47,064][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:46:52,095][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:46:52,101][257371] Reward + Measures: [[  49.19208189    0.24689999    0.56999999    0.30029997    0.67189997
     3.50621796]
 [  52.52630383    0.33860001    0.51859999    0.32590002    0.58109999
     3.00014615]
 [ -17.13291126    0.27309999    0.55109996    0.31900001    0.58249998
     3.53588605]
 ...
 [-144.29829981    0.28570002    0.47669998    0.29450002    0.50159997
     3.95354462]
 [ -24.79101872    0.29370001    0.49200001    0.23810001    0.55100006
     4.15167904]
 [ 115.08783422    0.24969999    0.29450002    0.22620001    0.31450003
     4.69329023]][0m
[37m[1m[2023-07-17 07:46:52,101][257371] Max Reward on eval: 331.61451299488544[0m
[37m[1m[2023-07-17 07:46:52,102][257371] Min Reward on eval: -266.11420691031964[0m
[37m[1m[2023-07-17 07:46:52,102][257371] Mean Reward across all agents: 28.04840496965467[0m
[37m[1m[2023-07-17 07:46:52,102][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:46:52,105][257371] mean_value=-409.8709885377957, max_value=143.74701241284166[0m
[37m[1m[2023-07-17 07:46:52,107][257371] New mean coefficients: [[-0.3151405   0.23660524  0.5189881   0.53776956  0.18221118 -0.809842  ]][0m
[37m[1m[2023-07-17 07:46:52,108][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:47:01,336][257371] train() took 9.23 seconds to complete[0m
[36m[2023-07-17 07:47:01,336][257371] FPS: 416211.92[0m
[36m[2023-07-17 07:47:01,338][257371] itr=872, itrs=2000, Progress: 43.60%[0m
[36m[2023-07-17 07:47:13,167][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 07:47:13,168][257371] FPS: 327436.71[0m
[36m[2023-07-17 07:47:17,423][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:47:17,423][257371] Reward + Measures: [[310.50603528   0.19845633   0.63229465   0.23871668   0.6928333
    2.60409904]][0m
[37m[1m[2023-07-17 07:47:17,424][257371] Max Reward on eval: 310.5060352818485[0m
[37m[1m[2023-07-17 07:47:17,424][257371] Min Reward on eval: 310.5060352818485[0m
[37m[1m[2023-07-17 07:47:17,424][257371] Mean Reward across all agents: 310.5060352818485[0m
[37m[1m[2023-07-17 07:47:17,424][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:47:22,373][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:47:22,374][257371] Reward + Measures: [[157.68079947   0.28150001   0.55940002   0.45590001   0.41279998
    3.21874857]
 [204.85049225   0.17830001   0.63920003   0.206        0.68200004
    3.11517954]
 [ 56.90895363   0.30580002   0.47240001   0.2617       0.4614
    3.33197069]
 ...
 [210.29531714   0.2599       0.4736       0.31020004   0.44670001
    4.3936162 ]
 [ 79.06035901   0.26539999   0.43770003   0.19         0.42729998
    4.05600071]
 [-32.12135088   0.33269998   0.3206       0.33500001   0.2947
    4.52729654]][0m
[37m[1m[2023-07-17 07:47:22,374][257371] Max Reward on eval: 344.297232632339[0m
[37m[1m[2023-07-17 07:47:22,374][257371] Min Reward on eval: -185.10293969400226[0m
[37m[1m[2023-07-17 07:47:22,374][257371] Mean Reward across all agents: 80.66365428620863[0m
[37m[1m[2023-07-17 07:47:22,375][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:47:22,378][257371] mean_value=-403.4821460627631, max_value=225.47927919305772[0m
[37m[1m[2023-07-17 07:47:22,381][257371] New mean coefficients: [[-0.35823038 -0.460782    0.6849783   0.46206862 -0.3659224  -0.35406125]][0m
[37m[1m[2023-07-17 07:47:22,382][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:47:31,339][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 07:47:31,339][257371] FPS: 428804.56[0m
[36m[2023-07-17 07:47:31,342][257371] itr=873, itrs=2000, Progress: 43.65%[0m
[36m[2023-07-17 07:47:43,105][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 07:47:43,105][257371] FPS: 329354.80[0m
[36m[2023-07-17 07:47:47,327][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:47:47,327][257371] Reward + Measures: [[299.03269263   0.19529769   0.64526802   0.23673333   0.70477402
    2.5673902 ]][0m
[37m[1m[2023-07-17 07:47:47,327][257371] Max Reward on eval: 299.03269263421436[0m
[37m[1m[2023-07-17 07:47:47,328][257371] Min Reward on eval: 299.03269263421436[0m
[37m[1m[2023-07-17 07:47:47,328][257371] Mean Reward across all agents: 299.03269263421436[0m
[37m[1m[2023-07-17 07:47:47,328][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:47:52,610][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:47:52,616][257371] Reward + Measures: [[ 164.9412992     0.20050001    0.6037001     0.2172        0.63230002
     2.93933487]
 [ 112.21965288    0.26989999    0.32260001    0.2333        0.36039999
     3.32310677]
 [   0.86248051    0.1613        0.71169996    0.22070001    0.73759997
     4.59141541]
 ...
 [  53.34011912    0.5176        0.1717        0.57419997    0.50029999
     5.94572783]
 [ -64.71379706    0.34100002    0.3698        0.27500001    0.33759999
     3.99873328]
 [-286.60579166    0.78000003    0.1239        0.79310006    0.68360001
     5.40571213]][0m
[37m[1m[2023-07-17 07:47:52,616][257371] Max Reward on eval: 322.89900017529726[0m
[37m[1m[2023-07-17 07:47:52,617][257371] Min Reward on eval: -370.70869361944494[0m
[37m[1m[2023-07-17 07:47:52,617][257371] Mean Reward across all agents: 68.24840238163205[0m
[37m[1m[2023-07-17 07:47:52,617][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:47:52,619][257371] mean_value=-534.0194791572375, max_value=121.37856438645827[0m
[37m[1m[2023-07-17 07:47:52,622][257371] New mean coefficients: [[-0.4092478  -0.9956729   0.6664392  -0.16543752 -0.3209939  -0.3683158 ]][0m
[37m[1m[2023-07-17 07:47:52,623][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:48:01,628][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 07:48:01,628][257371] FPS: 426518.31[0m
[36m[2023-07-17 07:48:01,631][257371] itr=874, itrs=2000, Progress: 43.70%[0m
[36m[2023-07-17 07:48:13,468][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 07:48:13,469][257371] FPS: 327193.74[0m
[36m[2023-07-17 07:48:17,693][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:48:17,694][257371] Reward + Measures: [[293.26186599   0.18925266   0.66050267   0.232499     0.71894777
    2.50841308]][0m
[37m[1m[2023-07-17 07:48:17,694][257371] Max Reward on eval: 293.26186598961465[0m
[37m[1m[2023-07-17 07:48:17,694][257371] Min Reward on eval: 293.26186598961465[0m
[37m[1m[2023-07-17 07:48:17,695][257371] Mean Reward across all agents: 293.26186598961465[0m
[37m[1m[2023-07-17 07:48:17,695][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:48:22,608][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:48:22,609][257371] Reward + Measures: [[  97.27165032    0.21750002    0.45280001    0.2775        0.47380003
     3.93112922]
 [-106.61371587    0.25280002    0.4296        0.35479999    0.41100001
     4.02374506]
 [  52.97422751    0.23190001    0.49189997    0.27950001    0.505
     3.48981547]
 ...
 [ 130.32836445    0.1882        0.56129998    0.24999999    0.61220002
     3.52326846]
 [  84.86588158    0.30290002    0.50019997    0.26950002    0.53360003
     2.80413175]
 [  47.70112625    0.26290002    0.4224        0.26420003    0.41260004
     2.86630535]][0m
[37m[1m[2023-07-17 07:48:22,609][257371] Max Reward on eval: 413.2409637823701[0m
[37m[1m[2023-07-17 07:48:22,609][257371] Min Reward on eval: -191.24501465559007[0m
[37m[1m[2023-07-17 07:48:22,609][257371] Mean Reward across all agents: 89.19749918035522[0m
[37m[1m[2023-07-17 07:48:22,610][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:48:22,612][257371] mean_value=-355.029798731252, max_value=156.93730035145896[0m
[37m[1m[2023-07-17 07:48:22,615][257371] New mean coefficients: [[-1.1916256  -1.6751436   0.06822139 -0.96458524 -0.79663086 -0.45191532]][0m
[37m[1m[2023-07-17 07:48:22,616][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:48:31,591][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 07:48:31,592][257371] FPS: 427923.27[0m
[36m[2023-07-17 07:48:31,594][257371] itr=875, itrs=2000, Progress: 43.75%[0m
[36m[2023-07-17 07:48:43,250][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 07:48:43,250][257371] FPS: 332278.08[0m
[36m[2023-07-17 07:48:47,544][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:48:47,550][257371] Reward + Measures: [[293.45325525   0.18461032   0.66390032   0.22985233   0.71941072
    2.45438409]][0m
[37m[1m[2023-07-17 07:48:47,550][257371] Max Reward on eval: 293.4532552507371[0m
[37m[1m[2023-07-17 07:48:47,550][257371] Min Reward on eval: 293.4532552507371[0m
[37m[1m[2023-07-17 07:48:47,551][257371] Mean Reward across all agents: 293.4532552507371[0m
[37m[1m[2023-07-17 07:48:47,551][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:48:52,440][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:48:52,441][257371] Reward + Measures: [[ 18.03145496   0.1292       0.81479996   0.57609999   0.86940002
    4.89951372]
 [-98.22597141   0.1071       0.7087       0.48300001   0.71360004
    4.98077154]
 [ 98.10801529   0.26890001   0.45989999   0.30570003   0.51739997
    2.91443801]
 ...
 [ 50.70101299   0.21110001   0.51300001   0.3946       0.54080003
    4.41468477]
 [299.36924077   0.27560002   0.55980003   0.33100003   0.67089999
    3.37772989]
 [  6.58537383   0.1832       0.59530002   0.34039998   0.65150005
    5.09117699]][0m
[37m[1m[2023-07-17 07:48:52,441][257371] Max Reward on eval: 327.85339261442425[0m
[37m[1m[2023-07-17 07:48:52,441][257371] Min Reward on eval: -306.3556042050477[0m
[37m[1m[2023-07-17 07:48:52,441][257371] Mean Reward across all agents: 43.98787023702353[0m
[37m[1m[2023-07-17 07:48:52,442][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:48:52,444][257371] mean_value=-451.8446857273538, max_value=269.3462902365697[0m
[37m[1m[2023-07-17 07:48:52,447][257371] New mean coefficients: [[-1.3744159  -1.251734   -0.02533813 -0.21079439 -0.5687064  -0.43575   ]][0m
[37m[1m[2023-07-17 07:48:52,448][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:49:01,479][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 07:49:01,479][257371] FPS: 425269.92[0m
[36m[2023-07-17 07:49:01,481][257371] itr=876, itrs=2000, Progress: 43.80%[0m
[36m[2023-07-17 07:49:13,548][257371] train() took 11.97 seconds to complete[0m
[36m[2023-07-17 07:49:13,548][257371] FPS: 320918.67[0m
[36m[2023-07-17 07:49:17,888][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:49:17,889][257371] Reward + Measures: [[278.33659686   0.18114266   0.66968727   0.22497265   0.72193295
    2.42911625]][0m
[37m[1m[2023-07-17 07:49:17,889][257371] Max Reward on eval: 278.3365968576075[0m
[37m[1m[2023-07-17 07:49:17,889][257371] Min Reward on eval: 278.3365968576075[0m
[37m[1m[2023-07-17 07:49:17,889][257371] Mean Reward across all agents: 278.3365968576075[0m
[37m[1m[2023-07-17 07:49:17,890][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:49:22,974][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:49:22,978][257371] Reward + Measures: [[103.22207904   0.51569998   0.28190002   0.52579999   0.38339999
    4.22434568]
 [104.68552252   0.37390003   0.34269997   0.42000005   0.42030001
    2.96791387]
 [ 59.53777255   0.3897       0.45170003   0.45190001   0.62989998
    3.84580874]
 ...
 [-20.07983256   0.50999999   0.39739999   0.46960002   0.49090004
    2.93943834]
 [110.21278089   0.18550001   0.6552       0.31420001   0.68370003
    2.93437076]
 [144.26610814   0.19510001   0.50239998   0.30399999   0.4382
    3.19886708]][0m
[37m[1m[2023-07-17 07:49:22,978][257371] Max Reward on eval: 452.49121475964785[0m
[37m[1m[2023-07-17 07:49:22,979][257371] Min Reward on eval: -146.42254686348025[0m
[37m[1m[2023-07-17 07:49:22,979][257371] Mean Reward across all agents: 97.57191746385423[0m
[37m[1m[2023-07-17 07:49:22,979][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:49:22,982][257371] mean_value=-347.1435788314371, max_value=198.1686054959383[0m
[37m[1m[2023-07-17 07:49:22,984][257371] New mean coefficients: [[-1.1564715  -1.5032184   0.82571465 -1.1255745   0.3089118  -0.3393434 ]][0m
[37m[1m[2023-07-17 07:49:22,985][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:49:32,055][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 07:49:32,055][257371] FPS: 423488.58[0m
[36m[2023-07-17 07:49:32,057][257371] itr=877, itrs=2000, Progress: 43.85%[0m
[36m[2023-07-17 07:49:43,705][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 07:49:43,706][257371] FPS: 332493.94[0m
[36m[2023-07-17 07:49:47,926][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:49:47,926][257371] Reward + Measures: [[279.26483621   0.16990334   0.68825531   0.21375498   0.74321598
    2.38331008]][0m
[37m[1m[2023-07-17 07:49:47,926][257371] Max Reward on eval: 279.264836205009[0m
[37m[1m[2023-07-17 07:49:47,927][257371] Min Reward on eval: 279.264836205009[0m
[37m[1m[2023-07-17 07:49:47,927][257371] Mean Reward across all agents: 279.264836205009[0m
[37m[1m[2023-07-17 07:49:47,927][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:49:52,956][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:49:52,957][257371] Reward + Measures: [[-18.99724281   0.38509998   0.4718       0.43790004   0.49390003
    3.18346834]
 [ 79.29133936   0.56689996   0.5855       0.64210004   0.70350003
    3.35276151]
 [162.20664309   0.17470001   0.78189999   0.23190002   0.77159995
    3.10172963]
 ...
 [-83.63852249   0.17489998   0.6455       0.23280001   0.63980001
    3.79595065]
 [-29.71629335   0.36420003   0.36230001   0.33220002   0.39750001
    3.30931664]
 [161.17007921   0.17549999   0.65060002   0.21820001   0.6559
    3.0486424 ]][0m
[37m[1m[2023-07-17 07:49:52,957][257371] Max Reward on eval: 287.3308581933379[0m
[37m[1m[2023-07-17 07:49:52,957][257371] Min Reward on eval: -172.96151928566397[0m
[37m[1m[2023-07-17 07:49:52,957][257371] Mean Reward across all agents: 48.0999030087951[0m
[37m[1m[2023-07-17 07:49:52,958][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:49:52,960][257371] mean_value=-481.5360432704026, max_value=132.89790905455737[0m
[37m[1m[2023-07-17 07:49:52,963][257371] New mean coefficients: [[-0.23347688 -1.7136891   0.80438006  0.10402727  0.6477884  -0.5087419 ]][0m
[37m[1m[2023-07-17 07:49:52,964][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:50:01,950][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 07:50:01,950][257371] FPS: 427402.55[0m
[36m[2023-07-17 07:50:01,952][257371] itr=878, itrs=2000, Progress: 43.90%[0m
[36m[2023-07-17 07:50:13,803][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 07:50:13,803][257371] FPS: 326818.67[0m
[36m[2023-07-17 07:50:18,146][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:50:18,147][257371] Reward + Measures: [[287.68743737   0.150472     0.72126138   0.20722133   0.78055263
    2.34931254]][0m
[37m[1m[2023-07-17 07:50:18,147][257371] Max Reward on eval: 287.68743736606604[0m
[37m[1m[2023-07-17 07:50:18,147][257371] Min Reward on eval: 287.68743736606604[0m
[37m[1m[2023-07-17 07:50:18,147][257371] Mean Reward across all agents: 287.68743736606604[0m
[37m[1m[2023-07-17 07:50:18,148][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:50:23,215][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:50:23,268][257371] Reward + Measures: [[  3.81078962   0.2404       0.53410006   0.2177       0.51820004
    3.28330779]
 [307.75027455   0.24070001   0.67790002   0.2615       0.67939997
    2.82058644]
 [-50.04472668   0.2333       0.64910001   0.21010001   0.68709999
    3.86406493]
 ...
 [-51.99149011   0.2369       0.53870004   0.34239998   0.62300009
    3.70579147]
 [242.61158799   0.21970001   0.54730004   0.23140001   0.58540004
    3.33985758]
 [ 81.74063972   0.20120001   0.7123       0.31430003   0.77090007
    3.89258361]][0m
[37m[1m[2023-07-17 07:50:23,268][257371] Max Reward on eval: 344.728193295002[0m
[37m[1m[2023-07-17 07:50:23,268][257371] Min Reward on eval: -185.6229577332735[0m
[37m[1m[2023-07-17 07:50:23,268][257371] Mean Reward across all agents: 67.69893185819123[0m
[37m[1m[2023-07-17 07:50:23,269][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:50:23,271][257371] mean_value=-223.1725324218011, max_value=134.5054772501484[0m
[37m[1m[2023-07-17 07:50:23,274][257371] New mean coefficients: [[ 0.24067813 -1.0451038   1.0368369   0.10294476  0.36708343 -0.6865797 ]][0m
[37m[1m[2023-07-17 07:50:23,275][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:50:32,421][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 07:50:32,421][257371] FPS: 419932.72[0m
[36m[2023-07-17 07:50:32,423][257371] itr=879, itrs=2000, Progress: 43.95%[0m
[36m[2023-07-17 07:50:44,302][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 07:50:44,303][257371] FPS: 326053.06[0m
[36m[2023-07-17 07:50:48,665][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:50:48,666][257371] Reward + Measures: [[288.55001658   0.138908     0.74389631   0.20387198   0.80603695
    2.27815223]][0m
[37m[1m[2023-07-17 07:50:48,666][257371] Max Reward on eval: 288.550016579501[0m
[37m[1m[2023-07-17 07:50:48,666][257371] Min Reward on eval: 288.550016579501[0m
[37m[1m[2023-07-17 07:50:48,667][257371] Mean Reward across all agents: 288.550016579501[0m
[37m[1m[2023-07-17 07:50:48,667][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:50:53,951][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:50:53,956][257371] Reward + Measures: [[  15.5432311     0.34870002    0.45030004    0.1556        0.4664
     3.48083425]
 [-142.07704895    0.1987        0.69480008    0.2024        0.74869996
     3.29308629]
 [ 123.41287774    0.1196        0.77429992    0.25050002    0.77220005
     2.50725245]
 ...
 [ 223.3189459     0.27770001    0.58520001    0.2086        0.68470001
     2.74724174]
 [  83.54022698    0.23889999    0.47620001    0.1322        0.4763
     3.19082904]
 [ 139.8173946     0.21350001    0.52179998    0.21560001    0.44150001
     3.8971982 ]][0m
[37m[1m[2023-07-17 07:50:53,957][257371] Max Reward on eval: 319.10202787686137[0m
[37m[1m[2023-07-17 07:50:53,957][257371] Min Reward on eval: -752.5497226746753[0m
[37m[1m[2023-07-17 07:50:53,957][257371] Mean Reward across all agents: 56.03175024757214[0m
[37m[1m[2023-07-17 07:50:53,957][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:50:53,960][257371] mean_value=-287.7862867651655, max_value=244.3112150499901[0m
[37m[1m[2023-07-17 07:50:53,963][257371] New mean coefficients: [[ 0.4191438  -0.17707402  1.2447366  -0.14517881 -0.4486571  -1.24085   ]][0m
[37m[1m[2023-07-17 07:50:53,964][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:51:03,023][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 07:51:03,024][257371] FPS: 423943.05[0m
[36m[2023-07-17 07:51:03,026][257371] itr=880, itrs=2000, Progress: 44.00%[0m
[37m[1m[2023-07-17 07:54:27,642][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000860[0m
[36m[2023-07-17 07:54:40,220][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-17 07:54:40,220][257371] FPS: 323825.33[0m
[36m[2023-07-17 07:54:44,506][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:54:44,506][257371] Reward + Measures: [[293.41043738   0.13719967   0.75122935   0.20241301   0.80906999
    2.2097733 ]][0m
[37m[1m[2023-07-17 07:54:44,506][257371] Max Reward on eval: 293.41043737736754[0m
[37m[1m[2023-07-17 07:54:44,507][257371] Min Reward on eval: 293.41043737736754[0m
[37m[1m[2023-07-17 07:54:44,507][257371] Mean Reward across all agents: 293.41043737736754[0m
[37m[1m[2023-07-17 07:54:44,507][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:54:49,413][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:54:49,413][257371] Reward + Measures: [[157.39237549   0.2538       0.54930001   0.24679999   0.60620004
    3.07310724]
 [342.77358437   0.2033       0.56769997   0.42789999   0.58470005
    2.90729499]
 [107.08368404   0.1517       0.7191       0.16580002   0.73920006
    4.01368856]
 ...
 [ 18.99201374   0.29819998   0.65009999   0.16110002   0.64770001
    3.41157842]
 [ 45.64006803   0.1163       0.73340005   0.1575       0.73900002
    4.53684473]
 [-35.88841499   0.20610002   0.59030002   0.18139999   0.62290001
    3.53090978]][0m
[37m[1m[2023-07-17 07:54:49,413][257371] Max Reward on eval: 373.5972080282867[0m
[37m[1m[2023-07-17 07:54:49,414][257371] Min Reward on eval: -153.17324822796508[0m
[37m[1m[2023-07-17 07:54:49,414][257371] Mean Reward across all agents: 65.2454284735788[0m
[37m[1m[2023-07-17 07:54:49,414][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:54:49,417][257371] mean_value=-304.10118452820603, max_value=119.2078584687124[0m
[37m[1m[2023-07-17 07:54:49,420][257371] New mean coefficients: [[ 1.4632282   0.06232087  0.17323887  0.0545031  -0.9402393  -0.88054883]][0m
[37m[1m[2023-07-17 07:54:49,421][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:54:58,354][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 07:54:58,354][257371] FPS: 429966.44[0m
[36m[2023-07-17 07:54:58,356][257371] itr=881, itrs=2000, Progress: 44.05%[0m
[36m[2023-07-17 07:55:10,019][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 07:55:10,019][257371] FPS: 332157.35[0m
[36m[2023-07-17 07:55:14,248][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:55:14,248][257371] Reward + Measures: [[302.48051431   0.14083867   0.74315006   0.20281164   0.80048698
    2.17150068]][0m
[37m[1m[2023-07-17 07:55:14,248][257371] Max Reward on eval: 302.4805143128203[0m
[37m[1m[2023-07-17 07:55:14,249][257371] Min Reward on eval: 302.4805143128203[0m
[37m[1m[2023-07-17 07:55:14,249][257371] Mean Reward across all agents: 302.4805143128203[0m
[37m[1m[2023-07-17 07:55:14,249][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:55:19,224][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:55:19,225][257371] Reward + Measures: [[122.56920573   0.1761       0.66119999   0.28060001   0.68830007
    2.6651094 ]
 [ 17.36541778   0.34460002   0.4883       0.3802       0.54900002
    2.93620801]
 [157.0063124    0.0895       0.74360001   0.3141       0.74000007
    2.49162388]
 ...
 [-21.36944716   0.26680002   0.53490001   0.32020003   0.50959998
    2.3917408 ]
 [-36.04631939   0.64149994   0.76160002   0.63889998   0.73379999
    4.51956177]
 [-54.14841405   0.32030004   0.3642       0.33899999   0.45769998
    3.93840265]][0m
[37m[1m[2023-07-17 07:55:19,225][257371] Max Reward on eval: 426.03002355396745[0m
[37m[1m[2023-07-17 07:55:19,225][257371] Min Reward on eval: -620.069847089611[0m
[37m[1m[2023-07-17 07:55:19,226][257371] Mean Reward across all agents: 65.23124445018517[0m
[37m[1m[2023-07-17 07:55:19,226][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:55:19,228][257371] mean_value=-263.8915691859087, max_value=148.84001214294136[0m
[37m[1m[2023-07-17 07:55:19,231][257371] New mean coefficients: [[ 2.0060737  -0.22903909 -0.05758201  0.33721197 -0.83595425 -0.4184061 ]][0m
[37m[1m[2023-07-17 07:55:19,232][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:55:28,280][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 07:55:28,281][257371] FPS: 424449.29[0m
[36m[2023-07-17 07:55:28,283][257371] itr=882, itrs=2000, Progress: 44.10%[0m
[36m[2023-07-17 07:55:40,193][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 07:55:40,193][257371] FPS: 325179.51[0m
[36m[2023-07-17 07:55:44,543][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:55:44,544][257371] Reward + Measures: [[327.31211059   0.14597899   0.72853732   0.20460701   0.79082668
    2.19614363]][0m
[37m[1m[2023-07-17 07:55:44,544][257371] Max Reward on eval: 327.31211058647386[0m
[37m[1m[2023-07-17 07:55:44,544][257371] Min Reward on eval: 327.31211058647386[0m
[37m[1m[2023-07-17 07:55:44,545][257371] Mean Reward across all agents: 327.31211058647386[0m
[37m[1m[2023-07-17 07:55:44,545][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:55:49,577][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:55:49,582][257371] Reward + Measures: [[110.76754617   0.1551       0.68630004   0.20370002   0.72290003
    3.09126234]
 [181.28605604   0.13680001   0.71040004   0.1822       0.73640007
    3.53918505]
 [ 69.15216397   0.2155       0.62330002   0.32820001   0.66429996
    3.09920144]
 ...
 [-48.61039242   0.52099997   0.40159997   0.56670004   0.56430006
    3.09980321]
 [-28.55216906   0.0862       0.79619998   0.45240003   0.81190008
    4.63866425]
 [284.03995894   0.17449999   0.66540003   0.1823       0.71870005
    2.37295985]][0m
[37m[1m[2023-07-17 07:55:49,583][257371] Max Reward on eval: 360.4639796875417[0m
[37m[1m[2023-07-17 07:55:49,583][257371] Min Reward on eval: -260.5623970806599[0m
[37m[1m[2023-07-17 07:55:49,583][257371] Mean Reward across all agents: 100.17764746660133[0m
[37m[1m[2023-07-17 07:55:49,584][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:55:49,587][257371] mean_value=-125.9681551517253, max_value=296.803080888338[0m
[37m[1m[2023-07-17 07:55:49,589][257371] New mean coefficients: [[ 2.199155   -0.39239156  0.3698365  -0.49045616 -0.5467444  -0.28707516]][0m
[37m[1m[2023-07-17 07:55:49,590][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:55:58,615][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 07:55:58,615][257371] FPS: 425576.04[0m
[36m[2023-07-17 07:55:58,617][257371] itr=883, itrs=2000, Progress: 44.15%[0m
[36m[2023-07-17 07:56:10,307][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 07:56:10,307][257371] FPS: 331419.87[0m
[36m[2023-07-17 07:56:14,560][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:56:14,561][257371] Reward + Measures: [[339.90306602   0.14683732   0.71816462   0.20409967   0.78202164
    2.20260453]][0m
[37m[1m[2023-07-17 07:56:14,561][257371] Max Reward on eval: 339.9030660248228[0m
[37m[1m[2023-07-17 07:56:14,561][257371] Min Reward on eval: 339.9030660248228[0m
[37m[1m[2023-07-17 07:56:14,561][257371] Mean Reward across all agents: 339.9030660248228[0m
[37m[1m[2023-07-17 07:56:14,562][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:56:19,776][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:56:19,783][257371] Reward + Measures: [[ 51.12149284   0.3549       0.44800001   0.47510004   0.491
    2.41865969]
 [ 40.73881982   0.26320001   0.62840003   0.2362       0.6771
    3.0201633 ]
 [-53.54487212   0.21510001   0.34209999   0.29609999   0.38890001
    3.18023753]
 ...
 [ 43.40648124   0.368        0.55899996   0.4224       0.5751
    2.49682188]
 [189.74149523   0.19770001   0.49429998   0.19719999   0.54010004
    3.15938759]
 [-81.29171887   0.47170001   0.33899999   0.46520001   0.60570002
    3.98994756]][0m
[37m[1m[2023-07-17 07:56:19,783][257371] Max Reward on eval: 371.5512714356184[0m
[37m[1m[2023-07-17 07:56:19,784][257371] Min Reward on eval: -198.72662548683584[0m
[37m[1m[2023-07-17 07:56:19,784][257371] Mean Reward across all agents: 72.13237108080529[0m
[37m[1m[2023-07-17 07:56:19,784][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:56:19,788][257371] mean_value=-280.24283198137925, max_value=478.7364357165841[0m
[37m[1m[2023-07-17 07:56:19,791][257371] New mean coefficients: [[ 1.2757514   0.08241376  0.80105895 -0.37859353 -0.40158886 -0.8807642 ]][0m
[37m[1m[2023-07-17 07:56:19,792][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:56:28,787][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 07:56:28,787][257371] FPS: 426965.24[0m
[36m[2023-07-17 07:56:28,790][257371] itr=884, itrs=2000, Progress: 44.20%[0m
[36m[2023-07-17 07:56:40,629][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 07:56:40,629][257371] FPS: 327154.19[0m
[36m[2023-07-17 07:56:44,879][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:56:44,879][257371] Reward + Measures: [[367.04569373   0.14765267   0.72133905   0.200065     0.78912663
    2.17268515]][0m
[37m[1m[2023-07-17 07:56:44,880][257371] Max Reward on eval: 367.04569372528215[0m
[37m[1m[2023-07-17 07:56:44,880][257371] Min Reward on eval: 367.04569372528215[0m
[37m[1m[2023-07-17 07:56:44,880][257371] Mean Reward across all agents: 367.04569372528215[0m
[37m[1m[2023-07-17 07:56:44,880][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:56:49,862][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:56:49,863][257371] Reward + Measures: [[ 140.33255392    0.21610001    0.55839998    0.2256        0.64210004
     3.0261476 ]
 [  51.55127458    0.62529999    0.34900001    0.61760008    0.35330001
     4.31636572]
 [-154.54903479    0.3055        0.52610004    0.45320001    0.75720006
     4.59525394]
 ...
 [  79.6158515     0.52750003    0.3527        0.42430001    0.35140002
     2.6088264 ]
 [ 103.15403101    0.64719993    0.46410003    0.625         0.57590002
     4.58545637]
 [ 109.79563692    0.21089999    0.68960005    0.28649998    0.72060007
     3.90170908]][0m
[37m[1m[2023-07-17 07:56:49,863][257371] Max Reward on eval: 456.10343170166016[0m
[37m[1m[2023-07-17 07:56:49,863][257371] Min Reward on eval: -580.3530566170812[0m
[37m[1m[2023-07-17 07:56:49,863][257371] Mean Reward across all agents: 57.080092655571185[0m
[37m[1m[2023-07-17 07:56:49,864][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:56:49,867][257371] mean_value=-338.3438210827151, max_value=158.23813764864332[0m
[37m[1m[2023-07-17 07:56:49,869][257371] New mean coefficients: [[ 1.5298624  -0.35999388  0.18701726  0.3783879  -0.6239786  -1.0758705 ]][0m
[37m[1m[2023-07-17 07:56:49,870][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:56:58,882][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 07:56:58,882][257371] FPS: 426186.66[0m
[36m[2023-07-17 07:56:58,885][257371] itr=885, itrs=2000, Progress: 44.25%[0m
[36m[2023-07-17 07:57:10,592][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 07:57:10,593][257371] FPS: 330923.86[0m
[36m[2023-07-17 07:57:14,789][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:57:14,789][257371] Reward + Measures: [[372.2278295    0.14528833   0.72191763   0.19405733   0.796785
    2.16570592]][0m
[37m[1m[2023-07-17 07:57:14,789][257371] Max Reward on eval: 372.2278295022798[0m
[37m[1m[2023-07-17 07:57:14,790][257371] Min Reward on eval: 372.2278295022798[0m
[37m[1m[2023-07-17 07:57:14,790][257371] Mean Reward across all agents: 372.2278295022798[0m
[37m[1m[2023-07-17 07:57:14,790][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:57:19,718][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:57:19,719][257371] Reward + Measures: [[226.24662984   0.18979999   0.75160003   0.2818       0.78820002
    2.52437425]
 [166.15839959   0.1175       0.75089997   0.1795       0.86730003
    2.31373811]
 [-15.47338248   0.211        0.64109999   0.28780001   0.71550006
    2.95715261]
 ...
 [441.62800218   0.16790001   0.73260003   0.20420001   0.81300002
    2.11264491]
 [450.26908494   0.1522       0.68920004   0.17349999   0.76359999
    2.04231   ]
 [134.95363787   0.26290002   0.51950002   0.29350001   0.5837
    3.02015805]][0m
[37m[1m[2023-07-17 07:57:19,719][257371] Max Reward on eval: 490.12061311006545[0m
[37m[1m[2023-07-17 07:57:19,719][257371] Min Reward on eval: -43.62112198807299[0m
[37m[1m[2023-07-17 07:57:19,719][257371] Mean Reward across all agents: 187.73270336650432[0m
[37m[1m[2023-07-17 07:57:19,720][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:57:19,723][257371] mean_value=-104.3546599408853, max_value=406.587321832729[0m
[37m[1m[2023-07-17 07:57:19,725][257371] New mean coefficients: [[-0.6179185  -0.28784513  0.5559728  -0.02799281 -0.7288052  -0.4894607 ]][0m
[37m[1m[2023-07-17 07:57:19,726][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:57:28,745][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 07:57:28,746][257371] FPS: 425836.40[0m
[36m[2023-07-17 07:57:28,748][257371] itr=886, itrs=2000, Progress: 44.30%[0m
[36m[2023-07-17 07:57:40,441][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 07:57:40,441][257371] FPS: 331243.69[0m
[36m[2023-07-17 07:57:44,723][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:57:44,724][257371] Reward + Measures: [[358.48918948   0.139588     0.73702508   0.19251168   0.80993766
    2.10958791]][0m
[37m[1m[2023-07-17 07:57:44,724][257371] Max Reward on eval: 358.48918948059344[0m
[37m[1m[2023-07-17 07:57:44,724][257371] Min Reward on eval: 358.48918948059344[0m
[37m[1m[2023-07-17 07:57:44,725][257371] Mean Reward across all agents: 358.48918948059344[0m
[37m[1m[2023-07-17 07:57:44,725][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:57:49,689][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:57:49,690][257371] Reward + Measures: [[-50.0555692    0.43790004   0.27959999   0.52280003   0.49020001
    5.04774046]
 [107.71216848   0.29960003   0.4192       0.3001       0.4262
    3.56027651]
 [  6.45937661   0.048        0.96079999   0.43509999   0.97799999
    5.60230255]
 ...
 [ 38.75358623   0.44730002   0.34270003   0.51780003   0.59439999
    4.07764578]
 [228.41426515   0.1401       0.71149999   0.29800001   0.79560006
    2.9375875 ]
 [125.13277934   0.25320002   0.67790002   0.32300001   0.66600007
    2.43961143]][0m
[37m[1m[2023-07-17 07:57:49,690][257371] Max Reward on eval: 412.21705913618206[0m
[37m[1m[2023-07-17 07:57:49,690][257371] Min Reward on eval: -124.47739839795977[0m
[37m[1m[2023-07-17 07:57:49,690][257371] Mean Reward across all agents: 104.16902223037741[0m
[37m[1m[2023-07-17 07:57:49,691][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:57:49,693][257371] mean_value=-253.08968667093131, max_value=233.41524776885765[0m
[37m[1m[2023-07-17 07:57:49,696][257371] New mean coefficients: [[ 0.34123814 -0.07587217  0.38048735 -0.35296264 -0.6332618   0.1089775 ]][0m
[37m[1m[2023-07-17 07:57:49,697][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:57:58,747][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 07:57:58,747][257371] FPS: 424397.09[0m
[36m[2023-07-17 07:57:58,749][257371] itr=887, itrs=2000, Progress: 44.35%[0m
[36m[2023-07-17 07:58:10,460][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 07:58:10,460][257371] FPS: 330791.58[0m
[36m[2023-07-17 07:58:14,828][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:58:14,829][257371] Reward + Measures: [[370.48082079   0.14322068   0.73033696   0.18965499   0.80213296
    2.11921525]][0m
[37m[1m[2023-07-17 07:58:14,829][257371] Max Reward on eval: 370.4808207946245[0m
[37m[1m[2023-07-17 07:58:14,829][257371] Min Reward on eval: 370.4808207946245[0m
[37m[1m[2023-07-17 07:58:14,830][257371] Mean Reward across all agents: 370.4808207946245[0m
[37m[1m[2023-07-17 07:58:14,830][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:58:19,864][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:58:19,869][257371] Reward + Measures: [[ 191.39949511    0.27120003    0.6401        0.15320002    0.75849998
     2.32808685]
 [  96.93419717    0.27090001    0.52279997    0.33770004    0.58390003
     2.48608589]
 [ 193.27602007    0.19069998    0.6232        0.2362        0.67309999
     2.51449513]
 ...
 [-142.97036515    0.33899999    0.5345        0.26619998    0.56330001
     3.17119122]
 [ -19.63337901    0.19560002    0.48020002    0.24750002    0.52329999
     3.22429919]
 [ 249.7761545     0.20899999    0.64670002    0.25050002    0.71550006
     2.14214873]][0m
[37m[1m[2023-07-17 07:58:19,870][257371] Max Reward on eval: 404.2197475641966[0m
[37m[1m[2023-07-17 07:58:19,870][257371] Min Reward on eval: -142.9703651547432[0m
[37m[1m[2023-07-17 07:58:19,870][257371] Mean Reward across all agents: 110.6991805399324[0m
[37m[1m[2023-07-17 07:58:19,870][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:58:19,873][257371] mean_value=-288.5028713914166, max_value=246.55100393639106[0m
[37m[1m[2023-07-17 07:58:19,876][257371] New mean coefficients: [[-0.21477836 -0.02072702  0.00180608 -1.2447273  -0.5294621  -0.04156159]][0m
[37m[1m[2023-07-17 07:58:19,877][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:58:28,860][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 07:58:28,861][257371] FPS: 427538.15[0m
[36m[2023-07-17 07:58:28,863][257371] itr=888, itrs=2000, Progress: 44.40%[0m
[36m[2023-07-17 07:58:40,578][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 07:58:40,578][257371] FPS: 330644.38[0m
[36m[2023-07-17 07:58:44,912][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:58:44,913][257371] Reward + Measures: [[357.12035122   0.145842     0.73087734   0.18252131   0.80371404
    2.12651992]][0m
[37m[1m[2023-07-17 07:58:44,913][257371] Max Reward on eval: 357.12035121508825[0m
[37m[1m[2023-07-17 07:58:44,913][257371] Min Reward on eval: 357.12035121508825[0m
[37m[1m[2023-07-17 07:58:44,914][257371] Mean Reward across all agents: 357.12035121508825[0m
[37m[1m[2023-07-17 07:58:44,914][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:58:49,974][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:58:49,976][257371] Reward + Measures: [[234.5255394    0.15100001   0.73089999   0.1671       0.74949998
    3.14427805]
 [-37.59198172   0.1337       0.6613       0.24459998   0.76440001
    2.92989802]
 [ 35.21523849   0.30200002   0.51410002   0.3838       0.60320002
    3.53262329]
 ...
 [189.76450061   0.2447       0.40960002   0.30249998   0.43860003
    3.04654193]
 [ 22.11598581   0.15000001   0.76400006   0.34610003   0.8143
    4.13021994]
 [ 45.73751041   0.33720002   0.42700002   0.52179998   0.62930006
    3.13947606]][0m
[37m[1m[2023-07-17 07:58:49,976][257371] Max Reward on eval: 481.7036704964936[0m
[37m[1m[2023-07-17 07:58:49,976][257371] Min Reward on eval: -129.08893371224403[0m
[37m[1m[2023-07-17 07:58:49,976][257371] Mean Reward across all agents: 97.14653415103129[0m
[37m[1m[2023-07-17 07:58:49,977][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:58:49,979][257371] mean_value=-195.21621120844057, max_value=224.52572974797667[0m
[37m[1m[2023-07-17 07:58:49,982][257371] New mean coefficients: [[-0.9455676  -0.34828073 -0.21796419 -0.9227694  -0.6337661  -0.9836292 ]][0m
[37m[1m[2023-07-17 07:58:49,983][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:58:59,066][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 07:58:59,066][257371] FPS: 422830.02[0m
[36m[2023-07-17 07:58:59,068][257371] itr=889, itrs=2000, Progress: 44.45%[0m
[36m[2023-07-17 07:59:10,967][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 07:59:10,967][257371] FPS: 325580.26[0m
[36m[2023-07-17 07:59:15,351][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:59:15,352][257371] Reward + Measures: [[344.53212668   0.145538     0.73906523   0.18150532   0.80381268
    2.0628612 ]][0m
[37m[1m[2023-07-17 07:59:15,352][257371] Max Reward on eval: 344.532126684422[0m
[37m[1m[2023-07-17 07:59:15,352][257371] Min Reward on eval: 344.532126684422[0m
[37m[1m[2023-07-17 07:59:15,352][257371] Mean Reward across all agents: 344.532126684422[0m
[37m[1m[2023-07-17 07:59:15,352][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:59:20,558][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 07:59:20,559][257371] Reward + Measures: [[392.22274778   0.1389       0.71450007   0.19230001   0.75620002
    2.09228539]
 [245.87406276   0.16940001   0.66659999   0.22379999   0.70970005
    2.39349246]
 [313.37426756   0.17290001   0.66759998   0.22049999   0.71320003
    2.12616467]
 ...
 [392.86140252   0.13859999   0.73290008   0.18090001   0.78420007
    2.11208558]
 [210.32569888   0.34750003   0.59190005   0.29190001   0.57490003
    2.11208415]
 [274.76922418   0.21370001   0.65150005   0.21270001   0.71420002
    2.24631763]][0m
[37m[1m[2023-07-17 07:59:20,559][257371] Max Reward on eval: 492.37257385998964[0m
[37m[1m[2023-07-17 07:59:20,559][257371] Min Reward on eval: 16.82868166361004[0m
[37m[1m[2023-07-17 07:59:20,559][257371] Mean Reward across all agents: 233.16801058748496[0m
[37m[1m[2023-07-17 07:59:20,560][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 07:59:20,562][257371] mean_value=-124.33651065554896, max_value=205.54085075850122[0m
[37m[1m[2023-07-17 07:59:20,565][257371] New mean coefficients: [[-0.8745074   0.28503996 -0.7983918  -0.8255654  -0.57095    -0.08023947]][0m
[37m[1m[2023-07-17 07:59:20,566][257371] Moving the mean solution point...[0m
[36m[2023-07-17 07:59:29,602][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 07:59:29,602][257371] FPS: 425031.31[0m
[36m[2023-07-17 07:59:29,605][257371] itr=890, itrs=2000, Progress: 44.50%[0m
[37m[1m[2023-07-17 08:02:50,511][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000870[0m
[36m[2023-07-17 08:03:02,577][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 08:03:02,577][257371] FPS: 330989.91[0m
[36m[2023-07-17 08:03:06,916][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:03:06,922][257371] Reward + Measures: [[309.22467942   0.15877934   0.72797704   0.18773267   0.77863002
    2.03909349]][0m
[37m[1m[2023-07-17 08:03:06,922][257371] Max Reward on eval: 309.2246794210652[0m
[37m[1m[2023-07-17 08:03:06,923][257371] Min Reward on eval: 309.2246794210652[0m
[37m[1m[2023-07-17 08:03:06,923][257371] Mean Reward across all agents: 309.2246794210652[0m
[37m[1m[2023-07-17 08:03:06,923][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:03:11,875][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:03:11,876][257371] Reward + Measures: [[113.19263506   0.1829       0.6408       0.21290003   0.73769999
    3.03945136]
 [ 34.80212336   0.2421       0.53149998   0.2323       0.56120002
    3.85298991]
 [109.6023742    0.39750001   0.47310001   0.36849999   0.55990005
    4.48550272]
 ...
 [ 59.75593586   0.33039999   0.52509999   0.32090002   0.5521
    4.02067518]
 [ 26.07002592   0.29790002   0.63479996   0.34470001   0.72170001
    4.16756535]
 [321.62896727   0.20539999   0.61560005   0.19160001   0.70990002
    2.71446085]][0m
[37m[1m[2023-07-17 08:03:11,876][257371] Max Reward on eval: 349.2888412170112[0m
[37m[1m[2023-07-17 08:03:11,876][257371] Min Reward on eval: -93.92393057551234[0m
[37m[1m[2023-07-17 08:03:11,876][257371] Mean Reward across all agents: 111.27882000393323[0m
[37m[1m[2023-07-17 08:03:11,877][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:03:11,878][257371] mean_value=-155.25097115071554, max_value=133.87626759711316[0m
[37m[1m[2023-07-17 08:03:11,881][257371] New mean coefficients: [[-0.611366   -0.04194427 -1.1382155  -0.7349077  -0.5187193  -0.22694093]][0m
[37m[1m[2023-07-17 08:03:11,882][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:03:20,900][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 08:03:20,901][257371] FPS: 425866.10[0m
[36m[2023-07-17 08:03:20,903][257371] itr=891, itrs=2000, Progress: 44.55%[0m
[36m[2023-07-17 08:03:32,768][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 08:03:32,768][257371] FPS: 326398.45[0m
[36m[2023-07-17 08:03:37,048][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:03:37,048][257371] Reward + Measures: [[278.20884732   0.16712934   0.71583831   0.18770032   0.76719636
    2.06234527]][0m
[37m[1m[2023-07-17 08:03:37,049][257371] Max Reward on eval: 278.2088473190725[0m
[37m[1m[2023-07-17 08:03:37,049][257371] Min Reward on eval: 278.2088473190725[0m
[37m[1m[2023-07-17 08:03:37,049][257371] Mean Reward across all agents: 278.2088473190725[0m
[37m[1m[2023-07-17 08:03:37,050][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:03:42,030][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:03:42,030][257371] Reward + Measures: [[216.43153767   0.21430002   0.63090003   0.23410001   0.67849994
    2.13196135]
 [ 32.4370339    0.14570001   0.6455       0.3425       0.70859998
    2.42776632]
 [137.33799648   0.20190001   0.69019997   0.40079999   0.69770002
    2.2674942 ]
 ...
 [144.05571699   0.22360002   0.63809997   0.24439998   0.70800006
    1.97493958]
 [ 88.39401027   0.34830001   0.49139997   0.36449999   0.50510007
    2.04060841]
 [127.52800751   0.2816       0.55970001   0.22189999   0.66250002
    2.47632909]][0m
[37m[1m[2023-07-17 08:03:42,030][257371] Max Reward on eval: 327.4382371947169[0m
[37m[1m[2023-07-17 08:03:42,031][257371] Min Reward on eval: -61.09604473747313[0m
[37m[1m[2023-07-17 08:03:42,031][257371] Mean Reward across all agents: 127.47680902787016[0m
[37m[1m[2023-07-17 08:03:42,031][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:03:42,033][257371] mean_value=-236.08702980710882, max_value=64.62928635141478[0m
[37m[1m[2023-07-17 08:03:42,036][257371] New mean coefficients: [[-0.36961618  0.25688696 -1.1055527  -0.63260174 -0.3950637   0.05907005]][0m
[37m[1m[2023-07-17 08:03:42,037][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:03:51,141][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 08:03:51,141][257371] FPS: 421865.00[0m
[36m[2023-07-17 08:03:51,143][257371] itr=892, itrs=2000, Progress: 44.60%[0m
[36m[2023-07-17 08:04:02,955][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 08:04:02,955][257371] FPS: 327969.16[0m
[36m[2023-07-17 08:04:07,183][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:04:07,184][257371] Reward + Measures: [[228.45150853   0.18823799   0.68573534   0.19475199   0.73636162
    2.15671492]][0m
[37m[1m[2023-07-17 08:04:07,184][257371] Max Reward on eval: 228.4515085259828[0m
[37m[1m[2023-07-17 08:04:07,184][257371] Min Reward on eval: 228.4515085259828[0m
[37m[1m[2023-07-17 08:04:07,184][257371] Mean Reward across all agents: 228.4515085259828[0m
[37m[1m[2023-07-17 08:04:07,185][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:04:12,120][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:04:12,120][257371] Reward + Measures: [[117.7886109    0.20079999   0.56590003   0.2448       0.66329998
    2.46415329]
 [ 62.51417166   0.3098       0.38679999   0.37470001   0.42449999
    3.54495239]
 [119.58278885   0.2375       0.50920004   0.266        0.53710002
    2.90187502]
 ...
 [-46.39827274   0.34520003   0.3425       0.39110002   0.44829997
    3.62465072]
 [259.83524515   0.3062       0.51029998   0.39360005   0.58429998
    2.20489812]
 [ 54.52688846   0.25229999   0.60540003   0.3096       0.61020005
    3.07087111]][0m
[37m[1m[2023-07-17 08:04:12,120][257371] Max Reward on eval: 423.4158916633576[0m
[37m[1m[2023-07-17 08:04:12,121][257371] Min Reward on eval: -95.94848262700252[0m
[37m[1m[2023-07-17 08:04:12,121][257371] Mean Reward across all agents: 97.87968018124933[0m
[37m[1m[2023-07-17 08:04:12,121][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:04:12,123][257371] mean_value=-307.48586234494246, max_value=297.53271457593496[0m
[37m[1m[2023-07-17 08:04:12,126][257371] New mean coefficients: [[ 0.32417282  0.3105179  -0.6599624   0.30848813 -0.16362499  0.24923833]][0m
[37m[1m[2023-07-17 08:04:12,126][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:04:21,117][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 08:04:21,117][257371] FPS: 427202.58[0m
[36m[2023-07-17 08:04:21,119][257371] itr=893, itrs=2000, Progress: 44.65%[0m
[36m[2023-07-17 08:04:32,856][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 08:04:32,857][257371] FPS: 329973.40[0m
[36m[2023-07-17 08:04:37,215][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:04:37,215][257371] Reward + Measures: [[213.02088108   0.19406067   0.67331332   0.20475097   0.72145736
    2.23818088]][0m
[37m[1m[2023-07-17 08:04:37,215][257371] Max Reward on eval: 213.02088108337531[0m
[37m[1m[2023-07-17 08:04:37,216][257371] Min Reward on eval: 213.02088108337531[0m
[37m[1m[2023-07-17 08:04:37,216][257371] Mean Reward across all agents: 213.02088108337531[0m
[37m[1m[2023-07-17 08:04:37,216][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:04:42,493][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:04:42,494][257371] Reward + Measures: [[211.19024375   0.42810002   0.27220002   0.46089998   0.32969999
    4.21519184]
 [112.28433121   0.32969999   0.48210001   0.30599999   0.55910003
    2.872895  ]
 [ 79.79690429   0.26159999   0.59190005   0.25910002   0.62790006
    2.06985021]
 ...
 [233.17653153   0.18699999   0.6408       0.20130001   0.70419997
    3.19735789]
 [  5.27617672   0.2638       0.57630008   0.1858       0.59740001
    2.83782554]
 [131.0808335    0.1795       0.68790001   0.19770001   0.75260001
    2.7248559 ]][0m
[37m[1m[2023-07-17 08:04:42,494][257371] Max Reward on eval: 352.00864792242646[0m
[37m[1m[2023-07-17 08:04:42,494][257371] Min Reward on eval: -165.94808483584785[0m
[37m[1m[2023-07-17 08:04:42,495][257371] Mean Reward across all agents: 96.91155297557613[0m
[37m[1m[2023-07-17 08:04:42,495][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:04:42,498][257371] mean_value=-187.05698590802814, max_value=167.00013316496228[0m
[37m[1m[2023-07-17 08:04:42,500][257371] New mean coefficients: [[ 1.959722    0.38444105 -0.0248912   0.54789424 -0.6627135  -0.25357872]][0m
[37m[1m[2023-07-17 08:04:42,501][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:04:51,603][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 08:04:51,604][257371] FPS: 421947.81[0m
[36m[2023-07-17 08:04:51,606][257371] itr=894, itrs=2000, Progress: 44.70%[0m
[36m[2023-07-17 08:05:03,296][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 08:05:03,296][257371] FPS: 331314.89[0m
[36m[2023-07-17 08:05:07,646][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:05:07,652][257371] Reward + Measures: [[231.81490671   0.20176733   0.66354501   0.20862131   0.71234429
    2.2332139 ]][0m
[37m[1m[2023-07-17 08:05:07,652][257371] Max Reward on eval: 231.81490670955714[0m
[37m[1m[2023-07-17 08:05:07,652][257371] Min Reward on eval: 231.81490670955714[0m
[37m[1m[2023-07-17 08:05:07,653][257371] Mean Reward across all agents: 231.81490670955714[0m
[37m[1m[2023-07-17 08:05:07,653][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:05:12,726][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:05:12,727][257371] Reward + Measures: [[134.0472686    0.17020001   0.65010005   0.25120002   0.7295
    2.43909144]
 [106.01193335   0.26179999   0.3989       0.28960001   0.44479999
    3.17862463]
 [151.75129784   0.30310002   0.50160003   0.33949998   0.6031
    2.8208673 ]
 ...
 [ 36.27270084   0.59219998   0.28330001   0.61219996   0.63590002
    4.29226542]
 [  9.13046919   0.2753       0.53240001   0.37420002   0.64460003
    3.62861991]
 [ 50.86878554   0.30419999   0.4982       0.37420002   0.62849998
    3.23942351]][0m
[37m[1m[2023-07-17 08:05:12,727][257371] Max Reward on eval: 308.387254729867[0m
[37m[1m[2023-07-17 08:05:12,727][257371] Min Reward on eval: -85.60100868567824[0m
[37m[1m[2023-07-17 08:05:12,728][257371] Mean Reward across all agents: 124.81856089835948[0m
[37m[1m[2023-07-17 08:05:12,728][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:05:12,730][257371] mean_value=-200.30755699893606, max_value=150.8952471748998[0m
[37m[1m[2023-07-17 08:05:12,732][257371] New mean coefficients: [[ 2.032182    0.34830433 -0.57332224  0.43607673 -0.22587016 -0.6378704 ]][0m
[37m[1m[2023-07-17 08:05:12,733][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:05:21,830][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 08:05:21,830][257371] FPS: 422209.43[0m
[36m[2023-07-17 08:05:21,832][257371] itr=895, itrs=2000, Progress: 44.75%[0m
[36m[2023-07-17 08:05:33,662][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 08:05:33,662][257371] FPS: 327390.77[0m
[36m[2023-07-17 08:05:37,937][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:05:37,937][257371] Reward + Measures: [[262.26341888   0.19703402   0.66437799   0.20710503   0.71237469
    2.21153617]][0m
[37m[1m[2023-07-17 08:05:37,938][257371] Max Reward on eval: 262.26341888206616[0m
[37m[1m[2023-07-17 08:05:37,938][257371] Min Reward on eval: 262.26341888206616[0m
[37m[1m[2023-07-17 08:05:37,938][257371] Mean Reward across all agents: 262.26341888206616[0m
[37m[1m[2023-07-17 08:05:37,938][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:05:42,926][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:05:42,927][257371] Reward + Measures: [[ 69.34663871   0.3362       0.57209998   0.1939       0.61400002
    2.92966771]
 [ 39.77930109   0.3211       0.57050002   0.25530002   0.65960002
    2.43831182]
 [ 10.51075178   0.3725       0.53529996   0.14389999   0.53290004
    3.36805987]
 ...
 [166.04108142   0.33790001   0.56940001   0.1577       0.5783
    3.30114865]
 [200.31139324   0.2656       0.5837       0.30650002   0.6656
    2.93026328]
 [148.23348485   0.18380001   0.61770004   0.32839999   0.57889998
    3.40798736]][0m
[37m[1m[2023-07-17 08:05:42,927][257371] Max Reward on eval: 439.9894523695111[0m
[37m[1m[2023-07-17 08:05:42,927][257371] Min Reward on eval: -30.792679504980335[0m
[37m[1m[2023-07-17 08:05:42,927][257371] Mean Reward across all agents: 115.29996622759231[0m
[37m[1m[2023-07-17 08:05:42,928][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:05:42,930][257371] mean_value=-269.7345943085567, max_value=113.1956209835513[0m
[37m[1m[2023-07-17 08:05:42,933][257371] New mean coefficients: [[ 0.7967806  -0.13810498 -1.3238106   0.11549369  0.5927048  -0.70634633]][0m
[37m[1m[2023-07-17 08:05:42,934][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:05:51,948][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 08:05:51,948][257371] FPS: 426081.25[0m
[36m[2023-07-17 08:05:51,950][257371] itr=896, itrs=2000, Progress: 44.80%[0m
[36m[2023-07-17 08:06:03,671][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 08:06:03,672][257371] FPS: 330455.50[0m
[36m[2023-07-17 08:06:07,950][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:06:07,950][257371] Reward + Measures: [[292.73076917   0.196484     0.66058266   0.20491      0.71212965
    2.14285159]][0m
[37m[1m[2023-07-17 08:06:07,950][257371] Max Reward on eval: 292.7307691676927[0m
[37m[1m[2023-07-17 08:06:07,951][257371] Min Reward on eval: 292.7307691676927[0m
[37m[1m[2023-07-17 08:06:07,951][257371] Mean Reward across all agents: 292.7307691676927[0m
[37m[1m[2023-07-17 08:06:07,951][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:06:12,976][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:06:12,976][257371] Reward + Measures: [[115.76047301   0.3071       0.56029999   0.37509999   0.68079996
    2.86615252]
 [409.47526362   0.1759       0.648        0.20879999   0.68340003
    2.29643607]
 [201.85230017   0.2034       0.66149998   0.22660001   0.70970005
    2.18934798]
 ...
 [100.22663521   0.42519999   0.50520003   0.32619998   0.47950003
    2.29246783]
 [ 60.38533866   0.38699996   0.47870001   0.42810002   0.54320002
    2.75774574]
 [103.31173263   0.19760001   0.68089998   0.24669997   0.72750008
    2.5458889 ]][0m
[37m[1m[2023-07-17 08:06:12,977][257371] Max Reward on eval: 411.7298984631896[0m
[37m[1m[2023-07-17 08:06:12,977][257371] Min Reward on eval: -168.34918643422424[0m
[37m[1m[2023-07-17 08:06:12,977][257371] Mean Reward across all agents: 127.53280526930772[0m
[37m[1m[2023-07-17 08:06:12,977][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:06:12,980][257371] mean_value=-195.56899216545665, max_value=309.1848830842812[0m
[37m[1m[2023-07-17 08:06:12,982][257371] New mean coefficients: [[ 0.58361995  0.11460769 -1.3492337  -0.09223109 -0.286138   -1.2121491 ]][0m
[37m[1m[2023-07-17 08:06:12,983][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:06:22,070][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 08:06:22,070][257371] FPS: 422675.65[0m
[36m[2023-07-17 08:06:22,072][257371] itr=897, itrs=2000, Progress: 44.85%[0m
[36m[2023-07-17 08:06:34,000][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 08:06:34,000][257371] FPS: 324731.42[0m
[36m[2023-07-17 08:06:38,282][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:06:38,282][257371] Reward + Measures: [[298.85625134   0.20512067   0.65199167   0.20579533   0.70346135
    2.12103701]][0m
[37m[1m[2023-07-17 08:06:38,282][257371] Max Reward on eval: 298.85625133781866[0m
[37m[1m[2023-07-17 08:06:38,283][257371] Min Reward on eval: 298.85625133781866[0m
[37m[1m[2023-07-17 08:06:38,283][257371] Mean Reward across all agents: 298.85625133781866[0m
[37m[1m[2023-07-17 08:06:38,283][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:06:43,344][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:06:43,344][257371] Reward + Measures: [[  32.14964029    0.3026        0.46000001    0.29900002    0.52280009
     2.32736778]
 [-174.80239155    0.45469999    0.52060002    0.2237        0.58240002
     3.02643585]
 [  23.51197744    0.36989999    0.46409997    0.3335        0.52329999
     2.71588826]
 ...
 [ -14.41572134    0.2924        0.25040001    0.2225        0.30809999
     3.38499761]
 [ 193.95603726    0.28479999    0.45809999    0.32139999    0.54890007
     2.53580523]
 [ -69.38150076    0.2093        0.50199997    0.3294        0.4443
     3.5850265 ]][0m
[37m[1m[2023-07-17 08:06:43,344][257371] Max Reward on eval: 364.0630779132247[0m
[37m[1m[2023-07-17 08:06:43,345][257371] Min Reward on eval: -189.58676950177178[0m
[37m[1m[2023-07-17 08:06:43,345][257371] Mean Reward across all agents: 53.4039229672585[0m
[37m[1m[2023-07-17 08:06:43,345][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:06:43,347][257371] mean_value=-304.7479396968523, max_value=131.4012194157542[0m
[37m[1m[2023-07-17 08:06:43,350][257371] New mean coefficients: [[ 0.7248683  -0.51386863 -0.5987414  -0.3177746   0.12270316 -1.4752103 ]][0m
[37m[1m[2023-07-17 08:06:43,351][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:06:52,519][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-17 08:06:52,519][257371] FPS: 418909.56[0m
[36m[2023-07-17 08:06:52,521][257371] itr=898, itrs=2000, Progress: 44.90%[0m
[36m[2023-07-17 08:07:04,363][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 08:07:04,363][257371] FPS: 327176.61[0m
[36m[2023-07-17 08:07:08,686][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:07:08,692][257371] Reward + Measures: [[327.55437776   0.19814798   0.65831298   0.19849968   0.71290696
    2.02659655]][0m
[37m[1m[2023-07-17 08:07:08,692][257371] Max Reward on eval: 327.5543777574951[0m
[37m[1m[2023-07-17 08:07:08,693][257371] Min Reward on eval: 327.5543777574951[0m
[37m[1m[2023-07-17 08:07:08,693][257371] Mean Reward across all agents: 327.5543777574951[0m
[37m[1m[2023-07-17 08:07:08,693][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:07:13,721][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:07:13,727][257371] Reward + Measures: [[ 68.66843798   0.38440001   0.51120001   0.49680001   0.6577
    3.08661699]
 [ 74.39842635   0.21900001   0.2861       0.21250001   0.2863
    3.90293884]
 [ 69.45641567   0.37120005   0.39340001   0.35820001   0.45860001
    3.28268933]
 ...
 [  5.40692297   0.38640004   0.49829999   0.46399999   0.65600008
    3.04553413]
 [-85.10334484   0.4039       0.37890002   0.34919998   0.465
    3.41593027]
 [171.56989383   0.34760004   0.73300004   0.0986       0.6311
    2.92174888]][0m
[37m[1m[2023-07-17 08:07:13,727][257371] Max Reward on eval: 384.3431739423424[0m
[37m[1m[2023-07-17 08:07:13,728][257371] Min Reward on eval: -173.97152334023266[0m
[37m[1m[2023-07-17 08:07:13,728][257371] Mean Reward across all agents: 93.51743108013493[0m
[37m[1m[2023-07-17 08:07:13,728][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:07:13,730][257371] mean_value=-347.45451720384716, max_value=104.71704584801111[0m
[37m[1m[2023-07-17 08:07:13,732][257371] New mean coefficients: [[ 1.2601757  -0.69634956  0.20873815  0.1737772  -0.00395757 -2.1536784 ]][0m
[37m[1m[2023-07-17 08:07:13,733][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:07:22,774][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 08:07:22,774][257371] FPS: 424821.26[0m
[36m[2023-07-17 08:07:22,776][257371] itr=899, itrs=2000, Progress: 44.95%[0m
[36m[2023-07-17 08:07:34,628][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 08:07:34,629][257371] FPS: 326790.60[0m
[36m[2023-07-17 08:07:38,839][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:07:38,839][257371] Reward + Measures: [[335.71805553   0.18738867   0.67098504   0.19093066   0.7291283
    1.96544397]][0m
[37m[1m[2023-07-17 08:07:38,840][257371] Max Reward on eval: 335.7180555285909[0m
[37m[1m[2023-07-17 08:07:38,840][257371] Min Reward on eval: 335.7180555285909[0m
[37m[1m[2023-07-17 08:07:38,840][257371] Mean Reward across all agents: 335.7180555285909[0m
[37m[1m[2023-07-17 08:07:38,840][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:07:44,039][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:07:44,039][257371] Reward + Measures: [[-23.02156745   0.2502       0.6146       0.65029997   0.75489998
    4.59261179]
 [ 49.68728254   0.66510004   0.81590003   0.0071       0.74580002
    2.85461879]
 [ 49.85612459   0.3136       0.6717       0.19289999   0.71489996
    2.25323415]
 ...
 [  0.469804     0.42680001   0.55470002   0.18300001   0.55419999
    3.40091491]
 [  1.29273563   0.5553       0.64230007   0.20380001   0.68989998
    3.17788267]
 [ -1.2037254    0.4289       0.63710004   0.1073       0.60680002
    3.18045688]][0m
[37m[1m[2023-07-17 08:07:44,040][257371] Max Reward on eval: 463.8446655217558[0m
[37m[1m[2023-07-17 08:07:44,040][257371] Min Reward on eval: -125.92742258752696[0m
[37m[1m[2023-07-17 08:07:44,040][257371] Mean Reward across all agents: 113.26908791438179[0m
[37m[1m[2023-07-17 08:07:44,040][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:07:44,044][257371] mean_value=-164.3632807598667, max_value=141.62705314272193[0m
[37m[1m[2023-07-17 08:07:44,046][257371] New mean coefficients: [[ 0.47296447 -0.4255917   0.09827246  0.20010294 -0.70398957 -2.560685  ]][0m
[37m[1m[2023-07-17 08:07:44,047][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:07:53,018][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 08:07:53,019][257371] FPS: 428114.70[0m
[36m[2023-07-17 08:07:53,021][257371] itr=900, itrs=2000, Progress: 45.00%[0m
[37m[1m[2023-07-17 08:11:20,153][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000880[0m
[36m[2023-07-17 08:11:32,328][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 08:11:32,328][257371] FPS: 329170.83[0m
[36m[2023-07-17 08:11:36,544][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:11:36,545][257371] Reward + Measures: [[336.11843092   0.18614699   0.68201596   0.19009833   0.73717999
    1.88008189]][0m
[37m[1m[2023-07-17 08:11:36,545][257371] Max Reward on eval: 336.1184309201197[0m
[37m[1m[2023-07-17 08:11:36,545][257371] Min Reward on eval: 336.1184309201197[0m
[37m[1m[2023-07-17 08:11:36,546][257371] Mean Reward across all agents: 336.1184309201197[0m
[37m[1m[2023-07-17 08:11:36,546][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:11:41,477][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:11:41,478][257371] Reward + Measures: [[279.14461903   0.1904       0.60190004   0.15480001   0.67460006
    2.3078537 ]
 [-65.30618534   0.17800002   0.55940002   0.3811       0.62729996
    3.12421417]
 [ 35.15477153   0.33600003   0.50139999   0.26550001   0.62229997
    2.84818125]
 ...
 [  8.67699967   0.26640001   0.3123       0.2484       0.39500001
    3.69274259]
 [220.07717039   0.13160001   0.65320003   0.12609999   0.70810002
    1.96660829]
 [ 51.05894467   0.34460002   0.45320001   0.37279996   0.45650002
    2.87906313]][0m
[37m[1m[2023-07-17 08:11:41,478][257371] Max Reward on eval: 494.5661220639944[0m
[37m[1m[2023-07-17 08:11:41,479][257371] Min Reward on eval: -237.55641178416553[0m
[37m[1m[2023-07-17 08:11:41,479][257371] Mean Reward across all agents: 121.16500895033009[0m
[37m[1m[2023-07-17 08:11:41,479][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:11:41,482][257371] mean_value=-343.0474380754486, max_value=146.99967624972038[0m
[37m[1m[2023-07-17 08:11:41,485][257371] New mean coefficients: [[ 1.143789   -0.8787962  -0.26226193  0.7177672  -0.0159896  -1.9575257 ]][0m
[37m[1m[2023-07-17 08:11:41,486][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:11:50,470][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 08:11:50,470][257371] FPS: 427495.03[0m
[36m[2023-07-17 08:11:50,472][257371] itr=901, itrs=2000, Progress: 45.05%[0m
[36m[2023-07-17 08:12:02,103][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 08:12:02,103][257371] FPS: 332993.11[0m
[36m[2023-07-17 08:12:06,361][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:12:06,366][257371] Reward + Measures: [[353.81934701   0.17715099   0.69111699   0.18544501   0.75319695
    1.80218506]][0m
[37m[1m[2023-07-17 08:12:06,367][257371] Max Reward on eval: 353.8193470059714[0m
[37m[1m[2023-07-17 08:12:06,367][257371] Min Reward on eval: 353.8193470059714[0m
[37m[1m[2023-07-17 08:12:06,367][257371] Mean Reward across all agents: 353.8193470059714[0m
[37m[1m[2023-07-17 08:12:06,368][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:12:11,372][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:12:11,378][257371] Reward + Measures: [[152.39023399   0.31599998   0.67070001   0.1063       0.63150007
    2.46591806]
 [170.87639111   0.23210001   0.61560005   0.21170001   0.62900001
    2.50145721]
 [251.71841004   0.20780002   0.51319999   0.2349       0.51030004
    2.83088946]
 ...
 [ 97.72440526   0.40550002   0.35880002   0.40799999   0.3651
    3.24963927]
 [164.16312328   0.31230003   0.43889999   0.27530003   0.45280001
    2.98293829]
 [ 10.83302809   0.46619996   0.42640001   0.38919997   0.4463
    3.56170654]][0m
[37m[1m[2023-07-17 08:12:11,378][257371] Max Reward on eval: 372.9725265301764[0m
[37m[1m[2023-07-17 08:12:11,378][257371] Min Reward on eval: -196.01665777284651[0m
[37m[1m[2023-07-17 08:12:11,379][257371] Mean Reward across all agents: 120.88826466227908[0m
[37m[1m[2023-07-17 08:12:11,379][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:12:11,381][257371] mean_value=-247.51090559180932, max_value=303.92965678661795[0m
[37m[1m[2023-07-17 08:12:11,384][257371] New mean coefficients: [[ 1.842597   -0.7844418  -0.5032015   1.2670836   0.10279644 -2.1322055 ]][0m
[37m[1m[2023-07-17 08:12:11,385][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:12:20,389][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 08:12:20,389][257371] FPS: 426539.73[0m
[36m[2023-07-17 08:12:20,392][257371] itr=902, itrs=2000, Progress: 45.10%[0m
[36m[2023-07-17 08:12:32,302][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 08:12:32,302][257371] FPS: 325241.08[0m
[36m[2023-07-17 08:12:36,649][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:12:36,650][257371] Reward + Measures: [[373.57067215   0.17581867   0.69893569   0.17924833   0.76791936
    1.755041  ]][0m
[37m[1m[2023-07-17 08:12:36,650][257371] Max Reward on eval: 373.57067215379016[0m
[37m[1m[2023-07-17 08:12:36,650][257371] Min Reward on eval: 373.57067215379016[0m
[37m[1m[2023-07-17 08:12:36,651][257371] Mean Reward across all agents: 373.57067215379016[0m
[37m[1m[2023-07-17 08:12:36,651][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:12:41,699][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:12:41,699][257371] Reward + Measures: [[103.04855549   0.12719999   0.70539999   0.36970001   0.78839999
    2.92480135]
 [272.82081889   0.25119999   0.59689999   0.22189999   0.72760004
    1.88491189]
 [103.79433373   0.30239999   0.48599997   0.27260002   0.46630001
    2.71837401]
 ...
 [ 60.56743284   0.39610001   0.37600002   0.40089998   0.54150003
    2.4783318 ]
 [189.74945068   0.29350001   0.56220001   0.3003       0.62699997
    2.4437058 ]
 [ 21.93806174   0.25610003   0.3308       0.2511       0.34310004
    3.18993258]][0m
[37m[1m[2023-07-17 08:12:41,699][257371] Max Reward on eval: 408.8925437912345[0m
[37m[1m[2023-07-17 08:12:41,700][257371] Min Reward on eval: -110.2304158342071[0m
[37m[1m[2023-07-17 08:12:41,700][257371] Mean Reward across all agents: 129.37351611137117[0m
[37m[1m[2023-07-17 08:12:41,700][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:12:41,703][257371] mean_value=-305.05236026854527, max_value=186.40900191297152[0m
[37m[1m[2023-07-17 08:12:41,706][257371] New mean coefficients: [[ 1.0694664  -1.0224777  -0.13200912  0.55688035 -0.09423915 -1.885062  ]][0m
[37m[1m[2023-07-17 08:12:41,707][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:12:50,818][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 08:12:50,818][257371] FPS: 421541.33[0m
[36m[2023-07-17 08:12:50,820][257371] itr=903, itrs=2000, Progress: 45.15%[0m
[36m[2023-07-17 08:13:02,706][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 08:13:02,706][257371] FPS: 325808.41[0m
[36m[2023-07-17 08:13:06,953][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:13:06,954][257371] Reward + Measures: [[376.01744811   0.15590768   0.72583395   0.17246033   0.79987532
    1.67252314]][0m
[37m[1m[2023-07-17 08:13:06,954][257371] Max Reward on eval: 376.0174481098391[0m
[37m[1m[2023-07-17 08:13:06,954][257371] Min Reward on eval: 376.0174481098391[0m
[37m[1m[2023-07-17 08:13:06,955][257371] Mean Reward across all agents: 376.0174481098391[0m
[37m[1m[2023-07-17 08:13:06,955][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:13:12,166][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:13:12,166][257371] Reward + Measures: [[  67.66514216    0.14210001    0.79519999    0.20350002    0.81449997
     2.39533281]
 [  22.76809355    0.2516        0.56550002    0.31399998    0.53220004
     3.2880497 ]
 [  27.13391738    0.37460002    0.68529999    0.62160003    0.4571
     5.14153624]
 ...
 [  54.27158504    0.37899998    0.54229999    0.57870001    0.6512
     4.51619816]
 [-101.30593794    0.1921        0.53310007    0.26830003    0.53719997
     3.77653289]
 [ 186.18583129    0.1745        0.69989997    0.2956        0.65600008
     3.37856841]][0m
[37m[1m[2023-07-17 08:13:12,167][257371] Max Reward on eval: 445.81897354424[0m
[37m[1m[2023-07-17 08:13:12,167][257371] Min Reward on eval: -118.63757923031226[0m
[37m[1m[2023-07-17 08:13:12,167][257371] Mean Reward across all agents: 155.3339451784568[0m
[37m[1m[2023-07-17 08:13:12,167][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:13:12,171][257371] mean_value=-193.89844197576943, max_value=106.57156389586527[0m
[37m[1m[2023-07-17 08:13:12,173][257371] New mean coefficients: [[ 0.8000351  -1.2757883  -1.1273806   1.0002351  -0.13183206 -1.5421999 ]][0m
[37m[1m[2023-07-17 08:13:12,174][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:13:21,115][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 08:13:21,116][257371] FPS: 429548.72[0m
[36m[2023-07-17 08:13:21,118][257371] itr=904, itrs=2000, Progress: 45.20%[0m
[36m[2023-07-17 08:13:32,927][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 08:13:32,927][257371] FPS: 328116.15[0m
[36m[2023-07-17 08:13:37,262][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:13:37,263][257371] Reward + Measures: [[390.55868306   0.15556633   0.72239202   0.17424768   0.79844433
    1.63929427]][0m
[37m[1m[2023-07-17 08:13:37,263][257371] Max Reward on eval: 390.5586830550738[0m
[37m[1m[2023-07-17 08:13:37,263][257371] Min Reward on eval: 390.5586830550738[0m
[37m[1m[2023-07-17 08:13:37,264][257371] Mean Reward across all agents: 390.5586830550738[0m
[37m[1m[2023-07-17 08:13:37,264][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:13:42,283][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:13:42,288][257371] Reward + Measures: [[ 33.04977645   0.20039999   0.14310001   0.15250002   0.1833
    3.49454355]
 [220.52996041   0.16510001   0.7274       0.27900001   0.74130005
    1.97245491]
 [134.37073253   0.39429998   0.38850003   0.43129998   0.58530003
    3.0440259 ]
 ...
 [ 79.58398941   0.58740002   0.15200001   0.58030003   0.46609998
    3.95748687]
 [ 29.7121869    0.19790001   0.65630001   0.4693       0.66210002
    3.54921317]
 [ 16.80080711   0.55989999   0.33219999   0.65059996   0.63920003
    3.7206955 ]][0m
[37m[1m[2023-07-17 08:13:42,289][257371] Max Reward on eval: 383.36391256451606[0m
[37m[1m[2023-07-17 08:13:42,289][257371] Min Reward on eval: -98.49256558008491[0m
[37m[1m[2023-07-17 08:13:42,289][257371] Mean Reward across all agents: 111.60389020154554[0m
[37m[1m[2023-07-17 08:13:42,290][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:13:42,293][257371] mean_value=-259.5622639433806, max_value=163.0783818338549[0m
[37m[1m[2023-07-17 08:13:42,296][257371] New mean coefficients: [[ 0.8947094  -0.80628896 -0.6461955   1.2199498  -0.8431358  -2.1540182 ]][0m
[37m[1m[2023-07-17 08:13:42,297][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:13:51,280][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 08:13:51,281][257371] FPS: 427528.63[0m
[36m[2023-07-17 08:13:51,283][257371] itr=905, itrs=2000, Progress: 45.25%[0m
[36m[2023-07-17 08:14:03,117][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 08:14:03,117][257371] FPS: 327358.74[0m
[36m[2023-07-17 08:14:07,387][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:14:07,388][257371] Reward + Measures: [[379.75185575   0.15058467   0.72820604   0.17159399   0.81532633
    1.6007663 ]][0m
[37m[1m[2023-07-17 08:14:07,388][257371] Max Reward on eval: 379.7518557532973[0m
[37m[1m[2023-07-17 08:14:07,388][257371] Min Reward on eval: 379.7518557532973[0m
[37m[1m[2023-07-17 08:14:07,388][257371] Mean Reward across all agents: 379.7518557532973[0m
[37m[1m[2023-07-17 08:14:07,389][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:14:12,397][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:14:12,397][257371] Reward + Measures: [[ 65.80024174   0.2393       0.52880001   0.31780002   0.66359997
    2.69917727]
 [-18.24843312   0.23870002   0.63900006   0.55440009   0.76739997
    3.72146297]
 [204.43053722   0.1788       0.69430006   0.27669999   0.74860001
    2.45288825]
 ...
 [-36.90920299   0.2323       0.61879998   0.5126       0.72470003
    3.38068891]
 [155.9438602    0.40699998   0.4348       0.50360006   0.47220001
    2.87453389]
 [256.65364742   0.22939999   0.60530001   0.21139999   0.71100003
    2.08390808]][0m
[37m[1m[2023-07-17 08:14:12,397][257371] Max Reward on eval: 522.1194076806307[0m
[37m[1m[2023-07-17 08:14:12,398][257371] Min Reward on eval: -179.26103087347002[0m
[37m[1m[2023-07-17 08:14:12,398][257371] Mean Reward across all agents: 84.39649868177263[0m
[37m[1m[2023-07-17 08:14:12,398][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:14:12,401][257371] mean_value=-220.4196668198475, max_value=307.531661721621[0m
[37m[1m[2023-07-17 08:14:12,404][257371] New mean coefficients: [[ 0.09383833 -0.39299893 -1.1374133   0.9594861  -0.40047422 -1.48529   ]][0m
[37m[1m[2023-07-17 08:14:12,405][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:14:21,426][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 08:14:21,426][257371] FPS: 425745.69[0m
[36m[2023-07-17 08:14:21,428][257371] itr=906, itrs=2000, Progress: 45.30%[0m
[36m[2023-07-17 08:14:33,305][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 08:14:33,305][257371] FPS: 326078.21[0m
[36m[2023-07-17 08:14:37,682][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:14:37,683][257371] Reward + Measures: [[374.34398909   0.15191834   0.72224867   0.17951065   0.80168366
    1.54021025]][0m
[37m[1m[2023-07-17 08:14:37,683][257371] Max Reward on eval: 374.34398908784226[0m
[37m[1m[2023-07-17 08:14:37,683][257371] Min Reward on eval: 374.34398908784226[0m
[37m[1m[2023-07-17 08:14:37,684][257371] Mean Reward across all agents: 374.34398908784226[0m
[37m[1m[2023-07-17 08:14:37,684][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:14:42,683][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:14:42,689][257371] Reward + Measures: [[238.06127931   0.28120002   0.52080005   0.24340001   0.60030001
    2.28902984]
 [-10.54840683   0.12150001   0.62220001   0.42389998   0.64079994
    2.87528491]
 [-85.71975964   0.08880001   0.76840001   0.51010007   0.77310002
    4.44339037]
 ...
 [-68.08333998   0.1109       0.66259998   0.39999998   0.70340002
    3.8513267 ]
 [ 16.32109967   0.0784       0.57840008   0.51420003   0.63770002
    4.7185235 ]
 [-27.79146624   0.111        0.60690004   0.46230003   0.61280006
    3.71643996]][0m
[37m[1m[2023-07-17 08:14:42,689][257371] Max Reward on eval: 451.7918014518917[0m
[37m[1m[2023-07-17 08:14:42,689][257371] Min Reward on eval: -194.41512348502874[0m
[37m[1m[2023-07-17 08:14:42,690][257371] Mean Reward across all agents: 72.14380703548194[0m
[37m[1m[2023-07-17 08:14:42,690][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:14:42,692][257371] mean_value=-218.3091581112186, max_value=196.2207776320116[0m
[37m[1m[2023-07-17 08:14:42,695][257371] New mean coefficients: [[ 0.28273126 -0.11370236 -0.67954797  0.9674112  -0.27157658 -1.6265675 ]][0m
[37m[1m[2023-07-17 08:14:42,696][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:14:51,699][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 08:14:51,700][257371] FPS: 426579.30[0m
[36m[2023-07-17 08:14:51,702][257371] itr=907, itrs=2000, Progress: 45.35%[0m
[36m[2023-07-17 08:15:03,610][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 08:15:03,610][257371] FPS: 325232.75[0m
[36m[2023-07-17 08:15:07,924][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:15:07,924][257371] Reward + Measures: [[364.73980388   0.146062     0.73027301   0.18748799   0.80195433
    1.47167933]][0m
[37m[1m[2023-07-17 08:15:07,925][257371] Max Reward on eval: 364.7398038843473[0m
[37m[1m[2023-07-17 08:15:07,925][257371] Min Reward on eval: 364.7398038843473[0m
[37m[1m[2023-07-17 08:15:07,925][257371] Mean Reward across all agents: 364.7398038843473[0m
[37m[1m[2023-07-17 08:15:07,926][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:15:12,897][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:15:12,898][257371] Reward + Measures: [[ 29.82926295   0.38459998   0.31540003   0.3757       0.37369999
    3.25159907]
 [112.57967711   0.24820001   0.69810003   0.37619999   0.6559
    2.72237492]
 [107.53480554   0.3184       0.62770003   0.2131       0.73409998
    2.7513237 ]
 ...
 [161.48441363   0.30669999   0.565        0.2807       0.61840004
    2.29032493]
 [181.88091945   0.24089999   0.73100007   0.24870001   0.80070001
    3.23243117]
 [188.33071566   0.26970002   0.75080007   0.1046       0.77710003
    2.56467319]][0m
[37m[1m[2023-07-17 08:15:12,898][257371] Max Reward on eval: 412.12921142801645[0m
[37m[1m[2023-07-17 08:15:12,898][257371] Min Reward on eval: -53.98799830526114[0m
[37m[1m[2023-07-17 08:15:12,899][257371] Mean Reward across all agents: 129.4965255928092[0m
[37m[1m[2023-07-17 08:15:12,899][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:15:12,902][257371] mean_value=-358.67875020371673, max_value=169.504665684295[0m
[37m[1m[2023-07-17 08:15:12,904][257371] New mean coefficients: [[ 0.17338088  0.0821625  -0.6140648   0.8903929  -0.28444472 -1.5753365 ]][0m
[37m[1m[2023-07-17 08:15:12,905][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:15:21,893][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 08:15:21,893][257371] FPS: 427328.10[0m
[36m[2023-07-17 08:15:21,895][257371] itr=908, itrs=2000, Progress: 45.40%[0m
[36m[2023-07-17 08:15:33,754][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 08:15:33,755][257371] FPS: 326676.11[0m
[36m[2023-07-17 08:15:37,940][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:15:37,941][257371] Reward + Measures: [[351.62120516   0.14455168   0.73957795   0.19118233   0.81230372
    1.39766335]][0m
[37m[1m[2023-07-17 08:15:37,941][257371] Max Reward on eval: 351.6212051578666[0m
[37m[1m[2023-07-17 08:15:37,941][257371] Min Reward on eval: 351.6212051578666[0m
[37m[1m[2023-07-17 08:15:37,941][257371] Mean Reward across all agents: 351.6212051578666[0m
[37m[1m[2023-07-17 08:15:37,942][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:15:42,941][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:15:42,942][257371] Reward + Measures: [[510.61601642   0.16150002   0.67980003   0.20679998   0.77019995
    1.62981987]
 [187.54884007   0.24000001   0.55530006   0.31060001   0.639
    2.40845847]
 [199.78508663   0.16540001   0.69960004   0.20869999   0.74580002
    2.30337763]
 ...
 [103.73211385   0.141        0.75749999   0.17549999   0.85570002
    1.92734706]
 [ 38.79373014   0.22389999   0.72649997   0.28220001   0.76820004
    2.64762282]
 [  8.24886927   0.19670002   0.75459999   0.25910002   0.80360001
    2.45897913]][0m
[37m[1m[2023-07-17 08:15:42,942][257371] Max Reward on eval: 510.616016419977[0m
[37m[1m[2023-07-17 08:15:42,942][257371] Min Reward on eval: -203.8266725554131[0m
[37m[1m[2023-07-17 08:15:42,943][257371] Mean Reward across all agents: 126.4025060218318[0m
[37m[1m[2023-07-17 08:15:42,943][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:15:42,946][257371] mean_value=-201.30597096265822, max_value=239.71799192515[0m
[37m[1m[2023-07-17 08:15:42,948][257371] New mean coefficients: [[-0.7706597   0.14214821 -0.8625567   0.69614494  0.21997836 -1.3606589 ]][0m
[37m[1m[2023-07-17 08:15:42,949][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:15:51,933][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 08:15:51,933][257371] FPS: 427533.05[0m
[36m[2023-07-17 08:15:51,935][257371] itr=909, itrs=2000, Progress: 45.45%[0m
[36m[2023-07-17 08:16:03,568][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 08:16:03,568][257371] FPS: 333006.28[0m
[36m[2023-07-17 08:16:07,875][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:16:07,876][257371] Reward + Measures: [[326.70999733   0.147636     0.75048631   0.19122033   0.81606102
    1.34085643]][0m
[37m[1m[2023-07-17 08:16:07,876][257371] Max Reward on eval: 326.70999733141764[0m
[37m[1m[2023-07-17 08:16:07,876][257371] Min Reward on eval: 326.70999733141764[0m
[37m[1m[2023-07-17 08:16:07,876][257371] Mean Reward across all agents: 326.70999733141764[0m
[37m[1m[2023-07-17 08:16:07,877][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:16:13,106][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:16:13,106][257371] Reward + Measures: [[ 28.61896672   0.20320001   0.69660008   0.22149999   0.75739998
    2.0008831 ]
 [-23.42847836   0.21350001   0.75190002   0.2502       0.81750005
    2.41809988]
 [ 40.02062953   0.1595       0.74379998   0.53560001   0.80480003
    3.45827937]
 ...
 [ 44.98940494   0.25620002   0.60080004   0.3213       0.65450001
    2.84297252]
 [452.98193359   0.1693       0.60680002   0.31009999   0.60470003
    1.81814992]
 [240.60000611   0.1664       0.69960004   0.13939999   0.78370005
    1.61096251]][0m
[37m[1m[2023-07-17 08:16:13,106][257371] Max Reward on eval: 452.9819335915148[0m
[37m[1m[2023-07-17 08:16:13,107][257371] Min Reward on eval: -78.02614508979022[0m
[37m[1m[2023-07-17 08:16:13,107][257371] Mean Reward across all agents: 111.90726570853313[0m
[37m[1m[2023-07-17 08:16:13,107][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:16:13,110][257371] mean_value=-232.2661680306534, max_value=158.8015472818754[0m
[37m[1m[2023-07-17 08:16:13,113][257371] New mean coefficients: [[ 0.6186913  -0.1901723  -0.8382376   0.7891443   0.20984863 -1.0591817 ]][0m
[37m[1m[2023-07-17 08:16:13,114][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:16:22,168][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 08:16:22,168][257371] FPS: 424208.90[0m
[36m[2023-07-17 08:16:22,171][257371] itr=910, itrs=2000, Progress: 45.50%[0m
[37m[1m[2023-07-17 08:19:47,524][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000890[0m
[36m[2023-07-17 08:19:59,663][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 08:19:59,663][257371] FPS: 331789.86[0m
[36m[2023-07-17 08:20:03,940][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:20:03,940][257371] Reward + Measures: [[324.23428614   0.14000167   0.75578696   0.19469601   0.82408464
    1.30479085]][0m
[37m[1m[2023-07-17 08:20:03,941][257371] Max Reward on eval: 324.23428613580717[0m
[37m[1m[2023-07-17 08:20:03,941][257371] Min Reward on eval: 324.23428613580717[0m
[37m[1m[2023-07-17 08:20:03,941][257371] Mean Reward across all agents: 324.23428613580717[0m
[37m[1m[2023-07-17 08:20:03,941][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:20:08,878][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:20:08,940][257371] Reward + Measures: [[234.73167422   0.17690001   0.69700003   0.2527       0.7263
    1.7339524 ]
 [110.20616531   0.3477       0.52010006   0.42940003   0.51700002
    2.89922595]
 [ 54.31964921   0.41090003   0.61230004   0.25910002   0.72099996
    1.75764394]
 ...
 [ 29.31505154   0.27130002   0.45660001   0.2958       0.52080005
    3.10370302]
 [201.55825807   0.23279999   0.75170004   0.35080001   0.78130001
    1.58163452]
 [195.78958704   0.15549999   0.80300009   0.25039998   0.83109999
    1.55455589]][0m
[37m[1m[2023-07-17 08:20:08,940][257371] Max Reward on eval: 482.9667129665613[0m
[37m[1m[2023-07-17 08:20:08,940][257371] Min Reward on eval: -80.30019001537002[0m
[37m[1m[2023-07-17 08:20:08,940][257371] Mean Reward across all agents: 136.77056943186034[0m
[37m[1m[2023-07-17 08:20:08,941][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:20:08,944][257371] mean_value=-355.35502177649073, max_value=169.38417212652098[0m
[37m[1m[2023-07-17 08:20:08,947][257371] New mean coefficients: [[ 0.42465973 -0.33688045 -0.567688    0.14388454  0.9989488  -0.6894445 ]][0m
[37m[1m[2023-07-17 08:20:08,948][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:20:17,997][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 08:20:17,997][257371] FPS: 424445.84[0m
[36m[2023-07-17 08:20:18,000][257371] itr=911, itrs=2000, Progress: 45.55%[0m
[36m[2023-07-17 08:20:29,893][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 08:20:29,893][257371] FPS: 325692.34[0m
[36m[2023-07-17 08:20:34,160][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:20:34,161][257371] Reward + Measures: [[287.83846598   0.12272066   0.78050029   0.20238768   0.85520768
    1.23827636]][0m
[37m[1m[2023-07-17 08:20:34,161][257371] Max Reward on eval: 287.8384659826527[0m
[37m[1m[2023-07-17 08:20:34,161][257371] Min Reward on eval: 287.8384659826527[0m
[37m[1m[2023-07-17 08:20:34,161][257371] Mean Reward across all agents: 287.8384659826527[0m
[37m[1m[2023-07-17 08:20:34,162][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:20:39,146][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:20:39,147][257371] Reward + Measures: [[ 184.76312733    0.12460001    0.72200006    0.26449999    0.81150001
     1.66894364]
 [  94.81341555    0.60970002    0.3558        0.67049998    0.82320005
     4.31547499]
 [-103.85751216    0.67550004    0.30360001    0.73500001    0.94280005
     5.18729925]
 ...
 [ 223.26551056    0.31259999    0.74440002    0.204         0.77969998
     1.25205648]
 [ 216.07297706    0.27649999    0.61879998    0.34440002    0.71709996
     2.7879467 ]
 [ 162.11882253    0.27789998    0.58099997    0.28530002    0.68870002
     2.04971743]][0m
[37m[1m[2023-07-17 08:20:39,147][257371] Max Reward on eval: 441.57133102267983[0m
[37m[1m[2023-07-17 08:20:39,148][257371] Min Reward on eval: -558.2269797176123[0m
[37m[1m[2023-07-17 08:20:39,148][257371] Mean Reward across all agents: 151.0121953026089[0m
[37m[1m[2023-07-17 08:20:39,148][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:20:39,153][257371] mean_value=-222.28254184343018, max_value=138.64862823234088[0m
[37m[1m[2023-07-17 08:20:39,156][257371] New mean coefficients: [[ 0.20965473 -0.13120228 -0.78291523  0.07783724  1.2699239  -0.7488864 ]][0m
[37m[1m[2023-07-17 08:20:39,157][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:20:48,176][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 08:20:48,177][257371] FPS: 425841.27[0m
[36m[2023-07-17 08:20:48,179][257371] itr=912, itrs=2000, Progress: 45.60%[0m
[36m[2023-07-17 08:20:59,959][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 08:20:59,960][257371] FPS: 328889.60[0m
[36m[2023-07-17 08:21:04,310][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:21:04,310][257371] Reward + Measures: [[251.93768297   0.10589166   0.81359798   0.20017932   0.8944087
    1.17134047]][0m
[37m[1m[2023-07-17 08:21:04,311][257371] Max Reward on eval: 251.9376829653966[0m
[37m[1m[2023-07-17 08:21:04,311][257371] Min Reward on eval: 251.9376829653966[0m
[37m[1m[2023-07-17 08:21:04,311][257371] Mean Reward across all agents: 251.9376829653966[0m
[37m[1m[2023-07-17 08:21:04,311][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:21:09,301][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:21:09,301][257371] Reward + Measures: [[131.48220826   0.1375       0.75590003   0.1453       0.81040001
    1.59859204]
 [142.05140019   0.1733       0.74349999   0.19790001   0.73379999
    1.92824936]
 [146.44235134   0.21760002   0.69740003   0.33450001   0.69450003
    2.22579718]
 ...
 [ 41.44815163   0.15899999   0.65679997   0.2529       0.76600003
    2.25979257]
 [214.4705944    0.22550002   0.68870002   0.1778       0.67390007
    1.89208353]
 [-10.86057982   0.31070003   0.48529997   0.2881       0.50800002
    3.04035258]][0m
[37m[1m[2023-07-17 08:21:09,301][257371] Max Reward on eval: 416.69835279881954[0m
[37m[1m[2023-07-17 08:21:09,302][257371] Min Reward on eval: -71.24796211738139[0m
[37m[1m[2023-07-17 08:21:09,302][257371] Mean Reward across all agents: 87.30336447538238[0m
[37m[1m[2023-07-17 08:21:09,302][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:21:09,304][257371] mean_value=-302.3836584746805, max_value=121.76147309712863[0m
[37m[1m[2023-07-17 08:21:09,307][257371] New mean coefficients: [[ 0.17746611  0.0811882  -0.4627665   0.10216477  0.71998656 -1.361362  ]][0m
[37m[1m[2023-07-17 08:21:09,308][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:21:18,293][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 08:21:18,293][257371] FPS: 427444.45[0m
[36m[2023-07-17 08:21:18,296][257371] itr=913, itrs=2000, Progress: 45.65%[0m
[36m[2023-07-17 08:21:30,113][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 08:21:30,113][257371] FPS: 327762.35[0m
[36m[2023-07-17 08:21:34,460][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:21:34,460][257371] Reward + Measures: [[215.79100952   0.32892001   0.69933969   0.23303567   0.81908202
    1.14197278]][0m
[37m[1m[2023-07-17 08:21:34,460][257371] Max Reward on eval: 215.79100951927512[0m
[37m[1m[2023-07-17 08:21:34,460][257371] Min Reward on eval: 215.79100951927512[0m
[37m[1m[2023-07-17 08:21:34,461][257371] Mean Reward across all agents: 215.79100951927512[0m
[37m[1m[2023-07-17 08:21:34,461][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:21:39,791][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:21:39,792][257371] Reward + Measures: [[100.04530275   0.47         0.23         0.5043       0.3752
    2.69028592]
 [124.64692497   0.37360001   0.63870001   0.3457       0.67970002
    1.66296613]
 [ 37.54409087   0.27509999   0.75100005   0.11900001   0.77910006
    2.23905349]
 ...
 [ 71.26747988   0.57590002   0.47639999   0.5273       0.48020002
    1.94731772]
 [138.11889552   0.2714       0.72079998   0.1115       0.79189998
    1.24793422]
 [  2.43850112   0.48640004   0.65719998   0.23310001   0.61689997
    1.84711099]][0m
[37m[1m[2023-07-17 08:21:39,792][257371] Max Reward on eval: 252.69076154604554[0m
[37m[1m[2023-07-17 08:21:39,792][257371] Min Reward on eval: -242.2468480795913[0m
[37m[1m[2023-07-17 08:21:39,793][257371] Mean Reward across all agents: 53.205703028262825[0m
[37m[1m[2023-07-17 08:21:39,793][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:21:39,796][257371] mean_value=-316.83635514150467, max_value=163.52213114645411[0m
[37m[1m[2023-07-17 08:21:39,799][257371] New mean coefficients: [[ 0.7949328  -0.27056447 -0.04493266  0.39013395  0.835317   -1.6397203 ]][0m
[37m[1m[2023-07-17 08:21:39,800][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:21:48,911][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 08:21:48,911][257371] FPS: 421549.00[0m
[36m[2023-07-17 08:21:48,913][257371] itr=914, itrs=2000, Progress: 45.70%[0m
[36m[2023-07-17 08:22:00,919][257371] train() took 11.90 seconds to complete[0m
[36m[2023-07-17 08:22:00,919][257371] FPS: 322632.34[0m
[36m[2023-07-17 08:22:05,174][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:22:05,174][257371] Reward + Measures: [[228.61241929   0.32721898   0.71389234   0.23778735   0.83194894
    1.05208671]][0m
[37m[1m[2023-07-17 08:22:05,175][257371] Max Reward on eval: 228.61241929216303[0m
[37m[1m[2023-07-17 08:22:05,175][257371] Min Reward on eval: 228.61241929216303[0m
[37m[1m[2023-07-17 08:22:05,175][257371] Mean Reward across all agents: 228.61241929216303[0m
[37m[1m[2023-07-17 08:22:05,175][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:22:10,201][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:22:10,202][257371] Reward + Measures: [[ 85.87311575   0.2563       0.75520003   0.17620002   0.76750004
    1.84964013]
 [ 31.85066755   0.22529998   0.57620001   0.19560002   0.66459996
    2.21153903]
 [136.28914355   0.3267       0.5169       0.34         0.57040006
    1.96130598]
 ...
 [ 65.15200405   0.30340001   0.62580001   0.31570002   0.66300005
    1.39397991]
 [ 75.37460732   0.26910001   0.62610006   0.2529       0.67809999
    1.73431146]
 [109.37283375   0.25110003   0.83589995   0.12730001   0.76980001
    1.47029746]][0m
[37m[1m[2023-07-17 08:22:10,202][257371] Max Reward on eval: 308.8096256583929[0m
[37m[1m[2023-07-17 08:22:10,202][257371] Min Reward on eval: -52.21003402317874[0m
[37m[1m[2023-07-17 08:22:10,203][257371] Mean Reward across all agents: 114.85811294359233[0m
[37m[1m[2023-07-17 08:22:10,203][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:22:10,206][257371] mean_value=-358.41042513856127, max_value=129.806556092707[0m
[37m[1m[2023-07-17 08:22:10,209][257371] New mean coefficients: [[ 0.40162057 -0.00552312 -0.52525073  0.34300652  0.4631226  -1.3278233 ]][0m
[37m[1m[2023-07-17 08:22:10,210][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:22:19,200][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 08:22:19,200][257371] FPS: 427215.14[0m
[36m[2023-07-17 08:22:19,202][257371] itr=915, itrs=2000, Progress: 45.75%[0m
[36m[2023-07-17 08:22:30,903][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 08:22:30,904][257371] FPS: 331063.91[0m
[36m[2023-07-17 08:22:35,237][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:22:35,237][257371] Reward + Measures: [[229.0399254    0.328747     0.72042495   0.25368899   0.83890265
    0.95975536]][0m
[37m[1m[2023-07-17 08:22:35,237][257371] Max Reward on eval: 229.03992539533198[0m
[37m[1m[2023-07-17 08:22:35,237][257371] Min Reward on eval: 229.03992539533198[0m
[37m[1m[2023-07-17 08:22:35,238][257371] Mean Reward across all agents: 229.03992539533198[0m
[37m[1m[2023-07-17 08:22:35,238][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:22:40,267][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:22:40,267][257371] Reward + Measures: [[175.36540698   0.1864       0.74329996   0.3425       0.86920005
    1.36307907]
 [134.97961329   0.44110003   0.4605       0.53249997   0.44010001
    2.34133601]
 [ 33.8691044    0.0333       0.95120001   0.3863       0.98229998
    2.46785116]
 ...
 [ 45.14056025   0.44640002   0.47350001   0.45750004   0.49400002
    2.12921715]
 [ 54.39885112   0.36680001   0.62799996   0.34910002   0.75830001
    1.55006909]
 [ 30.40503034   0.38340002   0.65619999   0.21250001   0.66539997
    1.38679397]][0m
[37m[1m[2023-07-17 08:22:40,268][257371] Max Reward on eval: 251.78106688186526[0m
[37m[1m[2023-07-17 08:22:40,268][257371] Min Reward on eval: -87.48901958223432[0m
[37m[1m[2023-07-17 08:22:40,268][257371] Mean Reward across all agents: 91.18481301031157[0m
[37m[1m[2023-07-17 08:22:40,268][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:22:40,272][257371] mean_value=-294.61844498455434, max_value=187.71334806280802[0m
[37m[1m[2023-07-17 08:22:40,275][257371] New mean coefficients: [[ 0.4878968  -0.28498587 -0.0281468   0.36431906  0.48494357 -1.5792676 ]][0m
[37m[1m[2023-07-17 08:22:40,276][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:22:49,341][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 08:22:49,341][257371] FPS: 423666.26[0m
[36m[2023-07-17 08:22:49,344][257371] itr=916, itrs=2000, Progress: 45.80%[0m
[36m[2023-07-17 08:23:01,206][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 08:23:01,206][257371] FPS: 326629.70[0m
[36m[2023-07-17 08:23:05,557][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:23:05,558][257371] Reward + Measures: [[230.83543263   0.328803     0.74133968   0.23823299   0.86131835
    0.86831933]][0m
[37m[1m[2023-07-17 08:23:05,558][257371] Max Reward on eval: 230.8354326304562[0m
[37m[1m[2023-07-17 08:23:05,558][257371] Min Reward on eval: 230.8354326304562[0m
[37m[1m[2023-07-17 08:23:05,559][257371] Mean Reward across all agents: 230.8354326304562[0m
[37m[1m[2023-07-17 08:23:05,559][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:23:10,620][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:23:10,621][257371] Reward + Measures: [[147.01258563   0.35190004   0.57380003   0.36040002   0.58969998
    1.86026561]
 [189.0915651    0.44670001   0.61020005   0.3159       0.74230003
    1.15951502]
 [ 46.50766609   0.0041       0.88640004   0.6943       0.958
    1.88628852]
 ...
 [104.23885919   0.31529999   0.71830004   0.24720001   0.79370004
    1.39865947]
 [210.87927437   0.36859998   0.7184       0.19640002   0.84440005
    1.0650183 ]
 [233.02547263   0.31760001   0.64219999   0.35530001   0.74270004
    1.45267987]][0m
[37m[1m[2023-07-17 08:23:10,621][257371] Max Reward on eval: 266.2791137844324[0m
[37m[1m[2023-07-17 08:23:10,621][257371] Min Reward on eval: -194.8615694358945[0m
[37m[1m[2023-07-17 08:23:10,621][257371] Mean Reward across all agents: 83.29249664743338[0m
[37m[1m[2023-07-17 08:23:10,622][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:23:10,626][257371] mean_value=-305.2562365709705, max_value=124.89118342508463[0m
[37m[1m[2023-07-17 08:23:10,629][257371] New mean coefficients: [[ 0.57840073  0.13675076 -0.1334466   0.63368416  0.41931492 -1.0821615 ]][0m
[37m[1m[2023-07-17 08:23:10,630][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:23:19,774][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 08:23:19,775][257371] FPS: 419994.92[0m
[36m[2023-07-17 08:23:19,777][257371] itr=917, itrs=2000, Progress: 45.85%[0m
[36m[2023-07-17 08:23:31,646][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 08:23:31,646][257371] FPS: 326407.43[0m
[36m[2023-07-17 08:23:35,969][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:23:35,969][257371] Reward + Measures: [[231.79241983   0.31248799   0.76500225   0.24450998   0.88450128
    0.77754116]][0m
[37m[1m[2023-07-17 08:23:35,969][257371] Max Reward on eval: 231.79241982508017[0m
[37m[1m[2023-07-17 08:23:35,969][257371] Min Reward on eval: 231.79241982508017[0m
[37m[1m[2023-07-17 08:23:35,970][257371] Mean Reward across all agents: 231.79241982508017[0m
[37m[1m[2023-07-17 08:23:35,970][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:23:40,920][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:23:40,921][257371] Reward + Measures: [[ 73.79075765   0.1142       0.75139999   0.289        0.82270002
    2.34816551]
 [231.61025144   0.33720002   0.59680003   0.34659997   0.72340006
    1.14171386]
 [136.08218978   0.41209999   0.4323       0.39149997   0.53380007
    1.89934087]
 ...
 [ 85.89232445   0.1309       0.80870003   0.25900003   0.87239999
    1.50984192]
 [118.03551536   0.35380003   0.69750005   0.28239998   0.75210005
    1.19616067]
 [ 65.71897816   0.3865       0.77859992   0.1567       0.69810003
    1.09746885]][0m
[37m[1m[2023-07-17 08:23:40,921][257371] Max Reward on eval: 335.94687462113797[0m
[37m[1m[2023-07-17 08:23:40,921][257371] Min Reward on eval: -74.4449758483097[0m
[37m[1m[2023-07-17 08:23:40,921][257371] Mean Reward across all agents: 101.16640715968379[0m
[37m[1m[2023-07-17 08:23:40,922][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:23:40,925][257371] mean_value=-273.03975532652976, max_value=166.24855989344266[0m
[37m[1m[2023-07-17 08:23:40,928][257371] New mean coefficients: [[-0.01393259  0.31236696 -0.16169791  0.26673532  0.531052   -0.48308784]][0m
[37m[1m[2023-07-17 08:23:40,929][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:23:49,967][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 08:23:49,967][257371] FPS: 424949.83[0m
[36m[2023-07-17 08:23:49,969][257371] itr=918, itrs=2000, Progress: 45.90%[0m
[36m[2023-07-17 08:24:01,740][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 08:24:01,740][257371] FPS: 329109.65[0m
[36m[2023-07-17 08:24:06,102][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:24:06,102][257371] Reward + Measures: [[211.55439635   0.34641665   0.78834331   0.24231166   0.88987863
    0.7003563 ]][0m
[37m[1m[2023-07-17 08:24:06,102][257371] Max Reward on eval: 211.55439634702074[0m
[37m[1m[2023-07-17 08:24:06,103][257371] Min Reward on eval: 211.55439634702074[0m
[37m[1m[2023-07-17 08:24:06,103][257371] Mean Reward across all agents: 211.55439634702074[0m
[37m[1m[2023-07-17 08:24:06,103][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:24:11,076][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:24:11,077][257371] Reward + Measures: [[169.54338265   0.20640002   0.76910001   0.2852       0.82310009
    1.05952954]
 [184.01577187   0.42589998   0.74360001   0.15290001   0.87010002
    0.85722202]
 [113.89910314   0.40859994   0.71820003   0.3163       0.71620005
    1.02367079]
 ...
 [241.35804369   0.30820003   0.70520002   0.31420001   0.8276
    0.97122306]
 [146.10637951   0.3371       0.68560004   0.25650001   0.79670006
    1.08655775]
 [198.49436854   0.35350001   0.68489999   0.23099999   0.745
    1.0449394 ]][0m
[37m[1m[2023-07-17 08:24:11,077][257371] Max Reward on eval: 268.1394882241264[0m
[37m[1m[2023-07-17 08:24:11,077][257371] Min Reward on eval: -117.78691338989884[0m
[37m[1m[2023-07-17 08:24:11,077][257371] Mean Reward across all agents: 89.31350125783594[0m
[37m[1m[2023-07-17 08:24:11,078][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:24:11,081][257371] mean_value=-333.04228473175505, max_value=121.07645416020515[0m
[37m[1m[2023-07-17 08:24:11,084][257371] New mean coefficients: [[-0.38525847  0.35006392  0.11971046 -0.20778543  0.62274325 -0.20153427]][0m
[37m[1m[2023-07-17 08:24:11,085][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:24:20,200][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 08:24:20,201][257371] FPS: 421328.43[0m
[36m[2023-07-17 08:24:20,203][257371] itr=919, itrs=2000, Progress: 45.95%[0m
[36m[2023-07-17 08:24:32,140][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 08:24:32,140][257371] FPS: 324559.93[0m
[36m[2023-07-17 08:24:36,520][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:24:36,520][257371] Reward + Measures: [[163.69640455   0.38725635   0.82755536   0.18889631   0.91452163
    0.6267305 ]][0m
[37m[1m[2023-07-17 08:24:36,520][257371] Max Reward on eval: 163.6964045512807[0m
[37m[1m[2023-07-17 08:24:36,521][257371] Min Reward on eval: 163.6964045512807[0m
[37m[1m[2023-07-17 08:24:36,521][257371] Mean Reward across all agents: 163.6964045512807[0m
[37m[1m[2023-07-17 08:24:36,521][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:24:41,867][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:24:41,868][257371] Reward + Measures: [[116.49288587   0.0067       0.83870012   0.61669999   0.70900005
    2.33875632]
 [195.15115642   0.38550001   0.68110001   0.45290002   0.73439997
    1.27098835]
 [199.08379363   0.45570001   0.65970004   0.51470006   0.70499992
    1.17772567]
 ...
 [120.28153275   0.4391       0.77230006   0.0594       0.91000003
    0.67158777]
 [237.49169348   0.31999999   0.74280006   0.31529999   0.82179993
    0.94912767]
 [164.29778388   0.17120001   0.68099993   0.39669999   0.74470001
    1.77191639]][0m
[37m[1m[2023-07-17 08:24:41,868][257371] Max Reward on eval: 247.41767500266434[0m
[37m[1m[2023-07-17 08:24:41,869][257371] Min Reward on eval: -181.76159287765623[0m
[37m[1m[2023-07-17 08:24:41,869][257371] Mean Reward across all agents: 100.5004974381449[0m
[37m[1m[2023-07-17 08:24:41,869][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:24:41,875][257371] mean_value=-144.4600545714693, max_value=170.3550210216085[0m
[37m[1m[2023-07-17 08:24:41,878][257371] New mean coefficients: [[0.38865587 0.7496984  0.06672426 0.04305696 0.31839448 0.24388382]][0m
[37m[1m[2023-07-17 08:24:41,879][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:24:50,994][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 08:24:50,994][257371] FPS: 421348.14[0m
[36m[2023-07-17 08:24:50,997][257371] itr=920, itrs=2000, Progress: 46.00%[0m
[37m[1m[2023-07-17 08:28:14,298][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000900[0m
[36m[2023-07-17 08:28:26,525][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 08:28:26,525][257371] FPS: 326274.50[0m
[36m[2023-07-17 08:28:30,776][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:28:30,781][257371] Reward + Measures: [[21.44623815  0.21757168  0.8544566   0.35861298  0.89449459  1.63567126]][0m
[37m[1m[2023-07-17 08:28:30,782][257371] Max Reward on eval: 21.44623815437936[0m
[37m[1m[2023-07-17 08:28:30,782][257371] Min Reward on eval: 21.44623815437936[0m
[37m[1m[2023-07-17 08:28:30,782][257371] Mean Reward across all agents: 21.44623815437936[0m
[37m[1m[2023-07-17 08:28:30,782][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:28:35,715][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:28:35,716][257371] Reward + Measures: [[ 88.17480517   0.20369999   0.67690003   0.37759998   0.74659997
    2.27297759]
 [124.36344243   0.40130001   0.66530001   0.3175       0.74520004
    1.58645749]
 [ 15.39845321   0.18900001   0.77899998   0.35709998   0.85839999
    1.64275932]
 ...
 [ 55.65281093   0.2066       0.74960005   0.354        0.81070006
    1.78809285]
 [ 13.91467504   0.40330002   0.73480004   0.41290003   0.6674
    1.38016951]
 [110.80819133   0.31710002   0.68129998   0.34190002   0.75740004
    1.54538155]][0m
[37m[1m[2023-07-17 08:28:35,716][257371] Max Reward on eval: 255.37853719154373[0m
[37m[1m[2023-07-17 08:28:35,716][257371] Min Reward on eval: -107.95838735783472[0m
[37m[1m[2023-07-17 08:28:35,717][257371] Mean Reward across all agents: 63.04780030455028[0m
[37m[1m[2023-07-17 08:28:35,717][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:28:35,720][257371] mean_value=-194.56731338209008, max_value=311.63022157667103[0m
[37m[1m[2023-07-17 08:28:35,723][257371] New mean coefficients: [[ 1.0747308   0.64492756  0.19073871  0.81800294  0.162645   -0.5806073 ]][0m
[37m[1m[2023-07-17 08:28:35,724][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:28:44,645][257371] train() took 8.92 seconds to complete[0m
[36m[2023-07-17 08:28:44,646][257371] FPS: 430488.47[0m
[36m[2023-07-17 08:28:44,648][257371] itr=921, itrs=2000, Progress: 46.05%[0m
[36m[2023-07-17 08:28:56,278][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 08:28:56,278][257371] FPS: 332999.72[0m
[36m[2023-07-17 08:29:00,573][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:29:00,573][257371] Reward + Measures: [[98.70893161  0.10290566  0.85636032  0.47735366  0.90389234  1.47653139]][0m
[37m[1m[2023-07-17 08:29:00,573][257371] Max Reward on eval: 98.70893161048942[0m
[37m[1m[2023-07-17 08:29:00,574][257371] Min Reward on eval: 98.70893161048942[0m
[37m[1m[2023-07-17 08:29:00,574][257371] Mean Reward across all agents: 98.70893161048942[0m
[37m[1m[2023-07-17 08:29:00,574][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:29:05,592][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:29:05,592][257371] Reward + Measures: [[135.97356127   0.48629999   0.85400003   0.0693       0.66769999
    1.87695026]
 [ 67.16781354   0.54960006   0.43179998   0.4364       0.46280003
    2.56842494]
 [165.68768407   0.3538       0.65560001   0.53659999   0.66180003
    2.24501538]
 ...
 [167.57239629   0.1912       0.63070005   0.221        0.7367
    2.142308  ]
 [ 97.69392968   0.38910002   0.74250001   0.2227       0.71329999
    2.01973104]
 [ 54.4382403    0.3989       0.43009996   0.37930003   0.38789997
    2.9087882 ]][0m
[37m[1m[2023-07-17 08:29:05,592][257371] Max Reward on eval: 252.04653030186893[0m
[37m[1m[2023-07-17 08:29:05,593][257371] Min Reward on eval: -52.16878011226654[0m
[37m[1m[2023-07-17 08:29:05,593][257371] Mean Reward across all agents: 110.49825783175494[0m
[37m[1m[2023-07-17 08:29:05,593][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:29:05,599][257371] mean_value=-170.57413022399234, max_value=509.2047034407984[0m
[37m[1m[2023-07-17 08:29:05,601][257371] New mean coefficients: [[ 1.1011305   0.5351963   0.41721362  0.5324841   0.29351085 -0.44636405]][0m
[37m[1m[2023-07-17 08:29:05,602][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:29:14,729][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 08:29:14,730][257371] FPS: 420795.34[0m
[36m[2023-07-17 08:29:14,732][257371] itr=922, itrs=2000, Progress: 46.10%[0m
[36m[2023-07-17 08:29:26,433][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 08:29:26,434][257371] FPS: 331149.51[0m
[36m[2023-07-17 08:29:30,669][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:29:30,669][257371] Reward + Measures: [[88.66144839  0.05748233  0.82102466  0.40833732  0.88485432  0.86419541]][0m
[37m[1m[2023-07-17 08:29:30,669][257371] Max Reward on eval: 88.66144839090516[0m
[37m[1m[2023-07-17 08:29:30,669][257371] Min Reward on eval: 88.66144839090516[0m
[37m[1m[2023-07-17 08:29:30,670][257371] Mean Reward across all agents: 88.66144839090516[0m
[37m[1m[2023-07-17 08:29:30,670][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:29:35,675][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:29:35,680][257371] Reward + Measures: [[ 84.187567     0.099        0.79980004   0.36950001   0.77910006
    1.34753644]
 [103.35221103   0.0244       0.74410003   0.40459999   0.82450002
    1.37929237]
 [ 68.7745612    0.0485       0.79319996   0.38569999   0.80400002
    1.2389524 ]
 ...
 [ 67.19821883   0.0974       0.89200002   0.26589999   0.89540005
    0.87136668]
 [ 78.05623342   0.007        0.74159998   0.54500002   0.83479995
    1.1626538 ]
 [103.85499765   0.23000002   0.80540001   0.34300002   0.87980002
    0.89466017]][0m
[37m[1m[2023-07-17 08:29:35,681][257371] Max Reward on eval: 180.52855111286044[0m
[37m[1m[2023-07-17 08:29:35,681][257371] Min Reward on eval: -48.05332067105919[0m
[37m[1m[2023-07-17 08:29:35,681][257371] Mean Reward across all agents: 68.71843335949679[0m
[37m[1m[2023-07-17 08:29:35,682][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:29:35,683][257371] mean_value=-192.40649310068787, max_value=-46.12517014310231[0m
[36m[2023-07-17 08:29:35,686][257371] XNES is restarting with a new solution whose measures are [0.1714     0.1779     0.1753     0.0277     2.24034548] and objective is 6287.657592774276[0m
[36m[2023-07-17 08:29:35,687][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 08:29:35,689][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 08:29:35,690][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:29:44,666][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 08:29:44,666][257371] FPS: 427898.19[0m
[36m[2023-07-17 08:29:44,668][257371] itr=923, itrs=2000, Progress: 46.15%[0m
[36m[2023-07-17 08:29:56,704][257371] train() took 11.93 seconds to complete[0m
[36m[2023-07-17 08:29:56,704][257371] FPS: 321829.05[0m
[36m[2023-07-17 08:30:01,310][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:30:01,310][257371] Reward + Measures: [[880.54574818   0.09330367   0.12494033   0.08535166   0.05170267
    1.36744857]][0m
[37m[1m[2023-07-17 08:30:01,311][257371] Max Reward on eval: 880.5457481772895[0m
[37m[1m[2023-07-17 08:30:01,311][257371] Min Reward on eval: 880.5457481772895[0m
[37m[1m[2023-07-17 08:30:01,311][257371] Mean Reward across all agents: 880.5457481772895[0m
[37m[1m[2023-07-17 08:30:01,312][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:30:06,310][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:30:06,316][257371] Reward + Measures: [[2328.27719116    0.18529999    0.33800003    0.2493        0.0926
     2.31743813]
 [ 419.63224508    0.0776        0.092         0.0648        0.06130001
     1.70856857]
 [ 831.439022      0.0788        0.132         0.0891        0.0493
     2.10598207]
 ...
 [1512.30656912    0.126         0.2014        0.1754        0.068
     1.92593479]
 [1640.38819122    0.1349        0.24259999    0.1902        0.0768
     2.0984323 ]
 [ 658.93628974    0.0891        0.1163        0.0831        0.0517
     2.02540398]][0m
[37m[1m[2023-07-17 08:30:06,317][257371] Max Reward on eval: 2345.0128574322907[0m
[37m[1m[2023-07-17 08:30:06,317][257371] Min Reward on eval: 286.42312292680145[0m
[37m[1m[2023-07-17 08:30:06,317][257371] Mean Reward across all agents: 1156.3377182808115[0m
[37m[1m[2023-07-17 08:30:06,317][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:30:06,319][257371] mean_value=-4853.57559882524, max_value=1604.9671459060705[0m
[37m[1m[2023-07-17 08:30:06,322][257371] New mean coefficients: [[-1.007344    1.3024255   0.19443226 -2.740911   -3.006664    0.7608272 ]][0m
[37m[1m[2023-07-17 08:30:06,322][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:30:15,356][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 08:30:15,356][257371] FPS: 425171.03[0m
[36m[2023-07-17 08:30:15,358][257371] itr=924, itrs=2000, Progress: 46.20%[0m
[36m[2023-07-17 08:30:27,131][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 08:30:27,131][257371] FPS: 329116.03[0m
[36m[2023-07-17 08:30:31,454][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:30:31,455][257371] Reward + Measures: [[738.52023998   0.09023833   0.11851599   0.07896134   0.05054333
    1.38052607]][0m
[37m[1m[2023-07-17 08:30:31,455][257371] Max Reward on eval: 738.5202399846312[0m
[37m[1m[2023-07-17 08:30:31,455][257371] Min Reward on eval: 738.5202399846312[0m
[37m[1m[2023-07-17 08:30:31,455][257371] Mean Reward across all agents: 738.5202399846312[0m
[37m[1m[2023-07-17 08:30:31,456][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:30:36,439][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:30:36,439][257371] Reward + Measures: [[ 889.28314975    0.12539999    0.18810001    0.12800001    0.0764
     2.71857119]
 [1208.58698076    0.12190001    0.17340001    0.1045        0.0521
     1.8032254 ]
 [ 843.20301724    0.16000001    0.18930002    0.1348        0.103
     2.80022883]
 ...
 [ 445.1975765     0.0786        0.102         0.0804        0.0538
     2.34821677]
 [1214.08341214    0.19099998    0.227         0.24630001    0.1287
     2.85847259]
 [ 884.4441166     0.10030001    0.1382        0.099         0.0523
     1.70892203]][0m
[37m[1m[2023-07-17 08:30:36,439][257371] Max Reward on eval: 2181.0117301996797[0m
[37m[1m[2023-07-17 08:30:36,440][257371] Min Reward on eval: 445.197576501593[0m
[37m[1m[2023-07-17 08:30:36,440][257371] Mean Reward across all agents: 1105.0959640481924[0m
[37m[1m[2023-07-17 08:30:36,440][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:30:36,441][257371] mean_value=-5002.780468305888, max_value=1045.1473907622253[0m
[37m[1m[2023-07-17 08:30:36,443][257371] New mean coefficients: [[-2.1501827   3.4480667  -0.36559057 -2.196889   -4.2240057   1.4116404 ]][0m
[37m[1m[2023-07-17 08:30:36,444][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:30:45,452][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 08:30:45,452][257371] FPS: 426353.93[0m
[36m[2023-07-17 08:30:45,455][257371] itr=925, itrs=2000, Progress: 46.25%[0m
[36m[2023-07-17 08:30:57,327][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 08:30:57,327][257371] FPS: 326229.09[0m
[36m[2023-07-17 08:31:01,732][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:31:01,732][257371] Reward + Measures: [[605.14786543   0.08605467   0.111461     0.07433233   0.050572
    1.41326773]][0m
[37m[1m[2023-07-17 08:31:01,732][257371] Max Reward on eval: 605.1478654324823[0m
[37m[1m[2023-07-17 08:31:01,733][257371] Min Reward on eval: 605.1478654324823[0m
[37m[1m[2023-07-17 08:31:01,733][257371] Mean Reward across all agents: 605.1478654324823[0m
[37m[1m[2023-07-17 08:31:01,733][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:31:06,752][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:31:06,753][257371] Reward + Measures: [[ 440.85647726    0.0957        0.1206        0.09769999    0.0617
     2.49215817]
 [ 756.21454718    0.16440001    0.1908        0.16600001    0.1213
     2.39685631]
 [ 581.89361191    0.1181        0.13570002    0.1175        0.07390001
     2.53854918]
 ...
 [ 912.9320832     0.14960001    0.19240001    0.18020001    0.0867
     2.78965378]
 [1104.69136812    0.14590001    0.20190001    0.1286        0.0673
     1.672212  ]
 [ 611.84850121    0.1054        0.1415        0.13630001    0.07300001
     2.18455505]][0m
[37m[1m[2023-07-17 08:31:06,753][257371] Max Reward on eval: 1674.0105056799948[0m
[37m[1m[2023-07-17 08:31:06,753][257371] Min Reward on eval: 277.2012272357941[0m
[37m[1m[2023-07-17 08:31:06,754][257371] Mean Reward across all agents: 832.085467470766[0m
[37m[1m[2023-07-17 08:31:06,754][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:31:06,755][257371] mean_value=-5316.322410366099, max_value=-2468.660760798463[0m
[36m[2023-07-17 08:31:06,757][257371] XNES is restarting with a new solution whose measures are [0.55470002 0.4086     0.509      0.47419998 3.10878253] and objective is 38.89643708127551[0m
[36m[2023-07-17 08:31:06,758][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 08:31:06,761][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 08:31:06,762][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:31:15,797][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 08:31:15,798][257371] FPS: 425045.72[0m
[36m[2023-07-17 08:31:15,800][257371] itr=926, itrs=2000, Progress: 46.30%[0m
[36m[2023-07-17 08:31:27,541][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 08:31:27,541][257371] FPS: 329986.28[0m
[36m[2023-07-17 08:31:31,897][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:31:31,903][257371] Reward + Measures: [[-47.32858351   0.56962568   0.59868801   0.59864599   0.61285633
    3.25761724]][0m
[37m[1m[2023-07-17 08:31:31,903][257371] Max Reward on eval: -47.32858350764778[0m
[37m[1m[2023-07-17 08:31:31,903][257371] Min Reward on eval: -47.32858350764778[0m
[37m[1m[2023-07-17 08:31:31,904][257371] Mean Reward across all agents: -47.32858350764778[0m
[37m[1m[2023-07-17 08:31:31,904][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:31:36,915][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:31:36,920][257371] Reward + Measures: [[ 21.91395305   0.24089999   0.39990002   0.25079998   0.36989999
    4.78788614]
 [-64.20422847   0.25419998   0.29679999   0.2872       0.30960003
    3.91392016]
 [155.13977835   0.2737       0.24660002   0.28079998   0.17
    4.35504484]
 ...
 [-17.84555555   0.1894       0.2131       0.24170001   0.22189999
    3.88838053]
 [ 75.71679331   0.32710001   0.27770001   0.4391       0.32969999
    3.2714231 ]
 [142.10120485   0.22090001   0.30099997   0.36980003   0.35390002
    4.2574029 ]][0m
[37m[1m[2023-07-17 08:31:36,921][257371] Max Reward on eval: 205.34252636656166[0m
[37m[1m[2023-07-17 08:31:36,921][257371] Min Reward on eval: -296.906577610597[0m
[37m[1m[2023-07-17 08:31:36,921][257371] Mean Reward across all agents: 18.690663676245332[0m
[37m[1m[2023-07-17 08:31:36,922][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:31:36,923][257371] mean_value=-687.7068289479761, max_value=48.682563696577766[0m
[37m[1m[2023-07-17 08:31:36,926][257371] New mean coefficients: [[ 0.1576181 -0.5422967 -0.7317208 -1.3392057 -1.3116816 -0.945563 ]][0m
[37m[1m[2023-07-17 08:31:36,927][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:31:46,005][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 08:31:46,006][257371] FPS: 423045.43[0m
[36m[2023-07-17 08:31:46,008][257371] itr=927, itrs=2000, Progress: 46.35%[0m
[36m[2023-07-17 08:31:57,737][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 08:31:57,737][257371] FPS: 330185.21[0m
[36m[2023-07-17 08:32:02,089][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:32:02,090][257371] Reward + Measures: [[23.16298142  0.35772565  0.36765429  0.33683369  0.38239202  2.98167896]][0m
[37m[1m[2023-07-17 08:32:02,090][257371] Max Reward on eval: 23.162981423975435[0m
[37m[1m[2023-07-17 08:32:02,090][257371] Min Reward on eval: 23.162981423975435[0m
[37m[1m[2023-07-17 08:32:02,091][257371] Mean Reward across all agents: 23.162981423975435[0m
[37m[1m[2023-07-17 08:32:02,091][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:32:07,094][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:32:07,099][257371] Reward + Measures: [[ 23.83729889   0.35800001   0.23519997   0.3594       0.21360002
    5.47591591]
 [-78.41718343   0.18920001   0.1736       0.1797       0.1079
    5.52262878]
 [-43.09799635   0.13930002   0.133        0.17739999   0.1635
    5.73490238]
 ...
 [-77.85339786   0.20799999   0.27069998   0.24250002   0.25970003
    4.58884048]
 [-14.76250843   0.28550002   0.29780003   0.29510003   0.31459999
    6.12707233]
 [  6.97385984   0.2053       0.59740001   0.2172       0.5618
    4.97530079]][0m
[37m[1m[2023-07-17 08:32:07,100][257371] Max Reward on eval: 380.2875971599715[0m
[37m[1m[2023-07-17 08:32:07,100][257371] Min Reward on eval: -488.74840978235005[0m
[37m[1m[2023-07-17 08:32:07,100][257371] Mean Reward across all agents: 6.6851482919079475[0m
[37m[1m[2023-07-17 08:32:07,100][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:32:07,101][257371] mean_value=-759.259395623122, max_value=119.31982979730935[0m
[37m[1m[2023-07-17 08:32:07,104][257371] New mean coefficients: [[-0.33337775 -0.02576363 -1.5299038  -1.9936293  -1.1587118  -0.5035738 ]][0m
[37m[1m[2023-07-17 08:32:07,104][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:32:16,205][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 08:32:16,206][257371] FPS: 422019.03[0m
[36m[2023-07-17 08:32:16,208][257371] itr=928, itrs=2000, Progress: 46.40%[0m
[36m[2023-07-17 08:32:28,027][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 08:32:28,027][257371] FPS: 327687.88[0m
[36m[2023-07-17 08:32:32,405][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:32:32,405][257371] Reward + Measures: [[58.10883622  0.30227369  0.46188068  0.27200699  0.45404369  3.07017779]][0m
[37m[1m[2023-07-17 08:32:32,405][257371] Max Reward on eval: 58.10883622024247[0m
[37m[1m[2023-07-17 08:32:32,406][257371] Min Reward on eval: 58.10883622024247[0m
[37m[1m[2023-07-17 08:32:32,406][257371] Mean Reward across all agents: 58.10883622024247[0m
[37m[1m[2023-07-17 08:32:32,406][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:32:37,660][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:32:37,666][257371] Reward + Measures: [[  49.17282005    0.1283        0.1425        0.06960001    0.116
     5.45248127]
 [-131.16040739    0.221         0.2474        0.25760001    0.30419999
     5.91092825]
 [  45.94157636    0.24150001    0.35520002    0.17420001    0.31030002
     4.81299543]
 ...
 [ -26.26119631    0.1358        0.16680001    0.1219        0.1166
     5.43381262]
 [ -72.01870274    0.2203        0.1707        0.21729998    0.18800001
     4.39405918]
 [  35.29727792    0.15970002    0.18249999    0.1079        0.161
     5.55503273]][0m
[37m[1m[2023-07-17 08:32:37,667][257371] Max Reward on eval: 161.35011478774248[0m
[37m[1m[2023-07-17 08:32:37,667][257371] Min Reward on eval: -281.2322826120304[0m
[37m[1m[2023-07-17 08:32:37,667][257371] Mean Reward across all agents: -6.053457765276175[0m
[37m[1m[2023-07-17 08:32:37,668][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:32:37,668][257371] mean_value=-617.9850617216772, max_value=-13.062286919031095[0m
[36m[2023-07-17 08:32:37,671][257371] XNES is restarting with a new solution whose measures are [0.2802     0.64199996 0.22350001 0.65740007 3.58040357] and objective is 209.66836263686417[0m
[36m[2023-07-17 08:32:37,672][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 08:32:37,674][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 08:32:37,675][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:32:46,734][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 08:32:46,734][257371] FPS: 423953.79[0m
[36m[2023-07-17 08:32:46,737][257371] itr=929, itrs=2000, Progress: 46.45%[0m
[36m[2023-07-17 08:32:58,659][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 08:32:58,659][257371] FPS: 324797.85[0m
[36m[2023-07-17 08:33:02,970][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:33:02,976][257371] Reward + Measures: [[144.23512833   0.24686866   0.49557498   0.22145365   0.51769036
    3.29754782]][0m
[37m[1m[2023-07-17 08:33:02,976][257371] Max Reward on eval: 144.2351283316105[0m
[37m[1m[2023-07-17 08:33:02,976][257371] Min Reward on eval: 144.2351283316105[0m
[37m[1m[2023-07-17 08:33:02,977][257371] Mean Reward across all agents: 144.2351283316105[0m
[37m[1m[2023-07-17 08:33:02,977][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:33:07,996][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:33:08,002][257371] Reward + Measures: [[128.62593117   0.25620002   0.48210001   0.22729997   0.51520002
    3.28797007]
 [ 85.19274662   0.19509999   0.43520004   0.21110001   0.46199998
    3.39550447]
 [140.4634197    0.23169999   0.4756       0.25349998   0.50490004
    3.31049275]
 ...
 [134.191809     0.25489998   0.49859998   0.24199997   0.52390003
    3.2249105 ]
 [123.10037091   0.2667       0.51860005   0.23699999   0.55220002
    3.27506495]
 [120.80229422   0.20650001   0.42130002   0.20960002   0.45380002
    3.4212296 ]][0m
[37m[1m[2023-07-17 08:33:08,002][257371] Max Reward on eval: 234.89731986057012[0m
[37m[1m[2023-07-17 08:33:08,002][257371] Min Reward on eval: 21.51158500285819[0m
[37m[1m[2023-07-17 08:33:08,003][257371] Mean Reward across all agents: 114.44676380102707[0m
[37m[1m[2023-07-17 08:33:08,003][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:33:08,004][257371] mean_value=-250.5804875400187, max_value=-27.46028394285517[0m
[36m[2023-07-17 08:33:08,007][257371] XNES is restarting with a new solution whose measures are [0.91400003 0.5176     0.92519999 0.06810001 6.13731146] and objective is 362.76368309110404[0m
[36m[2023-07-17 08:33:08,008][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 08:33:08,010][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 08:33:08,011][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:33:17,112][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 08:33:17,113][257371] FPS: 421964.80[0m
[36m[2023-07-17 08:33:17,115][257371] itr=930, itrs=2000, Progress: 46.50%[0m
[37m[1m[2023-07-17 08:36:37,226][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000910[0m
[36m[2023-07-17 08:36:49,850][257371] train() took 11.88 seconds to complete[0m
[36m[2023-07-17 08:36:49,851][257371] FPS: 323276.22[0m
[36m[2023-07-17 08:36:54,030][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:36:54,030][257371] Reward + Measures: [[141.57795222   0.70461422   0.41762334   0.68673903   0.18965067
    5.77934933]][0m
[37m[1m[2023-07-17 08:36:54,030][257371] Max Reward on eval: 141.57795222133453[0m
[37m[1m[2023-07-17 08:36:54,031][257371] Min Reward on eval: 141.57795222133453[0m
[37m[1m[2023-07-17 08:36:54,031][257371] Mean Reward across all agents: 141.57795222133453[0m
[37m[1m[2023-07-17 08:36:54,031][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:36:58,985][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:36:58,985][257371] Reward + Measures: [[ 75.83374346   0.25719997   0.1358       0.3213       0.22520001
    5.5237422 ]
 [ 81.84230705   0.3488       0.34129998   0.37820002   0.33580002
    5.73129034]
 [ 24.4437463    0.44670001   0.22160001   0.43430001   0.22320001
    5.08811903]
 ...
 [ -3.32334993   0.28820002   0.24240001   0.26809999   0.26980001
    4.37952662]
 [ 34.47336692   0.38429999   0.25940001   0.3883       0.25190002
    4.32351208]
 [-25.35492646   0.2827       0.17119999   0.24749999   0.18730001
    4.84039354]][0m
[37m[1m[2023-07-17 08:36:58,986][257371] Max Reward on eval: 496.6642265021801[0m
[37m[1m[2023-07-17 08:36:58,986][257371] Min Reward on eval: -470.8559560919181[0m
[37m[1m[2023-07-17 08:36:58,986][257371] Mean Reward across all agents: 2.5750131439694286[0m
[37m[1m[2023-07-17 08:36:58,986][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:36:58,989][257371] mean_value=-332.9919276442401, max_value=373.77587952236706[0m
[37m[1m[2023-07-17 08:36:58,996][257371] New mean coefficients: [[-0.95932305 -0.85763896 -0.08819175 -1.488944   -0.7084463  -1.078816  ]][0m
[37m[1m[2023-07-17 08:36:58,997][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:37:08,015][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 08:37:08,016][257371] FPS: 425875.12[0m
[36m[2023-07-17 08:37:08,018][257371] itr=931, itrs=2000, Progress: 46.55%[0m
[36m[2023-07-17 08:37:19,892][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 08:37:19,892][257371] FPS: 326295.84[0m
[36m[2023-07-17 08:37:24,134][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:37:24,135][257371] Reward + Measures: [[120.70585491   0.66119868   0.38196501   0.65072864   0.19774601
    5.69143677]][0m
[37m[1m[2023-07-17 08:37:24,135][257371] Max Reward on eval: 120.7058549086707[0m
[37m[1m[2023-07-17 08:37:24,135][257371] Min Reward on eval: 120.7058549086707[0m
[37m[1m[2023-07-17 08:37:24,136][257371] Mean Reward across all agents: 120.7058549086707[0m
[37m[1m[2023-07-17 08:37:24,136][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:37:29,300][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:37:29,309][257371] Reward + Measures: [[  78.83138305    0.3741        0.22650002    0.3294        0.33880001
     4.44530249]
 [ -83.46049238    0.40149999    0.40570003    0.35880002    0.52610004
     5.24346638]
 [-102.82945201    0.13950001    0.68100005    0.4745        0.72439998
     6.28875685]
 ...
 [  93.73923121    0.57440007    0.14420001    0.56          0.52759999
     5.34981632]
 [  23.7880512     0.16589999    0.1424        0.14350002    0.18419999
     4.46614647]
 [ -21.49170045    0.47400004    0.37680003    0.35820004    0.44549999
     4.64122152]][0m
[37m[1m[2023-07-17 08:37:29,310][257371] Max Reward on eval: 440.86269757971166[0m
[37m[1m[2023-07-17 08:37:29,310][257371] Min Reward on eval: -386.03406909443436[0m
[37m[1m[2023-07-17 08:37:29,310][257371] Mean Reward across all agents: -9.46836658112344[0m
[37m[1m[2023-07-17 08:37:29,311][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:37:29,314][257371] mean_value=-321.4999476996787, max_value=528.0082941525616[0m
[37m[1m[2023-07-17 08:37:29,317][257371] New mean coefficients: [[-1.129225   -0.6743357  -0.35446423 -0.45980382 -1.4055703  -1.2667342 ]][0m
[37m[1m[2023-07-17 08:37:29,318][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:37:38,291][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 08:37:38,291][257371] FPS: 428000.77[0m
[36m[2023-07-17 08:37:38,294][257371] itr=932, itrs=2000, Progress: 46.60%[0m
[36m[2023-07-17 08:37:50,075][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 08:37:50,075][257371] FPS: 328861.89[0m
[36m[2023-07-17 08:37:54,380][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:37:54,381][257371] Reward + Measures: [[99.61392139  0.63569903  0.36489567  0.62425929  0.21012834  5.61699486]][0m
[37m[1m[2023-07-17 08:37:54,381][257371] Max Reward on eval: 99.61392139182776[0m
[37m[1m[2023-07-17 08:37:54,381][257371] Min Reward on eval: 99.61392139182776[0m
[37m[1m[2023-07-17 08:37:54,382][257371] Mean Reward across all agents: 99.61392139182776[0m
[37m[1m[2023-07-17 08:37:54,382][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:37:59,438][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:37:59,438][257371] Reward + Measures: [[117.1703529    0.31370002   0.34370002   0.28819999   0.40219998
    4.05218744]
 [ 55.83758909   0.0859       0.0948       0.0884       0.08490001
    5.96159267]
 [-33.86836112   0.46370003   0.11059999   0.48650002   0.42799997
    4.80568647]
 ...
 [ -8.71137385   0.163        0.24180003   0.1723       0.28100005
    4.4216857 ]
 [ 25.41177573   0.19220001   0.12100001   0.19579999   0.16839999
    5.06582594]
 [ 24.8656876    0.42950001   0.2457       0.37810001   0.36209998
    4.77544165]][0m
[37m[1m[2023-07-17 08:37:59,438][257371] Max Reward on eval: 616.4032173004001[0m
[37m[1m[2023-07-17 08:37:59,439][257371] Min Reward on eval: -678.0021858073771[0m
[37m[1m[2023-07-17 08:37:59,439][257371] Mean Reward across all agents: 28.10990215246967[0m
[37m[1m[2023-07-17 08:37:59,439][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:37:59,442][257371] mean_value=-781.5230554405604, max_value=509.655077601519[0m
[37m[1m[2023-07-17 08:37:59,445][257371] New mean coefficients: [[ 0.7103064  -0.6055982   0.46030176 -0.8955863  -0.282408   -1.3189015 ]][0m
[37m[1m[2023-07-17 08:37:59,446][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:38:08,514][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 08:38:08,514][257371] FPS: 423539.35[0m
[36m[2023-07-17 08:38:08,517][257371] itr=933, itrs=2000, Progress: 46.65%[0m
[36m[2023-07-17 08:38:20,308][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 08:38:20,309][257371] FPS: 328525.20[0m
[36m[2023-07-17 08:38:24,646][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:38:24,646][257371] Reward + Measures: [[84.32999168  0.60124737  0.34038532  0.59864837  0.21388333  5.53760529]][0m
[37m[1m[2023-07-17 08:38:24,647][257371] Max Reward on eval: 84.32999167732629[0m
[37m[1m[2023-07-17 08:38:24,647][257371] Min Reward on eval: 84.32999167732629[0m
[37m[1m[2023-07-17 08:38:24,647][257371] Mean Reward across all agents: 84.32999167732629[0m
[37m[1m[2023-07-17 08:38:24,647][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:38:29,645][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:38:29,646][257371] Reward + Measures: [[-112.49238011    0.3362        0.27329999    0.4962        0.41570002
     5.38473272]
 [   6.22121507    0.41869998    0.33870003    0.48059997    0.43520004
     5.5281539 ]
 [  30.05435071    0.46730003    0.39690003    0.43360001    0.46269998
     6.35665846]
 ...
 [ -15.37294072    0.34490001    0.3274        0.1461        0.41540003
     5.71566534]
 [ 124.65790179    0.42860004    0.192         0.42210004    0.39930001
     5.01817656]
 [  89.7031093     0.75500005    0.0902        0.62910002    0.639
     6.09907484]][0m
[37m[1m[2023-07-17 08:38:29,646][257371] Max Reward on eval: 613.0155143715441[0m
[37m[1m[2023-07-17 08:38:29,646][257371] Min Reward on eval: -457.19957345575096[0m
[37m[1m[2023-07-17 08:38:29,647][257371] Mean Reward across all agents: -4.0053100818737475[0m
[37m[1m[2023-07-17 08:38:29,647][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:38:29,651][257371] mean_value=-233.54443749713192, max_value=707.248872122309[0m
[37m[1m[2023-07-17 08:38:29,653][257371] New mean coefficients: [[ 0.8499425   0.40158498  0.07850996 -1.3070633  -0.10162362 -1.5785064 ]][0m
[37m[1m[2023-07-17 08:38:29,654][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:38:38,757][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 08:38:38,757][257371] FPS: 421947.13[0m
[36m[2023-07-17 08:38:38,759][257371] itr=934, itrs=2000, Progress: 46.70%[0m
[36m[2023-07-17 08:38:50,715][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-17 08:38:50,716][257371] FPS: 324048.43[0m
[36m[2023-07-17 08:38:55,101][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:38:55,101][257371] Reward + Measures: [[78.83587401  0.57630169  0.32428235  0.57855362  0.21389332  5.45081234]][0m
[37m[1m[2023-07-17 08:38:55,102][257371] Max Reward on eval: 78.83587400564093[0m
[37m[1m[2023-07-17 08:38:55,102][257371] Min Reward on eval: 78.83587400564093[0m
[37m[1m[2023-07-17 08:38:55,102][257371] Mean Reward across all agents: 78.83587400564093[0m
[37m[1m[2023-07-17 08:38:55,102][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:39:00,105][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:39:00,106][257371] Reward + Measures: [[ 274.33472353    0.68950003    0.41409999    0.71650004    0.233
     6.33629179]
 [ 147.07757474    0.64379996    0.78150004    0.0779        0.75059998
     6.06765175]
 [ -25.09878612    0.287         0.35600001    0.32790002    0.40920001
     5.31076527]
 ...
 [-122.46280388    0.5535        0.08200001    0.6347        0.58270007
     6.61809015]
 [ -17.25688036    0.4454        0.5108        0.4894        0.58120006
     5.37022972]
 [ 104.63107248    0.29770002    0.29620001    0.3125        0.2339
     4.69894505]][0m
[37m[1m[2023-07-17 08:39:00,106][257371] Max Reward on eval: 579.299880997464[0m
[37m[1m[2023-07-17 08:39:00,106][257371] Min Reward on eval: -502.2125139201991[0m
[37m[1m[2023-07-17 08:39:00,107][257371] Mean Reward across all agents: 31.716325523200478[0m
[37m[1m[2023-07-17 08:39:00,107][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:39:00,111][257371] mean_value=-314.91925248836907, max_value=301.4732722879759[0m
[37m[1m[2023-07-17 08:39:00,113][257371] New mean coefficients: [[ 0.00263834  1.1965992  -0.54727006 -1.9488423  -0.30530545 -1.4632187 ]][0m
[37m[1m[2023-07-17 08:39:00,114][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:39:09,144][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 08:39:09,145][257371] FPS: 425324.58[0m
[36m[2023-07-17 08:39:09,147][257371] itr=935, itrs=2000, Progress: 46.75%[0m
[36m[2023-07-17 08:39:21,235][257371] train() took 11.98 seconds to complete[0m
[36m[2023-07-17 08:39:21,235][257371] FPS: 320509.44[0m
[36m[2023-07-17 08:39:25,585][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:39:25,585][257371] Reward + Measures: [[63.12260125  0.52277195  0.29505533  0.52670598  0.22015366  5.31256962]][0m
[37m[1m[2023-07-17 08:39:25,585][257371] Max Reward on eval: 63.122601247648284[0m
[37m[1m[2023-07-17 08:39:25,586][257371] Min Reward on eval: 63.122601247648284[0m
[37m[1m[2023-07-17 08:39:25,586][257371] Mean Reward across all agents: 63.122601247648284[0m
[37m[1m[2023-07-17 08:39:25,586][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:39:30,557][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:39:30,557][257371] Reward + Measures: [[ 27.44021209   0.31119999   0.3026       0.29679999   0.3019
    4.24062872]
 [ 33.11897655   0.54570001   0.20449999   0.56639999   0.54619998
    5.13782978]
 [ 95.29596483   0.87509996   0.34340003   0.8846001    0.53070003
    7.74194288]
 ...
 [-30.43751481   0.8434       0.29029998   0.83380002   0.49109998
    7.84055328]
 [195.54237173   0.71799999   0.2447       0.71750003   0.50959998
    6.64996672]
 [103.11650789   0.16430001   0.16870001   0.17         0.16290002
    5.58648252]][0m
[37m[1m[2023-07-17 08:39:30,558][257371] Max Reward on eval: 608.1613388087601[0m
[37m[1m[2023-07-17 08:39:30,558][257371] Min Reward on eval: -413.1769423393533[0m
[37m[1m[2023-07-17 08:39:30,558][257371] Mean Reward across all agents: 74.287881638368[0m
[37m[1m[2023-07-17 08:39:30,558][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:39:30,563][257371] mean_value=-376.2714349826537, max_value=526.631172593027[0m
[37m[1m[2023-07-17 08:39:30,566][257371] New mean coefficients: [[-0.3118765   2.4393148  -0.9343338  -1.4619865  -0.31391212 -1.1530504 ]][0m
[37m[1m[2023-07-17 08:39:30,566][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:39:39,608][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 08:39:39,608][257371] FPS: 424795.12[0m
[36m[2023-07-17 08:39:39,610][257371] itr=936, itrs=2000, Progress: 46.80%[0m
[36m[2023-07-17 08:39:51,307][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 08:39:51,308][257371] FPS: 331243.42[0m
[36m[2023-07-17 08:39:55,636][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:39:55,636][257371] Reward + Measures: [[52.99637705  0.49044099  0.26763234  0.49434432  0.22714767  5.22862387]][0m
[37m[1m[2023-07-17 08:39:55,637][257371] Max Reward on eval: 52.996377045566994[0m
[37m[1m[2023-07-17 08:39:55,637][257371] Min Reward on eval: 52.996377045566994[0m
[37m[1m[2023-07-17 08:39:55,637][257371] Mean Reward across all agents: 52.996377045566994[0m
[37m[1m[2023-07-17 08:39:55,637][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:40:00,702][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:40:00,703][257371] Reward + Measures: [[ 282.61386251    0.95450002    0.58089995    0.95650005    0.19990002
     7.85683966]
 [   5.05392819    0.23459999    0.0927        0.28850001    0.21999998
     5.2324872 ]
 [-164.44546679    0.59940004    0.1754        0.58649999    0.56279999
     6.0718174 ]
 ...
 [-103.20043655    0.19579999    0.64899999    0.72610003    0.7398001
     7.24196959]
 [-115.84801061    0.2414        0.34660003    0.29340002    0.46030003
     4.99919033]
 [ -57.41353486    0.45269999    0.17120001    0.42150003    0.26800001
     4.68451643]][0m
[37m[1m[2023-07-17 08:40:00,703][257371] Max Reward on eval: 605.7347984308377[0m
[37m[1m[2023-07-17 08:40:00,703][257371] Min Reward on eval: -709.4118918932975[0m
[37m[1m[2023-07-17 08:40:00,704][257371] Mean Reward across all agents: -16.036038710782865[0m
[37m[1m[2023-07-17 08:40:00,704][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:40:00,708][257371] mean_value=-304.3660359571822, max_value=527.3396644819528[0m
[37m[1m[2023-07-17 08:40:00,710][257371] New mean coefficients: [[ 0.5787257   2.8485336  -0.58080703 -1.5215006  -0.5382833  -1.0093389 ]][0m
[37m[1m[2023-07-17 08:40:00,711][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:40:09,769][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 08:40:09,769][257371] FPS: 424030.51[0m
[36m[2023-07-17 08:40:09,772][257371] itr=937, itrs=2000, Progress: 46.85%[0m
[36m[2023-07-17 08:40:21,577][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 08:40:21,577][257371] FPS: 328176.71[0m
[36m[2023-07-17 08:40:25,880][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:40:25,881][257371] Reward + Measures: [[56.37787347  0.46894369  0.25499299  0.4702473   0.23396565  5.18056488]][0m
[37m[1m[2023-07-17 08:40:25,881][257371] Max Reward on eval: 56.377873474535456[0m
[37m[1m[2023-07-17 08:40:25,881][257371] Min Reward on eval: 56.377873474535456[0m
[37m[1m[2023-07-17 08:40:25,881][257371] Mean Reward across all agents: 56.377873474535456[0m
[37m[1m[2023-07-17 08:40:25,882][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:40:31,144][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:40:31,144][257371] Reward + Measures: [[ 84.45441033   0.81870002   0.10390001   0.84810001   0.67220002
    6.0650425 ]
 [ 38.35042614   0.53760004   0.17819999   0.55470008   0.24010001
    5.52086496]
 [ 57.20403214   0.67189997   0.15270001   0.63430005   0.38820001
    6.3904171 ]
 ...
 [-30.2939549    0.45259997   0.25260001   0.45490003   0.1232
    5.8305831 ]
 [-84.10843613   0.77460003   0.71560001   0.764        0.7719
    6.47232676]
 [ 68.376727     0.22490001   0.249        0.21900001   0.24700002
    4.36989641]][0m
[37m[1m[2023-07-17 08:40:31,145][257371] Max Reward on eval: 393.51307943686845[0m
[37m[1m[2023-07-17 08:40:31,145][257371] Min Reward on eval: -602.3107237547636[0m
[37m[1m[2023-07-17 08:40:31,145][257371] Mean Reward across all agents: -6.123429405875483[0m
[37m[1m[2023-07-17 08:40:31,145][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:40:31,149][257371] mean_value=-270.0171130086714, max_value=488.5286002313574[0m
[37m[1m[2023-07-17 08:40:31,152][257371] New mean coefficients: [[ 0.49393976  2.3603787  -0.28919595 -2.0650926  -0.66654944 -1.9104228 ]][0m
[37m[1m[2023-07-17 08:40:31,152][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:40:40,212][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 08:40:40,212][257371] FPS: 423963.12[0m
[36m[2023-07-17 08:40:40,214][257371] itr=938, itrs=2000, Progress: 46.90%[0m
[36m[2023-07-17 08:40:52,001][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 08:40:52,001][257371] FPS: 328716.03[0m
[36m[2023-07-17 08:40:56,309][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:40:56,310][257371] Reward + Measures: [[56.08186818  0.44831929  0.24613534  0.45015466  0.238281    5.11657381]][0m
[37m[1m[2023-07-17 08:40:56,310][257371] Max Reward on eval: 56.08186818462276[0m
[37m[1m[2023-07-17 08:40:56,310][257371] Min Reward on eval: 56.08186818462276[0m
[37m[1m[2023-07-17 08:40:56,311][257371] Mean Reward across all agents: 56.08186818462276[0m
[37m[1m[2023-07-17 08:40:56,311][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:41:01,286][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:41:01,287][257371] Reward + Measures: [[-44.47902205   0.81980002   0.064        0.84720004   0.77310002
    7.04438639]
 [598.60432817   0.90690005   0.40039998   0.87750006   0.317
    6.30900526]
 [ -8.88526497   0.2773       0.198        0.26159999   0.2586
    4.17237425]
 ...
 [-27.88993011   0.87980002   0.0527       0.90230006   0.8592
    6.85959339]
 [  6.71846411   0.46079999   0.1538       0.42439994   0.38170001
    5.49270725]
 [ -3.14884827   0.25010002   0.14050001   0.19170001   0.22089998
    5.0765214 ]][0m
[37m[1m[2023-07-17 08:41:01,287][257371] Max Reward on eval: 598.604328167066[0m
[37m[1m[2023-07-17 08:41:01,287][257371] Min Reward on eval: -508.3661303612404[0m
[37m[1m[2023-07-17 08:41:01,288][257371] Mean Reward across all agents: -4.1994134226761535[0m
[37m[1m[2023-07-17 08:41:01,288][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:41:01,291][257371] mean_value=-329.8937772202104, max_value=411.91353551163934[0m
[37m[1m[2023-07-17 08:41:01,293][257371] New mean coefficients: [[ 1.0310829   2.0497348  -0.97019094 -2.121069   -0.91120636 -1.9203368 ]][0m
[37m[1m[2023-07-17 08:41:01,294][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:41:10,274][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 08:41:10,275][257371] FPS: 427668.10[0m
[36m[2023-07-17 08:41:10,277][257371] itr=939, itrs=2000, Progress: 46.95%[0m
[36m[2023-07-17 08:41:21,956][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 08:41:21,956][257371] FPS: 331786.45[0m
[36m[2023-07-17 08:41:26,227][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:41:26,227][257371] Reward + Measures: [[52.84112915  0.41191334  0.23018631  0.4167023   0.23871765  4.99209738]][0m
[37m[1m[2023-07-17 08:41:26,227][257371] Max Reward on eval: 52.84112914811971[0m
[37m[1m[2023-07-17 08:41:26,227][257371] Min Reward on eval: 52.84112914811971[0m
[37m[1m[2023-07-17 08:41:26,228][257371] Mean Reward across all agents: 52.84112914811971[0m
[37m[1m[2023-07-17 08:41:26,228][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:41:31,219][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:41:31,220][257371] Reward + Measures: [[ 24.55706635   0.37509999   0.0732       0.4691       0.30759999
    5.89822054]
 [  0.04915093   0.64280003   0.21809998   0.6785       0.31909999
    6.81713867]
 [-72.3348656    0.31510001   0.35830003   0.30030003   0.49530002
    4.25136805]
 ...
 [-20.82582138   0.54790002   0.23799999   0.52710003   0.44610006
    5.64866209]
 [  4.98250445   0.10290001   0.1178       0.11600001   0.13700001
    5.15103722]
 [144.54313279   0.29809999   0.36320001   0.12730001   0.40599999
    5.17904997]][0m
[37m[1m[2023-07-17 08:41:31,220][257371] Max Reward on eval: 480.3061561450362[0m
[37m[1m[2023-07-17 08:41:31,220][257371] Min Reward on eval: -403.1926040314138[0m
[37m[1m[2023-07-17 08:41:31,221][257371] Mean Reward across all agents: 24.46571410404802[0m
[37m[1m[2023-07-17 08:41:31,221][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:41:31,224][257371] mean_value=-254.13144520774003, max_value=567.9875346924363[0m
[37m[1m[2023-07-17 08:41:31,226][257371] New mean coefficients: [[ 1.3318622   1.1617112  -1.687237   -2.0933325  -0.54650456 -1.419538  ]][0m
[37m[1m[2023-07-17 08:41:31,227][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:41:40,193][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 08:41:40,194][257371] FPS: 428349.12[0m
[36m[2023-07-17 08:41:40,196][257371] itr=940, itrs=2000, Progress: 47.00%[0m
[37m[1m[2023-07-17 08:44:56,981][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000920[0m
[36m[2023-07-17 08:45:09,322][257371] train() took 11.88 seconds to complete[0m
[36m[2023-07-17 08:45:09,323][257371] FPS: 323342.67[0m
[36m[2023-07-17 08:45:13,656][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:45:13,656][257371] Reward + Measures: [[57.13450448  0.37995866  0.21223767  0.385582    0.24325     4.91705704]][0m
[37m[1m[2023-07-17 08:45:13,656][257371] Max Reward on eval: 57.13450447897621[0m
[37m[1m[2023-07-17 08:45:13,657][257371] Min Reward on eval: 57.13450447897621[0m
[37m[1m[2023-07-17 08:45:13,657][257371] Mean Reward across all agents: 57.13450447897621[0m
[37m[1m[2023-07-17 08:45:13,657][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:45:18,793][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:45:18,794][257371] Reward + Measures: [[  32.57414534    0.84919995    0.3671        0.87369996    0.38839999
     7.73283863]
 [-155.6366615     0.65740001    0.27020001    0.68020004    0.30879998
     6.72130156]
 [ -92.76305818    0.70940006    0.16350001    0.70030004    0.65350002
     7.00268698]
 ...
 [ 123.72130041    0.713         0.1347        0.73800004    0.68169999
     6.68426466]
 [-105.25322506    0.64340001    0.1617        0.66140002    0.56050003
     5.32563686]
 [ -98.92902007    0.80070001    0.17820001    0.84119999    0.3348
     6.27384996]][0m
[37m[1m[2023-07-17 08:45:18,794][257371] Max Reward on eval: 332.7888121971861[0m
[37m[1m[2023-07-17 08:45:18,794][257371] Min Reward on eval: -528.7719783023[0m
[37m[1m[2023-07-17 08:45:18,795][257371] Mean Reward across all agents: -62.98226009656733[0m
[37m[1m[2023-07-17 08:45:18,795][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:45:18,799][257371] mean_value=-279.5129986540801, max_value=390.7473379101085[0m
[37m[1m[2023-07-17 08:45:18,806][257371] New mean coefficients: [[ 1.5403148  1.4477144 -1.9780784 -2.3979747 -1.3250172 -1.2146682]][0m
[37m[1m[2023-07-17 08:45:18,807][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:45:27,864][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 08:45:27,864][257371] FPS: 424084.42[0m
[36m[2023-07-17 08:45:27,866][257371] itr=941, itrs=2000, Progress: 47.05%[0m
[36m[2023-07-17 08:45:39,729][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 08:45:39,729][257371] FPS: 326621.34[0m
[36m[2023-07-17 08:45:44,031][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:45:44,032][257371] Reward + Measures: [[58.88924112  0.35581067  0.19737701  0.366512    0.23930199  4.83406973]][0m
[37m[1m[2023-07-17 08:45:44,032][257371] Max Reward on eval: 58.88924111625277[0m
[37m[1m[2023-07-17 08:45:44,032][257371] Min Reward on eval: 58.88924111625277[0m
[37m[1m[2023-07-17 08:45:44,033][257371] Mean Reward across all agents: 58.88924111625277[0m
[37m[1m[2023-07-17 08:45:44,033][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:45:49,041][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:45:49,042][257371] Reward + Measures: [[197.64237732   0.9648       0.43990001   0.96490002   0.38439998
    7.56163502]
 [ 39.41443331   0.72790003   0.0806       0.7299       0.63099998
    6.05864716]
 [ 92.11550836   0.63249999   0.23190001   0.6257       0.43670002
    6.83258057]
 ...
 [  4.46834659   0.93010008   0.10520001   0.92030001   0.81829995
    7.0445323 ]
 [272.00192833   0.93619996   0.3048       0.92670006   0.45200005
    7.24009848]
 [ 36.48918104   0.52839994   0.1028       0.56110001   0.38800001
    6.54202032]][0m
[37m[1m[2023-07-17 08:45:49,042][257371] Max Reward on eval: 635.0405313253402[0m
[37m[1m[2023-07-17 08:45:49,042][257371] Min Reward on eval: -594.271326123667[0m
[37m[1m[2023-07-17 08:45:49,042][257371] Mean Reward across all agents: 17.34499141909578[0m
[37m[1m[2023-07-17 08:45:49,043][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:45:49,048][257371] mean_value=-249.7196536939614, max_value=501.84611967027774[0m
[37m[1m[2023-07-17 08:45:49,051][257371] New mean coefficients: [[ 0.8297149  1.6975876 -1.6238396 -2.2173932 -1.413361  -1.8377342]][0m
[37m[1m[2023-07-17 08:45:49,052][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:45:58,148][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 08:45:58,149][257371] FPS: 422221.26[0m
[36m[2023-07-17 08:45:58,151][257371] itr=942, itrs=2000, Progress: 47.10%[0m
[36m[2023-07-17 08:46:09,910][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 08:46:09,911][257371] FPS: 329388.60[0m
[36m[2023-07-17 08:46:14,121][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:46:14,121][257371] Reward + Measures: [[59.25853657  0.33476567  0.19079     0.34530398  0.23420633  4.77286339]][0m
[37m[1m[2023-07-17 08:46:14,122][257371] Max Reward on eval: 59.25853656815121[0m
[37m[1m[2023-07-17 08:46:14,122][257371] Min Reward on eval: 59.25853656815121[0m
[37m[1m[2023-07-17 08:46:14,122][257371] Mean Reward across all agents: 59.25853656815121[0m
[37m[1m[2023-07-17 08:46:14,123][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:46:19,118][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:46:19,119][257371] Reward + Measures: [[   4.96486617    0.2059        0.16300002    0.2665        0.1499
     5.57141924]
 [  -7.45438124    0.29920003    0.17050001    0.33400002    0.21530001
     5.51459455]
 [ -20.16016648    0.31599998    0.17220001    0.3292        0.25229999
     4.9994688 ]
 ...
 [-154.04947323    0.67589998    0.0508        0.69769996    0.65039998
     5.98110914]
 [  17.40421057    0.7888        0.0821        0.82620001    0.64180005
     6.13557529]
 [  22.86561349    0.64250004    0.0535        0.66219997    0.63059998
     6.81574249]][0m
[37m[1m[2023-07-17 08:46:19,119][257371] Max Reward on eval: 400.19686221964656[0m
[37m[1m[2023-07-17 08:46:19,119][257371] Min Reward on eval: -284.024656900391[0m
[37m[1m[2023-07-17 08:46:19,119][257371] Mean Reward across all agents: 3.416153587048562[0m
[37m[1m[2023-07-17 08:46:19,120][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:46:19,122][257371] mean_value=-267.3312445589701, max_value=273.7787425691281[0m
[37m[1m[2023-07-17 08:46:19,124][257371] New mean coefficients: [[ 0.49691713  1.7463145  -0.73269826 -2.35625    -1.5169747  -2.0229445 ]][0m
[37m[1m[2023-07-17 08:46:19,126][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:46:28,186][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 08:46:28,187][257371] FPS: 423902.80[0m
[36m[2023-07-17 08:46:28,189][257371] itr=943, itrs=2000, Progress: 47.15%[0m
[36m[2023-07-17 08:46:39,810][257371] train() took 11.52 seconds to complete[0m
[36m[2023-07-17 08:46:39,811][257371] FPS: 333394.43[0m
[36m[2023-07-17 08:46:44,204][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:46:44,204][257371] Reward + Measures: [[60.64342069  0.32788432  0.18327799  0.339407    0.23927298  4.73806   ]][0m
[37m[1m[2023-07-17 08:46:44,204][257371] Max Reward on eval: 60.643420689541266[0m
[37m[1m[2023-07-17 08:46:44,205][257371] Min Reward on eval: 60.643420689541266[0m
[37m[1m[2023-07-17 08:46:44,205][257371] Mean Reward across all agents: 60.643420689541266[0m
[37m[1m[2023-07-17 08:46:44,205][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:46:49,266][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:46:49,267][257371] Reward + Measures: [[252.53595096   0.60729998   0.41669998   0.61600006   0.1831
    5.34937239]
 [ 37.28423499   0.58120006   0.123        0.65430003   0.52319998
    5.48934507]
 [ 43.60187542   0.65640002   0.12060001   0.66710001   0.46530005
    6.34213209]
 ...
 [-37.96591233   0.81919998   0.73269999   0.58340007   0.28709999
    6.99813795]
 [ 51.58825042   0.60720003   0.43920001   0.43660003   0.49450001
    6.48180389]
 [147.4419698    0.82569999   0.6439001    0.74230003   0.14649999
    6.81105518]][0m
[37m[1m[2023-07-17 08:46:49,267][257371] Max Reward on eval: 673.7774553177878[0m
[37m[1m[2023-07-17 08:46:49,267][257371] Min Reward on eval: -765.8980941779912[0m
[37m[1m[2023-07-17 08:46:49,268][257371] Mean Reward across all agents: -24.800701989980745[0m
[37m[1m[2023-07-17 08:46:49,268][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:46:49,272][257371] mean_value=-302.48673295934316, max_value=487.34853779072114[0m
[37m[1m[2023-07-17 08:46:49,275][257371] New mean coefficients: [[ 0.65756184  1.6508077  -0.81323856 -2.7134788  -1.1914396  -2.2381322 ]][0m
[37m[1m[2023-07-17 08:46:49,277][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:46:58,478][257371] train() took 9.20 seconds to complete[0m
[36m[2023-07-17 08:46:58,483][257371] FPS: 417421.02[0m
[36m[2023-07-17 08:46:58,486][257371] itr=944, itrs=2000, Progress: 47.20%[0m
[36m[2023-07-17 08:47:10,332][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 08:47:10,332][257371] FPS: 326950.35[0m
[36m[2023-07-17 08:47:14,656][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:47:14,656][257371] Reward + Measures: [[58.73569719  0.29983002  0.17100199  0.30987     0.23445766  4.66681194]][0m
[37m[1m[2023-07-17 08:47:14,657][257371] Max Reward on eval: 58.735697187043726[0m
[37m[1m[2023-07-17 08:47:14,657][257371] Min Reward on eval: 58.735697187043726[0m
[37m[1m[2023-07-17 08:47:14,657][257371] Mean Reward across all agents: 58.735697187043726[0m
[37m[1m[2023-07-17 08:47:14,657][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:47:19,717][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:47:19,718][257371] Reward + Measures: [[ 99.70961259   0.1811       0.22750001   0.22629999   0.29130003
    4.60339165]
 [ 70.41064468   0.23060003   0.4429       0.3477       0.49960002
    5.02919912]
 [ 59.73051403   0.27480003   0.33740002   0.3493       0.38999999
    4.28203821]
 ...
 [190.25953486   0.3299       0.20060001   0.3892       0.27309999
    5.42381763]
 [297.15165629   0.5735001    0.0756       0.64069998   0.54189998
    5.84363031]
 [-41.23134275   0.31620002   0.33650002   0.32150003   0.303
    3.8091054 ]][0m
[37m[1m[2023-07-17 08:47:19,718][257371] Max Reward on eval: 564.6571216661483[0m
[37m[1m[2023-07-17 08:47:19,718][257371] Min Reward on eval: -140.13514766283333[0m
[37m[1m[2023-07-17 08:47:19,719][257371] Mean Reward across all agents: 68.42034648783843[0m
[37m[1m[2023-07-17 08:47:19,719][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:47:19,722][257371] mean_value=-797.7129925877686, max_value=440.3099263984144[0m
[37m[1m[2023-07-17 08:47:19,724][257371] New mean coefficients: [[-0.3277164  1.1698201 -0.9741236 -1.6186943 -1.2551268 -3.0745337]][0m
[37m[1m[2023-07-17 08:47:19,726][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:47:28,850][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 08:47:28,850][257371] FPS: 420936.77[0m
[36m[2023-07-17 08:47:28,852][257371] itr=945, itrs=2000, Progress: 47.25%[0m
[36m[2023-07-17 08:47:40,661][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 08:47:40,662][257371] FPS: 328092.61[0m
[36m[2023-07-17 08:47:44,936][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:47:44,937][257371] Reward + Measures: [[55.25681635  0.30193734  0.17191499  0.30937666  0.23497199  4.64109993]][0m
[37m[1m[2023-07-17 08:47:44,937][257371] Max Reward on eval: 55.25681635388107[0m
[37m[1m[2023-07-17 08:47:44,937][257371] Min Reward on eval: 55.25681635388107[0m
[37m[1m[2023-07-17 08:47:44,938][257371] Mean Reward across all agents: 55.25681635388107[0m
[37m[1m[2023-07-17 08:47:44,938][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:47:49,857][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:47:49,857][257371] Reward + Measures: [[  5.2599051    0.78149998   0.45509997   0.83070004   0.15530001
    5.50567722]
 [ 96.31735823   0.32890001   0.16600001   0.3479       0.22070001
    4.86370182]
 [257.44973268   0.6067       0.27020001   0.62410003   0.2721
    5.22545385]
 ...
 [152.60808349   0.4206       0.2465       0.44899997   0.17529999
    4.98141432]
 [122.8383412    0.50150001   0.206        0.52790004   0.18170001
    5.45639038]
 [  0.25012119   0.5212       0.44989997   0.54100001   0.55769998
    4.84266806]][0m
[37m[1m[2023-07-17 08:47:49,858][257371] Max Reward on eval: 374.76521538076923[0m
[37m[1m[2023-07-17 08:47:49,858][257371] Min Reward on eval: -240.85306352712215[0m
[37m[1m[2023-07-17 08:47:49,858][257371] Mean Reward across all agents: 51.666144503099275[0m
[37m[1m[2023-07-17 08:47:49,858][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:47:49,862][257371] mean_value=-277.47327288554646, max_value=638.0913982290808[0m
[37m[1m[2023-07-17 08:47:49,864][257371] New mean coefficients: [[ 0.04590848  1.2351468  -1.4197787  -1.1890807  -1.2045553  -3.876269  ]][0m
[37m[1m[2023-07-17 08:47:49,866][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:47:58,851][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 08:47:58,852][257371] FPS: 427411.16[0m
[36m[2023-07-17 08:47:58,854][257371] itr=946, itrs=2000, Progress: 47.30%[0m
[36m[2023-07-17 08:48:10,756][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 08:48:10,756][257371] FPS: 325406.64[0m
[36m[2023-07-17 08:48:15,043][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:48:15,044][257371] Reward + Measures: [[55.99918199  0.29496601  0.16812366  0.30313802  0.23686498  4.60276079]][0m
[37m[1m[2023-07-17 08:48:15,044][257371] Max Reward on eval: 55.99918199360654[0m
[37m[1m[2023-07-17 08:48:15,044][257371] Min Reward on eval: 55.99918199360654[0m
[37m[1m[2023-07-17 08:48:15,045][257371] Mean Reward across all agents: 55.99918199360654[0m
[37m[1m[2023-07-17 08:48:15,045][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:48:20,306][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:48:20,312][257371] Reward + Measures: [[ 32.92831377   0.33809999   0.18610001   0.39070001   0.27299997
    4.65881062]
 [123.38869361   0.3448       0.35010001   0.3716       0.3976
    4.28443003]
 [ 58.4708503    0.41580001   0.1497       0.4127       0.4059
    5.33436823]
 ...
 [ 85.32448591   0.67010003   0.12149999   0.66460001   0.6347
    5.86866999]
 [ 20.05507935   0.65110004   0.14720002   0.63540006   0.58770001
    5.68616629]
 [ 79.58534258   0.37609997   0.1627       0.44300005   0.29099998
    4.87593794]][0m
[37m[1m[2023-07-17 08:48:20,312][257371] Max Reward on eval: 468.0647068273276[0m
[37m[1m[2023-07-17 08:48:20,312][257371] Min Reward on eval: -429.80411314070227[0m
[37m[1m[2023-07-17 08:48:20,313][257371] Mean Reward across all agents: 42.05809487148046[0m
[37m[1m[2023-07-17 08:48:20,313][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:48:20,315][257371] mean_value=-243.21710472587506, max_value=314.26000270149666[0m
[37m[1m[2023-07-17 08:48:20,318][257371] New mean coefficients: [[ 0.3605939  1.5690202 -1.4118985 -1.5020341 -0.8113177 -3.7880042]][0m
[37m[1m[2023-07-17 08:48:20,319][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:48:29,314][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 08:48:29,314][257371] FPS: 426990.86[0m
[36m[2023-07-17 08:48:29,316][257371] itr=947, itrs=2000, Progress: 47.35%[0m
[36m[2023-07-17 08:48:41,125][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 08:48:41,126][257371] FPS: 328117.10[0m
[36m[2023-07-17 08:48:45,454][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:48:45,455][257371] Reward + Measures: [[54.59962532  0.28931332  0.16658866  0.29580164  0.23428768  4.56362867]][0m
[37m[1m[2023-07-17 08:48:45,455][257371] Max Reward on eval: 54.59962531747979[0m
[37m[1m[2023-07-17 08:48:45,455][257371] Min Reward on eval: 54.59962531747979[0m
[37m[1m[2023-07-17 08:48:45,455][257371] Mean Reward across all agents: 54.59962531747979[0m
[37m[1m[2023-07-17 08:48:45,456][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:48:50,495][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:48:50,495][257371] Reward + Measures: [[ 189.99289861    0.78280002    0.0697        0.80270004    0.70230001
     6.28001404]
 [  85.05716289    0.41370001    0.2175        0.4817        0.17560001
     6.49738932]
 [  17.78003345    0.47530004    0.25390002    0.46849999    0.23029999
     4.91009378]
 ...
 [ 246.82901287    0.71340001    0.2811        0.69760001    0.3585
     6.21926785]
 [-245.40391349    0.65199995    0.06810001    0.67880005    0.64350003
     6.23401976]
 [ -39.82892088    0.2076        0.2263        0.2491        0.2388
     4.68471289]][0m
[37m[1m[2023-07-17 08:48:50,495][257371] Max Reward on eval: 433.3066978704184[0m
[37m[1m[2023-07-17 08:48:50,496][257371] Min Reward on eval: -382.35334515422585[0m
[37m[1m[2023-07-17 08:48:50,496][257371] Mean Reward across all agents: -7.881108529496126[0m
[37m[1m[2023-07-17 08:48:50,496][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:48:50,500][257371] mean_value=-502.7864535716074, max_value=380.1209889942941[0m
[37m[1m[2023-07-17 08:48:50,503][257371] New mean coefficients: [[ 1.1822574  2.165458  -1.46265   -1.6633859 -0.6315091 -3.538769 ]][0m
[37m[1m[2023-07-17 08:48:50,504][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:48:59,633][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 08:48:59,633][257371] FPS: 420704.36[0m
[36m[2023-07-17 08:48:59,636][257371] itr=948, itrs=2000, Progress: 47.40%[0m
[36m[2023-07-17 08:49:11,564][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 08:49:11,564][257371] FPS: 324851.68[0m
[36m[2023-07-17 08:49:15,974][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:49:15,975][257371] Reward + Measures: [[56.67333596  0.28218767  0.16327766  0.288122    0.23172666  4.51834869]][0m
[37m[1m[2023-07-17 08:49:15,975][257371] Max Reward on eval: 56.67333596268273[0m
[37m[1m[2023-07-17 08:49:15,975][257371] Min Reward on eval: 56.67333596268273[0m
[37m[1m[2023-07-17 08:49:15,975][257371] Mean Reward across all agents: 56.67333596268273[0m
[37m[1m[2023-07-17 08:49:15,975][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:49:20,992][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:49:20,993][257371] Reward + Measures: [[  30.60922847    0.17930001    0.36829999    0.2825        0.3836
     4.81660891]
 [  39.17994603    0.21519999    0.11740001    0.25139999    0.17300001
     5.00406361]
 [ -45.26212554    0.31119999    0.08940001    0.34250003    0.2692
     5.70776176]
 ...
 [  75.4093226     0.27020001    0.17089999    0.29290003    0.2902
     4.87040663]
 [-109.22830517    0.18119998    0.34819999    0.22070001    0.31890002
     5.03932858]
 [  -8.93542121    0.57860005    0.39900002    0.58939999    0.2383
     5.76309395]][0m
[37m[1m[2023-07-17 08:49:20,993][257371] Max Reward on eval: 410.7737674370408[0m
[37m[1m[2023-07-17 08:49:20,993][257371] Min Reward on eval: -489.6202242091298[0m
[37m[1m[2023-07-17 08:49:20,993][257371] Mean Reward across all agents: 6.248458896240783[0m
[37m[1m[2023-07-17 08:49:20,993][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:49:20,995][257371] mean_value=-324.269786527783, max_value=127.52955385725193[0m
[37m[1m[2023-07-17 08:49:20,998][257371] New mean coefficients: [[ 1.0928959   1.2652686  -0.98819613 -1.464614   -0.3637147  -3.7719479 ]][0m
[37m[1m[2023-07-17 08:49:20,999][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:49:30,032][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 08:49:30,032][257371] FPS: 425160.00[0m
[36m[2023-07-17 08:49:30,034][257371] itr=949, itrs=2000, Progress: 47.45%[0m
[36m[2023-07-17 08:49:41,710][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 08:49:41,710][257371] FPS: 331825.50[0m
[36m[2023-07-17 08:49:45,965][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:49:45,965][257371] Reward + Measures: [[53.97358079  0.27602234  0.16220333  0.27905801  0.22809468  4.49613762]][0m
[37m[1m[2023-07-17 08:49:45,965][257371] Max Reward on eval: 53.973580792800526[0m
[37m[1m[2023-07-17 08:49:45,966][257371] Min Reward on eval: 53.973580792800526[0m
[37m[1m[2023-07-17 08:49:45,966][257371] Mean Reward across all agents: 53.973580792800526[0m
[37m[1m[2023-07-17 08:49:45,966][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:49:50,965][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:49:50,966][257371] Reward + Measures: [[  3.10699699   0.30890003   0.13960001   0.34020004   0.29960001
    5.50283051]
 [ 34.09905786   0.38240001   0.1972       0.40660006   0.2527
    5.02998924]
 [-64.63415122   0.59130001   0.1163       0.65530002   0.46440002
    5.6565299 ]
 ...
 [ 14.65016815   0.1569       0.16849999   0.18960001   0.2165
    4.83154392]
 [ 49.00785556   0.22390001   0.14860001   0.2332       0.2156
    4.27016163]
 [446.6981087    0.94470006   0.4646       0.94980013   0.0636
    6.15680456]][0m
[37m[1m[2023-07-17 08:49:50,966][257371] Max Reward on eval: 703.1696853376925[0m
[37m[1m[2023-07-17 08:49:50,966][257371] Min Reward on eval: -249.43080832734705[0m
[37m[1m[2023-07-17 08:49:50,967][257371] Mean Reward across all agents: 51.04490455269212[0m
[37m[1m[2023-07-17 08:49:50,967][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:49:50,970][257371] mean_value=-596.9009305323002, max_value=390.4990872837962[0m
[37m[1m[2023-07-17 08:49:50,972][257371] New mean coefficients: [[ 1.6810002  1.7147092 -1.641512  -0.8558248  1.192799  -3.2230682]][0m
[37m[1m[2023-07-17 08:49:50,973][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:50:00,001][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 08:50:00,001][257371] FPS: 425448.91[0m
[36m[2023-07-17 08:50:00,003][257371] itr=950, itrs=2000, Progress: 47.50%[0m
[37m[1m[2023-07-17 08:53:27,196][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000930[0m
[36m[2023-07-17 08:53:39,609][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 08:53:39,609][257371] FPS: 326922.13[0m
[36m[2023-07-17 08:53:43,918][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:53:43,919][257371] Reward + Measures: [[57.63454951  0.27572864  0.16255401  0.27826533  0.22655131  4.49281263]][0m
[37m[1m[2023-07-17 08:53:43,920][257371] Max Reward on eval: 57.63454951415125[0m
[37m[1m[2023-07-17 08:53:43,920][257371] Min Reward on eval: 57.63454951415125[0m
[37m[1m[2023-07-17 08:53:43,921][257371] Mean Reward across all agents: 57.63454951415125[0m
[37m[1m[2023-07-17 08:53:43,921][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:53:48,897][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:53:48,898][257371] Reward + Measures: [[ 93.84129934   0.12650001   0.1707       0.16139999   0.108
    4.6497736 ]
 [-45.99313599   0.42310005   0.0763       0.45050001   0.42480001
    6.73475027]
 [-45.48570707   0.83590001   0.0781       0.83719999   0.67040008
    6.78359842]
 ...
 [106.19415192   0.30250001   0.0574       0.3145       0.32120001
    5.89722061]
 [ 14.30941957   0.71470004   0.1168       0.68430007   0.55729997
    5.93518591]
 [  0.31119521   0.36089998   0.29480001   0.34650001   0.3527
    5.11744452]][0m
[37m[1m[2023-07-17 08:53:48,898][257371] Max Reward on eval: 360.8895874112844[0m
[37m[1m[2023-07-17 08:53:48,898][257371] Min Reward on eval: -205.8002264291048[0m
[37m[1m[2023-07-17 08:53:48,898][257371] Mean Reward across all agents: 34.65350621852838[0m
[37m[1m[2023-07-17 08:53:48,899][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:53:48,901][257371] mean_value=-488.929831694942, max_value=67.73671023995621[0m
[37m[1m[2023-07-17 08:53:48,912][257371] New mean coefficients: [[ 1.7705101   0.75861156 -0.2685548  -0.31446743  1.490161   -2.6445832 ]][0m
[37m[1m[2023-07-17 08:53:48,914][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:53:57,890][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 08:53:57,890][257371] FPS: 427960.91[0m
[36m[2023-07-17 08:53:57,892][257371] itr=951, itrs=2000, Progress: 47.55%[0m
[36m[2023-07-17 08:54:09,519][257371] train() took 11.52 seconds to complete[0m
[36m[2023-07-17 08:54:09,520][257371] FPS: 333235.38[0m
[36m[2023-07-17 08:54:13,737][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:54:13,737][257371] Reward + Measures: [[58.5406619   0.28011966  0.16321966  0.28196269  0.23196533  4.45755339]][0m
[37m[1m[2023-07-17 08:54:13,737][257371] Max Reward on eval: 58.54066189577291[0m
[37m[1m[2023-07-17 08:54:13,738][257371] Min Reward on eval: 58.54066189577291[0m
[37m[1m[2023-07-17 08:54:13,738][257371] Mean Reward across all agents: 58.54066189577291[0m
[37m[1m[2023-07-17 08:54:13,738][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:54:18,750][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:54:18,751][257371] Reward + Measures: [[ 78.16948712   0.2899       0.1825       0.30570003   0.2902
    4.51921463]
 [ 76.7654626    0.27680001   0.17620002   0.30430001   0.25470001
    4.54508543]
 [148.75178876   0.26789999   0.15320002   0.2863       0.26990002
    4.81865168]
 ...
 [ 65.53071737   0.40450001   0.2172       0.40159997   0.2139
    4.84175968]
 [107.55774512   0.66189998   0.40510002   0.64210004   0.12279999
    5.71242905]
 [ 18.16467145   0.45870003   0.1674       0.46860003   0.44670001
    5.73939562]][0m
[37m[1m[2023-07-17 08:54:18,751][257371] Max Reward on eval: 377.47689908109606[0m
[37m[1m[2023-07-17 08:54:18,751][257371] Min Reward on eval: -80.91465998291969[0m
[37m[1m[2023-07-17 08:54:18,751][257371] Mean Reward across all agents: 64.92201407362292[0m
[37m[1m[2023-07-17 08:54:18,752][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:54:18,753][257371] mean_value=-270.93943335537733, max_value=209.51357877096657[0m
[37m[1m[2023-07-17 08:54:18,756][257371] New mean coefficients: [[ 2.15695     0.8731204  -0.18235576 -0.28077632  1.3273838  -3.2274954 ]][0m
[37m[1m[2023-07-17 08:54:18,757][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:54:27,801][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 08:54:27,802][257371] FPS: 424649.41[0m
[36m[2023-07-17 08:54:27,804][257371] itr=952, itrs=2000, Progress: 47.60%[0m
[36m[2023-07-17 08:54:39,722][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 08:54:39,722][257371] FPS: 325064.77[0m
[36m[2023-07-17 08:54:44,046][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:54:44,046][257371] Reward + Measures: [[63.07384995  0.29128933  0.16921265  0.29214734  0.23812166  4.43646002]][0m
[37m[1m[2023-07-17 08:54:44,046][257371] Max Reward on eval: 63.073849951383814[0m
[37m[1m[2023-07-17 08:54:44,046][257371] Min Reward on eval: 63.073849951383814[0m
[37m[1m[2023-07-17 08:54:44,047][257371] Mean Reward across all agents: 63.073849951383814[0m
[37m[1m[2023-07-17 08:54:44,047][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:54:49,302][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:54:49,308][257371] Reward + Measures: [[ 61.18104801   0.2789       0.13999999   0.3048       0.2343
    4.66702795]
 [ 11.4326973    0.32539999   0.20179999   0.31990001   0.2802
    4.23322916]
 [ 70.58309011   0.39219999   0.19140001   0.39809999   0.28670001
    4.59123707]
 ...
 [121.4522285    0.39359996   0.1798       0.40820003   0.28200004
    4.78173876]
 [ -8.74413379   0.17019999   0.1138       0.1716       0.15310001
    4.58166218]
 [ 83.73907754   0.32249999   0.1655       0.3136       0.23699999
    4.44179869]][0m
[37m[1m[2023-07-17 08:54:49,308][257371] Max Reward on eval: 164.11030527260155[0m
[37m[1m[2023-07-17 08:54:49,308][257371] Min Reward on eval: -8.744133790582419[0m
[37m[1m[2023-07-17 08:54:49,309][257371] Mean Reward across all agents: 63.201037468817546[0m
[37m[1m[2023-07-17 08:54:49,309][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:54:49,310][257371] mean_value=-595.0626926536196, max_value=-51.71342909778892[0m
[36m[2023-07-17 08:54:49,313][257371] XNES is restarting with a new solution whose measures are [0.26429999 0.27610001 0.13939999 0.1372     1.45003736] and objective is 4678.915023773705[0m
[36m[2023-07-17 08:54:49,314][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 08:54:49,316][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 08:54:49,317][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:54:58,307][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 08:54:58,307][257371] FPS: 427192.36[0m
[36m[2023-07-17 08:54:58,310][257371] itr=953, itrs=2000, Progress: 47.65%[0m
[36m[2023-07-17 08:55:10,144][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 08:55:10,145][257371] FPS: 327347.80[0m
[36m[2023-07-17 08:55:14,499][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:55:14,499][257371] Reward + Measures: [[4003.81594288    0.39041632    0.33015364    0.13802467    0.13026766
     1.32991886]][0m
[37m[1m[2023-07-17 08:55:14,500][257371] Max Reward on eval: 4003.815942876816[0m
[37m[1m[2023-07-17 08:55:14,500][257371] Min Reward on eval: 4003.815942876816[0m
[37m[1m[2023-07-17 08:55:14,500][257371] Mean Reward across all agents: 4003.815942876816[0m
[37m[1m[2023-07-17 08:55:14,500][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:55:19,543][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:55:19,543][257371] Reward + Measures: [[2035.25713725    0.36260003    0.35630003    0.1284        0.0988
     1.37736642]
 [1464.03970717    0.35970002    0.38430002    0.24259999    0.31329998
     1.38101137]
 [1639.50720214    0.2714        0.39340001    0.2256        0.17330001
     1.63867342]
 ...
 [ 289.90112396    0.1734        0.16540001    0.17690001    0.18500002
     2.31391025]
 [1258.42551802    0.23300003    0.22330001    0.19829999    0.21830001
     1.8724997 ]
 [2138.13663481    0.25470001    0.25370002    0.16700001    0.17350002
     1.55167162]][0m
[37m[1m[2023-07-17 08:55:19,543][257371] Max Reward on eval: 3693.909118638735[0m
[37m[1m[2023-07-17 08:55:19,544][257371] Min Reward on eval: 60.03287799358368[0m
[37m[1m[2023-07-17 08:55:19,544][257371] Mean Reward across all agents: 880.3044762204801[0m
[37m[1m[2023-07-17 08:55:19,544][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:55:19,547][257371] mean_value=-4041.0522431647382, max_value=1347.3231332888647[0m
[37m[1m[2023-07-17 08:55:19,550][257371] New mean coefficients: [[ 1.074346   -0.20184654 -0.9391937  -0.14891446 -1.1618778  -0.7283977 ]][0m
[37m[1m[2023-07-17 08:55:19,551][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:55:28,618][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 08:55:28,619][257371] FPS: 423563.25[0m
[36m[2023-07-17 08:55:28,621][257371] itr=954, itrs=2000, Progress: 47.70%[0m
[36m[2023-07-17 08:55:40,542][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 08:55:40,542][257371] FPS: 324956.34[0m
[36m[2023-07-17 08:55:44,848][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:55:44,848][257371] Reward + Measures: [[4241.73023528    0.38479337    0.31830099    0.137372      0.13267434
     1.34967101]][0m
[37m[1m[2023-07-17 08:55:44,849][257371] Max Reward on eval: 4241.730235275479[0m
[37m[1m[2023-07-17 08:55:44,849][257371] Min Reward on eval: 4241.730235275479[0m
[37m[1m[2023-07-17 08:55:44,849][257371] Mean Reward across all agents: 4241.730235275479[0m
[37m[1m[2023-07-17 08:55:44,849][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:55:49,776][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:55:49,782][257371] Reward + Measures: [[2765.22659305    0.38210002    0.44240004    0.1391        0.16180001
     1.31816971]
 [2788.18130493    0.43449998    0.36429998    0.15350001    0.1549
     1.26483333]
 [1257.27526093    0.41060001    0.43080002    0.21009998    0.22789998
     1.52408874]
 ...
 [2806.89123536    0.43109998    0.35619998    0.1613        0.13530001
     1.2123282 ]
 [1941.02345273    0.2674        0.39770001    0.13690001    0.19990002
     1.80203819]
 [3396.94839478    0.41210005    0.35600001    0.1446        0.1437
     1.29826379]][0m
[37m[1m[2023-07-17 08:55:49,782][257371] Max Reward on eval: 4243.0147704879755[0m
[37m[1m[2023-07-17 08:55:49,782][257371] Min Reward on eval: 236.10969348552172[0m
[37m[1m[2023-07-17 08:55:49,783][257371] Mean Reward across all agents: 2597.5204855464376[0m
[37m[1m[2023-07-17 08:55:49,783][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:55:49,785][257371] mean_value=-2250.7621826400637, max_value=1362.459704918097[0m
[37m[1m[2023-07-17 08:55:49,788][257371] New mean coefficients: [[ 0.36836106 -0.39700663 -1.1007209   1.091669   -0.674003   -1.6930754 ]][0m
[37m[1m[2023-07-17 08:55:49,789][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:55:58,766][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 08:55:58,767][257371] FPS: 427820.01[0m
[36m[2023-07-17 08:55:58,769][257371] itr=955, itrs=2000, Progress: 47.75%[0m
[36m[2023-07-17 08:56:10,524][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 08:56:10,525][257371] FPS: 329531.50[0m
[36m[2023-07-17 08:56:14,942][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:56:14,943][257371] Reward + Measures: [[4341.28371312    0.376937      0.30526       0.13575       0.13236466
     1.27186739]][0m
[37m[1m[2023-07-17 08:56:14,943][257371] Max Reward on eval: 4341.283713123256[0m
[37m[1m[2023-07-17 08:56:14,943][257371] Min Reward on eval: 4341.283713123256[0m
[37m[1m[2023-07-17 08:56:14,944][257371] Mean Reward across all agents: 4341.283713123256[0m
[37m[1m[2023-07-17 08:56:14,944][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:56:19,914][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:56:19,915][257371] Reward + Measures: [[1400.56509971    0.34670001    0.32580003    0.15109999    0.2052
     1.78778064]
 [ 201.0748072     0.1869        0.21890001    0.16680001    0.20639999
     4.24549341]
 [ 791.53493786    0.31029999    0.26320001    0.1947        0.15990001
     2.51976085]
 ...
 [ 513.42674041    0.20030001    0.16309999    0.14130001    0.14460002
     3.22847247]
 [  40.52790306    0.17110001    0.1339        0.13940001    0.1517
     3.69469309]
 [ 215.11009601    0.39410001    0.4104        0.21689999    0.33430001
     2.52900648]][0m
[37m[1m[2023-07-17 08:56:19,915][257371] Max Reward on eval: 3834.5770569470246[0m
[37m[1m[2023-07-17 08:56:19,915][257371] Min Reward on eval: -32.017135698534545[0m
[37m[1m[2023-07-17 08:56:19,915][257371] Mean Reward across all agents: 1008.8798249768528[0m
[37m[1m[2023-07-17 08:56:19,916][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:56:19,919][257371] mean_value=-3557.55618691337, max_value=1399.0792786535944[0m
[37m[1m[2023-07-17 08:56:19,921][257371] New mean coefficients: [[ 0.43802863 -1.0702661  -1.013957    1.2033927  -1.2035258  -1.9337575 ]][0m
[37m[1m[2023-07-17 08:56:19,922][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:56:28,986][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 08:56:28,986][257371] FPS: 423776.89[0m
[36m[2023-07-17 08:56:28,988][257371] itr=956, itrs=2000, Progress: 47.80%[0m
[36m[2023-07-17 08:56:40,965][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-17 08:56:40,965][257371] FPS: 323516.29[0m
[36m[2023-07-17 08:56:45,216][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:56:45,216][257371] Reward + Measures: [[4333.65646087    0.36493167    0.29641867    0.13699867    0.12976001
     1.16193426]][0m
[37m[1m[2023-07-17 08:56:45,216][257371] Max Reward on eval: 4333.6564608700055[0m
[37m[1m[2023-07-17 08:56:45,217][257371] Min Reward on eval: 4333.6564608700055[0m
[37m[1m[2023-07-17 08:56:45,217][257371] Mean Reward across all agents: 4333.6564608700055[0m
[37m[1m[2023-07-17 08:56:45,217][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:56:50,218][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:56:50,224][257371] Reward + Measures: [[1168.68989755    0.27330002    0.28519997    0.20149998    0.20190001
     1.49745238]
 [2479.74704169    0.34240001    0.30970001    0.25250003    0.22260001
     1.39404798]
 [ 355.91962432    0.45660001    0.22810002    0.3646        0.24249999
     2.60505176]
 ...
 [2398.8269348     0.49499997    0.36149999    0.16540001    0.14210001
     1.16291702]
 [1082.83028412    0.18270001    0.30090001    0.1672        0.20970002
     1.60327077]
 [ 784.19096948    0.18980001    0.34029999    0.1814        0.2536
     2.06531715]][0m
[37m[1m[2023-07-17 08:56:50,224][257371] Max Reward on eval: 4282.798370352643[0m
[37m[1m[2023-07-17 08:56:50,225][257371] Min Reward on eval: 80.57958617210389[0m
[37m[1m[2023-07-17 08:56:50,225][257371] Mean Reward across all agents: 1738.8274164392597[0m
[37m[1m[2023-07-17 08:56:50,225][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:56:50,227][257371] mean_value=-3140.993080616273, max_value=1874.2781653602283[0m
[37m[1m[2023-07-17 08:56:50,230][257371] New mean coefficients: [[ 1.3034787  -0.19262087  0.05085039  0.87907183 -0.34849066 -1.65169   ]][0m
[37m[1m[2023-07-17 08:56:50,231][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:56:59,210][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 08:56:59,210][257371] FPS: 427744.39[0m
[36m[2023-07-17 08:56:59,212][257371] itr=957, itrs=2000, Progress: 47.85%[0m
[36m[2023-07-17 08:57:11,124][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 08:57:11,125][257371] FPS: 325227.89[0m
[36m[2023-07-17 08:57:15,425][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:57:15,425][257371] Reward + Measures: [[4708.15309345    0.36185399    0.29088867    0.140196      0.13140467
     1.18447495]][0m
[37m[1m[2023-07-17 08:57:15,425][257371] Max Reward on eval: 4708.153093453838[0m
[37m[1m[2023-07-17 08:57:15,426][257371] Min Reward on eval: 4708.153093453838[0m
[37m[1m[2023-07-17 08:57:15,426][257371] Mean Reward across all agents: 4708.153093453838[0m
[37m[1m[2023-07-17 08:57:15,426][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:57:20,727][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:57:20,727][257371] Reward + Measures: [[2723.75642392    0.34899998    0.39489999    0.1533        0.19590001
     1.05566347]
 [2687.85339359    0.39190003    0.33519998    0.18020001    0.16329999
     1.17325962]
 [2430.55857848    0.32570001    0.41700003    0.1868        0.19990002
     1.04821336]
 ...
 [3936.59292604    0.36749998    0.35429999    0.15260001    0.15510002
     1.12850189]
 [3383.88871766    0.37750003    0.35879999    0.13780001    0.15790001
     1.10336065]
 [2104.69485945    0.3283        0.35190001    0.18429999    0.18679999
     1.16256797]][0m
[37m[1m[2023-07-17 08:57:20,728][257371] Max Reward on eval: 4520.248657260125[0m
[37m[1m[2023-07-17 08:57:20,728][257371] Min Reward on eval: 504.4123954784824[0m
[37m[1m[2023-07-17 08:57:20,728][257371] Mean Reward across all agents: 2814.380797290762[0m
[37m[1m[2023-07-17 08:57:20,728][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:57:20,731][257371] mean_value=-1536.2411659812674, max_value=1605.1164910424368[0m
[37m[1m[2023-07-17 08:57:20,734][257371] New mean coefficients: [[ 1.7710468  -0.19944045  0.5406205   1.0372866   0.54464465 -1.621401  ]][0m
[37m[1m[2023-07-17 08:57:20,735][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:57:29,748][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 08:57:29,748][257371] FPS: 426126.60[0m
[36m[2023-07-17 08:57:29,750][257371] itr=958, itrs=2000, Progress: 47.90%[0m
[36m[2023-07-17 08:57:41,494][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 08:57:41,495][257371] FPS: 329839.76[0m
[36m[2023-07-17 08:57:45,784][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:57:45,784][257371] Reward + Measures: [[4910.29722446    0.36631396    0.28458935    0.141013      0.13327
     1.20164633]][0m
[37m[1m[2023-07-17 08:57:45,784][257371] Max Reward on eval: 4910.297224458637[0m
[37m[1m[2023-07-17 08:57:45,784][257371] Min Reward on eval: 4910.297224458637[0m
[37m[1m[2023-07-17 08:57:45,785][257371] Mean Reward across all agents: 4910.297224458637[0m
[37m[1m[2023-07-17 08:57:45,785][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:57:50,730][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:57:50,730][257371] Reward + Measures: [[ 334.39546579    0.45339999    0.3425        0.22319999    0.21350001
     1.87350738]
 [ 129.395516      0.2067        0.22319999    0.2115        0.2271
     4.12608767]
 [ 624.263258      0.4526        0.47310001    0.22840002    0.28890002
     3.09478831]
 ...
 [1338.16223715    0.30160004    0.31          0.14960001    0.15930001
     1.77042508]
 [ 115.6023608     0.2297        0.38780001    0.28160003    0.27320001
     3.80689692]
 [ 128.24352431    0.28670001    0.39569998    0.2369        0.3028
     3.40201449]][0m
[37m[1m[2023-07-17 08:57:50,730][257371] Max Reward on eval: 3650.422729506844[0m
[37m[1m[2023-07-17 08:57:50,731][257371] Min Reward on eval: 2.007128176558763[0m
[37m[1m[2023-07-17 08:57:50,731][257371] Mean Reward across all agents: 913.1593078113076[0m
[37m[1m[2023-07-17 08:57:50,731][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:57:50,734][257371] mean_value=-2905.177255813583, max_value=1054.8545715305631[0m
[37m[1m[2023-07-17 08:57:50,737][257371] New mean coefficients: [[ 1.8514974  -0.50908995 -0.44071573  0.6819877   0.8780572  -1.5825195 ]][0m
[37m[1m[2023-07-17 08:57:50,738][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:57:59,804][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 08:57:59,804][257371] FPS: 423651.38[0m
[36m[2023-07-17 08:57:59,806][257371] itr=959, itrs=2000, Progress: 47.95%[0m
[36m[2023-07-17 08:58:11,565][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 08:58:11,565][257371] FPS: 329488.69[0m
[36m[2023-07-17 08:58:15,898][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:58:15,898][257371] Reward + Measures: [[5196.57113881    0.36422333    0.27210867    0.14574566    0.13819167
     1.23492491]][0m
[37m[1m[2023-07-17 08:58:15,899][257371] Max Reward on eval: 5196.571138812034[0m
[37m[1m[2023-07-17 08:58:15,899][257371] Min Reward on eval: 5196.571138812034[0m
[37m[1m[2023-07-17 08:58:15,899][257371] Mean Reward across all agents: 5196.571138812034[0m
[37m[1m[2023-07-17 08:58:15,899][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:58:20,935][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 08:58:20,936][257371] Reward + Measures: [[ 158.89166893    0.65879995    0.60479993    0.56070006    0.53800005
     4.18594122]
 [1821.98268126    0.40310001    0.3515        0.22230001    0.18359999
     1.68114054]
 [ 392.19926634    0.33130002    0.33969998    0.205         0.23580001
     2.86481476]
 ...
 [ 346.82344103    0.76499999    0.71040004    0.66939998    0.68659997
     2.03736186]
 [ 247.13812396    0.29359999    0.39979997    0.21250001    0.26530001
     1.89024317]
 [1483.63777352    0.22830001    0.2142        0.1881        0.17920001
     1.85990751]][0m
[37m[1m[2023-07-17 08:58:20,936][257371] Max Reward on eval: 3928.344787608739[0m
[37m[1m[2023-07-17 08:58:20,936][257371] Min Reward on eval: 37.62679298389703[0m
[37m[1m[2023-07-17 08:58:20,936][257371] Mean Reward across all agents: 927.0732735353876[0m
[37m[1m[2023-07-17 08:58:20,937][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 08:58:20,941][257371] mean_value=-2634.2921505909926, max_value=1282.841297603998[0m
[37m[1m[2023-07-17 08:58:20,944][257371] New mean coefficients: [[ 2.4311862   0.2142613  -0.5882154  -0.68668586  0.11085218 -1.461057  ]][0m
[37m[1m[2023-07-17 08:58:20,945][257371] Moving the mean solution point...[0m
[36m[2023-07-17 08:58:30,044][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 08:58:30,044][257371] FPS: 422091.87[0m
[36m[2023-07-17 08:58:30,047][257371] itr=960, itrs=2000, Progress: 48.00%[0m
[37m[1m[2023-07-17 09:01:54,595][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000940[0m
[36m[2023-07-17 09:02:06,762][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 09:02:06,763][257371] FPS: 330810.43[0m
[36m[2023-07-17 09:02:10,960][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:02:10,961][257371] Reward + Measures: [[5359.46232512    0.36652535    0.25323799    0.14966133    0.14186899
     1.27293766]][0m
[37m[1m[2023-07-17 09:02:10,961][257371] Max Reward on eval: 5359.462325120461[0m
[37m[1m[2023-07-17 09:02:10,961][257371] Min Reward on eval: 5359.462325120461[0m
[37m[1m[2023-07-17 09:02:10,962][257371] Mean Reward across all agents: 5359.462325120461[0m
[37m[1m[2023-07-17 09:02:10,962][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:02:16,214][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:02:16,221][257371] Reward + Measures: [[1228.15331266    0.37269998    0.27809998    0.17620002    0.21950002
     2.08240318]
 [ 294.94436457    0.14170001    0.1998        0.10469999    0.1366
     4.22811651]
 [1016.76086807    0.29350001    0.42460003    0.12          0.29089999
     3.93395686]
 ...
 [ 102.26024335    0.43259999    0.3206        0.39439997    0.27070001
     1.83967268]
 [1126.91144756    0.1653        0.26830003    0.11529999    0.24380003
     3.01322412]
 [  46.35568651    0.078         0.1093        0.0781        0.08670001
     4.86064482]][0m
[37m[1m[2023-07-17 09:02:16,222][257371] Max Reward on eval: 4340.150238044653[0m
[37m[1m[2023-07-17 09:02:16,222][257371] Min Reward on eval: -39.43706737812609[0m
[37m[1m[2023-07-17 09:02:16,223][257371] Mean Reward across all agents: 836.2406008223735[0m
[37m[1m[2023-07-17 09:02:16,224][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:02:16,233][257371] mean_value=-2601.417630423294, max_value=972.2226323049463[0m
[37m[1m[2023-07-17 09:02:16,237][257371] New mean coefficients: [[ 2.490845   -0.592083   -0.82289445 -0.12642622  0.2800489  -1.1344647 ]][0m
[37m[1m[2023-07-17 09:02:16,238][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:02:25,153][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-17 09:02:25,153][257371] FPS: 430824.85[0m
[36m[2023-07-17 09:02:25,156][257371] itr=961, itrs=2000, Progress: 48.05%[0m
[36m[2023-07-17 09:02:36,991][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 09:02:36,992][257371] FPS: 327408.86[0m
[36m[2023-07-17 09:02:41,174][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:02:41,174][257371] Reward + Measures: [[5493.42727582    0.36563399    0.24967301    0.14883333    0.14052901
     1.2602253 ]][0m
[37m[1m[2023-07-17 09:02:41,174][257371] Max Reward on eval: 5493.427275823145[0m
[37m[1m[2023-07-17 09:02:41,175][257371] Min Reward on eval: 5493.427275823145[0m
[37m[1m[2023-07-17 09:02:41,175][257371] Mean Reward across all agents: 5493.427275823145[0m
[37m[1m[2023-07-17 09:02:41,175][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:02:46,144][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:02:46,144][257371] Reward + Measures: [[594.10864068   0.15980001   0.37720001   0.23720001   0.1468
    2.36323857]
 [319.04451584   0.17550001   0.23509999   0.18100001   0.1938
    3.08110023]
 [727.53811648   0.29000002   0.31459999   0.2036       0.19679999
    2.05507851]
 ...
 [141.10933855   0.17940001   0.27919999   0.23639999   0.23559999
    3.75153422]
 [246.41879081   0.1472       0.221        0.22070001   0.13130002
    4.75285196]
 [ 13.32757562   0.21630001   0.2149       0.13259999   0.19509999
    4.2003541 ]][0m
[37m[1m[2023-07-17 09:02:46,145][257371] Max Reward on eval: 4428.495910641749[0m
[37m[1m[2023-07-17 09:02:46,145][257371] Min Reward on eval: -106.65885746867862[0m
[37m[1m[2023-07-17 09:02:46,145][257371] Mean Reward across all agents: 602.696433129923[0m
[37m[1m[2023-07-17 09:02:46,145][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:02:46,148][257371] mean_value=-3300.9749132834063, max_value=729.7607540804875[0m
[37m[1m[2023-07-17 09:02:46,150][257371] New mean coefficients: [[ 2.3930612  -0.05379468 -1.1614137  -0.24180669 -0.02454951 -0.991314  ]][0m
[37m[1m[2023-07-17 09:02:46,151][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:02:55,172][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 09:02:55,173][257371] FPS: 425746.64[0m
[36m[2023-07-17 09:02:55,175][257371] itr=962, itrs=2000, Progress: 48.10%[0m
[36m[2023-07-17 09:03:06,893][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 09:03:06,893][257371] FPS: 330636.91[0m
[36m[2023-07-17 09:03:11,268][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:03:11,268][257371] Reward + Measures: [[5621.54863088    0.35165864    0.25053164    0.150868      0.141776
     1.28121769]][0m
[37m[1m[2023-07-17 09:03:11,268][257371] Max Reward on eval: 5621.548630879081[0m
[37m[1m[2023-07-17 09:03:11,269][257371] Min Reward on eval: 5621.548630879081[0m
[37m[1m[2023-07-17 09:03:11,269][257371] Mean Reward across all agents: 5621.548630879081[0m
[37m[1m[2023-07-17 09:03:11,269][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:03:16,275][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:03:16,276][257371] Reward + Measures: [[267.61751079   0.2059       0.1996       0.1488       0.16410001
    2.89604235]
 [ 79.62913573   0.1737       0.1684       0.1081       0.12329999
    3.83638072]
 [ 10.84111214   0.0803       0.13540001   0.1269       0.11390001
    4.19103146]
 ...
 [ 51.23675526   0.1025       0.085        0.0762       0.1344
    4.86320877]
 [ 73.5819659    0.12630001   0.11440001   0.1079       0.0809
    5.06056452]
 [204.45103644   0.17640002   0.27239999   0.2194       0.2142
    4.19009542]][0m
[37m[1m[2023-07-17 09:03:16,276][257371] Max Reward on eval: 3868.416595462337[0m
[37m[1m[2023-07-17 09:03:16,276][257371] Min Reward on eval: -72.84552390258759[0m
[37m[1m[2023-07-17 09:03:16,277][257371] Mean Reward across all agents: 401.31340506551015[0m
[37m[1m[2023-07-17 09:03:16,277][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:03:16,279][257371] mean_value=-3471.416783937814, max_value=602.5575701247958[0m
[37m[1m[2023-07-17 09:03:16,282][257371] New mean coefficients: [[ 2.5447774   0.8631121  -0.68710077  0.6458045  -0.7027806  -1.432177  ]][0m
[37m[1m[2023-07-17 09:03:16,282][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:03:25,294][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 09:03:25,294][257371] FPS: 426197.42[0m
[36m[2023-07-17 09:03:25,297][257371] itr=963, itrs=2000, Progress: 48.15%[0m
[36m[2023-07-17 09:03:37,015][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 09:03:37,016][257371] FPS: 330726.45[0m
[36m[2023-07-17 09:03:41,262][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:03:41,262][257371] Reward + Measures: [[5690.54792108    0.34775969    0.24863867    0.1539        0.14552365
     1.29844356]][0m
[37m[1m[2023-07-17 09:03:41,263][257371] Max Reward on eval: 5690.547921076417[0m
[37m[1m[2023-07-17 09:03:41,263][257371] Min Reward on eval: 5690.547921076417[0m
[37m[1m[2023-07-17 09:03:41,263][257371] Mean Reward across all agents: 5690.547921076417[0m
[37m[1m[2023-07-17 09:03:41,263][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:03:46,237][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:03:46,238][257371] Reward + Measures: [[ 230.52262685    0.2507        0.1945        0.28509998    0.17809999
     3.41874695]
 [1143.90461732    0.37909999    0.36590001    0.25939998    0.28780001
     1.45622742]
 [ 319.36142159    0.19579999    0.26219997    0.1451        0.15870002
     4.21977758]
 ...
 [ 997.36331029    0.22409999    0.17840001    0.1754        0.1339
     1.87429082]
 [ 227.77462763    0.1531        0.27079999    0.14930001    0.13770001
     3.31336212]
 [  13.02630391    0.23360001    0.3554        0.1706        0.30329999
     4.78378582]][0m
[37m[1m[2023-07-17 09:03:46,238][257371] Max Reward on eval: 4307.322486862167[0m
[37m[1m[2023-07-17 09:03:46,238][257371] Min Reward on eval: -57.98824158688076[0m
[37m[1m[2023-07-17 09:03:46,238][257371] Mean Reward across all agents: 567.7376248502129[0m
[37m[1m[2023-07-17 09:03:46,239][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:03:46,242][257371] mean_value=-3193.1740693730667, max_value=1898.820004054562[0m
[37m[1m[2023-07-17 09:03:46,245][257371] New mean coefficients: [[ 2.1024363   1.15027    -1.3976583   0.31204215 -0.9701793  -1.4759983 ]][0m
[37m[1m[2023-07-17 09:03:46,246][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:03:55,173][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 09:03:55,173][257371] FPS: 430215.91[0m
[36m[2023-07-17 09:03:55,176][257371] itr=964, itrs=2000, Progress: 48.20%[0m
[36m[2023-07-17 09:04:07,130][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-17 09:04:07,131][257371] FPS: 324014.39[0m
[36m[2023-07-17 09:04:11,422][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:04:11,423][257371] Reward + Measures: [[5816.67278338    0.35795498    0.23484665    0.156138      0.146308
     1.30800116]][0m
[37m[1m[2023-07-17 09:04:11,423][257371] Max Reward on eval: 5816.672783382179[0m
[37m[1m[2023-07-17 09:04:11,423][257371] Min Reward on eval: 5816.672783382179[0m
[37m[1m[2023-07-17 09:04:11,423][257371] Mean Reward across all agents: 5816.672783382179[0m
[37m[1m[2023-07-17 09:04:11,424][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:04:16,453][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:04:16,454][257371] Reward + Measures: [[  77.08443248    0.23360001    0.1851        0.18520001    0.16610001
     4.17759705]
 [ 608.9740982     0.1398        0.17550001    0.13020001    0.11689999
     3.83792925]
 [  87.51584309    0.12840001    0.2034        0.1498        0.1275
     5.04557562]
 ...
 [2022.32289124    0.2464        0.39309999    0.20030001    0.16599999
     1.71130311]
 [ 155.0485443     0.20320001    0.16410001    0.14490001    0.1611
     2.84173632]
 [ 296.28942683    0.38240001    0.2448        0.24270001    0.27250001
     3.70002055]][0m
[37m[1m[2023-07-17 09:04:16,454][257371] Max Reward on eval: 3555.6021728350956[0m
[37m[1m[2023-07-17 09:04:16,454][257371] Min Reward on eval: -67.42979413862341[0m
[37m[1m[2023-07-17 09:04:16,454][257371] Mean Reward across all agents: 317.79806214231206[0m
[37m[1m[2023-07-17 09:04:16,455][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:04:16,456][257371] mean_value=-2812.1478964352154, max_value=566.3162297928145[0m
[37m[1m[2023-07-17 09:04:16,458][257371] New mean coefficients: [[ 1.8869046   0.63956594 -1.1959088  -0.69438136 -0.40209186 -1.4520195 ]][0m
[37m[1m[2023-07-17 09:04:16,459][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:04:25,524][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 09:04:25,524][257371] FPS: 423687.19[0m
[36m[2023-07-17 09:04:25,527][257371] itr=965, itrs=2000, Progress: 48.25%[0m
[36m[2023-07-17 09:04:37,200][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 09:04:37,201][257371] FPS: 331999.19[0m
[36m[2023-07-17 09:04:41,465][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:04:41,465][257371] Reward + Measures: [[5957.78801841    0.35739297    0.22649698    0.15715       0.14713767
     1.32300293]][0m
[37m[1m[2023-07-17 09:04:41,465][257371] Max Reward on eval: 5957.788018414425[0m
[37m[1m[2023-07-17 09:04:41,465][257371] Min Reward on eval: 5957.788018414425[0m
[37m[1m[2023-07-17 09:04:41,466][257371] Mean Reward across all agents: 5957.788018414425[0m
[37m[1m[2023-07-17 09:04:41,466][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:04:46,447][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:04:46,448][257371] Reward + Measures: [[206.78121161   0.19150001   0.18430001   0.19429998   0.1473
    4.87862301]
 [324.26451829   0.3123       0.29770002   0.2967       0.24569999
    3.80522656]
 [223.31464713   0.27200001   0.28760001   0.10469999   0.24399999
    4.01357794]
 ...
 [494.89038179   0.33770001   0.26200005   0.28299999   0.1876
    3.0165422 ]
 [514.54294398   0.3784       0.35440001   0.2271       0.27809998
    2.39565301]
 [ 21.52472201   0.1366       0.1104       0.0658       0.08620001
    5.62358427]][0m
[37m[1m[2023-07-17 09:04:46,448][257371] Max Reward on eval: 2765.121322607761[0m
[37m[1m[2023-07-17 09:04:46,448][257371] Min Reward on eval: -31.061220820620655[0m
[37m[1m[2023-07-17 09:04:46,448][257371] Mean Reward across all agents: 411.0259951023421[0m
[37m[1m[2023-07-17 09:04:46,449][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:04:46,452][257371] mean_value=-2087.20242509915, max_value=403.08274018377915[0m
[37m[1m[2023-07-17 09:04:46,454][257371] New mean coefficients: [[ 1.6253637   0.44226468 -1.8695664  -2.0569804   0.56144696 -1.046106  ]][0m
[37m[1m[2023-07-17 09:04:46,455][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:04:55,390][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 09:04:55,390][257371] FPS: 429880.02[0m
[36m[2023-07-17 09:04:55,392][257371] itr=966, itrs=2000, Progress: 48.30%[0m
[36m[2023-07-17 09:05:07,306][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 09:05:07,307][257371] FPS: 325219.49[0m
[36m[2023-07-17 09:05:11,664][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:05:11,669][257371] Reward + Measures: [[5964.1048585     0.34776932    0.220531      0.155494      0.14468867
     1.30100715]][0m
[37m[1m[2023-07-17 09:05:11,669][257371] Max Reward on eval: 5964.104858499639[0m
[37m[1m[2023-07-17 09:05:11,669][257371] Min Reward on eval: 5964.104858499639[0m
[37m[1m[2023-07-17 09:05:11,670][257371] Mean Reward across all agents: 5964.104858499639[0m
[37m[1m[2023-07-17 09:05:11,670][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:05:17,003][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:05:17,008][257371] Reward + Measures: [[  59.82763799    0.33130002    0.30380002    0.29850003    0.29800001
     4.74116373]
 [1831.99609374    0.40599999    0.41339999    0.26089999    0.2124
     1.24082017]
 [ 512.62296145    0.29260001    0.17780001    0.23889999    0.235
     2.76523662]
 ...
 [  21.36833458    0.32390004    0.29500002    0.2798        0.289
     4.68231297]
 [   7.27898054    0.23790002    0.24099998    0.2181        0.23930001
     5.41121483]
 [  19.15642626    0.2177        0.22040001    0.22320001    0.19700001
     4.01626062]][0m
[37m[1m[2023-07-17 09:05:17,009][257371] Max Reward on eval: 5256.239440936782[0m
[37m[1m[2023-07-17 09:05:17,009][257371] Min Reward on eval: -98.52945890855044[0m
[37m[1m[2023-07-17 09:05:17,009][257371] Mean Reward across all agents: 276.2532247493425[0m
[37m[1m[2023-07-17 09:05:17,010][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:05:17,012][257371] mean_value=-1870.8035819848917, max_value=418.98303599016197[0m
[37m[1m[2023-07-17 09:05:17,015][257371] New mean coefficients: [[ 1.7168341   1.7307795  -1.8375696  -1.6729878  -0.62111825 -0.94230735]][0m
[37m[1m[2023-07-17 09:05:17,016][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:05:26,036][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 09:05:26,036][257371] FPS: 425805.23[0m
[36m[2023-07-17 09:05:26,038][257371] itr=967, itrs=2000, Progress: 48.35%[0m
[36m[2023-07-17 09:05:37,891][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 09:05:37,891][257371] FPS: 326905.63[0m
[36m[2023-07-17 09:05:42,104][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:05:42,104][257371] Reward + Measures: [[6115.75578947    0.3473843     0.21328068    0.15709966    0.14578533
     1.31683362]][0m
[37m[1m[2023-07-17 09:05:42,105][257371] Max Reward on eval: 6115.7557894664515[0m
[37m[1m[2023-07-17 09:05:42,105][257371] Min Reward on eval: 6115.7557894664515[0m
[37m[1m[2023-07-17 09:05:42,105][257371] Mean Reward across all agents: 6115.7557894664515[0m
[37m[1m[2023-07-17 09:05:42,105][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:05:47,097][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:05:47,098][257371] Reward + Measures: [[687.90975572   0.34459999   0.31780002   0.2748       0.2951
    2.77884436]
 [163.61568357   0.16860001   0.18009999   0.1433       0.12029999
    4.97945023]
 [887.90762521   0.24630001   0.21560001   0.17349999   0.24389999
    3.26152968]
 ...
 [-18.9942633    0.13090001   0.0954       0.1851       0.1212
    5.33307362]
 [468.21215057   0.3177       0.47550002   0.32690001   0.32990003
    1.51644063]
 [944.41340068   0.27350003   0.19289999   0.18310001   0.21990001
    3.91460347]][0m
[37m[1m[2023-07-17 09:05:47,098][257371] Max Reward on eval: 1833.944152865745[0m
[37m[1m[2023-07-17 09:05:47,098][257371] Min Reward on eval: -44.13420822157059[0m
[37m[1m[2023-07-17 09:05:47,098][257371] Mean Reward across all agents: 291.10068500801435[0m
[37m[1m[2023-07-17 09:05:47,099][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:05:47,101][257371] mean_value=-2263.4719019073987, max_value=767.333070706221[0m
[37m[1m[2023-07-17 09:05:47,103][257371] New mean coefficients: [[ 2.093886   1.2671965 -2.184661  -2.2762358  1.4850855 -0.818363 ]][0m
[37m[1m[2023-07-17 09:05:47,104][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:05:56,129][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 09:05:56,129][257371] FPS: 425563.94[0m
[36m[2023-07-17 09:05:56,131][257371] itr=968, itrs=2000, Progress: 48.40%[0m
[36m[2023-07-17 09:06:07,842][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 09:06:07,842][257371] FPS: 330847.50[0m
[36m[2023-07-17 09:06:12,135][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:06:12,136][257371] Reward + Measures: [[6169.56485188    0.33826232    0.22048832    0.15338334    0.143003
     1.30001342]][0m
[37m[1m[2023-07-17 09:06:12,136][257371] Max Reward on eval: 6169.564851875851[0m
[37m[1m[2023-07-17 09:06:12,136][257371] Min Reward on eval: 6169.564851875851[0m
[37m[1m[2023-07-17 09:06:12,137][257371] Mean Reward across all agents: 6169.564851875851[0m
[37m[1m[2023-07-17 09:06:12,137][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:06:17,141][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:06:17,142][257371] Reward + Measures: [[163.84184272   0.19219999   0.1105       0.16190001   0.15220001
    5.69571257]
 [309.84070774   0.51160002   0.53299999   0.41359997   0.52130002
    4.47910023]
 [114.66486831   0.1373       0.2775       0.1717       0.24580002
    4.10932398]
 ...
 [ 55.1341912    0.20469999   0.18880001   0.1516       0.20080002
    4.38360548]
 [ 84.23466109   0.10820001   0.1066       0.10389999   0.1497
    5.37583637]
 [-47.61851105   0.1445       0.2484       0.15580001   0.1761
    5.18955898]][0m
[37m[1m[2023-07-17 09:06:17,142][257371] Max Reward on eval: 2342.100677454029[0m
[37m[1m[2023-07-17 09:06:17,143][257371] Min Reward on eval: -132.43487456813455[0m
[37m[1m[2023-07-17 09:06:17,143][257371] Mean Reward across all agents: 182.27642896202786[0m
[37m[1m[2023-07-17 09:06:17,143][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:06:17,146][257371] mean_value=-1309.8763273522366, max_value=255.26481260194825[0m
[37m[1m[2023-07-17 09:06:17,148][257371] New mean coefficients: [[ 2.0715437   0.13407433 -2.9788618  -2.1709461   3.0157623  -0.70969033]][0m
[37m[1m[2023-07-17 09:06:17,149][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:06:26,168][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 09:06:26,169][257371] FPS: 425841.86[0m
[36m[2023-07-17 09:06:26,171][257371] itr=969, itrs=2000, Progress: 48.45%[0m
[36m[2023-07-17 09:06:38,256][257371] train() took 11.98 seconds to complete[0m
[36m[2023-07-17 09:06:38,257][257371] FPS: 320473.52[0m
[36m[2023-07-17 09:06:42,614][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:06:42,614][257371] Reward + Measures: [[6294.28264632    0.30714533    0.20348202    0.15274233    0.14459233
     1.32142663]][0m
[37m[1m[2023-07-17 09:06:42,614][257371] Max Reward on eval: 6294.282646320209[0m
[37m[1m[2023-07-17 09:06:42,615][257371] Min Reward on eval: 6294.282646320209[0m
[37m[1m[2023-07-17 09:06:42,615][257371] Mean Reward across all agents: 6294.282646320209[0m
[37m[1m[2023-07-17 09:06:42,615][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:06:47,640][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:06:47,640][257371] Reward + Measures: [[ 72.82817236   0.35120001   0.27560002   0.3522       0.10030001
    5.77353334]
 [  6.29037464   0.15820001   0.1577       0.1621       0.2027
    5.0179162 ]
 [ 90.65814729   0.21689999   0.2705       0.2103       0.20010002
    5.36800814]
 ...
 [188.78605363   0.45210001   0.53670001   0.35609999   0.35299999
    3.67587161]
 [ 96.42700183   0.1323       0.17110001   0.1071       0.1176
    4.99454927]
 [ 46.07267599   0.36920002   0.30700001   0.30160001   0.2227
    5.1587286 ]][0m
[37m[1m[2023-07-17 09:06:47,641][257371] Max Reward on eval: 1205.9214477190749[0m
[37m[1m[2023-07-17 09:06:47,641][257371] Min Reward on eval: -176.0021524446085[0m
[37m[1m[2023-07-17 09:06:47,641][257371] Mean Reward across all agents: 70.5419045187432[0m
[37m[1m[2023-07-17 09:06:47,641][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:06:47,644][257371] mean_value=-771.8363907629387, max_value=180.50271536431677[0m
[37m[1m[2023-07-17 09:06:47,646][257371] New mean coefficients: [[ 2.328115   -0.12234071 -2.3198466  -1.900071    2.1160302  -0.34704372]][0m
[37m[1m[2023-07-17 09:06:47,647][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:06:56,698][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 09:06:56,699][257371] FPS: 424322.00[0m
[36m[2023-07-17 09:06:56,701][257371] itr=970, itrs=2000, Progress: 48.50%[0m
[37m[1m[2023-07-17 09:10:17,303][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000950[0m
[36m[2023-07-17 09:10:29,519][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 09:10:29,519][257371] FPS: 329729.80[0m
[36m[2023-07-17 09:10:33,721][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:10:33,722][257371] Reward + Measures: [[6329.46514566    0.30628002    0.19301534    0.15166099    0.14419332
     1.34529734]][0m
[37m[1m[2023-07-17 09:10:33,722][257371] Max Reward on eval: 6329.465145662314[0m
[37m[1m[2023-07-17 09:10:33,722][257371] Min Reward on eval: 6329.465145662314[0m
[37m[1m[2023-07-17 09:10:33,722][257371] Mean Reward across all agents: 6329.465145662314[0m
[37m[1m[2023-07-17 09:10:33,723][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:10:38,740][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:10:38,741][257371] Reward + Measures: [[ 41.74486195   0.18430001   0.20470002   0.2326       0.17880002
    4.70942831]
 [ -8.85079028   0.1158       0.1002       0.12050001   0.1195
    5.66400433]
 [289.39586054   0.20510001   0.2392       0.18749999   0.17960002
    3.2201519 ]
 ...
 [425.8442765    0.29480001   0.2103       0.27849999   0.22980002
    2.86612082]
 [298.61478062   0.15820001   0.19599999   0.1221       0.15320002
    4.55453968]
 [ -3.21400603   0.12970001   0.1151       0.0944       0.12420001
    6.54230595]][0m
[37m[1m[2023-07-17 09:10:38,741][257371] Max Reward on eval: 4239.959411600546[0m
[37m[1m[2023-07-17 09:10:38,741][257371] Min Reward on eval: -102.76911263032817[0m
[37m[1m[2023-07-17 09:10:38,742][257371] Mean Reward across all agents: 229.21212328877493[0m
[37m[1m[2023-07-17 09:10:38,742][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:10:38,744][257371] mean_value=-1895.2066252463494, max_value=699.8532688283434[0m
[37m[1m[2023-07-17 09:10:38,747][257371] New mean coefficients: [[ 1.3311411  -2.4158518  -2.6978455  -2.380191    2.404089   -0.39570722]][0m
[37m[1m[2023-07-17 09:10:38,748][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:10:47,818][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 09:10:47,818][257371] FPS: 423424.52[0m
[36m[2023-07-17 09:10:47,821][257371] itr=971, itrs=2000, Progress: 48.55%[0m
[36m[2023-07-17 09:10:59,674][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 09:10:59,674][257371] FPS: 326834.44[0m
[36m[2023-07-17 09:11:03,906][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:11:03,906][257371] Reward + Measures: [[6391.03255958    0.28677332    0.18307199    0.14435133    0.13972734
     1.3522085 ]][0m
[37m[1m[2023-07-17 09:11:03,907][257371] Max Reward on eval: 6391.032559584841[0m
[37m[1m[2023-07-17 09:11:03,907][257371] Min Reward on eval: 6391.032559584841[0m
[37m[1m[2023-07-17 09:11:03,907][257371] Mean Reward across all agents: 6391.032559584841[0m
[37m[1m[2023-07-17 09:11:03,907][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:11:08,808][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:11:08,809][257371] Reward + Measures: [[256.40521789   0.55089998   0.37140003   0.37280002   0.38960001
    3.90368652]
 [ 15.05424201   0.6735       0.1445       0.6207       0.13630001
    4.62871456]
 [171.02002658   0.1737       0.1105       0.19320002   0.1344
    3.21547484]
 ...
 [275.04932474   0.15989999   0.1645       0.17989999   0.16850001
    3.00703478]
 [104.84496622   0.2163       0.21400002   0.12150001   0.2014
    4.02810049]
 [205.46204954   0.37380001   0.25470003   0.27869999   0.2402
    3.41049957]][0m
[37m[1m[2023-07-17 09:11:08,809][257371] Max Reward on eval: 2060.6967849358916[0m
[37m[1m[2023-07-17 09:11:08,809][257371] Min Reward on eval: -377.1105883810669[0m
[37m[1m[2023-07-17 09:11:08,810][257371] Mean Reward across all agents: 272.6695192040493[0m
[37m[1m[2023-07-17 09:11:08,810][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:11:08,813][257371] mean_value=-1950.601787786648, max_value=531.9525267434387[0m
[37m[1m[2023-07-17 09:11:08,816][257371] New mean coefficients: [[ 1.1658285  -3.7519813  -2.156094   -4.0722446   2.4753058   0.12101677]][0m
[37m[1m[2023-07-17 09:11:08,817][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:11:17,768][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 09:11:17,768][257371] FPS: 429088.44[0m
[36m[2023-07-17 09:11:17,770][257371] itr=972, itrs=2000, Progress: 48.60%[0m
[36m[2023-07-17 09:11:29,732][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-17 09:11:29,733][257371] FPS: 323750.44[0m
[36m[2023-07-17 09:11:34,014][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:11:34,014][257371] Reward + Measures: [[6474.04673462    0.270594      0.18284701    0.14822534    0.146906
     1.41252565]][0m
[37m[1m[2023-07-17 09:11:34,014][257371] Max Reward on eval: 6474.046734622013[0m
[37m[1m[2023-07-17 09:11:34,014][257371] Min Reward on eval: 6474.046734622013[0m
[37m[1m[2023-07-17 09:11:34,015][257371] Mean Reward across all agents: 6474.046734622013[0m
[37m[1m[2023-07-17 09:11:34,015][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:11:39,270][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:11:39,271][257371] Reward + Measures: [[188.99670675   0.2124       0.25330001   0.2438       0.22040001
    4.55165052]
 [ 36.83949188   0.2227       0.2093       0.20910001   0.1999
    5.51823902]
 [ 32.15230025   0.1807       0.19240001   0.19649999   0.1374
    5.40625381]
 ...
 [183.66308381   0.2376       0.23560002   0.23409998   0.21510001
    4.36429453]
 [ 97.74232207   0.19319999   0.153        0.0897       0.0905
    4.30525446]
 [-55.69927985   0.14650001   0.1488       0.1441       0.1183
    5.63629389]][0m
[37m[1m[2023-07-17 09:11:39,271][257371] Max Reward on eval: 678.6376963242889[0m
[37m[1m[2023-07-17 09:11:39,271][257371] Min Reward on eval: -145.41743999272586[0m
[37m[1m[2023-07-17 09:11:39,271][257371] Mean Reward across all agents: 75.39718474729835[0m
[37m[1m[2023-07-17 09:11:39,272][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:11:39,273][257371] mean_value=-1314.544705371869, max_value=341.75160636976295[0m
[37m[1m[2023-07-17 09:11:39,276][257371] New mean coefficients: [[ 1.2099638  -3.2063668  -2.6558335  -4.39079     3.2468498   0.56050926]][0m
[37m[1m[2023-07-17 09:11:39,277][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:11:48,369][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 09:11:48,369][257371] FPS: 422426.18[0m
[36m[2023-07-17 09:11:48,371][257371] itr=973, itrs=2000, Progress: 48.65%[0m
[36m[2023-07-17 09:12:00,172][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 09:12:00,172][257371] FPS: 328295.59[0m
[36m[2023-07-17 09:12:04,503][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:12:04,504][257371] Reward + Measures: [[6541.88899178    0.25114766    0.17836666    0.14877465    0.14883134
     1.44988966]][0m
[37m[1m[2023-07-17 09:12:04,504][257371] Max Reward on eval: 6541.888991775716[0m
[37m[1m[2023-07-17 09:12:04,504][257371] Min Reward on eval: 6541.888991775716[0m
[37m[1m[2023-07-17 09:12:04,504][257371] Mean Reward across all agents: 6541.888991775716[0m
[37m[1m[2023-07-17 09:12:04,505][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:12:09,460][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:12:09,460][257371] Reward + Measures: [[ 24.25027146   0.0882       0.0773       0.0902       0.0708
    4.66590691]
 [110.57865297   0.39489999   0.20990001   0.22690001   0.34059998
    5.61803198]
 [ 11.97515063   0.1147       0.15539999   0.1129       0.149
    6.14645624]
 ...
 [ 17.89647675   0.14839999   0.18009999   0.12770002   0.17220001
    5.99426985]
 [122.34522126   0.1709       0.1473       0.1138       0.16910002
    5.11208296]
 [ 19.02592079   0.1193       0.1406       0.1453       0.1348
    6.20051575]][0m
[37m[1m[2023-07-17 09:12:09,461][257371] Max Reward on eval: 2660.9376373179257[0m
[37m[1m[2023-07-17 09:12:09,461][257371] Min Reward on eval: -62.71097643268295[0m
[37m[1m[2023-07-17 09:12:09,461][257371] Mean Reward across all agents: 130.42703426940608[0m
[37m[1m[2023-07-17 09:12:09,461][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:12:09,464][257371] mean_value=-1040.4325758703296, max_value=115.10364951825528[0m
[37m[1m[2023-07-17 09:12:09,466][257371] New mean coefficients: [[ 0.58511424 -4.949565   -3.383255   -4.6686225   1.0096738   0.15744805]][0m
[37m[1m[2023-07-17 09:12:09,467][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:12:18,440][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 09:12:18,440][257371] FPS: 428049.99[0m
[36m[2023-07-17 09:12:18,442][257371] itr=974, itrs=2000, Progress: 48.70%[0m
[36m[2023-07-17 09:12:30,214][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 09:12:30,214][257371] FPS: 329052.09[0m
[36m[2023-07-17 09:12:34,557][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:12:34,557][257371] Reward + Measures: [[6602.39239178    0.24912333    0.17449667    0.14973934    0.14958733
     1.48614681]][0m
[37m[1m[2023-07-17 09:12:34,558][257371] Max Reward on eval: 6602.392391777652[0m
[37m[1m[2023-07-17 09:12:34,558][257371] Min Reward on eval: 6602.392391777652[0m
[37m[1m[2023-07-17 09:12:34,558][257371] Mean Reward across all agents: 6602.392391777652[0m
[37m[1m[2023-07-17 09:12:34,558][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:12:39,580][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:12:39,580][257371] Reward + Measures: [[ 14.68291233   0.2113       0.3364       0.1727       0.3197
    4.8154273 ]
 [-36.45334089   0.2538       0.23899999   0.20050001   0.2247
    5.41362524]
 [409.3367381    0.2066       0.2203       0.20509999   0.1718
    2.96285987]
 ...
 [908.85867301   0.317        0.3276       0.18200001   0.26030001
    2.24809194]
 [135.933414     0.22090001   0.2185       0.15710001   0.19890001
    4.39100981]
 [ 37.52341435   0.30920002   0.27160001   0.25650001   0.26440001
    5.5601387 ]][0m
[37m[1m[2023-07-17 09:12:39,581][257371] Max Reward on eval: 1433.6232337731867[0m
[37m[1m[2023-07-17 09:12:39,581][257371] Min Reward on eval: -116.4800222656224[0m
[37m[1m[2023-07-17 09:12:39,581][257371] Mean Reward across all agents: 118.00440685526488[0m
[37m[1m[2023-07-17 09:12:39,581][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:12:39,582][257371] mean_value=-1786.7615239915997, max_value=6.202464947867014[0m
[37m[1m[2023-07-17 09:12:39,585][257371] New mean coefficients: [[ 0.14582962 -4.385347   -3.3163958  -6.3122063   1.7840784   0.01250599]][0m
[37m[1m[2023-07-17 09:12:39,586][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:12:48,592][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 09:12:48,592][257371] FPS: 426457.28[0m
[36m[2023-07-17 09:12:48,594][257371] itr=975, itrs=2000, Progress: 48.75%[0m
[36m[2023-07-17 09:13:00,499][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 09:13:00,499][257371] FPS: 325420.99[0m
[36m[2023-07-17 09:13:04,749][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:13:04,750][257371] Reward + Measures: [[6304.77516488    0.21849166    0.18013932    0.14786966    0.15046467
     1.486081  ]][0m
[37m[1m[2023-07-17 09:13:04,750][257371] Max Reward on eval: 6304.775164876426[0m
[37m[1m[2023-07-17 09:13:04,750][257371] Min Reward on eval: 6304.775164876426[0m
[37m[1m[2023-07-17 09:13:04,750][257371] Mean Reward across all agents: 6304.775164876426[0m
[37m[1m[2023-07-17 09:13:04,751][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:13:09,721][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:13:09,722][257371] Reward + Measures: [[138.43327091   0.16429999   0.17920001   0.21619999   0.21180001
    4.90798807]
 [285.94766999   0.24320002   0.20480001   0.2211       0.18670002
    4.42432261]
 [187.67364934   0.1348       0.16610001   0.175        0.1674
    4.66250181]
 ...
 [ 11.17398568   0.17690001   0.12809999   0.212        0.17349999
    5.52544546]
 [161.49626297   0.1384       0.1628       0.19060001   0.18359999
    4.98363447]
 [ 28.4083588    0.1593       0.1232       0.16800001   0.13789999
    5.66527319]][0m
[37m[1m[2023-07-17 09:13:09,722][257371] Max Reward on eval: 2582.1582107760014[0m
[37m[1m[2023-07-17 09:13:09,722][257371] Min Reward on eval: -53.2028445049189[0m
[37m[1m[2023-07-17 09:13:09,722][257371] Mean Reward across all agents: 125.50351657450689[0m
[37m[1m[2023-07-17 09:13:09,723][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:13:09,724][257371] mean_value=-1021.063158000273, max_value=147.72784285254534[0m
[37m[1m[2023-07-17 09:13:09,727][257371] New mean coefficients: [[-0.48302245 -6.1882553  -4.028747   -5.83649     1.7452209  -0.08196682]][0m
[37m[1m[2023-07-17 09:13:09,728][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:13:18,786][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 09:13:18,786][257371] FPS: 423996.80[0m
[36m[2023-07-17 09:13:18,788][257371] itr=976, itrs=2000, Progress: 48.80%[0m
[36m[2023-07-17 09:13:30,649][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 09:13:30,649][257371] FPS: 326621.17[0m
[36m[2023-07-17 09:13:34,971][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:13:34,972][257371] Reward + Measures: [[1800.16060241    0.19769       0.23360799    0.164553      0.17519768
     1.28718388]][0m
[37m[1m[2023-07-17 09:13:34,972][257371] Max Reward on eval: 1800.1606024092432[0m
[37m[1m[2023-07-17 09:13:34,972][257371] Min Reward on eval: 1800.1606024092432[0m
[37m[1m[2023-07-17 09:13:34,972][257371] Mean Reward across all agents: 1800.1606024092432[0m
[37m[1m[2023-07-17 09:13:34,973][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:13:40,010][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:13:40,010][257371] Reward + Measures: [[  9.29334289   0.1007       0.1008       0.1086       0.09769999
    6.27654409]
 [  5.13426889   0.10420001   0.13950001   0.1525       0.1151
    5.75022364]
 [ 13.06181482   0.1026       0.11369999   0.1285       0.11790001
    5.15566874]
 ...
 [ 18.06965946   0.077        0.1163       0.16599999   0.08
    5.65669632]
 [128.67405071   0.19090001   0.41279998   0.25190002   0.2678
    3.47768712]
 [  4.50865868   0.0725       0.0878       0.1056       0.0837
    6.10307407]][0m
[37m[1m[2023-07-17 09:13:40,010][257371] Max Reward on eval: 652.608078932995[0m
[37m[1m[2023-07-17 09:13:40,011][257371] Min Reward on eval: -191.0813961170614[0m
[37m[1m[2023-07-17 09:13:40,011][257371] Mean Reward across all agents: 27.9757356970895[0m
[37m[1m[2023-07-17 09:13:40,011][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:13:40,012][257371] mean_value=-534.0537063313786, max_value=-109.30890076812375[0m
[36m[2023-07-17 09:13:40,015][257371] XNES is restarting with a new solution whose measures are [0.23220001 0.69459999 0.40170002 0.60100001 7.69239759] and objective is 138.82989232093095[0m
[36m[2023-07-17 09:13:40,016][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 09:13:40,018][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 09:13:40,019][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:13:49,064][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 09:13:49,065][257371] FPS: 424565.65[0m
[36m[2023-07-17 09:13:49,067][257371] itr=977, itrs=2000, Progress: 48.85%[0m
[36m[2023-07-17 09:14:00,765][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 09:14:00,766][257371] FPS: 331260.73[0m
[36m[2023-07-17 09:14:04,984][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:14:04,984][257371] Reward + Measures: [[-75.3340452    0.230828     0.35915062   0.08077266   0.26589733
    7.52879667]][0m
[37m[1m[2023-07-17 09:14:04,985][257371] Max Reward on eval: -75.33404520438559[0m
[37m[1m[2023-07-17 09:14:04,985][257371] Min Reward on eval: -75.33404520438559[0m
[37m[1m[2023-07-17 09:14:04,985][257371] Mean Reward across all agents: -75.33404520438559[0m
[37m[1m[2023-07-17 09:14:04,986][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:14:09,997][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:14:09,998][257371] Reward + Measures: [[ -3.17089297   0.10860001   0.16010001   0.0782       0.0795
    6.91006422]
 [-41.44069704   0.28959998   0.1063       0.26570001   0.22790001
    6.54507589]
 [ -5.13397712   0.1505       0.2184       0.1129       0.1266
    7.25851536]
 ...
 [ -7.50921659   0.1285       0.16489999   0.09980001   0.1138
    7.28159046]
 [ 30.98127165   0.1637       0.22850001   0.0763       0.1619
    7.44944859]
 [-44.31948038   0.0903       0.164        0.10399999   0.11980001
    7.08826399]][0m
[37m[1m[2023-07-17 09:14:09,998][257371] Max Reward on eval: 547.068690783903[0m
[37m[1m[2023-07-17 09:14:09,998][257371] Min Reward on eval: -283.11098767730874[0m
[37m[1m[2023-07-17 09:14:09,999][257371] Mean Reward across all agents: 16.46375428165374[0m
[37m[1m[2023-07-17 09:14:09,999][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:14:10,002][257371] mean_value=-157.3727006730808, max_value=755.9588171449211[0m
[37m[1m[2023-07-17 09:14:10,005][257371] New mean coefficients: [[-0.21305707 -1.5379493  -1.5695854  -1.5779263  -2.411062   -0.8920504 ]][0m
[37m[1m[2023-07-17 09:14:10,006][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:14:19,158][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-17 09:14:19,159][257371] FPS: 419655.51[0m
[36m[2023-07-17 09:14:19,161][257371] itr=978, itrs=2000, Progress: 48.90%[0m
[36m[2023-07-17 09:14:31,016][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 09:14:31,016][257371] FPS: 326813.29[0m
[36m[2023-07-17 09:14:35,302][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:14:35,302][257371] Reward + Measures: [[-73.15313395   0.18270731   0.29092899   0.08140834   0.21433032
    7.17069244]][0m
[37m[1m[2023-07-17 09:14:35,302][257371] Max Reward on eval: -73.1531339471718[0m
[37m[1m[2023-07-17 09:14:35,303][257371] Min Reward on eval: -73.1531339471718[0m
[37m[1m[2023-07-17 09:14:35,303][257371] Mean Reward across all agents: -73.1531339471718[0m
[37m[1m[2023-07-17 09:14:35,303][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:14:40,569][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:14:40,575][257371] Reward + Measures: [[ -6.92326899   0.14650001   0.15979999   0.1349       0.07430001
    6.62396955]
 [ 11.91411485   0.13000001   0.11570001   0.1631       0.1313
    7.60766935]
 [-45.83553746   0.42799997   0.40890002   0.39729998   0.0386
    7.27576971]
 ...
 [ 91.23553314   0.30310002   0.18280001   0.30939999   0.16760002
    7.20546103]
 [ 18.61591543   0.49260002   0.5668       0.32319999   0.52880001
    7.65744162]
 [ 42.13992743   0.34990001   0.1823       0.2811       0.2493
    6.86315775]][0m
[37m[1m[2023-07-17 09:14:40,576][257371] Max Reward on eval: 507.5732307834551[0m
[37m[1m[2023-07-17 09:14:40,576][257371] Min Reward on eval: -352.5859393994324[0m
[37m[1m[2023-07-17 09:14:40,576][257371] Mean Reward across all agents: 24.073599291446516[0m
[37m[1m[2023-07-17 09:14:40,576][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:14:40,580][257371] mean_value=-185.84198394488232, max_value=458.19577787586604[0m
[37m[1m[2023-07-17 09:14:40,582][257371] New mean coefficients: [[ 1.7365843  -2.3258038  -1.0304308  -1.0768552  -2.9616723  -0.70314133]][0m
[37m[1m[2023-07-17 09:14:40,583][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:14:49,680][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 09:14:49,681][257371] FPS: 422191.01[0m
[36m[2023-07-17 09:14:49,683][257371] itr=979, itrs=2000, Progress: 48.95%[0m
[36m[2023-07-17 09:15:01,615][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 09:15:01,616][257371] FPS: 324665.38[0m
[36m[2023-07-17 09:15:05,939][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:15:05,945][257371] Reward + Measures: [[-42.25790012   0.132085     0.19179899   0.11107566   0.15121533
    6.88049126]][0m
[37m[1m[2023-07-17 09:15:05,945][257371] Max Reward on eval: -42.2579001210011[0m
[37m[1m[2023-07-17 09:15:05,945][257371] Min Reward on eval: -42.2579001210011[0m
[37m[1m[2023-07-17 09:15:05,945][257371] Mean Reward across all agents: -42.2579001210011[0m
[37m[1m[2023-07-17 09:15:05,946][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:15:11,023][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:15:11,028][257371] Reward + Measures: [[  17.52926145    0.0986        0.1609        0.0759        0.07099999
     7.07234192]
 [ -88.5058756     0.1728        0.1014        0.16679999    0.17309999
     6.39530039]
 [ -23.44917401    0.098         0.0822        0.07910001    0.0714
     5.93008423]
 ...
 [  14.02989376    0.11190001    0.13270001    0.07230001    0.0861
     7.01774693]
 [-107.51003185    0.1771        0.4989        0.34909996    0.51640004
     7.22263193]
 [  -5.5955537     0.15109999    0.13970001    0.11009999    0.1224
     7.2215991 ]][0m
[37m[1m[2023-07-17 09:15:11,029][257371] Max Reward on eval: 707.187080373615[0m
[37m[1m[2023-07-17 09:15:11,029][257371] Min Reward on eval: -425.8292083803564[0m
[37m[1m[2023-07-17 09:15:11,029][257371] Mean Reward across all agents: 83.28544276915711[0m
[37m[1m[2023-07-17 09:15:11,030][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:15:11,033][257371] mean_value=-206.1623447718869, max_value=635.6919619883262[0m
[37m[1m[2023-07-17 09:15:11,035][257371] New mean coefficients: [[ 1.8472502 -3.4399981 -1.9852633 -1.1820221 -1.2609049 -1.3321122]][0m
[37m[1m[2023-07-17 09:15:11,036][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:15:20,082][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 09:15:20,082][257371] FPS: 424591.01[0m
[36m[2023-07-17 09:15:20,085][257371] itr=980, itrs=2000, Progress: 49.00%[0m
[37m[1m[2023-07-17 09:18:38,732][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000960[0m
[36m[2023-07-17 09:18:50,838][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 09:18:50,839][257371] FPS: 330326.17[0m
[36m[2023-07-17 09:18:55,108][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:18:55,108][257371] Reward + Measures: [[-40.94221879   0.16710199   0.23507999   0.10434867   0.19551001
    6.94245768]][0m
[37m[1m[2023-07-17 09:18:55,108][257371] Max Reward on eval: -40.942218794683654[0m
[37m[1m[2023-07-17 09:18:55,109][257371] Min Reward on eval: -40.942218794683654[0m
[37m[1m[2023-07-17 09:18:55,109][257371] Mean Reward across all agents: -40.942218794683654[0m
[37m[1m[2023-07-17 09:18:55,109][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:19:00,313][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:19:00,313][257371] Reward + Measures: [[-59.06953902   0.171        0.25610003   0.1786       0.19559999
    7.01402998]
 [ 64.39883923   0.1665       0.1046       0.175        0.17089999
    6.53879118]
 [  4.70438825   0.0695       0.08310001   0.0907       0.0798
    6.35468578]
 ...
 [491.84539794   0.0283       0.84689999   0.78430003   0.78920001
    7.84360218]
 [-36.51757051   0.15700001   0.27910003   0.21110001   0.1629
    7.08999205]
 [  7.8188642    0.0601       0.0728       0.07830001   0.09
    6.02790785]][0m
[37m[1m[2023-07-17 09:19:00,313][257371] Max Reward on eval: 491.845397944469[0m
[37m[1m[2023-07-17 09:19:00,314][257371] Min Reward on eval: -225.62772742989472[0m
[37m[1m[2023-07-17 09:19:00,314][257371] Mean Reward across all agents: 1.3888362860827859[0m
[37m[1m[2023-07-17 09:19:00,314][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:19:00,316][257371] mean_value=-250.72497431642094, max_value=400.96596854584493[0m
[37m[1m[2023-07-17 09:19:00,318][257371] New mean coefficients: [[ 0.53035176 -2.5208664  -2.1631641   0.20329106 -0.62104875 -1.7497542 ]][0m
[37m[1m[2023-07-17 09:19:00,319][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:19:09,271][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 09:19:09,271][257371] FPS: 429038.49[0m
[36m[2023-07-17 09:19:09,274][257371] itr=981, itrs=2000, Progress: 49.05%[0m
[36m[2023-07-17 09:19:20,914][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 09:19:20,914][257371] FPS: 332816.91[0m
[36m[2023-07-17 09:19:25,165][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:19:25,165][257371] Reward + Measures: [[-15.71751718   0.110089     0.15570232   0.09183233   0.09396634
    6.42064571]][0m
[37m[1m[2023-07-17 09:19:25,165][257371] Max Reward on eval: -15.717517176193768[0m
[37m[1m[2023-07-17 09:19:25,166][257371] Min Reward on eval: -15.717517176193768[0m
[37m[1m[2023-07-17 09:19:25,166][257371] Mean Reward across all agents: -15.717517176193768[0m
[37m[1m[2023-07-17 09:19:25,166][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:19:30,150][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:19:30,151][257371] Reward + Measures: [[-53.90256863   0.08880001   0.08809999   0.0665       0.0808
    7.02792215]
 [  6.01618619   0.33080003   0.25349998   0.244        0.2483
    6.62811375]
 [102.7623496    0.40349999   0.52740002   0.1794       0.41799998
    7.35321188]
 ...
 [-53.55356557   0.0641       0.0624       0.0797       0.089
    6.91915607]
 [ -4.57242128   0.33290002   0.21490002   0.19160001   0.17560001
    7.05305052]
 [108.92074365   0.3876       0.31400001   0.4113       0.30779999
    7.1156373 ]][0m
[37m[1m[2023-07-17 09:19:30,151][257371] Max Reward on eval: 423.8501720538363[0m
[37m[1m[2023-07-17 09:19:30,151][257371] Min Reward on eval: -312.72469408009204[0m
[37m[1m[2023-07-17 09:19:30,151][257371] Mean Reward across all agents: 12.108529781875083[0m
[37m[1m[2023-07-17 09:19:30,152][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:19:30,154][257371] mean_value=-210.28614630785847, max_value=632.5443054046202[0m
[37m[1m[2023-07-17 09:19:30,156][257371] New mean coefficients: [[ 0.6848075  -2.23738    -1.600522    1.9837972   0.06056237 -1.3730683 ]][0m
[37m[1m[2023-07-17 09:19:30,157][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:19:39,140][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 09:19:39,140][257371] FPS: 427589.35[0m
[36m[2023-07-17 09:19:39,142][257371] itr=982, itrs=2000, Progress: 49.10%[0m
[36m[2023-07-17 09:19:50,904][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 09:19:50,905][257371] FPS: 329342.26[0m
[36m[2023-07-17 09:19:55,220][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:19:55,221][257371] Reward + Measures: [[10.60445889  0.109695    0.15994333  0.08949167  0.09096666  6.42247915]][0m
[37m[1m[2023-07-17 09:19:55,221][257371] Max Reward on eval: 10.604458887170264[0m
[37m[1m[2023-07-17 09:19:55,221][257371] Min Reward on eval: 10.604458887170264[0m
[37m[1m[2023-07-17 09:19:55,222][257371] Mean Reward across all agents: 10.604458887170264[0m
[37m[1m[2023-07-17 09:19:55,222][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:20:00,165][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:20:00,165][257371] Reward + Measures: [[  94.8022315     0.1689        0.42610002    0.3105        0.3168
     7.84280348]
 [ -12.78662603    0.21879999    0.23980001    0.15989999    0.15350001
     7.37767029]
 [  15.91214771    0.18180001    0.1283        0.18800001    0.18079999
     7.21829081]
 ...
 [-315.31942363    0.479         0.57240003    0.12630001    0.62900001
     7.38619852]
 [  41.80162636    0.0861        0.0895        0.0666        0.0726
     6.18777037]
 [  47.65870603    0.29240003    0.2271        0.22239999    0.2251
     7.24744558]][0m
[37m[1m[2023-07-17 09:20:00,165][257371] Max Reward on eval: 471.69052506685256[0m
[37m[1m[2023-07-17 09:20:00,166][257371] Min Reward on eval: -315.3194236272946[0m
[37m[1m[2023-07-17 09:20:00,166][257371] Mean Reward across all agents: 12.959038888234218[0m
[37m[1m[2023-07-17 09:20:00,166][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:20:00,169][257371] mean_value=-183.40490432168093, max_value=495.6459192120583[0m
[37m[1m[2023-07-17 09:20:00,171][257371] New mean coefficients: [[-0.9719073  -3.0967946  -0.07056451  2.0546107   0.05861199 -0.97449535]][0m
[37m[1m[2023-07-17 09:20:00,172][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:20:09,217][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 09:20:09,218][257371] FPS: 424595.15[0m
[36m[2023-07-17 09:20:09,220][257371] itr=983, itrs=2000, Progress: 49.15%[0m
[36m[2023-07-17 09:20:21,053][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 09:20:21,053][257371] FPS: 327456.75[0m
[36m[2023-07-17 09:20:25,382][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:20:25,383][257371] Reward + Measures: [[-2.8755619   0.10124633  0.14528233  0.10227366  0.10328101  6.41207552]][0m
[37m[1m[2023-07-17 09:20:25,383][257371] Max Reward on eval: -2.8755618999037735[0m
[37m[1m[2023-07-17 09:20:25,383][257371] Min Reward on eval: -2.8755618999037735[0m
[37m[1m[2023-07-17 09:20:25,383][257371] Mean Reward across all agents: -2.8755618999037735[0m
[37m[1m[2023-07-17 09:20:25,384][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:20:30,305][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:20:30,305][257371] Reward + Measures: [[ -25.40987459    0.10930001    0.1129        0.1199        0.12119999
     7.21725702]
 [   3.10572536    0.09949999    0.1382        0.11740001    0.11110001
     6.57214832]
 [-229.26782664    0.71869999    0.90550005    0.57950002    0.88700002
     7.39194059]
 ...
 [  32.06492358    0.13850002    0.20939998    0.10699999    0.0986
     7.16495752]
 [-213.71810342    0.69119996    0.83460009    0.4914        0.83700001
     7.21502924]
 [-250.26785933    0.33700001    0.18410002    0.31330001    0.30890003
     6.76122284]][0m
[37m[1m[2023-07-17 09:20:30,305][257371] Max Reward on eval: 160.94089468610474[0m
[37m[1m[2023-07-17 09:20:30,306][257371] Min Reward on eval: -305.41599082490427[0m
[37m[1m[2023-07-17 09:20:30,306][257371] Mean Reward across all agents: -30.035963810829944[0m
[37m[1m[2023-07-17 09:20:30,306][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:20:30,308][257371] mean_value=-260.4850973198715, max_value=120.89861428339066[0m
[37m[1m[2023-07-17 09:20:30,311][257371] New mean coefficients: [[-0.6748258  -2.6264231   1.2512398   1.419058    0.44320828 -0.6351232 ]][0m
[37m[1m[2023-07-17 09:20:30,312][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:20:39,448][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 09:20:39,448][257371] FPS: 420385.73[0m
[36m[2023-07-17 09:20:39,450][257371] itr=984, itrs=2000, Progress: 49.20%[0m
[36m[2023-07-17 09:20:51,182][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 09:20:51,182][257371] FPS: 330274.35[0m
[36m[2023-07-17 09:20:55,418][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:20:55,424][257371] Reward + Measures: [[-27.36265016   0.20056966   0.17183532   0.181615     0.11717767
    6.93115044]][0m
[37m[1m[2023-07-17 09:20:55,424][257371] Max Reward on eval: -27.362650162514065[0m
[37m[1m[2023-07-17 09:20:55,425][257371] Min Reward on eval: -27.362650162514065[0m
[37m[1m[2023-07-17 09:20:55,425][257371] Mean Reward across all agents: -27.362650162514065[0m
[37m[1m[2023-07-17 09:20:55,425][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:21:00,506][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:21:00,512][257371] Reward + Measures: [[-66.41651135   0.1904       0.31729999   0.17160001   0.1719
    7.41080856]
 [ 16.07971738   0.14530002   0.15039998   0.13170001   0.122
    7.46611357]
 [ 20.58247107   0.09960001   0.22239999   0.0382       0.032
    7.57866383]
 ...
 [-78.1034975    0.06820001   0.08630001   0.0925       0.0738
    6.64536667]
 [-12.18631743   0.1127       0.14650001   0.0823       0.0879
    6.13146544]
 [ 24.3813339    0.1362       0.09190001   0.11680001   0.12150001
    6.98816442]][0m
[37m[1m[2023-07-17 09:21:00,512][257371] Max Reward on eval: 154.22746160561218[0m
[37m[1m[2023-07-17 09:21:00,512][257371] Min Reward on eval: -118.10209664262365[0m
[37m[1m[2023-07-17 09:21:00,513][257371] Mean Reward across all agents: 2.3814729227034257[0m
[37m[1m[2023-07-17 09:21:00,513][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:21:00,514][257371] mean_value=-230.06953785544513, max_value=-4.014310251704785[0m
[36m[2023-07-17 09:21:00,517][257371] XNES is restarting with a new solution whose measures are [0.46100003 0.0899     0.48850003 0.46500006 7.6805582 ] and objective is 57.714088410139084[0m
[36m[2023-07-17 09:21:00,518][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 09:21:00,520][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 09:21:00,521][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:21:09,624][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 09:21:09,624][257371] FPS: 421914.45[0m
[36m[2023-07-17 09:21:09,627][257371] itr=985, itrs=2000, Progress: 49.25%[0m
[36m[2023-07-17 09:21:21,452][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 09:21:21,452][257371] FPS: 327616.66[0m
[36m[2023-07-17 09:21:25,690][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:21:25,690][257371] Reward + Measures: [[71.77446042  0.24204566  0.33004898  0.36371237  0.42222437  7.69794703]][0m
[37m[1m[2023-07-17 09:21:25,691][257371] Max Reward on eval: 71.77446041724555[0m
[37m[1m[2023-07-17 09:21:25,691][257371] Min Reward on eval: 71.77446041724555[0m
[37m[1m[2023-07-17 09:21:25,691][257371] Mean Reward across all agents: 71.77446041724555[0m
[37m[1m[2023-07-17 09:21:25,691][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:21:30,895][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:21:30,896][257371] Reward + Measures: [[132.5847183    0.31619999   0.10160001   0.34919998   0.2987
    6.60834455]
 [ 13.34300031   0.0702       0.12709999   0.11180001   0.109
    7.18079853]
 [ 17.7898125    0.1645       0.26040003   0.1181       0.23319998
    7.50675583]
 ...
 [ 56.19232191   0.0638       0.15109999   0.0981       0.0535
    7.76831055]
 [ 21.51590775   0.24440001   0.3161       0.13810001   0.28749999
    7.91308308]
 [103.29826523   0.0576       0.10640001   0.0878       0.079
    7.27722502]][0m
[37m[1m[2023-07-17 09:21:30,896][257371] Max Reward on eval: 588.3511152523104[0m
[37m[1m[2023-07-17 09:21:30,896][257371] Min Reward on eval: -297.459642650187[0m
[37m[1m[2023-07-17 09:21:30,897][257371] Mean Reward across all agents: 109.9345732573381[0m
[37m[1m[2023-07-17 09:21:30,897][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:21:30,902][257371] mean_value=-60.57750399521343, max_value=650.1808998241042[0m
[37m[1m[2023-07-17 09:21:30,905][257371] New mean coefficients: [[-1.555268   -0.49079782 -0.8809697  -2.963522   -1.5558193  -0.5933694 ]][0m
[37m[1m[2023-07-17 09:21:30,906][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:21:39,893][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 09:21:39,893][257371] FPS: 427365.83[0m
[36m[2023-07-17 09:21:39,896][257371] itr=986, itrs=2000, Progress: 49.30%[0m
[36m[2023-07-17 09:21:51,670][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 09:21:51,671][257371] FPS: 329037.64[0m
[36m[2023-07-17 09:21:56,040][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:21:56,040][257371] Reward + Measures: [[20.17753294  0.12411834  0.14636199  0.16208801  0.12923166  7.58420897]][0m
[37m[1m[2023-07-17 09:21:56,041][257371] Max Reward on eval: 20.17753294040314[0m
[37m[1m[2023-07-17 09:21:56,041][257371] Min Reward on eval: 20.17753294040314[0m
[37m[1m[2023-07-17 09:21:56,041][257371] Mean Reward across all agents: 20.17753294040314[0m
[37m[1m[2023-07-17 09:21:56,041][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:22:01,043][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:22:01,044][257371] Reward + Measures: [[ 7.80403682  0.19880001  0.31760001  0.0789      0.22909999  7.86903238]
 [ 4.18830825  0.0859      0.12220001  0.1332      0.09890001  7.36107588]
 [19.01863133  0.1892      0.25059998  0.09680001  0.20449999  7.15592051]
 ...
 [ 4.86310623  0.094       0.12800001  0.10219999  0.0895      7.23475742]
 [73.74929546  0.0839      0.1866      0.0914      0.05320001  7.71083975]
 [99.01445513  0.13630001  0.1649      0.18179999  0.14300001  7.13145685]][0m
[37m[1m[2023-07-17 09:22:01,044][257371] Max Reward on eval: 656.0297794414219[0m
[37m[1m[2023-07-17 09:22:01,044][257371] Min Reward on eval: -143.88495209454558[0m
[37m[1m[2023-07-17 09:22:01,045][257371] Mean Reward across all agents: 86.75936504766472[0m
[37m[1m[2023-07-17 09:22:01,045][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:22:01,048][257371] mean_value=-111.07126445534931, max_value=406.01016613765046[0m
[37m[1m[2023-07-17 09:22:01,051][257371] New mean coefficients: [[-2.0189872   0.85186905  0.6871124  -3.4076347  -1.465148   -0.5894144 ]][0m
[37m[1m[2023-07-17 09:22:01,052][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:22:10,056][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 09:22:10,057][257371] FPS: 426527.46[0m
[36m[2023-07-17 09:22:10,059][257371] itr=987, itrs=2000, Progress: 49.35%[0m
[36m[2023-07-17 09:22:21,754][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 09:22:21,754][257371] FPS: 331332.65[0m
[36m[2023-07-17 09:22:26,091][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:22:26,092][257371] Reward + Measures: [[46.22712167  0.126416    0.17051966  0.15281467  0.13732466  7.61863518]][0m
[37m[1m[2023-07-17 09:22:26,092][257371] Max Reward on eval: 46.227121671188506[0m
[37m[1m[2023-07-17 09:22:26,092][257371] Min Reward on eval: 46.227121671188506[0m
[37m[1m[2023-07-17 09:22:26,093][257371] Mean Reward across all agents: 46.227121671188506[0m
[37m[1m[2023-07-17 09:22:26,093][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:22:31,118][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:22:31,118][257371] Reward + Measures: [[ 58.78341165   0.18889999   0.23480001   0.22049999   0.19880001
    7.52970839]
 [-65.76617934   0.16830002   0.32100001   0.13929999   0.24610002
    7.51934576]
 [ 16.11236797   0.127        0.17470001   0.148        0.1202
    7.5286994 ]
 ...
 [ 44.62112369   0.1007       0.1797       0.1054       0.08270001
    7.5798049 ]
 [  0.57701102   0.22049999   0.15869999   0.2543       0.22860001
    7.44860697]
 [ 50.6883715    0.17230001   0.18640001   0.18290001   0.1574
    7.62074757]][0m
[37m[1m[2023-07-17 09:22:31,118][257371] Max Reward on eval: 325.12063214071094[0m
[37m[1m[2023-07-17 09:22:31,119][257371] Min Reward on eval: -431.616666794382[0m
[37m[1m[2023-07-17 09:22:31,119][257371] Mean Reward across all agents: 66.48027418180017[0m
[37m[1m[2023-07-17 09:22:31,119][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:22:31,122][257371] mean_value=-98.45875275177667, max_value=153.62473411801088[0m
[37m[1m[2023-07-17 09:22:31,124][257371] New mean coefficients: [[-2.3034465   1.765398    1.6359248  -4.0058937  -2.6469138  -0.51213324]][0m
[37m[1m[2023-07-17 09:22:31,125][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:22:40,179][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 09:22:40,179][257371] FPS: 424208.84[0m
[36m[2023-07-17 09:22:40,182][257371] itr=988, itrs=2000, Progress: 49.40%[0m
[36m[2023-07-17 09:22:52,028][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 09:22:52,028][257371] FPS: 327026.92[0m
[36m[2023-07-17 09:22:56,391][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:22:56,392][257371] Reward + Measures: [[53.92200288  0.08705633  0.14792334  0.12283899  0.08953467  7.56455326]][0m
[37m[1m[2023-07-17 09:22:56,392][257371] Max Reward on eval: 53.92200287718682[0m
[37m[1m[2023-07-17 09:22:56,392][257371] Min Reward on eval: 53.92200287718682[0m
[37m[1m[2023-07-17 09:22:56,393][257371] Mean Reward across all agents: 53.92200287718682[0m
[37m[1m[2023-07-17 09:22:56,393][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:23:01,396][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:23:01,402][257371] Reward + Measures: [[146.4947261    0.123        0.21820001   0.2027       0.1962
    7.4227128 ]
 [ 27.77307675   0.101        0.12460001   0.0877       0.09890001
    7.30434513]
 [139.03812719   0.1794       0.42960006   0.57139999   0.5869
    7.2809639 ]
 ...
 [ 11.23798537   0.14740001   0.2142       0.25240001   0.21519999
    6.7252841 ]
 [115.85011926   0.0938       0.17470001   0.12060001   0.0818
    7.59500647]
 [349.87286234   0.33070001   0.73149997   0.34039998   0.72170001
    7.27967405]][0m
[37m[1m[2023-07-17 09:23:01,402][257371] Max Reward on eval: 587.7112846479461[0m
[37m[1m[2023-07-17 09:23:01,403][257371] Min Reward on eval: -197.49216772236394[0m
[37m[1m[2023-07-17 09:23:01,403][257371] Mean Reward across all agents: 147.8038664024725[0m
[37m[1m[2023-07-17 09:23:01,403][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:23:01,408][257371] mean_value=-111.87420849787219, max_value=840.1386617062147[0m
[37m[1m[2023-07-17 09:23:01,411][257371] New mean coefficients: [[-2.063909   1.3159482  1.6849103 -4.821982  -2.3051398 -0.330886 ]][0m
[37m[1m[2023-07-17 09:23:01,412][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:23:10,472][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 09:23:10,473][257371] FPS: 423872.04[0m
[36m[2023-07-17 09:23:10,475][257371] itr=989, itrs=2000, Progress: 49.45%[0m
[36m[2023-07-17 09:23:22,260][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 09:23:22,260][257371] FPS: 328826.74[0m
[36m[2023-07-17 09:23:26,559][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:23:26,560][257371] Reward + Measures: [[50.91873748  0.10188066  0.16205367  0.13270801  0.110595    7.61751795]][0m
[37m[1m[2023-07-17 09:23:26,560][257371] Max Reward on eval: 50.91873747804722[0m
[37m[1m[2023-07-17 09:23:26,560][257371] Min Reward on eval: 50.91873747804722[0m
[37m[1m[2023-07-17 09:23:26,560][257371] Mean Reward across all agents: 50.91873747804722[0m
[37m[1m[2023-07-17 09:23:26,561][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:23:31,613][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:23:31,614][257371] Reward + Measures: [[  0.24578347   0.12530001   0.2395       0.0603       0.0899
    7.72963715]
 [ 13.41436009   0.0926       0.1234       0.0979       0.0798
    7.39588308]
 [151.14679813   0.32399997   0.1418       0.42159995   0.32659999
    7.21141291]
 ...
 [ 22.80641168   0.102        0.22830001   0.0594       0.0554
    7.89587164]
 [ 90.55740154   0.0873       0.15650001   0.083        0.0643
    7.51489639]
 [  0.59368945   0.1391       0.24699998   0.0533       0.13450001
    7.82987928]][0m
[37m[1m[2023-07-17 09:23:31,614][257371] Max Reward on eval: 181.36425962124486[0m
[37m[1m[2023-07-17 09:23:31,614][257371] Min Reward on eval: -99.29553296319209[0m
[37m[1m[2023-07-17 09:23:31,615][257371] Mean Reward across all agents: 35.22392585230824[0m
[37m[1m[2023-07-17 09:23:31,615][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:23:31,617][257371] mean_value=-129.03813579976824, max_value=40.104560007037946[0m
[37m[1m[2023-07-17 09:23:31,620][257371] New mean coefficients: [[-1.2258399   0.34815383  2.145719   -4.5185394  -2.0794625  -0.00955623]][0m
[37m[1m[2023-07-17 09:23:31,620][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:23:40,687][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 09:23:40,687][257371] FPS: 423605.76[0m
[36m[2023-07-17 09:23:40,689][257371] itr=990, itrs=2000, Progress: 49.50%[0m
[37m[1m[2023-07-17 09:26:57,124][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000970[0m
[36m[2023-07-17 09:27:09,727][257371] train() took 11.92 seconds to complete[0m
[36m[2023-07-17 09:27:09,727][257371] FPS: 322265.50[0m
[36m[2023-07-17 09:27:13,935][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:27:13,936][257371] Reward + Measures: [[51.54299205  0.109394    0.18366565  0.14102668  0.12854266  7.66512489]][0m
[37m[1m[2023-07-17 09:27:13,936][257371] Max Reward on eval: 51.54299204683465[0m
[37m[1m[2023-07-17 09:27:13,936][257371] Min Reward on eval: 51.54299204683465[0m
[37m[1m[2023-07-17 09:27:13,937][257371] Mean Reward across all agents: 51.54299204683465[0m
[37m[1m[2023-07-17 09:27:13,937][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:27:19,128][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:27:19,135][257371] Reward + Measures: [[ 29.96370297   0.14960001   0.2793       0.0669       0.0616
    7.94301176]
 [ 55.60745847   0.0711       0.1134       0.08990001   0.0637
    7.36173582]
 [ 24.37940066   0.0962       0.12150001   0.11059999   0.1067
    6.89104939]
 ...
 [ 18.70072344   0.1074       0.12490001   0.13569999   0.10130002
    7.17271137]
 [ 38.82745116   0.06949999   0.13330001   0.1194       0.12879999
    6.92482996]
 [-23.95297529   0.0717       0.07920001   0.06280001   0.0595
    7.1371932 ]][0m
[37m[1m[2023-07-17 09:27:19,136][257371] Max Reward on eval: 546.2777042635716[0m
[37m[1m[2023-07-17 09:27:19,136][257371] Min Reward on eval: -139.56516264937818[0m
[37m[1m[2023-07-17 09:27:19,136][257371] Mean Reward across all agents: 59.04512815656211[0m
[37m[1m[2023-07-17 09:27:19,136][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:27:19,139][257371] mean_value=-152.47886159533059, max_value=268.2094930993835[0m
[37m[1m[2023-07-17 09:27:19,141][257371] New mean coefficients: [[-1.5506363  -0.9161283   3.8815136  -4.157357   -0.9657228  -0.07293054]][0m
[37m[1m[2023-07-17 09:27:19,143][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:27:28,192][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 09:27:28,192][257371] FPS: 424410.53[0m
[36m[2023-07-17 09:27:28,195][257371] itr=991, itrs=2000, Progress: 49.55%[0m
[36m[2023-07-17 09:27:39,939][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 09:27:39,939][257371] FPS: 329941.49[0m
[36m[2023-07-17 09:27:44,211][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:27:44,211][257371] Reward + Measures: [[41.79453498  0.11407133  0.17257734  0.134896    0.12137365  7.66844606]][0m
[37m[1m[2023-07-17 09:27:44,211][257371] Max Reward on eval: 41.794534984448234[0m
[37m[1m[2023-07-17 09:27:44,212][257371] Min Reward on eval: 41.794534984448234[0m
[37m[1m[2023-07-17 09:27:44,212][257371] Mean Reward across all agents: 41.794534984448234[0m
[37m[1m[2023-07-17 09:27:44,212][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:27:49,155][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:27:49,155][257371] Reward + Measures: [[ 2.80587288  0.1074      0.109       0.097       0.11129999  7.73384333]
 [56.82846105  0.059       0.0941      0.0958      0.0914      7.44705296]
 [15.61147947  0.0764      0.0439      0.1471      0.13319999  7.87889051]
 ...
 [90.09958131  0.14049999  0.1655      0.1443      0.1318      7.75539494]
 [ 8.12040966  0.0886      0.08400001  0.0898      0.081       7.72242308]
 [16.09777847  0.15969999  0.09850001  0.1912      0.16680001  7.71731901]][0m
[37m[1m[2023-07-17 09:27:49,155][257371] Max Reward on eval: 180.5304369878839[0m
[37m[1m[2023-07-17 09:27:49,156][257371] Min Reward on eval: -247.04766945308074[0m
[37m[1m[2023-07-17 09:27:49,156][257371] Mean Reward across all agents: 36.36053287885203[0m
[37m[1m[2023-07-17 09:27:49,156][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:27:49,158][257371] mean_value=-139.76384933800534, max_value=12.846420152958444[0m
[37m[1m[2023-07-17 09:27:49,160][257371] New mean coefficients: [[-1.7926089   0.5585073   3.4124022  -4.7124496  -0.81619483  0.19181603]][0m
[37m[1m[2023-07-17 09:27:49,161][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:27:58,177][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 09:27:58,178][257371] FPS: 425989.55[0m
[36m[2023-07-17 09:27:58,180][257371] itr=992, itrs=2000, Progress: 49.60%[0m
[36m[2023-07-17 09:28:10,138][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-17 09:28:10,138][257371] FPS: 324060.62[0m
[36m[2023-07-17 09:28:14,456][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:28:14,462][257371] Reward + Measures: [[33.34096184  0.11605868  0.15972733  0.14423901  0.118992    7.65710592]][0m
[37m[1m[2023-07-17 09:28:14,462][257371] Max Reward on eval: 33.34096183931557[0m
[37m[1m[2023-07-17 09:28:14,462][257371] Min Reward on eval: 33.34096183931557[0m
[37m[1m[2023-07-17 09:28:14,463][257371] Mean Reward across all agents: 33.34096183931557[0m
[37m[1m[2023-07-17 09:28:14,463][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:28:19,485][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:28:19,491][257371] Reward + Measures: [[-32.23624143   0.0681       0.0983       0.10930001   0.07130001
    7.33873987]
 [100.88529158   0.10090001   0.1609       0.0919       0.0714
    7.50772858]
 [ 44.76321176   0.0731       0.0996       0.12750001   0.0806
    7.61678934]
 ...
 [ -7.69546201   0.1373       0.1745       0.19419999   0.18050002
    7.17343473]
 [ 36.62698508   0.11379999   0.11210001   0.15280001   0.11260001
    7.60728836]
 [ 30.8611703    0.19520001   0.44589996   0.23120001   0.39900002
    7.3626771 ]][0m
[37m[1m[2023-07-17 09:28:19,491][257371] Max Reward on eval: 430.4421044593444[0m
[37m[1m[2023-07-17 09:28:19,492][257371] Min Reward on eval: -81.56117824073881[0m
[37m[1m[2023-07-17 09:28:19,492][257371] Mean Reward across all agents: 71.42739448975452[0m
[37m[1m[2023-07-17 09:28:19,492][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:28:19,494][257371] mean_value=-116.89015973737254, max_value=228.52640036498275[0m
[37m[1m[2023-07-17 09:28:19,497][257371] New mean coefficients: [[-1.865487   -0.7638227   3.2318661  -4.797381   -1.3793895   0.17802013]][0m
[37m[1m[2023-07-17 09:28:19,498][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:28:28,551][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 09:28:28,552][257371] FPS: 424247.98[0m
[36m[2023-07-17 09:28:28,554][257371] itr=993, itrs=2000, Progress: 49.65%[0m
[36m[2023-07-17 09:28:40,466][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 09:28:40,466][257371] FPS: 325298.99[0m
[36m[2023-07-17 09:28:44,750][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:28:44,751][257371] Reward + Measures: [[30.64001638  0.12080999  0.16907467  0.14972566  0.1303      7.65504599]][0m
[37m[1m[2023-07-17 09:28:44,751][257371] Max Reward on eval: 30.640016378655098[0m
[37m[1m[2023-07-17 09:28:44,751][257371] Min Reward on eval: 30.640016378655098[0m
[37m[1m[2023-07-17 09:28:44,751][257371] Mean Reward across all agents: 30.640016378655098[0m
[37m[1m[2023-07-17 09:28:44,752][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:28:49,763][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:28:49,763][257371] Reward + Measures: [[ 83.59980125   0.10699999   0.24009998   0.0998       0.12539999
    7.46227884]
 [335.88331457   0.2251       0.71900004   0.62940001   0.78290004
    7.94106007]
 [-29.91249865   0.0857       0.11079999   0.0781       0.09060001
    7.27110529]
 ...
 [359.52605988   0.1793       0.80909997   0.70300007   0.84619999
    7.98013306]
 [-69.22844521   0.1488       0.0895       0.17560001   0.1673
    7.26174879]
 [-30.27821942   0.0825       0.0865       0.0782       0.0769
    7.07897329]][0m
[37m[1m[2023-07-17 09:28:49,764][257371] Max Reward on eval: 547.8607557393145[0m
[37m[1m[2023-07-17 09:28:49,764][257371] Min Reward on eval: -282.020304580871[0m
[37m[1m[2023-07-17 09:28:49,764][257371] Mean Reward across all agents: 46.80564183805672[0m
[37m[1m[2023-07-17 09:28:49,764][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:28:49,768][257371] mean_value=-207.92911979277181, max_value=432.5756961002517[0m
[37m[1m[2023-07-17 09:28:49,771][257371] New mean coefficients: [[-1.6826569 -2.1542048  1.0829904 -5.021036  -1.9035573  0.4801867]][0m
[37m[1m[2023-07-17 09:28:49,772][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:28:58,774][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 09:28:58,774][257371] FPS: 426643.64[0m
[36m[2023-07-17 09:28:58,777][257371] itr=994, itrs=2000, Progress: 49.70%[0m
[36m[2023-07-17 09:29:10,537][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 09:29:10,538][257371] FPS: 329398.19[0m
[36m[2023-07-17 09:29:14,916][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:29:14,921][257371] Reward + Measures: [[18.99466188  0.107444    0.18465267  0.13373867  0.12875199  7.65094662]][0m
[37m[1m[2023-07-17 09:29:14,921][257371] Max Reward on eval: 18.99466187794533[0m
[37m[1m[2023-07-17 09:29:14,922][257371] Min Reward on eval: 18.99466187794533[0m
[37m[1m[2023-07-17 09:29:14,922][257371] Mean Reward across all agents: 18.99466187794533[0m
[37m[1m[2023-07-17 09:29:14,922][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:29:19,970][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:29:19,976][257371] Reward + Measures: [[ 27.9606418    0.0814       0.1964       0.2086       0.2016
    7.11941671]
 [ 41.16052861   0.0599       0.1063       0.1033       0.0731
    7.3024745 ]
 [341.37683678   0.2929       0.90149993   0.62880003   0.94349998
    7.97220469]
 ...
 [ 50.36795199   0.0749       0.0841       0.09450001   0.0816
    7.03827381]
 [175.56876825   0.2667       0.37310001   0.41309997   0.49390003
    7.26361847]
 [ 79.93082803   0.1304       0.19599999   0.15539999   0.1087
    7.40380049]][0m
[37m[1m[2023-07-17 09:29:19,976][257371] Max Reward on eval: 695.6820793295279[0m
[37m[1m[2023-07-17 09:29:19,977][257371] Min Reward on eval: -85.58219033032655[0m
[37m[1m[2023-07-17 09:29:19,977][257371] Mean Reward across all agents: 134.7672966777991[0m
[37m[1m[2023-07-17 09:29:19,977][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:29:19,980][257371] mean_value=-142.01258687368056, max_value=275.1692252333581[0m
[37m[1m[2023-07-17 09:29:19,982][257371] New mean coefficients: [[-1.4531255  -1.5162847   1.8741488  -5.1766276  -3.372579    0.62782204]][0m
[37m[1m[2023-07-17 09:29:19,983][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:29:29,109][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 09:29:29,110][257371] FPS: 420840.56[0m
[36m[2023-07-17 09:29:29,112][257371] itr=995, itrs=2000, Progress: 49.75%[0m
[36m[2023-07-17 09:29:41,051][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 09:29:41,051][257371] FPS: 324560.64[0m
[36m[2023-07-17 09:29:45,406][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:29:45,406][257371] Reward + Measures: [[20.81581114  0.09008399  0.17131934  0.12658501  0.10713867  7.6001749 ]][0m
[37m[1m[2023-07-17 09:29:45,407][257371] Max Reward on eval: 20.815811141656674[0m
[37m[1m[2023-07-17 09:29:45,407][257371] Min Reward on eval: 20.815811141656674[0m
[37m[1m[2023-07-17 09:29:45,407][257371] Mean Reward across all agents: 20.815811141656674[0m
[37m[1m[2023-07-17 09:29:45,408][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:29:50,483][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:29:50,484][257371] Reward + Measures: [[-54.93361795   0.16360001   0.23550001   0.1609       0.22259998
    7.68057632]
 [-28.61685315   0.07120001   0.1269       0.0909       0.08419999
    7.1578064 ]
 [-20.68458375   0.15159999   0.12150001   0.13939999   0.11110001
    7.59467411]
 ...
 [ 52.10978589   0.0777       0.1807       0.0908       0.0829
    7.23035288]
 [ 62.58038216   0.21850002   0.24349999   0.3242       0.32660002
    7.20857573]
 [ 34.12549086   0.0793       0.1382       0.0954       0.0823
    7.28113174]][0m
[37m[1m[2023-07-17 09:29:50,484][257371] Max Reward on eval: 619.3810462925583[0m
[37m[1m[2023-07-17 09:29:50,484][257371] Min Reward on eval: -143.9285500049591[0m
[37m[1m[2023-07-17 09:29:50,485][257371] Mean Reward across all agents: 29.930114257687006[0m
[37m[1m[2023-07-17 09:29:50,485][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:29:50,487][257371] mean_value=-175.03142286185587, max_value=464.14587754363066[0m
[37m[1m[2023-07-17 09:29:50,490][257371] New mean coefficients: [[-1.4308391  -2.367321    2.4393146  -4.2808475  -3.7705748   0.31930968]][0m
[37m[1m[2023-07-17 09:29:50,491][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:29:59,563][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 09:29:59,568][257371] FPS: 423355.52[0m
[36m[2023-07-17 09:29:59,571][257371] itr=996, itrs=2000, Progress: 49.80%[0m
[36m[2023-07-17 09:30:11,338][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 09:30:11,338][257371] FPS: 329344.99[0m
[36m[2023-07-17 09:30:15,658][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:30:15,658][257371] Reward + Measures: [[38.9427616   0.09291767  0.20553401  0.14475034  0.13982433  7.62546825]][0m
[37m[1m[2023-07-17 09:30:15,658][257371] Max Reward on eval: 38.942761604855995[0m
[37m[1m[2023-07-17 09:30:15,659][257371] Min Reward on eval: 38.942761604855995[0m
[37m[1m[2023-07-17 09:30:15,659][257371] Mean Reward across all agents: 38.942761604855995[0m
[37m[1m[2023-07-17 09:30:15,659][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:30:20,877][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:30:20,883][257371] Reward + Measures: [[194.60685588   0.34440002   0.11930001   0.3901       0.34190002
    7.36530924]
 [186.81277178   0.18009999   0.44169998   0.30840001   0.4348
    7.76051712]
 [ 74.6307934    0.12770002   0.23280001   0.1541       0.16760001
    7.59460449]
 ...
 [ 70.99633406   0.16500001   0.24969998   0.2172       0.2131
    7.29067373]
 [403.87280609   0.1321       0.59450001   0.66160005   0.65519994
    7.82123804]
 [574.75841045   0.0313       0.81049997   0.74050003   0.78300005
    7.9568572 ]][0m
[37m[1m[2023-07-17 09:30:20,883][257371] Max Reward on eval: 626.0755203980603[0m
[37m[1m[2023-07-17 09:30:20,884][257371] Min Reward on eval: -103.25634690234438[0m
[37m[1m[2023-07-17 09:30:20,884][257371] Mean Reward across all agents: 144.48449524468464[0m
[37m[1m[2023-07-17 09:30:20,884][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:30:20,888][257371] mean_value=-71.00756467369155, max_value=349.33099412719906[0m
[37m[1m[2023-07-17 09:30:20,891][257371] New mean coefficients: [[-2.9242725  -2.8208694   3.12182    -4.5954075  -4.0209136   0.15919213]][0m
[37m[1m[2023-07-17 09:30:20,892][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:30:29,917][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 09:30:29,917][257371] FPS: 425580.22[0m
[36m[2023-07-17 09:30:29,919][257371] itr=997, itrs=2000, Progress: 49.85%[0m
[36m[2023-07-17 09:30:41,714][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 09:30:41,714][257371] FPS: 328540.27[0m
[36m[2023-07-17 09:30:46,024][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:30:46,025][257371] Reward + Measures: [[47.7314889   0.094906    0.229994    0.16158967  0.163894    7.63024712]][0m
[37m[1m[2023-07-17 09:30:46,025][257371] Max Reward on eval: 47.73148890268593[0m
[37m[1m[2023-07-17 09:30:46,025][257371] Min Reward on eval: 47.73148890268593[0m
[37m[1m[2023-07-17 09:30:46,026][257371] Mean Reward across all agents: 47.73148890268593[0m
[37m[1m[2023-07-17 09:30:46,026][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:30:51,016][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:30:51,017][257371] Reward + Measures: [[  -8.05881094    0.0746        0.1532        0.11570001    0.1247
     6.76178837]
 [  72.78316641    0.1058        0.1608        0.12989999    0.1323
     6.7159524 ]
 [  55.91280581    0.0958        0.21080001    0.13440001    0.17550001
     7.52628946]
 ...
 [-116.12314586    0.10210001    0.25170001    0.15550001    0.17819999
     7.35815954]
 [-129.61021962    0.2798        0.49539995    0.14470001    0.44230005
     7.65490198]
 [  79.17341324    0.24080001    0.29369998    0.2277        0.25760001
     7.25262976]][0m
[37m[1m[2023-07-17 09:30:51,017][257371] Max Reward on eval: 541.1206674417015[0m
[37m[1m[2023-07-17 09:30:51,017][257371] Min Reward on eval: -328.9458763862029[0m
[37m[1m[2023-07-17 09:30:51,017][257371] Mean Reward across all agents: 12.40795961250578[0m
[37m[1m[2023-07-17 09:30:51,018][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:30:51,019][257371] mean_value=-232.9674160246039, max_value=263.7338266077692[0m
[37m[1m[2023-07-17 09:30:51,022][257371] New mean coefficients: [[-2.7535605  -1.8187981   2.7938256  -3.9910598  -3.771366   -0.30540293]][0m
[37m[1m[2023-07-17 09:30:51,027][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:31:00,115][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 09:31:00,116][257371] FPS: 422596.08[0m
[36m[2023-07-17 09:31:00,118][257371] itr=998, itrs=2000, Progress: 49.90%[0m
[36m[2023-07-17 09:31:11,943][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 09:31:11,944][257371] FPS: 327618.13[0m
[36m[2023-07-17 09:31:16,143][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:31:16,143][257371] Reward + Measures: [[84.48902342  0.09559067  0.29220399  0.21109265  0.22997801  7.66047144]][0m
[37m[1m[2023-07-17 09:31:16,144][257371] Max Reward on eval: 84.48902341871819[0m
[37m[1m[2023-07-17 09:31:16,144][257371] Min Reward on eval: 84.48902341871819[0m
[37m[1m[2023-07-17 09:31:16,144][257371] Mean Reward across all agents: 84.48902341871819[0m
[37m[1m[2023-07-17 09:31:16,144][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:31:21,109][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:31:21,110][257371] Reward + Measures: [[312.48808021   0.36279997   0.53710002   0.1068       0.49359998
    7.72370243]
 [450.64781052   0.50200003   0.72819996   0.0681       0.67720002
    7.84821033]
 [ 90.00586607   0.18920001   0.15580001   0.23339999   0.20820001
    7.72663736]
 ...
 [ 76.1023978    0.09850001   0.12409999   0.14570001   0.1033
    7.47151327]
 [284.09214928   0.30350003   0.43560001   0.10770001   0.39309999
    7.61720371]
 [103.8732157    0.12499999   0.178        0.2066       0.1646
    7.29676914]][0m
[37m[1m[2023-07-17 09:31:21,110][257371] Max Reward on eval: 550.6783314179629[0m
[37m[1m[2023-07-17 09:31:21,111][257371] Min Reward on eval: -103.49691940918565[0m
[37m[1m[2023-07-17 09:31:21,111][257371] Mean Reward across all agents: 169.17295907891554[0m
[37m[1m[2023-07-17 09:31:21,111][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:31:21,115][257371] mean_value=-58.155120056724606, max_value=207.95441789647185[0m
[37m[1m[2023-07-17 09:31:21,118][257371] New mean coefficients: [[-3.3706954  -3.2806163   3.7410188  -4.1996937  -3.985036   -0.29842594]][0m
[37m[1m[2023-07-17 09:31:21,120][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:31:30,131][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 09:31:30,131][257371] FPS: 426196.92[0m
[36m[2023-07-17 09:31:30,134][257371] itr=999, itrs=2000, Progress: 49.95%[0m
[36m[2023-07-17 09:31:41,862][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 09:31:41,863][257371] FPS: 330404.80[0m
[36m[2023-07-17 09:31:46,174][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:31:46,174][257371] Reward + Measures: [[82.22131455  0.096761    0.29641566  0.214967    0.23622933  7.66530085]][0m
[37m[1m[2023-07-17 09:31:46,174][257371] Max Reward on eval: 82.22131454551551[0m
[37m[1m[2023-07-17 09:31:46,175][257371] Min Reward on eval: 82.22131454551551[0m
[37m[1m[2023-07-17 09:31:46,175][257371] Mean Reward across all agents: 82.22131454551551[0m
[37m[1m[2023-07-17 09:31:46,175][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:31:51,185][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:31:51,185][257371] Reward + Measures: [[-216.06629277    0.34019998    0.0875        0.4612        0.41840002
     7.27467585]
 [ -36.57525305    0.1451        0.15250002    0.19849999    0.1956
     7.3038125 ]
 [ 264.9424119     0.13689999    0.80940002    0.7301001     0.86540002
     7.91660404]
 ...
 [  74.01509421    0.22719999    0.24760003    0.38390002    0.27610001
     7.68483591]
 [  10.45316415    0.30630001    0.43189999    0.62260002    0.52290004
     7.89947891]
 [  58.17659931    0.1605        0.41370001    0.35069999    0.3924
     7.75537109]][0m
[37m[1m[2023-07-17 09:31:51,186][257371] Max Reward on eval: 317.9493856220273[0m
[37m[1m[2023-07-17 09:31:51,186][257371] Min Reward on eval: -517.5306205672211[0m
[37m[1m[2023-07-17 09:31:51,186][257371] Mean Reward across all agents: -20.324607962788377[0m
[37m[1m[2023-07-17 09:31:51,186][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:31:51,189][257371] mean_value=-230.75127944657928, max_value=334.21990564373084[0m
[37m[1m[2023-07-17 09:31:51,192][257371] New mean coefficients: [[-3.8214033 -2.8260565  5.15037   -3.5213864 -3.2122912 -0.6200162]][0m
[37m[1m[2023-07-17 09:31:51,193][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:32:00,217][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 09:32:00,217][257371] FPS: 425613.62[0m
[36m[2023-07-17 09:32:00,219][257371] itr=1000, itrs=2000, Progress: 50.00%[0m
[37m[1m[2023-07-17 09:35:26,925][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000980[0m
[36m[2023-07-17 09:35:39,533][257371] train() took 11.95 seconds to complete[0m
[36m[2023-07-17 09:35:39,533][257371] FPS: 321386.55[0m
[36m[2023-07-17 09:35:43,804][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:35:43,805][257371] Reward + Measures: [[105.87498212   0.10625433   0.36458069   0.25903466   0.30663699
    7.68694782]][0m
[37m[1m[2023-07-17 09:35:43,805][257371] Max Reward on eval: 105.87498212400219[0m
[37m[1m[2023-07-17 09:35:43,805][257371] Min Reward on eval: 105.87498212400219[0m
[37m[1m[2023-07-17 09:35:43,806][257371] Mean Reward across all agents: 105.87498212400219[0m
[37m[1m[2023-07-17 09:35:43,806][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:35:48,785][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:35:48,786][257371] Reward + Measures: [[-49.83266705   0.10550001   0.21169999   0.09850001   0.1655
    6.91548491]
 [ 26.81705732   0.0648       0.091        0.1292       0.13700001
    7.05072355]
 [ -2.36931566   0.0795       0.1753       0.0823       0.07970001
    7.53632689]
 ...
 [  7.81962993   0.072        0.1408       0.11890002   0.0979
    7.45909834]
 [ 27.38306706   0.044        0.0622       0.09200001   0.10290001
    7.22789383]
 [-59.37673447   0.1688       0.21879999   0.14030001   0.18539999
    7.43592405]][0m
[37m[1m[2023-07-17 09:35:48,786][257371] Max Reward on eval: 287.94958947449925[0m
[37m[1m[2023-07-17 09:35:48,786][257371] Min Reward on eval: -442.0324821451679[0m
[37m[1m[2023-07-17 09:35:48,787][257371] Mean Reward across all agents: -20.797173557947044[0m
[37m[1m[2023-07-17 09:35:48,787][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:35:48,788][257371] mean_value=-237.8374040094498, max_value=29.876973241176415[0m
[37m[1m[2023-07-17 09:35:48,791][257371] New mean coefficients: [[-2.3583279  -2.5147421   4.509762   -2.9441006  -2.9450371  -0.12999704]][0m
[37m[1m[2023-07-17 09:35:48,792][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:35:57,905][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 09:35:57,906][257371] FPS: 421428.02[0m
[36m[2023-07-17 09:35:57,908][257371] itr=1001, itrs=2000, Progress: 50.05%[0m
[36m[2023-07-17 09:36:09,751][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 09:36:09,751][257371] FPS: 327090.98[0m
[36m[2023-07-17 09:36:14,026][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:36:14,026][257371] Reward + Measures: [[119.07154248   0.10364967   0.37701997   0.27425566   0.32178935
    7.69322109]][0m
[37m[1m[2023-07-17 09:36:14,026][257371] Max Reward on eval: 119.07154247898309[0m
[37m[1m[2023-07-17 09:36:14,027][257371] Min Reward on eval: 119.07154247898309[0m
[37m[1m[2023-07-17 09:36:14,027][257371] Mean Reward across all agents: 119.07154247898309[0m
[37m[1m[2023-07-17 09:36:14,027][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:36:19,335][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:36:19,336][257371] Reward + Measures: [[  58.76414824    0.0831        0.1318        0.17349999    0.17980002
     7.73521423]
 [  25.65336687    0.0782        0.223         0.12609999    0.1679
     7.07629538]
 [-108.39649118    0.0578        0.0708        0.1577        0.1382
     7.5328536 ]
 ...
 [  -7.46214911    0.1494        0.17119999    0.0801        0.1462
     7.80503941]
 [  23.68850312    0.15460001    0.15750001    0.15390001    0.2041
     7.1652565 ]
 [  36.15280265    0.1201        0.1248        0.14109999    0.1418
     7.04905033]][0m
[37m[1m[2023-07-17 09:36:19,336][257371] Max Reward on eval: 268.5867822345346[0m
[37m[1m[2023-07-17 09:36:19,336][257371] Min Reward on eval: -449.5102345890831[0m
[37m[1m[2023-07-17 09:36:19,337][257371] Mean Reward across all agents: -0.7714040783872069[0m
[37m[1m[2023-07-17 09:36:19,337][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:36:19,339][257371] mean_value=-223.04171807703358, max_value=257.9381895294221[0m
[37m[1m[2023-07-17 09:36:19,341][257371] New mean coefficients: [[-2.8226662 -1.4407749  5.1883345 -1.9628103 -2.7541268  0.2812432]][0m
[37m[1m[2023-07-17 09:36:19,342][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:36:28,286][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 09:36:28,287][257371] FPS: 429412.28[0m
[36m[2023-07-17 09:36:28,289][257371] itr=1002, itrs=2000, Progress: 50.10%[0m
[36m[2023-07-17 09:36:40,218][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 09:36:40,218][257371] FPS: 324844.80[0m
[36m[2023-07-17 09:36:44,480][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:36:44,481][257371] Reward + Measures: [[101.85760663   0.10338267   0.3527123    0.25154167   0.29367799
    7.67963934]][0m
[37m[1m[2023-07-17 09:36:44,481][257371] Max Reward on eval: 101.85760663163425[0m
[37m[1m[2023-07-17 09:36:44,481][257371] Min Reward on eval: 101.85760663163425[0m
[37m[1m[2023-07-17 09:36:44,481][257371] Mean Reward across all agents: 101.85760663163425[0m
[37m[1m[2023-07-17 09:36:44,482][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:36:49,388][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:36:49,388][257371] Reward + Measures: [[156.13714927   0.0804       0.13880001   0.1267       0.087
    7.44283295]
 [ 58.66690657   0.1464       0.25910002   0.14180002   0.2052
    7.62083817]
 [ 81.5328913    0.065        0.38480002   0.3572       0.35110003
    7.40763426]
 ...
 [142.37628362   0.0866       0.20550001   0.19139999   0.16970001
    7.42102003]
 [  0.28960941   0.1244       0.51750004   0.30330002   0.44229999
    7.59857273]
 [196.85173512   0.21760002   0.47839999   0.55669999   0.5923
    7.6723361 ]][0m
[37m[1m[2023-07-17 09:36:49,388][257371] Max Reward on eval: 532.6409845463[0m
[37m[1m[2023-07-17 09:36:49,389][257371] Min Reward on eval: -142.11524416897447[0m
[37m[1m[2023-07-17 09:36:49,389][257371] Mean Reward across all agents: 112.51582123053736[0m
[37m[1m[2023-07-17 09:36:49,389][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:36:49,391][257371] mean_value=-122.07739556545343, max_value=127.07917803626293[0m
[37m[1m[2023-07-17 09:36:49,394][257371] New mean coefficients: [[-3.3270414 -1.4289837  6.923877  -2.1161175 -3.7198498  0.7167102]][0m
[37m[1m[2023-07-17 09:36:49,395][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:36:58,451][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 09:36:58,452][257371] FPS: 424074.56[0m
[36m[2023-07-17 09:36:58,454][257371] itr=1003, itrs=2000, Progress: 50.15%[0m
[36m[2023-07-17 09:37:10,266][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 09:37:10,267][257371] FPS: 328009.20[0m
[36m[2023-07-17 09:37:14,556][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:37:14,556][257371] Reward + Measures: [[98.71306478  0.10616933  0.35903266  0.25117266  0.29951432  7.68693352]][0m
[37m[1m[2023-07-17 09:37:14,556][257371] Max Reward on eval: 98.71306478002559[0m
[37m[1m[2023-07-17 09:37:14,557][257371] Min Reward on eval: 98.71306478002559[0m
[37m[1m[2023-07-17 09:37:14,557][257371] Mean Reward across all agents: 98.71306478002559[0m
[37m[1m[2023-07-17 09:37:14,557][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:37:19,574][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:37:19,575][257371] Reward + Measures: [[ 84.80675917   0.07920001   0.23580001   0.19170001   0.1962
    7.4233346 ]
 [-84.22492452   0.0704       0.1025       0.1063       0.11220001
    7.3634181 ]
 [-50.73985757   0.24460001   0.259        0.0671       0.25750002
    7.65505075]
 ...
 [-22.10094641   0.0636       0.0961       0.0983       0.0843
    7.09058762]
 [ 20.41749643   0.0662       0.264        0.24609999   0.24429999
    7.46690702]
 [ 96.30352723   0.0723       0.39659998   0.32440001   0.37470001
    7.60363007]][0m
[37m[1m[2023-07-17 09:37:19,575][257371] Max Reward on eval: 305.69418121997734[0m
[37m[1m[2023-07-17 09:37:19,575][257371] Min Reward on eval: -375.14822079894367[0m
[37m[1m[2023-07-17 09:37:19,575][257371] Mean Reward across all agents: -16.47841949938467[0m
[37m[1m[2023-07-17 09:37:19,575][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:37:19,576][257371] mean_value=-242.10946438443736, max_value=-18.129217951954672[0m
[36m[2023-07-17 09:37:19,578][257371] XNES is restarting with a new solution whose measures are [0.37670001 0.63339996 0.0759     0.74809998 0.99561805] and objective is 33.78056893441826[0m
[36m[2023-07-17 09:37:19,579][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 09:37:19,581][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 09:37:19,582][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:37:28,665][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 09:37:28,665][257371] FPS: 422820.50[0m
[36m[2023-07-17 09:37:28,668][257371] itr=1004, itrs=2000, Progress: 50.20%[0m
[36m[2023-07-17 09:37:40,390][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 09:37:40,390][257371] FPS: 330641.30[0m
[36m[2023-07-17 09:37:44,745][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:37:44,745][257371] Reward + Measures: [[-59.09663462   0.39812532   0.57915831   0.152931     0.5943073
    1.32809353]][0m
[37m[1m[2023-07-17 09:37:44,746][257371] Max Reward on eval: -59.096634620697124[0m
[37m[1m[2023-07-17 09:37:44,746][257371] Min Reward on eval: -59.096634620697124[0m
[37m[1m[2023-07-17 09:37:44,746][257371] Mean Reward across all agents: -59.096634620697124[0m
[37m[1m[2023-07-17 09:37:44,746][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:37:49,751][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:37:49,751][257371] Reward + Measures: [[-14.7648527    0.37919998   0.4384       0.22390001   0.44899997
    3.586025  ]
 [  9.88939437   0.24530001   0.42550001   0.30050001   0.4621
    3.23373413]
 [-40.03429915   0.45280001   0.39560002   0.39830002   0.36429998
    3.09604907]
 ...
 [ 63.19083039   0.33129999   0.35470003   0.3373       0.33329999
    3.31941462]
 [ 36.15172479   0.43120003   0.40019998   0.45620003   0.347
    2.9037354 ]
 [ 51.79064227   0.35710001   0.36289999   0.40229997   0.4269
    3.53358006]][0m
[37m[1m[2023-07-17 09:37:49,751][257371] Max Reward on eval: 211.80348256062717[0m
[37m[1m[2023-07-17 09:37:49,752][257371] Min Reward on eval: -172.98351375982165[0m
[37m[1m[2023-07-17 09:37:49,752][257371] Mean Reward across all agents: -5.4805308477121[0m
[37m[1m[2023-07-17 09:37:49,752][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:37:49,753][257371] mean_value=-1351.4222737531643, max_value=-14.241382034805142[0m
[36m[2023-07-17 09:37:49,756][257371] XNES is restarting with a new solution whose measures are [0.64100003 0.67680001 0.64320004 0.27060002 3.65084004] and objective is 248.71479227803647[0m
[36m[2023-07-17 09:37:49,757][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 09:37:49,759][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 09:37:49,760][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:37:58,792][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 09:37:58,792][257371] FPS: 425216.81[0m
[36m[2023-07-17 09:37:58,794][257371] itr=1005, itrs=2000, Progress: 50.25%[0m
[36m[2023-07-17 09:38:10,471][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 09:38:10,471][257371] FPS: 331817.68[0m
[36m[2023-07-17 09:38:14,821][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:38:14,822][257371] Reward + Measures: [[112.09364762   0.34988368   0.48844603   0.32220566   0.403034
    3.19118881]][0m
[37m[1m[2023-07-17 09:38:14,822][257371] Max Reward on eval: 112.09364761750517[0m
[37m[1m[2023-07-17 09:38:14,822][257371] Min Reward on eval: 112.09364761750517[0m
[37m[1m[2023-07-17 09:38:14,823][257371] Mean Reward across all agents: 112.09364761750517[0m
[37m[1m[2023-07-17 09:38:14,823][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:38:19,888][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:38:19,889][257371] Reward + Measures: [[108.29845999   0.88370007   0.1051       0.88779992   0.85710001
    6.77024841]
 [ 33.84631499   0.338        0.37059999   0.3448       0.28060001
    4.55593443]
 [ 15.86891608   0.244        0.2811       0.29210001   0.27920002
    4.08802462]
 ...
 [ 41.27557665   0.34720001   0.37960002   0.31430003   0.30070001
    4.42319345]
 [  4.02141238   0.2784       0.2775       0.3175       0.26339999
    4.26178503]
 [ 65.5668472    0.34169999   0.34840003   0.33730003   0.31110001
    4.83937216]][0m
[37m[1m[2023-07-17 09:38:19,889][257371] Max Reward on eval: 285.82417109534146[0m
[37m[1m[2023-07-17 09:38:19,889][257371] Min Reward on eval: -284.5191173588857[0m
[37m[1m[2023-07-17 09:38:19,890][257371] Mean Reward across all agents: 29.78479137506336[0m
[37m[1m[2023-07-17 09:38:19,890][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:38:19,892][257371] mean_value=-561.9725359001329, max_value=26.14270109603268[0m
[37m[1m[2023-07-17 09:38:19,894][257371] New mean coefficients: [[ 0.6499989   0.4258926  -0.25839847 -1.770646   -1.4744992  -0.16087347]][0m
[37m[1m[2023-07-17 09:38:19,895][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:38:29,086][257371] train() took 9.19 seconds to complete[0m
[36m[2023-07-17 09:38:29,086][257371] FPS: 417888.36[0m
[36m[2023-07-17 09:38:29,088][257371] itr=1006, itrs=2000, Progress: 50.30%[0m
[36m[2023-07-17 09:38:41,148][257371] train() took 11.95 seconds to complete[0m
[36m[2023-07-17 09:38:41,148][257371] FPS: 321214.07[0m
[36m[2023-07-17 09:38:45,368][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:38:45,369][257371] Reward + Measures: [[92.88229575  0.29097533  0.42973232  0.32767734  0.38074666  3.69285941]][0m
[37m[1m[2023-07-17 09:38:45,369][257371] Max Reward on eval: 92.88229574889566[0m
[37m[1m[2023-07-17 09:38:45,369][257371] Min Reward on eval: 92.88229574889566[0m
[37m[1m[2023-07-17 09:38:45,369][257371] Mean Reward across all agents: 92.88229574889566[0m
[37m[1m[2023-07-17 09:38:45,370][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:38:50,644][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:38:50,645][257371] Reward + Measures: [[  68.13321254    0.30230001    0.4032        0.31690001    0.3698
     5.83538198]
 [-138.17233477    0.43580005    0.46810004    0.55540001    0.15120001
     5.99395323]
 [   8.6328296     0.3134        0.36140001    0.33329999    0.30449998
     5.94191837]
 ...
 [ -93.51555177    0.34779999    0.45439997    0.29539999    0.43209997
     5.31193113]
 [  20.58144744    0.1566        0.16970001    0.1496        0.18980001
     5.68267822]
 [  21.47424917    0.36660001    0.34030002    0.3513        0.35829997
     5.42934799]][0m
[37m[1m[2023-07-17 09:38:50,645][257371] Max Reward on eval: 238.17500684764235[0m
[37m[1m[2023-07-17 09:38:50,645][257371] Min Reward on eval: -536.6802139457316[0m
[37m[1m[2023-07-17 09:38:50,645][257371] Mean Reward across all agents: 5.63222163234845[0m
[37m[1m[2023-07-17 09:38:50,646][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:38:50,647][257371] mean_value=-513.7282110723067, max_value=61.64412237708656[0m
[37m[1m[2023-07-17 09:38:50,649][257371] New mean coefficients: [[ 1.7408192   0.32500604  0.36185044 -2.153242   -1.295069    0.12239355]][0m
[37m[1m[2023-07-17 09:38:50,650][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:38:59,617][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 09:38:59,618][257371] FPS: 428279.99[0m
[36m[2023-07-17 09:38:59,620][257371] itr=1007, itrs=2000, Progress: 50.35%[0m
[36m[2023-07-17 09:39:11,289][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 09:39:11,289][257371] FPS: 332011.48[0m
[36m[2023-07-17 09:39:15,552][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:39:15,553][257371] Reward + Measures: [[46.84162249  0.28645501  0.45455533  0.34600464  0.45356002  3.84115744]][0m
[37m[1m[2023-07-17 09:39:15,553][257371] Max Reward on eval: 46.84162248822783[0m
[37m[1m[2023-07-17 09:39:15,553][257371] Min Reward on eval: 46.84162248822783[0m
[37m[1m[2023-07-17 09:39:15,554][257371] Mean Reward across all agents: 46.84162248822783[0m
[37m[1m[2023-07-17 09:39:15,554][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:39:20,555][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:39:20,555][257371] Reward + Measures: [[127.59606862   0.24110003   0.2678       0.23340002   0.22379999
    4.06891966]
 [-47.55734293   0.18569998   0.3335       0.17830001   0.32860002
    5.68439198]
 [ 28.99313857   0.20050001   0.34689999   0.22220002   0.3019
    4.89953279]
 ...
 [117.74142363   0.1832       0.22070001   0.19920002   0.16560002
    4.58706379]
 [125.1126825    0.26140001   0.25310001   0.35339999   0.24220002
    6.01773453]
 [ 52.04199892   0.1646       0.17429999   0.1679       0.1962
    4.79120827]][0m
[37m[1m[2023-07-17 09:39:20,556][257371] Max Reward on eval: 285.03067847639323[0m
[37m[1m[2023-07-17 09:39:20,556][257371] Min Reward on eval: -183.12630142676645[0m
[37m[1m[2023-07-17 09:39:20,556][257371] Mean Reward across all agents: 55.01696549497513[0m
[37m[1m[2023-07-17 09:39:20,556][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:39:20,558][257371] mean_value=-773.2588829407563, max_value=80.5758880059546[0m
[37m[1m[2023-07-17 09:39:20,561][257371] New mean coefficients: [[ 2.6411226  -0.3229927   0.44475803 -0.96981883 -1.6352212   0.22010583]][0m
[37m[1m[2023-07-17 09:39:20,561][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:39:29,561][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 09:39:29,561][257371] FPS: 426761.69[0m
[36m[2023-07-17 09:39:29,564][257371] itr=1008, itrs=2000, Progress: 50.40%[0m
[36m[2023-07-17 09:39:41,497][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 09:39:41,497][257371] FPS: 324662.73[0m
[36m[2023-07-17 09:39:45,779][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:39:45,779][257371] Reward + Measures: [[23.73156309  0.23969568  0.42263898  0.35500532  0.41129702  3.84141302]][0m
[37m[1m[2023-07-17 09:39:45,779][257371] Max Reward on eval: 23.731563086913948[0m
[37m[1m[2023-07-17 09:39:45,780][257371] Min Reward on eval: 23.731563086913948[0m
[37m[1m[2023-07-17 09:39:45,780][257371] Mean Reward across all agents: 23.731563086913948[0m
[37m[1m[2023-07-17 09:39:45,780][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:39:50,815][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:39:50,815][257371] Reward + Measures: [[  27.98818113    0.37750003    0.31349999    0.36970001    0.36849999
     6.08686447]
 [  22.62207666    0.14119999    0.20809999    0.1807        0.21519999
     4.51340723]
 [  56.7551582     0.27070001    0.3881        0.22839999    0.35910001
     6.05387211]
 ...
 [-173.34952308    0.17040001    0.45429999    0.21929999    0.46250001
     4.82744741]
 [  16.43104065    0.36500001    0.2633        0.35110003    0.32950002
     5.63636684]
 [  45.18781194    0.14250001    0.1875        0.16790001    0.1803
     4.25908089]][0m
[37m[1m[2023-07-17 09:39:50,815][257371] Max Reward on eval: 360.1539841059595[0m
[37m[1m[2023-07-17 09:39:50,816][257371] Min Reward on eval: -298.57028544098137[0m
[37m[1m[2023-07-17 09:39:50,816][257371] Mean Reward across all agents: 33.01528367319239[0m
[37m[1m[2023-07-17 09:39:50,816][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:39:50,818][257371] mean_value=-834.3539460245654, max_value=30.203708059396362[0m
[37m[1m[2023-07-17 09:39:50,820][257371] New mean coefficients: [[ 1.4511292   0.27194342  0.56904596  0.95372295 -1.650754    0.44607627]][0m
[37m[1m[2023-07-17 09:39:50,821][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:39:59,935][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 09:39:59,935][257371] FPS: 421436.11[0m
[36m[2023-07-17 09:39:59,937][257371] itr=1009, itrs=2000, Progress: 50.45%[0m
[36m[2023-07-17 09:40:11,797][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 09:40:11,797][257371] FPS: 326775.05[0m
[36m[2023-07-17 09:40:16,157][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:40:16,158][257371] Reward + Measures: [[102.61566501   0.24174133   0.37552896   0.33002499   0.36673567
    3.78823018]][0m
[37m[1m[2023-07-17 09:40:16,158][257371] Max Reward on eval: 102.61566500980979[0m
[37m[1m[2023-07-17 09:40:16,158][257371] Min Reward on eval: 102.61566500980979[0m
[37m[1m[2023-07-17 09:40:16,158][257371] Mean Reward across all agents: 102.61566500980979[0m
[37m[1m[2023-07-17 09:40:16,159][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:40:21,236][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:40:21,236][257371] Reward + Measures: [[ 77.5139725    0.17020001   0.24159999   0.25130001   0.24320002
    4.33004236]
 [ 60.50980133   0.15990001   0.15670002   0.2145       0.1586
    4.38067389]
 [  8.77525702   0.13600001   0.15350001   0.13139999   0.12920001
    5.90231705]
 ...
 [122.71045304   0.22500001   0.39190003   0.32810003   0.36090001
    3.7609818 ]
 [-18.86797142   0.16150001   0.2007       0.20130001   0.1815
    5.12885523]
 [ 68.42886134   0.24340001   0.2244       0.2155       0.25490001
    4.25542021]][0m
[37m[1m[2023-07-17 09:40:21,237][257371] Max Reward on eval: 265.83758917450905[0m
[37m[1m[2023-07-17 09:40:21,237][257371] Min Reward on eval: -170.31914520934225[0m
[37m[1m[2023-07-17 09:40:21,237][257371] Mean Reward across all agents: 61.398857079346634[0m
[37m[1m[2023-07-17 09:40:21,238][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:40:21,238][257371] mean_value=-977.3609388413927, max_value=-0.9506189601504502[0m
[36m[2023-07-17 09:40:21,241][257371] XNES is restarting with a new solution whose measures are [0.74100006 0.81469995 0.48120004 0.81470007 3.92556453] and objective is -58.17137932730839[0m
[36m[2023-07-17 09:40:21,242][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 09:40:21,244][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 09:40:21,245][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:40:30,391][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 09:40:30,392][257371] FPS: 419923.55[0m
[36m[2023-07-17 09:40:30,394][257371] itr=1010, itrs=2000, Progress: 50.50%[0m
[37m[1m[2023-07-17 09:43:56,247][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000990[0m
[36m[2023-07-17 09:44:08,578][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 09:44:08,578][257371] FPS: 329132.60[0m
[36m[2023-07-17 09:44:12,784][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:44:12,785][257371] Reward + Measures: [[16.07560649  0.47021532  0.55056     0.285707    0.4908593   3.66807318]][0m
[37m[1m[2023-07-17 09:44:12,785][257371] Max Reward on eval: 16.07560648666[0m
[37m[1m[2023-07-17 09:44:12,785][257371] Min Reward on eval: 16.07560648666[0m
[37m[1m[2023-07-17 09:44:12,786][257371] Mean Reward across all agents: 16.07560648666[0m
[37m[1m[2023-07-17 09:44:12,786][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:44:17,710][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:44:17,711][257371] Reward + Measures: [[ 82.25834247   0.59240001   0.44420004   0.37279999   0.52800006
    3.60514879]
 [ 59.95346998   0.4034       0.4553       0.2899       0.39189997
    3.67624021]
 [-62.23877852   0.35989997   0.41800004   0.27350003   0.34990001
    3.91386724]
 ...
 [ 73.18064998   0.42550001   0.5424       0.20670001   0.44769999
    3.87663722]
 [-28.88523915   0.39899999   0.43660003   0.26019999   0.3831
    3.83957744]
 [-52.53723115   0.4914       0.51529998   0.35349998   0.39520001
    3.49606562]][0m
[37m[1m[2023-07-17 09:44:17,711][257371] Max Reward on eval: 263.41639566216617[0m
[37m[1m[2023-07-17 09:44:17,711][257371] Min Reward on eval: -290.72160432748495[0m
[37m[1m[2023-07-17 09:44:17,712][257371] Mean Reward across all agents: 52.77611574391718[0m
[37m[1m[2023-07-17 09:44:17,712][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:44:17,714][257371] mean_value=-440.7775695303143, max_value=321.18583082314075[0m
[37m[1m[2023-07-17 09:44:17,717][257371] New mean coefficients: [[ 0.74026215  0.23614246  1.8380361  -0.922609   -1.2108278  -1.7122592 ]][0m
[37m[1m[2023-07-17 09:44:17,718][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:44:26,631][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-17 09:44:26,632][257371] FPS: 430898.09[0m
[36m[2023-07-17 09:44:26,634][257371] itr=1011, itrs=2000, Progress: 50.55%[0m
[36m[2023-07-17 09:44:38,227][257371] train() took 11.49 seconds to complete[0m
[36m[2023-07-17 09:44:38,227][257371] FPS: 334236.19[0m
[36m[2023-07-17 09:44:42,484][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:44:42,485][257371] Reward + Measures: [[-13.4800823    0.47157869   0.55141062   0.30804101   0.47440267
    3.53784347]][0m
[37m[1m[2023-07-17 09:44:42,485][257371] Max Reward on eval: -13.480082303160277[0m
[37m[1m[2023-07-17 09:44:42,485][257371] Min Reward on eval: -13.480082303160277[0m
[37m[1m[2023-07-17 09:44:42,486][257371] Mean Reward across all agents: -13.480082303160277[0m
[37m[1m[2023-07-17 09:44:42,486][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:44:47,471][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:44:47,472][257371] Reward + Measures: [[ 97.08097606   0.4007       0.47680002   0.18870001   0.44759998
    3.54016805]
 [162.64969923   0.51109999   0.78610009   0.16680001   0.74520004
    3.59465265]
 [ 72.97566141   0.3389       0.31710002   0.26200002   0.25710002
    3.55361676]
 ...
 [ 95.41236157   0.44040003   0.52390003   0.18179999   0.48449999
    3.65044188]
 [ 12.04047218   0.47779998   0.49440002   0.30810001   0.4258
    3.43576288]
 [ 78.5090628    0.3565       0.46580002   0.19939999   0.46210003
    3.748209  ]][0m
[37m[1m[2023-07-17 09:44:47,472][257371] Max Reward on eval: 290.17294783107934[0m
[37m[1m[2023-07-17 09:44:47,473][257371] Min Reward on eval: -118.80318076610565[0m
[37m[1m[2023-07-17 09:44:47,473][257371] Mean Reward across all agents: 68.71314099102055[0m
[37m[1m[2023-07-17 09:44:47,473][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:44:47,476][257371] mean_value=-444.9712889995653, max_value=142.982192833953[0m
[37m[1m[2023-07-17 09:44:47,478][257371] New mean coefficients: [[ 1.442211  -1.5366757  3.9710486 -1.1759529 -1.5767827 -1.0633719]][0m
[37m[1m[2023-07-17 09:44:47,479][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:44:56,505][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 09:44:56,505][257371] FPS: 425543.07[0m
[36m[2023-07-17 09:44:56,507][257371] itr=1012, itrs=2000, Progress: 50.60%[0m
[36m[2023-07-17 09:45:08,456][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-17 09:45:08,456][257371] FPS: 324239.06[0m
[36m[2023-07-17 09:45:12,806][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:45:12,807][257371] Reward + Measures: [[4.14100684 0.45705134 0.61541134 0.28467998 0.54451263 3.4354825 ]][0m
[37m[1m[2023-07-17 09:45:12,807][257371] Max Reward on eval: 4.141006844627826[0m
[37m[1m[2023-07-17 09:45:12,807][257371] Min Reward on eval: 4.141006844627826[0m
[37m[1m[2023-07-17 09:45:12,808][257371] Mean Reward across all agents: 4.141006844627826[0m
[37m[1m[2023-07-17 09:45:12,808][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:45:18,030][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:45:18,030][257371] Reward + Measures: [[228.95715332   0.49790001   0.84639996   0.12810001   0.79479998
    3.43600535]
 [ 45.55416998   0.41260001   0.70099998   0.26620001   0.71619999
    3.11758614]
 [111.01127538   0.53560001   0.87770003   0.28920004   0.83319998
    3.77626872]
 ...
 [232.95951846   0.53540003   0.80110008   0.10290001   0.76550001
    3.93621564]
 [-99.22428872   0.34329998   0.69750005   0.36880001   0.64969999
    3.05011439]
 [205.44399742   0.59170002   0.82860005   0.20729999   0.82609999
    3.3010354 ]][0m
[37m[1m[2023-07-17 09:45:18,031][257371] Max Reward on eval: 264.74835013076665[0m
[37m[1m[2023-07-17 09:45:18,031][257371] Min Reward on eval: -153.65117167504505[0m
[37m[1m[2023-07-17 09:45:18,031][257371] Mean Reward across all agents: 86.27078924431939[0m
[37m[1m[2023-07-17 09:45:18,031][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:45:18,035][257371] mean_value=-233.27745194132916, max_value=314.4093742874405[0m
[37m[1m[2023-07-17 09:45:18,038][257371] New mean coefficients: [[ 0.9302363  0.8751571  5.2610145 -1.1148874 -1.4384739 -1.0945238]][0m
[37m[1m[2023-07-17 09:45:18,039][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:45:27,150][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 09:45:27,150][257371] FPS: 421541.43[0m
[36m[2023-07-17 09:45:27,152][257371] itr=1013, itrs=2000, Progress: 50.65%[0m
[36m[2023-07-17 09:45:39,101][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-17 09:45:39,101][257371] FPS: 324303.64[0m
[36m[2023-07-17 09:45:43,449][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:45:43,450][257371] Reward + Measures: [[85.80053144  0.50896901  0.71614665  0.22889566  0.663885    3.43168068]][0m
[37m[1m[2023-07-17 09:45:43,450][257371] Max Reward on eval: 85.80053144257667[0m
[37m[1m[2023-07-17 09:45:43,451][257371] Min Reward on eval: 85.80053144257667[0m
[37m[1m[2023-07-17 09:45:43,451][257371] Mean Reward across all agents: 85.80053144257667[0m
[37m[1m[2023-07-17 09:45:43,451][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:45:48,516][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:45:48,516][257371] Reward + Measures: [[139.62479596   0.56489998   0.83289999   0.23550001   0.76980001
    3.71427011]
 [ 96.19514802   0.59910005   0.80890006   0.30520001   0.77780002
    3.94409728]
 [-28.28982359   0.74060005   0.80050004   0.61069995   0.75330001
    3.66153073]
 ...
 [-55.21182393   0.94040006   0.94779998   0.86510003   0.92480004
    3.69639587]
 [150.20787046   0.58950001   0.88450003   0.2326       0.84250003
    3.72030234]
 [ 12.17741157   0.64569998   0.86650002   0.20460001   0.86259997
    3.86757898]][0m
[37m[1m[2023-07-17 09:45:48,517][257371] Max Reward on eval: 279.9373455243185[0m
[37m[1m[2023-07-17 09:45:48,517][257371] Min Reward on eval: -147.6528926008381[0m
[37m[1m[2023-07-17 09:45:48,517][257371] Mean Reward across all agents: 96.7866341793976[0m
[37m[1m[2023-07-17 09:45:48,517][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:45:48,522][257371] mean_value=-102.34077406226731, max_value=276.44077498192155[0m
[37m[1m[2023-07-17 09:45:48,525][257371] New mean coefficients: [[-0.75056106  1.2086742   5.812786   -1.3409079  -0.54913855 -0.47488797]][0m
[37m[1m[2023-07-17 09:45:48,526][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:45:57,726][257371] train() took 9.20 seconds to complete[0m
[36m[2023-07-17 09:45:57,726][257371] FPS: 417467.54[0m
[36m[2023-07-17 09:45:57,728][257371] itr=1014, itrs=2000, Progress: 50.70%[0m
[36m[2023-07-17 09:46:09,650][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 09:46:09,650][257371] FPS: 325082.95[0m
[36m[2023-07-17 09:46:13,917][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:46:13,917][257371] Reward + Measures: [[161.51640001   0.59870464   0.81463432   0.15605034   0.78730071
    3.57859755]][0m
[37m[1m[2023-07-17 09:46:13,917][257371] Max Reward on eval: 161.5164000079983[0m
[37m[1m[2023-07-17 09:46:13,918][257371] Min Reward on eval: 161.5164000079983[0m
[37m[1m[2023-07-17 09:46:13,918][257371] Mean Reward across all agents: 161.5164000079983[0m
[37m[1m[2023-07-17 09:46:13,918][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:46:18,922][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:46:18,927][257371] Reward + Measures: [[ -81.1048226     0.44749999    0.66740006    0.44070002    0.64340001
     4.01212645]
 [  41.37833696    0.28150001    0.33730003    0.25759998    0.33620003
     3.94282317]
 [ -97.01688171    0.61070007    0.73320001    0.46900001    0.70180005
     4.10081959]
 ...
 [ 194.06396333    0.5205        0.87589997    0.1092        0.84580004
     3.37983632]
 [-124.4989829     0.61300004    0.73619998    0.546         0.70860004
     4.1499877 ]
 [-284.66145901    0.73019999    0.82909995    0.50189996    0.86829996
     4.17967558]][0m
[37m[1m[2023-07-17 09:46:18,928][257371] Max Reward on eval: 360.2064132362604[0m
[37m[1m[2023-07-17 09:46:18,928][257371] Min Reward on eval: -284.6614590078592[0m
[37m[1m[2023-07-17 09:46:18,928][257371] Mean Reward across all agents: 21.386331471686717[0m
[37m[1m[2023-07-17 09:46:18,928][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:46:18,931][257371] mean_value=-430.5442925081418, max_value=166.70721222816778[0m
[37m[1m[2023-07-17 09:46:18,934][257371] New mean coefficients: [[-0.5738702  -0.31847477  6.0873504  -1.0607759  -0.80833983 -0.7771783 ]][0m
[37m[1m[2023-07-17 09:46:18,935][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:46:27,938][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 09:46:27,938][257371] FPS: 426584.14[0m
[36m[2023-07-17 09:46:27,941][257371] itr=1015, itrs=2000, Progress: 50.75%[0m
[36m[2023-07-17 09:46:39,781][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 09:46:39,782][257371] FPS: 327348.20[0m
[36m[2023-07-17 09:46:44,115][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:46:44,116][257371] Reward + Measures: [[34.08495927  0.6943509   0.88572997  0.11860099  0.87922132  3.68617344]][0m
[37m[1m[2023-07-17 09:46:44,116][257371] Max Reward on eval: 34.084959269311405[0m
[37m[1m[2023-07-17 09:46:44,116][257371] Min Reward on eval: 34.084959269311405[0m
[37m[1m[2023-07-17 09:46:44,117][257371] Mean Reward across all agents: 34.084959269311405[0m
[37m[1m[2023-07-17 09:46:44,117][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:46:49,203][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:46:49,204][257371] Reward + Measures: [[ 232.42549327    0.72570002    0.91670001    0.1097        0.84799999
     4.61792898]
 [-186.81001144    0.5104        0.35890004    0.56450003    0.58289999
     3.42513394]
 [ 395.51972961    0.42060003    0.97269994    0.34670001    0.95749998
     6.90479755]
 ...
 [  84.1045382     0.5966        0.47459999    0.47100002    0.73149997
     4.46250868]
 [ 284.32966711    0.1145        0.98940003    0.44790003    0.95730001
     6.83653784]
 [ 455.43276594    0.1165        0.95850003    0.57979995    0.94360012
     7.02722263]][0m
[37m[1m[2023-07-17 09:46:49,204][257371] Max Reward on eval: 626.2528724707663[0m
[37m[1m[2023-07-17 09:46:49,204][257371] Min Reward on eval: -530.6723138276487[0m
[37m[1m[2023-07-17 09:46:49,205][257371] Mean Reward across all agents: 208.6139812068091[0m
[37m[1m[2023-07-17 09:46:49,205][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:46:49,211][257371] mean_value=-221.8694742398142, max_value=310.2333099982877[0m
[37m[1m[2023-07-17 09:46:49,214][257371] New mean coefficients: [[-0.5827905  -0.5347166   6.534691   -2.4403577  -1.2845592  -0.33786082]][0m
[37m[1m[2023-07-17 09:46:49,214][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:46:58,310][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 09:46:58,310][257371] FPS: 422253.34[0m
[36m[2023-07-17 09:46:58,313][257371] itr=1016, itrs=2000, Progress: 50.80%[0m
[36m[2023-07-17 09:47:10,243][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 09:47:10,244][257371] FPS: 324685.41[0m
[36m[2023-07-17 09:47:14,665][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:47:14,665][257371] Reward + Measures: [[-84.31731399   0.81466925   0.95624363   0.034267     0.95876431
    3.88328743]][0m
[37m[1m[2023-07-17 09:47:14,665][257371] Max Reward on eval: -84.3173139877977[0m
[37m[1m[2023-07-17 09:47:14,666][257371] Min Reward on eval: -84.3173139877977[0m
[37m[1m[2023-07-17 09:47:14,666][257371] Mean Reward across all agents: -84.3173139877977[0m
[37m[1m[2023-07-17 09:47:14,666][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:47:19,717][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:47:19,717][257371] Reward + Measures: [[-335.83111636    0.83179998    0.722         0.61689997    0.87559998
     5.07616711]
 [ 213.93487359    0.52970004    0.86720002    0.32650003    0.87740004
     5.45021582]
 [-198.89825627    0.86250001    0.81999999    0.79310006    0.73830003
     5.91099739]
 ...
 [-468.45817948    0.90280002    0.8847        0.66189998    0.97809994
     5.91433477]
 [-448.32709597    0.88789999    0.88230002    0.82190001    0.93740004
     6.29586649]
 [  74.51719475    0.43990001    0.83200008    0.2405        0.86910003
     5.80934095]][0m
[37m[1m[2023-07-17 09:47:19,718][257371] Max Reward on eval: 363.48325158488007[0m
[37m[1m[2023-07-17 09:47:19,718][257371] Min Reward on eval: -628.9056587165221[0m
[37m[1m[2023-07-17 09:47:19,718][257371] Mean Reward across all agents: -202.631466089237[0m
[37m[1m[2023-07-17 09:47:19,719][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:47:19,724][257371] mean_value=-428.8922606639282, max_value=212.4106555438907[0m
[37m[1m[2023-07-17 09:47:19,727][257371] New mean coefficients: [[-1.6531187  -0.04318795  4.2045274  -2.4228892  -1.3703284  -0.96470296]][0m
[37m[1m[2023-07-17 09:47:19,728][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:47:28,815][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 09:47:28,815][257371] FPS: 422685.55[0m
[36m[2023-07-17 09:47:28,817][257371] itr=1017, itrs=2000, Progress: 50.85%[0m
[36m[2023-07-17 09:47:40,579][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 09:47:40,579][257371] FPS: 329412.77[0m
[36m[2023-07-17 09:47:44,903][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:47:44,904][257371] Reward + Measures: [[-152.52291256    0.82853466    0.96532434    0.02739367    0.96791667
     3.85231137]][0m
[37m[1m[2023-07-17 09:47:44,904][257371] Max Reward on eval: -152.52291256362037[0m
[37m[1m[2023-07-17 09:47:44,904][257371] Min Reward on eval: -152.52291256362037[0m
[37m[1m[2023-07-17 09:47:44,905][257371] Mean Reward across all agents: -152.52291256362037[0m
[37m[1m[2023-07-17 09:47:44,905][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:47:49,971][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:47:49,971][257371] Reward + Measures: [[ -80.59834162    0.20910001    0.96119994    0.37509999    0.9325
     4.44646215]
 [ -57.49884398    0.85129994    0.95060009    0.77509999    0.93529999
     4.40557432]
 [  34.83807917    0.44890004    0.95170003    0.30950001    0.92259997
     4.16636944]
 ...
 [   1.28077936    0.44840002    0.93870002    0.31619999    0.90570003
     3.69912958]
 [ -73.17693268    0.71900004    0.95150006    0.57480001    0.9174
     3.73868823]
 [-123.13473099    0.65570003    0.75830001    0.067         0.80159998
     3.79833674]][0m
[37m[1m[2023-07-17 09:47:49,971][257371] Max Reward on eval: 374.8593788092956[0m
[37m[1m[2023-07-17 09:47:49,972][257371] Min Reward on eval: -234.10253716893493[0m
[37m[1m[2023-07-17 09:47:49,972][257371] Mean Reward across all agents: 59.67801580729861[0m
[37m[1m[2023-07-17 09:47:49,972][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:47:49,978][257371] mean_value=-166.71128729956348, max_value=232.5495044304493[0m
[37m[1m[2023-07-17 09:47:49,981][257371] New mean coefficients: [[-3.1605275   0.9214709   3.453089   -3.5669303  -1.8465433  -0.26733375]][0m
[37m[1m[2023-07-17 09:47:49,982][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:47:59,017][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 09:47:59,017][257371] FPS: 425067.37[0m
[36m[2023-07-17 09:47:59,020][257371] itr=1018, itrs=2000, Progress: 50.90%[0m
[36m[2023-07-17 09:48:10,875][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 09:48:10,875][257371] FPS: 326833.51[0m
[36m[2023-07-17 09:48:15,159][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:48:15,159][257371] Reward + Measures: [[-289.87904715    0.88672662    0.97670436    0.018337      0.97899163
     3.92568684]][0m
[37m[1m[2023-07-17 09:48:15,159][257371] Max Reward on eval: -289.8790471530059[0m
[37m[1m[2023-07-17 09:48:15,160][257371] Min Reward on eval: -289.8790471530059[0m
[37m[1m[2023-07-17 09:48:15,160][257371] Mean Reward across all agents: -289.8790471530059[0m
[37m[1m[2023-07-17 09:48:15,160][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:48:20,508][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:48:20,509][257371] Reward + Measures: [[  62.25219681    0.39840004    0.61540002    0.27870002    0.52520001
     4.98808384]
 [-312.13631059    0.85009998    0.96130002    0.0212        0.98250002
     5.12634468]
 [  57.5911832     0.30090001    0.75430006    0.63429999    0.54089999
     5.7508769 ]
 ...
 [ 123.01089383    0.1822        0.47530004    0.32350001    0.36380002
     4.47628641]
 [   1.12468748    0.26700002    0.48380002    0.39450002    0.42159995
     6.05473471]
 [  30.64026134    0.3193        0.44969997    0.29010001    0.41170001
     4.75586414]][0m
[37m[1m[2023-07-17 09:48:20,509][257371] Max Reward on eval: 440.1537642478943[0m
[37m[1m[2023-07-17 09:48:20,509][257371] Min Reward on eval: -828.9594650187995[0m
[37m[1m[2023-07-17 09:48:20,510][257371] Mean Reward across all agents: -114.47187836152196[0m
[37m[1m[2023-07-17 09:48:20,510][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:48:20,512][257371] mean_value=-663.1602076091323, max_value=129.15443282950133[0m
[37m[1m[2023-07-17 09:48:20,515][257371] New mean coefficients: [[-2.7204995   0.43625823  3.585636   -3.714713   -1.7421801  -0.3225045 ]][0m
[37m[1m[2023-07-17 09:48:20,516][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:48:29,488][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 09:48:29,488][257371] FPS: 428106.25[0m
[36m[2023-07-17 09:48:29,490][257371] itr=1019, itrs=2000, Progress: 50.95%[0m
[36m[2023-07-17 09:48:41,399][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 09:48:41,399][257371] FPS: 325446.37[0m
[36m[2023-07-17 09:48:45,698][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:48:45,699][257371] Reward + Measures: [[-328.35450132    0.92863071    0.98426527    0.014423      0.98461264
     3.91947556]][0m
[37m[1m[2023-07-17 09:48:45,699][257371] Max Reward on eval: -328.3545013228097[0m
[37m[1m[2023-07-17 09:48:45,699][257371] Min Reward on eval: -328.3545013228097[0m
[37m[1m[2023-07-17 09:48:45,700][257371] Mean Reward across all agents: -328.3545013228097[0m
[37m[1m[2023-07-17 09:48:45,700][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:48:50,751][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:48:50,752][257371] Reward + Measures: [[  23.46629289    0.68609995    0.67160004    0.1116        0.7240001
     4.94418812]
 [ -22.00023662    0.53680003    0.49250004    0.45590001    0.32679999
     4.26923513]
 [ -23.99501489    0.27059999    0.31140003    0.27239999    0.30290002
     5.28210068]
 ...
 [ 105.82934235    0.39129996    0.2579        0.3475        0.26800001
     3.63583374]
 [-316.85110092    0.75410002    0.39359999    0.60390002    0.67160004
     5.43601894]
 [ -48.79858996    0.37340003    0.3053        0.34549999    0.3901
     4.14907551]][0m
[37m[1m[2023-07-17 09:48:50,752][257371] Max Reward on eval: 182.9781285017729[0m
[37m[1m[2023-07-17 09:48:50,752][257371] Min Reward on eval: -711.4876403534785[0m
[37m[1m[2023-07-17 09:48:50,752][257371] Mean Reward across all agents: -93.12562523482636[0m
[37m[1m[2023-07-17 09:48:50,753][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:48:50,756][257371] mean_value=-398.58587990321314, max_value=138.67293463080136[0m
[37m[1m[2023-07-17 09:48:50,759][257371] New mean coefficients: [[-2.1339347   0.25492546  3.34754    -3.4511986  -0.65255475  0.29619387]][0m
[37m[1m[2023-07-17 09:48:50,760][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:48:59,849][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 09:48:59,849][257371] FPS: 422570.02[0m
[36m[2023-07-17 09:48:59,851][257371] itr=1020, itrs=2000, Progress: 51.00%[0m
[37m[1m[2023-07-17 09:52:24,540][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001000[0m
[36m[2023-07-17 09:52:37,032][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-17 09:52:37,033][257371] FPS: 324028.26[0m
[36m[2023-07-17 09:52:41,400][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:52:41,400][257371] Reward + Measures: [[-353.86642928    0.94064003    0.98584002    0.01147167    0.98719937
     4.00296307]][0m
[37m[1m[2023-07-17 09:52:41,401][257371] Max Reward on eval: -353.8664292752948[0m
[37m[1m[2023-07-17 09:52:41,401][257371] Min Reward on eval: -353.8664292752948[0m
[37m[1m[2023-07-17 09:52:41,401][257371] Mean Reward across all agents: -353.8664292752948[0m
[37m[1m[2023-07-17 09:52:41,401][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:52:46,467][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:52:46,468][257371] Reward + Measures: [[-157.83307694    0.73040003    0.49689999    0.51780003    0.53640002
     5.08047628]
 [  31.17146542    0.5327        0.55479997    0.35600001    0.53189999
     3.65039372]
 [  68.03024703    0.30600002    0.25610003    0.0968        0.27860001
     5.81684828]
 ...
 [  92.63606624    0.60579997    0.47080001    0.2597        0.51130003
     5.98971891]
 [ -60.53551828    0.81520003    0.90109998    0.47999999    0.86849993
     5.20082331]
 [  -8.2143547     0.79880005    0.88920003    0.1798        0.82590002
     3.38675046]][0m
[37m[1m[2023-07-17 09:52:46,468][257371] Max Reward on eval: 444.70343046709894[0m
[37m[1m[2023-07-17 09:52:46,468][257371] Min Reward on eval: -457.63892364278433[0m
[37m[1m[2023-07-17 09:52:46,469][257371] Mean Reward across all agents: 17.62387588461477[0m
[37m[1m[2023-07-17 09:52:46,469][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:52:46,475][257371] mean_value=-191.26464221042053, max_value=519.4626754444192[0m
[37m[1m[2023-07-17 09:52:46,478][257371] New mean coefficients: [[-3.9157338  0.5937327  3.2727757 -3.573574  -1.3123032 -0.1107415]][0m
[37m[1m[2023-07-17 09:52:46,479][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:52:55,665][257371] train() took 9.18 seconds to complete[0m
[36m[2023-07-17 09:52:55,666][257371] FPS: 418104.39[0m
[36m[2023-07-17 09:52:55,668][257371] itr=1021, itrs=2000, Progress: 51.05%[0m
[36m[2023-07-17 09:53:07,441][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 09:53:07,441][257371] FPS: 329093.63[0m
[36m[2023-07-17 09:53:11,703][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:53:11,703][257371] Reward + Measures: [[-316.76984569    0.96589458    0.99036562    0.00716167    0.99185163
     4.11801291]][0m
[37m[1m[2023-07-17 09:53:11,703][257371] Max Reward on eval: -316.76984568574386[0m
[37m[1m[2023-07-17 09:53:11,704][257371] Min Reward on eval: -316.76984568574386[0m
[37m[1m[2023-07-17 09:53:11,704][257371] Mean Reward across all agents: -316.76984568574386[0m
[37m[1m[2023-07-17 09:53:11,704][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:53:16,677][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:53:16,678][257371] Reward + Measures: [[ 183.65239956    0.84770006    0.96630001    0.32780001    0.93119997
     4.60173559]
 [  37.71657364    0.80740005    0.70620006    0.20550001    0.72609997
     5.76961851]
 [-241.33711041    0.77019995    0.84540004    0.39719999    0.58980006
     3.53201342]
 ...
 [ -62.58218429    0.91149998    0.94180006    0.77420002    0.90920001
     4.38749743]
 [  67.6665995     0.82410002    0.949         0.53109998    0.90210003
     4.82327747]
 [  67.21917533    0.62330002    0.76359999    0.55809999    0.75100005
     4.52362204]][0m
[37m[1m[2023-07-17 09:53:16,678][257371] Max Reward on eval: 481.1901412537321[0m
[37m[1m[2023-07-17 09:53:16,678][257371] Min Reward on eval: -534.3040619004518[0m
[37m[1m[2023-07-17 09:53:16,678][257371] Mean Reward across all agents: -59.910012038424576[0m
[37m[1m[2023-07-17 09:53:16,679][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:53:16,684][257371] mean_value=-250.34503043917735, max_value=560.0710635573976[0m
[37m[1m[2023-07-17 09:53:16,687][257371] New mean coefficients: [[-4.1462507  -0.03842121  3.5283165  -4.142957   -3.456514   -0.13136756]][0m
[37m[1m[2023-07-17 09:53:16,688][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:53:25,422][257371] train() took 8.73 seconds to complete[0m
[36m[2023-07-17 09:53:25,422][257371] FPS: 439744.39[0m
[36m[2023-07-17 09:53:25,424][257371] itr=1022, itrs=2000, Progress: 51.10%[0m
[36m[2023-07-17 09:53:37,083][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 09:53:37,084][257371] FPS: 332399.89[0m
[36m[2023-07-17 09:53:41,393][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:53:41,394][257371] Reward + Measures: [[-331.01804763    0.96723533    0.99059063    0.00559       0.99273437
     4.11318445]][0m
[37m[1m[2023-07-17 09:53:41,394][257371] Max Reward on eval: -331.01804762757456[0m
[37m[1m[2023-07-17 09:53:41,394][257371] Min Reward on eval: -331.01804762757456[0m
[37m[1m[2023-07-17 09:53:41,395][257371] Mean Reward across all agents: -331.01804762757456[0m
[37m[1m[2023-07-17 09:53:41,395][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:53:46,639][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:53:46,644][257371] Reward + Measures: [[  67.7526885     0.22589998    0.15550001    0.1832        0.1911
     4.75329447]
 [ 104.5821445     0.1962        0.0629        0.1621        0.17
     6.09862566]
 [ -36.00207661    0.50299996    0.68099999    0.38349998    0.43249997
     4.89899969]
 ...
 [  21.46643357    0.2773        0.21529999    0.2507        0.1304
     5.61451817]
 [-241.4966469     0.72850001    0.75900006    0.37400001    0.4869
     4.83338118]
 [ -12.64199617    0.33670002    0.20700002    0.39429998    0.21760002
     4.95933056]][0m
[37m[1m[2023-07-17 09:53:46,645][257371] Max Reward on eval: 744.5465278757736[0m
[37m[1m[2023-07-17 09:53:46,645][257371] Min Reward on eval: -641.2502746766434[0m
[37m[1m[2023-07-17 09:53:46,645][257371] Mean Reward across all agents: 28.966217443184807[0m
[37m[1m[2023-07-17 09:53:46,646][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:53:46,649][257371] mean_value=-500.31407185573323, max_value=431.0196051026725[0m
[37m[1m[2023-07-17 09:53:46,651][257371] New mean coefficients: [[-4.484939    0.5932035   3.95022    -3.2659059  -2.6482775  -0.34472483]][0m
[37m[1m[2023-07-17 09:53:46,652][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:53:55,641][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 09:53:55,641][257371] FPS: 427283.81[0m
[36m[2023-07-17 09:53:55,644][257371] itr=1023, itrs=2000, Progress: 51.15%[0m
[36m[2023-07-17 09:54:07,369][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 09:54:07,369][257371] FPS: 330482.51[0m
[36m[2023-07-17 09:54:11,651][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:54:11,652][257371] Reward + Measures: [[-290.20413728    0.97186261    0.99102336    0.004924      0.99347866
     4.12100697]][0m
[37m[1m[2023-07-17 09:54:11,652][257371] Max Reward on eval: -290.20413727571224[0m
[37m[1m[2023-07-17 09:54:11,652][257371] Min Reward on eval: -290.20413727571224[0m
[37m[1m[2023-07-17 09:54:11,653][257371] Mean Reward across all agents: -290.20413727571224[0m
[37m[1m[2023-07-17 09:54:11,653][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:54:16,622][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:54:16,623][257371] Reward + Measures: [[  33.50558017    0.20310001    0.15120001    0.13560002    0.20020001
     4.98172522]
 [ 230.2603875     0.47690001    0.1146        0.47440001    0.40690002
     4.60905075]
 [-116.68775501    0.67869997    0.79389995    0.43899998    0.80910009
     3.75095868]
 ...
 [  27.70518407    0.2102        0.28020003    0.1339        0.25670001
     4.02892971]
 [  33.50859735    0.12400001    0.1787        0.15439999    0.15350001
     5.77263021]
 [  36.05379426    0.23480001    0.32970002    0.1437        0.31969997
     3.97586131]][0m
[37m[1m[2023-07-17 09:54:16,623][257371] Max Reward on eval: 370.94855879060924[0m
[37m[1m[2023-07-17 09:54:16,623][257371] Min Reward on eval: -448.79749680440875[0m
[37m[1m[2023-07-17 09:54:16,624][257371] Mean Reward across all agents: 27.662048597824686[0m
[37m[1m[2023-07-17 09:54:16,624][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:54:16,628][257371] mean_value=-634.4508809091137, max_value=796.5867901453748[0m
[37m[1m[2023-07-17 09:54:16,630][257371] New mean coefficients: [[-3.6558316   1.1326274   3.9640243  -1.1542072  -2.2993991  -0.63090575]][0m
[37m[1m[2023-07-17 09:54:16,631][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:54:25,638][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 09:54:25,638][257371] FPS: 426410.15[0m
[36m[2023-07-17 09:54:25,641][257371] itr=1024, itrs=2000, Progress: 51.20%[0m
[36m[2023-07-17 09:54:37,515][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 09:54:37,515][257371] FPS: 326420.13[0m
[36m[2023-07-17 09:54:41,742][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:54:41,748][257371] Reward + Measures: [[-299.22278956    0.97371835    0.9908343     0.00419867    0.99339932
     4.06913662]][0m
[37m[1m[2023-07-17 09:54:41,748][257371] Max Reward on eval: -299.22278955625706[0m
[37m[1m[2023-07-17 09:54:41,748][257371] Min Reward on eval: -299.22278955625706[0m
[37m[1m[2023-07-17 09:54:41,748][257371] Mean Reward across all agents: -299.22278955625706[0m
[37m[1m[2023-07-17 09:54:41,749][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:54:46,742][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:54:46,747][257371] Reward + Measures: [[-257.32736539    0.63070005    0.72319996    0.5359        0.74370003
     4.40206861]
 [-349.70981276    0.96029997    0.94099998    0.91409999    0.96800005
     5.01938534]
 [ 105.0890999     0.27219999    0.84910005    0.57540005    0.833
     4.62910032]
 ...
 [  12.40227577    0.40490004    0.57880002    0.31209999    0.60219997
     3.6484921 ]
 [  68.44576733    0.4025        0.52809995    0.30790001    0.61540002
     3.68158078]
 [-546.45660401    0.96700001    0.95480007    0.88700002    0.98550004
     4.41107368]][0m
[37m[1m[2023-07-17 09:54:46,747][257371] Max Reward on eval: 404.27304656468334[0m
[37m[1m[2023-07-17 09:54:46,747][257371] Min Reward on eval: -643.4949645815417[0m
[37m[1m[2023-07-17 09:54:46,748][257371] Mean Reward across all agents: -188.64468267764963[0m
[37m[1m[2023-07-17 09:54:46,748][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:54:46,750][257371] mean_value=-519.8708575079042, max_value=-16.4220709239126[0m
[36m[2023-07-17 09:54:46,753][257371] XNES is restarting with a new solution whose measures are [0.69569999 0.63770002 0.39250001 0.1869     5.56553698] and objective is 11.806746735796333[0m
[36m[2023-07-17 09:54:46,754][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 09:54:46,756][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 09:54:46,757][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:54:55,799][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 09:54:55,805][257371] FPS: 424767.28[0m
[36m[2023-07-17 09:54:55,807][257371] itr=1025, itrs=2000, Progress: 51.25%[0m
[36m[2023-07-17 09:55:07,449][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 09:55:07,450][257371] FPS: 332844.19[0m
[36m[2023-07-17 09:55:11,672][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:55:11,673][257371] Reward + Measures: [[-69.34901313   0.39787802   0.52413869   0.52303767   0.43831098
    3.82054806]][0m
[37m[1m[2023-07-17 09:55:11,673][257371] Max Reward on eval: -69.34901312754702[0m
[37m[1m[2023-07-17 09:55:11,673][257371] Min Reward on eval: -69.34901312754702[0m
[37m[1m[2023-07-17 09:55:11,673][257371] Mean Reward across all agents: -69.34901312754702[0m
[37m[1m[2023-07-17 09:55:11,674][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:55:16,618][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:55:16,618][257371] Reward + Measures: [[  45.43543643    0.1796        0.18789999    0.17770001    0.18309999
     5.29004908]
 [  22.40605117    0.16860001    0.233         0.15390001    0.19499999
     5.97071552]
 [  67.18629448    0.1594        0.4113        0.4082        0.4276
     5.69636106]
 ...
 [-128.77139734    0.48840004    0.58859998    0.50960004    0.21799998
     5.16710949]
 [ -44.64598739    0.18429999    0.23870002    0.18620001    0.23220001
     5.32805634]
 [   8.03527327    0.3012        0.36209998    0.32049999    0.34750003
     5.89345121]][0m
[37m[1m[2023-07-17 09:55:16,619][257371] Max Reward on eval: 413.04304074542597[0m
[37m[1m[2023-07-17 09:55:16,619][257371] Min Reward on eval: -412.09585355687886[0m
[37m[1m[2023-07-17 09:55:16,619][257371] Mean Reward across all agents: -3.9590473895955127[0m
[37m[1m[2023-07-17 09:55:16,619][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:55:16,621][257371] mean_value=-355.75427845400463, max_value=153.75363829623538[0m
[37m[1m[2023-07-17 09:55:16,624][257371] New mean coefficients: [[ 0.8776649 -0.7088697 -1.3612949 -2.2724705 -1.5977871 -0.692224 ]][0m
[37m[1m[2023-07-17 09:55:16,624][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:55:25,567][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 09:55:25,567][257371] FPS: 429487.93[0m
[36m[2023-07-17 09:55:25,569][257371] itr=1026, itrs=2000, Progress: 51.30%[0m
[36m[2023-07-17 09:55:37,553][257371] train() took 11.88 seconds to complete[0m
[36m[2023-07-17 09:55:37,553][257371] FPS: 323327.19[0m
[36m[2023-07-17 09:55:41,843][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:55:41,843][257371] Reward + Measures: [[-19.29406116   0.3751587    0.32408902   0.35299903   0.354691
    4.0551815 ]][0m
[37m[1m[2023-07-17 09:55:41,843][257371] Max Reward on eval: -19.294061162793813[0m
[37m[1m[2023-07-17 09:55:41,844][257371] Min Reward on eval: -19.294061162793813[0m
[37m[1m[2023-07-17 09:55:41,844][257371] Mean Reward across all agents: -19.294061162793813[0m
[37m[1m[2023-07-17 09:55:41,844][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:55:46,836][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:55:46,836][257371] Reward + Measures: [[-124.89081695    0.30489999    0.27740002    0.26400003    0.16410001
     5.77562571]
 [  26.66106255    0.092         0.08190001    0.0691        0.0973
     5.76436806]
 [  10.46591116    0.33269998    0.30350003    0.33040002    0.31900001
     6.8886714 ]
 ...
 [  29.63824892    0.48799998    0.49729997    0.0656        0.50240004
     6.41204691]
 [ -51.83994699    0.21350001    0.15830001    0.19860001    0.15790001
     6.02534199]
 [-139.96114039    0.39680001    0.37540004    0.1085        0.44500002
     5.62469673]][0m
[37m[1m[2023-07-17 09:55:46,837][257371] Max Reward on eval: 137.50879328374285[0m
[37m[1m[2023-07-17 09:55:46,837][257371] Min Reward on eval: -394.8306379519403[0m
[37m[1m[2023-07-17 09:55:46,837][257371] Mean Reward across all agents: -21.089411149004192[0m
[37m[1m[2023-07-17 09:55:46,837][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:55:46,838][257371] mean_value=-434.5148333354725, max_value=-33.808834298430725[0m
[36m[2023-07-17 09:55:46,841][257371] XNES is restarting with a new solution whose measures are [0.0061     0.99350005 0.98339999 0.99159998 1.22526276] and objective is 208.77966308788163[0m
[36m[2023-07-17 09:55:46,842][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 09:55:46,844][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 09:55:46,845][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:55:55,869][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 09:55:55,875][257371] FPS: 425611.12[0m
[36m[2023-07-17 09:55:55,877][257371] itr=1027, itrs=2000, Progress: 51.35%[0m
[36m[2023-07-17 09:56:07,672][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 09:56:07,673][257371] FPS: 328587.20[0m
[36m[2023-07-17 09:56:11,981][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:56:11,987][257371] Reward + Measures: [[22.43297101  0.00738667  0.91024268  0.77985507  0.76248902  0.57084936]][0m
[37m[1m[2023-07-17 09:56:11,987][257371] Max Reward on eval: 22.43297100910473[0m
[37m[1m[2023-07-17 09:56:11,987][257371] Min Reward on eval: 22.43297100910473[0m
[37m[1m[2023-07-17 09:56:11,988][257371] Mean Reward across all agents: 22.43297100910473[0m
[37m[1m[2023-07-17 09:56:11,988][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:56:17,007][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:56:17,013][257371] Reward + Measures: [[-27.20200413   0.77200001   0.65410006   0.70240003   0.64099997
    2.18528867]
 [ 53.04974769   0.198        0.58950001   0.30939999   0.43039998
    3.05561948]
 [-64.04984712   0.29390001   0.47259998   0.31140003   0.32829997
    3.5164659 ]
 ...
 [ 97.24209656   0.53820008   0.6462       0.62860006   0.42140004
    1.73150384]
 [-57.17618483   0.4039       0.66820002   0.40770003   0.60290003
    1.26746905]
 [ 88.77502635   0.54839998   0.57109994   0.62370002   0.26970002
    3.82818675]][0m
[37m[1m[2023-07-17 09:56:17,013][257371] Max Reward on eval: 267.641942892503[0m
[37m[1m[2023-07-17 09:56:17,013][257371] Min Reward on eval: -361.5089016949758[0m
[37m[1m[2023-07-17 09:56:17,014][257371] Mean Reward across all agents: -11.484317383118206[0m
[37m[1m[2023-07-17 09:56:17,014][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:56:17,018][257371] mean_value=-629.5420308241457, max_value=391.21716767933077[0m
[37m[1m[2023-07-17 09:56:17,021][257371] New mean coefficients: [[ 0.81184447 -0.14752322 -1.7564356  -1.6265457  -1.6757472  -0.81358266]][0m
[37m[1m[2023-07-17 09:56:17,022][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:56:26,180][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-17 09:56:26,180][257371] FPS: 419388.04[0m
[36m[2023-07-17 09:56:26,182][257371] itr=1028, itrs=2000, Progress: 51.40%[0m
[36m[2023-07-17 09:56:38,133][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-17 09:56:38,133][257371] FPS: 324154.84[0m
[36m[2023-07-17 09:56:42,488][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:56:42,493][257371] Reward + Measures: [[-20.10029388   0.33315533   0.65348798   0.69640964   0.43288565
    1.25806499]][0m
[37m[1m[2023-07-17 09:56:42,493][257371] Max Reward on eval: -20.100293881607126[0m
[37m[1m[2023-07-17 09:56:42,494][257371] Min Reward on eval: -20.100293881607126[0m
[37m[1m[2023-07-17 09:56:42,494][257371] Mean Reward across all agents: -20.100293881607126[0m
[37m[1m[2023-07-17 09:56:42,494][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:56:47,808][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:56:47,808][257371] Reward + Measures: [[ 74.9513423    0.2138       0.67219996   0.42009997   0.55510002
    2.43200994]
 [ 48.68746449   0.2969       0.3448       0.27879998   0.33930001
    3.41734934]
 [-69.778365     0.31300002   0.27150002   0.24800001   0.18329999
    5.07855749]
 ...
 [-30.70426559   0.55559999   0.37020001   0.42230001   0.1464
    4.02244139]
 [  9.01987205   0.31850001   0.51739997   0.40190002   0.45010003
    4.71588469]
 [-33.52516208   0.3188       0.43700001   0.30380002   0.39200002
    3.23824859]][0m
[37m[1m[2023-07-17 09:56:47,809][257371] Max Reward on eval: 556.2739048507065[0m
[37m[1m[2023-07-17 09:56:47,809][257371] Min Reward on eval: -166.57095518372952[0m
[37m[1m[2023-07-17 09:56:47,809][257371] Mean Reward across all agents: 16.509176832449537[0m
[37m[1m[2023-07-17 09:56:47,809][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:56:47,812][257371] mean_value=-986.9798004161887, max_value=343.84852329069895[0m
[37m[1m[2023-07-17 09:56:47,814][257371] New mean coefficients: [[ 0.5789759   0.46890324 -0.9473012  -1.814826   -2.0222833  -0.9924498 ]][0m
[37m[1m[2023-07-17 09:56:47,815][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:56:56,927][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 09:56:56,927][257371] FPS: 421495.23[0m
[36m[2023-07-17 09:56:56,930][257371] itr=1029, itrs=2000, Progress: 51.45%[0m
[36m[2023-07-17 09:57:08,582][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 09:57:08,582][257371] FPS: 332720.60[0m
[36m[2023-07-17 09:57:12,950][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:57:12,950][257371] Reward + Measures: [[-17.86854475   0.55800301   0.59141862   0.68430763   0.17216633
    1.20707631]][0m
[37m[1m[2023-07-17 09:57:12,950][257371] Max Reward on eval: -17.86854475128675[0m
[37m[1m[2023-07-17 09:57:12,951][257371] Min Reward on eval: -17.86854475128675[0m
[37m[1m[2023-07-17 09:57:12,951][257371] Mean Reward across all agents: -17.86854475128675[0m
[37m[1m[2023-07-17 09:57:12,951][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:57:17,994][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 09:57:17,995][257371] Reward + Measures: [[-72.06554579   0.53890008   0.31599998   0.33849999   0.32600003
    3.75844073]
 [ 80.86116115   0.21570002   0.45280004   0.2316       0.41440001
    4.84448004]
 [  6.15929983   0.2737       0.17830001   0.1454       0.22319999
    4.24240446]
 ...
 [ 49.32141151   0.65859997   0.51950002   0.60630006   0.25370002
    5.14896393]
 [120.6286663    0.2458       0.3576       0.23300003   0.26400003
    2.8638916 ]
 [-62.68871463   0.41980001   0.28190002   0.37920001   0.29609999
    3.38134003]][0m
[37m[1m[2023-07-17 09:57:17,995][257371] Max Reward on eval: 418.6805439045653[0m
[37m[1m[2023-07-17 09:57:17,995][257371] Min Reward on eval: -267.1769929738715[0m
[37m[1m[2023-07-17 09:57:17,996][257371] Mean Reward across all agents: 28.677508049028244[0m
[37m[1m[2023-07-17 09:57:17,996][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 09:57:17,999][257371] mean_value=-1303.1345827463952, max_value=323.3605798038355[0m
[37m[1m[2023-07-17 09:57:18,001][257371] New mean coefficients: [[-0.5499389  1.0680826 -1.689357  -1.9606401 -3.0001602 -1.0396942]][0m
[37m[1m[2023-07-17 09:57:18,002][257371] Moving the mean solution point...[0m
[36m[2023-07-17 09:57:27,062][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 09:57:27,062][257371] FPS: 423928.73[0m
[36m[2023-07-17 09:57:27,064][257371] itr=1030, itrs=2000, Progress: 51.50%[0m
[37m[1m[2023-07-17 10:00:50,302][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001010[0m
[36m[2023-07-17 10:01:02,541][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 10:01:02,542][257371] FPS: 330216.33[0m
[36m[2023-07-17 10:01:06,823][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:01:06,823][257371] Reward + Measures: [[46.7907041   0.516132    0.432127    0.46460399  0.33786932  1.22738731]][0m
[37m[1m[2023-07-17 10:01:06,823][257371] Max Reward on eval: 46.79070409539408[0m
[37m[1m[2023-07-17 10:01:06,824][257371] Min Reward on eval: 46.79070409539408[0m
[37m[1m[2023-07-17 10:01:06,824][257371] Mean Reward across all agents: 46.79070409539408[0m
[37m[1m[2023-07-17 10:01:06,824][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:01:12,132][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:01:12,133][257371] Reward + Measures: [[ 72.43530156   0.24080001   0.24320002   0.29410002   0.14799999
    4.18234015]
 [  1.91895699   0.37510002   0.65620005   0.13150001   0.73200005
    1.96237814]
 [-28.34022184   0.12560001   0.14800002   0.1222       0.142
    5.72447109]
 ...
 [ 11.67822823   0.15700001   0.22230001   0.2184       0.22090001
    4.3791976 ]
 [ 53.19783452   0.29169998   0.24589999   0.32390001   0.22660001
    3.65206456]
 [  7.08684619   0.17640001   0.226        0.13890001   0.18679999
    3.9876802 ]][0m
[37m[1m[2023-07-17 10:01:12,133][257371] Max Reward on eval: 146.96190785812215[0m
[37m[1m[2023-07-17 10:01:12,133][257371] Min Reward on eval: -236.586065239273[0m
[37m[1m[2023-07-17 10:01:12,134][257371] Mean Reward across all agents: -4.5164294587373[0m
[37m[1m[2023-07-17 10:01:12,134][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:01:12,137][257371] mean_value=-1616.9533576453312, max_value=311.6363009705008[0m
[37m[1m[2023-07-17 10:01:12,141][257371] New mean coefficients: [[-1.2691772  1.5208871 -1.895402  -1.0024598 -1.9300395 -1.7461628]][0m
[37m[1m[2023-07-17 10:01:12,142][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:01:21,037][257371] train() took 8.89 seconds to complete[0m
[36m[2023-07-17 10:01:21,037][257371] FPS: 431807.38[0m
[36m[2023-07-17 10:01:21,040][257371] itr=1031, itrs=2000, Progress: 51.55%[0m
[36m[2023-07-17 10:01:32,863][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 10:01:32,864][257371] FPS: 327765.20[0m
[36m[2023-07-17 10:01:37,107][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:01:37,108][257371] Reward + Measures: [[40.97554104  0.53798068  0.40988931  0.46108666  0.32639435  1.20021248]][0m
[37m[1m[2023-07-17 10:01:37,108][257371] Max Reward on eval: 40.97554104421833[0m
[37m[1m[2023-07-17 10:01:37,108][257371] Min Reward on eval: 40.97554104421833[0m
[37m[1m[2023-07-17 10:01:37,109][257371] Mean Reward across all agents: 40.97554104421833[0m
[37m[1m[2023-07-17 10:01:37,109][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:01:42,127][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:01:42,127][257371] Reward + Measures: [[-40.41006941   0.2852       0.38350001   0.48760006   0.1989
    3.2011621 ]
 [-22.38716348   0.1684       0.1973       0.161        0.14
    4.15594006]
 [-94.4454017    0.58840001   0.43309999   0.3831       0.27010003
    3.70113492]
 ...
 [-20.93705807   0.27519998   0.30329999   0.31650004   0.29960001
    2.88935089]
 [-66.74355639   0.2366       0.24450003   0.28220001   0.14659999
    3.7714684 ]
 [114.66323977   0.32689998   0.3664       0.3249       0.2782
    2.64140296]][0m
[37m[1m[2023-07-17 10:01:42,128][257371] Max Reward on eval: 263.28278770957843[0m
[37m[1m[2023-07-17 10:01:42,128][257371] Min Reward on eval: -216.97160152336582[0m
[37m[1m[2023-07-17 10:01:42,128][257371] Mean Reward across all agents: 24.50708793453442[0m
[37m[1m[2023-07-17 10:01:42,128][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:01:42,131][257371] mean_value=-1900.873805174008, max_value=347.1423776549989[0m
[37m[1m[2023-07-17 10:01:42,134][257371] New mean coefficients: [[-1.4939473   2.7146604  -0.73604524 -0.23359299 -0.9659177  -1.678021  ]][0m
[37m[1m[2023-07-17 10:01:42,135][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:01:51,201][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 10:01:51,201][257371] FPS: 423646.98[0m
[36m[2023-07-17 10:01:51,203][257371] itr=1032, itrs=2000, Progress: 51.60%[0m
[36m[2023-07-17 10:02:02,910][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 10:02:02,911][257371] FPS: 330984.11[0m
[36m[2023-07-17 10:02:07,187][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:02:07,188][257371] Reward + Measures: [[30.62383663  0.62308002  0.43181869  0.56198901  0.24831167  1.16996694]][0m
[37m[1m[2023-07-17 10:02:07,188][257371] Max Reward on eval: 30.62383663104135[0m
[37m[1m[2023-07-17 10:02:07,188][257371] Min Reward on eval: 30.62383663104135[0m
[37m[1m[2023-07-17 10:02:07,189][257371] Mean Reward across all agents: 30.62383663104135[0m
[37m[1m[2023-07-17 10:02:07,189][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:02:12,188][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:02:12,189][257371] Reward + Measures: [[-24.84796165   0.6268       0.71350002   0.60300004   0.6577
    4.72321272]
 [ 96.36737955   0.38780001   0.24240001   0.39319998   0.17569999
    4.18231964]
 [-24.70621739   0.20350002   0.14829999   0.16260001   0.16120002
    5.53066874]
 ...
 [-96.966288     0.38839999   0.22790001   0.36859998   0.21010001
    4.94458485]
 [-49.48826702   0.13759999   0.16680001   0.1133       0.14600001
    4.64456749]
 [  0.01307297   0.11440001   0.11360001   0.11090001   0.1048
    4.74779081]][0m
[37m[1m[2023-07-17 10:02:12,189][257371] Max Reward on eval: 463.96553131872787[0m
[37m[1m[2023-07-17 10:02:12,189][257371] Min Reward on eval: -247.90027389146854[0m
[37m[1m[2023-07-17 10:02:12,190][257371] Mean Reward across all agents: -3.6129069673752836[0m
[37m[1m[2023-07-17 10:02:12,190][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:02:12,191][257371] mean_value=-1322.4331256341588, max_value=366.97527614936325[0m
[37m[1m[2023-07-17 10:02:12,194][257371] New mean coefficients: [[-1.5156778   1.3902256   0.3802904  -0.10427178 -1.9463991  -1.2751875 ]][0m
[37m[1m[2023-07-17 10:02:12,195][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:02:21,209][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 10:02:21,209][257371] FPS: 426065.26[0m
[36m[2023-07-17 10:02:21,212][257371] itr=1033, itrs=2000, Progress: 51.65%[0m
[36m[2023-07-17 10:02:32,966][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 10:02:32,966][257371] FPS: 329757.63[0m
[36m[2023-07-17 10:02:37,274][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:02:37,275][257371] Reward + Measures: [[12.1000464   0.70822597  0.51360101  0.66239536  0.18440934  1.13330209]][0m
[37m[1m[2023-07-17 10:02:37,275][257371] Max Reward on eval: 12.100046397484354[0m
[37m[1m[2023-07-17 10:02:37,275][257371] Min Reward on eval: 12.100046397484354[0m
[37m[1m[2023-07-17 10:02:37,275][257371] Mean Reward across all agents: 12.100046397484354[0m
[37m[1m[2023-07-17 10:02:37,275][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:02:42,248][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:02:42,249][257371] Reward + Measures: [[-109.76395682    0.40180001    0.37850001    0.37930003    0.14649999
     4.82715225]
 [ -34.82069621    0.76550001    0.7288        0.74050009    0.69319999
     4.37768126]
 [ -43.07543462    0.13430001    0.19419999    0.16110002    0.15800001
     5.3378253 ]
 ...
 [ -20.14963086    0.19000001    0.21759999    0.20080002    0.15699999
     3.94924235]
 [ -18.17595761    0.1435        0.1567        0.14039999    0.1275
     4.61313963]
 [-174.37039734    0.69269997    0.4032        0.55610001    0.1443
     4.4707222 ]][0m
[37m[1m[2023-07-17 10:02:42,249][257371] Max Reward on eval: 104.19905836666003[0m
[37m[1m[2023-07-17 10:02:42,249][257371] Min Reward on eval: -419.2986588507891[0m
[37m[1m[2023-07-17 10:02:42,250][257371] Mean Reward across all agents: -30.47076277051971[0m
[37m[1m[2023-07-17 10:02:42,250][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:02:42,251][257371] mean_value=-1536.3786989329378, max_value=268.65159715293237[0m
[37m[1m[2023-07-17 10:02:42,254][257371] New mean coefficients: [[-0.9645062   0.87016046 -0.99564314 -0.65287673 -1.7117721  -1.6102035 ]][0m
[37m[1m[2023-07-17 10:02:42,255][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:02:51,256][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 10:02:51,256][257371] FPS: 426680.43[0m
[36m[2023-07-17 10:02:51,259][257371] itr=1034, itrs=2000, Progress: 51.70%[0m
[36m[2023-07-17 10:03:03,086][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 10:03:03,086][257371] FPS: 327692.28[0m
[36m[2023-07-17 10:03:07,354][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:03:07,360][257371] Reward + Measures: [[7.14597594 0.75038898 0.527776   0.70655698 0.15029965 1.07325149]][0m
[37m[1m[2023-07-17 10:03:07,360][257371] Max Reward on eval: 7.1459759366712925[0m
[37m[1m[2023-07-17 10:03:07,361][257371] Min Reward on eval: 7.1459759366712925[0m
[37m[1m[2023-07-17 10:03:07,361][257371] Mean Reward across all agents: 7.1459759366712925[0m
[37m[1m[2023-07-17 10:03:07,361][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:03:12,310][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:03:12,310][257371] Reward + Measures: [[-18.93313789   0.2895       0.24460001   0.303        0.15120001
    2.79604435]
 [-36.40100394   0.28860003   0.1804       0.24010001   0.24700001
    2.83654928]
 [  3.52884343   0.138        0.1532       0.18830001   0.15360001
    4.15901136]
 ...
 [ 36.45787385   0.2237       0.2626       0.1807       0.24879999
    3.01152682]
 [ 63.19097588   0.50559998   0.31800002   0.25910002   0.49169999
    2.15647578]
 [-14.74439161   0.19890001   0.25569999   0.1688       0.27770001
    4.82911634]][0m
[37m[1m[2023-07-17 10:03:12,311][257371] Max Reward on eval: 342.9282503332943[0m
[37m[1m[2023-07-17 10:03:12,311][257371] Min Reward on eval: -235.5608682755381[0m
[37m[1m[2023-07-17 10:03:12,311][257371] Mean Reward across all agents: 1.9842022951739096[0m
[37m[1m[2023-07-17 10:03:12,311][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:03:12,313][257371] mean_value=-2798.237882623848, max_value=393.2284470423916[0m
[37m[1m[2023-07-17 10:03:12,316][257371] New mean coefficients: [[-0.08178401  0.9413025   0.2725259   0.22608745 -0.5746126  -1.5499574 ]][0m
[37m[1m[2023-07-17 10:03:12,317][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:03:21,277][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 10:03:21,277][257371] FPS: 428653.76[0m
[36m[2023-07-17 10:03:21,279][257371] itr=1035, itrs=2000, Progress: 51.75%[0m
[36m[2023-07-17 10:03:33,266][257371] train() took 11.88 seconds to complete[0m
[36m[2023-07-17 10:03:33,266][257371] FPS: 323249.71[0m
[36m[2023-07-17 10:03:37,497][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:03:37,503][257371] Reward + Measures: [[-2.2590548   0.8051883   0.60863298  0.77306169  0.112971    0.97152507]][0m
[37m[1m[2023-07-17 10:03:37,503][257371] Max Reward on eval: -2.259054797380503[0m
[37m[1m[2023-07-17 10:03:37,503][257371] Min Reward on eval: -2.259054797380503[0m
[37m[1m[2023-07-17 10:03:37,504][257371] Mean Reward across all agents: -2.259054797380503[0m
[37m[1m[2023-07-17 10:03:37,504][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:03:42,507][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:03:42,517][257371] Reward + Measures: [[  14.03397123    0.28650001    0.3118        0.34370002    0.3529
     1.5935154 ]
 [-112.92952354    0.38079998    0.35870001    0.27410001    0.42120001
     2.44958472]
 [  -4.15893333    0.093         0.1078        0.083         0.11130001
     3.46983337]
 ...
 [  11.71652618    0.24689999    0.1338        0.26560003    0.17150001
     3.89016008]
 [  96.73222899    0.39100003    0.32860002    0.36919996    0.2481
     3.45207763]
 [  32.55440192    0.1109        0.1303        0.0891        0.11430001
     4.99629116]][0m
[37m[1m[2023-07-17 10:03:42,517][257371] Max Reward on eval: 346.43838702198116[0m
[37m[1m[2023-07-17 10:03:42,517][257371] Min Reward on eval: -146.46900380402803[0m
[37m[1m[2023-07-17 10:03:42,518][257371] Mean Reward across all agents: 38.23525485725353[0m
[37m[1m[2023-07-17 10:03:42,518][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:03:42,520][257371] mean_value=-2074.1376639531254, max_value=340.7658952900342[0m
[37m[1m[2023-07-17 10:03:42,522][257371] New mean coefficients: [[ 1.0035485   0.9392242  -0.18712354 -0.49588352 -0.7072385  -1.3405869 ]][0m
[37m[1m[2023-07-17 10:03:42,523][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:03:51,504][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 10:03:51,504][257371] FPS: 427663.82[0m
[36m[2023-07-17 10:03:51,507][257371] itr=1036, itrs=2000, Progress: 51.80%[0m
[36m[2023-07-17 10:04:03,289][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 10:04:03,289][257371] FPS: 328954.88[0m
[36m[2023-07-17 10:04:07,528][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:04:07,534][257371] Reward + Measures: [[-6.72314548  0.82768768  0.64167702  0.80053735  0.099254    0.86864436]][0m
[37m[1m[2023-07-17 10:04:07,535][257371] Max Reward on eval: -6.723145479783765[0m
[37m[1m[2023-07-17 10:04:07,535][257371] Min Reward on eval: -6.723145479783765[0m
[37m[1m[2023-07-17 10:04:07,536][257371] Mean Reward across all agents: -6.723145479783765[0m
[37m[1m[2023-07-17 10:04:07,537][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:04:12,758][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:04:12,763][257371] Reward + Measures: [[  4.89390665   0.42880002   0.29969999   0.25560001   0.4763
    2.95286155]
 [ 50.69873768   0.34209999   0.2462       0.29589999   0.31620002
    3.17828655]
 [ -9.90221935   0.2881       0.28850004   0.17200001   0.26560003
    5.46767569]
 ...
 [101.94531587   0.28099999   0.2696       0.2791       0.25350001
    3.15813136]
 [-75.84030559   0.0672       0.13410001   0.09         0.0648
    5.36043215]
 [-34.4276047    0.33769998   0.48720002   0.28739998   0.46830001
    3.76820993]][0m
[37m[1m[2023-07-17 10:04:12,764][257371] Max Reward on eval: 148.52314659655093[0m
[37m[1m[2023-07-17 10:04:12,764][257371] Min Reward on eval: -235.57336091455073[0m
[37m[1m[2023-07-17 10:04:12,764][257371] Mean Reward across all agents: 1.554183807073306[0m
[37m[1m[2023-07-17 10:04:12,765][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:04:12,766][257371] mean_value=-1937.6417865354445, max_value=114.72933531227665[0m
[37m[1m[2023-07-17 10:04:12,769][257371] New mean coefficients: [[ 0.97125405  0.49491647 -1.1460687  -1.2000403   0.2416724  -1.4860157 ]][0m
[37m[1m[2023-07-17 10:04:12,770][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:04:21,700][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 10:04:21,700][257371] FPS: 430090.16[0m
[36m[2023-07-17 10:04:21,703][257371] itr=1037, itrs=2000, Progress: 51.85%[0m
[36m[2023-07-17 10:04:33,437][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 10:04:33,437][257371] FPS: 330217.36[0m
[36m[2023-07-17 10:04:37,728][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:04:37,728][257371] Reward + Measures: [[3.22492281 0.81766826 0.53424233 0.77581394 0.11971501 0.8505944 ]][0m
[37m[1m[2023-07-17 10:04:37,728][257371] Max Reward on eval: 3.2249228090020963[0m
[37m[1m[2023-07-17 10:04:37,729][257371] Min Reward on eval: 3.2249228090020963[0m
[37m[1m[2023-07-17 10:04:37,729][257371] Mean Reward across all agents: 3.2249228090020963[0m
[37m[1m[2023-07-17 10:04:37,729][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:04:42,722][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:04:42,728][257371] Reward + Measures: [[-119.44578539    0.2404        0.32080001    0.22110002    0.35159999
     3.11096931]
 [ -12.82182443    0.25209999    0.29590002    0.21760002    0.25060001
     3.08768392]
 [  -4.94750867    0.15500002    0.37510002    0.2131        0.324
     4.73785114]
 ...
 [ 143.08829003    0.26030001    0.28330001    0.2465        0.2771
     3.23901153]
 [  28.62511594    0.31940001    0.35420001    0.37529999    0.25960001
     1.9026159 ]
 [ -19.40123201    0.13410001    0.26430002    0.2112        0.1777
     3.56669664]][0m
[37m[1m[2023-07-17 10:04:42,728][257371] Max Reward on eval: 389.4471501831431[0m
[37m[1m[2023-07-17 10:04:42,728][257371] Min Reward on eval: -148.25671484917402[0m
[37m[1m[2023-07-17 10:04:42,729][257371] Mean Reward across all agents: 15.598709837046554[0m
[37m[1m[2023-07-17 10:04:42,729][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:04:42,731][257371] mean_value=-2377.241426575746, max_value=-9.95141468784775[0m
[36m[2023-07-17 10:04:42,733][257371] XNES is restarting with a new solution whose measures are [0.10030001 0.10750001 0.0903     0.09080001 7.80157995] and objective is 268.5867822345346[0m
[36m[2023-07-17 10:04:42,734][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 10:04:42,737][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 10:04:42,737][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:04:51,822][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 10:04:51,822][257371] FPS: 422773.44[0m
[36m[2023-07-17 10:04:51,824][257371] itr=1038, itrs=2000, Progress: 51.90%[0m
[36m[2023-07-17 10:05:03,571][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 10:05:03,571][257371] FPS: 329903.28[0m
[36m[2023-07-17 10:05:07,845][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:05:07,845][257371] Reward + Measures: [[-46.44717277   0.12719867   0.13814633   0.12773401   0.16201833
    7.63689852]][0m
[37m[1m[2023-07-17 10:05:07,845][257371] Max Reward on eval: -46.447172774555725[0m
[37m[1m[2023-07-17 10:05:07,845][257371] Min Reward on eval: -46.447172774555725[0m
[37m[1m[2023-07-17 10:05:07,846][257371] Mean Reward across all agents: -46.447172774555725[0m
[37m[1m[2023-07-17 10:05:07,846][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:05:12,803][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:05:12,809][257371] Reward + Measures: [[-111.06532981    0.18569998    0.61620003    0.38860002    0.6595
     7.52436066]
 [  27.63325584    0.1815        0.17160001    0.17090002    0.0971
     7.6466713 ]
 [ -22.8973509     0.0794        0.0961        0.0933        0.0931
     7.01676798]
 ...
 [ -37.03310318    0.06          0.0675        0.1436        0.1283
     7.78038406]
 [  18.62058113    0.21689999    0.2428        0.10420001    0.2036
     7.49010038]
 [ -54.44385192    0.0594        0.0823        0.12460001    0.1278
     7.73268223]][0m
[37m[1m[2023-07-17 10:05:12,809][257371] Max Reward on eval: 267.74046182176096[0m
[37m[1m[2023-07-17 10:05:12,809][257371] Min Reward on eval: -379.11985728761647[0m
[37m[1m[2023-07-17 10:05:12,810][257371] Mean Reward across all agents: -30.438114346107238[0m
[37m[1m[2023-07-17 10:05:12,810][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:05:12,812][257371] mean_value=-258.46678654920106, max_value=205.47858173848763[0m
[37m[1m[2023-07-17 10:05:12,814][257371] New mean coefficients: [[ 0.5930145  -0.1893681  -0.92658466 -1.6587188  -0.55435777 -1.0026996 ]][0m
[37m[1m[2023-07-17 10:05:12,815][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:05:21,757][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 10:05:21,758][257371] FPS: 429501.43[0m
[36m[2023-07-17 10:05:21,760][257371] itr=1039, itrs=2000, Progress: 51.95%[0m
[36m[2023-07-17 10:05:33,552][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 10:05:33,552][257371] FPS: 328606.61[0m
[36m[2023-07-17 10:05:37,883][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:05:37,888][257371] Reward + Measures: [[9.05621703 0.09324833 0.10877234 0.12515067 0.13560334 7.42045259]][0m
[37m[1m[2023-07-17 10:05:37,888][257371] Max Reward on eval: 9.056217031740085[0m
[37m[1m[2023-07-17 10:05:37,889][257371] Min Reward on eval: 9.056217031740085[0m
[37m[1m[2023-07-17 10:05:37,889][257371] Mean Reward across all agents: 9.056217031740085[0m
[37m[1m[2023-07-17 10:05:37,889][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:05:42,865][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:05:42,866][257371] Reward + Measures: [[ -12.74871571    0.06330001    0.09750001    0.0741        0.091
     7.43871021]
 [  18.62392081    0.068         0.10040001    0.0958        0.10160001
     7.17354202]
 [  24.27035153    0.076         0.0966        0.0962        0.08279999
     7.39938831]
 ...
 [-204.15755611    0.24790001    0.28650001    0.0616        0.30210003
     7.5731349 ]
 [  43.78479866    0.15369999    0.14570001    0.16540001    0.0956
     7.71942139]
 [ -15.56759879    0.08020001    0.1321        0.1006        0.11370001
     7.37296009]][0m
[37m[1m[2023-07-17 10:05:42,866][257371] Max Reward on eval: 200.98533913732973[0m
[37m[1m[2023-07-17 10:05:42,866][257371] Min Reward on eval: -345.71620463896545[0m
[37m[1m[2023-07-17 10:05:42,866][257371] Mean Reward across all agents: -15.012348168676475[0m
[37m[1m[2023-07-17 10:05:42,867][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:05:42,868][257371] mean_value=-236.504158313678, max_value=43.62844564378389[0m
[37m[1m[2023-07-17 10:05:42,870][257371] New mean coefficients: [[ 1.0442611   0.10106277 -0.92756134 -2.375861   -0.39643323 -0.49550205]][0m
[37m[1m[2023-07-17 10:05:42,871][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:05:51,799][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 10:05:51,799][257371] FPS: 430210.27[0m
[36m[2023-07-17 10:05:51,801][257371] itr=1040, itrs=2000, Progress: 52.00%[0m
[37m[1m[2023-07-17 10:09:13,060][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001020[0m
[36m[2023-07-17 10:09:25,420][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 10:09:25,420][257371] FPS: 325937.96[0m
[36m[2023-07-17 10:09:29,636][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:09:29,637][257371] Reward + Measures: [[-51.10222815   0.13611533   0.13384399   0.12723966   0.15108834
    7.75237417]][0m
[37m[1m[2023-07-17 10:09:29,637][257371] Max Reward on eval: -51.10222814630587[0m
[37m[1m[2023-07-17 10:09:29,637][257371] Min Reward on eval: -51.10222814630587[0m
[37m[1m[2023-07-17 10:09:29,637][257371] Mean Reward across all agents: -51.10222814630587[0m
[37m[1m[2023-07-17 10:09:29,638][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:09:34,767][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:09:34,767][257371] Reward + Measures: [[ 25.61778938   0.0776       0.11440001   0.0776       0.0974
    7.08228397]
 [-61.77938382   0.16110002   0.15040001   0.15799999   0.1753
    7.50265646]
 [ 41.76657198   0.1734       0.1699       0.1202       0.21750002
    7.87882328]
 ...
 [-28.86837457   0.3766       0.3371       0.34689999   0.1892
    7.69992399]
 [-86.73088114   0.0977       0.0974       0.1084       0.10930001
    7.39397764]
 [ 28.45079304   0.0873       0.0964       0.0671       0.07210001
    7.69613028]][0m
[37m[1m[2023-07-17 10:09:34,768][257371] Max Reward on eval: 222.88683223314584[0m
[37m[1m[2023-07-17 10:09:34,768][257371] Min Reward on eval: -249.24267628118395[0m
[37m[1m[2023-07-17 10:09:34,768][257371] Mean Reward across all agents: -8.047708533989322[0m
[37m[1m[2023-07-17 10:09:34,768][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:09:34,770][257371] mean_value=-231.61828887786834, max_value=-8.033500480981203[0m
[36m[2023-07-17 10:09:34,772][257371] XNES is restarting with a new solution whose measures are [0.83359998 0.1504     0.8391     0.92439997 5.32609892] and objective is 202.40371730923653[0m
[36m[2023-07-17 10:09:34,773][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 10:09:34,775][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 10:09:34,776][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:09:43,771][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 10:09:43,771][257371] FPS: 426989.83[0m
[36m[2023-07-17 10:09:43,774][257371] itr=1041, itrs=2000, Progress: 52.05%[0m
[36m[2023-07-17 10:09:55,588][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 10:09:55,588][257371] FPS: 328071.39[0m
[36m[2023-07-17 10:09:59,843][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:09:59,844][257371] Reward + Measures: [[-18.86225999   0.488087     0.44579533   0.43482602   0.65275633
    3.18354106]][0m
[37m[1m[2023-07-17 10:09:59,844][257371] Max Reward on eval: -18.86225998545277[0m
[37m[1m[2023-07-17 10:09:59,844][257371] Min Reward on eval: -18.86225998545277[0m
[37m[1m[2023-07-17 10:09:59,844][257371] Mean Reward across all agents: -18.86225998545277[0m
[37m[1m[2023-07-17 10:09:59,845][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:10:04,819][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:10:04,820][257371] Reward + Measures: [[  37.79907513    0.35880002    0.3883        0.31319997    0.39450002
     2.77420115]
 [  52.9645602     0.48660001    0.4474        0.41570002    0.30250001
     4.65251303]
 [ -35.59383634    0.2825        0.35929999    0.20910001    0.44829997
     3.4649179 ]
 ...
 [-178.27109619    0.29390001    0.41700003    0.36740002    0.43170005
     3.60446429]
 [-209.99835214    0.2771        0.46790001    0.2789        0.5679
     3.01531911]
 [ -67.61892717    0.2712        0.40939999    0.31720003    0.4269
     3.88297009]][0m
[37m[1m[2023-07-17 10:10:04,820][257371] Max Reward on eval: 173.88409258052707[0m
[37m[1m[2023-07-17 10:10:04,820][257371] Min Reward on eval: -312.08258095355706[0m
[37m[1m[2023-07-17 10:10:04,821][257371] Mean Reward across all agents: -24.693529036537075[0m
[37m[1m[2023-07-17 10:10:04,821][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:10:04,823][257371] mean_value=-868.8370887206493, max_value=10.349095189585512[0m
[37m[1m[2023-07-17 10:10:04,825][257371] New mean coefficients: [[-0.64887905 -1.1888824  -2.402543   -0.72254264 -1.3488541  -1.0690353 ]][0m
[37m[1m[2023-07-17 10:10:04,826][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:10:13,840][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 10:10:13,840][257371] FPS: 426088.24[0m
[36m[2023-07-17 10:10:13,842][257371] itr=1042, itrs=2000, Progress: 52.10%[0m
[36m[2023-07-17 10:10:25,646][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 10:10:25,646][257371] FPS: 328336.16[0m
[36m[2023-07-17 10:10:29,929][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:10:29,930][257371] Reward + Measures: [[71.44255582  0.32623768  0.70600164  0.167908    0.69565642  3.33989024]][0m
[37m[1m[2023-07-17 10:10:29,930][257371] Max Reward on eval: 71.44255582072866[0m
[37m[1m[2023-07-17 10:10:29,931][257371] Min Reward on eval: 71.44255582072866[0m
[37m[1m[2023-07-17 10:10:29,931][257371] Mean Reward across all agents: 71.44255582072866[0m
[37m[1m[2023-07-17 10:10:29,931][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:10:34,914][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:10:34,915][257371] Reward + Measures: [[174.80656199   0.2572       0.24030001   0.17560002   0.29969999
    4.15990257]
 [-94.10081889   0.22740002   0.21500002   0.18069999   0.25050002
    4.66264391]
 [ 18.01774841   0.24310003   0.24970002   0.1824       0.2172
    4.91802311]
 ...
 [ 53.80171051   0.3378       0.40440002   0.1948       0.34130001
    3.98478937]
 [ 54.71257341   0.35580003   0.32249999   0.3276       0.25910002
    5.1133914 ]
 [-18.40423419   0.27919999   0.61309999   0.2211       0.64440006
    4.40404463]][0m
[37m[1m[2023-07-17 10:10:34,915][257371] Max Reward on eval: 255.69608401060106[0m
[37m[1m[2023-07-17 10:10:34,915][257371] Min Reward on eval: -163.90923038725742[0m
[37m[1m[2023-07-17 10:10:34,916][257371] Mean Reward across all agents: 9.701542416077416[0m
[37m[1m[2023-07-17 10:10:34,916][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:10:34,918][257371] mean_value=-1267.4499575893074, max_value=39.83326128745063[0m
[37m[1m[2023-07-17 10:10:34,920][257371] New mean coefficients: [[-0.1603117   0.5904566  -2.736607   -0.08668053 -0.63166404 -1.2615068 ]][0m
[37m[1m[2023-07-17 10:10:34,921][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:10:43,938][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 10:10:43,938][257371] FPS: 425950.97[0m
[36m[2023-07-17 10:10:43,940][257371] itr=1043, itrs=2000, Progress: 52.15%[0m
[36m[2023-07-17 10:10:55,650][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 10:10:55,651][257371] FPS: 330888.36[0m
[36m[2023-07-17 10:10:59,933][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:10:59,934][257371] Reward + Measures: [[73.97927691  0.32130331  0.50568336  0.24145535  0.4834283   3.16252899]][0m
[37m[1m[2023-07-17 10:10:59,934][257371] Max Reward on eval: 73.9792769130506[0m
[37m[1m[2023-07-17 10:10:59,934][257371] Min Reward on eval: 73.9792769130506[0m
[37m[1m[2023-07-17 10:10:59,934][257371] Mean Reward across all agents: 73.9792769130506[0m
[37m[1m[2023-07-17 10:10:59,935][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:11:04,880][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:11:04,880][257371] Reward + Measures: [[ 12.910003     0.29860002   0.44120002   0.33710003   0.47480002
    4.98776007]
 [-86.19167696   0.25620005   0.2115       0.18650001   0.20020001
    4.8921032 ]
 [ 88.40460788   0.27250001   0.31459999   0.3026       0.32539999
    4.15345526]
 ...
 [ 55.34291869   0.25409999   0.33329999   0.23769999   0.29480001
    4.76544333]
 [ 62.07559467   0.38130003   0.41440001   0.44000003   0.43980002
    6.06246519]
 [ 26.60592529   0.2332       0.44160005   0.3091       0.48800001
    4.84390259]][0m
[37m[1m[2023-07-17 10:11:04,881][257371] Max Reward on eval: 411.97264767289164[0m
[37m[1m[2023-07-17 10:11:04,881][257371] Min Reward on eval: -399.33720110757275[0m
[37m[1m[2023-07-17 10:11:04,881][257371] Mean Reward across all agents: 32.18687073935973[0m
[37m[1m[2023-07-17 10:11:04,881][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:11:04,883][257371] mean_value=-887.6866565526503, max_value=332.76563168521045[0m
[37m[1m[2023-07-17 10:11:04,885][257371] New mean coefficients: [[ 0.4718961   1.8666816  -2.1354027  -0.10905609  0.31586194 -1.3017702 ]][0m
[37m[1m[2023-07-17 10:11:04,886][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:11:13,859][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 10:11:13,859][257371] FPS: 428041.02[0m
[36m[2023-07-17 10:11:13,861][257371] itr=1044, itrs=2000, Progress: 52.20%[0m
[36m[2023-07-17 10:11:25,733][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 10:11:25,734][257371] FPS: 326449.75[0m
[36m[2023-07-17 10:11:30,064][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:11:30,064][257371] Reward + Measures: [[22.37728769  0.34351498  0.38090301  0.295894    0.37163037  3.43755198]][0m
[37m[1m[2023-07-17 10:11:30,065][257371] Max Reward on eval: 22.377287690625334[0m
[37m[1m[2023-07-17 10:11:30,065][257371] Min Reward on eval: 22.377287690625334[0m
[37m[1m[2023-07-17 10:11:30,065][257371] Mean Reward across all agents: 22.377287690625334[0m
[37m[1m[2023-07-17 10:11:30,066][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:11:35,008][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:11:35,008][257371] Reward + Measures: [[ 11.94252554   0.26110002   0.38300002   0.25049999   0.32529998
    4.72373819]
 [-25.32172148   0.3319       0.31580001   0.3457       0.46630001
    4.72292709]
 [ 15.09626247   0.22830002   0.2375       0.16800001   0.27370003
    4.44454288]
 ...
 [-10.83051384   0.266        0.25650001   0.26089999   0.28560001
    5.16381788]
 [ 72.09189001   0.75940001   0.1793       0.76230001   0.4434
    6.19044447]
 [ 13.98986418   0.22880001   0.22470002   0.1982       0.24309997
    4.81673384]][0m
[37m[1m[2023-07-17 10:11:35,009][257371] Max Reward on eval: 338.06627654442565[0m
[37m[1m[2023-07-17 10:11:35,009][257371] Min Reward on eval: -456.3854814428836[0m
[37m[1m[2023-07-17 10:11:35,009][257371] Mean Reward across all agents: 40.62941911109411[0m
[37m[1m[2023-07-17 10:11:35,009][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:11:35,012][257371] mean_value=-604.1310891969787, max_value=318.9110046311583[0m
[37m[1m[2023-07-17 10:11:35,015][257371] New mean coefficients: [[ 1.3786839   1.39592    -3.5008183   1.9659089  -0.54598755 -1.9017122 ]][0m
[37m[1m[2023-07-17 10:11:35,016][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:11:43,976][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 10:11:43,976][257371] FPS: 428647.50[0m
[36m[2023-07-17 10:11:43,978][257371] itr=1045, itrs=2000, Progress: 52.25%[0m
[36m[2023-07-17 10:11:56,084][257371] train() took 12.00 seconds to complete[0m
[36m[2023-07-17 10:11:56,084][257371] FPS: 320011.36[0m
[36m[2023-07-17 10:12:00,373][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:12:00,374][257371] Reward + Measures: [[53.39795626  0.29996634  0.34287769  0.34514934  0.31294766  3.0851953 ]][0m
[37m[1m[2023-07-17 10:12:00,374][257371] Max Reward on eval: 53.397956259844165[0m
[37m[1m[2023-07-17 10:12:00,374][257371] Min Reward on eval: 53.397956259844165[0m
[37m[1m[2023-07-17 10:12:00,375][257371] Mean Reward across all agents: 53.397956259844165[0m
[37m[1m[2023-07-17 10:12:00,375][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:12:05,375][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:12:05,376][257371] Reward + Measures: [[ 53.6123623    0.1577       0.15900001   0.13690001   0.13259999
    5.05612135]
 [-36.64070657   0.2221       0.24800001   0.2076       0.22860001
    5.34317636]
 [  8.58351553   0.147        0.1533       0.1276       0.15390001
    5.5874753 ]
 ...
 [ 47.21445641   0.13530001   0.1176       0.0997       0.1099
    4.06905556]
 [ 39.74781839   0.15120001   0.1754       0.14919999   0.15700002
    5.20101881]
 [ 58.21750761   0.26139998   0.28009999   0.2467       0.27079999
    5.13846064]][0m
[37m[1m[2023-07-17 10:12:05,376][257371] Max Reward on eval: 157.8740976423025[0m
[37m[1m[2023-07-17 10:12:05,376][257371] Min Reward on eval: -578.8291332265595[0m
[37m[1m[2023-07-17 10:12:05,377][257371] Mean Reward across all agents: 3.2433569193192153[0m
[37m[1m[2023-07-17 10:12:05,377][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:12:05,378][257371] mean_value=-1444.8531719899663, max_value=-63.071090039617815[0m
[36m[2023-07-17 10:12:05,380][257371] XNES is restarting with a new solution whose measures are [0.42449996 0.64829999 0.38530001 0.70450002 5.47497272] and objective is 204.87586686462163[0m
[36m[2023-07-17 10:12:05,381][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 10:12:05,384][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 10:12:05,384][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:12:14,466][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 10:12:14,466][257371] FPS: 422910.85[0m
[36m[2023-07-17 10:12:14,468][257371] itr=1046, itrs=2000, Progress: 52.30%[0m
[36m[2023-07-17 10:12:26,211][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 10:12:26,211][257371] FPS: 329994.99[0m
[36m[2023-07-17 10:12:30,543][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:12:30,543][257371] Reward + Measures: [[203.29528564   0.63554633   0.73284465   0.34854898   0.71968299
    5.42282104]][0m
[37m[1m[2023-07-17 10:12:30,543][257371] Max Reward on eval: 203.29528564471127[0m
[37m[1m[2023-07-17 10:12:30,544][257371] Min Reward on eval: 203.29528564471127[0m
[37m[1m[2023-07-17 10:12:30,544][257371] Mean Reward across all agents: 203.29528564471127[0m
[37m[1m[2023-07-17 10:12:30,544][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:12:35,832][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:12:35,832][257371] Reward + Measures: [[-227.43980741    0.22550002    0.52880001    0.62279999    0.65109998
     5.72623158]
 [ 122.68262036    0.3312        0.47890002    0.42989999    0.22379999
     6.73420572]
 [  67.55530648    0.19939999    0.1945        0.19829999    0.1742
     6.04213142]
 ...
 [ 102.57689351    0.3829        0.3608        0.34909999    0.3448
     4.34673643]
 [  53.99475459    0.19849999    0.2296        0.19310001    0.2177
     3.90781331]
 [  13.30149043    0.18029998    0.1752        0.171         0.091
     5.68052912]][0m
[37m[1m[2023-07-17 10:12:35,832][257371] Max Reward on eval: 396.7087108810432[0m
[37m[1m[2023-07-17 10:12:35,833][257371] Min Reward on eval: -379.4836015999317[0m
[37m[1m[2023-07-17 10:12:35,833][257371] Mean Reward across all agents: 17.47296263548425[0m
[37m[1m[2023-07-17 10:12:35,833][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:12:35,835][257371] mean_value=-515.8294519719268, max_value=196.6986760937705[0m
[37m[1m[2023-07-17 10:12:35,838][257371] New mean coefficients: [[ 2.3355424  0.7743299 -1.0435976 -0.9055754 -1.0890323 -0.713001 ]][0m
[37m[1m[2023-07-17 10:12:35,839][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:12:44,925][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 10:12:44,926][257371] FPS: 422664.52[0m
[36m[2023-07-17 10:12:44,928][257371] itr=1047, itrs=2000, Progress: 52.35%[0m
[36m[2023-07-17 10:12:56,774][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 10:12:56,774][257371] FPS: 327192.23[0m
[36m[2023-07-17 10:13:01,134][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:13:01,134][257371] Reward + Measures: [[66.45723806  0.30962235  0.26787966  0.25328434  0.27207533  4.04432487]][0m
[37m[1m[2023-07-17 10:13:01,135][257371] Max Reward on eval: 66.45723806171438[0m
[37m[1m[2023-07-17 10:13:01,135][257371] Min Reward on eval: 66.45723806171438[0m
[37m[1m[2023-07-17 10:13:01,135][257371] Mean Reward across all agents: 66.45723806171438[0m
[37m[1m[2023-07-17 10:13:01,135][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:13:06,199][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:13:06,199][257371] Reward + Measures: [[-16.17147745   0.17390001   0.1096       0.1469       0.12409999
    5.41643095]
 [102.83896052   0.2658       0.20249999   0.24560001   0.1296
    6.23223877]
 [ 58.58027302   0.54110003   0.14250001   0.49530002   0.3089
    7.20763731]
 ...
 [ 32.52820365   0.1873       0.11059999   0.12740001   0.1295
    5.71491766]
 [-27.02654114   0.10999999   0.10150001   0.0952       0.0904
    6.0625968 ]
 [-79.68535685   0.49650002   0.43560001   0.47450003   0.45890003
    6.25895977]][0m
[37m[1m[2023-07-17 10:13:06,199][257371] Max Reward on eval: 184.26030908450485[0m
[37m[1m[2023-07-17 10:13:06,200][257371] Min Reward on eval: -697.6280739486217[0m
[37m[1m[2023-07-17 10:13:06,200][257371] Mean Reward across all agents: -12.912006267238251[0m
[37m[1m[2023-07-17 10:13:06,200][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:13:06,202][257371] mean_value=-475.76232440706013, max_value=41.74298492205548[0m
[37m[1m[2023-07-17 10:13:06,204][257371] New mean coefficients: [[ 3.1945462   0.93439835 -1.2205287  -0.7557886   0.204126   -0.96100426]][0m
[37m[1m[2023-07-17 10:13:06,205][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:13:15,373][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-17 10:13:15,373][257371] FPS: 418941.91[0m
[36m[2023-07-17 10:13:15,375][257371] itr=1048, itrs=2000, Progress: 52.40%[0m
[36m[2023-07-17 10:13:27,192][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 10:13:27,192][257371] FPS: 327957.46[0m
[36m[2023-07-17 10:13:31,555][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:13:31,555][257371] Reward + Measures: [[-160.03548687    0.40985066    0.13292401    0.449927      0.4066827
     4.68766212]][0m
[37m[1m[2023-07-17 10:13:31,556][257371] Max Reward on eval: -160.0354868670404[0m
[37m[1m[2023-07-17 10:13:31,556][257371] Min Reward on eval: -160.0354868670404[0m
[37m[1m[2023-07-17 10:13:31,556][257371] Mean Reward across all agents: -160.0354868670404[0m
[37m[1m[2023-07-17 10:13:31,557][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:13:36,528][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:13:36,529][257371] Reward + Measures: [[ -22.52027865    0.2           0.147         0.16250001    0.11130001
     5.88496256]
 [   8.34298354    0.34649998    0.24139999    0.31219998    0.3012
     6.10610008]
 [  33.91968102    0.26659998    0.2299        0.2418        0.2313
     6.29630899]
 ...
 [ -37.21245185    0.0992        0.0881        0.07750001    0.0811
     5.13250065]
 [-252.96488979    0.5097        0.43979999    0.67870003    0.55000001
     7.08027267]
 [  45.2451785     0.551         0.54570001    0.59160006    0.5359
     6.27586412]][0m
[37m[1m[2023-07-17 10:13:36,529][257371] Max Reward on eval: 357.4138927639753[0m
[37m[1m[2023-07-17 10:13:36,529][257371] Min Reward on eval: -558.9822568885982[0m
[37m[1m[2023-07-17 10:13:36,530][257371] Mean Reward across all agents: -39.89007048350362[0m
[37m[1m[2023-07-17 10:13:36,530][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:13:36,532][257371] mean_value=-479.61286785339695, max_value=151.19816292807093[0m
[37m[1m[2023-07-17 10:13:36,534][257371] New mean coefficients: [[ 3.6559858   0.28102177  0.16115499 -0.667994    2.2495923  -0.46387774]][0m
[37m[1m[2023-07-17 10:13:36,535][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:13:45,543][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 10:13:45,543][257371] FPS: 426399.99[0m
[36m[2023-07-17 10:13:45,545][257371] itr=1049, itrs=2000, Progress: 52.45%[0m
[36m[2023-07-17 10:13:57,218][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 10:13:57,218][257371] FPS: 332061.16[0m
[36m[2023-07-17 10:14:01,459][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:14:01,460][257371] Reward + Measures: [[-34.77787463   0.65844303   0.16750734   0.657462     0.50013632
    4.7236371 ]][0m
[37m[1m[2023-07-17 10:14:01,460][257371] Max Reward on eval: -34.77787462898788[0m
[37m[1m[2023-07-17 10:14:01,460][257371] Min Reward on eval: -34.77787462898788[0m
[37m[1m[2023-07-17 10:14:01,460][257371] Mean Reward across all agents: -34.77787462898788[0m
[37m[1m[2023-07-17 10:14:01,461][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:14:06,533][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:14:06,533][257371] Reward + Measures: [[   4.67687253    0.21009998    0.0707        0.27060002    0.2141
     5.79355097]
 [-117.65353709    0.46709999    0.48100001    0.47170001    0.47759995
     7.16448975]
 [ 101.1519475     0.19470002    0.19510001    0.19170001    0.0856
     6.72328138]
 ...
 [   6.8340111     0.73690003    0.72730005    0.67750001    0.70199996
     6.7432723 ]
 [ -54.59339916    0.16189998    0.13810001    0.12730001    0.1452
     5.42398453]
 [ -65.13032169    0.11539999    0.11240001    0.1067        0.1069
     5.50807238]][0m
[37m[1m[2023-07-17 10:14:06,534][257371] Max Reward on eval: 227.42448496744038[0m
[37m[1m[2023-07-17 10:14:06,534][257371] Min Reward on eval: -602.2158355275169[0m
[37m[1m[2023-07-17 10:14:06,534][257371] Mean Reward across all agents: -21.628519235187415[0m
[37m[1m[2023-07-17 10:14:06,534][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:14:06,536][257371] mean_value=-362.1553089008899, max_value=17.442185764624924[0m
[37m[1m[2023-07-17 10:14:06,538][257371] New mean coefficients: [[ 2.5739708  2.359537   1.1415167 -1.0429418  2.8237636 -0.2596654]][0m
[37m[1m[2023-07-17 10:14:06,539][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:14:15,582][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 10:14:15,582][257371] FPS: 424725.92[0m
[36m[2023-07-17 10:14:15,585][257371] itr=1050, itrs=2000, Progress: 52.50%[0m
[37m[1m[2023-07-17 10:17:36,161][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001030[0m
[36m[2023-07-17 10:17:48,493][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 10:17:48,493][257371] FPS: 325442.65[0m
[36m[2023-07-17 10:17:52,712][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:17:52,712][257371] Reward + Measures: [[46.37620614  0.24997501  0.18852568  0.23625101  0.20698132  4.07110167]][0m
[37m[1m[2023-07-17 10:17:52,712][257371] Max Reward on eval: 46.37620614399827[0m
[37m[1m[2023-07-17 10:17:52,713][257371] Min Reward on eval: 46.37620614399827[0m
[37m[1m[2023-07-17 10:17:52,713][257371] Mean Reward across all agents: 46.37620614399827[0m
[37m[1m[2023-07-17 10:17:52,713][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:17:57,915][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:17:57,916][257371] Reward + Measures: [[-102.85062153    0.1104        0.13500001    0.1468        0.17120002
     5.45845366]
 [  24.15586361    0.08180001    0.0647        0.08350001    0.0856
     6.20334816]
 [-136.4935324     0.22090001    0.16960001    0.19860001    0.23940001
     5.95231104]
 ...
 [ -35.67148197    0.12440001    0.10520001    0.11180001    0.1124
     4.47872877]
 [  16.94701933    0.38330004    0.20280002    0.23770002    0.33039996
     6.14763784]
 [ -38.9208523     0.1437        0.1097        0.12409999    0.1099
     5.15770292]][0m
[37m[1m[2023-07-17 10:17:57,916][257371] Max Reward on eval: 244.85702374107206[0m
[37m[1m[2023-07-17 10:17:57,916][257371] Min Reward on eval: -447.9917449832894[0m
[37m[1m[2023-07-17 10:17:57,916][257371] Mean Reward across all agents: -22.711429357524743[0m
[37m[1m[2023-07-17 10:17:57,916][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:17:57,918][257371] mean_value=-477.2906925101594, max_value=236.43472643341275[0m
[37m[1m[2023-07-17 10:17:57,921][257371] New mean coefficients: [[ 4.3774157   2.4149623   1.3400457   0.25565732  1.4146739  -0.23317873]][0m
[37m[1m[2023-07-17 10:17:57,922][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:18:06,951][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 10:18:06,951][257371] FPS: 425375.49[0m
[36m[2023-07-17 10:18:06,953][257371] itr=1051, itrs=2000, Progress: 52.55%[0m
[36m[2023-07-17 10:18:18,689][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 10:18:18,689][257371] FPS: 330205.39[0m
[36m[2023-07-17 10:18:22,976][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:18:22,977][257371] Reward + Measures: [[49.20176264  0.25028569  0.19162968  0.23741467  0.214811    4.06511974]][0m
[37m[1m[2023-07-17 10:18:22,977][257371] Max Reward on eval: 49.2017626381401[0m
[37m[1m[2023-07-17 10:18:22,977][257371] Min Reward on eval: 49.2017626381401[0m
[37m[1m[2023-07-17 10:18:22,977][257371] Mean Reward across all agents: 49.2017626381401[0m
[37m[1m[2023-07-17 10:18:22,978][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:18:27,962][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:18:27,963][257371] Reward + Measures: [[-89.66330596   0.2791       0.07120001   0.25349998   0.25799999
    5.96093464]
 [ 40.36499283   0.183        0.14440002   0.1445       0.149
    4.390625  ]
 [ 30.8394813    0.28389999   0.1671       0.2036       0.27309999
    5.66826057]
 ...
 [289.7727547    0.40159997   0.50430006   0.1961       0.4808
    6.37549591]
 [ 29.70657934   0.15350001   0.09500001   0.1267       0.1498
    5.71600437]
 [  9.15758549   0.12730001   0.0747       0.09940001   0.139
    6.01769066]][0m
[37m[1m[2023-07-17 10:18:27,963][257371] Max Reward on eval: 506.1329822484404[0m
[37m[1m[2023-07-17 10:18:27,963][257371] Min Reward on eval: -154.69395395042375[0m
[37m[1m[2023-07-17 10:18:27,964][257371] Mean Reward across all agents: 33.52036724331752[0m
[37m[1m[2023-07-17 10:18:27,964][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:18:27,966][257371] mean_value=-438.8174159379027, max_value=96.63208754447274[0m
[37m[1m[2023-07-17 10:18:27,968][257371] New mean coefficients: [[ 3.7443166   4.454261   -0.0718081   0.55038524  1.3677233  -0.19562435]][0m
[37m[1m[2023-07-17 10:18:27,969][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:18:36,979][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 10:18:36,980][257371] FPS: 426245.23[0m
[36m[2023-07-17 10:18:36,982][257371] itr=1052, itrs=2000, Progress: 52.60%[0m
[36m[2023-07-17 10:18:48,744][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 10:18:48,744][257371] FPS: 329476.16[0m
[36m[2023-07-17 10:18:52,970][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:18:52,971][257371] Reward + Measures: [[49.48101044  0.249689    0.19081567  0.23850468  0.21204199  4.0602107 ]][0m
[37m[1m[2023-07-17 10:18:52,971][257371] Max Reward on eval: 49.48101044004826[0m
[37m[1m[2023-07-17 10:18:52,971][257371] Min Reward on eval: 49.48101044004826[0m
[37m[1m[2023-07-17 10:18:52,971][257371] Mean Reward across all agents: 49.48101044004826[0m
[37m[1m[2023-07-17 10:18:52,972][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:18:57,909][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:18:57,910][257371] Reward + Measures: [[-11.21917225   0.12249999   0.131        0.11529999   0.15830001
    5.61005497]
 [ -3.03107033   0.1559       0.16919999   0.1772       0.1767
    5.00559759]
 [ 17.3437469    0.0725       0.08810001   0.08410001   0.0817
    4.91107798]
 ...
 [-47.58814792   0.0838       0.08000001   0.08490001   0.0895
    5.53718662]
 [-60.29821014   0.18570001   0.2014       0.1707       0.17970002
    5.14083576]
 [ -1.5202024    0.14350002   0.08639999   0.1365       0.1089
    5.05566978]][0m
[37m[1m[2023-07-17 10:18:57,910][257371] Max Reward on eval: 520.8552703887224[0m
[37m[1m[2023-07-17 10:18:57,910][257371] Min Reward on eval: -393.07229340067136[0m
[37m[1m[2023-07-17 10:18:57,911][257371] Mean Reward across all agents: 1.3870353843016796[0m
[37m[1m[2023-07-17 10:18:57,911][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:18:57,913][257371] mean_value=-399.90149325325734, max_value=207.9114904286384[0m
[37m[1m[2023-07-17 10:18:57,915][257371] New mean coefficients: [[ 3.0026078   5.1036234  -0.3036819   0.9339709   2.3357747  -0.24819085]][0m
[37m[1m[2023-07-17 10:18:57,916][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:19:06,884][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 10:19:06,885][257371] FPS: 428234.68[0m
[36m[2023-07-17 10:19:06,887][257371] itr=1053, itrs=2000, Progress: 52.65%[0m
[36m[2023-07-17 10:19:18,539][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 10:19:18,539][257371] FPS: 332562.93[0m
[36m[2023-07-17 10:19:22,781][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:19:22,782][257371] Reward + Measures: [[50.00381453  0.25816199  0.19388101  0.250375    0.21525434  4.04171228]][0m
[37m[1m[2023-07-17 10:19:22,782][257371] Max Reward on eval: 50.003814529634354[0m
[37m[1m[2023-07-17 10:19:22,782][257371] Min Reward on eval: 50.003814529634354[0m
[37m[1m[2023-07-17 10:19:22,782][257371] Mean Reward across all agents: 50.003814529634354[0m
[37m[1m[2023-07-17 10:19:22,783][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:19:27,768][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:19:27,769][257371] Reward + Measures: [[-120.23471593    0.31670001    0.29089999    0.20739999    0.26190001
     4.67707062]
 [ -26.42048683    0.14070001    0.1001        0.1001        0.0869
     5.69906902]
 [-101.74855037    0.13880001    0.1265        0.1204        0.13239999
     5.6354208 ]
 ...
 [   9.52677423    0.24169998    0.2052        0.23810001    0.20750001
     6.05804586]
 [ -40.19659123    0.13090001    0.12719999    0.15180001    0.13810001
     5.94036055]
 [  14.27548595    0.1899        0.1788        0.08110001    0.17550001
     6.28414011]][0m
[37m[1m[2023-07-17 10:19:27,769][257371] Max Reward on eval: 368.2444211919792[0m
[37m[1m[2023-07-17 10:19:27,769][257371] Min Reward on eval: -395.9317591505125[0m
[37m[1m[2023-07-17 10:19:27,769][257371] Mean Reward across all agents: -21.41552883951401[0m
[37m[1m[2023-07-17 10:19:27,770][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:19:27,772][257371] mean_value=-442.1100983882647, max_value=208.61372352037517[0m
[37m[1m[2023-07-17 10:19:27,774][257371] New mean coefficients: [[ 3.2398832   6.554183   -1.0473363   0.72400564  0.9319093  -0.22616984]][0m
[37m[1m[2023-07-17 10:19:27,775][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:19:36,757][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 10:19:36,757][257371] FPS: 427602.50[0m
[36m[2023-07-17 10:19:36,759][257371] itr=1054, itrs=2000, Progress: 52.70%[0m
[36m[2023-07-17 10:19:48,691][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 10:19:48,692][257371] FPS: 324746.22[0m
[36m[2023-07-17 10:19:52,955][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:19:52,955][257371] Reward + Measures: [[53.42631975  0.26675469  0.199737    0.25830534  0.22056632  4.02429771]][0m
[37m[1m[2023-07-17 10:19:52,956][257371] Max Reward on eval: 53.426319747955404[0m
[37m[1m[2023-07-17 10:19:52,956][257371] Min Reward on eval: 53.426319747955404[0m
[37m[1m[2023-07-17 10:19:52,956][257371] Mean Reward across all agents: 53.426319747955404[0m
[37m[1m[2023-07-17 10:19:52,956][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:19:57,931][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:19:57,931][257371] Reward + Measures: [[  65.48661253    0.40450001    0.41050002    0.10030001    0.4271
     5.40529776]
 [-228.0257478     0.77039999    0.7177        0.76410002    0.10950001
     5.66223955]
 [  23.2910064     0.1461        0.13610001    0.14850001    0.19600001
     5.49526834]
 ...
 [ -12.64843224    0.08269999    0.0913        0.0811        0.098
     5.26588535]
 [  20.88854693    0.13920002    0.33969998    0.30640003    0.35550001
     4.8854866 ]
 [ -87.136292      0.1204        0.1169        0.1376        0.1525
     6.16500616]][0m
[37m[1m[2023-07-17 10:19:57,931][257371] Max Reward on eval: 283.39130782103166[0m
[37m[1m[2023-07-17 10:19:57,932][257371] Min Reward on eval: -327.1286782853305[0m
[37m[1m[2023-07-17 10:19:57,932][257371] Mean Reward across all agents: 9.033416571694575[0m
[37m[1m[2023-07-17 10:19:57,932][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:19:57,934][257371] mean_value=-431.1665161035793, max_value=89.49825530951077[0m
[37m[1m[2023-07-17 10:19:57,937][257371] New mean coefficients: [[ 2.7327285   6.3797703  -1.0324705   0.31825423  0.89210176 -0.24349158]][0m
[37m[1m[2023-07-17 10:19:57,938][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:20:06,865][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 10:20:06,865][257371] FPS: 430221.04[0m
[36m[2023-07-17 10:20:06,867][257371] itr=1055, itrs=2000, Progress: 52.75%[0m
[36m[2023-07-17 10:20:18,646][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 10:20:18,646][257371] FPS: 329086.50[0m
[36m[2023-07-17 10:20:22,934][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:20:22,935][257371] Reward + Measures: [[54.01995596  0.28100765  0.20607133  0.27080834  0.22412699  4.01176405]][0m
[37m[1m[2023-07-17 10:20:22,935][257371] Max Reward on eval: 54.01995595788422[0m
[37m[1m[2023-07-17 10:20:22,936][257371] Min Reward on eval: 54.01995595788422[0m
[37m[1m[2023-07-17 10:20:22,936][257371] Mean Reward across all agents: 54.01995595788422[0m
[37m[1m[2023-07-17 10:20:22,936][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:20:28,156][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:20:28,156][257371] Reward + Measures: [[ -90.60140017    0.40100002    0.24970002    0.37439999    0.1355
     6.7655654 ]
 [ -89.47202557    0.33580002    0.2217        0.29950002    0.15210001
     6.54566669]
 [-127.58812804    0.3441        0.34730002    0.36180001    0.36309999
     7.24701452]
 ...
 [  -8.72760594    0.15030001    0.1569        0.15519999    0.17590001
     4.79546785]
 [  17.69103201    0.3177        0.373         0.22239999    0.41170001
     5.98765516]
 [ -61.15601936    0.30829999    0.34490001    0.35090002    0.3256
     6.167274  ]][0m
[37m[1m[2023-07-17 10:20:28,156][257371] Max Reward on eval: 309.9699214192107[0m
[37m[1m[2023-07-17 10:20:28,157][257371] Min Reward on eval: -317.77385609876364[0m
[37m[1m[2023-07-17 10:20:28,157][257371] Mean Reward across all agents: -45.79620040333055[0m
[37m[1m[2023-07-17 10:20:28,157][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:20:28,159][257371] mean_value=-342.68095552656, max_value=115.59414779847924[0m
[37m[1m[2023-07-17 10:20:28,161][257371] New mean coefficients: [[ 1.9131367   7.6656294  -0.690735    0.23062986  0.421748   -0.27188438]][0m
[37m[1m[2023-07-17 10:20:28,162][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:20:37,180][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 10:20:37,181][257371] FPS: 425884.19[0m
[36m[2023-07-17 10:20:37,183][257371] itr=1056, itrs=2000, Progress: 52.80%[0m
[36m[2023-07-17 10:20:49,013][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 10:20:49,013][257371] FPS: 327525.61[0m
[36m[2023-07-17 10:20:53,354][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:20:53,354][257371] Reward + Measures: [[48.44120333  0.306573    0.22088598  0.30094367  0.22827299  4.01408052]][0m
[37m[1m[2023-07-17 10:20:53,355][257371] Max Reward on eval: 48.44120332595275[0m
[37m[1m[2023-07-17 10:20:53,355][257371] Min Reward on eval: 48.44120332595275[0m
[37m[1m[2023-07-17 10:20:53,355][257371] Mean Reward across all agents: 48.44120332595275[0m
[37m[1m[2023-07-17 10:20:53,355][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:20:58,384][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:20:58,390][257371] Reward + Measures: [[-192.35420657    0.44039997    0.43149996    0.16410001    0.37100002
     4.55607748]
 [ 482.44654605    0.3734        0.85030001    0.32839999    0.82709998
     6.73192167]
 [ -26.85297498    0.1823        0.1356        0.1619        0.13959999
     5.8274827 ]
 ...
 [ -19.46540041    0.1912        0.17340001    0.17560001    0.1758
     4.51518917]
 [  27.77176963    0.22000001    0.22330001    0.17779998    0.26019999
     5.3581748 ]
 [   0.35200648    0.21139999    0.18380001    0.1894        0.2062
     5.50518513]][0m
[37m[1m[2023-07-17 10:20:58,390][257371] Max Reward on eval: 548.4472050301731[0m
[37m[1m[2023-07-17 10:20:58,390][257371] Min Reward on eval: -475.42908264707074[0m
[37m[1m[2023-07-17 10:20:58,390][257371] Mean Reward across all agents: -2.0069779994365033[0m
[37m[1m[2023-07-17 10:20:58,391][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:20:58,393][257371] mean_value=-547.8486350706189, max_value=314.2215811264142[0m
[37m[1m[2023-07-17 10:20:58,395][257371] New mean coefficients: [[ 3.4199433   6.8493514   0.78653073  1.1220248   1.3474888  -0.44814724]][0m
[37m[1m[2023-07-17 10:20:58,396][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:21:07,437][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 10:21:07,438][257371] FPS: 424805.51[0m
[36m[2023-07-17 10:21:07,440][257371] itr=1057, itrs=2000, Progress: 52.85%[0m
[36m[2023-07-17 10:21:19,224][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 10:21:19,224][257371] FPS: 328777.81[0m
[36m[2023-07-17 10:21:23,602][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:21:23,602][257371] Reward + Measures: [[41.92836971  0.327672    0.235314    0.32046267  0.22015199  4.00807953]][0m
[37m[1m[2023-07-17 10:21:23,602][257371] Max Reward on eval: 41.928369705650994[0m
[37m[1m[2023-07-17 10:21:23,602][257371] Min Reward on eval: 41.928369705650994[0m
[37m[1m[2023-07-17 10:21:23,603][257371] Mean Reward across all agents: 41.928369705650994[0m
[37m[1m[2023-07-17 10:21:23,603][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:21:28,644][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:21:28,645][257371] Reward + Measures: [[ 35.52937325   0.13779999   0.19140002   0.11740001   0.20050001
    5.96493053]
 [ -0.01378194   0.24650002   0.30759999   0.24150001   0.30130002
    4.49865818]
 [-37.01959543   0.149        0.12479999   0.11370001   0.1287
    5.62974977]
 ...
 [ 18.90351886   0.20420001   0.18380001   0.17290001   0.1894
    6.17686749]
 [ -9.77278525   0.1947       0.18979999   0.22000001   0.18069999
    5.36394262]
 [ 35.17559634   0.1732       0.2608       0.14359999   0.28490004
    4.99255514]][0m
[37m[1m[2023-07-17 10:21:28,645][257371] Max Reward on eval: 190.00733754392712[0m
[37m[1m[2023-07-17 10:21:28,645][257371] Min Reward on eval: -146.79177495902405[0m
[37m[1m[2023-07-17 10:21:28,645][257371] Mean Reward across all agents: -20.71864364104519[0m
[37m[1m[2023-07-17 10:21:28,646][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:21:28,647][257371] mean_value=-526.3812981651344, max_value=91.2667117291446[0m
[37m[1m[2023-07-17 10:21:28,649][257371] New mean coefficients: [[ 4.096431    7.380068    1.1673671   0.22191858  1.9247661  -0.61067003]][0m
[37m[1m[2023-07-17 10:21:28,650][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:21:37,752][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 10:21:37,752][257371] FPS: 421954.36[0m
[36m[2023-07-17 10:21:37,755][257371] itr=1058, itrs=2000, Progress: 52.90%[0m
[36m[2023-07-17 10:21:49,549][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 10:21:49,549][257371] FPS: 328532.54[0m
[36m[2023-07-17 10:21:53,820][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:21:53,821][257371] Reward + Measures: [[44.94993407  0.35564965  0.241347    0.34759432  0.22536168  3.98326778]][0m
[37m[1m[2023-07-17 10:21:53,821][257371] Max Reward on eval: 44.94993407479037[0m
[37m[1m[2023-07-17 10:21:53,821][257371] Min Reward on eval: 44.94993407479037[0m
[37m[1m[2023-07-17 10:21:53,822][257371] Mean Reward across all agents: 44.94993407479037[0m
[37m[1m[2023-07-17 10:21:53,822][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:21:58,852][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:21:58,853][257371] Reward + Measures: [[ 34.23003626   0.25489998   0.21029997   0.19660001   0.25640002
    5.4999404 ]
 [ 29.85852024   0.1193       0.10829999   0.0929       0.1072
    6.01886415]
 [-54.57243469   0.13500001   0.10519999   0.1096       0.13280001
    5.37485504]
 ...
 [-16.11842185   0.1059       0.1041       0.10210001   0.12049999
    6.10971212]
 [ 28.14947346   0.1261       0.11310001   0.1067       0.1418
    6.04592466]
 [-42.81547774   0.1059       0.09170001   0.09660001   0.1054
    5.54436731]][0m
[37m[1m[2023-07-17 10:21:58,853][257371] Max Reward on eval: 162.55007854923605[0m
[37m[1m[2023-07-17 10:21:58,853][257371] Min Reward on eval: -127.71538617946207[0m
[37m[1m[2023-07-17 10:21:58,854][257371] Mean Reward across all agents: -3.650868187592802[0m
[37m[1m[2023-07-17 10:21:58,854][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:21:58,855][257371] mean_value=-623.4811756132804, max_value=8.243116721626734[0m
[37m[1m[2023-07-17 10:21:58,857][257371] New mean coefficients: [[ 4.1396194  9.026239  -1.1669365 -1.9276221  2.2621415  0.5987423]][0m
[37m[1m[2023-07-17 10:21:58,858][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:22:07,899][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 10:22:07,905][257371] FPS: 424806.55[0m
[36m[2023-07-17 10:22:07,908][257371] itr=1059, itrs=2000, Progress: 52.95%[0m
[36m[2023-07-17 10:22:19,664][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 10:22:19,665][257371] FPS: 329595.57[0m
[36m[2023-07-17 10:22:23,994][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:22:23,999][257371] Reward + Measures: [[41.13940429  0.37670696  0.249446    0.37290502  0.233835    3.97160292]][0m
[37m[1m[2023-07-17 10:22:24,000][257371] Max Reward on eval: 41.13940429000487[0m
[37m[1m[2023-07-17 10:22:24,000][257371] Min Reward on eval: 41.13940429000487[0m
[37m[1m[2023-07-17 10:22:24,000][257371] Mean Reward across all agents: 41.13940429000487[0m
[37m[1m[2023-07-17 10:22:24,000][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:22:29,052][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:22:29,053][257371] Reward + Measures: [[67.31605143  0.20680001  0.22389999  0.21440001  0.21359999  5.24021339]
 [33.42483809  0.2414      0.1463      0.23919998  0.25510001  6.09177732]
 [14.30432109  0.41939998  0.12620001  0.40820003  0.4154      6.7397356 ]
 ...
 [61.15410081  0.22019999  0.20230003  0.20510001  0.199       6.25925493]
 [-9.02325075  0.2316      0.20349999  0.257       0.1735      4.73217487]
 [-2.62196989  0.17040001  0.13700001  0.1214      0.1213      6.09822989]][0m
[37m[1m[2023-07-17 10:22:29,053][257371] Max Reward on eval: 248.57906410219147[0m
[37m[1m[2023-07-17 10:22:29,053][257371] Min Reward on eval: -320.3931398652494[0m
[37m[1m[2023-07-17 10:22:29,054][257371] Mean Reward across all agents: 23.351292248611028[0m
[37m[1m[2023-07-17 10:22:29,054][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:22:29,056][257371] mean_value=-476.8575790833924, max_value=5.58710873047599[0m
[37m[1m[2023-07-17 10:22:29,058][257371] New mean coefficients: [[ 3.8052638   9.491409   -0.77786475  0.16050649  1.5800135   0.190269  ]][0m
[37m[1m[2023-07-17 10:22:29,059][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:22:38,190][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 10:22:38,190][257371] FPS: 420630.42[0m
[36m[2023-07-17 10:22:38,193][257371] itr=1060, itrs=2000, Progress: 53.00%[0m
[37m[1m[2023-07-17 10:25:58,245][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001040[0m
[36m[2023-07-17 10:26:10,431][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 10:26:10,432][257371] FPS: 332183.19[0m
[36m[2023-07-17 10:26:14,633][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:26:14,634][257371] Reward + Measures: [[42.57614711  0.4177613   0.25831267  0.4129957   0.24407466  3.95682263]][0m
[37m[1m[2023-07-17 10:26:14,634][257371] Max Reward on eval: 42.5761471096051[0m
[37m[1m[2023-07-17 10:26:14,634][257371] Min Reward on eval: 42.5761471096051[0m
[37m[1m[2023-07-17 10:26:14,635][257371] Mean Reward across all agents: 42.5761471096051[0m
[37m[1m[2023-07-17 10:26:14,635][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:26:19,625][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:26:19,626][257371] Reward + Measures: [[-10.01629255   0.58070004   0.20299999   0.49530002   0.4278
    4.97625303]
 [ 47.89580307   0.3294       0.1488       0.31160003   0.22189999
    4.37071991]
 [-43.11234503   0.42829999   0.1661       0.42280003   0.29230002
    4.91268444]
 ...
 [ 17.06206654   0.15339999   0.15650001   0.13380001   0.1552
    4.72894526]
 [ -2.75770276   0.2362       0.19289999   0.1693       0.20549999
    4.13885975]
 [-35.71543323   0.1996       0.09320001   0.2184       0.1602
    4.62966633]][0m
[37m[1m[2023-07-17 10:26:19,626][257371] Max Reward on eval: 212.82800208795817[0m
[37m[1m[2023-07-17 10:26:19,626][257371] Min Reward on eval: -145.29437873363494[0m
[37m[1m[2023-07-17 10:26:19,626][257371] Mean Reward across all agents: -21.257204885660165[0m
[37m[1m[2023-07-17 10:26:19,627][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:26:19,628][257371] mean_value=-2770.345928633239, max_value=373.12869039380087[0m
[37m[1m[2023-07-17 10:26:19,630][257371] New mean coefficients: [[ 2.7772636   9.055091   -1.1288362  -0.44411552  2.910895    0.48074082]][0m
[37m[1m[2023-07-17 10:26:19,631][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:26:28,728][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 10:26:28,728][257371] FPS: 422206.57[0m
[36m[2023-07-17 10:26:28,730][257371] itr=1061, itrs=2000, Progress: 53.05%[0m
[36m[2023-07-17 10:26:40,525][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 10:26:40,526][257371] FPS: 328564.11[0m
[36m[2023-07-17 10:26:44,807][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:26:44,808][257371] Reward + Measures: [[33.29844139  0.47989136  0.27944466  0.47427702  0.25216532  3.96839976]][0m
[37m[1m[2023-07-17 10:26:44,808][257371] Max Reward on eval: 33.29844138805625[0m
[37m[1m[2023-07-17 10:26:44,808][257371] Min Reward on eval: 33.29844138805625[0m
[37m[1m[2023-07-17 10:26:44,808][257371] Mean Reward across all agents: 33.29844138805625[0m
[37m[1m[2023-07-17 10:26:44,809][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:26:49,823][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:26:49,824][257371] Reward + Measures: [[  88.73653661    0.36240003    0.43589997    0.33469999    0.42500001
     5.04311514]
 [ -94.79978801    0.16849999    0.1637        0.1734        0.1917
     5.09235716]
 [-191.48499656    0.32610002    0.22239999    0.35780001    0.31420001
     5.05966663]
 ...
 [-102.79163781    0.34600002    0.1999        0.3673        0.31649998
     6.10879993]
 [  -2.15026676    0.21760002    0.2385        0.1839        0.27280003
     5.15213156]
 [-149.8973529     0.17209999    0.21870001    0.19599999    0.25030002
     4.51989508]][0m
[37m[1m[2023-07-17 10:26:49,824][257371] Max Reward on eval: 286.13357166876085[0m
[37m[1m[2023-07-17 10:26:49,824][257371] Min Reward on eval: -430.2916717927437[0m
[37m[1m[2023-07-17 10:26:49,824][257371] Mean Reward across all agents: -63.976569734913646[0m
[37m[1m[2023-07-17 10:26:49,825][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:26:49,826][257371] mean_value=-438.2566076314121, max_value=273.35128413110283[0m
[37m[1m[2023-07-17 10:26:49,829][257371] New mean coefficients: [[ 2.0093057  9.645443  -1.7575895  0.0260475  1.183007   0.4177154]][0m
[37m[1m[2023-07-17 10:26:49,830][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:26:58,827][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 10:26:58,828][257371] FPS: 426871.19[0m
[36m[2023-07-17 10:26:58,830][257371] itr=1062, itrs=2000, Progress: 53.10%[0m
[36m[2023-07-17 10:27:10,577][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 10:27:10,577][257371] FPS: 329989.08[0m
[36m[2023-07-17 10:27:14,804][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:27:14,804][257371] Reward + Measures: [[13.33615334  0.57417095  0.32358199  0.57735896  0.28688303  3.96265078]][0m
[37m[1m[2023-07-17 10:27:14,804][257371] Max Reward on eval: 13.336153343319543[0m
[37m[1m[2023-07-17 10:27:14,804][257371] Min Reward on eval: 13.336153343319543[0m
[37m[1m[2023-07-17 10:27:14,805][257371] Mean Reward across all agents: 13.336153343319543[0m
[37m[1m[2023-07-17 10:27:14,805][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:27:19,807][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:27:19,807][257371] Reward + Measures: [[-20.43880107   0.26460001   0.2203       0.2199       0.28079998
    4.29496145]
 [ 29.68382811   0.2244       0.13270001   0.19960001   0.1663
    4.04021597]
 [ 53.51660751   0.26229998   0.22839999   0.22720002   0.22660001
    5.57152843]
 ...
 [-47.05696959   0.24150001   0.20020001   0.24640003   0.0959
    4.63594151]
 [ 38.22238381   0.2868       0.2809       0.19860001   0.20820001
    5.22574711]
 [  2.51833857   0.29820001   0.2333       0.2538       0.2595
    4.22516394]][0m
[37m[1m[2023-07-17 10:27:19,808][257371] Max Reward on eval: 191.08307833429427[0m
[37m[1m[2023-07-17 10:27:19,808][257371] Min Reward on eval: -233.87736678030342[0m
[37m[1m[2023-07-17 10:27:19,808][257371] Mean Reward across all agents: -3.7667636105215845[0m
[37m[1m[2023-07-17 10:27:19,808][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:27:19,809][257371] mean_value=-2398.1478083176726, max_value=-27.935718464777125[0m
[36m[2023-07-17 10:27:19,812][257371] XNES is restarting with a new solution whose measures are [0.23430002 0.65650004 0.34120002 0.59030002 2.89464378] and objective is 362.4628524631262[0m
[36m[2023-07-17 10:27:19,813][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 10:27:19,815][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 10:27:19,816][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:27:28,729][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-17 10:27:28,730][257371] FPS: 430869.98[0m
[36m[2023-07-17 10:27:28,732][257371] itr=1063, itrs=2000, Progress: 53.15%[0m
[36m[2023-07-17 10:27:40,368][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 10:27:40,368][257371] FPS: 333015.85[0m
[36m[2023-07-17 10:27:44,649][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:27:44,649][257371] Reward + Measures: [[173.61021615   0.19577934   0.71551561   0.37396097   0.65294433
    3.63060546]][0m
[37m[1m[2023-07-17 10:27:44,649][257371] Max Reward on eval: 173.61021614900193[0m
[37m[1m[2023-07-17 10:27:44,650][257371] Min Reward on eval: 173.61021614900193[0m
[37m[1m[2023-07-17 10:27:44,650][257371] Mean Reward across all agents: 173.61021614900193[0m
[37m[1m[2023-07-17 10:27:44,650][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:27:49,603][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:27:49,603][257371] Reward + Measures: [[ 76.93094558   0.43129998   0.52360004   0.3233       0.47140002
    3.09455109]
 [151.20370315   0.68699998   0.35890004   0.71680003   0.29340002
    5.81702662]
 [123.89171201   0.18900001   0.39669999   0.27760002   0.32069999
    3.8584125 ]
 ...
 [211.91722941   0.16410001   0.62150002   0.25260001   0.63500005
    3.8047998 ]
 [180.24193337   0.1365       0.53549999   0.23529999   0.50680006
    3.23612833]
 [170.81875704   0.39920002   0.40050003   0.4632       0.33540002
    4.80153751]][0m
[37m[1m[2023-07-17 10:27:49,604][257371] Max Reward on eval: 429.6833687081933[0m
[37m[1m[2023-07-17 10:27:49,604][257371] Min Reward on eval: -537.8713798047975[0m
[37m[1m[2023-07-17 10:27:49,604][257371] Mean Reward across all agents: 19.84170672089985[0m
[37m[1m[2023-07-17 10:27:49,605][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:27:49,607][257371] mean_value=-949.4751219832243, max_value=159.86705049807904[0m
[37m[1m[2023-07-17 10:27:49,609][257371] New mean coefficients: [[ 1.4661794  -1.0082333  -2.4543314  -1.9143449  -1.2103155  -0.34567568]][0m
[37m[1m[2023-07-17 10:27:49,610][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:27:58,570][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 10:27:58,571][257371] FPS: 428651.60[0m
[36m[2023-07-17 10:27:58,573][257371] itr=1064, itrs=2000, Progress: 53.20%[0m
[36m[2023-07-17 10:28:10,509][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 10:28:10,510][257371] FPS: 324626.58[0m
[36m[2023-07-17 10:28:14,802][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:28:14,803][257371] Reward + Measures: [[121.29978006   0.58488137   0.31536099   0.6430763    0.480508
    4.12222767]][0m
[37m[1m[2023-07-17 10:28:14,803][257371] Max Reward on eval: 121.29978006232817[0m
[37m[1m[2023-07-17 10:28:14,803][257371] Min Reward on eval: 121.29978006232817[0m
[37m[1m[2023-07-17 10:28:14,803][257371] Mean Reward across all agents: 121.29978006232817[0m
[37m[1m[2023-07-17 10:28:14,804][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:28:19,794][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:28:19,795][257371] Reward + Measures: [[-22.65260609   0.15900001   0.79959995   0.88479996   0.89589995
    5.66594601]
 [ 59.11829069   0.22790001   0.22490001   0.22930001   0.3046
    5.36971188]
 [118.2828869    0.3335       0.28299999   0.32430002   0.24000001
    5.46931934]
 ...
 [272.55686952   0.77459997   0.50120002   0.77059996   0.12730001
    5.90883732]
 [ 70.18315819   0.15269999   0.2271       0.18530001   0.13520001
    5.85730982]
 [ 82.76568582   0.35100001   0.38600001   0.36789998   0.38270003
    5.60692596]][0m
[37m[1m[2023-07-17 10:28:19,795][257371] Max Reward on eval: 513.3713035993278[0m
[37m[1m[2023-07-17 10:28:19,796][257371] Min Reward on eval: -388.1515548054129[0m
[37m[1m[2023-07-17 10:28:19,796][257371] Mean Reward across all agents: 56.300742644110045[0m
[37m[1m[2023-07-17 10:28:19,796][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:28:19,799][257371] mean_value=-446.90492272664727, max_value=213.97971948430632[0m
[37m[1m[2023-07-17 10:28:19,802][257371] New mean coefficients: [[ 2.1060832  -0.5027709  -2.514034   -1.7662828  -1.380745   -0.49126536]][0m
[37m[1m[2023-07-17 10:28:19,803][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:28:28,814][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 10:28:28,814][257371] FPS: 426203.80[0m
[36m[2023-07-17 10:28:28,816][257371] itr=1065, itrs=2000, Progress: 53.25%[0m
[36m[2023-07-17 10:28:40,575][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 10:28:40,576][257371] FPS: 329588.27[0m
[36m[2023-07-17 10:28:44,935][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:28:44,936][257371] Reward + Measures: [[37.28295518  0.49424097  0.33869666  0.51554966  0.42532736  3.37157488]][0m
[37m[1m[2023-07-17 10:28:44,936][257371] Max Reward on eval: 37.28295518438183[0m
[37m[1m[2023-07-17 10:28:44,936][257371] Min Reward on eval: 37.28295518438183[0m
[37m[1m[2023-07-17 10:28:44,936][257371] Mean Reward across all agents: 37.28295518438183[0m
[37m[1m[2023-07-17 10:28:44,937][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:28:49,959][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:28:49,959][257371] Reward + Measures: [[  67.01793909    0.25840002    0.36390001    0.23559999    0.3845
     4.63739729]
 [ 110.1519274     0.41780001    0.42530003    0.22359999    0.53630006
     6.29522467]
 [-470.74465824    0.86800003    0.0452        0.85939997    0.81570005
     6.87572575]
 ...
 [ -13.80553129    0.28390002    0.2974        0.27070001    0.26719999
     5.72981882]
 [  11.54924194    0.22140001    0.1919        0.2218        0.20420001
     4.72032499]
 [ 112.97453789    0.38529998    0.4152        0.4165        0.42519999
     5.29456663]][0m
[37m[1m[2023-07-17 10:28:49,960][257371] Max Reward on eval: 318.27084318008275[0m
[37m[1m[2023-07-17 10:28:49,960][257371] Min Reward on eval: -532.1380195757374[0m
[37m[1m[2023-07-17 10:28:49,960][257371] Mean Reward across all agents: -7.095945903597522[0m
[37m[1m[2023-07-17 10:28:49,961][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:28:49,962][257371] mean_value=-942.4537138549349, max_value=73.00239590296204[0m
[37m[1m[2023-07-17 10:28:49,965][257371] New mean coefficients: [[ 2.9165049   0.72382665 -2.1054924  -1.359342   -1.3353316  -0.12468874]][0m
[37m[1m[2023-07-17 10:28:49,966][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:28:59,011][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 10:28:59,011][257371] FPS: 424605.08[0m
[36m[2023-07-17 10:28:59,014][257371] itr=1066, itrs=2000, Progress: 53.30%[0m
[36m[2023-07-17 10:29:10,696][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 10:29:10,696][257371] FPS: 331773.92[0m
[36m[2023-07-17 10:29:14,897][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:29:14,897][257371] Reward + Measures: [[1.5618188  0.407821   0.35733032 0.4858273  0.360432   3.42587686]][0m
[37m[1m[2023-07-17 10:29:14,898][257371] Max Reward on eval: 1.561818798887068[0m
[37m[1m[2023-07-17 10:29:14,898][257371] Min Reward on eval: 1.561818798887068[0m
[37m[1m[2023-07-17 10:29:14,898][257371] Mean Reward across all agents: 1.561818798887068[0m
[37m[1m[2023-07-17 10:29:14,898][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:29:19,872][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:29:19,873][257371] Reward + Measures: [[ 11.53631097   0.30779999   0.37960002   0.33719999   0.36140001
    3.66531754]
 [-72.05571232   0.29260001   0.38819996   0.30630001   0.41910002
    4.72260427]
 [ -8.79379901   0.16849999   0.1955       0.152        0.1859
    4.55255508]
 ...
 [-36.0582524    0.48559999   0.3184       0.52209997   0.5693
    5.14082432]
 [ -4.84866566   0.2656       0.37110001   0.41430002   0.37890002
    4.43742085]
 [ 33.36513123   0.29820004   0.35800001   0.33909997   0.40810004
    5.58400297]][0m
[37m[1m[2023-07-17 10:29:19,873][257371] Max Reward on eval: 379.2586975120008[0m
[37m[1m[2023-07-17 10:29:19,873][257371] Min Reward on eval: -494.36812685546465[0m
[37m[1m[2023-07-17 10:29:19,873][257371] Mean Reward across all agents: -1.9062465553267434[0m
[37m[1m[2023-07-17 10:29:19,873][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:29:19,876][257371] mean_value=-1680.121302860291, max_value=152.9865729882552[0m
[37m[1m[2023-07-17 10:29:19,878][257371] New mean coefficients: [[ 1.8290246   0.37787634 -3.0163589  -2.328357    0.425604   -0.00205449]][0m
[37m[1m[2023-07-17 10:29:19,879][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:29:28,881][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 10:29:28,881][257371] FPS: 426649.48[0m
[36m[2023-07-17 10:29:28,884][257371] itr=1067, itrs=2000, Progress: 53.35%[0m
[36m[2023-07-17 10:29:40,719][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 10:29:40,719][257371] FPS: 327415.75[0m
[36m[2023-07-17 10:29:45,024][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:29:45,024][257371] Reward + Measures: [[4.14614819 0.40714067 0.35489836 0.48407933 0.35779732 3.43803048]][0m
[37m[1m[2023-07-17 10:29:45,024][257371] Max Reward on eval: 4.146148190968602[0m
[37m[1m[2023-07-17 10:29:45,025][257371] Min Reward on eval: 4.146148190968602[0m
[37m[1m[2023-07-17 10:29:45,025][257371] Mean Reward across all agents: 4.146148190968602[0m
[37m[1m[2023-07-17 10:29:45,025][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:29:50,007][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:29:50,007][257371] Reward + Measures: [[-109.51152299    0.26109999    0.30650002    0.2516        0.26980004
     4.36050749]
 [ -28.50086899    0.20429997    0.28200004    0.27500001    0.2264
     4.64521265]
 [ -51.86624979    0.153         0.1549        0.1235        0.1376
     4.9817977 ]
 ...
 [  -0.29859013    0.2404        0.24430001    0.20920001    0.22430001
     4.48809195]
 [ -64.64767976    0.26180002    0.33010003    0.27420002    0.26550001
     4.22536707]
 [ -43.88450384    0.17060001    0.2023        0.2096        0.20750001
     4.38337278]][0m
[37m[1m[2023-07-17 10:29:50,008][257371] Max Reward on eval: 174.82481754757464[0m
[37m[1m[2023-07-17 10:29:50,008][257371] Min Reward on eval: -152.29414104688914[0m
[37m[1m[2023-07-17 10:29:50,008][257371] Mean Reward across all agents: -18.11115923217569[0m
[37m[1m[2023-07-17 10:29:50,008][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:29:50,010][257371] mean_value=-1659.5851414651727, max_value=68.6131687699291[0m
[37m[1m[2023-07-17 10:29:50,012][257371] New mean coefficients: [[ 0.5901836   0.5069315  -1.6168216  -1.3151813  -0.62418854 -0.26392102]][0m
[37m[1m[2023-07-17 10:29:50,013][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:29:59,062][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 10:29:59,062][257371] FPS: 424416.49[0m
[36m[2023-07-17 10:29:59,065][257371] itr=1068, itrs=2000, Progress: 53.40%[0m
[36m[2023-07-17 10:30:10,835][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 10:30:10,835][257371] FPS: 329318.44[0m
[36m[2023-07-17 10:30:15,085][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:30:15,090][257371] Reward + Measures: [[8.24323367 0.4089663  0.35657331 0.48414034 0.35579133 3.42751551]][0m
[37m[1m[2023-07-17 10:30:15,091][257371] Max Reward on eval: 8.243233668468228[0m
[37m[1m[2023-07-17 10:30:15,091][257371] Min Reward on eval: 8.243233668468228[0m
[37m[1m[2023-07-17 10:30:15,091][257371] Mean Reward across all agents: 8.243233668468228[0m
[37m[1m[2023-07-17 10:30:15,091][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:30:20,316][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:30:20,321][257371] Reward + Measures: [[-36.39741116   0.35430002   0.38340002   0.37729999   0.29570004
    4.19731092]
 [ 92.09731208   0.21370001   0.26289999   0.23669998   0.23319998
    5.13965082]
 [ 25.35169433   0.2325       0.19690001   0.16430001   0.1874
    5.60693598]
 ...
 [-19.97492687   0.10919999   0.11310001   0.08880001   0.0903
    4.44922733]
 [ 13.00397612   0.19939999   0.22420001   0.19700001   0.22330001
    5.10504103]
 [ -1.10969447   0.2494       0.34540001   0.2405       0.30490002
    5.63037872]][0m
[37m[1m[2023-07-17 10:30:20,322][257371] Max Reward on eval: 403.5272710460238[0m
[37m[1m[2023-07-17 10:30:20,322][257371] Min Reward on eval: -412.221748844767[0m
[37m[1m[2023-07-17 10:30:20,322][257371] Mean Reward across all agents: 9.630615408517[0m
[37m[1m[2023-07-17 10:30:20,322][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:30:20,324][257371] mean_value=-1331.8406699569655, max_value=108.25568412320911[0m
[37m[1m[2023-07-17 10:30:20,327][257371] New mean coefficients: [[ 2.0466723  -0.43640542 -0.6854427  -0.89186275 -1.0523493  -0.39040524]][0m
[37m[1m[2023-07-17 10:30:20,328][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:30:29,341][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 10:30:29,341][257371] FPS: 426110.28[0m
[36m[2023-07-17 10:30:29,344][257371] itr=1069, itrs=2000, Progress: 53.45%[0m
[36m[2023-07-17 10:30:41,049][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 10:30:41,049][257371] FPS: 331145.91[0m
[36m[2023-07-17 10:30:45,374][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:30:45,375][257371] Reward + Measures: [[10.55972121  0.40419966  0.35597166  0.48034969  0.35543934  3.40359378]][0m
[37m[1m[2023-07-17 10:30:45,375][257371] Max Reward on eval: 10.559721206063085[0m
[37m[1m[2023-07-17 10:30:45,375][257371] Min Reward on eval: 10.559721206063085[0m
[37m[1m[2023-07-17 10:30:45,376][257371] Mean Reward across all agents: 10.559721206063085[0m
[37m[1m[2023-07-17 10:30:45,376][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:30:50,401][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:30:50,407][257371] Reward + Measures: [[ -1.61306029   0.0733       0.0774       0.086        0.0726
    4.64977551]
 [ 73.54846523   0.40470001   0.3222       0.30860001   0.28390002
    4.81384039]
 [-13.36998676   0.1294       0.1372       0.1649       0.1268
    4.72296715]
 ...
 [-22.21730812   0.1565       0.2254       0.2359       0.19540001
    3.46594501]
 [-74.24014388   0.19         0.18340001   0.1851       0.17199999
    4.87233353]
 [ 20.62311428   0.2395       0.26860002   0.26890001   0.19539998
    4.61609364]][0m
[37m[1m[2023-07-17 10:30:50,407][257371] Max Reward on eval: 350.1404865018092[0m
[37m[1m[2023-07-17 10:30:50,407][257371] Min Reward on eval: -256.9143618594855[0m
[37m[1m[2023-07-17 10:30:50,408][257371] Mean Reward across all agents: 29.173738882029703[0m
[37m[1m[2023-07-17 10:30:50,408][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:30:50,410][257371] mean_value=-1017.7593588857093, max_value=345.9146775263583[0m
[37m[1m[2023-07-17 10:30:50,413][257371] New mean coefficients: [[ 2.9081478  -0.7907633  -2.0205762  -1.6389289   0.36096704 -0.69489175]][0m
[37m[1m[2023-07-17 10:30:50,414][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:30:59,496][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 10:30:59,497][257371] FPS: 422867.03[0m
[36m[2023-07-17 10:30:59,499][257371] itr=1070, itrs=2000, Progress: 53.50%[0m
[37m[1m[2023-07-17 10:34:30,613][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001050[0m
[36m[2023-07-17 10:34:42,828][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 10:34:42,828][257371] FPS: 333153.29[0m
[36m[2023-07-17 10:34:47,103][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:34:47,103][257371] Reward + Measures: [[14.5370839   0.40230134  0.35465536  0.480793    0.35382169  3.38362241]][0m
[37m[1m[2023-07-17 10:34:47,103][257371] Max Reward on eval: 14.537083898883731[0m
[37m[1m[2023-07-17 10:34:47,104][257371] Min Reward on eval: 14.537083898883731[0m
[37m[1m[2023-07-17 10:34:47,104][257371] Mean Reward across all agents: 14.537083898883731[0m
[37m[1m[2023-07-17 10:34:47,104][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:34:52,055][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:34:52,055][257371] Reward + Measures: [[89.5736517   0.25729999  0.29930001  0.25259998  0.26710004  4.35664082]
 [36.24294684  0.27200001  0.3759      0.35640001  0.31950003  3.380373  ]
 [-9.82307018  0.30300003  0.32250002  0.28200004  0.229       4.66015291]
 ...
 [ 2.99413487  0.17470001  0.20079999  0.1917      0.17479999  4.91350126]
 [10.29892898  0.17680001  0.23010002  0.19030002  0.189       4.50770092]
 [-4.07299157  0.2141      0.2757      0.2184      0.21300001  4.23870611]][0m
[37m[1m[2023-07-17 10:34:52,056][257371] Max Reward on eval: 390.9183969948441[0m
[37m[1m[2023-07-17 10:34:52,056][257371] Min Reward on eval: -149.60384549722076[0m
[37m[1m[2023-07-17 10:34:52,056][257371] Mean Reward across all agents: 16.41458518253022[0m
[37m[1m[2023-07-17 10:34:52,056][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:34:52,058][257371] mean_value=-1261.8606467638149, max_value=281.84134644471857[0m
[37m[1m[2023-07-17 10:34:52,060][257371] New mean coefficients: [[ 0.9419925  -0.97405267 -1.2750688  -3.327512   -0.39338565 -0.7372047 ]][0m
[37m[1m[2023-07-17 10:34:52,061][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:35:01,016][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 10:35:01,016][257371] FPS: 428917.71[0m
[36m[2023-07-17 10:35:01,018][257371] itr=1071, itrs=2000, Progress: 53.55%[0m
[36m[2023-07-17 10:35:12,737][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 10:35:12,738][257371] FPS: 330610.88[0m
[36m[2023-07-17 10:35:17,027][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:35:17,027][257371] Reward + Measures: [[-7.58156984  0.41943169  0.39571768  0.49970132  0.369156    3.65561771]][0m
[37m[1m[2023-07-17 10:35:17,027][257371] Max Reward on eval: -7.581569843678847[0m
[37m[1m[2023-07-17 10:35:17,028][257371] Min Reward on eval: -7.581569843678847[0m
[37m[1m[2023-07-17 10:35:17,028][257371] Mean Reward across all agents: -7.581569843678847[0m
[37m[1m[2023-07-17 10:35:17,028][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:35:22,061][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:35:22,061][257371] Reward + Measures: [[  7.11913477   0.43470001   0.35050002   0.46450001   0.34800002
    4.29033375]
 [ 98.07633424   0.22199999   0.45479998   0.28919998   0.41139999
    4.19623423]
 [ 41.32924357   0.25         0.27700001   0.26679999   0.25999999
    3.65749097]
 ...
 [-23.84125692   0.32460001   0.35370001   0.26809999   0.3303
    5.5212698 ]
 [ 69.82624805   0.19050001   0.37480003   0.30850002   0.29240003
    4.8403964 ]
 [-28.47799531   0.132        0.1749       0.16680001   0.1639
    4.7559371 ]][0m
[37m[1m[2023-07-17 10:35:22,062][257371] Max Reward on eval: 224.61515112146736[0m
[37m[1m[2023-07-17 10:35:22,062][257371] Min Reward on eval: -197.1861713170074[0m
[37m[1m[2023-07-17 10:35:22,062][257371] Mean Reward across all agents: 15.481935423153796[0m
[37m[1m[2023-07-17 10:35:22,062][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:35:22,064][257371] mean_value=-1132.1726657827476, max_value=-26.698552373408575[0m
[36m[2023-07-17 10:35:22,066][257371] XNES is restarting with a new solution whose measures are [0.12329999 0.16070001 0.1068     0.1549     7.7835288 ] and objective is 200.98533913732973[0m
[36m[2023-07-17 10:35:22,067][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 10:35:22,069][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 10:35:22,070][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:35:31,132][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 10:35:31,133][257371] FPS: 423805.61[0m
[36m[2023-07-17 10:35:31,135][257371] itr=1072, itrs=2000, Progress: 53.60%[0m
[36m[2023-07-17 10:35:42,824][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 10:35:42,824][257371] FPS: 331611.87[0m
[36m[2023-07-17 10:35:47,137][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:35:47,138][257371] Reward + Measures: [[-3.29498175  0.15031067  0.19163932  0.07947867  0.16813733  7.43439817]][0m
[37m[1m[2023-07-17 10:35:47,138][257371] Max Reward on eval: -3.294981754135806[0m
[37m[1m[2023-07-17 10:35:47,138][257371] Min Reward on eval: -3.294981754135806[0m
[37m[1m[2023-07-17 10:35:47,138][257371] Mean Reward across all agents: -3.294981754135806[0m
[37m[1m[2023-07-17 10:35:47,139][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:35:52,108][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:35:52,109][257371] Reward + Measures: [[ -17.43031151    0.1141        0.13450001    0.1141        0.15970002
     7.41026688]
 [ -78.60783623    0.1727        0.17729998    0.15100001    0.17020001
     7.40698957]
 [-254.59110848    0.23529999    0.25880003    0.05650001    0.2649
     7.66655827]
 ...
 [-284.55707983    0.3592        0.44370005    0.11630001    0.47009999
     6.90318441]
 [ -22.97787366    0.09780001    0.1275        0.076         0.11129999
     7.06881857]
 [-190.42274403    0.42230007    0.40310001    0.39050004    0.4179
     7.51571274]][0m
[37m[1m[2023-07-17 10:35:52,109][257371] Max Reward on eval: 139.77588747207773[0m
[37m[1m[2023-07-17 10:35:52,109][257371] Min Reward on eval: -598.0628752928227[0m
[37m[1m[2023-07-17 10:35:52,109][257371] Mean Reward across all agents: -64.79794110832123[0m
[37m[1m[2023-07-17 10:35:52,110][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:35:52,111][257371] mean_value=-296.22013757845457, max_value=37.07460601139394[0m
[37m[1m[2023-07-17 10:35:52,113][257371] New mean coefficients: [[-0.9630945  -0.15759599  0.14255065 -1.797312   -1.133155   -0.7929795 ]][0m
[37m[1m[2023-07-17 10:35:52,114][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:36:01,095][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 10:36:01,096][257371] FPS: 427639.80[0m
[36m[2023-07-17 10:36:01,098][257371] itr=1073, itrs=2000, Progress: 53.65%[0m
[36m[2023-07-17 10:36:12,772][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 10:36:12,772][257371] FPS: 331876.89[0m
[36m[2023-07-17 10:36:16,990][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:36:16,990][257371] Reward + Measures: [[-83.46315919   0.27078602   0.33266833   0.07953867   0.31765601
    7.52871227]][0m
[37m[1m[2023-07-17 10:36:16,991][257371] Max Reward on eval: -83.46315918887441[0m
[37m[1m[2023-07-17 10:36:16,991][257371] Min Reward on eval: -83.46315918887441[0m
[37m[1m[2023-07-17 10:36:16,991][257371] Mean Reward across all agents: -83.46315918887441[0m
[37m[1m[2023-07-17 10:36:16,991][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:36:22,221][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:36:22,221][257371] Reward + Measures: [[  21.58519403    0.0623        0.0486        0.1285        0.14579999
     7.81781626]
 [  36.71442202    0.14650001    0.2211        0.20979999    0.1954
     7.71693897]
 [-107.75808265    0.13610001    0.09710001    0.1111        0.11860001
     7.31858015]
 ...
 [-148.27700977    0.33380002    0.1061        0.31210002    0.29920003
     7.53766966]
 [  39.16788154    0.09729999    0.0785        0.10339999    0.09860001
     7.45789671]
 [ -44.58599103    0.21010001    0.28210002    0.2067        0.27610001
     7.62876463]][0m
[37m[1m[2023-07-17 10:36:22,222][257371] Max Reward on eval: 273.6917743575759[0m
[37m[1m[2023-07-17 10:36:22,222][257371] Min Reward on eval: -306.3947420101613[0m
[37m[1m[2023-07-17 10:36:22,222][257371] Mean Reward across all agents: -14.732171649355141[0m
[37m[1m[2023-07-17 10:36:22,222][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:36:22,224][257371] mean_value=-230.72265408674642, max_value=-14.907169449295566[0m
[36m[2023-07-17 10:36:22,226][257371] XNES is restarting with a new solution whose measures are [0.28599998 0.41930005 0.36410004 0.37170002 3.81590724] and objective is 322.1286797888577[0m
[36m[2023-07-17 10:36:22,227][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 10:36:22,230][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 10:36:22,230][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:36:31,207][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 10:36:31,207][257371] FPS: 427852.00[0m
[36m[2023-07-17 10:36:31,210][257371] itr=1074, itrs=2000, Progress: 53.70%[0m
[36m[2023-07-17 10:36:42,979][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 10:36:42,980][257371] FPS: 329256.25[0m
[36m[2023-07-17 10:36:47,275][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:36:47,275][257371] Reward + Measures: [[142.01631634   0.34152865   0.43246168   0.31380099   0.43036166
    4.11116123]][0m
[37m[1m[2023-07-17 10:36:47,275][257371] Max Reward on eval: 142.01631634197776[0m
[37m[1m[2023-07-17 10:36:47,276][257371] Min Reward on eval: 142.01631634197776[0m
[37m[1m[2023-07-17 10:36:47,276][257371] Mean Reward across all agents: 142.01631634197776[0m
[37m[1m[2023-07-17 10:36:47,276][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:36:52,194][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:36:52,195][257371] Reward + Measures: [[-21.39881936   0.26770002   0.30510002   0.24190001   0.33250001
    4.88703918]
 [ 39.1763827    0.22260001   0.3057       0.31819999   0.22189999
    4.34853077]
 [ 12.53682048   0.32210001   0.35250002   0.35610002   0.31570002
    4.6993804 ]
 ...
 [ 40.20029356   0.23509999   0.34900001   0.22850001   0.35780001
    4.55907202]
 [ -5.1248065    0.27579999   0.34850001   0.285        0.36540005
    4.27982759]
 [ 70.81011462   0.25180003   0.38999999   0.35160002   0.33510002
    4.14747095]][0m
[37m[1m[2023-07-17 10:36:52,195][257371] Max Reward on eval: 153.7795054614544[0m
[37m[1m[2023-07-17 10:36:52,195][257371] Min Reward on eval: -128.48570630196483[0m
[37m[1m[2023-07-17 10:36:52,195][257371] Mean Reward across all agents: 20.794966434637274[0m
[37m[1m[2023-07-17 10:36:52,196][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:36:52,197][257371] mean_value=-521.2874552936377, max_value=-71.30314432188008[0m
[36m[2023-07-17 10:36:52,199][257371] XNES is restarting with a new solution whose measures are [0.58179998 0.3292     0.55059999 0.0917     1.53434813] and objective is 443.62416076272496[0m
[36m[2023-07-17 10:36:52,200][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 10:36:52,202][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 10:36:52,203][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:37:01,204][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 10:37:01,204][257371] FPS: 426705.28[0m
[36m[2023-07-17 10:37:01,206][257371] itr=1075, itrs=2000, Progress: 53.75%[0m
[36m[2023-07-17 10:37:12,898][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 10:37:12,899][257371] FPS: 331466.70[0m
[36m[2023-07-17 10:37:17,118][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:37:17,118][257371] Reward + Measures: [[653.43391228   0.58967096   0.34349537   0.47097299   0.08205866
    1.4042598 ]][0m
[37m[1m[2023-07-17 10:37:17,118][257371] Max Reward on eval: 653.4339122846037[0m
[37m[1m[2023-07-17 10:37:17,119][257371] Min Reward on eval: 653.4339122846037[0m
[37m[1m[2023-07-17 10:37:17,119][257371] Mean Reward across all agents: 653.4339122846037[0m
[37m[1m[2023-07-17 10:37:17,119][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:37:22,135][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:37:22,135][257371] Reward + Measures: [[ 42.18737624   0.47159997   0.37640002   0.46490002   0.12449999
    2.94720459]
 [679.31529996   0.2685       0.24980001   0.23819999   0.20299999
    2.69272757]
 [163.07421543   0.13630001   0.13640001   0.12890001   0.18350001
    4.22353411]
 ...
 [183.94677565   0.14300001   0.1346       0.1743       0.15890001
    3.96289301]
 [155.6803674    0.16629998   0.142        0.1793       0.1654
    3.77381754]
 [217.47488382   0.26370004   0.18080001   0.24710003   0.22089998
    2.94800067]][0m
[37m[1m[2023-07-17 10:37:22,135][257371] Max Reward on eval: 1191.1752548270858[0m
[37m[1m[2023-07-17 10:37:22,136][257371] Min Reward on eval: -59.34061645455658[0m
[37m[1m[2023-07-17 10:37:22,136][257371] Mean Reward across all agents: 194.64896504859038[0m
[37m[1m[2023-07-17 10:37:22,136][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:37:22,138][257371] mean_value=-1775.410419947408, max_value=607.0283560233466[0m
[37m[1m[2023-07-17 10:37:22,140][257371] New mean coefficients: [[ 0.7209573  -0.8966534  -0.9788112  -2.2215729  -0.78664166 -0.81014776]][0m
[37m[1m[2023-07-17 10:37:22,141][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:37:31,160][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 10:37:31,160][257371] FPS: 425870.77[0m
[36m[2023-07-17 10:37:31,162][257371] itr=1076, itrs=2000, Progress: 53.80%[0m
[36m[2023-07-17 10:37:43,078][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 10:37:43,078][257371] FPS: 325211.81[0m
[36m[2023-07-17 10:37:47,327][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:37:47,327][257371] Reward + Measures: [[540.25306733   0.49886498   0.258113     0.44993269   0.10103399
    1.74218392]][0m
[37m[1m[2023-07-17 10:37:47,328][257371] Max Reward on eval: 540.2530673265181[0m
[37m[1m[2023-07-17 10:37:47,328][257371] Min Reward on eval: 540.2530673265181[0m
[37m[1m[2023-07-17 10:37:47,328][257371] Mean Reward across all agents: 540.2530673265181[0m
[37m[1m[2023-07-17 10:37:47,328][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:37:52,379][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:37:52,379][257371] Reward + Measures: [[ 73.78077712   0.23310001   0.18970001   0.18539999   0.29940003
    4.838552  ]
 [ 89.76472158   0.1133       0.07050001   0.0631       0.0977
    5.60283422]
 [130.96577019   0.32210001   0.36149999   0.27630001   0.38209996
    4.21281147]
 ...
 [254.46990774   0.33659998   0.33419999   0.29209998   0.36750001
    3.33828807]
 [ -0.8319419    0.0769       0.0859       0.0947       0.1046
    5.38477421]
 [115.96001867   0.38599998   0.29410002   0.26220003   0.30939999
    3.57187343]][0m
[37m[1m[2023-07-17 10:37:52,380][257371] Max Reward on eval: 835.1297073418507[0m
[37m[1m[2023-07-17 10:37:52,380][257371] Min Reward on eval: -58.93085484784096[0m
[37m[1m[2023-07-17 10:37:52,380][257371] Mean Reward across all agents: 128.69163662703144[0m
[37m[1m[2023-07-17 10:37:52,380][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:37:52,382][257371] mean_value=-1742.5239940456624, max_value=153.094017369262[0m
[37m[1m[2023-07-17 10:37:52,385][257371] New mean coefficients: [[ 0.34548852 -1.1506395  -1.3846786  -1.913402   -0.91433525 -0.8394304 ]][0m
[37m[1m[2023-07-17 10:37:52,386][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:38:01,527][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 10:38:01,527][257371] FPS: 420142.77[0m
[36m[2023-07-17 10:38:01,530][257371] itr=1077, itrs=2000, Progress: 53.85%[0m
[36m[2023-07-17 10:38:13,356][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 10:38:13,356][257371] FPS: 327684.46[0m
[36m[2023-07-17 10:38:17,648][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:38:17,649][257371] Reward + Measures: [[714.40923627   0.40015665   0.24567899   0.33499566   0.09292199
    1.60745966]][0m
[37m[1m[2023-07-17 10:38:17,649][257371] Max Reward on eval: 714.4092362674616[0m
[37m[1m[2023-07-17 10:38:17,649][257371] Min Reward on eval: 714.4092362674616[0m
[37m[1m[2023-07-17 10:38:17,650][257371] Mean Reward across all agents: 714.4092362674616[0m
[37m[1m[2023-07-17 10:38:17,650][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:38:22,636][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:38:22,636][257371] Reward + Measures: [[600.40040255   0.3664       0.21280001   0.2069       0.1696
    2.59237647]
 [-78.46453934   0.34470001   0.3436       0.33710003   0.2529
    5.45301723]
 [389.00831696   0.27500001   0.2339       0.15400001   0.21690002
    2.02102828]
 ...
 [ 60.10148269   0.19570002   0.15900001   0.15390001   0.1171
    3.7320118 ]
 [303.94360925   0.2352       0.2175       0.22290002   0.22270003
    4.13314629]
 [277.13243106   0.29300001   0.32390001   0.33039999   0.27540001
    2.37874079]][0m
[37m[1m[2023-07-17 10:38:22,637][257371] Max Reward on eval: 1242.649829876423[0m
[37m[1m[2023-07-17 10:38:22,637][257371] Min Reward on eval: -78.46453934176824[0m
[37m[1m[2023-07-17 10:38:22,637][257371] Mean Reward across all agents: 204.08877656248234[0m
[37m[1m[2023-07-17 10:38:22,637][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:38:22,639][257371] mean_value=-2375.427669742255, max_value=349.56001413337196[0m
[37m[1m[2023-07-17 10:38:22,642][257371] New mean coefficients: [[ 0.22713143 -2.139978   -0.99516845 -1.0689257  -0.78639805 -0.9169692 ]][0m
[37m[1m[2023-07-17 10:38:22,643][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:38:31,668][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 10:38:31,668][257371] FPS: 425562.94[0m
[36m[2023-07-17 10:38:31,670][257371] itr=1078, itrs=2000, Progress: 53.90%[0m
[36m[2023-07-17 10:38:43,743][257371] train() took 11.97 seconds to complete[0m
[36m[2023-07-17 10:38:43,744][257371] FPS: 320860.14[0m
[36m[2023-07-17 10:38:48,129][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:38:48,130][257371] Reward + Measures: [[883.94143426   0.32064465   0.23386534   0.26605234   0.089072
    1.53182352]][0m
[37m[1m[2023-07-17 10:38:48,130][257371] Max Reward on eval: 883.9414342581565[0m
[37m[1m[2023-07-17 10:38:48,130][257371] Min Reward on eval: 883.9414342581565[0m
[37m[1m[2023-07-17 10:38:48,130][257371] Mean Reward across all agents: 883.9414342581565[0m
[37m[1m[2023-07-17 10:38:48,131][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:38:53,392][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:38:53,393][257371] Reward + Measures: [[ 46.61619409   0.15040001   0.14459999   0.1049       0.0852
    4.90333843]
 [202.95523218   0.28790003   0.20999999   0.2375       0.1409
    3.60305095]
 [ 39.04675109   0.17690001   0.23050001   0.2282       0.1797
    5.1074791 ]
 ...
 [115.41376278   0.25869998   0.25540003   0.227        0.24679999
    3.46908426]
 [367.09301662   0.233        0.2217       0.21790002   0.1344
    4.09665728]
 [ 32.36786226   0.1454       0.1109       0.0957       0.08710001
    4.90954733]][0m
[37m[1m[2023-07-17 10:38:53,393][257371] Max Reward on eval: 1028.518852255121[0m
[37m[1m[2023-07-17 10:38:53,394][257371] Min Reward on eval: -121.94107348555117[0m
[37m[1m[2023-07-17 10:38:53,394][257371] Mean Reward across all agents: 175.31974781668285[0m
[37m[1m[2023-07-17 10:38:53,394][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:38:53,396][257371] mean_value=-2139.3350904573444, max_value=122.22895554065906[0m
[37m[1m[2023-07-17 10:38:53,399][257371] New mean coefficients: [[ 0.64308643 -2.0835912  -2.016634   -0.54601884 -0.6674347  -0.6775772 ]][0m
[37m[1m[2023-07-17 10:38:53,400][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:39:02,366][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 10:39:02,366][257371] FPS: 428353.40[0m
[36m[2023-07-17 10:39:02,368][257371] itr=1079, itrs=2000, Progress: 53.95%[0m
[36m[2023-07-17 10:39:14,153][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 10:39:14,153][257371] FPS: 328812.12[0m
[36m[2023-07-17 10:39:18,511][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:39:18,511][257371] Reward + Measures: [[900.47375532   0.27816468   0.21830167   0.23985432   0.09296267
    1.49289894]][0m
[37m[1m[2023-07-17 10:39:18,512][257371] Max Reward on eval: 900.4737553173314[0m
[37m[1m[2023-07-17 10:39:18,512][257371] Min Reward on eval: 900.4737553173314[0m
[37m[1m[2023-07-17 10:39:18,512][257371] Mean Reward across all agents: 900.4737553173314[0m
[37m[1m[2023-07-17 10:39:18,512][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:39:23,491][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:39:23,492][257371] Reward + Measures: [[102.2361615    0.2156       0.18810001   0.2247       0.2545
    3.94397926]
 [270.28887888   0.2782       0.23469999   0.23720002   0.2823
    2.78125429]
 [ 95.15468499   0.2775       0.25900003   0.25959998   0.23649998
    3.85117388]
 ...
 [106.97060927   0.15620001   0.1286       0.1481       0.13499999
    4.8503418 ]
 [114.42464853   0.1894       0.26540002   0.18340002   0.2397
    3.93511653]
 [202.23533796   0.28390002   0.16000001   0.1948       0.13200001
    3.43841147]][0m
[37m[1m[2023-07-17 10:39:23,492][257371] Max Reward on eval: 1134.1559143246384[0m
[37m[1m[2023-07-17 10:39:23,493][257371] Min Reward on eval: -157.92804368454964[0m
[37m[1m[2023-07-17 10:39:23,493][257371] Mean Reward across all agents: 168.00458691851244[0m
[37m[1m[2023-07-17 10:39:23,493][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:39:23,495][257371] mean_value=-1986.6439795645947, max_value=170.80593959355224[0m
[37m[1m[2023-07-17 10:39:23,497][257371] New mean coefficients: [[ 0.9054046  -2.4914155  -1.4173527  -0.18598643 -0.31972298 -0.40068105]][0m
[37m[1m[2023-07-17 10:39:23,498][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:39:32,513][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 10:39:32,513][257371] FPS: 426034.78[0m
[36m[2023-07-17 10:39:32,516][257371] itr=1080, itrs=2000, Progress: 54.00%[0m
[37m[1m[2023-07-17 10:43:02,730][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001060[0m
[36m[2023-07-17 10:43:15,326][257371] train() took 11.90 seconds to complete[0m
[36m[2023-07-17 10:43:15,326][257371] FPS: 322800.46[0m
[36m[2023-07-17 10:43:19,528][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:43:19,534][257371] Reward + Measures: [[1014.84096404    0.263026      0.22299634    0.24031532    0.09420167
     1.48739064]][0m
[37m[1m[2023-07-17 10:43:19,534][257371] Max Reward on eval: 1014.8409640441566[0m
[37m[1m[2023-07-17 10:43:19,534][257371] Min Reward on eval: 1014.8409640441566[0m
[37m[1m[2023-07-17 10:43:19,535][257371] Mean Reward across all agents: 1014.8409640441566[0m
[37m[1m[2023-07-17 10:43:19,535][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:43:24,805][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:43:24,811][257371] Reward + Measures: [[100.46863503   0.40770003   0.26420003   0.4068       0.2217
    3.97664142]
 [123.14056279   0.32930002   0.16140001   0.30410001   0.18569998
    4.95064688]
 [ 39.52844119   0.12990001   0.1211       0.11470001   0.10520001
    5.24685717]
 ...
 [ 80.30027854   0.14070001   0.11400001   0.1195       0.14650001
    4.63948202]
 [ 28.3447179    0.2034       0.22070001   0.19319999   0.13259999
    4.6644001 ]
 [-32.0982529    0.1716       0.17490001   0.17150001   0.1229
    5.1639452 ]][0m
[37m[1m[2023-07-17 10:43:24,811][257371] Max Reward on eval: 1083.6856689750916[0m
[37m[1m[2023-07-17 10:43:24,811][257371] Min Reward on eval: -131.1320302397944[0m
[37m[1m[2023-07-17 10:43:24,812][257371] Mean Reward across all agents: 148.41044946071779[0m
[37m[1m[2023-07-17 10:43:24,812][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:43:24,814][257371] mean_value=-1717.110692904939, max_value=242.76869572185439[0m
[37m[1m[2023-07-17 10:43:24,816][257371] New mean coefficients: [[ 0.1335743  -2.929098   -1.0159849   0.09340909 -0.28913233 -0.03910726]][0m
[37m[1m[2023-07-17 10:43:24,817][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:43:33,855][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 10:43:33,856][257371] FPS: 424932.25[0m
[36m[2023-07-17 10:43:33,858][257371] itr=1081, itrs=2000, Progress: 54.05%[0m
[36m[2023-07-17 10:43:45,486][257371] train() took 11.52 seconds to complete[0m
[36m[2023-07-17 10:43:45,486][257371] FPS: 333364.99[0m
[36m[2023-07-17 10:43:49,793][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:43:49,794][257371] Reward + Measures: [[989.17238662   0.22661634   0.20433667   0.22408      0.09480932
    1.47595501]][0m
[37m[1m[2023-07-17 10:43:49,794][257371] Max Reward on eval: 989.172386622423[0m
[37m[1m[2023-07-17 10:43:49,794][257371] Min Reward on eval: 989.172386622423[0m
[37m[1m[2023-07-17 10:43:49,794][257371] Mean Reward across all agents: 989.172386622423[0m
[37m[1m[2023-07-17 10:43:49,795][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:43:54,760][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:43:54,760][257371] Reward + Measures: [[ -5.59832465   0.25400001   0.1473       0.23080002   0.23639999
    5.71677065]
 [ 50.0155475    0.18539999   0.17209999   0.18420002   0.1339
    4.16771173]
 [ 65.04966761   0.1663       0.13130002   0.152        0.14560001
    5.2786212 ]
 ...
 [290.14790577   0.40910003   0.28470001   0.3362       0.1714
    2.57990623]
 [  4.60763244   0.10500001   0.09640001   0.1019       0.1048
    5.62840366]
 [124.94506077   0.21999998   0.27360001   0.18719999   0.2494
    4.26031208]][0m
[37m[1m[2023-07-17 10:43:54,760][257371] Max Reward on eval: 803.7235755812377[0m
[37m[1m[2023-07-17 10:43:54,761][257371] Min Reward on eval: -90.2231039173901[0m
[37m[1m[2023-07-17 10:43:54,761][257371] Mean Reward across all agents: 136.7391743485195[0m
[37m[1m[2023-07-17 10:43:54,761][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:43:54,762][257371] mean_value=-2180.0838310117038, max_value=-22.71997440101717[0m
[36m[2023-07-17 10:43:54,765][257371] XNES is restarting with a new solution whose measures are [0.27900001 0.7177     0.70450002 0.5        5.18943024] and objective is 169.43689395710825[0m
[36m[2023-07-17 10:43:54,766][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 10:43:54,768][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 10:43:54,769][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:44:03,777][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 10:44:03,777][257371] FPS: 426369.34[0m
[36m[2023-07-17 10:44:03,779][257371] itr=1082, itrs=2000, Progress: 54.10%[0m
[36m[2023-07-17 10:44:15,452][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 10:44:15,452][257371] FPS: 332088.51[0m
[36m[2023-07-17 10:44:19,807][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:44:19,808][257371] Reward + Measures: [[-101.19667581    0.38103035    0.41713166    0.40983137    0.18753599
     5.41565561]][0m
[37m[1m[2023-07-17 10:44:19,808][257371] Max Reward on eval: -101.19667580851835[0m
[37m[1m[2023-07-17 10:44:19,808][257371] Min Reward on eval: -101.19667580851835[0m
[37m[1m[2023-07-17 10:44:19,808][257371] Mean Reward across all agents: -101.19667580851835[0m
[37m[1m[2023-07-17 10:44:19,809][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:44:24,691][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:44:24,692][257371] Reward + Measures: [[-1.61241243  0.25999999  0.24489999  0.2888      0.1736      5.48313284]
 [81.98723147  0.11740001  0.14310001  0.13870001  0.12580001  5.54232121]
 [ 1.68792952  0.48580003  0.53000003  0.377       0.24010003  6.50981665]
 ...
 [14.35946091  0.18770002  0.20290001  0.2103      0.1699      5.37073708]
 [87.40317464  0.23710003  0.35870004  0.34849998  0.2271      6.00404835]
 [39.83680807  0.1244      0.1464      0.1401      0.15000002  5.40002155]][0m
[37m[1m[2023-07-17 10:44:24,692][257371] Max Reward on eval: 443.51460191062654[0m
[37m[1m[2023-07-17 10:44:24,693][257371] Min Reward on eval: -115.70044687837363[0m
[37m[1m[2023-07-17 10:44:24,693][257371] Mean Reward across all agents: 15.126231508411623[0m
[37m[1m[2023-07-17 10:44:24,693][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:44:24,695][257371] mean_value=-355.3263242584911, max_value=320.22161873554603[0m
[37m[1m[2023-07-17 10:44:24,697][257371] New mean coefficients: [[-0.9444126  -0.9112607   1.9269438  -1.6804804  -1.001423   -0.84416497]][0m
[37m[1m[2023-07-17 10:44:24,698][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:44:33,697][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 10:44:33,697][257371] FPS: 426787.03[0m
[36m[2023-07-17 10:44:33,699][257371] itr=1083, itrs=2000, Progress: 54.15%[0m
[36m[2023-07-17 10:44:45,331][257371] train() took 11.52 seconds to complete[0m
[36m[2023-07-17 10:44:45,331][257371] FPS: 333202.10[0m
[36m[2023-07-17 10:44:49,619][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:44:49,619][257371] Reward + Measures: [[48.42423214  0.451781    0.51748365  0.44447836  0.21127202  5.54084635]][0m
[37m[1m[2023-07-17 10:44:49,619][257371] Max Reward on eval: 48.42423213683855[0m
[37m[1m[2023-07-17 10:44:49,620][257371] Min Reward on eval: 48.42423213683855[0m
[37m[1m[2023-07-17 10:44:49,620][257371] Mean Reward across all agents: 48.42423213683855[0m
[37m[1m[2023-07-17 10:44:49,620][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:44:54,572][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:44:54,572][257371] Reward + Measures: [[-2.5012748   0.12199999  0.15800001  0.14130001  0.1045      6.16965485]
 [27.82920861  0.11700001  0.1053      0.12460001  0.0758      6.60441589]
 [ 2.63542046  0.0981      0.18440002  0.17150001  0.1602      6.9262476 ]
 ...
 [42.55610085  0.1134      0.11730001  0.1033      0.0668      6.19691467]
 [ 9.11240946  0.21250001  0.21429999  0.2678      0.222       6.31414652]
 [29.73771902  0.36990005  0.36849999  0.1869      0.33829999  6.29718542]][0m
[37m[1m[2023-07-17 10:44:54,573][257371] Max Reward on eval: 251.2899585451931[0m
[37m[1m[2023-07-17 10:44:54,573][257371] Min Reward on eval: -131.7473496677354[0m
[37m[1m[2023-07-17 10:44:54,573][257371] Mean Reward across all agents: 6.373670845525902[0m
[37m[1m[2023-07-17 10:44:54,573][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:44:54,575][257371] mean_value=-332.56312469503524, max_value=15.701264590886069[0m
[37m[1m[2023-07-17 10:44:54,577][257371] New mean coefficients: [[ 0.6358253  -1.2321913   0.512082   -1.5863935  -0.44496292 -0.58776164]][0m
[37m[1m[2023-07-17 10:44:54,578][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:45:03,541][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 10:45:03,541][257371] FPS: 428520.23[0m
[36m[2023-07-17 10:45:03,543][257371] itr=1084, itrs=2000, Progress: 54.20%[0m
[36m[2023-07-17 10:45:15,275][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 10:45:15,275][257371] FPS: 330438.30[0m
[36m[2023-07-17 10:45:19,527][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:45:19,528][257371] Reward + Measures: [[31.11800656  0.26203299  0.18686299  0.25853232  0.131485    5.24666452]][0m
[37m[1m[2023-07-17 10:45:19,528][257371] Max Reward on eval: 31.118006563995813[0m
[37m[1m[2023-07-17 10:45:19,528][257371] Min Reward on eval: 31.118006563995813[0m
[37m[1m[2023-07-17 10:45:19,529][257371] Mean Reward across all agents: 31.118006563995813[0m
[37m[1m[2023-07-17 10:45:19,529][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:45:24,437][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:45:24,443][257371] Reward + Measures: [[-65.67217036   0.20870002   0.1601       0.16140001   0.1415
    5.85693026]
 [-26.49334149   0.14369999   0.1178       0.14490001   0.15020001
    6.78812265]
 [ 77.42524652   0.226        0.17209999   0.2457       0.0785
    6.84776783]
 ...
 [ -5.71476096   0.16200002   0.13250001   0.13419999   0.1073
    6.22355509]
 [ 32.91806654   0.1437       0.12539999   0.1442       0.1235
    6.35398817]
 [-23.49217785   0.1749       0.1497       0.15720001   0.15840001
    6.17436361]][0m
[37m[1m[2023-07-17 10:45:24,443][257371] Max Reward on eval: 331.3394181869924[0m
[37m[1m[2023-07-17 10:45:24,444][257371] Min Reward on eval: -204.1108231363818[0m
[37m[1m[2023-07-17 10:45:24,444][257371] Mean Reward across all agents: 20.014803769931433[0m
[37m[1m[2023-07-17 10:45:24,444][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:45:24,446][257371] mean_value=-273.61728068456324, max_value=77.43949513311983[0m
[37m[1m[2023-07-17 10:45:24,448][257371] New mean coefficients: [[ 2.304594   -0.9835309   0.23291475 -1.8055178   0.32385522 -0.69107425]][0m
[37m[1m[2023-07-17 10:45:24,449][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:45:33,384][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 10:45:33,384][257371] FPS: 429869.62[0m
[36m[2023-07-17 10:45:33,386][257371] itr=1085, itrs=2000, Progress: 54.25%[0m
[36m[2023-07-17 10:45:45,038][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 10:45:45,038][257371] FPS: 332645.03[0m
[36m[2023-07-17 10:45:49,281][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:45:49,281][257371] Reward + Measures: [[49.31449952  0.14051433  0.12657967  0.159375    0.10715366  5.81419182]][0m
[37m[1m[2023-07-17 10:45:49,281][257371] Max Reward on eval: 49.31449952353358[0m
[37m[1m[2023-07-17 10:45:49,282][257371] Min Reward on eval: 49.31449952353358[0m
[37m[1m[2023-07-17 10:45:49,282][257371] Mean Reward across all agents: 49.31449952353358[0m
[37m[1m[2023-07-17 10:45:49,282][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:45:54,259][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:45:54,259][257371] Reward + Measures: [[-28.30344889   0.33579999   0.20219998   0.3008       0.1621
    5.81764317]
 [-11.1406497    0.24610002   0.20810001   0.2339       0.1833
    6.38888502]
 [ -6.66052848   0.20820001   0.20770001   0.21870001   0.16470002
    6.53185272]
 ...
 [139.19585428   0.2033       0.36550003   0.35819998   0.33619997
    7.04849768]
 [ 74.4540904    0.2685       0.21879999   0.25909999   0.19149999
    5.79892302]
 [150.74341965   0.31850001   0.2667       0.38320002   0.1477
    6.21595573]][0m
[37m[1m[2023-07-17 10:45:54,259][257371] Max Reward on eval: 353.2051980313845[0m
[37m[1m[2023-07-17 10:45:54,260][257371] Min Reward on eval: -412.34238527454437[0m
[37m[1m[2023-07-17 10:45:54,260][257371] Mean Reward across all agents: 17.755697170891253[0m
[37m[1m[2023-07-17 10:45:54,260][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:45:54,261][257371] mean_value=-285.4823218625189, max_value=121.82627188964983[0m
[37m[1m[2023-07-17 10:45:54,264][257371] New mean coefficients: [[ 2.322272    0.37958252  0.38244814 -1.8263489   2.3983808  -1.0043818 ]][0m
[37m[1m[2023-07-17 10:45:54,265][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:46:03,290][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 10:46:03,290][257371] FPS: 425552.50[0m
[36m[2023-07-17 10:46:03,292][257371] itr=1086, itrs=2000, Progress: 54.30%[0m
[36m[2023-07-17 10:46:15,118][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 10:46:15,118][257371] FPS: 327718.27[0m
[36m[2023-07-17 10:46:19,375][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:46:19,375][257371] Reward + Measures: [[-17.94384982   0.310453     0.29494736   0.33586636   0.21374935
    6.26888418]][0m
[37m[1m[2023-07-17 10:46:19,376][257371] Max Reward on eval: -17.943849822992014[0m
[37m[1m[2023-07-17 10:46:19,376][257371] Min Reward on eval: -17.943849822992014[0m
[37m[1m[2023-07-17 10:46:19,376][257371] Mean Reward across all agents: -17.943849822992014[0m
[37m[1m[2023-07-17 10:46:19,376][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:46:24,623][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:46:24,624][257371] Reward + Measures: [[-81.05395396   0.36210001   0.31689999   0.39660001   0.36219999
    6.72404051]
 [-58.1231819    0.35569999   0.2888       0.38369998   0.31550002
    6.63913441]
 [-73.05657907   0.26879999   0.24249999   0.24700001   0.2395
    6.24538898]
 ...
 [ 44.45404394   0.32710001   0.24850002   0.37080002   0.30239999
    6.59732676]
 [-60.93693316   0.39449999   0.30140001   0.37760001   0.27760002
    6.15913868]
 [-82.97542623   0.42249998   0.22750001   0.42340001   0.27340001
    7.01103735]][0m
[37m[1m[2023-07-17 10:46:24,624][257371] Max Reward on eval: 121.05113679394125[0m
[37m[1m[2023-07-17 10:46:24,624][257371] Min Reward on eval: -225.39934725929052[0m
[37m[1m[2023-07-17 10:46:24,624][257371] Mean Reward across all agents: -22.938679080432532[0m
[37m[1m[2023-07-17 10:46:24,625][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:46:24,626][257371] mean_value=-287.6950924446953, max_value=107.04559183471618[0m
[37m[1m[2023-07-17 10:46:24,628][257371] New mean coefficients: [[ 2.5061228  -0.45704168  0.43932244 -2.1554825   1.1577245  -2.0852485 ]][0m
[37m[1m[2023-07-17 10:46:24,629][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:46:33,633][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 10:46:33,633][257371] FPS: 426556.04[0m
[36m[2023-07-17 10:46:33,636][257371] itr=1087, itrs=2000, Progress: 54.35%[0m
[36m[2023-07-17 10:46:45,445][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 10:46:45,445][257371] FPS: 328138.81[0m
[36m[2023-07-17 10:46:49,752][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:46:49,753][257371] Reward + Measures: [[-75.18567559   0.15699166   0.55975366   0.28169203   0.48774901
    6.06304693]][0m
[37m[1m[2023-07-17 10:46:49,753][257371] Max Reward on eval: -75.18567559482227[0m
[37m[1m[2023-07-17 10:46:49,753][257371] Min Reward on eval: -75.18567559482227[0m
[37m[1m[2023-07-17 10:46:49,753][257371] Mean Reward across all agents: -75.18567559482227[0m
[37m[1m[2023-07-17 10:46:49,754][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:46:54,750][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:46:54,750][257371] Reward + Measures: [[-46.85181933   0.1454       0.14920001   0.1346       0.13810001
    6.12889099]
 [-70.58896736   0.30130002   0.28150001   0.35670003   0.27129999
    6.63600016]
 [-62.03692581   0.13880001   0.1303       0.1461       0.1177
    6.64190912]
 ...
 [107.83368018   0.20490001   0.1646       0.18670002   0.14560001
    6.39720583]
 [ -4.31963915   0.1107       0.0811       0.09350001   0.07919999
    6.21837759]
 [-18.89342948   0.1177       0.1048       0.12230001   0.0882
    6.59547901]][0m
[37m[1m[2023-07-17 10:46:54,751][257371] Max Reward on eval: 312.62163454871626[0m
[37m[1m[2023-07-17 10:46:54,751][257371] Min Reward on eval: -189.35649103987961[0m
[37m[1m[2023-07-17 10:46:54,751][257371] Mean Reward across all agents: 16.126524376909376[0m
[37m[1m[2023-07-17 10:46:54,751][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:46:54,753][257371] mean_value=-269.70313094494793, max_value=127.34149356531572[0m
[37m[1m[2023-07-17 10:46:54,755][257371] New mean coefficients: [[ 2.715293  -1.0281337 -1.2231126 -2.5382204  1.1829742 -1.7652218]][0m
[37m[1m[2023-07-17 10:46:54,756][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:47:03,779][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 10:47:03,779][257371] FPS: 425658.37[0m
[36m[2023-07-17 10:47:03,782][257371] itr=1088, itrs=2000, Progress: 54.40%[0m
[36m[2023-07-17 10:47:15,474][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 10:47:15,474][257371] FPS: 331544.17[0m
[36m[2023-07-17 10:47:19,762][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:47:19,762][257371] Reward + Measures: [[58.39348147  0.21407832  0.22927299  0.19961633  0.222322    5.86492968]][0m
[37m[1m[2023-07-17 10:47:19,762][257371] Max Reward on eval: 58.39348147042144[0m
[37m[1m[2023-07-17 10:47:19,763][257371] Min Reward on eval: 58.39348147042144[0m
[37m[1m[2023-07-17 10:47:19,763][257371] Mean Reward across all agents: 58.39348147042144[0m
[37m[1m[2023-07-17 10:47:19,763][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:47:24,723][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:47:24,724][257371] Reward + Measures: [[ 42.32195158   0.1962       0.1928       0.2422       0.2052
    6.69255924]
 [149.22261078   0.28760001   0.37280002   0.1816       0.38300002
    6.93026352]
 [-42.00354747   0.30650002   0.20799999   0.28010002   0.15439999
    6.06877279]
 ...
 [ 55.14380067   0.15790001   0.1247       0.1416       0.19210002
    6.40021658]
 [ 17.71576167   0.138        0.1145       0.14980002   0.1145
    6.64387226]
 [-32.77180613   0.0973       0.28420001   0.26370001   0.27350003
    6.30087328]][0m
[37m[1m[2023-07-17 10:47:24,724][257371] Max Reward on eval: 472.4756642982364[0m
[37m[1m[2023-07-17 10:47:24,725][257371] Min Reward on eval: -174.825709341513[0m
[37m[1m[2023-07-17 10:47:24,725][257371] Mean Reward across all agents: 43.01785361275545[0m
[37m[1m[2023-07-17 10:47:24,725][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:47:24,727][257371] mean_value=-234.81373138823736, max_value=208.39618198013096[0m
[37m[1m[2023-07-17 10:47:24,729][257371] New mean coefficients: [[ 2.9011486  -1.0130659  -1.9435053  -3.599915   -0.26523614 -0.4907366 ]][0m
[37m[1m[2023-07-17 10:47:24,730][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:47:33,739][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 10:47:33,739][257371] FPS: 426321.17[0m
[36m[2023-07-17 10:47:33,742][257371] itr=1089, itrs=2000, Progress: 54.45%[0m
[36m[2023-07-17 10:47:45,431][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 10:47:45,431][257371] FPS: 331465.59[0m
[36m[2023-07-17 10:47:49,742][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:47:49,742][257371] Reward + Measures: [[24.33173155  0.17163965  0.183038    0.17738566  0.19873299  5.76531744]][0m
[37m[1m[2023-07-17 10:47:49,743][257371] Max Reward on eval: 24.331731554623964[0m
[37m[1m[2023-07-17 10:47:49,743][257371] Min Reward on eval: 24.331731554623964[0m
[37m[1m[2023-07-17 10:47:49,743][257371] Mean Reward across all agents: 24.331731554623964[0m
[37m[1m[2023-07-17 10:47:49,743][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:47:54,725][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:47:54,725][257371] Reward + Measures: [[ 47.98129778   0.0976       0.0949       0.10650001   0.0963
    6.19183064]
 [ 24.40661001   0.1116       0.1212       0.0909       0.15060002
    6.4442873 ]
 [  6.34314462   0.1125       0.11200001   0.1129       0.1182
    6.24949455]
 ...
 [ -8.03531864   0.1152       0.0944       0.1054       0.103
    6.36198187]
 [-28.48154996   0.18910001   0.1243       0.1464       0.15699999
    6.22312403]
 [ 45.68830602   0.1267       0.13749999   0.12160001   0.12990001
    6.24456263]][0m
[37m[1m[2023-07-17 10:47:54,725][257371] Max Reward on eval: 190.6517109597102[0m
[37m[1m[2023-07-17 10:47:54,726][257371] Min Reward on eval: -87.29778933357447[0m
[37m[1m[2023-07-17 10:47:54,726][257371] Mean Reward across all agents: 21.2175066923853[0m
[37m[1m[2023-07-17 10:47:54,726][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:47:54,727][257371] mean_value=-277.7757703036362, max_value=70.54735738312161[0m
[37m[1m[2023-07-17 10:47:54,730][257371] New mean coefficients: [[ 1.7550272  -0.8474479  -0.45996642 -4.8096747  -0.39125925 -0.4268239 ]][0m
[37m[1m[2023-07-17 10:47:54,731][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:48:03,853][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 10:48:03,854][257371] FPS: 421004.11[0m
[36m[2023-07-17 10:48:03,856][257371] itr=1090, itrs=2000, Progress: 54.50%[0m
[37m[1m[2023-07-17 10:51:33,196][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001070[0m
[36m[2023-07-17 10:51:45,642][257371] train() took 11.98 seconds to complete[0m
[36m[2023-07-17 10:51:45,642][257371] FPS: 320653.33[0m
[36m[2023-07-17 10:51:49,896][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:51:49,896][257371] Reward + Measures: [[5.11593473 0.17146267 0.12537266 0.17128736 0.13355733 5.64763927]][0m
[37m[1m[2023-07-17 10:51:49,896][257371] Max Reward on eval: 5.115934727646682[0m
[37m[1m[2023-07-17 10:51:49,897][257371] Min Reward on eval: 5.115934727646682[0m
[37m[1m[2023-07-17 10:51:49,897][257371] Mean Reward across all agents: 5.115934727646682[0m
[37m[1m[2023-07-17 10:51:49,897][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:51:54,853][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:51:54,914][257371] Reward + Measures: [[ -9.53734684   0.16320001   0.10640001   0.1399       0.123
    5.90033102]
 [-29.8265134    0.13599999   0.11059999   0.12710001   0.0819
    6.16140366]
 [ -5.00167504   0.1288       0.1217       0.12229999   0.1015
    6.53535795]
 ...
 [-29.3979859    0.0796       0.0801       0.0829       0.0824
    6.65189886]
 [ -8.18053085   0.0982       0.101        0.08220001   0.0717
    6.85245752]
 [-31.0864185    0.11920001   0.1019       0.098        0.1089
    6.97219038]][0m
[37m[1m[2023-07-17 10:51:54,914][257371] Max Reward on eval: 69.5627731859684[0m
[37m[1m[2023-07-17 10:51:54,914][257371] Min Reward on eval: -84.98674177825451[0m
[37m[1m[2023-07-17 10:51:54,914][257371] Mean Reward across all agents: -10.661427768563366[0m
[37m[1m[2023-07-17 10:51:54,915][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:51:54,916][257371] mean_value=-302.42668433967316, max_value=-191.6260837231518[0m
[36m[2023-07-17 10:51:54,918][257371] XNES is restarting with a new solution whose measures are [0.4513     0.5474     0.78750002 0.81350005 5.32130337] and objective is 200.27309002913535[0m
[36m[2023-07-17 10:51:54,919][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 10:51:54,922][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 10:51:54,923][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:52:03,953][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 10:52:03,954][257371] FPS: 425307.02[0m
[36m[2023-07-17 10:52:03,956][257371] itr=1091, itrs=2000, Progress: 54.55%[0m
[36m[2023-07-17 10:52:15,636][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 10:52:15,636][257371] FPS: 331800.53[0m
[36m[2023-07-17 10:52:19,923][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:52:19,923][257371] Reward + Measures: [[158.38282394   0.39461634   0.50027966   0.77259201   0.75240999
    5.01457167]][0m
[37m[1m[2023-07-17 10:52:19,923][257371] Max Reward on eval: 158.38282393611726[0m
[37m[1m[2023-07-17 10:52:19,924][257371] Min Reward on eval: 158.38282393611726[0m
[37m[1m[2023-07-17 10:52:19,924][257371] Mean Reward across all agents: 158.38282393611726[0m
[37m[1m[2023-07-17 10:52:19,924][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:52:25,118][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:52:25,119][257371] Reward + Measures: [[  87.97830186    0.53820002    0.26540002    0.72990006    0.70669997
     6.66120291]
 [ -22.28832933    0.18790001    0.67819995    0.20039999    0.78210002
     3.74529767]
 [ -82.41332016    0.33660004    0.24759999    0.37100002    0.0927
     4.26681614]
 ...
 [ 233.10486186    0.17150001    0.4436        0.44520003    0.42160001
     5.51526213]
 [  -0.84192872    0.29070002    0.27560002    0.36900002    0.1829
     4.80904007]
 [-146.40170693    0.20710002    0.41230002    0.24420002    0.45970002
     4.10867453]][0m
[37m[1m[2023-07-17 10:52:25,119][257371] Max Reward on eval: 665.6453380526276[0m
[37m[1m[2023-07-17 10:52:25,119][257371] Min Reward on eval: -599.8664550550282[0m
[37m[1m[2023-07-17 10:52:25,119][257371] Mean Reward across all agents: 8.973193327607099[0m
[37m[1m[2023-07-17 10:52:25,119][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:52:25,122][257371] mean_value=-402.06872637680686, max_value=307.8360293598432[0m
[37m[1m[2023-07-17 10:52:25,124][257371] New mean coefficients: [[-0.4574183  -0.49105632 -0.27715164 -1.5897508  -0.3252517  -0.84546816]][0m
[37m[1m[2023-07-17 10:52:25,125][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:52:34,211][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 10:52:34,211][257371] FPS: 422738.43[0m
[36m[2023-07-17 10:52:34,213][257371] itr=1092, itrs=2000, Progress: 54.60%[0m
[36m[2023-07-17 10:52:46,040][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 10:52:46,041][257371] FPS: 327620.21[0m
[36m[2023-07-17 10:52:50,303][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:52:50,304][257371] Reward + Measures: [[117.11506991   0.43935499   0.42676398   0.72489601   0.66399896
    4.8555336 ]][0m
[37m[1m[2023-07-17 10:52:50,304][257371] Max Reward on eval: 117.11506991293417[0m
[37m[1m[2023-07-17 10:52:50,304][257371] Min Reward on eval: 117.11506991293417[0m
[37m[1m[2023-07-17 10:52:50,304][257371] Mean Reward across all agents: 117.11506991293417[0m
[37m[1m[2023-07-17 10:52:50,305][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:52:55,221][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:52:55,222][257371] Reward + Measures: [[ 80.34361575   0.24990001   0.3441       0.2845       0.27860001
    5.40371084]
 [-26.121714     0.25389999   0.67770004   0.27700001   0.58650005
    6.06567907]
 [126.5935826    0.17810002   0.26460001   0.22129999   0.24290001
    5.00957298]
 ...
 [ 61.37779191   0.25110003   0.43109998   0.42430001   0.35190001
    3.8179574 ]
 [ 14.35451988   0.18970001   0.3303       0.2924       0.3479
    5.22784758]
 [-38.55835169   0.035        0.56310004   0.56420004   0.74090004
    5.76115561]][0m
[37m[1m[2023-07-17 10:52:55,222][257371] Max Reward on eval: 539.4315376251936[0m
[37m[1m[2023-07-17 10:52:55,222][257371] Min Reward on eval: -453.3024635299924[0m
[37m[1m[2023-07-17 10:52:55,222][257371] Mean Reward across all agents: 39.319715205975605[0m
[37m[1m[2023-07-17 10:52:55,223][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:52:55,226][257371] mean_value=-508.6121734557781, max_value=218.80927252309624[0m
[37m[1m[2023-07-17 10:52:55,228][257371] New mean coefficients: [[-0.52553463 -0.7025102   0.5513803  -1.0616446   0.44071347 -0.95653313]][0m
[37m[1m[2023-07-17 10:52:55,229][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:53:04,302][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 10:53:04,302][257371] FPS: 423338.52[0m
[36m[2023-07-17 10:53:04,304][257371] itr=1093, itrs=2000, Progress: 54.65%[0m
[36m[2023-07-17 10:53:15,990][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 10:53:15,990][257371] FPS: 331627.66[0m
[36m[2023-07-17 10:53:20,320][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:53:20,320][257371] Reward + Measures: [[105.49062906   0.44790471   0.41166601   0.68880892   0.62815768
    4.75735188]][0m
[37m[1m[2023-07-17 10:53:20,320][257371] Max Reward on eval: 105.49062906027628[0m
[37m[1m[2023-07-17 10:53:20,321][257371] Min Reward on eval: 105.49062906027628[0m
[37m[1m[2023-07-17 10:53:20,321][257371] Mean Reward across all agents: 105.49062906027628[0m
[37m[1m[2023-07-17 10:53:20,321][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:53:25,394][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:53:25,394][257371] Reward + Measures: [[ 12.18290087   0.40459999   0.38729998   0.43810001   0.33129999
    4.25046921]
 [ 54.0534574    0.12639999   0.19550002   0.1437       0.19939999
    5.38868666]
 [-58.7618878    0.21200001   0.35769996   0.3233       0.3238
    2.67580318]
 ...
 [ 36.31807149   0.45120001   0.41929999   0.43660003   0.42930004
    3.70542002]
 [  7.00225922   0.96390003   0.0347       0.9386       0.96429998
    7.07266235]
 [ -3.97468498   0.31909999   0.2852       0.32930002   0.27580002
    4.96938848]][0m
[37m[1m[2023-07-17 10:53:25,395][257371] Max Reward on eval: 493.33250142168254[0m
[37m[1m[2023-07-17 10:53:25,395][257371] Min Reward on eval: -299.2180537212524[0m
[37m[1m[2023-07-17 10:53:25,395][257371] Mean Reward across all agents: 26.34647609883631[0m
[37m[1m[2023-07-17 10:53:25,395][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:53:25,399][257371] mean_value=-623.759299687219, max_value=239.91401192139162[0m
[37m[1m[2023-07-17 10:53:25,402][257371] New mean coefficients: [[-0.449467   -1.5899631  -0.47913754 -1.3068179   1.4836171  -1.2577457 ]][0m
[37m[1m[2023-07-17 10:53:25,402][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:53:34,484][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 10:53:34,484][257371] FPS: 422932.46[0m
[36m[2023-07-17 10:53:34,486][257371] itr=1094, itrs=2000, Progress: 54.70%[0m
[36m[2023-07-17 10:53:46,255][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 10:53:46,255][257371] FPS: 329303.09[0m
[36m[2023-07-17 10:53:50,552][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:53:50,552][257371] Reward + Measures: [[74.27706928  0.68071532  0.31576467  0.70752865  0.30352899  3.74773884]][0m
[37m[1m[2023-07-17 10:53:50,552][257371] Max Reward on eval: 74.27706927504026[0m
[37m[1m[2023-07-17 10:53:50,553][257371] Min Reward on eval: 74.27706927504026[0m
[37m[1m[2023-07-17 10:53:50,553][257371] Mean Reward across all agents: 74.27706927504026[0m
[37m[1m[2023-07-17 10:53:50,553][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:53:55,508][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:53:55,509][257371] Reward + Measures: [[-39.56348611   0.34209999   0.26920003   0.37850001   0.42700002
    5.45423412]
 [ 44.92224327   0.1329       0.39120004   0.2757       0.41820002
    5.50650167]
 [ -2.53953017   0.16340001   0.19660001   0.24089999   0.23470001
    5.30182791]
 ...
 [ 18.32106912   0.3082       0.25980002   0.27450001   0.27449998
    4.43579102]
 [118.90916636   0.60480005   0.59709996   0.28910002   0.56459993
    5.33014917]
 [ 35.99751333   0.17479999   0.27250001   0.14490001   0.28240001
    5.94317627]][0m
[37m[1m[2023-07-17 10:53:55,509][257371] Max Reward on eval: 352.20532044712456[0m
[37m[1m[2023-07-17 10:53:55,509][257371] Min Reward on eval: -291.34077187888323[0m
[37m[1m[2023-07-17 10:53:55,509][257371] Mean Reward across all agents: 27.756282066436608[0m
[37m[1m[2023-07-17 10:53:55,509][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:53:55,511][257371] mean_value=-408.32259089657043, max_value=146.91951656930235[0m
[37m[1m[2023-07-17 10:53:55,514][257371] New mean coefficients: [[-0.6112665 -1.487594  -0.9678854 -1.1172374  1.3140757 -1.0551008]][0m
[37m[1m[2023-07-17 10:53:55,515][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:54:04,567][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 10:54:04,567][257371] FPS: 424288.00[0m
[36m[2023-07-17 10:54:04,569][257371] itr=1095, itrs=2000, Progress: 54.75%[0m
[36m[2023-07-17 10:54:16,413][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 10:54:16,413][257371] FPS: 327239.09[0m
[36m[2023-07-17 10:54:20,738][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:54:20,739][257371] Reward + Measures: [[76.2982364   0.64295232  0.25514632  0.6778416   0.36825964  3.72701359]][0m
[37m[1m[2023-07-17 10:54:20,739][257371] Max Reward on eval: 76.29823640194535[0m
[37m[1m[2023-07-17 10:54:20,739][257371] Min Reward on eval: 76.29823640194535[0m
[37m[1m[2023-07-17 10:54:20,739][257371] Mean Reward across all agents: 76.29823640194535[0m
[37m[1m[2023-07-17 10:54:20,740][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:54:25,698][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:54:25,699][257371] Reward + Measures: [[-373.48259449    0.0114        0.7245        0.82249993    0.8028
     5.80995655]
 [ 125.36652033    0.24629998    0.36199999    0.28749999    0.35600004
     4.48221302]
 [ 108.43451637    0.17560001    0.61630005    0.2043        0.65669996
     6.20159149]
 ...
 [ 325.58993041    0.55080003    0.43430001    0.61479998    0.3211
     5.73778439]
 [ 120.85827992    0.17209999    0.39040002    0.1811        0.4244
     4.1429553 ]
 [ 185.59943955    0.3057        0.53440005    0.59250003    0.6311
     6.32933474]][0m
[37m[1m[2023-07-17 10:54:25,699][257371] Max Reward on eval: 458.3337678759359[0m
[37m[1m[2023-07-17 10:54:25,699][257371] Min Reward on eval: -373.4825944874436[0m
[37m[1m[2023-07-17 10:54:25,700][257371] Mean Reward across all agents: 55.76509913301331[0m
[37m[1m[2023-07-17 10:54:25,700][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:54:25,704][257371] mean_value=-514.9426921574876, max_value=186.1004380293362[0m
[37m[1m[2023-07-17 10:54:25,707][257371] New mean coefficients: [[-0.4730682  -0.9938651  -0.13108873 -1.2460058   1.4951086  -0.5408865 ]][0m
[37m[1m[2023-07-17 10:54:25,708][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:54:34,712][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 10:54:34,713][257371] FPS: 426554.46[0m
[36m[2023-07-17 10:54:34,715][257371] itr=1096, itrs=2000, Progress: 54.80%[0m
[36m[2023-07-17 10:54:46,511][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 10:54:46,511][257371] FPS: 328636.15[0m
[36m[2023-07-17 10:54:50,851][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:54:50,852][257371] Reward + Measures: [[77.30016692  0.57159668  0.27119333  0.62080997  0.39919198  3.67408681]][0m
[37m[1m[2023-07-17 10:54:50,852][257371] Max Reward on eval: 77.30016691661378[0m
[37m[1m[2023-07-17 10:54:50,852][257371] Min Reward on eval: 77.30016691661378[0m
[37m[1m[2023-07-17 10:54:50,853][257371] Mean Reward across all agents: 77.30016691661378[0m
[37m[1m[2023-07-17 10:54:50,853][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:54:55,920][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:54:55,921][257371] Reward + Measures: [[225.35557624   0.0237       0.61309999   0.53969997   0.62160009
    6.26624155]
 [ 79.22400763   0.48640004   0.2974       0.6771       0.4578
    5.21457338]
 [  9.89735027   0.26229998   0.16140001   0.26149997   0.18529999
    5.56590748]
 ...
 [-40.14839203   0.17730002   0.14210001   0.18990001   0.1452
    5.54605436]
 [ 57.47599906   0.1305       0.13070001   0.13090001   0.1796
    5.52290392]
 [ 28.48826283   0.1214       0.101        0.08450001   0.12620001
    5.23664045]][0m
[37m[1m[2023-07-17 10:54:55,921][257371] Max Reward on eval: 510.49011046970264[0m
[37m[1m[2023-07-17 10:54:55,922][257371] Min Reward on eval: -200.18433858072385[0m
[37m[1m[2023-07-17 10:54:55,922][257371] Mean Reward across all agents: 47.731180857840975[0m
[37m[1m[2023-07-17 10:54:55,922][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:54:55,925][257371] mean_value=-556.6178090425241, max_value=338.6581901984472[0m
[37m[1m[2023-07-17 10:54:55,928][257371] New mean coefficients: [[-1.1535659  -1.246628   -0.46374515 -0.8798557   1.3254244  -0.04709691]][0m
[37m[1m[2023-07-17 10:54:55,929][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:55:05,020][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 10:55:05,021][257371] FPS: 422442.65[0m
[36m[2023-07-17 10:55:05,023][257371] itr=1097, itrs=2000, Progress: 54.85%[0m
[36m[2023-07-17 10:55:16,895][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 10:55:16,896][257371] FPS: 326479.23[0m
[36m[2023-07-17 10:55:21,213][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:55:21,214][257371] Reward + Measures: [[73.89645081  0.45820296  0.38949731  0.57867634  0.48566201  3.71617389]][0m
[37m[1m[2023-07-17 10:55:21,214][257371] Max Reward on eval: 73.89645080561839[0m
[37m[1m[2023-07-17 10:55:21,214][257371] Min Reward on eval: 73.89645080561839[0m
[37m[1m[2023-07-17 10:55:21,215][257371] Mean Reward across all agents: 73.89645080561839[0m
[37m[1m[2023-07-17 10:55:21,215][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:55:26,566][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:55:26,567][257371] Reward + Measures: [[ 99.64937709   0.1948       0.56980002   0.4955       0.49160001
    4.17469072]
 [ 46.21752588   0.10829999   0.62930006   0.4323       0.60910004
    5.21919012]
 [ 12.56553184   0.1195       0.31040001   0.1963       0.29080001
    4.23058844]
 ...
 [ -6.14980786   0.23409998   0.28650001   0.33990002   0.27690002
    4.59709883]
 [177.29095009   0.4797       0.83599997   0.22680001   0.76319999
    7.53672123]
 [ -9.65076336   0.57249999   0.37870002   0.49400002   0.72689998
    5.56221151]][0m
[37m[1m[2023-07-17 10:55:26,567][257371] Max Reward on eval: 490.3593411334674[0m
[37m[1m[2023-07-17 10:55:26,567][257371] Min Reward on eval: -351.41368768289686[0m
[37m[1m[2023-07-17 10:55:26,567][257371] Mean Reward across all agents: 39.96288682984029[0m
[37m[1m[2023-07-17 10:55:26,568][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:55:26,572][257371] mean_value=-604.1164928130922, max_value=418.4274437453363[0m
[37m[1m[2023-07-17 10:55:26,575][257371] New mean coefficients: [[ 0.08361423 -2.051709   -0.38940868  0.289195    2.0894446  -0.27169266]][0m
[37m[1m[2023-07-17 10:55:26,576][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:55:35,611][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 10:55:35,611][257371] FPS: 425127.15[0m
[36m[2023-07-17 10:55:35,613][257371] itr=1098, itrs=2000, Progress: 54.90%[0m
[36m[2023-07-17 10:55:47,519][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 10:55:47,519][257371] FPS: 325583.99[0m
[36m[2023-07-17 10:55:51,933][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:55:51,933][257371] Reward + Measures: [[72.25186758  0.28062364  0.55972332  0.61201066  0.6205467   3.95506644]][0m
[37m[1m[2023-07-17 10:55:51,933][257371] Max Reward on eval: 72.25186757721357[0m
[37m[1m[2023-07-17 10:55:51,934][257371] Min Reward on eval: 72.25186757721357[0m
[37m[1m[2023-07-17 10:55:51,934][257371] Mean Reward across all agents: 72.25186757721357[0m
[37m[1m[2023-07-17 10:55:51,934][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:55:56,982][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:55:56,983][257371] Reward + Measures: [[  41.00602461    0.58250004    0.38260001    0.51960003    0.35249999
     5.67634249]
 [   3.56140521    0.4488        0.77869999    0.38939998    0.81500006
     5.16440916]
 [ 140.93487751    0.78979999    0.72590005    0.76610005    0.0185
     6.05116749]
 ...
 [ -76.49654797    0.0674        0.5413        0.44189999    0.5485
     4.28555441]
 [ 305.39530611    0.8118        0.74940008    0.76440001    0.0173
     6.05469894]
 [-105.00330299    0.6832        0.61740005    0.6728        0.0472
     7.3208518 ]][0m
[37m[1m[2023-07-17 10:55:56,983][257371] Max Reward on eval: 727.6688346892595[0m
[37m[1m[2023-07-17 10:55:56,983][257371] Min Reward on eval: -314.6573061842006[0m
[37m[1m[2023-07-17 10:55:56,983][257371] Mean Reward across all agents: 49.52274740542983[0m
[37m[1m[2023-07-17 10:55:56,984][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:55:56,986][257371] mean_value=-529.3662427548645, max_value=152.04729581479796[0m
[37m[1m[2023-07-17 10:55:56,989][257371] New mean coefficients: [[-0.2003201  -1.8613245  -1.2319756   0.58850133  2.2471242  -0.45970663]][0m
[37m[1m[2023-07-17 10:55:56,990][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:56:06,072][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 10:56:06,072][257371] FPS: 422919.19[0m
[36m[2023-07-17 10:56:06,074][257371] itr=1099, itrs=2000, Progress: 54.95%[0m
[36m[2023-07-17 10:56:17,996][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 10:56:17,996][257371] FPS: 325112.94[0m
[36m[2023-07-17 10:56:22,285][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:56:22,286][257371] Reward + Measures: [[90.28099961  0.10502967  0.78963929  0.6133737   0.83020574  3.49859023]][0m
[37m[1m[2023-07-17 10:56:22,286][257371] Max Reward on eval: 90.28099960819299[0m
[37m[1m[2023-07-17 10:56:22,286][257371] Min Reward on eval: 90.28099960819299[0m
[37m[1m[2023-07-17 10:56:22,287][257371] Mean Reward across all agents: 90.28099960819299[0m
[37m[1m[2023-07-17 10:56:22,287][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:56:27,255][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 10:56:27,255][257371] Reward + Measures: [[-62.94223867   0.3493       0.36029997   0.3804       0.47129998
    3.88534975]
 [312.47791771   0.45469999   0.1261       0.45770001   0.29239997
    5.01581144]
 [ 76.82429827   0.31500003   0.34470001   0.46610004   0.27690002
    3.81641841]
 ...
 [ 44.77229688   0.42020002   0.1565       0.41120002   0.2263
    5.49243164]
 [ 30.13064247   0.4364       0.26230001   0.46919999   0.2994
    6.30887365]
 [-15.76797273   0.1153       0.1383       0.14470001   0.0884
    5.17733526]][0m
[37m[1m[2023-07-17 10:56:27,256][257371] Max Reward on eval: 341.56947849662976[0m
[37m[1m[2023-07-17 10:56:27,256][257371] Min Reward on eval: -204.98863865518942[0m
[37m[1m[2023-07-17 10:56:27,256][257371] Mean Reward across all agents: 66.46716472818974[0m
[37m[1m[2023-07-17 10:56:27,256][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 10:56:27,260][257371] mean_value=-495.8537220519238, max_value=222.05443924489026[0m
[37m[1m[2023-07-17 10:56:27,262][257371] New mean coefficients: [[-0.60914487 -1.6411663  -2.6779294   0.3083252   1.8456584  -0.07305837]][0m
[37m[1m[2023-07-17 10:56:27,263][257371] Moving the mean solution point...[0m
[36m[2023-07-17 10:56:36,215][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 10:56:36,216][257371] FPS: 429022.01[0m
[36m[2023-07-17 10:56:36,218][257371] itr=1100, itrs=2000, Progress: 55.00%[0m
[37m[1m[2023-07-17 11:00:04,617][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001080[0m
[36m[2023-07-17 11:00:17,243][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-17 11:00:17,243][257371] FPS: 323505.36[0m
[36m[2023-07-17 11:00:21,488][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:00:21,488][257371] Reward + Measures: [[102.21468872   0.08253234   0.82818002   0.66947633   0.86474073
    3.54957509]][0m
[37m[1m[2023-07-17 11:00:21,489][257371] Max Reward on eval: 102.21468871831422[0m
[37m[1m[2023-07-17 11:00:21,489][257371] Min Reward on eval: 102.21468871831422[0m
[37m[1m[2023-07-17 11:00:21,489][257371] Mean Reward across all agents: 102.21468871831422[0m
[37m[1m[2023-07-17 11:00:21,489][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:00:26,469][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:00:26,470][257371] Reward + Measures: [[-36.85164622   0.1142       0.13870001   0.2          0.25260001
    4.93373871]
 [-33.28626154   0.1283       0.12800001   0.1605       0.18300001
    5.57119226]
 [-25.58213623   0.5571       0.435        0.44260001   0.61020005
    4.83890104]
 ...
 [ 57.81633095   0.1788       0.30679998   0.27040002   0.3353
    5.05456877]
 [101.39346992   0.09249999   0.75720006   0.75330001   0.74870008
    6.41539764]
 [ 87.12697375   0.23740001   0.36559999   0.28480002   0.39360002
    4.92366362]][0m
[37m[1m[2023-07-17 11:00:26,470][257371] Max Reward on eval: 380.3912973176455[0m
[37m[1m[2023-07-17 11:00:26,470][257371] Min Reward on eval: -586.8430151652544[0m
[37m[1m[2023-07-17 11:00:26,470][257371] Mean Reward across all agents: -48.66783252425401[0m
[37m[1m[2023-07-17 11:00:26,470][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:00:26,474][257371] mean_value=-693.0006734387523, max_value=210.65163252626257[0m
[37m[1m[2023-07-17 11:00:26,477][257371] New mean coefficients: [[-0.6087534  -1.050731   -2.3006668   1.4834158   0.57314265 -0.05812896]][0m
[37m[1m[2023-07-17 11:00:26,478][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:00:35,589][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 11:00:35,589][257371] FPS: 421530.73[0m
[36m[2023-07-17 11:00:35,591][257371] itr=1101, itrs=2000, Progress: 55.05%[0m
[36m[2023-07-17 11:00:47,388][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 11:00:47,388][257371] FPS: 328497.98[0m
[36m[2023-07-17 11:00:51,653][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:00:51,654][257371] Reward + Measures: [[94.07884326  0.087364    0.82350099  0.71776098  0.862921    3.5403161 ]][0m
[37m[1m[2023-07-17 11:00:51,654][257371] Max Reward on eval: 94.07884325814096[0m
[37m[1m[2023-07-17 11:00:51,654][257371] Min Reward on eval: 94.07884325814096[0m
[37m[1m[2023-07-17 11:00:51,654][257371] Mean Reward across all agents: 94.07884325814096[0m
[37m[1m[2023-07-17 11:00:51,655][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:00:56,610][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:00:56,611][257371] Reward + Measures: [[-34.66867711   0.1646       0.13430001   0.14240001   0.169
    4.30585575]
 [ 36.23320219   0.26890001   0.15120001   0.23320003   0.1882
    3.90958643]
 [102.44844174   0.22409999   0.3619       0.50019997   0.48830006
    6.76802778]
 ...
 [ 53.35243609   0.20109999   0.30039999   0.33989999   0.29859996
    4.6928072 ]
 [162.40556998   0.0692       0.80389994   0.70020002   0.8526001
    5.32703543]
 [127.72263935   0.25840002   0.22140001   0.40650001   0.33389997
    5.06989098]][0m
[37m[1m[2023-07-17 11:00:56,611][257371] Max Reward on eval: 464.389993686229[0m
[37m[1m[2023-07-17 11:00:56,611][257371] Min Reward on eval: -141.14501141887158[0m
[37m[1m[2023-07-17 11:00:56,612][257371] Mean Reward across all agents: 51.41726266418166[0m
[37m[1m[2023-07-17 11:00:56,612][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:00:56,615][257371] mean_value=-1294.4897395457351, max_value=264.3869537663833[0m
[37m[1m[2023-07-17 11:00:56,618][257371] New mean coefficients: [[-0.85532224  0.19212437 -1.686636    0.6903457  -1.5324825  -0.86906815]][0m
[37m[1m[2023-07-17 11:00:56,619][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:01:05,685][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 11:01:05,685][257371] FPS: 423639.14[0m
[36m[2023-07-17 11:01:05,687][257371] itr=1102, itrs=2000, Progress: 55.10%[0m
[36m[2023-07-17 11:01:17,302][257371] train() took 11.51 seconds to complete[0m
[36m[2023-07-17 11:01:17,302][257371] FPS: 333682.75[0m
[36m[2023-07-17 11:01:21,578][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:01:21,578][257371] Reward + Measures: [[73.29074915  0.120479    0.76595628  0.68903965  0.81810766  3.45923162]][0m
[37m[1m[2023-07-17 11:01:21,578][257371] Max Reward on eval: 73.290749146561[0m
[37m[1m[2023-07-17 11:01:21,579][257371] Min Reward on eval: 73.290749146561[0m
[37m[1m[2023-07-17 11:01:21,579][257371] Mean Reward across all agents: 73.290749146561[0m
[37m[1m[2023-07-17 11:01:21,579][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:01:26,821][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:01:26,822][257371] Reward + Measures: [[ 70.91377326   0.46410003   0.3637       0.44949999   0.12810001
    4.05255604]
 [ 76.66018629   0.45979998   0.2658       0.64100003   0.61390001
    5.654109  ]
 [172.33829892   0.39449999   0.17709999   0.45280001   0.4312
    5.29123306]
 ...
 [ 71.84809829   0.63770002   0.16140001   0.70559996   0.59420007
    3.97600102]
 [ 90.91581924   0.60750002   0.1867       0.68640006   0.43400002
    3.97380257]
 [258.11860037   0.1415       0.73550004   0.51700002   0.70170003
    6.31919718]][0m
[37m[1m[2023-07-17 11:01:26,822][257371] Max Reward on eval: 304.6695365641266[0m
[37m[1m[2023-07-17 11:01:26,822][257371] Min Reward on eval: -257.54286001678554[0m
[37m[1m[2023-07-17 11:01:26,823][257371] Mean Reward across all agents: 82.38841685897475[0m
[37m[1m[2023-07-17 11:01:26,823][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:01:26,828][257371] mean_value=-615.2959176801668, max_value=527.2862452377242[0m
[37m[1m[2023-07-17 11:01:26,831][257371] New mean coefficients: [[-0.6088336 -0.4062754 -1.6557992  2.0131528 -2.0570748 -0.8847009]][0m
[37m[1m[2023-07-17 11:01:26,832][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:01:35,793][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 11:01:35,793][257371] FPS: 428587.59[0m
[36m[2023-07-17 11:01:35,796][257371] itr=1103, itrs=2000, Progress: 55.15%[0m
[36m[2023-07-17 11:01:47,506][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 11:01:47,506][257371] FPS: 330940.67[0m
[36m[2023-07-17 11:01:51,692][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:01:51,693][257371] Reward + Measures: [[25.02673003  0.20827633  0.61699933  0.61527634  0.7118727   3.23733068]][0m
[37m[1m[2023-07-17 11:01:51,693][257371] Max Reward on eval: 25.02673003084613[0m
[37m[1m[2023-07-17 11:01:51,693][257371] Min Reward on eval: 25.02673003084613[0m
[37m[1m[2023-07-17 11:01:51,693][257371] Mean Reward across all agents: 25.02673003084613[0m
[37m[1m[2023-07-17 11:01:51,694][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:01:56,668][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:01:56,668][257371] Reward + Measures: [[ 222.99792682    0.17209999    0.19359998    0.25310001    0.21460001
     4.66606855]
 [ 117.05881862    0.18449999    0.22689998    0.26540002    0.2441
     3.70336342]
 [  75.99663876    0.48699999    0.25150001    0.62120003    0.59119999
     2.82727599]
 ...
 [  44.31041093    0.30609998    0.17310001    0.33290002    0.2931
     3.2116096 ]
 [-300.09849266    0.40019998    0.34150001    0.29770002    0.37830001
     3.41635752]
 [  61.17372791    0.47539997    0.18990001    0.50040001    0.17459999
     4.79787588]][0m
[37m[1m[2023-07-17 11:01:56,668][257371] Max Reward on eval: 380.4866533262655[0m
[37m[1m[2023-07-17 11:01:56,669][257371] Min Reward on eval: -313.0471973452717[0m
[37m[1m[2023-07-17 11:01:56,669][257371] Mean Reward across all agents: 37.74741487133377[0m
[37m[1m[2023-07-17 11:01:56,669][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:01:56,672][257371] mean_value=-969.8893709445696, max_value=597.0077305879095[0m
[37m[1m[2023-07-17 11:01:56,675][257371] New mean coefficients: [[-0.4850983  -0.5701013  -0.95423436  2.6086369  -1.7561979  -1.3576568 ]][0m
[37m[1m[2023-07-17 11:01:56,676][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:02:05,693][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 11:02:05,693][257371] FPS: 425946.58[0m
[36m[2023-07-17 11:02:05,695][257371] itr=1104, itrs=2000, Progress: 55.20%[0m
[36m[2023-07-17 11:02:17,498][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 11:02:17,498][257371] FPS: 328476.70[0m
[36m[2023-07-17 11:02:21,855][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:02:21,861][257371] Reward + Measures: [[14.31358732  0.28426132  0.50700766  0.60741633  0.62653702  3.14565229]][0m
[37m[1m[2023-07-17 11:02:21,861][257371] Max Reward on eval: 14.313587317796928[0m
[37m[1m[2023-07-17 11:02:21,861][257371] Min Reward on eval: 14.313587317796928[0m
[37m[1m[2023-07-17 11:02:21,862][257371] Mean Reward across all agents: 14.313587317796928[0m
[37m[1m[2023-07-17 11:02:21,862][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:02:26,941][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:02:26,947][257371] Reward + Measures: [[-11.22426749   0.49800006   0.2158       0.48629999   0.3048
    3.4439621 ]
 [-55.57081965   0.2016       0.2863       0.33100003   0.39210001
    6.17521477]
 [226.83759547   0.14119999   0.62620002   0.61899996   0.65950006
    5.72893524]
 ...
 [111.34331776   0.72640002   0.48329997   0.75489998   0.29730001
    3.26885676]
 [ 47.3170104    0.30939999   0.1689       0.31060001   0.2431
    4.78042316]
 [165.83789925   0.4492       0.25130004   0.60360003   0.55730003
    2.87481809]][0m
[37m[1m[2023-07-17 11:02:26,947][257371] Max Reward on eval: 372.44679240426046[0m
[37m[1m[2023-07-17 11:02:26,947][257371] Min Reward on eval: -185.07104615219868[0m
[37m[1m[2023-07-17 11:02:26,948][257371] Mean Reward across all agents: 32.383035002254786[0m
[37m[1m[2023-07-17 11:02:26,948][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:02:26,951][257371] mean_value=-725.5490305308952, max_value=530.3018452722937[0m
[37m[1m[2023-07-17 11:02:26,953][257371] New mean coefficients: [[ 0.29128337  0.45648974  0.97227156  2.0969534  -2.3253908  -1.2734631 ]][0m
[37m[1m[2023-07-17 11:02:26,954][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:02:35,997][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 11:02:35,998][257371] FPS: 424707.84[0m
[36m[2023-07-17 11:02:36,000][257371] itr=1105, itrs=2000, Progress: 55.25%[0m
[36m[2023-07-17 11:02:47,723][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 11:02:47,723][257371] FPS: 330591.96[0m
[36m[2023-07-17 11:02:52,043][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:02:52,043][257371] Reward + Measures: [[9.73197282 0.4085893  0.34625834 0.60431838 0.49334797 3.00624204]][0m
[37m[1m[2023-07-17 11:02:52,044][257371] Max Reward on eval: 9.73197282265117[0m
[37m[1m[2023-07-17 11:02:52,044][257371] Min Reward on eval: 9.73197282265117[0m
[37m[1m[2023-07-17 11:02:52,044][257371] Mean Reward across all agents: 9.73197282265117[0m
[37m[1m[2023-07-17 11:02:52,044][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:02:57,039][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:02:57,040][257371] Reward + Measures: [[  58.50981407    0.14790002    0.71889997    0.68270004    0.72399998
     3.48996902]
 [   0.49962589    0.3682        0.13350001    0.42809996    0.28889999
     3.57927823]
 [  -5.11200156    0.2626        0.55440009    0.4041        0.67640001
     2.93718982]
 ...
 [ -23.19219812    0.43560001    0.37480003    0.49179998    0.149
     3.68434405]
 [-138.51500998    0.396         0.3258        0.39000002    0.53140002
     2.84646916]
 [ -49.66292859    0.54250002    0.49189997    0.58160001    0.24879999
     4.26136541]][0m
[37m[1m[2023-07-17 11:02:57,040][257371] Max Reward on eval: 226.11101530659943[0m
[37m[1m[2023-07-17 11:02:57,040][257371] Min Reward on eval: -246.58961346975994[0m
[37m[1m[2023-07-17 11:02:57,041][257371] Mean Reward across all agents: -10.188213405705145[0m
[37m[1m[2023-07-17 11:02:57,046][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:02:57,048][257371] mean_value=-716.8155360368096, max_value=215.50996298508667[0m
[37m[1m[2023-07-17 11:02:57,051][257371] New mean coefficients: [[-0.26510185  0.36456805  0.7469613   2.1635454  -1.2901832  -0.14738226]][0m
[37m[1m[2023-07-17 11:02:57,052][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:03:06,032][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 11:03:06,032][257371] FPS: 427708.27[0m
[36m[2023-07-17 11:03:06,034][257371] itr=1106, itrs=2000, Progress: 55.30%[0m
[36m[2023-07-17 11:03:17,672][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 11:03:17,673][257371] FPS: 333077.55[0m
[36m[2023-07-17 11:03:21,956][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:03:21,961][257371] Reward + Measures: [[6.50467495 0.48217797 0.29216269 0.61566001 0.41585132 2.94650745]][0m
[37m[1m[2023-07-17 11:03:21,962][257371] Max Reward on eval: 6.504674950696402[0m
[37m[1m[2023-07-17 11:03:21,962][257371] Min Reward on eval: 6.504674950696402[0m
[37m[1m[2023-07-17 11:03:21,962][257371] Mean Reward across all agents: 6.504674950696402[0m
[37m[1m[2023-07-17 11:03:21,962][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:03:26,946][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:03:26,947][257371] Reward + Measures: [[ 48.45784967   0.47149998   0.0846       0.64780003   0.27480003
    4.98704672]
 [ 29.0722009    0.30950001   0.62769997   0.80629998   0.66309994
    3.36670876]
 [ -6.92172041   0.52290004   0.26230001   0.59649998   0.4429
    2.97464633]
 ...
 [300.69657898   0.0556       0.70480001   0.73439997   0.86700004
    3.57122111]
 [ 92.57565833   0.48620006   0.1613       0.51570004   0.51440001
    3.23737192]
 [ 60.72773141   0.28670001   0.20440002   0.34190002   0.2888
    3.60913014]][0m
[37m[1m[2023-07-17 11:03:26,947][257371] Max Reward on eval: 537.1986312747001[0m
[37m[1m[2023-07-17 11:03:26,947][257371] Min Reward on eval: -187.10706206671892[0m
[37m[1m[2023-07-17 11:03:26,948][257371] Mean Reward across all agents: 56.90007835508774[0m
[37m[1m[2023-07-17 11:03:26,948][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:03:26,954][257371] mean_value=-456.4544051013853, max_value=327.6332223757089[0m
[37m[1m[2023-07-17 11:03:26,957][257371] New mean coefficients: [[-0.3790002   1.2076503   0.5704108   1.9925339  -0.9737002   0.73931396]][0m
[37m[1m[2023-07-17 11:03:26,957][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:03:35,962][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 11:03:35,963][257371] FPS: 426505.44[0m
[36m[2023-07-17 11:03:35,965][257371] itr=1107, itrs=2000, Progress: 55.35%[0m
[36m[2023-07-17 11:03:47,763][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 11:03:47,763][257371] FPS: 328516.64[0m
[36m[2023-07-17 11:03:52,144][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:03:52,144][257371] Reward + Measures: [[9.59768405 0.54107934 0.32341999 0.66268671 0.37481734 2.97530913]][0m
[37m[1m[2023-07-17 11:03:52,145][257371] Max Reward on eval: 9.59768404807629[0m
[37m[1m[2023-07-17 11:03:52,145][257371] Min Reward on eval: 9.59768404807629[0m
[37m[1m[2023-07-17 11:03:52,145][257371] Mean Reward across all agents: 9.59768404807629[0m
[37m[1m[2023-07-17 11:03:52,146][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:03:57,211][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:03:57,216][257371] Reward + Measures: [[-41.30560195   0.38200003   0.63749999   0.2237       0.64490002
    3.26234508]
 [ 75.56392614   0.68990004   0.55850005   0.62639999   0.69589996
    3.84001541]
 [-34.90991377   0.12890001   0.1987       0.1355       0.19759999
    4.9562993 ]
 ...
 [-62.36608643   0.22070001   0.24200001   0.24749999   0.28240001
    3.11214471]
 [-37.65401508   0.32910001   0.42459998   0.29380003   0.44450003
    3.13135791]
 [ -4.95513015   0.2631       0.35590002   0.26120001   0.37330002
    4.36864138]][0m
[37m[1m[2023-07-17 11:03:57,217][257371] Max Reward on eval: 400.77831986588427[0m
[37m[1m[2023-07-17 11:03:57,217][257371] Min Reward on eval: -456.7151422351599[0m
[37m[1m[2023-07-17 11:03:57,217][257371] Mean Reward across all agents: 16.052614668513634[0m
[37m[1m[2023-07-17 11:03:57,217][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:03:57,221][257371] mean_value=-850.9491096013147, max_value=251.07392066184502[0m
[37m[1m[2023-07-17 11:03:57,224][257371] New mean coefficients: [[1.3991605  1.1149769  1.2827723  2.2604122  0.10661238 0.20777857]][0m
[37m[1m[2023-07-17 11:03:57,225][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:04:06,308][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 11:04:06,308][257371] FPS: 422861.59[0m
[36m[2023-07-17 11:04:06,310][257371] itr=1108, itrs=2000, Progress: 55.40%[0m
[36m[2023-07-17 11:04:18,247][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 11:04:18,247][257371] FPS: 324683.21[0m
[36m[2023-07-17 11:04:22,607][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:04:22,613][257371] Reward + Measures: [[27.21026384  0.63324398  0.35015532  0.72328562  0.31124634  2.91716576]][0m
[37m[1m[2023-07-17 11:04:22,613][257371] Max Reward on eval: 27.210263839585547[0m
[37m[1m[2023-07-17 11:04:22,614][257371] Min Reward on eval: 27.210263839585547[0m
[37m[1m[2023-07-17 11:04:22,614][257371] Mean Reward across all agents: 27.210263839585547[0m
[37m[1m[2023-07-17 11:04:22,614][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:04:27,938][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:04:27,945][257371] Reward + Measures: [[ 37.0910551    0.48569998   0.32650003   0.4165       0.38100001
    3.37555623]
 [ 54.09573346   0.53200001   0.2155       0.51700002   0.29389998
    2.69699454]
 [-88.55855413   0.82299995   0.77459997   0.80159998   0.1007
    3.54171872]
 ...
 [  8.50642443   0.39649999   0.4355       0.47220001   0.16879998
    4.14143896]
 [-27.93811471   0.34639999   0.28999999   0.3786       0.22189999
    4.03966141]
 [-48.41797043   0.13850002   0.13079999   0.1858       0.20730002
    4.22972536]][0m
[37m[1m[2023-07-17 11:04:27,945][257371] Max Reward on eval: 489.31128108482807[0m
[37m[1m[2023-07-17 11:04:27,946][257371] Min Reward on eval: -232.62833182420582[0m
[37m[1m[2023-07-17 11:04:27,946][257371] Mean Reward across all agents: 40.709551218296134[0m
[37m[1m[2023-07-17 11:04:27,946][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:04:27,949][257371] mean_value=-647.9607057910385, max_value=340.5960283833288[0m
[37m[1m[2023-07-17 11:04:27,952][257371] New mean coefficients: [[ 0.98755354  0.7820838  -0.00281334  1.6290524  -0.897523    0.23627707]][0m
[37m[1m[2023-07-17 11:04:27,952][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:04:36,930][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 11:04:36,931][257371] FPS: 427801.31[0m
[36m[2023-07-17 11:04:36,933][257371] itr=1109, itrs=2000, Progress: 55.45%[0m
[36m[2023-07-17 11:04:48,665][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 11:04:48,666][257371] FPS: 330428.94[0m
[36m[2023-07-17 11:04:52,993][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:04:52,994][257371] Reward + Measures: [[43.48618122  0.72311401  0.409592    0.78426677  0.23037699  2.91516137]][0m
[37m[1m[2023-07-17 11:04:52,994][257371] Max Reward on eval: 43.48618121856392[0m
[37m[1m[2023-07-17 11:04:52,994][257371] Min Reward on eval: 43.48618121856392[0m
[37m[1m[2023-07-17 11:04:52,995][257371] Mean Reward across all agents: 43.48618121856392[0m
[37m[1m[2023-07-17 11:04:52,995][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:04:57,966][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:04:57,967][257371] Reward + Measures: [[  70.09189965    0.52340001    0.46720001    0.61290002    0.64969999
     5.68959188]
 [-134.76554635    0.87019998    0.1962        0.87090009    0.58050007
     6.230546  ]
 [  89.67092257    0.48370001    0.53850001    0.58929998    0.2335
     5.70699072]
 ...
 [ -93.27150371    0.37139997    0.33860001    0.2561        0.25600001
     5.61563349]
 [  89.54572868    0.56590003    0.44020006    0.5776        0.54319996
     6.82591772]
 [  43.12878684    0.49770004    0.13340001    0.60869998    0.24010001
     3.73573279]][0m
[37m[1m[2023-07-17 11:04:57,967][257371] Max Reward on eval: 271.3218996678479[0m
[37m[1m[2023-07-17 11:04:57,967][257371] Min Reward on eval: -372.35793851166966[0m
[37m[1m[2023-07-17 11:04:57,967][257371] Mean Reward across all agents: 25.250144885547307[0m
[37m[1m[2023-07-17 11:04:57,968][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:04:57,972][257371] mean_value=-393.2921158445383, max_value=414.2477010488138[0m
[37m[1m[2023-07-17 11:04:57,974][257371] New mean coefficients: [[ 0.7249034   0.84622383  1.1132835   1.8181548  -0.34873605 -0.1794874 ]][0m
[37m[1m[2023-07-17 11:04:57,975][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:05:07,056][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 11:05:07,056][257371] FPS: 422964.54[0m
[36m[2023-07-17 11:05:07,058][257371] itr=1110, itrs=2000, Progress: 55.50%[0m
[37m[1m[2023-07-17 11:08:36,245][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001090[0m
[36m[2023-07-17 11:08:48,391][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 11:08:48,391][257371] FPS: 331373.95[0m
[36m[2023-07-17 11:08:52,624][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:08:52,625][257371] Reward + Measures: [[53.54161447  0.78197402  0.49679932  0.83257663  0.189886    2.90831256]][0m
[37m[1m[2023-07-17 11:08:52,625][257371] Max Reward on eval: 53.54161447323236[0m
[37m[1m[2023-07-17 11:08:52,625][257371] Min Reward on eval: 53.54161447323236[0m
[37m[1m[2023-07-17 11:08:52,625][257371] Mean Reward across all agents: 53.54161447323236[0m
[37m[1m[2023-07-17 11:08:52,626][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:08:57,686][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:08:57,692][257371] Reward + Measures: [[196.05365989   0.2843       0.22319999   0.45180002   0.33250001
    3.88591456]
 [166.33126164   0.79400003   0.15250002   0.77160001   0.46960002
    4.92321014]
 [ 86.97334873   0.18099999   0.36840001   0.41819999   0.30600002
    6.44851637]
 ...
 [136.01459367   0.2481       0.3488       0.32430002   0.34329998
    4.67577124]
 [100.51016379   0.53500003   0.37690002   0.51880002   0.31729999
    5.13837767]
 [152.93082326   0.49460003   0.20610002   0.4684       0.2201
    5.21709585]][0m
[37m[1m[2023-07-17 11:08:57,692][257371] Max Reward on eval: 553.0182032909245[0m
[37m[1m[2023-07-17 11:08:57,692][257371] Min Reward on eval: -340.9353637821972[0m
[37m[1m[2023-07-17 11:08:57,692][257371] Mean Reward across all agents: 82.83203284624311[0m
[37m[1m[2023-07-17 11:08:57,693][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:08:57,698][257371] mean_value=-509.4812988532674, max_value=228.65824286661518[0m
[37m[1m[2023-07-17 11:08:57,701][257371] New mean coefficients: [[ 1.4438088   0.5009645   1.3666133   1.4750032  -0.81747013 -0.26902008]][0m
[37m[1m[2023-07-17 11:08:57,702][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:09:06,633][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 11:09:06,633][257371] FPS: 430098.71[0m
[36m[2023-07-17 11:09:06,635][257371] itr=1111, itrs=2000, Progress: 55.55%[0m
[36m[2023-07-17 11:09:18,398][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 11:09:18,398][257371] FPS: 329533.49[0m
[36m[2023-07-17 11:09:22,746][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:09:22,747][257371] Reward + Measures: [[65.12686658  0.83250827  0.55936134  0.86833632  0.14827034  2.91162586]][0m
[37m[1m[2023-07-17 11:09:22,747][257371] Max Reward on eval: 65.12686657614864[0m
[37m[1m[2023-07-17 11:09:22,747][257371] Min Reward on eval: 65.12686657614864[0m
[37m[1m[2023-07-17 11:09:22,748][257371] Mean Reward across all agents: 65.12686657614864[0m
[37m[1m[2023-07-17 11:09:22,748][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:09:27,669][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:09:27,669][257371] Reward + Measures: [[-157.35363337    0.90250009    0.0463        0.82959998    0.57140005
     4.78574371]
 [  90.3316536     0.4686        0.45660001    0.45070001    0.35480002
     4.22122431]
 [ -46.95784109    0.67060006    0.51969999    0.51660001    0.2472
     4.02233076]
 ...
 [  12.81061471    0.421         0.29620001    0.40880004    0.33390003
     2.9477253 ]
 [   3.75241459    0.48119998    0.55559999    0.52710003    0.29010001
     3.55399561]
 [  28.92186449    0.60429996    0.41840002    0.5467        0.0841
     4.5662117 ]][0m
[37m[1m[2023-07-17 11:09:27,670][257371] Max Reward on eval: 269.62987803416326[0m
[37m[1m[2023-07-17 11:09:27,670][257371] Min Reward on eval: -590.0274791905656[0m
[37m[1m[2023-07-17 11:09:27,670][257371] Mean Reward across all agents: 27.803949764671653[0m
[37m[1m[2023-07-17 11:09:27,670][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:09:27,676][257371] mean_value=-255.19843917859276, max_value=290.43971993260175[0m
[37m[1m[2023-07-17 11:09:27,678][257371] New mean coefficients: [[ 1.6451306   1.2022331   1.3657303   1.1530688  -0.29034275 -0.5991956 ]][0m
[37m[1m[2023-07-17 11:09:27,679][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:09:36,586][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-17 11:09:36,587][257371] FPS: 431199.08[0m
[36m[2023-07-17 11:09:36,589][257371] itr=1112, itrs=2000, Progress: 55.60%[0m
[36m[2023-07-17 11:09:48,303][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 11:09:48,304][257371] FPS: 330915.60[0m
[36m[2023-07-17 11:09:52,611][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:09:52,611][257371] Reward + Measures: [[72.60326803  0.86740768  0.6009143   0.89342332  0.12418433  2.9076786 ]][0m
[37m[1m[2023-07-17 11:09:52,612][257371] Max Reward on eval: 72.603268031887[0m
[37m[1m[2023-07-17 11:09:52,612][257371] Min Reward on eval: 72.603268031887[0m
[37m[1m[2023-07-17 11:09:52,612][257371] Mean Reward across all agents: 72.603268031887[0m
[37m[1m[2023-07-17 11:09:52,612][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:09:57,597][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:09:57,598][257371] Reward + Measures: [[ 48.87091048   0.3019       0.1921       0.37220001   0.2843
    4.87506008]
 [ -3.57916663   0.43330002   0.3768       0.52130002   0.3299
    5.08560514]
 [ -7.2638063    0.20350002   0.2431       0.255        0.32960001
    4.6617732 ]
 ...
 [127.64199005   0.29190001   0.31920001   0.27150002   0.3888
    4.02249002]
 [ 94.54578983   0.35300002   0.20650001   0.38030002   0.25040004
    3.55622363]
 [ 59.43428393   0.40889999   0.40529999   0.5808       0.29169998
    4.02467108]][0m
[37m[1m[2023-07-17 11:09:57,598][257371] Max Reward on eval: 648.3568706433289[0m
[37m[1m[2023-07-17 11:09:57,598][257371] Min Reward on eval: -73.12524670800194[0m
[37m[1m[2023-07-17 11:09:57,599][257371] Mean Reward across all agents: 87.14682381046569[0m
[37m[1m[2023-07-17 11:09:57,599][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:09:57,604][257371] mean_value=-600.2387109118681, max_value=409.93231851501764[0m
[37m[1m[2023-07-17 11:09:57,606][257371] New mean coefficients: [[ 2.0285566   0.5465707   0.91787827  0.6657199  -0.48982713 -0.5416092 ]][0m
[37m[1m[2023-07-17 11:09:57,607][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:10:06,655][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 11:10:06,655][257371] FPS: 424495.72[0m
[36m[2023-07-17 11:10:06,658][257371] itr=1113, itrs=2000, Progress: 55.65%[0m
[36m[2023-07-17 11:10:18,497][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 11:10:18,498][257371] FPS: 327383.99[0m
[36m[2023-07-17 11:10:22,707][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:10:22,707][257371] Reward + Measures: [[77.89877927  0.88928801  0.63426     0.90984392  0.10159867  2.8291347 ]][0m
[37m[1m[2023-07-17 11:10:22,708][257371] Max Reward on eval: 77.89877926673037[0m
[37m[1m[2023-07-17 11:10:22,708][257371] Min Reward on eval: 77.89877926673037[0m
[37m[1m[2023-07-17 11:10:22,708][257371] Mean Reward across all agents: 77.89877926673037[0m
[37m[1m[2023-07-17 11:10:22,708][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:10:27,686][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:10:27,687][257371] Reward + Measures: [[ 22.34711749   0.30559999   0.17390001   0.33050004   0.1847
    5.75620365]
 [-11.86132784   0.23369999   0.27239999   0.21529999   0.23899999
    4.22554779]
 [ 66.15400466   0.667        0.0916       0.70359999   0.3687
    5.09052277]
 ...
 [ 88.53817317   0.69779998   0.045        0.72490001   0.65410006
    5.46673965]
 [ 51.25071607   0.66400003   0.13770001   0.68660003   0.30590001
    4.14025211]
 [ -2.60124372   0.63210005   0.1715       0.61969995   0.21140002
    3.99552083]][0m
[37m[1m[2023-07-17 11:10:27,687][257371] Max Reward on eval: 339.0451597739011[0m
[37m[1m[2023-07-17 11:10:27,688][257371] Min Reward on eval: -241.05926444393117[0m
[37m[1m[2023-07-17 11:10:27,688][257371] Mean Reward across all agents: 36.23400998236678[0m
[37m[1m[2023-07-17 11:10:27,688][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:10:27,692][257371] mean_value=-616.0612448386112, max_value=415.28101211895085[0m
[37m[1m[2023-07-17 11:10:27,695][257371] New mean coefficients: [[ 0.8545941   0.2936748  -0.8465483   0.2052204  -0.37404406  0.27942896]][0m
[37m[1m[2023-07-17 11:10:27,696][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:10:36,642][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 11:10:36,642][257371] FPS: 429304.62[0m
[36m[2023-07-17 11:10:36,644][257371] itr=1114, itrs=2000, Progress: 55.70%[0m
[36m[2023-07-17 11:10:48,331][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 11:10:48,331][257371] FPS: 331611.64[0m
[36m[2023-07-17 11:10:52,682][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:10:52,682][257371] Reward + Measures: [[87.47707088  0.90174234  0.640387    0.91670197  0.09749467  2.91467714]][0m
[37m[1m[2023-07-17 11:10:52,683][257371] Max Reward on eval: 87.47707087534492[0m
[37m[1m[2023-07-17 11:10:52,683][257371] Min Reward on eval: 87.47707087534492[0m
[37m[1m[2023-07-17 11:10:52,683][257371] Mean Reward across all agents: 87.47707087534492[0m
[37m[1m[2023-07-17 11:10:52,684][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:10:57,690][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:10:57,691][257371] Reward + Measures: [[-46.15260859   0.62550002   0.24010001   0.61379999   0.1428
    3.28979468]
 [ 23.63136399   0.81899995   0.42150003   0.83840001   0.0841
    3.10106254]
 [ 32.63133782   0.43289995   0.34710002   0.43099999   0.14610001
    3.47068834]
 ...
 [ 28.3119203    0.4145       0.41449997   0.4474       0.2484
    3.0852263 ]
 [ -4.69724455   0.3644       0.2113       0.4154       0.27429998
    5.72889948]
 [ 56.66275178   0.49200001   0.20810001   0.45809999   0.25750002
    3.55972934]][0m
[37m[1m[2023-07-17 11:10:57,691][257371] Max Reward on eval: 289.0802835658193[0m
[37m[1m[2023-07-17 11:10:57,691][257371] Min Reward on eval: -210.8350415895693[0m
[37m[1m[2023-07-17 11:10:57,691][257371] Mean Reward across all agents: 7.454654345931634[0m
[37m[1m[2023-07-17 11:10:57,692][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:10:57,695][257371] mean_value=-492.3905498331494, max_value=520.9015596736316[0m
[37m[1m[2023-07-17 11:10:57,698][257371] New mean coefficients: [[-0.25486887  0.6991409   0.0868122   0.5965494   0.37638754  0.47002083]][0m
[37m[1m[2023-07-17 11:10:57,699][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:11:06,736][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 11:11:06,737][257371] FPS: 424990.35[0m
[36m[2023-07-17 11:11:06,739][257371] itr=1115, itrs=2000, Progress: 55.75%[0m
[36m[2023-07-17 11:11:18,550][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 11:11:18,550][257371] FPS: 328122.21[0m
[36m[2023-07-17 11:11:22,900][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:11:22,900][257371] Reward + Measures: [[82.5484212   0.90573126  0.63082367  0.91796499  0.105684    2.98308682]][0m
[37m[1m[2023-07-17 11:11:22,900][257371] Max Reward on eval: 82.54842119755247[0m
[37m[1m[2023-07-17 11:11:22,901][257371] Min Reward on eval: 82.54842119755247[0m
[37m[1m[2023-07-17 11:11:22,901][257371] Mean Reward across all agents: 82.54842119755247[0m
[37m[1m[2023-07-17 11:11:22,901][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:11:27,869][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:11:27,870][257371] Reward + Measures: [[ 64.69282113   0.39479998   0.3917       0.4061       0.33770001
    5.1774292 ]
 [ 60.10160698   0.49530002   0.25409999   0.58599997   0.43879995
    2.98263168]
 [ 62.25723073   0.69020003   0.33199999   0.74060005   0.12800001
    4.13884592]
 ...
 [-77.96691127   0.1981       0.20860003   0.19860001   0.19960003
    4.34718847]
 [ 33.6720151    0.40840003   0.33810002   0.45950004   0.3617
    5.94807434]
 [ 30.29643516   0.38730001   0.24700001   0.40540001   0.21540001
    3.65527225]][0m
[37m[1m[2023-07-17 11:11:27,870][257371] Max Reward on eval: 269.92548555135727[0m
[37m[1m[2023-07-17 11:11:27,870][257371] Min Reward on eval: -286.2715900376206[0m
[37m[1m[2023-07-17 11:11:27,870][257371] Mean Reward across all agents: 33.95029304601845[0m
[37m[1m[2023-07-17 11:11:27,871][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:11:27,874][257371] mean_value=-648.8215704159688, max_value=335.2656361218212[0m
[37m[1m[2023-07-17 11:11:27,877][257371] New mean coefficients: [[-0.4195402  -0.3446784   0.06185831  0.42955947  0.20955865  0.20345035]][0m
[37m[1m[2023-07-17 11:11:27,878][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:11:36,905][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 11:11:36,906][257371] FPS: 425430.27[0m
[36m[2023-07-17 11:11:36,908][257371] itr=1116, itrs=2000, Progress: 55.80%[0m
[36m[2023-07-17 11:11:48,603][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 11:11:48,603][257371] FPS: 331463.21[0m
[36m[2023-07-17 11:11:52,850][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:11:52,851][257371] Reward + Measures: [[75.94908594  0.90339273  0.61499536  0.91790265  0.12656067  3.11745071]][0m
[37m[1m[2023-07-17 11:11:52,851][257371] Max Reward on eval: 75.94908593973446[0m
[37m[1m[2023-07-17 11:11:52,851][257371] Min Reward on eval: 75.94908593973446[0m
[37m[1m[2023-07-17 11:11:52,851][257371] Mean Reward across all agents: 75.94908593973446[0m
[37m[1m[2023-07-17 11:11:52,852][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:11:57,856][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:11:57,856][257371] Reward + Measures: [[62.61502825  0.68290007  0.4052      0.62400001  0.0922      3.71862197]
 [13.48961231  0.14579999  0.15350001  0.14909999  0.19170001  5.27007914]
 [44.57339171  0.72229999  0.60409999  0.72999996  0.0631      3.59328961]
 ...
 [68.18061928  0.80289996  0.46470004  0.79500002  0.1868      4.41225386]
 [24.42633968  0.33759999  0.27790001  0.37480003  0.27220002  3.82719111]
 [25.77962735  0.53940004  0.61849999  0.56300002  0.6627      3.70338488]][0m
[37m[1m[2023-07-17 11:11:57,857][257371] Max Reward on eval: 387.228538762033[0m
[37m[1m[2023-07-17 11:11:57,857][257371] Min Reward on eval: -219.19060843512415[0m
[37m[1m[2023-07-17 11:11:57,857][257371] Mean Reward across all agents: 13.83320051613052[0m
[37m[1m[2023-07-17 11:11:57,857][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:11:57,861][257371] mean_value=-640.7084860912238, max_value=278.0244238090361[0m
[37m[1m[2023-07-17 11:11:57,864][257371] New mean coefficients: [[-0.8499005  -0.18502365 -0.21507062  0.12556198  0.2771651   0.37912497]][0m
[37m[1m[2023-07-17 11:11:57,865][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:12:06,807][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 11:12:06,808][257371] FPS: 429503.15[0m
[36m[2023-07-17 11:12:06,810][257371] itr=1117, itrs=2000, Progress: 55.85%[0m
[36m[2023-07-17 11:12:18,413][257371] train() took 11.50 seconds to complete[0m
[36m[2023-07-17 11:12:18,413][257371] FPS: 333971.56[0m
[36m[2023-07-17 11:12:22,692][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:12:22,692][257371] Reward + Measures: [[75.05940226  0.8910073   0.63677132  0.91952401  0.13081266  3.21712112]][0m
[37m[1m[2023-07-17 11:12:22,692][257371] Max Reward on eval: 75.05940226068958[0m
[37m[1m[2023-07-17 11:12:22,693][257371] Min Reward on eval: 75.05940226068958[0m
[37m[1m[2023-07-17 11:12:22,693][257371] Mean Reward across all agents: 75.05940226068958[0m
[37m[1m[2023-07-17 11:12:22,693][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:12:27,669][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:12:27,670][257371] Reward + Measures: [[54.30760912  0.30150002  0.38499999  0.36100003  0.4488      4.99930525]
 [10.66766228  0.23210001  0.35779998  0.50560004  0.41959998  7.25076389]
 [75.68246021  0.33180004  0.3202      0.36989999  0.133       5.10342264]
 ...
 [ 7.30043505  0.2436      0.54490006  0.65020001  0.51229995  5.64908457]
 [44.60471083  0.37579998  0.33800003  0.64300007  0.60440004  4.16259289]
 [42.42994009  0.40260002  0.34740001  0.42990002  0.39129999  3.66904521]][0m
[37m[1m[2023-07-17 11:12:27,670][257371] Max Reward on eval: 343.0893494759686[0m
[37m[1m[2023-07-17 11:12:27,670][257371] Min Reward on eval: -187.70576349487527[0m
[37m[1m[2023-07-17 11:12:27,670][257371] Mean Reward across all agents: 33.60849139130446[0m
[37m[1m[2023-07-17 11:12:27,670][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:12:27,674][257371] mean_value=-478.3493076467476, max_value=386.31294826384[0m
[37m[1m[2023-07-17 11:12:27,677][257371] New mean coefficients: [[-0.60293484 -0.27875322  0.7062392   0.12394556 -0.11155459  0.24901654]][0m
[37m[1m[2023-07-17 11:12:27,678][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:12:36,678][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 11:12:36,679][257371] FPS: 426716.05[0m
[36m[2023-07-17 11:12:36,681][257371] itr=1118, itrs=2000, Progress: 55.90%[0m
[36m[2023-07-17 11:12:48,409][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 11:12:48,409][257371] FPS: 330538.47[0m
[36m[2023-07-17 11:12:52,714][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:12:52,714][257371] Reward + Measures: [[68.05212448  0.88157696  0.62775999  0.91390502  0.13804266  3.21929097]][0m
[37m[1m[2023-07-17 11:12:52,715][257371] Max Reward on eval: 68.0521244824765[0m
[37m[1m[2023-07-17 11:12:52,715][257371] Min Reward on eval: 68.0521244824765[0m
[37m[1m[2023-07-17 11:12:52,715][257371] Mean Reward across all agents: 68.0521244824765[0m
[37m[1m[2023-07-17 11:12:52,715][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:12:57,983][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:12:57,983][257371] Reward + Measures: [[  21.59998444    0.7069        0.48920003    0.69629997    0.0654
     4.87697268]
 [  -7.22178381    0.6767        0.1777        0.64560002    0.2277
     4.4036088 ]
 [-106.82694315    0.4883        0.2701        0.49380001    0.30500004
     3.43673491]
 ...
 [  56.84820447    0.71960002    0.1574        0.75000006    0.1762
     4.3983283 ]
 [  39.67239096    0.3757        0.26750001    0.3567        0.2863
     3.49391031]
 [-242.70567398    0.50780004    0.2863        0.36699998    0.25909999
     4.85673285]][0m
[37m[1m[2023-07-17 11:12:57,984][257371] Max Reward on eval: 330.14759396407754[0m
[37m[1m[2023-07-17 11:12:57,984][257371] Min Reward on eval: -501.416772839427[0m
[37m[1m[2023-07-17 11:12:57,984][257371] Mean Reward across all agents: 9.932510091551006[0m
[37m[1m[2023-07-17 11:12:57,984][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:12:57,988][257371] mean_value=-670.8188187215432, max_value=312.83532866814085[0m
[37m[1m[2023-07-17 11:12:57,991][257371] New mean coefficients: [[-1.28317     0.51639134  0.12000269 -0.63828456  0.08724767  0.28722677]][0m
[37m[1m[2023-07-17 11:12:57,992][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:13:07,001][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 11:13:07,001][257371] FPS: 426318.19[0m
[36m[2023-07-17 11:13:07,003][257371] itr=1119, itrs=2000, Progress: 55.95%[0m
[36m[2023-07-17 11:13:18,844][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 11:13:18,844][257371] FPS: 327389.31[0m
[36m[2023-07-17 11:13:23,146][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:13:23,146][257371] Reward + Measures: [[63.41753711  0.88382065  0.64370966  0.91536301  0.13255766  3.22898054]][0m
[37m[1m[2023-07-17 11:13:23,146][257371] Max Reward on eval: 63.417537108362694[0m
[37m[1m[2023-07-17 11:13:23,147][257371] Min Reward on eval: 63.417537108362694[0m
[37m[1m[2023-07-17 11:13:23,147][257371] Mean Reward across all agents: 63.417537108362694[0m
[37m[1m[2023-07-17 11:13:23,147][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:13:28,187][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:13:28,187][257371] Reward + Measures: [[ -8.91649317   0.4276       0.24650002   0.44720003   0.26550001
    2.31258202]
 [-53.3430634    0.51630002   0.42120001   0.58880007   0.191
    3.23084497]
 [-49.17693881   0.73139995   0.1235       0.67189997   0.4061
    2.90048146]
 ...
 [-56.75423303   0.46750003   0.4585       0.57730001   0.2879
    2.37573314]
 [169.40861797   0.2757       0.78170007   0.77959996   0.67730004
    5.30856037]
 [ 90.08494819   0.53590006   0.43940002   0.66350001   0.30319998
    3.18272328]][0m
[37m[1m[2023-07-17 11:13:28,187][257371] Max Reward on eval: 234.45979928737506[0m
[37m[1m[2023-07-17 11:13:28,188][257371] Min Reward on eval: -374.63539885673674[0m
[37m[1m[2023-07-17 11:13:28,188][257371] Mean Reward across all agents: 50.8716211902459[0m
[37m[1m[2023-07-17 11:13:28,188][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:13:28,193][257371] mean_value=-341.42016298286524, max_value=361.0211966993197[0m
[37m[1m[2023-07-17 11:13:28,196][257371] New mean coefficients: [[-1.0806185   1.3732674   0.8524905  -0.12815756 -0.1339809   0.2491311 ]][0m
[37m[1m[2023-07-17 11:13:28,197][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:13:37,309][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 11:13:37,309][257371] FPS: 421516.11[0m
[36m[2023-07-17 11:13:37,311][257371] itr=1120, itrs=2000, Progress: 56.00%[0m
[37m[1m[2023-07-17 11:17:06,573][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001100[0m
[36m[2023-07-17 11:17:19,040][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 11:17:19,040][257371] FPS: 325442.31[0m
[36m[2023-07-17 11:17:23,328][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:17:23,329][257371] Reward + Measures: [[51.16572165  0.87013525  0.64308399  0.90991503  0.14195099  3.27764511]][0m
[37m[1m[2023-07-17 11:17:23,329][257371] Max Reward on eval: 51.165721647464935[0m
[37m[1m[2023-07-17 11:17:23,329][257371] Min Reward on eval: 51.165721647464935[0m
[37m[1m[2023-07-17 11:17:23,330][257371] Mean Reward across all agents: 51.165721647464935[0m
[37m[1m[2023-07-17 11:17:23,330][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:17:28,509][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:17:28,510][257371] Reward + Measures: [[ 130.57283007    0.29719999    0.2967        0.3633        0.317
     4.85620975]
 [ 241.67238759    0.06600001    0.52590001    0.54809999    0.55330002
     5.71478796]
 [-108.75103302    0.6286        0.56919998    0.56989998    0.57969999
     3.07241416]
 ...
 [ 135.96492648    0.22239999    0.42430001    0.447         0.43850002
     4.05486059]
 [  -2.15980065    0.0958        0.2617        0.211         0.1965
     3.95940256]
 [ 296.85560225    0.06769999    0.83900005    0.85179996    0.86899996
     4.06981134]][0m
[37m[1m[2023-07-17 11:17:28,510][257371] Max Reward on eval: 491.6044693149044[0m
[37m[1m[2023-07-17 11:17:28,510][257371] Min Reward on eval: -222.44842906136765[0m
[37m[1m[2023-07-17 11:17:28,511][257371] Mean Reward across all agents: 45.41304333228051[0m
[37m[1m[2023-07-17 11:17:28,511][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:17:28,515][257371] mean_value=-644.2420420387573, max_value=353.3425046673266[0m
[37m[1m[2023-07-17 11:17:28,517][257371] New mean coefficients: [[-0.7210876   1.1212802   0.00532001 -0.2777635  -0.56471646  1.0709554 ]][0m
[37m[1m[2023-07-17 11:17:28,518][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:17:37,459][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 11:17:37,459][257371] FPS: 429565.46[0m
[36m[2023-07-17 11:17:37,461][257371] itr=1121, itrs=2000, Progress: 56.05%[0m
[36m[2023-07-17 11:17:49,509][257371] train() took 11.94 seconds to complete[0m
[36m[2023-07-17 11:17:49,509][257371] FPS: 321650.42[0m
[36m[2023-07-17 11:17:53,797][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:17:53,798][257371] Reward + Measures: [[50.48013694  0.88999838  0.64945692  0.91592526  0.12197266  3.29686689]][0m
[37m[1m[2023-07-17 11:17:53,798][257371] Max Reward on eval: 50.480136943950086[0m
[37m[1m[2023-07-17 11:17:53,798][257371] Min Reward on eval: 50.480136943950086[0m
[37m[1m[2023-07-17 11:17:53,798][257371] Mean Reward across all agents: 50.480136943950086[0m
[37m[1m[2023-07-17 11:17:53,799][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:17:58,799][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:17:58,800][257371] Reward + Measures: [[-176.40974663    0.70360005    0.14979999    0.68040001    0.26040003
     2.79629493]
 [ -37.742638      0.7823        0.5478        0.75529999    0.0815
     3.31071544]
 [ 125.37037132    0.3448        0.3576        0.43309999    0.46940002
     4.52218771]
 ...
 [ -54.60790308    0.71380001    0.36090001    0.62720007    0.0913
     3.75155759]
 [ -20.22623566    0.49770004    0.14600001    0.528         0.414
     4.1573081 ]
 [-197.56432861    0.40960002    0.27309999    0.45170003    0.29249999
     4.3106823 ]][0m
[37m[1m[2023-07-17 11:17:58,800][257371] Max Reward on eval: 230.57361221741886[0m
[37m[1m[2023-07-17 11:17:58,800][257371] Min Reward on eval: -212.60752463974057[0m
[37m[1m[2023-07-17 11:17:58,801][257371] Mean Reward across all agents: 14.77772015837583[0m
[37m[1m[2023-07-17 11:17:58,801][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:17:58,803][257371] mean_value=-380.96307519637395, max_value=101.12605575927012[0m
[37m[1m[2023-07-17 11:17:58,806][257371] New mean coefficients: [[-0.33190244  0.9955861  -0.14970297  0.0092555  -0.7739266   1.1276779 ]][0m
[37m[1m[2023-07-17 11:17:58,807][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:18:07,790][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 11:18:07,791][257371] FPS: 427516.41[0m
[36m[2023-07-17 11:18:07,793][257371] itr=1122, itrs=2000, Progress: 56.10%[0m
[36m[2023-07-17 11:18:19,452][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 11:18:19,453][257371] FPS: 332535.98[0m
[36m[2023-07-17 11:18:23,730][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:18:23,730][257371] Reward + Measures: [[50.42499493  0.90190363  0.68426055  0.92268258  0.09799767  3.31938219]][0m
[37m[1m[2023-07-17 11:18:23,730][257371] Max Reward on eval: 50.42499493493409[0m
[37m[1m[2023-07-17 11:18:23,731][257371] Min Reward on eval: 50.42499493493409[0m
[37m[1m[2023-07-17 11:18:23,731][257371] Mean Reward across all agents: 50.42499493493409[0m
[37m[1m[2023-07-17 11:18:23,731][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:18:28,761][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:18:28,762][257371] Reward + Measures: [[ -0.89088413   0.727        0.29500002   0.74290001   0.32070002
    3.17273879]
 [ 56.38299304   0.65689999   0.5377       0.63480002   0.14670001
    5.39987326]
 [ 43.95789679   0.31560001   0.27330002   0.3989       0.21230002
    5.1365242 ]
 ...
 [117.19522669   0.67110002   0.3919       0.69730002   0.14100002
    3.7210443 ]
 [179.56022739   0.95600003   0.81540006   0.92939997   0.0086
    4.26356459]
 [167.26247788   0.35280001   0.56590003   0.48779997   0.54500002
    4.98117304]][0m
[37m[1m[2023-07-17 11:18:28,762][257371] Max Reward on eval: 311.83452342234085[0m
[37m[1m[2023-07-17 11:18:28,763][257371] Min Reward on eval: -142.1710918033961[0m
[37m[1m[2023-07-17 11:18:28,763][257371] Mean Reward across all agents: 65.97523301170831[0m
[37m[1m[2023-07-17 11:18:28,763][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:18:28,770][257371] mean_value=-251.8950530976331, max_value=325.4714340795133[0m
[37m[1m[2023-07-17 11:18:28,773][257371] New mean coefficients: [[ 0.06968081  1.4647698  -0.18351865  0.23707736  0.05744046  1.296171  ]][0m
[37m[1m[2023-07-17 11:18:28,774][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:18:37,776][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 11:18:37,776][257371] FPS: 426631.25[0m
[36m[2023-07-17 11:18:37,779][257371] itr=1123, itrs=2000, Progress: 56.15%[0m
[36m[2023-07-17 11:18:49,807][257371] train() took 11.91 seconds to complete[0m
[36m[2023-07-17 11:18:49,808][257371] FPS: 322286.69[0m
[36m[2023-07-17 11:18:54,119][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:18:54,120][257371] Reward + Measures: [[58.33437266  0.90791565  0.69044691  0.92935562  0.09394667  3.37489963]][0m
[37m[1m[2023-07-17 11:18:54,120][257371] Max Reward on eval: 58.3343726623958[0m
[37m[1m[2023-07-17 11:18:54,120][257371] Min Reward on eval: 58.3343726623958[0m
[37m[1m[2023-07-17 11:18:54,121][257371] Mean Reward across all agents: 58.3343726623958[0m
[37m[1m[2023-07-17 11:18:54,121][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:18:59,154][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:18:59,154][257371] Reward + Measures: [[  1.19708075   0.44490001   0.25079998   0.43530002   0.39320001
    3.44094586]
 [  2.96178515   0.70890003   0.0884       0.77829999   0.38250002
    3.87830782]
 [119.98095273   0.87340003   0.78800005   0.70380002   0.1797
    4.72270012]
 ...
 [  2.88798827   0.64679998   0.5467       0.6552       0.1242
    4.91983509]
 [ 39.73605331   0.89659995   0.58890003   0.89969999   0.1434
    3.49079442]
 [ 32.4373792    0.69480002   0.62620002   0.6925       0.0599
    2.93108416]][0m
[37m[1m[2023-07-17 11:18:59,155][257371] Max Reward on eval: 206.19788928413763[0m
[37m[1m[2023-07-17 11:18:59,155][257371] Min Reward on eval: -153.55451907664536[0m
[37m[1m[2023-07-17 11:18:59,155][257371] Mean Reward across all agents: 25.848526265616176[0m
[37m[1m[2023-07-17 11:18:59,155][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:18:59,159][257371] mean_value=-249.78902532424297, max_value=302.33461756336897[0m
[37m[1m[2023-07-17 11:18:59,161][257371] New mean coefficients: [[ 0.18026558  0.62763524  0.45879728  0.6303235  -0.05597642  0.768921  ]][0m
[37m[1m[2023-07-17 11:18:59,162][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:19:08,217][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 11:19:08,218][257371] FPS: 424152.07[0m
[36m[2023-07-17 11:19:08,220][257371] itr=1124, itrs=2000, Progress: 56.20%[0m
[36m[2023-07-17 11:19:20,068][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 11:19:20,068][257371] FPS: 327130.42[0m
[36m[2023-07-17 11:19:24,341][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:19:24,341][257371] Reward + Measures: [[68.71815271  0.91298795  0.69462037  0.93182302  0.08582499  3.4441433 ]][0m
[37m[1m[2023-07-17 11:19:24,342][257371] Max Reward on eval: 68.7181527063481[0m
[37m[1m[2023-07-17 11:19:24,342][257371] Min Reward on eval: 68.7181527063481[0m
[37m[1m[2023-07-17 11:19:24,342][257371] Mean Reward across all agents: 68.7181527063481[0m
[37m[1m[2023-07-17 11:19:24,342][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:19:29,339][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:19:29,339][257371] Reward + Measures: [[ 80.28475529   0.68129998   0.75480002   0.71869999   0.1925
    3.01385784]
 [140.16738308   0.38799998   0.47709998   0.65740001   0.6893
    5.48952389]
 [ -1.68766238   0.63639998   0.42610002   0.58590001   0.1142
    3.54188204]
 ...
 [ -4.48093986   0.63170004   0.23370002   0.58160001   0.2167
    3.58020091]
 [ 41.21095561   0.56660002   0.54150003   0.81700003   0.48670003
    3.7988708 ]
 [ 62.69944874   0.29100001   0.29189998   0.47510001   0.43720004
    5.4876442 ]][0m
[37m[1m[2023-07-17 11:19:29,339][257371] Max Reward on eval: 193.56427669879048[0m
[37m[1m[2023-07-17 11:19:29,340][257371] Min Reward on eval: -275.08411283064635[0m
[37m[1m[2023-07-17 11:19:29,340][257371] Mean Reward across all agents: 38.375086831623214[0m
[37m[1m[2023-07-17 11:19:29,340][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:19:29,345][257371] mean_value=-343.56500688667677, max_value=327.1111347643092[0m
[37m[1m[2023-07-17 11:19:29,347][257371] New mean coefficients: [[0.4551403  1.2525253  0.7296166  0.42072934 0.04541943 1.2145824 ]][0m
[37m[1m[2023-07-17 11:19:29,348][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:19:38,320][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 11:19:38,321][257371] FPS: 428075.80[0m
[36m[2023-07-17 11:19:38,323][257371] itr=1125, itrs=2000, Progress: 56.25%[0m
[36m[2023-07-17 11:19:50,155][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 11:19:50,155][257371] FPS: 327586.67[0m
[36m[2023-07-17 11:19:54,454][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:19:54,454][257371] Reward + Measures: [[71.30194232  0.90891665  0.69482106  0.9345156   0.090786    3.48611903]][0m
[37m[1m[2023-07-17 11:19:54,454][257371] Max Reward on eval: 71.30194231811073[0m
[37m[1m[2023-07-17 11:19:54,454][257371] Min Reward on eval: 71.30194231811073[0m
[37m[1m[2023-07-17 11:19:54,455][257371] Mean Reward across all agents: 71.30194231811073[0m
[37m[1m[2023-07-17 11:19:54,455][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:19:59,435][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:19:59,436][257371] Reward + Measures: [[  48.04734164    0.35300002    0.6049        0.52880001    0.61059999
     5.53881216]
 [ 217.10726739    0.18370001    0.87320006    0.78420001    0.75740004
     5.64992666]
 [ 136.52038549    0.44280005    0.61330003    0.62290001    0.57910001
     6.17979622]
 ...
 [  12.46066524    0.32550001    0.62150002    0.73260003    0.56230003
     4.4064784 ]
 [ -88.93549921    0.44910002    0.1857        0.39970002    0.1983
     2.94170642]
 [-163.04860567    0.4488        0.1415        0.44479999    0.35520002
     2.37936568]][0m
[37m[1m[2023-07-17 11:19:59,436][257371] Max Reward on eval: 407.56062581606676[0m
[37m[1m[2023-07-17 11:19:59,436][257371] Min Reward on eval: -258.0178665722022[0m
[37m[1m[2023-07-17 11:19:59,437][257371] Mean Reward across all agents: 14.157722443334903[0m
[37m[1m[2023-07-17 11:19:59,437][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:19:59,439][257371] mean_value=-483.2251584090163, max_value=140.23475116402346[0m
[37m[1m[2023-07-17 11:19:59,442][257371] New mean coefficients: [[ 0.33681652  0.48413223  0.41164562  0.20680168 -0.27221745  0.9121103 ]][0m
[37m[1m[2023-07-17 11:19:59,443][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:20:08,426][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 11:20:08,426][257371] FPS: 427552.23[0m
[36m[2023-07-17 11:20:08,428][257371] itr=1126, itrs=2000, Progress: 56.30%[0m
[36m[2023-07-17 11:20:20,543][257371] train() took 12.00 seconds to complete[0m
[36m[2023-07-17 11:20:20,543][257371] FPS: 319951.78[0m
[36m[2023-07-17 11:20:24,842][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:20:24,842][257371] Reward + Measures: [[74.33219865  0.919653    0.6954326   0.93721801  0.07953266  3.52937293]][0m
[37m[1m[2023-07-17 11:20:24,842][257371] Max Reward on eval: 74.33219864547178[0m
[37m[1m[2023-07-17 11:20:24,843][257371] Min Reward on eval: 74.33219864547178[0m
[37m[1m[2023-07-17 11:20:24,843][257371] Mean Reward across all agents: 74.33219864547178[0m
[37m[1m[2023-07-17 11:20:24,843][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:20:30,119][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:20:30,119][257371] Reward + Measures: [[-103.86408905    0.74769998    0.0318        0.83219999    0.4434
     4.04859781]
 [  30.27250702    0.76990002    0.36760002    0.76240003    0.0964
     2.82338738]
 [  89.89168767    0.28189999    0.83209991    0.58720005    0.83459997
     5.33526993]
 ...
 [-116.04491996    0.37459999    0.2181        0.45720002    0.2458
     3.60154223]
 [  40.35440476    0.30809999    0.211         0.3062        0.13789999
     4.97988367]
 [  45.92869667    0.7051        0.28310001    0.70580006    0.1609
     2.7794919 ]][0m
[37m[1m[2023-07-17 11:20:30,119][257371] Max Reward on eval: 348.39366165550894[0m
[37m[1m[2023-07-17 11:20:30,120][257371] Min Reward on eval: -150.99851463953965[0m
[37m[1m[2023-07-17 11:20:30,120][257371] Mean Reward across all agents: 39.301612834132314[0m
[37m[1m[2023-07-17 11:20:30,120][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:20:30,125][257371] mean_value=-398.50995357215766, max_value=156.65902856256054[0m
[37m[1m[2023-07-17 11:20:30,128][257371] New mean coefficients: [[ 0.1577876   0.10542077  0.39356753  0.0886024  -0.03573085  1.0190449 ]][0m
[37m[1m[2023-07-17 11:20:30,129][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:20:39,198][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 11:20:39,199][257371] FPS: 423476.40[0m
[36m[2023-07-17 11:20:39,201][257371] itr=1127, itrs=2000, Progress: 56.35%[0m
[36m[2023-07-17 11:20:50,896][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 11:20:50,896][257371] FPS: 331543.67[0m
[36m[2023-07-17 11:20:55,257][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:20:55,258][257371] Reward + Measures: [[81.97396948  0.91649032  0.68892831  0.93885624  0.08267967  3.69594407]][0m
[37m[1m[2023-07-17 11:20:55,258][257371] Max Reward on eval: 81.97396947861839[0m
[37m[1m[2023-07-17 11:20:55,258][257371] Min Reward on eval: 81.97396947861839[0m
[37m[1m[2023-07-17 11:20:55,259][257371] Mean Reward across all agents: 81.97396947861839[0m
[37m[1m[2023-07-17 11:20:55,259][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:21:00,236][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:21:00,236][257371] Reward + Measures: [[-100.38976992    0.4729        0.45830002    0.54069996    0.33990002
     4.30588055]
 [ 152.41258464    0.0741        0.90109998    0.68950003    0.91769999
     5.60363293]
 [ 274.85487294    0.15610002    0.74430001    0.84520006    0.81440002
     5.00176096]
 ...
 [  79.30314464    0.45709997    0.1194        0.44710001    0.23200002
     3.8723259 ]
 [  61.81468601    0.74000001    0.4756        0.61050004    0.08450001
     3.55108452]
 [  29.67553594    0.19789998    0.51290005    0.53170002    0.60729998
     4.12716866]][0m
[37m[1m[2023-07-17 11:21:00,237][257371] Max Reward on eval: 382.4976301148534[0m
[37m[1m[2023-07-17 11:21:00,237][257371] Min Reward on eval: -330.5636304893531[0m
[37m[1m[2023-07-17 11:21:00,237][257371] Mean Reward across all agents: 43.89529094673226[0m
[37m[1m[2023-07-17 11:21:00,237][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:21:00,241][257371] mean_value=-380.35045654064464, max_value=609.9177965603769[0m
[37m[1m[2023-07-17 11:21:00,244][257371] New mean coefficients: [[ 0.1710568   0.293347   -0.87393355 -0.26082334  0.11286189  1.2096446 ]][0m
[37m[1m[2023-07-17 11:21:00,245][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:21:09,268][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 11:21:09,268][257371] FPS: 425662.03[0m
[36m[2023-07-17 11:21:09,270][257371] itr=1128, itrs=2000, Progress: 56.40%[0m
[36m[2023-07-17 11:21:21,028][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 11:21:21,028][257371] FPS: 329670.13[0m
[36m[2023-07-17 11:21:25,274][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:21:25,274][257371] Reward + Measures: [[78.70482207  0.91098732  0.67319167  0.93758935  0.09679     3.79510117]][0m
[37m[1m[2023-07-17 11:21:25,275][257371] Max Reward on eval: 78.70482206798629[0m
[37m[1m[2023-07-17 11:21:25,275][257371] Min Reward on eval: 78.70482206798629[0m
[37m[1m[2023-07-17 11:21:25,275][257371] Mean Reward across all agents: 78.70482206798629[0m
[37m[1m[2023-07-17 11:21:25,275][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:21:30,271][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:21:30,272][257371] Reward + Measures: [[-20.13659363   0.42419997   0.34580001   0.41670004   0.26000002
    3.54105997]
 [ 13.4925329    0.34200001   0.35390002   0.41079998   0.18200001
    3.19208932]
 [-81.99309761   0.704        0.33140001   0.69740003   0.10320001
    3.95140886]
 ...
 [-79.44370024   0.80639994   0.0775       0.77189994   0.59119999
    4.85131979]
 [-53.99293339   0.48979998   0.15879999   0.50660002   0.2185
    2.85234261]
 [ 44.61260672   0.4276       0.15580001   0.45609999   0.23050001
    3.5646503 ]][0m
[37m[1m[2023-07-17 11:21:30,272][257371] Max Reward on eval: 196.38725831927732[0m
[37m[1m[2023-07-17 11:21:30,272][257371] Min Reward on eval: -296.18084813430903[0m
[37m[1m[2023-07-17 11:21:30,273][257371] Mean Reward across all agents: 23.57419370706636[0m
[37m[1m[2023-07-17 11:21:30,273][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:21:30,276][257371] mean_value=-473.62841999726993, max_value=67.7573835587757[0m
[37m[1m[2023-07-17 11:21:30,278][257371] New mean coefficients: [[ 0.3199287   0.11002833 -0.38987285  0.2267549   0.30230528  1.1764005 ]][0m
[37m[1m[2023-07-17 11:21:30,279][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:21:39,353][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 11:21:39,353][257371] FPS: 423288.05[0m
[36m[2023-07-17 11:21:39,356][257371] itr=1129, itrs=2000, Progress: 56.45%[0m
[36m[2023-07-17 11:21:51,089][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 11:21:51,089][257371] FPS: 330426.70[0m
[36m[2023-07-17 11:21:55,302][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:21:55,302][257371] Reward + Measures: [[75.87780199  0.91532665  0.64002866  0.93879932  0.11537067  3.8794558 ]][0m
[37m[1m[2023-07-17 11:21:55,303][257371] Max Reward on eval: 75.87780198658099[0m
[37m[1m[2023-07-17 11:21:55,303][257371] Min Reward on eval: 75.87780198658099[0m
[37m[1m[2023-07-17 11:21:55,303][257371] Mean Reward across all agents: 75.87780198658099[0m
[37m[1m[2023-07-17 11:21:55,303][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:22:00,280][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:22:00,281][257371] Reward + Measures: [[  30.83350161    0.39820001    0.27580002    0.49610001    0.2256
     3.11925435]
 [   7.04828644    0.65570003    0.48899999    0.74129999    0.30710003
     3.995749  ]
 [  23.64156307    0.58600003    0.32700002    0.64009994    0.1353
     3.54647255]
 ...
 [  -7.076317      0.2217        0.17909999    0.2999        0.25300002
     3.73565984]
 [   7.63790675    0.76279998    0.1533        0.8021        0.60079998
     4.6374917 ]
 [-263.73779967    0.51539999    0.0953        0.46180001    0.3046
     3.80228353]][0m
[37m[1m[2023-07-17 11:22:00,281][257371] Max Reward on eval: 193.68935679458082[0m
[37m[1m[2023-07-17 11:22:00,281][257371] Min Reward on eval: -263.7377996746451[0m
[37m[1m[2023-07-17 11:22:00,281][257371] Mean Reward across all agents: 36.49319727977397[0m
[37m[1m[2023-07-17 11:22:00,282][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:22:00,285][257371] mean_value=-392.62041668454833, max_value=79.92190879935009[0m
[37m[1m[2023-07-17 11:22:00,288][257371] New mean coefficients: [[ 0.09956686 -0.30654564 -0.41550094  0.02417888  0.23150477  0.90168345]][0m
[37m[1m[2023-07-17 11:22:00,289][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:22:09,245][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 11:22:09,245][257371] FPS: 428838.89[0m
[36m[2023-07-17 11:22:09,248][257371] itr=1130, itrs=2000, Progress: 56.50%[0m
[37m[1m[2023-07-17 11:25:39,301][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001110[0m
[36m[2023-07-17 11:25:51,527][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 11:25:51,528][257371] FPS: 328466.89[0m
[36m[2023-07-17 11:25:55,768][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:25:55,768][257371] Reward + Measures: [[70.45758619  0.90645564  0.61655337  0.93616503  0.13872066  3.94351292]][0m
[37m[1m[2023-07-17 11:25:55,768][257371] Max Reward on eval: 70.45758618916075[0m
[37m[1m[2023-07-17 11:25:55,769][257371] Min Reward on eval: 70.45758618916075[0m
[37m[1m[2023-07-17 11:25:55,769][257371] Mean Reward across all agents: 70.45758618916075[0m
[37m[1m[2023-07-17 11:25:55,769][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:26:00,709][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:26:00,710][257371] Reward + Measures: [[ 80.49808217   0.74119997   0.16270001   0.80319995   0.28690001
    4.2608037 ]
 [ 38.59669626   0.66549999   0.0707       0.57790005   0.53560007
    3.83655024]
 [ 20.26223969   0.17290001   0.16650002   0.31490001   0.1987
    3.89236999]
 ...
 [ 90.89074405   0.22819999   0.171        0.21269999   0.26280001
    4.75755548]
 [-14.72575992   0.13399999   0.13609999   0.1523       0.20939998
    4.44585037]
 [  8.07618573   0.6674       0.2705       0.66530001   0.25460002
    4.0083499 ]][0m
[37m[1m[2023-07-17 11:26:00,710][257371] Max Reward on eval: 257.34617613777516[0m
[37m[1m[2023-07-17 11:26:00,710][257371] Min Reward on eval: -105.04994376730174[0m
[37m[1m[2023-07-17 11:26:00,711][257371] Mean Reward across all agents: 59.20423398335026[0m
[37m[1m[2023-07-17 11:26:00,711][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:26:00,714][257371] mean_value=-514.1577597567134, max_value=99.9849280420722[0m
[37m[1m[2023-07-17 11:26:00,718][257371] New mean coefficients: [[-0.2858284  -0.01845261 -0.34783694  0.01881334  0.6296823   1.796637  ]][0m
[37m[1m[2023-07-17 11:26:00,718][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:26:09,723][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 11:26:09,724][257371] FPS: 426513.33[0m
[36m[2023-07-17 11:26:09,726][257371] itr=1131, itrs=2000, Progress: 56.55%[0m
[36m[2023-07-17 11:26:21,420][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 11:26:21,420][257371] FPS: 331469.11[0m
[36m[2023-07-17 11:26:25,613][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:26:25,614][257371] Reward + Measures: [[60.81312998  0.89624828  0.57412761  0.93300301  0.18568298  4.03275299]][0m
[37m[1m[2023-07-17 11:26:25,614][257371] Max Reward on eval: 60.81312997943156[0m
[37m[1m[2023-07-17 11:26:25,614][257371] Min Reward on eval: 60.81312997943156[0m
[37m[1m[2023-07-17 11:26:25,614][257371] Mean Reward across all agents: 60.81312997943156[0m
[37m[1m[2023-07-17 11:26:25,615][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:26:30,605][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:26:30,606][257371] Reward + Measures: [[-12.95968079   0.56510001   0.16440001   0.63120002   0.30030003
    3.85740089]
 [ 67.49984757   0.72000003   0.49130002   0.73850006   0.2184
    3.6089046 ]
 [ 75.59443046   0.83789998   0.57429999   0.91070002   0.1504
    3.83243251]
 ...
 [ -8.77369796   0.86630005   0.69769996   0.83160001   0.0506
    4.62140083]
 [ 69.88430335   0.79980004   0.69469994   0.84759998   0.15899999
    4.56338072]
 [ 61.85517197   0.9217       0.64649999   0.86289996   0.0164
    4.04932976]][0m
[37m[1m[2023-07-17 11:26:30,606][257371] Max Reward on eval: 393.29311938658356[0m
[37m[1m[2023-07-17 11:26:30,606][257371] Min Reward on eval: -182.1237628193572[0m
[37m[1m[2023-07-17 11:26:30,607][257371] Mean Reward across all agents: 74.51768691052477[0m
[37m[1m[2023-07-17 11:26:30,607][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:26:30,612][257371] mean_value=-238.82337501517875, max_value=578.464117939811[0m
[37m[1m[2023-07-17 11:26:30,615][257371] New mean coefficients: [[-0.5242167   0.2728706  -0.38124982  0.01304322  0.9368017   1.87817   ]][0m
[37m[1m[2023-07-17 11:26:30,616][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:26:39,670][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 11:26:39,670][257371] FPS: 424189.07[0m
[36m[2023-07-17 11:26:39,672][257371] itr=1132, itrs=2000, Progress: 56.60%[0m
[36m[2023-07-17 11:26:51,752][257371] train() took 11.97 seconds to complete[0m
[36m[2023-07-17 11:26:51,752][257371] FPS: 320741.62[0m
[36m[2023-07-17 11:26:56,071][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:26:56,071][257371] Reward + Measures: [[54.02485124  0.88459831  0.54742396  0.93011701  0.22408901  4.1369915 ]][0m
[37m[1m[2023-07-17 11:26:56,072][257371] Max Reward on eval: 54.02485123892225[0m
[37m[1m[2023-07-17 11:26:56,072][257371] Min Reward on eval: 54.02485123892225[0m
[37m[1m[2023-07-17 11:26:56,072][257371] Mean Reward across all agents: 54.02485123892225[0m
[37m[1m[2023-07-17 11:26:56,072][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:27:00,962][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:27:00,962][257371] Reward + Measures: [[  61.50754588    0.25640002    0.87519997    0.67840004    0.86280006
     4.64887571]
 [  78.43056654    0.93510002    0.6024        0.93959999    0.10209999
     4.03948307]
 [-108.7300284     0.80699998    0.1184        0.7726        0.50590003
     3.81728292]
 ...
 [  11.24726582    0.86809999    0.34319997    0.86110002    0.1682
     3.67333293]
 [  65.4881331     0.33579999    0.866         0.88529998    0.64570004
     4.56255102]
 [  19.65999487    0.86690009    0.47220001    0.86300004    0.21009998
     3.90102935]][0m
[37m[1m[2023-07-17 11:27:00,963][257371] Max Reward on eval: 133.39654157643673[0m
[37m[1m[2023-07-17 11:27:00,963][257371] Min Reward on eval: -160.15005571700166[0m
[37m[1m[2023-07-17 11:27:00,963][257371] Mean Reward across all agents: 26.156457423345014[0m
[37m[1m[2023-07-17 11:27:00,963][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:27:00,967][257371] mean_value=-164.60707063145233, max_value=47.634670518614534[0m
[37m[1m[2023-07-17 11:27:00,970][257371] New mean coefficients: [[ 0.23598367  0.13251708 -0.10164917  0.5972382   0.75393605  1.9711919 ]][0m
[37m[1m[2023-07-17 11:27:00,971][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:27:09,915][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 11:27:09,915][257371] FPS: 429434.27[0m
[36m[2023-07-17 11:27:09,917][257371] itr=1133, itrs=2000, Progress: 56.65%[0m
[36m[2023-07-17 11:27:21,677][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 11:27:21,678][257371] FPS: 329670.75[0m
[36m[2023-07-17 11:27:25,909][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:27:25,910][257371] Reward + Measures: [[46.36887583  0.85880393  0.54827666  0.92641062  0.26314202  4.22375059]][0m
[37m[1m[2023-07-17 11:27:25,910][257371] Max Reward on eval: 46.36887582938119[0m
[37m[1m[2023-07-17 11:27:25,910][257371] Min Reward on eval: 46.36887582938119[0m
[37m[1m[2023-07-17 11:27:25,911][257371] Mean Reward across all agents: 46.36887582938119[0m
[37m[1m[2023-07-17 11:27:25,911][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:27:30,904][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:27:30,905][257371] Reward + Measures: [[  35.02239006    0.56479996    0.1481        0.56840003    0.27070001
     3.58677149]
 [  62.39276563    0.36989999    0.2832        0.45949998    0.25750002
     3.74547124]
 [  18.45489888    0.76359999    0.2897        0.80600005    0.4237
     5.67348242]
 ...
 [  41.11061067    0.28420001    0.2175        0.31959999    0.29459998
     4.75763273]
 [-117.60397963    0.71240002    0.0684        0.76010007    0.38949999
     5.52196264]
 [ -57.70842288    0.46650001    0.35429999    0.5614        0.36160001
     5.71824217]][0m
[37m[1m[2023-07-17 11:27:30,905][257371] Max Reward on eval: 338.4720356740057[0m
[37m[1m[2023-07-17 11:27:30,905][257371] Min Reward on eval: -286.42457385957243[0m
[37m[1m[2023-07-17 11:27:30,906][257371] Mean Reward across all agents: 3.6004366536310863[0m
[37m[1m[2023-07-17 11:27:30,906][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:27:30,911][257371] mean_value=-298.90734381233466, max_value=209.09926457938707[0m
[37m[1m[2023-07-17 11:27:30,914][257371] New mean coefficients: [[ 0.20669006 -0.31158727  0.01504125  0.07530451  0.7583665   1.4527231 ]][0m
[37m[1m[2023-07-17 11:27:30,915][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:27:39,892][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 11:27:39,893][257371] FPS: 427810.45[0m
[36m[2023-07-17 11:27:39,895][257371] itr=1134, itrs=2000, Progress: 56.70%[0m
[36m[2023-07-17 11:27:52,107][257371] train() took 12.10 seconds to complete[0m
[36m[2023-07-17 11:27:52,107][257371] FPS: 317341.53[0m
[36m[2023-07-17 11:27:56,400][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:27:56,400][257371] Reward + Measures: [[46.97594641  0.82239133  0.56954694  0.92093474  0.29416865  4.34416056]][0m
[37m[1m[2023-07-17 11:27:56,400][257371] Max Reward on eval: 46.97594640519794[0m
[37m[1m[2023-07-17 11:27:56,401][257371] Min Reward on eval: 46.97594640519794[0m
[37m[1m[2023-07-17 11:27:56,401][257371] Mean Reward across all agents: 46.97594640519794[0m
[37m[1m[2023-07-17 11:27:56,401][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:28:01,371][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:28:01,377][257371] Reward + Measures: [[ 69.73098945   0.46919999   0.6336       0.72649997   0.45480004
    4.07312918]
 [  3.90466587   0.93330002   0.1382       0.95199996   0.26929998
    4.66413212]
 [ 39.81701762   0.83759993   0.27990001   0.85159999   0.127
    4.08292198]
 ...
 [ 66.09627615   0.58410007   0.52410001   0.81840003   0.36360002
    4.08895588]
 [ 32.72289998   0.4614       0.40710002   0.62420005   0.52759999
    4.11355305]
 [131.07020187   0.68150002   0.66820002   0.86830008   0.30620003
    4.30354118]][0m
[37m[1m[2023-07-17 11:28:01,377][257371] Max Reward on eval: 181.5650618068874[0m
[37m[1m[2023-07-17 11:28:01,377][257371] Min Reward on eval: -130.46582155618816[0m
[37m[1m[2023-07-17 11:28:01,378][257371] Mean Reward across all agents: 39.06111297143039[0m
[37m[1m[2023-07-17 11:28:01,378][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:28:01,381][257371] mean_value=-188.71374287806606, max_value=87.74047051796677[0m
[37m[1m[2023-07-17 11:28:01,384][257371] New mean coefficients: [[0.25493792 0.14134896 0.14759806 0.14271986 0.5888998  2.3786058 ]][0m
[37m[1m[2023-07-17 11:28:01,385][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:28:10,367][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 11:28:10,367][257371] FPS: 427581.45[0m
[36m[2023-07-17 11:28:10,370][257371] itr=1135, itrs=2000, Progress: 56.75%[0m
[36m[2023-07-17 11:28:22,178][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 11:28:22,178][257371] FPS: 328293.06[0m
[36m[2023-07-17 11:28:26,527][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:28:26,527][257371] Reward + Measures: [[44.90701523  0.81434506  0.56807101  0.92105663  0.30164701  4.35595226]][0m
[37m[1m[2023-07-17 11:28:26,527][257371] Max Reward on eval: 44.90701523271292[0m
[37m[1m[2023-07-17 11:28:26,528][257371] Min Reward on eval: 44.90701523271292[0m
[37m[1m[2023-07-17 11:28:26,528][257371] Mean Reward across all agents: 44.90701523271292[0m
[37m[1m[2023-07-17 11:28:26,528][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:28:31,563][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:28:31,564][257371] Reward + Measures: [[ 76.41046043   0.29580003   0.35859999   0.45200005   0.39570001
    4.63652611]
 [  6.0309588    0.4824       0.38410002   0.78620005   0.68430001
    5.453897  ]
 [-10.78242477   0.70230001   0.31619999   0.71820003   0.2613
    4.7509613 ]
 ...
 [-84.58202421   0.76130003   0.30819997   0.74119997   0.13890001
    3.93837714]
 [159.9017069    0.44940001   0.34849998   0.75169998   0.68570006
    4.88409424]
 [-67.63279703   0.82190001   0.3955       0.7899       0.1244
    3.46472168]][0m
[37m[1m[2023-07-17 11:28:31,564][257371] Max Reward on eval: 507.47020812053233[0m
[37m[1m[2023-07-17 11:28:31,564][257371] Min Reward on eval: -583.0305290291086[0m
[37m[1m[2023-07-17 11:28:31,565][257371] Mean Reward across all agents: -0.08023372561671446[0m
[37m[1m[2023-07-17 11:28:31,565][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:28:31,569][257371] mean_value=-305.88132580008954, max_value=159.05061814524197[0m
[37m[1m[2023-07-17 11:28:31,571][257371] New mean coefficients: [[-0.03557679  0.4130625  -0.0410998   0.20628025  0.2202119   2.7830207 ]][0m
[37m[1m[2023-07-17 11:28:31,572][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:28:40,634][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 11:28:40,634][257371] FPS: 423857.11[0m
[36m[2023-07-17 11:28:40,636][257371] itr=1136, itrs=2000, Progress: 56.80%[0m
[36m[2023-07-17 11:28:52,422][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 11:28:52,422][257371] FPS: 328983.16[0m
[36m[2023-07-17 11:28:56,706][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:28:56,707][257371] Reward + Measures: [[47.62242071  0.76314926  0.59437102  0.91356426  0.33579031  4.43561792]][0m
[37m[1m[2023-07-17 11:28:56,707][257371] Max Reward on eval: 47.62242071459757[0m
[37m[1m[2023-07-17 11:28:56,707][257371] Min Reward on eval: 47.62242071459757[0m
[37m[1m[2023-07-17 11:28:56,707][257371] Mean Reward across all agents: 47.62242071459757[0m
[37m[1m[2023-07-17 11:28:56,708][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:29:01,739][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:29:01,739][257371] Reward + Measures: [[ -11.11775664    0.66860002    0.61660004    0.68080008    0.20829999
     4.34325361]
 [  30.98492278    0.62320006    0.3876        0.85869998    0.53979999
     4.13020611]
 [-108.42157831    0.55440003    0.13270001    0.53719997    0.24089999
     4.51333761]
 ...
 [  33.7217303     0.76110005    0.49890003    0.79110003    0.11989999
     3.52002764]
 [  29.60637286    0.72320002    0.10339999    0.84040004    0.296
     3.90260315]
 [  30.65623558    0.50800002    0.442         0.542         0.19679999
     3.23452163]][0m
[37m[1m[2023-07-17 11:29:01,739][257371] Max Reward on eval: 198.7949973449111[0m
[37m[1m[2023-07-17 11:29:01,740][257371] Min Reward on eval: -460.2826026795432[0m
[37m[1m[2023-07-17 11:29:01,740][257371] Mean Reward across all agents: 19.163767769717193[0m
[37m[1m[2023-07-17 11:29:01,740][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:29:01,743][257371] mean_value=-242.85943038425904, max_value=159.49308746519404[0m
[37m[1m[2023-07-17 11:29:01,746][257371] New mean coefficients: [[ 0.31240076  0.01515436 -0.01055236 -0.04608862 -0.44230494  2.4759955 ]][0m
[37m[1m[2023-07-17 11:29:01,747][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:29:10,813][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 11:29:10,813][257371] FPS: 423648.05[0m
[36m[2023-07-17 11:29:10,816][257371] itr=1137, itrs=2000, Progress: 56.85%[0m
[36m[2023-07-17 11:29:22,799][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-17 11:29:22,799][257371] FPS: 323437.32[0m
[36m[2023-07-17 11:29:27,029][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:29:27,029][257371] Reward + Measures: [[55.9773386   0.72731227  0.63380432  0.9063527   0.33250165  4.52490139]][0m
[37m[1m[2023-07-17 11:29:27,030][257371] Max Reward on eval: 55.97733860277529[0m
[37m[1m[2023-07-17 11:29:27,030][257371] Min Reward on eval: 55.97733860277529[0m
[37m[1m[2023-07-17 11:29:27,030][257371] Mean Reward across all agents: 55.97733860277529[0m
[37m[1m[2023-07-17 11:29:27,030][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:29:32,019][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:29:32,020][257371] Reward + Measures: [[15.9689636   0.74520004  0.41190001  0.90790004  0.37780002  4.50220633]
 [99.66269232  0.5485      0.3091      0.55540001  0.25940001  5.34117746]
 [-9.29079937  0.67970002  0.28680003  0.66590005  0.22979999  3.77126551]
 ...
 [78.19384306  0.79889995  0.30419999  0.8028      0.2854      5.21164513]
 [18.71233857  0.47540003  0.53570002  0.75450003  0.46339998  4.01953983]
 [30.11926256  0.64669997  0.20899999  0.67250001  0.27830002  3.52052546]][0m
[37m[1m[2023-07-17 11:29:32,020][257371] Max Reward on eval: 267.19499426567927[0m
[37m[1m[2023-07-17 11:29:32,020][257371] Min Reward on eval: -73.44812652207911[0m
[37m[1m[2023-07-17 11:29:32,021][257371] Mean Reward across all agents: 50.13620703723148[0m
[37m[1m[2023-07-17 11:29:32,021][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:29:32,024][257371] mean_value=-263.0412228737896, max_value=68.70114780627932[0m
[37m[1m[2023-07-17 11:29:32,027][257371] New mean coefficients: [[ 0.2988017  -0.00834847  0.2898163   0.07597301  0.19910899  2.6884959 ]][0m
[37m[1m[2023-07-17 11:29:32,028][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:29:41,049][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 11:29:41,049][257371] FPS: 425738.05[0m
[36m[2023-07-17 11:29:41,052][257371] itr=1138, itrs=2000, Progress: 56.90%[0m
[36m[2023-07-17 11:29:52,895][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 11:29:52,895][257371] FPS: 327260.52[0m
[36m[2023-07-17 11:29:57,177][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:29:57,177][257371] Reward + Measures: [[56.72304513  0.7142176   0.65128964  0.90253931  0.32866198  4.57867241]][0m
[37m[1m[2023-07-17 11:29:57,178][257371] Max Reward on eval: 56.72304512712271[0m
[37m[1m[2023-07-17 11:29:57,178][257371] Min Reward on eval: 56.72304512712271[0m
[37m[1m[2023-07-17 11:29:57,178][257371] Mean Reward across all agents: 56.72304512712271[0m
[37m[1m[2023-07-17 11:29:57,178][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:30:02,433][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:30:02,434][257371] Reward + Measures: [[-17.25923101   0.6318       0.08180001   0.63269997   0.41440001
    3.57658815]
 [ -2.87486874   0.61490005   0.1973       0.65819997   0.156
    3.81474185]
 [ 98.25380194   0.94740003   0.60420001   0.94929999   0.0412
    4.0638628 ]
 ...
 [ 27.06702053   0.8059001    0.28519997   0.80320007   0.17839999
    3.66890001]
 [132.18037081   0.2036       0.82800007   0.52200001   0.72470003
    5.24748468]
 [148.882885     0.15250002   0.77700001   0.81389999   0.72970003
    5.17571449]][0m
[37m[1m[2023-07-17 11:30:02,435][257371] Max Reward on eval: 404.38504505767486[0m
[37m[1m[2023-07-17 11:30:02,435][257371] Min Reward on eval: -276.5149231046438[0m
[37m[1m[2023-07-17 11:30:02,435][257371] Mean Reward across all agents: 87.11644431560985[0m
[37m[1m[2023-07-17 11:30:02,435][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:30:02,440][257371] mean_value=-259.3314740007142, max_value=145.9758846903858[0m
[37m[1m[2023-07-17 11:30:02,443][257371] New mean coefficients: [[ 0.04283363  0.4124095   0.44956195 -0.2002922   0.36982366  2.379975  ]][0m
[37m[1m[2023-07-17 11:30:02,444][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:30:11,495][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 11:30:11,495][257371] FPS: 424321.84[0m
[36m[2023-07-17 11:30:11,498][257371] itr=1139, itrs=2000, Progress: 56.95%[0m
[36m[2023-07-17 11:30:23,600][257371] train() took 11.99 seconds to complete[0m
[36m[2023-07-17 11:30:23,601][257371] FPS: 320310.07[0m
[36m[2023-07-17 11:30:27,860][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:30:27,860][257371] Reward + Measures: [[56.0081877   0.69212896  0.65849465  0.89708805  0.3471303   4.63707256]][0m
[37m[1m[2023-07-17 11:30:27,861][257371] Max Reward on eval: 56.008187695982926[0m
[37m[1m[2023-07-17 11:30:27,861][257371] Min Reward on eval: 56.008187695982926[0m
[37m[1m[2023-07-17 11:30:27,861][257371] Mean Reward across all agents: 56.008187695982926[0m
[37m[1m[2023-07-17 11:30:27,861][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:30:32,828][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:30:32,829][257371] Reward + Measures: [[ 65.01963992   0.77019995   0.86940002   0.85519999   0.183
    5.02862024]
 [ 31.11336482   0.76249999   0.58650005   0.84860003   0.18609999
    5.19181061]
 [128.4777199    0.1098       0.96160001   0.73239994   0.96639997
    5.68327236]
 ...
 [112.75293755   0.81529999   0.78759998   0.8247       0.1128
    4.72200823]
 [133.5797634    0.92570001   0.87349999   0.87989998   0.0316
    5.29000092]
 [-30.0495696    0.50060004   0.17330001   0.509        0.14219999
    4.00619984]][0m
[37m[1m[2023-07-17 11:30:32,829][257371] Max Reward on eval: 268.5921320725232[0m
[37m[1m[2023-07-17 11:30:32,829][257371] Min Reward on eval: -212.40752793629653[0m
[37m[1m[2023-07-17 11:30:32,829][257371] Mean Reward across all agents: 39.02711831884412[0m
[37m[1m[2023-07-17 11:30:32,830][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:30:32,832][257371] mean_value=-350.01894983623174, max_value=290.5972695883567[0m
[37m[1m[2023-07-17 11:30:32,835][257371] New mean coefficients: [[-0.05955184  0.10574582  0.09496284 -0.07049939  0.22934313  1.8645298 ]][0m
[37m[1m[2023-07-17 11:30:32,836][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:30:41,844][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 11:30:41,845][257371] FPS: 426342.14[0m
[36m[2023-07-17 11:30:41,847][257371] itr=1140, itrs=2000, Progress: 57.00%[0m
[37m[1m[2023-07-17 11:34:12,435][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001120[0m
[36m[2023-07-17 11:34:24,699][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 11:34:24,699][257371] FPS: 330714.14[0m
[36m[2023-07-17 11:34:28,943][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:34:28,943][257371] Reward + Measures: [[57.1605047   0.66338068  0.69553727  0.89049041  0.35329965  4.6769948 ]][0m
[37m[1m[2023-07-17 11:34:28,943][257371] Max Reward on eval: 57.16050469511334[0m
[37m[1m[2023-07-17 11:34:28,944][257371] Min Reward on eval: 57.16050469511334[0m
[37m[1m[2023-07-17 11:34:28,944][257371] Mean Reward across all agents: 57.16050469511334[0m
[37m[1m[2023-07-17 11:34:28,944][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:34:33,956][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:34:34,018][257371] Reward + Measures: [[-26.51796211   0.83270007   0.0902       0.82400006   0.37760001
    3.8445065 ]
 [203.77891206   0.14510001   0.79010004   0.7525       0.79750001
    4.66578913]
 [ 34.37615084   0.3714       0.33070001   0.36919999   0.26659998
    3.38448381]
 ...
 [ 46.62810904   0.29360002   0.3554       0.48950002   0.3475
    4.49326277]
 [ 47.50538757   0.77620006   0.17300001   0.77290004   0.4725
    3.32416892]
 [ 77.53702523   0.1708       0.4752       0.45479998   0.44190001
    4.32152128]][0m
[37m[1m[2023-07-17 11:34:34,019][257371] Max Reward on eval: 349.77808811180296[0m
[37m[1m[2023-07-17 11:34:34,020][257371] Min Reward on eval: -324.4574327401817[0m
[37m[1m[2023-07-17 11:34:34,021][257371] Mean Reward across all agents: 47.89867339124438[0m
[37m[1m[2023-07-17 11:34:34,022][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:34:34,035][257371] mean_value=-260.0648808559473, max_value=133.0620144248939[0m
[37m[1m[2023-07-17 11:34:34,046][257371] New mean coefficients: [[-0.39126584 -0.09765956 -0.10881223 -0.27582383 -0.15692477  1.7091179 ]][0m
[37m[1m[2023-07-17 11:34:34,050][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:34:42,970][257371] train() took 8.92 seconds to complete[0m
[36m[2023-07-17 11:34:42,970][257371] FPS: 430707.53[0m
[36m[2023-07-17 11:34:42,972][257371] itr=1141, itrs=2000, Progress: 57.05%[0m
[36m[2023-07-17 11:34:54,614][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 11:34:54,614][257371] FPS: 333078.48[0m
[36m[2023-07-17 11:34:58,796][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:34:58,797][257371] Reward + Measures: [[60.49994618  0.62522531  0.69071597  0.88514233  0.39823633  4.74745417]][0m
[37m[1m[2023-07-17 11:34:58,797][257371] Max Reward on eval: 60.49994617654123[0m
[37m[1m[2023-07-17 11:34:58,797][257371] Min Reward on eval: 60.49994617654123[0m
[37m[1m[2023-07-17 11:34:58,797][257371] Mean Reward across all agents: 60.49994617654123[0m
[37m[1m[2023-07-17 11:34:58,798][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:35:03,739][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:35:03,740][257371] Reward + Measures: [[  72.61912619    0.46230003    0.76560003    0.85780001    0.57279998
     5.1175313 ]
 [ -45.90195897    0.42020002    0.29459998    0.44650003    0.29699999
     3.50794148]
 [-155.13015821    0.59389997    0.38690001    0.82359999    0.7665
     4.19824076]
 ...
 [  86.24468926    0.44860002    0.1944        0.43189999    0.33759999
     3.85911608]
 [  96.49984286    0.2809        0.6609        0.58039999    0.66220003
     5.34502554]
 [  65.62256418    0.59960002    0.19960001    0.65889996    0.2402
     4.54731274]][0m
[37m[1m[2023-07-17 11:35:03,740][257371] Max Reward on eval: 496.44393918593414[0m
[37m[1m[2023-07-17 11:35:03,740][257371] Min Reward on eval: -208.444978834223[0m
[37m[1m[2023-07-17 11:35:03,740][257371] Mean Reward across all agents: 53.59736024751911[0m
[37m[1m[2023-07-17 11:35:03,741][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:35:03,745][257371] mean_value=-270.5998268696887, max_value=149.9425954189149[0m
[37m[1m[2023-07-17 11:35:03,748][257371] New mean coefficients: [[-0.3904448   0.19861042 -0.10563869 -0.03762773 -0.13226448  1.7428077 ]][0m
[37m[1m[2023-07-17 11:35:03,749][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:35:12,695][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 11:35:12,695][257371] FPS: 429326.08[0m
[36m[2023-07-17 11:35:12,697][257371] itr=1142, itrs=2000, Progress: 57.10%[0m
[36m[2023-07-17 11:35:24,385][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 11:35:24,385][257371] FPS: 331646.74[0m
[36m[2023-07-17 11:35:28,601][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:35:28,601][257371] Reward + Measures: [[65.51925094  0.61897999  0.70714295  0.88992065  0.404006    4.85472059]][0m
[37m[1m[2023-07-17 11:35:28,602][257371] Max Reward on eval: 65.51925093815856[0m
[37m[1m[2023-07-17 11:35:28,602][257371] Min Reward on eval: 65.51925093815856[0m
[37m[1m[2023-07-17 11:35:28,602][257371] Mean Reward across all agents: 65.51925093815856[0m
[37m[1m[2023-07-17 11:35:28,602][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:35:33,581][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:35:33,581][257371] Reward + Measures: [[ -9.33452336   0.6494       0.0843       0.75440001   0.28350002
    4.21875811]
 [179.18180419   0.1433       0.92309999   0.86429995   0.84910005
    5.96467018]
 [ 14.17043991   0.74990004   0.25230002   0.64849997   0.18700002
    3.68455887]
 ...
 [ 68.72479851   0.81630003   0.30520001   0.71679997   0.16429999
    3.59862638]
 [-58.45205975   0.62659997   0.34450001   0.48680001   0.40470004
    3.28040242]
 [-19.80994341   0.52010006   0.28749999   0.54640001   0.2606
    3.44624209]][0m
[37m[1m[2023-07-17 11:35:33,582][257371] Max Reward on eval: 194.39024592528585[0m
[37m[1m[2023-07-17 11:35:33,582][257371] Min Reward on eval: -415.96790413940323[0m
[37m[1m[2023-07-17 11:35:33,582][257371] Mean Reward across all agents: -4.895999013754334[0m
[37m[1m[2023-07-17 11:35:33,582][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:35:33,585][257371] mean_value=-359.7911324369201, max_value=165.58930948079652[0m
[37m[1m[2023-07-17 11:35:33,588][257371] New mean coefficients: [[-0.06200534  0.37739104 -0.18504265  0.04861115 -0.7823432   1.480823  ]][0m
[37m[1m[2023-07-17 11:35:33,589][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:35:42,561][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 11:35:42,561][257371] FPS: 428103.01[0m
[36m[2023-07-17 11:35:42,563][257371] itr=1143, itrs=2000, Progress: 57.15%[0m
[36m[2023-07-17 11:35:54,188][257371] train() took 11.51 seconds to complete[0m
[36m[2023-07-17 11:35:54,188][257371] FPS: 333605.91[0m
[36m[2023-07-17 11:35:58,413][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:35:58,413][257371] Reward + Measures: [[63.45738807  0.6462217   0.70579165  0.88884002  0.36620831  4.88200951]][0m
[37m[1m[2023-07-17 11:35:58,413][257371] Max Reward on eval: 63.457388065274614[0m
[37m[1m[2023-07-17 11:35:58,414][257371] Min Reward on eval: 63.457388065274614[0m
[37m[1m[2023-07-17 11:35:58,414][257371] Mean Reward across all agents: 63.457388065274614[0m
[37m[1m[2023-07-17 11:35:58,414][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:36:03,388][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:36:03,394][257371] Reward + Measures: [[-237.89037514    0.56709999    0.16230002    0.54899997    0.2651
     4.58693695]
 [  21.22304328    0.51959997    0.19020002    0.5765        0.36669999
     3.60445189]
 [  42.16120628    0.52470005    0.49489999    0.79390001    0.46720001
     4.26778889]
 ...
 [  11.91332837    0.6997        0.58109999    0.8308        0.26859999
     4.97869968]
 [   3.24520011    0.78040004    0.1911        0.76279998    0.21269999
     3.80814624]
 [-201.74013521    0.62169999    0.18950002    0.64740002    0.2994
     4.69761848]][0m
[37m[1m[2023-07-17 11:36:03,394][257371] Max Reward on eval: 205.47725676763804[0m
[37m[1m[2023-07-17 11:36:03,394][257371] Min Reward on eval: -241.1548218525946[0m
[37m[1m[2023-07-17 11:36:03,395][257371] Mean Reward across all agents: 13.156873892324414[0m
[37m[1m[2023-07-17 11:36:03,395][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:36:03,398][257371] mean_value=-280.2945157498809, max_value=381.5610824916304[0m
[37m[1m[2023-07-17 11:36:03,401][257371] New mean coefficients: [[ 0.13258839  0.9044857  -0.05488861  0.5272871  -1.0225137   1.5455781 ]][0m
[37m[1m[2023-07-17 11:36:03,402][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:36:12,353][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 11:36:12,353][257371] FPS: 429072.04[0m
[36m[2023-07-17 11:36:12,355][257371] itr=1144, itrs=2000, Progress: 57.20%[0m
[36m[2023-07-17 11:36:24,083][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 11:36:24,084][257371] FPS: 330517.08[0m
[36m[2023-07-17 11:36:28,315][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:36:28,316][257371] Reward + Measures: [[67.08367086  0.76820767  0.67952734  0.91274625  0.23887233  4.8898015 ]][0m
[37m[1m[2023-07-17 11:36:28,316][257371] Max Reward on eval: 67.08367086224982[0m
[37m[1m[2023-07-17 11:36:28,316][257371] Min Reward on eval: 67.08367086224982[0m
[37m[1m[2023-07-17 11:36:28,317][257371] Mean Reward across all agents: 67.08367086224982[0m
[37m[1m[2023-07-17 11:36:28,317][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:36:33,334][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:36:33,334][257371] Reward + Measures: [[206.4407876    0.76179999   0.64650005   0.75200003   0.0852
    6.09712219]
 [113.60388269   0.36739999   0.54290003   0.5654       0.42740002
    4.7772522 ]
 [-27.6281319    0.63350004   0.27090001   0.67020005   0.47340003
    5.51277256]
 ...
 [ 21.83008981   0.75710005   0.4075       0.72259998   0.0432
    4.15703344]
 [200.25229119   0.93589991   0.49090001   0.94160002   0.0923
    4.63433027]
 [ 23.18435771   0.62330002   0.32010004   0.5546       0.20650001
    4.76636076]][0m
[37m[1m[2023-07-17 11:36:33,335][257371] Max Reward on eval: 471.16765306661836[0m
[37m[1m[2023-07-17 11:36:33,335][257371] Min Reward on eval: -764.2746811066521[0m
[37m[1m[2023-07-17 11:36:33,335][257371] Mean Reward across all agents: 42.847197235227824[0m
[37m[1m[2023-07-17 11:36:33,335][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:36:33,339][257371] mean_value=-254.84187897845462, max_value=145.98045636870444[0m
[37m[1m[2023-07-17 11:36:33,342][257371] New mean coefficients: [[-0.1688149   0.50459504 -0.34209162  0.162678   -0.83441275  2.1507404 ]][0m
[37m[1m[2023-07-17 11:36:33,343][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:36:42,421][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 11:36:42,421][257371] FPS: 423065.74[0m
[36m[2023-07-17 11:36:42,424][257371] itr=1145, itrs=2000, Progress: 57.25%[0m
[36m[2023-07-17 11:36:54,302][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 11:36:54,302][257371] FPS: 326381.62[0m
[36m[2023-07-17 11:36:58,692][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:36:58,692][257371] Reward + Measures: [[68.15305522  0.82705867  0.67162967  0.92184031  0.17899033  4.90416098]][0m
[37m[1m[2023-07-17 11:36:58,693][257371] Max Reward on eval: 68.15305521833618[0m
[37m[1m[2023-07-17 11:36:58,693][257371] Min Reward on eval: 68.15305521833618[0m
[37m[1m[2023-07-17 11:36:58,693][257371] Mean Reward across all agents: 68.15305521833618[0m
[37m[1m[2023-07-17 11:36:58,693][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:37:03,734][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:37:03,734][257371] Reward + Measures: [[   5.69408096    0.86630005    0.296         0.86189997    0.1008
     3.82621431]
 [  48.70229901    0.92880005    0.2938        0.93740004    0.31160003
     4.76035643]
 [ -10.79930676    0.44540006    0.23150001    0.55250001    0.25440001
     2.88922501]
 ...
 [-171.65091702    0.54320002    0.20509999    0.4862        0.44080001
     3.75327301]
 [-149.89486507    0.43240005    0.33230001    0.51669997    0.51980001
     3.94861412]
 [  86.78811382    0.31230003    0.72789997    0.8563        0.70340002
     5.11785698]][0m
[37m[1m[2023-07-17 11:37:03,734][257371] Max Reward on eval: 442.0736408950761[0m
[37m[1m[2023-07-17 11:37:03,735][257371] Min Reward on eval: -326.354135516379[0m
[37m[1m[2023-07-17 11:37:03,735][257371] Mean Reward across all agents: 10.33255184765816[0m
[37m[1m[2023-07-17 11:37:03,735][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:37:03,738][257371] mean_value=-310.78696977850217, max_value=20.427827749935304[0m
[37m[1m[2023-07-17 11:37:03,741][257371] New mean coefficients: [[ 0.356363   -0.05859065 -0.48711917  0.12025842 -0.89660966  2.0307133 ]][0m
[37m[1m[2023-07-17 11:37:03,742][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:37:12,815][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 11:37:12,815][257371] FPS: 423294.57[0m
[36m[2023-07-17 11:37:12,817][257371] itr=1146, itrs=2000, Progress: 57.30%[0m
[36m[2023-07-17 11:37:24,567][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 11:37:24,567][257371] FPS: 329973.22[0m
[36m[2023-07-17 11:37:28,832][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:37:28,833][257371] Reward + Measures: [[58.94973009  0.94622701  0.62050229  0.95213532  0.081212    5.00132322]][0m
[37m[1m[2023-07-17 11:37:28,833][257371] Max Reward on eval: 58.949730092287375[0m
[37m[1m[2023-07-17 11:37:28,833][257371] Min Reward on eval: 58.949730092287375[0m
[37m[1m[2023-07-17 11:37:28,833][257371] Mean Reward across all agents: 58.949730092287375[0m
[37m[1m[2023-07-17 11:37:28,834][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:37:33,826][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:37:33,827][257371] Reward + Measures: [[-158.00921632    0.41949996    0.28690001    0.37810001    0.24659999
     5.23650742]
 [-128.71523303    0.47120005    0.22359999    0.40620002    0.37279999
     5.04972076]
 [  18.49763607    0.73479998    0.0533        0.82400006    0.58330005
     5.45540237]
 ...
 [  60.26711784    0.3732        0.78280002    0.84940004    0.6092
     5.29640484]
 [  63.10411378    0.70970005    0.14820002    0.67399997    0.49039999
     5.41303062]
 [  77.85901048    0.68110007    0.44029999    0.58330005    0.1182
     4.29296923]][0m
[37m[1m[2023-07-17 11:37:33,827][257371] Max Reward on eval: 266.7466985411942[0m
[37m[1m[2023-07-17 11:37:33,827][257371] Min Reward on eval: -603.911817551218[0m
[37m[1m[2023-07-17 11:37:33,828][257371] Mean Reward across all agents: 0.14255779868583146[0m
[37m[1m[2023-07-17 11:37:33,828][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:37:33,830][257371] mean_value=-246.54327767256527, max_value=131.49483484676568[0m
[37m[1m[2023-07-17 11:37:33,833][257371] New mean coefficients: [[ 0.33497685  0.41204578 -0.35145628  0.70413244 -0.7399862   2.8355622 ]][0m
[37m[1m[2023-07-17 11:37:33,834][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:37:42,809][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 11:37:42,809][257371] FPS: 427950.55[0m
[36m[2023-07-17 11:37:42,811][257371] itr=1147, itrs=2000, Progress: 57.35%[0m
[36m[2023-07-17 11:37:54,644][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 11:37:54,644][257371] FPS: 327597.75[0m
[36m[2023-07-17 11:37:58,910][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:37:58,911][257371] Reward + Measures: [[58.4572324   0.93721038  0.59997231  0.94906092  0.09184033  5.05695677]][0m
[37m[1m[2023-07-17 11:37:58,911][257371] Max Reward on eval: 58.457232398040475[0m
[37m[1m[2023-07-17 11:37:58,911][257371] Min Reward on eval: 58.457232398040475[0m
[37m[1m[2023-07-17 11:37:58,911][257371] Mean Reward across all agents: 58.457232398040475[0m
[37m[1m[2023-07-17 11:37:58,912][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:38:03,870][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:38:03,870][257371] Reward + Measures: [[-281.39226388    0.65140003    0.1147        0.63440001    0.42599997
     5.40512466]
 [-318.87416929    0.45420003    0.19949999    0.5086        0.35470003
     5.50240803]
 [  49.59013794    0.94300002    0.2261        0.95859998    0.29980001
     4.77488756]
 ...
 [ 109.83814748    0.64780003    0.59280002    0.41380006    0.61059999
     4.21504974]
 [  92.03164316    0.56800002    0.199         0.60089999    0.27180001
     4.23138571]
 [-275.24644124    0.56090003    0.14240001    0.52080005    0.30239999
     5.37419271]][0m
[37m[1m[2023-07-17 11:38:03,870][257371] Max Reward on eval: 494.05205586245285[0m
[37m[1m[2023-07-17 11:38:03,871][257371] Min Reward on eval: -700.7612419066951[0m
[37m[1m[2023-07-17 11:38:03,871][257371] Mean Reward across all agents: -13.93826602624673[0m
[37m[1m[2023-07-17 11:38:03,871][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:38:03,875][257371] mean_value=-302.73723170654046, max_value=264.8645506826653[0m
[37m[1m[2023-07-17 11:38:03,878][257371] New mean coefficients: [[ 0.92300814  0.76276886 -0.3287922   0.8223274  -0.6386376   2.4311726 ]][0m
[37m[1m[2023-07-17 11:38:03,879][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:38:12,902][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 11:38:12,902][257371] FPS: 425636.23[0m
[36m[2023-07-17 11:38:12,905][257371] itr=1148, itrs=2000, Progress: 57.40%[0m
[36m[2023-07-17 11:38:25,018][257371] train() took 12.00 seconds to complete[0m
[36m[2023-07-17 11:38:25,018][257371] FPS: 319986.69[0m
[36m[2023-07-17 11:38:29,388][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:38:29,388][257371] Reward + Measures: [[61.65912384  0.93563998  0.59412098  0.94561034  0.08465733  5.08484888]][0m
[37m[1m[2023-07-17 11:38:29,389][257371] Max Reward on eval: 61.659123841073566[0m
[37m[1m[2023-07-17 11:38:29,389][257371] Min Reward on eval: 61.659123841073566[0m
[37m[1m[2023-07-17 11:38:29,389][257371] Mean Reward across all agents: 61.659123841073566[0m
[37m[1m[2023-07-17 11:38:29,390][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:38:34,688][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:38:34,689][257371] Reward + Measures: [[-14.62832123   0.23800002   0.67190003   0.71810001   0.72170001
    5.44484234]
 [  0.12521575   0.83429998   0.062        0.86539996   0.35460001
    4.40498018]
 [ 86.97843155   0.64300007   0.47049999   0.71239996   0.36199996
    4.781919  ]
 ...
 [102.791523     0.42950001   0.32790002   0.4391       0.2714
    4.63662815]
 [265.50616453   0.47370002   0.228        0.41020003   0.32969999
    4.89835262]
 [351.69297934   0.80529994   0.0209       0.86900008   0.75889999
    5.53436756]][0m
[37m[1m[2023-07-17 11:38:34,689][257371] Max Reward on eval: 425.85517903342844[0m
[37m[1m[2023-07-17 11:38:34,689][257371] Min Reward on eval: -212.04630272053183[0m
[37m[1m[2023-07-17 11:38:34,689][257371] Mean Reward across all agents: 62.64159252264878[0m
[37m[1m[2023-07-17 11:38:34,690][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:38:34,693][257371] mean_value=-198.61733058133632, max_value=124.2247020212805[0m
[37m[1m[2023-07-17 11:38:34,696][257371] New mean coefficients: [[ 1.2322145   1.1241615   0.19546497  1.5352571  -0.9776187   1.9806601 ]][0m
[37m[1m[2023-07-17 11:38:34,697][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:38:43,786][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 11:38:43,786][257371] FPS: 422579.26[0m
[36m[2023-07-17 11:38:43,788][257371] itr=1149, itrs=2000, Progress: 57.45%[0m
[36m[2023-07-17 11:38:55,667][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 11:38:55,668][257371] FPS: 326415.24[0m
[36m[2023-07-17 11:39:00,002][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:39:00,003][257371] Reward + Measures: [[66.00547673  0.93663102  0.63548332  0.94795734  0.07274767  5.11453199]][0m
[37m[1m[2023-07-17 11:39:00,003][257371] Max Reward on eval: 66.00547672517024[0m
[37m[1m[2023-07-17 11:39:00,003][257371] Min Reward on eval: 66.00547672517024[0m
[37m[1m[2023-07-17 11:39:00,003][257371] Mean Reward across all agents: 66.00547672517024[0m
[37m[1m[2023-07-17 11:39:00,004][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:39:05,040][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:39:05,041][257371] Reward + Measures: [[193.35829642   0.2552       0.79500002   0.55739999   0.7319001
    7.09518433]
 [147.588902     0.85000002   0.61230004   0.94350004   0.176
    5.31883669]
 [148.91183293   0.34640002   0.76680005   0.71120006   0.65170002
    6.48036337]
 ...
 [ 18.79612408   0.42430001   0.36499998   0.5467       0.45580003
    6.37736177]
 [121.84986257   0.64840001   0.63         0.87750006   0.40050003
    5.299685  ]
 [ 35.27664467   0.71820003   0.67270005   0.85340005   0.23989999
    5.21903944]][0m
[37m[1m[2023-07-17 11:39:05,041][257371] Max Reward on eval: 275.63439370002595[0m
[37m[1m[2023-07-17 11:39:05,041][257371] Min Reward on eval: -114.45131927803159[0m
[37m[1m[2023-07-17 11:39:05,041][257371] Mean Reward across all agents: 92.77527295885554[0m
[37m[1m[2023-07-17 11:39:05,042][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:39:05,046][257371] mean_value=-288.40455320741853, max_value=34.223608902587046[0m
[37m[1m[2023-07-17 11:39:05,048][257371] New mean coefficients: [[ 1.4967071  1.4453014  0.7181244  1.9046609 -1.043943   1.769536 ]][0m
[37m[1m[2023-07-17 11:39:05,049][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:39:14,180][257371] train() took 9.13 seconds to complete[0m
[36m[2023-07-17 11:39:14,180][257371] FPS: 420641.11[0m
[36m[2023-07-17 11:39:14,183][257371] itr=1150, itrs=2000, Progress: 57.50%[0m
[37m[1m[2023-07-17 11:42:45,991][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001130[0m
[36m[2023-07-17 11:42:58,303][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 11:42:58,304][257371] FPS: 327490.94[0m
[36m[2023-07-17 11:43:02,570][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:43:02,570][257371] Reward + Measures: [[88.16732186  0.95060498  0.66432565  0.95598727  0.047999    5.11442852]][0m
[37m[1m[2023-07-17 11:43:02,571][257371] Max Reward on eval: 88.16732185806362[0m
[37m[1m[2023-07-17 11:43:02,571][257371] Min Reward on eval: 88.16732185806362[0m
[37m[1m[2023-07-17 11:43:02,571][257371] Mean Reward across all agents: 88.16732185806362[0m
[37m[1m[2023-07-17 11:43:02,572][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:43:07,573][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:43:07,574][257371] Reward + Measures: [[165.16823909   0.94110006   0.48950002   0.94999999   0.10030001
    4.7874465 ]
 [ 63.95677338   0.88310003   0.38659999   0.90329999   0.16110002
    4.83471823]
 [123.63947461   0.45290002   0.58230001   0.64090008   0.34470001
    5.32487917]
 ...
 [125.57577555   0.80720007   0.66689998   0.82969999   0.1629
    5.02430344]
 [ 61.39162766   0.24679999   0.42640001   0.35880002   0.3599
    4.76031208]
 [104.50804568   0.85979998   0.0362       0.88280004   0.8215
    4.49139404]][0m
[37m[1m[2023-07-17 11:43:07,574][257371] Max Reward on eval: 224.4146723240614[0m
[37m[1m[2023-07-17 11:43:07,574][257371] Min Reward on eval: -204.65738677084445[0m
[37m[1m[2023-07-17 11:43:07,575][257371] Mean Reward across all agents: 66.79952378225849[0m
[37m[1m[2023-07-17 11:43:07,575][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:43:07,578][257371] mean_value=-200.63031270066404, max_value=96.15126106250851[0m
[37m[1m[2023-07-17 11:43:07,581][257371] New mean coefficients: [[ 1.1507039   0.560594    0.34829843  1.8032392  -1.1468021   1.547477  ]][0m
[37m[1m[2023-07-17 11:43:07,582][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:43:16,604][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 11:43:16,604][257371] FPS: 425707.09[0m
[36m[2023-07-17 11:43:16,606][257371] itr=1151, itrs=2000, Progress: 57.55%[0m
[36m[2023-07-17 11:43:28,526][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 11:43:28,527][257371] FPS: 325276.07[0m
[36m[2023-07-17 11:43:32,506][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:43:32,506][257371] Reward + Measures: [[97.17095473  0.95118999  0.66993064  0.95651597  0.04517167  5.18808794]][0m
[37m[1m[2023-07-17 11:43:32,506][257371] Max Reward on eval: 97.17095473158543[0m
[37m[1m[2023-07-17 11:43:32,507][257371] Min Reward on eval: 97.17095473158543[0m
[37m[1m[2023-07-17 11:43:32,507][257371] Mean Reward across all agents: 97.17095473158543[0m
[37m[1m[2023-07-17 11:43:32,507][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:43:37,326][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:43:37,327][257371] Reward + Measures: [[  50.47937058    0.75979996    0.53479999    0.84930003    0.20369999
     4.90721226]
 [  83.88028465    0.95340008    0.56240004    0.96560001    0.06330001
     5.40684032]
 [-104.88981068    0.61980003    0.19780003    0.62440002    0.20219998
     4.92608118]
 ...
 [ -19.39443912    0.70619994    0.2339        0.73210001    0.16680001
     4.88315916]
 [ 135.38640649    0.94420004    0.55369997    0.94659996    0.0561
     5.50826025]
 [-136.2157181     0.41840002    0.3184        0.53739995    0.39120004
     4.47222281]][0m
[37m[1m[2023-07-17 11:43:37,327][257371] Max Reward on eval: 148.72893050936983[0m
[37m[1m[2023-07-17 11:43:37,327][257371] Min Reward on eval: -281.47735666849184[0m
[37m[1m[2023-07-17 11:43:37,327][257371] Mean Reward across all agents: -18.873255663179947[0m
[37m[1m[2023-07-17 11:43:37,328][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:43:37,330][257371] mean_value=-306.70602848980246, max_value=20.938602452467137[0m
[37m[1m[2023-07-17 11:43:37,332][257371] New mean coefficients: [[ 1.1367569  -0.33205062  0.04269284  1.1000295  -0.8575239   1.6244373 ]][0m
[37m[1m[2023-07-17 11:43:37,333][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:43:46,294][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 11:43:46,294][257371] FPS: 428592.26[0m
[36m[2023-07-17 11:43:46,296][257371] itr=1152, itrs=2000, Progress: 57.60%[0m
[36m[2023-07-17 11:43:57,926][257371] train() took 11.52 seconds to complete[0m
[36m[2023-07-17 11:43:57,926][257371] FPS: 333445.05[0m
[36m[2023-07-17 11:44:02,254][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:44:02,255][257371] Reward + Measures: [[101.45529456   0.95222598   0.67869365   0.95704371   0.03981233
    5.19269705]][0m
[37m[1m[2023-07-17 11:44:02,255][257371] Max Reward on eval: 101.45529455865668[0m
[37m[1m[2023-07-17 11:44:02,255][257371] Min Reward on eval: 101.45529455865668[0m
[37m[1m[2023-07-17 11:44:02,255][257371] Mean Reward across all agents: 101.45529455865668[0m
[37m[1m[2023-07-17 11:44:02,256][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:44:07,517][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:44:07,518][257371] Reward + Measures: [[  3.62503022   0.51899999   0.23819999   0.50150007   0.31170002
    5.09843159]
 [ 26.02113216   0.89790004   0.13630001   0.88079995   0.35840002
    4.42444563]
 [-55.63727604   0.2694       0.17549999   0.30040002   0.2016
    5.07897949]
 ...
 [-11.14957658   0.32930002   0.4842       0.42400002   0.48990002
    6.73200703]
 [ 27.09627779   0.94360012   0.20900002   0.93769997   0.37760001
    4.46410751]
 [ 81.46776936   0.90340006   0.68629998   0.91309994   0.06940001
    4.61579275]][0m
[37m[1m[2023-07-17 11:44:07,518][257371] Max Reward on eval: 299.6675348907709[0m
[37m[1m[2023-07-17 11:44:07,518][257371] Min Reward on eval: -421.4672193393111[0m
[37m[1m[2023-07-17 11:44:07,518][257371] Mean Reward across all agents: 24.812215126438637[0m
[37m[1m[2023-07-17 11:44:07,519][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:44:07,522][257371] mean_value=-244.9833232550641, max_value=158.8888868214841[0m
[37m[1m[2023-07-17 11:44:07,524][257371] New mean coefficients: [[ 0.9992543   0.02191663  0.16228622  0.8710837  -0.31454855  2.3509974 ]][0m
[37m[1m[2023-07-17 11:44:07,526][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:44:16,553][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 11:44:16,553][257371] FPS: 425467.57[0m
[36m[2023-07-17 11:44:16,555][257371] itr=1153, itrs=2000, Progress: 57.65%[0m
[36m[2023-07-17 11:44:28,420][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 11:44:28,420][257371] FPS: 326783.90[0m
[36m[2023-07-17 11:44:32,788][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:44:32,788][257371] Reward + Measures: [[104.73816113   0.95576465   0.67312396   0.95932597   0.03871367
    5.16730547]][0m
[37m[1m[2023-07-17 11:44:32,789][257371] Max Reward on eval: 104.73816112560375[0m
[37m[1m[2023-07-17 11:44:32,789][257371] Min Reward on eval: 104.73816112560375[0m
[37m[1m[2023-07-17 11:44:32,789][257371] Mean Reward across all agents: 104.73816112560375[0m
[37m[1m[2023-07-17 11:44:32,790][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:44:37,879][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:44:37,880][257371] Reward + Measures: [[174.87255381   0.89040005   0.67110002   0.85929996   0.0105
    5.34973288]
 [ 78.89400152   0.68130004   0.1576       0.616        0.26390001
    4.19577742]
 [ 61.48166303   0.78310007   0.38480002   0.75690001   0.1169
    4.57304335]
 ...
 [  6.6852134    0.59569997   0.25319999   0.60519999   0.18810001
    4.61822796]
 [ 47.60639514   0.8872       0.6189       0.85329992   0.0126
    4.91072512]
 [-22.59786217   0.55050004   0.28490001   0.52600002   0.24879999
    3.84642529]][0m
[37m[1m[2023-07-17 11:44:37,880][257371] Max Reward on eval: 393.69048499800266[0m
[37m[1m[2023-07-17 11:44:37,880][257371] Min Reward on eval: -212.45418073274195[0m
[37m[1m[2023-07-17 11:44:37,880][257371] Mean Reward across all agents: 66.91119551487031[0m
[37m[1m[2023-07-17 11:44:37,881][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:44:37,884][257371] mean_value=-253.21805794506054, max_value=130.82017435838037[0m
[37m[1m[2023-07-17 11:44:37,886][257371] New mean coefficients: [[ 0.7295346  -0.48142284  0.22900203  0.6528269  -0.10306273  2.4188337 ]][0m
[37m[1m[2023-07-17 11:44:37,887][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:44:46,958][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 11:44:46,958][257371] FPS: 423423.89[0m
[36m[2023-07-17 11:44:46,961][257371] itr=1154, itrs=2000, Progress: 57.70%[0m
[36m[2023-07-17 11:44:58,789][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 11:44:58,789][257371] FPS: 327788.71[0m
[36m[2023-07-17 11:45:03,058][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:45:03,059][257371] Reward + Measures: [[104.79832213   0.95320833   0.65780097   0.95806104   0.04242333
    5.23536062]][0m
[37m[1m[2023-07-17 11:45:03,059][257371] Max Reward on eval: 104.7983221334229[0m
[37m[1m[2023-07-17 11:45:03,059][257371] Min Reward on eval: 104.7983221334229[0m
[37m[1m[2023-07-17 11:45:03,059][257371] Mean Reward across all agents: 104.7983221334229[0m
[37m[1m[2023-07-17 11:45:03,060][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:45:08,031][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:45:08,031][257371] Reward + Measures: [[  84.37412967    0.78009999    0.26429999    0.74290001    0.23280001
     4.93963385]
 [  19.99601548    0.56309998    0.33739999    0.49119997    0.21080001
     4.70812082]
 [  22.40279828    0.70249999    0.15979999    0.64930004    0.29860002
     4.82471418]
 ...
 [ 132.73448311    0.76429999    0.50170004    0.92250007    0.40480003
     5.54271269]
 [-124.33255357    0.59569997    0.17559999    0.838         0.70410007
     5.88264751]
 [  29.8206739     0.87080002    0.0465        0.88009995    0.53080004
     4.61636925]][0m
[37m[1m[2023-07-17 11:45:08,032][257371] Max Reward on eval: 163.52246049905662[0m
[37m[1m[2023-07-17 11:45:08,032][257371] Min Reward on eval: -250.43945168219506[0m
[37m[1m[2023-07-17 11:45:08,032][257371] Mean Reward across all agents: 4.873242925139508[0m
[37m[1m[2023-07-17 11:45:08,032][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:45:08,035][257371] mean_value=-289.95454405933395, max_value=263.2707379299494[0m
[37m[1m[2023-07-17 11:45:08,037][257371] New mean coefficients: [[ 0.5263531  -0.93328273  0.31844264  0.04785037  0.21082817  2.5785937 ]][0m
[37m[1m[2023-07-17 11:45:08,038][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:45:17,034][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 11:45:17,034][257371] FPS: 426923.32[0m
[36m[2023-07-17 11:45:17,036][257371] itr=1155, itrs=2000, Progress: 57.75%[0m
[36m[2023-07-17 11:45:28,655][257371] train() took 11.51 seconds to complete[0m
[36m[2023-07-17 11:45:28,655][257371] FPS: 333707.65[0m
[36m[2023-07-17 11:45:32,879][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:45:32,880][257371] Reward + Measures: [[112.05965818   0.94984764   0.64943665   0.95676666   0.04477333
    5.33964729]][0m
[37m[1m[2023-07-17 11:45:32,880][257371] Max Reward on eval: 112.05965817640136[0m
[37m[1m[2023-07-17 11:45:32,880][257371] Min Reward on eval: 112.05965817640136[0m
[37m[1m[2023-07-17 11:45:32,880][257371] Mean Reward across all agents: 112.05965817640136[0m
[37m[1m[2023-07-17 11:45:32,880][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:45:37,834][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:45:37,834][257371] Reward + Measures: [[  15.12522365    0.78380007    0.0759        0.81659997    0.33800003
     4.64512539]
 [-155.70705005    0.4729        0.21959999    0.46430001    0.3414
     4.53020096]
 [  94.06516705    0.78260005    0.32160002    0.82660002    0.1512
     5.36403179]
 ...
 [  58.10320445    0.81619996    0.3901        0.76340002    0.0893
     4.34440327]
 [  34.70650427    0.6178        0.16140001    0.58350003    0.20479999
     4.33238649]
 [ 160.80220991    0.95289993    0.59230006    0.95260012    0.0313
     5.32981634]][0m
[37m[1m[2023-07-17 11:45:37,834][257371] Max Reward on eval: 345.04578015320004[0m
[37m[1m[2023-07-17 11:45:37,835][257371] Min Reward on eval: -223.9382825873792[0m
[37m[1m[2023-07-17 11:45:37,835][257371] Mean Reward across all agents: 39.37748243916387[0m
[37m[1m[2023-07-17 11:45:37,835][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:45:37,837][257371] mean_value=-200.89314696369433, max_value=117.38707008168404[0m
[37m[1m[2023-07-17 11:45:37,840][257371] New mean coefficients: [[ 0.47153512 -0.41388732  0.42909992  0.35328582 -0.02930789  2.5235603 ]][0m
[37m[1m[2023-07-17 11:45:37,841][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:45:46,824][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 11:45:46,824][257371] FPS: 427559.82[0m
[36m[2023-07-17 11:45:46,826][257371] itr=1156, itrs=2000, Progress: 57.80%[0m
[36m[2023-07-17 11:45:58,837][257371] train() took 11.90 seconds to complete[0m
[36m[2023-07-17 11:45:58,837][257371] FPS: 322749.81[0m
[36m[2023-07-17 11:46:03,136][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:46:03,136][257371] Reward + Measures: [[109.62594228   0.95037329   0.65744603   0.95594203   0.043573
    5.38444138]][0m
[37m[1m[2023-07-17 11:46:03,136][257371] Max Reward on eval: 109.62594227969782[0m
[37m[1m[2023-07-17 11:46:03,137][257371] Min Reward on eval: 109.62594227969782[0m
[37m[1m[2023-07-17 11:46:03,137][257371] Mean Reward across all agents: 109.62594227969782[0m
[37m[1m[2023-07-17 11:46:03,137][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:46:08,121][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:46:08,121][257371] Reward + Measures: [[ 18.3795078    0.42110005   0.38999999   0.45890003   0.36279997
    5.18465042]
 [ 19.73443236   0.72840005   0.24159999   0.74239999   0.47310001
    4.98724985]
 [ 81.43313241   0.94690001   0.52739996   0.95660001   0.10399999
    5.40171671]
 ...
 [ -2.01888933   0.75349998   0.48500004   0.7834       0.24029998
    5.26820326]
 [ 96.15929703   0.94980001   0.4021       0.94630003   0.09240001
    4.68890238]
 [179.20494213   0.78450006   0.80830002   0.77419996   0.21470001
    6.86536407]][0m
[37m[1m[2023-07-17 11:46:08,121][257371] Max Reward on eval: 294.8137149604503[0m
[37m[1m[2023-07-17 11:46:08,122][257371] Min Reward on eval: -213.46366159925236[0m
[37m[1m[2023-07-17 11:46:08,122][257371] Mean Reward across all agents: 17.138381801397014[0m
[37m[1m[2023-07-17 11:46:08,122][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:46:08,124][257371] mean_value=-349.91539234642784, max_value=24.913639540945663[0m
[37m[1m[2023-07-17 11:46:08,127][257371] New mean coefficients: [[ 0.392845    0.3736118   0.67030066  0.45362478 -0.06367829  2.1527874 ]][0m
[37m[1m[2023-07-17 11:46:08,128][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:46:17,109][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 11:46:17,109][257371] FPS: 427641.38[0m
[36m[2023-07-17 11:46:17,111][257371] itr=1157, itrs=2000, Progress: 57.85%[0m
[36m[2023-07-17 11:46:28,858][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 11:46:28,858][257371] FPS: 330150.20[0m
[36m[2023-07-17 11:46:33,089][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:46:33,090][257371] Reward + Measures: [[109.70827977   0.94388473   0.67496067   0.95177358   0.052192
    5.40705585]][0m
[37m[1m[2023-07-17 11:46:33,090][257371] Max Reward on eval: 109.70827977217843[0m
[37m[1m[2023-07-17 11:46:33,090][257371] Min Reward on eval: 109.70827977217843[0m
[37m[1m[2023-07-17 11:46:33,091][257371] Mean Reward across all agents: 109.70827977217843[0m
[37m[1m[2023-07-17 11:46:33,091][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:46:38,037][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:46:38,037][257371] Reward + Measures: [[  23.19632102    0.94320005    0.36050001    0.95040005    0.17130001
     4.82991362]
 [  49.60832157    0.6024        0.29419997    0.55410004    0.10700001
     4.75173855]
 [  29.47003675    0.56340003    0.17050001    0.66930002    0.27129999
     4.32213354]
 ...
 [-159.31755681    0.49709997    0.2314        0.51920003    0.29799998
     4.05175543]
 [  -5.72088462    0.64820004    0.25790003    0.61970001    0.2888
     3.60457802]
 [  38.24978635    0.38319999    0.4032        0.49689999    0.47910005
     5.18952036]][0m
[37m[1m[2023-07-17 11:46:38,038][257371] Max Reward on eval: 224.929550162144[0m
[37m[1m[2023-07-17 11:46:38,038][257371] Min Reward on eval: -416.934740479663[0m
[37m[1m[2023-07-17 11:46:38,038][257371] Mean Reward across all agents: 22.743070143827122[0m
[37m[1m[2023-07-17 11:46:38,038][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:46:38,040][257371] mean_value=-415.42375837125417, max_value=338.8498759574321[0m
[37m[1m[2023-07-17 11:46:38,043][257371] New mean coefficients: [[ 0.5966175   0.24319485  0.42120677  0.5461977  -0.23145072  2.1371765 ]][0m
[37m[1m[2023-07-17 11:46:38,044][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:46:46,997][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 11:46:46,997][257371] FPS: 429007.15[0m
[36m[2023-07-17 11:46:46,999][257371] itr=1158, itrs=2000, Progress: 57.90%[0m
[36m[2023-07-17 11:46:58,753][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 11:46:58,753][257371] FPS: 329869.55[0m
[36m[2023-07-17 11:47:02,953][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:47:02,953][257371] Reward + Measures: [[110.36437378   0.92163497   0.66977966   0.94849837   0.08650867
    5.43416357]][0m
[37m[1m[2023-07-17 11:47:02,953][257371] Max Reward on eval: 110.36437377517686[0m
[37m[1m[2023-07-17 11:47:02,954][257371] Min Reward on eval: 110.36437377517686[0m
[37m[1m[2023-07-17 11:47:02,954][257371] Mean Reward across all agents: 110.36437377517686[0m
[37m[1m[2023-07-17 11:47:02,954][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:47:08,206][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:47:08,207][257371] Reward + Measures: [[ 77.45477472   0.94130003   0.65650004   0.92290002   0.0314
    5.23820877]
 [ -3.20172594   0.88959998   0.70410007   0.82050002   0.0454
    4.16708803]
 [ 93.16031316   0.77869999   0.67870003   0.93219995   0.18709999
    5.08391809]
 ...
 [144.72187713   0.67939997   0.61360002   0.83810008   0.289
    5.81826925]
 [  5.56493863   0.55239999   0.21140002   0.63010001   0.3098
    4.31791306]
 [136.60009958   0.49179998   0.48890001   0.61190003   0.40000001
    5.30735588]][0m
[37m[1m[2023-07-17 11:47:08,207][257371] Max Reward on eval: 235.4376545412466[0m
[37m[1m[2023-07-17 11:47:08,207][257371] Min Reward on eval: -194.90444826856256[0m
[37m[1m[2023-07-17 11:47:08,208][257371] Mean Reward across all agents: 70.11031979111009[0m
[37m[1m[2023-07-17 11:47:08,208][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:47:08,211][257371] mean_value=-232.34992394254988, max_value=14.288693770413659[0m
[37m[1m[2023-07-17 11:47:08,214][257371] New mean coefficients: [[ 0.7366739   0.02891371  0.24432653  0.841809   -0.26947117  2.8809745 ]][0m
[37m[1m[2023-07-17 11:47:08,215][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:47:17,215][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 11:47:17,215][257371] FPS: 426732.95[0m
[36m[2023-07-17 11:47:17,218][257371] itr=1159, itrs=2000, Progress: 57.95%[0m
[36m[2023-07-17 11:47:28,981][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 11:47:28,981][257371] FPS: 329553.03[0m
[36m[2023-07-17 11:47:33,265][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:47:33,266][257371] Reward + Measures: [[112.01486795   0.92491001   0.65954465   0.95057404   0.08599366
    5.45835781]][0m
[37m[1m[2023-07-17 11:47:33,266][257371] Max Reward on eval: 112.01486795420661[0m
[37m[1m[2023-07-17 11:47:33,266][257371] Min Reward on eval: 112.01486795420661[0m
[37m[1m[2023-07-17 11:47:33,266][257371] Mean Reward across all agents: 112.01486795420661[0m
[37m[1m[2023-07-17 11:47:33,267][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:47:38,294][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:47:38,295][257371] Reward + Measures: [[ 12.81030611   0.67970002   0.48020002   0.66799998   0.25560004
    4.08103466]
 [222.34855841   0.6918       0.81350005   0.82950002   0.35680002
    4.94767141]
 [ -0.92666889   0.66230005   0.0253       0.88380003   0.38279998
    4.5232954 ]
 ...
 [ 28.71962941   0.91619998   0.0489       0.92959994   0.49160001
    5.3058939 ]
 [  4.82855986   0.88570005   0.0594       0.88999999   0.4849
    5.77854538]
 [ -6.45766253   0.77430004   0.12650001   0.83649999   0.41529998
    4.98772621]][0m
[37m[1m[2023-07-17 11:47:38,295][257371] Max Reward on eval: 329.26463793106376[0m
[37m[1m[2023-07-17 11:47:38,295][257371] Min Reward on eval: -527.4978714223951[0m
[37m[1m[2023-07-17 11:47:38,295][257371] Mean Reward across all agents: 16.532158320932957[0m
[37m[1m[2023-07-17 11:47:38,296][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:47:38,299][257371] mean_value=-243.89779847401562, max_value=300.39436141190697[0m
[37m[1m[2023-07-17 11:47:38,302][257371] New mean coefficients: [[ 0.31820312 -0.6076299   0.42720133  0.5556668  -0.19455518  3.5058794 ]][0m
[37m[1m[2023-07-17 11:47:38,303][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:47:47,393][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 11:47:47,393][257371] FPS: 422530.60[0m
[36m[2023-07-17 11:47:47,395][257371] itr=1160, itrs=2000, Progress: 58.00%[0m
[37m[1m[2023-07-17 11:51:21,131][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001140[0m
[36m[2023-07-17 11:51:33,402][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 11:51:33,402][257371] FPS: 329475.28[0m
[36m[2023-07-17 11:51:37,579][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:51:37,580][257371] Reward + Measures: [[113.44846789   0.92715806   0.66569465   0.94924772   0.078082
    5.48025179]][0m
[37m[1m[2023-07-17 11:51:37,580][257371] Max Reward on eval: 113.44846788875712[0m
[37m[1m[2023-07-17 11:51:37,580][257371] Min Reward on eval: 113.44846788875712[0m
[37m[1m[2023-07-17 11:51:37,580][257371] Mean Reward across all agents: 113.44846788875712[0m
[37m[1m[2023-07-17 11:51:37,581][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:51:42,457][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:51:42,458][257371] Reward + Measures: [[  6.65260234   0.21439998   0.27790001   0.33180004   0.35160002
    5.48205805]
 [ 10.87200947   0.22450002   0.29260001   0.34370002   0.37049997
    5.88781309]
 [ 19.35270085   0.54699999   0.38940004   0.57080001   0.39030001
    4.28435612]
 ...
 [-57.12970416   0.57390004   0.26019999   0.59009999   0.32230002
    4.06912231]
 [ 52.12188679   0.79229999   0.18810001   0.83529997   0.171
    4.57313013]
 [ 12.67336796   0.89869994   0.1988       0.91009998   0.2247
    5.18722677]][0m
[37m[1m[2023-07-17 11:51:42,458][257371] Max Reward on eval: 281.1588090909645[0m
[37m[1m[2023-07-17 11:51:42,458][257371] Min Reward on eval: -160.2523669656366[0m
[37m[1m[2023-07-17 11:51:42,459][257371] Mean Reward across all agents: 29.44104165401509[0m
[37m[1m[2023-07-17 11:51:42,459][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:51:42,461][257371] mean_value=-267.61242860664913, max_value=88.32059153574875[0m
[37m[1m[2023-07-17 11:51:42,463][257371] New mean coefficients: [[ 0.11697908 -0.57084703  0.31701598  0.3723952   0.12052138  4.3756785 ]][0m
[37m[1m[2023-07-17 11:51:42,464][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:51:51,562][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 11:51:51,563][257371] FPS: 422131.43[0m
[36m[2023-07-17 11:51:51,565][257371] itr=1161, itrs=2000, Progress: 58.05%[0m
[36m[2023-07-17 11:52:03,273][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 11:52:03,274][257371] FPS: 331112.69[0m
[36m[2023-07-17 11:52:07,521][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:52:07,522][257371] Reward + Measures: [[112.6128581    0.91718602   0.65404993   0.94790232   0.09226801
    5.51483345]][0m
[37m[1m[2023-07-17 11:52:07,522][257371] Max Reward on eval: 112.61285809955967[0m
[37m[1m[2023-07-17 11:52:07,522][257371] Min Reward on eval: 112.61285809955967[0m
[37m[1m[2023-07-17 11:52:07,523][257371] Mean Reward across all agents: 112.61285809955967[0m
[37m[1m[2023-07-17 11:52:07,523][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:52:12,456][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:52:12,457][257371] Reward + Measures: [[ 75.59110084   0.89569998   0.16769999   0.89260006   0.2181
    6.38952208]
 [ 70.54889911   0.67309999   0.66320002   0.62540001   0.2428
    4.64594126]
 [ 78.99423672   0.69410002   0.10570001   0.74309999   0.32080001
    5.74941635]
 ...
 [ 34.50109964   0.92950004   0.0583       0.93180001   0.41630003
    5.23104429]
 [108.31150386   0.94929999   0.59909999   0.94929999   0.0524
    5.09560871]
 [ -3.93282913   0.71709996   0.0905       0.73840004   0.24880002
    4.23065186]][0m
[37m[1m[2023-07-17 11:52:12,457][257371] Max Reward on eval: 485.7085689678788[0m
[37m[1m[2023-07-17 11:52:12,457][257371] Min Reward on eval: -325.2576904019341[0m
[37m[1m[2023-07-17 11:52:12,458][257371] Mean Reward across all agents: 48.486327673920094[0m
[37m[1m[2023-07-17 11:52:12,458][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:52:12,461][257371] mean_value=-206.2620785420605, max_value=228.5413066021652[0m
[37m[1m[2023-07-17 11:52:12,464][257371] New mean coefficients: [[0.34562278 0.7122631  0.9022528  1.1039127  0.32672584 4.148423  ]][0m
[37m[1m[2023-07-17 11:52:12,465][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:52:21,404][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 11:52:21,404][257371] FPS: 429664.22[0m
[36m[2023-07-17 11:52:21,406][257371] itr=1162, itrs=2000, Progress: 58.10%[0m
[36m[2023-07-17 11:52:33,352][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 11:52:33,353][257371] FPS: 324479.05[0m
[36m[2023-07-17 11:52:37,665][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:52:37,666][257371] Reward + Measures: [[91.43137854  0.79421759  0.61539066  0.92318165  0.25167301  5.53273344]][0m
[37m[1m[2023-07-17 11:52:37,666][257371] Max Reward on eval: 91.4313785428963[0m
[37m[1m[2023-07-17 11:52:37,666][257371] Min Reward on eval: 91.4313785428963[0m
[37m[1m[2023-07-17 11:52:37,666][257371] Mean Reward across all agents: 91.4313785428963[0m
[37m[1m[2023-07-17 11:52:37,667][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:52:42,965][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:52:42,965][257371] Reward + Measures: [[ 197.53057766    0.70230001    0.52689999    0.72200006    0.25370002
     5.54412985]
 [ 141.06420389    0.442         0.6983        0.7766        0.5485
     5.27812338]
 [ -67.02694702    0.76069993    0.75129998    0.79809999    0.32960001
     5.84255552]
 ...
 [-103.08833288    0.69519997    0.6904        0.63859999    0.2359
     6.36887217]
 [ 141.31305408    0.2764        0.87460005    0.78890002    0.7256
     5.76499701]
 [ 126.50222587    0.58230001    0.79930001    0.8962        0.4131
     5.4340744 ]][0m
[37m[1m[2023-07-17 11:52:42,965][257371] Max Reward on eval: 523.0901046943385[0m
[37m[1m[2023-07-17 11:52:42,966][257371] Min Reward on eval: -240.45874930601568[0m
[37m[1m[2023-07-17 11:52:42,966][257371] Mean Reward across all agents: 25.319423477171892[0m
[37m[1m[2023-07-17 11:52:42,966][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:52:42,970][257371] mean_value=-324.71750655567394, max_value=354.6924319692946[0m
[37m[1m[2023-07-17 11:52:42,973][257371] New mean coefficients: [[ 0.37022722  0.04091549  0.5782248   0.39821595 -0.14022541  4.456785  ]][0m
[37m[1m[2023-07-17 11:52:42,974][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:52:52,078][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 11:52:52,078][257371] FPS: 421851.97[0m
[36m[2023-07-17 11:52:52,081][257371] itr=1163, itrs=2000, Progress: 58.15%[0m
[36m[2023-07-17 11:53:03,964][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 11:53:03,964][257371] FPS: 326295.79[0m
[36m[2023-07-17 11:53:08,310][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:53:08,311][257371] Reward + Measures: [[95.56514307  0.81811833  0.61417067  0.92998171  0.22345833  5.5694294 ]][0m
[37m[1m[2023-07-17 11:53:08,311][257371] Max Reward on eval: 95.5651430684713[0m
[37m[1m[2023-07-17 11:53:08,311][257371] Min Reward on eval: 95.5651430684713[0m
[37m[1m[2023-07-17 11:53:08,311][257371] Mean Reward across all agents: 95.5651430684713[0m
[37m[1m[2023-07-17 11:53:08,312][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:53:13,376][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:53:13,382][257371] Reward + Measures: [[ 60.84026763   0.1418       0.76969999   0.61540002   0.79949999
    5.17984152]
 [ 71.35533547   0.62449998   0.2615       0.67589998   0.20510001
    5.44633055]
 [ 51.30268489   0.31489998   0.71470004   0.51789999   0.6771
    4.76976109]
 ...
 [-38.81409392   0.81639999   0.0247       0.89489996   0.72329998
    4.68744802]
 [ 81.12459539   0.36950001   0.35870001   0.36610001   0.3026
    3.4772861 ]
 [-20.7361093    0.80949992   0.0644       0.88529998   0.55309999
    3.81586909]][0m
[37m[1m[2023-07-17 11:53:13,382][257371] Max Reward on eval: 320.12193605527284[0m
[37m[1m[2023-07-17 11:53:13,382][257371] Min Reward on eval: -590.9453130122274[0m
[37m[1m[2023-07-17 11:53:13,383][257371] Mean Reward across all agents: 41.08290721269721[0m
[37m[1m[2023-07-17 11:53:13,383][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:53:13,386][257371] mean_value=-287.5176614158086, max_value=400.1313136617852[0m
[37m[1m[2023-07-17 11:53:13,388][257371] New mean coefficients: [[ 0.51505554  0.33296773  0.32620826  0.87519383 -0.22365987  3.6895638 ]][0m
[37m[1m[2023-07-17 11:53:13,389][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:53:22,350][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 11:53:22,350][257371] FPS: 428608.31[0m
[36m[2023-07-17 11:53:22,353][257371] itr=1164, itrs=2000, Progress: 58.20%[0m
[36m[2023-07-17 11:53:34,000][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 11:53:34,001][257371] FPS: 332811.78[0m
[36m[2023-07-17 11:53:38,227][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:53:38,228][257371] Reward + Measures: [[95.75132754  0.85485703  0.60176432  0.93451703  0.18383066  5.60135174]][0m
[37m[1m[2023-07-17 11:53:38,228][257371] Max Reward on eval: 95.75132754492438[0m
[37m[1m[2023-07-17 11:53:38,228][257371] Min Reward on eval: 95.75132754492438[0m
[37m[1m[2023-07-17 11:53:38,228][257371] Mean Reward across all agents: 95.75132754492438[0m
[37m[1m[2023-07-17 11:53:38,229][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:53:43,197][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:53:43,198][257371] Reward + Measures: [[166.83456029   0.86300004   0.43109998   0.91239995   0.20559998
    5.41724157]
 [145.41429203   0.48140001   0.16949999   0.65710008   0.51620001
    5.26514292]
 [ 80.94577788   0.84829998   0.52089995   0.81540006   0.1242
    4.49829102]
 ...
 [-26.79281343   0.64209998   0.19930001   0.77820003   0.47750002
    4.55050039]
 [221.21819827   0.28569999   0.63499999   0.81500006   0.7841
    5.40204477]
 [ 14.51423456   0.36590001   0.42090002   0.51920003   0.51319999
    4.42861605]][0m
[37m[1m[2023-07-17 11:53:43,198][257371] Max Reward on eval: 338.95085774734616[0m
[37m[1m[2023-07-17 11:53:43,198][257371] Min Reward on eval: -301.14359947182237[0m
[37m[1m[2023-07-17 11:53:43,199][257371] Mean Reward across all agents: 67.71863980206503[0m
[37m[1m[2023-07-17 11:53:43,199][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:53:43,203][257371] mean_value=-227.72121624156134, max_value=127.79565945268305[0m
[37m[1m[2023-07-17 11:53:43,206][257371] New mean coefficients: [[ 0.725232    0.23227578  0.14104229  1.0272892  -0.2933511   3.9140372 ]][0m
[37m[1m[2023-07-17 11:53:43,207][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:53:52,155][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 11:53:52,156][257371] FPS: 429202.63[0m
[36m[2023-07-17 11:53:52,158][257371] itr=1165, itrs=2000, Progress: 58.25%[0m
[36m[2023-07-17 11:54:03,766][257371] train() took 11.49 seconds to complete[0m
[36m[2023-07-17 11:54:03,766][257371] FPS: 334138.05[0m
[36m[2023-07-17 11:54:08,085][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:54:08,091][257371] Reward + Measures: [[97.32160243  0.86576271  0.58790767  0.93699968  0.177387    5.64204311]][0m
[37m[1m[2023-07-17 11:54:08,091][257371] Max Reward on eval: 97.32160242966935[0m
[37m[1m[2023-07-17 11:54:08,091][257371] Min Reward on eval: 97.32160242966935[0m
[37m[1m[2023-07-17 11:54:08,092][257371] Mean Reward across all agents: 97.32160242966935[0m
[37m[1m[2023-07-17 11:54:08,092][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:54:13,039][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:54:13,039][257371] Reward + Measures: [[152.1528213    0.71149999   0.3118       0.78640002   0.4914
    5.64558029]
 [-11.74160373   0.80699998   0.1825       0.82120001   0.1389
    3.84176254]
 [  7.42454186   0.29410002   0.72509998   0.4377       0.74440002
    5.25761795]
 ...
 [ 92.3666596    0.48720002   0.24620001   0.51019996   0.36450002
    4.07895899]
 [ -9.94399674   0.6936       0.2392       0.72399998   0.2119
    4.13549566]
 [ 21.91896402   0.53450006   0.31780002   0.58240002   0.32539999
    3.87857318]][0m
[37m[1m[2023-07-17 11:54:13,040][257371] Max Reward on eval: 306.71482278755576[0m
[37m[1m[2023-07-17 11:54:13,040][257371] Min Reward on eval: -217.26023911908268[0m
[37m[1m[2023-07-17 11:54:13,040][257371] Mean Reward across all agents: 45.970465941886005[0m
[37m[1m[2023-07-17 11:54:13,040][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:54:13,044][257371] mean_value=-226.81298344248606, max_value=91.74741301826225[0m
[37m[1m[2023-07-17 11:54:13,046][257371] New mean coefficients: [[ 0.7902764   1.2232494   0.47987193  1.5573087  -0.2616107   4.1075397 ]][0m
[37m[1m[2023-07-17 11:54:13,047][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:54:21,942][257371] train() took 8.89 seconds to complete[0m
[36m[2023-07-17 11:54:21,942][257371] FPS: 431813.89[0m
[36m[2023-07-17 11:54:21,944][257371] itr=1166, itrs=2000, Progress: 58.30%[0m
[36m[2023-07-17 11:54:33,741][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 11:54:33,741][257371] FPS: 328631.93[0m
[36m[2023-07-17 11:54:38,052][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:54:38,052][257371] Reward + Measures: [[101.57858996   0.92625666   0.57625568   0.95221436   0.113928
    5.67401314]][0m
[37m[1m[2023-07-17 11:54:38,052][257371] Max Reward on eval: 101.57858995966521[0m
[37m[1m[2023-07-17 11:54:38,053][257371] Min Reward on eval: 101.57858995966521[0m
[37m[1m[2023-07-17 11:54:38,053][257371] Mean Reward across all agents: 101.57858995966521[0m
[37m[1m[2023-07-17 11:54:38,053][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:54:43,025][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:54:43,026][257371] Reward + Measures: [[-129.74707278    0.32570001    0.19240001    0.31019998    0.3055
     5.26411772]
 [ -53.01790836    0.51820004    0.2043        0.47690001    0.25240001
     4.9024806 ]
 [  17.69637869    0.61669999    0.1128        0.59130001    0.29899999
     5.01033354]
 ...
 [ 131.03709074    0.493         0.39399999    0.72930002    0.5765
     5.89592028]
 [  74.22620004    0.7184        0.12950002    0.7349        0.4368
     5.94758129]
 [ -33.86428185    0.52360004    0.35590002    0.57089996    0.4745
     5.39582777]][0m
[37m[1m[2023-07-17 11:54:43,026][257371] Max Reward on eval: 274.83174671037125[0m
[37m[1m[2023-07-17 11:54:43,026][257371] Min Reward on eval: -283.0789041557349[0m
[37m[1m[2023-07-17 11:54:43,026][257371] Mean Reward across all agents: 35.25437814519176[0m
[37m[1m[2023-07-17 11:54:43,027][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:54:43,030][257371] mean_value=-215.0232548454157, max_value=289.399402122805[0m
[37m[1m[2023-07-17 11:54:43,032][257371] New mean coefficients: [[ 1.0848289   0.20850599  0.28543723  1.1839073  -0.5145113   4.5462766 ]][0m
[37m[1m[2023-07-17 11:54:43,033][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:54:52,194][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-17 11:54:52,194][257371] FPS: 419252.96[0m
[36m[2023-07-17 11:54:52,197][257371] itr=1167, itrs=2000, Progress: 58.35%[0m
[36m[2023-07-17 11:55:03,917][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 11:55:03,917][257371] FPS: 330825.31[0m
[36m[2023-07-17 11:55:08,136][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:55:08,136][257371] Reward + Measures: [[97.57487645  0.91334432  0.55747366  0.94837737  0.13534433  5.67928791]][0m
[37m[1m[2023-07-17 11:55:08,136][257371] Max Reward on eval: 97.57487644708476[0m
[37m[1m[2023-07-17 11:55:08,136][257371] Min Reward on eval: 97.57487644708476[0m
[37m[1m[2023-07-17 11:55:08,137][257371] Mean Reward across all agents: 97.57487644708476[0m
[37m[1m[2023-07-17 11:55:08,137][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:55:13,112][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:55:13,112][257371] Reward + Measures: [[ 28.40994387   0.38080001   0.58270001   0.72249997   0.4842
    5.26600218]
 [  3.25627445   0.91070002   0.0779       0.91850007   0.32359999
    5.32323503]
 [ 81.8299458    0.65799999   0.55870003   0.90469998   0.38460001
    5.41193485]
 ...
 [-37.72551241   0.44080001   0.65650004   0.6749       0.5733
    5.50856495]
 [129.91721626   0.71170002   0.67820007   0.88599998   0.31890002
    5.44834423]
 [ 92.08916703   0.84310001   0.69080001   0.92690003   0.13520001
    5.63757086]][0m
[37m[1m[2023-07-17 11:55:13,113][257371] Max Reward on eval: 206.46281625889242[0m
[37m[1m[2023-07-17 11:55:13,113][257371] Min Reward on eval: -145.4563360184431[0m
[37m[1m[2023-07-17 11:55:13,113][257371] Mean Reward across all agents: 47.64933787471689[0m
[37m[1m[2023-07-17 11:55:13,113][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:55:13,117][257371] mean_value=-253.45683174595644, max_value=16.603562696586465[0m
[37m[1m[2023-07-17 11:55:13,120][257371] New mean coefficients: [[ 1.2850908  -0.0366974   0.20701754  1.116036   -0.3459009   5.494437  ]][0m
[37m[1m[2023-07-17 11:55:13,121][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:55:22,099][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 11:55:22,100][257371] FPS: 427750.17[0m
[36m[2023-07-17 11:55:22,102][257371] itr=1168, itrs=2000, Progress: 58.40%[0m
[36m[2023-07-17 11:55:33,869][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 11:55:33,869][257371] FPS: 329486.55[0m
[36m[2023-07-17 11:55:38,235][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:55:38,235][257371] Reward + Measures: [[94.58009312  0.91340524  0.51592201  0.94839162  0.150819    5.71677256]][0m
[37m[1m[2023-07-17 11:55:38,235][257371] Max Reward on eval: 94.58009312423644[0m
[37m[1m[2023-07-17 11:55:38,236][257371] Min Reward on eval: 94.58009312423644[0m
[37m[1m[2023-07-17 11:55:38,236][257371] Mean Reward across all agents: 94.58009312423644[0m
[37m[1m[2023-07-17 11:55:38,236][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:55:43,641][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:55:43,641][257371] Reward + Measures: [[  3.68324859   0.6523       0.2949       0.63830006   0.20990001
    5.18000317]
 [-14.4348697    0.74419999   0.5248       0.73210001   0.11619999
    5.10699034]
 [-40.11325814   0.90480006   0.0658       0.90380001   0.44110003
    6.32848358]
 ...
 [ 77.33424144   0.6433       0.44080001   0.74440002   0.32069999
    5.59414434]
 [-56.17132092   0.9314       0.0394       0.92960006   0.50730002
    5.86351347]
 [  7.58848937   0.44039997   0.53839999   0.57190001   0.46160004
    4.95001221]][0m
[37m[1m[2023-07-17 11:55:43,641][257371] Max Reward on eval: 149.43268121369184[0m
[37m[1m[2023-07-17 11:55:43,642][257371] Min Reward on eval: -196.91117667630314[0m
[37m[1m[2023-07-17 11:55:43,642][257371] Mean Reward across all agents: 11.370852283355596[0m
[37m[1m[2023-07-17 11:55:43,642][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:55:43,645][257371] mean_value=-256.5063292700842, max_value=30.36423843179982[0m
[37m[1m[2023-07-17 11:55:43,647][257371] New mean coefficients: [[ 0.7621971  -0.09669245  0.3205881   1.0324925  -0.4456241   5.6571445 ]][0m
[37m[1m[2023-07-17 11:55:43,648][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:55:52,725][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 11:55:52,725][257371] FPS: 423141.99[0m
[36m[2023-07-17 11:55:52,727][257371] itr=1169, itrs=2000, Progress: 58.45%[0m
[36m[2023-07-17 11:56:04,399][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 11:56:04,400][257371] FPS: 332217.96[0m
[36m[2023-07-17 11:56:08,816][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:56:08,816][257371] Reward + Measures: [[98.90092385  0.91880661  0.50717801  0.94725001  0.13939933  5.73183346]][0m
[37m[1m[2023-07-17 11:56:08,817][257371] Max Reward on eval: 98.90092384637748[0m
[37m[1m[2023-07-17 11:56:08,817][257371] Min Reward on eval: 98.90092384637748[0m
[37m[1m[2023-07-17 11:56:08,817][257371] Mean Reward across all agents: 98.90092384637748[0m
[37m[1m[2023-07-17 11:56:08,817][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:56:13,900][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 11:56:13,901][257371] Reward + Measures: [[132.23030543   0.74620003   0.59490007   0.91219997   0.25040001
    5.78313398]
 [126.56718492   0.72290003   0.57660002   0.83990002   0.25
    5.5937748 ]
 [ 46.34887452   0.56760001   0.40980002   0.6031       0.4382
    5.55906439]
 ...
 [ 23.54255733   0.79879999   0.31050003   0.78439999   0.3538
    5.2470336 ]
 [ 84.72515182   0.67260003   0.35820001   0.77270001   0.3168
    5.18626261]
 [127.38330176   0.81440002   0.57730001   0.91030008   0.21700001
    6.00408697]][0m
[37m[1m[2023-07-17 11:56:13,901][257371] Max Reward on eval: 231.3083305373788[0m
[37m[1m[2023-07-17 11:56:13,902][257371] Min Reward on eval: -299.76866150833666[0m
[37m[1m[2023-07-17 11:56:13,902][257371] Mean Reward across all agents: 69.6007353202045[0m
[37m[1m[2023-07-17 11:56:13,902][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 11:56:13,906][257371] mean_value=-198.78898709217037, max_value=295.1145891925539[0m
[37m[1m[2023-07-17 11:56:13,909][257371] New mean coefficients: [[ 0.6556493  -0.43475387  0.04655671  1.1347324  -0.4790818   5.4774985 ]][0m
[37m[1m[2023-07-17 11:56:13,910][257371] Moving the mean solution point...[0m
[36m[2023-07-17 11:56:22,994][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 11:56:22,994][257371] FPS: 422804.02[0m
[36m[2023-07-17 11:56:22,996][257371] itr=1170, itrs=2000, Progress: 58.50%[0m
[37m[1m[2023-07-17 11:59:57,608][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001150[0m
[36m[2023-07-17 12:00:09,869][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 12:00:09,869][257371] FPS: 329523.50[0m
[36m[2023-07-17 12:00:14,072][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:00:14,073][257371] Reward + Measures: [[99.33517129  0.92672133  0.50340933  0.95025504  0.133334    5.79156446]][0m
[37m[1m[2023-07-17 12:00:14,073][257371] Max Reward on eval: 99.33517129024429[0m
[37m[1m[2023-07-17 12:00:14,073][257371] Min Reward on eval: 99.33517129024429[0m
[37m[1m[2023-07-17 12:00:14,074][257371] Mean Reward across all agents: 99.33517129024429[0m
[37m[1m[2023-07-17 12:00:14,074][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:00:18,853][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:00:18,854][257371] Reward + Measures: [[ 56.55978571   0.80510008   0.2728       0.82340002   0.1235
    4.50748587]
 [ 28.32087991   0.52710003   0.28280002   0.53360003   0.2802
    4.75552511]
 [ 79.55348439   0.86740011   0.2218       0.87439996   0.2696
    5.20777273]
 ...
 [117.50198279   0.9320001    0.35320002   0.93810004   0.21359999
    5.83453083]
 [143.89727882   0.87709999   0.53969997   0.86339998   0.04550001
    4.76063347]
 [ 31.98804604   0.33670002   0.44460002   0.5097       0.49160004
    4.94090223]][0m
[37m[1m[2023-07-17 12:00:18,854][257371] Max Reward on eval: 573.7984438292682[0m
[37m[1m[2023-07-17 12:00:18,854][257371] Min Reward on eval: -59.290222835401075[0m
[37m[1m[2023-07-17 12:00:18,855][257371] Mean Reward across all agents: 100.75090914086283[0m
[37m[1m[2023-07-17 12:00:18,855][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:00:18,858][257371] mean_value=-183.67890889119644, max_value=191.7071998645226[0m
[37m[1m[2023-07-17 12:00:18,861][257371] New mean coefficients: [[ 0.10927337 -1.2718055  -0.0787345   0.82297695 -0.40307903  5.27486   ]][0m
[37m[1m[2023-07-17 12:00:18,862][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:00:27,850][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 12:00:27,851][257371] FPS: 427307.29[0m
[36m[2023-07-17 12:00:27,853][257371] itr=1171, itrs=2000, Progress: 58.55%[0m
[36m[2023-07-17 12:00:39,698][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 12:00:39,698][257371] FPS: 327197.82[0m
[36m[2023-07-17 12:00:43,967][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:00:43,967][257371] Reward + Measures: [[93.39293147  0.77777606  0.53436297  0.90943968  0.29413235  5.83329964]][0m
[37m[1m[2023-07-17 12:00:43,967][257371] Max Reward on eval: 93.39293147428691[0m
[37m[1m[2023-07-17 12:00:43,968][257371] Min Reward on eval: 93.39293147428691[0m
[37m[1m[2023-07-17 12:00:43,968][257371] Mean Reward across all agents: 93.39293147428691[0m
[37m[1m[2023-07-17 12:00:43,968][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:00:48,941][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:00:48,941][257371] Reward + Measures: [[-147.75556592    0.52910006    0.18740001    0.53789997    0.31529999
     4.60116529]
 [ 137.3074163     0.95109999    0.53439999    0.95090002    0.09060001
     5.78828192]
 [ 122.43526694    0.59149998    0.59250003    0.83570004    0.36339998
     5.46699286]
 ...
 [  -0.73759705    0.44299999    0.25319999    0.41050002    0.31329998
     4.14986515]
 [  30.41701612    0.89370006    0.22260001    0.89480013    0.3073
     5.93438816]
 [-113.94497104    0.5909        0.43560001    0.63609999    0.1055
     4.99100685]][0m
[37m[1m[2023-07-17 12:00:48,942][257371] Max Reward on eval: 274.47174832243473[0m
[37m[1m[2023-07-17 12:00:48,942][257371] Min Reward on eval: -233.26130963600008[0m
[37m[1m[2023-07-17 12:00:48,942][257371] Mean Reward across all agents: 41.573973124744235[0m
[37m[1m[2023-07-17 12:00:48,942][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:00:48,945][257371] mean_value=-252.69389096711967, max_value=46.41695206725659[0m
[37m[1m[2023-07-17 12:00:48,948][257371] New mean coefficients: [[ 0.29190382 -0.81279194  0.02577329  1.2635311  -0.20040837  5.407835  ]][0m
[37m[1m[2023-07-17 12:00:48,949][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:00:58,012][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 12:00:58,012][257371] FPS: 423795.30[0m
[36m[2023-07-17 12:00:58,014][257371] itr=1172, itrs=2000, Progress: 58.60%[0m
[36m[2023-07-17 12:01:09,698][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 12:01:09,698][257371] FPS: 331798.62[0m
[36m[2023-07-17 12:01:13,989][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:01:13,989][257371] Reward + Measures: [[94.83216584  0.80181491  0.51243466  0.91311365  0.26861534  5.86363792]][0m
[37m[1m[2023-07-17 12:01:13,989][257371] Max Reward on eval: 94.8321658377746[0m
[37m[1m[2023-07-17 12:01:13,990][257371] Min Reward on eval: 94.8321658377746[0m
[37m[1m[2023-07-17 12:01:13,990][257371] Mean Reward across all agents: 94.8321658377746[0m
[37m[1m[2023-07-17 12:01:13,990][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:01:18,962][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:01:18,963][257371] Reward + Measures: [[-41.29210614   0.8118       0.4244       0.65639997   0.024
    4.27882385]
 [110.5359049    0.31710002   0.5862       0.5887       0.51600003
    5.79996443]
 [136.75290533   0.49850002   0.67299998   0.8434       0.5226
    6.08277845]
 ...
 [ 38.42278892   0.79479998   0.1904       0.7999       0.40720001
    5.17199135]
 [ 81.44401602   0.88659996   0.23599999   0.92159998   0.29359999
    5.82267189]
 [ 69.70228768   0.9382       0.89200002   0.93699998   0.0306
    5.13526917]][0m
[37m[1m[2023-07-17 12:01:18,963][257371] Max Reward on eval: 681.8856620469247[0m
[37m[1m[2023-07-17 12:01:18,963][257371] Min Reward on eval: -250.55861376686954[0m
[37m[1m[2023-07-17 12:01:18,963][257371] Mean Reward across all agents: 82.343508327378[0m
[37m[1m[2023-07-17 12:01:18,964][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:01:18,968][257371] mean_value=-219.15268097153233, max_value=211.58117829895411[0m
[37m[1m[2023-07-17 12:01:18,970][257371] New mean coefficients: [[ 0.37412494 -0.767352   -0.04565919  0.89194417 -0.5411304   4.9439235 ]][0m
[37m[1m[2023-07-17 12:01:18,972][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:01:27,856][257371] train() took 8.88 seconds to complete[0m
[36m[2023-07-17 12:01:27,856][257371] FPS: 432291.28[0m
[36m[2023-07-17 12:01:27,859][257371] itr=1173, itrs=2000, Progress: 58.65%[0m
[36m[2023-07-17 12:01:39,815][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-17 12:01:39,815][257371] FPS: 324151.16[0m
[36m[2023-07-17 12:01:44,130][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:01:44,130][257371] Reward + Measures: [[93.90768753  0.83391893  0.46831232  0.920075    0.25940168  5.89246798]][0m
[37m[1m[2023-07-17 12:01:44,130][257371] Max Reward on eval: 93.90768753449021[0m
[37m[1m[2023-07-17 12:01:44,131][257371] Min Reward on eval: 93.90768753449021[0m
[37m[1m[2023-07-17 12:01:44,131][257371] Mean Reward across all agents: 93.90768753449021[0m
[37m[1m[2023-07-17 12:01:44,131][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:01:49,103][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:01:49,103][257371] Reward + Measures: [[ 75.37855113   0.66149998   0.60000008   0.83660001   0.32109997
    5.34118605]
 [ 12.29092718   0.3457       0.33120003   0.49309999   0.48210001
    4.87766409]
 [ 63.70010204   0.20810001   0.54809999   0.39750001   0.55040002
    4.71456242]
 ...
 [120.00059412   0.84069997   0.50459999   0.93810004   0.19579999
    5.85818577]
 [ 64.3306198    0.77220005   0.37849998   0.74439996   0.0511
    4.54334974]
 [ 16.46733419   0.37840003   0.3671       0.44530001   0.3524
    4.05218077]][0m
[37m[1m[2023-07-17 12:01:49,103][257371] Max Reward on eval: 181.16415118947626[0m
[37m[1m[2023-07-17 12:01:49,103][257371] Min Reward on eval: -130.50559079907833[0m
[37m[1m[2023-07-17 12:01:49,103][257371] Mean Reward across all agents: 44.23454335730558[0m
[37m[1m[2023-07-17 12:01:49,104][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:01:49,106][257371] mean_value=-326.61535737213285, max_value=24.268884693139086[0m
[37m[1m[2023-07-17 12:01:49,108][257371] New mean coefficients: [[ 0.20187405 -1.283681   -0.40871826  0.10829306 -0.45194533  3.9329762 ]][0m
[37m[1m[2023-07-17 12:01:49,109][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:01:58,027][257371] train() took 8.92 seconds to complete[0m
[36m[2023-07-17 12:01:58,027][257371] FPS: 430640.64[0m
[36m[2023-07-17 12:01:58,030][257371] itr=1174, itrs=2000, Progress: 58.70%[0m
[36m[2023-07-17 12:02:09,896][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 12:02:09,897][257371] FPS: 326711.39[0m
[36m[2023-07-17 12:02:14,142][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:02:14,143][257371] Reward + Measures: [[92.12718442  0.84770465  0.43975165  0.92448825  0.24524     5.92744017]][0m
[37m[1m[2023-07-17 12:02:14,143][257371] Max Reward on eval: 92.12718441515028[0m
[37m[1m[2023-07-17 12:02:14,143][257371] Min Reward on eval: 92.12718441515028[0m
[37m[1m[2023-07-17 12:02:14,143][257371] Mean Reward across all agents: 92.12718441515028[0m
[37m[1m[2023-07-17 12:02:14,144][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:02:19,072][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:02:19,072][257371] Reward + Measures: [[  -3.12354547    0.58319998    0.43200001    0.52210003    0.1754
     5.65463877]
 [ -76.3008704     0.57129997    0.71520001    0.70700002    0.32089999
     6.48546076]
 [  58.83228348    0.5334        0.40450001    0.67609996    0.37509999
     5.29947186]
 ...
 [-259.89023915    0.31760001    0.68040001    0.56119996    0.68990004
     6.71898508]
 [  85.37732187    0.70160007    0.56060004    0.50050002    0.16190001
     6.13233709]
 [  86.49164526    0.8247        0.69440001    0.76480001    0.047
     4.63581085]][0m
[37m[1m[2023-07-17 12:02:19,073][257371] Max Reward on eval: 411.1515312542848[0m
[37m[1m[2023-07-17 12:02:19,073][257371] Min Reward on eval: -310.657992319297[0m
[37m[1m[2023-07-17 12:02:19,073][257371] Mean Reward across all agents: 30.59379555141825[0m
[37m[1m[2023-07-17 12:02:19,073][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:02:19,076][257371] mean_value=-293.43048298678417, max_value=96.13803093666738[0m
[37m[1m[2023-07-17 12:02:19,079][257371] New mean coefficients: [[-0.6872697  -0.7141871  -0.01410225  0.1929939  -0.34931123  3.954694  ]][0m
[37m[1m[2023-07-17 12:02:19,080][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:02:28,133][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 12:02:28,133][257371] FPS: 424224.17[0m
[36m[2023-07-17 12:02:28,136][257371] itr=1175, itrs=2000, Progress: 58.75%[0m
[36m[2023-07-17 12:02:39,747][257371] train() took 11.50 seconds to complete[0m
[36m[2023-07-17 12:02:39,747][257371] FPS: 333984.81[0m
[36m[2023-07-17 12:02:44,025][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:02:44,026][257371] Reward + Measures: [[90.45142235  0.87064534  0.39189664  0.929263    0.23760298  5.9545064 ]][0m
[37m[1m[2023-07-17 12:02:44,026][257371] Max Reward on eval: 90.45142235423528[0m
[37m[1m[2023-07-17 12:02:44,026][257371] Min Reward on eval: 90.45142235423528[0m
[37m[1m[2023-07-17 12:02:44,026][257371] Mean Reward across all agents: 90.45142235423528[0m
[37m[1m[2023-07-17 12:02:44,027][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:02:49,008][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:02:49,009][257371] Reward + Measures: [[ 44.39855134   0.375        0.5334       0.57700008   0.46040002
    4.92840576]
 [ -7.21345243   0.19500001   0.75520003   0.68330002   0.72250003
    4.99306107]
 [ 29.75989721   0.55089998   0.61730003   0.68370003   0.3741
    5.24484015]
 ...
 [104.75572247   0.80229998   0.61329997   0.87210006   0.17999999
    5.68059874]
 [ 75.41608788   0.94709998   0.264        0.95240003   0.21500002
    5.92102337]
 [ 87.9758985    0.89569998   0.18130001   0.94890004   0.38579997
    5.94290972]][0m
[37m[1m[2023-07-17 12:02:49,009][257371] Max Reward on eval: 250.5873641833663[0m
[37m[1m[2023-07-17 12:02:49,009][257371] Min Reward on eval: -63.22350387927145[0m
[37m[1m[2023-07-17 12:02:49,010][257371] Mean Reward across all agents: 87.92848840825029[0m
[37m[1m[2023-07-17 12:02:49,010][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:02:49,013][257371] mean_value=-266.58917488146886, max_value=109.89587012069094[0m
[37m[1m[2023-07-17 12:02:49,016][257371] New mean coefficients: [[-0.31343895  0.8163591   0.47987857  0.5216198  -0.37336037  4.523144  ]][0m
[37m[1m[2023-07-17 12:02:49,017][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:02:57,949][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 12:02:57,949][257371] FPS: 429976.69[0m
[36m[2023-07-17 12:02:57,952][257371] itr=1176, itrs=2000, Progress: 58.80%[0m
[36m[2023-07-17 12:03:09,603][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 12:03:09,603][257371] FPS: 332711.61[0m
[36m[2023-07-17 12:03:13,906][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:03:13,906][257371] Reward + Measures: [[97.17227128  0.8811096   0.37629133  0.9279483   0.22935766  5.98581362]][0m
[37m[1m[2023-07-17 12:03:13,906][257371] Max Reward on eval: 97.17227128302548[0m
[37m[1m[2023-07-17 12:03:13,907][257371] Min Reward on eval: 97.17227128302548[0m
[37m[1m[2023-07-17 12:03:13,907][257371] Mean Reward across all agents: 97.17227128302548[0m
[37m[1m[2023-07-17 12:03:13,907][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:03:18,894][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:03:18,895][257371] Reward + Measures: [[ 22.79024062   0.95450002   0.14220001   0.95710003   0.3184
    5.53772211]
 [ 22.80112877   0.31739998   0.36850002   0.42459998   0.41350004
    5.08162117]
 [119.62752985   0.73159999   0.52290004   0.83169997   0.22789998
    5.20972443]
 ...
 [ 68.22225099   0.55700004   0.51310003   0.86120003   0.55310005
    5.74267149]
 [ 70.02792413   0.66300005   0.1171       0.68590003   0.2685
    5.35586309]
 [-32.51004585   0.59370005   0.44110003   0.71940005   0.46040002
    5.9889679 ]][0m
[37m[1m[2023-07-17 12:03:18,895][257371] Max Reward on eval: 596.0961408814416[0m
[37m[1m[2023-07-17 12:03:18,895][257371] Min Reward on eval: -146.8020415406674[0m
[37m[1m[2023-07-17 12:03:18,896][257371] Mean Reward across all agents: 77.21242407131406[0m
[37m[1m[2023-07-17 12:03:18,896][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:03:18,899][257371] mean_value=-186.68104338066158, max_value=269.17787962194245[0m
[37m[1m[2023-07-17 12:03:18,902][257371] New mean coefficients: [[-0.24729766  0.6025154   0.88303673  0.24316284 -0.21787512  4.2014213 ]][0m
[37m[1m[2023-07-17 12:03:18,902][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:03:27,874][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 12:03:27,875][257371] FPS: 428075.93[0m
[36m[2023-07-17 12:03:27,877][257371] itr=1177, itrs=2000, Progress: 58.85%[0m
[36m[2023-07-17 12:03:39,675][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 12:03:39,676][257371] FPS: 328536.91[0m
[36m[2023-07-17 12:03:43,917][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:03:43,918][257371] Reward + Measures: [[100.91725795   0.90258396   0.365639     0.936243     0.20770933
    6.00213861]][0m
[37m[1m[2023-07-17 12:03:43,918][257371] Max Reward on eval: 100.91725795081877[0m
[37m[1m[2023-07-17 12:03:43,918][257371] Min Reward on eval: 100.91725795081877[0m
[37m[1m[2023-07-17 12:03:43,919][257371] Mean Reward across all agents: 100.91725795081877[0m
[37m[1m[2023-07-17 12:03:43,919][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:03:48,955][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:03:48,955][257371] Reward + Measures: [[ 64.52878385   0.68250006   0.52490008   0.7166       0.32810003
    5.17810774]
 [155.06764197   0.96309996   0.48049998   0.96530002   0.14130001
    6.18636131]
 [112.81764077   0.85229999   0.53319997   0.87889999   0.13990001
    5.68184042]
 ...
 [ 90.20790809   0.77240002   0.47510001   0.9084       0.30329999
    5.96127176]
 [ 33.58190611   0.84160006   0.27239999   0.84559995   0.10699999
    5.36798286]
 [ 41.89296271   0.6081       0.58429998   0.57980001   0.49420005
    5.15265751]][0m
[37m[1m[2023-07-17 12:03:48,955][257371] Max Reward on eval: 189.56292915139346[0m
[37m[1m[2023-07-17 12:03:48,956][257371] Min Reward on eval: -62.027687231823805[0m
[37m[1m[2023-07-17 12:03:48,956][257371] Mean Reward across all agents: 84.48810639773401[0m
[37m[1m[2023-07-17 12:03:48,956][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:03:48,959][257371] mean_value=-235.48926680966412, max_value=51.99150784177613[0m
[37m[1m[2023-07-17 12:03:48,962][257371] New mean coefficients: [[-0.9710269   0.10599214  0.9441041   0.00156854  0.16691366  4.5062213 ]][0m
[37m[1m[2023-07-17 12:03:48,963][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:03:58,065][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 12:03:58,065][257371] FPS: 421943.97[0m
[36m[2023-07-17 12:03:58,067][257371] itr=1178, itrs=2000, Progress: 58.90%[0m
[36m[2023-07-17 12:04:09,866][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 12:04:09,866][257371] FPS: 328549.01[0m
[36m[2023-07-17 12:04:14,191][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:04:14,191][257371] Reward + Measures: [[106.62620443   0.90412968   0.31350732   0.92766333   0.22144832
    6.04946613]][0m
[37m[1m[2023-07-17 12:04:14,192][257371] Max Reward on eval: 106.62620443431757[0m
[37m[1m[2023-07-17 12:04:14,192][257371] Min Reward on eval: 106.62620443431757[0m
[37m[1m[2023-07-17 12:04:14,192][257371] Mean Reward across all agents: 106.62620443431757[0m
[37m[1m[2023-07-17 12:04:14,192][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:04:19,441][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:04:19,441][257371] Reward + Measures: [[ 38.61061056   0.91680002   0.08220001   0.92240012   0.34110001
    5.99390364]
 [ 66.14097548   0.58840001   0.38739997   0.64000005   0.42449999
    5.64939308]
 [  5.1997932    0.44420004   0.30150002   0.4501       0.40510002
    5.38600016]
 ...
 [ 54.54101284   0.2552       0.71239996   0.45619997   0.73860008
    5.75327015]
 [ 83.71408288   0.56050003   0.3734       0.64120001   0.39049998
    5.56738758]
 [-10.71912957   0.4409       0.4086       0.4165       0.47839999
    5.41024208]][0m
[37m[1m[2023-07-17 12:04:19,442][257371] Max Reward on eval: 203.55131339449434[0m
[37m[1m[2023-07-17 12:04:19,442][257371] Min Reward on eval: -210.9851312218234[0m
[37m[1m[2023-07-17 12:04:19,442][257371] Mean Reward across all agents: 19.74591881322334[0m
[37m[1m[2023-07-17 12:04:19,442][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:04:19,445][257371] mean_value=-241.35498542211886, max_value=298.285543351497[0m
[37m[1m[2023-07-17 12:04:19,447][257371] New mean coefficients: [[-0.9078464  -0.76593167  0.03676903 -0.22189407  0.14185634  4.205069  ]][0m
[37m[1m[2023-07-17 12:04:19,448][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:04:28,417][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 12:04:28,417][257371] FPS: 428238.94[0m
[36m[2023-07-17 12:04:28,419][257371] itr=1179, itrs=2000, Progress: 58.95%[0m
[36m[2023-07-17 12:04:40,133][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 12:04:40,133][257371] FPS: 331004.01[0m
[36m[2023-07-17 12:04:44,381][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:04:44,382][257371] Reward + Measures: [[110.26887379   0.91091162   0.31117398   0.92986304   0.21394099
    6.10246325]][0m
[37m[1m[2023-07-17 12:04:44,382][257371] Max Reward on eval: 110.26887378977734[0m
[37m[1m[2023-07-17 12:04:44,382][257371] Min Reward on eval: 110.26887378977734[0m
[37m[1m[2023-07-17 12:04:44,383][257371] Mean Reward across all agents: 110.26887378977734[0m
[37m[1m[2023-07-17 12:04:44,383][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:04:49,418][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:04:49,418][257371] Reward + Measures: [[27.02563653  0.7428      0.48359999  0.72470003  0.08660001  5.48991632]
 [20.03519329  0.51900005  0.46560001  0.62110001  0.4402      4.53203964]
 [46.92599811  0.89569998  0.16110002  0.90410006  0.42610002  6.34721804]
 ...
 [42.12781446  0.94589996  0.07740001  0.94160002  0.45930001  6.37740088]
 [36.82432786  0.58860004  0.373       0.53740001  0.18240002  5.59261942]
 [ 9.16916291  0.82050002  0.15889999  0.78940004  0.29850003  4.96832228]][0m
[37m[1m[2023-07-17 12:04:49,419][257371] Max Reward on eval: 292.0241866664961[0m
[37m[1m[2023-07-17 12:04:49,419][257371] Min Reward on eval: -142.89778711413965[0m
[37m[1m[2023-07-17 12:04:49,419][257371] Mean Reward across all agents: 60.649028968719406[0m
[37m[1m[2023-07-17 12:04:49,419][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:04:49,422][257371] mean_value=-212.14761843453644, max_value=62.51853339961022[0m
[37m[1m[2023-07-17 12:04:49,425][257371] New mean coefficients: [[-0.9427461  -0.8592768   0.2128224  -0.12958829  0.10201497  4.224059  ]][0m
[37m[1m[2023-07-17 12:04:49,426][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:04:58,468][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 12:04:58,468][257371] FPS: 424759.72[0m
[36m[2023-07-17 12:04:58,470][257371] itr=1180, itrs=2000, Progress: 59.00%[0m
[37m[1m[2023-07-17 12:08:40,758][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001160[0m
[36m[2023-07-17 12:08:53,349][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 12:08:53,350][257371] FPS: 327267.90[0m
[36m[2023-07-17 12:08:57,634][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:08:57,640][257371] Reward + Measures: [[112.52688714   0.88873899   0.32464731   0.91902637   0.22242032
    6.112988  ]][0m
[37m[1m[2023-07-17 12:08:57,640][257371] Max Reward on eval: 112.52688714391566[0m
[37m[1m[2023-07-17 12:08:57,641][257371] Min Reward on eval: 112.52688714391566[0m
[37m[1m[2023-07-17 12:08:57,641][257371] Mean Reward across all agents: 112.52688714391566[0m
[37m[1m[2023-07-17 12:08:57,641][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:09:02,591][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:09:02,592][257371] Reward + Measures: [[ 85.23291971   0.58569998   0.40910003   0.83999997   0.50850004
    6.0470252 ]
 [ 89.36390663   0.35460001   0.60180002   0.62630004   0.64860004
    5.66025114]
 [ 97.92793012   0.63980001   0.56900007   0.78830004   0.43160006
    5.96620321]
 ...
 [133.92783177   0.85589999   0.41370001   0.93940002   0.3053
    6.16371965]
 [ 73.35641555   0.66769999   0.39640003   0.73760003   0.29160002
    5.76086569]
 [162.49011895   0.92430001   0.52350003   0.92810005   0.10699999
    6.09248686]][0m
[37m[1m[2023-07-17 12:09:02,592][257371] Max Reward on eval: 217.2389890536666[0m
[37m[1m[2023-07-17 12:09:02,592][257371] Min Reward on eval: -13.920303405821324[0m
[37m[1m[2023-07-17 12:09:02,593][257371] Mean Reward across all agents: 90.26922462324477[0m
[37m[1m[2023-07-17 12:09:02,593][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:09:02,594][257371] mean_value=-223.31362070122782, max_value=-10.154996855827193[0m
[36m[2023-07-17 12:09:02,598][257371] XNES is restarting with a new solution whose measures are [0.76980001 0.61250001 0.52020001 0.71259993 3.75136495] and objective is 68.72561624180526[0m
[36m[2023-07-17 12:09:02,600][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 12:09:02,602][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 12:09:02,603][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:09:11,602][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 12:09:11,602][257371] FPS: 426773.01[0m
[36m[2023-07-17 12:09:11,604][257371] itr=1181, itrs=2000, Progress: 59.05%[0m
[36m[2023-07-17 12:09:23,448][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 12:09:23,448][257371] FPS: 327429.09[0m
[36m[2023-07-17 12:09:27,722][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:09:27,722][257371] Reward + Measures: [[35.94227086  0.211105    0.16605566  0.20055567  0.17662899  4.9780097 ]][0m
[37m[1m[2023-07-17 12:09:27,723][257371] Max Reward on eval: 35.94227086469478[0m
[37m[1m[2023-07-17 12:09:27,723][257371] Min Reward on eval: 35.94227086469478[0m
[37m[1m[2023-07-17 12:09:27,723][257371] Mean Reward across all agents: 35.94227086469478[0m
[37m[1m[2023-07-17 12:09:27,723][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:09:32,970][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:09:32,971][257371] Reward + Measures: [[  -8.93704689    0.41139999    0.14229999    0.4364        0.40419999
     5.84123802]
 [ -62.21044065    0.23339999    0.33849999    0.1534        0.26530001
     5.69802809]
 [ -20.26995048    0.13850001    0.10289999    0.07910001    0.1099
     6.06209707]
 ...
 [-162.5728874     0.45749998    0.67030001    0.0521        0.62830001
     5.7953887 ]
 [-119.2925132     0.23660003    0.41920003    0.3211        0.34580001
     5.83468628]
 [ -36.46512891    0.15000001    0.33979997    0.20180002    0.2395
     6.26880646]][0m
[37m[1m[2023-07-17 12:09:32,971][257371] Max Reward on eval: 289.4398908063769[0m
[37m[1m[2023-07-17 12:09:32,972][257371] Min Reward on eval: -477.4905963030178[0m
[37m[1m[2023-07-17 12:09:32,972][257371] Mean Reward across all agents: -43.25747950134544[0m
[37m[1m[2023-07-17 12:09:32,972][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:09:32,973][257371] mean_value=-389.8209099632697, max_value=5.107534589080615[0m
[37m[1m[2023-07-17 12:09:32,975][257371] New mean coefficients: [[ 1.4227219  -0.26153213 -1.7252457  -2.4729252  -1.59336    -0.11288977]][0m
[37m[1m[2023-07-17 12:09:32,976][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:09:41,928][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 12:09:41,928][257371] FPS: 429044.96[0m
[36m[2023-07-17 12:09:41,931][257371] itr=1182, itrs=2000, Progress: 59.10%[0m
[36m[2023-07-17 12:09:53,591][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 12:09:53,591][257371] FPS: 332512.92[0m
[36m[2023-07-17 12:09:57,877][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:09:57,877][257371] Reward + Measures: [[49.90553354  0.18904166  0.14013033  0.18755835  0.14950934  5.11241341]][0m
[37m[1m[2023-07-17 12:09:57,878][257371] Max Reward on eval: 49.90553353804115[0m
[37m[1m[2023-07-17 12:09:57,878][257371] Min Reward on eval: 49.90553353804115[0m
[37m[1m[2023-07-17 12:09:57,878][257371] Mean Reward across all agents: 49.90553353804115[0m
[37m[1m[2023-07-17 12:09:57,878][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:10:02,812][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:10:02,813][257371] Reward + Measures: [[  1.9241696    0.10910001   0.1471       0.13450001   0.1175
    7.42770481]
 [-56.43027439   0.0983       0.27849999   0.25040004   0.2726
    7.33879089]
 [ 33.05796429   0.27079999   0.53659999   0.26860002   0.49429998
    7.77674961]
 ...
 [-32.62452497   0.0921       0.15640001   0.1219       0.1275
    7.14620209]
 [ 60.43830377   0.1516       0.1556       0.1139       0.15940002
    7.12344313]
 [ 73.10380263   0.16600001   0.57350004   0.36680001   0.52829999
    7.15628672]][0m
[37m[1m[2023-07-17 12:10:02,813][257371] Max Reward on eval: 216.8467009173706[0m
[37m[1m[2023-07-17 12:10:02,813][257371] Min Reward on eval: -357.1521555367857[0m
[37m[1m[2023-07-17 12:10:02,814][257371] Mean Reward across all agents: 0.5280765428066654[0m
[37m[1m[2023-07-17 12:10:02,814][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:10:02,815][257371] mean_value=-276.8485472538157, max_value=36.06235447714295[0m
[37m[1m[2023-07-17 12:10:02,817][257371] New mean coefficients: [[ 2.4344664  -0.80517757 -1.9248852  -3.1626859  -1.5244375  -0.31662387]][0m
[37m[1m[2023-07-17 12:10:02,818][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:10:11,900][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 12:10:11,900][257371] FPS: 422889.59[0m
[36m[2023-07-17 12:10:11,903][257371] itr=1183, itrs=2000, Progress: 59.15%[0m
[36m[2023-07-17 12:10:23,663][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 12:10:23,663][257371] FPS: 329689.86[0m
[36m[2023-07-17 12:10:27,985][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:10:27,986][257371] Reward + Measures: [[62.82372055  0.145099    0.16319633  0.17747265  0.12912299  5.18485594]][0m
[37m[1m[2023-07-17 12:10:27,986][257371] Max Reward on eval: 62.82372055362448[0m
[37m[1m[2023-07-17 12:10:27,986][257371] Min Reward on eval: 62.82372055362448[0m
[37m[1m[2023-07-17 12:10:27,986][257371] Mean Reward across all agents: 62.82372055362448[0m
[37m[1m[2023-07-17 12:10:27,987][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:10:32,985][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:10:32,985][257371] Reward + Measures: [[-54.61383701   0.26610002   0.2899       0.21710001   0.33750001
    5.73840666]
 [ 49.58577465   0.18609999   0.15699999   0.17010002   0.19930001
    5.5498209 ]
 [ 30.07292525   0.13950001   0.2359       0.1955       0.21429999
    5.86587143]
 ...
 [-27.66887763   0.1269       0.24820001   0.23989999   0.2572
    6.4283371 ]
 [ 30.78444043   0.11189999   0.1103       0.1122       0.10650001
    6.1757226 ]
 [ 15.02681108   0.13410001   0.18080001   0.1445       0.1593
    6.47293043]][0m
[37m[1m[2023-07-17 12:10:32,986][257371] Max Reward on eval: 151.5841000497341[0m
[37m[1m[2023-07-17 12:10:32,986][257371] Min Reward on eval: -272.5896134148701[0m
[37m[1m[2023-07-17 12:10:32,986][257371] Mean Reward across all agents: 18.076123919881727[0m
[37m[1m[2023-07-17 12:10:32,986][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:10:32,987][257371] mean_value=-287.867512863223, max_value=-34.01653633248203[0m
[36m[2023-07-17 12:10:32,989][257371] XNES is restarting with a new solution whose measures are [0.51910001 0.82930005 0.56620002 0.55419999 6.04501152] and objective is 505.4987058103085[0m
[36m[2023-07-17 12:10:32,990][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 12:10:32,993][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 12:10:32,994][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:10:42,027][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 12:10:42,027][257371] FPS: 425176.58[0m
[36m[2023-07-17 12:10:42,029][257371] itr=1184, itrs=2000, Progress: 59.20%[0m
[36m[2023-07-17 12:10:53,843][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 12:10:53,843][257371] FPS: 328155.30[0m
[36m[2023-07-17 12:10:58,181][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:10:58,181][257371] Reward + Measures: [[178.08282705   0.44308531   0.60087937   0.24625365   0.4545857
    5.58578825]][0m
[37m[1m[2023-07-17 12:10:58,181][257371] Max Reward on eval: 178.08282705330492[0m
[37m[1m[2023-07-17 12:10:58,182][257371] Min Reward on eval: 178.08282705330492[0m
[37m[1m[2023-07-17 12:10:58,182][257371] Mean Reward across all agents: 178.08282705330492[0m
[37m[1m[2023-07-17 12:10:58,182][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:11:03,164][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:11:03,165][257371] Reward + Measures: [[242.40999113   0.56800002   0.62799996   0.50089997   0.22659998
    5.98333216]
 [155.99766299   0.45570001   0.67000002   0.14720002   0.62980002
    5.24064445]
 [-24.42770248   0.37240002   0.46669999   0.27989998   0.30690002
    5.06875992]
 ...
 [196.37640783   0.42230007   0.61160004   0.25839999   0.47349998
    5.60108089]
 [181.16135282   0.33989999   0.47839999   0.21180001   0.3831
    5.42929411]
 [312.70707398   0.43179998   0.57730001   0.1653       0.46730003
    5.44858599]][0m
[37m[1m[2023-07-17 12:11:03,165][257371] Max Reward on eval: 516.4945599585772[0m
[37m[1m[2023-07-17 12:11:03,165][257371] Min Reward on eval: -140.63746864851564[0m
[37m[1m[2023-07-17 12:11:03,166][257371] Mean Reward across all agents: 179.56523372039206[0m
[37m[1m[2023-07-17 12:11:03,166][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:11:03,171][257371] mean_value=-124.67041826807996, max_value=250.59321879093443[0m
[37m[1m[2023-07-17 12:11:03,174][257371] New mean coefficients: [[ 0.658543  -1.0103436 -1.4499197 -3.1754375 -1.0152695 -1.0721269]][0m
[37m[1m[2023-07-17 12:11:03,175][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:11:12,218][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 12:11:12,219][257371] FPS: 424685.26[0m
[36m[2023-07-17 12:11:12,221][257371] itr=1185, itrs=2000, Progress: 59.25%[0m
[36m[2023-07-17 12:11:24,095][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 12:11:24,095][257371] FPS: 326604.81[0m
[36m[2023-07-17 12:11:28,410][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:11:28,410][257371] Reward + Measures: [[103.5707398    0.4022797    0.57070899   0.19783767   0.47987133
    5.36304092]][0m
[37m[1m[2023-07-17 12:11:28,410][257371] Max Reward on eval: 103.57073980081321[0m
[37m[1m[2023-07-17 12:11:28,411][257371] Min Reward on eval: 103.57073980081321[0m
[37m[1m[2023-07-17 12:11:28,411][257371] Mean Reward across all agents: 103.57073980081321[0m
[37m[1m[2023-07-17 12:11:28,411][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:11:33,435][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:11:33,435][257371] Reward + Measures: [[  59.69966636    0.3876        0.52770007    0.2502        0.41420004
     5.11610556]
 [ 186.02689837    0.34660003    0.43210003    0.1199        0.39780003
     4.91482782]
 [  99.95248643    0.34740001    0.33650002    0.30850002    0.1367
     4.88489294]
 ...
 [  36.54939663    0.55430001    0.69270003    0.3836        0.414
     5.57756186]
 [-206.83178238    0.5571        0.70349997    0.42600003    0.37889999
     5.59120417]
 [ 396.34302424    0.46159998    0.65959996    0.2289        0.574
     5.21783018]][0m
[37m[1m[2023-07-17 12:11:33,436][257371] Max Reward on eval: 451.1977119823452[0m
[37m[1m[2023-07-17 12:11:33,436][257371] Min Reward on eval: -206.83178237918764[0m
[37m[1m[2023-07-17 12:11:33,436][257371] Mean Reward across all agents: 91.26300735833402[0m
[37m[1m[2023-07-17 12:11:33,436][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:11:33,439][257371] mean_value=-240.4919718485614, max_value=106.49537521363806[0m
[37m[1m[2023-07-17 12:11:33,441][257371] New mean coefficients: [[ 1.7946619  -1.6499165  -0.9485528  -3.4194057   0.42267382 -0.95959085]][0m
[37m[1m[2023-07-17 12:11:33,442][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:11:42,548][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 12:11:42,549][257371] FPS: 421765.70[0m
[36m[2023-07-17 12:11:42,551][257371] itr=1186, itrs=2000, Progress: 59.30%[0m
[36m[2023-07-17 12:11:54,459][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 12:11:54,460][257371] FPS: 325490.03[0m
[36m[2023-07-17 12:11:58,743][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:11:58,743][257371] Reward + Measures: [[231.57816861   0.34272367   0.49366      0.167495     0.45095134
    5.07320833]][0m
[37m[1m[2023-07-17 12:11:58,743][257371] Max Reward on eval: 231.57816861015286[0m
[37m[1m[2023-07-17 12:11:58,744][257371] Min Reward on eval: 231.57816861015286[0m
[37m[1m[2023-07-17 12:11:58,744][257371] Mean Reward across all agents: 231.57816861015286[0m
[37m[1m[2023-07-17 12:11:58,744][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:12:03,755][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:12:03,756][257371] Reward + Measures: [[251.63215754   0.34950003   0.42670003   0.18630001   0.39049998
    4.87755966]
 [ 53.01304209   0.38799998   0.55829996   0.22140001   0.45840001
    4.72549772]
 [252.14911499   0.4104       0.6099       0.1389       0.56940001
    4.95678425]
 ...
 [241.00762407   0.42309999   0.47019997   0.18009999   0.41289997
    4.97203016]
 [176.46916545   0.28529999   0.49220005   0.20290001   0.47210002
    4.61626816]
 [232.91990914   0.34940001   0.44580004   0.1267       0.42010003
    4.94898605]][0m
[37m[1m[2023-07-17 12:12:03,756][257371] Max Reward on eval: 573.7278303972446[0m
[37m[1m[2023-07-17 12:12:03,756][257371] Min Reward on eval: -92.1238515551202[0m
[37m[1m[2023-07-17 12:12:03,757][257371] Mean Reward across all agents: 257.32442627789544[0m
[37m[1m[2023-07-17 12:12:03,757][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:12:03,761][257371] mean_value=-82.85459139180222, max_value=144.06576229276442[0m
[37m[1m[2023-07-17 12:12:03,764][257371] New mean coefficients: [[ 2.0607524  -1.4266863  -1.2061826  -3.6983914  -0.33595872 -1.405006  ]][0m
[37m[1m[2023-07-17 12:12:03,765][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:12:12,820][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 12:12:12,820][257371] FPS: 424135.69[0m
[36m[2023-07-17 12:12:12,822][257371] itr=1187, itrs=2000, Progress: 59.35%[0m
[36m[2023-07-17 12:12:24,463][257371] train() took 11.52 seconds to complete[0m
[36m[2023-07-17 12:12:24,463][257371] FPS: 333230.73[0m
[36m[2023-07-17 12:12:28,745][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:12:28,745][257371] Reward + Measures: [[240.5648924    0.34210202   0.44571295   0.16472933   0.41189864
    4.81916189]][0m
[37m[1m[2023-07-17 12:12:28,745][257371] Max Reward on eval: 240.5648924044028[0m
[37m[1m[2023-07-17 12:12:28,746][257371] Min Reward on eval: 240.5648924044028[0m
[37m[1m[2023-07-17 12:12:28,746][257371] Mean Reward across all agents: 240.5648924044028[0m
[37m[1m[2023-07-17 12:12:28,746][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:12:33,996][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:12:33,996][257371] Reward + Measures: [[ 28.8895794    0.18139999   0.21759999   0.15009999   0.19050001
    4.43347025]
 [ -4.75559952   0.3691       0.49020001   0.23179999   0.39229998
    4.88184738]
 [122.2084696    0.28260002   0.49190003   0.22490001   0.50760001
    4.73318577]
 ...
 [  5.04881121   0.3619       0.62150002   0.1646       0.57880002
    5.37060976]
 [118.23788365   0.23200002   0.27760002   0.11540001   0.24499999
    4.4430995 ]
 [105.84454284   0.4921       0.48160002   0.44549999   0.17700002
    4.69817114]][0m
[37m[1m[2023-07-17 12:12:33,996][257371] Max Reward on eval: 539.2164209070615[0m
[37m[1m[2023-07-17 12:12:33,997][257371] Min Reward on eval: -142.31462168004364[0m
[37m[1m[2023-07-17 12:12:33,997][257371] Mean Reward across all agents: 94.0667110177473[0m
[37m[1m[2023-07-17 12:12:33,997][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:12:33,999][257371] mean_value=-1068.9901575128347, max_value=114.96863602530459[0m
[37m[1m[2023-07-17 12:12:34,002][257371] New mean coefficients: [[ 2.9425783  -0.5303864  -0.9625312  -2.877868   -0.4039545  -0.17884684]][0m
[37m[1m[2023-07-17 12:12:34,003][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:12:43,026][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 12:12:43,026][257371] FPS: 425678.09[0m
[36m[2023-07-17 12:12:43,028][257371] itr=1188, itrs=2000, Progress: 59.40%[0m
[36m[2023-07-17 12:12:54,731][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 12:12:54,731][257371] FPS: 331339.16[0m
[36m[2023-07-17 12:12:59,044][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:12:59,045][257371] Reward + Measures: [[238.62480086   0.332526     0.42133564   0.15488866   0.39191169
    4.77930641]][0m
[37m[1m[2023-07-17 12:12:59,045][257371] Max Reward on eval: 238.6248008601181[0m
[37m[1m[2023-07-17 12:12:59,045][257371] Min Reward on eval: 238.6248008601181[0m
[37m[1m[2023-07-17 12:12:59,046][257371] Mean Reward across all agents: 238.6248008601181[0m
[37m[1m[2023-07-17 12:12:59,046][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:13:04,031][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:13:04,032][257371] Reward + Measures: [[233.04405146   0.2836       0.32360002   0.1028       0.29480001
    4.52972889]
 [365.93436687   0.4296       0.50440001   0.20979999   0.4355
    4.70154333]
 [119.02021811   0.2359       0.23699999   0.15400001   0.22319999
    4.09981966]
 ...
 [257.19279138   0.36100003   0.4729       0.16560002   0.39919999
    4.7207222 ]
 [202.81174159   0.27079999   0.2967       0.1374       0.28400001
    4.51232862]
 [169.40494556   0.2586       0.3064       0.11430001   0.28099999
    4.5706048 ]][0m
[37m[1m[2023-07-17 12:13:04,032][257371] Max Reward on eval: 522.1962833683007[0m
[37m[1m[2023-07-17 12:13:04,032][257371] Min Reward on eval: -51.52988005205989[0m
[37m[1m[2023-07-17 12:13:04,033][257371] Mean Reward across all agents: 241.16081698305302[0m
[37m[1m[2023-07-17 12:13:04,033][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:13:04,037][257371] mean_value=-465.042503447917, max_value=216.78505472982124[0m
[37m[1m[2023-07-17 12:13:04,039][257371] New mean coefficients: [[ 3.0646057  -0.30649495 -0.93278533 -3.473153   -0.68064207 -0.94557655]][0m
[37m[1m[2023-07-17 12:13:04,040][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:13:13,089][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 12:13:13,089][257371] FPS: 424464.52[0m
[36m[2023-07-17 12:13:13,091][257371] itr=1189, itrs=2000, Progress: 59.45%[0m
[36m[2023-07-17 12:13:25,163][257371] train() took 11.96 seconds to complete[0m
[36m[2023-07-17 12:13:25,163][257371] FPS: 321083.95[0m
[36m[2023-07-17 12:13:29,482][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:13:29,482][257371] Reward + Measures: [[251.463457     0.34779263   0.40578166   0.13610867   0.38510835
    4.69133806]][0m
[37m[1m[2023-07-17 12:13:29,482][257371] Max Reward on eval: 251.46345699765268[0m
[37m[1m[2023-07-17 12:13:29,483][257371] Min Reward on eval: 251.46345699765268[0m
[37m[1m[2023-07-17 12:13:29,483][257371] Mean Reward across all agents: 251.46345699765268[0m
[37m[1m[2023-07-17 12:13:29,483][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:13:34,494][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:13:34,498][257371] Reward + Measures: [[236.53616433   0.3409       0.38420001   0.12740001   0.37710002
    4.69615602]
 [261.3856036    0.34419999   0.3554       0.18190001   0.38850001
    4.25246   ]
 [198.93739126   0.32760003   0.44189999   0.23539999   0.38980001
    4.52020168]
 ...
 [225.37690356   0.41980001   0.40689999   0.31220001   0.21100001
    4.67658186]
 [278.07872333   0.29899999   0.61190003   0.2106       0.59450001
    5.00360441]
 [279.85770799   0.37760001   0.5636       0.0975       0.5187
    5.29790545]][0m
[37m[1m[2023-07-17 12:13:34,499][257371] Max Reward on eval: 658.1401129705831[0m
[37m[1m[2023-07-17 12:13:34,499][257371] Min Reward on eval: 65.11564807007089[0m
[37m[1m[2023-07-17 12:13:34,499][257371] Mean Reward across all agents: 301.58797440543225[0m
[37m[1m[2023-07-17 12:13:34,499][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:13:34,504][257371] mean_value=-355.36405422268234, max_value=362.7825405383808[0m
[37m[1m[2023-07-17 12:13:34,507][257371] New mean coefficients: [[ 3.8983374  -0.09322929 -0.4157095  -4.729395   -1.802784   -2.1723146 ]][0m
[37m[1m[2023-07-17 12:13:34,508][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:13:43,598][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 12:13:43,598][257371] FPS: 422508.86[0m
[36m[2023-07-17 12:13:43,600][257371] itr=1190, itrs=2000, Progress: 59.50%[0m
[37m[1m[2023-07-17 12:17:10,285][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001170[0m
[36m[2023-07-17 12:17:22,478][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 12:17:22,479][257371] FPS: 329571.10[0m
[36m[2023-07-17 12:17:26,698][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:17:26,698][257371] Reward + Measures: [[254.61403461   0.35283667   0.39349732   0.131955     0.37525198
    4.57014894]][0m
[37m[1m[2023-07-17 12:17:26,699][257371] Max Reward on eval: 254.61403461285315[0m
[37m[1m[2023-07-17 12:17:26,699][257371] Min Reward on eval: 254.61403461285315[0m
[37m[1m[2023-07-17 12:17:26,699][257371] Mean Reward across all agents: 254.61403461285315[0m
[37m[1m[2023-07-17 12:17:26,699][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:17:31,663][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:17:31,664][257371] Reward + Measures: [[  3.73801541   0.27629998   0.2568       0.1795       0.20370002
    4.38087416]
 [387.45335865   0.48300001   0.54520005   0.11310001   0.50629997
    4.93568373]
 [ 93.54702965   0.1688       0.18159999   0.10469999   0.1612
    4.30699682]
 ...
 [137.55005748   0.25490001   0.27350003   0.11750001   0.26250002
    4.49023199]
 [ -6.48263403   0.3626       0.37689999   0.24929999   0.27020001
    4.36911726]
 [524.95627882   0.54369998   0.65990001   0.12840001   0.61260003
    5.10901403]][0m
[37m[1m[2023-07-17 12:17:31,664][257371] Max Reward on eval: 552.1100964793935[0m
[37m[1m[2023-07-17 12:17:31,664][257371] Min Reward on eval: -6.482634026417509[0m
[37m[1m[2023-07-17 12:17:31,665][257371] Mean Reward across all agents: 211.6269914623758[0m
[37m[1m[2023-07-17 12:17:31,665][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:17:31,667][257371] mean_value=-725.0024141529182, max_value=138.43035396455366[0m
[37m[1m[2023-07-17 12:17:31,669][257371] New mean coefficients: [[ 3.6517107  -0.37873286 -0.81725746 -4.706875   -1.3520298  -2.6749263 ]][0m
[37m[1m[2023-07-17 12:17:31,670][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:17:40,692][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 12:17:40,693][257371] FPS: 425699.63[0m
[36m[2023-07-17 12:17:40,695][257371] itr=1191, itrs=2000, Progress: 59.55%[0m
[36m[2023-07-17 12:17:52,610][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 12:17:52,610][257371] FPS: 325366.67[0m
[36m[2023-07-17 12:17:56,934][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:17:56,934][257371] Reward + Measures: [[244.01688518   0.33068401   0.36134136   0.13626432   0.34357762
    4.37615967]][0m
[37m[1m[2023-07-17 12:17:56,934][257371] Max Reward on eval: 244.01688517671016[0m
[37m[1m[2023-07-17 12:17:56,935][257371] Min Reward on eval: 244.01688517671016[0m
[37m[1m[2023-07-17 12:17:56,935][257371] Mean Reward across all agents: 244.01688517671016[0m
[37m[1m[2023-07-17 12:17:56,935][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:18:02,220][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:18:02,221][257371] Reward + Measures: [[-14.82775731   0.27850002   0.30749997   0.1954       0.2352
    4.23568106]
 [119.77184774   0.36019999   0.34780002   0.18640001   0.34840003
    4.04429007]
 [140.08017195   0.23720002   0.30410001   0.102        0.29299998
    4.32275009]
 ...
 [204.40584477   0.44080001   0.44260001   0.1349       0.42990002
    4.2961669 ]
 [ 51.38104438   0.20479999   0.21169999   0.09670001   0.205
    4.21813059]
 [300.86524728   0.433        0.50830001   0.1717       0.47639999
    4.57325745]][0m
[37m[1m[2023-07-17 12:18:02,221][257371] Max Reward on eval: 411.27271741889416[0m
[37m[1m[2023-07-17 12:18:02,221][257371] Min Reward on eval: -64.1232329503633[0m
[37m[1m[2023-07-17 12:18:02,221][257371] Mean Reward across all agents: 144.75075327898506[0m
[37m[1m[2023-07-17 12:18:02,221][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:18:02,223][257371] mean_value=-1940.8672963868019, max_value=108.77529103325111[0m
[37m[1m[2023-07-17 12:18:02,226][257371] New mean coefficients: [[ 3.9917326  -0.80430317 -0.8536062  -3.4995246  -0.25241554 -1.7258327 ]][0m
[37m[1m[2023-07-17 12:18:02,227][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:18:11,197][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 12:18:11,197][257371] FPS: 428179.34[0m
[36m[2023-07-17 12:18:11,199][257371] itr=1192, itrs=2000, Progress: 59.60%[0m
[36m[2023-07-17 12:18:22,938][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 12:18:22,939][257371] FPS: 330349.15[0m
[36m[2023-07-17 12:18:27,256][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:18:27,257][257371] Reward + Measures: [[245.0546749    0.32678968   0.353329     0.141368     0.33848667
    4.23266125]][0m
[37m[1m[2023-07-17 12:18:27,257][257371] Max Reward on eval: 245.05467489663604[0m
[37m[1m[2023-07-17 12:18:27,257][257371] Min Reward on eval: 245.05467489663604[0m
[37m[1m[2023-07-17 12:18:27,258][257371] Mean Reward across all agents: 245.05467489663604[0m
[37m[1m[2023-07-17 12:18:27,258][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:18:32,292][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:18:32,292][257371] Reward + Measures: [[239.52441429   0.3247       0.35980001   0.13780001   0.33660001
    4.29141235]
 [268.82856973   0.36450002   0.3883       0.11900001   0.35179999
    4.38294363]
 [206.75286355   0.333        0.37900004   0.16489999   0.35859999
    4.23650694]
 ...
 [260.56545497   0.32370001   0.3705       0.2148       0.34800002
    3.93106461]
 [255.36379358   0.38640004   0.40599999   0.14829998   0.37320003
    4.39052153]
 [186.82260631   0.42539999   0.4657       0.14600001   0.43670002
    4.62177896]][0m
[37m[1m[2023-07-17 12:18:32,293][257371] Max Reward on eval: 541.2062740251422[0m
[37m[1m[2023-07-17 12:18:32,293][257371] Min Reward on eval: 7.331723619159311[0m
[37m[1m[2023-07-17 12:18:32,293][257371] Mean Reward across all agents: 224.01198805540193[0m
[37m[1m[2023-07-17 12:18:32,293][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:18:32,296][257371] mean_value=-1486.419001212615, max_value=94.0581601233564[0m
[37m[1m[2023-07-17 12:18:32,298][257371] New mean coefficients: [[ 4.670042   -0.45916015 -1.0457746  -2.7983286  -0.50300074 -0.7226759 ]][0m
[37m[1m[2023-07-17 12:18:32,299][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:18:41,347][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 12:18:41,347][257371] FPS: 424505.17[0m
[36m[2023-07-17 12:18:41,349][257371] itr=1193, itrs=2000, Progress: 59.65%[0m
[36m[2023-07-17 12:18:53,076][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 12:18:53,076][257371] FPS: 330638.89[0m
[36m[2023-07-17 12:18:57,294][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:18:57,295][257371] Reward + Measures: [[249.08620414   0.31869766   0.34000367   0.14490767   0.32984298
    4.10983086]][0m
[37m[1m[2023-07-17 12:18:57,295][257371] Max Reward on eval: 249.08620413738086[0m
[37m[1m[2023-07-17 12:18:57,295][257371] Min Reward on eval: 249.08620413738086[0m
[37m[1m[2023-07-17 12:18:57,295][257371] Mean Reward across all agents: 249.08620413738086[0m
[37m[1m[2023-07-17 12:18:57,296][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:19:02,245][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:19:02,255][257371] Reward + Measures: [[212.8732656    0.48410001   0.54829997   0.1115       0.51620001
    4.83765507]
 [302.4619049    0.44730002   0.49200001   0.15500002   0.48519999
    4.49710512]
 [271.1189165    0.46650001   0.51159996   0.20439999   0.45140001
    4.51563025]
 ...
 [188.1941263    0.35789999   0.3513       0.1859       0.3272
    3.99416995]
 [280.68571208   0.42719999   0.4492       0.1258       0.44379997
    4.45670462]
 [137.34612598   0.35330001   0.36219999   0.17639999   0.34900004
    4.116611  ]][0m
[37m[1m[2023-07-17 12:19:02,255][257371] Max Reward on eval: 432.51656165823806[0m
[37m[1m[2023-07-17 12:19:02,255][257371] Min Reward on eval: 28.87241269205697[0m
[37m[1m[2023-07-17 12:19:02,256][257371] Mean Reward across all agents: 228.2475067430074[0m
[37m[1m[2023-07-17 12:19:02,256][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:19:02,258][257371] mean_value=-1219.5051709499764, max_value=79.12585518348368[0m
[37m[1m[2023-07-17 12:19:02,261][257371] New mean coefficients: [[ 4.804353   -0.17756602 -1.0635742  -2.301721    0.13143647 -1.1951617 ]][0m
[37m[1m[2023-07-17 12:19:02,262][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:19:11,205][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 12:19:11,205][257371] FPS: 429463.17[0m
[36m[2023-07-17 12:19:11,207][257371] itr=1194, itrs=2000, Progress: 59.70%[0m
[36m[2023-07-17 12:19:22,939][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 12:19:22,940][257371] FPS: 330453.23[0m
[36m[2023-07-17 12:19:27,237][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:19:27,238][257371] Reward + Measures: [[251.05533999   0.31663832   0.33580399   0.14089066   0.32628998
    4.03956366]][0m
[37m[1m[2023-07-17 12:19:27,238][257371] Max Reward on eval: 251.05533998683413[0m
[37m[1m[2023-07-17 12:19:27,238][257371] Min Reward on eval: 251.05533998683413[0m
[37m[1m[2023-07-17 12:19:27,239][257371] Mean Reward across all agents: 251.05533998683413[0m
[37m[1m[2023-07-17 12:19:27,239][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:19:32,223][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:19:32,229][257371] Reward + Measures: [[143.98705941   0.2203       0.24460001   0.1384       0.2491
    3.84791732]
 [252.58248289   0.40760002   0.4269       0.1842       0.40190002
    4.3019228 ]
 [310.36883663   0.3996       0.44870001   0.20610002   0.43220001
    4.10056973]
 ...
 [309.52785443   0.36160001   0.39359999   0.21020003   0.3996
    4.24462271]
 [318.15996366   0.42479998   0.47470003   0.15090001   0.44370005
    4.55322695]
 [192.31643183   0.36290002   0.37540001   0.1265       0.3434
    4.2516799 ]][0m
[37m[1m[2023-07-17 12:19:32,229][257371] Max Reward on eval: 550.3298227295279[0m
[37m[1m[2023-07-17 12:19:32,230][257371] Min Reward on eval: 54.34471846427768[0m
[37m[1m[2023-07-17 12:19:32,230][257371] Mean Reward across all agents: 258.00534254313294[0m
[37m[1m[2023-07-17 12:19:32,230][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:19:32,233][257371] mean_value=-1252.1027024923178, max_value=74.6575093986479[0m
[37m[1m[2023-07-17 12:19:32,235][257371] New mean coefficients: [[ 4.833446   -0.73527956 -0.79690397 -2.808127    0.5065787  -0.9216427 ]][0m
[37m[1m[2023-07-17 12:19:32,236][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:19:41,252][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 12:19:41,253][257371] FPS: 425975.71[0m
[36m[2023-07-17 12:19:41,255][257371] itr=1195, itrs=2000, Progress: 59.75%[0m
[36m[2023-07-17 12:19:52,943][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 12:19:52,943][257371] FPS: 331855.28[0m
[36m[2023-07-17 12:19:57,273][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:19:57,273][257371] Reward + Measures: [[275.09163732   0.32460234   0.34525102   0.14385667   0.33713332
    3.98310637]][0m
[37m[1m[2023-07-17 12:19:57,273][257371] Max Reward on eval: 275.09163731785907[0m
[37m[1m[2023-07-17 12:19:57,274][257371] Min Reward on eval: 275.09163731785907[0m
[37m[1m[2023-07-17 12:19:57,274][257371] Mean Reward across all agents: 275.09163731785907[0m
[37m[1m[2023-07-17 12:19:57,274][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:20:02,238][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:20:02,239][257371] Reward + Measures: [[293.37627358   0.44940001   0.51360005   0.10140001   0.50080001
    4.49792814]
 [235.36781719   0.29280001   0.3312       0.1507       0.3188
    3.9133389 ]
 [132.42019859   0.235        0.26729998   0.12890001   0.22430001
    4.32410288]
 ...
 [180.5434914    0.2335       0.26949999   0.1538       0.26840001
    4.23588705]
 [226.02267656   0.2665       0.28090003   0.1567       0.2958
    3.75952268]
 [319.12541796   0.33320001   0.38049999   0.18550001   0.39400002
    3.9476223 ]][0m
[37m[1m[2023-07-17 12:20:02,239][257371] Max Reward on eval: 433.2267501642462[0m
[37m[1m[2023-07-17 12:20:02,239][257371] Min Reward on eval: -0.9913840318797156[0m
[37m[1m[2023-07-17 12:20:02,240][257371] Mean Reward across all agents: 206.82002387102668[0m
[37m[1m[2023-07-17 12:20:02,240][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:20:02,241][257371] mean_value=-2145.5025498640853, max_value=40.60447580878588[0m
[37m[1m[2023-07-17 12:20:02,244][257371] New mean coefficients: [[ 3.0548656  -1.1230326  -0.8870193  -2.5541084   0.45911756 -0.7029917 ]][0m
[37m[1m[2023-07-17 12:20:02,245][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:20:11,220][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 12:20:11,220][257371] FPS: 427922.52[0m
[36m[2023-07-17 12:20:11,222][257371] itr=1196, itrs=2000, Progress: 59.80%[0m
[36m[2023-07-17 12:20:22,915][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 12:20:22,915][257371] FPS: 331615.29[0m
[36m[2023-07-17 12:20:27,186][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:20:27,186][257371] Reward + Measures: [[294.2253837    0.31491065   0.33873001   0.14796866   0.33483699
    3.89494777]][0m
[37m[1m[2023-07-17 12:20:27,186][257371] Max Reward on eval: 294.22538369616274[0m
[37m[1m[2023-07-17 12:20:27,187][257371] Min Reward on eval: 294.22538369616274[0m
[37m[1m[2023-07-17 12:20:27,187][257371] Mean Reward across all agents: 294.22538369616274[0m
[37m[1m[2023-07-17 12:20:27,187][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:20:32,113][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:20:32,114][257371] Reward + Measures: [[441.75190577   0.44830003   0.51950002   0.10680001   0.50640005
    4.4365139 ]
 [221.65259187   0.29780003   0.29870003   0.19710001   0.30430004
    3.657758  ]
 [256.07500829   0.3996       0.48890001   0.13870001   0.47630006
    4.55103445]
 ...
 [346.40190885   0.39579999   0.41940004   0.17940001   0.39190003
    3.84512186]
 [166.80952278   0.20749998   0.1991       0.1803       0.21209998
    3.56636119]
 [246.92447242   0.273        0.34549999   0.17840001   0.36100003
    3.4811554 ]][0m
[37m[1m[2023-07-17 12:20:32,114][257371] Max Reward on eval: 554.8396313454025[0m
[37m[1m[2023-07-17 12:20:32,114][257371] Min Reward on eval: 27.975630129512865[0m
[37m[1m[2023-07-17 12:20:32,115][257371] Mean Reward across all agents: 271.68229082161815[0m
[37m[1m[2023-07-17 12:20:32,115][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:20:32,118][257371] mean_value=-1179.0484592094017, max_value=255.59959187329324[0m
[37m[1m[2023-07-17 12:20:32,120][257371] New mean coefficients: [[ 1.669532  -1.2430415 -1.7665162 -3.1001363  0.430193  -1.7705668]][0m
[37m[1m[2023-07-17 12:20:32,121][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:20:41,102][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 12:20:41,102][257371] FPS: 427674.03[0m
[36m[2023-07-17 12:20:41,104][257371] itr=1197, itrs=2000, Progress: 59.85%[0m
[36m[2023-07-17 12:20:53,292][257371] train() took 12.07 seconds to complete[0m
[36m[2023-07-17 12:20:53,292][257371] FPS: 318080.76[0m
[36m[2023-07-17 12:20:57,661][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:20:57,661][257371] Reward + Measures: [[297.07727273   0.30415535   0.32950565   0.14661367   0.32829401
    3.81196666]][0m
[37m[1m[2023-07-17 12:20:57,661][257371] Max Reward on eval: 297.0772727275907[0m
[37m[1m[2023-07-17 12:20:57,661][257371] Min Reward on eval: 297.0772727275907[0m
[37m[1m[2023-07-17 12:20:57,662][257371] Mean Reward across all agents: 297.0772727275907[0m
[37m[1m[2023-07-17 12:20:57,662][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:21:02,909][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:21:02,909][257371] Reward + Measures: [[112.47775758   0.20490001   0.233        0.13020001   0.24520002
    3.54159999]
 [191.95722678   0.24759999   0.28010002   0.16180001   0.26910001
    3.80807376]
 [285.84292937   0.32549998   0.36840001   0.1086       0.3619
    3.92724991]
 ...
 [190.08727065   0.20879999   0.22720002   0.13689999   0.23029999
    3.5455575 ]
 [273.62344803   0.33880004   0.37709999   0.1794       0.39390001
    3.66043401]
 [254.99316377   0.30440003   0.33740002   0.1152       0.33610001
    3.92012763]][0m
[37m[1m[2023-07-17 12:21:02,910][257371] Max Reward on eval: 564.6411227716133[0m
[37m[1m[2023-07-17 12:21:02,910][257371] Min Reward on eval: 73.76229844195768[0m
[37m[1m[2023-07-17 12:21:02,910][257371] Mean Reward across all agents: 265.9089317257185[0m
[37m[1m[2023-07-17 12:21:02,911][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:21:02,913][257371] mean_value=-1704.453170950095, max_value=221.03667191423472[0m
[37m[1m[2023-07-17 12:21:02,916][257371] New mean coefficients: [[ 2.781013  -1.9099202 -1.018868  -1.8607901  0.5202961 -1.1100233]][0m
[37m[1m[2023-07-17 12:21:02,917][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:21:11,989][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 12:21:11,990][257371] FPS: 423327.01[0m
[36m[2023-07-17 12:21:11,992][257371] itr=1198, itrs=2000, Progress: 59.90%[0m
[36m[2023-07-17 12:21:23,813][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 12:21:23,813][257371] FPS: 327969.92[0m
[36m[2023-07-17 12:21:28,089][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:21:28,090][257371] Reward + Measures: [[319.69922545   0.30243734   0.3319883    0.15237767   0.33829802
    3.71678901]][0m
[37m[1m[2023-07-17 12:21:28,090][257371] Max Reward on eval: 319.69922545261977[0m
[37m[1m[2023-07-17 12:21:28,090][257371] Min Reward on eval: 319.69922545261977[0m
[37m[1m[2023-07-17 12:21:28,091][257371] Mean Reward across all agents: 319.69922545261977[0m
[37m[1m[2023-07-17 12:21:28,091][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:21:33,101][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:21:33,102][257371] Reward + Measures: [[261.97659699   0.29500002   0.35489997   0.14659999   0.3567
    3.80604434]
 [313.30800488   0.32320002   0.35339999   0.1373       0.35830003
    3.87909317]
 [195.1252042    0.29710001   0.33469999   0.16769999   0.33960003
    3.72356534]
 ...
 [321.47196172   0.34419999   0.40549999   0.15290001   0.40620002
    3.99029398]
 [ 27.00380134   0.2617       0.2579       0.19330001   0.1912
    3.84708858]
 [400.94853303   0.4219       0.49429998   0.1451       0.49419999
    4.36666441]][0m
[37m[1m[2023-07-17 12:21:33,102][257371] Max Reward on eval: 492.6608297549421[0m
[37m[1m[2023-07-17 12:21:33,102][257371] Min Reward on eval: -0.9975850228045602[0m
[37m[1m[2023-07-17 12:21:33,102][257371] Mean Reward across all agents: 277.571252855144[0m
[37m[1m[2023-07-17 12:21:33,103][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:21:33,105][257371] mean_value=-1698.6367425646254, max_value=72.87557291577599[0m
[37m[1m[2023-07-17 12:21:33,107][257371] New mean coefficients: [[ 2.6873167 -1.6506728 -1.0927665 -1.9636899  0.4908942 -0.7671441]][0m
[37m[1m[2023-07-17 12:21:33,108][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:21:42,206][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 12:21:42,206][257371] FPS: 422161.43[0m
[36m[2023-07-17 12:21:42,208][257371] itr=1199, itrs=2000, Progress: 59.95%[0m
[36m[2023-07-17 12:21:53,994][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 12:21:53,994][257371] FPS: 328972.12[0m
[36m[2023-07-17 12:21:58,274][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:21:58,275][257371] Reward + Measures: [[321.78466304   0.29070801   0.31521466   0.15281732   0.33238932
    3.60986638]][0m
[37m[1m[2023-07-17 12:21:58,275][257371] Max Reward on eval: 321.7846630393208[0m
[37m[1m[2023-07-17 12:21:58,275][257371] Min Reward on eval: 321.7846630393208[0m
[37m[1m[2023-07-17 12:21:58,276][257371] Mean Reward across all agents: 321.7846630393208[0m
[37m[1m[2023-07-17 12:21:58,276][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:22:03,271][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:22:03,272][257371] Reward + Measures: [[174.47753936   0.26800001   0.25330001   0.17180002   0.2362
    3.85884261]
 [300.00868024   0.30650005   0.31860003   0.1274       0.32269999
    3.96789432]
 [383.83110252   0.37090001   0.41289997   0.1321       0.41370001
    3.93608594]
 ...
 [233.4342948    0.18640001   0.18490002   0.14659999   0.19140001
    3.16217303]
 [327.07239595   0.31889999   0.3448       0.1146       0.33100003
    3.87689471]
 [203.46914367   0.22930001   0.28570002   0.1258       0.28729999
    3.94744849]][0m
[37m[1m[2023-07-17 12:22:03,272][257371] Max Reward on eval: 486.5945570333861[0m
[37m[1m[2023-07-17 12:22:03,272][257371] Min Reward on eval: 50.82008616277017[0m
[37m[1m[2023-07-17 12:22:03,273][257371] Mean Reward across all agents: 273.6736936360811[0m
[37m[1m[2023-07-17 12:22:03,273][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:22:03,275][257371] mean_value=-2013.5101328879487, max_value=137.38208445504722[0m
[37m[1m[2023-07-17 12:22:03,277][257371] New mean coefficients: [[ 2.4176905  -1.0823624  -0.96630967 -1.8687198   0.8080994  -0.66332936]][0m
[37m[1m[2023-07-17 12:22:03,278][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:22:12,281][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 12:22:12,282][257371] FPS: 426591.26[0m
[36m[2023-07-17 12:22:12,284][257371] itr=1200, itrs=2000, Progress: 60.00%[0m
[37m[1m[2023-07-17 12:25:40,175][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001180[0m
[36m[2023-07-17 12:25:52,527][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 12:25:52,527][257371] FPS: 326405.41[0m
[36m[2023-07-17 12:25:56,772][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:25:56,773][257371] Reward + Measures: [[338.90169368   0.28963232   0.31514567   0.15467733   0.33677864
    3.54810667]][0m
[37m[1m[2023-07-17 12:25:56,773][257371] Max Reward on eval: 338.9016936822913[0m
[37m[1m[2023-07-17 12:25:56,773][257371] Min Reward on eval: 338.9016936822913[0m
[37m[1m[2023-07-17 12:25:56,773][257371] Mean Reward across all agents: 338.9016936822913[0m
[37m[1m[2023-07-17 12:25:56,774][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:26:01,980][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:26:01,980][257371] Reward + Measures: [[400.3736179    0.30700001   0.3303       0.18310001   0.37369999
    3.36115384]
 [397.78539795   0.41540003   0.43689996   0.1961       0.39360005
    3.97470474]
 [276.34713342   0.33080003   0.34240001   0.1734       0.38179997
    3.91304374]
 ...
 [375.13035947   0.34600002   0.36250001   0.17290001   0.36820003
    3.78462768]
 [284.24102388   0.27830002   0.29930001   0.18570001   0.32819998
    3.61818767]
 [325.61133057   0.32269999   0.34200001   0.1635       0.36380002
    3.62319112]][0m
[37m[1m[2023-07-17 12:26:01,980][257371] Max Reward on eval: 575.4070300699211[0m
[37m[1m[2023-07-17 12:26:01,981][257371] Min Reward on eval: 101.2439023363404[0m
[37m[1m[2023-07-17 12:26:01,981][257371] Mean Reward across all agents: 302.2211980321443[0m
[37m[1m[2023-07-17 12:26:01,981][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:26:01,984][257371] mean_value=-2387.0289332422267, max_value=233.51431474102714[0m
[37m[1m[2023-07-17 12:26:01,986][257371] New mean coefficients: [[ 2.328281   -0.677714   -1.2285316  -1.4418304   0.14663237 -0.5070744 ]][0m
[37m[1m[2023-07-17 12:26:01,987][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:26:11,017][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 12:26:11,017][257371] FPS: 425333.50[0m
[36m[2023-07-17 12:26:11,020][257371] itr=1201, itrs=2000, Progress: 60.05%[0m
[36m[2023-07-17 12:26:22,730][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 12:26:22,731][257371] FPS: 331209.51[0m
[36m[2023-07-17 12:26:26,960][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:26:26,961][257371] Reward + Measures: [[369.37372967   0.29201433   0.32017532   0.15655768   0.34479037
    3.50621247]][0m
[37m[1m[2023-07-17 12:26:26,961][257371] Max Reward on eval: 369.3737296727271[0m
[37m[1m[2023-07-17 12:26:26,961][257371] Min Reward on eval: 369.3737296727271[0m
[37m[1m[2023-07-17 12:26:26,961][257371] Mean Reward across all agents: 369.3737296727271[0m
[37m[1m[2023-07-17 12:26:26,962][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:26:32,015][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:26:32,016][257371] Reward + Measures: [[281.82936502   0.2368       0.22720002   0.18699999   0.27700004
    3.23753214]
 [ 96.3160278    0.16069999   0.1605       0.08000001   0.161
    3.86552167]
 [175.27104917   0.1714       0.17         0.14770001   0.18339999
    3.14484692]
 ...
 [525.88804655   0.44679999   0.46540004   0.18619999   0.49390003
    3.96006775]
 [227.20438726   0.27110001   0.29299998   0.10120001   0.28730002
    4.19139242]
 [246.60286942   0.26770002   0.28490001   0.12850001   0.28590003
    3.5601213 ]][0m
[37m[1m[2023-07-17 12:26:32,016][257371] Max Reward on eval: 550.5062939848751[0m
[37m[1m[2023-07-17 12:26:32,016][257371] Min Reward on eval: 52.17567630857229[0m
[37m[1m[2023-07-17 12:26:32,017][257371] Mean Reward across all agents: 313.89340103715074[0m
[37m[1m[2023-07-17 12:26:32,017][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:26:32,019][257371] mean_value=-2200.1565598348984, max_value=169.1137945668233[0m
[37m[1m[2023-07-17 12:26:32,022][257371] New mean coefficients: [[ 1.5827199  -0.9386722  -0.72583705 -1.7282277   0.07110352 -0.5757563 ]][0m
[37m[1m[2023-07-17 12:26:32,023][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:26:41,148][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 12:26:41,149][257371] FPS: 420870.20[0m
[36m[2023-07-17 12:26:41,151][257371] itr=1202, itrs=2000, Progress: 60.10%[0m
[36m[2023-07-17 12:26:53,017][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 12:26:53,017][257371] FPS: 326855.04[0m
[36m[2023-07-17 12:26:57,262][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:26:57,262][257371] Reward + Measures: [[386.88914702   0.28588834   0.31317398   0.16167967   0.34517166
    3.41681314]][0m
[37m[1m[2023-07-17 12:26:57,262][257371] Max Reward on eval: 386.88914701769596[0m
[37m[1m[2023-07-17 12:26:57,263][257371] Min Reward on eval: 386.88914701769596[0m
[37m[1m[2023-07-17 12:26:57,263][257371] Mean Reward across all agents: 386.88914701769596[0m
[37m[1m[2023-07-17 12:26:57,263][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:27:02,252][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:27:02,253][257371] Reward + Measures: [[389.36733597   0.34989998   0.40060002   0.14         0.42150003
    3.79808545]
 [305.11934108   0.2237       0.26750001   0.13579999   0.27880001
    3.42352152]
 [266.5669288    0.28         0.29700002   0.18450001   0.31820002
    3.49926996]
 ...
 [337.62815301   0.29960001   0.3265       0.1468       0.3346
    3.62007904]
 [197.59144561   0.22140001   0.24070001   0.1251       0.25799999
    3.63491869]
 [408.08191629   0.34030002   0.3655       0.15620001   0.38040003
    3.7893548 ]][0m
[37m[1m[2023-07-17 12:27:02,253][257371] Max Reward on eval: 590.4958133960142[0m
[37m[1m[2023-07-17 12:27:02,253][257371] Min Reward on eval: 71.82698519518598[0m
[37m[1m[2023-07-17 12:27:02,254][257371] Mean Reward across all agents: 338.70018538154886[0m
[37m[1m[2023-07-17 12:27:02,254][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:27:02,257][257371] mean_value=-2505.3734195803822, max_value=180.03812236856942[0m
[37m[1m[2023-07-17 12:27:02,259][257371] New mean coefficients: [[ 2.5691285  -1.2438718  -0.3608558  -1.0360384   0.15102829  0.08957148]][0m
[37m[1m[2023-07-17 12:27:02,260][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:27:11,363][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 12:27:11,363][257371] FPS: 421943.69[0m
[36m[2023-07-17 12:27:11,365][257371] itr=1203, itrs=2000, Progress: 60.15%[0m
[36m[2023-07-17 12:27:23,350][257371] train() took 11.87 seconds to complete[0m
[36m[2023-07-17 12:27:23,350][257371] FPS: 323591.46[0m
[36m[2023-07-17 12:27:27,697][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:27:27,697][257371] Reward + Measures: [[417.32743116   0.29506233   0.32003233   0.16139801   0.35244364
    3.44861674]][0m
[37m[1m[2023-07-17 12:27:27,697][257371] Max Reward on eval: 417.327431155265[0m
[37m[1m[2023-07-17 12:27:27,698][257371] Min Reward on eval: 417.327431155265[0m
[37m[1m[2023-07-17 12:27:27,698][257371] Mean Reward across all agents: 417.327431155265[0m
[37m[1m[2023-07-17 12:27:27,698][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:27:32,684][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:27:32,685][257371] Reward + Measures: [[329.84231218   0.25250003   0.2651       0.1781       0.29550001
    3.50320053]
 [265.16567193   0.26350003   0.2543       0.14210001   0.26069999
    3.61444831]
 [501.32518455   0.3558       0.38860002   0.2142       0.42910004
    3.76153302]
 ...
 [352.24461801   0.27239999   0.29010001   0.1557       0.33680001
    3.34217644]
 [284.27052643   0.23560002   0.24560001   0.1611       0.23199999
    3.40022469]
 [299.97231265   0.2493       0.24920002   0.17980002   0.26539999
    3.52558589]][0m
[37m[1m[2023-07-17 12:27:32,685][257371] Max Reward on eval: 660.7339286939241[0m
[37m[1m[2023-07-17 12:27:32,685][257371] Min Reward on eval: 41.273307730117814[0m
[37m[1m[2023-07-17 12:27:32,686][257371] Mean Reward across all agents: 371.4235167236841[0m
[37m[1m[2023-07-17 12:27:32,686][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:27:32,688][257371] mean_value=-2162.205538972439, max_value=280.65779028421207[0m
[37m[1m[2023-07-17 12:27:32,691][257371] New mean coefficients: [[ 2.3993635  -1.8101683   0.46601185 -0.45883363  1.0260061  -0.12387894]][0m
[37m[1m[2023-07-17 12:27:32,692][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:27:41,577][257371] train() took 8.88 seconds to complete[0m
[36m[2023-07-17 12:27:41,577][257371] FPS: 432253.17[0m
[36m[2023-07-17 12:27:41,579][257371] itr=1204, itrs=2000, Progress: 60.20%[0m
[36m[2023-07-17 12:27:53,446][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 12:27:53,446][257371] FPS: 326824.16[0m
[36m[2023-07-17 12:27:57,761][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:27:57,762][257371] Reward + Measures: [[430.70125616   0.288297     0.31579399   0.16048901   0.35112834
    3.42403483]][0m
[37m[1m[2023-07-17 12:27:57,762][257371] Max Reward on eval: 430.701256161605[0m
[37m[1m[2023-07-17 12:27:57,762][257371] Min Reward on eval: 430.701256161605[0m
[37m[1m[2023-07-17 12:27:57,762][257371] Mean Reward across all agents: 430.701256161605[0m
[37m[1m[2023-07-17 12:27:57,763][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:28:02,747][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:28:02,748][257371] Reward + Measures: [[494.27457482   0.40459999   0.43660003   0.1401       0.47740003
    3.86233902]
 [403.91694069   0.3186       0.39519998   0.1463       0.41749999
    4.01707458]
 [411.35617678   0.3691       0.41969997   0.1556       0.42319998
    3.87795305]
 ...
 [414.49152323   0.33750001   0.39230001   0.1186       0.39850003
    3.91876268]
 [283.21384063   0.25419998   0.28840002   0.13600001   0.28690001
    3.69867015]
 [247.2233929    0.2088       0.2586       0.12060001   0.2712
    3.63355684]][0m
[37m[1m[2023-07-17 12:28:02,748][257371] Max Reward on eval: 625.0093421996804[0m
[37m[1m[2023-07-17 12:28:02,748][257371] Min Reward on eval: 107.5075836901553[0m
[37m[1m[2023-07-17 12:28:02,748][257371] Mean Reward across all agents: 383.5368557085303[0m
[37m[1m[2023-07-17 12:28:02,749][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:28:02,751][257371] mean_value=-2321.1072756909616, max_value=114.98491640225257[0m
[37m[1m[2023-07-17 12:28:02,754][257371] New mean coefficients: [[ 2.5234606  -0.8669479  -0.06458065 -0.4039611   0.91279036  0.11941169]][0m
[37m[1m[2023-07-17 12:28:02,754][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:28:11,841][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 12:28:11,842][257371] FPS: 422660.47[0m
[36m[2023-07-17 12:28:11,844][257371] itr=1205, itrs=2000, Progress: 60.25%[0m
[36m[2023-07-17 12:28:23,800][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-17 12:28:23,800][257371] FPS: 324265.58[0m
[36m[2023-07-17 12:28:28,054][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:28:28,055][257371] Reward + Measures: [[457.31608047   0.29169366   0.32188565   0.15937565   0.36133128
    3.43260288]][0m
[37m[1m[2023-07-17 12:28:28,055][257371] Max Reward on eval: 457.3160804698854[0m
[37m[1m[2023-07-17 12:28:28,055][257371] Min Reward on eval: 457.3160804698854[0m
[37m[1m[2023-07-17 12:28:28,055][257371] Mean Reward across all agents: 457.3160804698854[0m
[37m[1m[2023-07-17 12:28:28,056][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:28:33,095][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:28:33,095][257371] Reward + Measures: [[420.34379061   0.38030002   0.44210002   0.15589999   0.46849999
    4.16824913]
 [372.90757489   0.38549998   0.46420002   0.09790001   0.48330003
    4.28939486]
 [416.97474954   0.42460003   0.44370005   0.13630001   0.44169998
    3.90190959]
 ...
 [445.74418984   0.36630002   0.4007       0.18550001   0.46670005
    3.76804972]
 [551.8147359    0.35050002   0.39420003   0.18520001   0.45900002
    3.6058166 ]
 [304.61931404   0.2719       0.26209998   0.19990002   0.27360001
    3.32763481]][0m
[37m[1m[2023-07-17 12:28:33,096][257371] Max Reward on eval: 653.8318825217896[0m
[37m[1m[2023-07-17 12:28:33,096][257371] Min Reward on eval: 113.74344845670275[0m
[37m[1m[2023-07-17 12:28:33,096][257371] Mean Reward across all agents: 369.19607092128444[0m
[37m[1m[2023-07-17 12:28:33,096][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:28:33,098][257371] mean_value=-2469.270648736196, max_value=147.76502727458353[0m
[37m[1m[2023-07-17 12:28:33,101][257371] New mean coefficients: [[ 1.3830721  -0.9729004   0.04558087 -0.2354882   1.7432852   0.6786745 ]][0m
[37m[1m[2023-07-17 12:28:33,102][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:28:42,150][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 12:28:42,151][257371] FPS: 424457.08[0m
[36m[2023-07-17 12:28:42,153][257371] itr=1206, itrs=2000, Progress: 60.30%[0m
[36m[2023-07-17 12:28:54,014][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 12:28:54,015][257371] FPS: 326934.71[0m
[36m[2023-07-17 12:28:58,365][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:28:58,365][257371] Reward + Measures: [[462.95625031   0.288432     0.31773633   0.15735833   0.361462
    3.44149661]][0m
[37m[1m[2023-07-17 12:28:58,366][257371] Max Reward on eval: 462.95625031026026[0m
[37m[1m[2023-07-17 12:28:58,366][257371] Min Reward on eval: 462.95625031026026[0m
[37m[1m[2023-07-17 12:28:58,366][257371] Mean Reward across all agents: 462.95625031026026[0m
[37m[1m[2023-07-17 12:28:58,366][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:29:03,640][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:29:03,641][257371] Reward + Measures: [[422.59527019   0.3917       0.39439997   0.22129999   0.40690002
    3.747298  ]
 [406.2248995    0.3082       0.3369       0.14579999   0.36110002
    3.65222049]
 [295.01831282   0.29550001   0.32209998   0.12310001   0.31720001
    3.77912116]
 ...
 [573.37643003   0.40370002   0.47299996   0.1753       0.50640005
    3.72565627]
 [524.28766728   0.3691       0.42280003   0.1548       0.44119999
    3.9693222 ]
 [561.0894656    0.4131       0.48629999   0.15259999   0.51809996
    3.89629292]][0m
[37m[1m[2023-07-17 12:29:03,641][257371] Max Reward on eval: 664.6646690549329[0m
[37m[1m[2023-07-17 12:29:03,641][257371] Min Reward on eval: 152.4399261780083[0m
[37m[1m[2023-07-17 12:29:03,641][257371] Mean Reward across all agents: 425.4135215198736[0m
[37m[1m[2023-07-17 12:29:03,642][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:29:03,644][257371] mean_value=-1813.0149384875642, max_value=484.0296317101671[0m
[37m[1m[2023-07-17 12:29:03,647][257371] New mean coefficients: [[ 1.3322852  -1.1796472   0.38530368 -0.71926236  2.3186402   0.90624225]][0m
[37m[1m[2023-07-17 12:29:03,648][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:29:12,729][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 12:29:12,729][257371] FPS: 422923.26[0m
[36m[2023-07-17 12:29:12,732][257371] itr=1207, itrs=2000, Progress: 60.35%[0m
[36m[2023-07-17 12:29:24,522][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 12:29:24,523][257371] FPS: 328840.83[0m
[36m[2023-07-17 12:29:28,782][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:29:28,783][257371] Reward + Measures: [[468.95452449   0.28651834   0.32041064   0.15444933   0.36960831
    3.50490665]][0m
[37m[1m[2023-07-17 12:29:28,783][257371] Max Reward on eval: 468.9545244857568[0m
[37m[1m[2023-07-17 12:29:28,783][257371] Min Reward on eval: 468.9545244857568[0m
[37m[1m[2023-07-17 12:29:28,784][257371] Mean Reward across all agents: 468.9545244857568[0m
[37m[1m[2023-07-17 12:29:28,784][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:29:33,762][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:29:33,763][257371] Reward + Measures: [[432.46346065   0.29050002   0.3858       0.13770001   0.41429996
    3.82651567]
 [352.32665098   0.23199999   0.25779998   0.1234       0.30689999
    3.5839653 ]
 [646.17120741   0.40869999   0.4684       0.16610001   0.52310002
    3.95072675]
 ...
 [414.56655813   0.30719998   0.34650001   0.15750001   0.35640001
    3.66718078]
 [380.78525202   0.3159       0.31580004   0.16040002   0.34599999
    3.56411147]
 [510.04964526   0.32590002   0.37939999   0.16340001   0.4429
    3.72463536]][0m
[37m[1m[2023-07-17 12:29:33,763][257371] Max Reward on eval: 702.4596157083288[0m
[37m[1m[2023-07-17 12:29:33,763][257371] Min Reward on eval: 143.3196450823918[0m
[37m[1m[2023-07-17 12:29:33,763][257371] Mean Reward across all agents: 432.86856013586794[0m
[37m[1m[2023-07-17 12:29:33,764][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:29:33,766][257371] mean_value=-2381.925117865112, max_value=522.6689168695965[0m
[37m[1m[2023-07-17 12:29:33,768][257371] New mean coefficients: [[ 1.7184927  -1.1818135   0.49329662 -0.51437867  2.1670046   0.25048476]][0m
[37m[1m[2023-07-17 12:29:33,769][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:29:42,806][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 12:29:42,806][257371] FPS: 425018.61[0m
[36m[2023-07-17 12:29:42,808][257371] itr=1208, itrs=2000, Progress: 60.40%[0m
[36m[2023-07-17 12:29:54,765][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-17 12:29:54,765][257371] FPS: 324267.43[0m
[36m[2023-07-17 12:29:59,070][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:29:59,071][257371] Reward + Measures: [[496.65308495   0.29569632   0.32940197   0.15750566   0.38312367
    3.52694893]][0m
[37m[1m[2023-07-17 12:29:59,071][257371] Max Reward on eval: 496.6530849528117[0m
[37m[1m[2023-07-17 12:29:59,071][257371] Min Reward on eval: 496.6530849528117[0m
[37m[1m[2023-07-17 12:29:59,071][257371] Mean Reward across all agents: 496.6530849528117[0m
[37m[1m[2023-07-17 12:29:59,072][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:30:04,107][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:30:04,107][257371] Reward + Measures: [[664.33128118   0.41280004   0.50600004   0.1506       0.55379999
    3.82371068]
 [613.24728352   0.4052       0.45499998   0.18740001   0.52130002
    3.76178217]
 [211.05740262   0.2148       0.2217       0.16229999   0.18260001
    3.59269691]
 ...
 [409.75158871   0.3378       0.34709999   0.1328       0.35030004
    3.8418541 ]
 [394.47277382   0.29190001   0.3407       0.17900001   0.38620001
    3.39040112]
 [411.44996227   0.32319999   0.38009998   0.14740001   0.40109998
    3.83844614]][0m
[37m[1m[2023-07-17 12:30:04,107][257371] Max Reward on eval: 740.428699482698[0m
[37m[1m[2023-07-17 12:30:04,108][257371] Min Reward on eval: 89.69292304124102[0m
[37m[1m[2023-07-17 12:30:04,108][257371] Mean Reward across all agents: 431.5354849456351[0m
[37m[1m[2023-07-17 12:30:04,108][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:30:04,110][257371] mean_value=-2473.61197597984, max_value=151.23267203141995[0m
[37m[1m[2023-07-17 12:30:04,113][257371] New mean coefficients: [[ 1.4404787  -0.38049555  0.82746124  0.39218068  1.7176751   0.9406418 ]][0m
[37m[1m[2023-07-17 12:30:04,114][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:30:13,172][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 12:30:13,172][257371] FPS: 424006.75[0m
[36m[2023-07-17 12:30:13,174][257371] itr=1209, itrs=2000, Progress: 60.45%[0m
[36m[2023-07-17 12:30:25,109][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 12:30:25,109][257371] FPS: 324831.22[0m
[36m[2023-07-17 12:30:29,358][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:30:29,358][257371] Reward + Measures: [[519.12270995   0.30219331   0.34021932   0.15964      0.39964965
    3.57297111]][0m
[37m[1m[2023-07-17 12:30:29,358][257371] Max Reward on eval: 519.1227099469401[0m
[37m[1m[2023-07-17 12:30:29,359][257371] Min Reward on eval: 519.1227099469401[0m
[37m[1m[2023-07-17 12:30:29,359][257371] Mean Reward across all agents: 519.1227099469401[0m
[37m[1m[2023-07-17 12:30:29,359][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:30:34,262][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:30:34,262][257371] Reward + Measures: [[475.6528969    0.4224       0.47979999   0.11669999   0.47220001
    4.48236322]
 [272.85880788   0.2631       0.29819998   0.10640001   0.30939999
    3.86883235]
 [354.80498996   0.23989999   0.26509997   0.14299999   0.31029999
    3.54108238]
 ...
 [573.32198405   0.47860003   0.54500002   0.11559999   0.5686
    4.41139221]
 [356.16944882   0.25170001   0.2814       0.1627       0.278
    3.34040618]
 [657.93482588   0.48310003   0.56089997   0.1452       0.5941
    4.38885212]][0m
[37m[1m[2023-07-17 12:30:34,263][257371] Max Reward on eval: 684.995659894403[0m
[37m[1m[2023-07-17 12:30:34,263][257371] Min Reward on eval: 155.28121394831686[0m
[37m[1m[2023-07-17 12:30:34,263][257371] Mean Reward across all agents: 447.1353046423518[0m
[37m[1m[2023-07-17 12:30:34,263][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:30:34,266][257371] mean_value=-1332.6412463619313, max_value=355.55750828947293[0m
[37m[1m[2023-07-17 12:30:34,268][257371] New mean coefficients: [[ 1.4988118  -0.6880466   0.79501635  0.23571256  1.8368509   1.7656841 ]][0m
[37m[1m[2023-07-17 12:30:34,269][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:30:43,244][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 12:30:43,244][257371] FPS: 427934.59[0m
[36m[2023-07-17 12:30:43,246][257371] itr=1210, itrs=2000, Progress: 60.50%[0m
[37m[1m[2023-07-17 12:34:13,420][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001190[0m
[36m[2023-07-17 12:34:25,963][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-17 12:34:25,963][257371] FPS: 323646.38[0m
[36m[2023-07-17 12:34:30,123][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:34:30,124][257371] Reward + Measures: [[497.80604678   0.29549864   0.33080733   0.153118     0.39157799
    3.59503794]][0m
[37m[1m[2023-07-17 12:34:30,124][257371] Max Reward on eval: 497.80604678096205[0m
[37m[1m[2023-07-17 12:34:30,124][257371] Min Reward on eval: 497.80604678096205[0m
[37m[1m[2023-07-17 12:34:30,124][257371] Mean Reward across all agents: 497.80604678096205[0m
[37m[1m[2023-07-17 12:34:30,125][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:34:35,300][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:34:35,301][257371] Reward + Measures: [[365.90291491   0.2561       0.2881       0.13510001   0.33520001
    3.65713859]
 [358.64646649   0.35230002   0.3572       0.191        0.39219999
    3.57030106]
 [485.85920629   0.25799999   0.3064       0.1655       0.37800002
    3.57324958]
 ...
 [529.01011928   0.37170002   0.42010003   0.1622       0.46079999
    3.91548133]
 [557.93249296   0.42160001   0.52069998   0.12390001   0.51719999
    4.21895313]
 [498.13652469   0.44800001   0.50940001   0.1053       0.53820002
    4.4745388 ]][0m
[37m[1m[2023-07-17 12:34:35,301][257371] Max Reward on eval: 744.6858558578417[0m
[37m[1m[2023-07-17 12:34:35,301][257371] Min Reward on eval: 129.67642372425763[0m
[37m[1m[2023-07-17 12:34:35,301][257371] Mean Reward across all agents: 448.2308622325002[0m
[37m[1m[2023-07-17 12:34:35,302][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:34:35,304][257371] mean_value=-1666.2494623185344, max_value=350.61216623592907[0m
[37m[1m[2023-07-17 12:34:35,307][257371] New mean coefficients: [[ 1.7568097  -1.400618    0.3825368  -0.30475107  1.8251759   1.6244469 ]][0m
[37m[1m[2023-07-17 12:34:35,308][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:34:44,181][257371] train() took 8.87 seconds to complete[0m
[36m[2023-07-17 12:34:44,182][257371] FPS: 432803.28[0m
[36m[2023-07-17 12:34:44,184][257371] itr=1211, itrs=2000, Progress: 60.55%[0m
[36m[2023-07-17 12:34:55,795][257371] train() took 11.49 seconds to complete[0m
[36m[2023-07-17 12:34:55,795][257371] FPS: 334087.05[0m
[36m[2023-07-17 12:35:00,044][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:35:00,044][257371] Reward + Measures: [[506.21223345   0.30286133   0.34029502   0.14938033   0.40126199
    3.71600652]][0m
[37m[1m[2023-07-17 12:35:00,044][257371] Max Reward on eval: 506.2122334512896[0m
[37m[1m[2023-07-17 12:35:00,045][257371] Min Reward on eval: 506.2122334512896[0m
[37m[1m[2023-07-17 12:35:00,045][257371] Mean Reward across all agents: 506.2122334512896[0m
[37m[1m[2023-07-17 12:35:00,045][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:35:04,975][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:35:04,975][257371] Reward + Measures: [[239.59036761   0.23         0.2577       0.0718       0.25740001
    3.82012033]
 [502.95191034   0.5467       0.6099       0.13260001   0.64280003
    4.63383818]
 [510.8787472    0.37280002   0.39540002   0.2194       0.46200004
    3.83688283]
 ...
 [339.73514226   0.1998       0.2009       0.14480001   0.26190001
    3.4686954 ]
 [518.03365259   0.35859999   0.40130001   0.1885       0.43120003
    3.95497894]
 [715.05806349   0.45410004   0.51260006   0.1735       0.58829993
    3.9754014 ]][0m
[37m[1m[2023-07-17 12:35:04,975][257371] Max Reward on eval: 715.0580634896644[0m
[37m[1m[2023-07-17 12:35:04,976][257371] Min Reward on eval: 130.99531153098215[0m
[37m[1m[2023-07-17 12:35:04,976][257371] Mean Reward across all agents: 451.5312824740362[0m
[37m[1m[2023-07-17 12:35:04,976][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:35:04,979][257371] mean_value=-1147.3708965824965, max_value=207.30449177680237[0m
[37m[1m[2023-07-17 12:35:04,981][257371] New mean coefficients: [[ 0.9705117  -1.8211032   0.45325485 -0.7176736   1.8266423   1.629198  ]][0m
[37m[1m[2023-07-17 12:35:04,982][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:35:13,952][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 12:35:13,952][257371] FPS: 428191.09[0m
[36m[2023-07-17 12:35:13,954][257371] itr=1212, itrs=2000, Progress: 60.60%[0m
[36m[2023-07-17 12:35:25,736][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 12:35:25,736][257371] FPS: 329051.59[0m
[36m[2023-07-17 12:35:30,065][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:35:30,065][257371] Reward + Measures: [[508.87288578   0.30923933   0.34830368   0.142855     0.41041133
    3.84827662]][0m
[37m[1m[2023-07-17 12:35:30,066][257371] Max Reward on eval: 508.8728857782731[0m
[37m[1m[2023-07-17 12:35:30,066][257371] Min Reward on eval: 508.8728857782731[0m
[37m[1m[2023-07-17 12:35:30,066][257371] Mean Reward across all agents: 508.8728857782731[0m
[37m[1m[2023-07-17 12:35:30,066][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:35:35,083][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:35:35,084][257371] Reward + Measures: [[475.16511534   0.28490001   0.34999999   0.16940001   0.42430001
    3.76901698]
 [561.60241034   0.391        0.40540001   0.15760002   0.46900001
    4.03982496]
 [352.43888863   0.26989999   0.27530003   0.14929999   0.333
    3.70731091]
 ...
 [474.39398575   0.39879999   0.43070003   0.1398       0.45740005
    4.16475582]
 [368.56418091   0.25800002   0.32550001   0.14009999   0.39219999
    3.73432517]
 [559.42586169   0.31369999   0.40620002   0.1664       0.45720002
    4.17862606]][0m
[37m[1m[2023-07-17 12:35:35,084][257371] Max Reward on eval: 695.5536705623381[0m
[37m[1m[2023-07-17 12:35:35,085][257371] Min Reward on eval: 221.74564639292657[0m
[37m[1m[2023-07-17 12:35:35,085][257371] Mean Reward across all agents: 448.02035168715423[0m
[37m[1m[2023-07-17 12:35:35,085][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:35:35,087][257371] mean_value=-1391.0240626274942, max_value=429.4811166630483[0m
[37m[1m[2023-07-17 12:35:35,090][257371] New mean coefficients: [[ 0.83721554 -0.91337895  0.57202566 -0.16759431  1.7258486   1.7809466 ]][0m
[37m[1m[2023-07-17 12:35:35,091][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:35:44,107][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 12:35:44,108][257371] FPS: 425971.62[0m
[36m[2023-07-17 12:35:44,110][257371] itr=1213, itrs=2000, Progress: 60.65%[0m
[36m[2023-07-17 12:35:55,991][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 12:35:55,991][257371] FPS: 326307.38[0m
[36m[2023-07-17 12:36:00,317][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:36:00,318][257371] Reward + Measures: [[507.06813501   0.30846265   0.34801432   0.141965     0.41543302
    3.90705204]][0m
[37m[1m[2023-07-17 12:36:00,318][257371] Max Reward on eval: 507.06813501147974[0m
[37m[1m[2023-07-17 12:36:00,318][257371] Min Reward on eval: 507.06813501147974[0m
[37m[1m[2023-07-17 12:36:00,319][257371] Mean Reward across all agents: 507.06813501147974[0m
[37m[1m[2023-07-17 12:36:00,319][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:36:05,306][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:36:05,307][257371] Reward + Measures: [[369.82328536   0.3008       0.317        0.13149999   0.35170001
    4.18342972]
 [473.65731719   0.31130001   0.33940002   0.15640001   0.4104
    3.90546489]
 [393.10127206   0.31399998   0.34199998   0.1323       0.37670001
    4.04225779]
 ...
 [461.14458634   0.3344       0.38970003   0.1454       0.46019998
    3.98985744]
 [515.69950285   0.37         0.55250001   0.1701       0.61720002
    4.51285696]
 [371.65076853   0.27940002   0.33109999   0.12580001   0.39859998
    4.11453962]][0m
[37m[1m[2023-07-17 12:36:05,307][257371] Max Reward on eval: 712.4043617271352[0m
[37m[1m[2023-07-17 12:36:05,308][257371] Min Reward on eval: 142.6049028580077[0m
[37m[1m[2023-07-17 12:36:05,308][257371] Mean Reward across all agents: 438.5068849326001[0m
[37m[1m[2023-07-17 12:36:05,308][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:36:05,310][257371] mean_value=-1019.6727702868743, max_value=546.7598546632235[0m
[37m[1m[2023-07-17 12:36:05,313][257371] New mean coefficients: [[ 0.71306723 -1.0895305   0.40842608 -0.90256816  1.8213508   1.7881932 ]][0m
[37m[1m[2023-07-17 12:36:05,314][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:36:14,288][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 12:36:14,288][257371] FPS: 427983.61[0m
[36m[2023-07-17 12:36:14,290][257371] itr=1214, itrs=2000, Progress: 60.70%[0m
[36m[2023-07-17 12:36:26,011][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 12:36:26,011][257371] FPS: 330826.96[0m
[36m[2023-07-17 12:36:30,335][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:36:30,335][257371] Reward + Measures: [[512.05335946   0.31678268   0.35857165   0.13853332   0.43134129
    4.00942993]][0m
[37m[1m[2023-07-17 12:36:30,335][257371] Max Reward on eval: 512.0533594564943[0m
[37m[1m[2023-07-17 12:36:30,336][257371] Min Reward on eval: 512.0533594564943[0m
[37m[1m[2023-07-17 12:36:30,336][257371] Mean Reward across all agents: 512.0533594564943[0m
[37m[1m[2023-07-17 12:36:30,336][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:36:35,341][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:36:35,341][257371] Reward + Measures: [[330.11927904   0.25260001   0.29190001   0.1242       0.33339998
    4.15002012]
 [561.19233665   0.50089997   0.53200001   0.1202       0.5844
    4.47328329]
 [365.25927544   0.29190001   0.32959998   0.14039999   0.41890001
    4.03470993]
 ...
 [384.92449994   0.39269999   0.45100003   0.132        0.50369996
    4.28042984]
 [375.69211741   0.2818       0.32269999   0.13890001   0.38900003
    3.74284601]
 [387.88203246   0.24850002   0.28849998   0.18190001   0.34209999
    3.76923561]][0m
[37m[1m[2023-07-17 12:36:35,341][257371] Max Reward on eval: 724.0450668583624[0m
[37m[1m[2023-07-17 12:36:35,342][257371] Min Reward on eval: 142.8196312353015[0m
[37m[1m[2023-07-17 12:36:35,342][257371] Mean Reward across all agents: 434.99240780379836[0m
[37m[1m[2023-07-17 12:36:35,342][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:36:35,345][257371] mean_value=-772.6997494629511, max_value=421.9189707012664[0m
[37m[1m[2023-07-17 12:36:35,348][257371] New mean coefficients: [[ 0.6151309  -1.235229    0.48495188 -1.1541955   2.1224625   0.7358912 ]][0m
[37m[1m[2023-07-17 12:36:35,349][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:36:44,275][257371] train() took 8.92 seconds to complete[0m
[36m[2023-07-17 12:36:44,275][257371] FPS: 430254.02[0m
[36m[2023-07-17 12:36:44,278][257371] itr=1215, itrs=2000, Progress: 60.75%[0m
[36m[2023-07-17 12:36:56,168][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 12:36:56,168][257371] FPS: 326198.78[0m
[36m[2023-07-17 12:37:00,415][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:37:00,415][257371] Reward + Measures: [[506.25055259   0.31838167   0.36846733   0.134894     0.44235033
    4.0893054 ]][0m
[37m[1m[2023-07-17 12:37:00,416][257371] Max Reward on eval: 506.2505525851003[0m
[37m[1m[2023-07-17 12:37:00,416][257371] Min Reward on eval: 506.2505525851003[0m
[37m[1m[2023-07-17 12:37:00,416][257371] Mean Reward across all agents: 506.2505525851003[0m
[37m[1m[2023-07-17 12:37:00,416][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:37:05,169][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:37:05,169][257371] Reward + Measures: [[593.81394624   0.51240003   0.57880002   0.1005       0.61760008
    4.74204206]
 [440.58969576   0.41850001   0.45830002   0.0862       0.4646
    4.42607594]
 [543.37322283   0.4068       0.4571       0.16360001   0.53789991
    4.05544996]
 ...
 [424.43609437   0.35839999   0.34979999   0.1919       0.4492
    3.71257472]
 [469.23975139   0.29910001   0.36100003   0.20200001   0.43210003
    4.0262351 ]
 [316.5182571    0.31019998   0.40790001   0.14500001   0.47389999
    4.29161787]][0m
[37m[1m[2023-07-17 12:37:05,169][257371] Max Reward on eval: 790.1726684446446[0m
[37m[1m[2023-07-17 12:37:05,170][257371] Min Reward on eval: 159.18497710078955[0m
[37m[1m[2023-07-17 12:37:05,170][257371] Mean Reward across all agents: 448.01837303351255[0m
[37m[1m[2023-07-17 12:37:05,170][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:37:05,173][257371] mean_value=-516.0448348582381, max_value=527.8279220900574[0m
[37m[1m[2023-07-17 12:37:05,175][257371] New mean coefficients: [[ 0.7926289  -0.69487876 -0.49334714 -0.8789703   2.1369812   0.55342054]][0m
[37m[1m[2023-07-17 12:37:05,176][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:37:13,274][257371] train() took 8.10 seconds to complete[0m
[36m[2023-07-17 12:37:13,274][257371] FPS: 474286.56[0m
[36m[2023-07-17 12:37:13,277][257371] itr=1216, itrs=2000, Progress: 60.80%[0m
[36m[2023-07-17 12:37:25,300][257371] train() took 11.90 seconds to complete[0m
[36m[2023-07-17 12:37:25,300][257371] FPS: 322726.14[0m
[36m[2023-07-17 12:37:29,611][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:37:29,612][257371] Reward + Measures: [[515.82952126   0.32322568   0.3804453    0.13498734   0.45726597
    4.1500721 ]][0m
[37m[1m[2023-07-17 12:37:29,612][257371] Max Reward on eval: 515.8295212562081[0m
[37m[1m[2023-07-17 12:37:29,612][257371] Min Reward on eval: 515.8295212562081[0m
[37m[1m[2023-07-17 12:37:29,613][257371] Mean Reward across all agents: 515.8295212562081[0m
[37m[1m[2023-07-17 12:37:29,613][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:37:34,933][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:37:34,939][257371] Reward + Measures: [[-49.86030302   0.27770001   0.25890002   0.2375       0.15250002
    5.38747501]
 [408.9632016    0.35980004   0.42390004   0.1436       0.42019996
    4.09760237]
 [429.48435881   0.33070001   0.47269997   0.24660002   0.58920002
    4.32597113]
 ...
 [382.82991598   0.29580003   0.5711       0.34999999   0.61989999
    4.80042362]
 [295.5770003    0.38840002   0.46700001   0.10830001   0.49920002
    4.31803179]
 [315.16602303   0.23309998   0.29180002   0.153        0.35660002
    3.99166489]][0m
[37m[1m[2023-07-17 12:37:34,939][257371] Max Reward on eval: 714.381841425877[0m
[37m[1m[2023-07-17 12:37:34,939][257371] Min Reward on eval: -277.8301209626719[0m
[37m[1m[2023-07-17 12:37:34,939][257371] Mean Reward across all agents: 323.5178320798917[0m
[37m[1m[2023-07-17 12:37:34,940][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:37:34,943][257371] mean_value=-323.83574170491437, max_value=414.341964456339[0m
[37m[1m[2023-07-17 12:37:34,946][257371] New mean coefficients: [[ 0.40820894 -0.99944293 -0.667068   -1.6505098   2.799539   -0.13065588]][0m
[37m[1m[2023-07-17 12:37:34,947][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:37:44,094][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-17 12:37:44,094][257371] FPS: 419887.75[0m
[36m[2023-07-17 12:37:44,097][257371] itr=1217, itrs=2000, Progress: 60.85%[0m
[36m[2023-07-17 12:37:55,951][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 12:37:55,952][257371] FPS: 327101.71[0m
[36m[2023-07-17 12:38:00,265][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:38:00,266][257371] Reward + Measures: [[521.37379892   0.324359     0.38546699   0.13543533   0.4719283
    4.13474751]][0m
[37m[1m[2023-07-17 12:38:00,266][257371] Max Reward on eval: 521.3737989203089[0m
[37m[1m[2023-07-17 12:38:00,266][257371] Min Reward on eval: 521.3737989203089[0m
[37m[1m[2023-07-17 12:38:00,266][257371] Mean Reward across all agents: 521.3737989203089[0m
[37m[1m[2023-07-17 12:38:00,267][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:38:05,185][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:38:05,186][257371] Reward + Measures: [[335.8087363    0.44889998   0.71969998   0.18800001   0.75010008
    5.28044653]
 [193.95138206   0.3804       0.419        0.20010002   0.42680001
    3.6587379 ]
 [317.03517481   0.36320001   0.39309999   0.18520001   0.4183
    3.74992228]
 ...
 [342.70172609   0.2076       0.2784       0.18889999   0.32679999
    3.63145709]
 [693.1863594    0.50089997   0.66580003   0.13         0.72690004
    4.74847555]
 [182.89465571   0.60219997   0.47770005   0.49869999   0.25009999
    6.22434759]][0m
[37m[1m[2023-07-17 12:38:05,186][257371] Max Reward on eval: 693.1863593967631[0m
[37m[1m[2023-07-17 12:38:05,186][257371] Min Reward on eval: -288.8654818163253[0m
[37m[1m[2023-07-17 12:38:05,186][257371] Mean Reward across all agents: 282.9850163740398[0m
[37m[1m[2023-07-17 12:38:05,186][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:38:05,190][257371] mean_value=-785.1475915722061, max_value=284.4883534518035[0m
[37m[1m[2023-07-17 12:38:05,193][257371] New mean coefficients: [[-0.11732963 -1.2336912   0.3271417  -1.2712016   3.2482982  -0.36760175]][0m
[37m[1m[2023-07-17 12:38:05,194][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:38:14,192][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 12:38:14,192][257371] FPS: 426837.10[0m
[36m[2023-07-17 12:38:14,194][257371] itr=1218, itrs=2000, Progress: 60.90%[0m
[36m[2023-07-17 12:38:25,913][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 12:38:25,913][257371] FPS: 331040.82[0m
[36m[2023-07-17 12:38:30,137][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:38:30,142][257371] Reward + Measures: [[525.68227064   0.31955734   0.39838463   0.14078066   0.49038365
    4.12512159]][0m
[37m[1m[2023-07-17 12:38:30,142][257371] Max Reward on eval: 525.682270643059[0m
[37m[1m[2023-07-17 12:38:30,143][257371] Min Reward on eval: 525.682270643059[0m
[37m[1m[2023-07-17 12:38:30,143][257371] Mean Reward across all agents: 525.682270643059[0m
[37m[1m[2023-07-17 12:38:30,143][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:38:35,109][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:38:35,110][257371] Reward + Measures: [[ 485.23246103    0.39109999    0.4368        0.16229999    0.54710001
     4.17337894]
 [-138.63621188    0.62520003    0.63889998    0.57440001    0.1789
     5.84441423]
 [ -14.81299027    0.3732        0.46430001    0.1           0.46739998
     4.81167126]
 ...
 [ 117.75941822    0.1714        0.1714        0.13880001    0.19590001
     3.44035578]
 [ 179.13630015    0.2172        0.2174        0.19389999    0.24589999
     3.44430351]
 [  67.27957137    0.1781        0.1937        0.12820001    0.21250001
     3.43733478]][0m
[37m[1m[2023-07-17 12:38:35,110][257371] Max Reward on eval: 628.0222243849188[0m
[37m[1m[2023-07-17 12:38:35,110][257371] Min Reward on eval: -262.1109883180819[0m
[37m[1m[2023-07-17 12:38:35,111][257371] Mean Reward across all agents: 256.8512960123347[0m
[37m[1m[2023-07-17 12:38:35,111][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:38:35,113][257371] mean_value=-2943.098155554135, max_value=165.5372308337341[0m
[37m[1m[2023-07-17 12:38:35,115][257371] New mean coefficients: [[ 0.51658607 -0.6890068   0.7105278  -1.0876825   1.0053592   0.75118995]][0m
[37m[1m[2023-07-17 12:38:35,116][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:38:44,125][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 12:38:44,125][257371] FPS: 426305.66[0m
[36m[2023-07-17 12:38:44,127][257371] itr=1219, itrs=2000, Progress: 60.95%[0m
[36m[2023-07-17 12:38:55,890][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 12:38:55,891][257371] FPS: 329589.28[0m
[36m[2023-07-17 12:39:00,161][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:39:00,161][257371] Reward + Measures: [[519.97380489   0.31841066   0.42606169   0.144592     0.51753402
    4.24547005]][0m
[37m[1m[2023-07-17 12:39:00,162][257371] Max Reward on eval: 519.973804889155[0m
[37m[1m[2023-07-17 12:39:00,162][257371] Min Reward on eval: 519.973804889155[0m
[37m[1m[2023-07-17 12:39:00,162][257371] Mean Reward across all agents: 519.973804889155[0m
[37m[1m[2023-07-17 12:39:00,162][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:39:05,149][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:39:05,150][257371] Reward + Measures: [[397.88649963   0.31280002   0.59610003   0.16410001   0.62420005
    4.6942234 ]
 [343.23866561   0.2516       0.2599       0.139        0.32500002
    3.81294227]
 [447.13695827   0.40389997   0.49790001   0.09480001   0.49219999
    4.68569708]
 ...
 [484.02709071   0.29389998   0.32409999   0.15350001   0.41319999
    3.96585536]
 [499.19690243   0.47779998   0.56240004   0.1058       0.62470001
    4.48699236]
 [362.80482672   0.30060002   0.33720002   0.1277       0.36450002
    4.09215975]][0m
[37m[1m[2023-07-17 12:39:05,150][257371] Max Reward on eval: 705.3576562740375[0m
[37m[1m[2023-07-17 12:39:05,150][257371] Min Reward on eval: 179.92192567037418[0m
[37m[1m[2023-07-17 12:39:05,150][257371] Mean Reward across all agents: 467.7610948113196[0m
[37m[1m[2023-07-17 12:39:05,151][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:39:05,155][257371] mean_value=-284.06214699312966, max_value=354.9731753211531[0m
[37m[1m[2023-07-17 12:39:05,158][257371] New mean coefficients: [[ 0.13010055 -0.7484416   1.2379416  -1.7476273   0.8346341   1.9163414 ]][0m
[37m[1m[2023-07-17 12:39:05,159][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:39:14,183][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 12:39:14,183][257371] FPS: 425585.53[0m
[36m[2023-07-17 12:39:14,185][257371] itr=1220, itrs=2000, Progress: 61.00%[0m
[37m[1m[2023-07-17 12:42:46,119][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001200[0m
[36m[2023-07-17 12:42:58,408][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 12:42:58,408][257371] FPS: 327497.30[0m
[36m[2023-07-17 12:43:02,663][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:43:02,664][257371] Reward + Measures: [[494.68885616   0.31231233   0.45867765   0.14887133   0.54129231
    4.39749908]][0m
[37m[1m[2023-07-17 12:43:02,664][257371] Max Reward on eval: 494.6888561563555[0m
[37m[1m[2023-07-17 12:43:02,664][257371] Min Reward on eval: 494.6888561563555[0m
[37m[1m[2023-07-17 12:43:02,665][257371] Mean Reward across all agents: 494.6888561563555[0m
[37m[1m[2023-07-17 12:43:02,665][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:43:07,843][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:43:07,844][257371] Reward + Measures: [[378.58010481   0.2933       0.58980006   0.1912       0.63210005
    4.97826433]
 [ 93.80200624   0.31099999   0.64239997   0.19240001   0.66339999
    5.15739584]
 [186.60592937   0.3267       0.6638       0.2023       0.64990002
    5.45162296]
 ...
 [584.40341943   0.43889999   0.61919999   0.1178       0.62939996
    5.11507797]
 [406.19557569   0.27520001   0.59319997   0.19039999   0.66620004
    4.94173765]
 [328.08741331   0.36770001   0.73019999   0.17300002   0.72719997
    5.32592201]][0m
[37m[1m[2023-07-17 12:43:07,845][257371] Max Reward on eval: 724.6693840054795[0m
[37m[1m[2023-07-17 12:43:07,845][257371] Min Reward on eval: -305.3680519973859[0m
[37m[1m[2023-07-17 12:43:07,845][257371] Mean Reward across all agents: 381.129146773832[0m
[37m[1m[2023-07-17 12:43:07,845][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:43:07,849][257371] mean_value=-184.12434729926272, max_value=301.2871861489567[0m
[37m[1m[2023-07-17 12:43:07,852][257371] New mean coefficients: [[ 0.32132912 -1.0164764   0.95954597 -2.262482    0.7385333   2.2739735 ]][0m
[37m[1m[2023-07-17 12:43:07,852][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:43:16,768][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-17 12:43:16,768][257371] FPS: 430813.66[0m
[36m[2023-07-17 12:43:16,770][257371] itr=1221, itrs=2000, Progress: 61.05%[0m
[36m[2023-07-17 12:43:28,412][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 12:43:28,412][257371] FPS: 333049.83[0m
[36m[2023-07-17 12:43:32,634][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:43:32,635][257371] Reward + Measures: [[470.10805616   0.29627034   0.52113563   0.16369233   0.59346771
    4.60796261]][0m
[37m[1m[2023-07-17 12:43:32,635][257371] Max Reward on eval: 470.1080561638092[0m
[37m[1m[2023-07-17 12:43:32,635][257371] Min Reward on eval: 470.1080561638092[0m
[37m[1m[2023-07-17 12:43:32,636][257371] Mean Reward across all agents: 470.1080561638092[0m
[37m[1m[2023-07-17 12:43:32,636][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:43:37,621][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:43:37,621][257371] Reward + Measures: [[264.51775781   0.36200002   0.77410001   0.21900001   0.79810005
    5.25967455]
 [243.33600036   0.30149999   0.37930003   0.1305       0.37020001
    4.33512211]
 [257.33796634   0.2447       0.30990002   0.13170001   0.40920001
    4.11938572]
 ...
 [341.88300263   0.2545       0.34259999   0.1063       0.3696
    4.25824499]
 [336.47921278   0.20899999   0.29749998   0.1596       0.45030004
    4.23625326]
 [522.36320159   0.31650004   0.41279998   0.1435       0.51720005
    4.43961143]][0m
[37m[1m[2023-07-17 12:43:37,622][257371] Max Reward on eval: 691.2202453253325[0m
[37m[1m[2023-07-17 12:43:37,622][257371] Min Reward on eval: 81.88056928439183[0m
[37m[1m[2023-07-17 12:43:37,622][257371] Mean Reward across all agents: 385.48249373379275[0m
[37m[1m[2023-07-17 12:43:37,622][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:43:37,626][257371] mean_value=-196.8006014561884, max_value=161.99001692320815[0m
[37m[1m[2023-07-17 12:43:37,628][257371] New mean coefficients: [[ 0.6401359  -0.6897025   1.3923315  -1.6036031  -0.47678608  3.980142  ]][0m
[37m[1m[2023-07-17 12:43:37,630][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:43:46,588][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 12:43:46,589][257371] FPS: 428703.64[0m
[36m[2023-07-17 12:43:46,591][257371] itr=1222, itrs=2000, Progress: 61.10%[0m
[36m[2023-07-17 12:43:58,456][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 12:43:58,456][257371] FPS: 326769.95[0m
[36m[2023-07-17 12:44:02,683][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:44:02,683][257371] Reward + Measures: [[432.40519378   0.27970099   0.57771164   0.17729132   0.62966567
    4.80691719]][0m
[37m[1m[2023-07-17 12:44:02,684][257371] Max Reward on eval: 432.4051937841545[0m
[37m[1m[2023-07-17 12:44:02,684][257371] Min Reward on eval: 432.4051937841545[0m
[37m[1m[2023-07-17 12:44:02,684][257371] Mean Reward across all agents: 432.4051937841545[0m
[37m[1m[2023-07-17 12:44:02,685][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:44:07,621][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:44:07,621][257371] Reward + Measures: [[296.44873059   0.27800003   0.27360001   0.16119999   0.31779999
    4.1671443 ]
 [619.07898744   0.46870002   0.75639993   0.13340001   0.73610002
    5.26222372]
 [332.4911128    0.21370001   0.68180001   0.25049999   0.75209999
    5.18151808]
 ...
 [200.30421175   0.20510001   0.1904       0.17709999   0.22919999
    4.00426245]
 [448.70189051   0.1605       0.80680001   0.34470004   0.83600008
    5.84693861]
 [348.76904215   0.29730001   0.63020003   0.24070001   0.65449995
    5.15815687]][0m
[37m[1m[2023-07-17 12:44:07,621][257371] Max Reward on eval: 726.5793533901102[0m
[37m[1m[2023-07-17 12:44:07,622][257371] Min Reward on eval: 14.490342743322254[0m
[37m[1m[2023-07-17 12:44:07,622][257371] Mean Reward across all agents: 365.09104997637445[0m
[37m[1m[2023-07-17 12:44:07,622][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:44:07,626][257371] mean_value=-466.5762553152613, max_value=285.6710616407986[0m
[37m[1m[2023-07-17 12:44:07,629][257371] New mean coefficients: [[ 1.4737955  -0.57238996  1.3623557  -1.6592338  -2.1291635   4.190731  ]][0m
[37m[1m[2023-07-17 12:44:07,630][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:44:16,597][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 12:44:16,597][257371] FPS: 428328.82[0m
[36m[2023-07-17 12:44:16,599][257371] itr=1223, itrs=2000, Progress: 61.15%[0m
[36m[2023-07-17 12:44:28,294][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 12:44:28,294][257371] FPS: 331521.96[0m
[36m[2023-07-17 12:44:32,464][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:44:32,464][257371] Reward + Measures: [[393.16921194   0.27614233   0.6340903    0.17993098   0.6738773
    5.01354313]][0m
[37m[1m[2023-07-17 12:44:32,465][257371] Max Reward on eval: 393.1692119437947[0m
[37m[1m[2023-07-17 12:44:32,465][257371] Min Reward on eval: 393.1692119437947[0m
[37m[1m[2023-07-17 12:44:32,465][257371] Mean Reward across all agents: 393.1692119437947[0m
[37m[1m[2023-07-17 12:44:32,465][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:44:37,364][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:44:37,365][257371] Reward + Measures: [[ 328.10931163    0.33130002    0.31349999    0.25040004    0.37549999
     3.36633754]
 [-106.73689891    0.56199998    0.58269995    0.42919999    0.23140001
     5.99500036]
 [ 262.07976442    0.41220003    0.39739999    0.352         0.39130002
     3.96468425]
 ...
 [ -75.50104985    0.47420001    0.37100002    0.41859999    0.41440001
     6.14276743]
 [-125.66024403    0.44210005    0.40500003    0.3973        0.46820003
     6.05888462]
 [ 228.67930607    0.39639997    0.40970001    0.3448        0.39559999
     3.92235255]][0m
[37m[1m[2023-07-17 12:44:37,365][257371] Max Reward on eval: 694.8638516891282[0m
[37m[1m[2023-07-17 12:44:37,365][257371] Min Reward on eval: -198.4417076275684[0m
[37m[1m[2023-07-17 12:44:37,365][257371] Mean Reward across all agents: 226.702458055928[0m
[37m[1m[2023-07-17 12:44:37,366][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:44:37,368][257371] mean_value=-829.2998242418244, max_value=180.85986190713933[0m
[37m[1m[2023-07-17 12:44:37,371][257371] New mean coefficients: [[ 0.78049207  0.0826894   1.0495843  -1.580898   -1.6106213   3.7423244 ]][0m
[37m[1m[2023-07-17 12:44:37,372][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:44:46,326][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 12:44:46,326][257371] FPS: 428940.86[0m
[36m[2023-07-17 12:44:46,329][257371] itr=1224, itrs=2000, Progress: 61.20%[0m
[36m[2023-07-17 12:44:58,052][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 12:44:58,052][257371] FPS: 330778.97[0m
[36m[2023-07-17 12:45:02,426][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:45:02,426][257371] Reward + Measures: [[375.70906051   0.28687933   0.68102533   0.18127733   0.70843303
    5.20857143]][0m
[37m[1m[2023-07-17 12:45:02,426][257371] Max Reward on eval: 375.70906050876266[0m
[37m[1m[2023-07-17 12:45:02,427][257371] Min Reward on eval: 375.70906050876266[0m
[37m[1m[2023-07-17 12:45:02,427][257371] Mean Reward across all agents: 375.70906050876266[0m
[37m[1m[2023-07-17 12:45:02,427][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:45:07,472][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:45:07,473][257371] Reward + Measures: [[282.39953037   0.41630003   0.65740001   0.1153       0.6232
    5.12029409]
 [433.68327751   0.32020003   0.65540004   0.2211       0.69570005
    4.94464827]
 [260.02007837   0.22629999   0.3558       0.1071       0.36279997
    4.42754889]
 ...
 [247.95312628   0.23720001   0.72660005   0.25570002   0.78399998
    5.40138769]
 [293.66862056   0.28029999   0.47349998   0.17399999   0.48429999
    4.61032438]
 [394.76524969   0.2198       0.60650003   0.27110001   0.64950001
    4.72943068]][0m
[37m[1m[2023-07-17 12:45:07,473][257371] Max Reward on eval: 733.1429786864435[0m
[37m[1m[2023-07-17 12:45:07,473][257371] Min Reward on eval: -9.6021642586682[0m
[37m[1m[2023-07-17 12:45:07,474][257371] Mean Reward across all agents: 388.15111342148305[0m
[37m[1m[2023-07-17 12:45:07,474][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:45:07,477][257371] mean_value=-128.63811826065358, max_value=189.46053469315433[0m
[37m[1m[2023-07-17 12:45:07,479][257371] New mean coefficients: [[ 1.3161565  -0.18102401  0.9474104  -1.5481292  -2.0469956   2.437696  ]][0m
[37m[1m[2023-07-17 12:45:07,480][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:45:16,502][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 12:45:16,502][257371] FPS: 425738.41[0m
[36m[2023-07-17 12:45:16,504][257371] itr=1225, itrs=2000, Progress: 61.25%[0m
[36m[2023-07-17 12:45:28,202][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 12:45:28,202][257371] FPS: 331485.71[0m
[36m[2023-07-17 12:45:32,488][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:45:32,489][257371] Reward + Measures: [[292.68975264   0.33546764   0.73564363   0.17549235   0.75151759
    5.49026823]][0m
[37m[1m[2023-07-17 12:45:32,489][257371] Max Reward on eval: 292.68975263653783[0m
[37m[1m[2023-07-17 12:45:32,489][257371] Min Reward on eval: 292.68975263653783[0m
[37m[1m[2023-07-17 12:45:32,490][257371] Mean Reward across all agents: 292.68975263653783[0m
[37m[1m[2023-07-17 12:45:32,490][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:45:37,459][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:45:37,460][257371] Reward + Measures: [[321.38426051   0.31380004   0.66819996   0.1592       0.7349
    5.16588688]
 [323.32917275   0.36490002   0.79680002   0.1814       0.8132
    5.53344774]
 [168.43045073   0.25800002   0.62889999   0.21209998   0.65749997
    5.36647224]
 ...
 [238.12979006   0.3238       0.66000003   0.22710001   0.65940005
    5.24860477]
 [260.65203224   0.17390001   0.51950002   0.16610001   0.53660005
    5.1866107 ]
 [277.33812799   0.19700001   0.5359       0.20560001   0.59330004
    4.87036562]][0m
[37m[1m[2023-07-17 12:45:37,460][257371] Max Reward on eval: 650.2259266600479[0m
[37m[1m[2023-07-17 12:45:37,460][257371] Min Reward on eval: -150.0731669629924[0m
[37m[1m[2023-07-17 12:45:37,460][257371] Mean Reward across all agents: 342.016331432101[0m
[37m[1m[2023-07-17 12:45:37,461][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:45:37,464][257371] mean_value=-153.37889729670098, max_value=266.39653389254727[0m
[37m[1m[2023-07-17 12:45:37,467][257371] New mean coefficients: [[ 1.4441152  0.8928718  1.1151493 -1.1680822 -2.457263   3.0359688]][0m
[37m[1m[2023-07-17 12:45:37,467][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:45:46,449][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 12:45:46,449][257371] FPS: 427619.59[0m
[36m[2023-07-17 12:45:46,451][257371] itr=1226, itrs=2000, Progress: 61.30%[0m
[36m[2023-07-17 12:45:58,459][257371] train() took 11.89 seconds to complete[0m
[36m[2023-07-17 12:45:58,459][257371] FPS: 322854.20[0m
[36m[2023-07-17 12:46:02,838][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:46:02,838][257371] Reward + Measures: [[303.08619077   0.33653167   0.73799366   0.17916068   0.7493906
    5.54696846]][0m
[37m[1m[2023-07-17 12:46:02,838][257371] Max Reward on eval: 303.0861907733682[0m
[37m[1m[2023-07-17 12:46:02,839][257371] Min Reward on eval: 303.0861907733682[0m
[37m[1m[2023-07-17 12:46:02,839][257371] Mean Reward across all agents: 303.0861907733682[0m
[37m[1m[2023-07-17 12:46:02,839][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:46:08,176][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:46:08,177][257371] Reward + Measures: [[398.47209265   0.25689998   0.82170004   0.31420001   0.85849994
    5.56582212]
 [451.80306699   0.33669999   0.6997       0.1806       0.72209996
    4.93133163]
 [379.8386371    0.2922       0.59110004   0.14650001   0.58960003
    4.91480541]
 ...
 [644.32046217   0.54360002   0.72640002   0.09850001   0.71470004
    5.10769176]
 [134.80024771   0.26360002   0.41319999   0.0968       0.43099996
    4.86800146]
 [381.45914296   0.29960001   0.5521       0.1857       0.62440002
    4.83502722]][0m
[37m[1m[2023-07-17 12:46:08,177][257371] Max Reward on eval: 788.9498863268643[0m
[37m[1m[2023-07-17 12:46:08,177][257371] Min Reward on eval: -345.03276157896033[0m
[37m[1m[2023-07-17 12:46:08,178][257371] Mean Reward across all agents: 345.7902402992325[0m
[37m[1m[2023-07-17 12:46:08,178][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:46:08,181][257371] mean_value=-199.975596116731, max_value=277.37447819285364[0m
[37m[1m[2023-07-17 12:46:08,183][257371] New mean coefficients: [[ 0.32937098  1.1607931   1.2967819  -0.392959   -2.2864566   3.0931718 ]][0m
[37m[1m[2023-07-17 12:46:08,184][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:46:17,220][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 12:46:17,220][257371] FPS: 425052.57[0m
[36m[2023-07-17 12:46:17,223][257371] itr=1227, itrs=2000, Progress: 61.35%[0m
[36m[2023-07-17 12:46:29,001][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 12:46:29,001][257371] FPS: 329192.45[0m
[36m[2023-07-17 12:46:33,186][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:46:33,186][257371] Reward + Measures: [[289.94406521   0.33663633   0.74973702   0.19464168   0.76047331
    5.61661577]][0m
[37m[1m[2023-07-17 12:46:33,187][257371] Max Reward on eval: 289.94406521475196[0m
[37m[1m[2023-07-17 12:46:33,187][257371] Min Reward on eval: 289.94406521475196[0m
[37m[1m[2023-07-17 12:46:33,187][257371] Mean Reward across all agents: 289.94406521475196[0m
[37m[1m[2023-07-17 12:46:33,187][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:46:38,131][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:46:38,132][257371] Reward + Measures: [[ 22.56525846   0.28730002   0.60530001   0.1811       0.64649999
    5.16710424]
 [277.73281483   0.4312       0.72780007   0.1199       0.73990005
    5.28462076]
 [123.98168061   0.32170001   0.70319998   0.28389999   0.66550004
    5.10405302]
 ...
 [193.9690572    0.31740001   0.55859995   0.17419998   0.54380006
    4.94258642]
 [327.53184233   0.51159996   0.74080002   0.1089       0.73119998
    5.3898139 ]
 [109.59812407   0.62379998   0.81189996   0.3398       0.5934
    5.44185162]][0m
[37m[1m[2023-07-17 12:46:38,132][257371] Max Reward on eval: 635.4753399185836[0m
[37m[1m[2023-07-17 12:46:38,132][257371] Min Reward on eval: -217.41981170764194[0m
[37m[1m[2023-07-17 12:46:38,133][257371] Mean Reward across all agents: 234.5411120209599[0m
[37m[1m[2023-07-17 12:46:38,133][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:46:38,135][257371] mean_value=-326.25802930308174, max_value=162.94758805454495[0m
[37m[1m[2023-07-17 12:46:38,138][257371] New mean coefficients: [[ 0.11579284  1.4821616   1.3046856  -0.934033   -1.737669    2.0126634 ]][0m
[37m[1m[2023-07-17 12:46:38,139][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:46:47,126][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 12:46:47,126][257371] FPS: 427371.18[0m
[36m[2023-07-17 12:46:47,128][257371] itr=1228, itrs=2000, Progress: 61.40%[0m
[36m[2023-07-17 12:46:58,920][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 12:46:58,920][257371] FPS: 328793.07[0m
[36m[2023-07-17 12:47:03,192][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:47:03,193][257371] Reward + Measures: [[312.97486741   0.33342266   0.76612163   0.22996667   0.77408558
    5.78236532]][0m
[37m[1m[2023-07-17 12:47:03,193][257371] Max Reward on eval: 312.9748674083451[0m
[37m[1m[2023-07-17 12:47:03,193][257371] Min Reward on eval: 312.9748674083451[0m
[37m[1m[2023-07-17 12:47:03,193][257371] Mean Reward across all agents: 312.9748674083451[0m
[37m[1m[2023-07-17 12:47:03,194][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:47:08,171][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:47:08,171][257371] Reward + Measures: [[396.97091096   0.1622       0.69550002   0.3484       0.69620001
    5.46321726]
 [242.91371936   0.27090001   0.75229996   0.29930001   0.75450003
    5.71174335]
 [278.2988506    0.25689998   0.64309996   0.17230001   0.65060002
    5.52146482]
 ...
 [156.54098805   0.3427       0.79439998   0.17480002   0.82559997
    5.77744341]
 [333.81479652   0.28960001   0.75730002   0.29389998   0.74980003
    5.66355753]
 [366.35056969   0.38390002   0.73980004   0.21200001   0.74399996
    5.71269608]][0m
[37m[1m[2023-07-17 12:47:08,171][257371] Max Reward on eval: 611.2700838847086[0m
[37m[1m[2023-07-17 12:47:08,172][257371] Min Reward on eval: -121.13473272835836[0m
[37m[1m[2023-07-17 12:47:08,172][257371] Mean Reward across all agents: 306.94817012075094[0m
[37m[1m[2023-07-17 12:47:08,172][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:47:08,175][257371] mean_value=-183.27162078041053, max_value=340.8395396568491[0m
[37m[1m[2023-07-17 12:47:08,178][257371] New mean coefficients: [[ 0.19648072  0.46729624  1.72732    -1.2894568  -1.7356017   1.804166  ]][0m
[37m[1m[2023-07-17 12:47:08,179][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:47:17,204][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 12:47:17,204][257371] FPS: 425558.09[0m
[36m[2023-07-17 12:47:17,206][257371] itr=1229, itrs=2000, Progress: 61.45%[0m
[36m[2023-07-17 12:47:28,860][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 12:47:28,861][257371] FPS: 332740.87[0m
[36m[2023-07-17 12:47:33,155][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:47:33,155][257371] Reward + Measures: [[347.80376514   0.326141     0.78557891   0.25837734   0.78873533
    5.94125986]][0m
[37m[1m[2023-07-17 12:47:33,156][257371] Max Reward on eval: 347.8037651386602[0m
[37m[1m[2023-07-17 12:47:33,156][257371] Min Reward on eval: 347.8037651386602[0m
[37m[1m[2023-07-17 12:47:33,156][257371] Mean Reward across all agents: 347.8037651386602[0m
[37m[1m[2023-07-17 12:47:33,156][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:47:38,163][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:47:38,168][257371] Reward + Measures: [[ 46.01000506   0.24520002   0.37580001   0.14670001   0.40360004
    4.4311738 ]
 [150.3676019    0.5395       0.60799998   0.47490001   0.62970001
    4.65784168]
 [243.18754975   0.23360001   0.64039999   0.33440003   0.59720004
    5.65416241]
 ...
 [236.11179309   0.49420005   0.51770002   0.1964       0.48300001
    4.49878693]
 [214.63194363   0.44619998   0.49400002   0.33320001   0.52710003
    4.39857817]
 [525.71014139   0.34239998   0.75160003   0.273        0.75060004
    5.60603476]][0m
[37m[1m[2023-07-17 12:47:38,169][257371] Max Reward on eval: 686.9011239845771[0m
[37m[1m[2023-07-17 12:47:38,169][257371] Min Reward on eval: -216.47059699466917[0m
[37m[1m[2023-07-17 12:47:38,169][257371] Mean Reward across all agents: 213.95596701092083[0m
[37m[1m[2023-07-17 12:47:38,169][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:47:38,173][257371] mean_value=-552.1877752937506, max_value=282.6922672057695[0m
[37m[1m[2023-07-17 12:47:38,176][257371] New mean coefficients: [[ 0.15642725 -0.02893567  2.017774    0.01589155 -1.5952312   2.2136586 ]][0m
[37m[1m[2023-07-17 12:47:38,177][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:47:47,167][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 12:47:47,167][257371] FPS: 427221.37[0m
[36m[2023-07-17 12:47:47,170][257371] itr=1230, itrs=2000, Progress: 61.50%[0m
[37m[1m[2023-07-17 12:51:37,976][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001210[0m
[36m[2023-07-17 12:51:50,400][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 12:51:50,400][257371] FPS: 327849.34[0m
[36m[2023-07-17 12:51:54,627][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:51:54,628][257371] Reward + Measures: [[372.76068808   0.31188366   0.78191561   0.27207533   0.78291965
    5.98260212]][0m
[37m[1m[2023-07-17 12:51:54,628][257371] Max Reward on eval: 372.76068808307605[0m
[37m[1m[2023-07-17 12:51:54,628][257371] Min Reward on eval: 372.76068808307605[0m
[37m[1m[2023-07-17 12:51:54,628][257371] Mean Reward across all agents: 372.76068808307605[0m
[37m[1m[2023-07-17 12:51:54,629][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:51:59,876][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:51:59,877][257371] Reward + Measures: [[514.70305831   0.55400002   0.9217       0.17389999   0.89370006
    6.10580587]
 [357.82691015   0.4501       0.76090002   0.14479999   0.71280003
    5.75445414]
 [401.63730146   0.29299998   0.87109995   0.23559999   0.89229995
    5.78443623]
 ...
 [365.93385242   0.34540001   0.75550002   0.32780001   0.6893
    5.73476934]
 [548.09594249   0.45430002   0.91949999   0.1663       0.90140003
    6.25075674]
 [296.68856544   0.39250001   0.83939999   0.24890001   0.83840007
    5.93610859]][0m
[37m[1m[2023-07-17 12:51:59,877][257371] Max Reward on eval: 751.8813668392598[0m
[37m[1m[2023-07-17 12:51:59,877][257371] Min Reward on eval: -42.182526533456986[0m
[37m[1m[2023-07-17 12:51:59,878][257371] Mean Reward across all agents: 361.1193769666014[0m
[37m[1m[2023-07-17 12:51:59,878][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:51:59,881][257371] mean_value=-131.45825232002488, max_value=313.5987293463742[0m
[37m[1m[2023-07-17 12:51:59,884][257371] New mean coefficients: [[ 0.19573422  0.7188153   2.207896    0.25192362 -1.9245992   0.9685637 ]][0m
[37m[1m[2023-07-17 12:51:59,885][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:52:08,911][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 12:52:08,911][257371] FPS: 425496.24[0m
[36m[2023-07-17 12:52:08,913][257371] itr=1231, itrs=2000, Progress: 61.55%[0m
[36m[2023-07-17 12:52:20,588][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 12:52:20,589][257371] FPS: 332252.77[0m
[36m[2023-07-17 12:52:24,775][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:52:24,776][257371] Reward + Measures: [[387.30199856   0.321224     0.78608733   0.27418298   0.78492707
    5.99403238]][0m
[37m[1m[2023-07-17 12:52:24,776][257371] Max Reward on eval: 387.3019985555131[0m
[37m[1m[2023-07-17 12:52:24,776][257371] Min Reward on eval: 387.3019985555131[0m
[37m[1m[2023-07-17 12:52:24,776][257371] Mean Reward across all agents: 387.3019985555131[0m
[37m[1m[2023-07-17 12:52:24,777][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:52:29,617][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:52:29,618][257371] Reward + Measures: [[183.55340149   0.23529999   0.64650005   0.31920001   0.59930003
    5.2113595 ]
 [355.81098439   0.3436       0.79459995   0.278        0.79570001
    5.74350595]
 [272.84898976   0.4973       0.76220006   0.15120001   0.73369998
    5.65141296]
 ...
 [142.19634354   0.29340002   0.49499997   0.42969999   0.35049999
    4.47284317]
 [468.87158097   0.40689999   0.81560004   0.2234       0.80159998
    5.7395587 ]
 [254.43116283   0.31690001   0.49720001   0.47960001   0.25170001
    4.53057623]][0m
[37m[1m[2023-07-17 12:52:29,618][257371] Max Reward on eval: 734.7547330789035[0m
[37m[1m[2023-07-17 12:52:29,618][257371] Min Reward on eval: -140.61328912666067[0m
[37m[1m[2023-07-17 12:52:29,618][257371] Mean Reward across all agents: 252.87393782711536[0m
[37m[1m[2023-07-17 12:52:29,619][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:52:29,623][257371] mean_value=-257.7267267505393, max_value=259.89071301922047[0m
[37m[1m[2023-07-17 12:52:29,625][257371] New mean coefficients: [[-0.06484775  0.1507147   2.5840976  -0.55407596 -1.3037287   1.2401335 ]][0m
[37m[1m[2023-07-17 12:52:29,626][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:52:38,624][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 12:52:38,625][257371] FPS: 426840.85[0m
[36m[2023-07-17 12:52:38,627][257371] itr=1232, itrs=2000, Progress: 61.60%[0m
[36m[2023-07-17 12:52:50,260][257371] train() took 11.52 seconds to complete[0m
[36m[2023-07-17 12:52:50,260][257371] FPS: 333456.84[0m
[36m[2023-07-17 12:52:54,588][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:52:54,588][257371] Reward + Measures: [[372.62900531   0.32267731   0.78625631   0.274598     0.78499526
    6.04560995]][0m
[37m[1m[2023-07-17 12:52:54,589][257371] Max Reward on eval: 372.6290053085179[0m
[37m[1m[2023-07-17 12:52:54,589][257371] Min Reward on eval: 372.6290053085179[0m
[37m[1m[2023-07-17 12:52:54,589][257371] Mean Reward across all agents: 372.6290053085179[0m
[37m[1m[2023-07-17 12:52:54,589][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:52:59,512][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:52:59,518][257371] Reward + Measures: [[357.65253781   0.11850001   0.79460001   0.30930001   0.8179
    5.62298346]
 [301.42631147   0.2174       0.79610002   0.25220004   0.81040001
    5.5889225 ]
 [538.22240356   0.23909998   0.90920001   0.32190001   0.91930002
    6.18131781]
 ...
 [302.87047193   0.23889999   0.70099992   0.22580002   0.75040007
    5.33115005]
 [233.67647454   0.24380003   0.5302       0.171        0.54949999
    4.64174366]
 [102.47437601   0.228        0.47419998   0.15320002   0.48600003
    4.88275385]][0m
[37m[1m[2023-07-17 12:52:59,518][257371] Max Reward on eval: 655.0945864213165[0m
[37m[1m[2023-07-17 12:52:59,518][257371] Min Reward on eval: 30.89853760241531[0m
[37m[1m[2023-07-17 12:52:59,519][257371] Mean Reward across all agents: 343.05606678329445[0m
[37m[1m[2023-07-17 12:52:59,519][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:52:59,522][257371] mean_value=-183.26693421599188, max_value=210.90396072783187[0m
[37m[1m[2023-07-17 12:52:59,524][257371] New mean coefficients: [[ 0.01554194 -1.3735228   2.224709   -0.60401714 -0.30565238  0.2046268 ]][0m
[37m[1m[2023-07-17 12:52:59,525][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:53:08,565][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 12:53:08,566][257371] FPS: 424840.18[0m
[36m[2023-07-17 12:53:08,568][257371] itr=1233, itrs=2000, Progress: 61.65%[0m
[36m[2023-07-17 12:53:20,219][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 12:53:20,219][257371] FPS: 332966.20[0m
[36m[2023-07-17 12:53:24,484][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:53:24,484][257371] Reward + Measures: [[381.99507172   0.31132066   0.79331964   0.29192135   0.79252863
    6.13252354]][0m
[37m[1m[2023-07-17 12:53:24,484][257371] Max Reward on eval: 381.99507172240214[0m
[37m[1m[2023-07-17 12:53:24,485][257371] Min Reward on eval: 381.99507172240214[0m
[37m[1m[2023-07-17 12:53:24,485][257371] Mean Reward across all agents: 381.99507172240214[0m
[37m[1m[2023-07-17 12:53:24,485][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:53:29,541][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:53:29,541][257371] Reward + Measures: [[118.53045738   0.38320002   0.25540003   0.368        0.3716
    4.47081327]
 [ 93.17253566   0.21159999   0.21170001   0.24889998   0.22020002
    4.3132453 ]
 [337.38177153   0.58929998   0.5772       0.56120002   0.1883
    4.4364481 ]
 ...
 [ 22.81225156   0.1707       0.1495       0.2034       0.20250002
    5.01739359]
 [193.92563008   0.24130002   0.35789999   0.123        0.40570003
    4.05323362]
 [ 79.45840987   0.28779998   0.24170001   0.2987       0.28840002
    4.6041503 ]][0m
[37m[1m[2023-07-17 12:53:29,542][257371] Max Reward on eval: 678.6673403156107[0m
[37m[1m[2023-07-17 12:53:29,542][257371] Min Reward on eval: -170.14379049269482[0m
[37m[1m[2023-07-17 12:53:29,542][257371] Mean Reward across all agents: 199.26531969497987[0m
[37m[1m[2023-07-17 12:53:29,542][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:53:29,546][257371] mean_value=-764.1296574331708, max_value=636.2081189884692[0m
[37m[1m[2023-07-17 12:53:29,549][257371] New mean coefficients: [[-0.5145148 -0.1930368  2.0255022 -0.8297236 -0.5658167  0.5986166]][0m
[37m[1m[2023-07-17 12:53:29,550][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:53:38,633][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 12:53:38,633][257371] FPS: 422848.80[0m
[36m[2023-07-17 12:53:38,635][257371] itr=1234, itrs=2000, Progress: 61.70%[0m
[36m[2023-07-17 12:53:50,562][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 12:53:50,562][257371] FPS: 325193.22[0m
[36m[2023-07-17 12:53:54,906][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:53:54,907][257371] Reward + Measures: [[377.0819093    0.30811533   0.79678369   0.29858398   0.79493934
    6.18797827]][0m
[37m[1m[2023-07-17 12:53:54,907][257371] Max Reward on eval: 377.0819092986595[0m
[37m[1m[2023-07-17 12:53:54,907][257371] Min Reward on eval: 377.0819092986595[0m
[37m[1m[2023-07-17 12:53:54,908][257371] Mean Reward across all agents: 377.0819092986595[0m
[37m[1m[2023-07-17 12:53:54,908][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:53:59,910][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:53:59,910][257371] Reward + Measures: [[ 398.40831853    0.18870001    0.75800002    0.41360003    0.76429999
     5.7191267 ]
 [-203.07616117    0.63260001    0.75880003    0.0786        0.73470002
     5.58405828]
 [ 409.44832224    0.52389997    0.61009997    0.11259999    0.66500008
     3.87856269]
 ...
 [ 437.81589766    0.28639999    0.7596001     0.33730003    0.74240005
     6.01374626]
 [ 157.54618792    0.3017        0.73589998    0.28889999    0.73260003
     5.66779947]
 [ 482.62318685    0.42139998    0.84829998    0.34540004    0.83770001
     6.08638382]][0m
[37m[1m[2023-07-17 12:53:59,910][257371] Max Reward on eval: 665.8063163736836[0m
[37m[1m[2023-07-17 12:53:59,911][257371] Min Reward on eval: -540.0613403605297[0m
[37m[1m[2023-07-17 12:53:59,911][257371] Mean Reward across all agents: 241.59223534910444[0m
[37m[1m[2023-07-17 12:53:59,911][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:53:59,914][257371] mean_value=-402.04045765310195, max_value=100.15489431930939[0m
[37m[1m[2023-07-17 12:53:59,917][257371] New mean coefficients: [[-0.6089825 -1.1114119  1.858745  -0.873066  -0.3722764  0.9277928]][0m
[37m[1m[2023-07-17 12:53:59,918][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:54:08,884][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 12:54:08,884][257371] FPS: 428342.09[0m
[36m[2023-07-17 12:54:08,886][257371] itr=1235, itrs=2000, Progress: 61.75%[0m
[36m[2023-07-17 12:54:20,791][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 12:54:20,792][257371] FPS: 325790.49[0m
[36m[2023-07-17 12:54:25,044][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:54:25,045][257371] Reward + Measures: [[373.89931725   0.30722901   0.80655062   0.27148435   0.80449134
    6.24365568]][0m
[37m[1m[2023-07-17 12:54:25,045][257371] Max Reward on eval: 373.89931725358946[0m
[37m[1m[2023-07-17 12:54:25,045][257371] Min Reward on eval: 373.89931725358946[0m
[37m[1m[2023-07-17 12:54:25,046][257371] Mean Reward across all agents: 373.89931725358946[0m
[37m[1m[2023-07-17 12:54:25,046][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:54:30,022][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:54:30,023][257371] Reward + Measures: [[304.89911964   0.35480002   0.75470001   0.20009999   0.74589998
    6.00279474]
 [449.64891396   0.33430001   0.66259998   0.1578       0.63459998
    5.69434977]
 [434.3281285    0.48400003   0.76310003   0.133        0.75850004
    5.84312201]
 ...
 [353.00038221   0.40780002   0.83630008   0.15449999   0.82550001
    5.91233826]
 [609.10409582   0.44320002   0.92910004   0.32210001   0.89989996
    6.36814976]
 [568.59981157   0.54879999   0.91350001   0.23279999   0.89590007
    6.37669706]][0m
[37m[1m[2023-07-17 12:54:30,023][257371] Max Reward on eval: 690.1806736033875[0m
[37m[1m[2023-07-17 12:54:30,023][257371] Min Reward on eval: 15.682640283554793[0m
[37m[1m[2023-07-17 12:54:30,024][257371] Mean Reward across all agents: 384.8224746391879[0m
[37m[1m[2023-07-17 12:54:30,024][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:54:30,028][257371] mean_value=-116.62309154122013, max_value=603.0608059230607[0m
[37m[1m[2023-07-17 12:54:30,030][257371] New mean coefficients: [[-1.0641515  -0.6276872   1.7319561  -0.2372728   0.12038752  0.9015231 ]][0m
[37m[1m[2023-07-17 12:54:30,031][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:54:38,974][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 12:54:38,974][257371] FPS: 429477.05[0m
[36m[2023-07-17 12:54:38,976][257371] itr=1236, itrs=2000, Progress: 61.80%[0m
[36m[2023-07-17 12:54:50,879][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 12:54:50,879][257371] FPS: 325737.59[0m
[36m[2023-07-17 12:54:55,167][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:54:55,167][257371] Reward + Measures: [[351.1496707    0.30990466   0.82836133   0.27205065   0.82610202
    6.35602808]][0m
[37m[1m[2023-07-17 12:54:55,168][257371] Max Reward on eval: 351.14967070351923[0m
[37m[1m[2023-07-17 12:54:55,168][257371] Min Reward on eval: 351.14967070351923[0m
[37m[1m[2023-07-17 12:54:55,168][257371] Mean Reward across all agents: 351.14967070351923[0m
[37m[1m[2023-07-17 12:54:55,168][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:55:00,410][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:55:00,416][257371] Reward + Measures: [[339.37111518   0.40869999   0.77229995   0.24689999   0.72970003
    5.93310976]
 [277.35081819   0.28500003   0.76789993   0.33919999   0.76730007
    6.1131196 ]
 [260.77865312   0.25129998   0.69440001   0.18200001   0.69780004
    5.34667492]
 ...
 [238.63822282   0.24070001   0.7665       0.3827       0.77249998
    6.42394209]
 [401.14225814   0.2534       0.85550004   0.24770001   0.85979998
    6.33907175]
 [370.81753631   0.31819999   0.85000002   0.35540006   0.83310002
    6.39441252]][0m
[37m[1m[2023-07-17 12:55:00,416][257371] Max Reward on eval: 732.6944209451322[0m
[37m[1m[2023-07-17 12:55:00,416][257371] Min Reward on eval: -156.59958037333564[0m
[37m[1m[2023-07-17 12:55:00,417][257371] Mean Reward across all agents: 317.810466426878[0m
[37m[1m[2023-07-17 12:55:00,417][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:55:00,420][257371] mean_value=-210.75510272217278, max_value=432.38070692453454[0m
[37m[1m[2023-07-17 12:55:00,422][257371] New mean coefficients: [[-0.72957075 -1.5927186   1.1700886  -0.9939501  -0.02759063 -0.25818348]][0m
[37m[1m[2023-07-17 12:55:00,423][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:55:09,440][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 12:55:09,440][257371] FPS: 425960.32[0m
[36m[2023-07-17 12:55:09,442][257371] itr=1237, itrs=2000, Progress: 61.85%[0m
[36m[2023-07-17 12:55:21,094][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 12:55:21,095][257371] FPS: 332878.25[0m
[36m[2023-07-17 12:55:25,402][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:55:25,402][257371] Reward + Measures: [[278.79143384   0.31815967   0.81717205   0.20599633   0.81844139
    6.30578375]][0m
[37m[1m[2023-07-17 12:55:25,402][257371] Max Reward on eval: 278.79143383716115[0m
[37m[1m[2023-07-17 12:55:25,403][257371] Min Reward on eval: 278.79143383716115[0m
[37m[1m[2023-07-17 12:55:25,403][257371] Mean Reward across all agents: 278.79143383716115[0m
[37m[1m[2023-07-17 12:55:25,403][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:55:30,373][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:55:30,374][257371] Reward + Measures: [[129.59668033   0.47200003   0.89729995   0.27869999   0.87620002
    4.78441763]
 [257.51942018   0.25030002   0.77070004   0.19760001   0.77800006
    5.92764997]
 [260.4342431    0.30780002   0.53240001   0.1947       0.574
    4.79823685]
 ...
 [277.81760956   0.54980004   0.92640001   0.15710001   0.93080008
    6.3826766 ]
 [-40.37850353   0.34819999   0.3513       0.2304       0.2017
    4.7507    ]
 [128.61014012   0.30300003   0.51389998   0.31030002   0.44189999
    5.147964  ]][0m
[37m[1m[2023-07-17 12:55:30,374][257371] Max Reward on eval: 751.9553584896028[0m
[37m[1m[2023-07-17 12:55:30,374][257371] Min Reward on eval: -214.21715783663095[0m
[37m[1m[2023-07-17 12:55:30,375][257371] Mean Reward across all agents: 232.35479709030002[0m
[37m[1m[2023-07-17 12:55:30,375][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:55:30,377][257371] mean_value=-284.86262598676365, max_value=132.156933064491[0m
[37m[1m[2023-07-17 12:55:30,380][257371] New mean coefficients: [[-0.33424544 -1.6606013   1.7662941  -0.6789669  -0.22299826 -0.09959041]][0m
[37m[1m[2023-07-17 12:55:30,380][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:55:39,415][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 12:55:39,416][257371] FPS: 425090.79[0m
[36m[2023-07-17 12:55:39,418][257371] itr=1238, itrs=2000, Progress: 61.90%[0m
[36m[2023-07-17 12:55:51,335][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 12:55:51,336][257371] FPS: 325352.81[0m
[36m[2023-07-17 12:55:55,544][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:55:55,549][257371] Reward + Measures: [[275.6859115    0.28657034   0.81392169   0.19827533   0.81573892
    6.26464558]][0m
[37m[1m[2023-07-17 12:55:55,549][257371] Max Reward on eval: 275.6859114989653[0m
[37m[1m[2023-07-17 12:55:55,549][257371] Min Reward on eval: 275.6859114989653[0m
[37m[1m[2023-07-17 12:55:55,550][257371] Mean Reward across all agents: 275.6859114989653[0m
[37m[1m[2023-07-17 12:55:55,550][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:56:00,532][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:56:00,533][257371] Reward + Measures: [[311.47238922   0.24590002   0.61590004   0.16580001   0.63350004
    5.33807611]
 [384.96215618   0.3127       0.69620007   0.1503       0.68970001
    5.46284723]
 [412.02905033   0.3373       0.73540002   0.18550001   0.75589997
    5.25854301]
 ...
 [523.48680163   0.39550003   0.833        0.12310001   0.81829995
    5.86500406]
 [294.68445115   0.1719       0.75760001   0.25260001   0.79479998
    5.63239479]
 [238.13000764   0.1911       0.63639998   0.16470002   0.64490002
    5.65486479]][0m
[37m[1m[2023-07-17 12:56:00,533][257371] Max Reward on eval: 575.372709515295[0m
[37m[1m[2023-07-17 12:56:00,533][257371] Min Reward on eval: -8.458318381104618[0m
[37m[1m[2023-07-17 12:56:00,534][257371] Mean Reward across all agents: 315.9804733326831[0m
[37m[1m[2023-07-17 12:56:00,534][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:56:00,536][257371] mean_value=-206.0181867380857, max_value=291.35806551196816[0m
[37m[1m[2023-07-17 12:56:00,539][257371] New mean coefficients: [[-0.11283514 -2.0717342   1.7846478  -0.88859993 -0.45590037 -0.92316777]][0m
[37m[1m[2023-07-17 12:56:00,540][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:56:09,531][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 12:56:09,531][257371] FPS: 427182.94[0m
[36m[2023-07-17 12:56:09,533][257371] itr=1239, itrs=2000, Progress: 61.95%[0m
[36m[2023-07-17 12:56:21,258][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 12:56:21,258][257371] FPS: 330744.75[0m
[36m[2023-07-17 12:56:25,513][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:56:25,514][257371] Reward + Measures: [[331.34700826   0.259413     0.80955899   0.20040999   0.81336135
    6.15733862]][0m
[37m[1m[2023-07-17 12:56:25,514][257371] Max Reward on eval: 331.3470082567745[0m
[37m[1m[2023-07-17 12:56:25,514][257371] Min Reward on eval: 331.3470082567745[0m
[37m[1m[2023-07-17 12:56:25,515][257371] Mean Reward across all agents: 331.3470082567745[0m
[37m[1m[2023-07-17 12:56:25,515][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:56:30,503][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 12:56:30,503][257371] Reward + Measures: [[311.6682289    0.20539999   0.63550001   0.221        0.64860004
    5.54352808]
 [327.99585917   0.20439999   0.76139998   0.24789999   0.80140001
    5.79683304]
 [349.4045777    0.32659999   0.73999995   0.21359999   0.73560005
    6.1076684 ]
 ...
 [377.5027548    0.1428       0.66420001   0.3344       0.6577
    6.06521749]
 [493.86765565   0.29749998   0.84820002   0.35100001   0.81720001
    6.13254309]
 [380.46307082   0.147        0.76949996   0.30500001   0.78420001
    6.05553436]][0m
[37m[1m[2023-07-17 12:56:30,504][257371] Max Reward on eval: 672.9571498554666[0m
[37m[1m[2023-07-17 12:56:30,504][257371] Min Reward on eval: -24.691602926701307[0m
[37m[1m[2023-07-17 12:56:30,504][257371] Mean Reward across all agents: 366.2637070459563[0m
[37m[1m[2023-07-17 12:56:30,504][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 12:56:30,508][257371] mean_value=-159.2878989700802, max_value=287.16291591131323[0m
[37m[1m[2023-07-17 12:56:30,510][257371] New mean coefficients: [[ 0.24009815 -2.5252495   1.8261619  -1.1143968  -0.39003533 -1.9007536 ]][0m
[37m[1m[2023-07-17 12:56:30,511][257371] Moving the mean solution point...[0m
[36m[2023-07-17 12:56:39,535][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 12:56:39,535][257371] FPS: 425635.20[0m
[36m[2023-07-17 12:56:39,537][257371] itr=1240, itrs=2000, Progress: 62.00%[0m
[37m[1m[2023-07-17 13:00:17,235][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001220[0m
[36m[2023-07-17 13:00:29,543][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 13:00:29,543][257371] FPS: 329876.59[0m
[36m[2023-07-17 13:00:33,760][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:00:33,760][257371] Reward + Measures: [[349.80925991   0.21212468   0.78151935   0.21701334   0.79155165
    6.00180006]][0m
[37m[1m[2023-07-17 13:00:33,760][257371] Max Reward on eval: 349.8092599128828[0m
[37m[1m[2023-07-17 13:00:33,761][257371] Min Reward on eval: 349.8092599128828[0m
[37m[1m[2023-07-17 13:00:33,761][257371] Mean Reward across all agents: 349.8092599128828[0m
[37m[1m[2023-07-17 13:00:33,761][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:00:38,906][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:00:38,912][257371] Reward + Measures: [[256.1202433    0.2323       0.46280003   0.12810001   0.47529998
    4.96349669]
 [364.02370235   0.27990001   0.6433       0.1346       0.63870001
    5.31506014]
 [232.88993298   0.33200002   0.81780005   0.18800001   0.82569999
    5.53607702]
 ...
 [555.0244641    0.39860001   0.85729998   0.22830001   0.85080004
    5.29331827]
 [360.07577134   0.29359999   0.6886       0.20120001   0.72749996
    5.05259752]
 [340.03775249   0.23430002   0.68869996   0.20119999   0.67720002
    5.65206623]][0m
[37m[1m[2023-07-17 13:00:38,913][257371] Max Reward on eval: 642.6995200830512[0m
[37m[1m[2023-07-17 13:00:38,913][257371] Min Reward on eval: -165.1359433269128[0m
[37m[1m[2023-07-17 13:00:38,913][257371] Mean Reward across all agents: 326.21375314959715[0m
[37m[1m[2023-07-17 13:00:38,913][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:00:38,916][257371] mean_value=-220.62713465024163, max_value=37.93393699543475[0m
[37m[1m[2023-07-17 13:00:38,918][257371] New mean coefficients: [[ 0.66639066 -1.7247055   1.9190531  -1.1219536  -1.3244146  -1.0834239 ]][0m
[37m[1m[2023-07-17 13:00:38,919][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:00:47,828][257371] train() took 8.91 seconds to complete[0m
[36m[2023-07-17 13:00:47,828][257371] FPS: 431116.50[0m
[36m[2023-07-17 13:00:47,831][257371] itr=1241, itrs=2000, Progress: 62.05%[0m
[36m[2023-07-17 13:00:59,843][257371] train() took 11.90 seconds to complete[0m
[36m[2023-07-17 13:00:59,843][257371] FPS: 322722.54[0m
[36m[2023-07-17 13:01:04,095][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:01:04,095][257371] Reward + Measures: [[354.75878241   0.21917267   0.77656531   0.208928     0.78656763
    5.91743612]][0m
[37m[1m[2023-07-17 13:01:04,096][257371] Max Reward on eval: 354.7587824081271[0m
[37m[1m[2023-07-17 13:01:04,096][257371] Min Reward on eval: 354.7587824081271[0m
[37m[1m[2023-07-17 13:01:04,096][257371] Mean Reward across all agents: 354.7587824081271[0m
[37m[1m[2023-07-17 13:01:04,096][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:01:09,011][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:01:09,012][257371] Reward + Measures: [[585.95542187   0.45940003   0.82670003   0.1093       0.7881
    5.93925858]
 [319.41902684   0.15189999   0.74519998   0.206        0.75419998
    5.7314353 ]
 [361.08481808   0.14790002   0.75949997   0.2753       0.76719999
    6.1434989 ]
 ...
 [291.59669784   0.44670001   0.75699997   0.10350001   0.75270003
    5.35387182]
 [288.80326558   0.45500001   0.73299998   0.11500001   0.68160003
    5.72411108]
 [288.74463348   0.28         0.67189997   0.1804       0.65679997
    5.79597473]][0m
[37m[1m[2023-07-17 13:01:09,012][257371] Max Reward on eval: 654.6960811828728[0m
[37m[1m[2023-07-17 13:01:09,012][257371] Min Reward on eval: 69.58600875823758[0m
[37m[1m[2023-07-17 13:01:09,012][257371] Mean Reward across all agents: 358.5054284700928[0m
[37m[1m[2023-07-17 13:01:09,013][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:01:09,015][257371] mean_value=-172.77103143482492, max_value=114.43928902422428[0m
[37m[1m[2023-07-17 13:01:09,017][257371] New mean coefficients: [[ 0.7351     -1.5080854   2.0416765  -0.99827975 -1.6565549  -0.01037633]][0m
[37m[1m[2023-07-17 13:01:09,018][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:01:17,968][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 13:01:17,968][257371] FPS: 429152.01[0m
[36m[2023-07-17 13:01:17,970][257371] itr=1242, itrs=2000, Progress: 62.10%[0m
[36m[2023-07-17 13:01:29,646][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 13:01:29,646][257371] FPS: 332115.15[0m
[36m[2023-07-17 13:01:33,972][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:01:33,973][257371] Reward + Measures: [[361.40972297   0.22088932   0.76566964   0.20970199   0.77041334
    5.87004137]][0m
[37m[1m[2023-07-17 13:01:33,973][257371] Max Reward on eval: 361.40972297216337[0m
[37m[1m[2023-07-17 13:01:33,973][257371] Min Reward on eval: 361.40972297216337[0m
[37m[1m[2023-07-17 13:01:33,973][257371] Mean Reward across all agents: 361.40972297216337[0m
[37m[1m[2023-07-17 13:01:33,974][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:01:39,010][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:01:39,016][257371] Reward + Measures: [[362.35034441   0.14210001   0.75140005   0.28200001   0.75750005
    6.09125137]
 [332.96628331   0.32600001   0.75819999   0.1858       0.75209999
    6.11430979]
 [445.26182747   0.2764       0.81549996   0.18620001   0.80540001
    5.91324568]
 ...
 [198.49499625   0.26430002   0.82269996   0.29450002   0.82459992
    6.06523561]
 [327.8197994    0.26710001   0.67189997   0.16240001   0.62940007
    5.71355915]
 [385.48983318   0.28449997   0.80610001   0.28040001   0.80450004
    5.94085884]][0m
[37m[1m[2023-07-17 13:01:39,016][257371] Max Reward on eval: 596.803834355573[0m
[37m[1m[2023-07-17 13:01:39,016][257371] Min Reward on eval: -82.47707685886417[0m
[37m[1m[2023-07-17 13:01:39,017][257371] Mean Reward across all agents: 317.2681338999649[0m
[37m[1m[2023-07-17 13:01:39,017][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:01:39,019][257371] mean_value=-210.0355479457441, max_value=269.9085474983059[0m
[37m[1m[2023-07-17 13:01:39,021][257371] New mean coefficients: [[ 1.4730189  -1.5357682   2.0071278  -0.7853581  -1.9368014  -0.96875393]][0m
[37m[1m[2023-07-17 13:01:39,022][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:01:48,095][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 13:01:48,095][257371] FPS: 423330.51[0m
[36m[2023-07-17 13:01:48,098][257371] itr=1243, itrs=2000, Progress: 62.15%[0m
[36m[2023-07-17 13:01:59,770][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 13:01:59,770][257371] FPS: 332368.49[0m
[36m[2023-07-17 13:02:03,986][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:02:03,986][257371] Reward + Measures: [[365.81099576   0.22232233   0.75442064   0.21307033   0.75974029
    5.75142527]][0m
[37m[1m[2023-07-17 13:02:03,987][257371] Max Reward on eval: 365.81099576226995[0m
[37m[1m[2023-07-17 13:02:03,987][257371] Min Reward on eval: 365.81099576226995[0m
[37m[1m[2023-07-17 13:02:03,987][257371] Mean Reward across all agents: 365.81099576226995[0m
[37m[1m[2023-07-17 13:02:03,987][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:02:08,892][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:02:08,893][257371] Reward + Measures: [[453.06097344   0.4427       0.77220005   0.13890001   0.76970005
    5.59180832]
 [474.759164     0.1635       0.78349996   0.3531       0.8021
    5.94292355]
 [589.36701968   0.46830001   0.88510001   0.16949999   0.86059999
    6.12003279]
 ...
 [445.02099044   0.40110001   0.75940001   0.1831       0.77380002
    5.5605979 ]
 [216.79425438   0.20850001   0.46779999   0.1141       0.46630001
    5.03912401]
 [294.18919608   0.2441       0.52540004   0.14530002   0.52630001
    5.2943697 ]][0m
[37m[1m[2023-07-17 13:02:08,893][257371] Max Reward on eval: 678.7958107048646[0m
[37m[1m[2023-07-17 13:02:08,893][257371] Min Reward on eval: 100.33723420542665[0m
[37m[1m[2023-07-17 13:02:08,893][257371] Mean Reward across all agents: 365.70200373500893[0m
[37m[1m[2023-07-17 13:02:08,894][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:02:08,896][257371] mean_value=-156.79697281917882, max_value=65.27669966430489[0m
[37m[1m[2023-07-17 13:02:08,898][257371] New mean coefficients: [[ 1.6723791  -1.2983608   1.4671772  -0.46377957 -1.566394   -1.3970362 ]][0m
[37m[1m[2023-07-17 13:02:08,899][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:02:17,858][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 13:02:17,858][257371] FPS: 428700.47[0m
[36m[2023-07-17 13:02:17,861][257371] itr=1244, itrs=2000, Progress: 62.20%[0m
[36m[2023-07-17 13:02:29,499][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 13:02:29,499][257371] FPS: 333167.27[0m
[36m[2023-07-17 13:02:33,720][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:02:33,721][257371] Reward + Measures: [[360.14178876   0.22405066   0.73621339   0.20615299   0.74411035
    5.60310793]][0m
[37m[1m[2023-07-17 13:02:33,721][257371] Max Reward on eval: 360.1417887579718[0m
[37m[1m[2023-07-17 13:02:33,721][257371] Min Reward on eval: 360.1417887579718[0m
[37m[1m[2023-07-17 13:02:33,721][257371] Mean Reward across all agents: 360.1417887579718[0m
[37m[1m[2023-07-17 13:02:33,722][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:02:38,681][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:02:38,681][257371] Reward + Measures: [[329.36971413   0.2172       0.62949997   0.17309999   0.64749998
    5.57249975]
 [422.7081671    0.29449999   0.6972       0.1638       0.6979
    5.49093914]
 [430.08038327   0.18350001   0.77069998   0.35699999   0.77490008
    5.72336245]
 ...
 [406.89163564   0.3536       0.56470001   0.1209       0.52200001
    5.12374783]
 [494.56131154   0.3039       0.70300001   0.2191       0.71709996
    5.24416351]
 [327.38926686   0.07179999   0.58640003   0.31940001   0.58890003
    5.6364994 ]][0m
[37m[1m[2023-07-17 13:02:38,681][257371] Max Reward on eval: 762.0785713284276[0m
[37m[1m[2023-07-17 13:02:38,682][257371] Min Reward on eval: 71.52885062526911[0m
[37m[1m[2023-07-17 13:02:38,682][257371] Mean Reward across all agents: 375.5587717043136[0m
[37m[1m[2023-07-17 13:02:38,682][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:02:38,685][257371] mean_value=-129.50899050090578, max_value=195.8093919335164[0m
[37m[1m[2023-07-17 13:02:38,687][257371] New mean coefficients: [[ 1.494398  -1.0378263  1.469464   0.338714  -1.3820043 -1.3381121]][0m
[37m[1m[2023-07-17 13:02:38,688][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:02:47,705][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 13:02:47,705][257371] FPS: 425945.69[0m
[36m[2023-07-17 13:02:47,708][257371] itr=1245, itrs=2000, Progress: 62.25%[0m
[36m[2023-07-17 13:02:59,385][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 13:02:59,385][257371] FPS: 332194.57[0m
[36m[2023-07-17 13:03:03,602][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:03:03,603][257371] Reward + Measures: [[375.52903503   0.229201     0.71926433   0.21191865   0.72830135
    5.48184156]][0m
[37m[1m[2023-07-17 13:03:03,603][257371] Max Reward on eval: 375.52903502615953[0m
[37m[1m[2023-07-17 13:03:03,603][257371] Min Reward on eval: 375.52903502615953[0m
[37m[1m[2023-07-17 13:03:03,603][257371] Mean Reward across all agents: 375.52903502615953[0m
[37m[1m[2023-07-17 13:03:03,604][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:03:08,570][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:03:08,571][257371] Reward + Measures: [[171.85369665   0.1408       0.32280001   0.14390001   0.3626
    4.4656105 ]
 [409.21918085   0.28910002   0.62449998   0.1734       0.62400001
    5.43900347]
 [400.15309122   0.31259999   0.72310001   0.14390001   0.7263
    5.67986155]
 ...
 [337.4018848    0.24749999   0.64670008   0.1865       0.6476
    5.03485155]
 [414.78834911   0.2658       0.82919997   0.24910001   0.84810001
    5.55048704]
 [328.10312816   0.25119999   0.63269997   0.1787       0.65880007
    5.14165115]][0m
[37m[1m[2023-07-17 13:03:08,571][257371] Max Reward on eval: 723.3592834175681[0m
[37m[1m[2023-07-17 13:03:08,571][257371] Min Reward on eval: 154.16958563975058[0m
[37m[1m[2023-07-17 13:03:08,571][257371] Mean Reward across all agents: 386.3785134120357[0m
[37m[1m[2023-07-17 13:03:08,572][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:03:08,574][257371] mean_value=-131.02387637522187, max_value=119.17235527179264[0m
[37m[1m[2023-07-17 13:03:08,576][257371] New mean coefficients: [[ 1.2125016  -1.5333105   1.805161   -0.14914533 -1.4318675  -1.1903392 ]][0m
[37m[1m[2023-07-17 13:03:08,577][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:03:17,669][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 13:03:17,670][257371] FPS: 422438.10[0m
[36m[2023-07-17 13:03:17,672][257371] itr=1246, itrs=2000, Progress: 62.30%[0m
[36m[2023-07-17 13:03:29,629][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-17 13:03:29,630][257371] FPS: 324296.82[0m
[36m[2023-07-17 13:03:33,974][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:03:33,975][257371] Reward + Measures: [[380.61666444   0.23716064   0.69699073   0.20536967   0.70934767
    5.30834532]][0m
[37m[1m[2023-07-17 13:03:33,975][257371] Max Reward on eval: 380.61666443841654[0m
[37m[1m[2023-07-17 13:03:33,975][257371] Min Reward on eval: 380.61666443841654[0m
[37m[1m[2023-07-17 13:03:33,975][257371] Mean Reward across all agents: 380.61666443841654[0m
[37m[1m[2023-07-17 13:03:33,976][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:03:39,278][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:03:39,279][257371] Reward + Measures: [[467.12810045   0.32620001   0.82140011   0.2198       0.82700008
    5.56981802]
 [449.46215435   0.24720001   0.73859996   0.24660002   0.77430004
    5.25101757]
 [417.62462376   0.27359998   0.72459996   0.20510001   0.73649997
    5.16535091]
 ...
 [420.50616102   0.12010001   0.83479995   0.44029999   0.83840001
    5.9423933 ]
 [447.16419317   0.28959998   0.78020006   0.26830003   0.79090005
    5.63446665]
 [312.64758854   0.18810001   0.61320001   0.31100002   0.64429998
    5.1829834 ]][0m
[37m[1m[2023-07-17 13:03:39,279][257371] Max Reward on eval: 643.5397757983767[0m
[37m[1m[2023-07-17 13:03:39,279][257371] Min Reward on eval: 56.71676592612639[0m
[37m[1m[2023-07-17 13:03:39,280][257371] Mean Reward across all agents: 370.4977022263658[0m
[37m[1m[2023-07-17 13:03:39,280][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:03:39,282][257371] mean_value=-153.62333342745833, max_value=31.88710772372582[0m
[37m[1m[2023-07-17 13:03:39,284][257371] New mean coefficients: [[ 1.8624905 -1.8431814  1.9605187 -0.197818  -1.0142968 -1.3852854]][0m
[37m[1m[2023-07-17 13:03:39,285][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:03:48,303][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 13:03:48,303][257371] FPS: 425910.51[0m
[36m[2023-07-17 13:03:48,305][257371] itr=1247, itrs=2000, Progress: 62.35%[0m
[36m[2023-07-17 13:04:00,025][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 13:04:00,025][257371] FPS: 330855.15[0m
[36m[2023-07-17 13:04:04,310][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:04:04,310][257371] Reward + Measures: [[391.27092301   0.24715267   0.67993838   0.20441198   0.69291663
    5.17992592]][0m
[37m[1m[2023-07-17 13:04:04,310][257371] Max Reward on eval: 391.2709230143729[0m
[37m[1m[2023-07-17 13:04:04,311][257371] Min Reward on eval: 391.2709230143729[0m
[37m[1m[2023-07-17 13:04:04,311][257371] Mean Reward across all agents: 391.2709230143729[0m
[37m[1m[2023-07-17 13:04:04,311][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:04:09,302][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:04:09,302][257371] Reward + Measures: [[321.85554914   0.2357       0.52529997   0.18810001   0.55929995
    4.4779582 ]
 [421.6177871    0.26100001   0.7123       0.20170002   0.71600002
    5.16070795]
 [467.60680594   0.3238       0.6225       0.1551       0.63849998
    4.89794922]
 ...
 [217.34199763   0.18620001   0.5697       0.1978       0.59579998
    4.82645178]
 [307.39155556   0.23650001   0.55409998   0.17620002   0.56419998
    4.95676136]
 [324.99985358   0.24520002   0.67769998   0.2271       0.70320004
    5.10417461]][0m
[37m[1m[2023-07-17 13:04:09,303][257371] Max Reward on eval: 658.6859569820575[0m
[37m[1m[2023-07-17 13:04:09,303][257371] Min Reward on eval: -49.024915103754026[0m
[37m[1m[2023-07-17 13:04:09,303][257371] Mean Reward across all agents: 379.5059150213751[0m
[37m[1m[2023-07-17 13:04:09,303][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:04:09,305][257371] mean_value=-157.15024715735748, max_value=64.60129659022425[0m
[37m[1m[2023-07-17 13:04:09,308][257371] New mean coefficients: [[ 1.8368123  -1.1870749   1.6500459   0.34554678 -1.3700929  -0.91345173]][0m
[37m[1m[2023-07-17 13:04:09,309][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:04:18,409][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 13:04:18,409][257371] FPS: 422051.31[0m
[36m[2023-07-17 13:04:18,411][257371] itr=1248, itrs=2000, Progress: 62.40%[0m
[36m[2023-07-17 13:04:30,184][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 13:04:30,184][257371] FPS: 329469.50[0m
[36m[2023-07-17 13:04:34,557][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:04:34,557][257371] Reward + Measures: [[400.26994698   0.253111     0.65670329   0.20012332   0.67290932
    5.07533503]][0m
[37m[1m[2023-07-17 13:04:34,558][257371] Max Reward on eval: 400.26994698266327[0m
[37m[1m[2023-07-17 13:04:34,558][257371] Min Reward on eval: 400.26994698266327[0m
[37m[1m[2023-07-17 13:04:34,558][257371] Mean Reward across all agents: 400.26994698266327[0m
[37m[1m[2023-07-17 13:04:34,558][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:04:39,531][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:04:39,531][257371] Reward + Measures: [[322.93573626   0.38429999   0.56549996   0.1127       0.58330005
    4.95206833]
 [334.38384815   0.19509999   0.6146       0.2385       0.66430002
    4.79607439]
 [284.0058269    0.2766       0.41460004   0.0962       0.43309999
    4.20639944]
 ...
 [367.11285087   0.24849999   0.53579998   0.198        0.57969999
    4.63113737]
 [475.41218713   0.41370001   0.69340003   0.12360001   0.67970002
    5.17801571]
 [396.07881147   0.23360001   0.6056       0.18170001   0.62349999
    5.05085468]][0m
[37m[1m[2023-07-17 13:04:39,532][257371] Max Reward on eval: 646.2108878667466[0m
[37m[1m[2023-07-17 13:04:39,532][257371] Min Reward on eval: 187.20615934644593[0m
[37m[1m[2023-07-17 13:04:39,532][257371] Mean Reward across all agents: 394.4054031362961[0m
[37m[1m[2023-07-17 13:04:39,533][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:04:39,535][257371] mean_value=-169.41554322371385, max_value=69.53255152466954[0m
[37m[1m[2023-07-17 13:04:39,537][257371] New mean coefficients: [[ 1.1819603  -0.71659446  2.0500803   0.5617902  -1.1372988  -0.39084744]][0m
[37m[1m[2023-07-17 13:04:39,538][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:04:48,505][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 13:04:48,505][257371] FPS: 428328.87[0m
[36m[2023-07-17 13:04:48,507][257371] itr=1249, itrs=2000, Progress: 62.45%[0m
[36m[2023-07-17 13:05:00,186][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 13:05:00,186][257371] FPS: 332002.27[0m
[36m[2023-07-17 13:05:04,494][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:05:04,494][257371] Reward + Measures: [[400.79160294   0.23218368   0.660604     0.22343266   0.67343736
    5.09073305]][0m
[37m[1m[2023-07-17 13:05:04,495][257371] Max Reward on eval: 400.7916029360216[0m
[37m[1m[2023-07-17 13:05:04,495][257371] Min Reward on eval: 400.7916029360216[0m
[37m[1m[2023-07-17 13:05:04,495][257371] Mean Reward across all agents: 400.7916029360216[0m
[37m[1m[2023-07-17 13:05:04,495][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:05:09,463][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:05:09,463][257371] Reward + Measures: [[318.81906254   0.2674       0.60040003   0.16120002   0.60519999
    5.4089632 ]
 [356.31118018   0.31150001   0.73290008   0.204        0.74470001
    5.02329493]
 [376.55141762   0.2342       0.59690005   0.1866       0.58960003
    5.034338  ]
 ...
 [440.60538635   0.38040003   0.75889999   0.19000001   0.74309999
    5.3029418 ]
 [298.93255601   0.1516       0.64810002   0.24429999   0.66480005
    5.03178167]
 [448.92244283   0.3249       0.61720002   0.14390002   0.58840007
    5.13267851]][0m
[37m[1m[2023-07-17 13:05:09,464][257371] Max Reward on eval: 642.642639157828[0m
[37m[1m[2023-07-17 13:05:09,464][257371] Min Reward on eval: 139.4182511202991[0m
[37m[1m[2023-07-17 13:05:09,464][257371] Mean Reward across all agents: 383.61461839276075[0m
[37m[1m[2023-07-17 13:05:09,464][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:05:09,466][257371] mean_value=-142.99528585356654, max_value=48.320865766605095[0m
[37m[1m[2023-07-17 13:05:09,469][257371] New mean coefficients: [[ 0.87823653 -1.0161616   1.941136    0.7394872  -0.51987535 -0.2408432 ]][0m
[37m[1m[2023-07-17 13:05:09,470][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:05:18,449][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 13:05:18,449][257371] FPS: 427737.11[0m
[36m[2023-07-17 13:05:18,451][257371] itr=1250, itrs=2000, Progress: 62.50%[0m
[37m[1m[2023-07-17 13:08:42,737][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001230[0m
[36m[2023-07-17 13:08:55,052][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 13:08:55,052][257371] FPS: 329375.43[0m
[36m[2023-07-17 13:08:59,305][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:08:59,306][257371] Reward + Measures: [[406.63311343   0.21522667   0.67346066   0.25210866   0.68316364
    5.15293455]][0m
[37m[1m[2023-07-17 13:08:59,306][257371] Max Reward on eval: 406.6331134267907[0m
[37m[1m[2023-07-17 13:08:59,306][257371] Min Reward on eval: 406.6331134267907[0m
[37m[1m[2023-07-17 13:08:59,307][257371] Mean Reward across all agents: 406.6331134267907[0m
[37m[1m[2023-07-17 13:08:59,307][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:09:04,549][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:09:04,550][257371] Reward + Measures: [[321.96664908   0.1477       0.60949999   0.26989999   0.62740004
    4.82597542]
 [316.85894345   0.19810002   0.57740003   0.26100001   0.57770002
    4.95332718]
 [368.48761885   0.17470001   0.68489999   0.27360001   0.7044
    5.18626356]
 ...
 [445.81702041   0.1921       0.73699999   0.2701       0.73970002
    5.18266439]
 [440.87740671   0.3044       0.65329999   0.198        0.65399998
    5.05397415]
 [481.05692825   0.26200002   0.67219996   0.28150001   0.67790002
    5.21314621]][0m
[37m[1m[2023-07-17 13:09:04,550][257371] Max Reward on eval: 662.6815833809785[0m
[37m[1m[2023-07-17 13:09:04,550][257371] Min Reward on eval: 145.15673325932585[0m
[37m[1m[2023-07-17 13:09:04,550][257371] Mean Reward across all agents: 407.0805767523711[0m
[37m[1m[2023-07-17 13:09:04,550][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:09:04,553][257371] mean_value=-115.68490134860573, max_value=138.767202974273[0m
[37m[1m[2023-07-17 13:09:04,555][257371] New mean coefficients: [[ 1.2911131  -0.9548472   2.026843    1.3907642  -0.2871295   0.65077424]][0m
[37m[1m[2023-07-17 13:09:04,556][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:09:13,617][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 13:09:13,618][257371] FPS: 423874.05[0m
[36m[2023-07-17 13:09:13,620][257371] itr=1251, itrs=2000, Progress: 62.55%[0m
[36m[2023-07-17 13:09:25,519][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 13:09:25,519][257371] FPS: 325970.70[0m
[36m[2023-07-17 13:09:29,943][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:09:29,943][257371] Reward + Measures: [[427.3196438    0.21690434   0.70002866   0.269171     0.70496565
    5.23618603]][0m
[37m[1m[2023-07-17 13:09:29,943][257371] Max Reward on eval: 427.3196438011169[0m
[37m[1m[2023-07-17 13:09:29,944][257371] Min Reward on eval: 427.3196438011169[0m
[37m[1m[2023-07-17 13:09:29,944][257371] Mean Reward across all agents: 427.3196438011169[0m
[37m[1m[2023-07-17 13:09:29,944][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:09:34,945][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:09:34,946][257371] Reward + Measures: [[ 438.24962236    0.22650002    0.78680003    0.29359999    0.75840002
     5.67904234]
 [ 609.19715404    0.64550006    0.70050001    0.0787        0.70640004
     4.815907  ]
 [ 152.47514087    0.1716        0.40099999    0.17209999    0.39820001
     4.9073987 ]
 ...
 [-152.81536246    0.50229996    0.62720001    0.32550001    0.4276
     4.50321531]
 [ 216.55941695    0.1296        0.70900005    0.31150004    0.73710006
     5.27995539]
 [  57.76389404    0.32870001    0.38690001    0.2579        0.32480001
     4.02238226]][0m
[37m[1m[2023-07-17 13:09:34,946][257371] Max Reward on eval: 609.1971540428931[0m
[37m[1m[2023-07-17 13:09:34,946][257371] Min Reward on eval: -227.3881228106562[0m
[37m[1m[2023-07-17 13:09:34,947][257371] Mean Reward across all agents: 130.28899354794504[0m
[37m[1m[2023-07-17 13:09:34,947][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:09:34,950][257371] mean_value=-553.3686263958914, max_value=170.19108755636[0m
[37m[1m[2023-07-17 13:09:34,952][257371] New mean coefficients: [[ 1.2163844  -0.5310223   1.9982121   0.08863735 -0.5608262   0.18561798]][0m
[37m[1m[2023-07-17 13:09:34,953][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:09:43,941][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 13:09:43,941][257371] FPS: 427345.79[0m
[36m[2023-07-17 13:09:43,943][257371] itr=1252, itrs=2000, Progress: 62.60%[0m
[36m[2023-07-17 13:09:55,676][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 13:09:55,676][257371] FPS: 330513.02[0m
[36m[2023-07-17 13:09:59,948][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:09:59,948][257371] Reward + Measures: [[449.22553334   0.18767032   0.73261929   0.32957697   0.73478937
    5.4170599 ]][0m
[37m[1m[2023-07-17 13:09:59,949][257371] Max Reward on eval: 449.2255333420654[0m
[37m[1m[2023-07-17 13:09:59,949][257371] Min Reward on eval: 449.2255333420654[0m
[37m[1m[2023-07-17 13:09:59,949][257371] Mean Reward across all agents: 449.2255333420654[0m
[37m[1m[2023-07-17 13:09:59,950][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:10:04,875][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:10:04,875][257371] Reward + Measures: [[424.96660331   0.2543       0.71689999   0.29770002   0.72049999
    5.16095448]
 [374.96660738   0.23380001   0.6153       0.30509999   0.63889998
    4.9429183 ]
 [435.08650765   0.1823       0.66670007   0.33070001   0.67230004
    5.49781799]
 ...
 [391.71165398   0.27670002   0.71640003   0.2816       0.7245
    5.07423878]
 [332.0573098    0.21700001   0.58139998   0.23980001   0.5557
    4.94277191]
 [434.0593069    0.15350001   0.71700001   0.3669       0.71820003
    5.32055235]][0m
[37m[1m[2023-07-17 13:10:04,876][257371] Max Reward on eval: 653.8770046817139[0m
[37m[1m[2023-07-17 13:10:04,876][257371] Min Reward on eval: -201.336294436967[0m
[37m[1m[2023-07-17 13:10:04,876][257371] Mean Reward across all agents: 382.0881433284569[0m
[37m[1m[2023-07-17 13:10:04,876][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:10:04,879][257371] mean_value=-146.00215235488048, max_value=78.67846994310423[0m
[37m[1m[2023-07-17 13:10:04,881][257371] New mean coefficients: [[ 1.2668068   0.2610224   1.4418821   0.2739789  -0.8934244  -0.95235866]][0m
[37m[1m[2023-07-17 13:10:04,882][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:10:13,811][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 13:10:13,811][257371] FPS: 430143.06[0m
[36m[2023-07-17 13:10:13,813][257371] itr=1253, itrs=2000, Progress: 62.65%[0m
[36m[2023-07-17 13:10:25,429][257371] train() took 11.50 seconds to complete[0m
[36m[2023-07-17 13:10:25,430][257371] FPS: 333829.55[0m
[36m[2023-07-17 13:10:29,701][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:10:29,707][257371] Reward + Measures: [[455.57664312   0.19056334   0.72933531   0.33116433   0.73108602
    5.41054583]][0m
[37m[1m[2023-07-17 13:10:29,707][257371] Max Reward on eval: 455.5766431187818[0m
[37m[1m[2023-07-17 13:10:29,708][257371] Min Reward on eval: 455.5766431187818[0m
[37m[1m[2023-07-17 13:10:29,708][257371] Mean Reward across all agents: 455.5766431187818[0m
[37m[1m[2023-07-17 13:10:29,708][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:10:34,715][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:10:34,720][257371] Reward + Measures: [[609.11042687   0.50889999   0.8434       0.16690002   0.8229
    5.63664293]
 [551.96527685   0.16140001   0.74189997   0.44390002   0.7712
    5.43753004]
 [386.14443878   0.1644       0.79280001   0.39019999   0.79520005
    5.33140278]
 ...
 [503.14625548   0.0715       0.82950002   0.50229996   0.8585
    6.01545668]
 [353.43850824   0.25420001   0.66750002   0.24270001   0.66200006
    5.06791258]
 [395.06117733   0.1629       0.61220002   0.30019999   0.6304
    5.28731871]][0m
[37m[1m[2023-07-17 13:10:34,721][257371] Max Reward on eval: 651.526283266861[0m
[37m[1m[2023-07-17 13:10:34,721][257371] Min Reward on eval: 24.51509922503028[0m
[37m[1m[2023-07-17 13:10:34,721][257371] Mean Reward across all agents: 418.37424591762505[0m
[37m[1m[2023-07-17 13:10:34,722][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:10:34,725][257371] mean_value=-112.59189911633524, max_value=136.97090532944696[0m
[37m[1m[2023-07-17 13:10:34,727][257371] New mean coefficients: [[ 1.6571494  -0.4877619   0.8239286  -0.37129527 -0.47732872 -0.52832013]][0m
[37m[1m[2023-07-17 13:10:34,728][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:10:43,741][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 13:10:43,741][257371] FPS: 426131.38[0m
[36m[2023-07-17 13:10:43,743][257371] itr=1254, itrs=2000, Progress: 62.70%[0m
[36m[2023-07-17 13:10:55,642][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 13:10:55,642][257371] FPS: 325890.42[0m
[36m[2023-07-17 13:10:59,908][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:10:59,913][257371] Reward + Measures: [[460.42267121   0.20688832   0.70883709   0.31042135   0.70990133
    5.29690409]][0m
[37m[1m[2023-07-17 13:10:59,914][257371] Max Reward on eval: 460.4226712110734[0m
[37m[1m[2023-07-17 13:10:59,914][257371] Min Reward on eval: 460.4226712110734[0m
[37m[1m[2023-07-17 13:10:59,914][257371] Mean Reward across all agents: 460.4226712110734[0m
[37m[1m[2023-07-17 13:10:59,915][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:11:05,002][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:11:05,005][257371] Reward + Measures: [[442.48956905   0.32350001   0.58680004   0.17130001   0.59759998
    4.54489708]
 [412.81123062   0.19569999   0.71570003   0.30829999   0.72540003
    5.06037092]
 [473.37033256   0.41370001   0.63890004   0.13590001   0.63620007
    4.980865  ]
 ...
 [321.84036647   0.17840001   0.59079999   0.2342       0.59990001
    4.95282888]
 [435.89298244   0.1508       0.7349       0.34730002   0.77290004
    5.04066038]
 [409.39695456   0.24689999   0.67539996   0.24790001   0.67740005
    5.23617506]][0m
[37m[1m[2023-07-17 13:11:05,006][257371] Max Reward on eval: 674.0252723488957[0m
[37m[1m[2023-07-17 13:11:05,006][257371] Min Reward on eval: 160.7634310688183[0m
[37m[1m[2023-07-17 13:11:05,006][257371] Mean Reward across all agents: 427.49085200721515[0m
[37m[1m[2023-07-17 13:11:05,007][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:11:05,009][257371] mean_value=-111.87938438300917, max_value=80.39616317274249[0m
[37m[1m[2023-07-17 13:11:05,012][257371] New mean coefficients: [[ 1.5531964  -0.9676772   0.8268314  -0.20757365 -0.163995   -0.2767963 ]][0m
[37m[1m[2023-07-17 13:11:05,013][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:11:14,027][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 13:11:14,027][257371] FPS: 426064.64[0m
[36m[2023-07-17 13:11:14,030][257371] itr=1255, itrs=2000, Progress: 62.75%[0m
[36m[2023-07-17 13:11:25,719][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 13:11:25,719][257371] FPS: 331815.45[0m
[36m[2023-07-17 13:11:30,083][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:11:30,088][257371] Reward + Measures: [[469.77171089   0.18944132   0.71300161   0.33133101   0.71641892
    5.32151079]][0m
[37m[1m[2023-07-17 13:11:30,089][257371] Max Reward on eval: 469.7717108870075[0m
[37m[1m[2023-07-17 13:11:30,089][257371] Min Reward on eval: 469.7717108870075[0m
[37m[1m[2023-07-17 13:11:30,089][257371] Mean Reward across all agents: 469.7717108870075[0m
[37m[1m[2023-07-17 13:11:30,090][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:11:35,132][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:11:35,138][257371] Reward + Measures: [[498.96568696   0.28729999   0.69780004   0.21089999   0.68970007
    5.20851851]
 [420.11270238   0.0958       0.8089       0.39580002   0.80150002
    5.65683222]
 [548.01772858   0.18279999   0.80620003   0.38600001   0.79650003
    5.61546183]
 ...
 [339.99745417   0.19170001   0.59030002   0.2256       0.58490008
    5.03836966]
 [361.89543645   0.0771       0.64749998   0.38030002   0.65700001
    5.24108839]
 [354.08517182   0.1749       0.60040003   0.30360001   0.62210006
    4.81935072]][0m
[37m[1m[2023-07-17 13:11:35,138][257371] Max Reward on eval: 771.4990157798863[0m
[37m[1m[2023-07-17 13:11:35,138][257371] Min Reward on eval: 150.84402392304037[0m
[37m[1m[2023-07-17 13:11:35,138][257371] Mean Reward across all agents: 444.3631427508616[0m
[37m[1m[2023-07-17 13:11:35,139][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:11:35,142][257371] mean_value=-99.2923837547308, max_value=137.2603842323847[0m
[37m[1m[2023-07-17 13:11:35,144][257371] New mean coefficients: [[ 1.7455971  -1.2032601   0.2651726   0.02313235  0.10050973 -0.716949  ]][0m
[37m[1m[2023-07-17 13:11:35,145][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:11:44,212][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 13:11:44,212][257371] FPS: 423612.17[0m
[36m[2023-07-17 13:11:44,214][257371] itr=1256, itrs=2000, Progress: 62.80%[0m
[36m[2023-07-17 13:11:56,209][257371] train() took 11.88 seconds to complete[0m
[36m[2023-07-17 13:11:56,209][257371] FPS: 323306.47[0m
[36m[2023-07-17 13:12:00,410][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:12:00,410][257371] Reward + Measures: [[468.05345099   0.19078167   0.69755369   0.32743299   0.70335364
    5.26240444]][0m
[37m[1m[2023-07-17 13:12:00,410][257371] Max Reward on eval: 468.0534509876954[0m
[37m[1m[2023-07-17 13:12:00,411][257371] Min Reward on eval: 468.0534509876954[0m
[37m[1m[2023-07-17 13:12:00,411][257371] Mean Reward across all agents: 468.0534509876954[0m
[37m[1m[2023-07-17 13:12:00,411][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:12:05,674][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:12:05,675][257371] Reward + Measures: [[100.6264507    0.48470002   0.68110007   0.20290001   0.63379997
    5.54166603]
 [515.09581612   0.22500001   0.68959999   0.31579998   0.6882
    5.45583963]
 [567.92450947   0.18800001   0.7647       0.41370001   0.75700003
    5.85582495]
 ...
 [388.3865765    0.17660001   0.58700001   0.27859998   0.60260004
    4.77626181]
 [353.75467315   0.2494       0.44720003   0.25530002   0.48860002
    3.78564239]
 [-82.11825947   0.5061       0.59000003   0.37470001   0.34520003
    6.36118841]][0m
[37m[1m[2023-07-17 13:12:05,675][257371] Max Reward on eval: 674.200694992661[0m
[37m[1m[2023-07-17 13:12:05,675][257371] Min Reward on eval: -364.02766039846466[0m
[37m[1m[2023-07-17 13:12:05,675][257371] Mean Reward across all agents: 148.60466107460704[0m
[37m[1m[2023-07-17 13:12:05,676][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:12:05,679][257371] mean_value=-248.38519058657573, max_value=285.88097220029965[0m
[37m[1m[2023-07-17 13:12:05,682][257371] New mean coefficients: [[ 1.7788849   0.18590343  0.5299731   0.20931605 -0.5002253  -0.72981364]][0m
[37m[1m[2023-07-17 13:12:05,683][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:12:14,691][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 13:12:14,691][257371] FPS: 426373.37[0m
[36m[2023-07-17 13:12:14,693][257371] itr=1257, itrs=2000, Progress: 62.85%[0m
[36m[2023-07-17 13:12:26,399][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 13:12:26,399][257371] FPS: 331259.77[0m
[36m[2023-07-17 13:12:30,648][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:12:30,649][257371] Reward + Measures: [[471.74185289   0.20501034   0.68357098   0.31368932   0.68789834
    5.16847181]][0m
[37m[1m[2023-07-17 13:12:30,649][257371] Max Reward on eval: 471.7418528859154[0m
[37m[1m[2023-07-17 13:12:30,649][257371] Min Reward on eval: 471.7418528859154[0m
[37m[1m[2023-07-17 13:12:30,649][257371] Mean Reward across all agents: 471.7418528859154[0m
[37m[1m[2023-07-17 13:12:30,650][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:12:35,643][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:12:35,644][257371] Reward + Measures: [[491.647953     0.13939999   0.68279999   0.3766       0.69449997
    5.45774221]
 [458.32219405   0.21780001   0.57050002   0.2726       0.61900002
    4.36556625]
 [510.34921645   0.26190004   0.84759998   0.32720003   0.82590002
    5.53828907]
 ...
 [451.45794963   0.25240001   0.66960001   0.24070001   0.68760002
    4.98367548]
 [295.41735147   0.19590001   0.61790001   0.25920001   0.63120002
    5.13260365]
 [461.61207703   0.31150001   0.59579998   0.1849       0.60590005
    4.66722488]][0m
[37m[1m[2023-07-17 13:12:35,644][257371] Max Reward on eval: 700.7438049092423[0m
[37m[1m[2023-07-17 13:12:35,644][257371] Min Reward on eval: 19.20384376095608[0m
[37m[1m[2023-07-17 13:12:35,644][257371] Mean Reward across all agents: 452.1206548040858[0m
[37m[1m[2023-07-17 13:12:35,645][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:12:35,648][257371] mean_value=-91.88957895531165, max_value=102.53921570946744[0m
[37m[1m[2023-07-17 13:12:35,651][257371] New mean coefficients: [[ 1.6003696  -0.17611316  0.3219499   0.17136347 -0.3559434  -0.5761046 ]][0m
[37m[1m[2023-07-17 13:12:35,652][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:12:44,652][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 13:12:44,653][257371] FPS: 426706.65[0m
[36m[2023-07-17 13:12:44,655][257371] itr=1258, itrs=2000, Progress: 62.90%[0m
[36m[2023-07-17 13:12:56,515][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 13:12:56,515][257371] FPS: 326878.17[0m
[36m[2023-07-17 13:13:00,824][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:13:00,824][257371] Reward + Measures: [[487.64436575   0.19464567   0.69155633   0.33587468   0.69656998
    5.23039055]][0m
[37m[1m[2023-07-17 13:13:00,824][257371] Max Reward on eval: 487.6443657460149[0m
[37m[1m[2023-07-17 13:13:00,825][257371] Min Reward on eval: 487.6443657460149[0m
[37m[1m[2023-07-17 13:13:00,825][257371] Mean Reward across all agents: 487.6443657460149[0m
[37m[1m[2023-07-17 13:13:00,825][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:13:05,837][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:13:05,843][257371] Reward + Measures: [[463.50417268   0.18450001   0.7906       0.45570001   0.76330006
    5.71691656]
 [496.79819922   0.3673       0.73630005   0.22350001   0.70810002
    5.56898308]
 [472.82997743   0.1679       0.75240004   0.3867       0.74970001
    5.30206919]
 ...
 [408.47329392   0.29499999   0.42630002   0.15680002   0.46180001
    4.25156355]
 [550.19947812   0.26340002   0.78400004   0.35049999   0.78680003
    5.74319601]
 [313.97268394   0.1717       0.52530003   0.23410001   0.55439997
    4.69697189]][0m
[37m[1m[2023-07-17 13:13:05,843][257371] Max Reward on eval: 726.4194145494839[0m
[37m[1m[2023-07-17 13:13:05,844][257371] Min Reward on eval: 165.59930755849928[0m
[37m[1m[2023-07-17 13:13:05,844][257371] Mean Reward across all agents: 470.4479210716038[0m
[37m[1m[2023-07-17 13:13:05,845][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:13:05,852][257371] mean_value=-71.53783779730615, max_value=151.66365973180393[0m
[37m[1m[2023-07-17 13:13:05,856][257371] New mean coefficients: [[ 1.8079518  -0.6763488   0.43620878  0.49081677 -0.33826476 -0.39289844]][0m
[37m[1m[2023-07-17 13:13:05,858][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:13:14,857][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 13:13:14,857][257371] FPS: 426797.19[0m
[36m[2023-07-17 13:13:14,860][257371] itr=1259, itrs=2000, Progress: 62.95%[0m
[36m[2023-07-17 13:13:26,865][257371] train() took 11.89 seconds to complete[0m
[36m[2023-07-17 13:13:26,865][257371] FPS: 323024.26[0m
[36m[2023-07-17 13:13:31,179][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:13:31,180][257371] Reward + Measures: [[495.22761297   0.20731901   0.68640643   0.326516     0.69051629
    5.17031288]][0m
[37m[1m[2023-07-17 13:13:31,180][257371] Max Reward on eval: 495.2276129691896[0m
[37m[1m[2023-07-17 13:13:31,180][257371] Min Reward on eval: 495.2276129691896[0m
[37m[1m[2023-07-17 13:13:31,180][257371] Mean Reward across all agents: 495.2276129691896[0m
[37m[1m[2023-07-17 13:13:31,181][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:13:36,184][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:13:36,189][257371] Reward + Measures: [[369.27713836   0.20719998   0.634        0.2843       0.6462
    5.00750637]
 [554.9324379    0.186        0.79530001   0.40220004   0.81069994
    5.35197973]
 [445.75492205   0.2369       0.62880003   0.25530002   0.64200002
    4.93319607]
 ...
 [320.15945305   0.134        0.48710003   0.24879999   0.4989
    4.58233118]
 [475.16636273   0.2687       0.67259997   0.29120001   0.68959999
    4.86403704]
 [489.96095139   0.14860001   0.70719999   0.41440001   0.7105
    5.29874468]][0m
[37m[1m[2023-07-17 13:13:36,189][257371] Max Reward on eval: 668.1008377418387[0m
[37m[1m[2023-07-17 13:13:36,190][257371] Min Reward on eval: 157.48478371286765[0m
[37m[1m[2023-07-17 13:13:36,190][257371] Mean Reward across all agents: 457.86632597060475[0m
[37m[1m[2023-07-17 13:13:36,190][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:13:36,193][257371] mean_value=-109.93889699238493, max_value=368.17168556365516[0m
[37m[1m[2023-07-17 13:13:36,196][257371] New mean coefficients: [[ 2.030799   -0.5857021   0.06333432  0.0111497  -0.2677843  -0.46942455]][0m
[37m[1m[2023-07-17 13:13:36,197][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:13:45,244][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 13:13:45,245][257371] FPS: 424496.60[0m
[36m[2023-07-17 13:13:45,247][257371] itr=1260, itrs=2000, Progress: 63.00%[0m
[37m[1m[2023-07-17 13:17:15,085][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001240[0m
[36m[2023-07-17 13:17:27,092][257371] train() took 11.47 seconds to complete[0m
[36m[2023-07-17 13:17:27,092][257371] FPS: 334766.20[0m
[36m[2023-07-17 13:17:31,317][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:17:31,317][257371] Reward + Measures: [[500.56676005   0.19964033   0.68867075   0.33935466   0.69825029
    5.13190413]][0m
[37m[1m[2023-07-17 13:17:31,317][257371] Max Reward on eval: 500.56676005431154[0m
[37m[1m[2023-07-17 13:17:31,318][257371] Min Reward on eval: 500.56676005431154[0m
[37m[1m[2023-07-17 13:17:31,318][257371] Mean Reward across all agents: 500.56676005431154[0m
[37m[1m[2023-07-17 13:17:31,318][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:17:36,318][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:17:36,318][257371] Reward + Measures: [[549.76522063   0.36719999   0.73299998   0.23370002   0.73520005
    5.04614639]
 [619.82916262   0.1134       0.83530009   0.49610001   0.85580009
    5.68049192]
 [473.67907356   0.36270005   0.60330003   0.149        0.61610001
    4.82216597]
 ...
 [454.88170684   0.23899999   0.55879998   0.24349999   0.5995
    4.31925774]
 [407.88897425   0.2096       0.48200002   0.19220002   0.49720001
    4.35535765]
 [380.796267     0.1726       0.65140003   0.32300001   0.64410001
    4.90333176]][0m
[37m[1m[2023-07-17 13:17:36,319][257371] Max Reward on eval: 739.275203728117[0m
[37m[1m[2023-07-17 13:17:36,319][257371] Min Reward on eval: 132.10555954426528[0m
[37m[1m[2023-07-17 13:17:36,319][257371] Mean Reward across all agents: 461.6001965883803[0m
[37m[1m[2023-07-17 13:17:36,320][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:17:36,322][257371] mean_value=-123.2382162474186, max_value=320.2283592251094[0m
[37m[1m[2023-07-17 13:17:36,325][257371] New mean coefficients: [[ 1.4855982  -0.25910878 -0.3226124  -0.50238377 -0.43095568 -0.5437528 ]][0m
[37m[1m[2023-07-17 13:17:36,326][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:17:45,548][257371] train() took 9.22 seconds to complete[0m
[36m[2023-07-17 13:17:45,549][257371] FPS: 416453.05[0m
[36m[2023-07-17 13:17:45,551][257371] itr=1261, itrs=2000, Progress: 63.05%[0m
[36m[2023-07-17 13:17:57,463][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 13:17:57,463][257371] FPS: 325609.19[0m
[36m[2023-07-17 13:18:01,799][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:18:01,800][257371] Reward + Measures: [[488.31109874   0.21440333   0.65557033   0.30899534   0.66551268
    5.00031281]][0m
[37m[1m[2023-07-17 13:18:01,800][257371] Max Reward on eval: 488.31109874272147[0m
[37m[1m[2023-07-17 13:18:01,800][257371] Min Reward on eval: 488.31109874272147[0m
[37m[1m[2023-07-17 13:18:01,800][257371] Mean Reward across all agents: 488.31109874272147[0m
[37m[1m[2023-07-17 13:18:01,801][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:18:06,794][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:18:06,794][257371] Reward + Measures: [[444.65852306   0.2105       0.56820005   0.27020001   0.58639997
    4.50828218]
 [343.42370534   0.14930001   0.56230003   0.28920001   0.55790007
    4.93884754]
 [474.55769381   0.31720001   0.60339999   0.207        0.63360006
    4.76585913]
 ...
 [602.48565479   0.42379999   0.72869992   0.19149999   0.73130006
    5.24193525]
 [455.55386598   0.30430001   0.713        0.2323       0.71950001
    5.03272247]
 [279.89884759   0.1724       0.35240003   0.182        0.37079999
    4.22510672]][0m
[37m[1m[2023-07-17 13:18:06,795][257371] Max Reward on eval: 678.5511589106521[0m
[37m[1m[2023-07-17 13:18:06,795][257371] Min Reward on eval: 178.7514547197381[0m
[37m[1m[2023-07-17 13:18:06,795][257371] Mean Reward across all agents: 449.72525695537[0m
[37m[1m[2023-07-17 13:18:06,795][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:18:06,798][257371] mean_value=-115.53876966449172, max_value=199.8756342074285[0m
[37m[1m[2023-07-17 13:18:06,806][257371] New mean coefficients: [[ 1.4579322  -0.64684415 -0.12787554 -0.2162461  -0.01312491 -0.96268535]][0m
[37m[1m[2023-07-17 13:18:06,807][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:18:15,802][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 13:18:15,802][257371] FPS: 426997.98[0m
[36m[2023-07-17 13:18:15,804][257371] itr=1262, itrs=2000, Progress: 63.10%[0m
[36m[2023-07-17 13:18:27,554][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 13:18:27,554][257371] FPS: 330134.99[0m
[36m[2023-07-17 13:18:31,771][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:18:31,771][257371] Reward + Measures: [[482.47079704   0.21550134   0.63226098   0.29450369   0.6437096
    4.87631416]][0m
[37m[1m[2023-07-17 13:18:31,772][257371] Max Reward on eval: 482.47079704004915[0m
[37m[1m[2023-07-17 13:18:31,772][257371] Min Reward on eval: 482.47079704004915[0m
[37m[1m[2023-07-17 13:18:31,772][257371] Mean Reward across all agents: 482.47079704004915[0m
[37m[1m[2023-07-17 13:18:31,772][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:18:36,730][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:18:36,731][257371] Reward + Measures: [[504.50933151   0.15100001   0.76260006   0.40900001   0.7651
    5.35257959]
 [409.80058293   0.37550002   0.75260001   0.21589999   0.75419998
    4.71285105]
 [534.92708964   0.45190001   0.59689999   0.1086       0.57779998
    4.93937731]
 ...
 [365.86975277   0.1257       0.52310002   0.3161       0.53240001
    4.84156179]
 [401.55289673   0.21710001   0.62900001   0.26470003   0.6401
    4.65029573]
 [398.17826749   0.1655       0.62919998   0.2881       0.65060002
    4.94112444]][0m
[37m[1m[2023-07-17 13:18:36,731][257371] Max Reward on eval: 674.8559990416746[0m
[37m[1m[2023-07-17 13:18:36,731][257371] Min Reward on eval: 245.9080074356869[0m
[37m[1m[2023-07-17 13:18:36,731][257371] Mean Reward across all agents: 456.78916090953095[0m
[37m[1m[2023-07-17 13:18:36,732][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:18:36,734][257371] mean_value=-114.93008752333265, max_value=210.38080229751682[0m
[37m[1m[2023-07-17 13:18:36,737][257371] New mean coefficients: [[ 1.0681357  -0.93933827 -0.31613144 -0.11682913 -0.08513643 -0.9793471 ]][0m
[37m[1m[2023-07-17 13:18:36,738][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:18:45,751][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 13:18:45,751][257371] FPS: 426130.49[0m
[36m[2023-07-17 13:18:45,753][257371] itr=1263, itrs=2000, Progress: 63.15%[0m
[36m[2023-07-17 13:18:57,503][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 13:18:57,503][257371] FPS: 330030.41[0m
[36m[2023-07-17 13:19:01,857][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:19:01,857][257371] Reward + Measures: [[480.12143527   0.23092966   0.60927165   0.27181566   0.61928368
    4.77741385]][0m
[37m[1m[2023-07-17 13:19:01,858][257371] Max Reward on eval: 480.12143526943[0m
[37m[1m[2023-07-17 13:19:01,858][257371] Min Reward on eval: 480.12143526943[0m
[37m[1m[2023-07-17 13:19:01,858][257371] Mean Reward across all agents: 480.12143526943[0m
[37m[1m[2023-07-17 13:19:01,859][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:19:06,889][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:19:06,895][257371] Reward + Measures: [[382.89287316   0.1749       0.61330003   0.26820001   0.63500005
    4.66750669]
 [668.48980184   0.48719999   0.73320001   0.13590001   0.71020001
    4.91188288]
 [427.18154444   0.23629999   0.59139997   0.25750002   0.59579998
    4.97002268]
 ...
 [550.39544603   0.1594       0.76740003   0.4179       0.76699996
    5.42885303]
 [514.53115084   0.206        0.72119999   0.35389999   0.69970006
    5.31774378]
 [434.24519568   0.1442       0.64790004   0.35829997   0.65150005
    5.29555082]][0m
[37m[1m[2023-07-17 13:19:06,895][257371] Max Reward on eval: 737.5422210753895[0m
[37m[1m[2023-07-17 13:19:06,895][257371] Min Reward on eval: 208.42837711083703[0m
[37m[1m[2023-07-17 13:19:06,896][257371] Mean Reward across all agents: 494.66370102185715[0m
[37m[1m[2023-07-17 13:19:06,896][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:19:06,899][257371] mean_value=-98.66576698274469, max_value=195.46913520029483[0m
[37m[1m[2023-07-17 13:19:06,902][257371] New mean coefficients: [[ 1.4113916  -0.8165849  -0.33333707  0.00854217 -0.19066629 -0.644318  ]][0m
[37m[1m[2023-07-17 13:19:06,903][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:19:15,943][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 13:19:15,943][257371] FPS: 424867.53[0m
[36m[2023-07-17 13:19:15,946][257371] itr=1264, itrs=2000, Progress: 63.20%[0m
[36m[2023-07-17 13:19:27,956][257371] train() took 11.89 seconds to complete[0m
[36m[2023-07-17 13:19:27,957][257371] FPS: 322834.91[0m
[36m[2023-07-17 13:19:32,237][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:19:32,238][257371] Reward + Measures: [[487.37167337   0.24008468   0.59112436   0.26032966   0.60309666
    4.7064724 ]][0m
[37m[1m[2023-07-17 13:19:32,238][257371] Max Reward on eval: 487.3716733715157[0m
[37m[1m[2023-07-17 13:19:32,238][257371] Min Reward on eval: 487.3716733715157[0m
[37m[1m[2023-07-17 13:19:32,238][257371] Mean Reward across all agents: 487.3716733715157[0m
[37m[1m[2023-07-17 13:19:32,239][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:19:37,248][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:19:37,248][257371] Reward + Measures: [[392.74622691   0.45370004   0.47410002   0.1285       0.47500005
    4.2722249 ]
 [560.18128201   0.35770002   0.83829993   0.29700002   0.82340002
    5.48778629]
 [381.57831654   0.3547       0.35789999   0.13950001   0.38299999
    4.05589151]
 ...
 [122.16300037   0.43829998   0.36350003   0.41940004   0.23050001
    5.60396814]
 [508.4014683    0.37200001   0.37750003   0.1734       0.44840002
    3.71353412]
 [ 63.33801124   0.40130001   0.63079995   0.39109999   0.42199999
    5.87645102]][0m
[37m[1m[2023-07-17 13:19:37,249][257371] Max Reward on eval: 756.765731778834[0m
[37m[1m[2023-07-17 13:19:37,249][257371] Min Reward on eval: -187.033259934932[0m
[37m[1m[2023-07-17 13:19:37,249][257371] Mean Reward across all agents: 286.82674530096205[0m
[37m[1m[2023-07-17 13:19:37,250][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:19:37,252][257371] mean_value=-323.024210473957, max_value=309.89420555403774[0m
[37m[1m[2023-07-17 13:19:37,255][257371] New mean coefficients: [[ 1.6346143  -0.17649025 -0.22119705  1.0109806  -0.39118224 -0.5145973 ]][0m
[37m[1m[2023-07-17 13:19:37,256][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:19:46,265][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 13:19:46,265][257371] FPS: 426300.85[0m
[36m[2023-07-17 13:19:46,268][257371] itr=1265, itrs=2000, Progress: 63.25%[0m
[36m[2023-07-17 13:19:58,029][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 13:19:58,029][257371] FPS: 329699.15[0m
[36m[2023-07-17 13:20:02,340][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:20:02,340][257371] Reward + Measures: [[492.52127809   0.24662597   0.56437296   0.25176033   0.57797068
    4.60572577]][0m
[37m[1m[2023-07-17 13:20:02,340][257371] Max Reward on eval: 492.52127808866294[0m
[37m[1m[2023-07-17 13:20:02,341][257371] Min Reward on eval: 492.52127808866294[0m
[37m[1m[2023-07-17 13:20:02,341][257371] Mean Reward across all agents: 492.52127808866294[0m
[37m[1m[2023-07-17 13:20:02,341][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:20:07,385][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:20:07,390][257371] Reward + Measures: [[447.26527595   0.26350003   0.45409998   0.19229999   0.46300003
    4.12383127]
 [572.7409854    0.26500002   0.67629999   0.28799999   0.67790002
    5.02040243]
 [341.75927947   0.22889999   0.41599998   0.20469999   0.45360002
    4.43735075]
 ...
 [334.98885873   0.28209999   0.55650002   0.22589998   0.55050004
    4.93756628]
 [530.47632977   0.17110001   0.72640002   0.41549999   0.74540007
    5.01737213]
 [562.83951042   0.27980003   0.56330001   0.2237       0.58380002
    4.4756546 ]][0m
[37m[1m[2023-07-17 13:20:07,391][257371] Max Reward on eval: 728.5476074217819[0m
[37m[1m[2023-07-17 13:20:07,391][257371] Min Reward on eval: 232.84172796211206[0m
[37m[1m[2023-07-17 13:20:07,391][257371] Mean Reward across all agents: 488.3795126479631[0m
[37m[1m[2023-07-17 13:20:07,392][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:20:07,394][257371] mean_value=-123.10979287046662, max_value=109.31137495598625[0m
[37m[1m[2023-07-17 13:20:07,397][257371] New mean coefficients: [[ 1.791369   0.0514285 -0.1421973  1.6823583 -0.881451  -0.5344392]][0m
[37m[1m[2023-07-17 13:20:07,398][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:20:16,489][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 13:20:16,489][257371] FPS: 422486.35[0m
[36m[2023-07-17 13:20:16,491][257371] itr=1266, itrs=2000, Progress: 63.30%[0m
[36m[2023-07-17 13:20:28,281][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 13:20:28,281][257371] FPS: 328887.24[0m
[36m[2023-07-17 13:20:32,517][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:20:32,518][257371] Reward + Measures: [[508.74349891   0.26299834   0.53731138   0.23410034   0.55460733
    4.48841286]][0m
[37m[1m[2023-07-17 13:20:32,518][257371] Max Reward on eval: 508.74349890785425[0m
[37m[1m[2023-07-17 13:20:32,518][257371] Min Reward on eval: 508.74349890785425[0m
[37m[1m[2023-07-17 13:20:32,518][257371] Mean Reward across all agents: 508.74349890785425[0m
[37m[1m[2023-07-17 13:20:32,519][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:20:37,431][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:20:37,432][257371] Reward + Measures: [[503.98032597   0.2411       0.52860004   0.24690001   0.56099999
    4.27863503]
 [229.60858703   0.3229       0.39489999   0.14740001   0.4066
    4.16948557]
 [493.34216027   0.3272       0.55419999   0.2033       0.53980005
    4.63309669]
 ...
 [502.65937557   0.29260001   0.4499       0.1682       0.47849998
    4.46943617]
 [531.101038     0.29769999   0.66209996   0.25940001   0.66829997
    4.74327564]
 [265.0727696    0.41680002   0.72200006   0.3371       0.60799998
    5.43341494]][0m
[37m[1m[2023-07-17 13:20:37,432][257371] Max Reward on eval: 660.3157729920931[0m
[37m[1m[2023-07-17 13:20:37,432][257371] Min Reward on eval: -303.7150631745346[0m
[37m[1m[2023-07-17 13:20:37,432][257371] Mean Reward across all agents: 346.8861538325742[0m
[37m[1m[2023-07-17 13:20:37,433][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:20:37,435][257371] mean_value=-324.5113893305221, max_value=181.66117111922034[0m
[37m[1m[2023-07-17 13:20:37,438][257371] New mean coefficients: [[ 1.9981136   1.1902635  -0.478195    1.6949377  -1.1383994  -0.42352706]][0m
[37m[1m[2023-07-17 13:20:37,439][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:20:46,437][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 13:20:46,437][257371] FPS: 426813.83[0m
[36m[2023-07-17 13:20:46,439][257371] itr=1267, itrs=2000, Progress: 63.35%[0m
[36m[2023-07-17 13:20:58,098][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 13:20:58,098][257371] FPS: 332690.71[0m
[36m[2023-07-17 13:21:02,354][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:21:02,355][257371] Reward + Measures: [[519.80715585   0.27894166   0.51883399   0.21910034   0.53706568
    4.41120243]][0m
[37m[1m[2023-07-17 13:21:02,355][257371] Max Reward on eval: 519.8071558538912[0m
[37m[1m[2023-07-17 13:21:02,355][257371] Min Reward on eval: 519.8071558538912[0m
[37m[1m[2023-07-17 13:21:02,355][257371] Mean Reward across all agents: 519.8071558538912[0m
[37m[1m[2023-07-17 13:21:02,356][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:21:07,564][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:21:07,565][257371] Reward + Measures: [[308.39180168   0.21040002   0.33470002   0.1681       0.3682
    4.00399542]
 [534.06144272   0.31640002   0.43340001   0.1478       0.44560003
    4.02305889]
 [575.91230008   0.37220004   0.59740007   0.21589999   0.60390002
    4.33401871]
 ...
 [404.5507485    0.2168       0.43140003   0.1969       0.43979999
    4.32511425]
 [469.58327981   0.28980002   0.48160002   0.18740001   0.52210003
    4.2938242 ]
 [518.63513471   0.44350004   0.55030006   0.101        0.53909999
    4.52039814]][0m
[37m[1m[2023-07-17 13:21:07,565][257371] Max Reward on eval: 739.3065528388136[0m
[37m[1m[2023-07-17 13:21:07,565][257371] Min Reward on eval: 75.3411394129449[0m
[37m[1m[2023-07-17 13:21:07,566][257371] Mean Reward across all agents: 483.28845487266517[0m
[37m[1m[2023-07-17 13:21:07,566][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:21:07,568][257371] mean_value=-170.5339504334518, max_value=141.66318444152103[0m
[37m[1m[2023-07-17 13:21:07,571][257371] New mean coefficients: [[ 1.7006631   1.5472437  -0.36893123  1.2637628  -0.94445086 -0.65029514]][0m
[37m[1m[2023-07-17 13:21:07,572][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:21:16,555][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 13:21:16,555][257371] FPS: 427569.11[0m
[36m[2023-07-17 13:21:16,557][257371] itr=1268, itrs=2000, Progress: 63.40%[0m
[36m[2023-07-17 13:21:28,285][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 13:21:28,285][257371] FPS: 330653.09[0m
[36m[2023-07-17 13:21:32,601][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:21:32,602][257371] Reward + Measures: [[523.08185917   0.2846033    0.48408234   0.20512      0.50501198
    4.284513  ]][0m
[37m[1m[2023-07-17 13:21:32,602][257371] Max Reward on eval: 523.0818591680556[0m
[37m[1m[2023-07-17 13:21:32,602][257371] Min Reward on eval: 523.0818591680556[0m
[37m[1m[2023-07-17 13:21:32,603][257371] Mean Reward across all agents: 523.0818591680556[0m
[37m[1m[2023-07-17 13:21:32,603][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:21:37,646][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:21:37,646][257371] Reward + Measures: [[233.07053575   0.347        0.70030004   0.345        0.62060004
    5.59528017]
 [476.21986961   0.44400001   0.42500001   0.182        0.48450002
    4.25608635]
 [577.34979249   0.2658       0.63020003   0.27989998   0.63440001
    4.92435503]
 ...
 [-72.14603208   0.78159994   0.91380006   0.1367       0.86490005
    6.13641214]
 [526.2346601    0.1992       0.78200001   0.41559997   0.76570004
    5.71333504]
 [532.33512348   0.32070002   0.54810005   0.22819999   0.55800003
    4.29327011]][0m
[37m[1m[2023-07-17 13:21:37,646][257371] Max Reward on eval: 707.8998279578227[0m
[37m[1m[2023-07-17 13:21:37,647][257371] Min Reward on eval: -280.9703762958408[0m
[37m[1m[2023-07-17 13:21:37,647][257371] Mean Reward across all agents: 279.13409732291007[0m
[37m[1m[2023-07-17 13:21:37,647][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:21:37,650][257371] mean_value=-477.22469894464484, max_value=207.1497327950808[0m
[37m[1m[2023-07-17 13:21:37,653][257371] New mean coefficients: [[ 1.6466906   1.1981598  -0.56608796  0.4206633  -0.59882104 -0.7412092 ]][0m
[37m[1m[2023-07-17 13:21:37,654][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:21:46,719][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 13:21:46,719][257371] FPS: 423686.60[0m
[36m[2023-07-17 13:21:46,721][257371] itr=1269, itrs=2000, Progress: 63.45%[0m
[36m[2023-07-17 13:21:58,564][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 13:21:58,564][257371] FPS: 327382.00[0m
[36m[2023-07-17 13:22:02,887][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:22:02,887][257371] Reward + Measures: [[522.53411726   0.28545201   0.47915465   0.20417066   0.49982935
    4.2693553 ]][0m
[37m[1m[2023-07-17 13:22:02,887][257371] Max Reward on eval: 522.5341172638425[0m
[37m[1m[2023-07-17 13:22:02,888][257371] Min Reward on eval: 522.5341172638425[0m
[37m[1m[2023-07-17 13:22:02,888][257371] Mean Reward across all agents: 522.5341172638425[0m
[37m[1m[2023-07-17 13:22:02,888][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:22:07,951][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:22:07,951][257371] Reward + Measures: [[436.25505255   0.4014       0.41140005   0.19050001   0.4206
    4.24384212]
 [ 51.73751683   0.15900001   0.15540001   0.0855       0.1496
    4.15388536]
 [167.46024413   0.42719999   0.84009999   0.40489998   0.68250006
    5.80681229]
 ...
 [430.12057403   0.43800002   0.46479997   0.19840001   0.46940002
    4.34476614]
 [423.49611233   0.39229998   0.4763       0.15280001   0.49119997
    4.3827424 ]
 [428.02880477   0.36929998   0.4513       0.16690001   0.46089998
    4.18587446]][0m
[37m[1m[2023-07-17 13:22:07,951][257371] Max Reward on eval: 690.9878082298267[0m
[37m[1m[2023-07-17 13:22:07,952][257371] Min Reward on eval: -207.69980271253735[0m
[37m[1m[2023-07-17 13:22:07,952][257371] Mean Reward across all agents: 292.3913245085473[0m
[37m[1m[2023-07-17 13:22:07,952][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:22:07,954][257371] mean_value=-543.2616657330998, max_value=148.4920343994434[0m
[37m[1m[2023-07-17 13:22:07,957][257371] New mean coefficients: [[ 1.7036299   0.9456471  -0.6313986  -0.2191531  -0.29074916 -0.83646494]][0m
[37m[1m[2023-07-17 13:22:07,958][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:22:17,014][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 13:22:17,015][257371] FPS: 424063.39[0m
[36m[2023-07-17 13:22:17,017][257371] itr=1270, itrs=2000, Progress: 63.50%[0m
[37m[1m[2023-07-17 13:26:04,722][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001250[0m
[36m[2023-07-17 13:26:16,912][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 13:26:16,912][257371] FPS: 331814.02[0m
[36m[2023-07-17 13:26:21,179][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:26:21,180][257371] Reward + Measures: [[523.00085248   0.28876433   0.45241299   0.19258635   0.47239599
    4.19720125]][0m
[37m[1m[2023-07-17 13:26:21,180][257371] Max Reward on eval: 523.0008524752617[0m
[37m[1m[2023-07-17 13:26:21,180][257371] Min Reward on eval: 523.0008524752617[0m
[37m[1m[2023-07-17 13:26:21,181][257371] Mean Reward across all agents: 523.0008524752617[0m
[37m[1m[2023-07-17 13:26:21,181][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:26:26,097][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:26:26,098][257371] Reward + Measures: [[356.56068487   0.27870002   0.45450002   0.16629998   0.46619996
    4.46555424]
 [354.49771784   0.1697       0.25209999   0.14400001   0.2789
    3.72687721]
 [723.5367127    0.3348       0.5952       0.23469999   0.63639998
    4.33909082]
 ...
 [614.2259769    0.38260004   0.56650001   0.2007       0.56830007
    4.29806519]
 [654.22616574   0.50879997   0.68329996   0.14719999   0.67340004
    4.71464872]
 [509.58531646   0.36980003   0.56         0.1557       0.56849998
    4.56678391]][0m
[37m[1m[2023-07-17 13:26:26,098][257371] Max Reward on eval: 743.4007911382243[0m
[37m[1m[2023-07-17 13:26:26,098][257371] Min Reward on eval: 255.4046363933012[0m
[37m[1m[2023-07-17 13:26:26,098][257371] Mean Reward across all agents: 499.98686015532786[0m
[37m[1m[2023-07-17 13:26:26,099][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:26:26,101][257371] mean_value=-255.684880882831, max_value=425.0736508144885[0m
[37m[1m[2023-07-17 13:26:26,104][257371] New mean coefficients: [[ 1.8505424   0.70069015 -0.5996923  -0.2209136  -0.3626715  -0.84774756]][0m
[37m[1m[2023-07-17 13:26:26,104][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:26:34,973][257371] train() took 8.87 seconds to complete[0m
[36m[2023-07-17 13:26:34,974][257371] FPS: 433048.26[0m
[36m[2023-07-17 13:26:34,976][257371] itr=1271, itrs=2000, Progress: 63.55%[0m
[36m[2023-07-17 13:26:46,780][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 13:26:46,780][257371] FPS: 328539.33[0m
[36m[2023-07-17 13:26:51,160][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:26:51,161][257371] Reward + Measures: [[526.50642762   0.29329768   0.45228031   0.19143432   0.46947765
    4.20369673]][0m
[37m[1m[2023-07-17 13:26:51,161][257371] Max Reward on eval: 526.5064276235694[0m
[37m[1m[2023-07-17 13:26:51,161][257371] Min Reward on eval: 526.5064276235694[0m
[37m[1m[2023-07-17 13:26:51,162][257371] Mean Reward across all agents: 526.5064276235694[0m
[37m[1m[2023-07-17 13:26:51,162][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:26:56,389][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:26:56,389][257371] Reward + Measures: [[ 38.85562361   0.35730001   0.22980002   0.32510003   0.2191
    3.87384224]
 [452.88978103   0.28850001   0.39050004   0.17110001   0.40039998
    4.04239368]
 [553.71733256   0.36360002   0.41799998   0.19490002   0.456
    3.51303029]
 ...
 [183.86994771   0.41580001   0.40720001   0.34990001   0.3089
    3.61208129]
 [-24.36986642   0.42840001   0.33329999   0.34839997   0.2692
    3.87936783]
 [382.95110487   0.2474       0.36360002   0.13060001   0.3759
    4.22804689]][0m
[37m[1m[2023-07-17 13:26:56,390][257371] Max Reward on eval: 762.2638013782911[0m
[37m[1m[2023-07-17 13:26:56,390][257371] Min Reward on eval: -30.713209106354043[0m
[37m[1m[2023-07-17 13:26:56,390][257371] Mean Reward across all agents: 402.3428629945487[0m
[37m[1m[2023-07-17 13:26:56,390][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:26:56,393][257371] mean_value=-938.4698668731048, max_value=118.73089752830435[0m
[37m[1m[2023-07-17 13:26:56,395][257371] New mean coefficients: [[ 1.4261411   0.3497521  -0.5526948   0.40836763 -0.16609013 -0.66743994]][0m
[37m[1m[2023-07-17 13:26:56,396][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:27:05,410][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 13:27:05,410][257371] FPS: 426082.27[0m
[36m[2023-07-17 13:27:05,413][257371] itr=1272, itrs=2000, Progress: 63.60%[0m
[36m[2023-07-17 13:27:17,255][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 13:27:17,255][257371] FPS: 327511.74[0m
[36m[2023-07-17 13:27:21,494][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:27:21,494][257371] Reward + Measures: [[530.72786812   0.29765266   0.44080201   0.18924198   0.45939299
    4.16989183]][0m
[37m[1m[2023-07-17 13:27:21,494][257371] Max Reward on eval: 530.7278681219959[0m
[37m[1m[2023-07-17 13:27:21,495][257371] Min Reward on eval: 530.7278681219959[0m
[37m[1m[2023-07-17 13:27:21,495][257371] Mean Reward across all agents: 530.7278681219959[0m
[37m[1m[2023-07-17 13:27:21,495][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:27:26,381][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:27:26,382][257371] Reward + Measures: [[629.71499948   0.35810003   0.45820004   0.1768       0.48119998
    4.10114717]
 [441.11710854   0.31740001   0.4542       0.17490001   0.44750005
    4.30549335]
 [451.21449834   0.24530001   0.29700002   0.15089999   0.32930002
    3.77769279]
 ...
 [547.90051461   0.34590003   0.44600001   0.1672       0.47130004
    3.98671722]
 [425.5784924    0.29190001   0.352        0.13270001   0.3786
    4.12849236]
 [510.52744985   0.30850002   0.44689998   0.177        0.42010003
    4.33881617]][0m
[37m[1m[2023-07-17 13:27:26,382][257371] Max Reward on eval: 778.2071380428039[0m
[37m[1m[2023-07-17 13:27:26,382][257371] Min Reward on eval: 269.86382604839747[0m
[37m[1m[2023-07-17 13:27:26,382][257371] Mean Reward across all agents: 545.6400809551329[0m
[37m[1m[2023-07-17 13:27:26,383][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:27:26,386][257371] mean_value=-476.0243540155625, max_value=156.6835113806809[0m
[37m[1m[2023-07-17 13:27:26,388][257371] New mean coefficients: [[ 2.1182075   0.13370575 -0.5003522   0.21547474  0.23357505 -0.09315622]][0m
[37m[1m[2023-07-17 13:27:26,389][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:27:35,372][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 13:27:35,373][257371] FPS: 427549.02[0m
[36m[2023-07-17 13:27:35,375][257371] itr=1273, itrs=2000, Progress: 63.65%[0m
[36m[2023-07-17 13:27:47,306][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 13:27:47,306][257371] FPS: 325063.75[0m
[36m[2023-07-17 13:27:51,595][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:27:51,596][257371] Reward + Measures: [[532.63437974   0.31218567   0.43256599   0.17570032   0.44994235
    4.16782761]][0m
[37m[1m[2023-07-17 13:27:51,596][257371] Max Reward on eval: 532.6343797403257[0m
[37m[1m[2023-07-17 13:27:51,596][257371] Min Reward on eval: 532.6343797403257[0m
[37m[1m[2023-07-17 13:27:51,597][257371] Mean Reward across all agents: 532.6343797403257[0m
[37m[1m[2023-07-17 13:27:51,597][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:27:56,586][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:27:56,586][257371] Reward + Measures: [[653.27860692   0.42589998   0.52710003   0.1605       0.54520005
    4.30429792]
 [520.45921007   0.35409999   0.40990001   0.141        0.40459999
    4.17983103]
 [654.94522855   0.2843       0.55719995   0.25970003   0.58669996
    4.39881849]
 ...
 [538.65228511   0.34310004   0.53670001   0.1919       0.57120001
    4.57878256]
 [579.69383161   0.39489999   0.63620001   0.1969       0.62970001
    4.73481989]
 [561.08821505   0.44700003   0.55080003   0.1168       0.56269997
    4.72829676]][0m
[37m[1m[2023-07-17 13:27:56,587][257371] Max Reward on eval: 757.77690889386[0m
[37m[1m[2023-07-17 13:27:56,587][257371] Min Reward on eval: 138.06936662192456[0m
[37m[1m[2023-07-17 13:27:56,587][257371] Mean Reward across all agents: 525.1469398087063[0m
[37m[1m[2023-07-17 13:27:56,587][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:27:56,590][257371] mean_value=-469.12069014833384, max_value=178.3635584561556[0m
[37m[1m[2023-07-17 13:27:56,593][257371] New mean coefficients: [[ 2.1682632  -0.13130139 -0.7497966   0.13460933  0.21993937 -0.1904882 ]][0m
[37m[1m[2023-07-17 13:27:56,594][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:28:05,710][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 13:28:05,710][257371] FPS: 421324.90[0m
[36m[2023-07-17 13:28:05,712][257371] itr=1274, itrs=2000, Progress: 63.70%[0m
[36m[2023-07-17 13:28:17,545][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 13:28:17,545][257371] FPS: 327706.70[0m
[36m[2023-07-17 13:28:21,784][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:28:21,784][257371] Reward + Measures: [[541.54233317   0.30374834   0.406955     0.17639433   0.43020064
    4.04792833]][0m
[37m[1m[2023-07-17 13:28:21,784][257371] Max Reward on eval: 541.5423331656665[0m
[37m[1m[2023-07-17 13:28:21,785][257371] Min Reward on eval: 541.5423331656665[0m
[37m[1m[2023-07-17 13:28:21,785][257371] Mean Reward across all agents: 541.5423331656665[0m
[37m[1m[2023-07-17 13:28:21,785][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:28:26,782][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:28:26,783][257371] Reward + Measures: [[520.5375696    0.32949999   0.40790001   0.16290002   0.4127
    3.83878684]
 [542.22012241   0.33129999   0.48480001   0.24759999   0.51130003
    4.11820078]
 [587.78722279   0.27070001   0.37970001   0.20190001   0.4068
    3.6240499 ]
 ...
 [569.1329427    0.28099999   0.3804       0.1705       0.39380002
    3.99666286]
 [612.66618905   0.37919998   0.49740002   0.17990001   0.52069998
    4.10730076]
 [519.84854853   0.31759998   0.52649999   0.2062       0.54460001
    4.48759556]][0m
[37m[1m[2023-07-17 13:28:26,783][257371] Max Reward on eval: 835.8744163587689[0m
[37m[1m[2023-07-17 13:28:26,783][257371] Min Reward on eval: 285.4469166378491[0m
[37m[1m[2023-07-17 13:28:26,784][257371] Mean Reward across all agents: 533.6079260202389[0m
[37m[1m[2023-07-17 13:28:26,784][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:28:26,786][257371] mean_value=-549.5206971074401, max_value=191.96516531684472[0m
[37m[1m[2023-07-17 13:28:26,789][257371] New mean coefficients: [[ 2.0570965  -0.14113373 -0.8119376   0.28317705  0.28098705 -0.89105034]][0m
[37m[1m[2023-07-17 13:28:26,790][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:28:35,725][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 13:28:35,725][257371] FPS: 429851.82[0m
[36m[2023-07-17 13:28:35,727][257371] itr=1275, itrs=2000, Progress: 63.75%[0m
[36m[2023-07-17 13:28:47,451][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 13:28:47,452][257371] FPS: 330855.10[0m
[36m[2023-07-17 13:28:51,800][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:28:51,800][257371] Reward + Measures: [[561.59059038   0.31755766   0.42033032   0.173287     0.44130468
    4.06228209]][0m
[37m[1m[2023-07-17 13:28:51,801][257371] Max Reward on eval: 561.590590378232[0m
[37m[1m[2023-07-17 13:28:51,801][257371] Min Reward on eval: 561.590590378232[0m
[37m[1m[2023-07-17 13:28:51,801][257371] Mean Reward across all agents: 561.590590378232[0m
[37m[1m[2023-07-17 13:28:51,802][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:28:56,827][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:28:56,828][257371] Reward + Measures: [[670.49598693   0.33449998   0.55540001   0.2316       0.58350003
    4.25408077]
 [394.67478783   0.24569999   0.29879999   0.1362       0.30070001
    3.8159349 ]
 [506.43088056   0.2122       0.25870001   0.15210001   0.29260001
    3.53039479]
 ...
 [682.48205757   0.36620003   0.42980003   0.17410001   0.47960001
    3.88034511]
 [304.18016135   0.26270005   0.30609998   0.0954       0.30710003
    3.97841525]
 [459.28929413   0.22680001   0.27610001   0.14860001   0.29890001
    3.76063514]][0m
[37m[1m[2023-07-17 13:28:56,828][257371] Max Reward on eval: 829.9426879790611[0m
[37m[1m[2023-07-17 13:28:56,828][257371] Min Reward on eval: 240.11507495413534[0m
[37m[1m[2023-07-17 13:28:56,829][257371] Mean Reward across all agents: 539.3243375124135[0m
[37m[1m[2023-07-17 13:28:56,829][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:28:56,831][257371] mean_value=-1124.217531563757, max_value=139.62359131341543[0m
[37m[1m[2023-07-17 13:28:56,833][257371] New mean coefficients: [[ 1.898687   -0.1810891  -0.6236664   0.39024788  0.23965304 -0.72581637]][0m
[37m[1m[2023-07-17 13:28:56,834][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:29:05,945][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 13:29:05,946][257371] FPS: 421534.80[0m
[36m[2023-07-17 13:29:05,948][257371] itr=1276, itrs=2000, Progress: 63.80%[0m
[36m[2023-07-17 13:29:17,824][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 13:29:17,825][257371] FPS: 326548.14[0m
[36m[2023-07-17 13:29:22,134][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:29:22,135][257371] Reward + Measures: [[575.69463977   0.31413835   0.40986669   0.17330098   0.43363598
    4.01832533]][0m
[37m[1m[2023-07-17 13:29:22,135][257371] Max Reward on eval: 575.6946397663341[0m
[37m[1m[2023-07-17 13:29:22,135][257371] Min Reward on eval: 575.6946397663341[0m
[37m[1m[2023-07-17 13:29:22,136][257371] Mean Reward across all agents: 575.6946397663341[0m
[37m[1m[2023-07-17 13:29:22,136][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:29:27,161][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:29:27,161][257371] Reward + Measures: [[471.15251632   0.28690001   0.34430003   0.185        0.36180004
    3.95307708]
 [252.81181593   0.41019997   0.602        0.1804       0.59560007
    5.1572051 ]
 [324.32621002   0.30650002   0.28990003   0.2342       0.33230001
    4.03599453]
 ...
 [576.1306794    0.33410001   0.43870002   0.18969999   0.42649999
    4.10471058]
 [501.07493159   0.34170002   0.38760003   0.18279999   0.39380002
    4.01241302]
 [468.46613658   0.2014       0.26980001   0.17110001   0.31090003
    3.68649173]][0m
[37m[1m[2023-07-17 13:29:27,161][257371] Max Reward on eval: 794.8065490962938[0m
[37m[1m[2023-07-17 13:29:27,162][257371] Min Reward on eval: 6.303380536241457[0m
[37m[1m[2023-07-17 13:29:27,162][257371] Mean Reward across all agents: 401.09461999505487[0m
[37m[1m[2023-07-17 13:29:27,162][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:29:27,164][257371] mean_value=-1012.4294760035359, max_value=193.11492805890276[0m
[37m[1m[2023-07-17 13:29:27,167][257371] New mean coefficients: [[ 2.0417962  -0.9993016  -0.6969302   0.47839528  0.67433894 -0.46899185]][0m
[37m[1m[2023-07-17 13:29:27,168][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:29:36,286][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 13:29:36,287][257371] FPS: 421198.56[0m
[36m[2023-07-17 13:29:36,289][257371] itr=1277, itrs=2000, Progress: 63.85%[0m
[36m[2023-07-17 13:29:48,083][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 13:29:48,084][257371] FPS: 328751.20[0m
[36m[2023-07-17 13:29:52,290][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:29:52,290][257371] Reward + Measures: [[574.63801086   0.30646333   0.39706567   0.16997266   0.42194733
    3.96325397]][0m
[37m[1m[2023-07-17 13:29:52,290][257371] Max Reward on eval: 574.6380108589689[0m
[37m[1m[2023-07-17 13:29:52,291][257371] Min Reward on eval: 574.6380108589689[0m
[37m[1m[2023-07-17 13:29:52,291][257371] Mean Reward across all agents: 574.6380108589689[0m
[37m[1m[2023-07-17 13:29:52,291][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:29:57,524][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:29:57,524][257371] Reward + Measures: [[611.45803891   0.3193       0.39900002   0.1675       0.40179998
    3.86188769]
 [599.58230555   0.3184       0.39649999   0.15570001   0.39129996
    3.99930954]
 [453.75258467   0.33659998   0.39750001   0.11849999   0.4131
    3.96333098]
 ...
 [619.23119253   0.27309999   0.41590005   0.2089       0.44860002
    3.9540062 ]
 [357.36943494   0.2102       0.2395       0.13569999   0.24580002
    3.76800227]
 [666.35975721   0.32869998   0.41339999   0.16929999   0.45359999
    3.91989684]][0m
[37m[1m[2023-07-17 13:29:57,525][257371] Max Reward on eval: 822.5889663821087[0m
[37m[1m[2023-07-17 13:29:57,525][257371] Min Reward on eval: 252.64486892460846[0m
[37m[1m[2023-07-17 13:29:57,525][257371] Mean Reward across all agents: 560.1459794913862[0m
[37m[1m[2023-07-17 13:29:57,525][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:29:57,527][257371] mean_value=-852.8188641739671, max_value=206.7319100621704[0m
[37m[1m[2023-07-17 13:29:57,530][257371] New mean coefficients: [[ 2.3857052  -1.0289261  -0.6356851   0.56055814  0.6728871  -0.5428564 ]][0m
[37m[1m[2023-07-17 13:29:57,531][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:30:06,474][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 13:30:06,474][257371] FPS: 429475.47[0m
[36m[2023-07-17 13:30:06,476][257371] itr=1278, itrs=2000, Progress: 63.90%[0m
[36m[2023-07-17 13:30:18,150][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 13:30:18,150][257371] FPS: 332316.81[0m
[36m[2023-07-17 13:30:22,480][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:30:22,480][257371] Reward + Measures: [[584.87875157   0.30809301   0.39729866   0.16964133   0.42379001
    3.95338607]][0m
[37m[1m[2023-07-17 13:30:22,480][257371] Max Reward on eval: 584.8787515681747[0m
[37m[1m[2023-07-17 13:30:22,481][257371] Min Reward on eval: 584.8787515681747[0m
[37m[1m[2023-07-17 13:30:22,481][257371] Mean Reward across all agents: 584.8787515681747[0m
[37m[1m[2023-07-17 13:30:22,481][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:30:27,467][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:30:27,468][257371] Reward + Measures: [[322.02979421   0.34020001   0.41920003   0.24759999   0.3964
    3.65916419]
 [615.4044018    0.28950003   0.35300002   0.18099999   0.37690002
    3.68732262]
 [698.25905394   0.36750004   0.45830002   0.1973       0.46709999
    4.02627134]
 ...
 [545.79397228   0.40220004   0.49960002   0.11390001   0.50590003
    4.35239983]
 [548.23161223   0.34740004   0.46480003   0.1523       0.48180005
    4.11913633]
 [583.92578883   0.44159999   0.53920001   0.17990001   0.55340004
    4.09140682]][0m
[37m[1m[2023-07-17 13:30:27,468][257371] Max Reward on eval: 768.89178086631[0m
[37m[1m[2023-07-17 13:30:27,468][257371] Min Reward on eval: 267.99074286483227[0m
[37m[1m[2023-07-17 13:30:27,468][257371] Mean Reward across all agents: 550.7855211755325[0m
[37m[1m[2023-07-17 13:30:27,469][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:30:27,471][257371] mean_value=-1098.5292456776306, max_value=91.20959893880013[0m
[37m[1m[2023-07-17 13:30:27,473][257371] New mean coefficients: [[ 2.1656723  -0.4696085  -0.8358364   0.5283654   0.4909113  -0.65512073]][0m
[37m[1m[2023-07-17 13:30:27,474][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:30:36,506][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 13:30:36,506][257371] FPS: 425235.59[0m
[36m[2023-07-17 13:30:36,508][257371] itr=1279, itrs=2000, Progress: 63.95%[0m
[36m[2023-07-17 13:30:48,335][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 13:30:48,335][257371] FPS: 327882.77[0m
[36m[2023-07-17 13:30:52,655][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:30:52,656][257371] Reward + Measures: [[602.49621482   0.31052667   0.39164999   0.17116798   0.42208833
    3.9130156 ]][0m
[37m[1m[2023-07-17 13:30:52,656][257371] Max Reward on eval: 602.496214824948[0m
[37m[1m[2023-07-17 13:30:52,656][257371] Min Reward on eval: 602.496214824948[0m
[37m[1m[2023-07-17 13:30:52,657][257371] Mean Reward across all agents: 602.496214824948[0m
[37m[1m[2023-07-17 13:30:52,657][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:30:57,660][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:30:57,665][257371] Reward + Measures: [[498.80263904   0.2886       0.3475       0.12620001   0.36070001
    3.87785769]
 [429.96032139   0.24159999   0.36310002   0.1848       0.3696
    4.07734108]
 [432.2527265    0.3143       0.41480002   0.18179999   0.40120003
    4.22318029]
 ...
 [449.53067114   0.43540001   0.51229995   0.16850001   0.49570003
    4.46372747]
 [435.16147055   0.35980001   0.52069998   0.20879999   0.49999997
    4.69293928]
 [654.48912434   0.31039998   0.5693       0.31430003   0.58120006
    4.42606926]][0m
[37m[1m[2023-07-17 13:30:57,666][257371] Max Reward on eval: 829.4170074300637[0m
[37m[1m[2023-07-17 13:30:57,666][257371] Min Reward on eval: -182.01015292927622[0m
[37m[1m[2023-07-17 13:30:57,666][257371] Mean Reward across all agents: 477.4926947401239[0m
[37m[1m[2023-07-17 13:30:57,667][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:30:57,669][257371] mean_value=-507.92676435579284, max_value=110.42719949371485[0m
[37m[1m[2023-07-17 13:30:57,671][257371] New mean coefficients: [[ 1.3533299  -0.68324435 -0.70552266  0.7117714   0.57526875 -0.7274296 ]][0m
[37m[1m[2023-07-17 13:30:57,672][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:31:06,753][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 13:31:06,754][257371] FPS: 422945.38[0m
[36m[2023-07-17 13:31:06,756][257371] itr=1280, itrs=2000, Progress: 64.00%[0m
[37m[1m[2023-07-17 13:34:51,234][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001260[0m
[36m[2023-07-17 13:35:03,578][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 13:35:03,578][257371] FPS: 326689.00[0m
[36m[2023-07-17 13:35:07,841][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:35:07,842][257371] Reward + Measures: [[605.77843235   0.30866599   0.38560465   0.16953534   0.41770065
    3.88399863]][0m
[37m[1m[2023-07-17 13:35:07,842][257371] Max Reward on eval: 605.7784323477188[0m
[37m[1m[2023-07-17 13:35:07,842][257371] Min Reward on eval: 605.7784323477188[0m
[37m[1m[2023-07-17 13:35:07,842][257371] Mean Reward across all agents: 605.7784323477188[0m
[37m[1m[2023-07-17 13:35:07,843][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:35:12,828][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:35:12,829][257371] Reward + Measures: [[592.68304822   0.259        0.3441       0.27070004   0.39719999
    3.80339789]
 [409.37279605   0.29620001   0.34450004   0.1875       0.38180003
    3.97996759]
 [484.20051877   0.31940004   0.46539998   0.16990001   0.48440003
    4.37175035]
 ...
 [463.29977366   0.33470002   0.38580003   0.1056       0.39019999
    4.0783844 ]
 [682.37585258   0.25040001   0.32459998   0.20100001   0.41370001
    3.54880381]
 [665.59414288   0.33399999   0.4172       0.18910001   0.46170002
    3.88502121]][0m
[37m[1m[2023-07-17 13:35:12,829][257371] Max Reward on eval: 808.9095840512775[0m
[37m[1m[2023-07-17 13:35:12,829][257371] Min Reward on eval: 250.1140026975423[0m
[37m[1m[2023-07-17 13:35:12,829][257371] Mean Reward across all agents: 538.033782095029[0m
[37m[1m[2023-07-17 13:35:12,830][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:35:12,832][257371] mean_value=-763.5441523510218, max_value=111.80665973523998[0m
[37m[1m[2023-07-17 13:35:12,834][257371] New mean coefficients: [[ 1.7095453  -0.3200198  -0.7552859   0.47275227  0.36749613 -0.9393827 ]][0m
[37m[1m[2023-07-17 13:35:12,835][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:35:21,931][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 13:35:21,931][257371] FPS: 422248.42[0m
[36m[2023-07-17 13:35:21,933][257371] itr=1281, itrs=2000, Progress: 64.05%[0m
[36m[2023-07-17 13:35:33,881][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 13:35:33,881][257371] FPS: 324606.89[0m
[36m[2023-07-17 13:35:38,135][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:35:38,135][257371] Reward + Measures: [[619.94630943   0.30966631   0.37959301   0.16842      0.41260099
    3.82949924]][0m
[37m[1m[2023-07-17 13:35:38,136][257371] Max Reward on eval: 619.9463094261615[0m
[37m[1m[2023-07-17 13:35:38,136][257371] Min Reward on eval: 619.9463094261615[0m
[37m[1m[2023-07-17 13:35:38,136][257371] Mean Reward across all agents: 619.9463094261615[0m
[37m[1m[2023-07-17 13:35:38,136][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:35:43,381][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:35:43,388][257371] Reward + Measures: [[314.26822405   0.1673       0.19950001   0.1433       0.23860002
    3.4944551 ]
 [587.63782874   0.30930001   0.38580003   0.13649999   0.42090002
    3.94779515]
 [635.48760363   0.3524       0.41069999   0.16320001   0.42230001
    4.08279753]
 ...
 [573.13277349   0.3328       0.39570004   0.1313       0.4434
    4.04354858]
 [775.27053451   0.46680003   0.56720001   0.15270001   0.58569998
    4.29394531]
 [683.68894718   0.45670006   0.51329994   0.14189999   0.56000006
    4.04585409]][0m
[37m[1m[2023-07-17 13:35:43,389][257371] Max Reward on eval: 854.65438843281[0m
[37m[1m[2023-07-17 13:35:43,389][257371] Min Reward on eval: 222.40819699866697[0m
[37m[1m[2023-07-17 13:35:43,390][257371] Mean Reward across all agents: 578.2486952325438[0m
[37m[1m[2023-07-17 13:35:43,390][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:35:43,394][257371] mean_value=-1166.6469162870683, max_value=148.60114790137834[0m
[37m[1m[2023-07-17 13:35:43,399][257371] New mean coefficients: [[ 1.7196306  -0.5195334  -0.27130258  0.50414693  0.08525589 -0.7167363 ]][0m
[37m[1m[2023-07-17 13:35:43,401][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:35:52,339][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 13:35:52,339][257371] FPS: 429709.28[0m
[36m[2023-07-17 13:35:52,342][257371] itr=1282, itrs=2000, Progress: 64.10%[0m
[36m[2023-07-17 13:36:04,146][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 13:36:04,146][257371] FPS: 328506.90[0m
[36m[2023-07-17 13:36:08,421][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:36:08,421][257371] Reward + Measures: [[633.97602867   0.31539899   0.37953931   0.16879334   0.41284797
    3.818856  ]][0m
[37m[1m[2023-07-17 13:36:08,422][257371] Max Reward on eval: 633.9760286720381[0m
[37m[1m[2023-07-17 13:36:08,422][257371] Min Reward on eval: 633.9760286720381[0m
[37m[1m[2023-07-17 13:36:08,422][257371] Mean Reward across all agents: 633.9760286720381[0m
[37m[1m[2023-07-17 13:36:08,422][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:36:13,422][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:36:13,423][257371] Reward + Measures: [[511.04785581   0.25310001   0.29270002   0.15890001   0.33400002
    3.70467997]
 [630.81353737   0.40109998   0.46650004   0.139        0.4896
    4.08057499]
 [710.76928088   0.29700002   0.36170003   0.18249999   0.40100002
    3.75504684]
 ...
 [663.11596915   0.2784       0.3416       0.17         0.3987
    3.69040918]
 [585.79184413   0.3064       0.34720001   0.1777       0.40090004
    3.7963798 ]
 [462.72838588   0.23459999   0.27690002   0.14320001   0.2987
    3.67278409]][0m
[37m[1m[2023-07-17 13:36:13,423][257371] Max Reward on eval: 848.2952651537023[0m
[37m[1m[2023-07-17 13:36:13,423][257371] Min Reward on eval: 331.62054490572774[0m
[37m[1m[2023-07-17 13:36:13,424][257371] Mean Reward across all agents: 620.7154318641859[0m
[37m[1m[2023-07-17 13:36:13,424][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:36:13,426][257371] mean_value=-1444.047551656632, max_value=130.22092436865807[0m
[37m[1m[2023-07-17 13:36:13,429][257371] New mean coefficients: [[ 1.7186842  -1.1751156  -0.30839118  0.51716125  0.21621342 -1.2250378 ]][0m
[37m[1m[2023-07-17 13:36:13,430][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:36:22,485][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 13:36:22,486][257371] FPS: 424123.83[0m
[36m[2023-07-17 13:36:22,488][257371] itr=1283, itrs=2000, Progress: 64.15%[0m
[36m[2023-07-17 13:36:34,246][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 13:36:34,247][257371] FPS: 329816.57[0m
[36m[2023-07-17 13:36:38,529][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:36:38,529][257371] Reward + Measures: [[626.49392189   0.30700198   0.36618564   0.16906199   0.39981934
    3.77911949]][0m
[37m[1m[2023-07-17 13:36:38,530][257371] Max Reward on eval: 626.493921888733[0m
[37m[1m[2023-07-17 13:36:38,530][257371] Min Reward on eval: 626.493921888733[0m
[37m[1m[2023-07-17 13:36:38,530][257371] Mean Reward across all agents: 626.493921888733[0m
[37m[1m[2023-07-17 13:36:38,530][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:36:43,518][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:36:43,524][257371] Reward + Measures: [[753.64416505   0.36649999   0.45299998   0.1983       0.49470001
    3.78938556]
 [500.54729368   0.24330001   0.68770003   0.46059999   0.66620004
    4.76741791]
 [429.63548552   0.2388       0.29439998   0.22050002   0.3053
    3.62263942]
 ...
 [516.77299696   0.3488       0.4384       0.31020001   0.4824
    3.83824396]
 [633.14695593   0.255        0.30440003   0.1948       0.34039998
    3.46159291]
 [428.51916803   0.23669998   0.34209999   0.25559998   0.35780001
    3.85956836]][0m
[37m[1m[2023-07-17 13:36:43,525][257371] Max Reward on eval: 831.1901855821488[0m
[37m[1m[2023-07-17 13:36:43,525][257371] Min Reward on eval: -5.9371876826509835[0m
[37m[1m[2023-07-17 13:36:43,525][257371] Mean Reward across all agents: 501.74337942296813[0m
[37m[1m[2023-07-17 13:36:43,525][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:36:43,529][257371] mean_value=-971.2914513750502, max_value=416.2268035256253[0m
[37m[1m[2023-07-17 13:36:43,531][257371] New mean coefficients: [[ 2.1276875  -1.5033488  -0.46864304  0.65714586 -0.05893053 -1.17056   ]][0m
[37m[1m[2023-07-17 13:36:43,532][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:36:52,631][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 13:36:52,632][257371] FPS: 422095.27[0m
[36m[2023-07-17 13:36:52,634][257371] itr=1284, itrs=2000, Progress: 64.20%[0m
[36m[2023-07-17 13:37:04,469][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 13:37:04,470][257371] FPS: 327764.60[0m
[36m[2023-07-17 13:37:08,801][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:37:08,801][257371] Reward + Measures: [[654.28029908   0.31471431   0.37321135   0.172243     0.40867597
    3.75490761]][0m
[37m[1m[2023-07-17 13:37:08,801][257371] Max Reward on eval: 654.2802990825483[0m
[37m[1m[2023-07-17 13:37:08,801][257371] Min Reward on eval: 654.2802990825483[0m
[37m[1m[2023-07-17 13:37:08,802][257371] Mean Reward across all agents: 654.2802990825483[0m
[37m[1m[2023-07-17 13:37:08,802][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:37:13,698][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:37:13,698][257371] Reward + Measures: [[660.01970482   0.32660002   0.37140003   0.17569999   0.40669999
    3.75885201]
 [423.11576433   0.33160001   0.38949999   0.1754       0.38830003
    4.16700506]
 [347.97444227   0.27490002   0.28410003   0.15580001   0.31590003
    3.62811446]
 ...
 [531.62800354   0.29710004   0.39070001   0.22229998   0.4066
    3.6966958 ]
 [812.26003263   0.43640003   0.54910004   0.18960001   0.59149998
    4.21334791]
 [512.86990476   0.25140002   0.30250001   0.15280001   0.33840001
    3.68549132]][0m
[37m[1m[2023-07-17 13:37:13,698][257371] Max Reward on eval: 820.0901909020264[0m
[37m[1m[2023-07-17 13:37:13,699][257371] Min Reward on eval: 129.7164259590674[0m
[37m[1m[2023-07-17 13:37:13,699][257371] Mean Reward across all agents: 544.808165727126[0m
[37m[1m[2023-07-17 13:37:13,699][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:37:13,701][257371] mean_value=-1781.7803100494527, max_value=125.06511121719848[0m
[37m[1m[2023-07-17 13:37:13,703][257371] New mean coefficients: [[ 2.0409787  -1.1162415  -0.58735126  0.66957843 -0.10657954 -1.5235802 ]][0m
[37m[1m[2023-07-17 13:37:13,704][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:37:22,699][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 13:37:22,699][257371] FPS: 427014.10[0m
[36m[2023-07-17 13:37:22,701][257371] itr=1285, itrs=2000, Progress: 64.25%[0m
[36m[2023-07-17 13:37:34,568][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 13:37:34,568][257371] FPS: 326742.72[0m
[36m[2023-07-17 13:37:38,882][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:37:38,883][257371] Reward + Measures: [[640.75380143   0.30989334   0.36399505   0.17084466   0.39942366
    3.72010422]][0m
[37m[1m[2023-07-17 13:37:38,883][257371] Max Reward on eval: 640.7538014310198[0m
[37m[1m[2023-07-17 13:37:38,883][257371] Min Reward on eval: 640.7538014310198[0m
[37m[1m[2023-07-17 13:37:38,883][257371] Mean Reward across all agents: 640.7538014310198[0m
[37m[1m[2023-07-17 13:37:38,883][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:37:43,894][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:37:43,895][257371] Reward + Measures: [[443.48937131   0.292        0.33269998   0.17910001   0.3705
    3.77938914]
 [663.94626641   0.35590002   0.44380003   0.1464       0.46269998
    3.966784  ]
 [493.91299628   0.37740001   0.43220001   0.2277       0.49120003
    3.84105992]
 ...
 [528.13710026   0.4673       0.50710005   0.25650001   0.54180002
    4.51712418]
 [519.71660709   0.3994       0.45020005   0.22930002   0.46129999
    4.13250113]
 [782.91394428   0.43689999   0.53730005   0.18719999   0.56040001
    4.0378108 ]][0m
[37m[1m[2023-07-17 13:37:43,895][257371] Max Reward on eval: 806.3019714121241[0m
[37m[1m[2023-07-17 13:37:43,895][257371] Min Reward on eval: 12.810904183797538[0m
[37m[1m[2023-07-17 13:37:43,895][257371] Mean Reward across all agents: 499.57127779880614[0m
[37m[1m[2023-07-17 13:37:43,896][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:37:43,898][257371] mean_value=-722.6541463607259, max_value=287.7960885969337[0m
[37m[1m[2023-07-17 13:37:43,901][257371] New mean coefficients: [[ 2.2718987  -1.1920632  -0.8147404   0.7907521   0.17035642 -1.5292481 ]][0m
[37m[1m[2023-07-17 13:37:43,902][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:37:53,003][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 13:37:53,003][257371] FPS: 422012.71[0m
[36m[2023-07-17 13:37:53,005][257371] itr=1286, itrs=2000, Progress: 64.30%[0m
[36m[2023-07-17 13:38:04,934][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 13:38:04,934][257371] FPS: 324977.23[0m
[36m[2023-07-17 13:38:09,179][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:38:09,179][257371] Reward + Measures: [[653.55686315   0.31454965   0.36852562   0.173154     0.40421802
    3.70299864]][0m
[37m[1m[2023-07-17 13:38:09,180][257371] Max Reward on eval: 653.5568631475916[0m
[37m[1m[2023-07-17 13:38:09,180][257371] Min Reward on eval: 653.5568631475916[0m
[37m[1m[2023-07-17 13:38:09,180][257371] Mean Reward across all agents: 653.5568631475916[0m
[37m[1m[2023-07-17 13:38:09,180][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:38:14,200][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:38:14,200][257371] Reward + Measures: [[570.81882334   0.34890002   0.37549999   0.1772       0.41009998
    3.73233676]
 [464.59732531   0.40150005   0.50019997   0.27779999   0.47810003
    4.16649389]
 [202.46595725   0.37010002   0.40820003   0.35089999   0.38069996
    4.59921789]
 ...
 [623.30389213   0.36450002   0.42829996   0.2414       0.4285
    3.60177684]
 [271.15901941   0.32170001   0.3134       0.23010002   0.2658
    3.66827321]
 [ 92.556436     0.29659998   0.23369999   0.30879998   0.14920001
    3.87850952]][0m
[37m[1m[2023-07-17 13:38:14,200][257371] Max Reward on eval: 747.1454315121285[0m
[37m[1m[2023-07-17 13:38:14,201][257371] Min Reward on eval: -275.1642942246981[0m
[37m[1m[2023-07-17 13:38:14,201][257371] Mean Reward across all agents: 383.9334464191529[0m
[37m[1m[2023-07-17 13:38:14,201][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:38:14,203][257371] mean_value=-1450.9991798673855, max_value=294.8780665178306[0m
[37m[1m[2023-07-17 13:38:14,206][257371] New mean coefficients: [[ 2.196478   -1.0667094  -0.4846349   0.35640383  0.27982745 -1.6728846 ]][0m
[37m[1m[2023-07-17 13:38:14,207][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:38:23,302][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 13:38:23,302][257371] FPS: 422279.21[0m
[36m[2023-07-17 13:38:23,304][257371] itr=1287, itrs=2000, Progress: 64.35%[0m
[36m[2023-07-17 13:38:35,225][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 13:38:35,225][257371] FPS: 325315.79[0m
[36m[2023-07-17 13:38:39,578][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:38:39,578][257371] Reward + Measures: [[666.42824133   0.31715268   0.37164533   0.176432     0.40648901
    3.68031144]][0m
[37m[1m[2023-07-17 13:38:39,579][257371] Max Reward on eval: 666.4282413296423[0m
[37m[1m[2023-07-17 13:38:39,579][257371] Min Reward on eval: 666.4282413296423[0m
[37m[1m[2023-07-17 13:38:39,579][257371] Mean Reward across all agents: 666.4282413296423[0m
[37m[1m[2023-07-17 13:38:39,579][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:38:44,883][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:38:44,884][257371] Reward + Measures: [[521.93262483   0.5442       0.69840002   0.22290002   0.71289998
    4.55101919]
 [582.20307921   0.35999998   0.43459997   0.1902       0.46020004
    4.15945292]
 [631.71067238   0.24129999   0.28889999   0.19629999   0.3396
    3.32809615]
 ...
 [464.59289697   0.32820001   0.35129997   0.29299998   0.26010001
    3.61794853]
 [752.16349415   0.29720002   0.34920001   0.19860001   0.37730002
    3.59471679]
 [530.74815174   0.41350004   0.56630003   0.19700001   0.59550005
    4.83164167]][0m
[37m[1m[2023-07-17 13:38:44,884][257371] Max Reward on eval: 869.1396178627386[0m
[37m[1m[2023-07-17 13:38:44,884][257371] Min Reward on eval: 24.521534258313476[0m
[37m[1m[2023-07-17 13:38:44,884][257371] Mean Reward across all agents: 459.9035196036279[0m
[37m[1m[2023-07-17 13:38:44,885][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:38:44,887][257371] mean_value=-1354.9268218483433, max_value=196.07180503752477[0m
[37m[1m[2023-07-17 13:38:44,890][257371] New mean coefficients: [[ 2.4458737  -1.1002601  -0.67335135  0.02209067  0.03697172 -1.468438  ]][0m
[37m[1m[2023-07-17 13:38:44,891][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:38:53,980][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 13:38:53,980][257371] FPS: 422555.02[0m
[36m[2023-07-17 13:38:53,982][257371] itr=1288, itrs=2000, Progress: 64.40%[0m
[36m[2023-07-17 13:39:06,033][257371] train() took 11.94 seconds to complete[0m
[36m[2023-07-17 13:39:06,033][257371] FPS: 321675.84[0m
[36m[2023-07-17 13:39:10,307][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:39:10,307][257371] Reward + Measures: [[677.52166759   0.31701234   0.37234834   0.17515267   0.40771565
    3.66293216]][0m
[37m[1m[2023-07-17 13:39:10,307][257371] Max Reward on eval: 677.521667586317[0m
[37m[1m[2023-07-17 13:39:10,308][257371] Min Reward on eval: 677.521667586317[0m
[37m[1m[2023-07-17 13:39:10,308][257371] Mean Reward across all agents: 677.521667586317[0m
[37m[1m[2023-07-17 13:39:10,308][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:39:15,290][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:39:15,291][257371] Reward + Measures: [[616.96276294   0.3427       0.39489999   0.14549999   0.42360002
    3.91511846]
 [463.45549769   0.34970003   0.40229997   0.1622       0.37310001
    3.9568429 ]
 [653.02268312   0.27320001   0.33059999   0.204        0.3414
    3.39982414]
 ...
 [513.92359444   0.30650002   0.35710001   0.1372       0.3601
    3.91548467]
 [501.83036388   0.245        0.30159998   0.1662       0.30520001
    3.58451271]
 [594.46276361   0.29170001   0.33450001   0.15530001   0.36690003
    3.58007884]][0m
[37m[1m[2023-07-17 13:39:15,291][257371] Max Reward on eval: 870.1240386974998[0m
[37m[1m[2023-07-17 13:39:15,291][257371] Min Reward on eval: 302.17130886996165[0m
[37m[1m[2023-07-17 13:39:15,292][257371] Mean Reward across all agents: 604.0813210925837[0m
[37m[1m[2023-07-17 13:39:15,292][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:39:15,294][257371] mean_value=-1313.2996311274676, max_value=156.77454170476005[0m
[37m[1m[2023-07-17 13:39:15,297][257371] New mean coefficients: [[ 1.8574312  -0.86947733 -0.61506397  0.2642033   0.10507592 -1.0569992 ]][0m
[37m[1m[2023-07-17 13:39:15,297][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:39:24,291][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 13:39:24,291][257371] FPS: 427062.13[0m
[36m[2023-07-17 13:39:24,293][257371] itr=1289, itrs=2000, Progress: 64.45%[0m
[36m[2023-07-17 13:39:36,291][257371] train() took 11.88 seconds to complete[0m
[36m[2023-07-17 13:39:36,291][257371] FPS: 323257.37[0m
[36m[2023-07-17 13:39:40,546][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:39:40,546][257371] Reward + Measures: [[686.36401214   0.31296232   0.36788166   0.17773533   0.40564868
    3.63984823]][0m
[37m[1m[2023-07-17 13:39:40,547][257371] Max Reward on eval: 686.3640121401411[0m
[37m[1m[2023-07-17 13:39:40,547][257371] Min Reward on eval: 686.3640121401411[0m
[37m[1m[2023-07-17 13:39:40,547][257371] Mean Reward across all agents: 686.3640121401411[0m
[37m[1m[2023-07-17 13:39:40,547][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:39:45,521][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:39:45,521][257371] Reward + Measures: [[474.85952808   0.30429998   0.41809997   0.16100001   0.44980001
    4.18087387]
 [155.16036655   0.4312       0.42630002   0.23050001   0.30200002
    4.69979429]
 [ 78.062093     0.27429998   0.27169999   0.1543       0.18490002
    5.1551733 ]
 ...
 [672.30140087   0.38699999   0.4526       0.15500002   0.49329996
    3.92682695]
 [184.75419488   0.2016       0.20220001   0.1122       0.21180001
    4.13052702]
 [ 86.19658841   0.28010002   0.32229999   0.17550001   0.24679999
    4.78072166]][0m
[37m[1m[2023-07-17 13:39:45,522][257371] Max Reward on eval: 819.9553566354094[0m
[37m[1m[2023-07-17 13:39:45,522][257371] Min Reward on eval: -327.9308546437416[0m
[37m[1m[2023-07-17 13:39:45,522][257371] Mean Reward across all agents: 262.0442809753104[0m
[37m[1m[2023-07-17 13:39:45,522][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:39:45,524][257371] mean_value=-1037.3385289821513, max_value=99.89390265819341[0m
[37m[1m[2023-07-17 13:39:45,527][257371] New mean coefficients: [[ 1.3176631  -0.82604945 -0.5155525  -0.35890228  0.09444808 -1.2563174 ]][0m
[37m[1m[2023-07-17 13:39:45,528][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:39:54,601][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 13:39:54,602][257371] FPS: 423269.48[0m
[36m[2023-07-17 13:39:54,604][257371] itr=1290, itrs=2000, Progress: 64.50%[0m
[37m[1m[2023-07-17 13:43:28,673][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001270[0m
[36m[2023-07-17 13:43:41,017][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 13:43:41,017][257371] FPS: 328473.67[0m
[36m[2023-07-17 13:43:45,313][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:43:45,314][257371] Reward + Measures: [[707.174345     0.31724998   0.37569866   0.17999068   0.41205603
    3.63317895]][0m
[37m[1m[2023-07-17 13:43:45,314][257371] Max Reward on eval: 707.174344995818[0m
[37m[1m[2023-07-17 13:43:45,314][257371] Min Reward on eval: 707.174344995818[0m
[37m[1m[2023-07-17 13:43:45,314][257371] Mean Reward across all agents: 707.174344995818[0m
[37m[1m[2023-07-17 13:43:45,315][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:43:50,243][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:43:50,244][257371] Reward + Measures: [[462.32155634   0.30009997   0.42220002   0.15820001   0.44720003
    4.14897346]
 [408.97620078   0.2647       0.33109999   0.2325       0.3779
    3.63503385]
 [560.41078565   0.31360003   0.45320001   0.19530001   0.5262
    3.74073482]
 ...
 [390.98367409   0.37450001   0.43970004   0.32589999   0.42020002
    4.02966261]
 [480.61099199   0.41280004   0.52270001   0.16150001   0.52460003
    4.58958244]
 [381.90657068   0.29530001   0.3177       0.1533       0.3585
    3.9894979 ]][0m
[37m[1m[2023-07-17 13:43:50,244][257371] Max Reward on eval: 816.0482330643572[0m
[37m[1m[2023-07-17 13:43:50,244][257371] Min Reward on eval: -88.3888000056846[0m
[37m[1m[2023-07-17 13:43:50,245][257371] Mean Reward across all agents: 416.93088495597476[0m
[37m[1m[2023-07-17 13:43:50,245][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:43:50,247][257371] mean_value=-902.4594332223293, max_value=462.5348355122869[0m
[37m[1m[2023-07-17 13:43:50,250][257371] New mean coefficients: [[ 1.0788813  -0.23729211 -0.60643196 -0.26311976 -0.5079877  -0.7909266 ]][0m
[37m[1m[2023-07-17 13:43:50,251][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:43:59,331][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 13:43:59,332][257371] FPS: 422966.38[0m
[36m[2023-07-17 13:43:59,335][257371] itr=1291, itrs=2000, Progress: 64.55%[0m
[36m[2023-07-17 13:44:11,155][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 13:44:11,155][257371] FPS: 329238.32[0m
[36m[2023-07-17 13:44:15,436][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:44:15,436][257371] Reward + Measures: [[712.60543239   0.31070697   0.36739933   0.17969735   0.40814099
    3.59910154]][0m
[37m[1m[2023-07-17 13:44:15,437][257371] Max Reward on eval: 712.6054323897242[0m
[37m[1m[2023-07-17 13:44:15,437][257371] Min Reward on eval: 712.6054323897242[0m
[37m[1m[2023-07-17 13:44:15,437][257371] Mean Reward across all agents: 712.6054323897242[0m
[37m[1m[2023-07-17 13:44:15,437][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:44:20,693][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:44:20,693][257371] Reward + Measures: [[617.92620658   0.3204       0.35429999   0.1675       0.39590001
    3.63985705]
 [608.16244879   0.31380001   0.36310002   0.17690001   0.4021
    3.77177477]
 [625.53057862   0.41760001   0.52030003   0.16499999   0.53299999
    4.20770884]
 ...
 [347.9081118    0.20509999   0.21259999   0.18270002   0.26999998
    3.44972157]
 [613.84043692   0.48129997   0.48410001   0.40270004   0.3337
    3.9671669 ]
 [501.79138008   0.35660002   0.3506       0.3328       0.19170001
    3.73753905]][0m
[37m[1m[2023-07-17 13:44:20,694][257371] Max Reward on eval: 892.2473602652084[0m
[37m[1m[2023-07-17 13:44:20,694][257371] Min Reward on eval: 197.82445072471165[0m
[37m[1m[2023-07-17 13:44:20,694][257371] Mean Reward across all agents: 565.7713333217283[0m
[37m[1m[2023-07-17 13:44:20,694][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:44:20,697][257371] mean_value=-1747.8399884547578, max_value=286.7981757965255[0m
[37m[1m[2023-07-17 13:44:20,699][257371] New mean coefficients: [[ 0.8820855  -0.14750531 -0.35521823 -0.17233795 -0.64099014 -0.63657933]][0m
[37m[1m[2023-07-17 13:44:20,700][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:44:29,735][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 13:44:29,735][257371] FPS: 425118.90[0m
[36m[2023-07-17 13:44:29,737][257371] itr=1292, itrs=2000, Progress: 64.60%[0m
[36m[2023-07-17 13:44:41,396][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 13:44:41,396][257371] FPS: 332702.52[0m
[36m[2023-07-17 13:44:45,599][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:44:45,599][257371] Reward + Measures: [[717.42953603   0.31360099   0.37246135   0.17869133   0.41258898
    3.5977006 ]][0m
[37m[1m[2023-07-17 13:44:45,599][257371] Max Reward on eval: 717.4295360257098[0m
[37m[1m[2023-07-17 13:44:45,599][257371] Min Reward on eval: 717.4295360257098[0m
[37m[1m[2023-07-17 13:44:45,600][257371] Mean Reward across all agents: 717.4295360257098[0m
[37m[1m[2023-07-17 13:44:45,600][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:44:50,585][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:44:50,586][257371] Reward + Measures: [[687.56885878   0.22979999   0.27809998   0.18959999   0.32149997
    3.22814798]
 [615.62052105   0.39179999   0.47319999   0.15280001   0.48119998
    4.09557676]
 [819.6351623    0.40230003   0.47890002   0.2016       0.50710005
    3.76299024]
 ...
 [651.9769716    0.3398       0.39389998   0.1901       0.42399999
    3.82977533]
 [860.53101351   0.3712       0.43649998   0.2079       0.46220002
    3.53491378]
 [606.0043559    0.30590001   0.36160001   0.17480001   0.4052
    3.43993187]][0m
[37m[1m[2023-07-17 13:44:50,586][257371] Max Reward on eval: 901.3414688410179[0m
[37m[1m[2023-07-17 13:44:50,586][257371] Min Reward on eval: 307.4382268924266[0m
[37m[1m[2023-07-17 13:44:50,586][257371] Mean Reward across all agents: 651.5949492650481[0m
[37m[1m[2023-07-17 13:44:50,586][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:44:50,588][257371] mean_value=-1477.998899524499, max_value=218.00491425007306[0m
[37m[1m[2023-07-17 13:44:50,591][257371] New mean coefficients: [[ 0.7349026  -0.16847406 -0.43451437  0.25501922 -0.3537899   0.01740301]][0m
[37m[1m[2023-07-17 13:44:50,592][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:44:59,599][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 13:44:59,599][257371] FPS: 426404.99[0m
[36m[2023-07-17 13:44:59,602][257371] itr=1293, itrs=2000, Progress: 64.65%[0m
[36m[2023-07-17 13:45:11,424][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 13:45:11,424][257371] FPS: 327956.54[0m
[36m[2023-07-17 13:45:15,683][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:45:15,683][257371] Reward + Measures: [[720.98874756   0.31230101   0.36971599   0.17957032   0.41039702
    3.58834529]][0m
[37m[1m[2023-07-17 13:45:15,684][257371] Max Reward on eval: 720.9887475621985[0m
[37m[1m[2023-07-17 13:45:15,684][257371] Min Reward on eval: 720.9887475621985[0m
[37m[1m[2023-07-17 13:45:15,684][257371] Mean Reward across all agents: 720.9887475621985[0m
[37m[1m[2023-07-17 13:45:15,684][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:45:20,654][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:45:20,655][257371] Reward + Measures: [[537.54365202   0.28150001   0.31889999   0.16420001   0.34510002
    3.53638077]
 [588.98076057   0.27700001   0.3642       0.2105       0.3691
    3.685817  ]
 [309.99653681   0.2613       0.28650004   0.145        0.30290005
    3.59772921]
 ...
 [678.70598979   0.40060002   0.46129999   0.16439998   0.47410002
    3.96276021]
 [657.37882422   0.36339998   0.45819998   0.1635       0.46090004
    3.9406383 ]
 [452.03036457   0.2784       0.35339999   0.19780001   0.34769997
    3.81222343]][0m
[37m[1m[2023-07-17 13:45:20,655][257371] Max Reward on eval: 908.8081054949201[0m
[37m[1m[2023-07-17 13:45:20,656][257371] Min Reward on eval: 134.05181860693264[0m
[37m[1m[2023-07-17 13:45:20,656][257371] Mean Reward across all agents: 559.3968472228859[0m
[37m[1m[2023-07-17 13:45:20,656][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:45:20,658][257371] mean_value=-1448.441884129952, max_value=109.30873020411047[0m
[37m[1m[2023-07-17 13:45:20,660][257371] New mean coefficients: [[ 0.44618157 -0.03218247 -0.2288764  -0.10900322 -0.60697836 -0.15164284]][0m
[37m[1m[2023-07-17 13:45:20,661][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:45:29,642][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 13:45:29,642][257371] FPS: 427648.90[0m
[36m[2023-07-17 13:45:29,645][257371] itr=1294, itrs=2000, Progress: 64.70%[0m
[36m[2023-07-17 13:45:41,598][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-17 13:45:41,598][257371] FPS: 324298.46[0m
[36m[2023-07-17 13:45:45,849][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:45:45,849][257371] Reward + Measures: [[728.4359973    0.30698267   0.36343065   0.17846632   0.40365767
    3.57877851]][0m
[37m[1m[2023-07-17 13:45:45,850][257371] Max Reward on eval: 728.4359972989857[0m
[37m[1m[2023-07-17 13:45:45,850][257371] Min Reward on eval: 728.4359972989857[0m
[37m[1m[2023-07-17 13:45:45,850][257371] Mean Reward across all agents: 728.4359972989857[0m
[37m[1m[2023-07-17 13:45:45,850][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:45:50,858][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:45:50,859][257371] Reward + Measures: [[702.53144169   0.33220002   0.4131       0.1806       0.43080002
    3.67732358]
 [540.55976612   0.32790002   0.3608       0.13560002   0.37910002
    3.77898026]
 [729.28561401   0.34970003   0.41459998   0.18540001   0.45280001
    3.71411681]
 ...
 [705.0722266    0.34320003   0.3998       0.17230001   0.4296
    3.93714023]
 [695.58650375   0.32319999   0.35280001   0.18190001   0.39669999
    3.48586726]
 [511.52147884   0.34399998   0.43169999   0.14650001   0.4316
    4.05681181]][0m
[37m[1m[2023-07-17 13:45:50,859][257371] Max Reward on eval: 908.9944839167525[0m
[37m[1m[2023-07-17 13:45:50,860][257371] Min Reward on eval: 380.55329981183166[0m
[37m[1m[2023-07-17 13:45:50,860][257371] Mean Reward across all agents: 666.9156225754555[0m
[37m[1m[2023-07-17 13:45:50,860][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:45:50,862][257371] mean_value=-1356.69063036701, max_value=117.08636038053555[0m
[37m[1m[2023-07-17 13:45:50,865][257371] New mean coefficients: [[ 0.28197318  0.00618374 -0.3161433  -0.2284086  -0.11342373 -0.9692314 ]][0m
[37m[1m[2023-07-17 13:45:50,866][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:45:59,900][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 13:45:59,900][257371] FPS: 425122.67[0m
[36m[2023-07-17 13:45:59,903][257371] itr=1295, itrs=2000, Progress: 64.75%[0m
[36m[2023-07-17 13:46:11,761][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 13:46:11,762][257371] FPS: 327043.80[0m
[36m[2023-07-17 13:46:16,009][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:46:16,010][257371] Reward + Measures: [[727.4082799    0.30181667   0.36142963   0.18036935   0.40475431
    3.537498  ]][0m
[37m[1m[2023-07-17 13:46:16,010][257371] Max Reward on eval: 727.4082798987926[0m
[37m[1m[2023-07-17 13:46:16,010][257371] Min Reward on eval: 727.4082798987926[0m
[37m[1m[2023-07-17 13:46:16,011][257371] Mean Reward across all agents: 727.4082798987926[0m
[37m[1m[2023-07-17 13:46:16,011][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:46:20,932][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:46:20,932][257371] Reward + Measures: [[701.8107646    0.33319998   0.42339998   0.1786       0.43619999
    3.69582558]
 [475.08226781   0.21500002   0.2227       0.162        0.26809999
    3.39655757]
 [582.87121776   0.26420003   0.30020002   0.16970001   0.34710002
    3.44634104]
 ...
 [788.9683075    0.2572       0.29670003   0.2247       0.36490002
    3.30902362]
 [451.26740745   0.1832       0.21080001   0.16370001   0.2502
    3.28195357]
 [619.44680761   0.33430001   0.36919999   0.1552       0.3881
    3.68120074]][0m
[37m[1m[2023-07-17 13:46:20,933][257371] Max Reward on eval: 875.6656265569502[0m
[37m[1m[2023-07-17 13:46:20,933][257371] Min Reward on eval: 310.1954732429702[0m
[37m[1m[2023-07-17 13:46:20,933][257371] Mean Reward across all agents: 668.8033291548485[0m
[37m[1m[2023-07-17 13:46:20,933][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:46:20,935][257371] mean_value=-1315.4282309236057, max_value=78.63353896141882[0m
[37m[1m[2023-07-17 13:46:20,937][257371] New mean coefficients: [[ 0.48543006 -0.05891164 -0.4733767  -0.2593463  -0.09426281 -0.9034938 ]][0m
[37m[1m[2023-07-17 13:46:20,938][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:46:29,922][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 13:46:29,922][257371] FPS: 427540.24[0m
[36m[2023-07-17 13:46:29,924][257371] itr=1296, itrs=2000, Progress: 64.80%[0m
[36m[2023-07-17 13:46:41,888][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-17 13:46:41,888][257371] FPS: 324027.80[0m
[36m[2023-07-17 13:46:46,132][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:46:46,132][257371] Reward + Measures: [[742.35086683   0.30483332   0.36463666   0.18248299   0.40895167
    3.52103114]][0m
[37m[1m[2023-07-17 13:46:46,133][257371] Max Reward on eval: 742.3508668338603[0m
[37m[1m[2023-07-17 13:46:46,133][257371] Min Reward on eval: 742.3508668338603[0m
[37m[1m[2023-07-17 13:46:46,133][257371] Mean Reward across all agents: 742.3508668338603[0m
[37m[1m[2023-07-17 13:46:46,133][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:46:51,134][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:46:51,134][257371] Reward + Measures: [[456.11132237   0.2333       0.25920001   0.14570001   0.3211
    3.41566706]
 [732.05588818   0.366        0.41580001   0.19680001   0.44899997
    3.59893656]
 [625.39837763   0.33830002   0.3996       0.16110002   0.43150002
    3.72293925]
 ...
 [955.52722935   0.32229999   0.39420003   0.21610001   0.42810002
    3.40569687]
 [707.49915695   0.40889999   0.4894       0.1662       0.4883
    4.04986906]
 [723.94096061   0.24660002   0.28870001   0.2027       0.34930006
    3.30709529]][0m
[37m[1m[2023-07-17 13:46:51,135][257371] Max Reward on eval: 955.5272293462418[0m
[37m[1m[2023-07-17 13:46:51,135][257371] Min Reward on eval: 422.2260609430261[0m
[37m[1m[2023-07-17 13:46:51,135][257371] Mean Reward across all agents: 714.5335294955416[0m
[37m[1m[2023-07-17 13:46:51,135][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:46:51,137][257371] mean_value=-1198.792776410776, max_value=109.88249181855406[0m
[37m[1m[2023-07-17 13:46:51,140][257371] New mean coefficients: [[ 0.25465947 -0.38030863 -0.7677666  -0.26114047  0.15325347 -0.6918012 ]][0m
[37m[1m[2023-07-17 13:46:51,141][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:47:00,180][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 13:47:00,180][257371] FPS: 424912.11[0m
[36m[2023-07-17 13:47:00,182][257371] itr=1297, itrs=2000, Progress: 64.85%[0m
[36m[2023-07-17 13:47:12,136][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 13:47:12,136][257371] FPS: 331805.75[0m
[36m[2023-07-17 13:47:16,359][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:47:16,360][257371] Reward + Measures: [[743.80543293   0.30371198   0.36135465   0.18575799   0.40837264
    3.48035812]][0m
[37m[1m[2023-07-17 13:47:16,360][257371] Max Reward on eval: 743.8054329332208[0m
[37m[1m[2023-07-17 13:47:16,360][257371] Min Reward on eval: 743.8054329332208[0m
[37m[1m[2023-07-17 13:47:16,361][257371] Mean Reward across all agents: 743.8054329332208[0m
[37m[1m[2023-07-17 13:47:16,361][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:47:21,333][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:47:21,339][257371] Reward + Measures: [[358.84939575   0.24460001   0.26610002   0.14210001   0.2474
    3.97958159]
 [730.99302748   0.26159999   0.3186       0.1937       0.352
    3.37290883]
 [649.15650676   0.34319997   0.3944       0.17539999   0.44229999
    3.64967537]
 ...
 [554.6493045    0.2201       0.26230001   0.18230002   0.29260001
    2.98301005]
 [650.10107066   0.22589998   0.2651       0.18859999   0.30930001
    3.34849548]
 [660.93992833   0.27560005   0.30560002   0.22980002   0.38400003
    3.46934056]][0m
[37m[1m[2023-07-17 13:47:21,339][257371] Max Reward on eval: 879.9404297060333[0m
[37m[1m[2023-07-17 13:47:21,339][257371] Min Reward on eval: -44.6878338136652[0m
[37m[1m[2023-07-17 13:47:21,339][257371] Mean Reward across all agents: 494.26059942787066[0m
[37m[1m[2023-07-17 13:47:21,340][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:47:21,342][257371] mean_value=-1821.260821322777, max_value=304.3797948504656[0m
[37m[1m[2023-07-17 13:47:21,344][257371] New mean coefficients: [[-0.29425687 -0.33116588 -0.6095372  -0.17228872  0.4577581  -0.72779167]][0m
[37m[1m[2023-07-17 13:47:21,345][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:47:30,389][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 13:47:30,389][257371] FPS: 424675.51[0m
[36m[2023-07-17 13:47:30,392][257371] itr=1298, itrs=2000, Progress: 64.90%[0m
[36m[2023-07-17 13:47:42,169][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 13:47:42,169][257371] FPS: 329258.26[0m
[36m[2023-07-17 13:47:46,435][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:47:46,436][257371] Reward + Measures: [[746.86555523   0.30531901   0.36148104   0.18755932   0.41275501
    3.45562983]][0m
[37m[1m[2023-07-17 13:47:46,436][257371] Max Reward on eval: 746.8655552313621[0m
[37m[1m[2023-07-17 13:47:46,436][257371] Min Reward on eval: 746.8655552313621[0m
[37m[1m[2023-07-17 13:47:46,437][257371] Mean Reward across all agents: 746.8655552313621[0m
[37m[1m[2023-07-17 13:47:46,437][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:47:51,431][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:47:51,437][257371] Reward + Measures: [[592.53296952   0.2793       0.31280002   0.1832       0.34890002
    3.46663976]
 [505.48433173   0.44280002   0.43310004   0.35479999   0.31689999
    3.86890769]
 [501.05820025   0.25240001   0.28050002   0.18840002   0.34129998
    3.55719304]
 ...
 [734.31768098   0.28960001   0.33489999   0.20310001   0.39200002
    3.25750422]
 [932.29030601   0.41529998   0.49130002   0.19229999   0.52380002
    3.50845122]
 [698.56041715   0.30039999   0.36310002   0.24159999   0.45039997
    3.44298029]][0m
[37m[1m[2023-07-17 13:47:51,437][257371] Max Reward on eval: 932.3012923912727[0m
[37m[1m[2023-07-17 13:47:51,438][257371] Min Reward on eval: 243.85228119182867[0m
[37m[1m[2023-07-17 13:47:51,438][257371] Mean Reward across all agents: 646.7971608886629[0m
[37m[1m[2023-07-17 13:47:51,438][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:47:51,440][257371] mean_value=-1295.640360952074, max_value=256.4630693155325[0m
[37m[1m[2023-07-17 13:47:51,442][257371] New mean coefficients: [[-0.07702997 -0.16684133 -0.5070679  -0.12135768  0.6854603  -0.6833328 ]][0m
[37m[1m[2023-07-17 13:47:51,443][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:48:00,440][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 13:48:00,441][257371] FPS: 426884.87[0m
[36m[2023-07-17 13:48:00,443][257371] itr=1299, itrs=2000, Progress: 64.95%[0m
[36m[2023-07-17 13:48:12,264][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 13:48:12,264][257371] FPS: 327989.21[0m
[36m[2023-07-17 13:48:16,520][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:48:16,521][257371] Reward + Measures: [[734.75604101   0.296453     0.35091966   0.18911065   0.40334496
    3.38013124]][0m
[37m[1m[2023-07-17 13:48:16,521][257371] Max Reward on eval: 734.7560410055559[0m
[37m[1m[2023-07-17 13:48:16,521][257371] Min Reward on eval: 734.7560410055559[0m
[37m[1m[2023-07-17 13:48:16,522][257371] Mean Reward across all agents: 734.7560410055559[0m
[37m[1m[2023-07-17 13:48:16,522][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:48:21,392][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:48:21,398][257371] Reward + Measures: [[737.65206906   0.24650002   0.2956       0.20320001   0.36489999
    3.18798041]
 [813.8468865    0.31220001   0.38170001   0.19719999   0.42940003
    3.46966052]
 [649.09425722   0.2649       0.29860002   0.19759999   0.33800003
    3.13228679]
 ...
 [772.08490991   0.37819999   0.44329998   0.18859999   0.47830001
    3.67453504]
 [519.11464362   0.31909999   0.37260002   0.1543       0.39110002
    3.78251958]
 [628.5560989    0.30399999   0.3554       0.17119999   0.39250001
    3.5794847 ]][0m
[37m[1m[2023-07-17 13:48:21,398][257371] Max Reward on eval: 891.0565948262811[0m
[37m[1m[2023-07-17 13:48:21,398][257371] Min Reward on eval: 384.1163196169306[0m
[37m[1m[2023-07-17 13:48:21,398][257371] Mean Reward across all agents: 691.2563662145689[0m
[37m[1m[2023-07-17 13:48:21,399][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:48:21,400][257371] mean_value=-1387.2614749220097, max_value=53.09349340197946[0m
[37m[1m[2023-07-17 13:48:21,403][257371] New mean coefficients: [[ 0.10026465 -0.12983435 -0.21130466 -0.0460908   0.59470636 -0.6795402 ]][0m
[37m[1m[2023-07-17 13:48:21,403][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:48:29,947][257371] train() took 8.54 seconds to complete[0m
[36m[2023-07-17 13:48:29,947][257371] FPS: 449559.47[0m
[36m[2023-07-17 13:48:29,949][257371] itr=1300, itrs=2000, Progress: 65.00%[0m
[37m[1m[2023-07-17 13:52:05,969][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001280[0m
[36m[2023-07-17 13:52:18,665][257371] train() took 12.13 seconds to complete[0m
[36m[2023-07-17 13:52:18,665][257371] FPS: 316681.40[0m
[36m[2023-07-17 13:52:22,932][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:52:22,937][257371] Reward + Measures: [[762.96321271   0.29892534   0.35795629   0.19405033   0.41736498
    3.34694552]][0m
[37m[1m[2023-07-17 13:52:22,938][257371] Max Reward on eval: 762.9632127145021[0m
[37m[1m[2023-07-17 13:52:22,938][257371] Min Reward on eval: 762.9632127145021[0m
[37m[1m[2023-07-17 13:52:22,938][257371] Mean Reward across all agents: 762.9632127145021[0m
[37m[1m[2023-07-17 13:52:22,939][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:52:28,217][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:52:28,218][257371] Reward + Measures: [[618.76341836   0.2457       0.27669999   0.1987       0.34080002
    3.13822532]
 [811.86321638   0.2658       0.33699998   0.2182       0.40489998
    3.10634017]
 [833.31692501   0.31349999   0.3637       0.2471       0.46440002
    3.13528633]
 ...
 [721.09384678   0.255        0.31720001   0.2059       0.3565
    3.19200158]
 [662.07206918   0.39130002   0.42150003   0.16170001   0.46419999
    3.79407001]
 [805.11024097   0.27610001   0.35489997   0.21949999   0.40060002
    3.17888713]][0m
[37m[1m[2023-07-17 13:52:28,218][257371] Max Reward on eval: 949.2910994844511[0m
[37m[1m[2023-07-17 13:52:28,218][257371] Min Reward on eval: 408.42134066582656[0m
[37m[1m[2023-07-17 13:52:28,219][257371] Mean Reward across all agents: 730.81089090419[0m
[37m[1m[2023-07-17 13:52:28,219][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:52:28,221][257371] mean_value=-1110.0461962230045, max_value=109.89410075993749[0m
[37m[1m[2023-07-17 13:52:28,223][257371] New mean coefficients: [[-0.14942434  0.17924154  0.05102345 -0.00433085  0.39512682 -0.08879846]][0m
[37m[1m[2023-07-17 13:52:28,224][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:52:37,227][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 13:52:37,227][257371] FPS: 426626.03[0m
[36m[2023-07-17 13:52:37,229][257371] itr=1301, itrs=2000, Progress: 65.05%[0m
[36m[2023-07-17 13:52:49,019][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 13:52:49,019][257371] FPS: 328985.08[0m
[36m[2023-07-17 13:52:53,387][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:52:53,387][257371] Reward + Measures: [[767.85371939   0.29780701   0.36107534   0.19453165   0.42025164
    3.33758688]][0m
[37m[1m[2023-07-17 13:52:53,387][257371] Max Reward on eval: 767.8537193935448[0m
[37m[1m[2023-07-17 13:52:53,388][257371] Min Reward on eval: 767.8537193935448[0m
[37m[1m[2023-07-17 13:52:53,388][257371] Mean Reward across all agents: 767.8537193935448[0m
[37m[1m[2023-07-17 13:52:53,388][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:52:58,433][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:52:58,448][257371] Reward + Measures: [[600.35922532   0.32870001   0.41840002   0.19029999   0.42670003
    4.06651688]
 [789.31574058   0.2719       0.32360002   0.22089998   0.41459998
    3.0801568 ]
 [623.62034324   0.29930001   0.35550001   0.1761       0.3928
    3.32092547]
 ...
 [645.18140767   0.266        0.30630001   0.1734       0.36480001
    3.32116508]
 [779.93635848   0.33520001   0.36539999   0.1894       0.4219
    3.44262171]
 [743.83583835   0.33270001   0.3549       0.22879998   0.43110004
    3.20843172]][0m
[37m[1m[2023-07-17 13:52:58,449][257371] Max Reward on eval: 913.7705460218247[0m
[37m[1m[2023-07-17 13:52:58,449][257371] Min Reward on eval: 315.34067084988345[0m
[37m[1m[2023-07-17 13:52:58,449][257371] Mean Reward across all agents: 670.3119082398114[0m
[37m[1m[2023-07-17 13:52:58,449][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:52:58,451][257371] mean_value=-1417.2853408992926, max_value=47.23050610517646[0m
[37m[1m[2023-07-17 13:52:58,454][257371] New mean coefficients: [[-0.19995901  0.5157383   0.21723457 -0.345406   -0.34347427  0.05708508]][0m
[37m[1m[2023-07-17 13:52:58,455][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:53:07,518][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 13:53:07,518][257371] FPS: 423770.64[0m
[36m[2023-07-17 13:53:07,520][257371] itr=1302, itrs=2000, Progress: 65.10%[0m
[36m[2023-07-17 13:53:19,455][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 13:53:19,455][257371] FPS: 324839.92[0m
[36m[2023-07-17 13:53:23,828][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:53:23,833][257371] Reward + Measures: [[754.2986215    0.291823     0.35388333   0.19512133   0.41416833
    3.31693196]][0m
[37m[1m[2023-07-17 13:53:23,834][257371] Max Reward on eval: 754.2986214959419[0m
[37m[1m[2023-07-17 13:53:23,834][257371] Min Reward on eval: 754.2986214959419[0m
[37m[1m[2023-07-17 13:53:23,834][257371] Mean Reward across all agents: 754.2986214959419[0m
[37m[1m[2023-07-17 13:53:23,834][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:53:28,852][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:53:28,853][257371] Reward + Measures: [[707.79476852   0.28530002   0.30840001   0.19410001   0.38030002
    3.16752887]
 [780.62946702   0.4179       0.47919998   0.1901       0.53949994
    3.53406191]
 [698.16334024   0.33410001   0.3964       0.21259999   0.43239999
    3.49076724]
 ...
 [516.60669708   0.3229       0.38709998   0.28280002   0.40100002
    3.34846926]
 [537.77066573   0.30710003   0.37390003   0.14760001   0.37240002
    3.62550807]
 [739.87622188   0.42840001   0.5079       0.164        0.51350003
    3.92525983]][0m
[37m[1m[2023-07-17 13:53:28,853][257371] Max Reward on eval: 913.6474456112832[0m
[37m[1m[2023-07-17 13:53:28,853][257371] Min Reward on eval: 355.41514484398067[0m
[37m[1m[2023-07-17 13:53:28,854][257371] Mean Reward across all agents: 674.2622112332701[0m
[37m[1m[2023-07-17 13:53:28,854][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:53:28,855][257371] mean_value=-1156.3043031494678, max_value=12.34343393527422[0m
[37m[1m[2023-07-17 13:53:28,858][257371] New mean coefficients: [[-0.41776124  0.32835793  0.2980615  -0.33075982 -0.30647007  0.2914253 ]][0m
[37m[1m[2023-07-17 13:53:28,859][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:53:37,854][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 13:53:37,854][257371] FPS: 426968.45[0m
[36m[2023-07-17 13:53:37,857][257371] itr=1303, itrs=2000, Progress: 65.15%[0m
[36m[2023-07-17 13:53:49,646][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 13:53:49,646][257371] FPS: 328939.90[0m
[36m[2023-07-17 13:53:53,980][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:53:53,981][257371] Reward + Measures: [[755.90118321   0.29358101   0.35836962   0.19581901   0.41711435
    3.33196545]][0m
[37m[1m[2023-07-17 13:53:53,981][257371] Max Reward on eval: 755.9011832097052[0m
[37m[1m[2023-07-17 13:53:53,981][257371] Min Reward on eval: 755.9011832097052[0m
[37m[1m[2023-07-17 13:53:53,982][257371] Mean Reward across all agents: 755.9011832097052[0m
[37m[1m[2023-07-17 13:53:53,982][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:53:58,976][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:53:58,977][257371] Reward + Measures: [[396.8355698    0.38960001   0.39229998   0.29940003   0.35499999
    3.55078673]
 [659.1307605    0.29270002   0.3574       0.18710001   0.41420004
    3.38628244]
 [476.11579319   0.28150001   0.30850002   0.1257       0.32319999
    3.56891418]
 ...
 [617.25867275   0.35039997   0.37369999   0.28480002   0.42000005
    3.36089063]
 [634.09153311   0.29130003   0.3409       0.19350001   0.36150002
    3.63244557]
 [582.92875481   0.3486       0.41009998   0.226        0.43670002
    3.53396106]][0m
[37m[1m[2023-07-17 13:53:58,977][257371] Max Reward on eval: 926.8020782246255[0m
[37m[1m[2023-07-17 13:53:58,977][257371] Min Reward on eval: -108.51983622179833[0m
[37m[1m[2023-07-17 13:53:58,978][257371] Mean Reward across all agents: 548.5149406917138[0m
[37m[1m[2023-07-17 13:53:58,978][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:53:58,979][257371] mean_value=-1218.5531223249623, max_value=82.91156665544872[0m
[37m[1m[2023-07-17 13:53:58,982][257371] New mean coefficients: [[-0.69892544  0.2773078   0.17413971  0.02808279 -0.07563393  0.27931532]][0m
[37m[1m[2023-07-17 13:53:58,987][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:54:07,999][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 13:54:07,999][257371] FPS: 426186.59[0m
[36m[2023-07-17 13:54:08,001][257371] itr=1304, itrs=2000, Progress: 65.20%[0m
[36m[2023-07-17 13:54:19,889][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 13:54:19,889][257371] FPS: 326153.88[0m
[36m[2023-07-17 13:54:24,177][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:54:24,178][257371] Reward + Measures: [[741.70810341   0.29518732   0.35797298   0.195637     0.41889766
    3.34172153]][0m
[37m[1m[2023-07-17 13:54:24,178][257371] Max Reward on eval: 741.7081034053297[0m
[37m[1m[2023-07-17 13:54:24,178][257371] Min Reward on eval: 741.7081034053297[0m
[37m[1m[2023-07-17 13:54:24,178][257371] Mean Reward across all agents: 741.7081034053297[0m
[37m[1m[2023-07-17 13:54:24,179][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:54:29,129][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:54:29,130][257371] Reward + Measures: [[744.90125274   0.34320003   0.43130001   0.2559       0.52670002
    3.01010966]
 [657.57850052   0.33249998   0.37810001   0.1433       0.4251
    3.74994397]
 [814.20810503   0.26830003   0.31669998   0.2181       0.391
    3.11271977]
 ...
 [356.2963538    0.37180001   0.34129998   0.3242       0.30920002
    3.4176929 ]
 [680.90056133   0.3759       0.4276       0.1831       0.48310003
    3.53560328]
 [513.57538795   0.38550001   0.37559998   0.32640001   0.32440001
    3.60328674]][0m
[37m[1m[2023-07-17 13:54:29,130][257371] Max Reward on eval: 903.1418533483869[0m
[37m[1m[2023-07-17 13:54:29,130][257371] Min Reward on eval: -89.64562418067362[0m
[37m[1m[2023-07-17 13:54:29,130][257371] Mean Reward across all agents: 540.8225908614039[0m
[37m[1m[2023-07-17 13:54:29,131][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:54:29,132][257371] mean_value=-1522.6801274165564, max_value=451.32470997755775[0m
[37m[1m[2023-07-17 13:54:29,135][257371] New mean coefficients: [[-0.4745124   0.35400373  0.33676952 -0.30010954 -0.24472003  0.62747073]][0m
[37m[1m[2023-07-17 13:54:29,136][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:54:38,148][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 13:54:38,148][257371] FPS: 426166.85[0m
[36m[2023-07-17 13:54:38,150][257371] itr=1305, itrs=2000, Progress: 65.25%[0m
[36m[2023-07-17 13:54:49,824][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 13:54:49,825][257371] FPS: 332161.68[0m
[36m[2023-07-17 13:54:54,174][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:54:54,175][257371] Reward + Measures: [[734.69989438   0.29670864   0.35843599   0.19497566   0.41722134
    3.36915755]][0m
[37m[1m[2023-07-17 13:54:54,175][257371] Max Reward on eval: 734.6998943786394[0m
[37m[1m[2023-07-17 13:54:54,175][257371] Min Reward on eval: 734.6998943786394[0m
[37m[1m[2023-07-17 13:54:54,175][257371] Mean Reward across all agents: 734.6998943786394[0m
[37m[1m[2023-07-17 13:54:54,176][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:54:59,161][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:54:59,162][257371] Reward + Measures: [[557.14605353   0.40580001   0.51030004   0.2036       0.53570002
    4.12890196]
 [600.06862367   0.3362       0.34010002   0.23190001   0.41050002
    3.44540191]
 [453.31270171   0.2511       0.53859997   0.27040002   0.55010003
    4.94438887]
 ...
 [332.38719604   0.2138       0.30990002   0.1337       0.3362
    4.02396536]
 [206.21744485   0.2554       0.30570003   0.15019999   0.31529999
    4.3335042 ]
 [635.98606867   0.37240002   0.40009999   0.17909999   0.42939997
    3.69451785]][0m
[37m[1m[2023-07-17 13:54:59,162][257371] Max Reward on eval: 899.8711815472692[0m
[37m[1m[2023-07-17 13:54:59,162][257371] Min Reward on eval: 116.06978988451883[0m
[37m[1m[2023-07-17 13:54:59,162][257371] Mean Reward across all agents: 524.0487492144341[0m
[37m[1m[2023-07-17 13:54:59,163][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:54:59,164][257371] mean_value=-1311.809577017008, max_value=69.2301871766677[0m
[37m[1m[2023-07-17 13:54:59,167][257371] New mean coefficients: [[-1.3678465   0.21563095  0.16553734 -0.0447275  -0.36612633  0.83461577]][0m
[37m[1m[2023-07-17 13:54:59,168][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:55:08,156][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 13:55:08,157][257371] FPS: 427284.98[0m
[36m[2023-07-17 13:55:08,159][257371] itr=1306, itrs=2000, Progress: 65.30%[0m
[36m[2023-07-17 13:55:19,896][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 13:55:19,896][257371] FPS: 330400.39[0m
[36m[2023-07-17 13:55:24,183][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:55:24,183][257371] Reward + Measures: [[717.50299251   0.29257101   0.35183698   0.19383802   0.40884098
    3.36902666]][0m
[37m[1m[2023-07-17 13:55:24,183][257371] Max Reward on eval: 717.5029925128792[0m
[37m[1m[2023-07-17 13:55:24,184][257371] Min Reward on eval: 717.5029925128792[0m
[37m[1m[2023-07-17 13:55:24,184][257371] Mean Reward across all agents: 717.5029925128792[0m
[37m[1m[2023-07-17 13:55:24,184][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:55:29,439][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:55:29,445][257371] Reward + Measures: [[781.93825912   0.37670001   0.35530001   0.25480002   0.4461
    3.2022202 ]
 [573.16573449   0.37170002   0.44900003   0.15880001   0.4738
    3.84284377]
 [310.71027227   0.2827       0.33570001   0.1973       0.285
    4.28841925]
 ...
 [694.15626914   0.33440003   0.38760003   0.19500001   0.4192
    3.65504336]
 [757.5161286    0.37470001   0.46919999   0.1936       0.5205
    3.79735994]
 [237.82299325   0.3788       0.44479999   0.21000002   0.38890001
    4.40236139]][0m
[37m[1m[2023-07-17 13:55:29,445][257371] Max Reward on eval: 914.0934753485024[0m
[37m[1m[2023-07-17 13:55:29,445][257371] Min Reward on eval: 62.63600283646956[0m
[37m[1m[2023-07-17 13:55:29,445][257371] Mean Reward across all agents: 611.661693140275[0m
[37m[1m[2023-07-17 13:55:29,446][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:55:29,448][257371] mean_value=-1382.2646562764075, max_value=486.48094998930935[0m
[37m[1m[2023-07-17 13:55:29,450][257371] New mean coefficients: [[-0.6475429   0.20957217  0.28345585 -0.7513367  -0.10239935  1.0233334 ]][0m
[37m[1m[2023-07-17 13:55:29,451][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:55:38,456][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 13:55:38,456][257371] FPS: 426518.89[0m
[36m[2023-07-17 13:55:38,459][257371] itr=1307, itrs=2000, Progress: 65.35%[0m
[36m[2023-07-17 13:55:50,261][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 13:55:50,261][257371] FPS: 328567.55[0m
[36m[2023-07-17 13:55:54,609][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:55:54,609][257371] Reward + Measures: [[711.00807225   0.29821098   0.36120498   0.193113     0.41484162
    3.42111969]][0m
[37m[1m[2023-07-17 13:55:54,610][257371] Max Reward on eval: 711.0080722515577[0m
[37m[1m[2023-07-17 13:55:54,610][257371] Min Reward on eval: 711.0080722515577[0m
[37m[1m[2023-07-17 13:55:54,610][257371] Mean Reward across all agents: 711.0080722515577[0m
[37m[1m[2023-07-17 13:55:54,610][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:55:59,513][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:55:59,513][257371] Reward + Measures: [[498.30451305   0.23660003   0.32879999   0.19660001   0.37530002
    3.40305448]
 [727.80434845   0.28860003   0.33580002   0.20580001   0.38359997
    3.22088027]
 [408.94405999   0.31640002   0.3748       0.156        0.41549999
    3.69857192]
 ...
 [717.49056243   0.37310001   0.42920002   0.21950002   0.47950003
    3.72622561]
 [565.22167588   0.3283       0.24530001   0.26500002   0.31150001
    3.13315344]
 [ 63.19130401   0.51160002   0.65430003   0.37779999   0.46480003
    4.75502205]][0m
[37m[1m[2023-07-17 13:55:59,514][257371] Max Reward on eval: 784.5434647365473[0m
[37m[1m[2023-07-17 13:55:59,514][257371] Min Reward on eval: -183.41384061565623[0m
[37m[1m[2023-07-17 13:55:59,514][257371] Mean Reward across all agents: 405.2151575774614[0m
[37m[1m[2023-07-17 13:55:59,514][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:55:59,516][257371] mean_value=-1612.322483544548, max_value=34.008997683626376[0m
[37m[1m[2023-07-17 13:55:59,519][257371] New mean coefficients: [[-0.27635148  0.29759416  0.32090786 -0.8610755   0.13153988  0.58089197]][0m
[37m[1m[2023-07-17 13:55:59,519][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:56:08,468][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 13:56:08,468][257371] FPS: 429196.66[0m
[36m[2023-07-17 13:56:08,470][257371] itr=1308, itrs=2000, Progress: 65.40%[0m
[36m[2023-07-17 13:56:20,136][257371] train() took 11.55 seconds to complete[0m
[36m[2023-07-17 13:56:20,137][257371] FPS: 332427.08[0m
[36m[2023-07-17 13:56:24,356][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:56:24,356][257371] Reward + Measures: [[703.22195629   0.29687399   0.36015233   0.19073567   0.41274732
    3.42721343]][0m
[37m[1m[2023-07-17 13:56:24,356][257371] Max Reward on eval: 703.221956289954[0m
[37m[1m[2023-07-17 13:56:24,357][257371] Min Reward on eval: 703.221956289954[0m
[37m[1m[2023-07-17 13:56:24,357][257371] Mean Reward across all agents: 703.221956289954[0m
[37m[1m[2023-07-17 13:56:24,357][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:56:29,332][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:56:29,332][257371] Reward + Measures: [[909.33821874   0.29210001   0.36129999   0.24690001   0.45250002
    3.1225934 ]
 [660.20255867   0.22669999   0.2782       0.19270001   0.31939998
    3.13971901]
 [776.88902534   0.33890003   0.40400001   0.18969999   0.45380002
    3.61856914]
 ...
 [597.3675761    0.3565       0.41659999   0.19550002   0.45359999
    3.97124863]
 [694.08247117   0.3391       0.41300002   0.1882       0.4508
    3.42866874]
 [763.91078732   0.24819998   0.29519999   0.20740001   0.35010001
    3.1899097 ]][0m
[37m[1m[2023-07-17 13:56:29,333][257371] Max Reward on eval: 909.3382187425857[0m
[37m[1m[2023-07-17 13:56:29,333][257371] Min Reward on eval: 318.69121767142786[0m
[37m[1m[2023-07-17 13:56:29,333][257371] Mean Reward across all agents: 650.9633778093428[0m
[37m[1m[2023-07-17 13:56:29,333][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:56:29,335][257371] mean_value=-1247.4835205161241, max_value=167.91674453148755[0m
[37m[1m[2023-07-17 13:56:29,337][257371] New mean coefficients: [[-0.5916838  -0.04348287  0.56559336 -1.0974797   0.04431603  0.5057104 ]][0m
[37m[1m[2023-07-17 13:56:29,338][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:56:38,297][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 13:56:38,298][257371] FPS: 428690.92[0m
[36m[2023-07-17 13:56:38,300][257371] itr=1309, itrs=2000, Progress: 65.45%[0m
[36m[2023-07-17 13:56:49,976][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 13:56:49,976][257371] FPS: 332216.80[0m
[36m[2023-07-17 13:56:54,256][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:56:54,256][257371] Reward + Measures: [[680.74504876   0.30080065   0.35994467   0.18999934   0.40978068
    3.46444654]][0m
[37m[1m[2023-07-17 13:56:54,256][257371] Max Reward on eval: 680.7450487599657[0m
[37m[1m[2023-07-17 13:56:54,256][257371] Min Reward on eval: 680.7450487599657[0m
[37m[1m[2023-07-17 13:56:54,257][257371] Mean Reward across all agents: 680.7450487599657[0m
[37m[1m[2023-07-17 13:56:54,257][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:56:59,267][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 13:56:59,267][257371] Reward + Measures: [[659.58362907   0.3159       0.36200002   0.20030001   0.42670003
    3.68537569]
 [401.44612304   0.29720002   0.2471       0.2321       0.30330002
    3.14771771]
 [406.83501907   0.42589998   0.40009999   0.3617       0.46739998
    3.19568014]
 ...
 [283.75898333   0.25209999   0.1804       0.19679999   0.23439999
    3.1223886 ]
 [466.02600497   0.24100001   0.22920001   0.18709999   0.26859999
    3.12311721]
 [573.69789526   0.28529999   0.27790001   0.2263       0.37459999
    3.21030307]][0m
[37m[1m[2023-07-17 13:56:59,268][257371] Max Reward on eval: 825.3324165259139[0m
[37m[1m[2023-07-17 13:56:59,268][257371] Min Reward on eval: 61.68837615693919[0m
[37m[1m[2023-07-17 13:56:59,268][257371] Mean Reward across all agents: 468.8604511943737[0m
[37m[1m[2023-07-17 13:56:59,268][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 13:56:59,270][257371] mean_value=-2131.7124555528585, max_value=219.60290848636618[0m
[37m[1m[2023-07-17 13:56:59,272][257371] New mean coefficients: [[ 0.487974    0.00017812  0.302092   -1.2752147   0.31199548  0.23271203]][0m
[37m[1m[2023-07-17 13:56:59,273][257371] Moving the mean solution point...[0m
[36m[2023-07-17 13:57:08,325][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 13:57:08,325][257371] FPS: 424312.26[0m
[36m[2023-07-17 13:57:08,327][257371] itr=1310, itrs=2000, Progress: 65.50%[0m
[37m[1m[2023-07-17 14:01:00,794][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001290[0m
[36m[2023-07-17 14:01:13,527][257371] train() took 12.07 seconds to complete[0m
[36m[2023-07-17 14:01:13,527][257371] FPS: 318013.77[0m
[36m[2023-07-17 14:01:17,805][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:01:17,805][257371] Reward + Measures: [[696.11950396   0.30761331   0.36574236   0.18848498   0.41481698
    3.48283529]][0m
[37m[1m[2023-07-17 14:01:17,805][257371] Max Reward on eval: 696.1195039629066[0m
[37m[1m[2023-07-17 14:01:17,806][257371] Min Reward on eval: 696.1195039629066[0m
[37m[1m[2023-07-17 14:01:17,806][257371] Mean Reward across all agents: 696.1195039629066[0m
[37m[1m[2023-07-17 14:01:17,806][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:01:23,082][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:01:23,082][257371] Reward + Measures: [[ 89.15103179   0.26640001   0.26089999   0.24489999   0.22480002
    3.46666837]
 [571.08419501   0.3994       0.44570002   0.1585       0.47620001
    3.63694358]
 [352.22430506   0.2203       0.26480001   0.15799999   0.27340001
    3.32442641]
 ...
 [521.81312979   0.25690001   0.28099999   0.16730002   0.3461
    3.17429614]
 [477.19260985   0.33240002   0.37989998   0.1469       0.41220003
    3.71349788]
 [717.78551867   0.30280003   0.3646       0.2203       0.43340001
    3.21110892]][0m
[37m[1m[2023-07-17 14:01:23,083][257371] Max Reward on eval: 879.2166709626559[0m
[37m[1m[2023-07-17 14:01:23,083][257371] Min Reward on eval: 89.15103179281577[0m
[37m[1m[2023-07-17 14:01:23,083][257371] Mean Reward across all agents: 524.8710806880015[0m
[37m[1m[2023-07-17 14:01:23,083][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:01:23,085][257371] mean_value=-1523.2642752919626, max_value=102.3297749749963[0m
[37m[1m[2023-07-17 14:01:23,087][257371] New mean coefficients: [[ 0.06674978 -0.20479669  0.06073606 -1.1971724   0.39741212  0.06246625]][0m
[37m[1m[2023-07-17 14:01:23,088][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:01:32,177][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 14:01:32,177][257371] FPS: 422564.80[0m
[36m[2023-07-17 14:01:32,179][257371] itr=1311, itrs=2000, Progress: 65.55%[0m
[36m[2023-07-17 14:01:43,950][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 14:01:43,950][257371] FPS: 329433.88[0m
[36m[2023-07-17 14:01:48,190][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:01:48,190][257371] Reward + Measures: [[696.95416989   0.30533966   0.36044601   0.18599333   0.41067803
    3.48736906]][0m
[37m[1m[2023-07-17 14:01:48,190][257371] Max Reward on eval: 696.9541698932399[0m
[37m[1m[2023-07-17 14:01:48,190][257371] Min Reward on eval: 696.9541698932399[0m
[37m[1m[2023-07-17 14:01:48,191][257371] Mean Reward across all agents: 696.9541698932399[0m
[37m[1m[2023-07-17 14:01:48,191][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:01:53,141][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:01:53,142][257371] Reward + Measures: [[587.48698481   0.25740001   0.28380001   0.22040001   0.31130001
    3.27612543]
 [621.78519215   0.2911       0.36160001   0.1847       0.41809997
    3.45567942]
 [599.52870753   0.3506       0.4384       0.19260001   0.49360004
    3.70412254]
 ...
 [740.10290913   0.32350001   0.39089999   0.22040001   0.47089997
    3.50951552]
 [813.34539035   0.38280001   0.48740003   0.19149999   0.54229999
    3.72028899]
 [625.91269199   0.32730001   0.36770001   0.1743       0.38900003
    3.5685432 ]][0m
[37m[1m[2023-07-17 14:01:53,142][257371] Max Reward on eval: 838.1976470479742[0m
[37m[1m[2023-07-17 14:01:53,142][257371] Min Reward on eval: 70.94878628477454[0m
[37m[1m[2023-07-17 14:01:53,142][257371] Mean Reward across all agents: 520.4199453590675[0m
[37m[1m[2023-07-17 14:01:53,143][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:01:53,144][257371] mean_value=-1183.8391515335618, max_value=78.83828367150215[0m
[37m[1m[2023-07-17 14:01:53,147][257371] New mean coefficients: [[ 0.19596754 -0.10448825  0.1322023  -0.98483276  0.2177092   0.20078485]][0m
[37m[1m[2023-07-17 14:01:53,148][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:02:02,082][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 14:02:02,082][257371] FPS: 429885.82[0m
[36m[2023-07-17 14:02:02,084][257371] itr=1312, itrs=2000, Progress: 65.60%[0m
[36m[2023-07-17 14:02:13,738][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 14:02:13,738][257371] FPS: 332728.37[0m
[36m[2023-07-17 14:02:17,972][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:02:17,972][257371] Reward + Measures: [[685.54871433   0.30337167   0.35799101   0.18244401   0.40662366
    3.52245569]][0m
[37m[1m[2023-07-17 14:02:17,972][257371] Max Reward on eval: 685.5487143319282[0m
[37m[1m[2023-07-17 14:02:17,973][257371] Min Reward on eval: 685.5487143319282[0m
[37m[1m[2023-07-17 14:02:17,973][257371] Mean Reward across all agents: 685.5487143319282[0m
[37m[1m[2023-07-17 14:02:17,973][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:02:22,927][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:02:22,928][257371] Reward + Measures: [[619.9724368    0.30080003   0.26690003   0.22160001   0.39260003
    2.98347425]
 [412.58509346   0.4348       0.53200001   0.0873       0.54330009
    4.43813705]
 [454.90780959   0.4686       0.54530001   0.11979999   0.57239997
    4.73860264]
 ...
 [700.0571461    0.35300002   0.41209999   0.15449999   0.43969998
    3.68860674]
 [521.90845491   0.39020002   0.33200005   0.3046       0.4276
    3.17464709]
 [603.96372988   0.34029999   0.3741       0.2102       0.40829998
    3.53601813]][0m
[37m[1m[2023-07-17 14:02:22,928][257371] Max Reward on eval: 892.7856712192297[0m
[37m[1m[2023-07-17 14:02:22,928][257371] Min Reward on eval: 133.80244505090522[0m
[37m[1m[2023-07-17 14:02:22,928][257371] Mean Reward across all agents: 558.1853240551495[0m
[37m[1m[2023-07-17 14:02:22,929][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:02:22,930][257371] mean_value=-1129.1620969265975, max_value=269.81353984104965[0m
[37m[1m[2023-07-17 14:02:22,933][257371] New mean coefficients: [[-0.2604224  -0.04613124  0.23193236 -0.7723688   0.27622175 -0.31762868]][0m
[37m[1m[2023-07-17 14:02:22,934][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:02:31,982][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 14:02:31,982][257371] FPS: 424465.38[0m
[36m[2023-07-17 14:02:31,984][257371] itr=1313, itrs=2000, Progress: 65.65%[0m
[36m[2023-07-17 14:02:43,740][257371] train() took 11.64 seconds to complete[0m
[36m[2023-07-17 14:02:43,740][257371] FPS: 329863.61[0m
[36m[2023-07-17 14:02:48,020][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:02:48,021][257371] Reward + Measures: [[698.3051236    0.30713633   0.36453095   0.18433499   0.41482666
    3.50030589]][0m
[37m[1m[2023-07-17 14:02:48,021][257371] Max Reward on eval: 698.3051235972313[0m
[37m[1m[2023-07-17 14:02:48,021][257371] Min Reward on eval: 698.3051235972313[0m
[37m[1m[2023-07-17 14:02:48,021][257371] Mean Reward across all agents: 698.3051235972313[0m
[37m[1m[2023-07-17 14:02:48,022][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:02:52,913][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:02:52,914][257371] Reward + Measures: [[392.05185441   0.27289999   0.2924       0.1549       0.3035
    3.47041392]
 [593.26465559   0.27340004   0.34620002   0.2246       0.31000003
    3.22213674]
 [790.73373795   0.35139999   0.46560001   0.19670001   0.49270001
    3.61733174]
 ...
 [750.61141773   0.43570003   0.55949998   0.1349       0.5873
    4.16699457]
 [731.484623     0.44309998   0.5068       0.229        0.48559999
    3.76775169]
 [703.07040784   0.39720002   0.4849       0.17850001   0.50400001
    3.80804372]][0m
[37m[1m[2023-07-17 14:02:52,914][257371] Max Reward on eval: 857.9050140529405[0m
[37m[1m[2023-07-17 14:02:52,914][257371] Min Reward on eval: 202.26143096357117[0m
[37m[1m[2023-07-17 14:02:52,914][257371] Mean Reward across all agents: 579.4344610439952[0m
[37m[1m[2023-07-17 14:02:52,915][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:02:52,916][257371] mean_value=-1518.8337975602303, max_value=97.54517169859491[0m
[37m[1m[2023-07-17 14:02:52,919][257371] New mean coefficients: [[-0.0683662  -0.11410387  0.3011119  -0.9124316   0.4752388  -0.4651878 ]][0m
[37m[1m[2023-07-17 14:02:52,920][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:03:01,852][257371] train() took 8.93 seconds to complete[0m
[36m[2023-07-17 14:03:01,852][257371] FPS: 429990.28[0m
[36m[2023-07-17 14:03:01,854][257371] itr=1314, itrs=2000, Progress: 65.70%[0m
[36m[2023-07-17 14:03:13,672][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 14:03:13,673][257371] FPS: 328074.47[0m
[36m[2023-07-17 14:03:18,057][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:03:18,057][257371] Reward + Measures: [[707.64321098   0.30975533   0.36646333   0.18669534   0.41890633
    3.46202993]][0m
[37m[1m[2023-07-17 14:03:18,057][257371] Max Reward on eval: 707.6432109758952[0m
[37m[1m[2023-07-17 14:03:18,058][257371] Min Reward on eval: 707.6432109758952[0m
[37m[1m[2023-07-17 14:03:18,058][257371] Mean Reward across all agents: 707.6432109758952[0m
[37m[1m[2023-07-17 14:03:18,058][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:03:23,115][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:03:23,116][257371] Reward + Measures: [[ 407.02463914    0.29820001    0.28850001    0.32430002    0.33230004
     3.22827005]
 [ 598.10042195    0.38549998    0.60869998    0.223         0.63980001
     4.42199898]
 [-149.7523562     0.47420001    0.51310003    0.40570003    0.23810001
     5.2057147 ]
 ...
 [ 405.55751995    0.29550001    0.24129999    0.28940001    0.27329999
     2.93395209]
 [ 620.85172655    0.33970004    0.4007        0.2142        0.44140002
     3.68015981]
 [ 567.58524338    0.2766        0.2868        0.17490001    0.34580001
     3.20370412]][0m
[37m[1m[2023-07-17 14:03:23,116][257371] Max Reward on eval: 813.3851203726605[0m
[37m[1m[2023-07-17 14:03:23,116][257371] Min Reward on eval: -265.02112718373536[0m
[37m[1m[2023-07-17 14:03:23,116][257371] Mean Reward across all agents: 452.6943896181294[0m
[37m[1m[2023-07-17 14:03:23,117][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:03:23,118][257371] mean_value=-1540.754206991184, max_value=204.1163948487565[0m
[37m[1m[2023-07-17 14:03:23,121][257371] New mean coefficients: [[-0.11933198 -0.11401756  0.20965329 -0.9984499   0.6242136  -0.68409   ]][0m
[37m[1m[2023-07-17 14:03:23,122][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:03:32,219][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 14:03:32,220][257371] FPS: 422173.60[0m
[36m[2023-07-17 14:03:32,222][257371] itr=1315, itrs=2000, Progress: 65.75%[0m
[36m[2023-07-17 14:03:44,091][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 14:03:44,092][257371] FPS: 326619.72[0m
[36m[2023-07-17 14:03:48,448][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:03:48,449][257371] Reward + Measures: [[697.31795551   0.30371967   0.36217764   0.18705668   0.41619369
    3.44005919]][0m
[37m[1m[2023-07-17 14:03:48,449][257371] Max Reward on eval: 697.3179555144127[0m
[37m[1m[2023-07-17 14:03:48,449][257371] Min Reward on eval: 697.3179555144127[0m
[37m[1m[2023-07-17 14:03:48,450][257371] Mean Reward across all agents: 697.3179555144127[0m
[37m[1m[2023-07-17 14:03:48,450][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:03:53,493][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:03:53,494][257371] Reward + Measures: [[428.33900178   0.2296       0.24499999   0.1856       0.24520002
    3.35138774]
 [419.3657164    0.26610002   0.27180001   0.22500001   0.26540002
    3.38521314]
 [646.26257418   0.43540001   0.4876       0.16489999   0.47839999
    3.81840396]
 ...
 [663.93458132   0.37200001   0.46580002   0.14359999   0.51550001
    3.88731265]
 [680.61192712   0.34020001   0.39429998   0.1882       0.39159998
    3.61056685]
 [522.65231129   0.3274       0.42989999   0.182        0.40970001
    3.83051491]][0m
[37m[1m[2023-07-17 14:03:53,494][257371] Max Reward on eval: 881.5009994266554[0m
[37m[1m[2023-07-17 14:03:53,494][257371] Min Reward on eval: 199.67942511644213[0m
[37m[1m[2023-07-17 14:03:53,494][257371] Mean Reward across all agents: 578.8202122258007[0m
[37m[1m[2023-07-17 14:03:53,495][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:03:53,496][257371] mean_value=-1594.2580618173924, max_value=21.05936122823789[0m
[37m[1m[2023-07-17 14:03:53,498][257371] New mean coefficients: [[-0.05856011  0.12422565  0.4024333  -0.7655289   0.49254465 -0.66476506]][0m
[37m[1m[2023-07-17 14:03:53,499][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:04:02,544][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 14:04:02,545][257371] FPS: 424619.75[0m
[36m[2023-07-17 14:04:02,547][257371] itr=1316, itrs=2000, Progress: 65.80%[0m
[36m[2023-07-17 14:04:14,318][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 14:04:14,318][257371] FPS: 329396.25[0m
[36m[2023-07-17 14:04:18,657][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:04:18,657][257371] Reward + Measures: [[700.09932607   0.30245402   0.36092699   0.18567799   0.41758999
    3.3993547 ]][0m
[37m[1m[2023-07-17 14:04:18,657][257371] Max Reward on eval: 700.0993260651488[0m
[37m[1m[2023-07-17 14:04:18,658][257371] Min Reward on eval: 700.0993260651488[0m
[37m[1m[2023-07-17 14:04:18,658][257371] Mean Reward across all agents: 700.0993260651488[0m
[37m[1m[2023-07-17 14:04:18,658][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:04:23,907][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:04:23,908][257371] Reward + Measures: [[419.75615894   0.26499999   0.2667       0.22069998   0.30890003
    3.14558411]
 [637.47667953   0.32890001   0.35609999   0.20970002   0.4048
    3.30533767]
 [710.57523727   0.3163       0.41490003   0.20999999   0.46870002
    3.63649058]
 ...
 [648.84660564   0.31780002   0.38110003   0.1868       0.44060001
    3.60710502]
 [406.02434872   0.37450001   0.44619998   0.132        0.44960004
    4.4423542 ]
 [302.86026675   0.31290001   0.465        0.1647       0.47370005
    4.09557867]][0m
[37m[1m[2023-07-17 14:04:23,908][257371] Max Reward on eval: 869.2547683514189[0m
[37m[1m[2023-07-17 14:04:23,908][257371] Min Reward on eval: -72.84852783333045[0m
[37m[1m[2023-07-17 14:04:23,909][257371] Mean Reward across all agents: 579.5021020678513[0m
[37m[1m[2023-07-17 14:04:23,909][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:04:23,911][257371] mean_value=-1116.8393617156391, max_value=277.1481117012399[0m
[37m[1m[2023-07-17 14:04:23,913][257371] New mean coefficients: [[ 0.5841998  -0.15854442  0.6697506  -1.119772    0.6060968  -0.7805271 ]][0m
[37m[1m[2023-07-17 14:04:23,914][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:04:32,914][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 14:04:32,914][257371] FPS: 426774.62[0m
[36m[2023-07-17 14:04:32,916][257371] itr=1317, itrs=2000, Progress: 65.85%[0m
[36m[2023-07-17 14:04:44,663][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 14:04:44,664][257371] FPS: 330041.12[0m
[36m[2023-07-17 14:04:48,924][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:04:48,925][257371] Reward + Measures: [[708.68060894   0.30249968   0.36557466   0.18973599   0.42510766
    3.37385964]][0m
[37m[1m[2023-07-17 14:04:48,925][257371] Max Reward on eval: 708.6806089350439[0m
[37m[1m[2023-07-17 14:04:48,925][257371] Min Reward on eval: 708.6806089350439[0m
[37m[1m[2023-07-17 14:04:48,926][257371] Mean Reward across all agents: 708.6806089350439[0m
[37m[1m[2023-07-17 14:04:48,926][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:04:53,846][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:04:53,847][257371] Reward + Measures: [[503.61523197   0.35050002   0.52160001   0.2333       0.50699997
    3.78396773]
 [ 79.41925466   0.21040002   0.21560001   0.17279999   0.16110002
    3.73476911]
 [450.81610913   0.22950001   0.34800002   0.15799999   0.37160003
    3.26091123]
 ...
 [ 34.22694793   0.3838       0.3651       0.2368       0.20279999
    5.08386087]
 [693.56805212   0.26789999   0.34219998   0.2343       0.40279999
    3.26464891]
 [ 86.86126315   0.26989999   0.38799998   0.1559       0.3673
    4.3359642 ]][0m
[37m[1m[2023-07-17 14:04:53,847][257371] Max Reward on eval: 880.1963997240179[0m
[37m[1m[2023-07-17 14:04:53,848][257371] Min Reward on eval: -115.97803161628545[0m
[37m[1m[2023-07-17 14:04:53,848][257371] Mean Reward across all agents: 393.40309638360776[0m
[37m[1m[2023-07-17 14:04:53,848][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:04:53,850][257371] mean_value=-1566.295278063075, max_value=70.66385113293782[0m
[37m[1m[2023-07-17 14:04:53,852][257371] New mean coefficients: [[ 0.6040036   0.3049247   0.62875044 -1.0501132   0.50133973 -0.932477  ]][0m
[37m[1m[2023-07-17 14:04:53,853][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:05:02,839][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 14:05:02,839][257371] FPS: 427416.62[0m
[36m[2023-07-17 14:05:02,842][257371] itr=1318, itrs=2000, Progress: 65.90%[0m
[36m[2023-07-17 14:05:14,618][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 14:05:14,618][257371] FPS: 329301.70[0m
[36m[2023-07-17 14:05:18,887][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:05:18,893][257371] Reward + Measures: [[708.58254841   0.29761267   0.36422232   0.18882801   0.42519265
    3.33883786]][0m
[37m[1m[2023-07-17 14:05:18,893][257371] Max Reward on eval: 708.5825484116605[0m
[37m[1m[2023-07-17 14:05:18,893][257371] Min Reward on eval: 708.5825484116605[0m
[37m[1m[2023-07-17 14:05:18,894][257371] Mean Reward across all agents: 708.5825484116605[0m
[37m[1m[2023-07-17 14:05:18,894][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:05:23,889][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:05:23,895][257371] Reward + Measures: [[686.55828213   0.32459998   0.42360002   0.1789       0.47340003
    3.60606241]
 [743.07368089   0.28819999   0.39520001   0.22379999   0.43920001
    3.32697868]
 [817.21445081   0.27490002   0.35280001   0.21399999   0.42070004
    3.17258525]
 ...
 [487.71843651   0.27509999   0.34110001   0.1586       0.38870001
    3.50824928]
 [803.57012176   0.4244       0.54329997   0.1622       0.59450001
    4.0380578 ]
 [594.63277054   0.40510002   0.6839       0.28850001   0.70200002
    4.91280556]][0m
[37m[1m[2023-07-17 14:05:23,895][257371] Max Reward on eval: 847.918670709501[0m
[37m[1m[2023-07-17 14:05:23,895][257371] Min Reward on eval: 297.48223613873125[0m
[37m[1m[2023-07-17 14:05:23,896][257371] Mean Reward across all agents: 609.0214711853941[0m
[37m[1m[2023-07-17 14:05:23,896][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:05:23,897][257371] mean_value=-998.3279087898637, max_value=66.57788787962693[0m
[37m[1m[2023-07-17 14:05:23,900][257371] New mean coefficients: [[ 0.40902054  0.00030583  0.5124341  -0.8988527   0.8974786  -0.9653412 ]][0m
[37m[1m[2023-07-17 14:05:23,901][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:05:32,910][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 14:05:32,910][257371] FPS: 426335.97[0m
[36m[2023-07-17 14:05:32,912][257371] itr=1319, itrs=2000, Progress: 65.95%[0m
[36m[2023-07-17 14:05:44,739][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 14:05:44,739][257371] FPS: 327922.00[0m
[36m[2023-07-17 14:05:49,037][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:05:49,038][257371] Reward + Measures: [[722.63477999   0.29947999   0.36978236   0.18987833   0.43406767
    3.29938865]][0m
[37m[1m[2023-07-17 14:05:49,038][257371] Max Reward on eval: 722.6347799863744[0m
[37m[1m[2023-07-17 14:05:49,038][257371] Min Reward on eval: 722.6347799863744[0m
[37m[1m[2023-07-17 14:05:49,038][257371] Mean Reward across all agents: 722.6347799863744[0m
[37m[1m[2023-07-17 14:05:49,039][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:05:54,021][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:05:54,021][257371] Reward + Measures: [[665.01494219   0.3258       0.44110003   0.20420001   0.49540001
    3.33968472]
 [612.08924102   0.36810002   0.4443       0.18550001   0.4585
    3.69736266]
 [510.24250143   0.19590001   0.28980002   0.1613       0.28999999
    3.45943427]
 ...
 [708.72203447   0.33519998   0.3484       0.266        0.49470001
    2.97830844]
 [294.66175442   0.4786       0.442        0.39110002   0.33430001
    3.70264435]
 [585.75790249   0.29539999   0.41219997   0.19310001   0.51480001
    3.44231606]][0m
[37m[1m[2023-07-17 14:05:54,021][257371] Max Reward on eval: 871.7415389918955[0m
[37m[1m[2023-07-17 14:05:54,022][257371] Min Reward on eval: 67.06384424315766[0m
[37m[1m[2023-07-17 14:05:54,022][257371] Mean Reward across all agents: 564.2041582975752[0m
[37m[1m[2023-07-17 14:05:54,022][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:05:54,024][257371] mean_value=-949.0484953302363, max_value=155.26160564635467[0m
[37m[1m[2023-07-17 14:05:54,026][257371] New mean coefficients: [[ 0.17573161  0.07332543  0.5443991  -0.59780693  0.7678344  -0.5956166 ]][0m
[37m[1m[2023-07-17 14:05:54,027][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:06:02,972][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 14:06:02,972][257371] FPS: 429402.18[0m
[36m[2023-07-17 14:06:02,974][257371] itr=1320, itrs=2000, Progress: 66.00%[0m
[37m[1m[2023-07-17 14:09:30,211][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001300[0m
[36m[2023-07-17 14:09:42,610][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 14:09:42,610][257371] FPS: 328868.64[0m
[36m[2023-07-17 14:09:46,823][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:09:46,823][257371] Reward + Measures: [[732.69520492   0.29645666   0.37080437   0.194763     0.43844998
    3.24816275]][0m
[37m[1m[2023-07-17 14:09:46,823][257371] Max Reward on eval: 732.6952049242932[0m
[37m[1m[2023-07-17 14:09:46,824][257371] Min Reward on eval: 732.6952049242932[0m
[37m[1m[2023-07-17 14:09:46,824][257371] Mean Reward across all agents: 732.6952049242932[0m
[37m[1m[2023-07-17 14:09:46,824][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:09:52,040][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:09:52,041][257371] Reward + Measures: [[497.59309579   0.22849999   0.31999999   0.18450001   0.3655
    3.59353423]
 [660.136807     0.35340002   0.42090002   0.21100001   0.44910002
    3.38673258]
 [679.24166487   0.32930002   0.3522       0.24239998   0.56330001
    2.94746661]
 ...
 [613.06280711   0.22789998   0.3389       0.19460002   0.39760002
    3.38321686]
 [583.13295171   0.38480002   0.37800002   0.22690001   0.50700003
    3.25325561]
 [870.01078799   0.32220003   0.4066       0.18620001   0.53380001
    3.18032575]][0m
[37m[1m[2023-07-17 14:09:52,041][257371] Max Reward on eval: 870.0107879931223[0m
[37m[1m[2023-07-17 14:09:52,041][257371] Min Reward on eval: -39.7252283771988[0m
[37m[1m[2023-07-17 14:09:52,041][257371] Mean Reward across all agents: 607.7648927481152[0m
[37m[1m[2023-07-17 14:09:52,042][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:09:52,043][257371] mean_value=-773.7222547892012, max_value=99.0307492180466[0m
[37m[1m[2023-07-17 14:09:52,046][257371] New mean coefficients: [[ 0.41412413 -0.09370861  0.46940112 -0.6658323   0.71777964 -0.59252614]][0m
[37m[1m[2023-07-17 14:09:52,047][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:10:01,041][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 14:10:01,041][257371] FPS: 427024.20[0m
[36m[2023-07-17 14:10:01,043][257371] itr=1321, itrs=2000, Progress: 66.05%[0m
[36m[2023-07-17 14:10:12,927][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 14:10:12,927][257371] FPS: 326279.51[0m
[36m[2023-07-17 14:10:17,220][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:10:17,220][257371] Reward + Measures: [[743.98750697   0.29348099   0.37078199   0.19887102   0.44221997
    3.21867824]][0m
[37m[1m[2023-07-17 14:10:17,220][257371] Max Reward on eval: 743.9875069652417[0m
[37m[1m[2023-07-17 14:10:17,220][257371] Min Reward on eval: 743.9875069652417[0m
[37m[1m[2023-07-17 14:10:17,221][257371] Mean Reward across all agents: 743.9875069652417[0m
[37m[1m[2023-07-17 14:10:17,221][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:10:22,160][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:10:22,161][257371] Reward + Measures: [[488.61302192   0.1329       0.66720003   0.45259997   0.67650002
    5.6356616 ]
 [545.90708638   0.2773       0.34209999   0.18530001   0.35340002
    3.66479802]
 [826.04768369   0.3434       0.42810002   0.20039999   0.4966
    3.59668016]
 ...
 [779.40721134   0.25580001   0.34889999   0.2059       0.43109998
    3.16105556]
 [404.34542179   0.21729998   0.82509995   0.42390004   0.80240005
    5.39953566]
 [661.62746292   0.30969998   0.36130002   0.2059       0.4585
    3.40635085]][0m
[37m[1m[2023-07-17 14:10:22,161][257371] Max Reward on eval: 880.1503448193428[0m
[37m[1m[2023-07-17 14:10:22,161][257371] Min Reward on eval: 76.27733825105243[0m
[37m[1m[2023-07-17 14:10:22,162][257371] Mean Reward across all agents: 513.09238994399[0m
[37m[1m[2023-07-17 14:10:22,162][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:10:22,164][257371] mean_value=-962.3376531838121, max_value=190.49113356828065[0m
[37m[1m[2023-07-17 14:10:22,167][257371] New mean coefficients: [[ 1.1375421  -0.12301382  0.27884004 -0.53319263  0.7418065  -0.72882044]][0m
[37m[1m[2023-07-17 14:10:22,168][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:10:31,171][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 14:10:31,171][257371] FPS: 426591.26[0m
[36m[2023-07-17 14:10:31,173][257371] itr=1322, itrs=2000, Progress: 66.10%[0m
[36m[2023-07-17 14:10:42,902][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 14:10:42,902][257371] FPS: 330649.85[0m
[36m[2023-07-17 14:10:47,126][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:10:47,126][257371] Reward + Measures: [[752.31849588   0.29274264   0.37056965   0.197512     0.44400597
    3.19372272]][0m
[37m[1m[2023-07-17 14:10:47,126][257371] Max Reward on eval: 752.318495884687[0m
[37m[1m[2023-07-17 14:10:47,127][257371] Min Reward on eval: 752.318495884687[0m
[37m[1m[2023-07-17 14:10:47,127][257371] Mean Reward across all agents: 752.318495884687[0m
[37m[1m[2023-07-17 14:10:47,127][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:10:52,022][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:10:52,022][257371] Reward + Measures: [[653.63143157   0.34650001   0.41619998   0.2119       0.46730003
    3.59024549]
 [440.95039933   0.32980001   0.3335       0.28639999   0.32249999
    3.19625711]
 [634.96131902   0.2967       0.42230001   0.1952       0.49910003
    3.14292336]
 ...
 [577.93878679   0.29910001   0.36500001   0.2057       0.46690002
    3.57602549]
 [570.48456331   0.25890002   0.36090001   0.1639       0.38400003
    3.31714034]
 [456.85988808   0.23839998   0.50590003   0.2168       0.55669999
    4.22175217]][0m
[37m[1m[2023-07-17 14:10:52,022][257371] Max Reward on eval: 880.5526428551414[0m
[37m[1m[2023-07-17 14:10:52,023][257371] Min Reward on eval: 197.4169255080633[0m
[37m[1m[2023-07-17 14:10:52,023][257371] Mean Reward across all agents: 599.318702215646[0m
[37m[1m[2023-07-17 14:10:52,023][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:10:52,025][257371] mean_value=-1425.5028122458284, max_value=177.57031483709557[0m
[37m[1m[2023-07-17 14:10:52,028][257371] New mean coefficients: [[ 0.5525832  -0.07662754  0.74535626 -0.4851447   0.68973905 -0.82059604]][0m
[37m[1m[2023-07-17 14:10:52,029][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:11:01,064][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 14:11:01,065][257371] FPS: 425065.11[0m
[36m[2023-07-17 14:11:01,067][257371] itr=1323, itrs=2000, Progress: 66.15%[0m
[36m[2023-07-17 14:11:13,314][257371] train() took 12.13 seconds to complete[0m
[36m[2023-07-17 14:11:13,314][257371] FPS: 316489.86[0m
[36m[2023-07-17 14:11:17,625][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:11:17,625][257371] Reward + Measures: [[759.61734364   0.29294997   0.37140265   0.198599     0.44847071
    3.16940665]][0m
[37m[1m[2023-07-17 14:11:17,625][257371] Max Reward on eval: 759.6173436374695[0m
[37m[1m[2023-07-17 14:11:17,626][257371] Min Reward on eval: 759.6173436374695[0m
[37m[1m[2023-07-17 14:11:17,626][257371] Mean Reward across all agents: 759.6173436374695[0m
[37m[1m[2023-07-17 14:11:17,626][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:11:22,639][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:11:22,639][257371] Reward + Measures: [[725.68809988   0.36719999   0.40809998   0.212        0.47129998
    3.47064018]
 [527.87807762   0.4955       0.53950006   0.14690001   0.58399999
    3.82186484]
 [806.9269848    0.28410003   0.32359999   0.22239999   0.40889999
    3.06093287]
 ...
 [670.85248373   0.34260002   0.38680002   0.2112       0.4481
    3.34388518]
 [661.85612102   0.3335       0.40889999   0.24889998   0.45700002
    3.21100616]
 [628.20137787   0.4041       0.51480001   0.12719999   0.56389999
    3.72315526]][0m
[37m[1m[2023-07-17 14:11:22,640][257371] Max Reward on eval: 896.0588608485647[0m
[37m[1m[2023-07-17 14:11:22,640][257371] Min Reward on eval: 167.326212477684[0m
[37m[1m[2023-07-17 14:11:22,640][257371] Mean Reward across all agents: 596.5332166881857[0m
[37m[1m[2023-07-17 14:11:22,640][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:11:22,642][257371] mean_value=-1032.14468431689, max_value=71.88541268078984[0m
[37m[1m[2023-07-17 14:11:22,644][257371] New mean coefficients: [[ 0.25695416  0.31525344  0.90014565 -0.37745243  0.10547662 -0.9464297 ]][0m
[37m[1m[2023-07-17 14:11:22,645][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:11:31,796][257371] train() took 9.15 seconds to complete[0m
[36m[2023-07-17 14:11:31,796][257371] FPS: 419738.35[0m
[36m[2023-07-17 14:11:31,798][257371] itr=1324, itrs=2000, Progress: 66.20%[0m
[36m[2023-07-17 14:11:43,748][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 14:11:43,748][257371] FPS: 324520.08[0m
[36m[2023-07-17 14:11:48,129][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:11:48,130][257371] Reward + Measures: [[765.91370137   0.29450667   0.37861702   0.200453     0.45510733
    3.14239764]][0m
[37m[1m[2023-07-17 14:11:48,130][257371] Max Reward on eval: 765.9137013660194[0m
[37m[1m[2023-07-17 14:11:48,130][257371] Min Reward on eval: 765.9137013660194[0m
[37m[1m[2023-07-17 14:11:48,130][257371] Mean Reward across all agents: 765.9137013660194[0m
[37m[1m[2023-07-17 14:11:48,131][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:11:53,154][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:11:53,155][257371] Reward + Measures: [[678.00070561   0.25370002   0.30720001   0.20609999   0.39129999
    2.99251199]
 [761.32433511   0.36770001   0.45770001   0.1816       0.49909997
    3.48268557]
 [744.71409607   0.46110001   0.51980001   0.17470001   0.56290001
    3.80430412]
 ...
 [788.9516297    0.29340002   0.38930002   0.2089       0.50740004
    3.38063931]
 [643.85531806   0.26270002   0.54090005   0.32370004   0.58319998
    4.1347971 ]
 [585.64630126   0.3448       0.43770003   0.26070002   0.54469997
    3.69940615]][0m
[37m[1m[2023-07-17 14:11:53,155][257371] Max Reward on eval: 871.0784683271311[0m
[37m[1m[2023-07-17 14:11:53,155][257371] Min Reward on eval: 62.569677957287055[0m
[37m[1m[2023-07-17 14:11:53,155][257371] Mean Reward across all agents: 574.2209263938282[0m
[37m[1m[2023-07-17 14:11:53,156][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:11:53,157][257371] mean_value=-903.7648106204953, max_value=275.2822108692895[0m
[37m[1m[2023-07-17 14:11:53,160][257371] New mean coefficients: [[ 0.24095593  0.3290936   0.9405807  -0.31170043  0.29446107 -1.350728  ]][0m
[37m[1m[2023-07-17 14:11:53,161][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:12:02,256][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 14:12:02,256][257371] FPS: 422276.60[0m
[36m[2023-07-17 14:12:02,259][257371] itr=1325, itrs=2000, Progress: 66.25%[0m
[36m[2023-07-17 14:12:14,121][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 14:12:14,122][257371] FPS: 326841.24[0m
[36m[2023-07-17 14:12:18,360][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:12:18,361][257371] Reward + Measures: [[769.26903092   0.29137665   0.37887099   0.20714933   0.45864266
    3.0875175 ]][0m
[37m[1m[2023-07-17 14:12:18,361][257371] Max Reward on eval: 769.2690309215096[0m
[37m[1m[2023-07-17 14:12:18,361][257371] Min Reward on eval: 769.2690309215096[0m
[37m[1m[2023-07-17 14:12:18,362][257371] Mean Reward across all agents: 769.2690309215096[0m
[37m[1m[2023-07-17 14:12:18,362][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:12:23,323][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:12:23,324][257371] Reward + Measures: [[865.37226104   0.32699999   0.42940003   0.23020001   0.48899999
    3.21044779]
 [649.91351676   0.2297       0.3071       0.1926       0.39449999
    2.98236275]
 [795.37981221   0.27010003   0.36660001   0.2362       0.45339999
    2.95450997]
 ...
 [521.83401367   0.33010003   0.4278       0.15190001   0.46400005
    3.71504283]
 [716.51102736   0.3082       0.39290005   0.17820001   0.44639999
    3.37126136]
 [754.75593824   0.36520001   0.45190001   0.16940001   0.53439999
    3.46977091]][0m
[37m[1m[2023-07-17 14:12:23,324][257371] Max Reward on eval: 894.0536652160342[0m
[37m[1m[2023-07-17 14:12:23,324][257371] Min Reward on eval: 255.38126214980147[0m
[37m[1m[2023-07-17 14:12:23,324][257371] Mean Reward across all agents: 676.808066529939[0m
[37m[1m[2023-07-17 14:12:23,325][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:12:23,326][257371] mean_value=-895.1362636837381, max_value=121.51721199254018[0m
[37m[1m[2023-07-17 14:12:23,328][257371] New mean coefficients: [[ 0.26861423  0.4511575   0.798103   -0.36795706 -0.27230126 -1.7986475 ]][0m
[37m[1m[2023-07-17 14:12:23,329][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:12:32,325][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 14:12:32,325][257371] FPS: 426942.66[0m
[36m[2023-07-17 14:12:32,328][257371] itr=1326, itrs=2000, Progress: 66.30%[0m
[36m[2023-07-17 14:12:44,351][257371] train() took 11.91 seconds to complete[0m
[36m[2023-07-17 14:12:44,351][257371] FPS: 322493.15[0m
[36m[2023-07-17 14:12:48,661][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:12:48,661][257371] Reward + Measures: [[765.60253971   0.28794      0.38262436   0.20727433   0.4570083
    3.05675983]][0m
[37m[1m[2023-07-17 14:12:48,661][257371] Max Reward on eval: 765.6025397112711[0m
[37m[1m[2023-07-17 14:12:48,662][257371] Min Reward on eval: 765.6025397112711[0m
[37m[1m[2023-07-17 14:12:48,662][257371] Mean Reward across all agents: 765.6025397112711[0m
[37m[1m[2023-07-17 14:12:48,662][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:12:53,972][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:12:53,973][257371] Reward + Measures: [[517.67546566   0.2902       0.38409999   0.21799998   0.39210001
    3.22817731]
 [831.83412932   0.42050001   0.53439999   0.19270001   0.58059996
    3.53043866]
 [390.61818477   0.24980001   0.2798       0.20679998   0.2852
    3.33585358]
 ...
 [486.73638958   0.40159997   0.46409997   0.17060001   0.49139997
    3.69084096]
 [379.50198623   0.2994       0.31610003   0.23450001   0.3053
    3.16381669]
 [253.95458269   0.31040001   0.32799998   0.2784       0.34850001
    3.48093724]][0m
[37m[1m[2023-07-17 14:12:53,973][257371] Max Reward on eval: 876.3344192970777[0m
[37m[1m[2023-07-17 14:12:53,973][257371] Min Reward on eval: 71.55918541398714[0m
[37m[1m[2023-07-17 14:12:53,974][257371] Mean Reward across all agents: 504.81107228229774[0m
[37m[1m[2023-07-17 14:12:53,974][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:12:53,975][257371] mean_value=-1907.0099397327324, max_value=49.77324166229414[0m
[37m[1m[2023-07-17 14:12:53,978][257371] New mean coefficients: [[ 0.61322683  0.27118254  0.1862753  -0.30246386 -0.10754716 -1.6468835 ]][0m
[37m[1m[2023-07-17 14:12:53,979][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:13:03,061][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 14:13:03,061][257371] FPS: 422888.73[0m
[36m[2023-07-17 14:13:03,063][257371] itr=1327, itrs=2000, Progress: 66.35%[0m
[36m[2023-07-17 14:13:14,798][257371] train() took 11.62 seconds to complete[0m
[36m[2023-07-17 14:13:14,799][257371] FPS: 330528.61[0m
[36m[2023-07-17 14:13:19,043][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:13:19,044][257371] Reward + Measures: [[782.34940867   0.29328132   0.38797298   0.20789766   0.46300098
    3.04784179]][0m
[37m[1m[2023-07-17 14:13:19,044][257371] Max Reward on eval: 782.3494086662231[0m
[37m[1m[2023-07-17 14:13:19,044][257371] Min Reward on eval: 782.3494086662231[0m
[37m[1m[2023-07-17 14:13:19,045][257371] Mean Reward across all agents: 782.3494086662231[0m
[37m[1m[2023-07-17 14:13:19,045][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:13:23,996][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:13:23,996][257371] Reward + Measures: [[461.42184433   0.43810001   0.39300001   0.32690001   0.4711
    3.76801991]
 [513.06145659   0.278        0.53429997   0.2445       0.55380005
    4.55890417]
 [625.77842921   0.28749999   0.33750001   0.2271       0.40120003
    3.16669106]
 ...
 [557.34528348   0.32690001   0.46070001   0.27150002   0.53119999
    2.86433148]
 [409.40659755   0.36170003   0.4402       0.32000002   0.49890003
    2.94552279]
 [681.23879885   0.38550004   0.36019999   0.24820001   0.51520002
    2.8905232 ]][0m
[37m[1m[2023-07-17 14:13:23,996][257371] Max Reward on eval: 833.9329623904894[0m
[37m[1m[2023-07-17 14:13:23,997][257371] Min Reward on eval: 24.005128786619753[0m
[37m[1m[2023-07-17 14:13:23,997][257371] Mean Reward across all agents: 515.2750163313076[0m
[37m[1m[2023-07-17 14:13:23,997][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:13:24,001][257371] mean_value=-472.785205018087, max_value=377.1690501727581[0m
[37m[1m[2023-07-17 14:13:24,003][257371] New mean coefficients: [[ 0.7425957   0.3295037   0.04089178 -0.00674453 -0.00527547 -1.5659025 ]][0m
[37m[1m[2023-07-17 14:13:24,004][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:13:33,088][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 14:13:33,089][257371] FPS: 422796.43[0m
[36m[2023-07-17 14:13:33,091][257371] itr=1328, itrs=2000, Progress: 66.40%[0m
[36m[2023-07-17 14:13:45,127][257371] train() took 11.92 seconds to complete[0m
[36m[2023-07-17 14:13:45,127][257371] FPS: 322169.99[0m
[36m[2023-07-17 14:13:49,174][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:13:49,174][257371] Reward + Measures: [[789.62539616   0.29548699   0.39319402   0.20824167   0.46778667
    3.03540373]][0m
[37m[1m[2023-07-17 14:13:49,174][257371] Max Reward on eval: 789.6253961615504[0m
[37m[1m[2023-07-17 14:13:49,175][257371] Min Reward on eval: 789.6253961615504[0m
[37m[1m[2023-07-17 14:13:49,175][257371] Mean Reward across all agents: 789.6253961615504[0m
[37m[1m[2023-07-17 14:13:49,175][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:13:53,893][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:13:53,894][257371] Reward + Measures: [[791.60000963   0.3096       0.41890001   0.212        0.50200003
    2.96457934]
 [880.48062898   0.3292       0.34690005   0.2631       0.45760003
    2.75951529]
 [345.5537315    0.22750001   0.23650002   0.22379999   0.278
    2.91490221]
 ...
 [624.80110982   0.29949999   0.36450002   0.2402       0.419
    2.93504834]
 [339.36955163   0.28420001   0.31169999   0.2825       0.3292
    3.26310468]
 [ 91.58089537   0.40570003   0.4154       0.23290001   0.23150001
    4.84670734]][0m
[37m[1m[2023-07-17 14:13:53,894][257371] Max Reward on eval: 925.7955474749208[0m
[37m[1m[2023-07-17 14:13:53,894][257371] Min Reward on eval: -20.690256413817405[0m
[37m[1m[2023-07-17 14:13:53,894][257371] Mean Reward across all agents: 526.0764626769143[0m
[37m[1m[2023-07-17 14:13:53,895][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:13:53,896][257371] mean_value=-1059.331891148072, max_value=835.8560512624412[0m
[37m[1m[2023-07-17 14:13:53,899][257371] New mean coefficients: [[ 0.13143098  0.6026387  -0.00119695  0.18912566 -0.06764489 -1.2982068 ]][0m
[37m[1m[2023-07-17 14:13:53,900][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:14:02,906][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 14:14:02,906][257371] FPS: 426445.43[0m
[36m[2023-07-17 14:14:02,908][257371] itr=1329, itrs=2000, Progress: 66.45%[0m
[36m[2023-07-17 14:14:14,633][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 14:14:14,633][257371] FPS: 330764.25[0m
[36m[2023-07-17 14:14:18,874][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:14:18,880][257371] Reward + Measures: [[790.67398644   0.28807166   0.38670164   0.21264099   0.466012
    2.97690821]][0m
[37m[1m[2023-07-17 14:14:18,880][257371] Max Reward on eval: 790.6739864420102[0m
[37m[1m[2023-07-17 14:14:18,881][257371] Min Reward on eval: 790.6739864420102[0m
[37m[1m[2023-07-17 14:14:18,881][257371] Mean Reward across all agents: 790.6739864420102[0m
[37m[1m[2023-07-17 14:14:18,881][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:14:23,873][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:14:23,879][257371] Reward + Measures: [[568.41234112   0.212        0.2895       0.1718       0.35300002
    2.93740606]
 [332.08672671   0.2149       0.48179999   0.2694       0.50089997
    4.02675581]
 [719.38706492   0.34050003   0.43410006   0.21300001   0.47819996
    3.00959134]
 ...
 [709.52103038   0.28230003   0.3876       0.2246       0.48140001
    2.7195096 ]
 [851.87382508   0.43439999   0.48740003   0.17419998   0.53810006
    3.47735786]
 [582.30030542   0.34020004   0.36429998   0.16410001   0.37439999
    3.27995682]][0m
[37m[1m[2023-07-17 14:14:23,879][257371] Max Reward on eval: 965.09637446329[0m
[37m[1m[2023-07-17 14:14:23,879][257371] Min Reward on eval: 76.91399863467086[0m
[37m[1m[2023-07-17 14:14:23,880][257371] Mean Reward across all agents: 624.5319046440544[0m
[37m[1m[2023-07-17 14:14:23,880][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:14:23,882][257371] mean_value=-629.9848640630943, max_value=332.9842359356903[0m
[37m[1m[2023-07-17 14:14:23,884][257371] New mean coefficients: [[-0.40776098  0.9713837  -0.22702461  0.20093343  0.11672807 -1.5459116 ]][0m
[37m[1m[2023-07-17 14:14:23,885][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:14:32,917][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 14:14:32,917][257371] FPS: 425258.58[0m
[36m[2023-07-17 14:14:32,919][257371] itr=1330, itrs=2000, Progress: 66.50%[0m
[37m[1m[2023-07-17 14:18:02,933][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001310[0m
[36m[2023-07-17 14:18:15,146][257371] train() took 11.53 seconds to complete[0m
[36m[2023-07-17 14:18:15,146][257371] FPS: 333049.33[0m
[36m[2023-07-17 14:18:19,323][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:18:19,323][257371] Reward + Measures: [[791.35812896   0.29378965   0.38969767   0.21435699   0.47208032
    2.95369911]][0m
[37m[1m[2023-07-17 14:18:19,323][257371] Max Reward on eval: 791.3581289560003[0m
[37m[1m[2023-07-17 14:18:19,324][257371] Min Reward on eval: 791.3581289560003[0m
[37m[1m[2023-07-17 14:18:19,324][257371] Mean Reward across all agents: 791.3581289560003[0m
[37m[1m[2023-07-17 14:18:19,324][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:18:24,254][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:18:24,254][257371] Reward + Measures: [[812.17223357   0.33790001   0.42790005   0.26750001   0.49779996
    2.89302516]
 [720.51374245   0.27599999   0.3348       0.23020001   0.42539999
    3.10877275]
 [298.91591472   0.29000002   0.49209997   0.25370002   0.47070003
    4.00620031]
 ...
 [754.17548372   0.40690002   0.47690001   0.2181       0.52349997
    3.49446607]
 [482.52034007   0.33070001   0.36610001   0.22360002   0.38890001
    3.12848401]
 [530.87370467   0.26789999   0.47389999   0.17500001   0.53170002
    3.5230844 ]][0m
[37m[1m[2023-07-17 14:18:24,255][257371] Max Reward on eval: 913.5906753454241[0m
[37m[1m[2023-07-17 14:18:24,255][257371] Min Reward on eval: -18.02594532445073[0m
[37m[1m[2023-07-17 14:18:24,255][257371] Mean Reward across all agents: 548.7197079385758[0m
[37m[1m[2023-07-17 14:18:24,255][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:18:24,257][257371] mean_value=-775.3821816523739, max_value=305.13695008236937[0m
[37m[1m[2023-07-17 14:18:24,260][257371] New mean coefficients: [[-0.16890277  0.53744507  0.02459589 -0.03305312  0.07292861 -1.2323921 ]][0m
[37m[1m[2023-07-17 14:18:24,261][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:18:33,227][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 14:18:33,227][257371] FPS: 428338.91[0m
[36m[2023-07-17 14:18:33,230][257371] itr=1331, itrs=2000, Progress: 66.55%[0m
[36m[2023-07-17 14:18:45,202][257371] train() took 11.86 seconds to complete[0m
[36m[2023-07-17 14:18:45,202][257371] FPS: 323781.27[0m
[36m[2023-07-17 14:18:49,484][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:18:49,484][257371] Reward + Measures: [[788.30213465   0.29538667   0.39228201   0.21719833   0.47572899
    2.91357231]][0m
[37m[1m[2023-07-17 14:18:49,484][257371] Max Reward on eval: 788.3021346517828[0m
[37m[1m[2023-07-17 14:18:49,485][257371] Min Reward on eval: 788.3021346517828[0m
[37m[1m[2023-07-17 14:18:49,485][257371] Mean Reward across all agents: 788.3021346517828[0m
[37m[1m[2023-07-17 14:18:49,485][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:18:54,530][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:18:54,531][257371] Reward + Measures: [[745.71255349   0.27870002   0.39270002   0.2304       0.4822
    2.82871628]
 [403.28164413   0.17300001   0.44670001   0.29979998   0.44479999
    4.2942872 ]
 [826.61146543   0.3892       0.49990007   0.20180002   0.58499998
    3.26221085]
 ...
 [769.30731585   0.27919999   0.35980001   0.2191       0.46380001
    2.8228128 ]
 [538.786901     0.31730002   0.33970001   0.25060001   0.38820001
    3.20959163]
 [584.88663198   0.22660001   0.30740002   0.18859999   0.38060001
    3.07369423]][0m
[37m[1m[2023-07-17 14:18:54,531][257371] Max Reward on eval: 954.1919326604344[0m
[37m[1m[2023-07-17 14:18:54,531][257371] Min Reward on eval: -79.64817666332237[0m
[37m[1m[2023-07-17 14:18:54,532][257371] Mean Reward across all agents: 615.1944357029702[0m
[37m[1m[2023-07-17 14:18:54,532][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:18:54,534][257371] mean_value=-429.07273916785215, max_value=583.6221516940916[0m
[37m[1m[2023-07-17 14:18:54,536][257371] New mean coefficients: [[ 0.16941698  0.07373473  0.02140155 -0.14434604  0.17609644 -1.3383067 ]][0m
[37m[1m[2023-07-17 14:18:54,537][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:19:03,512][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 14:19:03,512][257371] FPS: 427930.50[0m
[36m[2023-07-17 14:19:03,515][257371] itr=1332, itrs=2000, Progress: 66.60%[0m
[36m[2023-07-17 14:19:15,446][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 14:19:15,447][257371] FPS: 324870.38[0m
[36m[2023-07-17 14:19:19,743][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:19:19,744][257371] Reward + Measures: [[786.35037353   0.29070434   0.39741966   0.21590534   0.477819
    2.88871002]][0m
[37m[1m[2023-07-17 14:19:19,744][257371] Max Reward on eval: 786.3503735314605[0m
[37m[1m[2023-07-17 14:19:19,744][257371] Min Reward on eval: 786.3503735314605[0m
[37m[1m[2023-07-17 14:19:19,745][257371] Mean Reward across all agents: 786.3503735314605[0m
[37m[1m[2023-07-17 14:19:19,745][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:19:24,727][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:19:24,727][257371] Reward + Measures: [[498.72551727   0.35600001   0.42010003   0.23870002   0.41929999
    3.42033553]
 [598.72475575   0.2784       0.3511       0.22570001   0.44409999
    2.91096139]
 [496.20816985   0.26730001   0.37040001   0.1973       0.40120003
    3.29924273]
 ...
 [434.92623094   0.25320002   0.28340003   0.18910001   0.32640001
    3.39747882]
 [628.74427411   0.51730001   0.55019999   0.23109999   0.53140002
    3.98069882]
 [420.63391418   0.32930002   0.41330001   0.27169999   0.41300002
    3.20041513]][0m
[37m[1m[2023-07-17 14:19:24,727][257371] Max Reward on eval: 893.0337524967268[0m
[37m[1m[2023-07-17 14:19:24,728][257371] Min Reward on eval: 91.09906763093313[0m
[37m[1m[2023-07-17 14:19:24,728][257371] Mean Reward across all agents: 625.1009249164971[0m
[37m[1m[2023-07-17 14:19:24,728][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:19:24,731][257371] mean_value=-548.3642620297517, max_value=573.1315032586332[0m
[37m[1m[2023-07-17 14:19:24,733][257371] New mean coefficients: [[ 0.31307745 -0.02394813  0.4804853  -0.2946782  -0.08444905 -0.9015877 ]][0m
[37m[1m[2023-07-17 14:19:24,734][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:19:33,805][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 14:19:33,805][257371] FPS: 423433.32[0m
[36m[2023-07-17 14:19:33,807][257371] itr=1333, itrs=2000, Progress: 66.65%[0m
[36m[2023-07-17 14:19:45,574][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 14:19:45,574][257371] FPS: 329521.79[0m
[36m[2023-07-17 14:19:49,820][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:19:49,821][257371] Reward + Measures: [[803.9386243    0.29611164   0.40503597   0.21773568   0.49064302
    2.87272382]][0m
[37m[1m[2023-07-17 14:19:49,821][257371] Max Reward on eval: 803.938624298772[0m
[37m[1m[2023-07-17 14:19:49,821][257371] Min Reward on eval: 803.938624298772[0m
[37m[1m[2023-07-17 14:19:49,821][257371] Mean Reward across all agents: 803.938624298772[0m
[37m[1m[2023-07-17 14:19:49,822][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:19:54,801][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:19:54,802][257371] Reward + Measures: [[615.75816395   0.3048       0.37900001   0.2471       0.4118
    3.15794492]
 [807.59729      0.32730001   0.41230002   0.26859999   0.50750005
    2.65050745]
 [585.25192677   0.28910002   0.37460002   0.1876       0.42560002
    2.97701693]
 ...
 [521.78997326   0.287        0.35530001   0.24560001   0.4014
    2.95269275]
 [626.72329567   0.25360003   0.33690003   0.21089999   0.389
    3.0971384 ]
 [436.70971494   0.39809999   0.47180006   0.3154       0.52780002
    3.40916514]][0m
[37m[1m[2023-07-17 14:19:54,802][257371] Max Reward on eval: 869.7062835698016[0m
[37m[1m[2023-07-17 14:19:54,802][257371] Min Reward on eval: 227.4398937162943[0m
[37m[1m[2023-07-17 14:19:54,803][257371] Mean Reward across all agents: 633.8792005587784[0m
[37m[1m[2023-07-17 14:19:54,803][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:19:54,805][257371] mean_value=-444.36798120621495, max_value=394.8444148983351[0m
[37m[1m[2023-07-17 14:19:54,807][257371] New mean coefficients: [[ 0.05568233  0.22731736  0.07560322 -0.06707029 -0.2275134  -1.423247  ]][0m
[37m[1m[2023-07-17 14:19:54,808][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:20:03,798][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 14:20:03,798][257371] FPS: 427248.38[0m
[36m[2023-07-17 14:20:03,800][257371] itr=1334, itrs=2000, Progress: 66.70%[0m
[36m[2023-07-17 14:20:15,676][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 14:20:15,676][257371] FPS: 326468.76[0m
[36m[2023-07-17 14:20:19,995][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:20:19,995][257371] Reward + Measures: [[793.18138023   0.29360998   0.39879668   0.21827535   0.47875732
    2.83422709]][0m
[37m[1m[2023-07-17 14:20:19,996][257371] Max Reward on eval: 793.1813802346804[0m
[37m[1m[2023-07-17 14:20:19,996][257371] Min Reward on eval: 793.1813802346804[0m
[37m[1m[2023-07-17 14:20:19,996][257371] Mean Reward across all agents: 793.1813802346804[0m
[37m[1m[2023-07-17 14:20:19,996][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:20:25,016][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:20:25,021][257371] Reward + Measures: [[705.16381906   0.2924       0.40470001   0.18630002   0.4835
    2.82294846]
 [780.94332693   0.31549999   0.4217       0.20100001   0.50700003
    2.83340001]
 [675.94220792   0.2978       0.36170003   0.24260001   0.42560002
    2.97138262]
 ...
 [806.27041629   0.39250001   0.48370001   0.2138       0.52600002
    3.22219443]
 [848.16677093   0.28860003   0.40920001   0.25310001   0.50019997
    2.57866716]
 [738.95170715   0.32210001   0.39810002   0.22160001   0.45350003
    2.96914363]][0m
[37m[1m[2023-07-17 14:20:25,022][257371] Max Reward on eval: 927.2657165369368[0m
[37m[1m[2023-07-17 14:20:25,022][257371] Min Reward on eval: 423.308582303673[0m
[37m[1m[2023-07-17 14:20:25,022][257371] Mean Reward across all agents: 727.5293652159038[0m
[37m[1m[2023-07-17 14:20:25,022][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:20:25,025][257371] mean_value=-242.87466561259677, max_value=309.5245041019592[0m
[37m[1m[2023-07-17 14:20:25,027][257371] New mean coefficients: [[ 0.12259402  0.17954281  0.27252787 -0.25964385 -0.25790644 -1.7278341 ]][0m
[37m[1m[2023-07-17 14:20:25,028][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:20:33,986][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 14:20:33,986][257371] FPS: 428764.11[0m
[36m[2023-07-17 14:20:33,988][257371] itr=1335, itrs=2000, Progress: 66.75%[0m
[36m[2023-07-17 14:20:45,950][257371] train() took 11.85 seconds to complete[0m
[36m[2023-07-17 14:20:45,951][257371] FPS: 324125.44[0m
[36m[2023-07-17 14:20:50,320][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:20:50,320][257371] Reward + Measures: [[803.30655162   0.30166835   0.40838599   0.22470133   0.48754102
    2.82701755]][0m
[37m[1m[2023-07-17 14:20:50,320][257371] Max Reward on eval: 803.3065516191932[0m
[37m[1m[2023-07-17 14:20:50,321][257371] Min Reward on eval: 803.3065516191932[0m
[37m[1m[2023-07-17 14:20:50,321][257371] Mean Reward across all agents: 803.3065516191932[0m
[37m[1m[2023-07-17 14:20:50,321][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:20:55,372][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:20:55,372][257371] Reward + Measures: [[594.26253246   0.30969998   0.42529997   0.17919999   0.51969999
    3.01241732]
 [541.14177417   0.31500003   0.38119999   0.2538       0.45989999
    3.00197721]
 [717.48222736   0.41619998   0.53960001   0.22479999   0.59860003
    3.42423749]
 ...
 [518.28128009   0.37900001   0.46960002   0.20480001   0.51820004
    3.52591705]
 [576.52251712   0.26980001   0.39879999   0.2007       0.48319998
    2.82556844]
 [641.40255881   0.37399998   0.43560001   0.24089999   0.47559997
    3.53949428]][0m
[37m[1m[2023-07-17 14:20:55,372][257371] Max Reward on eval: 899.2882308927831[0m
[37m[1m[2023-07-17 14:20:55,373][257371] Min Reward on eval: 231.85858773933722[0m
[37m[1m[2023-07-17 14:20:55,373][257371] Mean Reward across all agents: 628.3293557938601[0m
[37m[1m[2023-07-17 14:20:55,373][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:20:55,375][257371] mean_value=-589.7244794351598, max_value=206.93511834954364[0m
[37m[1m[2023-07-17 14:20:55,377][257371] New mean coefficients: [[ 0.02771933  0.2659703   0.11519782 -0.1250231  -0.2993788  -1.5507118 ]][0m
[37m[1m[2023-07-17 14:20:55,378][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:21:04,505][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 14:21:04,505][257371] FPS: 420836.31[0m
[36m[2023-07-17 14:21:04,507][257371] itr=1336, itrs=2000, Progress: 66.80%[0m
[36m[2023-07-17 14:21:16,366][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 14:21:16,366][257371] FPS: 326916.86[0m
[36m[2023-07-17 14:21:20,668][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:21:20,669][257371] Reward + Measures: [[801.79581699   0.29395533   0.40661499   0.22324499   0.48920035
    2.77379298]][0m
[37m[1m[2023-07-17 14:21:20,669][257371] Max Reward on eval: 801.7958169933685[0m
[37m[1m[2023-07-17 14:21:20,669][257371] Min Reward on eval: 801.7958169933685[0m
[37m[1m[2023-07-17 14:21:20,670][257371] Mean Reward across all agents: 801.7958169933685[0m
[37m[1m[2023-07-17 14:21:20,670][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:21:25,675][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:21:25,676][257371] Reward + Measures: [[858.27809145   0.35050002   0.46169996   0.20910001   0.53590006
    3.02799964]
 [722.37164586   0.3409       0.40850002   0.22160001   0.48559999
    3.10875106]
 [725.22078709   0.31930003   0.40509996   0.21469998   0.43990001
    3.17835402]
 ...
 [644.36201493   0.3096       0.38920003   0.2017       0.49020001
    3.03498578]
 [613.19320275   0.2823       0.45720002   0.26229998   0.54100001
    3.71675038]
 [566.38820387   0.32110003   0.37200001   0.25100002   0.39499998
    2.82735062]][0m
[37m[1m[2023-07-17 14:21:25,676][257371] Max Reward on eval: 957.9255904904334[0m
[37m[1m[2023-07-17 14:21:25,676][257371] Min Reward on eval: -82.00682974974625[0m
[37m[1m[2023-07-17 14:21:25,676][257371] Mean Reward across all agents: 562.7949267823514[0m
[37m[1m[2023-07-17 14:21:25,677][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:21:25,679][257371] mean_value=-876.3264672442764, max_value=399.3645648968686[0m
[37m[1m[2023-07-17 14:21:25,682][257371] New mean coefficients: [[ 0.02182733  0.16936296  0.36937428  0.10507643 -0.25412583 -1.2794882 ]][0m
[37m[1m[2023-07-17 14:21:25,683][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:21:34,646][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 14:21:34,647][257371] FPS: 428472.03[0m
[36m[2023-07-17 14:21:34,649][257371] itr=1337, itrs=2000, Progress: 66.85%[0m
[36m[2023-07-17 14:21:46,509][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 14:21:46,509][257371] FPS: 326988.80[0m
[36m[2023-07-17 14:21:50,770][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:21:50,770][257371] Reward + Measures: [[787.81672321   0.293585     0.4068653    0.22151598   0.48801365
    2.75487041]][0m
[37m[1m[2023-07-17 14:21:50,770][257371] Max Reward on eval: 787.8167232111205[0m
[37m[1m[2023-07-17 14:21:50,771][257371] Min Reward on eval: 787.8167232111205[0m
[37m[1m[2023-07-17 14:21:50,771][257371] Mean Reward across all agents: 787.8167232111205[0m
[37m[1m[2023-07-17 14:21:50,771][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:21:55,724][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:21:55,724][257371] Reward + Measures: [[606.51324351   0.29229999   0.45550004   0.20680001   0.49440002
    3.14421177]
 [780.50827795   0.27739999   0.44569999   0.25         0.55980003
    2.5285306 ]
 [748.76741409   0.33500001   0.4738       0.21519999   0.53260005
    3.2984128 ]
 ...
 [489.9155666    0.31350002   0.4192       0.1944       0.40470004
    3.92683339]
 [805.64760212   0.30609998   0.41230002   0.22500001   0.47319999
    2.95406604]
 [516.27982905   0.34510002   0.4357       0.25960001   0.51550001
    3.21747446]][0m
[37m[1m[2023-07-17 14:21:55,725][257371] Max Reward on eval: 911.1032638110686[0m
[37m[1m[2023-07-17 14:21:55,725][257371] Min Reward on eval: 356.0652297159657[0m
[37m[1m[2023-07-17 14:21:55,725][257371] Mean Reward across all agents: 681.8488067684235[0m
[37m[1m[2023-07-17 14:21:55,725][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:21:55,727][257371] mean_value=-387.1401834430231, max_value=425.8757861146264[0m
[37m[1m[2023-07-17 14:21:55,730][257371] New mean coefficients: [[-0.0724549   0.09011545  0.19565251 -0.20549902 -0.45394123 -1.299455  ]][0m
[37m[1m[2023-07-17 14:21:55,731][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:22:04,776][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 14:22:04,776][257371] FPS: 424600.16[0m
[36m[2023-07-17 14:22:04,779][257371] itr=1338, itrs=2000, Progress: 66.90%[0m
[36m[2023-07-17 14:22:16,780][257371] train() took 11.89 seconds to complete[0m
[36m[2023-07-17 14:22:16,780][257371] FPS: 323076.72[0m
[36m[2023-07-17 14:22:21,148][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:22:21,149][257371] Reward + Measures: [[795.11614087   0.29431599   0.40377197   0.22541602   0.49284366
    2.71615171]][0m
[37m[1m[2023-07-17 14:22:21,149][257371] Max Reward on eval: 795.1161408691028[0m
[37m[1m[2023-07-17 14:22:21,149][257371] Min Reward on eval: 795.1161408691028[0m
[37m[1m[2023-07-17 14:22:21,150][257371] Mean Reward across all agents: 795.1161408691028[0m
[37m[1m[2023-07-17 14:22:21,150][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:22:26,483][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:22:26,484][257371] Reward + Measures: [[760.60429381   0.35530001   0.3831       0.28959998   0.55290002
    2.55444837]
 [520.30703358   0.46329999   0.34959999   0.4075       0.43669996
    2.66715145]
 [499.35195065   0.3651       0.47609997   0.31410003   0.47810003
    2.99236417]
 ...
 [473.90235391   0.36620003   0.32270002   0.37650001   0.46650001
    2.69323111]
 [419.78547517   0.31619999   0.41160002   0.24779999   0.4364
    3.34827662]
 [581.88247939   0.35800001   0.4039       0.18340001   0.44169998
    3.23906493]][0m
[37m[1m[2023-07-17 14:22:26,484][257371] Max Reward on eval: 922.3068619094789[0m
[37m[1m[2023-07-17 14:22:26,484][257371] Min Reward on eval: 50.600638539344075[0m
[37m[1m[2023-07-17 14:22:26,484][257371] Mean Reward across all agents: 617.2533573913647[0m
[37m[1m[2023-07-17 14:22:26,485][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:22:26,487][257371] mean_value=-452.30744089773157, max_value=433.7139313548489[0m
[37m[1m[2023-07-17 14:22:26,490][257371] New mean coefficients: [[-0.20542468  0.11337247  0.06306414 -0.01324181 -0.6318842  -0.99589765]][0m
[37m[1m[2023-07-17 14:22:26,491][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:22:35,658][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-17 14:22:35,658][257371] FPS: 418970.05[0m
[36m[2023-07-17 14:22:35,660][257371] itr=1339, itrs=2000, Progress: 66.95%[0m
[36m[2023-07-17 14:22:47,544][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 14:22:47,545][257371] FPS: 326318.43[0m
[36m[2023-07-17 14:22:51,800][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:22:51,805][257371] Reward + Measures: [[793.89225182   0.29598168   0.40384266   0.22588266   0.4963153
    2.70434999]][0m
[37m[1m[2023-07-17 14:22:51,806][257371] Max Reward on eval: 793.892251815022[0m
[37m[1m[2023-07-17 14:22:51,806][257371] Min Reward on eval: 793.892251815022[0m
[37m[1m[2023-07-17 14:22:51,806][257371] Mean Reward across all agents: 793.892251815022[0m
[37m[1m[2023-07-17 14:22:51,806][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:22:56,763][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:22:56,763][257371] Reward + Measures: [[556.08863951   0.23379998   0.3184       0.19510001   0.42570001
    2.8558023 ]
 [825.6133957    0.3969       0.49560004   0.21280001   0.57080001
    3.15876174]
 [686.50828168   0.29760003   0.3678       0.25209999   0.4698
    2.98685217]
 ...
 [701.2466255    0.25550002   0.34549999   0.23269999   0.42340001
    2.65253091]
 [771.75027853   0.29639998   0.43669996   0.17979999   0.51680005
    2.91222882]
 [706.7753372    0.4172       0.49129996   0.2059       0.57770002
    3.22809029]][0m
[37m[1m[2023-07-17 14:22:56,763][257371] Max Reward on eval: 970.4645004365593[0m
[37m[1m[2023-07-17 14:22:56,764][257371] Min Reward on eval: 183.2052089897217[0m
[37m[1m[2023-07-17 14:22:56,764][257371] Mean Reward across all agents: 683.5912551813589[0m
[37m[1m[2023-07-17 14:22:56,764][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:22:56,766][257371] mean_value=-601.0137604377877, max_value=160.84524342934628[0m
[37m[1m[2023-07-17 14:22:56,769][257371] New mean coefficients: [[ 0.36930192  0.25425497  0.2314144  -0.27518767 -0.78616476 -0.8432877 ]][0m
[37m[1m[2023-07-17 14:22:56,770][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:23:05,762][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 14:23:05,762][257371] FPS: 427122.12[0m
[36m[2023-07-17 14:23:05,764][257371] itr=1340, itrs=2000, Progress: 67.00%[0m
[37m[1m[2023-07-17 14:26:36,841][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001320[0m
[36m[2023-07-17 14:26:49,023][257371] train() took 11.65 seconds to complete[0m
[36m[2023-07-17 14:26:49,023][257371] FPS: 329684.96[0m
[36m[2023-07-17 14:26:53,288][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:26:53,288][257371] Reward + Measures: [[794.29280345   0.29124165   0.40304834   0.22778733   0.4915733
    2.67845368]][0m
[37m[1m[2023-07-17 14:26:53,288][257371] Max Reward on eval: 794.2928034504624[0m
[37m[1m[2023-07-17 14:26:53,288][257371] Min Reward on eval: 794.2928034504624[0m
[37m[1m[2023-07-17 14:26:53,289][257371] Mean Reward across all agents: 794.2928034504624[0m
[37m[1m[2023-07-17 14:26:53,289][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:26:58,333][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:26:58,334][257371] Reward + Measures: [[647.89265726   0.3369       0.38680002   0.24889998   0.4506
    2.96307421]
 [509.33321166   0.51430005   0.58420002   0.22029999   0.62859994
    2.93880439]
 [813.07703401   0.29660001   0.38390002   0.23480001   0.4763
    2.7640779 ]
 ...
 [323.00560248   0.22669999   0.3263       0.23670001   0.36060002
    2.96018934]
 [463.98309947   0.38760003   0.39149997   0.27520001   0.44840002
    2.62195253]
 [351.04508703   0.28580004   0.33720002   0.1761       0.3951
    2.83402634]][0m
[37m[1m[2023-07-17 14:26:58,334][257371] Max Reward on eval: 871.5279693472664[0m
[37m[1m[2023-07-17 14:26:58,334][257371] Min Reward on eval: 104.182540376205[0m
[37m[1m[2023-07-17 14:26:58,334][257371] Mean Reward across all agents: 595.7889576235012[0m
[37m[1m[2023-07-17 14:26:58,335][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:26:58,336][257371] mean_value=-490.4463112852511, max_value=442.10757441981707[0m
[37m[1m[2023-07-17 14:26:58,339][257371] New mean coefficients: [[-0.15596968  0.43240532  0.07489291  0.03905329 -0.46583423 -0.8728757 ]][0m
[37m[1m[2023-07-17 14:26:58,340][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:27:07,344][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 14:27:07,345][257371] FPS: 426529.89[0m
[36m[2023-07-17 14:27:07,347][257371] itr=1341, itrs=2000, Progress: 67.05%[0m
[36m[2023-07-17 14:27:19,360][257371] train() took 11.90 seconds to complete[0m
[36m[2023-07-17 14:27:19,360][257371] FPS: 322679.54[0m
[36m[2023-07-17 14:27:23,659][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:27:23,660][257371] Reward + Measures: [[795.64755356   0.292191     0.40730932   0.231609     0.49537531
    2.63794565]][0m
[37m[1m[2023-07-17 14:27:23,660][257371] Max Reward on eval: 795.6475535563578[0m
[37m[1m[2023-07-17 14:27:23,660][257371] Min Reward on eval: 795.6475535563578[0m
[37m[1m[2023-07-17 14:27:23,660][257371] Mean Reward across all agents: 795.6475535563578[0m
[37m[1m[2023-07-17 14:27:23,661][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:27:28,629][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:27:28,629][257371] Reward + Measures: [[645.78405552   0.3242       0.37189999   0.29610002   0.42209998
    2.82050133]
 [460.96576485   0.2228       0.35690001   0.18810001   0.39860001
    3.07224202]
 [787.01990504   0.28490004   0.40180001   0.24530001   0.47550002
    2.83103442]
 ...
 [320.50454269   0.1961       0.47489998   0.2462       0.44660002
    3.89392471]
 [504.2917804    0.34250003   0.3673       0.28959998   0.41540003
    2.96314287]
 [557.58127019   0.30840001   0.31360003   0.27160001   0.37560001
    2.60750508]][0m
[37m[1m[2023-07-17 14:27:28,630][257371] Max Reward on eval: 854.9946136567276[0m
[37m[1m[2023-07-17 14:27:28,630][257371] Min Reward on eval: 120.75633796958718[0m
[37m[1m[2023-07-17 14:27:28,630][257371] Mean Reward across all agents: 642.7647882597424[0m
[37m[1m[2023-07-17 14:27:28,630][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:27:28,633][257371] mean_value=-407.2124021066602, max_value=527.927100467784[0m
[37m[1m[2023-07-17 14:27:28,636][257371] New mean coefficients: [[-0.35261798  0.42674217 -0.43533835  0.20484339 -0.66342884 -0.54010034]][0m
[37m[1m[2023-07-17 14:27:28,637][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:27:37,680][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 14:27:37,680][257371] FPS: 424728.09[0m
[36m[2023-07-17 14:27:37,682][257371] itr=1342, itrs=2000, Progress: 67.10%[0m
[36m[2023-07-17 14:27:49,471][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 14:27:49,471][257371] FPS: 328946.00[0m
[36m[2023-07-17 14:27:53,675][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:27:53,676][257371] Reward + Measures: [[781.79713869   0.29106632   0.40340933   0.23148766   0.49168965
    2.63576484]][0m
[37m[1m[2023-07-17 14:27:53,676][257371] Max Reward on eval: 781.7971386900296[0m
[37m[1m[2023-07-17 14:27:53,676][257371] Min Reward on eval: 781.7971386900296[0m
[37m[1m[2023-07-17 14:27:53,677][257371] Mean Reward across all agents: 781.7971386900296[0m
[37m[1m[2023-07-17 14:27:53,677][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:27:58,629][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:27:58,630][257371] Reward + Measures: [[372.53298094   0.28910002   0.29749998   0.22379999   0.31060001
    3.18596554]
 [377.5939078    0.40159997   0.4262       0.35119998   0.4337
    3.49494338]
 [555.90804094   0.31530002   0.34020001   0.2624       0.38970003
    3.11048532]
 ...
 [476.13394211   0.29550001   0.31349999   0.25759998   0.34059998
    3.15367961]
 [672.95754189   0.32809997   0.3732       0.22040001   0.4278
    2.9040451 ]
 [367.42294597   0.40970001   0.39879999   0.3098       0.28760001
    3.75928164]][0m
[37m[1m[2023-07-17 14:27:58,630][257371] Max Reward on eval: 828.2314071732574[0m
[37m[1m[2023-07-17 14:27:58,630][257371] Min Reward on eval: 53.98395389104262[0m
[37m[1m[2023-07-17 14:27:58,630][257371] Mean Reward across all agents: 528.1803854008297[0m
[37m[1m[2023-07-17 14:27:58,631][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:27:58,632][257371] mean_value=-1163.1246846115464, max_value=360.558429538213[0m
[37m[1m[2023-07-17 14:27:58,634][257371] New mean coefficients: [[-0.3109014   0.22408612 -0.4059976   0.17353734 -0.11471266 -0.42302763]][0m
[37m[1m[2023-07-17 14:27:58,635][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:28:07,559][257371] train() took 8.92 seconds to complete[0m
[36m[2023-07-17 14:28:07,559][257371] FPS: 430409.32[0m
[36m[2023-07-17 14:28:07,561][257371] itr=1343, itrs=2000, Progress: 67.15%[0m
[36m[2023-07-17 14:28:19,213][257371] train() took 11.54 seconds to complete[0m
[36m[2023-07-17 14:28:19,213][257371] FPS: 332805.11[0m
[36m[2023-07-17 14:28:23,451][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:28:23,452][257371] Reward + Measures: [[777.55569912   0.29057333   0.40246964   0.23153733   0.49285799
    2.62005615]][0m
[37m[1m[2023-07-17 14:28:23,452][257371] Max Reward on eval: 777.5556991154393[0m
[37m[1m[2023-07-17 14:28:23,452][257371] Min Reward on eval: 777.5556991154393[0m
[37m[1m[2023-07-17 14:28:23,453][257371] Mean Reward across all agents: 777.5556991154393[0m
[37m[1m[2023-07-17 14:28:23,453][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:28:28,414][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:28:28,414][257371] Reward + Measures: [[737.55448153   0.29539999   0.46259999   0.23659997   0.5557
    2.58303189]
 [536.68159392   0.30500004   0.35670003   0.18270002   0.38330001
    2.97190499]
 [766.62995913   0.34059998   0.4506       0.26540002   0.48680001
    2.86604452]
 ...
 [573.41674938   0.29850003   0.33680001   0.21170001   0.40289998
    2.75899315]
 [657.95024875   0.30300003   0.40650001   0.25940001   0.48799998
    2.82860756]
 [414.25495888   0.22910002   0.28200001   0.2023       0.33919999
    2.96995234]][0m
[37m[1m[2023-07-17 14:28:28,414][257371] Max Reward on eval: 846.0981597673614[0m
[37m[1m[2023-07-17 14:28:28,415][257371] Min Reward on eval: 66.7706701008603[0m
[37m[1m[2023-07-17 14:28:28,415][257371] Mean Reward across all agents: 555.0898564602529[0m
[37m[1m[2023-07-17 14:28:28,415][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:28:28,417][257371] mean_value=-918.5477880152414, max_value=67.63973978634544[0m
[37m[1m[2023-07-17 14:28:28,419][257371] New mean coefficients: [[ 0.02284226  0.13201451 -0.1932147   0.05779752 -0.05603338 -0.17605121]][0m
[37m[1m[2023-07-17 14:28:28,420][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:28:37,472][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 14:28:37,478][257371] FPS: 424290.96[0m
[36m[2023-07-17 14:28:37,481][257371] itr=1344, itrs=2000, Progress: 67.20%[0m
[36m[2023-07-17 14:28:49,191][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 14:28:49,191][257371] FPS: 331119.49[0m
[36m[2023-07-17 14:28:53,435][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:28:53,435][257371] Reward + Measures: [[770.58389958   0.29055732   0.39879867   0.23050067   0.48965833
    2.62817574]][0m
[37m[1m[2023-07-17 14:28:53,436][257371] Max Reward on eval: 770.5838995796613[0m
[37m[1m[2023-07-17 14:28:53,436][257371] Min Reward on eval: 770.5838995796613[0m
[37m[1m[2023-07-17 14:28:53,436][257371] Mean Reward across all agents: 770.5838995796613[0m
[37m[1m[2023-07-17 14:28:53,436][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:28:58,386][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:28:58,386][257371] Reward + Measures: [[650.61285424   0.30879998   0.42729998   0.21780001   0.50880003
    2.58595777]
 [695.92713948   0.26630002   0.34630001   0.19400001   0.43940002
    2.85210228]
 [659.32891318   0.2956       0.36879998   0.19149999   0.45030004
    3.04278493]
 ...
 [813.33854675   0.2825       0.42220002   0.24519999   0.52380002
    2.47365689]
 [778.81904221   0.32480001   0.4578       0.21710001   0.56409997
    2.73337221]
 [794.04675293   0.31580001   0.37939999   0.23889999   0.4542
    2.8641243 ]][0m
[37m[1m[2023-07-17 14:28:58,387][257371] Max Reward on eval: 866.632347119879[0m
[37m[1m[2023-07-17 14:28:58,387][257371] Min Reward on eval: -6.3252945018932225[0m
[37m[1m[2023-07-17 14:28:58,387][257371] Mean Reward across all agents: 654.3733650245106[0m
[37m[1m[2023-07-17 14:28:58,387][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:28:58,389][257371] mean_value=-498.8634521943187, max_value=336.2094867614222[0m
[37m[1m[2023-07-17 14:28:58,391][257371] New mean coefficients: [[-0.22622739 -0.0178891  -0.24242885  0.01279781  0.14780182 -0.12073086]][0m
[37m[1m[2023-07-17 14:28:58,392][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:29:07,330][257371] train() took 8.94 seconds to complete[0m
[36m[2023-07-17 14:29:07,331][257371] FPS: 429716.27[0m
[36m[2023-07-17 14:29:07,338][257371] itr=1345, itrs=2000, Progress: 67.25%[0m
[36m[2023-07-17 14:29:19,060][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 14:29:19,060][257371] FPS: 330749.43[0m
[36m[2023-07-17 14:29:23,350][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:29:23,351][257371] Reward + Measures: [[756.73216702   0.29231066   0.39411166   0.23370731   0.49539998
    2.59899974]][0m
[37m[1m[2023-07-17 14:29:23,351][257371] Max Reward on eval: 756.7321670161896[0m
[37m[1m[2023-07-17 14:29:23,351][257371] Min Reward on eval: 756.7321670161896[0m
[37m[1m[2023-07-17 14:29:23,351][257371] Mean Reward across all agents: 756.7321670161896[0m
[37m[1m[2023-07-17 14:29:23,351][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:29:28,300][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:29:28,300][257371] Reward + Measures: [[667.66121578   0.3215       0.34670001   0.25030002   0.4224
    2.8704319 ]
 [614.56581734   0.35010001   0.48610002   0.24249999   0.47659999
    3.22567725]
 [605.97976778   0.37680003   0.54050004   0.17709999   0.54229999
    3.62228751]
 ...
 [702.50198362   0.33150002   0.35499999   0.2665       0.52060002
    2.59549403]
 [738.39356613   0.45560002   0.39219999   0.26870003   0.5535
    2.64163208]
 [486.35059332   0.3651       0.51599997   0.4481       0.56210005
    3.65644503]][0m
[37m[1m[2023-07-17 14:29:28,301][257371] Max Reward on eval: 898.3223037881777[0m
[37m[1m[2023-07-17 14:29:28,301][257371] Min Reward on eval: 231.08525226836792[0m
[37m[1m[2023-07-17 14:29:28,301][257371] Mean Reward across all agents: 653.4660612615252[0m
[37m[1m[2023-07-17 14:29:28,301][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:29:28,303][257371] mean_value=-560.1955182082146, max_value=351.865338358695[0m
[37m[1m[2023-07-17 14:29:28,306][257371] New mean coefficients: [[-0.22516514 -0.5259229   0.0325747  -0.14122048  0.01862994 -0.08334016]][0m
[37m[1m[2023-07-17 14:29:28,307][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:29:37,277][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 14:29:37,277][257371] FPS: 428161.62[0m
[36m[2023-07-17 14:29:37,280][257371] itr=1346, itrs=2000, Progress: 67.30%[0m
[36m[2023-07-17 14:29:48,996][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 14:29:48,996][257371] FPS: 331102.70[0m
[36m[2023-07-17 14:29:53,335][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:29:53,336][257371] Reward + Measures: [[747.79411719   0.29212299   0.39574569   0.23620133   0.50209796
    2.58052826]][0m
[37m[1m[2023-07-17 14:29:53,336][257371] Max Reward on eval: 747.7941171949217[0m
[37m[1m[2023-07-17 14:29:53,336][257371] Min Reward on eval: 747.7941171949217[0m
[37m[1m[2023-07-17 14:29:53,336][257371] Mean Reward across all agents: 747.7941171949217[0m
[37m[1m[2023-07-17 14:29:53,337][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:29:58,267][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:29:58,268][257371] Reward + Measures: [[787.80770111   0.27870002   0.43529996   0.23870002   0.52630007
    2.41987324]
 [598.19203372   0.2956       0.37190002   0.20380001   0.43380004
    2.72733355]
 [768.60066986   0.30039999   0.39610001   0.26250002   0.52940005
    2.50994802]
 ...
 [583.98679566   0.25959998   0.37010002   0.22259998   0.44950005
    2.52372146]
 [741.9361553    0.32750002   0.40559998   0.21800001   0.47410002
    3.02902102]
 [696.53293661   0.32249999   0.4118       0.21689999   0.51610005
    2.7566371 ]][0m
[37m[1m[2023-07-17 14:29:58,268][257371] Max Reward on eval: 822.3897933979518[0m
[37m[1m[2023-07-17 14:29:58,268][257371] Min Reward on eval: 174.714475417044[0m
[37m[1m[2023-07-17 14:29:58,268][257371] Mean Reward across all agents: 629.0153453066249[0m
[37m[1m[2023-07-17 14:29:58,269][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:29:58,270][257371] mean_value=-765.5358542276401, max_value=238.1294345192352[0m
[37m[1m[2023-07-17 14:29:58,272][257371] New mean coefficients: [[ 0.04913847 -0.6163541  -0.08573442 -0.00393568  0.01761893  0.24627262]][0m
[37m[1m[2023-07-17 14:29:58,273][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:30:07,324][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 14:30:07,324][257371] FPS: 424376.75[0m
[36m[2023-07-17 14:30:07,326][257371] itr=1347, itrs=2000, Progress: 67.35%[0m
[36m[2023-07-17 14:30:19,152][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 14:30:19,152][257371] FPS: 327970.32[0m
[36m[2023-07-17 14:30:23,460][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:30:23,465][257371] Reward + Measures: [[762.45100421   0.29360166   0.39931297   0.23684733   0.50478297
    2.60531807]][0m
[37m[1m[2023-07-17 14:30:23,466][257371] Max Reward on eval: 762.4510042081115[0m
[37m[1m[2023-07-17 14:30:23,466][257371] Min Reward on eval: 762.4510042081115[0m
[37m[1m[2023-07-17 14:30:23,466][257371] Mean Reward across all agents: 762.4510042081115[0m
[37m[1m[2023-07-17 14:30:23,467][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:30:28,516][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:30:28,522][257371] Reward + Measures: [[595.81174993   0.31680003   0.43560001   0.2376       0.51630002
    2.88250947]
 [484.29792144   0.3407       0.39520001   0.25170001   0.47459999
    2.90927219]
 [574.14882677   0.28169999   0.3976       0.2185       0.45370004
    2.67403221]
 ...
 [799.09355167   0.43130001   0.5406       0.22669999   0.60119998
    3.10694408]
 [659.0646565    0.29990003   0.36859998   0.24229999   0.4941
    2.60077071]
 [484.22414634   0.33440003   0.43810001   0.26159999   0.4982
    2.9006269 ]][0m
[37m[1m[2023-07-17 14:30:28,522][257371] Max Reward on eval: 837.0159301784821[0m
[37m[1m[2023-07-17 14:30:28,522][257371] Min Reward on eval: 331.5045256845653[0m
[37m[1m[2023-07-17 14:30:28,523][257371] Mean Reward across all agents: 624.1582621538385[0m
[37m[1m[2023-07-17 14:30:28,523][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:30:28,524][257371] mean_value=-529.6559634781167, max_value=307.88787743799116[0m
[37m[1m[2023-07-17 14:30:28,527][257371] New mean coefficients: [[ 0.06808029 -0.33904678 -0.00150544  0.0493079  -0.20875202  0.11287035]][0m
[37m[1m[2023-07-17 14:30:28,528][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:30:37,522][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 14:30:37,522][257371] FPS: 427032.04[0m
[36m[2023-07-17 14:30:37,524][257371] itr=1348, itrs=2000, Progress: 67.40%[0m
[36m[2023-07-17 14:30:49,222][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 14:30:49,222][257371] FPS: 331544.73[0m
[36m[2023-07-17 14:30:53,524][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:30:53,530][257371] Reward + Measures: [[755.11712736   0.29157001   0.39317065   0.23358233   0.49247265
    2.6350987 ]][0m
[37m[1m[2023-07-17 14:30:53,530][257371] Max Reward on eval: 755.1171273578697[0m
[37m[1m[2023-07-17 14:30:53,531][257371] Min Reward on eval: 755.1171273578697[0m
[37m[1m[2023-07-17 14:30:53,531][257371] Mean Reward across all agents: 755.1171273578697[0m
[37m[1m[2023-07-17 14:30:53,531][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:30:58,741][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:30:58,747][257371] Reward + Measures: [[609.87673185   0.29700002   0.3644       0.20079999   0.45579997
    2.91948652]
 [725.48866394   0.31399998   0.39309999   0.21729998   0.46950004
    3.02690005]
 [693.28833824   0.30580002   0.37900001   0.25619999   0.49390003
    2.46833992]
 ...
 [844.90734096   0.28120002   0.40690002   0.24509998   0.4693
    2.66409802]
 [671.89252516   0.3339       0.43250003   0.23580001   0.52430004
    2.72924542]
 [650.15645601   0.3337       0.38649997   0.27550003   0.56650001
    2.61005473]][0m
[37m[1m[2023-07-17 14:30:58,747][257371] Max Reward on eval: 868.6007232896052[0m
[37m[1m[2023-07-17 14:30:58,747][257371] Min Reward on eval: 393.0779283516807[0m
[37m[1m[2023-07-17 14:30:58,747][257371] Mean Reward across all agents: 680.7634778407532[0m
[37m[1m[2023-07-17 14:30:58,748][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:30:58,749][257371] mean_value=-555.5061181398322, max_value=448.5699281348544[0m
[37m[1m[2023-07-17 14:30:58,752][257371] New mean coefficients: [[-0.01591448 -0.26097006 -0.07930086  0.16371313 -0.01522399 -0.22628035]][0m
[37m[1m[2023-07-17 14:30:58,753][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:31:07,718][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 14:31:07,718][257371] FPS: 428406.08[0m
[36m[2023-07-17 14:31:07,720][257371] itr=1349, itrs=2000, Progress: 67.45%[0m
[36m[2023-07-17 14:31:19,646][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 14:31:19,646][257371] FPS: 325034.51[0m
[36m[2023-07-17 14:31:23,920][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:31:23,926][257371] Reward + Measures: [[756.40716708   0.29038832   0.39583635   0.23668133   0.497403
    2.60498405]][0m
[37m[1m[2023-07-17 14:31:23,926][257371] Max Reward on eval: 756.407167081501[0m
[37m[1m[2023-07-17 14:31:23,926][257371] Min Reward on eval: 756.407167081501[0m
[37m[1m[2023-07-17 14:31:23,927][257371] Mean Reward across all agents: 756.407167081501[0m
[37m[1m[2023-07-17 14:31:23,927][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:31:28,868][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:31:28,869][257371] Reward + Measures: [[637.68087639   0.2536       0.37970003   0.2297       0.47840005
    2.47269821]
 [503.58912981   0.22690001   0.30590001   0.1973       0.34960002
    2.74707484]
 [740.45246123   0.34150004   0.45590001   0.25240001   0.51349998
    2.92446375]
 ...
 [617.94756501   0.2895       0.41710001   0.20510001   0.45459995
    3.0784173 ]
 [634.63241145   0.31490001   0.37660003   0.23080002   0.4567
    2.64836478]
 [685.84805684   0.40700004   0.42910001   0.2933       0.55989999
    2.59548116]][0m
[37m[1m[2023-07-17 14:31:28,869][257371] Max Reward on eval: 900.1355896113906[0m
[37m[1m[2023-07-17 14:31:28,869][257371] Min Reward on eval: 196.38462045839987[0m
[37m[1m[2023-07-17 14:31:28,869][257371] Mean Reward across all agents: 632.2320906212012[0m
[37m[1m[2023-07-17 14:31:28,870][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:31:28,871][257371] mean_value=-791.8362313268316, max_value=221.52933645781064[0m
[37m[1m[2023-07-17 14:31:28,874][257371] New mean coefficients: [[ 0.23316428 -0.34421706 -0.14845222  0.16491456 -0.05118772 -0.26932296]][0m
[37m[1m[2023-07-17 14:31:28,875][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:31:37,862][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 14:31:37,863][257371] FPS: 427322.56[0m
[36m[2023-07-17 14:31:37,865][257371] itr=1350, itrs=2000, Progress: 67.50%[0m
[37m[1m[2023-07-17 14:35:11,223][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001330[0m
[36m[2023-07-17 14:35:23,759][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 14:35:23,760][257371] FPS: 325763.41[0m
[36m[2023-07-17 14:35:28,026][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:35:28,031][257371] Reward + Measures: [[755.94173027   0.28783301   0.39369068   0.23702267   0.494645
    2.59814668]][0m
[37m[1m[2023-07-17 14:35:28,031][257371] Max Reward on eval: 755.9417302656162[0m
[37m[1m[2023-07-17 14:35:28,032][257371] Min Reward on eval: 755.9417302656162[0m
[37m[1m[2023-07-17 14:35:28,032][257371] Mean Reward across all agents: 755.9417302656162[0m
[37m[1m[2023-07-17 14:35:28,032][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:35:32,962][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:35:32,963][257371] Reward + Measures: [[482.30125413   0.22850001   0.2983       0.18860002   0.37860003
    2.68807054]
 [625.54876326   0.3364       0.46040002   0.28040001   0.56980002
    2.54880881]
 [648.55748372   0.27269998   0.42209998   0.2726       0.53590006
    2.43791747]
 ...
 [777.02450181   0.35800001   0.4034       0.24390002   0.49509999
    3.02388215]
 [398.43769698   0.36579999   0.3612       0.29170001   0.32530004
    3.35295844]
 [613.45062241   0.34650001   0.43700001   0.20869999   0.52530003
    2.87795758]][0m
[37m[1m[2023-07-17 14:35:32,963][257371] Max Reward on eval: 868.3693694733083[0m
[37m[1m[2023-07-17 14:35:32,963][257371] Min Reward on eval: 301.99275251231154[0m
[37m[1m[2023-07-17 14:35:32,963][257371] Mean Reward across all agents: 599.2993874069884[0m
[37m[1m[2023-07-17 14:35:32,964][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:35:32,965][257371] mean_value=-548.832469612748, max_value=81.9227860650708[0m
[37m[1m[2023-07-17 14:35:32,968][257371] New mean coefficients: [[ 0.1165913  -0.08113977 -0.07616895 -0.14792071 -0.05071258 -0.49369806]][0m
[37m[1m[2023-07-17 14:35:32,968][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:35:41,934][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 14:35:41,934][257371] FPS: 428385.33[0m
[36m[2023-07-17 14:35:41,937][257371] itr=1351, itrs=2000, Progress: 67.55%[0m
[36m[2023-07-17 14:35:53,837][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 14:35:53,838][257371] FPS: 325812.52[0m
[36m[2023-07-17 14:35:58,091][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:35:58,092][257371] Reward + Measures: [[758.29229135   0.28537202   0.39556471   0.23619632   0.49801233
    2.56862688]][0m
[37m[1m[2023-07-17 14:35:58,092][257371] Max Reward on eval: 758.2922913537991[0m
[37m[1m[2023-07-17 14:35:58,092][257371] Min Reward on eval: 758.2922913537991[0m
[37m[1m[2023-07-17 14:35:58,092][257371] Mean Reward across all agents: 758.2922913537991[0m
[37m[1m[2023-07-17 14:35:58,093][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:36:03,047][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:36:03,048][257371] Reward + Measures: [[620.18091113   0.30060002   0.39059997   0.24589999   0.43759999
    3.06514049]
 [570.59227093   0.29310003   0.36840001   0.24749999   0.44930002
    3.03781462]
 [521.77774376   0.24050002   0.32969999   0.20820001   0.40939999
    2.83017612]
 ...
 [618.79284292   0.30180001   0.40760002   0.20369999   0.46199998
    2.92133164]
 [267.19705522   0.17479999   0.21679997   0.1444       0.23550001
    3.53896332]
 [691.78405784   0.27869999   0.36950001   0.24229999   0.46300003
    2.52866912]][0m
[37m[1m[2023-07-17 14:36:03,048][257371] Max Reward on eval: 882.2059020346496[0m
[37m[1m[2023-07-17 14:36:03,048][257371] Min Reward on eval: 25.560672887065447[0m
[37m[1m[2023-07-17 14:36:03,048][257371] Mean Reward across all agents: 648.1840297976173[0m
[37m[1m[2023-07-17 14:36:03,049][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:36:03,050][257371] mean_value=-794.3380749206049, max_value=470.2933282359013[0m
[37m[1m[2023-07-17 14:36:03,053][257371] New mean coefficients: [[-0.30655476 -0.4786735  -0.06668366 -0.12452103  0.08855405 -0.06282735]][0m
[37m[1m[2023-07-17 14:36:03,054][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:36:12,101][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 14:36:12,101][257371] FPS: 424510.56[0m
[36m[2023-07-17 14:36:12,103][257371] itr=1352, itrs=2000, Progress: 67.60%[0m
[36m[2023-07-17 14:36:23,946][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 14:36:23,946][257371] FPS: 327390.25[0m
[36m[2023-07-17 14:36:28,203][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:36:28,203][257371] Reward + Measures: [[735.76000297   0.28208268   0.39794499   0.23458301   0.50511533
    2.57814717]][0m
[37m[1m[2023-07-17 14:36:28,204][257371] Max Reward on eval: 735.760002973444[0m
[37m[1m[2023-07-17 14:36:28,204][257371] Min Reward on eval: 735.760002973444[0m
[37m[1m[2023-07-17 14:36:28,204][257371] Mean Reward across all agents: 735.760002973444[0m
[37m[1m[2023-07-17 14:36:28,204][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:36:33,174][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:36:33,174][257371] Reward + Measures: [[713.44971085   0.34300002   0.419        0.271        0.48070002
    2.81423926]
 [604.64073181   0.3985       0.45389995   0.27990001   0.56250006
    2.80484319]
 [577.75996061   0.43979999   0.54759997   0.16340001   0.55620003
    3.43918657]
 ...
 [420.87285735   0.23099999   0.35380003   0.16659999   0.44220001
    2.8621366 ]
 [738.79472734   0.33969998   0.45109996   0.2299       0.54889995
    2.76724672]
 [642.8070907    0.30359998   0.43099999   0.25470001   0.50190002
    2.59091187]][0m
[37m[1m[2023-07-17 14:36:33,174][257371] Max Reward on eval: 831.1161575574428[0m
[37m[1m[2023-07-17 14:36:33,175][257371] Min Reward on eval: 28.858227452496067[0m
[37m[1m[2023-07-17 14:36:33,175][257371] Mean Reward across all agents: 553.9843750651477[0m
[37m[1m[2023-07-17 14:36:33,175][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:36:33,177][257371] mean_value=-736.6717374679148, max_value=598.0963455583583[0m
[37m[1m[2023-07-17 14:36:33,180][257371] New mean coefficients: [[ 0.05374858 -0.35063985 -0.01675091 -0.13812247 -0.13407677 -0.2331129 ]][0m
[37m[1m[2023-07-17 14:36:33,181][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:36:42,135][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 14:36:42,135][257371] FPS: 428923.38[0m
[36m[2023-07-17 14:36:42,138][257371] itr=1353, itrs=2000, Progress: 67.65%[0m
[36m[2023-07-17 14:36:53,914][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 14:36:53,914][257371] FPS: 329312.57[0m
[36m[2023-07-17 14:36:58,204][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:36:58,205][257371] Reward + Measures: [[733.5153353    0.27552369   0.39692333   0.23217233   0.50078195
    2.55785608]][0m
[37m[1m[2023-07-17 14:36:58,205][257371] Max Reward on eval: 733.515335304101[0m
[37m[1m[2023-07-17 14:36:58,205][257371] Min Reward on eval: 733.515335304101[0m
[37m[1m[2023-07-17 14:36:58,206][257371] Mean Reward across all agents: 733.515335304101[0m
[37m[1m[2023-07-17 14:36:58,206][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:37:03,479][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:37:03,486][257371] Reward + Measures: [[732.00676348   0.33919999   0.3734       0.28190002   0.4896
    2.64296794]
 [765.26953504   0.31430003   0.41700003   0.24689999   0.50870001
    2.51393867]
 [713.74930184   0.34800002   0.41620001   0.23669998   0.51590002
    2.51766443]
 ...
 [695.16339492   0.28789997   0.42729998   0.2362       0.55680007
    2.66109037]
 [561.49697121   0.24430001   0.28779998   0.20419998   0.38030002
    2.54395914]
 [501.33745776   0.26879999   0.36939999   0.19240001   0.40760002
    3.0741601 ]][0m
[37m[1m[2023-07-17 14:37:03,486][257371] Max Reward on eval: 852.0527801657794[0m
[37m[1m[2023-07-17 14:37:03,486][257371] Min Reward on eval: 246.83667631621938[0m
[37m[1m[2023-07-17 14:37:03,487][257371] Mean Reward across all agents: 637.2710079435195[0m
[37m[1m[2023-07-17 14:37:03,487][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:37:03,488][257371] mean_value=-623.8388235608585, max_value=167.53020171159056[0m
[37m[1m[2023-07-17 14:37:03,491][257371] New mean coefficients: [[-0.16877933 -0.49771476 -0.0681823   0.13195366 -0.20331132 -0.14420563]][0m
[37m[1m[2023-07-17 14:37:03,492][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:37:12,554][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 14:37:12,555][257371] FPS: 423803.23[0m
[36m[2023-07-17 14:37:12,557][257371] itr=1354, itrs=2000, Progress: 67.70%[0m
[36m[2023-07-17 14:37:24,369][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 14:37:24,370][257371] FPS: 328184.25[0m
[36m[2023-07-17 14:37:28,725][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:37:28,726][257371] Reward + Measures: [[736.47047009   0.27452567   0.40276766   0.23422335   0.50964296
    2.53625059]][0m
[37m[1m[2023-07-17 14:37:28,726][257371] Max Reward on eval: 736.4704700933415[0m
[37m[1m[2023-07-17 14:37:28,726][257371] Min Reward on eval: 736.4704700933415[0m
[37m[1m[2023-07-17 14:37:28,727][257371] Mean Reward across all agents: 736.4704700933415[0m
[37m[1m[2023-07-17 14:37:28,727][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:37:33,793][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:37:33,794][257371] Reward + Measures: [[529.09421246   0.39330003   0.50319999   0.1974       0.51160002
    3.61873317]
 [660.7680626    0.28509998   0.43860003   0.27180001   0.57249999
    2.34399915]
 [579.51205909   0.29270002   0.43889999   0.184        0.52650005
    2.9560535 ]
 ...
 [760.64084627   0.28379998   0.41         0.25060001   0.58310002
    2.21400428]
 [716.71576315   0.2823       0.40650001   0.23340002   0.48919997
    2.67207789]
 [648.15615084   0.37600002   0.53820002   0.22849999   0.59009999
    3.01306915]][0m
[37m[1m[2023-07-17 14:37:33,794][257371] Max Reward on eval: 833.5767440512311[0m
[37m[1m[2023-07-17 14:37:33,795][257371] Min Reward on eval: 156.8038996361196[0m
[37m[1m[2023-07-17 14:37:33,795][257371] Mean Reward across all agents: 602.5409410711553[0m
[37m[1m[2023-07-17 14:37:33,795][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:37:33,797][257371] mean_value=-545.8738369774829, max_value=401.06437200330055[0m
[37m[1m[2023-07-17 14:37:33,799][257371] New mean coefficients: [[ 0.00565083 -0.14843464 -0.17491952  0.3318858  -0.5885715  -0.7070561 ]][0m
[37m[1m[2023-07-17 14:37:33,800][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:37:42,829][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 14:37:42,830][257371] FPS: 425360.11[0m
[36m[2023-07-17 14:37:42,832][257371] itr=1355, itrs=2000, Progress: 67.75%[0m
[36m[2023-07-17 14:37:54,655][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 14:37:54,655][257371] FPS: 327940.58[0m
[36m[2023-07-17 14:37:58,892][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:37:58,893][257371] Reward + Measures: [[728.07630858   0.27111799   0.39966199   0.23167999   0.50506264
    2.52512932]][0m
[37m[1m[2023-07-17 14:37:58,893][257371] Max Reward on eval: 728.0763085847396[0m
[37m[1m[2023-07-17 14:37:58,893][257371] Min Reward on eval: 728.0763085847396[0m
[37m[1m[2023-07-17 14:37:58,893][257371] Mean Reward across all agents: 728.0763085847396[0m
[37m[1m[2023-07-17 14:37:58,894][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:38:03,881][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:38:03,882][257371] Reward + Measures: [[545.68537569   0.32949999   0.38770002   0.2254       0.43540001
    2.92530298]
 [674.45780751   0.33160001   0.43350002   0.23959999   0.50510007
    2.49303007]
 [571.70272185   0.32600001   0.51610005   0.25970003   0.57240003
    3.10403895]
 ...
 [503.77669292   0.3028       0.38150001   0.2527       0.39580002
    3.24144745]
 [559.65657516   0.33660004   0.39969999   0.2656       0.42040005
    2.9927125 ]
 [676.52342417   0.28560001   0.3619       0.23980001   0.44299999
    2.88281107]][0m
[37m[1m[2023-07-17 14:38:03,882][257371] Max Reward on eval: 852.2799682699144[0m
[37m[1m[2023-07-17 14:38:03,882][257371] Min Reward on eval: 310.2781107533723[0m
[37m[1m[2023-07-17 14:38:03,882][257371] Mean Reward across all agents: 628.1247855339584[0m
[37m[1m[2023-07-17 14:38:03,883][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:38:03,884][257371] mean_value=-513.9900676759461, max_value=346.8148609672975[0m
[37m[1m[2023-07-17 14:38:03,887][257371] New mean coefficients: [[ 0.0381145   0.14310637 -0.25221473  0.25200686 -0.61017424 -1.0312264 ]][0m
[37m[1m[2023-07-17 14:38:03,888][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:38:12,881][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 14:38:12,887][257371] FPS: 427036.08[0m
[36m[2023-07-17 14:38:12,890][257371] itr=1356, itrs=2000, Progress: 67.80%[0m
[36m[2023-07-17 14:38:24,574][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 14:38:24,575][257371] FPS: 331802.42[0m
[36m[2023-07-17 14:38:28,949][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:38:28,949][257371] Reward + Measures: [[731.50249721   0.27561232   0.40846568   0.23787366   0.51201999
    2.49720144]][0m
[37m[1m[2023-07-17 14:38:28,949][257371] Max Reward on eval: 731.5024972059997[0m
[37m[1m[2023-07-17 14:38:28,950][257371] Min Reward on eval: 731.5024972059997[0m
[37m[1m[2023-07-17 14:38:28,950][257371] Mean Reward across all agents: 731.5024972059997[0m
[37m[1m[2023-07-17 14:38:28,950][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:38:33,933][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:38:33,933][257371] Reward + Measures: [[683.93965922   0.27199998   0.4118       0.27380002   0.48890001
    2.49038434]
 [703.70047857   0.31610003   0.419        0.24099998   0.47490001
    2.87445664]
 [615.3356955    0.23029999   0.33169997   0.20200001   0.41339999
    2.68098712]
 ...
 [654.23402958   0.22939999   0.35060003   0.2218       0.42340001
    2.4012773 ]
 [689.99398065   0.26430002   0.40159997   0.2404       0.46430001
    2.3050251 ]
 [284.96473644   0.47349998   0.42180005   0.3804       0.30110002
    3.96531487]][0m
[37m[1m[2023-07-17 14:38:33,933][257371] Max Reward on eval: 846.8560103958473[0m
[37m[1m[2023-07-17 14:38:33,934][257371] Min Reward on eval: 121.02515595118311[0m
[37m[1m[2023-07-17 14:38:33,934][257371] Mean Reward across all agents: 603.7088097538074[0m
[37m[1m[2023-07-17 14:38:33,934][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:38:33,936][257371] mean_value=-903.1849875367757, max_value=339.9224003344212[0m
[37m[1m[2023-07-17 14:38:33,938][257371] New mean coefficients: [[ 0.15573461  0.132021    0.0141249   0.15280986 -0.5002774  -1.3648872 ]][0m
[37m[1m[2023-07-17 14:38:33,939][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:38:42,975][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 14:38:42,975][257371] FPS: 425075.24[0m
[36m[2023-07-17 14:38:42,977][257371] itr=1357, itrs=2000, Progress: 67.85%[0m
[36m[2023-07-17 14:38:54,902][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 14:38:54,902][257371] FPS: 325273.18[0m
[36m[2023-07-17 14:38:59,278][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:38:59,278][257371] Reward + Measures: [[733.68068296   0.28084567   0.4129017    0.23844099   0.51363301
    2.48597026]][0m
[37m[1m[2023-07-17 14:38:59,279][257371] Max Reward on eval: 733.6806829582077[0m
[37m[1m[2023-07-17 14:38:59,279][257371] Min Reward on eval: 733.6806829582077[0m
[37m[1m[2023-07-17 14:38:59,279][257371] Mean Reward across all agents: 733.6806829582077[0m
[37m[1m[2023-07-17 14:38:59,279][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:39:04,293][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:39:04,294][257371] Reward + Measures: [[584.44712452   0.39860001   0.35960001   0.32729998   0.45280001
    2.12174678]
 [477.22369541   0.2561       0.36760002   0.28690001   0.49840003
    2.74953389]
 [637.51649853   0.38259998   0.40019998   0.3204       0.53920001
    2.12050104]
 ...
 [770.76504513   0.31259999   0.42500001   0.28599998   0.52740002
    2.22951937]
 [731.14458464   0.36129996   0.41509995   0.30380002   0.49239999
    2.3614769 ]
 [449.96115882   0.29169998   0.30270001   0.26050001   0.35390002
    2.31867194]][0m
[37m[1m[2023-07-17 14:39:04,294][257371] Max Reward on eval: 851.8548126567155[0m
[37m[1m[2023-07-17 14:39:04,294][257371] Min Reward on eval: 1.8228532964363695[0m
[37m[1m[2023-07-17 14:39:04,294][257371] Mean Reward across all agents: 591.9868231403294[0m
[37m[1m[2023-07-17 14:39:04,295][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:39:04,297][257371] mean_value=-904.2115755387657, max_value=326.95409835330395[0m
[37m[1m[2023-07-17 14:39:04,300][257371] New mean coefficients: [[ 0.40844458 -0.04388188  0.1018482   0.19007614 -0.5490896  -1.1825051 ]][0m
[37m[1m[2023-07-17 14:39:04,301][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:39:13,337][257371] train() took 9.03 seconds to complete[0m
[36m[2023-07-17 14:39:13,337][257371] FPS: 425026.49[0m
[36m[2023-07-17 14:39:13,339][257371] itr=1358, itrs=2000, Progress: 67.90%[0m
[36m[2023-07-17 14:39:25,112][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 14:39:25,113][257371] FPS: 329373.76[0m
[36m[2023-07-17 14:39:29,440][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:39:29,441][257371] Reward + Measures: [[725.56966865   0.27758533   0.41543669   0.23895466   0.51394528
    2.45122623]][0m
[37m[1m[2023-07-17 14:39:29,441][257371] Max Reward on eval: 725.569668654813[0m
[37m[1m[2023-07-17 14:39:29,441][257371] Min Reward on eval: 725.569668654813[0m
[37m[1m[2023-07-17 14:39:29,441][257371] Mean Reward across all agents: 725.569668654813[0m
[37m[1m[2023-07-17 14:39:29,442][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:39:34,720][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:39:34,725][257371] Reward + Measures: [[601.03639412   0.3831       0.4305       0.31879997   0.44770002
    2.91103673]
 [829.05087277   0.32640001   0.40159997   0.2705       0.51130003
    2.42353249]
 [613.34743754   0.25390002   0.41319999   0.24790001   0.51639998
    2.48234725]
 ...
 [755.27255244   0.30149999   0.40170002   0.25320002   0.52759999
    2.34306765]
 [535.1111488    0.33080003   0.46730003   0.317        0.50119996
    2.85017085]
 [437.48275947   0.40619999   0.40699998   0.3466       0.35480002
    3.30389261]][0m
[37m[1m[2023-07-17 14:39:34,726][257371] Max Reward on eval: 829.0508727692068[0m
[37m[1m[2023-07-17 14:39:34,726][257371] Min Reward on eval: -23.74029145240784[0m
[37m[1m[2023-07-17 14:39:34,726][257371] Mean Reward across all agents: 577.5362781731943[0m
[37m[1m[2023-07-17 14:39:34,727][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:39:34,728][257371] mean_value=-502.6748534359619, max_value=286.83054618983635[0m
[37m[1m[2023-07-17 14:39:34,731][257371] New mean coefficients: [[ 0.26115766 -0.177765    0.0236029   0.06546131 -0.40710396 -1.1386547 ]][0m
[37m[1m[2023-07-17 14:39:34,731][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:39:43,834][257371] train() took 9.10 seconds to complete[0m
[36m[2023-07-17 14:39:43,835][257371] FPS: 421912.06[0m
[36m[2023-07-17 14:39:43,837][257371] itr=1359, itrs=2000, Progress: 67.95%[0m
[36m[2023-07-17 14:39:55,778][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 14:39:55,778][257371] FPS: 324718.66[0m
[36m[2023-07-17 14:40:00,085][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:40:00,085][257371] Reward + Measures: [[721.10441208   0.275327     0.4242053    0.23796801   0.52372104
    2.42711425]][0m
[37m[1m[2023-07-17 14:40:00,086][257371] Max Reward on eval: 721.104412081096[0m
[37m[1m[2023-07-17 14:40:00,086][257371] Min Reward on eval: 721.104412081096[0m
[37m[1m[2023-07-17 14:40:00,086][257371] Mean Reward across all agents: 721.104412081096[0m
[37m[1m[2023-07-17 14:40:00,086][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:40:05,161][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:40:05,161][257371] Reward + Measures: [[430.67972231   0.20299999   0.29160002   0.17479999   0.34730002
    2.62299109]
 [720.76546134   0.26690003   0.39790002   0.23539999   0.46899995
    2.54231501]
 [601.55135111   0.27000004   0.4048       0.23379998   0.51880002
    2.58409429]
 ...
 [759.12725066   0.28440002   0.4628       0.25620002   0.49750003
    2.38287067]
 [526.52216108   0.22290002   0.3436       0.21080001   0.38610002
    2.49958873]
 [680.59728625   0.41330001   0.53080004   0.22850001   0.55419999
    3.06014419]][0m
[37m[1m[2023-07-17 14:40:05,162][257371] Max Reward on eval: 819.6501694114879[0m
[37m[1m[2023-07-17 14:40:05,162][257371] Min Reward on eval: 248.44298656731843[0m
[37m[1m[2023-07-17 14:40:05,162][257371] Mean Reward across all agents: 659.3499635625066[0m
[37m[1m[2023-07-17 14:40:05,162][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:40:05,164][257371] mean_value=-417.53902442769265, max_value=181.07805121587876[0m
[37m[1m[2023-07-17 14:40:05,166][257371] New mean coefficients: [[ 0.17290215 -0.09929708  0.24004187  0.11514121 -0.37815082 -1.5405462 ]][0m
[37m[1m[2023-07-17 14:40:05,167][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:40:14,311][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 14:40:14,312][257371] FPS: 420007.12[0m
[36m[2023-07-17 14:40:14,314][257371] itr=1360, itrs=2000, Progress: 68.00%[0m
[37m[1m[2023-07-17 14:43:50,232][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001340[0m
[36m[2023-07-17 14:44:02,526][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 14:44:02,526][257371] FPS: 330194.85[0m
[36m[2023-07-17 14:44:06,763][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:44:06,764][257371] Reward + Measures: [[714.8650929    0.27541199   0.42645332   0.24240735   0.52703696
    2.38924193]][0m
[37m[1m[2023-07-17 14:44:06,764][257371] Max Reward on eval: 714.865092904433[0m
[37m[1m[2023-07-17 14:44:06,764][257371] Min Reward on eval: 714.865092904433[0m
[37m[1m[2023-07-17 14:44:06,764][257371] Mean Reward across all agents: 714.865092904433[0m
[37m[1m[2023-07-17 14:44:06,765][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:44:11,654][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:44:11,655][257371] Reward + Measures: [[478.51336243   0.20369999   0.40360004   0.176        0.48070002
    2.60831141]
 [583.744726     0.2624       0.3513       0.23190001   0.41190001
    2.49408484]
 [624.03043316   0.27069998   0.38850001   0.2422       0.51210004
    2.52443194]
 ...
 [642.93635985   0.2782       0.46510002   0.20799999   0.49190003
    2.80660868]
 [694.48013303   0.2922       0.49719998   0.2359       0.60480005
    2.45672035]
 [649.06528858   0.25310001   0.45140001   0.2027       0.55839998
    2.82723093]][0m
[37m[1m[2023-07-17 14:44:11,655][257371] Max Reward on eval: 858.083213750762[0m
[37m[1m[2023-07-17 14:44:11,655][257371] Min Reward on eval: 209.15183738311754[0m
[37m[1m[2023-07-17 14:44:11,656][257371] Mean Reward across all agents: 654.8757301211305[0m
[37m[1m[2023-07-17 14:44:11,656][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:44:11,658][257371] mean_value=-293.454335143375, max_value=293.8635461406971[0m
[37m[1m[2023-07-17 14:44:11,661][257371] New mean coefficients: [[ 0.14042619 -0.21035859  0.35201722  0.1954852  -0.4612882  -1.5306307 ]][0m
[37m[1m[2023-07-17 14:44:11,662][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:44:20,563][257371] train() took 8.90 seconds to complete[0m
[36m[2023-07-17 14:44:20,563][257371] FPS: 431500.21[0m
[36m[2023-07-17 14:44:20,565][257371] itr=1361, itrs=2000, Progress: 68.05%[0m
[36m[2023-07-17 14:44:32,388][257371] train() took 11.71 seconds to complete[0m
[36m[2023-07-17 14:44:32,388][257371] FPS: 327910.77[0m
[36m[2023-07-17 14:44:36,611][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:44:36,611][257371] Reward + Measures: [[706.86828118   0.27347931   0.43578666   0.24089001   0.53160733
    2.37238789]][0m
[37m[1m[2023-07-17 14:44:36,611][257371] Max Reward on eval: 706.8682811846063[0m
[37m[1m[2023-07-17 14:44:36,612][257371] Min Reward on eval: 706.8682811846063[0m
[37m[1m[2023-07-17 14:44:36,612][257371] Mean Reward across all agents: 706.8682811846063[0m
[37m[1m[2023-07-17 14:44:36,612][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:44:41,578][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:44:41,579][257371] Reward + Measures: [[579.42959211   0.37279999   0.4197       0.2419       0.40580001
    3.12265444]
 [685.81604003   0.28099999   0.44619998   0.26550001   0.5431
    2.44565654]
 [619.42858885   0.3574       0.414        0.25740001   0.41780001
    2.84755993]
 ...
 [554.28015141   0.37960002   0.43920001   0.24820001   0.48450002
    3.16807604]
 [722.28948213   0.30560002   0.5151       0.21799998   0.62120003
    2.5128293 ]
 [659.09945223   0.32180002   0.48119998   0.2158       0.54759997
    2.7345891 ]][0m
[37m[1m[2023-07-17 14:44:41,579][257371] Max Reward on eval: 838.6590957740322[0m
[37m[1m[2023-07-17 14:44:41,579][257371] Min Reward on eval: 256.8012020442635[0m
[37m[1m[2023-07-17 14:44:41,580][257371] Mean Reward across all agents: 656.2391325309936[0m
[37m[1m[2023-07-17 14:44:41,580][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:44:41,582][257371] mean_value=-448.37376266898576, max_value=344.84917417260266[0m
[37m[1m[2023-07-17 14:44:41,584][257371] New mean coefficients: [[ 0.15731454 -0.14776176  0.3676907   0.0929636  -0.37466723 -1.4457593 ]][0m
[37m[1m[2023-07-17 14:44:41,585][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:44:50,555][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 14:44:50,555][257371] FPS: 428166.29[0m
[36m[2023-07-17 14:44:50,557][257371] itr=1362, itrs=2000, Progress: 68.10%[0m
[36m[2023-07-17 14:45:02,228][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 14:45:02,228][257371] FPS: 332213.10[0m
[36m[2023-07-17 14:45:06,524][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:45:06,529][257371] Reward + Measures: [[697.97195878   0.27341965   0.43653968   0.24288501   0.52885431
    2.36055326]][0m
[37m[1m[2023-07-17 14:45:06,530][257371] Max Reward on eval: 697.9719587794938[0m
[37m[1m[2023-07-17 14:45:06,530][257371] Min Reward on eval: 697.9719587794938[0m
[37m[1m[2023-07-17 14:45:06,530][257371] Mean Reward across all agents: 697.9719587794938[0m
[37m[1m[2023-07-17 14:45:06,531][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:45:11,604][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:45:11,609][257371] Reward + Measures: [[658.36539458   0.29180002   0.43700001   0.25240001   0.56420004
    2.32501245]
 [761.28625489   0.26079997   0.45230004   0.23710001   0.56389999
    2.3182559 ]
 [650.57913156   0.34760001   0.41710001   0.24829999   0.48120004
    2.72313547]
 ...
 [784.35742192   0.26370001   0.4461       0.255        0.55430001
    2.34503412]
 [664.52593996   0.2665       0.4488       0.23190001   0.55919999
    2.2159555 ]
 [496.31497479   0.36620003   0.50830001   0.28330001   0.58140004
    2.74247622]][0m
[37m[1m[2023-07-17 14:45:11,610][257371] Max Reward on eval: 821.6631622171029[0m
[37m[1m[2023-07-17 14:45:11,610][257371] Min Reward on eval: 213.8652619241504[0m
[37m[1m[2023-07-17 14:45:11,610][257371] Mean Reward across all agents: 625.2859281192807[0m
[37m[1m[2023-07-17 14:45:11,610][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:45:11,612][257371] mean_value=-338.45934285368605, max_value=93.04929586407923[0m
[37m[1m[2023-07-17 14:45:11,614][257371] New mean coefficients: [[-0.01845895 -0.26345846  0.2897993   0.2201893  -0.29241467 -1.1437579 ]][0m
[37m[1m[2023-07-17 14:45:11,615][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:45:20,616][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 14:45:20,616][257371] FPS: 426738.63[0m
[36m[2023-07-17 14:45:20,618][257371] itr=1363, itrs=2000, Progress: 68.15%[0m
[36m[2023-07-17 14:45:32,344][257371] train() took 11.61 seconds to complete[0m
[36m[2023-07-17 14:45:32,344][257371] FPS: 330607.24[0m
[36m[2023-07-17 14:45:36,570][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:45:36,570][257371] Reward + Measures: [[682.73315011   0.27272999   0.44183269   0.24613701   0.53715998
    2.33641219]][0m
[37m[1m[2023-07-17 14:45:36,571][257371] Max Reward on eval: 682.733150110163[0m
[37m[1m[2023-07-17 14:45:36,571][257371] Min Reward on eval: 682.733150110163[0m
[37m[1m[2023-07-17 14:45:36,571][257371] Mean Reward across all agents: 682.733150110163[0m
[37m[1m[2023-07-17 14:45:36,571][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:45:41,564][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:45:41,565][257371] Reward + Measures: [[716.38707068   0.26809999   0.37620002   0.26050001   0.4436
    2.55489159]
 [732.07842256   0.2915       0.45160004   0.27239999   0.53130007
    2.28779578]
 [745.57889941   0.2608       0.42550001   0.2428       0.51899999
    2.44476008]
 ...
 [628.20099445   0.25580001   0.35380003   0.23469999   0.4323
    2.51741338]
 [620.3120346    0.23339999   0.42080003   0.2263       0.52230006
    2.33750129]
 [771.27059933   0.27690002   0.44600001   0.25030002   0.50139999
    2.4879334 ]][0m
[37m[1m[2023-07-17 14:45:41,565][257371] Max Reward on eval: 799.0245285205543[0m
[37m[1m[2023-07-17 14:45:41,565][257371] Min Reward on eval: 57.4274192834273[0m
[37m[1m[2023-07-17 14:45:41,565][257371] Mean Reward across all agents: 617.154552061528[0m
[37m[1m[2023-07-17 14:45:41,566][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:45:41,568][257371] mean_value=-646.6494397816941, max_value=185.11454407663052[0m
[37m[1m[2023-07-17 14:45:41,570][257371] New mean coefficients: [[ 0.09370026 -0.24370337  0.45004484 -0.00185399 -0.19408372 -1.0123758 ]][0m
[37m[1m[2023-07-17 14:45:41,571][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:45:50,608][257371] train() took 9.04 seconds to complete[0m
[36m[2023-07-17 14:45:50,608][257371] FPS: 425010.46[0m
[36m[2023-07-17 14:45:50,610][257371] itr=1364, itrs=2000, Progress: 68.20%[0m
[36m[2023-07-17 14:46:02,289][257371] train() took 11.57 seconds to complete[0m
[36m[2023-07-17 14:46:02,289][257371] FPS: 331967.13[0m
[36m[2023-07-17 14:46:06,632][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:46:06,637][257371] Reward + Measures: [[673.84933771   0.27403164   0.4493793    0.24820533   0.5421347
    2.30794764]][0m
[37m[1m[2023-07-17 14:46:06,638][257371] Max Reward on eval: 673.849337705064[0m
[37m[1m[2023-07-17 14:46:06,638][257371] Min Reward on eval: 673.849337705064[0m
[37m[1m[2023-07-17 14:46:06,638][257371] Mean Reward across all agents: 673.849337705064[0m
[37m[1m[2023-07-17 14:46:06,639][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:46:11,585][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:46:11,585][257371] Reward + Measures: [[564.66828731   0.49990001   0.61880004   0.21329999   0.59069997
    3.68632507]
 [648.81207684   0.32799998   0.41589999   0.22690001   0.49880001
    2.77674913]
 [572.8723542    0.29730001   0.37970001   0.23629999   0.45180002
    2.63752437]
 ...
 [487.40468426   0.29889998   0.43070003   0.23049998   0.51109999
    2.58566546]
 [604.96870351   0.29870003   0.39750001   0.23269999   0.45430002
    2.82479525]
 [662.16533036   0.3283       0.4612       0.1963       0.55409998
    2.59009027]][0m
[37m[1m[2023-07-17 14:46:11,585][257371] Max Reward on eval: 788.9939956935123[0m
[37m[1m[2023-07-17 14:46:11,586][257371] Min Reward on eval: 369.36977646318263[0m
[37m[1m[2023-07-17 14:46:11,586][257371] Mean Reward across all agents: 628.4946087294809[0m
[37m[1m[2023-07-17 14:46:11,586][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:46:11,588][257371] mean_value=-337.4240853894636, max_value=302.58573805003954[0m
[37m[1m[2023-07-17 14:46:11,590][257371] New mean coefficients: [[ 0.24751057 -0.06012364  0.3207606   0.03712796 -0.24194741 -1.1538024 ]][0m
[37m[1m[2023-07-17 14:46:11,591][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:46:20,591][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 14:46:20,591][257371] FPS: 426773.95[0m
[36m[2023-07-17 14:46:20,593][257371] itr=1365, itrs=2000, Progress: 68.25%[0m
[36m[2023-07-17 14:46:32,282][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 14:46:32,282][257371] FPS: 331680.62[0m
[36m[2023-07-17 14:46:36,558][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:46:36,559][257371] Reward + Measures: [[674.22933128   0.27557665   0.45434737   0.25155431   0.54812199
    2.26164317]][0m
[37m[1m[2023-07-17 14:46:36,559][257371] Max Reward on eval: 674.2293312799318[0m
[37m[1m[2023-07-17 14:46:36,559][257371] Min Reward on eval: 674.2293312799318[0m
[37m[1m[2023-07-17 14:46:36,560][257371] Mean Reward across all agents: 674.2293312799318[0m
[37m[1m[2023-07-17 14:46:36,560][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:46:41,539][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:46:41,545][257371] Reward + Measures: [[499.71202569   0.24779999   0.40290004   0.22669999   0.48619995
    2.47005081]
 [570.31644026   0.27869999   0.42940003   0.1973       0.49379998
    2.60129976]
 [615.37871914   0.34099999   0.44369999   0.24130002   0.50520003
    2.60568547]
 ...
 [722.5299988    0.3646       0.42090002   0.29199997   0.53609997
    2.25939798]
 [696.04478077   0.33339998   0.46680003   0.24510001   0.56740004
    2.52893257]
 [517.9634189    0.3786       0.4763       0.2545       0.57709998
    2.6423912 ]][0m
[37m[1m[2023-07-17 14:46:41,545][257371] Max Reward on eval: 767.8028907830827[0m
[37m[1m[2023-07-17 14:46:41,546][257371] Min Reward on eval: 263.55787950220986[0m
[37m[1m[2023-07-17 14:46:41,546][257371] Mean Reward across all agents: 616.6202991009997[0m
[37m[1m[2023-07-17 14:46:41,546][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:46:41,548][257371] mean_value=-326.21681514948983, max_value=723.3836540928728[0m
[37m[1m[2023-07-17 14:46:41,551][257371] New mean coefficients: [[ 0.2411118  -0.26023442  0.30527622 -0.03712717 -0.34800208 -0.91024274]][0m
[37m[1m[2023-07-17 14:46:41,552][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:46:50,526][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 14:46:50,526][257371] FPS: 427985.39[0m
[36m[2023-07-17 14:46:50,528][257371] itr=1366, itrs=2000, Progress: 68.30%[0m
[36m[2023-07-17 14:47:02,440][257371] train() took 11.80 seconds to complete[0m
[36m[2023-07-17 14:47:02,441][257371] FPS: 325381.33[0m
[36m[2023-07-17 14:47:06,766][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:47:06,767][257371] Reward + Measures: [[671.31141606   0.275823     0.45004731   0.25288334   0.544182
    2.23867965]][0m
[37m[1m[2023-07-17 14:47:06,767][257371] Max Reward on eval: 671.3114160551507[0m
[37m[1m[2023-07-17 14:47:06,767][257371] Min Reward on eval: 671.3114160551507[0m
[37m[1m[2023-07-17 14:47:06,767][257371] Mean Reward across all agents: 671.3114160551507[0m
[37m[1m[2023-07-17 14:47:06,768][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:47:11,837][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:47:11,838][257371] Reward + Measures: [[554.29533317   0.3114       0.35690001   0.27919999   0.45719996
    2.39200664]
 [669.03736884   0.3854       0.41799998   0.35620004   0.53109998
    2.21352911]
 [677.38144683   0.29969999   0.47490001   0.294        0.56339997
    2.24211812]
 ...
 [635.95175941   0.36590001   0.3829       0.31119999   0.46750003
    2.56253815]
 [576.95172123   0.35490003   0.44549999   0.35450003   0.53670001
    2.3020556 ]
 [678.4784927    0.28350002   0.45790002   0.2746       0.5456
    2.41286588]][0m
[37m[1m[2023-07-17 14:47:11,838][257371] Max Reward on eval: 779.8557700750883[0m
[37m[1m[2023-07-17 14:47:11,838][257371] Min Reward on eval: 60.34722796343267[0m
[37m[1m[2023-07-17 14:47:11,838][257371] Mean Reward across all agents: 641.213095944453[0m
[37m[1m[2023-07-17 14:47:11,839][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:47:11,841][257371] mean_value=-303.3246396005275, max_value=125.3788891682882[0m
[37m[1m[2023-07-17 14:47:11,843][257371] New mean coefficients: [[ 0.22090718 -0.4481418   0.3345685  -0.13873416 -0.13016517 -0.6742692 ]][0m
[37m[1m[2023-07-17 14:47:11,844][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:47:20,908][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 14:47:20,908][257371] FPS: 423723.64[0m
[36m[2023-07-17 14:47:20,910][257371] itr=1367, itrs=2000, Progress: 68.35%[0m
[36m[2023-07-17 14:47:32,701][257371] train() took 11.68 seconds to complete[0m
[36m[2023-07-17 14:47:32,701][257371] FPS: 328901.25[0m
[36m[2023-07-17 14:47:37,032][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:47:37,037][257371] Reward + Measures: [[672.97176417   0.27578467   0.456655     0.25176167   0.54203737
    2.23159099]][0m
[37m[1m[2023-07-17 14:47:37,038][257371] Max Reward on eval: 672.9717641737358[0m
[37m[1m[2023-07-17 14:47:37,038][257371] Min Reward on eval: 672.9717641737358[0m
[37m[1m[2023-07-17 14:47:37,038][257371] Mean Reward across all agents: 672.9717641737358[0m
[37m[1m[2023-07-17 14:47:37,038][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:47:42,111][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:47:42,116][257371] Reward + Measures: [[520.27166537   0.26630002   0.35279998   0.23309998   0.41589999
    2.38290381]
 [493.14545727   0.36170003   0.48470002   0.30860001   0.53909999
    2.62997317]
 [643.52312282   0.2667       0.44230005   0.26269999   0.51249999
    2.25627089]
 ...
 [543.9854282    0.30510002   0.3547       0.1894       0.40780002
    2.61740541]
 [565.93784236   0.26120001   0.44930002   0.2411       0.54539996
    2.20909452]
 [598.07401654   0.36219999   0.41940004   0.28620002   0.4903
    2.3659327 ]][0m
[37m[1m[2023-07-17 14:47:42,117][257371] Max Reward on eval: 836.7309112597256[0m
[37m[1m[2023-07-17 14:47:42,117][257371] Min Reward on eval: 247.99254088902381[0m
[37m[1m[2023-07-17 14:47:42,117][257371] Mean Reward across all agents: 637.8062978792748[0m
[37m[1m[2023-07-17 14:47:42,118][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:47:42,119][257371] mean_value=-511.7725419247003, max_value=394.93105406218643[0m
[37m[1m[2023-07-17 14:47:42,122][257371] New mean coefficients: [[ 0.1693388  -0.7003231   0.28035727 -0.04056125  0.03810684 -0.11057824]][0m
[37m[1m[2023-07-17 14:47:42,123][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:47:51,187][257371] train() took 9.06 seconds to complete[0m
[36m[2023-07-17 14:47:51,187][257371] FPS: 423712.80[0m
[36m[2023-07-17 14:47:51,190][257371] itr=1368, itrs=2000, Progress: 68.40%[0m
[36m[2023-07-17 14:48:03,076][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 14:48:03,076][257371] FPS: 326288.09[0m
[36m[2023-07-17 14:48:07,389][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:48:07,395][257371] Reward + Measures: [[684.82728121   0.27552369   0.45951366   0.25159931   0.54331201
    2.20636606]][0m
[37m[1m[2023-07-17 14:48:07,396][257371] Max Reward on eval: 684.827281211437[0m
[37m[1m[2023-07-17 14:48:07,396][257371] Min Reward on eval: 684.827281211437[0m
[37m[1m[2023-07-17 14:48:07,397][257371] Mean Reward across all agents: 684.827281211437[0m
[37m[1m[2023-07-17 14:48:07,398][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:48:12,705][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:48:12,711][257371] Reward + Measures: [[653.96298977   0.29640001   0.45000002   0.25050002   0.52270001
    2.49055982]
 [506.7181015    0.30960003   0.42350003   0.29720002   0.4777
    2.55237913]
 [732.34494018   0.3116       0.41760001   0.2791       0.45790002
    2.45477939]
 ...
 [534.78658459   0.29560003   0.38800001   0.27649999   0.43870002
    2.5422833 ]
 [614.40245636   0.28710002   0.41799998   0.27500001   0.47989997
    2.3745594 ]
 [670.77927019   0.28360003   0.4542       0.25280002   0.53190005
    2.43268085]][0m
[37m[1m[2023-07-17 14:48:12,711][257371] Max Reward on eval: 792.5934906223789[0m
[37m[1m[2023-07-17 14:48:12,711][257371] Min Reward on eval: 360.52117561001796[0m
[37m[1m[2023-07-17 14:48:12,712][257371] Mean Reward across all agents: 594.7736388318025[0m
[37m[1m[2023-07-17 14:48:12,712][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:48:12,713][257371] mean_value=-442.87104536747785, max_value=-0.6331703732764709[0m
[36m[2023-07-17 14:48:12,717][257371] XNES is restarting with a new solution whose measures are [0.4605     0.2861     0.36560002 0.41870004 1.97407568] and objective is 287.67674483405426[0m
[36m[2023-07-17 14:48:12,718][257371] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-17 14:48:12,721][257371] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-17 14:48:12,722][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:48:21,794][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 14:48:21,794][257371] FPS: 423323.35[0m
[36m[2023-07-17 14:48:21,796][257371] itr=1369, itrs=2000, Progress: 68.45%[0m
[36m[2023-07-17 14:48:33,671][257371] train() took 11.76 seconds to complete[0m
[36m[2023-07-17 14:48:33,671][257371] FPS: 326540.77[0m
[36m[2023-07-17 14:48:37,984][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:48:37,984][257371] Reward + Measures: [[312.96263653   0.44402865   0.29809865   0.34839034   0.39788634
    2.05473542]][0m
[37m[1m[2023-07-17 14:48:37,984][257371] Max Reward on eval: 312.96263652690214[0m
[37m[1m[2023-07-17 14:48:37,985][257371] Min Reward on eval: 312.96263652690214[0m
[37m[1m[2023-07-17 14:48:37,985][257371] Mean Reward across all agents: 312.96263652690214[0m
[37m[1m[2023-07-17 14:48:37,985][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:48:43,047][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:48:43,047][257371] Reward + Measures: [[218.81210464   0.41010004   0.34460002   0.31899998   0.36219999
    2.47371745]
 [405.71998406   0.41560003   0.29010001   0.35100004   0.41760001
    2.02961683]
 [214.18051396   0.35789999   0.25260001   0.29530001   0.31280002
    2.11868644]
 ...
 [325.46253322   0.41810003   0.29010001   0.34559998   0.37409997
    2.08733249]
 [248.99335128   0.45479998   0.29449999   0.33849999   0.3863
    2.13296366]
 [378.47485356   0.45230004   0.31210002   0.35110003   0.41760001
    2.11357117]][0m
[37m[1m[2023-07-17 14:48:43,047][257371] Max Reward on eval: 437.15190884443[0m
[37m[1m[2023-07-17 14:48:43,048][257371] Min Reward on eval: 76.57642991365864[0m
[37m[1m[2023-07-17 14:48:43,048][257371] Mean Reward across all agents: 266.9113422527522[0m
[37m[1m[2023-07-17 14:48:43,048][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:48:43,050][257371] mean_value=-1049.4113808967259, max_value=328.82460241086756[0m
[37m[1m[2023-07-17 14:48:43,053][257371] New mean coefficients: [[ 0.7868781  -0.11392462 -1.463433   -0.73253036 -1.3153446  -0.8221319 ]][0m
[37m[1m[2023-07-17 14:48:43,054][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:48:52,226][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-17 14:48:52,226][257371] FPS: 418764.12[0m
[36m[2023-07-17 14:48:52,228][257371] itr=1370, itrs=2000, Progress: 68.50%[0m
[37m[1m[2023-07-17 14:52:29,539][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001350[0m
[36m[2023-07-17 14:52:41,962][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 14:52:41,962][257371] FPS: 328537.94[0m
[36m[2023-07-17 14:52:46,258][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:52:46,259][257371] Reward + Measures: [[350.10438991   0.43228766   0.28734732   0.33411366   0.38507062
    2.03124475]][0m
[37m[1m[2023-07-17 14:52:46,259][257371] Max Reward on eval: 350.1043899057481[0m
[37m[1m[2023-07-17 14:52:46,259][257371] Min Reward on eval: 350.1043899057481[0m
[37m[1m[2023-07-17 14:52:46,259][257371] Mean Reward across all agents: 350.1043899057481[0m
[37m[1m[2023-07-17 14:52:46,260][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:52:51,269][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:52:51,274][257371] Reward + Measures: [[106.25609796   0.23629999   0.22830001   0.19460002   0.22360002
    2.15901375]
 [399.08289335   0.44169998   0.32119998   0.35770002   0.45359999
    2.06892681]
 [342.20682318   0.2992       0.28840002   0.23930001   0.3432
    2.19834328]
 ...
 [430.68747327   0.43560001   0.30230001   0.3348       0.41009998
    2.01188874]
 [256.71814934   0.44409999   0.32839999   0.33469996   0.36069998
    2.16209674]
 [337.82658124   0.32550001   0.2471       0.25840002   0.33640003
    2.0953815 ]][0m
[37m[1m[2023-07-17 14:52:51,275][257371] Max Reward on eval: 523.6741218252107[0m
[37m[1m[2023-07-17 14:52:51,275][257371] Min Reward on eval: 106.2560979624046[0m
[37m[1m[2023-07-17 14:52:51,275][257371] Mean Reward across all agents: 342.111757926747[0m
[37m[1m[2023-07-17 14:52:51,276][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:52:51,278][257371] mean_value=-1699.523165629717, max_value=401.68101505834863[0m
[37m[1m[2023-07-17 14:52:51,281][257371] New mean coefficients: [[ 0.2720685  -0.41641507 -1.941061   -0.09185612 -1.0439731   0.0514667 ]][0m
[37m[1m[2023-07-17 14:52:51,282][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:53:00,446][257371] train() took 9.16 seconds to complete[0m
[36m[2023-07-17 14:53:00,447][257371] FPS: 419070.24[0m
[36m[2023-07-17 14:53:00,449][257371] itr=1371, itrs=2000, Progress: 68.55%[0m
[36m[2023-07-17 14:53:12,226][257371] train() took 11.66 seconds to complete[0m
[36m[2023-07-17 14:53:12,226][257371] FPS: 329317.46[0m
[36m[2023-07-17 14:53:16,469][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:53:16,469][257371] Reward + Measures: [[364.12753106   0.41757095   0.27284667   0.32316667   0.36764035
    2.07350612]][0m
[37m[1m[2023-07-17 14:53:16,469][257371] Max Reward on eval: 364.1275310568358[0m
[37m[1m[2023-07-17 14:53:16,470][257371] Min Reward on eval: 364.1275310568358[0m
[37m[1m[2023-07-17 14:53:16,470][257371] Mean Reward across all agents: 364.1275310568358[0m
[37m[1m[2023-07-17 14:53:16,470][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:53:21,652][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:53:21,652][257371] Reward + Measures: [[388.12686161   0.48600003   0.33150002   0.35030001   0.40830001
    2.05145931]
 [114.43175086   0.3348       0.39209998   0.26440001   0.3249
    2.80140567]
 [454.84946256   0.41270003   0.2624       0.31080002   0.3935
    2.08644342]
 ...
 [481.16258238   0.4506       0.30960003   0.333        0.40910003
    2.16999817]
 [320.29687306   0.4007       0.28269997   0.33379999   0.34479997
    2.1526134 ]
 [342.28685142   0.42990002   0.28989998   0.32279998   0.35409999
    2.08561683]][0m
[37m[1m[2023-07-17 14:53:21,652][257371] Max Reward on eval: 529.1381228301674[0m
[37m[1m[2023-07-17 14:53:21,653][257371] Min Reward on eval: 114.43175086291157[0m
[37m[1m[2023-07-17 14:53:21,653][257371] Mean Reward across all agents: 359.1679333093573[0m
[37m[1m[2023-07-17 14:53:21,653][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:53:21,655][257371] mean_value=-985.4960305053845, max_value=196.23564990883034[0m
[37m[1m[2023-07-17 14:53:21,658][257371] New mean coefficients: [[ 0.33676082  1.2219934  -2.791439   -0.6143611  -1.1641618   0.68909985]][0m
[37m[1m[2023-07-17 14:53:21,659][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:53:30,634][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 14:53:30,635][257371] FPS: 427884.37[0m
[36m[2023-07-17 14:53:30,637][257371] itr=1372, itrs=2000, Progress: 68.60%[0m
[36m[2023-07-17 14:53:42,340][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 14:53:42,340][257371] FPS: 331331.37[0m
[36m[2023-07-17 14:53:46,547][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:53:46,548][257371] Reward + Measures: [[384.61402399   0.41417533   0.262106     0.319166     0.35573703
    2.14033437]][0m
[37m[1m[2023-07-17 14:53:46,548][257371] Max Reward on eval: 384.6140239877312[0m
[37m[1m[2023-07-17 14:53:46,548][257371] Min Reward on eval: 384.6140239877312[0m
[37m[1m[2023-07-17 14:53:46,549][257371] Mean Reward across all agents: 384.6140239877312[0m
[37m[1m[2023-07-17 14:53:46,549][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:53:51,473][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:53:51,474][257371] Reward + Measures: [[273.40805953   0.41730005   0.24460001   0.34290001   0.31990001
    2.0602119 ]
 [360.27672815   0.29449999   0.23290001   0.23         0.26760003
    2.36676383]
 [155.09050334   0.44329998   0.29840001   0.35590002   0.28400001
    2.33139396]
 ...
 [304.28711895   0.53909999   0.21820001   0.44900003   0.3793
    1.96410644]
 [339.25026082   0.43560001   0.25300002   0.34670001   0.38230002
    2.07544017]
 [288.81678102   0.3637       0.30650002   0.28940001   0.31760001
    2.39893603]][0m
[37m[1m[2023-07-17 14:53:51,474][257371] Max Reward on eval: 469.94372938689776[0m
[37m[1m[2023-07-17 14:53:51,474][257371] Min Reward on eval: 125.76660480033607[0m
[37m[1m[2023-07-17 14:53:51,474][257371] Mean Reward across all agents: 294.01239422428677[0m
[37m[1m[2023-07-17 14:53:51,475][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:53:51,476][257371] mean_value=-785.5840417000348, max_value=7.994939404134016[0m
[37m[1m[2023-07-17 14:53:51,479][257371] New mean coefficients: [[ 0.57436675  1.5446044  -2.119453   -0.14692512 -0.8077789   0.55125475]][0m
[37m[1m[2023-07-17 14:53:51,479][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:54:00,447][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 14:54:00,447][257371] FPS: 428293.17[0m
[36m[2023-07-17 14:54:00,449][257371] itr=1373, itrs=2000, Progress: 68.65%[0m
[36m[2023-07-17 14:54:12,290][257371] train() took 11.73 seconds to complete[0m
[36m[2023-07-17 14:54:12,290][257371] FPS: 327474.35[0m
[36m[2023-07-17 14:54:16,638][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:54:16,638][257371] Reward + Measures: [[387.70840701   0.40515965   0.254897     0.31036198   0.34174231
    2.21278834]][0m
[37m[1m[2023-07-17 14:54:16,639][257371] Max Reward on eval: 387.70840700646676[0m
[37m[1m[2023-07-17 14:54:16,639][257371] Min Reward on eval: 387.70840700646676[0m
[37m[1m[2023-07-17 14:54:16,639][257371] Mean Reward across all agents: 387.70840700646676[0m
[37m[1m[2023-07-17 14:54:16,639][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:54:21,697][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:54:21,703][257371] Reward + Measures: [[369.00958635   0.50740004   0.28030002   0.39179999   0.38210002
    2.22938466]
 [222.73002075   0.39260003   0.23049998   0.30650002   0.31300002
    2.28693366]
 [343.57445057   0.41680002   0.22130001   0.34299999   0.29929999
    2.34547663]
 ...
 [456.11164182   0.45049998   0.25689998   0.336        0.36640003
    2.12658429]
 [303.06535294   0.4086       0.25230002   0.32870001   0.30060002
    2.40304828]
 [456.66572893   0.4348       0.25920001   0.30920002   0.41890001
    2.2472477 ]][0m
[37m[1m[2023-07-17 14:54:21,703][257371] Max Reward on eval: 510.6348533773795[0m
[37m[1m[2023-07-17 14:54:21,703][257371] Min Reward on eval: 175.35836359323002[0m
[37m[1m[2023-07-17 14:54:21,704][257371] Mean Reward across all agents: 338.78957948769033[0m
[37m[1m[2023-07-17 14:54:21,704][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:54:21,706][257371] mean_value=-739.4067337572998, max_value=90.13946671840546[0m
[37m[1m[2023-07-17 14:54:21,708][257371] New mean coefficients: [[-0.5254399  1.5322614 -1.9119085 -1.2724397 -0.6639656 -1.1220733]][0m
[37m[1m[2023-07-17 14:54:21,709][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:54:30,822][257371] train() took 9.11 seconds to complete[0m
[36m[2023-07-17 14:54:30,822][257371] FPS: 421473.55[0m
[36m[2023-07-17 14:54:30,824][257371] itr=1374, itrs=2000, Progress: 68.70%[0m
[36m[2023-07-17 14:54:42,712][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 14:54:42,713][257371] FPS: 326255.94[0m
[36m[2023-07-17 14:54:47,044][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:54:47,044][257371] Reward + Measures: [[389.31656826   0.42566332   0.25097132   0.32192701   0.34858236
    2.12266946]][0m
[37m[1m[2023-07-17 14:54:47,045][257371] Max Reward on eval: 389.3165682600957[0m
[37m[1m[2023-07-17 14:54:47,045][257371] Min Reward on eval: 389.3165682600957[0m
[37m[1m[2023-07-17 14:54:47,045][257371] Mean Reward across all agents: 389.3165682600957[0m
[37m[1m[2023-07-17 14:54:47,045][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:54:51,996][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:54:51,997][257371] Reward + Measures: [[314.33647766   0.38250002   0.26069999   0.27309999   0.34399995
    2.16798186]
 [283.63217809   0.39610001   0.26629999   0.3231       0.30940002
    2.14510417]
 [284.85386527   0.45070001   0.23480001   0.36680001   0.29850003
    2.12368178]
 ...
 [146.24579085   0.38530001   0.19589999   0.3091       0.25729999
    2.05272055]
 [331.40299201   0.42669997   0.26760003   0.32449999   0.3443
    2.24124026]
 [347.98796172   0.4237       0.36090001   0.3098       0.38949999
    2.42950153]][0m
[37m[1m[2023-07-17 14:54:51,997][257371] Max Reward on eval: 459.25409698132427[0m
[37m[1m[2023-07-17 14:54:51,997][257371] Min Reward on eval: 133.51780638804195[0m
[37m[1m[2023-07-17 14:54:51,997][257371] Mean Reward across all agents: 327.6580418477801[0m
[37m[1m[2023-07-17 14:54:51,998][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:54:51,999][257371] mean_value=-731.1917800647651, max_value=89.89137897002273[0m
[37m[1m[2023-07-17 14:54:52,001][257371] New mean coefficients: [[-1.8034668   0.8303378  -1.5645027  -1.0239453   0.07335508 -0.93869215]][0m
[37m[1m[2023-07-17 14:54:52,002][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:55:00,956][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 14:55:00,956][257371] FPS: 428963.35[0m
[36m[2023-07-17 14:55:00,958][257371] itr=1375, itrs=2000, Progress: 68.75%[0m
[36m[2023-07-17 14:55:12,762][257371] train() took 11.69 seconds to complete[0m
[36m[2023-07-17 14:55:12,763][257371] FPS: 328583.04[0m
[36m[2023-07-17 14:55:16,996][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:55:16,996][257371] Reward + Measures: [[352.01447404   0.44541472   0.249033     0.33837065   0.35160831
    2.06887746]][0m
[37m[1m[2023-07-17 14:55:16,996][257371] Max Reward on eval: 352.01447404184347[0m
[37m[1m[2023-07-17 14:55:16,997][257371] Min Reward on eval: 352.01447404184347[0m
[37m[1m[2023-07-17 14:55:16,997][257371] Mean Reward across all agents: 352.01447404184347[0m
[37m[1m[2023-07-17 14:55:16,997][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:55:21,964][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:55:21,969][257371] Reward + Measures: [[291.64093588   0.53280002   0.287        0.38270003   0.36160001
    1.90066075]
 [416.55479243   0.49810001   0.28099999   0.34129998   0.42939997
    2.08437014]
 [160.93223403   0.4217       0.29959998   0.35689998   0.30950001
    2.30278969]
 ...
 [313.65876007   0.54479998   0.29440001   0.40599999   0.39180002
    1.99990368]
 [200.89567612   0.43260002   0.391        0.3425       0.37850001
    2.37881255]
 [390.53331      0.49440002   0.24270001   0.38100001   0.39700001
    2.04435611]][0m
[37m[1m[2023-07-17 14:55:21,970][257371] Max Reward on eval: 522.6188840989023[0m
[37m[1m[2023-07-17 14:55:21,970][257371] Min Reward on eval: 150.02143564149736[0m
[37m[1m[2023-07-17 14:55:21,970][257371] Mean Reward across all agents: 310.5802156680572[0m
[37m[1m[2023-07-17 14:55:21,971][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:55:21,972][257371] mean_value=-678.4855988193813, max_value=41.33265280165432[0m
[37m[1m[2023-07-17 14:55:21,974][257371] New mean coefficients: [[-1.2232957   0.8621528  -2.2743986  -2.6726024   0.66782403 -1.2032769 ]][0m
[37m[1m[2023-07-17 14:55:21,975][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:55:30,937][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 14:55:30,938][257371] FPS: 428556.77[0m
[36m[2023-07-17 14:55:30,940][257371] itr=1376, itrs=2000, Progress: 68.80%[0m
[36m[2023-07-17 14:55:42,874][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 14:55:42,875][257371] FPS: 324848.45[0m
[36m[2023-07-17 14:55:47,123][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:55:47,123][257371] Reward + Measures: [[345.91264102   0.46384797   0.24822932   0.34552863   0.36886367
    1.98048604]][0m
[37m[1m[2023-07-17 14:55:47,123][257371] Max Reward on eval: 345.9126410184099[0m
[37m[1m[2023-07-17 14:55:47,124][257371] Min Reward on eval: 345.9126410184099[0m
[37m[1m[2023-07-17 14:55:47,124][257371] Mean Reward across all agents: 345.9126410184099[0m
[37m[1m[2023-07-17 14:55:47,124][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:55:52,105][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:55:52,106][257371] Reward + Measures: [[434.73958017   0.47150001   0.2782       0.35120001   0.43889999
    2.01866364]
 [138.32561638   0.32710001   0.16779999   0.23410001   0.2335
    2.05835819]
 [343.08861921   0.4725       0.27800003   0.3836       0.42360002
    2.11138415]
 ...
 [417.43343658   0.49670005   0.23169999   0.32420003   0.3955
    1.89934623]
 [432.0361042    0.49499997   0.2626       0.36520001   0.43470001
    2.00191569]
 [179.46442578   0.48500004   0.21180001   0.38420001   0.37269998
    2.08786201]][0m
[37m[1m[2023-07-17 14:55:52,106][257371] Max Reward on eval: 492.4994163871743[0m
[37m[1m[2023-07-17 14:55:52,106][257371] Min Reward on eval: 101.82424071300775[0m
[37m[1m[2023-07-17 14:55:52,106][257371] Mean Reward across all agents: 287.78794328958156[0m
[37m[1m[2023-07-17 14:55:52,107][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:55:52,108][257371] mean_value=-773.8286719996551, max_value=33.00940222186625[0m
[37m[1m[2023-07-17 14:55:52,111][257371] New mean coefficients: [[ 0.31269348  0.8923431  -1.5700921  -3.1062791   1.1891754  -1.1836331 ]][0m
[37m[1m[2023-07-17 14:55:52,112][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:56:01,078][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 14:56:01,078][257371] FPS: 428325.31[0m
[36m[2023-07-17 14:56:01,081][257371] itr=1377, itrs=2000, Progress: 68.85%[0m
[36m[2023-07-17 14:56:12,938][257371] train() took 11.74 seconds to complete[0m
[36m[2023-07-17 14:56:12,938][257371] FPS: 327074.30[0m
[36m[2023-07-17 14:56:17,271][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:56:17,271][257371] Reward + Measures: [[376.02422504   0.46876535   0.24530533   0.33451399   0.38669702
    1.92120671]][0m
[37m[1m[2023-07-17 14:56:17,271][257371] Max Reward on eval: 376.024225037283[0m
[37m[1m[2023-07-17 14:56:17,272][257371] Min Reward on eval: 376.024225037283[0m
[37m[1m[2023-07-17 14:56:17,272][257371] Mean Reward across all agents: 376.024225037283[0m
[37m[1m[2023-07-17 14:56:17,272][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:56:22,614][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:56:22,614][257371] Reward + Measures: [[237.94220286   0.33080003   0.20020001   0.24679999   0.28029999
    2.14771819]
 [410.97594127   0.42570001   0.26280004   0.30469999   0.3752
    2.13815856]
 [285.29800417   0.41979995   0.23819999   0.34280002   0.33979997
    1.91969573]
 ...
 [483.23424125   0.40890002   0.26010001   0.30080003   0.3626
    2.1394155 ]
 [346.95966782   0.37239999   0.32070002   0.25690001   0.35900003
    2.25778794]
 [147.66771179   0.39270002   0.30850002   0.333        0.29710004
    2.23085642]][0m
[37m[1m[2023-07-17 14:56:22,615][257371] Max Reward on eval: 483.2342412465252[0m
[37m[1m[2023-07-17 14:56:22,615][257371] Min Reward on eval: 112.26983232866041[0m
[37m[1m[2023-07-17 14:56:22,615][257371] Mean Reward across all agents: 303.8380565107457[0m
[37m[1m[2023-07-17 14:56:22,615][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:56:22,617][257371] mean_value=-1227.558415850278, max_value=33.56087761104624[0m
[37m[1m[2023-07-17 14:56:22,619][257371] New mean coefficients: [[ 0.00378847 -0.01804751 -1.1363753  -1.9902523   0.9178612  -1.2248117 ]][0m
[37m[1m[2023-07-17 14:56:22,620][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:56:31,587][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 14:56:31,588][257371] FPS: 428309.63[0m
[36m[2023-07-17 14:56:31,590][257371] itr=1378, itrs=2000, Progress: 68.90%[0m
[36m[2023-07-17 14:56:43,959][257371] train() took 12.25 seconds to complete[0m
[36m[2023-07-17 14:56:43,959][257371] FPS: 313394.52[0m
[36m[2023-07-17 14:56:48,280][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:56:48,280][257371] Reward + Measures: [[390.96688914   0.48208332   0.24713132   0.33193702   0.4073
    1.83000481]][0m
[37m[1m[2023-07-17 14:56:48,281][257371] Max Reward on eval: 390.9668891441146[0m
[37m[1m[2023-07-17 14:56:48,281][257371] Min Reward on eval: 390.9668891441146[0m
[37m[1m[2023-07-17 14:56:48,281][257371] Mean Reward across all agents: 390.9668891441146[0m
[37m[1m[2023-07-17 14:56:48,282][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:56:53,294][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:56:53,294][257371] Reward + Measures: [[350.05568649   0.40100002   0.29550001   0.26650003   0.39159998
    2.18890786]
 [269.61728853   0.35619998   0.23980001   0.2586       0.34250003
    2.18324947]
 [314.6659202    0.46079999   0.27340001   0.3066       0.4402
    1.73552895]
 ...
 [305.45570491   0.4686       0.33450001   0.28120002   0.44260001
    2.1306932 ]
 [310.79783248   0.5575       0.26570001   0.40740004   0.40510002
    1.95524788]
 [336.31163601   0.45879999   0.32290003   0.26799998   0.48820001
    1.98127365]][0m
[37m[1m[2023-07-17 14:56:53,295][257371] Max Reward on eval: 526.6811256688088[0m
[37m[1m[2023-07-17 14:56:53,295][257371] Min Reward on eval: 196.19712572121063[0m
[37m[1m[2023-07-17 14:56:53,295][257371] Mean Reward across all agents: 359.21553369599116[0m
[37m[1m[2023-07-17 14:56:53,295][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:56:53,297][257371] mean_value=-1628.6425373642319, max_value=870.4995585087249[0m
[37m[1m[2023-07-17 14:56:53,300][257371] New mean coefficients: [[ 0.5011568   0.31929964 -1.5136759  -1.3294616   1.2503501  -1.8372886 ]][0m
[37m[1m[2023-07-17 14:56:53,301][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:57:02,371][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 14:57:02,372][257371] FPS: 423427.75[0m
[36m[2023-07-17 14:57:02,374][257371] itr=1379, itrs=2000, Progress: 68.95%[0m
[36m[2023-07-17 14:57:14,273][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 14:57:14,274][257371] FPS: 325862.17[0m
[36m[2023-07-17 14:57:18,583][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:57:18,583][257371] Reward + Measures: [[400.08609056   0.48900032   0.24248999   0.32830468   0.41695365
    1.76680434]][0m
[37m[1m[2023-07-17 14:57:18,583][257371] Max Reward on eval: 400.08609055902025[0m
[37m[1m[2023-07-17 14:57:18,584][257371] Min Reward on eval: 400.08609055902025[0m
[37m[1m[2023-07-17 14:57:18,584][257371] Mean Reward across all agents: 400.08609055902025[0m
[37m[1m[2023-07-17 14:57:18,584][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:57:23,578][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 14:57:23,578][257371] Reward + Measures: [[400.58705734   0.44230005   0.2483       0.2924       0.42199999
    1.97602832]
 [416.37077949   0.45500001   0.24130002   0.30620003   0.42290002
    1.86394203]
 [241.47813199   0.52080005   0.23580001   0.3802       0.39559999
    1.8042233 ]
 ...
 [330.18371154   0.50349998   0.26810002   0.3461       0.41290003
    1.82139456]
 [366.0929091    0.49070001   0.23769999   0.34629998   0.39400002
    1.76733863]
 [204.74029448   0.50229996   0.2527       0.36600003   0.32750002
    1.91972291]][0m
[37m[1m[2023-07-17 14:57:23,578][257371] Max Reward on eval: 543.9120597435161[0m
[37m[1m[2023-07-17 14:57:23,579][257371] Min Reward on eval: 150.5881093321368[0m
[37m[1m[2023-07-17 14:57:23,579][257371] Mean Reward across all agents: 362.60482439720346[0m
[37m[1m[2023-07-17 14:57:23,579][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 14:57:23,581][257371] mean_value=-1133.8019470953157, max_value=528.7167115807723[0m
[37m[1m[2023-07-17 14:57:23,584][257371] New mean coefficients: [[ 1.0870101  -0.33910465 -2.7767572  -0.9305649   2.4402208  -1.1882522 ]][0m
[37m[1m[2023-07-17 14:57:23,585][257371] Moving the mean solution point...[0m
[36m[2023-07-17 14:57:32,571][257371] train() took 8.99 seconds to complete[0m
[36m[2023-07-17 14:57:32,571][257371] FPS: 427378.22[0m
[36m[2023-07-17 14:57:32,574][257371] itr=1380, itrs=2000, Progress: 69.00%[0m
[37m[1m[2023-07-17 15:01:25,982][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001360[0m
[36m[2023-07-17 15:01:38,659][257371] train() took 11.83 seconds to complete[0m
[36m[2023-07-17 15:01:38,660][257371] FPS: 324574.64[0m
[36m[2023-07-17 15:01:42,943][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:01:42,943][257371] Reward + Measures: [[435.30030125   0.48876533   0.24560401   0.31487066   0.43963599
    1.72161615]][0m
[37m[1m[2023-07-17 15:01:42,943][257371] Max Reward on eval: 435.30030125375634[0m
[37m[1m[2023-07-17 15:01:42,944][257371] Min Reward on eval: 435.30030125375634[0m
[37m[1m[2023-07-17 15:01:42,944][257371] Mean Reward across all agents: 435.30030125375634[0m
[37m[1m[2023-07-17 15:01:42,944][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:01:48,153][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:01:48,154][257371] Reward + Measures: [[368.10337308   0.50520003   0.23270002   0.331        0.42659998
    1.67672241]
 [445.1002461    0.4337       0.2332       0.28650004   0.41569996
    1.79843962]
 [369.02777767   0.37799999   0.23169999   0.2911       0.41090003
    2.04229593]
 ...
 [247.94792226   0.37540001   0.31420001   0.27670002   0.38979998
    2.24068475]
 [461.99309728   0.50940001   0.27520001   0.34260002   0.47549996
    1.78871143]
 [414.14035417   0.49070001   0.25110003   0.31560001   0.45240003
    1.7187283 ]][0m
[37m[1m[2023-07-17 15:01:48,154][257371] Max Reward on eval: 565.4939956992864[0m
[37m[1m[2023-07-17 15:01:48,154][257371] Min Reward on eval: 198.92525543533264[0m
[37m[1m[2023-07-17 15:01:48,155][257371] Mean Reward across all agents: 405.4738626050254[0m
[37m[1m[2023-07-17 15:01:48,155][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:01:48,157][257371] mean_value=-1067.464249097842, max_value=177.62237858777445[0m
[37m[1m[2023-07-17 15:01:48,160][257371] New mean coefficients: [[ 0.7981783  -0.7460601  -1.9097095  -0.24293107  3.559885   -1.0747705 ]][0m
[37m[1m[2023-07-17 15:01:48,161][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:01:57,179][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 15:01:57,179][257371] FPS: 425894.22[0m
[36m[2023-07-17 15:01:57,181][257371] itr=1381, itrs=2000, Progress: 69.05%[0m
[36m[2023-07-17 15:02:08,895][257371] train() took 11.60 seconds to complete[0m
[36m[2023-07-17 15:02:08,895][257371] FPS: 331084.58[0m
[36m[2023-07-17 15:02:13,110][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:02:13,111][257371] Reward + Measures: [[459.39236345   0.49570164   0.24752767   0.31619233   0.46595466
    1.67556882]][0m
[37m[1m[2023-07-17 15:02:13,111][257371] Max Reward on eval: 459.3923634524244[0m
[37m[1m[2023-07-17 15:02:13,111][257371] Min Reward on eval: 459.3923634524244[0m
[37m[1m[2023-07-17 15:02:13,111][257371] Mean Reward across all agents: 459.3923634524244[0m
[37m[1m[2023-07-17 15:02:13,112][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:02:18,046][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:02:18,047][257371] Reward + Measures: [[300.76059436   0.39669999   0.32640001   0.27990001   0.41490003
    2.05875826]
 [462.02297024   0.42460003   0.22049999   0.28860003   0.42070004
    1.75210416]
 [375.65830234   0.46720001   0.26470003   0.3515       0.41659999
    1.91389453]
 ...
 [453.71264364   0.49650002   0.23600002   0.32550001   0.47609997
    1.68321884]
 [384.76097494   0.4851       0.244        0.31560001   0.43630001
    1.79730475]
 [350.90891129   0.35310003   0.26819998   0.25049999   0.33830002
    2.15866637]][0m
[37m[1m[2023-07-17 15:02:18,047][257371] Max Reward on eval: 643.1770286097192[0m
[37m[1m[2023-07-17 15:02:18,047][257371] Min Reward on eval: 236.08903060033919[0m
[37m[1m[2023-07-17 15:02:18,048][257371] Mean Reward across all agents: 440.2120518658512[0m
[37m[1m[2023-07-17 15:02:18,048][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:02:18,050][257371] mean_value=-1364.5504792989411, max_value=172.6032477359944[0m
[37m[1m[2023-07-17 15:02:18,053][257371] New mean coefficients: [[ 1.0612786  -0.05445528 -2.230027   -2.1507084   3.0648365  -0.7342751 ]][0m
[37m[1m[2023-07-17 15:02:18,054][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:02:27,075][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 15:02:27,075][257371] FPS: 425750.63[0m
[36m[2023-07-17 15:02:27,077][257371] itr=1382, itrs=2000, Progress: 69.10%[0m
[36m[2023-07-17 15:02:38,892][257371] train() took 11.70 seconds to complete[0m
[36m[2023-07-17 15:02:38,892][257371] FPS: 328200.51[0m
[36m[2023-07-17 15:02:43,245][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:02:43,246][257371] Reward + Measures: [[506.07346539   0.4944663    0.24297132   0.30755964   0.48163265
    1.64681184]][0m
[37m[1m[2023-07-17 15:02:43,246][257371] Max Reward on eval: 506.07346538980784[0m
[37m[1m[2023-07-17 15:02:43,246][257371] Min Reward on eval: 506.07346538980784[0m
[37m[1m[2023-07-17 15:02:43,246][257371] Mean Reward across all agents: 506.07346538980784[0m
[37m[1m[2023-07-17 15:02:43,247][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:02:48,223][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:02:48,224][257371] Reward + Measures: [[369.64983512   0.41080004   0.27220002   0.27089998   0.40760002
    1.98962677]
 [547.71404647   0.51160002   0.25920001   0.3012       0.48769999
    1.74208105]
 [375.91015237   0.39990002   0.21069999   0.2577       0.40330002
    1.88760757]
 ...
 [389.83449026   0.45790002   0.26359999   0.28130001   0.42140004
    1.9351784 ]
 [395.39395883   0.4197       0.2509       0.3098       0.41580001
    1.89001203]
 [340.31105066   0.42269999   0.23120001   0.28509998   0.40150005
    1.97376406]][0m
[37m[1m[2023-07-17 15:02:48,224][257371] Max Reward on eval: 626.7317009062274[0m
[37m[1m[2023-07-17 15:02:48,224][257371] Min Reward on eval: 157.71275266967714[0m
[37m[1m[2023-07-17 15:02:48,224][257371] Mean Reward across all agents: 421.7185920480714[0m
[37m[1m[2023-07-17 15:02:48,225][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:02:48,226][257371] mean_value=-1419.7040649104288, max_value=55.034867505767465[0m
[37m[1m[2023-07-17 15:02:48,229][257371] New mean coefficients: [[ 1.9546978  -0.20123951 -1.0897161  -0.6836165   2.6484962  -1.39462   ]][0m
[37m[1m[2023-07-17 15:02:48,229][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:02:57,234][257371] train() took 9.00 seconds to complete[0m
[36m[2023-07-17 15:02:57,240][257371] FPS: 426516.60[0m
[36m[2023-07-17 15:02:57,242][257371] itr=1383, itrs=2000, Progress: 69.15%[0m
[36m[2023-07-17 15:03:09,165][257371] train() took 11.81 seconds to complete[0m
[36m[2023-07-17 15:03:09,166][257371] FPS: 325196.32[0m
[36m[2023-07-17 15:03:13,524][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:03:13,524][257371] Reward + Measures: [[533.54136933   0.49345434   0.24298331   0.29710197   0.49364999
    1.60894334]][0m
[37m[1m[2023-07-17 15:03:13,525][257371] Max Reward on eval: 533.5413693256145[0m
[37m[1m[2023-07-17 15:03:13,525][257371] Min Reward on eval: 533.5413693256145[0m
[37m[1m[2023-07-17 15:03:13,525][257371] Mean Reward across all agents: 533.5413693256145[0m
[37m[1m[2023-07-17 15:03:13,525][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:03:18,575][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:03:18,581][257371] Reward + Measures: [[485.43164065   0.49320003   0.25620002   0.31470001   0.48989996
    1.70333028]
 [485.05545172   0.47420001   0.24320002   0.27560002   0.45440003
    1.67417967]
 [497.61884479   0.43250003   0.23040001   0.28620002   0.442
    1.79463732]
 ...
 [567.90793227   0.52019995   0.2615       0.30679998   0.51710004
    1.61054099]
 [603.06797791   0.50510001   0.26630002   0.28569999   0.49980003
    1.60257852]
 [663.89275357   0.4822       0.24679999   0.26650003   0.48719999
    1.66569364]][0m
[37m[1m[2023-07-17 15:03:18,581][257371] Max Reward on eval: 698.6867217880674[0m
[37m[1m[2023-07-17 15:03:18,581][257371] Min Reward on eval: 303.22707039220256[0m
[37m[1m[2023-07-17 15:03:18,581][257371] Mean Reward across all agents: 516.4939181125576[0m
[37m[1m[2023-07-17 15:03:18,582][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:03:18,585][257371] mean_value=-402.7288188029355, max_value=158.02254640417027[0m
[37m[1m[2023-07-17 15:03:18,588][257371] New mean coefficients: [[ 3.587041   -2.0642927  -0.2905147  -0.43752778  2.1761627  -1.9018939 ]][0m
[37m[1m[2023-07-17 15:03:18,589][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:03:27,672][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 15:03:27,673][257371] FPS: 422801.20[0m
[36m[2023-07-17 15:03:27,675][257371] itr=1384, itrs=2000, Progress: 69.20%[0m
[36m[2023-07-17 15:03:39,564][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 15:03:39,564][257371] FPS: 326149.02[0m
[36m[2023-07-17 15:03:43,901][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:03:43,902][257371] Reward + Measures: [[580.90383688   0.48927498   0.24803434   0.29042801   0.51000798
    1.58133936]][0m
[37m[1m[2023-07-17 15:03:43,902][257371] Max Reward on eval: 580.9038368781927[0m
[37m[1m[2023-07-17 15:03:43,902][257371] Min Reward on eval: 580.9038368781927[0m
[37m[1m[2023-07-17 15:03:43,902][257371] Mean Reward across all agents: 580.9038368781927[0m
[37m[1m[2023-07-17 15:03:43,903][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:03:48,961][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:03:48,962][257371] Reward + Measures: [[522.30075451   0.43490002   0.26980001   0.29260001   0.54820007
    1.73863304]
 [642.6370964    0.44230005   0.2678       0.26320001   0.53220004
    1.69240224]
 [614.73075103   0.43899998   0.27560002   0.2705       0.53290004
    1.69418812]
 ...
 [538.08120753   0.39160001   0.2277       0.22860001   0.4179
    1.66278744]
 [647.02438354   0.47480002   0.26690003   0.27379999   0.53459996
    1.61349452]
 [403.88096734   0.3714       0.24059999   0.21710001   0.42940003
    1.70160162]][0m
[37m[1m[2023-07-17 15:03:48,962][257371] Max Reward on eval: 704.1718635299709[0m
[37m[1m[2023-07-17 15:03:48,962][257371] Min Reward on eval: 288.46467794007623[0m
[37m[1m[2023-07-17 15:03:48,962][257371] Mean Reward across all agents: 553.6600770766516[0m
[37m[1m[2023-07-17 15:03:48,963][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:03:48,966][257371] mean_value=-485.1118704738383, max_value=161.54557280108287[0m
[37m[1m[2023-07-17 15:03:48,969][257371] New mean coefficients: [[ 4.6369967 -2.2398844 -0.3107061 -0.8679434  1.9959676 -2.3837404]][0m
[37m[1m[2023-07-17 15:03:48,970][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:03:58,056][257371] train() took 9.08 seconds to complete[0m
[36m[2023-07-17 15:03:58,056][257371] FPS: 422693.79[0m
[36m[2023-07-17 15:03:58,059][257371] itr=1385, itrs=2000, Progress: 69.25%[0m
[36m[2023-07-17 15:04:10,020][257371] train() took 11.84 seconds to complete[0m
[36m[2023-07-17 15:04:10,021][257371] FPS: 324227.09[0m
[36m[2023-07-17 15:04:14,317][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:04:14,317][257371] Reward + Measures: [[630.32355779   0.47712734   0.24475932   0.27787367   0.50838804
    1.56102812]][0m
[37m[1m[2023-07-17 15:04:14,318][257371] Max Reward on eval: 630.3235577930905[0m
[37m[1m[2023-07-17 15:04:14,318][257371] Min Reward on eval: 630.3235577930905[0m
[37m[1m[2023-07-17 15:04:14,318][257371] Mean Reward across all agents: 630.3235577930905[0m
[37m[1m[2023-07-17 15:04:14,318][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:04:19,300][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:04:19,301][257371] Reward + Measures: [[628.1771355    0.48410001   0.25650001   0.28370002   0.51550001
    1.59693992]
 [443.23341731   0.31620002   0.19670001   0.19520001   0.34690002
    1.90433407]
 [571.56688648   0.49939999   0.22579999   0.26550004   0.46230003
    1.56592643]
 ...
 [690.64008333   0.47760001   0.26120001   0.27070004   0.48280001
    1.63351238]
 [503.1272012    0.40549999   0.23559999   0.26200002   0.38249999
    1.8725512 ]
 [566.18423845   0.51820004   0.28560001   0.30469999   0.55360001
    1.57564235]][0m
[37m[1m[2023-07-17 15:04:19,301][257371] Max Reward on eval: 722.0331954602152[0m
[37m[1m[2023-07-17 15:04:19,301][257371] Min Reward on eval: 364.35679004564884[0m
[37m[1m[2023-07-17 15:04:19,302][257371] Mean Reward across all agents: 584.303226430472[0m
[37m[1m[2023-07-17 15:04:19,302][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:04:19,305][257371] mean_value=-1099.3183879800022, max_value=199.89545294215674[0m
[37m[1m[2023-07-17 15:04:19,308][257371] New mean coefficients: [[ 5.6263843  -1.9185692  -0.02711138  0.09132963  1.8043576  -1.4788682 ]][0m
[37m[1m[2023-07-17 15:04:19,309][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:04:28,273][257371] train() took 8.96 seconds to complete[0m
[36m[2023-07-17 15:04:28,274][257371] FPS: 428442.81[0m
[36m[2023-07-17 15:04:28,276][257371] itr=1386, itrs=2000, Progress: 69.30%[0m
[36m[2023-07-17 15:04:39,983][257371] train() took 11.59 seconds to complete[0m
[36m[2023-07-17 15:04:39,984][257371] FPS: 331209.78[0m
[36m[2023-07-17 15:04:44,245][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:04:44,245][257371] Reward + Measures: [[683.45071366   0.46397534   0.23924898   0.27071968   0.49683633
    1.57050085]][0m
[37m[1m[2023-07-17 15:04:44,245][257371] Max Reward on eval: 683.4507136599354[0m
[37m[1m[2023-07-17 15:04:44,246][257371] Min Reward on eval: 683.4507136599354[0m
[37m[1m[2023-07-17 15:04:44,246][257371] Mean Reward across all agents: 683.4507136599354[0m
[37m[1m[2023-07-17 15:04:44,246][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:04:49,502][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:04:49,502][257371] Reward + Measures: [[713.37093344   0.46849999   0.25909999   0.27869999   0.50690001
    1.69011247]
 [576.91677381   0.44729996   0.29170001   0.28739998   0.45280004
    1.75175345]
 [342.83102475   0.29300001   0.18449999   0.20050001   0.33930001
    1.64780128]
 ...
 [548.63279246   0.43650004   0.24949999   0.27770001   0.48330003
    1.63725555]
 [507.15821456   0.51020002   0.28849998   0.37550002   0.46669999
    1.59307134]
 [614.42381287   0.44109997   0.27970001   0.29010001   0.53839999
    1.72457302]][0m
[37m[1m[2023-07-17 15:04:49,503][257371] Max Reward on eval: 777.9441833293065[0m
[37m[1m[2023-07-17 15:04:49,503][257371] Min Reward on eval: 325.22817489877343[0m
[37m[1m[2023-07-17 15:04:49,503][257371] Mean Reward across all agents: 587.4352719527782[0m
[37m[1m[2023-07-17 15:04:49,503][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:04:49,506][257371] mean_value=-749.0728286661964, max_value=188.6062180371323[0m
[37m[1m[2023-07-17 15:04:49,509][257371] New mean coefficients: [[ 4.1614203  -1.4084458  -0.62551606 -0.0121992   1.8165345  -0.2810992 ]][0m
[37m[1m[2023-07-17 15:04:49,510][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:04:58,527][257371] train() took 9.02 seconds to complete[0m
[36m[2023-07-17 15:04:58,528][257371] FPS: 425937.88[0m
[36m[2023-07-17 15:04:58,530][257371] itr=1387, itrs=2000, Progress: 69.35%[0m
[36m[2023-07-17 15:05:10,428][257371] train() took 11.78 seconds to complete[0m
[36m[2023-07-17 15:05:10,429][257371] FPS: 325853.31[0m
[36m[2023-07-17 15:05:14,751][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:05:14,751][257371] Reward + Measures: [[721.51778069   0.46281999   0.23415568   0.26804835   0.49462962
    1.57551122]][0m
[37m[1m[2023-07-17 15:05:14,751][257371] Max Reward on eval: 721.5177806911897[0m
[37m[1m[2023-07-17 15:05:14,751][257371] Min Reward on eval: 721.5177806911897[0m
[37m[1m[2023-07-17 15:05:14,752][257371] Mean Reward across all agents: 721.5177806911897[0m
[37m[1m[2023-07-17 15:05:14,752][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:05:19,724][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:05:19,725][257371] Reward + Measures: [[790.80989838   0.45269999   0.2361       0.26880002   0.51449996
    1.66089559]
 [656.56597138   0.43350002   0.24820001   0.2719       0.42570001
    1.83645082]
 [709.54759976   0.50200003   0.235        0.28690001   0.48380002
    1.59298837]
 ...
 [748.90679171   0.46250007   0.22290002   0.2879       0.42820001
    1.71006811]
 [462.51614916   0.34010002   0.2177       0.2131       0.40050003
    1.88783574]
 [786.15663145   0.40770003   0.24180003   0.24420002   0.48430005
    1.72809923]][0m
[37m[1m[2023-07-17 15:05:19,725][257371] Max Reward on eval: 829.9980468760244[0m
[37m[1m[2023-07-17 15:05:19,725][257371] Min Reward on eval: 343.17792799149174[0m
[37m[1m[2023-07-17 15:05:19,726][257371] Mean Reward across all agents: 651.6329008928438[0m
[37m[1m[2023-07-17 15:05:19,726][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:05:19,730][257371] mean_value=-851.0245762047775, max_value=128.23284561044636[0m
[37m[1m[2023-07-17 15:05:19,732][257371] New mean coefficients: [[ 2.854838   -0.9557196  -0.8031621  -0.42157608  2.3943436  -0.38716486]][0m
[37m[1m[2023-07-17 15:05:19,733][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:05:28,707][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 15:05:28,707][257371] FPS: 428009.98[0m
[36m[2023-07-17 15:05:28,709][257371] itr=1388, itrs=2000, Progress: 69.40%[0m
[36m[2023-07-17 15:05:40,548][257371] train() took 11.72 seconds to complete[0m
[36m[2023-07-17 15:05:40,548][257371] FPS: 327622.94[0m
[36m[2023-07-17 15:05:44,882][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:05:44,883][257371] Reward + Measures: [[759.76726643   0.45008364   0.23361467   0.26132864   0.49883834
    1.58538055]][0m
[37m[1m[2023-07-17 15:05:44,883][257371] Max Reward on eval: 759.7672664336571[0m
[37m[1m[2023-07-17 15:05:44,883][257371] Min Reward on eval: 759.7672664336571[0m
[37m[1m[2023-07-17 15:05:44,884][257371] Mean Reward across all agents: 759.7672664336571[0m
[37m[1m[2023-07-17 15:05:44,884][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:05:49,916][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:05:49,917][257371] Reward + Measures: [[561.22318652   0.33900002   0.24950002   0.2282       0.41659999
    1.85314357]
 [680.64922335   0.40290004   0.25139999   0.26550004   0.52260005
    1.81169975]
 [601.67884544   0.34970003   0.25150001   0.23080002   0.42020002
    1.9589684 ]
 ...
 [768.27137751   0.44689998   0.21690002   0.29359999   0.45380002
    1.62335718]
 [780.10645201   0.41249999   0.20799999   0.25820002   0.44590002
    1.72783506]
 [509.93504557   0.34960005   0.24269998   0.24860001   0.44179997
    1.99870837]][0m
[37m[1m[2023-07-17 15:05:49,917][257371] Max Reward on eval: 823.8934554746841[0m
[37m[1m[2023-07-17 15:05:49,918][257371] Min Reward on eval: 251.74488322976975[0m
[37m[1m[2023-07-17 15:05:49,918][257371] Mean Reward across all agents: 633.2000793179103[0m
[37m[1m[2023-07-17 15:05:49,918][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:05:49,920][257371] mean_value=-813.0416121804153, max_value=87.21452622881282[0m
[37m[1m[2023-07-17 15:05:49,923][257371] New mean coefficients: [[ 2.4889977  -1.0450929   0.6050575  -0.34523463  1.2550579   0.00217071]][0m
[37m[1m[2023-07-17 15:05:49,924][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:05:58,899][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 15:05:58,899][257371] FPS: 427932.06[0m
[36m[2023-07-17 15:05:58,901][257371] itr=1389, itrs=2000, Progress: 69.45%[0m
[36m[2023-07-17 15:06:11,080][257371] train() took 12.06 seconds to complete[0m
[36m[2023-07-17 15:06:11,080][257371] FPS: 318312.34[0m
[36m[2023-07-17 15:06:15,488][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:06:15,488][257371] Reward + Measures: [[795.16758343   0.44707966   0.23097865   0.25810966   0.50241065
    1.59765494]][0m
[37m[1m[2023-07-17 15:06:15,488][257371] Max Reward on eval: 795.1675834285425[0m
[37m[1m[2023-07-17 15:06:15,489][257371] Min Reward on eval: 795.1675834285425[0m
[37m[1m[2023-07-17 15:06:15,489][257371] Mean Reward across all agents: 795.1675834285425[0m
[37m[1m[2023-07-17 15:06:15,489][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:06:20,585][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:06:20,585][257371] Reward + Measures: [[553.95424127   0.35890004   0.18359999   0.2225       0.38270003
    1.56070459]
 [605.25679017   0.41220003   0.2494       0.29899999   0.48929998
    1.8256768 ]
 [582.77330779   0.42899999   0.28870001   0.2784       0.50350004
    1.85166895]
 ...
 [635.78277395   0.33740002   0.22000001   0.22329998   0.42950001
    1.66826499]
 [415.96665596   0.33140001   0.199        0.24010001   0.36829999
    1.76760387]
 [678.93682099   0.45559999   0.26479998   0.28860003   0.56069994
    1.6259861 ]][0m
[37m[1m[2023-07-17 15:06:20,586][257371] Max Reward on eval: 861.043762221653[0m
[37m[1m[2023-07-17 15:06:20,586][257371] Min Reward on eval: 351.38811865099126[0m
[37m[1m[2023-07-17 15:06:20,586][257371] Mean Reward across all agents: 670.1166987060218[0m
[37m[1m[2023-07-17 15:06:20,586][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:06:20,589][257371] mean_value=-572.2966158766299, max_value=137.78428193089746[0m
[37m[1m[2023-07-17 15:06:20,591][257371] New mean coefficients: [[ 1.7002864  -0.10296047 -0.8915596  -0.9018249   0.5856836   0.41210678]][0m
[37m[1m[2023-07-17 15:06:20,592][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:06:29,719][257371] train() took 9.12 seconds to complete[0m
[36m[2023-07-17 15:06:29,719][257371] FPS: 420839.65[0m
[36m[2023-07-17 15:06:29,721][257371] itr=1390, itrs=2000, Progress: 69.50%[0m
[37m[1m[2023-07-17 15:10:03,300][257371] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001370[0m
[36m[2023-07-17 15:10:15,501][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 15:10:15,501][257371] FPS: 330317.08[0m
[36m[2023-07-17 15:10:19,830][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:10:19,830][257371] Reward + Measures: [[826.02427723   0.44156131   0.22668333   0.25127798   0.501284
    1.62459433]][0m
[37m[1m[2023-07-17 15:10:19,831][257371] Max Reward on eval: 826.024277228118[0m
[37m[1m[2023-07-17 15:10:19,831][257371] Min Reward on eval: 826.024277228118[0m
[37m[1m[2023-07-17 15:10:19,831][257371] Mean Reward across all agents: 826.024277228118[0m
[37m[1m[2023-07-17 15:10:19,831][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:10:25,101][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:10:25,101][257371] Reward + Measures: [[771.88089374   0.43490002   0.2836       0.24520002   0.48289999
    1.80085838]
 [717.29458239   0.45549998   0.2911       0.2447       0.53540003
    1.77893531]
 [718.38491126   0.41360003   0.2184       0.2414       0.42209998
    1.68566442]
 ...
 [772.29265595   0.49380001   0.23330002   0.28029999   0.54229999
    1.60394371]
 [789.64423374   0.49100003   0.2368       0.27580002   0.5618
    1.5555656 ]
 [642.63037722   0.44210002   0.21140002   0.2615       0.45320001
    1.73449254]][0m
[37m[1m[2023-07-17 15:10:25,101][257371] Max Reward on eval: 922.5732650745892[0m
[37m[1m[2023-07-17 15:10:25,102][257371] Min Reward on eval: 361.63862684001214[0m
[37m[1m[2023-07-17 15:10:25,102][257371] Mean Reward across all agents: 723.0636538564804[0m
[37m[1m[2023-07-17 15:10:25,102][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:10:25,114][257371] mean_value=-752.1758555631784, max_value=133.20213801540024[0m
[37m[1m[2023-07-17 15:10:25,121][257371] New mean coefficients: [[ 1.1471584  -0.68789643 -0.5509207  -0.45655936  0.23902956  0.5019876 ]][0m
[37m[1m[2023-07-17 15:10:25,123][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:10:34,218][257371] train() took 9.09 seconds to complete[0m
[36m[2023-07-17 15:10:34,263][257371] FPS: 422306.01[0m
[36m[2023-07-17 15:10:34,266][257371] itr=1391, itrs=2000, Progress: 69.55%[0m
[36m[2023-07-17 15:10:46,369][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 15:10:46,369][257371] FPS: 326683.23[0m
[36m[2023-07-17 15:10:50,749][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:10:50,749][257371] Reward + Measures: [[863.53220094   0.42613566   0.22508101   0.24249968   0.49138168
    1.66245651]][0m
[37m[1m[2023-07-17 15:10:50,749][257371] Max Reward on eval: 863.532200942666[0m
[37m[1m[2023-07-17 15:10:50,750][257371] Min Reward on eval: 863.532200942666[0m
[37m[1m[2023-07-17 15:10:50,750][257371] Mean Reward across all agents: 863.532200942666[0m
[37m[1m[2023-07-17 15:10:50,750][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:10:55,824][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:10:55,824][257371] Reward + Measures: [[803.21212774   0.46520001   0.25120002   0.27920005   0.49260002
    1.85903513]
 [693.46764756   0.52209997   0.23270002   0.32740003   0.48839998
    1.51407456]
 [853.49568936   0.39470002   0.2314       0.23559999   0.46059999
    1.88431382]
 ...
 [912.98587797   0.43529996   0.2428       0.2395       0.546
    1.80034065]
 [741.82474173   0.36339998   0.2052       0.2086       0.42350003
    1.80038893]
 [669.40723851   0.33269998   0.22049999   0.19840001   0.40439996
    1.83383715]][0m
[37m[1m[2023-07-17 15:10:55,824][257371] Max Reward on eval: 961.2617187313735[0m
[37m[1m[2023-07-17 15:10:55,825][257371] Min Reward on eval: 506.37789575643836[0m
[37m[1m[2023-07-17 15:10:55,825][257371] Mean Reward across all agents: 799.5535024414578[0m
[37m[1m[2023-07-17 15:10:55,825][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:10:55,829][257371] mean_value=-277.8456466972176, max_value=142.38874129127714[0m
[37m[1m[2023-07-17 15:10:55,831][257371] New mean coefficients: [[ 1.2035192 -0.8407184 -1.5051011 -0.6191344  0.6475303  0.7601624]][0m
[37m[1m[2023-07-17 15:10:55,832][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:11:04,848][257371] train() took 9.01 seconds to complete[0m
[36m[2023-07-17 15:11:04,848][257371] FPS: 425995.17[0m
[36m[2023-07-17 15:11:04,850][257371] itr=1392, itrs=2000, Progress: 69.60%[0m
[36m[2023-07-17 15:11:16,735][257371] train() took 11.77 seconds to complete[0m
[36m[2023-07-17 15:11:16,736][257371] FPS: 326214.04[0m
[36m[2023-07-17 15:11:20,853][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:11:20,859][257371] Reward + Measures: [[883.10559003   0.412258     0.22000565   0.23556267   0.48731136
    1.70432353]][0m
[37m[1m[2023-07-17 15:11:20,859][257371] Max Reward on eval: 883.1055900271358[0m
[37m[1m[2023-07-17 15:11:20,860][257371] Min Reward on eval: 883.1055900271358[0m
[37m[1m[2023-07-17 15:11:20,860][257371] Mean Reward across all agents: 883.1055900271358[0m
[37m[1m[2023-07-17 15:11:20,860][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:11:25,834][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:11:25,835][257371] Reward + Measures: [[711.68876386   0.35100001   0.1937       0.2235       0.40400001
    1.70984042]
 [701.98022185   0.3427       0.17820001   0.21959999   0.40130001
    1.70813274]
 [937.81782531   0.42290002   0.25209999   0.2412       0.50490004
    1.78442562]
 ...
 [888.4330177    0.38610002   0.23099999   0.21470001   0.45580003
    1.78636491]
 [836.73908236   0.41         0.22850001   0.22859998   0.47670004
    1.73514295]
 [867.64821244   0.40640002   0.2244       0.2404       0.47760001
    1.69762158]][0m
[37m[1m[2023-07-17 15:11:25,835][257371] Max Reward on eval: 979.4311981149018[0m
[37m[1m[2023-07-17 15:11:25,835][257371] Min Reward on eval: 532.1499896381516[0m
[37m[1m[2023-07-17 15:11:25,835][257371] Mean Reward across all agents: 802.0981113286875[0m
[37m[1m[2023-07-17 15:11:25,836][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:11:25,838][257371] mean_value=-500.9955885995187, max_value=104.73307022565984[0m
[37m[1m[2023-07-17 15:11:25,841][257371] New mean coefficients: [[ 1.49684    -0.9283422  -0.8546352  -0.75992197 -0.01934624  0.2877587 ]][0m
[37m[1m[2023-07-17 15:11:25,842][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:11:35,018][257371] train() took 9.17 seconds to complete[0m
[36m[2023-07-17 15:11:35,019][257371] FPS: 418543.39[0m
[36m[2023-07-17 15:11:35,021][257371] itr=1393, itrs=2000, Progress: 69.65%[0m
[36m[2023-07-17 15:11:46,930][257371] train() took 11.79 seconds to complete[0m
[36m[2023-07-17 15:11:46,930][257371] FPS: 325601.11[0m
[36m[2023-07-17 15:11:51,216][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:11:51,216][257371] Reward + Measures: [[916.65282767   0.39268464   0.22051667   0.22856966   0.48070565
    1.73224342]][0m
[37m[1m[2023-07-17 15:11:51,216][257371] Max Reward on eval: 916.6528276693756[0m
[37m[1m[2023-07-17 15:11:51,217][257371] Min Reward on eval: 916.6528276693756[0m
[37m[1m[2023-07-17 15:11:51,217][257371] Mean Reward across all agents: 916.6528276693756[0m
[37m[1m[2023-07-17 15:11:51,217][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:11:56,261][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:11:56,261][257371] Reward + Measures: [[ 803.64259339    0.37970003    0.21610001    0.2429        0.46890002
     1.74042475]
 [ 730.38589493    0.35090002    0.18499999    0.21390001    0.41220003
     1.76233351]
 [1007.83055115    0.40010005    0.2352        0.21510001    0.45860001
     1.7986027 ]
 ...
 [ 866.33093616    0.36400002    0.23609999    0.17649999    0.44299999
     1.8502686 ]
 [ 816.7899513     0.39910001    0.22719999    0.25170001    0.47049999
     1.75039136]
 [1009.98612214    0.40540001    0.24420002    0.21689999    0.47880003
     1.771963  ]][0m
[37m[1m[2023-07-17 15:11:56,261][257371] Max Reward on eval: 1046.2216262714937[0m
[37m[1m[2023-07-17 15:11:56,262][257371] Min Reward on eval: 566.6704400066286[0m
[37m[1m[2023-07-17 15:11:56,262][257371] Mean Reward across all agents: 859.4590666855822[0m
[37m[1m[2023-07-17 15:11:56,262][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:11:56,265][257371] mean_value=-600.6273054079744, max_value=129.43092129122533[0m
[37m[1m[2023-07-17 15:11:56,268][257371] New mean coefficients: [[ 2.0085368  -0.79259336 -0.87102336 -0.97284424 -0.00952545 -0.45175713]][0m
[37m[1m[2023-07-17 15:11:56,269][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:12:05,324][257371] train() took 9.05 seconds to complete[0m
[36m[2023-07-17 15:12:05,324][257371] FPS: 424163.90[0m
[36m[2023-07-17 15:12:05,326][257371] itr=1394, itrs=2000, Progress: 69.70%[0m
[36m[2023-07-17 15:12:16,998][257371] train() took 11.56 seconds to complete[0m
[36m[2023-07-17 15:12:16,999][257371] FPS: 332178.97[0m
[36m[2023-07-17 15:12:21,331][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:12:21,331][257371] Reward + Measures: [[941.93747138   0.38668433   0.21889266   0.22120133   0.47628963
    1.72566605]][0m
[37m[1m[2023-07-17 15:12:21,331][257371] Max Reward on eval: 941.9374713835757[0m
[37m[1m[2023-07-17 15:12:21,332][257371] Min Reward on eval: 941.9374713835757[0m
[37m[1m[2023-07-17 15:12:21,332][257371] Mean Reward across all agents: 941.9374713835757[0m
[37m[1m[2023-07-17 15:12:21,332][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:12:26,306][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:12:26,307][257371] Reward + Measures: [[916.85917667   0.44259998   0.2185       0.25050002   0.50310004
    1.85980189]
 [537.42452619   0.41999999   0.27910003   0.2775       0.44510004
    2.22870541]
 [813.47753677   0.3845       0.21180001   0.23190001   0.45170003
    1.82729423]
 ...
 [699.90205093   0.37469998   0.19990002   0.21360002   0.42930004
    1.91358602]
 [787.20845413   0.37919998   0.2343       0.22950001   0.50190008
    1.8245436 ]
 [841.20062206   0.34170005   0.19710001   0.21010001   0.42539999
    1.67145622]][0m
[37m[1m[2023-07-17 15:12:26,307][257371] Max Reward on eval: 1020.6307678293437[0m
[37m[1m[2023-07-17 15:12:26,307][257371] Min Reward on eval: 268.4380118731409[0m
[37m[1m[2023-07-17 15:12:26,308][257371] Mean Reward across all agents: 827.6627350445331[0m
[37m[1m[2023-07-17 15:12:26,308][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:12:26,310][257371] mean_value=-594.9522950413813, max_value=182.0250769066073[0m
[37m[1m[2023-07-17 15:12:26,312][257371] New mean coefficients: [[ 1.9921645  -0.49940628 -0.8973981  -0.71120393 -0.71170694  0.04496434]][0m
[37m[1m[2023-07-17 15:12:26,313][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:12:35,385][257371] train() took 9.07 seconds to complete[0m
[36m[2023-07-17 15:12:35,385][257371] FPS: 423383.96[0m
[36m[2023-07-17 15:12:35,387][257371] itr=1395, itrs=2000, Progress: 69.75%[0m
[36m[2023-07-17 15:12:47,168][257371] train() took 11.67 seconds to complete[0m
[36m[2023-07-17 15:12:47,168][257371] FPS: 329126.29[0m
[36m[2023-07-17 15:12:51,539][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:12:51,540][257371] Reward + Measures: [[972.41212821   0.3809523    0.21921568   0.21738733   0.47464433
    1.7354002 ]][0m
[37m[1m[2023-07-17 15:12:51,540][257371] Max Reward on eval: 972.412128212008[0m
[37m[1m[2023-07-17 15:12:51,540][257371] Min Reward on eval: 972.412128212008[0m
[37m[1m[2023-07-17 15:12:51,540][257371] Mean Reward across all agents: 972.412128212008[0m
[37m[1m[2023-07-17 15:12:51,540][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:12:56,465][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:12:56,465][257371] Reward + Measures: [[ 871.25883242    0.35880002    0.22390001    0.22649999    0.42149997
     1.79432452]
 [ 793.39122012    0.44010001    0.2273        0.26659998    0.49090001
     1.72389281]
 [ 806.04920172    0.39910004    0.21070002    0.21959999    0.5086
     1.77141118]
 ...
 [1040.24815369    0.41409999    0.22909999    0.22490001    0.5268001
     1.74418008]
 [ 918.88278388    0.36160001    0.2034        0.21170001    0.479
     1.76984632]
 [ 905.87424849    0.39449999    0.2349        0.23009999    0.47310001
     1.81536031]][0m
[37m[1m[2023-07-17 15:12:56,466][257371] Max Reward on eval: 1074.6516799902543[0m
[37m[1m[2023-07-17 15:12:56,466][257371] Min Reward on eval: 541.7001610371051[0m
[37m[1m[2023-07-17 15:12:56,466][257371] Mean Reward across all agents: 867.4186346631747[0m
[37m[1m[2023-07-17 15:12:56,466][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:12:56,468][257371] mean_value=-490.2788128390634, max_value=221.18833056294727[0m
[37m[1m[2023-07-17 15:12:56,471][257371] New mean coefficients: [[ 1.3833328  -0.9376719  -0.59601414 -0.65683734 -0.5562837  -0.7673869 ]][0m
[37m[1m[2023-07-17 15:12:56,472][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:13:05,458][257371] train() took 8.98 seconds to complete[0m
[36m[2023-07-17 15:13:05,458][257371] FPS: 427387.54[0m
[36m[2023-07-17 15:13:05,461][257371] itr=1396, itrs=2000, Progress: 69.80%[0m
[36m[2023-07-17 15:13:17,210][257371] train() took 11.63 seconds to complete[0m
[36m[2023-07-17 15:13:17,210][257371] FPS: 330085.88[0m
[36m[2023-07-17 15:13:21,554][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:13:21,555][257371] Reward + Measures: [[1016.00169725    0.37545565    0.218913      0.21550567    0.47318
     1.71990585]][0m
[37m[1m[2023-07-17 15:13:21,555][257371] Max Reward on eval: 1016.0016972458247[0m
[37m[1m[2023-07-17 15:13:21,555][257371] Min Reward on eval: 1016.0016972458247[0m
[37m[1m[2023-07-17 15:13:21,555][257371] Mean Reward across all agents: 1016.0016972458247[0m
[37m[1m[2023-07-17 15:13:21,556][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:13:26,784][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:13:26,785][257371] Reward + Measures: [[970.75947568   0.389        0.2428       0.2168       0.48400003
    1.87872434]
 [746.80962662   0.33049998   0.2059       0.19980001   0.42389998
    1.88619506]
 [963.87593076   0.34850001   0.2405       0.24170001   0.44070002
    1.89311445]
 ...
 [920.2169452    0.3416       0.21139999   0.20509999   0.44650003
    1.81068695]
 [767.95309401   0.29950002   0.1882       0.2016       0.35010004
    1.78982472]
 [939.12860868   0.39750004   0.22280002   0.22449999   0.50349998
    1.77245069]][0m
[37m[1m[2023-07-17 15:13:26,785][257371] Max Reward on eval: 1116.7515946006401[0m
[37m[1m[2023-07-17 15:13:26,785][257371] Min Reward on eval: 544.892530363379[0m
[37m[1m[2023-07-17 15:13:26,785][257371] Mean Reward across all agents: 909.5452705396231[0m
[37m[1m[2023-07-17 15:13:26,786][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:13:26,788][257371] mean_value=-1090.0775272663277, max_value=107.51972841758686[0m
[37m[1m[2023-07-17 15:13:26,791][257371] New mean coefficients: [[ 1.3621689  -0.23167044 -0.84993917 -0.7629045  -0.07619706 -0.9236285 ]][0m
[37m[1m[2023-07-17 15:13:26,792][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:13:35,740][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 15:13:35,740][257371] FPS: 429217.88[0m
[36m[2023-07-17 15:13:35,742][257371] itr=1397, itrs=2000, Progress: 69.85%[0m
[36m[2023-07-17 15:13:47,440][257371] train() took 11.58 seconds to complete[0m
[36m[2023-07-17 15:13:47,440][257371] FPS: 331691.90[0m
[36m[2023-07-17 15:13:51,749][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:13:51,749][257371] Reward + Measures: [[1046.06084286    0.36998233    0.21894966    0.21027799    0.47187936
     1.70558155]][0m
[37m[1m[2023-07-17 15:13:51,750][257371] Max Reward on eval: 1046.0608428574064[0m
[37m[1m[2023-07-17 15:13:51,750][257371] Min Reward on eval: 1046.0608428574064[0m
[37m[1m[2023-07-17 15:13:51,750][257371] Mean Reward across all agents: 1046.0608428574064[0m
[37m[1m[2023-07-17 15:13:51,750][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:13:56,801][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:13:56,801][257371] Reward + Measures: [[ 835.5254109     0.33230001    0.19719999    0.18880001    0.40850002
     1.71130848]
 [ 967.31104087    0.37069997    0.22040001    0.21870001    0.43190002
     1.74911106]
 [ 755.183074      0.3585        0.1953        0.19670001    0.43870002
     1.69255066]
 ...
 [ 972.80779267    0.3845        0.21949999    0.20130001    0.46990004
     1.6925987 ]
 [1059.40140531    0.36489999    0.22049999    0.2059        0.47490001
     1.68360603]
 [1105.93459321    0.38530001    0.2357        0.2053        0.4923
     1.73482001]][0m
[37m[1m[2023-07-17 15:13:56,801][257371] Max Reward on eval: 1194.9647445442156[0m
[37m[1m[2023-07-17 15:13:56,802][257371] Min Reward on eval: 628.4999528967776[0m
[37m[1m[2023-07-17 15:13:56,802][257371] Mean Reward across all agents: 968.9722430888766[0m
[37m[1m[2023-07-17 15:13:56,802][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:13:56,805][257371] mean_value=-528.7390614369423, max_value=159.60233135470082[0m
[37m[1m[2023-07-17 15:13:56,808][257371] New mean coefficients: [[ 1.2674001   0.6109729  -0.5985453  -0.956899   -0.1308766  -0.43769228]][0m
[37m[1m[2023-07-17 15:13:56,809][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:14:05,950][257371] train() took 9.14 seconds to complete[0m
[36m[2023-07-17 15:14:05,950][257371] FPS: 420160.45[0m
[36m[2023-07-17 15:14:05,952][257371] itr=1398, itrs=2000, Progress: 69.90%[0m
[36m[2023-07-17 15:14:17,825][257371] train() took 11.75 seconds to complete[0m
[36m[2023-07-17 15:14:17,825][257371] FPS: 326707.95[0m
[36m[2023-07-17 15:14:22,101][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:14:22,101][257371] Reward + Measures: [[1069.64455268    0.37219432    0.21727067    0.209953      0.47492129
     1.68679786]][0m
[37m[1m[2023-07-17 15:14:22,101][257371] Max Reward on eval: 1069.6445526795121[0m
[37m[1m[2023-07-17 15:14:22,102][257371] Min Reward on eval: 1069.6445526795121[0m
[37m[1m[2023-07-17 15:14:22,102][257371] Mean Reward across all agents: 1069.6445526795121[0m
[37m[1m[2023-07-17 15:14:22,102][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:14:27,086][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:14:27,086][257371] Reward + Measures: [[ 855.23007205    0.39309999    0.24029998    0.27560002    0.48670003
     1.79692614]
 [ 958.72277069    0.38329998    0.2342        0.2595        0.44450003
     1.89629972]
 [ 930.8511343     0.36160001    0.21589999    0.23029999    0.45070001
     1.72606659]
 ...
 [ 896.17709545    0.3089        0.19270001    0.18779999    0.3705
     1.85660958]
 [1000.03829728    0.33489999    0.20939998    0.1665        0.40120003
     1.78419518]
 [ 985.67099285    0.33520001    0.2026        0.1982        0.41780001
     1.79927695]][0m
[37m[1m[2023-07-17 15:14:27,087][257371] Max Reward on eval: 1229.2447814903803[0m
[37m[1m[2023-07-17 15:14:27,087][257371] Min Reward on eval: 488.49191166665406[0m
[37m[1m[2023-07-17 15:14:27,087][257371] Mean Reward across all agents: 921.8094858023416[0m
[37m[1m[2023-07-17 15:14:27,087][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:14:27,089][257371] mean_value=-903.6236828953291, max_value=177.26735059074736[0m
[37m[1m[2023-07-17 15:14:27,092][257371] New mean coefficients: [[ 0.69864386  1.3246121  -0.0874179  -1.0149683  -0.2604056  -0.13092798]][0m
[37m[1m[2023-07-17 15:14:27,093][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:14:36,061][257371] train() took 8.97 seconds to complete[0m
[36m[2023-07-17 15:14:36,061][257371] FPS: 428258.71[0m
[36m[2023-07-17 15:14:36,064][257371] itr=1399, itrs=2000, Progress: 69.95%[0m
[36m[2023-07-17 15:14:48,001][257371] train() took 11.82 seconds to complete[0m
[36m[2023-07-17 15:14:48,001][257371] FPS: 324772.10[0m
[36m[2023-07-17 15:14:52,245][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:14:52,246][257371] Reward + Measures: [[1093.98767676    0.379035      0.21851233    0.20629834    0.47500733
     1.6804446 ]][0m
[37m[1m[2023-07-17 15:14:52,246][257371] Max Reward on eval: 1093.9876767594913[0m
[37m[1m[2023-07-17 15:14:52,246][257371] Min Reward on eval: 1093.9876767594913[0m
[37m[1m[2023-07-17 15:14:52,246][257371] Mean Reward across all agents: 1093.9876767594913[0m
[37m[1m[2023-07-17 15:14:52,246][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:14:56,780][257371] Finished Evaluation Step[0m
[37m[1m[2023-07-17 15:14:56,780][257371] Reward + Measures: [[1074.03924561    0.3888        0.21970001    0.2131        0.48480001
     1.69744074]
 [ 882.84963694    0.42140004    0.19700001    0.23810001    0.48409995
     1.62131691]
 [ 940.82756808    0.41140005    0.2033        0.2422        0.49920002
     1.62927473]
 ...
 [1070.82593535    0.37559998    0.2264        0.21599999    0.4711
     1.73411167]
 [1108.67269136    0.40249997    0.24440001    0.2155        0.51279998
     1.71726096]
 [ 951.34439062    0.39380002    0.2103        0.2208        0.47979999
     1.66464508]][0m
[37m[1m[2023-07-17 15:14:56,781][257371] Max Reward on eval: 1193.4134979734197[0m
[37m[1m[2023-07-17 15:14:56,781][257371] Min Reward on eval: 538.0085633583367[0m
[37m[1m[2023-07-17 15:14:56,781][257371] Mean Reward across all agents: 988.3539290533114[0m
[37m[1m[2023-07-17 15:14:56,782][257371] Average Trajectory Length: 1000.0[0m
[36m[2023-07-17 15:14:56,784][257371] mean_value=-427.4490999637428, max_value=96.0944761494319[0m
[37m[1m[2023-07-17 15:14:56,787][257371] New mean coefficients: [[ 0.7846523   0.8522746  -0.14778681 -1.3147545  -0.5009923  -0.6587231 ]][0m
[37m[1m[2023-07-17 15:14:56,788][257371] Moving the mean solution point...[0m
[36m[2023-07-17 15:15:05,737][257371] train() took 8.95 seconds to complete[0m
[36m[2023-07-17 15:15:05,737][257371] FPS: 429153.18[0m
[36m[2023-07-17 15:15:05,740][257371] itr=1400, itrs=2000, Progress: 70.00%[0m
./runners/local/train_ppga_ant.sh: line 42: 257371 Killed                  python -u -m algorithm.train_ppga --env_name=$ENV_NAME --rollout_length=128 --use_wandb=True --seed=$SEED --wandb_group=qdrl --num_dims=5 --num_minibatches=8 --update_epochs=4 --normalize_obs=True --normalize_returns=True --wandb_run_name=$RUN_NAME --popsize=300 --env_batch_size=3000 --learning_rate=0.001 --vf_coef=2 --max_grad_norm=1 --torch_deterministic=False --total_iterations=2000 --dqd_algorithm=cma_maega --calc_gradient_iters=10 --move_mean_iters=10 --archive_lr=0.1 --restart_rule=no_improvement --sigma0=3.0 --threshold_min=-500 --grid_size=$GRID_SIZE --take_archive_snapshots=True --use_cvt_archive=True --cvt_cells=1000 --cvt_samples=100000 --cvt_use_kd_tree=True --is_energy_measures=True --expdir=./experiments/paper_ppga_"$ENV_NAME"
