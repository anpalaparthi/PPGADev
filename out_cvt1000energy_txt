energy_1000_paper_ppga_ant_seed_1111
wandb: Currently logged in as: anishapv (qdrl). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/icaros/Documents/PPGADev/wandb/run-20230703_004135-mx9hiurg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run energy_1000_paper_ppga_ant_seed_1111
wandb: ‚≠êÔ∏è View project at https://wandb.ai/qdrl/PPGA
wandb: üöÄ View run at https://wandb.ai/qdrl/PPGA/runs/mx9hiurg
[36m[2023-07-03 00:41:38,682][188188] Environment ant, action_dim=8, obs_dim=87[0m
[36m[2023-07-03 00:41:43,211][188188] Created Scheduler for cma_maega with an archive learning rate of 0.1, and add mode batch, using solution dim 28816 and archive dims [7, 7, 7, 7, 7]. Min threshold is -500.0. Restart rule is no_improvement[0m
[36m[2023-07-03 00:42:01,082][188188] train() took 13.90 seconds to complete[0m
[36m[2023-07-03 00:42:01,082][188188] FPS: 276225.97[0m
[36m[2023-07-03 00:42:05,218][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:42:05,219][188188] Reward + Measures: [[0.15953506 0.19033597 0.19046932 0.19079934 0.19051066 2.06406021]][0m
[37m[1m[2023-07-03 00:42:05,219][188188] Max Reward on eval: 0.15953505795116266[0m
[37m[1m[2023-07-03 00:42:05,219][188188] Min Reward on eval: 0.15953505795116266[0m
[37m[1m[2023-07-03 00:42:05,220][188188] Mean Reward across all agents: 0.15953505795116266[0m
[37m[1m[2023-07-03 00:42:05,220][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:42:10,903][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:42:10,904][188188] Reward + Measures: [[-71.04766231   0.24590002   0.20009999   0.2339       0.19679999
    2.25312209]
 [ 43.75833677   0.1525       0.153        0.15530001   0.16110002
    2.18607903]
 [ 44.13899712   0.228        0.21659999   0.23049998   0.226
    2.15890574]
 ...
 [ 35.64034989   0.16440001   0.17870001   0.16590001   0.16660002
    2.12064719]
 [  1.10190596   0.1744       0.17580001   0.17389999   0.1696
    2.15287852]
 [ 69.91767659   0.13689999   0.1397       0.1432       0.1372
    2.21102452]][0m
[37m[1m[2023-07-03 00:42:10,904][188188] Max Reward on eval: 140.23415774982422[0m
[37m[1m[2023-07-03 00:42:10,905][188188] Min Reward on eval: -71.04766231146641[0m
[37m[1m[2023-07-03 00:42:10,905][188188] Mean Reward across all agents: 10.87691528144611[0m
[37m[1m[2023-07-03 00:42:10,905][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:42:10,925][188188] mean_value=468.3633548015203, max_value=613.0481095248833[0m
[37m[1m[2023-07-03 00:42:10,959][188188] New mean coefficients: [[ 2.6749837  -0.5415882  -0.35329807 -1.223468   -1.6744953  -0.6578969 ]][0m
[37m[1m[2023-07-03 00:42:10,960][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:42:19,712][188188] train() took 8.75 seconds to complete[0m
[36m[2023-07-03 00:42:19,712][188188] FPS: 438827.96[0m
[36m[2023-07-03 00:42:19,714][188188] itr=0, itrs=2000, Progress: 0.00%[0m
[36m[2023-07-03 00:42:31,141][188188] train() took 11.41 seconds to complete[0m
[36m[2023-07-03 00:42:31,141][188188] FPS: 336501.57[0m
[36m[2023-07-03 00:42:35,336][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:42:35,336][188188] Reward + Measures: [[145.3270595    0.17726432   0.18277234   0.17729533   0.17002633
    1.9309988 ]][0m
[37m[1m[2023-07-03 00:42:35,336][188188] Max Reward on eval: 145.32705950353284[0m
[37m[1m[2023-07-03 00:42:35,337][188188] Min Reward on eval: 145.32705950353284[0m
[37m[1m[2023-07-03 00:42:35,337][188188] Mean Reward across all agents: 145.32705950353284[0m
[37m[1m[2023-07-03 00:42:35,337][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:42:40,278][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:42:40,279][188188] Reward + Measures: [[ 65.42957775   0.1496       0.1837       0.1349       0.15119998
    2.1024375 ]
 [ 42.36361246   0.086        0.11700001   0.098        0.1085
    2.28055644]
 [ 85.80933225   0.1244       0.11689999   0.11719999   0.11140001
    2.3262856 ]
 ...
 [208.4845643    0.14470001   0.14570001   0.13739999   0.11799999
    2.14740682]
 [ 89.81088822   0.1592       0.14670001   0.1191       0.1285
    2.24519515]
 [ 97.16148553   0.1073       0.1182       0.0956       0.10910001
    2.16570282]][0m
[37m[1m[2023-07-03 00:42:40,279][188188] Max Reward on eval: 296.2781914835796[0m
[37m[1m[2023-07-03 00:42:40,280][188188] Min Reward on eval: -14.363389656692743[0m
[37m[1m[2023-07-03 00:42:40,280][188188] Mean Reward across all agents: 104.23589749819371[0m
[37m[1m[2023-07-03 00:42:40,280][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:42:40,289][188188] mean_value=79.08689121329839, max_value=270.20929252860344[0m
[37m[1m[2023-07-03 00:42:40,292][188188] New mean coefficients: [[ 4.9211817  -0.93608123  0.2842419  -0.01986301 -0.8909158  -0.20846856]][0m
[37m[1m[2023-07-03 00:42:40,293][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:42:49,247][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 00:42:49,247][188188] FPS: 428957.00[0m
[36m[2023-07-03 00:42:49,249][188188] itr=1, itrs=2000, Progress: 0.05%[0m
[36m[2023-07-03 00:43:00,881][188188] train() took 11.62 seconds to complete[0m
[36m[2023-07-03 00:43:00,881][188188] FPS: 330577.48[0m
[36m[2023-07-03 00:43:04,726][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:43:04,726][188188] Reward + Measures: [[277.02654873   0.155782     0.16586      0.15964533   0.15145867
    1.87643945]][0m
[37m[1m[2023-07-03 00:43:04,726][188188] Max Reward on eval: 277.0265487259914[0m
[37m[1m[2023-07-03 00:43:04,727][188188] Min Reward on eval: 277.0265487259914[0m
[37m[1m[2023-07-03 00:43:04,727][188188] Mean Reward across all agents: 277.0265487259914[0m
[37m[1m[2023-07-03 00:43:04,727][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:43:09,818][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:43:09,819][188188] Reward + Measures: [[248.35487041   0.1627       0.1691       0.1507       0.154
    1.92399406]
 [202.12620492   0.15860002   0.22239999   0.19770001   0.1673
    2.3737874 ]
 [263.88318037   0.14         0.14909999   0.14100002   0.126
    2.21792388]
 ...
 [189.54569832   0.1647       0.13600001   0.1516       0.11790001
    2.000916  ]
 [264.73917246   0.15710001   0.1621       0.20560001   0.1586
    2.23145771]
 [318.96107149   0.19310002   0.23800002   0.1859       0.19930001
    2.00685811]][0m
[37m[1m[2023-07-03 00:43:09,819][188188] Max Reward on eval: 572.9044496998191[0m
[37m[1m[2023-07-03 00:43:09,819][188188] Min Reward on eval: 41.31035631122067[0m
[37m[1m[2023-07-03 00:43:09,820][188188] Mean Reward across all agents: 227.14858941298596[0m
[37m[1m[2023-07-03 00:43:09,820][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:43:09,827][188188] mean_value=100.81439558638007, max_value=794.416449096147[0m
[37m[1m[2023-07-03 00:43:09,830][188188] New mean coefficients: [[ 4.5623693  -0.0002116   2.0347605   0.3006647  -0.51403445  0.07431573]][0m
[37m[1m[2023-07-03 00:43:09,831][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:43:18,803][188188] train() took 8.97 seconds to complete[0m
[36m[2023-07-03 00:43:18,803][188188] FPS: 428049.82[0m
[36m[2023-07-03 00:43:18,806][188188] itr=2, itrs=2000, Progress: 0.10%[0m
[36m[2023-07-03 00:43:30,394][188188] train() took 11.57 seconds to complete[0m
[36m[2023-07-03 00:43:30,395][188188] FPS: 331810.13[0m
[36m[2023-07-03 00:43:34,674][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:43:34,675][188188] Reward + Measures: [[399.890762     0.14341432   0.15864167   0.14839366   0.13943899
    1.83640707]][0m
[37m[1m[2023-07-03 00:43:34,675][188188] Max Reward on eval: 399.89076200046003[0m
[37m[1m[2023-07-03 00:43:34,675][188188] Min Reward on eval: 399.89076200046003[0m
[37m[1m[2023-07-03 00:43:34,676][188188] Mean Reward across all agents: 399.89076200046003[0m
[37m[1m[2023-07-03 00:43:34,676][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:43:39,651][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:43:39,651][188188] Reward + Measures: [[206.37689782   0.13699999   0.14299999   0.15840001   0.1122
    2.0738914 ]
 [299.73298082   0.105        0.1727       0.1151       0.14330001
    2.0770936 ]
 [394.03755377   0.21550003   0.29360005   0.14790002   0.2438
    2.24787688]
 ...
 [306.50166319   0.17019999   0.18340002   0.15700001   0.1592
    1.95134759]
 [338.54790636   0.12630001   0.1636       0.15080002   0.12810001
    2.00117087]
 [461.34699159   0.15109999   0.21339999   0.16669999   0.19940002
    1.98412287]][0m
[37m[1m[2023-07-03 00:43:39,652][188188] Max Reward on eval: 646.1399631451816[0m
[37m[1m[2023-07-03 00:43:39,652][188188] Min Reward on eval: 95.59696952030063[0m
[37m[1m[2023-07-03 00:43:39,652][188188] Mean Reward across all agents: 327.0150296632381[0m
[37m[1m[2023-07-03 00:43:39,652][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:43:39,658][188188] mean_value=71.5616589562636, max_value=390.6865924382071[0m
[37m[1m[2023-07-03 00:43:39,660][188188] New mean coefficients: [[ 4.4384713   0.43063948  2.1210785   0.02984038 -0.15255132  0.08423723]][0m
[37m[1m[2023-07-03 00:43:39,662][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:43:48,287][188188] train() took 8.62 seconds to complete[0m
[36m[2023-07-03 00:43:48,287][188188] FPS: 445276.15[0m
[36m[2023-07-03 00:43:48,290][188188] itr=3, itrs=2000, Progress: 0.15%[0m
[36m[2023-07-03 00:43:59,997][188188] train() took 11.69 seconds to complete[0m
[36m[2023-07-03 00:43:59,997][188188] FPS: 328518.13[0m
[36m[2023-07-03 00:44:04,210][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:44:04,211][188188] Reward + Measures: [[539.59731436   0.14160201   0.15959266   0.14566967   0.13429667
    1.8133496 ]][0m
[37m[1m[2023-07-03 00:44:04,211][188188] Max Reward on eval: 539.5973143574643[0m
[37m[1m[2023-07-03 00:44:04,211][188188] Min Reward on eval: 539.5973143574643[0m
[37m[1m[2023-07-03 00:44:04,211][188188] Mean Reward across all agents: 539.5973143574643[0m
[37m[1m[2023-07-03 00:44:04,212][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:44:09,109][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:44:09,110][188188] Reward + Measures: [[325.05703786   0.13280001   0.12560001   0.1322       0.11310001
    1.8119278 ]
 [491.63985249   0.1691       0.16429999   0.14540002   0.12230001
    1.97195745]
 [425.72945428   0.14650001   0.15350001   0.15640001   0.1327
    2.01226044]
 ...
 [309.39679195   0.1213       0.14299999   0.12260001   0.11719999
    1.78690398]
 [423.58650778   0.12330001   0.1278       0.11719999   0.11229999
    1.98108542]
 [245.60094685   0.13340001   0.14840001   0.12560001   0.1348
    1.88869154]][0m
[37m[1m[2023-07-03 00:44:09,110][188188] Max Reward on eval: 867.8703479590826[0m
[37m[1m[2023-07-03 00:44:09,110][188188] Min Reward on eval: 195.40654137124073[0m
[37m[1m[2023-07-03 00:44:09,111][188188] Mean Reward across all agents: 472.8425968401521[0m
[37m[1m[2023-07-03 00:44:09,111][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:44:09,116][188188] mean_value=94.15518081070971, max_value=489.1829319296401[0m
[37m[1m[2023-07-03 00:44:09,119][188188] New mean coefficients: [[ 4.410686    0.60487705  1.58635     0.53908074 -0.12309062  0.66373986]][0m
[37m[1m[2023-07-03 00:44:09,120][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:44:17,919][188188] train() took 8.80 seconds to complete[0m
[36m[2023-07-03 00:44:17,919][188188] FPS: 436471.15[0m
[36m[2023-07-03 00:44:17,921][188188] itr=4, itrs=2000, Progress: 0.20%[0m
[36m[2023-07-03 00:44:29,517][188188] train() took 11.58 seconds to complete[0m
[36m[2023-07-03 00:44:29,517][188188] FPS: 331619.18[0m
[36m[2023-07-03 00:44:33,700][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:44:33,700][188188] Reward + Measures: [[708.39035164   0.14673734   0.168842     0.153539     0.13698666
    1.82950485]][0m
[37m[1m[2023-07-03 00:44:33,701][188188] Max Reward on eval: 708.3903516373907[0m
[37m[1m[2023-07-03 00:44:33,701][188188] Min Reward on eval: 708.3903516373907[0m
[37m[1m[2023-07-03 00:44:33,701][188188] Mean Reward across all agents: 708.3903516373907[0m
[37m[1m[2023-07-03 00:44:33,701][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:44:38,640][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:44:38,641][188188] Reward + Measures: [[353.75866745   0.1158       0.1533       0.11850001   0.11620001
    1.76179218]
 [674.43742562   0.14790002   0.16989999   0.1725       0.1498
    1.8879385 ]
 [403.36475374   0.17110001   0.1547       0.1336       0.1402
    2.08870149]
 ...
 [426.83714886   0.18300001   0.16560002   0.1684       0.1175
    1.96111035]
 [236.12712956   0.10520001   0.1344       0.0911       0.1076
    2.01485515]
 [299.21723041   0.1245       0.1533       0.0984       0.1408
    2.05384779]][0m
[37m[1m[2023-07-03 00:44:38,641][188188] Max Reward on eval: 997.3824004389113[0m
[37m[1m[2023-07-03 00:44:38,641][188188] Min Reward on eval: 197.80779064819217[0m
[37m[1m[2023-07-03 00:44:38,641][188188] Mean Reward across all agents: 556.5891087166307[0m
[37m[1m[2023-07-03 00:44:38,642][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:44:38,645][188188] mean_value=27.996136960054947, max_value=529.2236608602702[0m
[37m[1m[2023-07-03 00:44:38,648][188188] New mean coefficients: [[ 3.5469828   0.72309476  1.2771078   0.6775099  -0.10205875  0.05755144]][0m
[37m[1m[2023-07-03 00:44:38,649][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:44:47,595][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 00:44:47,595][188188] FPS: 429344.73[0m
[36m[2023-07-03 00:44:47,597][188188] itr=5, itrs=2000, Progress: 0.25%[0m
[36m[2023-07-03 00:44:59,216][188188] train() took 11.60 seconds to complete[0m
[36m[2023-07-03 00:44:59,216][188188] FPS: 330959.91[0m
[36m[2023-07-03 00:45:03,512][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:45:03,512][188188] Reward + Measures: [[886.40401847   0.152568     0.17787398   0.16101268   0.13763733
    1.8278898 ]][0m
[37m[1m[2023-07-03 00:45:03,512][188188] Max Reward on eval: 886.4040184694632[0m
[37m[1m[2023-07-03 00:45:03,512][188188] Min Reward on eval: 886.4040184694632[0m
[37m[1m[2023-07-03 00:45:03,513][188188] Mean Reward across all agents: 886.4040184694632[0m
[37m[1m[2023-07-03 00:45:03,513][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:45:08,649][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:45:08,649][188188] Reward + Measures: [[718.033699     0.18340001   0.21440001   0.21250001   0.12800001
    1.93717229]
 [652.49355503   0.14240001   0.15900001   0.14040001   0.12809999
    1.78474545]
 [660.63407037   0.15810001   0.1963       0.1946       0.12990001
    1.88118446]
 ...
 [890.49859906   0.1904       0.21589999   0.1849       0.16970001
    1.92689705]
 [297.33091708   0.1191       0.1568       0.1508       0.1237
    1.8113699 ]
 [959.4956455    0.16070001   0.1866       0.19770001   0.14850001
    1.93288577]][0m
[37m[1m[2023-07-03 00:45:08,650][188188] Max Reward on eval: 1164.7994727992452[0m
[37m[1m[2023-07-03 00:45:08,650][188188] Min Reward on eval: 276.73550772508605[0m
[37m[1m[2023-07-03 00:45:08,650][188188] Mean Reward across all agents: 756.3278408833675[0m
[37m[1m[2023-07-03 00:45:08,650][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:45:08,655][188188] mean_value=90.48709365171509, max_value=1386.2990166947152[0m
[37m[1m[2023-07-03 00:45:08,658][188188] New mean coefficients: [[ 2.628943    0.72188663  1.8762054   0.34746248  0.30721033 -0.01621124]][0m
[37m[1m[2023-07-03 00:45:08,659][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:45:17,710][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 00:45:17,710][188188] FPS: 424343.20[0m
[36m[2023-07-03 00:45:17,713][188188] itr=6, itrs=2000, Progress: 0.30%[0m
[36m[2023-07-03 00:45:29,259][188188] train() took 11.53 seconds to complete[0m
[36m[2023-07-03 00:45:29,259][188188] FPS: 333062.66[0m
[36m[2023-07-03 00:45:33,454][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:45:33,454][188188] Reward + Measures: [[1109.42292631    0.15922633    0.19138299    0.17363933    0.13955666
     1.82880831]][0m
[37m[1m[2023-07-03 00:45:33,455][188188] Max Reward on eval: 1109.4229263139846[0m
[37m[1m[2023-07-03 00:45:33,455][188188] Min Reward on eval: 1109.4229263139846[0m
[37m[1m[2023-07-03 00:45:33,455][188188] Mean Reward across all agents: 1109.4229263139846[0m
[37m[1m[2023-07-03 00:45:33,456][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:45:38,412][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:45:38,413][188188] Reward + Measures: [[ 973.26686669    0.1374        0.20079999    0.1705        0.14670001
     1.89707458]
 [ 963.97957039    0.19820002    0.2332        0.2115        0.1865
     1.83169007]
 [1032.10212138    0.1714        0.20709999    0.183         0.17550001
     1.87551141]
 ...
 [ 678.90435412    0.1327        0.1586        0.1735        0.12820001
     1.88043582]
 [ 920.88939949    0.1476        0.16360001    0.17550001    0.1371
     1.90482366]
 [ 582.39293819    0.15769999    0.18069999    0.17160001    0.1463
     1.98009264]][0m
[37m[1m[2023-07-03 00:45:38,413][188188] Max Reward on eval: 1515.5173034474253[0m
[37m[1m[2023-07-03 00:45:38,414][188188] Min Reward on eval: 320.6094832064584[0m
[37m[1m[2023-07-03 00:45:38,414][188188] Mean Reward across all agents: 917.9778114294725[0m
[37m[1m[2023-07-03 00:45:38,414][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:45:38,418][188188] mean_value=52.79804799455386, max_value=1200.4433692975745[0m
[37m[1m[2023-07-03 00:45:38,421][188188] New mean coefficients: [[2.0116081  1.1837487  1.4348094  0.25611088 0.7339181  0.42514458]][0m
[37m[1m[2023-07-03 00:45:38,422][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:45:47,351][188188] train() took 8.93 seconds to complete[0m
[36m[2023-07-03 00:45:47,352][188188] FPS: 430107.96[0m
[36m[2023-07-03 00:45:47,354][188188] itr=7, itrs=2000, Progress: 0.35%[0m
[36m[2023-07-03 00:45:58,973][188188] train() took 11.60 seconds to complete[0m
[36m[2023-07-03 00:45:58,973][188188] FPS: 330994.49[0m
[36m[2023-07-03 00:46:03,217][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:46:03,217][188188] Reward + Measures: [[1330.95985348    0.16580634    0.20666534    0.18178235    0.14426734
     1.85984159]][0m
[37m[1m[2023-07-03 00:46:03,217][188188] Max Reward on eval: 1330.959853483148[0m
[37m[1m[2023-07-03 00:46:03,218][188188] Min Reward on eval: 1330.959853483148[0m
[37m[1m[2023-07-03 00:46:03,218][188188] Mean Reward across all agents: 1330.959853483148[0m
[37m[1m[2023-07-03 00:46:03,218][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:46:08,290][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:46:08,291][188188] Reward + Measures: [[ 775.41304588    0.1219        0.17040001    0.14860001    0.125
     1.9005425 ]
 [1420.75454329    0.16360001    0.21949999    0.19290002    0.15220001
     1.88545167]
 [1323.22356417    0.1795        0.25830001    0.20609999    0.1908
     1.99658287]
 ...
 [ 921.8432064     0.14839999    0.19230001    0.1723        0.1217
     1.85603547]
 [ 768.15417003    0.13789999    0.1937        0.15270001    0.14130001
     1.80595136]
 [ 886.14779283    0.16510001    0.22880001    0.1858        0.20009999
     2.06269217]][0m
[37m[1m[2023-07-03 00:46:08,291][188188] Max Reward on eval: 1663.0124969240278[0m
[37m[1m[2023-07-03 00:46:08,291][188188] Min Reward on eval: 502.8918266259134[0m
[37m[1m[2023-07-03 00:46:08,292][188188] Mean Reward across all agents: 1096.745268088663[0m
[37m[1m[2023-07-03 00:46:08,292][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:46:08,295][188188] mean_value=39.97177249175029, max_value=947.1148194124358[0m
[37m[1m[2023-07-03 00:46:08,298][188188] New mean coefficients: [[1.2254738  1.3016685  0.68320763 0.5130663  0.41712868 0.46463454]][0m
[37m[1m[2023-07-03 00:46:08,299][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:46:17,267][188188] train() took 8.97 seconds to complete[0m
[36m[2023-07-03 00:46:17,268][188188] FPS: 428261.75[0m
[36m[2023-07-03 00:46:17,270][188188] itr=8, itrs=2000, Progress: 0.40%[0m
[36m[2023-07-03 00:46:28,794][188188] train() took 11.51 seconds to complete[0m
[36m[2023-07-03 00:46:28,795][188188] FPS: 333710.20[0m
[36m[2023-07-03 00:46:33,020][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:46:33,021][188188] Reward + Measures: [[1568.66647352    0.17675202    0.22239701    0.19374201    0.14630167
     1.90933239]][0m
[37m[1m[2023-07-03 00:46:33,021][188188] Max Reward on eval: 1568.666473521545[0m
[37m[1m[2023-07-03 00:46:33,021][188188] Min Reward on eval: 1568.666473521545[0m
[37m[1m[2023-07-03 00:46:33,022][188188] Mean Reward across all agents: 1568.666473521545[0m
[37m[1m[2023-07-03 00:46:33,022][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:46:38,112][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:46:38,113][188188] Reward + Measures: [[1392.46533588    0.16690002    0.2203        0.1868        0.16470002
     2.0575521 ]
 [ 949.3523231     0.13870001    0.15629999    0.1565        0.1169
     2.01503682]
 [1475.35430906    0.1954        0.3003        0.20490001    0.18240002
     1.96703494]
 ...
 [1037.42881777    0.18790001    0.22090001    0.21870001    0.16949999
     1.77654827]
 [1145.27017784    0.19660001    0.21180001    0.20159999    0.14260001
     1.95732439]
 [ 717.10291196    0.14140001    0.15140001    0.1365        0.10950001
     1.78942323]][0m
[37m[1m[2023-07-03 00:46:38,113][188188] Max Reward on eval: 1872.3555450968445[0m
[37m[1m[2023-07-03 00:46:38,114][188188] Min Reward on eval: 717.1029119633138[0m
[37m[1m[2023-07-03 00:46:38,114][188188] Mean Reward across all agents: 1305.2405662899705[0m
[37m[1m[2023-07-03 00:46:38,114][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:46:38,118][188188] mean_value=195.7244191786025, max_value=2040.460962780971[0m
[37m[1m[2023-07-03 00:46:38,120][188188] New mean coefficients: [[ 0.13882136  0.4311027   1.7973883   0.7600703  -0.01109171  0.28163457]][0m
[37m[1m[2023-07-03 00:46:38,121][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:46:47,084][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 00:46:47,084][188188] FPS: 428519.61[0m
[36m[2023-07-03 00:46:47,087][188188] itr=9, itrs=2000, Progress: 0.45%[0m
[36m[2023-07-03 00:46:58,587][188188] train() took 11.48 seconds to complete[0m
[36m[2023-07-03 00:46:58,587][188188] FPS: 334351.24[0m
[36m[2023-07-03 00:47:02,811][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:47:02,811][188188] Reward + Measures: [[1812.33297449    0.19427234    0.27288899    0.22272299    0.160117
     1.95580709]][0m
[37m[1m[2023-07-03 00:47:02,811][188188] Max Reward on eval: 1812.3329744873608[0m
[37m[1m[2023-07-03 00:47:02,812][188188] Min Reward on eval: 1812.3329744873608[0m
[37m[1m[2023-07-03 00:47:02,812][188188] Mean Reward across all agents: 1812.3329744873608[0m
[37m[1m[2023-07-03 00:47:02,812][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:47:07,764][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:47:07,765][188188] Reward + Measures: [[1601.90185545    0.19579999    0.28660002    0.2203        0.17840001
     1.92450452]
 [1066.44931792    0.1461        0.19409999    0.16430001    0.11369999
     2.00362802]
 [1452.38746635    0.20539999    0.29910001    0.2234        0.17900001
     1.93655813]
 ...
 [1591.61870199    0.18800001    0.3057        0.2296        0.1813
     2.01001811]
 [1469.85543639    0.1824        0.28830001    0.1963        0.14659999
     1.99108887]
 [1393.60444647    0.22939999    0.26530001    0.21210001    0.17690001
     1.91969168]][0m
[37m[1m[2023-07-03 00:47:07,765][188188] Max Reward on eval: 1973.497543320991[0m
[37m[1m[2023-07-03 00:47:07,765][188188] Min Reward on eval: 927.3641929330304[0m
[37m[1m[2023-07-03 00:47:07,765][188188] Mean Reward across all agents: 1485.6869230763025[0m
[37m[1m[2023-07-03 00:47:07,766][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:47:07,770][188188] mean_value=93.31584123688599, max_value=1061.7565561181239[0m
[37m[1m[2023-07-03 00:47:07,772][188188] New mean coefficients: [[-0.31564656  1.2328093   1.1546404   1.7164311   0.9334736   0.375913  ]][0m
[37m[1m[2023-07-03 00:47:07,773][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:47:16,688][188188] train() took 8.91 seconds to complete[0m
[36m[2023-07-03 00:47:16,694][188188] FPS: 430810.88[0m
[36m[2023-07-03 00:47:16,697][188188] itr=10, itrs=2000, Progress: 0.50%[0m
[36m[2023-07-03 00:47:30,249][188188] train() took 11.69 seconds to complete[0m
[36m[2023-07-03 00:47:30,249][188188] FPS: 328494.29[0m
[36m[2023-07-03 00:47:34,507][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:47:34,508][188188] Reward + Measures: [[1642.35556155    0.222138      0.33613166    0.26760468    0.19816199
     2.00995922]][0m
[37m[1m[2023-07-03 00:47:34,508][188188] Max Reward on eval: 1642.3555615496705[0m
[37m[1m[2023-07-03 00:47:34,508][188188] Min Reward on eval: 1642.3555615496705[0m
[37m[1m[2023-07-03 00:47:34,509][188188] Mean Reward across all agents: 1642.3555615496705[0m
[37m[1m[2023-07-03 00:47:34,509][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:47:39,547][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:47:39,547][188188] Reward + Measures: [[1534.10725404    0.25079998    0.34960002    0.25729999    0.20430003
     2.0389564 ]
 [1272.39986415    0.22360002    0.36180001    0.25830004    0.2211
     1.94484925]
 [1308.04164124    0.1839        0.26560003    0.22979999    0.17749999
     2.01984668]
 ...
 [1502.35484305    0.21570002    0.27860001    0.25180003    0.1478
     2.09453702]
 [1546.20039367    0.23029999    0.34999999    0.27860004    0.2077
     2.01446319]
 [1142.44168094    0.2247        0.38050002    0.2217        0.21640001
     1.88551986]][0m
[37m[1m[2023-07-03 00:47:39,547][188188] Max Reward on eval: 1788.5258636265062[0m
[37m[1m[2023-07-03 00:47:39,548][188188] Min Reward on eval: 839.8770980594679[0m
[37m[1m[2023-07-03 00:47:39,548][188188] Mean Reward across all agents: 1381.5987566370097[0m
[37m[1m[2023-07-03 00:47:39,548][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:47:39,551][188188] mean_value=-86.30801649649365, max_value=1681.0683885595988[0m
[37m[1m[2023-07-03 00:47:39,553][188188] New mean coefficients: [[0.5492207  0.77532995 1.1322243  1.3611451  1.1405576  0.32886958]][0m
[37m[1m[2023-07-03 00:47:39,554][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:47:48,544][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 00:47:48,545][188188] FPS: 427209.63[0m
[36m[2023-07-03 00:47:48,547][188188] itr=11, itrs=2000, Progress: 0.55%[0m
[36m[2023-07-03 00:48:00,077][188188] train() took 11.51 seconds to complete[0m
[36m[2023-07-03 00:48:00,077][188188] FPS: 333575.32[0m
[36m[2023-07-03 00:48:04,335][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:48:04,336][188188] Reward + Measures: [[1834.54044506    0.21874866    0.33506867    0.28579399    0.19729899
     2.05552983]][0m
[37m[1m[2023-07-03 00:48:04,336][188188] Max Reward on eval: 1834.540445063378[0m
[37m[1m[2023-07-03 00:48:04,336][188188] Min Reward on eval: 1834.540445063378[0m
[37m[1m[2023-07-03 00:48:04,336][188188] Mean Reward across all agents: 1834.540445063378[0m
[37m[1m[2023-07-03 00:48:04,337][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:48:09,409][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:48:09,415][188188] Reward + Measures: [[1270.24266432    0.20550001    0.2719        0.24679999    0.18959999
     1.91157997]
 [1551.10137651    0.18070002    0.27239999    0.2385        0.1714
     1.98324513]
 [1362.43177792    0.22119999    0.31529999    0.29200003    0.22379999
     2.02155614]
 ...
 [1754.44561768    0.22810002    0.32979998    0.28379998    0.198
     2.02003646]
 [1453.25732039    0.20380001    0.2868        0.27400002    0.185
     2.01630616]
 [ 923.1502094     0.18930002    0.27450001    0.18770002    0.17380002
     1.94886339]][0m
[37m[1m[2023-07-03 00:48:09,415][188188] Max Reward on eval: 1935.7543640365825[0m
[37m[1m[2023-07-03 00:48:09,416][188188] Min Reward on eval: 923.1502094042953[0m
[37m[1m[2023-07-03 00:48:09,416][188188] Mean Reward across all agents: 1577.158125079595[0m
[37m[1m[2023-07-03 00:48:09,416][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:48:09,419][188188] mean_value=-28.46261581126004, max_value=861.4161140558134[0m
[37m[1m[2023-07-03 00:48:09,422][188188] New mean coefficients: [[0.58505434 0.12112695 0.8764835  0.50682074 0.8238472  0.58866465]][0m
[37m[1m[2023-07-03 00:48:09,423][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:48:18,395][188188] train() took 8.97 seconds to complete[0m
[36m[2023-07-03 00:48:18,395][188188] FPS: 428081.82[0m
[36m[2023-07-03 00:48:18,397][188188] itr=12, itrs=2000, Progress: 0.60%[0m
[36m[2023-07-03 00:48:29,898][188188] train() took 11.48 seconds to complete[0m
[36m[2023-07-03 00:48:29,898][188188] FPS: 334385.14[0m
[36m[2023-07-03 00:48:34,237][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:48:34,237][188188] Reward + Measures: [[2099.60905051    0.20056832    0.31992066    0.27220866    0.18276967
     2.14016294]][0m
[37m[1m[2023-07-03 00:48:34,238][188188] Max Reward on eval: 2099.6090505055395[0m
[37m[1m[2023-07-03 00:48:34,238][188188] Min Reward on eval: 2099.6090505055395[0m
[37m[1m[2023-07-03 00:48:34,238][188188] Mean Reward across all agents: 2099.6090505055395[0m
[37m[1m[2023-07-03 00:48:34,239][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:48:39,266][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:48:39,267][188188] Reward + Measures: [[1983.5103531     0.1913        0.32119998    0.2467        0.1885
     2.13865137]
 [1959.93583683    0.20130001    0.3585        0.27470002    0.19780003
     2.10505867]
 [1733.55439759    0.19770001    0.3346        0.25560004    0.1891
     2.08250999]
 ...
 [1829.85638424    0.19959998    0.30500001    0.2552        0.1653
     2.14015198]
 [1835.87601852    0.1938        0.29009998    0.25219998    0.17390002
     2.11110282]
 [2029.09347531    0.2184        0.32880002    0.2667        0.1875
     2.17966127]][0m
[37m[1m[2023-07-03 00:48:39,267][188188] Max Reward on eval: 2242.7014923182314[0m
[37m[1m[2023-07-03 00:48:39,267][188188] Min Reward on eval: 1173.281928996276[0m
[37m[1m[2023-07-03 00:48:39,267][188188] Mean Reward across all agents: 1850.7104583727823[0m
[37m[1m[2023-07-03 00:48:39,268][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:48:39,272][188188] mean_value=100.20659156804062, max_value=614.2291006029027[0m
[37m[1m[2023-07-03 00:48:39,274][188188] New mean coefficients: [[0.9615401  0.25953913 0.6630043  0.5647156  0.5036775  0.41829965]][0m
[37m[1m[2023-07-03 00:48:39,275][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:48:48,186][188188] train() took 8.91 seconds to complete[0m
[36m[2023-07-03 00:48:48,186][188188] FPS: 431029.26[0m
[36m[2023-07-03 00:48:48,188][188188] itr=13, itrs=2000, Progress: 0.65%[0m
[36m[2023-07-03 00:48:59,795][188188] train() took 11.59 seconds to complete[0m
[36m[2023-07-03 00:48:59,795][188188] FPS: 331290.47[0m
[36m[2023-07-03 00:49:04,059][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:49:04,059][188188] Reward + Measures: [[2343.09055819    0.19371967    0.29041299    0.25676399    0.15910934
     2.16171384]][0m
[37m[1m[2023-07-03 00:49:04,060][188188] Max Reward on eval: 2343.090558189938[0m
[37m[1m[2023-07-03 00:49:04,060][188188] Min Reward on eval: 2343.090558189938[0m
[37m[1m[2023-07-03 00:49:04,060][188188] Mean Reward across all agents: 2343.090558189938[0m
[37m[1m[2023-07-03 00:49:04,060][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:49:09,037][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:49:09,037][188188] Reward + Measures: [[1803.03037643    0.17150001    0.25780001    0.2256        0.14470001
     2.12904239]
 [2181.59439086    0.19880001    0.28830001    0.2595        0.16430001
     2.14772677]
 [2017.61536026    0.19030002    0.26519999    0.24020003    0.14350002
     2.18824577]
 ...
 [2042.16133492    0.1795        0.2811        0.2518        0.1487
     2.15275049]
 [1945.55446999    0.19940001    0.3206        0.26030001    0.1725
     2.05037761]
 [2099.8526459     0.18859999    0.26760003    0.23280001    0.15339999
     2.13091135]][0m
[37m[1m[2023-07-03 00:49:09,038][188188] Max Reward on eval: 2479.7136535302734[0m
[37m[1m[2023-07-03 00:49:09,038][188188] Min Reward on eval: 1096.5939082127995[0m
[37m[1m[2023-07-03 00:49:09,038][188188] Mean Reward across all agents: 2071.585109346946[0m
[37m[1m[2023-07-03 00:49:09,038][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:49:09,044][188188] mean_value=158.03914267591074, max_value=613.2972999419003[0m
[37m[1m[2023-07-03 00:49:09,046][188188] New mean coefficients: [[1.201561   0.47026408 0.28525525 0.6266334  0.49272308 0.5903283 ]][0m
[37m[1m[2023-07-03 00:49:09,047][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:49:17,922][188188] train() took 8.87 seconds to complete[0m
[36m[2023-07-03 00:49:17,923][188188] FPS: 432756.03[0m
[36m[2023-07-03 00:49:17,925][188188] itr=14, itrs=2000, Progress: 0.70%[0m
[36m[2023-07-03 00:49:29,615][188188] train() took 11.67 seconds to complete[0m
[36m[2023-07-03 00:49:29,615][188188] FPS: 328911.75[0m
[36m[2023-07-03 00:49:33,948][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:49:33,949][188188] Reward + Measures: [[2505.330662      0.181839      0.26903069    0.24141133    0.14441165
     2.18917084]][0m
[37m[1m[2023-07-03 00:49:33,949][188188] Max Reward on eval: 2505.3306620031676[0m
[37m[1m[2023-07-03 00:49:33,949][188188] Min Reward on eval: 2505.3306620031676[0m
[37m[1m[2023-07-03 00:49:33,950][188188] Mean Reward across all agents: 2505.3306620031676[0m
[37m[1m[2023-07-03 00:49:33,950][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:49:38,913][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:49:38,914][188188] Reward + Measures: [[1861.42527383    0.18870001    0.24759999    0.24700001    0.15390001
     2.07839012]
 [2208.71668243    0.1866        0.24870002    0.22409999    0.13240002
     2.24022985]
 [2150.18778996    0.1655        0.23840001    0.22230001    0.12720001
     2.15741658]
 ...
 [2384.77740859    0.18320002    0.26019999    0.2397        0.1375
     2.19000888]
 [2511.19070436    0.20020001    0.27490002    0.25650001    0.1494
     2.2356286 ]
 [1986.71656421    0.1672        0.29050002    0.23010002    0.14740001
     2.1786418 ]][0m
[37m[1m[2023-07-03 00:49:38,914][188188] Max Reward on eval: 2771.101638818346[0m
[37m[1m[2023-07-03 00:49:38,915][188188] Min Reward on eval: 1162.8509388452571[0m
[37m[1m[2023-07-03 00:49:38,915][188188] Mean Reward across all agents: 2232.4562322539477[0m
[37m[1m[2023-07-03 00:49:38,915][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:49:38,920][188188] mean_value=142.70769657534163, max_value=647.6494463348643[0m
[37m[1m[2023-07-03 00:49:38,922][188188] New mean coefficients: [[ 1.4838623   0.5173234  -0.19099873  0.4269644   0.00105581  1.0365682 ]][0m
[37m[1m[2023-07-03 00:49:38,923][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:49:47,922][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 00:49:47,923][188188] FPS: 426794.95[0m
[36m[2023-07-03 00:49:47,925][188188] itr=15, itrs=2000, Progress: 0.75%[0m
[36m[2023-07-03 00:49:59,765][188188] train() took 11.82 seconds to complete[0m
[36m[2023-07-03 00:49:59,765][188188] FPS: 324811.09[0m
[36m[2023-07-03 00:50:03,989][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:50:03,989][188188] Reward + Measures: [[2645.13471186    0.17578065    0.24594831    0.22977166    0.12970167
     2.2330296 ]][0m
[37m[1m[2023-07-03 00:50:03,989][188188] Max Reward on eval: 2645.1347118649137[0m
[37m[1m[2023-07-03 00:50:03,990][188188] Min Reward on eval: 2645.1347118649137[0m
[37m[1m[2023-07-03 00:50:03,990][188188] Mean Reward across all agents: 2645.1347118649137[0m
[37m[1m[2023-07-03 00:50:03,990][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:50:08,997][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:50:08,997][188188] Reward + Measures: [[2258.98602292    0.18529999    0.23539999    0.2251        0.1384
     2.15137315]
 [2188.72734067    0.1732        0.22420001    0.21660002    0.11610001
     2.21280885]
 [2084.22703931    0.1639        0.22520001    0.20639999    0.1251
     2.14728832]
 ...
 [2464.7707901     0.1777        0.2309        0.21430002    0.11850001
     2.22605705]
 [2266.15587616    0.1672        0.25050002    0.2023        0.1339
     2.14539528]
 [2451.86059568    0.20899999    0.27269998    0.2343        0.16510001
     2.13638926]][0m
[37m[1m[2023-07-03 00:50:08,998][188188] Max Reward on eval: 3052.0894775249994[0m
[37m[1m[2023-07-03 00:50:08,998][188188] Min Reward on eval: 1312.108371776901[0m
[37m[1m[2023-07-03 00:50:08,998][188188] Mean Reward across all agents: 2304.4353032796216[0m
[37m[1m[2023-07-03 00:50:08,998][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:50:09,002][188188] mean_value=51.898379444459984, max_value=735.3921135220467[0m
[37m[1m[2023-07-03 00:50:09,005][188188] New mean coefficients: [[ 1.0980135   0.18954727 -0.32241696  0.42337295  0.2142685   0.72877103]][0m
[37m[1m[2023-07-03 00:50:09,006][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:50:18,008][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 00:50:18,008][188188] FPS: 426650.91[0m
[36m[2023-07-03 00:50:18,010][188188] itr=16, itrs=2000, Progress: 0.80%[0m
[36m[2023-07-03 00:50:29,540][188188] train() took 11.51 seconds to complete[0m
[36m[2023-07-03 00:50:29,541][188188] FPS: 333540.31[0m
[36m[2023-07-03 00:50:33,788][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:50:33,788][188188] Reward + Measures: [[2765.35815938    0.17504035    0.23534098    0.22309367    0.12307832
     2.2572968 ]][0m
[37m[1m[2023-07-03 00:50:33,788][188188] Max Reward on eval: 2765.358159375936[0m
[37m[1m[2023-07-03 00:50:33,789][188188] Min Reward on eval: 2765.358159375936[0m
[37m[1m[2023-07-03 00:50:33,789][188188] Mean Reward across all agents: 2765.358159375936[0m
[37m[1m[2023-07-03 00:50:33,789][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:50:38,849][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:50:38,850][188188] Reward + Measures: [[2441.76249697    0.17309999    0.2511        0.21159999    0.13250001
     2.19854259]
 [2093.25863648    0.1688        0.23020001    0.2041        0.13010001
     2.24690413]
 [3027.29748537    0.1829        0.2701        0.2342        0.1384
     2.26166034]
 ...
 [2378.12605285    0.15530001    0.21760002    0.19890001    0.10650001
     2.23797965]
 [2163.23322298    0.1822        0.25229999    0.22220002    0.1411
     2.23558545]
 [2631.60230254    0.19319999    0.24200001    0.23380001    0.13450001
     2.18184137]][0m
[37m[1m[2023-07-03 00:50:38,850][188188] Max Reward on eval: 3181.1808777134984[0m
[37m[1m[2023-07-03 00:50:38,851][188188] Min Reward on eval: 1392.8391952219886[0m
[37m[1m[2023-07-03 00:50:38,851][188188] Mean Reward across all agents: 2500.5365961555826[0m
[37m[1m[2023-07-03 00:50:38,851][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:50:38,854][188188] mean_value=-17.870838010546287, max_value=689.7303928535866[0m
[37m[1m[2023-07-03 00:50:38,857][188188] New mean coefficients: [[ 0.9709537  -0.14748946 -0.31191206  0.54634136  0.07088989  0.96904963]][0m
[37m[1m[2023-07-03 00:50:38,858][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:50:47,840][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 00:50:47,840][188188] FPS: 427601.41[0m
[36m[2023-07-03 00:50:47,842][188188] itr=17, itrs=2000, Progress: 0.85%[0m
[36m[2023-07-03 00:50:59,426][188188] train() took 11.57 seconds to complete[0m
[36m[2023-07-03 00:50:59,426][188188] FPS: 332018.00[0m
[36m[2023-07-03 00:51:03,685][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:51:03,686][188188] Reward + Measures: [[2966.40908227    0.17427032    0.237361      0.22093567    0.12005
     2.29852867]][0m
[37m[1m[2023-07-03 00:51:03,686][188188] Max Reward on eval: 2966.4090822715984[0m
[37m[1m[2023-07-03 00:51:03,686][188188] Min Reward on eval: 2966.4090822715984[0m
[37m[1m[2023-07-03 00:51:03,687][188188] Mean Reward across all agents: 2966.4090822715984[0m
[37m[1m[2023-07-03 00:51:03,687][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:51:08,746][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:51:08,746][188188] Reward + Measures: [[2833.19210819    0.18370001    0.2476        0.22080003    0.13569999
     2.28980327]
 [2740.88477332    0.16199999    0.22849999    0.205         0.11980001
     2.26828814]
 [2212.38581848    0.1567        0.21760002    0.1926        0.11290001
     2.22776222]
 ...
 [2311.87941167    0.1513        0.21560001    0.1911        0.1068
     2.2621963 ]
 [1772.06097032    0.1586        0.19859999    0.17160001    0.1204
     2.23143363]
 [2687.58370974    0.1754        0.23870002    0.22309999    0.1243
     2.33197737]][0m
[37m[1m[2023-07-03 00:51:08,746][188188] Max Reward on eval: 3327.300079352409[0m
[37m[1m[2023-07-03 00:51:08,746][188188] Min Reward on eval: 1546.9289893882349[0m
[37m[1m[2023-07-03 00:51:08,747][188188] Mean Reward across all agents: 2545.6576794366542[0m
[37m[1m[2023-07-03 00:51:08,747][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:51:08,749][188188] mean_value=-207.44308918631447, max_value=575.7909645953669[0m
[37m[1m[2023-07-03 00:51:08,752][188188] New mean coefficients: [[ 0.22769505 -0.2744754  -0.6536316  -0.17833358  0.05985473  0.52494025]][0m
[37m[1m[2023-07-03 00:51:08,753][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:51:17,775][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 00:51:17,776][188188] FPS: 425683.48[0m
[36m[2023-07-03 00:51:17,778][188188] itr=18, itrs=2000, Progress: 0.90%[0m
[36m[2023-07-03 00:51:29,466][188188] train() took 11.67 seconds to complete[0m
[36m[2023-07-03 00:51:29,466][188188] FPS: 329060.46[0m
[36m[2023-07-03 00:51:33,801][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:51:33,801][188188] Reward + Measures: [[3022.17324778    0.17435433    0.22985367    0.21708634    0.11562567
     2.32281423]][0m
[37m[1m[2023-07-03 00:51:33,802][188188] Max Reward on eval: 3022.173247775434[0m
[37m[1m[2023-07-03 00:51:33,802][188188] Min Reward on eval: 3022.173247775434[0m
[37m[1m[2023-07-03 00:51:33,802][188188] Mean Reward across all agents: 3022.173247775434[0m
[37m[1m[2023-07-03 00:51:33,803][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:51:38,818][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:51:38,818][188188] Reward + Measures: [[2159.46328356    0.1515        0.18970001    0.18190001    0.10640001
     2.36300945]
 [2695.83882718    0.1653        0.22659998    0.21089999    0.11110001
     2.23736835]
 [2216.80173879    0.14749999    0.2041        0.17889999    0.1087
     2.30132079]
 ...
 [2139.48686985    0.139         0.17840001    0.17269999    0.0918
     2.25682139]
 [2688.17834852    0.1655        0.21079998    0.19500001    0.1153
     2.35064149]
 [2744.13053699    0.1768        0.23780003    0.21519999    0.1294
     2.25086021]][0m
[37m[1m[2023-07-03 00:51:38,818][188188] Max Reward on eval: 3417.9482116808185[0m
[37m[1m[2023-07-03 00:51:38,819][188188] Min Reward on eval: 1447.2568664268824[0m
[37m[1m[2023-07-03 00:51:38,819][188188] Mean Reward across all agents: 2683.261044388169[0m
[37m[1m[2023-07-03 00:51:38,819][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:51:38,821][188188] mean_value=-266.25088678694465, max_value=507.86104612532563[0m
[37m[1m[2023-07-03 00:51:38,824][188188] New mean coefficients: [[ 0.06603345 -0.19314352  0.2653569  -0.24541406  0.3185202   0.00093019]][0m
[37m[1m[2023-07-03 00:51:38,825][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:51:47,834][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 00:51:47,834][188188] FPS: 426318.14[0m
[36m[2023-07-03 00:51:47,836][188188] itr=19, itrs=2000, Progress: 0.95%[0m
[36m[2023-07-03 00:51:59,572][188188] train() took 11.72 seconds to complete[0m
[36m[2023-07-03 00:51:59,572][188188] FPS: 327638.68[0m
[36m[2023-07-03 00:52:03,942][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:52:03,942][188188] Reward + Measures: [[3121.02748601    0.17738199    0.24359533    0.22354867    0.12557933
     2.32819104]][0m
[37m[1m[2023-07-03 00:52:03,942][188188] Max Reward on eval: 3121.027486006651[0m
[37m[1m[2023-07-03 00:52:03,943][188188] Min Reward on eval: 3121.027486006651[0m
[37m[1m[2023-07-03 00:52:03,943][188188] Mean Reward across all agents: 3121.027486006651[0m
[37m[1m[2023-07-03 00:52:03,943][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:52:08,952][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:52:08,953][188188] Reward + Measures: [[3080.88441468    0.1805        0.24489999    0.2218        0.1257
     2.36073732]
 [2702.67975229    0.1727        0.24679999    0.22610001    0.1283
     2.1668992 ]
 [3267.7402039     0.19679999    0.28290001    0.23220001    0.1452
     2.24694443]
 ...
 [2921.6766892     0.17640001    0.2422        0.2105        0.1244
     2.23633122]
 [2658.6897736     0.1674        0.23459999    0.21269999    0.1277
     2.2058351 ]
 [3123.52340699    0.20030001    0.27290002    0.2502        0.15980001
     2.25821614]][0m
[37m[1m[2023-07-03 00:52:08,953][188188] Max Reward on eval: 3508.0051269636374[0m
[37m[1m[2023-07-03 00:52:08,953][188188] Min Reward on eval: 1795.1175499440637[0m
[37m[1m[2023-07-03 00:52:08,953][188188] Mean Reward across all agents: 2819.895274267447[0m
[37m[1m[2023-07-03 00:52:08,954][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:52:08,956][188188] mean_value=-299.9277871589926, max_value=601.8206547130958[0m
[37m[1m[2023-07-03 00:52:08,958][188188] New mean coefficients: [[ 0.06950735 -0.47154614 -0.10642684  0.41776848  0.36251736  0.13406448]][0m
[37m[1m[2023-07-03 00:52:08,959][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:52:17,936][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 00:52:17,936][188188] FPS: 427838.43[0m
[36m[2023-07-03 00:52:17,939][188188] itr=20, itrs=2000, Progress: 1.00%[0m
[36m[2023-07-03 00:52:31,541][188188] train() took 11.75 seconds to complete[0m
[36m[2023-07-03 00:52:31,541][188188] FPS: 326704.70[0m
[36m[2023-07-03 00:52:35,740][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:52:35,740][188188] Reward + Measures: [[3170.08353202    0.17035767    0.24004333    0.23132633    0.128153
     2.36686158]][0m
[37m[1m[2023-07-03 00:52:35,741][188188] Max Reward on eval: 3170.083532015669[0m
[37m[1m[2023-07-03 00:52:35,741][188188] Min Reward on eval: 3170.083532015669[0m
[37m[1m[2023-07-03 00:52:35,741][188188] Mean Reward across all agents: 3170.083532015669[0m
[37m[1m[2023-07-03 00:52:35,741][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:52:40,831][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:52:40,832][188188] Reward + Measures: [[2522.50215533    0.15290001    0.2313        0.2036        0.1441
     2.31366348]
 [2674.14543151    0.16539998    0.2306        0.21079998    0.14060001
     2.3315289 ]
 [2889.57952119    0.1735        0.2289        0.2191        0.1244
     2.29365849]
 ...
 [2930.79504394    0.17389999    0.2516        0.2149        0.14040001
     2.35762691]
 [3498.47253418    0.1825        0.26479998    0.2455        0.12840001
     2.41961551]
 [3009.3786621     0.1789        0.24300002    0.2462        0.11539999
     2.37188506]][0m
[37m[1m[2023-07-03 00:52:40,832][188188] Max Reward on eval: 3567.449523907155[0m
[37m[1m[2023-07-03 00:52:40,832][188188] Min Reward on eval: 1551.3724174700678[0m
[37m[1m[2023-07-03 00:52:40,832][188188] Mean Reward across all agents: 2856.8315965381958[0m
[37m[1m[2023-07-03 00:52:40,833][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:52:40,834][188188] mean_value=-417.1516965912284, max_value=511.256780508108[0m
[37m[1m[2023-07-03 00:52:40,837][188188] New mean coefficients: [[-0.21726452 -0.0930874   0.04253301  0.40702853  0.03471035 -0.19901179]][0m
[37m[1m[2023-07-03 00:52:40,838][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:52:49,772][188188] train() took 8.93 seconds to complete[0m
[36m[2023-07-03 00:52:49,772][188188] FPS: 429916.79[0m
[36m[2023-07-03 00:52:49,774][188188] itr=21, itrs=2000, Progress: 1.05%[0m
[36m[2023-07-03 00:53:01,335][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 00:53:01,335][188188] FPS: 332593.38[0m
[36m[2023-07-03 00:53:05,678][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:53:05,679][188188] Reward + Measures: [[3127.40275756    0.17843066    0.25104633    0.24773368    0.14096533
     2.33588052]][0m
[37m[1m[2023-07-03 00:53:05,679][188188] Max Reward on eval: 3127.402757563258[0m
[37m[1m[2023-07-03 00:53:05,679][188188] Min Reward on eval: 3127.402757563258[0m
[37m[1m[2023-07-03 00:53:05,679][188188] Mean Reward across all agents: 3127.402757563258[0m
[37m[1m[2023-07-03 00:53:05,679][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:53:10,691][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:53:10,692][188188] Reward + Measures: [[2962.05723571    0.18269999    0.23909998    0.26070002    0.14780001
     2.25140977]
 [2270.78638453    0.1477        0.21000002    0.21500002    0.1098
     2.31357169]
 [3070.34049991    0.192         0.24620001    0.2613        0.15610002
     2.28704429]
 ...
 [3129.542099      0.1813        0.2658        0.25960001    0.15620001
     2.30610824]
 [3134.99459833    0.18719999    0.2525        0.25240001    0.15019999
     2.33415437]
 [2613.65659328    0.16200002    0.22479999    0.22159998    0.13569999
     2.28319526]][0m
[37m[1m[2023-07-03 00:53:10,692][188188] Max Reward on eval: 3412.1333618314384[0m
[37m[1m[2023-07-03 00:53:10,692][188188] Min Reward on eval: 1833.9252758581192[0m
[37m[1m[2023-07-03 00:53:10,692][188188] Mean Reward across all agents: 2873.8394919032967[0m
[37m[1m[2023-07-03 00:53:10,692][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:53:10,694][188188] mean_value=-428.91825837684354, max_value=247.9093793839902[0m
[37m[1m[2023-07-03 00:53:10,697][188188] New mean coefficients: [[-0.21265984 -0.04537264  0.10579363  0.734097    0.08859648  0.11917593]][0m
[37m[1m[2023-07-03 00:53:10,698][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:53:19,712][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 00:53:19,712][188188] FPS: 426082.74[0m
[36m[2023-07-03 00:53:19,715][188188] itr=22, itrs=2000, Progress: 1.10%[0m
[36m[2023-07-03 00:53:31,322][188188] train() took 11.59 seconds to complete[0m
[36m[2023-07-03 00:53:31,322][188188] FPS: 331299.67[0m
[36m[2023-07-03 00:53:35,607][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:53:35,612][188188] Reward + Measures: [[2958.36131236    0.184525      0.26596397    0.26979735    0.16142899
     2.34161329]][0m
[37m[1m[2023-07-03 00:53:35,613][188188] Max Reward on eval: 2958.3613123582913[0m
[37m[1m[2023-07-03 00:53:35,613][188188] Min Reward on eval: 2958.3613123582913[0m
[37m[1m[2023-07-03 00:53:35,613][188188] Mean Reward across all agents: 2958.3613123582913[0m
[37m[1m[2023-07-03 00:53:35,614][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:53:40,629][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:53:40,634][188188] Reward + Measures: [[3066.40640259    0.1865        0.27700001    0.27680001    0.18250002
     2.33387566]
 [3022.50442507    0.20209999    0.28639999    0.28400001    0.17019999
     2.30722857]
 [2878.20014189    0.1912        0.24949999    0.2631        0.15000001
     2.33276844]
 ...
 [3118.34991455    0.1946        0.26820001    0.27509999    0.15620001
     2.35353255]
 [2780.4943695     0.19160001    0.26770002    0.26990002    0.1652
     2.29697776]
 [2808.00485227    0.1727        0.26050001    0.25760001    0.17260002
     2.31064963]][0m
[37m[1m[2023-07-03 00:53:40,635][188188] Max Reward on eval: 3220.155670166388[0m
[37m[1m[2023-07-03 00:53:40,635][188188] Min Reward on eval: 2146.1959285821767[0m
[37m[1m[2023-07-03 00:53:40,635][188188] Mean Reward across all agents: 2754.5357497175637[0m
[37m[1m[2023-07-03 00:53:40,635][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:53:40,637][188188] mean_value=-536.4956072592198, max_value=-43.096136297382145[0m
[36m[2023-07-03 00:53:40,639][188188] XNES is restarting with a new solution whose measures are [0.18120001 0.2651     0.26990002 0.1393     2.34144282] and objective is 3412.1333618314384[0m
[36m[2023-07-03 00:53:40,640][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 00:53:40,643][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 00:53:40,644][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:53:49,700][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 00:53:49,700][188188] FPS: 424090.09[0m
[36m[2023-07-03 00:53:49,702][188188] itr=23, itrs=2000, Progress: 1.15%[0m
[36m[2023-07-03 00:54:01,471][188188] train() took 11.75 seconds to complete[0m
[36m[2023-07-03 00:54:01,471][188188] FPS: 326731.80[0m
[36m[2023-07-03 00:54:05,714][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:54:05,714][188188] Reward + Measures: [[3082.8013008     0.16724832    0.22873934    0.22784565    0.12346199
     2.24732828]][0m
[37m[1m[2023-07-03 00:54:05,714][188188] Max Reward on eval: 3082.8013008049925[0m
[37m[1m[2023-07-03 00:54:05,715][188188] Min Reward on eval: 3082.8013008049925[0m
[37m[1m[2023-07-03 00:54:05,715][188188] Mean Reward across all agents: 3082.8013008049925[0m
[37m[1m[2023-07-03 00:54:05,715][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:54:10,753][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:54:10,753][188188] Reward + Measures: [[2711.8020401     0.1821        0.22879998    0.2357        0.1406
     2.26625419]
 [2846.73980715    0.1662        0.24430001    0.23480001    0.12
     2.15087557]
 [2059.59602545    0.13370001    0.17570001    0.1754        0.0812
     2.08188891]
 ...
 [2953.99329378    0.17560001    0.2299        0.2349        0.12639999
     2.13631582]
 [2721.05701449    0.17760001    0.21680002    0.22589998    0.11849999
     2.15319514]
 [2757.01789085    0.1585        0.2027        0.2145        0.1027
     2.16210985]][0m
[37m[1m[2023-07-03 00:54:10,753][188188] Max Reward on eval: 3421.5691070300527[0m
[37m[1m[2023-07-03 00:54:10,754][188188] Min Reward on eval: 952.7803601676599[0m
[37m[1m[2023-07-03 00:54:10,754][188188] Mean Reward across all agents: 2360.812802128694[0m
[37m[1m[2023-07-03 00:54:10,754][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:54:10,756][188188] mean_value=-990.3541319842844, max_value=64.17266180996148[0m
[37m[1m[2023-07-03 00:54:10,758][188188] New mean coefficients: [[ 0.8685287  -0.35721594  0.01211059 -0.98167145 -0.74475396 -0.30441973]][0m
[37m[1m[2023-07-03 00:54:10,759][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:54:19,706][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 00:54:19,706][188188] FPS: 429282.96[0m
[36m[2023-07-03 00:54:19,708][188188] itr=24, itrs=2000, Progress: 1.20%[0m
[36m[2023-07-03 00:54:31,201][188188] train() took 11.48 seconds to complete[0m
[36m[2023-07-03 00:54:31,202][188188] FPS: 334546.27[0m
[36m[2023-07-03 00:54:35,429][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:54:35,430][188188] Reward + Measures: [[3149.6205049     0.16590065    0.22288699    0.21908432    0.11786533
     2.23906112]][0m
[37m[1m[2023-07-03 00:54:35,430][188188] Max Reward on eval: 3149.620504898943[0m
[37m[1m[2023-07-03 00:54:35,430][188188] Min Reward on eval: 3149.620504898943[0m
[37m[1m[2023-07-03 00:54:35,431][188188] Mean Reward across all agents: 3149.620504898943[0m
[37m[1m[2023-07-03 00:54:35,431][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:54:40,417][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:54:40,423][188188] Reward + Measures: [[2086.8934641     0.1426        0.2026        0.20120001    0.1269
     2.14105844]
 [2196.86050034    0.1427        0.1919        0.1875        0.13580002
     2.07852697]
 [1840.31456371    0.1508        0.19770001    0.20819998    0.1322
     2.03539658]
 ...
 [2855.17887498    0.1566        0.22319999    0.2122        0.12080001
     2.20051289]
 [2730.74929806    0.19500001    0.26290002    0.24340001    0.1446
     2.0993619 ]
 [2643.51685331    0.14569999    0.18960001    0.20220001    0.1184
     2.14487267]][0m
[37m[1m[2023-07-03 00:54:40,424][188188] Max Reward on eval: 3483.660064720921[0m
[37m[1m[2023-07-03 00:54:40,425][188188] Min Reward on eval: 1238.4646053269505[0m
[37m[1m[2023-07-03 00:54:40,425][188188] Mean Reward across all agents: 2540.190904337049[0m
[37m[1m[2023-07-03 00:54:40,426][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:54:40,429][188188] mean_value=-821.0737671708856, max_value=119.84635331983418[0m
[37m[1m[2023-07-03 00:54:40,434][188188] New mean coefficients: [[ 0.9073748   0.3245203   0.01886361 -0.7335117  -0.06893075 -0.43959224]][0m
[37m[1m[2023-07-03 00:54:40,435][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:54:49,458][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 00:54:49,458][188188] FPS: 425700.91[0m
[36m[2023-07-03 00:54:49,461][188188] itr=25, itrs=2000, Progress: 1.25%[0m
[36m[2023-07-03 00:55:01,111][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 00:55:01,111][188188] FPS: 330046.91[0m
[36m[2023-07-03 00:55:05,444][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:55:05,444][188188] Reward + Measures: [[3251.735178      0.16410099    0.22178766    0.21918298    0.11440299
     2.23248196]][0m
[37m[1m[2023-07-03 00:55:05,445][188188] Max Reward on eval: 3251.7351780021736[0m
[37m[1m[2023-07-03 00:55:05,445][188188] Min Reward on eval: 3251.7351780021736[0m
[37m[1m[2023-07-03 00:55:05,445][188188] Mean Reward across all agents: 3251.7351780021736[0m
[37m[1m[2023-07-03 00:55:05,445][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:55:10,540][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:55:10,546][188188] Reward + Measures: [[3284.38090515    0.18010001    0.2361        0.23730002    0.11170001
     2.12827063]
 [1636.23637199    0.1182        0.20060001    0.1795        0.11430001
     2.14077854]
 [2833.43002316    0.16550002    0.23169999    0.22849999    0.12979999
     2.14954734]
 ...
 [2951.87561416    0.1636        0.22690001    0.22290002    0.11869999
     2.15647101]
 [2722.96286011    0.18450001    0.2386        0.2247        0.13810001
     2.15845537]
 [2545.38327024    0.16429999    0.21079998    0.20420001    0.116
     2.07899094]][0m
[37m[1m[2023-07-03 00:55:10,547][188188] Max Reward on eval: 3721.712768619321[0m
[37m[1m[2023-07-03 00:55:10,547][188188] Min Reward on eval: 1459.6592559898272[0m
[37m[1m[2023-07-03 00:55:10,547][188188] Mean Reward across all agents: 2750.240465141552[0m
[37m[1m[2023-07-03 00:55:10,547][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:55:10,549][188188] mean_value=-619.6300104056929, max_value=345.05649304832286[0m
[37m[1m[2023-07-03 00:55:10,552][188188] New mean coefficients: [[ 0.69729984  0.7271347   0.30004263 -1.0912331  -0.02708341 -0.29358208]][0m
[37m[1m[2023-07-03 00:55:10,553][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:55:19,607][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 00:55:19,607][188188] FPS: 424199.08[0m
[36m[2023-07-03 00:55:19,609][188188] itr=26, itrs=2000, Progress: 1.30%[0m
[36m[2023-07-03 00:55:31,213][188188] train() took 11.59 seconds to complete[0m
[36m[2023-07-03 00:55:31,213][188188] FPS: 331418.27[0m
[36m[2023-07-03 00:55:35,417][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:55:35,422][188188] Reward + Measures: [[3361.79208518    0.16463333    0.22102468    0.216296      0.11398366
     2.22250581]][0m
[37m[1m[2023-07-03 00:55:35,423][188188] Max Reward on eval: 3361.7920851754247[0m
[37m[1m[2023-07-03 00:55:35,423][188188] Min Reward on eval: 3361.7920851754247[0m
[37m[1m[2023-07-03 00:55:35,423][188188] Mean Reward across all agents: 3361.7920851754247[0m
[37m[1m[2023-07-03 00:55:35,423][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:55:40,389][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:55:40,389][188188] Reward + Measures: [[3237.56009292    0.16300002    0.23          0.2211        0.1099
     2.20955849]
 [3346.79743953    0.18620001    0.2737        0.25840002    0.13270001
     2.17668271]
 [2321.94581985    0.178         0.24870001    0.23979998    0.1595
     2.06866527]
 ...
 [2096.00959403    0.1516        0.21039999    0.18880001    0.12810001
     2.13701749]
 [3160.64150998    0.16330002    0.22720003    0.22059999    0.1112
     2.2143662 ]
 [2860.2539215     0.20680001    0.30509999    0.2606        0.18159999
     2.00116515]][0m
[37m[1m[2023-07-03 00:55:40,390][188188] Max Reward on eval: 3743.4141845838167[0m
[37m[1m[2023-07-03 00:55:40,390][188188] Min Reward on eval: 1542.1848030319438[0m
[37m[1m[2023-07-03 00:55:40,390][188188] Mean Reward across all agents: 2727.35563155517[0m
[37m[1m[2023-07-03 00:55:40,390][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:55:40,392][188188] mean_value=-768.8291761985394, max_value=357.94714484278165[0m
[37m[1m[2023-07-03 00:55:40,395][188188] New mean coefficients: [[ 0.58720607 -0.1578412   0.28392544 -0.58833766  0.3187812  -0.53429115]][0m
[37m[1m[2023-07-03 00:55:40,395][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:55:49,340][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 00:55:49,340][188188] FPS: 429393.82[0m
[36m[2023-07-03 00:55:49,343][188188] itr=27, itrs=2000, Progress: 1.35%[0m
[36m[2023-07-03 00:56:00,925][188188] train() took 11.57 seconds to complete[0m
[36m[2023-07-03 00:56:00,925][188188] FPS: 332022.39[0m
[36m[2023-07-03 00:56:05,151][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:56:05,152][188188] Reward + Measures: [[3455.00076056    0.16628599    0.21703532    0.21210966    0.10978165
     2.21143222]][0m
[37m[1m[2023-07-03 00:56:05,152][188188] Max Reward on eval: 3455.0007605577[0m
[37m[1m[2023-07-03 00:56:05,152][188188] Min Reward on eval: 3455.0007605577[0m
[37m[1m[2023-07-03 00:56:05,152][188188] Mean Reward across all agents: 3455.0007605577[0m
[37m[1m[2023-07-03 00:56:05,153][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:56:10,171][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:56:10,172][188188] Reward + Measures: [[2755.59394839    0.1552        0.18669999    0.1928        0.11800001
     2.10791135]
 [3094.27062229    0.17580001    0.21340001    0.21250001    0.11979999
     2.12893295]
 [1723.92896267    0.12249999    0.15180002    0.1559        0.1013
     2.10609984]
 ...
 [2651.05668832    0.14940001    0.19840001    0.1892        0.12459999
     2.14431763]
 [2245.62215044    0.13060001    0.16339999    0.16590001    0.0868
     2.11578465]
 [1778.21817974    0.14950001    0.16739999    0.1592        0.1237
     2.04320598]][0m
[37m[1m[2023-07-03 00:56:10,172][188188] Max Reward on eval: 3898.5126342520816[0m
[37m[1m[2023-07-03 00:56:10,172][188188] Min Reward on eval: 1257.2031430954114[0m
[37m[1m[2023-07-03 00:56:10,172][188188] Mean Reward across all agents: 2756.847289277367[0m
[37m[1m[2023-07-03 00:56:10,173][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:56:10,174][188188] mean_value=-787.6247142002583, max_value=351.9079550651177[0m
[37m[1m[2023-07-03 00:56:10,177][188188] New mean coefficients: [[ 0.29534352 -0.3835391  -0.0709424  -1.0203345  -0.40643257  0.33075827]][0m
[37m[1m[2023-07-03 00:56:10,178][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:56:19,116][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 00:56:19,117][188188] FPS: 429674.51[0m
[36m[2023-07-03 00:56:19,119][188188] itr=28, itrs=2000, Progress: 1.40%[0m
[36m[2023-07-03 00:56:30,713][188188] train() took 11.58 seconds to complete[0m
[36m[2023-07-03 00:56:30,713][188188] FPS: 331645.58[0m
[36m[2023-07-03 00:56:35,040][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:56:35,040][188188] Reward + Measures: [[3520.49151494    0.164683      0.21349034    0.20731033    0.10777166
     2.23138642]][0m
[37m[1m[2023-07-03 00:56:35,040][188188] Max Reward on eval: 3520.4915149423928[0m
[37m[1m[2023-07-03 00:56:35,041][188188] Min Reward on eval: 3520.4915149423928[0m
[37m[1m[2023-07-03 00:56:35,041][188188] Mean Reward across all agents: 3520.4915149423928[0m
[37m[1m[2023-07-03 00:56:35,041][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:56:40,108][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:56:40,115][188188] Reward + Measures: [[2740.90179823    0.16150001    0.2182        0.2124        0.14030001
     2.06251788]
 [3303.44583126    0.18189999    0.2326        0.23099999    0.1201
     2.13660502]
 [2390.85983848    0.14579999    0.18999998    0.18780001    0.1025
     2.19859481]
 ...
 [1660.83046721    0.12809999    0.1524        0.15000001    0.0949
     2.13685322]
 [3136.08395383    0.15290001    0.22679999    0.20810001    0.11740001
     2.24090815]
 [2217.9148102     0.13700001    0.21730001    0.19329999    0.1109
     2.21436381]][0m
[37m[1m[2023-07-03 00:56:40,116][188188] Max Reward on eval: 3987.979919458553[0m
[37m[1m[2023-07-03 00:56:40,116][188188] Min Reward on eval: 1177.5244789223652[0m
[37m[1m[2023-07-03 00:56:40,117][188188] Mean Reward across all agents: 2874.580859126145[0m
[37m[1m[2023-07-03 00:56:40,117][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:56:40,121][188188] mean_value=-735.8567430841413, max_value=374.32458524835147[0m
[37m[1m[2023-07-03 00:56:40,126][188188] New mean coefficients: [[ 0.29438373 -0.48230946 -0.435777   -0.4544819   0.03562108  0.5946752 ]][0m
[37m[1m[2023-07-03 00:56:40,128][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:56:49,188][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 00:56:49,188][188188] FPS: 423950.12[0m
[36m[2023-07-03 00:56:49,191][188188] itr=29, itrs=2000, Progress: 1.45%[0m
[36m[2023-07-03 00:57:00,830][188188] train() took 11.62 seconds to complete[0m
[36m[2023-07-03 00:57:00,830][188188] FPS: 330353.02[0m
[36m[2023-07-03 00:57:05,152][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:57:05,153][188188] Reward + Measures: [[3551.46984669    0.16282433    0.20821235    0.20307699    0.10506801
     2.26220798]][0m
[37m[1m[2023-07-03 00:57:05,153][188188] Max Reward on eval: 3551.4698466911036[0m
[37m[1m[2023-07-03 00:57:05,153][188188] Min Reward on eval: 3551.4698466911036[0m
[37m[1m[2023-07-03 00:57:05,153][188188] Mean Reward across all agents: 3551.4698466911036[0m
[37m[1m[2023-07-03 00:57:05,154][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:57:10,376][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:57:10,381][188188] Reward + Measures: [[2270.59100153    0.1689        0.1999        0.18080002    0.09860001
     2.08753371]
 [3564.64220432    0.17290001    0.20739999    0.21529999    0.10160001
     2.22673368]
 [3069.81887812    0.1762        0.22159998    0.24150001    0.1164
     2.20233417]
 ...
 [1908.37187767    0.12990001    0.16240001    0.1507        0.087
     2.11023998]
 [2449.56262966    0.17159998    0.2254        0.2174        0.12810001
     2.12864065]
 [3332.48356246    0.15970001    0.21339999    0.2184        0.11349999
     2.23040247]][0m
[37m[1m[2023-07-03 00:57:10,382][188188] Max Reward on eval: 4102.272369410563[0m
[37m[1m[2023-07-03 00:57:10,382][188188] Min Reward on eval: 1169.450034148572[0m
[37m[1m[2023-07-03 00:57:10,382][188188] Mean Reward across all agents: 3011.598272629773[0m
[37m[1m[2023-07-03 00:57:10,382][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:57:10,384][188188] mean_value=-739.7916884880494, max_value=350.06303585047317[0m
[37m[1m[2023-07-03 00:57:10,387][188188] New mean coefficients: [[-0.36223128 -1.0458455  -0.19651338  0.31065857 -0.17249832  0.52575654]][0m
[37m[1m[2023-07-03 00:57:10,388][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:57:19,417][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 00:57:19,417][188188] FPS: 425374.62[0m
[36m[2023-07-03 00:57:19,419][188188] itr=30, itrs=2000, Progress: 1.50%[0m
[37m[1m[2023-07-03 00:57:21,242][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000010[0m
[36m[2023-07-03 00:57:33,201][188188] train() took 11.64 seconds to complete[0m
[36m[2023-07-03 00:57:33,201][188188] FPS: 330030.64[0m
[36m[2023-07-03 00:57:37,507][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:57:37,508][188188] Reward + Measures: [[3487.9023645     0.16066666    0.20938233    0.205832      0.10851499
     2.28688073]][0m
[37m[1m[2023-07-03 00:57:37,508][188188] Max Reward on eval: 3487.9023645045445[0m
[37m[1m[2023-07-03 00:57:37,508][188188] Min Reward on eval: 3487.9023645045445[0m
[37m[1m[2023-07-03 00:57:37,509][188188] Mean Reward across all agents: 3487.9023645045445[0m
[37m[1m[2023-07-03 00:57:37,509][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:57:42,517][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:57:42,518][188188] Reward + Measures: [[3594.45376586    0.16110002    0.23089997    0.2095        0.1211
     2.31514859]
 [3592.44076535    0.1745        0.22430001    0.22490001    0.1191
     2.24040675]
 [3101.53939243    0.1675        0.23020001    0.20630001    0.13200001
     2.29891849]
 ...
 [3013.65426255    0.1481        0.19430001    0.1992        0.0882
     2.19407463]
 [2434.03742602    0.13060001    0.15769999    0.1532        0.0905
     2.22338104]
 [1954.75425715    0.1437        0.18390001    0.16960001    0.1235
     2.16631627]][0m
[37m[1m[2023-07-03 00:57:42,518][188188] Max Reward on eval: 4063.6689453387635[0m
[37m[1m[2023-07-03 00:57:42,518][188188] Min Reward on eval: 1439.5859508471563[0m
[37m[1m[2023-07-03 00:57:42,518][188188] Mean Reward across all agents: 2879.501744235612[0m
[37m[1m[2023-07-03 00:57:42,519][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:57:42,520][188188] mean_value=-978.2956861675352, max_value=205.87151493561623[0m
[37m[1m[2023-07-03 00:57:42,522][188188] New mean coefficients: [[ 0.23087922 -0.87120974 -0.13507378 -0.4471079  -0.11072351  0.31181386]][0m
[37m[1m[2023-07-03 00:57:42,523][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:57:51,578][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 00:57:51,578][188188] FPS: 424185.42[0m
[36m[2023-07-03 00:57:51,580][188188] itr=31, itrs=2000, Progress: 1.55%[0m
[36m[2023-07-03 00:58:03,122][188188] train() took 11.53 seconds to complete[0m
[36m[2023-07-03 00:58:03,123][188188] FPS: 333141.59[0m
[36m[2023-07-03 00:58:07,455][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:58:07,455][188188] Reward + Measures: [[3482.16199873    0.15869732    0.20622668    0.19971332    0.10596199
     2.29581523]][0m
[37m[1m[2023-07-03 00:58:07,455][188188] Max Reward on eval: 3482.1619987262475[0m
[37m[1m[2023-07-03 00:58:07,456][188188] Min Reward on eval: 3482.1619987262475[0m
[37m[1m[2023-07-03 00:58:07,456][188188] Mean Reward across all agents: 3482.1619987262475[0m
[37m[1m[2023-07-03 00:58:07,456][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:58:12,439][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:58:12,440][188188] Reward + Measures: [[2379.85465238    0.13590001    0.17230001    0.1693        0.09240001
     2.25059247]
 [3047.66074369    0.1574        0.1815        0.19310001    0.09460001
     2.29117203]
 [2246.59820739    0.1383        0.1621        0.156         0.1157
     2.16193652]
 ...
 [2648.87528228    0.1446        0.1849        0.1886        0.0927
     2.28735352]
 [2000.17090605    0.15100001    0.1815        0.17220001    0.13270001
     2.07394743]
 [1739.56328966    0.15750001    0.1657        0.15710001    0.13940001
     2.05139947]][0m
[37m[1m[2023-07-03 00:58:12,440][188188] Max Reward on eval: 3945.544952377025[0m
[37m[1m[2023-07-03 00:58:12,440][188188] Min Reward on eval: 1307.5799121848308[0m
[37m[1m[2023-07-03 00:58:12,441][188188] Mean Reward across all agents: 2863.7455128705415[0m
[37m[1m[2023-07-03 00:58:12,441][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:58:12,442][188188] mean_value=-1026.3268509009463, max_value=54.189395418608[0m
[37m[1m[2023-07-03 00:58:12,445][188188] New mean coefficients: [[ 0.5145126  -0.23669118 -0.4910621  -0.0670878  -0.5490954   0.16200233]][0m
[37m[1m[2023-07-03 00:58:12,446][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:58:21,475][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 00:58:21,476][188188] FPS: 425335.27[0m
[36m[2023-07-03 00:58:21,478][188188] itr=32, itrs=2000, Progress: 1.60%[0m
[36m[2023-07-03 00:58:32,951][188188] train() took 11.46 seconds to complete[0m
[36m[2023-07-03 00:58:32,951][188188] FPS: 335213.92[0m
[36m[2023-07-03 00:58:37,208][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:58:37,208][188188] Reward + Measures: [[3449.67802646    0.15600766    0.20088932    0.19322701    0.10118432
     2.29280639]][0m
[37m[1m[2023-07-03 00:58:37,208][188188] Max Reward on eval: 3449.6780264585323[0m
[37m[1m[2023-07-03 00:58:37,209][188188] Min Reward on eval: 3449.6780264585323[0m
[37m[1m[2023-07-03 00:58:37,209][188188] Mean Reward across all agents: 3449.6780264585323[0m
[37m[1m[2023-07-03 00:58:37,209][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:58:42,253][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:58:42,254][188188] Reward + Measures: [[2773.90660095    0.133         0.19219999    0.17550001    0.0978
     2.36019325]
 [2679.01403039    0.13340001    0.19780001    0.1851        0.10879999
     2.26600528]
 [2585.74480818    0.142         0.1912        0.17560001    0.1033
     2.24319935]
 ...
 [2554.50920485    0.17590001    0.20809999    0.21970001    0.1392
     2.06155038]
 [2868.39105223    0.16440003    0.2024        0.18540001    0.11110001
     2.14349365]
 [2313.99567415    0.15150002    0.18609999    0.1662        0.1054
     2.10927701]][0m
[37m[1m[2023-07-03 00:58:42,254][188188] Max Reward on eval: 4098.741226199828[0m
[37m[1m[2023-07-03 00:58:42,254][188188] Min Reward on eval: 838.4991064412519[0m
[37m[1m[2023-07-03 00:58:42,255][188188] Mean Reward across all agents: 2961.758482516124[0m
[37m[1m[2023-07-03 00:58:42,255][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:58:42,256][188188] mean_value=-937.3549768907412, max_value=199.62776679296257[0m
[37m[1m[2023-07-03 00:58:42,259][188188] New mean coefficients: [[-0.13386995 -0.08912191 -0.16975209  0.54852605 -0.2754531   0.15831265]][0m
[37m[1m[2023-07-03 00:58:42,260][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:58:51,285][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 00:58:51,285][188188] FPS: 425543.73[0m
[36m[2023-07-03 00:58:51,288][188188] itr=33, itrs=2000, Progress: 1.65%[0m
[36m[2023-07-03 00:59:02,960][188188] train() took 11.66 seconds to complete[0m
[36m[2023-07-03 00:59:02,960][188188] FPS: 329467.89[0m
[36m[2023-07-03 00:59:07,181][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:59:07,181][188188] Reward + Measures: [[3517.77863396    0.162421      0.20405765    0.19924733    0.10324567
     2.29712272]][0m
[37m[1m[2023-07-03 00:59:07,182][188188] Max Reward on eval: 3517.778633958607[0m
[37m[1m[2023-07-03 00:59:07,182][188188] Min Reward on eval: 3517.778633958607[0m
[37m[1m[2023-07-03 00:59:07,182][188188] Mean Reward across all agents: 3517.778633958607[0m
[37m[1m[2023-07-03 00:59:07,183][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:59:12,161][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:59:12,162][188188] Reward + Measures: [[3131.7272987     0.1715        0.1944        0.2067        0.10079999
     2.28913593]
 [3264.72827153    0.17130001    0.20200001    0.21350001    0.1024
     2.2285912 ]
 [2670.26699063    0.13810001    0.1928        0.16800001    0.11180001
     2.24949336]
 ...
 [3774.07534794    0.1693        0.23740001    0.23280001    0.14040001
     2.31918645]
 [3098.22352603    0.14649999    0.20469999    0.18780001    0.10780001
     2.22336411]
 [2911.19104009    0.16140001    0.20990001    0.1813        0.1026
     2.18325782]][0m
[37m[1m[2023-07-03 00:59:12,162][188188] Max Reward on eval: 4135.258544963423[0m
[37m[1m[2023-07-03 00:59:12,163][188188] Min Reward on eval: 1707.1419257334433[0m
[37m[1m[2023-07-03 00:59:12,163][188188] Mean Reward across all agents: 3062.261239450337[0m
[37m[1m[2023-07-03 00:59:12,163][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:59:12,164][188188] mean_value=-864.90407158242, max_value=208.09323393066643[0m
[37m[1m[2023-07-03 00:59:12,167][188188] New mean coefficients: [[ 0.2723159  -0.24377878 -0.15021914 -0.03965133 -0.12806058  0.7154933 ]][0m
[37m[1m[2023-07-03 00:59:12,168][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:59:21,220][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 00:59:21,220][188188] FPS: 424288.03[0m
[36m[2023-07-03 00:59:21,222][188188] itr=34, itrs=2000, Progress: 1.70%[0m
[36m[2023-07-03 00:59:32,757][188188] train() took 11.52 seconds to complete[0m
[36m[2023-07-03 00:59:32,757][188188] FPS: 333356.36[0m
[36m[2023-07-03 00:59:37,043][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:59:37,043][188188] Reward + Measures: [[3506.01992919    0.16084899    0.20302734    0.19686167    0.10264
     2.32179618]][0m
[37m[1m[2023-07-03 00:59:37,043][188188] Max Reward on eval: 3506.019929185651[0m
[37m[1m[2023-07-03 00:59:37,044][188188] Min Reward on eval: 3506.019929185651[0m
[37m[1m[2023-07-03 00:59:37,044][188188] Mean Reward across all agents: 3506.019929185651[0m
[37m[1m[2023-07-03 00:59:37,044][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:59:42,208][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:59:42,209][188188] Reward + Measures: [[3326.35886382    0.15390001    0.1952        0.18980001    0.093
     2.33260226]
 [3623.12062071    0.17569999    0.22040001    0.21259999    0.10710001
     2.33057404]
 [2743.65167995    0.1515        0.1728        0.1842        0.10469999
     2.25644612]
 ...
 [2654.82492067    0.16700001    0.17910001    0.18120001    0.11130001
     2.16194606]
 [3884.72064211    0.1811        0.23169999    0.22089998    0.11650001
     2.3447473 ]
 [3310.71798706    0.1584        0.2228        0.2043        0.102
     2.32446361]][0m
[37m[1m[2023-07-03 00:59:42,209][188188] Max Reward on eval: 4081.1372070083626[0m
[37m[1m[2023-07-03 00:59:42,210][188188] Min Reward on eval: 1878.5955238157883[0m
[37m[1m[2023-07-03 00:59:42,210][188188] Mean Reward across all agents: 3068.24753227071[0m
[37m[1m[2023-07-03 00:59:42,210][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:59:42,211][188188] mean_value=-928.9594350426877, max_value=83.93023969496517[0m
[37m[1m[2023-07-03 00:59:42,214][188188] New mean coefficients: [[-0.04107612  0.01142226 -0.10147642  0.18037975 -0.1141995   0.7217581 ]][0m
[37m[1m[2023-07-03 00:59:42,214][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:59:51,194][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 00:59:51,194][188188] FPS: 427705.52[0m
[36m[2023-07-03 00:59:51,197][188188] itr=35, itrs=2000, Progress: 1.75%[0m
[36m[2023-07-03 01:00:02,876][188188] train() took 11.66 seconds to complete[0m
[36m[2023-07-03 01:00:02,876][188188] FPS: 329251.70[0m
[36m[2023-07-03 01:00:07,150][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:00:07,151][188188] Reward + Measures: [[3487.99289898    0.16004133    0.20125267    0.19745798    0.102199
     2.3630054 ]][0m
[37m[1m[2023-07-03 01:00:07,151][188188] Max Reward on eval: 3487.992898979533[0m
[37m[1m[2023-07-03 01:00:07,151][188188] Min Reward on eval: 3487.992898979533[0m
[37m[1m[2023-07-03 01:00:07,151][188188] Mean Reward across all agents: 3487.992898979533[0m
[37m[1m[2023-07-03 01:00:07,152][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:00:12,178][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:00:12,179][188188] Reward + Measures: [[2384.42026524    0.12789999    0.17690001    0.19219999    0.10880001
     2.19615793]
 [3009.00248341    0.1462        0.1902        0.19270001    0.10089999
     2.28641963]
 [2931.44676591    0.1506        0.18270001    0.19780001    0.116
     2.31723952]
 ...
 [3171.30957031    0.1453        0.20190001    0.1957        0.1067
     2.26636887]
 [3265.50102616    0.15750001    0.19659999    0.193         0.1079
     2.30784035]
 [3093.30600738    0.1638        0.221         0.2119        0.12280001
     2.35981917]][0m
[37m[1m[2023-07-03 01:00:12,179][188188] Max Reward on eval: 4054.2978821208003[0m
[37m[1m[2023-07-03 01:00:12,180][188188] Min Reward on eval: 1771.7801437212154[0m
[37m[1m[2023-07-03 01:00:12,180][188188] Mean Reward across all agents: 3067.811836989626[0m
[37m[1m[2023-07-03 01:00:12,180][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:00:12,181][188188] mean_value=-942.163974381435, max_value=44.322070749739396[0m
[37m[1m[2023-07-03 01:00:12,184][188188] New mean coefficients: [[ 0.01387748 -0.00946689 -0.3890851   0.13262726  0.1671076   0.7516448 ]][0m
[37m[1m[2023-07-03 01:00:12,185][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:00:21,230][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 01:00:21,230][188188] FPS: 424598.69[0m
[36m[2023-07-03 01:00:21,233][188188] itr=36, itrs=2000, Progress: 1.80%[0m
[36m[2023-07-03 01:00:32,958][188188] train() took 11.71 seconds to complete[0m
[36m[2023-07-03 01:00:32,958][188188] FPS: 327993.15[0m
[36m[2023-07-03 01:00:37,202][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:00:37,203][188188] Reward + Measures: [[3424.10305783    0.15736367    0.19672868    0.19557033    0.09974267
     2.41596103]][0m
[37m[1m[2023-07-03 01:00:37,203][188188] Max Reward on eval: 3424.103057828124[0m
[37m[1m[2023-07-03 01:00:37,203][188188] Min Reward on eval: 3424.103057828124[0m
[37m[1m[2023-07-03 01:00:37,203][188188] Mean Reward across all agents: 3424.103057828124[0m
[37m[1m[2023-07-03 01:00:37,203][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:00:42,167][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:00:42,168][188188] Reward + Measures: [[2843.49749378    0.13750002    0.16849999    0.1743        0.0895
     2.41267705]
 [3139.94938278    0.1585        0.19920002    0.19170001    0.10820001
     2.38389444]
 [3678.67877198    0.16869999    0.22000001    0.2027        0.1053
     2.43034983]
 ...
 [4003.45224008    0.17010002    0.22230001    0.21589999    0.10300001
     2.45985341]
 [2913.77243041    0.1442        0.17549999    0.1804        0.0947
     2.39592957]
 [3637.31651311    0.16680001    0.21780001    0.20850001    0.09320001
     2.45712519]][0m
[37m[1m[2023-07-03 01:00:42,168][188188] Max Reward on eval: 4003.4522400842047[0m
[37m[1m[2023-07-03 01:00:42,168][188188] Min Reward on eval: 1688.0865402415395[0m
[37m[1m[2023-07-03 01:00:42,169][188188] Mean Reward across all agents: 3084.0898429854733[0m
[37m[1m[2023-07-03 01:00:42,169][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:00:42,170][188188] mean_value=-930.3181754605617, max_value=-10.955778361830198[0m
[36m[2023-07-03 01:00:42,172][188188] XNES is restarting with a new solution whose measures are [0.20449999 0.35510001 0.2746     0.23369999 2.08462715] and objective is 1741.595870957151[0m
[36m[2023-07-03 01:00:42,173][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:00:42,175][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:00:42,176][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:00:51,173][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 01:00:51,173][188188] FPS: 426882.46[0m
[36m[2023-07-03 01:00:51,175][188188] itr=37, itrs=2000, Progress: 1.85%[0m
[36m[2023-07-03 01:01:02,717][188188] train() took 11.53 seconds to complete[0m
[36m[2023-07-03 01:01:02,717][188188] FPS: 333146.31[0m
[36m[2023-07-03 01:01:06,946][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:01:06,947][188188] Reward + Measures: [[692.80150519   0.16363066   0.20055598   0.16794999   0.16195533
    2.42900586]][0m
[37m[1m[2023-07-03 01:01:06,947][188188] Max Reward on eval: 692.8015051888384[0m
[37m[1m[2023-07-03 01:01:06,947][188188] Min Reward on eval: 692.8015051888384[0m
[37m[1m[2023-07-03 01:01:06,947][188188] Mean Reward across all agents: 692.8015051888384[0m
[37m[1m[2023-07-03 01:01:06,948][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:01:11,888][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:01:11,888][188188] Reward + Measures: [[473.66899873   0.1471       0.1698       0.14040001   0.1372
    2.45042658]
 [479.76835677   0.14030001   0.1664       0.14909999   0.14060001
    2.41675782]
 [700.66221814   0.1698       0.20050001   0.1811       0.1585
    2.44346952]
 ...
 [611.27974445   0.1693       0.2059       0.1684       0.1737
    2.42907739]
 [623.44201566   0.16760001   0.2057       0.1548       0.1655
    2.4571135 ]
 [533.240551     0.132        0.1698       0.13410001   0.13240001
    2.45816016]][0m
[37m[1m[2023-07-03 01:01:11,888][188188] Max Reward on eval: 914.3249778824859[0m
[37m[1m[2023-07-03 01:01:11,889][188188] Min Reward on eval: 146.57779258452356[0m
[37m[1m[2023-07-03 01:01:11,889][188188] Mean Reward across all agents: 472.54930051265654[0m
[37m[1m[2023-07-03 01:01:11,889][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:01:11,890][188188] mean_value=-3534.347155813556, max_value=-2357.2291929299045[0m
[36m[2023-07-03 01:01:11,893][188188] XNES is restarting with a new solution whose measures are [0.20449999 0.35510001 0.2746     0.23369999 2.08462715] and objective is 1741.595870957151[0m
[36m[2023-07-03 01:01:11,894][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:01:11,896][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:01:11,897][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:01:20,757][188188] train() took 8.86 seconds to complete[0m
[36m[2023-07-03 01:01:20,757][188188] FPS: 433453.57[0m
[36m[2023-07-03 01:01:20,759][188188] itr=38, itrs=2000, Progress: 1.90%[0m
[36m[2023-07-03 01:01:32,325][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 01:01:32,326][188188] FPS: 332446.61[0m
[36m[2023-07-03 01:01:36,568][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:01:36,569][188188] Reward + Measures: [[574.74940433   0.139237     0.16455133   0.137477     0.13388334
    2.38565421]][0m
[37m[1m[2023-07-03 01:01:36,569][188188] Max Reward on eval: 574.7494043277401[0m
[37m[1m[2023-07-03 01:01:36,569][188188] Min Reward on eval: 574.7494043277401[0m
[37m[1m[2023-07-03 01:01:36,570][188188] Mean Reward across all agents: 574.7494043277401[0m
[37m[1m[2023-07-03 01:01:36,570][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:01:41,696][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:01:41,696][188188] Reward + Measures: [[775.99120717   0.1724       0.2138       0.1689       0.1559
    2.44287181]
 [294.04472926   0.1069       0.11290001   0.1015       0.10820001
    2.34944582]
 [363.50101476   0.11309999   0.14         0.12050001   0.102
    2.39527702]
 ...
 [602.35558315   0.13759999   0.16419999   0.1416       0.1332
    2.39032412]
 [400.84864477   0.12730001   0.1433       0.116        0.11009999
    2.39367867]
 [643.17253303   0.13869999   0.1706       0.14839999   0.13999999
    2.41484618]][0m
[37m[1m[2023-07-03 01:01:41,696][188188] Max Reward on eval: 846.904415124096[0m
[37m[1m[2023-07-03 01:01:41,697][188188] Min Reward on eval: 169.9712832154706[0m
[37m[1m[2023-07-03 01:01:41,697][188188] Mean Reward across all agents: 434.81165298759646[0m
[37m[1m[2023-07-03 01:01:41,697][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:01:41,698][188188] mean_value=-3579.5963654584393, max_value=-3167.503603321939[0m
[36m[2023-07-03 01:01:41,700][188188] XNES is restarting with a new solution whose measures are [0.17309999 0.2624     0.2388     0.11610001 2.24748564] and objective is 3784.336608915776[0m
[36m[2023-07-03 01:01:41,701][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:01:41,704][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:01:41,705][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:01:50,743][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 01:01:50,743][188188] FPS: 424916.72[0m
[36m[2023-07-03 01:01:50,746][188188] itr=39, itrs=2000, Progress: 1.95%[0m
[36m[2023-07-03 01:02:02,415][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 01:02:02,415][188188] FPS: 329539.14[0m
[36m[2023-07-03 01:02:06,657][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:02:06,657][188188] Reward + Measures: [[1845.55518815    0.12857433    0.159777      0.15609665    0.09715999
     2.23595381]][0m
[37m[1m[2023-07-03 01:02:06,658][188188] Max Reward on eval: 1845.5551881464396[0m
[37m[1m[2023-07-03 01:02:06,658][188188] Min Reward on eval: 1845.5551881464396[0m
[37m[1m[2023-07-03 01:02:06,658][188188] Mean Reward across all agents: 1845.5551881464396[0m
[37m[1m[2023-07-03 01:02:06,658][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:02:11,641][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:02:11,641][188188] Reward + Measures: [[787.78377529   0.28470001   0.25480002   0.29930001   0.21179998
    2.48185706]
 [869.35206602   0.1523       0.19100001   0.21139999   0.13630001
    2.46624374]
 [182.35872748   0.10390001   0.1551       0.131        0.1371
    2.96298862]
 ...
 [434.73797566   0.21659999   0.2102       0.25139999   0.15620001
    2.47747207]
 [541.09359261   0.24570003   0.17380001   0.24480002   0.12630001
    2.56967449]
 [390.82437135   0.08790001   0.14909999   0.13780001   0.13520001
    2.50575113]][0m
[37m[1m[2023-07-03 01:02:11,641][188188] Max Reward on eval: 2200.0887680485844[0m
[37m[1m[2023-07-03 01:02:11,642][188188] Min Reward on eval: -18.601505582220852[0m
[37m[1m[2023-07-03 01:02:11,642][188188] Mean Reward across all agents: 391.60562137753254[0m
[37m[1m[2023-07-03 01:02:11,642][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:02:11,644][188188] mean_value=-3399.3825352084955, max_value=635.1573887946084[0m
[37m[1m[2023-07-03 01:02:11,646][188188] New mean coefficients: [[ 0.6002805  -0.06781268 -0.6866852  -1.2881061  -0.77349865  0.07184756]][0m
[37m[1m[2023-07-03 01:02:11,647][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:02:20,702][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 01:02:20,703][188188] FPS: 424147.13[0m
[36m[2023-07-03 01:02:20,705][188188] itr=40, itrs=2000, Progress: 2.00%[0m
[37m[1m[2023-07-03 01:02:22,573][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000020[0m
[36m[2023-07-03 01:02:34,682][188188] train() took 11.80 seconds to complete[0m
[36m[2023-07-03 01:02:34,682][188188] FPS: 325424.14[0m
[36m[2023-07-03 01:02:38,924][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:02:38,924][188188] Reward + Measures: [[1874.4896901     0.127196      0.15583965    0.16187432    0.087158
     2.21329117]][0m
[37m[1m[2023-07-03 01:02:38,924][188188] Max Reward on eval: 1874.4896900998856[0m
[37m[1m[2023-07-03 01:02:38,925][188188] Min Reward on eval: 1874.4896900998856[0m
[37m[1m[2023-07-03 01:02:38,925][188188] Mean Reward across all agents: 1874.4896900998856[0m
[37m[1m[2023-07-03 01:02:38,925][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:02:43,948][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:02:43,954][188188] Reward + Measures: [[ 142.67143468    0.09729999    0.09540001    0.11010001    0.0933
     2.80363393]
 [ 285.64716527    0.11619999    0.1705        0.142         0.13380001
     2.90918207]
 [ 544.27671623    0.1636        0.19060001    0.2861        0.23109999
     2.88117099]
 ...
 [1262.25101666    0.17099999    0.2131        0.30230001    0.19040002
     2.42064452]
 [  92.81343012    0.1087        0.08980002    0.0901        0.0851
     2.94473386]
 [ 357.19812913    0.0971        0.12529999    0.1234        0.1251
     2.3843441 ]][0m
[37m[1m[2023-07-03 01:02:43,954][188188] Max Reward on eval: 1831.2817230043934[0m
[37m[1m[2023-07-03 01:02:43,954][188188] Min Reward on eval: 3.5825369080528615[0m
[37m[1m[2023-07-03 01:02:43,955][188188] Mean Reward across all agents: 405.0866299636055[0m
[37m[1m[2023-07-03 01:02:43,955][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:02:43,956][188188] mean_value=-3496.0731165316934, max_value=654.287383651859[0m
[37m[1m[2023-07-03 01:02:43,959][188188] New mean coefficients: [[ 1.4175828   0.01036894 -0.09269553  0.31639397 -0.7360873   0.55751204]][0m
[37m[1m[2023-07-03 01:02:43,960][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:02:52,947][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 01:02:52,947][188188] FPS: 427388.60[0m
[36m[2023-07-03 01:02:52,949][188188] itr=41, itrs=2000, Progress: 2.05%[0m
[36m[2023-07-03 01:03:04,540][188188] train() took 11.58 seconds to complete[0m
[36m[2023-07-03 01:03:04,540][188188] FPS: 331733.44[0m
[36m[2023-07-03 01:03:08,864][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:03:08,865][188188] Reward + Measures: [[1425.41174586    0.13772233    0.17837167    0.17085534    0.13596466
     2.21644664]][0m
[37m[1m[2023-07-03 01:03:08,865][188188] Max Reward on eval: 1425.4117458596243[0m
[37m[1m[2023-07-03 01:03:08,865][188188] Min Reward on eval: 1425.4117458596243[0m
[37m[1m[2023-07-03 01:03:08,866][188188] Mean Reward across all agents: 1425.4117458596243[0m
[37m[1m[2023-07-03 01:03:08,866][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:03:13,874][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:03:13,875][188188] Reward + Measures: [[384.38368363   0.184        0.2464       0.16320001   0.204
    2.63379097]
 [140.71712405   0.11260001   0.11390001   0.1321       0.1259
    3.20282936]
 [294.53067016   0.0976       0.119        0.1232       0.13040002
    2.57647395]
 ...
 [195.38677266   0.20750001   0.30469999   0.12450001   0.26910001
    2.81434178]
 [170.25985855   0.0855       0.1051       0.0954       0.0903
    2.98513269]
 [288.76468991   0.0978       0.1091       0.13399999   0.1017
    2.81005168]][0m
[37m[1m[2023-07-03 01:03:13,875][188188] Max Reward on eval: 1465.8944777851925[0m
[37m[1m[2023-07-03 01:03:13,875][188188] Min Reward on eval: 3.061170904804021[0m
[37m[1m[2023-07-03 01:03:13,876][188188] Mean Reward across all agents: 390.34041232347386[0m
[37m[1m[2023-07-03 01:03:13,876][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:03:13,877][188188] mean_value=-3491.0217757389064, max_value=-660.1743885816902[0m
[36m[2023-07-03 01:03:13,880][188188] XNES is restarting with a new solution whose measures are [0.20449999 0.35510001 0.2746     0.23369999 2.08462715] and objective is 1741.595870957151[0m
[36m[2023-07-03 01:03:13,881][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:03:13,883][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:03:13,884][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:03:22,852][188188] train() took 8.97 seconds to complete[0m
[36m[2023-07-03 01:03:22,852][188188] FPS: 428259.69[0m
[36m[2023-07-03 01:03:22,855][188188] itr=42, itrs=2000, Progress: 2.10%[0m
[36m[2023-07-03 01:03:34,480][188188] train() took 11.61 seconds to complete[0m
[36m[2023-07-03 01:03:34,481][188188] FPS: 330793.17[0m
[36m[2023-07-03 01:03:38,795][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:03:38,795][188188] Reward + Measures: [[932.84035015   0.18031067   0.24346898   0.19966333   0.18110266
    2.48527479]][0m
[37m[1m[2023-07-03 01:03:38,795][188188] Max Reward on eval: 932.840350149034[0m
[37m[1m[2023-07-03 01:03:38,796][188188] Min Reward on eval: 932.840350149034[0m
[37m[1m[2023-07-03 01:03:38,796][188188] Mean Reward across all agents: 932.840350149034[0m
[37m[1m[2023-07-03 01:03:38,796][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:03:43,822][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:03:43,823][188188] Reward + Measures: [[ 826.85481167    0.20830002    0.2626        0.21040002    0.1899
     2.51321769]
 [ 680.50048112    0.1445        0.18889999    0.17559998    0.15100001
     2.42826366]
 [ 559.93175171    0.14320001    0.175         0.15460001    0.14469999
     2.45827746]
 ...
 [1017.43844225    0.2041        0.2942        0.21300001    0.21600001
     2.49180651]
 [ 825.95124243    0.17999999    0.2554        0.193         0.1806
     2.48520017]
 [ 707.47853568    0.13440001    0.1735        0.1473        0.1277
     2.47639823]][0m
[37m[1m[2023-07-03 01:03:43,823][188188] Max Reward on eval: 1136.4307384297251[0m
[37m[1m[2023-07-03 01:03:43,823][188188] Min Reward on eval: 167.97488175085746[0m
[37m[1m[2023-07-03 01:03:43,824][188188] Mean Reward across all agents: 599.7245638192983[0m
[37m[1m[2023-07-03 01:03:43,824][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:03:43,825][188188] mean_value=-3342.8821734498374, max_value=-2126.8210680340453[0m
[36m[2023-07-03 01:03:43,827][188188] XNES is restarting with a new solution whose measures are [0.17309999 0.2624     0.2388     0.11610001 2.24748564] and objective is 3784.336608915776[0m
[36m[2023-07-03 01:03:43,828][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:03:43,831][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:03:43,831][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:03:52,907][188188] train() took 9.07 seconds to complete[0m
[36m[2023-07-03 01:03:52,907][188188] FPS: 423197.73[0m
[36m[2023-07-03 01:03:52,909][188188] itr=43, itrs=2000, Progress: 2.15%[0m
[36m[2023-07-03 01:04:04,605][188188] train() took 11.68 seconds to complete[0m
[36m[2023-07-03 01:04:04,606][188188] FPS: 328815.09[0m
[36m[2023-07-03 01:04:08,874][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:04:08,875][188188] Reward + Measures: [[2193.58643464    0.19094433    0.26179469    0.21881799    0.103569
     1.8966608 ]][0m
[37m[1m[2023-07-03 01:04:08,875][188188] Max Reward on eval: 2193.5864346442195[0m
[37m[1m[2023-07-03 01:04:08,875][188188] Min Reward on eval: 2193.5864346442195[0m
[37m[1m[2023-07-03 01:04:08,875][188188] Mean Reward across all agents: 2193.5864346442195[0m
[37m[1m[2023-07-03 01:04:08,876][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:04:14,005][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:04:14,006][188188] Reward + Measures: [[520.2875414    0.1416       0.16939999   0.14570001   0.10219999
    2.19434333]
 [738.91471285   0.1287       0.1653       0.1737       0.15629999
    2.34743309]
 [273.78835871   0.1569       0.25320002   0.18780002   0.2418
    2.89635539]
 ...
 [608.35692784   0.14660001   0.15019999   0.1328       0.113
    2.66069579]
 [536.74422263   0.15269999   0.19050001   0.14650001   0.15390001
    2.70450878]
 [764.03898719   0.1698       0.19719999   0.18349999   0.15939999
    2.45459867]][0m
[37m[1m[2023-07-03 01:04:14,006][188188] Max Reward on eval: 1821.4162406882738[0m
[37m[1m[2023-07-03 01:04:14,006][188188] Min Reward on eval: -42.5823463652283[0m
[37m[1m[2023-07-03 01:04:14,007][188188] Mean Reward across all agents: 337.05279699590244[0m
[37m[1m[2023-07-03 01:04:14,007][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:04:14,008][188188] mean_value=-3540.665197010492, max_value=480.6787525928575[0m
[37m[1m[2023-07-03 01:04:14,011][188188] New mean coefficients: [[ 1.2436776  -2.1516502  -0.47161892 -2.025159   -1.1569418  -0.8039235 ]][0m
[37m[1m[2023-07-03 01:04:14,011][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:04:23,040][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 01:04:23,041][188188] FPS: 425377.50[0m
[36m[2023-07-03 01:04:23,043][188188] itr=44, itrs=2000, Progress: 2.20%[0m
[36m[2023-07-03 01:04:34,577][188188] train() took 11.52 seconds to complete[0m
[36m[2023-07-03 01:04:34,577][188188] FPS: 333366.93[0m
[36m[2023-07-03 01:04:38,880][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:04:38,881][188188] Reward + Measures: [[1935.04255671    0.195071      0.28315368    0.20860836    0.16464834
     1.98936522]][0m
[37m[1m[2023-07-03 01:04:38,881][188188] Max Reward on eval: 1935.0425567147868[0m
[37m[1m[2023-07-03 01:04:38,881][188188] Min Reward on eval: 1935.0425567147868[0m
[37m[1m[2023-07-03 01:04:38,881][188188] Mean Reward across all agents: 1935.0425567147868[0m
[37m[1m[2023-07-03 01:04:38,881][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:04:43,905][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:04:43,905][188188] Reward + Measures: [[120.15537405   0.12210001   0.0947       0.13169999   0.10399999
    2.98233604]
 [203.29244408   0.1374       0.1344       0.09720001   0.12880002
    2.73970842]
 [ 27.63970633   0.0846       0.06940001   0.0773       0.0805
    3.06666183]
 ...
 [204.46644686   0.15390001   0.142        0.12539999   0.138
    2.64030147]
 [141.61346916   0.14470001   0.1375       0.1717       0.13350001
    2.90048862]
 [214.34845878   0.0945       0.0729       0.0975       0.08759999
    2.55227351]][0m
[37m[1m[2023-07-03 01:04:43,906][188188] Max Reward on eval: 1858.199516237632[0m
[37m[1m[2023-07-03 01:04:43,906][188188] Min Reward on eval: -6.973642454668879[0m
[37m[1m[2023-07-03 01:04:43,906][188188] Mean Reward across all agents: 322.9730068066365[0m
[37m[1m[2023-07-03 01:04:43,906][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:04:43,907][188188] mean_value=-3650.4180688273946, max_value=-1648.198084642057[0m
[36m[2023-07-03 01:04:43,909][188188] XNES is restarting with a new solution whose measures are [0.17309999 0.2624     0.2388     0.11610001 2.24748564] and objective is 3784.336608915776[0m
[36m[2023-07-03 01:04:43,910][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:04:43,913][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:04:43,914][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:04:52,955][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 01:04:52,955][188188] FPS: 424794.13[0m
[36m[2023-07-03 01:04:52,957][188188] itr=45, itrs=2000, Progress: 2.25%[0m
[36m[2023-07-03 01:05:04,826][188188] train() took 11.85 seconds to complete[0m
[36m[2023-07-03 01:05:04,826][188188] FPS: 324047.87[0m
[36m[2023-07-03 01:05:09,159][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:05:09,160][188188] Reward + Measures: [[1875.29186343    0.14576633    0.17671566    0.15116866    0.10593633
     2.04928875]][0m
[37m[1m[2023-07-03 01:05:09,160][188188] Max Reward on eval: 1875.2918634257946[0m
[37m[1m[2023-07-03 01:05:09,160][188188] Min Reward on eval: 1875.2918634257946[0m
[37m[1m[2023-07-03 01:05:09,160][188188] Mean Reward across all agents: 1875.2918634257946[0m
[37m[1m[2023-07-03 01:05:09,160][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:05:14,205][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:05:14,205][188188] Reward + Measures: [[493.57645798   0.14430001   0.1857       0.1743       0.16730002
    2.28706217]
 [925.77621652   0.1849       0.23509999   0.17949998   0.16440001
    2.4128983 ]
 [211.61332737   0.16520001   0.20540002   0.1425       0.13890001
    3.06368876]
 ...
 [598.12511441   0.16380002   0.18180001   0.2027       0.1348
    2.55674219]
 [812.6028042    0.20340002   0.22090001   0.2297       0.1568
    2.62996912]
 [666.82037642   0.13959999   0.17290001   0.13869999   0.1199
    2.58709931]][0m
[37m[1m[2023-07-03 01:05:14,206][188188] Max Reward on eval: 1621.3755645669996[0m
[37m[1m[2023-07-03 01:05:14,206][188188] Min Reward on eval: 43.929135508090255[0m
[37m[1m[2023-07-03 01:05:14,206][188188] Mean Reward across all agents: 415.10616001367504[0m
[37m[1m[2023-07-03 01:05:14,206][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:05:14,208][188188] mean_value=-3474.716063604104, max_value=-915.9009377841734[0m
[36m[2023-07-03 01:05:14,210][188188] XNES is restarting with a new solution whose measures are [0.52739996 0.42180005 0.51520002 0.0862     3.51523829] and objective is 154.05791189819575[0m
[36m[2023-07-03 01:05:14,211][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:05:14,214][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:05:14,214][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:05:23,155][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 01:05:23,156][188188] FPS: 429550.50[0m
[36m[2023-07-03 01:05:23,158][188188] itr=46, itrs=2000, Progress: 2.30%[0m
[36m[2023-07-03 01:05:34,722][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 01:05:34,723][188188] FPS: 332555.47[0m
[36m[2023-07-03 01:05:39,102][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:05:39,102][188188] Reward + Measures: [[106.16064737   0.17083801   0.16348267   0.17639367   0.15098533
    3.15124416]][0m
[37m[1m[2023-07-03 01:05:39,102][188188] Max Reward on eval: 106.16064736891344[0m
[37m[1m[2023-07-03 01:05:39,103][188188] Min Reward on eval: 106.16064736891344[0m
[37m[1m[2023-07-03 01:05:39,103][188188] Mean Reward across all agents: 106.16064736891344[0m
[37m[1m[2023-07-03 01:05:39,103][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:05:44,209][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:05:44,210][188188] Reward + Measures: [[ 81.86086381   0.11000001   0.1038       0.11740001   0.10390001
    3.20489097]
 [ 65.43614469   0.12049999   0.1276       0.1094       0.1038
    3.45316243]
 [ 32.07243322   0.13710001   0.15030001   0.14490001   0.1451
    3.31164861]
 ...
 [ 15.65561603   0.21589999   0.20439999   0.2086       0.17750001
    3.23535347]
 [ 63.2659075    0.1349       0.15090001   0.1476       0.12030001
    3.32253718]
 [-18.2513441    0.0913       0.1214       0.133        0.0987
    3.42224431]][0m
[37m[1m[2023-07-03 01:05:44,210][188188] Max Reward on eval: 262.6750366633758[0m
[37m[1m[2023-07-03 01:05:44,210][188188] Min Reward on eval: -102.47949697049334[0m
[37m[1m[2023-07-03 01:05:44,211][188188] Mean Reward across all agents: 66.24849175657549[0m
[37m[1m[2023-07-03 01:05:44,211][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:05:44,212][188188] mean_value=-3780.7085580311978, max_value=470.86780945338734[0m
[37m[1m[2023-07-03 01:05:44,215][188188] New mean coefficients: [[ 1.4732842   0.3114285  -1.4993446  -0.67126584 -2.1232178  -0.42280486]][0m
[37m[1m[2023-07-03 01:05:44,216][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:05:53,294][188188] train() took 9.08 seconds to complete[0m
[36m[2023-07-03 01:05:53,294][188188] FPS: 423084.86[0m
[36m[2023-07-03 01:05:53,296][188188] itr=47, itrs=2000, Progress: 2.35%[0m
[36m[2023-07-03 01:06:05,059][188188] train() took 11.75 seconds to complete[0m
[36m[2023-07-03 01:06:05,060][188188] FPS: 326879.96[0m
[36m[2023-07-03 01:06:09,398][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:06:09,398][188188] Reward + Measures: [[93.40383113  0.214552    0.19966568  0.20477702  0.185682    3.14828324]][0m
[37m[1m[2023-07-03 01:06:09,398][188188] Max Reward on eval: 93.40383113235771[0m
[37m[1m[2023-07-03 01:06:09,398][188188] Min Reward on eval: 93.40383113235771[0m
[37m[1m[2023-07-03 01:06:09,399][188188] Mean Reward across all agents: 93.40383113235771[0m
[37m[1m[2023-07-03 01:06:09,399][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:06:14,369][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:06:14,370][188188] Reward + Measures: [[ 69.05601054   0.18770002   0.22679999   0.1869       0.2045
    3.30340886]
 [104.23365113   0.1284       0.13160001   0.1542       0.1049
    3.36035705]
 [119.57822121   0.14909999   0.16760001   0.1427       0.1543
    3.42366457]
 ...
 [131.14830882   0.13869999   0.139        0.1672       0.1629
    3.48582196]
 [ 30.06247856   0.14579999   0.22040001   0.15150002   0.2192
    3.37378764]
 [ 46.88803957   0.10390001   0.13959999   0.1202       0.1006
    3.38297892]][0m
[37m[1m[2023-07-03 01:06:14,370][188188] Max Reward on eval: 268.870863000676[0m
[37m[1m[2023-07-03 01:06:14,370][188188] Min Reward on eval: -168.30133866108955[0m
[37m[1m[2023-07-03 01:06:14,371][188188] Mean Reward across all agents: 75.03416228638474[0m
[37m[1m[2023-07-03 01:06:14,371][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:06:14,373][188188] mean_value=-3450.462501923649, max_value=656.2568878624588[0m
[37m[1m[2023-07-03 01:06:14,375][188188] New mean coefficients: [[ 0.7707237   0.74923635 -0.8059731  -1.4279668  -3.376117    0.43489626]][0m
[37m[1m[2023-07-03 01:06:14,376][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:06:23,335][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 01:06:23,335][188188] FPS: 428728.38[0m
[36m[2023-07-03 01:06:23,337][188188] itr=48, itrs=2000, Progress: 2.40%[0m
[36m[2023-07-03 01:06:34,841][188188] train() took 11.49 seconds to complete[0m
[36m[2023-07-03 01:06:34,842][188188] FPS: 334234.76[0m
[36m[2023-07-03 01:06:39,069][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:06:39,070][188188] Reward + Measures: [[73.77781473  0.13543066  0.11304966  0.14216499  0.10875401  3.11073065]][0m
[37m[1m[2023-07-03 01:06:39,070][188188] Max Reward on eval: 73.77781472651036[0m
[37m[1m[2023-07-03 01:06:39,070][188188] Min Reward on eval: 73.77781472651036[0m
[37m[1m[2023-07-03 01:06:39,071][188188] Mean Reward across all agents: 73.77781472651036[0m
[37m[1m[2023-07-03 01:06:39,071][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:06:44,231][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:06:44,231][188188] Reward + Measures: [[24.3626386   0.09890001  0.10710001  0.10950001  0.1059      3.36061835]
 [34.31094616  0.13270001  0.15360001  0.14489999  0.13420001  3.51275563]
 [ 8.38677073  0.14390001  0.1096      0.14070001  0.12049999  3.50982094]
 ...
 [93.12905747  0.13860001  0.13590001  0.1671      0.1452      3.43889475]
 [72.67525383  0.0865      0.09680001  0.0988      0.0831      3.52477121]
 [96.06883499  0.20370002  0.13240001  0.14580001  0.19290002  3.51021767]][0m
[37m[1m[2023-07-03 01:06:44,232][188188] Max Reward on eval: 215.47932629766873[0m
[37m[1m[2023-07-03 01:06:44,232][188188] Min Reward on eval: -79.15194500219077[0m
[37m[1m[2023-07-03 01:06:44,232][188188] Mean Reward across all agents: 47.62987684173741[0m
[37m[1m[2023-07-03 01:06:44,232][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:06:44,234][188188] mean_value=-3870.4032409844376, max_value=138.00817526074886[0m
[37m[1m[2023-07-03 01:06:44,236][188188] New mean coefficients: [[ 0.83699816 -0.3355974  -1.7277694  -1.4503738  -2.8911304   1.292353  ]][0m
[37m[1m[2023-07-03 01:06:44,237][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:06:53,247][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 01:06:53,247][188188] FPS: 426285.97[0m
[36m[2023-07-03 01:06:53,249][188188] itr=49, itrs=2000, Progress: 2.45%[0m
[36m[2023-07-03 01:07:04,997][188188] train() took 11.73 seconds to complete[0m
[36m[2023-07-03 01:07:04,997][188188] FPS: 327335.88[0m
[36m[2023-07-03 01:07:09,266][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:07:09,267][188188] Reward + Measures: [[139.6105654    0.20567834   0.09407666   0.21930599   0.19471468
    3.43599868]][0m
[37m[1m[2023-07-03 01:07:09,267][188188] Max Reward on eval: 139.6105654027393[0m
[37m[1m[2023-07-03 01:07:09,267][188188] Min Reward on eval: 139.6105654027393[0m
[37m[1m[2023-07-03 01:07:09,267][188188] Mean Reward across all agents: 139.6105654027393[0m
[37m[1m[2023-07-03 01:07:09,268][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:07:14,238][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:07:14,239][188188] Reward + Measures: [[  9.20494783   0.1038       0.1074       0.1058       0.0884
    3.63465881]
 [ 21.81351478   0.10560001   0.10140001   0.1079       0.1079
    3.40329409]
 [-75.84085203   0.17560001   0.08570001   0.1278       0.1331
    3.63345194]
 ...
 [ 12.51861968   0.0737       0.0684       0.0887       0.0841
    3.53125453]
 [ -4.30333916   0.1239       0.12400001   0.12150001   0.10039999
    3.53296828]
 [135.81412945   0.25310001   0.2638       0.24159999   0.25180003
    3.46855044]][0m
[37m[1m[2023-07-03 01:07:14,239][188188] Max Reward on eval: 322.45730141401293[0m
[37m[1m[2023-07-03 01:07:14,239][188188] Min Reward on eval: -128.45984694017096[0m
[37m[1m[2023-07-03 01:07:14,240][188188] Mean Reward across all agents: 22.49943750982764[0m
[37m[1m[2023-07-03 01:07:14,240][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:07:14,242][188188] mean_value=-3608.6866918625046, max_value=499.2216320338659[0m
[37m[1m[2023-07-03 01:07:14,244][188188] New mean coefficients: [[ 1.660586    0.3099314  -1.6058241  -1.5911217  -3.9177315   0.21437132]][0m
[37m[1m[2023-07-03 01:07:14,245][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:07:23,180][188188] train() took 8.93 seconds to complete[0m
[36m[2023-07-03 01:07:23,180][188188] FPS: 429869.39[0m
[36m[2023-07-03 01:07:23,182][188188] itr=50, itrs=2000, Progress: 2.50%[0m
[37m[1m[2023-07-03 01:07:25,063][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000030[0m
[36m[2023-07-03 01:07:37,345][188188] train() took 11.97 seconds to complete[0m
[36m[2023-07-03 01:07:37,345][188188] FPS: 320705.03[0m
[36m[2023-07-03 01:07:41,588][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:07:41,593][188188] Reward + Measures: [[-54.3031274    0.09420534   0.11706866   0.10474332   0.103486
    3.3694036 ]][0m
[37m[1m[2023-07-03 01:07:41,594][188188] Max Reward on eval: -54.3031274013586[0m
[37m[1m[2023-07-03 01:07:41,594][188188] Min Reward on eval: -54.3031274013586[0m
[37m[1m[2023-07-03 01:07:41,594][188188] Mean Reward across all agents: -54.3031274013586[0m
[37m[1m[2023-07-03 01:07:41,594][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:07:46,566][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:07:46,567][188188] Reward + Measures: [[  17.01372393    0.1127        0.1348        0.1392        0.1637
     3.41617203]
 [ -21.82019693    0.1366        0.16760001    0.17310001    0.17120001
     3.56926727]
 [-128.71919489    0.11240001    0.1128        0.13689999    0.1348
     3.53251195]
 ...
 [   1.62157256    0.09710001    0.12050001    0.1139        0.12840001
     3.43839955]
 [ -87.74992372    0.12720001    0.1365        0.1346        0.13430001
     3.58327651]
 [   9.54042765    0.102         0.10569999    0.0965        0.1066
     3.59297752]][0m
[37m[1m[2023-07-03 01:07:46,567][188188] Max Reward on eval: 383.7018057966605[0m
[37m[1m[2023-07-03 01:07:46,567][188188] Min Reward on eval: -385.31515375380405[0m
[37m[1m[2023-07-03 01:07:46,568][188188] Mean Reward across all agents: -31.40505872126494[0m
[37m[1m[2023-07-03 01:07:46,568][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:07:46,570][188188] mean_value=-3777.871622497466, max_value=425.74176227778196[0m
[37m[1m[2023-07-03 01:07:46,572][188188] New mean coefficients: [[ 0.57177067 -1.5066228  -0.15572631 -1.395173   -3.1264849   0.26358542]][0m
[37m[1m[2023-07-03 01:07:46,573][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:07:55,511][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 01:07:55,511][188188] FPS: 429725.64[0m
[36m[2023-07-03 01:07:55,514][188188] itr=51, itrs=2000, Progress: 2.55%[0m
[36m[2023-07-03 01:08:07,099][188188] train() took 11.57 seconds to complete[0m
[36m[2023-07-03 01:08:07,099][188188] FPS: 331956.87[0m
[36m[2023-07-03 01:08:11,336][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:08:11,336][188188] Reward + Measures: [[-55.06950978   0.11066166   0.129768     0.11009067   0.12137866
    3.39925981]][0m
[37m[1m[2023-07-03 01:08:11,337][188188] Max Reward on eval: -55.06950977645694[0m
[37m[1m[2023-07-03 01:08:11,337][188188] Min Reward on eval: -55.06950977645694[0m
[37m[1m[2023-07-03 01:08:11,337][188188] Mean Reward across all agents: -55.06950977645694[0m
[37m[1m[2023-07-03 01:08:11,338][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:08:16,348][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:08:16,348][188188] Reward + Measures: [[ 15.35785543   0.1097       0.12609999   0.087        0.1007
    3.67007995]
 [-84.48518221   0.0733       0.15629999   0.14740001   0.15869999
    3.63957024]
 [ 73.75484276   0.1066       0.0987       0.1084       0.0969
    3.37911344]
 ...
 [ 94.25933679   0.0771       0.0961       0.1104       0.0868
    3.43895721]
 [ 14.59490834   0.0939       0.10210001   0.0949       0.1069
    3.39688468]
 [ 48.44757321   0.11850001   0.1207       0.10680001   0.12990001
    3.49714971]][0m
[37m[1m[2023-07-03 01:08:16,349][188188] Max Reward on eval: 156.66903399392032[0m
[37m[1m[2023-07-03 01:08:16,349][188188] Min Reward on eval: -140.39987575290723[0m
[37m[1m[2023-07-03 01:08:16,349][188188] Mean Reward across all agents: -12.282877060914567[0m
[37m[1m[2023-07-03 01:08:16,349][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:08:16,350][188188] mean_value=-3987.2361380366833, max_value=-1225.2425376721553[0m
[36m[2023-07-03 01:08:16,353][188188] XNES is restarting with a new solution whose measures are [0.72040004 0.0267     0.7141     0.67950004 3.92822576] and objective is -385.31515375380405[0m
[36m[2023-07-03 01:08:16,354][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:08:16,356][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:08:16,357][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:08:25,343][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 01:08:25,343][188188] FPS: 427385.60[0m
[36m[2023-07-03 01:08:25,346][188188] itr=52, itrs=2000, Progress: 2.60%[0m
[36m[2023-07-03 01:08:37,014][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 01:08:37,014][188188] FPS: 329585.93[0m
[36m[2023-07-03 01:08:41,272][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:08:41,278][188188] Reward + Measures: [[-51.31287907   0.13245633   0.15669167   0.13869333   0.13334766
    3.68273759]][0m
[37m[1m[2023-07-03 01:08:41,278][188188] Max Reward on eval: -51.3128790671288[0m
[37m[1m[2023-07-03 01:08:41,278][188188] Min Reward on eval: -51.3128790671288[0m
[37m[1m[2023-07-03 01:08:41,279][188188] Mean Reward across all agents: -51.3128790671288[0m
[37m[1m[2023-07-03 01:08:41,279][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:08:46,371][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:08:46,377][188188] Reward + Measures: [[  -3.78916721    0.1103        0.10089999    0.0838        0.0963
     3.73935556]
 [  20.63613839    0.09100001    0.08650001    0.1           0.08639999
     3.52982402]
 [  13.07614344    0.127         0.21109998    0.17240001    0.205
     3.72623563]
 ...
 [ -53.78925872    0.20729999    0.12890001    0.14749999    0.1208
     3.71942687]
 [-322.62767459    0.52689999    0.0457        0.52969998    0.48240003
     3.93393946]
 [ -86.80449932    0.1084        0.20630001    0.153         0.1904
     3.59725451]][0m
[37m[1m[2023-07-03 01:08:46,377][188188] Max Reward on eval: 276.48776150164196[0m
[37m[1m[2023-07-03 01:08:46,377][188188] Min Reward on eval: -754.4822578598745[0m
[37m[1m[2023-07-03 01:08:46,378][188188] Mean Reward across all agents: -44.91497440130683[0m
[37m[1m[2023-07-03 01:08:46,378][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:08:46,383][188188] mean_value=-2815.7155439913035, max_value=640.5180664885277[0m
[37m[1m[2023-07-03 01:08:46,386][188188] New mean coefficients: [[-0.9485347 -0.4916897 -1.1307179 -3.0171318 -3.3384867 -1.1108787]][0m
[37m[1m[2023-07-03 01:08:46,387][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:08:55,447][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 01:08:55,447][188188] FPS: 423919.23[0m
[36m[2023-07-03 01:08:55,450][188188] itr=53, itrs=2000, Progress: 2.65%[0m
[36m[2023-07-03 01:09:07,107][188188] train() took 11.64 seconds to complete[0m
[36m[2023-07-03 01:09:07,107][188188] FPS: 329889.68[0m
[36m[2023-07-03 01:09:11,399][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:09:11,400][188188] Reward + Measures: [[-21.46372292   0.14099467   0.21827465   0.21083532   0.22427131
    3.81185436]][0m
[37m[1m[2023-07-03 01:09:11,400][188188] Max Reward on eval: -21.463722920728546[0m
[37m[1m[2023-07-03 01:09:11,400][188188] Min Reward on eval: -21.463722920728546[0m
[37m[1m[2023-07-03 01:09:11,401][188188] Mean Reward across all agents: -21.463722920728546[0m
[37m[1m[2023-07-03 01:09:11,401][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:09:16,550][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:09:16,551][188188] Reward + Measures: [[249.51660056   0.63659996   0.05929999   0.63090003   0.57809997
    3.91177297]
 [-26.51116023   0.1027       0.18130001   0.13790001   0.1759
    3.74576807]
 [-22.95325087   0.15910001   0.1418       0.1363       0.1163
    3.80970621]
 ...
 [ 51.1322238    0.16230001   0.25999999   0.27560002   0.35520002
    3.83520579]
 [-29.5979092    0.18770002   0.0637       0.15549999   0.1696
    3.88612556]
 [  8.91261546   0.0838       0.17940001   0.12379999   0.16850001
    3.74690676]][0m
[37m[1m[2023-07-03 01:09:16,551][188188] Max Reward on eval: 689.1161766018835[0m
[37m[1m[2023-07-03 01:09:16,551][188188] Min Reward on eval: -915.968315136712[0m
[37m[1m[2023-07-03 01:09:16,552][188188] Mean Reward across all agents: -34.91833773254408[0m
[37m[1m[2023-07-03 01:09:16,552][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:09:16,555][188188] mean_value=-2692.05887293103, max_value=1076.7368443341256[0m
[37m[1m[2023-07-03 01:09:16,558][188188] New mean coefficients: [[ 0.4216019   0.47795358  1.1059037  -3.5497994  -4.192942   -1.7982233 ]][0m
[37m[1m[2023-07-03 01:09:16,559][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:09:25,596][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 01:09:25,597][188188] FPS: 424964.06[0m
[36m[2023-07-03 01:09:25,599][188188] itr=54, itrs=2000, Progress: 2.70%[0m
[36m[2023-07-03 01:09:37,367][188188] train() took 11.75 seconds to complete[0m
[36m[2023-07-03 01:09:37,368][188188] FPS: 326732.84[0m
[36m[2023-07-03 01:09:41,655][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:09:41,656][188188] Reward + Measures: [[34.69524401  0.34147766  0.10840666  0.32736665  0.26049501  3.88638544]][0m
[37m[1m[2023-07-03 01:09:41,656][188188] Max Reward on eval: 34.695244009840124[0m
[37m[1m[2023-07-03 01:09:41,656][188188] Min Reward on eval: 34.695244009840124[0m
[37m[1m[2023-07-03 01:09:41,657][188188] Mean Reward across all agents: 34.695244009840124[0m
[37m[1m[2023-07-03 01:09:41,657][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:09:46,683][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:09:46,684][188188] Reward + Measures: [[ 42.99906087   0.0992       0.08490001   0.10120001   0.0781
    3.70220041]
 [ -6.59955196   0.133        0.10030001   0.1202       0.0971
    3.85102773]
 [-18.91033908   0.08270001   0.0697       0.0713       0.0675
    3.68076324]
 ...
 [ -9.20885279   0.10090001   0.09810001   0.10880001   0.10829999
    3.81802988]
 [-87.28096904   0.12149999   0.24190001   0.20060001   0.21690002
    3.79052663]
 [ 22.92150561   0.11320001   0.1033       0.11310001   0.106
    3.68731856]][0m
[37m[1m[2023-07-03 01:09:46,684][188188] Max Reward on eval: 412.94411180354655[0m
[37m[1m[2023-07-03 01:09:46,684][188188] Min Reward on eval: -336.04685414375274[0m
[37m[1m[2023-07-03 01:09:46,684][188188] Mean Reward across all agents: 5.754302051832318[0m
[37m[1m[2023-07-03 01:09:46,684][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:09:46,686][188188] mean_value=-3519.8896214099445, max_value=263.0517751912566[0m
[37m[1m[2023-07-03 01:09:46,688][188188] New mean coefficients: [[-0.32605737 -0.57803166 -0.7912668  -2.4123888  -4.1750836  -2.5914192 ]][0m
[37m[1m[2023-07-03 01:09:46,689][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:09:55,820][188188] train() took 9.13 seconds to complete[0m
[36m[2023-07-03 01:09:55,820][188188] FPS: 420651.44[0m
[36m[2023-07-03 01:09:55,822][188188] itr=55, itrs=2000, Progress: 2.75%[0m
[36m[2023-07-03 01:10:07,319][188188] train() took 11.48 seconds to complete[0m
[36m[2023-07-03 01:10:07,320][188188] FPS: 334461.77[0m
[36m[2023-07-03 01:10:11,569][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:10:11,570][188188] Reward + Measures: [[2.0063006  0.13995366 0.16476865 0.16894001 0.16460866 3.85606408]][0m
[37m[1m[2023-07-03 01:10:11,570][188188] Max Reward on eval: 2.006300602579807[0m
[37m[1m[2023-07-03 01:10:11,570][188188] Min Reward on eval: 2.006300602579807[0m
[37m[1m[2023-07-03 01:10:11,571][188188] Mean Reward across all agents: 2.006300602579807[0m
[37m[1m[2023-07-03 01:10:11,571][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:10:16,536][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:10:16,536][188188] Reward + Measures: [[42.83122481  0.077       0.09240001  0.10650001  0.0957      3.69067454]
 [19.03463336  0.0954      0.0909      0.10630001  0.0658      3.8091774 ]
 [ 6.82218688  0.18609999  0.1851      0.25430003  0.17030001  3.93570876]
 ...
 [32.24177084  0.1452      0.1258      0.1322      0.1427      3.68767619]
 [22.35540078  0.067       0.0931      0.0977      0.1017      3.59014964]
 [21.47688254  0.1596      0.1489      0.1813      0.1576      3.56163144]][0m
[37m[1m[2023-07-03 01:10:16,537][188188] Max Reward on eval: 112.5922391295433[0m
[37m[1m[2023-07-03 01:10:16,537][188188] Min Reward on eval: -158.54849511273204[0m
[37m[1m[2023-07-03 01:10:16,537][188188] Mean Reward across all agents: 4.899176852431924[0m
[37m[1m[2023-07-03 01:10:16,537][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:10:16,539][188188] mean_value=-3630.0134263532195, max_value=-162.3394852745041[0m
[36m[2023-07-03 01:10:16,541][188188] XNES is restarting with a new solution whose measures are [0.99440002 0.0044     0.97579998 0.96439993 3.9757812 ] and objective is -415.6331386616512[0m
[36m[2023-07-03 01:10:16,542][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:10:16,545][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:10:16,545][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:10:25,448][188188] train() took 8.90 seconds to complete[0m
[36m[2023-07-03 01:10:25,448][188188] FPS: 431399.15[0m
[36m[2023-07-03 01:10:25,451][188188] itr=56, itrs=2000, Progress: 2.80%[0m
[36m[2023-07-03 01:10:37,055][188188] train() took 11.59 seconds to complete[0m
[36m[2023-07-03 01:10:37,056][188188] FPS: 331411.66[0m
[36m[2023-07-03 01:10:41,317][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:10:41,317][188188] Reward + Measures: [[-223.43478403    0.59332663    0.07620066    0.59582269    0.54529071
     3.85062623]][0m
[37m[1m[2023-07-03 01:10:41,318][188188] Max Reward on eval: -223.43478402644652[0m
[37m[1m[2023-07-03 01:10:41,318][188188] Min Reward on eval: -223.43478402644652[0m
[37m[1m[2023-07-03 01:10:41,318][188188] Mean Reward across all agents: -223.43478402644652[0m
[37m[1m[2023-07-03 01:10:41,318][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:10:46,308][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:10:46,309][188188] Reward + Measures: [[  31.18089698    0.1091        0.32960001    0.2658        0.29360002
     3.78684664]
 [   7.34189713    0.15580001    0.0901        0.1178        0.12590002
     3.67192507]
 [  11.25779497    0.75099993    0.71040004    0.73720002    0.75550002
     3.91151667]
 ...
 [-483.95087051    0.7227        0.1248        0.75710005    0.75920004
     3.90859079]
 [-222.85322741    0.4355        0.28439999    0.61379999    0.60980004
     3.85825801]
 [-322.18168311    0.50910002    0.16720001    0.55829996    0.59060001
     3.90538406]][0m
[37m[1m[2023-07-03 01:10:46,309][188188] Max Reward on eval: 697.4066772377118[0m
[37m[1m[2023-07-03 01:10:46,309][188188] Min Reward on eval: -740.2786607884802[0m
[37m[1m[2023-07-03 01:10:46,309][188188] Mean Reward across all agents: -33.24565114805424[0m
[37m[1m[2023-07-03 01:10:46,310][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:10:46,314][188188] mean_value=-2270.3962117789847, max_value=1188.969991103877[0m
[37m[1m[2023-07-03 01:10:46,317][188188] New mean coefficients: [[ 0.5608589  -0.7457194  -0.8232049  -1.6132219  -4.0491357   0.17311358]][0m
[37m[1m[2023-07-03 01:10:46,318][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:10:55,371][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 01:10:55,371][188188] FPS: 424246.58[0m
[36m[2023-07-03 01:10:55,374][188188] itr=57, itrs=2000, Progress: 2.85%[0m
[36m[2023-07-03 01:11:06,912][188188] train() took 11.52 seconds to complete[0m
[36m[2023-07-03 01:11:06,912][188188] FPS: 333341.59[0m
[36m[2023-07-03 01:11:11,154][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:11:11,154][188188] Reward + Measures: [[-19.37540223   0.15752533   0.10148466   0.16066432   0.10509866
    3.79795575]][0m
[37m[1m[2023-07-03 01:11:11,155][188188] Max Reward on eval: -19.375402232639704[0m
[37m[1m[2023-07-03 01:11:11,155][188188] Min Reward on eval: -19.375402232639704[0m
[37m[1m[2023-07-03 01:11:11,155][188188] Mean Reward across all agents: -19.375402232639704[0m
[37m[1m[2023-07-03 01:11:11,155][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:11:16,272][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:11:16,279][188188] Reward + Measures: [[ 51.33628194   0.06220001   0.0763       0.07250001   0.0577
    3.65816689]
 [ 19.51053361   0.0947       0.0868       0.16059999   0.108
    3.77749825]
 [-15.34047985   0.11290001   0.0796       0.16690001   0.1202
    3.82171702]
 ...
 [ 34.74623265   0.23150001   0.1181       0.27760002   0.2581
    3.76523185]
 [-38.12473074   0.24660002   0.24270001   0.2525       0.24130002
    3.73172188]
 [-28.81386994   0.0891       0.06940001   0.19679999   0.0861
    3.84964943]][0m
[37m[1m[2023-07-03 01:11:16,279][188188] Max Reward on eval: 254.40118639115244[0m
[37m[1m[2023-07-03 01:11:16,279][188188] Min Reward on eval: -275.6311511675769[0m
[37m[1m[2023-07-03 01:11:16,280][188188] Mean Reward across all agents: 13.34072255311336[0m
[37m[1m[2023-07-03 01:11:16,280][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:11:16,281][188188] mean_value=-3320.2495115615943, max_value=168.009887079694[0m
[37m[1m[2023-07-03 01:11:16,284][188188] New mean coefficients: [[ 1.275256    1.4656756  -1.6149147  -0.23639357 -5.059547    0.36795357]][0m
[37m[1m[2023-07-03 01:11:16,285][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:11:25,248][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 01:11:25,248][188188] FPS: 428493.11[0m
[36m[2023-07-03 01:11:25,251][188188] itr=58, itrs=2000, Progress: 2.90%[0m
[36m[2023-07-03 01:11:36,768][188188] train() took 11.50 seconds to complete[0m
[36m[2023-07-03 01:11:36,768][188188] FPS: 333870.58[0m
[36m[2023-07-03 01:11:41,012][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:11:41,012][188188] Reward + Measures: [[3.45033757 0.10347433 0.09218933 0.12040733 0.09264867 3.73452473]][0m
[37m[1m[2023-07-03 01:11:41,012][188188] Max Reward on eval: 3.450337565341819[0m
[37m[1m[2023-07-03 01:11:41,012][188188] Min Reward on eval: 3.450337565341819[0m
[37m[1m[2023-07-03 01:11:41,013][188188] Mean Reward across all agents: 3.450337565341819[0m
[37m[1m[2023-07-03 01:11:41,013][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:11:46,030][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:11:46,030][188188] Reward + Measures: [[  63.63213693    0.20610002    0.13399999    0.16250001    0.22720002
     3.86733747]
 [  85.27972298    0.11719999    0.0737        0.0895        0.101
     3.90038037]
 [ 140.88307378    0.79209995    0.14960001    0.69870001    0.76800007
     3.98609614]
 ...
 [ 165.70861535    0.40920001    0.2392        0.27509999    0.40939999
     3.94748378]
 [-227.52331303    0.48630005    0.0537        0.48270002    0.45369998
     3.92374349]
 [ -77.93165816    0.44510004    0.0869        0.41479999    0.4007
     3.84221148]][0m
[37m[1m[2023-07-03 01:11:46,030][188188] Max Reward on eval: 432.73826594497075[0m
[37m[1m[2023-07-03 01:11:46,031][188188] Min Reward on eval: -382.96540536098183[0m
[37m[1m[2023-07-03 01:11:46,031][188188] Mean Reward across all agents: 27.64729169054189[0m
[37m[1m[2023-07-03 01:11:46,031][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:11:46,034][188188] mean_value=-3035.9058747262056, max_value=727.4788636856014[0m
[37m[1m[2023-07-03 01:11:46,037][188188] New mean coefficients: [[ 2.1283503   3.785788   -0.87104887  0.2631904  -6.4380713   0.3285848 ]][0m
[37m[1m[2023-07-03 01:11:46,038][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:11:55,077][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 01:11:55,077][188188] FPS: 424909.45[0m
[36m[2023-07-03 01:11:55,079][188188] itr=59, itrs=2000, Progress: 2.95%[0m
[36m[2023-07-03 01:12:06,725][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 01:12:06,726][188188] FPS: 330224.72[0m
[36m[2023-07-03 01:12:10,945][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:12:10,945][188188] Reward + Measures: [[-96.20894468   0.26021668   0.09766001   0.29173967   0.22325167
    3.84035683]][0m
[37m[1m[2023-07-03 01:12:10,945][188188] Max Reward on eval: -96.20894467693026[0m
[37m[1m[2023-07-03 01:12:10,946][188188] Min Reward on eval: -96.20894467693026[0m
[37m[1m[2023-07-03 01:12:10,946][188188] Mean Reward across all agents: -96.20894467693026[0m
[37m[1m[2023-07-03 01:12:10,946][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:12:15,968][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:12:15,974][188188] Reward + Measures: [[ 219.75086652    0.54250002    0.0634        0.54590005    0.49430004
     3.89906812]
 [  -6.28838067    0.1206        0.1059        0.16410001    0.0895
     3.83049655]
 [-183.48578232    0.31909999    0.1001        0.28770003    0.2814
     3.83112717]
 ...
 [  51.2885549     0.22720002    0.13169999    0.2622        0.26989999
     3.78309894]
 [ -77.76728421    0.2138        0.1559        0.22070001    0.20480001
     3.81059122]
 [ -48.32761741    0.1628        0.11719999    0.12980001    0.09420001
     3.92333722]][0m
[37m[1m[2023-07-03 01:12:15,974][188188] Max Reward on eval: 219.75086651965975[0m
[37m[1m[2023-07-03 01:12:15,975][188188] Min Reward on eval: -441.3281610123813[0m
[37m[1m[2023-07-03 01:12:15,975][188188] Mean Reward across all agents: -41.619280847790264[0m
[37m[1m[2023-07-03 01:12:15,975][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:12:15,976][188188] mean_value=-3253.9879490059784, max_value=-11.739275610675207[0m
[36m[2023-07-03 01:12:15,979][188188] XNES is restarting with a new solution whose measures are [0.18010001 0.27309999 0.26280001 0.15030001 2.38573027] and objective is 3409.690246542171[0m
[36m[2023-07-03 01:12:15,980][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:12:15,982][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:12:15,983][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:12:24,943][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 01:12:24,943][188188] FPS: 428637.58[0m
[36m[2023-07-03 01:12:24,946][188188] itr=60, itrs=2000, Progress: 3.00%[0m
[37m[1m[2023-07-03 01:12:26,909][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000040[0m
[36m[2023-07-03 01:12:38,870][188188] train() took 11.64 seconds to complete[0m
[36m[2023-07-03 01:12:38,870][188188] FPS: 329919.84[0m
[36m[2023-07-03 01:12:43,151][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:12:43,151][188188] Reward + Measures: [[2293.71959115    0.15051399    0.21018168    0.19087733    0.11000234
     2.37336421]][0m
[37m[1m[2023-07-03 01:12:43,152][188188] Max Reward on eval: 2293.7195911474514[0m
[37m[1m[2023-07-03 01:12:43,152][188188] Min Reward on eval: 2293.7195911474514[0m
[37m[1m[2023-07-03 01:12:43,152][188188] Mean Reward across all agents: 2293.7195911474514[0m
[37m[1m[2023-07-03 01:12:43,152][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:12:48,136][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:12:48,136][188188] Reward + Measures: [[2127.11106492    0.16900001    0.22950001    0.20369999    0.11540001
     2.43081832]
 [1537.68833349    0.12710001    0.17150001    0.16700001    0.11310001
     2.29708505]
 [1396.11130146    0.1311        0.15720001    0.1515        0.0931
     2.32914329]
 ...
 [1757.12080954    0.1304        0.1811        0.16530001    0.0975
     2.26281524]
 [1467.07287027    0.13          0.17310001    0.1487        0.09170001
     2.31021976]
 [1085.9113846     0.1028        0.1287        0.1211        0.08440001
     2.31618094]][0m
[37m[1m[2023-07-03 01:12:48,136][188188] Max Reward on eval: 2641.7766724088697[0m
[37m[1m[2023-07-03 01:12:48,137][188188] Min Reward on eval: 641.681650152523[0m
[37m[1m[2023-07-03 01:12:48,137][188188] Mean Reward across all agents: 1689.791547903417[0m
[37m[1m[2023-07-03 01:12:48,137][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:12:48,138][188188] mean_value=-2291.7043875444783, max_value=-1097.481058829726[0m
[36m[2023-07-03 01:12:48,141][188188] XNES is restarting with a new solution whose measures are [0.88859999 0.09689999 0.76810002 0.83830005 3.99279475] and objective is 227.4788636856014[0m
[36m[2023-07-03 01:12:48,142][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:12:48,144][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:12:48,145][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:12:57,195][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 01:12:57,195][188188] FPS: 424371.36[0m
[36m[2023-07-03 01:12:57,198][188188] itr=61, itrs=2000, Progress: 3.05%[0m
[36m[2023-07-03 01:13:08,798][188188] train() took 11.58 seconds to complete[0m
[36m[2023-07-03 01:13:08,798][188188] FPS: 331524.52[0m
[36m[2023-07-03 01:13:13,002][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:13:13,002][188188] Reward + Measures: [[27.41417781  0.78536165  0.24120365  0.56894064  0.75440395  3.99414682]][0m
[37m[1m[2023-07-03 01:13:13,003][188188] Max Reward on eval: 27.414177805793496[0m
[37m[1m[2023-07-03 01:13:13,003][188188] Min Reward on eval: 27.414177805793496[0m
[37m[1m[2023-07-03 01:13:13,003][188188] Mean Reward across all agents: 27.414177805793496[0m
[37m[1m[2023-07-03 01:13:13,003][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:13:17,908][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:13:17,908][188188] Reward + Measures: [[-14.71871759   0.64169997   0.71340001   0.0647       0.66960001
    3.97981191]
 [102.99145011   0.98269999   0.09990001   0.86639994   0.92250007
    3.99847913]
 [101.36001492   0.4738       0.12500001   0.34730002   0.45760003
    3.97971511]
 ...
 [ 52.97850945   0.71710008   0.41280004   0.352        0.68990004
    3.99407744]
 [ 57.99008846   0.67970002   0.49720001   0.23249999   0.69010001
    3.98944926]
 [-65.7006354    0.50839996   0.25139999   0.54879999   0.71090001
    3.82958198]][0m
[37m[1m[2023-07-03 01:13:17,909][188188] Max Reward on eval: 301.44316242759817[0m
[37m[1m[2023-07-03 01:13:17,909][188188] Min Reward on eval: -542.7532577702775[0m
[37m[1m[2023-07-03 01:13:17,909][188188] Mean Reward across all agents: 12.247595311579305[0m
[37m[1m[2023-07-03 01:13:17,909][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:13:17,919][188188] mean_value=-604.6243270204567, max_value=700.523475443176[0m
[37m[1m[2023-07-03 01:13:17,922][188188] New mean coefficients: [[ 0.8460368 -1.5221384 -0.4085751 -2.528911  -1.6697395  0.7628355]][0m
[37m[1m[2023-07-03 01:13:17,923][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:13:26,880][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 01:13:26,880][188188] FPS: 428806.75[0m
[36m[2023-07-03 01:13:26,882][188188] itr=62, itrs=2000, Progress: 3.10%[0m
[36m[2023-07-03 01:13:38,672][188188] train() took 11.77 seconds to complete[0m
[36m[2023-07-03 01:13:38,673][188188] FPS: 326207.75[0m
[36m[2023-07-03 01:13:43,067][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:13:43,067][188188] Reward + Measures: [[31.20037926  0.83306968  0.53001601  0.34603301  0.83115095  3.99575162]][0m
[37m[1m[2023-07-03 01:13:43,068][188188] Max Reward on eval: 31.200379255176212[0m
[37m[1m[2023-07-03 01:13:43,068][188188] Min Reward on eval: 31.200379255176212[0m
[37m[1m[2023-07-03 01:13:43,068][188188] Mean Reward across all agents: 31.200379255176212[0m
[37m[1m[2023-07-03 01:13:43,069][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:13:48,283][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:13:48,284][188188] Reward + Measures: [[  9.49528481   0.63069999   0.50220007   0.26139998   0.64870006
    3.93140268]
 [ 27.10563945   0.27110001   0.0672       0.25150001   0.27830002
    3.95420122]
 [344.84634776   0.13080001   0.27790001   0.35639998   0.32570001
    3.95634627]
 ...
 [-49.71441613   0.86370003   0.6656       0.2296       0.84840006
    3.99112248]
 [  5.33130551   0.40740004   0.28730002   0.16740002   0.41770002
    3.98485541]
 [174.93452485   0.61840004   0.38609999   0.30930004   0.59799999
    3.96016884]][0m
[37m[1m[2023-07-03 01:13:48,284][188188] Max Reward on eval: 344.8463477581739[0m
[37m[1m[2023-07-03 01:13:48,284][188188] Min Reward on eval: -351.1996707805665[0m
[37m[1m[2023-07-03 01:13:48,284][188188] Mean Reward across all agents: 12.971691955842394[0m
[37m[1m[2023-07-03 01:13:48,285][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:13:48,294][188188] mean_value=-429.1469527975083, max_value=721.8912339249393[0m
[37m[1m[2023-07-03 01:13:48,297][188188] New mean coefficients: [[-0.38652718 -0.6911514  -1.23775    -2.0964007  -2.4480076   0.6143956 ]][0m
[37m[1m[2023-07-03 01:13:48,298][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:13:57,267][188188] train() took 8.97 seconds to complete[0m
[36m[2023-07-03 01:13:57,268][188188] FPS: 428174.03[0m
[36m[2023-07-03 01:13:57,270][188188] itr=63, itrs=2000, Progress: 3.15%[0m
[36m[2023-07-03 01:14:08,938][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 01:14:08,938][188188] FPS: 329563.01[0m
[36m[2023-07-03 01:14:13,265][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:14:13,266][188188] Reward + Measures: [[43.93090608  0.82392401  0.63566631  0.24256799  0.83424473  3.9941895 ]][0m
[37m[1m[2023-07-03 01:14:13,266][188188] Max Reward on eval: 43.93090607706363[0m
[37m[1m[2023-07-03 01:14:13,266][188188] Min Reward on eval: 43.93090607706363[0m
[37m[1m[2023-07-03 01:14:13,267][188188] Mean Reward across all agents: 43.93090607706363[0m
[37m[1m[2023-07-03 01:14:13,267][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:14:18,218][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:14:18,219][188188] Reward + Measures: [[ -29.00847277    0.0987        0.0519        0.05709999    0.0918
     3.77204895]
 [ 307.25519255    0.0564        0.65310001    0.60440004    0.65140003
     3.63640904]
 [  98.55376735    0.69250005    0.70240003    0.13330001    0.73579997
     3.97389007]
 ...
 [ -88.43631977    0.19739999    0.2518        0.26340002    0.20250002
     3.9521668 ]
 [-227.33592217    0.199         0.7906        0.97789997    0.98829997
     3.99790359]
 [ -31.81649777    0.1427        0.1218        0.20920001    0.061
     3.63231707]][0m
[37m[1m[2023-07-03 01:14:18,219][188188] Max Reward on eval: 342.21312141418457[0m
[37m[1m[2023-07-03 01:14:18,219][188188] Min Reward on eval: -402.9868484828388[0m
[37m[1m[2023-07-03 01:14:18,220][188188] Mean Reward across all agents: 8.802138904664538[0m
[37m[1m[2023-07-03 01:14:18,220][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:14:18,225][188188] mean_value=-1366.1345657459528, max_value=539.5977668902153[0m
[37m[1m[2023-07-03 01:14:18,228][188188] New mean coefficients: [[ 0.8654406 -2.0871835 -0.087515  -2.0765183 -2.3338065  0.8383833]][0m
[37m[1m[2023-07-03 01:14:18,229][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:14:27,149][188188] train() took 8.92 seconds to complete[0m
[36m[2023-07-03 01:14:27,149][188188] FPS: 430566.76[0m
[36m[2023-07-03 01:14:27,151][188188] itr=64, itrs=2000, Progress: 3.20%[0m
[36m[2023-07-03 01:14:38,653][188188] train() took 11.49 seconds to complete[0m
[36m[2023-07-03 01:14:38,653][188188] FPS: 334341.35[0m
[36m[2023-07-03 01:14:42,947][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:14:42,952][188188] Reward + Measures: [[13.18851961  0.77003092  0.74477732  0.09673733  0.79959428  3.98205566]][0m
[37m[1m[2023-07-03 01:14:42,952][188188] Max Reward on eval: 13.188519612629811[0m
[37m[1m[2023-07-03 01:14:42,952][188188] Min Reward on eval: 13.188519612629811[0m
[37m[1m[2023-07-03 01:14:42,953][188188] Mean Reward across all agents: 13.188519612629811[0m
[37m[1m[2023-07-03 01:14:42,953][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:14:47,973][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:14:47,974][188188] Reward + Measures: [[341.33895876   0.9368       0.69020003   0.28990003   0.96149999
    3.97024512]
 [184.21898859   0.68730003   0.54759997   0.22850001   0.72420007
    3.94824004]
 [ 48.51298727   0.86630005   0.79230005   0.10820001   0.89799994
    3.99563789]
 ...
 [ 73.0631518    0.25149998   0.38240001   0.08940001   0.35320002
    3.77683687]
 [-14.06609592   0.51700002   0.46779999   0.2897       0.62949997
    3.87041926]
 [-52.19438555   0.43000004   0.34909999   0.3558       0.4165
    3.95118976]][0m
[37m[1m[2023-07-03 01:14:47,974][188188] Max Reward on eval: 420.0851361902431[0m
[37m[1m[2023-07-03 01:14:47,974][188188] Min Reward on eval: -414.2124634058215[0m
[37m[1m[2023-07-03 01:14:47,974][188188] Mean Reward across all agents: -14.836088736417402[0m
[37m[1m[2023-07-03 01:14:47,975][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:14:47,978][188188] mean_value=-1959.0672110743374, max_value=621.8091573890881[0m
[37m[1m[2023-07-03 01:14:47,981][188188] New mean coefficients: [[ 1.591002  -1.886337   1.8195847 -1.323334  -1.1454327  0.4997708]][0m
[37m[1m[2023-07-03 01:14:47,982][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:14:57,043][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 01:14:57,043][188188] FPS: 423870.13[0m
[36m[2023-07-03 01:14:57,045][188188] itr=65, itrs=2000, Progress: 3.25%[0m
[36m[2023-07-03 01:15:09,043][188188] train() took 11.98 seconds to complete[0m
[36m[2023-07-03 01:15:09,043][188188] FPS: 320556.18[0m
[36m[2023-07-03 01:15:13,298][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:15:13,299][188188] Reward + Measures: [[76.18218274  0.74788231  0.51854438  0.28550667  0.75829232  3.98270774]][0m
[37m[1m[2023-07-03 01:15:13,299][188188] Max Reward on eval: 76.18218274464152[0m
[37m[1m[2023-07-03 01:15:13,299][188188] Min Reward on eval: 76.18218274464152[0m
[37m[1m[2023-07-03 01:15:13,300][188188] Mean Reward across all agents: 76.18218274464152[0m
[37m[1m[2023-07-03 01:15:13,300][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:15:18,332][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:15:18,332][188188] Reward + Measures: [[-42.58791423   0.049        0.66709995   0.65310001   0.75380003
    3.85862803]
 [237.14266229   0.5570001    0.08580001   0.55080003   0.52370006
    3.84224772]
 [-34.69533898   0.12260001   0.1201       0.10820001   0.1221
    3.72948503]
 ...
 [-43.32453326   0.74970001   0.78940004   0.0239       0.79329997
    3.89135098]
 [114.40095236   0.3955       0.27340001   0.3441       0.4165
    3.82078552]
 [-76.79784509   0.11409999   0.24430001   0.2036       0.28379998
    3.53702617]][0m
[37m[1m[2023-07-03 01:15:18,332][188188] Max Reward on eval: 463.4358682712074[0m
[37m[1m[2023-07-03 01:15:18,333][188188] Min Reward on eval: -464.30303469970823[0m
[37m[1m[2023-07-03 01:15:18,333][188188] Mean Reward across all agents: -24.01769762941224[0m
[37m[1m[2023-07-03 01:15:18,333][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:15:18,337][188188] mean_value=-1794.4807518255343, max_value=586.7684158873471[0m
[37m[1m[2023-07-03 01:15:18,340][188188] New mean coefficients: [[ 0.5828304  -1.5297445  -0.0858326  -1.5110003  -2.750139    0.43950373]][0m
[37m[1m[2023-07-03 01:15:18,341][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:15:27,419][188188] train() took 9.08 seconds to complete[0m
[36m[2023-07-03 01:15:27,419][188188] FPS: 423061.55[0m
[36m[2023-07-03 01:15:27,422][188188] itr=66, itrs=2000, Progress: 3.30%[0m
[36m[2023-07-03 01:15:39,316][188188] train() took 11.87 seconds to complete[0m
[36m[2023-07-03 01:15:39,316][188188] FPS: 323379.11[0m
[36m[2023-07-03 01:15:43,568][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:15:43,568][188188] Reward + Measures: [[34.94252929  0.79647428  0.64964998  0.20598799  0.81078863  3.98847532]][0m
[37m[1m[2023-07-03 01:15:43,568][188188] Max Reward on eval: 34.94252929349168[0m
[37m[1m[2023-07-03 01:15:43,569][188188] Min Reward on eval: 34.94252929349168[0m
[37m[1m[2023-07-03 01:15:43,569][188188] Mean Reward across all agents: 34.94252929349168[0m
[37m[1m[2023-07-03 01:15:43,569][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:15:48,563][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:15:48,564][188188] Reward + Measures: [[  66.46797768    0.87489998    0.30810001    0.59110004    0.84119999
     3.99407387]
 [-317.94027283    0.62519997    0.10739999    0.52560002    0.57809997
     3.6691978 ]
 [ -21.90381409    0.0804        0.08639999    0.08710001    0.0758
     3.79254842]
 ...
 [  15.35566319    0.2421        0.1671        0.32940003    0.31890002
     3.70278859]
 [  27.11909371    0.40760002    0.38530001    0.0975        0.40459999
     3.95965242]
 [ -66.05289681    0.51600003    0.53420001    0.0495        0.53960001
     3.97296762]][0m
[37m[1m[2023-07-03 01:15:48,564][188188] Max Reward on eval: 342.42743011340497[0m
[37m[1m[2023-07-03 01:15:48,564][188188] Min Reward on eval: -691.7105562455952[0m
[37m[1m[2023-07-03 01:15:48,564][188188] Mean Reward across all agents: 28.810458471667612[0m
[37m[1m[2023-07-03 01:15:48,565][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:15:48,569][188188] mean_value=-772.9887015886137, max_value=746.5542871478104[0m
[37m[1m[2023-07-03 01:15:48,572][188188] New mean coefficients: [[ 0.6031304  -1.9961071  -0.02715718 -2.2704425  -3.376503    0.88701206]][0m
[37m[1m[2023-07-03 01:15:48,573][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:15:57,511][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 01:15:57,511][188188] FPS: 429709.53[0m
[36m[2023-07-03 01:15:57,513][188188] itr=67, itrs=2000, Progress: 3.35%[0m
[36m[2023-07-03 01:16:09,373][188188] train() took 11.84 seconds to complete[0m
[36m[2023-07-03 01:16:09,373][188188] FPS: 324308.76[0m
[36m[2023-07-03 01:16:13,647][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:16:13,648][188188] Reward + Measures: [[-0.6714204   0.78072232  0.78807533  0.06431533  0.80864298  3.98931956]][0m
[37m[1m[2023-07-03 01:16:13,648][188188] Max Reward on eval: -0.6714203990870447[0m
[37m[1m[2023-07-03 01:16:13,648][188188] Min Reward on eval: -0.6714203990870447[0m
[37m[1m[2023-07-03 01:16:13,648][188188] Mean Reward across all agents: -0.6714203990870447[0m
[37m[1m[2023-07-03 01:16:13,649][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:16:18,780][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:16:18,781][188188] Reward + Measures: [[ 45.0323796    0.6785       0.60290003   0.2105       0.574
    3.83384776]
 [208.67462836   0.0788       0.3721       0.30070001   0.36400002
    3.90042496]
 [-11.73255892   0.20349999   0.14330001   0.04         0.20939998
    3.92770743]
 ...
 [-38.12872508   0.57069999   0.60030001   0.07910001   0.57530004
    3.97695351]
 [ 21.1010838    0.25619999   0.29100001   0.0889       0.28790003
    3.84686399]
 [  8.92910167   0.1284       0.419        0.33090001   0.454
    3.97955513]][0m
[37m[1m[2023-07-03 01:16:18,781][188188] Max Reward on eval: 401.0070092724636[0m
[37m[1m[2023-07-03 01:16:18,782][188188] Min Reward on eval: -683.9845075492631[0m
[37m[1m[2023-07-03 01:16:18,782][188188] Mean Reward across all agents: 51.02363345374069[0m
[37m[1m[2023-07-03 01:16:18,782][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:16:18,786][188188] mean_value=-1434.4150057656007, max_value=280.767763615741[0m
[37m[1m[2023-07-03 01:16:18,789][188188] New mean coefficients: [[ 0.9902284  -0.9629133   0.94904894 -1.1980184  -3.0524209   1.5351439 ]][0m
[37m[1m[2023-07-03 01:16:18,789][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:16:27,796][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 01:16:27,797][188188] FPS: 426415.20[0m
[36m[2023-07-03 01:16:27,799][188188] itr=68, itrs=2000, Progress: 3.40%[0m
[36m[2023-07-03 01:16:39,354][188188] train() took 11.54 seconds to complete[0m
[36m[2023-07-03 01:16:39,354][188188] FPS: 332880.51[0m
[36m[2023-07-03 01:16:43,594][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:16:43,595][188188] Reward + Measures: [[-5.87323149  0.74722654  0.75447571  0.06283333  0.77007276  3.99129915]][0m
[37m[1m[2023-07-03 01:16:43,595][188188] Max Reward on eval: -5.873231489920591[0m
[37m[1m[2023-07-03 01:16:43,595][188188] Min Reward on eval: -5.873231489920591[0m
[37m[1m[2023-07-03 01:16:43,595][188188] Mean Reward across all agents: -5.873231489920591[0m
[37m[1m[2023-07-03 01:16:43,596][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:16:48,599][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:16:48,599][188188] Reward + Measures: [[ -26.76603316    0.13270001    0.1225        0.1236        0.13710001
     3.90050054]
 [  71.79278106    0.46240002    0.47020003    0.1813        0.62900001
     3.86628699]
 [-129.6864319     0.70990008    0.68540001    0.68580008    0.70069999
     3.90256119]
 ...
 [  23.15797664    0.1864        0.29000002    0.11870001    0.29010001
     3.86921167]
 [  -0.13070157    0.20869999    0.2454        0.10289999    0.23520003
     3.92580676]
 [-191.27382533    0.12560001    0.4842        0.28370002    0.44830003
     3.92262387]][0m
[37m[1m[2023-07-03 01:16:48,599][188188] Max Reward on eval: 326.8019075427204[0m
[37m[1m[2023-07-03 01:16:48,600][188188] Min Reward on eval: -191.27382532861083[0m
[37m[1m[2023-07-03 01:16:48,600][188188] Mean Reward across all agents: 16.452463553056216[0m
[37m[1m[2023-07-03 01:16:48,600][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:16:48,602][188188] mean_value=-1780.8961769689663, max_value=228.8378688940627[0m
[37m[1m[2023-07-03 01:16:48,605][188188] New mean coefficients: [[-0.14202249  0.22619808  0.10897577 -2.078518   -3.124555    2.0678387 ]][0m
[37m[1m[2023-07-03 01:16:48,606][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:16:57,721][188188] train() took 9.11 seconds to complete[0m
[36m[2023-07-03 01:16:57,721][188188] FPS: 421369.46[0m
[36m[2023-07-03 01:16:57,723][188188] itr=69, itrs=2000, Progress: 3.45%[0m
[36m[2023-07-03 01:17:09,449][188188] train() took 11.71 seconds to complete[0m
[36m[2023-07-03 01:17:09,449][188188] FPS: 328038.24[0m
[36m[2023-07-03 01:17:13,739][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:17:13,739][188188] Reward + Measures: [[4.43670845 0.67575765 0.65035897 0.08804166 0.69366473 3.98471165]][0m
[37m[1m[2023-07-03 01:17:13,739][188188] Max Reward on eval: 4.436708449442386[0m
[37m[1m[2023-07-03 01:17:13,740][188188] Min Reward on eval: 4.436708449442386[0m
[37m[1m[2023-07-03 01:17:13,740][188188] Mean Reward across all agents: 4.436708449442386[0m
[37m[1m[2023-07-03 01:17:13,740][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:17:18,719][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:17:18,720][188188] Reward + Measures: [[ -3.19932389   0.1928       0.19520001   0.22550002   0.18460001
    3.94895148]
 [-51.42001187   0.08220001   0.1038       0.1107       0.0912
    3.68170214]
 [-31.80406337   0.75530005   0.77710003   0.0309       0.78109998
    3.99233413]
 ...
 [-81.612938     0.19950001   0.31950003   0.36110002   0.35640001
    3.72931647]
 [ 25.85689568   0.22060001   0.24419999   0.0985       0.2457
    3.6940248 ]
 [ 41.51866629   0.6013       0.4237       0.2194       0.58740002
    3.98095107]][0m
[37m[1m[2023-07-03 01:17:18,720][188188] Max Reward on eval: 311.09094570670277[0m
[37m[1m[2023-07-03 01:17:18,721][188188] Min Reward on eval: -439.46598961381244[0m
[37m[1m[2023-07-03 01:17:18,721][188188] Mean Reward across all agents: -9.33385445746797[0m
[37m[1m[2023-07-03 01:17:18,721][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:17:18,723][188188] mean_value=-2224.607247603561, max_value=59.02996336596516[0m
[37m[1m[2023-07-03 01:17:18,725][188188] New mean coefficients: [[ 0.00953206  1.2860839  -0.42430753 -1.2723641  -2.4394732   2.8484707 ]][0m
[37m[1m[2023-07-03 01:17:18,726][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:17:27,771][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 01:17:27,771][188188] FPS: 424649.15[0m
[36m[2023-07-03 01:17:27,773][188188] itr=70, itrs=2000, Progress: 3.50%[0m
[37m[1m[2023-07-03 01:17:29,942][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000050[0m
[36m[2023-07-03 01:17:42,266][188188] train() took 12.01 seconds to complete[0m
[36m[2023-07-03 01:17:42,266][188188] FPS: 319735.00[0m
[36m[2023-07-03 01:17:46,566][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:17:46,566][188188] Reward + Measures: [[-2.41930101  0.55259603  0.5237667   0.08492067  0.56457466  3.97927213]][0m
[37m[1m[2023-07-03 01:17:46,566][188188] Max Reward on eval: -2.4193010115353766[0m
[37m[1m[2023-07-03 01:17:46,567][188188] Min Reward on eval: -2.4193010115353766[0m
[37m[1m[2023-07-03 01:17:46,567][188188] Mean Reward across all agents: -2.4193010115353766[0m
[37m[1m[2023-07-03 01:17:46,567][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:17:51,669][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:17:51,669][188188] Reward + Measures: [[-66.21338999   0.3116       0.29029998   0.0565       0.31939998
    3.8621223 ]
 [-40.27849507   0.23130003   0.24330001   0.0724       0.28780001
    3.91722107]
 [-16.92877749   0.0964       0.0918       0.0837       0.076
    3.79152369]
 ...
 [249.73118113   0.58910006   0.10030001   0.57350004   0.59270006
    3.91417098]
 [ 12.6988845    0.26040003   0.24340001   0.05620001   0.2588
    3.96207428]
 [-16.00945575   0.0847       0.1056       0.1044       0.10030001
    3.77016187]][0m
[37m[1m[2023-07-03 01:17:51,670][188188] Max Reward on eval: 249.73118112934753[0m
[37m[1m[2023-07-03 01:17:51,670][188188] Min Reward on eval: -248.6988188335672[0m
[37m[1m[2023-07-03 01:17:51,670][188188] Mean Reward across all agents: 3.947989091329617[0m
[37m[1m[2023-07-03 01:17:51,670][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:17:51,672][188188] mean_value=-2338.4050491259054, max_value=-6.546910405388246[0m
[36m[2023-07-03 01:17:51,674][188188] XNES is restarting with a new solution whose measures are [0.77430004 0.76119995 0.74959999 0.75700003 3.98435664] and objective is 132.3311785681639[0m
[36m[2023-07-03 01:17:51,675][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:17:51,677][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:17:51,678][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:18:00,739][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 01:18:00,739][188188] FPS: 423878.81[0m
[36m[2023-07-03 01:18:00,741][188188] itr=71, itrs=2000, Progress: 3.55%[0m
[36m[2023-07-03 01:18:12,504][188188] train() took 11.75 seconds to complete[0m
[36m[2023-07-03 01:18:12,504][188188] FPS: 326934.79[0m
[36m[2023-07-03 01:18:16,827][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:18:16,827][188188] Reward + Measures: [[28.65818752  0.17757167  0.17897868  0.18636701  0.17532633  3.97197199]][0m
[37m[1m[2023-07-03 01:18:16,828][188188] Max Reward on eval: 28.658187521275472[0m
[37m[1m[2023-07-03 01:18:16,828][188188] Min Reward on eval: 28.658187521275472[0m
[37m[1m[2023-07-03 01:18:16,828][188188] Mean Reward across all agents: 28.658187521275472[0m
[37m[1m[2023-07-03 01:18:16,828][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:18:21,999][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:18:22,000][188188] Reward + Measures: [[-53.18247274   0.1499       0.1375       0.147        0.14729999
    3.9682548 ]
 [ 48.74389812   0.086        0.1096       0.1097       0.0723
    3.9613533 ]
 [ 50.21646834   0.21510001   0.20519999   0.1858       0.18969999
    3.97796631]
 ...
 [ 32.85238929   0.08100001   0.0831       0.1211       0.10699999
    3.98322749]
 [-27.0051189    0.0641       0.1017       0.15100001   0.0729
    3.97780609]
 [ 75.2409495    0.4237       0.44650003   0.48589998   0.43050003
    3.9862721 ]][0m
[37m[1m[2023-07-03 01:18:22,000][188188] Max Reward on eval: 229.79275751863605[0m
[37m[1m[2023-07-03 01:18:22,000][188188] Min Reward on eval: -163.67501346776262[0m
[37m[1m[2023-07-03 01:18:22,001][188188] Mean Reward across all agents: 11.708778881041612[0m
[37m[1m[2023-07-03 01:18:22,001][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:18:22,003][188188] mean_value=-3437.8416991179442, max_value=175.16370541807868[0m
[37m[1m[2023-07-03 01:18:22,005][188188] New mean coefficients: [[ 1.9752746  -0.3842064  -0.45398536 -3.1394677  -0.7264756   0.78482544]][0m
[37m[1m[2023-07-03 01:18:22,006][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:18:31,055][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 01:18:31,055][188188] FPS: 424445.43[0m
[36m[2023-07-03 01:18:31,057][188188] itr=72, itrs=2000, Progress: 3.60%[0m
[36m[2023-07-03 01:18:42,833][188188] train() took 11.76 seconds to complete[0m
[36m[2023-07-03 01:18:42,834][188188] FPS: 326553.39[0m
[36m[2023-07-03 01:18:47,155][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:18:47,156][188188] Reward + Measures: [[9.43694403 0.12688167 0.13324067 0.14703065 0.12862432 3.96669006]][0m
[37m[1m[2023-07-03 01:18:47,156][188188] Max Reward on eval: 9.436944025116876[0m
[37m[1m[2023-07-03 01:18:47,156][188188] Min Reward on eval: 9.436944025116876[0m
[37m[1m[2023-07-03 01:18:47,156][188188] Mean Reward across all agents: 9.436944025116876[0m
[37m[1m[2023-07-03 01:18:47,157][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:18:52,177][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:18:52,177][188188] Reward + Measures: [[ 82.94860146   0.43459997   0.41279998   0.40990001   0.4172
    3.95895743]
 [ -3.13414182   0.0772       0.0891       0.1329       0.1033
    3.96700215]
 [-10.50118336   0.54080003   0.51739997   0.52039999   0.53750002
    3.97995043]
 ...
 [-36.46126725   0.177        0.1691       0.1617       0.16410001
    3.96851969]
 [ 67.16854034   0.176        0.1952       0.2027       0.16700001
    3.94852185]
 [ 17.51289407   0.12520002   0.1296       0.12419999   0.1245
    3.96273851]][0m
[37m[1m[2023-07-03 01:18:52,177][188188] Max Reward on eval: 217.70559841822833[0m
[37m[1m[2023-07-03 01:18:52,178][188188] Min Reward on eval: -116.92649778409395[0m
[37m[1m[2023-07-03 01:18:52,178][188188] Mean Reward across all agents: 7.668009976468341[0m
[37m[1m[2023-07-03 01:18:52,178][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:18:52,180][188188] mean_value=-3496.9992458792544, max_value=153.94973308218047[0m
[37m[1m[2023-07-03 01:18:52,182][188188] New mean coefficients: [[ 2.3413103   0.8722842  -0.89890826 -4.451642   -1.6091588  -0.48498058]][0m
[37m[1m[2023-07-03 01:18:52,183][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:19:01,252][188188] train() took 9.07 seconds to complete[0m
[36m[2023-07-03 01:19:01,253][188188] FPS: 423499.34[0m
[36m[2023-07-03 01:19:01,255][188188] itr=73, itrs=2000, Progress: 3.65%[0m
[36m[2023-07-03 01:19:12,966][188188] train() took 11.69 seconds to complete[0m
[36m[2023-07-03 01:19:12,966][188188] FPS: 328399.52[0m
[36m[2023-07-03 01:19:17,304][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:19:17,304][188188] Reward + Measures: [[-6.89601887  0.10025     0.10963733  0.12636633  0.10437433  3.96340942]][0m
[37m[1m[2023-07-03 01:19:17,305][188188] Max Reward on eval: -6.896018873284785[0m
[37m[1m[2023-07-03 01:19:17,305][188188] Min Reward on eval: -6.896018873284785[0m
[37m[1m[2023-07-03 01:19:17,305][188188] Mean Reward across all agents: -6.896018873284785[0m
[37m[1m[2023-07-03 01:19:17,305][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:19:22,368][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:19:22,369][188188] Reward + Measures: [[-20.94966646   0.0988       0.0737       0.1151       0.099
    3.96735573]
 [ 12.63253457   0.0822       0.077        0.087        0.0696
    3.87786269]
 [  5.1894114    0.0658       0.10640001   0.17110001   0.109
    3.97898412]
 ...
 [ -4.11860033   0.18480001   0.1718       0.20700002   0.24609999
    3.97977829]
 [ 13.07771458   0.09240001   0.085        0.0688       0.0615
    3.93475389]
 [ 53.44183423   0.1865       0.20439999   0.1885       0.1663
    3.79817891]][0m
[37m[1m[2023-07-03 01:19:22,369][188188] Max Reward on eval: 104.94318134584464[0m
[37m[1m[2023-07-03 01:19:22,369][188188] Min Reward on eval: -126.71224982999266[0m
[37m[1m[2023-07-03 01:19:22,370][188188] Mean Reward across all agents: -8.370832983217603[0m
[37m[1m[2023-07-03 01:19:22,370][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:19:22,371][188188] mean_value=-3908.2084727607607, max_value=-1229.800126879326[0m
[36m[2023-07-03 01:19:22,373][188188] XNES is restarting with a new solution whose measures are [0.2823     0.81580001 0.51340002 0.82919997 3.90131235] and objective is 218.65752350352705[0m
[36m[2023-07-03 01:19:22,374][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:19:22,376][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:19:22,377][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:19:31,400][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 01:19:31,405][188188] FPS: 425657.78[0m
[36m[2023-07-03 01:19:31,408][188188] itr=74, itrs=2000, Progress: 3.70%[0m
[36m[2023-07-03 01:19:43,097][188188] train() took 11.67 seconds to complete[0m
[36m[2023-07-03 01:19:43,097][188188] FPS: 329047.96[0m
[36m[2023-07-03 01:19:47,401][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:19:47,402][188188] Reward + Measures: [[-43.22015862   0.097772     0.11547133   0.08524567   0.12671766
    3.73333716]][0m
[37m[1m[2023-07-03 01:19:47,402][188188] Max Reward on eval: -43.220158622404725[0m
[37m[1m[2023-07-03 01:19:47,402][188188] Min Reward on eval: -43.220158622404725[0m
[37m[1m[2023-07-03 01:19:47,402][188188] Mean Reward across all agents: -43.220158622404725[0m
[37m[1m[2023-07-03 01:19:47,403][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:19:52,470][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:19:52,476][188188] Reward + Measures: [[  -5.87839909    0.0858        0.15560001    0.1338        0.19940002
     3.92513084]
 [-126.54990574    0.25210002    0.5011        0.41850001    0.44319996
     3.79272199]
 [ -87.53867114    0.09029999    0.0849        0.08270001    0.09320001
     3.70696998]
 ...
 [ -31.70687704    0.3127        0.0724        0.32080001    0.2978
     3.91278458]
 [ -25.64819632    0.0989        0.12290001    0.1041        0.13010001
     3.93260741]
 [ -67.09002919    0.23459999    0.16160001    0.33809999    0.32619998
     3.92567492]][0m
[37m[1m[2023-07-03 01:19:52,476][188188] Max Reward on eval: 277.16567998190874[0m
[37m[1m[2023-07-03 01:19:52,476][188188] Min Reward on eval: -492.8592624746263[0m
[37m[1m[2023-07-03 01:19:52,476][188188] Mean Reward across all agents: -30.04723699784623[0m
[37m[1m[2023-07-03 01:19:52,477][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:19:52,479][188188] mean_value=-2953.4115865003723, max_value=233.22176276933624[0m
[37m[1m[2023-07-03 01:19:52,482][188188] New mean coefficients: [[ 2.2245665   0.04358697  0.06242919 -2.5345492   0.05346704 -0.42917874]][0m
[37m[1m[2023-07-03 01:19:52,483][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:20:01,437][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 01:20:01,437][188188] FPS: 428938.57[0m
[36m[2023-07-03 01:20:01,439][188188] itr=75, itrs=2000, Progress: 3.75%[0m
[36m[2023-07-03 01:20:13,094][188188] train() took 11.64 seconds to complete[0m
[36m[2023-07-03 01:20:13,095][188188] FPS: 329952.68[0m
[36m[2023-07-03 01:20:17,409][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:20:17,410][188188] Reward + Measures: [[-169.98271577    0.30883598    0.40331399    0.16128133    0.39823434
     3.96359897]][0m
[37m[1m[2023-07-03 01:20:17,410][188188] Max Reward on eval: -169.98271576568456[0m
[37m[1m[2023-07-03 01:20:17,410][188188] Min Reward on eval: -169.98271576568456[0m
[37m[1m[2023-07-03 01:20:17,411][188188] Mean Reward across all agents: -169.98271576568456[0m
[37m[1m[2023-07-03 01:20:17,411][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:20:22,422][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:20:22,423][188188] Reward + Measures: [[  21.30693627    0.0495        0.23660003    0.24240001    0.25460002
     3.84171844]
 [   8.28724736    0.14070001    0.14930001    0.0728        0.1688
     3.80617833]
 [ -45.22210163    0.0484        0.28729999    0.28940001    0.32400003
     3.7403791 ]
 ...
 [  -1.52186068    0.13899998    0.18430001    0.13680001    0.17570001
     3.90037274]
 [  13.4773668     0.0622        0.31200001    0.28770003    0.3283
     3.92051244]
 [-123.42240422    0.12150002    0.0488        0.0424        0.11740001
     3.80879259]][0m
[37m[1m[2023-07-03 01:20:22,423][188188] Max Reward on eval: 342.85189392890317[0m
[37m[1m[2023-07-03 01:20:22,423][188188] Min Reward on eval: -656.4319934933446[0m
[37m[1m[2023-07-03 01:20:22,423][188188] Mean Reward across all agents: -38.36939081207245[0m
[37m[1m[2023-07-03 01:20:22,424][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:20:22,426][188188] mean_value=-2975.071815090667, max_value=168.09449030035483[0m
[37m[1m[2023-07-03 01:20:22,429][188188] New mean coefficients: [[ 1.5970411   3.1461384   0.44583088 -2.242787    0.37075776 -1.5571142 ]][0m
[37m[1m[2023-07-03 01:20:22,430][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:20:31,475][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 01:20:31,475][188188] FPS: 424605.90[0m
[36m[2023-07-03 01:20:31,477][188188] itr=76, itrs=2000, Progress: 3.80%[0m
[36m[2023-07-03 01:20:43,144][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 01:20:43,144][188188] FPS: 329639.87[0m
[36m[2023-07-03 01:20:47,495][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:20:47,495][188188] Reward + Measures: [[-109.00129501    0.17105168    0.20752132    0.119585      0.20636067
     3.94517779]][0m
[37m[1m[2023-07-03 01:20:47,496][188188] Max Reward on eval: -109.00129501360847[0m
[37m[1m[2023-07-03 01:20:47,496][188188] Min Reward on eval: -109.00129501360847[0m
[37m[1m[2023-07-03 01:20:47,496][188188] Mean Reward across all agents: -109.00129501360847[0m
[37m[1m[2023-07-03 01:20:47,496][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:20:52,658][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:20:52,663][188188] Reward + Measures: [[  13.34242894    0.0652        0.0907        0.0735        0.11310001
     3.75599146]
 [   2.47979753    0.0696        0.0983        0.0586        0.1046
     3.72059894]
 [  -9.31118955    0.072         0.116         0.0833        0.0902
     3.67897964]
 ...
 [-132.83773374    0.101         0.3669        0.2938        0.36129999
     3.84178233]
 [ -42.61099941    0.1044        0.1112        0.09330001    0.13150001
     3.79695392]
 [ -20.98279714    0.0668        0.08000001    0.071         0.09519999
     3.7703588 ]][0m
[37m[1m[2023-07-03 01:20:52,664][188188] Max Reward on eval: 311.7097344700247[0m
[37m[1m[2023-07-03 01:20:52,664][188188] Min Reward on eval: -488.2177716151811[0m
[37m[1m[2023-07-03 01:20:52,664][188188] Mean Reward across all agents: -45.59188947949117[0m
[37m[1m[2023-07-03 01:20:52,664][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:20:52,666][188188] mean_value=-3382.903792556654, max_value=305.14589186048374[0m
[37m[1m[2023-07-03 01:20:52,668][188188] New mean coefficients: [[ 1.5321401   1.4066343  -0.90163976 -1.886251   -1.3252699  -0.99859965]][0m
[37m[1m[2023-07-03 01:20:52,669][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:21:01,725][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 01:21:01,725][188188] FPS: 424136.51[0m
[36m[2023-07-03 01:21:01,727][188188] itr=77, itrs=2000, Progress: 3.85%[0m
[36m[2023-07-03 01:21:13,416][188188] train() took 11.67 seconds to complete[0m
[36m[2023-07-03 01:21:13,416][188188] FPS: 329077.07[0m
[36m[2023-07-03 01:21:17,639][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:21:17,639][188188] Reward + Measures: [[-191.68280071    0.2663843     0.27562132    0.10467533    0.28483731
     3.96487832]][0m
[37m[1m[2023-07-03 01:21:17,640][188188] Max Reward on eval: -191.68280070969936[0m
[37m[1m[2023-07-03 01:21:17,640][188188] Min Reward on eval: -191.68280070969936[0m
[37m[1m[2023-07-03 01:21:17,640][188188] Mean Reward across all agents: -191.68280070969936[0m
[37m[1m[2023-07-03 01:21:17,640][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:21:22,636][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:21:22,636][188188] Reward + Measures: [[-135.37020961    0.21080001    0.26570001    0.1031        0.25240001
     3.91769409]
 [-388.27777967    0.1086        0.1689        0.16830002    0.18020003
     3.93819809]
 [ -50.77992768    0.0796        0.0903        0.0826        0.10180002
     3.73607564]
 ...
 [ -22.18344205    0.0612        0.1081        0.0875        0.1323
     3.79104424]
 [-406.01146123    0.37480006    0.35800001    0.059         0.4052
     3.98202968]
 [ -68.88878013    0.0981        0.13509999    0.0967        0.13440001
     3.85829425]][0m
[37m[1m[2023-07-03 01:21:22,637][188188] Max Reward on eval: 213.1610245216638[0m
[37m[1m[2023-07-03 01:21:22,637][188188] Min Reward on eval: -700.8469886362552[0m
[37m[1m[2023-07-03 01:21:22,637][188188] Mean Reward across all agents: -116.07556155320485[0m
[37m[1m[2023-07-03 01:21:22,637][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:21:22,639][188188] mean_value=-3261.4016730720264, max_value=-314.7722153841123[0m
[36m[2023-07-03 01:21:22,641][188188] XNES is restarting with a new solution whose measures are [0.3098     0.6699     0.87560004 0.78890002 3.97739339] and objective is -178.80962838018314[0m
[36m[2023-07-03 01:21:22,642][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:21:22,645][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:21:22,645][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:21:31,678][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 01:21:31,678][188188] FPS: 425187.05[0m
[36m[2023-07-03 01:21:31,681][188188] itr=78, itrs=2000, Progress: 3.90%[0m
[36m[2023-07-03 01:21:43,258][188188] train() took 11.56 seconds to complete[0m
[36m[2023-07-03 01:21:43,258][188188] FPS: 332187.78[0m
[36m[2023-07-03 01:21:47,499][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:21:47,499][188188] Reward + Measures: [[10.23089855  0.14712334  0.121257    0.12554199  0.18055899  3.87904263]][0m
[37m[1m[2023-07-03 01:21:47,500][188188] Max Reward on eval: 10.230898550782507[0m
[37m[1m[2023-07-03 01:21:47,500][188188] Min Reward on eval: 10.230898550782507[0m
[37m[1m[2023-07-03 01:21:47,500][188188] Mean Reward across all agents: 10.230898550782507[0m
[37m[1m[2023-07-03 01:21:47,500][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:21:52,460][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:21:52,461][188188] Reward + Measures: [[-331.3493321     0.78710002    0.57110006    0.97980005    0.58710003
     3.99165726]
 [-196.87484784    0.16770001    0.3777        0.2825        0.352
     3.93230176]
 [  28.76962889    0.1587        0.1533        0.0582        0.1793
     3.83550572]
 ...
 [  13.6369249     0.0375        0.59409994    0.60699999    0.62480003
     3.84728312]
 [-109.81250432    0.29539999    0.51690006    0.58490002    0.35960004
     3.81796837]
 [ -24.66357607    0.0755        0.11409999    0.103         0.12400001
     3.72206616]][0m
[37m[1m[2023-07-03 01:21:52,461][188188] Max Reward on eval: 512.2189400482923[0m
[37m[1m[2023-07-03 01:21:52,461][188188] Min Reward on eval: -490.3171958978986[0m
[37m[1m[2023-07-03 01:21:52,461][188188] Mean Reward across all agents: -26.330398321373973[0m
[37m[1m[2023-07-03 01:21:52,462][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:21:52,467][188188] mean_value=-1876.262001182393, max_value=790.8902700205473[0m
[37m[1m[2023-07-03 01:21:52,470][188188] New mean coefficients: [[-1.6793109   0.64633334  0.03433228 -2.6120052  -0.65878165 -0.21013802]][0m
[37m[1m[2023-07-03 01:21:52,471][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:22:01,363][188188] train() took 8.89 seconds to complete[0m
[36m[2023-07-03 01:22:01,363][188188] FPS: 431912.73[0m
[36m[2023-07-03 01:22:01,366][188188] itr=79, itrs=2000, Progress: 3.95%[0m
[36m[2023-07-03 01:22:12,980][188188] train() took 11.59 seconds to complete[0m
[36m[2023-07-03 01:22:12,980][188188] FPS: 331186.39[0m
[36m[2023-07-03 01:22:17,315][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:22:17,315][188188] Reward + Measures: [[-8.69213634  0.12605666  0.09636065  0.11734633  0.14918099  3.90333986]][0m
[37m[1m[2023-07-03 01:22:17,315][188188] Max Reward on eval: -8.692136342322872[0m
[37m[1m[2023-07-03 01:22:17,315][188188] Min Reward on eval: -8.692136342322872[0m
[37m[1m[2023-07-03 01:22:17,316][188188] Mean Reward across all agents: -8.692136342322872[0m
[37m[1m[2023-07-03 01:22:17,316][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:22:22,402][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:22:22,403][188188] Reward + Measures: [[-234.58296651    0.36180001    0.3642        0.68059999    0.59330004
     3.98694229]
 [  99.53772498    0.1121        0.28420001    0.34419999    0.33840001
     3.82181406]
 [  28.59393513    0.15710001    0.117         0.1031        0.17040001
     3.80996394]
 ...
 [  70.29808593    0.25149998    0.20709999    0.40579996    0.36139998
     3.87736297]
 [ -40.15922143    0.0887        0.1671        0.1603        0.1767
     3.69917989]
 [ 156.74967       0.17930001    0.3299        0.45769998    0.44170004
     3.93489504]][0m
[37m[1m[2023-07-03 01:22:22,403][188188] Max Reward on eval: 344.7194113792968[0m
[37m[1m[2023-07-03 01:22:22,403][188188] Min Reward on eval: -551.9102020372171[0m
[37m[1m[2023-07-03 01:22:22,404][188188] Mean Reward across all agents: -12.454554736289689[0m
[37m[1m[2023-07-03 01:22:22,404][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:22:22,406][188188] mean_value=-2400.760917060762, max_value=111.54330382453887[0m
[37m[1m[2023-07-03 01:22:22,409][188188] New mean coefficients: [[-1.1328578  -0.5757836   1.2862906  -3.7526019  -0.64643323 -1.0561025 ]][0m
[37m[1m[2023-07-03 01:22:22,410][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:22:31,318][188188] train() took 8.91 seconds to complete[0m
[36m[2023-07-03 01:22:31,319][188188] FPS: 431134.21[0m
[36m[2023-07-03 01:22:31,321][188188] itr=80, itrs=2000, Progress: 4.00%[0m
[37m[1m[2023-07-03 01:22:33,544][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000060[0m
[36m[2023-07-03 01:22:45,629][188188] train() took 11.77 seconds to complete[0m
[36m[2023-07-03 01:22:45,629][188188] FPS: 326210.76[0m
[36m[2023-07-03 01:22:49,948][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:22:49,949][188188] Reward + Measures: [[266.67561604   0.11860534   0.50124532   0.47437268   0.52098632
    3.98356938]][0m
[37m[1m[2023-07-03 01:22:49,949][188188] Max Reward on eval: 266.6756160398257[0m
[37m[1m[2023-07-03 01:22:49,949][188188] Min Reward on eval: 266.6756160398257[0m
[37m[1m[2023-07-03 01:22:49,950][188188] Mean Reward across all agents: 266.6756160398257[0m
[37m[1m[2023-07-03 01:22:49,950][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:22:55,156][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:22:55,156][188188] Reward + Measures: [[ -37.91765628    0.0762        0.08099999    0.0859        0.0837
     3.62844157]
 [ -70.2721113     0.18800001    0.43199998    0.29650003    0.52430004
     3.94943309]
 [ -16.04851505    0.0597        0.0839        0.0851        0.1006
     3.65484619]
 ...
 [-205.69081519    0.1           0.1189        0.13630001    0.1407
     3.83120775]
 [ 186.92898175    0.48669997    0.87200004    0.59580004    0.70769995
     3.99755478]
 [ 129.10867136    0.2247        0.0596        0.16720001    0.17820001
     3.77689481]][0m
[37m[1m[2023-07-03 01:22:55,156][188188] Max Reward on eval: 623.3842258695979[0m
[37m[1m[2023-07-03 01:22:55,157][188188] Min Reward on eval: -444.82009001048283[0m
[37m[1m[2023-07-03 01:22:55,157][188188] Mean Reward across all agents: 31.91873312990166[0m
[37m[1m[2023-07-03 01:22:55,157][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:22:55,161][188188] mean_value=-2217.865637085931, max_value=524.4155730503818[0m
[37m[1m[2023-07-03 01:22:55,163][188188] New mean coefficients: [[-0.35208142 -1.3238281   0.7872643  -2.6370258  -1.3113823  -2.730003  ]][0m
[37m[1m[2023-07-03 01:22:55,164][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:23:04,158][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 01:23:04,159][188188] FPS: 427029.58[0m
[36m[2023-07-03 01:23:04,161][188188] itr=81, itrs=2000, Progress: 4.05%[0m
[36m[2023-07-03 01:23:15,731][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 01:23:15,731][188188] FPS: 332419.85[0m
[36m[2023-07-03 01:23:19,986][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:23:19,992][188188] Reward + Measures: [[232.66123376   0.123935     0.51875198   0.44240698   0.52288532
    3.97805262]][0m
[37m[1m[2023-07-03 01:23:19,992][188188] Max Reward on eval: 232.66123375828556[0m
[37m[1m[2023-07-03 01:23:19,992][188188] Min Reward on eval: 232.66123375828556[0m
[37m[1m[2023-07-03 01:23:19,993][188188] Mean Reward across all agents: 232.66123375828556[0m
[37m[1m[2023-07-03 01:23:19,993][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:23:24,969][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:23:24,970][188188] Reward + Measures: [[ 71.09620286   0.48390004   0.97290003   0.4738       0.97789997
    3.99753237]
 [413.75441358   0.0808       0.59860003   0.58749998   0.65970004
    3.98722005]
 [-86.15786925   0.0237       0.52140003   0.4914       0.38409999
    3.97411966]
 ...
 [ 39.89651091   0.1514       0.16590001   0.0936       0.19649999
    3.8751297 ]
 [  6.27419839   0.0697       0.2454       0.23040001   0.30379999
    3.74986768]
 [-81.30592777   0.1079       0.1007       0.1208       0.09820001
    3.96206594]][0m
[37m[1m[2023-07-03 01:23:24,970][188188] Max Reward on eval: 773.2448921192438[0m
[37m[1m[2023-07-03 01:23:24,970][188188] Min Reward on eval: -736.1788158321754[0m
[37m[1m[2023-07-03 01:23:24,971][188188] Mean Reward across all agents: 88.34518408949974[0m
[37m[1m[2023-07-03 01:23:24,971][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:23:24,974][188188] mean_value=-1547.7967807013638, max_value=826.2617774402034[0m
[37m[1m[2023-07-03 01:23:24,977][188188] New mean coefficients: [[-0.5611421 -0.589686   2.4110737 -1.6184366 -1.2494109 -2.651707 ]][0m
[37m[1m[2023-07-03 01:23:24,978][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:23:34,024][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 01:23:34,024][188188] FPS: 424599.32[0m
[36m[2023-07-03 01:23:34,026][188188] itr=82, itrs=2000, Progress: 4.10%[0m
[36m[2023-07-03 01:23:45,675][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 01:23:45,675][188188] FPS: 330203.65[0m
[36m[2023-07-03 01:23:49,990][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:23:49,990][188188] Reward + Measures: [[-35.6254364    0.07095566   0.21316101   0.22988068   0.27169567
    3.85756302]][0m
[37m[1m[2023-07-03 01:23:49,990][188188] Max Reward on eval: -35.62543640202408[0m
[37m[1m[2023-07-03 01:23:49,991][188188] Min Reward on eval: -35.62543640202408[0m
[37m[1m[2023-07-03 01:23:49,991][188188] Mean Reward across all agents: -35.62543640202408[0m
[37m[1m[2023-07-03 01:23:49,991][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:23:54,966][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:23:54,967][188188] Reward + Measures: [[-46.78125327   0.0218       0.64740008   0.65099996   0.66080004
    3.96191454]
 [ -6.59292488   0.0603       0.0884       0.0789       0.10640001
    3.62674499]
 [-24.62373682   0.0379       0.54220003   0.54119998   0.52369994
    3.92542005]
 ...
 [ 25.97730588   0.05929999   0.1239       0.1156       0.1532
    3.74986386]
 [ 42.70289658   0.06699999   0.2595       0.25279999   0.2669
    3.88648224]
 [ 22.47728738   0.06340001   0.0934       0.0772       0.0988
    3.75783134]][0m
[37m[1m[2023-07-03 01:23:54,967][188188] Max Reward on eval: 613.4709101031301[0m
[37m[1m[2023-07-03 01:23:54,967][188188] Min Reward on eval: -193.41282689720393[0m
[37m[1m[2023-07-03 01:23:54,967][188188] Mean Reward across all agents: 50.099127952589384[0m
[37m[1m[2023-07-03 01:23:54,967][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:23:54,969][188188] mean_value=-2841.343500896504, max_value=193.11783593164256[0m
[37m[1m[2023-07-03 01:23:54,971][188188] New mean coefficients: [[ 0.78529143 -0.7474115   0.8866571  -1.5533602   0.6302824  -1.8987043 ]][0m
[37m[1m[2023-07-03 01:23:54,972][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:24:03,909][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 01:24:03,910][188188] FPS: 429747.81[0m
[36m[2023-07-03 01:24:03,912][188188] itr=83, itrs=2000, Progress: 4.15%[0m
[36m[2023-07-03 01:24:15,431][188188] train() took 11.50 seconds to complete[0m
[36m[2023-07-03 01:24:15,431][188188] FPS: 333939.70[0m
[36m[2023-07-03 01:24:19,666][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:24:19,666][188188] Reward + Measures: [[-42.48705931   0.04119033   0.47672868   0.46464765   0.49756533
    3.85658789]][0m
[37m[1m[2023-07-03 01:24:19,666][188188] Max Reward on eval: -42.48705930573575[0m
[37m[1m[2023-07-03 01:24:19,667][188188] Min Reward on eval: -42.48705930573575[0m
[37m[1m[2023-07-03 01:24:19,667][188188] Mean Reward across all agents: -42.48705930573575[0m
[37m[1m[2023-07-03 01:24:19,667][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:24:24,618][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:24:24,618][188188] Reward + Measures: [[-92.06719115   0.0123       0.70730001   0.73649997   0.80369997
    3.83913589]
 [  3.53416563   0.0466       0.20060001   0.1996       0.22390001
    3.88082814]
 [708.15181544   0.0219       0.79460001   0.78209996   0.80660003
    3.99224472]
 ...
 [ 53.62696313   0.0058       0.87760001   0.87880003   0.89100009
    3.96062779]
 [ 83.78053595   0.0701       0.2007       0.1937       0.31290001
    3.6723969 ]
 [360.51960374   0.0134       0.7626       0.73680001   0.73819995
    3.93149424]][0m
[37m[1m[2023-07-03 01:24:24,619][188188] Max Reward on eval: 708.1518154427409[0m
[37m[1m[2023-07-03 01:24:24,619][188188] Min Reward on eval: -542.4416809111368[0m
[37m[1m[2023-07-03 01:24:24,619][188188] Mean Reward across all agents: 62.12449912162231[0m
[37m[1m[2023-07-03 01:24:24,619][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:24:24,621][188188] mean_value=-1428.4198478327223, max_value=269.33902378916173[0m
[37m[1m[2023-07-03 01:24:24,624][188188] New mean coefficients: [[-0.636147    1.3471096   0.69361746 -2.2218988   1.5721388  -1.7941816 ]][0m
[37m[1m[2023-07-03 01:24:24,625][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:24:33,612][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 01:24:33,612][188188] FPS: 427356.28[0m
[36m[2023-07-03 01:24:33,615][188188] itr=84, itrs=2000, Progress: 4.20%[0m
[36m[2023-07-03 01:24:45,192][188188] train() took 11.56 seconds to complete[0m
[36m[2023-07-03 01:24:45,193][188188] FPS: 332194.22[0m
[36m[2023-07-03 01:24:49,464][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:24:49,464][188188] Reward + Measures: [[89.39060077  0.115173    0.44630766  0.52589834  0.55307364  3.77470851]][0m
[37m[1m[2023-07-03 01:24:49,465][188188] Max Reward on eval: 89.3906007654937[0m
[37m[1m[2023-07-03 01:24:49,465][188188] Min Reward on eval: 89.3906007654937[0m
[37m[1m[2023-07-03 01:24:49,465][188188] Mean Reward across all agents: 89.3906007654937[0m
[37m[1m[2023-07-03 01:24:49,465][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:24:54,630][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:24:54,636][188188] Reward + Measures: [[510.4225924    0.0381       0.56159997   0.54689997   0.53740001
    3.99233365]
 [100.35998884   0.0297       0.53010005   0.5352       0.54430002
    3.94149709]
 [436.30968188   0.0203       0.60780001   0.63609999   0.64710003
    3.9951179 ]
 ...
 [-24.11433228   0.0388       0.40990001   0.41440001   0.43290004
    3.86441612]
 [ 65.4818451    0.0535       0.3511       0.38930002   0.38320002
    3.99209571]
 [-32.21748876   0.048        0.1648       0.1674       0.15730003
    3.82958221]][0m
[37m[1m[2023-07-03 01:24:54,637][188188] Max Reward on eval: 694.4363014442846[0m
[37m[1m[2023-07-03 01:24:54,637][188188] Min Reward on eval: -424.5764312453568[0m
[37m[1m[2023-07-03 01:24:54,637][188188] Mean Reward across all agents: 60.96878703032055[0m
[37m[1m[2023-07-03 01:24:54,637][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:24:54,640][188188] mean_value=-1470.1906743277582, max_value=361.57332368249865[0m
[37m[1m[2023-07-03 01:24:54,642][188188] New mean coefficients: [[ 0.13257414  1.6647142   0.97692657 -1.1433659   0.71146125 -3.0906417 ]][0m
[37m[1m[2023-07-03 01:24:54,643][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:25:03,665][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 01:25:03,666][188188] FPS: 425691.61[0m
[36m[2023-07-03 01:25:03,668][188188] itr=85, itrs=2000, Progress: 4.25%[0m
[36m[2023-07-03 01:25:15,229][188188] train() took 11.54 seconds to complete[0m
[36m[2023-07-03 01:25:15,229][188188] FPS: 332659.79[0m
[36m[2023-07-03 01:25:19,520][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:25:19,520][188188] Reward + Measures: [[53.11863782  0.00141033  0.95581168  0.95660901  0.96767169  3.90369129]][0m
[37m[1m[2023-07-03 01:25:19,520][188188] Max Reward on eval: 53.118637821114234[0m
[37m[1m[2023-07-03 01:25:19,521][188188] Min Reward on eval: 53.118637821114234[0m
[37m[1m[2023-07-03 01:25:19,521][188188] Mean Reward across all agents: 53.118637821114234[0m
[37m[1m[2023-07-03 01:25:19,521][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:25:24,510][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:25:24,511][188188] Reward + Measures: [[ 48.01182288   0.0004       0.96029997   0.96310008   0.97649997
    3.99152565]
 [-60.263161     0.0645       0.1974       0.1921       0.2472
    3.45485044]
 [ 24.36218666   0.0549       0.34200001   0.31300002   0.32290003
    3.72430539]
 ...
 [  1.33531752   0.004        0.87580007   0.87550002   0.88700002
    3.87651706]
 [-30.82958821   0.0089       0.87509996   0.87599993   0.88330001
    3.98477793]
 [-78.43635938   0.0588       0.3874       0.40330002   0.40760002
    3.89260554]][0m
[37m[1m[2023-07-03 01:25:24,511][188188] Max Reward on eval: 563.0221781587461[0m
[37m[1m[2023-07-03 01:25:24,511][188188] Min Reward on eval: -322.15892845559864[0m
[37m[1m[2023-07-03 01:25:24,512][188188] Mean Reward across all agents: 61.11543149934303[0m
[37m[1m[2023-07-03 01:25:24,512][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:25:24,514][188188] mean_value=-1101.0652861031947, max_value=450.7476446699817[0m
[37m[1m[2023-07-03 01:25:24,517][188188] New mean coefficients: [[ 1.5705254   0.21061587  1.3222868  -0.81841487  0.402047   -2.0076752 ]][0m
[37m[1m[2023-07-03 01:25:24,518][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:25:33,467][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 01:25:33,468][188188] FPS: 429139.55[0m
[36m[2023-07-03 01:25:33,470][188188] itr=86, itrs=2000, Progress: 4.30%[0m
[36m[2023-07-03 01:25:45,127][188188] train() took 11.64 seconds to complete[0m
[36m[2023-07-03 01:25:45,127][188188] FPS: 329903.30[0m
[36m[2023-07-03 01:25:49,401][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:25:49,401][188188] Reward + Measures: [[297.0879623    0.01315867   0.62901866   0.68785197   0.77290493
    3.7103765 ]][0m
[37m[1m[2023-07-03 01:25:49,401][188188] Max Reward on eval: 297.0879623036437[0m
[37m[1m[2023-07-03 01:25:49,402][188188] Min Reward on eval: 297.0879623036437[0m
[37m[1m[2023-07-03 01:25:49,402][188188] Mean Reward across all agents: 297.0879623036437[0m
[37m[1m[2023-07-03 01:25:49,402][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:25:54,360][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:25:54,360][188188] Reward + Measures: [[ 77.04192122   0.0078       0.81540006   0.81760007   0.82680005
    3.987674  ]
 [321.53584198   0.0521       0.5413       0.55240005   0.62440002
    3.81975746]
 [334.55519291   0.0033       0.65829998   0.71710002   0.84040004
    3.65888667]
 ...
 [ 39.5846621    0.061        0.2145       0.18080001   0.21920002
    3.58091283]
 [141.58430719   0.0004       0.92049998   0.92620003   0.94200003
    3.83956337]
 [474.88259171   0.1087       0.63620001   0.54430002   0.61359996
    3.97909045]][0m
[37m[1m[2023-07-03 01:25:54,361][188188] Max Reward on eval: 774.9505729307654[0m
[37m[1m[2023-07-03 01:25:54,361][188188] Min Reward on eval: -218.63479712866246[0m
[37m[1m[2023-07-03 01:25:54,361][188188] Mean Reward across all agents: 119.9360086543833[0m
[37m[1m[2023-07-03 01:25:54,361][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:25:54,364][188188] mean_value=-891.4442450828377, max_value=525.1450418857315[0m
[37m[1m[2023-07-03 01:25:54,367][188188] New mean coefficients: [[ 1.6298704   0.36734092  0.5114721  -1.1099138   1.8587537  -1.6151265 ]][0m
[37m[1m[2023-07-03 01:25:54,367][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:26:03,350][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 01:26:03,355][188188] FPS: 427561.10[0m
[36m[2023-07-03 01:26:03,359][188188] itr=87, itrs=2000, Progress: 4.35%[0m
[36m[2023-07-03 01:26:14,968][188188] train() took 11.59 seconds to complete[0m
[36m[2023-07-03 01:26:14,969][188188] FPS: 331278.50[0m
[36m[2023-07-03 01:26:19,308][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:26:19,308][188188] Reward + Measures: [[336.86252714   0.06296567   0.49282801   0.49106964   0.52088535
    3.9639616 ]][0m
[37m[1m[2023-07-03 01:26:19,309][188188] Max Reward on eval: 336.86252714124043[0m
[37m[1m[2023-07-03 01:26:19,309][188188] Min Reward on eval: 336.86252714124043[0m
[37m[1m[2023-07-03 01:26:19,309][188188] Mean Reward across all agents: 336.86252714124043[0m
[37m[1m[2023-07-03 01:26:19,309][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:26:24,353][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:26:24,353][188188] Reward + Measures: [[456.25422765   0.0657       0.60939997   0.59530002   0.64130002
    3.99070716]
 [220.54976906   0.0447       0.34990001   0.3348       0.32799998
    3.8206017 ]
 [ 12.26336063   0.0468       0.2256       0.2484       0.22330001
    3.94518471]
 ...
 [-16.64001513   0.0971       0.1602       0.0849       0.1596
    3.64022231]
 [529.56932444   0.0302       0.70160002   0.70319998   0.7123
    3.99446106]
 [-43.62104201   0.2516       0.52219999   0.32930002   0.55120003
    3.97338653]][0m
[37m[1m[2023-07-03 01:26:24,353][188188] Max Reward on eval: 825.3637618873269[0m
[37m[1m[2023-07-03 01:26:24,354][188188] Min Reward on eval: -329.886519945506[0m
[37m[1m[2023-07-03 01:26:24,354][188188] Mean Reward across all agents: 155.1526311142072[0m
[37m[1m[2023-07-03 01:26:24,354][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:26:24,357][188188] mean_value=-835.3668529915869, max_value=876.5937289678835[0m
[37m[1m[2023-07-03 01:26:24,360][188188] New mean coefficients: [[ 1.8437088   0.8651892  -0.23112178 -1.566315    2.3419302  -0.33796918]][0m
[37m[1m[2023-07-03 01:26:24,361][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:26:33,402][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 01:26:33,402][188188] FPS: 424802.94[0m
[36m[2023-07-03 01:26:33,405][188188] itr=88, itrs=2000, Progress: 4.40%[0m
[36m[2023-07-03 01:26:45,076][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 01:26:45,076][188188] FPS: 329518.48[0m
[36m[2023-07-03 01:26:49,390][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:26:49,391][188188] Reward + Measures: [[-26.78227757   0.084168     0.152913     0.14574033   0.21599333
    3.85911393]][0m
[37m[1m[2023-07-03 01:26:49,391][188188] Max Reward on eval: -26.782277571858295[0m
[37m[1m[2023-07-03 01:26:49,391][188188] Min Reward on eval: -26.782277571858295[0m
[37m[1m[2023-07-03 01:26:49,392][188188] Mean Reward across all agents: -26.782277571858295[0m
[37m[1m[2023-07-03 01:26:49,392][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:26:54,391][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:26:54,397][188188] Reward + Measures: [[ -0.74781538   0.0706       0.0649       0.0567       0.1081
    3.71557689]
 [666.50864885   0.0273       0.70850003   0.68949997   0.71509999
    3.94539714]
 [ 65.03907961   0.30090001   0.74449998   0.44210002   0.69320005
    3.99636722]
 ...
 [164.53904648   0.15570001   0.51929998   0.39469999   0.52689999
    3.93660712]
 [353.07291031   0.0803       0.43940002   0.44510004   0.4686
    3.98424006]
 [118.32617741   0.11310001   0.0559       0.069        0.11589999
    3.77705383]][0m
[37m[1m[2023-07-03 01:26:54,397][188188] Max Reward on eval: 828.2773399353027[0m
[37m[1m[2023-07-03 01:26:54,397][188188] Min Reward on eval: -381.4054970606929[0m
[37m[1m[2023-07-03 01:26:54,398][188188] Mean Reward across all agents: 90.11256250890847[0m
[37m[1m[2023-07-03 01:26:54,398][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:26:54,400][188188] mean_value=-2141.8619923950373, max_value=252.4319797053654[0m
[37m[1m[2023-07-03 01:26:54,402][188188] New mean coefficients: [[ 2.5860095  0.7318899  1.504857  -2.563734   1.6430093  0.9260292]][0m
[37m[1m[2023-07-03 01:26:54,403][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:27:03,331][188188] train() took 8.93 seconds to complete[0m
[36m[2023-07-03 01:27:03,331][188188] FPS: 430184.16[0m
[36m[2023-07-03 01:27:03,334][188188] itr=89, itrs=2000, Progress: 4.45%[0m
[36m[2023-07-03 01:27:14,881][188188] train() took 11.53 seconds to complete[0m
[36m[2023-07-03 01:27:14,881][188188] FPS: 333117.54[0m
[36m[2023-07-03 01:27:19,175][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:27:19,175][188188] Reward + Measures: [[72.53848808  0.10545933  0.29440865  0.25613964  0.30778235  3.93322587]][0m
[37m[1m[2023-07-03 01:27:19,175][188188] Max Reward on eval: 72.53848807646119[0m
[37m[1m[2023-07-03 01:27:19,176][188188] Min Reward on eval: 72.53848807646119[0m
[37m[1m[2023-07-03 01:27:19,176][188188] Mean Reward across all agents: 72.53848807646119[0m
[37m[1m[2023-07-03 01:27:19,176][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:27:24,325][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:27:24,331][188188] Reward + Measures: [[  20.91171074    0.0753        0.27290002    0.27649999    0.26700002
     3.94692874]
 [-134.02365239    0.059         0.33519998    0.32590002    0.35879999
     3.77982187]
 [  19.29918684    0.0731        0.10570001    0.0754        0.0784
     3.72871566]
 ...
 [  82.77238603    0.06420001    0.0826        0.08020001    0.10140001
     3.67140698]
 [ 212.44285246    0.0889        0.52410001    0.46670005    0.54580003
     3.96250319]
 [ 278.98546216    0.30239999    0.81460011    0.51899999    0.81580001
     3.98594451]][0m
[37m[1m[2023-07-03 01:27:24,331][188188] Max Reward on eval: 646.1256713606417[0m
[37m[1m[2023-07-03 01:27:24,332][188188] Min Reward on eval: -361.2875335346442[0m
[37m[1m[2023-07-03 01:27:24,332][188188] Mean Reward across all agents: 40.43180758506037[0m
[37m[1m[2023-07-03 01:27:24,332][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:27:24,334][188188] mean_value=-2444.7946831238687, max_value=15.31696660308694[0m
[37m[1m[2023-07-03 01:27:24,336][188188] New mean coefficients: [[ 0.9909036   1.3628327  -0.53651917 -3.9770708   1.5546919   0.13060987]][0m
[37m[1m[2023-07-03 01:27:24,337][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:27:33,358][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 01:27:33,359][188188] FPS: 425718.08[0m
[36m[2023-07-03 01:27:33,361][188188] itr=90, itrs=2000, Progress: 4.50%[0m
[37m[1m[2023-07-03 01:27:35,619][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000070[0m
[36m[2023-07-03 01:27:47,737][188188] train() took 11.79 seconds to complete[0m
[36m[2023-07-03 01:27:47,738][188188] FPS: 325724.05[0m
[36m[2023-07-03 01:27:52,042][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:27:52,042][188188] Reward + Measures: [[46.57072394  0.19858268  0.457288    0.31661168  0.47262564  3.96193886]][0m
[37m[1m[2023-07-03 01:27:52,043][188188] Max Reward on eval: 46.57072393946566[0m
[37m[1m[2023-07-03 01:27:52,043][188188] Min Reward on eval: 46.57072393946566[0m
[37m[1m[2023-07-03 01:27:52,043][188188] Mean Reward across all agents: 46.57072393946566[0m
[37m[1m[2023-07-03 01:27:52,043][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:27:57,094][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:27:57,147][188188] Reward + Measures: [[ 482.84011746    0.0324        0.67160004    0.61110002    0.74830002
     3.9569366 ]
 [ 400.37895199    0.20060001    0.89869994    0.6821        0.9156
     3.99012446]
 [ 430.00027015    0.025         0.67500001    0.64630002    0.66650003
     3.95631862]
 ...
 [ 374.00325965    0.0016        0.96999997    0.95179999    0.99110001
     3.98441768]
 [-114.37263848    0.0408        0.57650006    0.51599997    0.62020004
     3.88988566]
 [ 394.35046481    0.0015        0.94910002    0.90570003    0.98080009
     3.99036527]][0m
[37m[1m[2023-07-03 01:27:57,147][188188] Max Reward on eval: 752.1576538203284[0m
[37m[1m[2023-07-03 01:27:57,147][188188] Min Reward on eval: -510.89685057805616[0m
[37m[1m[2023-07-03 01:27:57,147][188188] Mean Reward across all agents: 118.80615711356525[0m
[37m[1m[2023-07-03 01:27:57,148][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:27:57,150][188188] mean_value=-909.6169539221622, max_value=120.05091020706209[0m
[37m[1m[2023-07-03 01:27:57,152][188188] New mean coefficients: [[ 1.7122269  1.3405991 -1.010818  -4.0451474  1.527805  -1.0740095]][0m
[37m[1m[2023-07-03 01:27:57,153][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:28:06,166][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 01:28:06,166][188188] FPS: 426133.69[0m
[36m[2023-07-03 01:28:06,168][188188] itr=91, itrs=2000, Progress: 4.55%[0m
[36m[2023-07-03 01:28:17,938][188188] train() took 11.75 seconds to complete[0m
[36m[2023-07-03 01:28:17,939][188188] FPS: 326802.36[0m
[36m[2023-07-03 01:28:22,241][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:28:22,242][188188] Reward + Measures: [[-28.2191405    0.05158033   0.38034829   0.38207665   0.39343199
    3.90941834]][0m
[37m[1m[2023-07-03 01:28:22,242][188188] Max Reward on eval: -28.219140498770887[0m
[37m[1m[2023-07-03 01:28:22,242][188188] Min Reward on eval: -28.219140498770887[0m
[37m[1m[2023-07-03 01:28:22,243][188188] Mean Reward across all agents: -28.219140498770887[0m
[37m[1m[2023-07-03 01:28:22,243][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:28:27,212][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:28:27,213][188188] Reward + Measures: [[ 36.06303069   0.076        0.21239999   0.17760001   0.2854
    3.88722801]
 [659.53518865   0.0815       0.93290007   0.82270002   0.94169998
    3.98688364]
 [ 46.32778977   0.078        0.31510001   0.30290002   0.38129997
    3.89655924]
 ...
 [-55.06147278   0.1575       0.7942       0.93129998   0.93239993
    3.99036765]
 [457.19193899   0.0234       0.64379996   0.61540002   0.62889999
    3.85572815]
 [-31.60275908   0.11390001   0.69130003   0.78330004   0.78420001
    3.97197509]][0m
[37m[1m[2023-07-03 01:28:27,213][188188] Max Reward on eval: 776.3566551661352[0m
[37m[1m[2023-07-03 01:28:27,213][188188] Min Reward on eval: -321.5651043627411[0m
[37m[1m[2023-07-03 01:28:27,214][188188] Mean Reward across all agents: 120.43224634319098[0m
[37m[1m[2023-07-03 01:28:27,214][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:28:27,216][188188] mean_value=-1134.422127836719, max_value=130.09315128508206[0m
[37m[1m[2023-07-03 01:28:27,218][188188] New mean coefficients: [[ 2.1531534  -0.8507707   0.05282831 -4.29119     1.5435004  -1.1197317 ]][0m
[37m[1m[2023-07-03 01:28:27,219][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:28:36,324][188188] train() took 9.10 seconds to complete[0m
[36m[2023-07-03 01:28:36,325][188188] FPS: 421819.45[0m
[36m[2023-07-03 01:28:36,327][188188] itr=92, itrs=2000, Progress: 4.60%[0m
[36m[2023-07-03 01:28:48,060][188188] train() took 11.72 seconds to complete[0m
[36m[2023-07-03 01:28:48,061][188188] FPS: 327749.65[0m
[36m[2023-07-03 01:28:52,366][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:28:52,366][188188] Reward + Measures: [[-69.21253566   0.033507     0.61676532   0.59679365   0.62154531
    3.91535401]][0m
[37m[1m[2023-07-03 01:28:52,366][188188] Max Reward on eval: -69.21253565827185[0m
[37m[1m[2023-07-03 01:28:52,367][188188] Min Reward on eval: -69.21253565827185[0m
[37m[1m[2023-07-03 01:28:52,367][188188] Mean Reward across all agents: -69.21253565827185[0m
[37m[1m[2023-07-03 01:28:52,367][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:28:57,417][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:28:57,418][188188] Reward + Measures: [[ -7.04063596   0.0565       0.2624       0.25650001   0.28040001
    3.93997812]
 [ 15.01334816   0.0497       0.1779       0.18209998   0.17510001
    3.82401252]
 [  5.69522686   0.15719999   0.85700005   0.94119996   0.88339996
    3.98812342]
 ...
 [-59.03466249   0.0522       0.22939999   0.2218       0.21480003
    3.90810943]
 [-47.98885494   0.0811       0.0911       0.06770001   0.0984
    3.72202563]
 [-54.51115821   0.13259999   0.44369999   0.51360005   0.53290004
    3.91220474]][0m
[37m[1m[2023-07-03 01:28:57,418][188188] Max Reward on eval: 408.1533787034452[0m
[37m[1m[2023-07-03 01:28:57,419][188188] Min Reward on eval: -382.49914072798566[0m
[37m[1m[2023-07-03 01:28:57,419][188188] Mean Reward across all agents: -20.32289686402355[0m
[37m[1m[2023-07-03 01:28:57,419][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:28:57,421][188188] mean_value=-2605.1060495799666, max_value=200.38357104307758[0m
[37m[1m[2023-07-03 01:28:57,423][188188] New mean coefficients: [[ 3.4081287  0.7942722 -0.3163281 -2.6310778  2.440122   0.6132169]][0m
[37m[1m[2023-07-03 01:28:57,424][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:29:06,497][188188] train() took 9.07 seconds to complete[0m
[36m[2023-07-03 01:29:06,497][188188] FPS: 423352.64[0m
[36m[2023-07-03 01:29:06,499][188188] itr=93, itrs=2000, Progress: 4.65%[0m
[36m[2023-07-03 01:29:18,099][188188] train() took 11.58 seconds to complete[0m
[36m[2023-07-03 01:29:18,100][188188] FPS: 331525.66[0m
[36m[2023-07-03 01:29:22,330][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:29:22,336][188188] Reward + Measures: [[-52.2390228    0.050793     0.31587866   0.30465233   0.31510532
    3.86222577]][0m
[37m[1m[2023-07-03 01:29:22,336][188188] Max Reward on eval: -52.23902279568341[0m
[37m[1m[2023-07-03 01:29:22,336][188188] Min Reward on eval: -52.23902279568341[0m
[37m[1m[2023-07-03 01:29:22,337][188188] Mean Reward across all agents: -52.23902279568341[0m
[37m[1m[2023-07-03 01:29:22,337][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:29:27,495][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:29:27,500][188188] Reward + Measures: [[ -89.52206788    0.0329        0.61709994    0.61370003    0.63980001
     3.9679172 ]
 [   8.63115936    0.0138        0.83540004    0.83560002    0.85470003
     3.98913002]
 [ 130.31194591    0.0874        0.80590004    0.87040007    0.88520002
     3.99135566]
 ...
 [ 118.43634334    0.14350002    0.46560001    0.56050003    0.49260002
     3.98504066]
 [-106.25985812    0.046         0.37100002    0.35540006    0.37499997
     3.95177507]
 [  15.57970134    0.0691        0.0938        0.10899999    0.114
     3.87410164]][0m
[37m[1m[2023-07-03 01:29:27,501][188188] Max Reward on eval: 304.0860208457336[0m
[37m[1m[2023-07-03 01:29:27,501][188188] Min Reward on eval: -319.3198440079112[0m
[37m[1m[2023-07-03 01:29:27,501][188188] Mean Reward across all agents: -8.312000971866958[0m
[37m[1m[2023-07-03 01:29:27,502][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:29:27,504][188188] mean_value=-1670.0481870508365, max_value=494.4192782928859[0m
[37m[1m[2023-07-03 01:29:27,506][188188] New mean coefficients: [[ 2.9038475  1.2038898 -2.9370916 -3.3624587  3.1497166  1.3405477]][0m
[37m[1m[2023-07-03 01:29:27,507][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:29:36,459][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 01:29:36,459][188188] FPS: 429044.21[0m
[36m[2023-07-03 01:29:36,461][188188] itr=94, itrs=2000, Progress: 4.70%[0m
[36m[2023-07-03 01:29:48,107][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 01:29:48,108][188188] FPS: 330211.30[0m
[36m[2023-07-03 01:29:52,392][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:29:52,392][188188] Reward + Measures: [[-58.74017006   0.05579634   0.30876765   0.2955927    0.31872201
    3.83647656]][0m
[37m[1m[2023-07-03 01:29:52,393][188188] Max Reward on eval: -58.740170060122715[0m
[37m[1m[2023-07-03 01:29:52,393][188188] Min Reward on eval: -58.740170060122715[0m
[37m[1m[2023-07-03 01:29:52,393][188188] Mean Reward across all agents: -58.740170060122715[0m
[37m[1m[2023-07-03 01:29:52,393][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:29:57,371][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:29:57,376][188188] Reward + Measures: [[  57.2179263     0.0645        0.1038        0.09330001    0.1214
     3.88611388]
 [  -3.50489841    0.1002        0.18190001    0.20809999    0.22250001
     3.93751574]
 [-171.13601097    0.12100001    0.10640001    0.0852        0.1012
     3.92343116]
 ...
 [ -84.28993953    0.0614        0.23740001    0.23109999    0.25560004
     3.82591295]
 [  -2.58933001    0.0921        0.1023        0.06330001    0.0916
     3.62976432]
 [  -5.21786346    0.06960001    0.1096        0.1026        0.12210001
     3.84529567]][0m
[37m[1m[2023-07-03 01:29:57,377][188188] Max Reward on eval: 389.35834592208266[0m
[37m[1m[2023-07-03 01:29:57,377][188188] Min Reward on eval: -362.50120194107296[0m
[37m[1m[2023-07-03 01:29:57,377][188188] Mean Reward across all agents: -43.852503364920196[0m
[37m[1m[2023-07-03 01:29:57,377][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:29:57,380][188188] mean_value=-2262.163218038394, max_value=414.39621547052303[0m
[37m[1m[2023-07-03 01:29:57,382][188188] New mean coefficients: [[ 3.1091173  -1.2365365  -2.7458096  -2.3644583   3.8195887  -0.18787777]][0m
[37m[1m[2023-07-03 01:29:57,383][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:30:06,361][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 01:30:06,361][188188] FPS: 427849.66[0m
[36m[2023-07-03 01:30:06,363][188188] itr=95, itrs=2000, Progress: 4.75%[0m
[36m[2023-07-03 01:30:17,954][188188] train() took 11.57 seconds to complete[0m
[36m[2023-07-03 01:30:17,954][188188] FPS: 331804.34[0m
[36m[2023-07-03 01:30:22,240][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:30:22,240][188188] Reward + Measures: [[59.50788612  0.04831534  0.42727399  0.41498831  0.43164799  3.91136932]][0m
[37m[1m[2023-07-03 01:30:22,241][188188] Max Reward on eval: 59.50788611540003[0m
[37m[1m[2023-07-03 01:30:22,241][188188] Min Reward on eval: 59.50788611540003[0m
[37m[1m[2023-07-03 01:30:22,241][188188] Mean Reward across all agents: 59.50788611540003[0m
[37m[1m[2023-07-03 01:30:22,241][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:30:27,230][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:30:27,231][188188] Reward + Measures: [[  54.66098422    0.14049999    0.13390002    0.21439998    0.2491
     3.8018887 ]
 [ -17.16849739    0.0783        0.35969999    0.40920001    0.3892
     3.89541888]
 [ -35.04572465    0.0502        0.41060001    0.39910004    0.4269
     3.85513997]
 ...
 [  14.97468094    0.07790001    0.3723        0.40089998    0.42139998
     3.78497696]
 [ -26.56895029    0.0713        0.11870001    0.11919999    0.1322
     3.7858963 ]
 [-215.55853717    0.0996        0.37550002    0.40990001    0.41919994
     3.91062737]][0m
[37m[1m[2023-07-03 01:30:27,231][188188] Max Reward on eval: 373.90146928317847[0m
[37m[1m[2023-07-03 01:30:27,231][188188] Min Reward on eval: -343.2923577653244[0m
[37m[1m[2023-07-03 01:30:27,231][188188] Mean Reward across all agents: -8.834523799212066[0m
[37m[1m[2023-07-03 01:30:27,232][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:30:27,233][188188] mean_value=-1930.4719816143368, max_value=22.14963193550267[0m
[37m[1m[2023-07-03 01:30:27,236][188188] New mean coefficients: [[ 3.3530962  0.814311  -2.1878488 -3.0595121  3.6782818  1.178686 ]][0m
[37m[1m[2023-07-03 01:30:27,237][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:30:36,191][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 01:30:36,192][188188] FPS: 428905.23[0m
[36m[2023-07-03 01:30:36,194][188188] itr=96, itrs=2000, Progress: 4.80%[0m
[36m[2023-07-03 01:30:47,859][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 01:30:47,859][188188] FPS: 329754.56[0m
[36m[2023-07-03 01:30:52,159][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:30:52,159][188188] Reward + Measures: [[-40.77520629   0.29425135   0.43672267   0.64572167   0.6123696
    3.95194459]][0m
[37m[1m[2023-07-03 01:30:52,159][188188] Max Reward on eval: -40.77520628548813[0m
[37m[1m[2023-07-03 01:30:52,160][188188] Min Reward on eval: -40.77520628548813[0m
[37m[1m[2023-07-03 01:30:52,160][188188] Mean Reward across all agents: -40.77520628548813[0m
[37m[1m[2023-07-03 01:30:52,160][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:30:57,182][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:30:57,182][188188] Reward + Measures: [[ -29.05848012    0.13530001    0.48460004    0.56599998    0.58120006
     3.96940923]
 [-159.10831594    0.2712        0.37170002    0.16779999    0.39140001
     3.96038795]
 [  37.71700909    0.0556        0.14460002    0.1314        0.1506
     3.85867548]
 ...
 [ -45.01766213    0.1516        0.37939999    0.45430002    0.43590003
     3.91582417]
 [ -18.20253542    0.0591        0.28200004    0.25540003    0.27380002
     3.91466713]
 [ -66.50543442    0.0723        0.27919999    0.25409999    0.28710002
     3.86871195]][0m
[37m[1m[2023-07-03 01:30:57,183][188188] Max Reward on eval: 497.5726471175265[0m
[37m[1m[2023-07-03 01:30:57,183][188188] Min Reward on eval: -356.38035061582923[0m
[37m[1m[2023-07-03 01:30:57,183][188188] Mean Reward across all agents: -41.538320741113026[0m
[37m[1m[2023-07-03 01:30:57,183][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:30:57,185][188188] mean_value=-2249.891719255185, max_value=-113.90196719232968[0m
[36m[2023-07-03 01:30:57,187][188188] XNES is restarting with a new solution whose measures are [0.54930007 0.51890004 0.55870003 0.04       3.79675221] and objective is -299.72908711768684[0m
[36m[2023-07-03 01:30:57,188][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:30:57,191][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:30:57,192][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:31:06,170][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 01:31:06,170][188188] FPS: 427777.41[0m
[36m[2023-07-03 01:31:06,172][188188] itr=97, itrs=2000, Progress: 4.85%[0m
[36m[2023-07-03 01:31:17,810][188188] train() took 11.62 seconds to complete[0m
[36m[2023-07-03 01:31:17,811][188188] FPS: 330441.46[0m
[36m[2023-07-03 01:31:22,046][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:31:22,046][188188] Reward + Measures: [[-59.55600047   0.219722     0.18276733   0.25393066   0.25382268
    3.59441328]][0m
[37m[1m[2023-07-03 01:31:22,046][188188] Max Reward on eval: -59.556000472071794[0m
[37m[1m[2023-07-03 01:31:22,047][188188] Min Reward on eval: -59.556000472071794[0m
[37m[1m[2023-07-03 01:31:22,047][188188] Mean Reward across all agents: -59.556000472071794[0m
[37m[1m[2023-07-03 01:31:22,047][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:31:27,116][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:31:27,116][188188] Reward + Measures: [[ -98.69397637    0.28830001    0.55010003    0.59030002    0.37639999
     3.90667892]
 [ 160.10966264    0.41510001    0.18240002    0.3872        0.38960001
     3.73405004]
 [  26.60262618    0.19069998    0.1693        0.17569999    0.1706
     3.59721375]
 ...
 [  45.58877938    0.13719998    0.16510001    0.16070001    0.10160001
     3.70466208]
 [-151.8460233     0.2467        0.25009999    0.27519998    0.12360001
     3.7857759 ]
 [  82.93575961    0.10940001    0.17910001    0.19380002    0.16779999
     3.67352152]][0m
[37m[1m[2023-07-03 01:31:27,116][188188] Max Reward on eval: 378.3432655526325[0m
[37m[1m[2023-07-03 01:31:27,117][188188] Min Reward on eval: -352.0051612995565[0m
[37m[1m[2023-07-03 01:31:27,117][188188] Mean Reward across all agents: 17.79000219884598[0m
[37m[1m[2023-07-03 01:31:27,117][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:31:27,119][188188] mean_value=-2900.764887878051, max_value=570.0488189582085[0m
[37m[1m[2023-07-03 01:31:27,122][188188] New mean coefficients: [[-0.78425336 -0.6344715  -0.82809407 -0.87339604 -1.9929088  -1.3979695 ]][0m
[37m[1m[2023-07-03 01:31:27,123][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:31:36,080][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 01:31:36,080][188188] FPS: 428774.80[0m
[36m[2023-07-03 01:31:36,083][188188] itr=98, itrs=2000, Progress: 4.90%[0m
[36m[2023-07-03 01:31:47,575][188188] train() took 11.47 seconds to complete[0m
[36m[2023-07-03 01:31:47,576][188188] FPS: 334670.79[0m
[36m[2023-07-03 01:31:51,903][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:31:51,904][188188] Reward + Measures: [[-208.39864128    0.31753966    0.416594      0.46455431    0.26670167
     3.76882935]][0m
[37m[1m[2023-07-03 01:31:51,904][188188] Max Reward on eval: -208.39864127555828[0m
[37m[1m[2023-07-03 01:31:51,904][188188] Min Reward on eval: -208.39864127555828[0m
[37m[1m[2023-07-03 01:31:51,904][188188] Mean Reward across all agents: -208.39864127555828[0m
[37m[1m[2023-07-03 01:31:51,905][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:31:56,939][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:31:56,939][188188] Reward + Measures: [[ 36.55534784   0.1213       0.33220002   0.3064       0.32539999
    3.86605954]
 [114.11932043   0.41799998   0.23529999   0.40460005   0.2318
    3.88581657]
 [ 46.84561918   0.15810001   0.3554       0.23729999   0.345
    3.7861805 ]
 ...
 [-90.05749457   0.1593       0.20899999   0.2509       0.30590001
    3.69343805]
 [ 22.89108925   0.16630001   0.211        0.1473       0.14970002
    3.72130942]
 [  0.10628793   0.1116       0.13700001   0.1432       0.14380001
    3.52993274]][0m
[37m[1m[2023-07-03 01:31:56,939][188188] Max Reward on eval: 231.26164991026744[0m
[37m[1m[2023-07-03 01:31:56,940][188188] Min Reward on eval: -344.0445260543376[0m
[37m[1m[2023-07-03 01:31:56,940][188188] Mean Reward across all agents: 7.4689840840291195[0m
[37m[1m[2023-07-03 01:31:56,940][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:31:56,942][188188] mean_value=-2702.437253151703, max_value=428.3277205794584[0m
[37m[1m[2023-07-03 01:31:56,945][188188] New mean coefficients: [[-0.19349581 -0.7401094   0.55641514 -0.2920665  -2.2255552  -1.2780385 ]][0m
[37m[1m[2023-07-03 01:31:56,946][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:32:05,912][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 01:32:05,912][188188] FPS: 428342.16[0m
[36m[2023-07-03 01:32:05,915][188188] itr=99, itrs=2000, Progress: 4.95%[0m
[36m[2023-07-03 01:32:17,486][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 01:32:17,486][188188] FPS: 332439.78[0m
[36m[2023-07-03 01:32:21,763][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:32:21,764][188188] Reward + Measures: [[-93.9067644    0.51615167   0.32380432   0.53168267   0.45410669
    3.78481841]][0m
[37m[1m[2023-07-03 01:32:21,764][188188] Max Reward on eval: -93.90676439971281[0m
[37m[1m[2023-07-03 01:32:21,764][188188] Min Reward on eval: -93.90676439971281[0m
[37m[1m[2023-07-03 01:32:21,764][188188] Mean Reward across all agents: -93.90676439971281[0m
[37m[1m[2023-07-03 01:32:21,765][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:32:26,737][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:32:26,738][188188] Reward + Measures: [[-410.83628851    0.86759996    0.82139999    0.93160003    0.1362
     3.893507  ]
 [  54.14340371    0.1102        0.0895        0.0983        0.1043
     3.61780548]
 [  39.18817405    0.1234        0.17620002    0.1753        0.18069999
     3.41816187]
 ...
 [  23.0233296     0.1296        0.10029999    0.09050001    0.0966
     3.67081118]
 [ 114.56799466    0.72649997    0.74230003    0.74270004    0.73839998
     3.8499341 ]
 [  75.30490973    0.81080002    0.79610002    0.81470007    0.80849999
     3.90904665]][0m
[37m[1m[2023-07-03 01:32:26,738][188188] Max Reward on eval: 290.7790742008016[0m
[37m[1m[2023-07-03 01:32:26,738][188188] Min Reward on eval: -423.8522949389182[0m
[37m[1m[2023-07-03 01:32:26,738][188188] Mean Reward across all agents: -42.85449179926961[0m
[37m[1m[2023-07-03 01:32:26,739][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:32:26,743][188188] mean_value=-1809.2324095944916, max_value=408.90558042011514[0m
[37m[1m[2023-07-03 01:32:26,745][188188] New mean coefficients: [[-0.6978487  -0.5118627   0.49477506 -0.08127679  0.45578265 -1.6983513 ]][0m
[37m[1m[2023-07-03 01:32:26,746][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:32:35,772][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 01:32:35,773][188188] FPS: 425509.00[0m
[36m[2023-07-03 01:32:35,775][188188] itr=100, itrs=2000, Progress: 5.00%[0m
[37m[1m[2023-07-03 01:32:38,119][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000080[0m
[36m[2023-07-03 01:32:50,126][188188] train() took 11.69 seconds to complete[0m
[36m[2023-07-03 01:32:50,126][188188] FPS: 328591.51[0m
[36m[2023-07-03 01:32:54,417][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:32:54,417][188188] Reward + Measures: [[-34.52641958   0.50152433   0.40046102   0.51762295   0.48900899
    3.80625939]][0m
[37m[1m[2023-07-03 01:32:54,418][188188] Max Reward on eval: -34.52641958045578[0m
[37m[1m[2023-07-03 01:32:54,418][188188] Min Reward on eval: -34.52641958045578[0m
[37m[1m[2023-07-03 01:32:54,418][188188] Mean Reward across all agents: -34.52641958045578[0m
[37m[1m[2023-07-03 01:32:54,418][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:32:59,411][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:32:59,411][188188] Reward + Measures: [[-129.54384083    0.57039994    0.704         0.713         0.20630001
     3.87664008]
 [  53.69874817    0.5011        0.40459999    0.49249998    0.46539998
     3.80125475]
 [ 106.70026433    0.4138        0.1688        0.29539999    0.39220002
     3.89313626]
 ...
 [  39.89488541    0.1166        0.0792        0.0656        0.0692
     3.59489608]
 [-122.07555098    0.63410002    0.61280006    0.64179999    0.0313
     3.8967514 ]
 [ -12.07694578    0.19970001    0.15380001    0.1823        0.1401
     3.55492663]][0m
[37m[1m[2023-07-03 01:32:59,411][188188] Max Reward on eval: 261.97864344054835[0m
[37m[1m[2023-07-03 01:32:59,412][188188] Min Reward on eval: -461.3504371609539[0m
[37m[1m[2023-07-03 01:32:59,412][188188] Mean Reward across all agents: -30.314938319030123[0m
[37m[1m[2023-07-03 01:32:59,412][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:32:59,415][188188] mean_value=-1721.9033283288547, max_value=413.5071141093998[0m
[37m[1m[2023-07-03 01:32:59,418][188188] New mean coefficients: [[-0.3379467  -0.06624317  0.2325958   0.3445558   0.8955827  -0.82832867]][0m
[37m[1m[2023-07-03 01:32:59,419][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:33:08,512][188188] train() took 9.09 seconds to complete[0m
[36m[2023-07-03 01:33:08,512][188188] FPS: 422389.18[0m
[36m[2023-07-03 01:33:08,514][188188] itr=101, itrs=2000, Progress: 5.05%[0m
[36m[2023-07-03 01:33:20,241][188188] train() took 11.71 seconds to complete[0m
[36m[2023-07-03 01:33:20,241][188188] FPS: 327967.47[0m
[36m[2023-07-03 01:33:24,490][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:33:24,491][188188] Reward + Measures: [[39.25386378  0.31435466  0.31912601  0.34237167  0.36897737  3.63343096]][0m
[37m[1m[2023-07-03 01:33:24,491][188188] Max Reward on eval: 39.253863784738[0m
[37m[1m[2023-07-03 01:33:24,491][188188] Min Reward on eval: 39.253863784738[0m
[37m[1m[2023-07-03 01:33:24,491][188188] Mean Reward across all agents: 39.253863784738[0m
[37m[1m[2023-07-03 01:33:24,492][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:33:29,483][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:33:29,484][188188] Reward + Measures: [[ 96.92357637   0.39700001   0.40279999   0.36179999   0.41480002
    3.72132468]
 [  8.3880247    0.20550001   0.30710003   0.2059       0.2924
    3.58891368]
 [ 74.2716577    0.36180001   0.43740001   0.34670001   0.44010001
    3.79275393]
 ...
 [ 11.03571383   0.53039998   0.52240002   0.55160004   0.55910003
    3.68965602]
 [117.14532661   0.4039       0.4179       0.41859999   0.4312
    3.75574994]
 [ -0.21176669   0.1353       0.18789999   0.13940001   0.1683
    3.62580872]][0m
[37m[1m[2023-07-03 01:33:29,484][188188] Max Reward on eval: 222.47039881497622[0m
[37m[1m[2023-07-03 01:33:29,484][188188] Min Reward on eval: -194.24543609451501[0m
[37m[1m[2023-07-03 01:33:29,484][188188] Mean Reward across all agents: 25.03326080372615[0m
[37m[1m[2023-07-03 01:33:29,485][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:33:29,486][188188] mean_value=-2275.823142033377, max_value=77.17800279626184[0m
[37m[1m[2023-07-03 01:33:29,488][188188] New mean coefficients: [[-1.1993713   0.2625592   0.15622684  2.1440854   1.8208888  -0.44402567]][0m
[37m[1m[2023-07-03 01:33:29,489][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:33:38,459][188188] train() took 8.97 seconds to complete[0m
[36m[2023-07-03 01:33:38,460][188188] FPS: 428180.94[0m
[36m[2023-07-03 01:33:38,462][188188] itr=102, itrs=2000, Progress: 5.10%[0m
[36m[2023-07-03 01:33:50,024][188188] train() took 11.54 seconds to complete[0m
[36m[2023-07-03 01:33:50,024][188188] FPS: 332620.21[0m
[36m[2023-07-03 01:33:54,280][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:33:54,280][188188] Reward + Measures: [[50.31349783  0.15130399  0.15599367  0.153565    0.16985367  3.56758142]][0m
[37m[1m[2023-07-03 01:33:54,281][188188] Max Reward on eval: 50.31349782502626[0m
[37m[1m[2023-07-03 01:33:54,281][188188] Min Reward on eval: 50.31349782502626[0m
[37m[1m[2023-07-03 01:33:54,281][188188] Mean Reward across all agents: 50.31349782502626[0m
[37m[1m[2023-07-03 01:33:54,282][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:33:59,432][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:33:59,433][188188] Reward + Measures: [[ 73.65652496   0.1127       0.0936       0.0847       0.1056
    3.72797465]
 [ 27.96755425   0.0855       0.0865       0.0739       0.0963
    3.5974443 ]
 [ 15.53755233   0.48550001   0.47070003   0.48010001   0.45860001
    3.82487178]
 ...
 [-57.57777626   0.3346       0.27849999   0.32790002   0.0682
    3.90261054]
 [ 90.99473626   0.4073       0.3989       0.42469999   0.3917
    3.83956075]
 [ 31.9226395    0.64320004   0.30939999   0.58630002   0.36559999
    3.88426661]][0m
[37m[1m[2023-07-03 01:33:59,433][188188] Max Reward on eval: 195.70801496468485[0m
[37m[1m[2023-07-03 01:33:59,433][188188] Min Reward on eval: -318.15024473853407[0m
[37m[1m[2023-07-03 01:33:59,433][188188] Mean Reward across all agents: 22.021787018523387[0m
[37m[1m[2023-07-03 01:33:59,434][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:33:59,435][188188] mean_value=-3249.6620050120696, max_value=278.770147225339[0m
[37m[1m[2023-07-03 01:33:59,437][188188] New mean coefficients: [[-0.13584292  2.2010038   0.71028554  1.6996613   2.1085005  -0.23385718]][0m
[37m[1m[2023-07-03 01:33:59,438][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:34:08,405][188188] train() took 8.97 seconds to complete[0m
[36m[2023-07-03 01:34:08,405][188188] FPS: 428326.81[0m
[36m[2023-07-03 01:34:08,407][188188] itr=103, itrs=2000, Progress: 5.15%[0m
[36m[2023-07-03 01:34:20,023][188188] train() took 11.60 seconds to complete[0m
[36m[2023-07-03 01:34:20,023][188188] FPS: 331083.74[0m
[36m[2023-07-03 01:34:24,281][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:34:24,281][188188] Reward + Measures: [[52.388305    0.25100568  0.20498334  0.24930766  0.21890667  3.67289257]][0m
[37m[1m[2023-07-03 01:34:24,281][188188] Max Reward on eval: 52.388305002609094[0m
[37m[1m[2023-07-03 01:34:24,282][188188] Min Reward on eval: 52.388305002609094[0m
[37m[1m[2023-07-03 01:34:24,282][188188] Mean Reward across all agents: 52.388305002609094[0m
[37m[1m[2023-07-03 01:34:24,282][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:34:29,186][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:34:29,186][188188] Reward + Measures: [[  3.71435007   0.19050001   0.16190001   0.1892       0.19630001
    3.69748855]
 [-29.13545488   0.22500001   0.32580003   0.1742       0.28300002
    3.78427315]
 [ 17.4650721    0.29700002   0.23480001   0.33540002   0.204
    3.76328659]
 ...
 [ 72.4246671    0.0999       0.1041       0.1135       0.1128
    3.67025924]
 [-34.7383037    0.15360001   0.1207       0.1673       0.1225
    3.70547295]
 [ 52.52438709   0.1525       0.17080002   0.1533       0.16860001
    3.70439982]][0m
[37m[1m[2023-07-03 01:34:29,186][188188] Max Reward on eval: 174.35558431539684[0m
[37m[1m[2023-07-03 01:34:29,187][188188] Min Reward on eval: -343.7443476016633[0m
[37m[1m[2023-07-03 01:34:29,187][188188] Mean Reward across all agents: 22.29086835180313[0m
[37m[1m[2023-07-03 01:34:29,187][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:34:29,189][188188] mean_value=-2568.4903600794332, max_value=382.53937097899615[0m
[37m[1m[2023-07-03 01:34:29,192][188188] New mean coefficients: [[-0.84867996  0.8459461   0.8472382   1.7731757   2.3468273  -2.0835223 ]][0m
[37m[1m[2023-07-03 01:34:29,192][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:34:38,183][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 01:34:38,183][188188] FPS: 427222.97[0m
[36m[2023-07-03 01:34:38,185][188188] itr=104, itrs=2000, Progress: 5.20%[0m
[36m[2023-07-03 01:34:49,730][188188] train() took 11.53 seconds to complete[0m
[36m[2023-07-03 01:34:49,730][188188] FPS: 333110.34[0m
[36m[2023-07-03 01:34:53,960][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:34:53,961][188188] Reward + Measures: [[60.39647329  0.14124434  0.17218432  0.16701767  0.20330866  3.60606766]][0m
[37m[1m[2023-07-03 01:34:53,961][188188] Max Reward on eval: 60.39647328620533[0m
[37m[1m[2023-07-03 01:34:53,961][188188] Min Reward on eval: 60.39647328620533[0m
[37m[1m[2023-07-03 01:34:53,961][188188] Mean Reward across all agents: 60.39647328620533[0m
[37m[1m[2023-07-03 01:34:53,962][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:34:58,931][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:34:58,931][188188] Reward + Measures: [[ 53.29840042   0.0764       0.1056       0.0908       0.0981
    3.46939778]
 [ 46.63209873   0.0822       0.0964       0.0741       0.1112
    3.80604339]
 [ 32.05959966   0.09120001   0.1117       0.1072       0.109
    3.50093269]
 ...
 [-34.56192066   0.1591       0.24090002   0.28400001   0.1859
    3.88439226]
 [ 49.81573831   0.0965       0.15620001   0.1656       0.1575
    3.82686424]
 [ 13.96091016   0.0907       0.06620001   0.0893       0.0719
    3.66343689]][0m
[37m[1m[2023-07-03 01:34:58,932][188188] Max Reward on eval: 198.36277768835424[0m
[37m[1m[2023-07-03 01:34:58,932][188188] Min Reward on eval: -427.3323192003183[0m
[37m[1m[2023-07-03 01:34:58,932][188188] Mean Reward across all agents: 18.045925982606324[0m
[37m[1m[2023-07-03 01:34:58,932][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:34:58,934][188188] mean_value=-3370.128597323917, max_value=205.81961676952707[0m
[37m[1m[2023-07-03 01:34:58,936][188188] New mean coefficients: [[-0.32202458  2.4142969   0.5249589   0.7868464   0.15791035 -1.4386053 ]][0m
[37m[1m[2023-07-03 01:34:58,937][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:35:07,925][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 01:35:07,925][188188] FPS: 427325.98[0m
[36m[2023-07-03 01:35:07,927][188188] itr=105, itrs=2000, Progress: 5.25%[0m
[36m[2023-07-03 01:35:19,593][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 01:35:19,594][188188] FPS: 329670.70[0m
[36m[2023-07-03 01:35:23,866][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:35:23,867][188188] Reward + Measures: [[53.06878654  0.10788666  0.14395165  0.13586299  0.16729265  3.67752075]][0m
[37m[1m[2023-07-03 01:35:23,867][188188] Max Reward on eval: 53.06878653733855[0m
[37m[1m[2023-07-03 01:35:23,867][188188] Min Reward on eval: 53.06878653733855[0m
[37m[1m[2023-07-03 01:35:23,867][188188] Mean Reward across all agents: 53.06878653733855[0m
[37m[1m[2023-07-03 01:35:23,867][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:35:28,945][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:35:28,946][188188] Reward + Measures: [[64.33591101  0.108       0.17650001  0.1312      0.1671      3.77815247]
 [ 4.93568928  0.1048      0.1085      0.13100001  0.15010001  3.70548701]
 [56.62571128  0.13410001  0.207       0.13820001  0.13500001  3.57295537]
 ...
 [40.46095975  0.1239      0.16070001  0.17740001  0.15900001  3.81997919]
 [73.4096571   0.14130001  0.3721      0.24890001  0.29190001  3.75222254]
 [-6.33284706  0.1269      0.20219998  0.1524      0.1249      3.69468927]][0m
[37m[1m[2023-07-03 01:35:28,946][188188] Max Reward on eval: 151.95165185919032[0m
[37m[1m[2023-07-03 01:35:28,946][188188] Min Reward on eval: -102.65513232126833[0m
[37m[1m[2023-07-03 01:35:28,946][188188] Mean Reward across all agents: 23.81255724139709[0m
[37m[1m[2023-07-03 01:35:28,947][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:35:28,948][188188] mean_value=-3454.2239148885965, max_value=-2.9351132788654866[0m
[36m[2023-07-03 01:35:28,950][188188] XNES is restarting with a new solution whose measures are [0.87330008 0.91920006 0.0201     0.90359992 3.96199012] and objective is 311.09094570670277[0m
[36m[2023-07-03 01:35:28,951][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:35:28,953][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:35:28,954][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:35:37,981][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 01:35:37,982][188188] FPS: 425459.59[0m
[36m[2023-07-03 01:35:37,984][188188] itr=106, itrs=2000, Progress: 5.30%[0m
[36m[2023-07-03 01:35:49,679][188188] train() took 11.67 seconds to complete[0m
[36m[2023-07-03 01:35:49,680][188188] FPS: 328913.77[0m
[36m[2023-07-03 01:35:53,976][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:35:53,976][188188] Reward + Measures: [[9.30999883 0.83990264 0.88198662 0.01984433 0.86896467 3.9822917 ]][0m
[37m[1m[2023-07-03 01:35:53,977][188188] Max Reward on eval: 9.309998828539241[0m
[37m[1m[2023-07-03 01:35:53,977][188188] Min Reward on eval: 9.309998828539241[0m
[37m[1m[2023-07-03 01:35:53,977][188188] Mean Reward across all agents: 9.309998828539241[0m
[37m[1m[2023-07-03 01:35:53,977][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:35:59,193][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:35:59,194][188188] Reward + Measures: [[-60.47529596   0.296        0.26289997   0.271        0.29260001
    3.8985734 ]
 [ 18.57086665   0.96239996   0.98899996   0.0022       0.98200005
    3.99622607]
 [ 26.27381709   0.9752       0.99270004   0.           0.99049997
    3.98790526]
 ...
 [-37.33624593   0.52130002   0.4894       0.42810002   0.55330002
    3.86287761]
 [-18.69322166   0.5212       0.51680005   0.50699997   0.53850001
    3.91302109]
 [-21.09523316   0.3671       0.57529998   0.27970001   0.56879997
    3.75872207]][0m
[37m[1m[2023-07-03 01:35:59,194][188188] Max Reward on eval: 432.3307799889706[0m
[37m[1m[2023-07-03 01:35:59,194][188188] Min Reward on eval: -296.389161605062[0m
[37m[1m[2023-07-03 01:35:59,194][188188] Mean Reward across all agents: 3.4362439263858247[0m
[37m[1m[2023-07-03 01:35:59,195][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:35:59,197][188188] mean_value=-1243.1522760487112, max_value=162.83302376082582[0m
[37m[1m[2023-07-03 01:35:59,200][188188] New mean coefficients: [[ 0.96444964 -1.2752111  -1.8795116  -2.7067697  -2.5758743  -0.69923586]][0m
[37m[1m[2023-07-03 01:35:59,201][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:36:08,247][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 01:36:08,247][188188] FPS: 424572.87[0m
[36m[2023-07-03 01:36:08,249][188188] itr=107, itrs=2000, Progress: 5.35%[0m
[36m[2023-07-03 01:36:19,778][188188] train() took 11.51 seconds to complete[0m
[36m[2023-07-03 01:36:19,779][188188] FPS: 333654.56[0m
[36m[2023-07-03 01:36:24,059][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:36:24,060][188188] Reward + Measures: [[2.7427659  0.77731264 0.81748706 0.02629067 0.80740833 3.98414087]][0m
[37m[1m[2023-07-03 01:36:24,060][188188] Max Reward on eval: 2.74276590404481[0m
[37m[1m[2023-07-03 01:36:24,060][188188] Min Reward on eval: 2.74276590404481[0m
[37m[1m[2023-07-03 01:36:24,060][188188] Mean Reward across all agents: 2.74276590404481[0m
[37m[1m[2023-07-03 01:36:24,060][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:36:29,033][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:36:29,034][188188] Reward + Measures: [[  -5.81574975    0.15700001    0.13609999    0.11439999    0.15640001
     3.78215408]
 [ -86.81333261    0.25940001    0.16870001    0.18970001    0.20209999
     3.78349185]
 [ -34.6386298     0.1397        0.1213        0.13649999    0.1261
     3.80638695]
 ...
 [-122.27064262    0.56470001    0.60120004    0.051         0.58740002
     3.87094736]
 [ -19.41528132    0.1089        0.13959999    0.0876        0.1199
     3.77384877]
 [-106.04809342    0.41019997    0.38649997    0.38180003    0.41029999
     3.9386425 ]][0m
[37m[1m[2023-07-03 01:36:29,034][188188] Max Reward on eval: 258.7344636583701[0m
[37m[1m[2023-07-03 01:36:29,034][188188] Min Reward on eval: -259.2556020760909[0m
[37m[1m[2023-07-03 01:36:29,035][188188] Mean Reward across all agents: -21.167502838329412[0m
[37m[1m[2023-07-03 01:36:29,035][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:36:29,036][188188] mean_value=-2412.2801768282707, max_value=-23.48755832105178[0m
[36m[2023-07-03 01:36:29,038][188188] XNES is restarting with a new solution whose measures are [0.87239999 0.30950001 0.58230001 0.85780001 3.99356914] and objective is 256.78027173979206[0m
[36m[2023-07-03 01:36:29,039][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:36:29,041][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:36:29,042][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:36:38,031][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 01:36:38,031][188188] FPS: 427268.16[0m
[36m[2023-07-03 01:36:38,034][188188] itr=108, itrs=2000, Progress: 5.40%[0m
[36m[2023-07-03 01:36:49,651][188188] train() took 11.60 seconds to complete[0m
[36m[2023-07-03 01:36:49,652][188188] FPS: 331113.46[0m
[36m[2023-07-03 01:36:53,904][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:36:53,904][188188] Reward + Measures: [[-0.46172881  0.77934796  0.75564063  0.08073833  0.79318035  3.99546409]][0m
[37m[1m[2023-07-03 01:36:53,905][188188] Max Reward on eval: -0.46172880747231343[0m
[37m[1m[2023-07-03 01:36:53,905][188188] Min Reward on eval: -0.46172880747231343[0m
[37m[1m[2023-07-03 01:36:53,905][188188] Mean Reward across all agents: -0.46172880747231343[0m
[37m[1m[2023-07-03 01:36:53,905][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:36:58,847][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:36:58,853][188188] Reward + Measures: [[ 99.43835728   0.80660003   0.63659996   0.26000002   0.83739996
    3.97764587]
 [  4.09352047   0.60339999   0.53119999   0.0932       0.59930003
    3.98884082]
 [-74.48418571   0.46849999   0.43889999   0.43179998   0.4736
    3.92586708]
 ...
 [-22.80506217   0.8344       0.78579998   0.11379999   0.8599
    3.99457622]
 [ 66.4859089    0.73430008   0.57159996   0.21570002   0.75470006
    3.97413683]
 [  5.10009699   0.35199997   0.3594       0.30489999   0.42180005
    3.70056605]][0m
[37m[1m[2023-07-03 01:36:58,853][188188] Max Reward on eval: 253.7300734376535[0m
[37m[1m[2023-07-03 01:36:58,853][188188] Min Reward on eval: -434.23028803709894[0m
[37m[1m[2023-07-03 01:36:58,854][188188] Mean Reward across all agents: 5.0601130529589815[0m
[37m[1m[2023-07-03 01:36:58,854][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:36:58,856][188188] mean_value=-879.1915966179132, max_value=26.255439557551853[0m
[37m[1m[2023-07-03 01:36:58,859][188188] New mean coefficients: [[-0.93911576 -0.719401   -1.6567223  -2.5556724  -1.321778   -0.5527203 ]][0m
[37m[1m[2023-07-03 01:36:58,860][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:37:07,807][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 01:37:07,807][188188] FPS: 429252.72[0m
[36m[2023-07-03 01:37:07,809][188188] itr=109, itrs=2000, Progress: 5.45%[0m
[36m[2023-07-03 01:37:19,361][188188] train() took 11.53 seconds to complete[0m
[36m[2023-07-03 01:37:19,361][188188] FPS: 332960.17[0m
[36m[2023-07-03 01:37:23,670][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:37:23,670][188188] Reward + Measures: [[-5.73273473  0.79649073  0.78088671  0.07198799  0.81160873  3.99560809]][0m
[37m[1m[2023-07-03 01:37:23,670][188188] Max Reward on eval: -5.732734733764024[0m
[37m[1m[2023-07-03 01:37:23,671][188188] Min Reward on eval: -5.732734733764024[0m
[37m[1m[2023-07-03 01:37:23,671][188188] Mean Reward across all agents: -5.732734733764024[0m
[37m[1m[2023-07-03 01:37:23,671][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:37:28,741][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:37:28,741][188188] Reward + Measures: [[-18.83398691   0.8531       0.7974       0.11969999   0.86630005
    3.99590993]
 [-84.16380703   0.49180004   0.52100003   0.0593       0.50239998
    3.79022479]
 [280.59032575   0.68050003   0.60530001   0.1674       0.69370002
    3.94579673]
 ...
 [-94.98346878   0.60789996   0.41169998   0.2218       0.59800005
    3.99258399]
 [-62.00376976   0.8556       0.88730001   0.027        0.88779992
    3.99855804]
 [-24.61669314   0.79890007   0.81119996   0.0221       0.8028
    3.99632239]][0m
[37m[1m[2023-07-03 01:37:28,741][188188] Max Reward on eval: 306.2010717144236[0m
[37m[1m[2023-07-03 01:37:28,742][188188] Min Reward on eval: -346.0729510494042[0m
[37m[1m[2023-07-03 01:37:28,742][188188] Mean Reward across all agents: -15.050517346244014[0m
[37m[1m[2023-07-03 01:37:28,742][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:37:28,744][188188] mean_value=-572.1367196160725, max_value=592.1677640501189[0m
[37m[1m[2023-07-03 01:37:28,747][188188] New mean coefficients: [[-0.28476226 -0.9655447  -1.4667387  -2.7346544  -1.8176908  -1.3223944 ]][0m
[37m[1m[2023-07-03 01:37:28,748][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:37:37,799][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 01:37:37,799][188188] FPS: 424321.11[0m
[36m[2023-07-03 01:37:37,802][188188] itr=110, itrs=2000, Progress: 5.50%[0m
[37m[1m[2023-07-03 01:37:40,179][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000090[0m
[36m[2023-07-03 01:37:52,275][188188] train() took 11.78 seconds to complete[0m
[36m[2023-07-03 01:37:52,276][188188] FPS: 326091.14[0m
[36m[2023-07-03 01:37:56,513][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:37:56,514][188188] Reward + Measures: [[10.39695803  0.74042559  0.66520303  0.13060066  0.74637908  3.99466133]][0m
[37m[1m[2023-07-03 01:37:56,514][188188] Max Reward on eval: 10.396958028741123[0m
[37m[1m[2023-07-03 01:37:56,514][188188] Min Reward on eval: 10.396958028741123[0m
[37m[1m[2023-07-03 01:37:56,514][188188] Mean Reward across all agents: 10.396958028741123[0m
[37m[1m[2023-07-03 01:37:56,515][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:38:01,601][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:38:01,602][188188] Reward + Measures: [[ 120.81558034    0.52109998    0.78630006    0.24700002    0.75419998
     3.98855209]
 [-160.37427956    0.42700002    0.43430001    0.42349997    0.41800004
     3.9592042 ]
 [ -19.71609212    0.77880007    0.80510008    0.0227        0.79769999
     3.99623609]
 ...
 [  90.41281318    0.3337        0.73460001    0.42630002    0.76240009
     3.99411058]
 [ -25.14284184    0.37900001    0.1444        0.31500003    0.34029999
     3.93814278]
 [-231.11464046    0.52310002    0.7446        0.226         0.73549998
     3.99113536]][0m
[37m[1m[2023-07-03 01:38:01,602][188188] Max Reward on eval: 426.8985223766416[0m
[37m[1m[2023-07-03 01:38:01,602][188188] Min Reward on eval: -689.9753036569571[0m
[37m[1m[2023-07-03 01:38:01,603][188188] Mean Reward across all agents: -36.85660701300511[0m
[37m[1m[2023-07-03 01:38:01,603][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:38:01,605][188188] mean_value=-1107.3005871266848, max_value=221.43260204954996[0m
[37m[1m[2023-07-03 01:38:01,607][188188] New mean coefficients: [[ 0.10601711  0.07066548 -1.2521259  -2.3617535  -0.97923374 -1.1132762 ]][0m
[37m[1m[2023-07-03 01:38:01,608][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:38:10,562][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 01:38:10,563][188188] FPS: 428938.08[0m
[36m[2023-07-03 01:38:10,565][188188] itr=111, itrs=2000, Progress: 5.55%[0m
[36m[2023-07-03 01:38:22,190][188188] train() took 11.60 seconds to complete[0m
[36m[2023-07-03 01:38:22,190][188188] FPS: 330900.69[0m
[36m[2023-07-03 01:38:26,454][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:38:26,454][188188] Reward + Measures: [[0.61298806 0.75243872 0.64225668 0.15956767 0.753497   3.99527526]][0m
[37m[1m[2023-07-03 01:38:26,455][188188] Max Reward on eval: 0.6129880597893789[0m
[37m[1m[2023-07-03 01:38:26,455][188188] Min Reward on eval: 0.6129880597893789[0m
[37m[1m[2023-07-03 01:38:26,455][188188] Mean Reward across all agents: 0.6129880597893789[0m
[37m[1m[2023-07-03 01:38:26,455][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:38:31,385][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:38:31,386][188188] Reward + Measures: [[-24.51790822   0.73979998   0.72640002   0.08350001   0.74060005
    3.98859024]
 [-25.80919019   0.90060008   0.0292       0.82789993   0.80140001
    3.99579167]
 [ 44.19430065   0.80040008   0.61969995   0.1927       0.79570001
    3.94650078]
 ...
 [-16.26951149   0.57050008   0.61869997   0.07969999   0.59860003
    3.99655461]
 [341.8173779    0.63679999   0.70600003   0.0378       0.71060002
    3.77976227]
 [-24.25556693   0.5618       0.55629998   0.54330003   0.57880002
    3.84336352]][0m
[37m[1m[2023-07-03 01:38:31,386][188188] Max Reward on eval: 341.8173778988421[0m
[37m[1m[2023-07-03 01:38:31,386][188188] Min Reward on eval: -490.36406326657163[0m
[37m[1m[2023-07-03 01:38:31,386][188188] Mean Reward across all agents: 25.866616621632364[0m
[37m[1m[2023-07-03 01:38:31,387][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:38:31,389][188188] mean_value=-451.34658258493187, max_value=521.2590570252378[0m
[37m[1m[2023-07-03 01:38:31,392][188188] New mean coefficients: [[ 0.83237    -0.73783505 -1.1700214  -3.1218219  -1.1434197  -0.25045973]][0m
[37m[1m[2023-07-03 01:38:31,392][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:38:40,348][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 01:38:40,349][188188] FPS: 428840.98[0m
[36m[2023-07-03 01:38:40,351][188188] itr=112, itrs=2000, Progress: 5.60%[0m
[36m[2023-07-03 01:38:51,932][188188] train() took 11.56 seconds to complete[0m
[36m[2023-07-03 01:38:51,932][188188] FPS: 332089.24[0m
[36m[2023-07-03 01:38:56,217][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:38:56,217][188188] Reward + Measures: [[4.01398023 0.66603899 0.57326132 0.15186267 0.66934603 3.9925983 ]][0m
[37m[1m[2023-07-03 01:38:56,217][188188] Max Reward on eval: 4.013980234815458[0m
[37m[1m[2023-07-03 01:38:56,218][188188] Min Reward on eval: 4.013980234815458[0m
[37m[1m[2023-07-03 01:38:56,218][188188] Mean Reward across all agents: 4.013980234815458[0m
[37m[1m[2023-07-03 01:38:56,218][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:39:01,196][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:39:01,197][188188] Reward + Measures: [[-29.76380124   0.26859999   0.26369998   0.25130001   0.27080002
    3.79041338]
 [-52.4890636    0.1842       0.2509       0.1268       0.2201
    3.7014451 ]
 [-38.22374533   0.6261       0.51740003   0.14820001   0.61150002
    3.99662209]
 ...
 [-20.09752545   0.13640001   0.13590001   0.14049999   0.13250001
    3.72792411]
 [-35.69163512   0.69819999   0.20550001   0.51470006   0.65259999
    3.9814775 ]
 [-40.05816686   0.14930001   0.14119999   0.19509999   0.2168
    3.82517743]][0m
[37m[1m[2023-07-03 01:39:01,197][188188] Max Reward on eval: 465.01048868577925[0m
[37m[1m[2023-07-03 01:39:01,197][188188] Min Reward on eval: -290.93459961223414[0m
[37m[1m[2023-07-03 01:39:01,198][188188] Mean Reward across all agents: 9.687705290158355[0m
[37m[1m[2023-07-03 01:39:01,198][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:39:01,200][188188] mean_value=-2051.8096498265227, max_value=242.99518897752768[0m
[37m[1m[2023-07-03 01:39:01,202][188188] New mean coefficients: [[ 0.76254267  0.07930458 -0.24925327 -2.247343    0.22096217  0.11303148]][0m
[37m[1m[2023-07-03 01:39:01,203][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:39:10,215][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 01:39:10,215][188188] FPS: 426186.23[0m
[36m[2023-07-03 01:39:10,217][188188] itr=113, itrs=2000, Progress: 5.65%[0m
[36m[2023-07-03 01:39:21,746][188188] train() took 11.51 seconds to complete[0m
[36m[2023-07-03 01:39:21,746][188188] FPS: 333641.53[0m
[36m[2023-07-03 01:39:25,963][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:39:25,968][188188] Reward + Measures: [[5.98820143 0.69858462 0.52770698 0.22387597 0.69245768 3.99207711]][0m
[37m[1m[2023-07-03 01:39:25,969][188188] Max Reward on eval: 5.988201430250657[0m
[37m[1m[2023-07-03 01:39:25,969][188188] Min Reward on eval: 5.988201430250657[0m
[37m[1m[2023-07-03 01:39:25,969][188188] Mean Reward across all agents: 5.988201430250657[0m
[37m[1m[2023-07-03 01:39:25,970][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:39:30,906][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:39:30,907][188188] Reward + Measures: [[-42.0085712    0.14720002   0.43860003   0.54240006   0.69770002
    3.84534502]
 [-25.69401315   0.83129996   0.778        0.12750001   0.84770006
    3.97061348]
 [-23.4951508    0.1409       0.1508       0.15089999   0.14710002
    3.8725152 ]
 ...
 [123.20873677   0.3299       0.36489996   0.13950001   0.27020001
    3.92783356]
 [ 40.44041233   0.63330001   0.53050005   0.1116       0.62700003
    3.99257469]
 [-12.07576611   0.0854       0.12219999   0.0946       0.0573
    3.93631911]][0m
[37m[1m[2023-07-03 01:39:30,907][188188] Max Reward on eval: 349.89371781731023[0m
[37m[1m[2023-07-03 01:39:30,907][188188] Min Reward on eval: -645.1366915656254[0m
[37m[1m[2023-07-03 01:39:30,907][188188] Mean Reward across all agents: -9.842755652531187[0m
[37m[1m[2023-07-03 01:39:30,908][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:39:30,910][188188] mean_value=-1095.9669419313068, max_value=22.466343467453896[0m
[37m[1m[2023-07-03 01:39:30,912][188188] New mean coefficients: [[-0.05879956  0.32423723 -0.6487745  -1.8662957  -0.11566347  0.9529743 ]][0m
[37m[1m[2023-07-03 01:39:30,913][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:39:39,896][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 01:39:39,896][188188] FPS: 427557.55[0m
[36m[2023-07-03 01:39:39,899][188188] itr=114, itrs=2000, Progress: 5.70%[0m
[36m[2023-07-03 01:39:51,598][188188] train() took 11.68 seconds to complete[0m
[36m[2023-07-03 01:39:51,598][188188] FPS: 328715.39[0m
[36m[2023-07-03 01:39:55,938][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:39:55,939][188188] Reward + Measures: [[-2.32230797  0.72561699  0.54981363  0.22049268  0.72104132  3.99257708]][0m
[37m[1m[2023-07-03 01:39:55,939][188188] Max Reward on eval: -2.3223079730770646[0m
[37m[1m[2023-07-03 01:39:55,939][188188] Min Reward on eval: -2.3223079730770646[0m
[37m[1m[2023-07-03 01:39:55,939][188188] Mean Reward across all agents: -2.3223079730770646[0m
[37m[1m[2023-07-03 01:39:55,940][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:40:00,976][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:40:00,982][188188] Reward + Measures: [[-47.26185399   0.10640001   0.0959       0.1115       0.1147
    3.76734161]
 [ 24.25618648   0.64890003   0.60050005   0.1012       0.66100001
    3.9796102 ]
 [-27.0186017    0.08050001   0.0893       0.0844       0.0949
    3.58863306]
 ...
 [-39.08122408   0.54890007   0.57450002   0.0583       0.56750005
    3.95737004]
 [-12.0142845    0.07050001   0.0922       0.08900001   0.1349
    3.6520431 ]
 [ 20.58821569   0.85540003   0.58770001   0.28810003   0.86650002
    3.99162912]][0m
[37m[1m[2023-07-03 01:40:00,982][188188] Max Reward on eval: 375.9163000966422[0m
[37m[1m[2023-07-03 01:40:00,982][188188] Min Reward on eval: -271.82950320700184[0m
[37m[1m[2023-07-03 01:40:00,983][188188] Mean Reward across all agents: -3.305182446778632[0m
[37m[1m[2023-07-03 01:40:00,983][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:40:00,985][188188] mean_value=-1258.002676099456, max_value=120.91046879998393[0m
[37m[1m[2023-07-03 01:40:00,987][188188] New mean coefficients: [[-0.725436   0.8631883 -2.1869197 -1.1641788 -0.2740118  0.5488571]][0m
[37m[1m[2023-07-03 01:40:00,988][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:40:10,046][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 01:40:10,046][188188] FPS: 424024.75[0m
[36m[2023-07-03 01:40:10,049][188188] itr=115, itrs=2000, Progress: 5.75%[0m
[36m[2023-07-03 01:40:21,802][188188] train() took 11.73 seconds to complete[0m
[36m[2023-07-03 01:40:21,802][188188] FPS: 327292.78[0m
[36m[2023-07-03 01:40:26,138][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:40:26,139][188188] Reward + Measures: [[-23.90077693   0.70533901   0.31024802   0.41860497   0.67316437
    3.9932375 ]][0m
[37m[1m[2023-07-03 01:40:26,139][188188] Max Reward on eval: -23.90077693323405[0m
[37m[1m[2023-07-03 01:40:26,139][188188] Min Reward on eval: -23.90077693323405[0m
[37m[1m[2023-07-03 01:40:26,139][188188] Mean Reward across all agents: -23.90077693323405[0m
[37m[1m[2023-07-03 01:40:26,139][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:40:31,295][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:40:31,296][188188] Reward + Measures: [[-233.83496814    0.69940001    0.2093        0.5176        0.63849998
     3.99377513]
 [-102.18747081    0.85729998    0.08550001    0.81949997    0.78730005
     3.77862334]
 [  32.12576271    0.8933        0.4003        0.9073        0.49330002
     3.99862671]
 ...
 [  99.06621932    0.61309999    0.25760004    0.3845        0.59120005
     3.94006467]
 [ 174.94090078    0.82580006    0.0121        0.79640001    0.78960001
     3.64550638]
 [ -16.67747428    0.09420001    0.0969        0.10039999    0.1191
     3.713696  ]][0m
[37m[1m[2023-07-03 01:40:31,296][188188] Max Reward on eval: 466.3257875395939[0m
[37m[1m[2023-07-03 01:40:31,296][188188] Min Reward on eval: -400.71519565824417[0m
[37m[1m[2023-07-03 01:40:31,296][188188] Mean Reward across all agents: -21.146137258939888[0m
[37m[1m[2023-07-03 01:40:31,297][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:40:31,300][188188] mean_value=-862.0087966713156, max_value=503.52771839648483[0m
[37m[1m[2023-07-03 01:40:31,303][188188] New mean coefficients: [[-1.2443323   1.2328497  -3.563321   -2.3082955   0.27742416  0.08055937]][0m
[37m[1m[2023-07-03 01:40:31,304][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:40:40,330][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 01:40:40,330][188188] FPS: 425525.76[0m
[36m[2023-07-03 01:40:40,332][188188] itr=116, itrs=2000, Progress: 5.80%[0m
[36m[2023-07-03 01:40:51,976][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 01:40:51,976][188188] FPS: 330303.17[0m
[36m[2023-07-03 01:40:56,207][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:40:56,208][188188] Reward + Measures: [[8.991628   0.71634001 0.60843933 0.15114467 0.71921468 3.98952413]][0m
[37m[1m[2023-07-03 01:40:56,208][188188] Max Reward on eval: 8.991628000328143[0m
[37m[1m[2023-07-03 01:40:56,208][188188] Min Reward on eval: 8.991628000328143[0m
[37m[1m[2023-07-03 01:40:56,209][188188] Mean Reward across all agents: 8.991628000328143[0m
[37m[1m[2023-07-03 01:40:56,209][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:41:01,145][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:41:01,146][188188] Reward + Measures: [[-338.91334884    0.65270001    0.7658        0.0466        0.77059996
     3.90196776]
 [ -25.48525328    0.81910002    0.86589998    0.0228        0.85290003
     3.98745537]
 [   7.60138775    0.42520005    0.39940003    0.15989999    0.50190002
     3.89316916]
 ...
 [ -37.1674635     0.27239999    0.33380002    0.075         0.28749999
     3.92146921]
 [ 141.91137363    0.7216        0.77200001    0.0227        0.81700003
     3.77653575]
 [  50.80463865    0.1436        0.1145        0.1389        0.14790002
     3.74985433]][0m
[37m[1m[2023-07-03 01:41:01,146][188188] Max Reward on eval: 405.3844084921293[0m
[37m[1m[2023-07-03 01:41:01,146][188188] Min Reward on eval: -368.6444528355263[0m
[37m[1m[2023-07-03 01:41:01,147][188188] Mean Reward across all agents: -21.219389246555764[0m
[37m[1m[2023-07-03 01:41:01,147][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:41:01,148][188188] mean_value=-1578.7485438247336, max_value=74.6818973135176[0m
[37m[1m[2023-07-03 01:41:01,151][188188] New mean coefficients: [[-2.7927885   0.1979351  -1.6394317  -1.8082407   0.46768722 -0.5395185 ]][0m
[37m[1m[2023-07-03 01:41:01,152][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:41:10,142][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 01:41:10,142][188188] FPS: 427219.21[0m
[36m[2023-07-03 01:41:10,144][188188] itr=117, itrs=2000, Progress: 5.85%[0m
[36m[2023-07-03 01:41:21,796][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 01:41:21,796][188188] FPS: 330059.41[0m
[36m[2023-07-03 01:41:26,083][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:41:26,083][188188] Reward + Measures: [[5.59317145 0.71831834 0.63334668 0.13226368 0.72296071 3.99085402]][0m
[37m[1m[2023-07-03 01:41:26,083][188188] Max Reward on eval: 5.5931714457341[0m
[37m[1m[2023-07-03 01:41:26,083][188188] Min Reward on eval: 5.5931714457341[0m
[37m[1m[2023-07-03 01:41:26,084][188188] Mean Reward across all agents: 5.5931714457341[0m
[37m[1m[2023-07-03 01:41:26,084][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:41:31,152][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:41:31,153][188188] Reward + Measures: [[ -9.38760441   0.54660004   0.0398       0.58470005   0.50780004
    3.99188542]
 [-54.84059328   0.68839997   0.1499       0.5255       0.62850004
    3.98928809]
 [-20.09542262   0.15899999   0.1707       0.0474       0.14229999
    3.89862299]
 ...
 [-48.4075855    0.001        0.98979998   0.98009998   0.9939
    3.9979744 ]
 [  1.90356239   0.15020001   0.10640001   0.1145       0.15480001
    3.94005322]
 [-35.55181289   0.0826       0.79689997   0.89289999   0.86180001
    3.99463511]][0m
[37m[1m[2023-07-03 01:41:31,153][188188] Max Reward on eval: 333.44337461832913[0m
[37m[1m[2023-07-03 01:41:31,153][188188] Min Reward on eval: -621.9026236955076[0m
[37m[1m[2023-07-03 01:41:31,154][188188] Mean Reward across all agents: -5.936102786343146[0m
[37m[1m[2023-07-03 01:41:31,154][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:41:31,157][188188] mean_value=-924.0886005721044, max_value=487.8503165857401[0m
[37m[1m[2023-07-03 01:41:31,159][188188] New mean coefficients: [[-1.743471    0.65167856 -1.750959   -1.4065545   1.9334234  -0.35831642]][0m
[37m[1m[2023-07-03 01:41:31,160][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:41:40,194][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 01:41:40,194][188188] FPS: 425160.93[0m
[36m[2023-07-03 01:41:40,196][188188] itr=118, itrs=2000, Progress: 5.90%[0m
[36m[2023-07-03 01:41:52,027][188188] train() took 11.81 seconds to complete[0m
[36m[2023-07-03 01:41:52,027][188188] FPS: 325112.17[0m
[36m[2023-07-03 01:41:56,295][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:41:56,295][188188] Reward + Measures: [[-3.6458407   0.83114266  0.80063802  0.07632434  0.84471595  3.99437308]][0m
[37m[1m[2023-07-03 01:41:56,296][188188] Max Reward on eval: -3.6458407022308092[0m
[37m[1m[2023-07-03 01:41:56,296][188188] Min Reward on eval: -3.6458407022308092[0m
[37m[1m[2023-07-03 01:41:56,296][188188] Mean Reward across all agents: -3.6458407022308092[0m
[37m[1m[2023-07-03 01:41:56,296][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:42:01,345][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:42:01,346][188188] Reward + Measures: [[ -22.17493193    0.96530002    0.97240001    0.0131        0.97749996
     3.99524999]
 [ -15.08750833    0.74669999    0.78880006    0.0238        0.77300006
     3.99122286]
 [-108.71848971    0.15369999    0.36090001    0.39050004    0.46030003
     3.90838933]
 ...
 [-183.85231783    0.11240001    0.69569999    0.73130006    0.87810004
     3.95801473]
 [ -45.34793409    0.73989999    0.7762        0.0396        0.76579994
     3.98020625]
 [   8.65045405    0.11310001    0.092         0.0966        0.1248
     3.75833058]][0m
[37m[1m[2023-07-03 01:42:01,346][188188] Max Reward on eval: 355.85679839830846[0m
[37m[1m[2023-07-03 01:42:01,346][188188] Min Reward on eval: -250.09475394757465[0m
[37m[1m[2023-07-03 01:42:01,346][188188] Mean Reward across all agents: -5.742464540399261[0m
[37m[1m[2023-07-03 01:42:01,347][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:42:01,348][188188] mean_value=-1121.975643635136, max_value=17.686097488345013[0m
[37m[1m[2023-07-03 01:42:01,351][188188] New mean coefficients: [[-1.1043551   0.2900857  -1.508708   -0.5241327   2.511502   -0.40919495]][0m
[37m[1m[2023-07-03 01:42:01,352][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:42:10,380][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 01:42:10,380][188188] FPS: 425403.10[0m
[36m[2023-07-03 01:42:10,383][188188] itr=119, itrs=2000, Progress: 5.95%[0m
[36m[2023-07-03 01:42:22,162][188188] train() took 11.76 seconds to complete[0m
[36m[2023-07-03 01:42:22,163][188188] FPS: 326562.99[0m
[36m[2023-07-03 01:42:26,401][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:42:26,402][188188] Reward + Measures: [[2.46866716 0.82323372 0.746297   0.11920166 0.83194464 3.9935813 ]][0m
[37m[1m[2023-07-03 01:42:26,402][188188] Max Reward on eval: 2.4686671561858384[0m
[37m[1m[2023-07-03 01:42:26,402][188188] Min Reward on eval: 2.4686671561858384[0m
[37m[1m[2023-07-03 01:42:26,402][188188] Mean Reward across all agents: 2.4686671561858384[0m
[37m[1m[2023-07-03 01:42:26,403][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:42:31,498][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:42:31,499][188188] Reward + Measures: [[ 23.97216422   0.76600003   0.7026       0.13160001   0.77439994
    3.99328732]
 [ 37.72075887   0.86700004   0.87490004   0.0081       0.88140005
    3.96854281]
 [-28.82472983   0.74089998   0.38589999   0.56199998   0.74610007
    3.79771233]
 ...
 [-80.8578727    0.54269999   0.25850001   0.3761       0.54729998
    3.94050455]
 [ 51.74846546   0.93540001   0.88330001   0.1028       0.96590006
    3.99683547]
 [ 27.43248179   0.30490002   0.26569998   0.16850002   0.41960001
    3.80307579]][0m
[37m[1m[2023-07-03 01:42:31,499][188188] Max Reward on eval: 283.4290942002088[0m
[37m[1m[2023-07-03 01:42:31,500][188188] Min Reward on eval: -418.60932085042003[0m
[37m[1m[2023-07-03 01:42:31,500][188188] Mean Reward across all agents: 10.060648139166094[0m
[37m[1m[2023-07-03 01:42:31,500][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:42:31,502][188188] mean_value=-679.5251410037894, max_value=489.25162387862804[0m
[37m[1m[2023-07-03 01:42:31,504][188188] New mean coefficients: [[-0.92563576  0.32287213 -3.065199   -0.63883185  2.9330285   0.39469045]][0m
[37m[1m[2023-07-03 01:42:31,505][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:42:40,505][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 01:42:40,505][188188] FPS: 426785.07[0m
[36m[2023-07-03 01:42:40,507][188188] itr=120, itrs=2000, Progress: 6.00%[0m
[37m[1m[2023-07-03 01:42:42,879][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000100[0m
[36m[2023-07-03 01:42:54,829][188188] train() took 11.62 seconds to complete[0m
[36m[2023-07-03 01:42:54,829][188188] FPS: 330480.04[0m
[36m[2023-07-03 01:42:59,104][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:42:59,104][188188] Reward + Measures: [[8.40559964 0.671709   0.4995023  0.217767   0.67227638 3.98697066]][0m
[37m[1m[2023-07-03 01:42:59,104][188188] Max Reward on eval: 8.405599643661349[0m
[37m[1m[2023-07-03 01:42:59,105][188188] Min Reward on eval: 8.405599643661349[0m
[37m[1m[2023-07-03 01:42:59,105][188188] Mean Reward across all agents: 8.405599643661349[0m
[37m[1m[2023-07-03 01:42:59,105][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:43:04,081][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:43:04,082][188188] Reward + Measures: [[ 212.44380492    0.3687        0.30350003    0.32519999    0.41620001
     3.91684651]
 [  62.24806924    0.46970001    0.3545        0.21730001    0.45860004
     3.74632955]
 [-175.89359259    0.91119999    0.60140002    0.78879994    0.4219
     3.98682332]
 ...
 [  39.1407213     0.28169999    0.1814        0.11229999    0.24950002
     3.78775024]
 [  68.59765624    0.43169999    0.54930001    0.4542        0.24150001
     3.97798848]
 [  98.47768171    0.53590006    0.3134        0.27110001    0.53480005
     3.9936409 ]][0m
[37m[1m[2023-07-03 01:43:04,082][188188] Max Reward on eval: 471.93714521601794[0m
[37m[1m[2023-07-03 01:43:04,082][188188] Min Reward on eval: -392.8198328361847[0m
[37m[1m[2023-07-03 01:43:04,083][188188] Mean Reward across all agents: 5.442495835623077[0m
[37m[1m[2023-07-03 01:43:04,083][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:43:04,086][188188] mean_value=-919.3959141049454, max_value=460.79875472926443[0m
[37m[1m[2023-07-03 01:43:04,088][188188] New mean coefficients: [[-0.7795286   0.28820106 -3.208542   -0.42292488  4.0844026  -0.37714672]][0m
[37m[1m[2023-07-03 01:43:04,089][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:43:13,108][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 01:43:13,108][188188] FPS: 425860.15[0m
[36m[2023-07-03 01:43:13,111][188188] itr=121, itrs=2000, Progress: 6.05%[0m
[36m[2023-07-03 01:43:24,661][188188] train() took 11.53 seconds to complete[0m
[36m[2023-07-03 01:43:24,661][188188] FPS: 333012.47[0m
[36m[2023-07-03 01:43:28,921][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:43:28,922][188188] Reward + Measures: [[20.67620688  0.5852977   0.21483466  0.39188534  0.55961233  3.96675897]][0m
[37m[1m[2023-07-03 01:43:28,922][188188] Max Reward on eval: 20.67620688445068[0m
[37m[1m[2023-07-03 01:43:28,922][188188] Min Reward on eval: 20.67620688445068[0m
[37m[1m[2023-07-03 01:43:28,922][188188] Mean Reward across all agents: 20.67620688445068[0m
[37m[1m[2023-07-03 01:43:28,923][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:43:33,995][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:43:33,995][188188] Reward + Measures: [[  -6.44841912    0.69889998    0.3091        0.40009999    0.63749999
     3.8133564 ]
 [ -58.21253673    0.81560004    0.84560007    0.0195        0.82699996
     3.98872423]
 [-105.74632263    0.4786        0.49400002    0.0368        0.51310003
     3.77867174]
 ...
 [   1.73091309    0.20710002    0.1981        0.19579999    0.20130001
     3.83716512]
 [-115.62808896    0.80919999    0.17959999    0.62800002    0.72260004
     3.96407318]
 [  93.99179266    0.65670007    0.63730001    0.64539999    0.62279999
     3.91395879]][0m
[37m[1m[2023-07-03 01:43:33,996][188188] Max Reward on eval: 632.3873634465156[0m
[37m[1m[2023-07-03 01:43:33,996][188188] Min Reward on eval: -539.0678634582088[0m
[37m[1m[2023-07-03 01:43:33,996][188188] Mean Reward across all agents: 5.1835054407417385[0m
[37m[1m[2023-07-03 01:43:33,996][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:43:33,999][188188] mean_value=-808.2559547856295, max_value=373.10364977984284[0m
[37m[1m[2023-07-03 01:43:34,001][188188] New mean coefficients: [[-0.33553034 -0.56320226 -3.832279   -0.8035587   3.3112552   0.54439443]][0m
[37m[1m[2023-07-03 01:43:34,002][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:43:42,994][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 01:43:42,994][188188] FPS: 427130.11[0m
[36m[2023-07-03 01:43:42,997][188188] itr=122, itrs=2000, Progress: 6.10%[0m
[36m[2023-07-03 01:43:54,952][188188] train() took 11.93 seconds to complete[0m
[36m[2023-07-03 01:43:54,952][188188] FPS: 321764.91[0m
[36m[2023-07-03 01:43:59,284][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:43:59,285][188188] Reward + Measures: [[32.88547103  0.56511164  0.3416543   0.26486766  0.55805498  3.98543882]][0m
[37m[1m[2023-07-03 01:43:59,285][188188] Max Reward on eval: 32.8854710275854[0m
[37m[1m[2023-07-03 01:43:59,285][188188] Min Reward on eval: 32.8854710275854[0m
[37m[1m[2023-07-03 01:43:59,286][188188] Mean Reward across all agents: 32.8854710275854[0m
[37m[1m[2023-07-03 01:43:59,286][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:44:04,240][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:44:04,247][188188] Reward + Measures: [[-45.87169838   0.43919998   0.54570001   0.24630001   0.43050003
    3.9961133 ]
 [162.11009627   0.4612       0.46970001   0.0417       0.4993
    3.79292941]
 [181.89264682   0.18350001   0.2701       0.2093       0.28650001
    3.9909699 ]
 ...
 [-55.83701765   0.24600001   0.2289       0.1797       0.27480003
    3.70281005]
 [167.27781321   0.87210006   0.0263       0.8495       0.80709994
    3.99494553]
 [423.85978743   0.22090001   0.61630005   0.54250002   0.53820002
    3.98531413]][0m
[37m[1m[2023-07-03 01:44:04,248][188188] Max Reward on eval: 423.8597874327912[0m
[37m[1m[2023-07-03 01:44:04,248][188188] Min Reward on eval: -365.13252353286373[0m
[37m[1m[2023-07-03 01:44:04,249][188188] Mean Reward across all agents: 27.65378301412169[0m
[37m[1m[2023-07-03 01:44:04,249][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:44:04,254][188188] mean_value=-1438.014093201178, max_value=88.22532375110586[0m
[37m[1m[2023-07-03 01:44:04,259][188188] New mean coefficients: [[ 0.08638805 -0.49272168 -3.1384618  -0.6386316   3.5159125  -0.83750457]][0m
[37m[1m[2023-07-03 01:44:04,260][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:44:13,156][188188] train() took 8.89 seconds to complete[0m
[36m[2023-07-03 01:44:13,157][188188] FPS: 431765.24[0m
[36m[2023-07-03 01:44:13,159][188188] itr=123, itrs=2000, Progress: 6.15%[0m
[36m[2023-07-03 01:44:24,669][188188] train() took 11.49 seconds to complete[0m
[36m[2023-07-03 01:44:24,670][188188] FPS: 334124.32[0m
[36m[2023-07-03 01:44:28,959][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:44:28,959][188188] Reward + Measures: [[-7.3278951   0.55892795  0.47357634  0.13452867  0.56402397  3.97894263]][0m
[37m[1m[2023-07-03 01:44:28,960][188188] Max Reward on eval: -7.3278951003932935[0m
[37m[1m[2023-07-03 01:44:28,960][188188] Min Reward on eval: -7.3278951003932935[0m
[37m[1m[2023-07-03 01:44:28,960][188188] Mean Reward across all agents: -7.3278951003932935[0m
[37m[1m[2023-07-03 01:44:28,960][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:44:34,109][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:44:34,110][188188] Reward + Measures: [[  11.96058199    0.1112        0.20280002    0.1056        0.1982
     3.658746  ]
 [ 143.67980393    0.2723        0.491         0.44970003    0.61809999
     3.91409802]
 [  71.08639127    0.6656        0.63589996    0.11530001    0.68199998
     3.97333145]
 ...
 [ -15.02846147    0.77610004    0.7913        0.0187        0.80299997
     3.97804999]
 [-197.1383109     0.32660004    0.39610001    0.0885        0.45650002
     3.75666976]
 [ -31.80397973    0.1947        0.1567        0.0707        0.2027
     3.9096458 ]][0m
[37m[1m[2023-07-03 01:44:34,110][188188] Max Reward on eval: 351.63375517595557[0m
[37m[1m[2023-07-03 01:44:34,110][188188] Min Reward on eval: -318.92290112227204[0m
[37m[1m[2023-07-03 01:44:34,111][188188] Mean Reward across all agents: 5.81234625892535[0m
[37m[1m[2023-07-03 01:44:34,111][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:44:34,112][188188] mean_value=-1126.5846168048163, max_value=128.54855001893213[0m
[37m[1m[2023-07-03 01:44:34,115][188188] New mean coefficients: [[ 0.49979964 -0.53493077 -4.0155416  -1.0650715   3.700798    0.20630258]][0m
[37m[1m[2023-07-03 01:44:34,116][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:44:43,216][188188] train() took 9.10 seconds to complete[0m
[36m[2023-07-03 01:44:43,216][188188] FPS: 422028.47[0m
[36m[2023-07-03 01:44:43,219][188188] itr=124, itrs=2000, Progress: 6.20%[0m
[36m[2023-07-03 01:44:55,050][188188] train() took 11.81 seconds to complete[0m
[36m[2023-07-03 01:44:55,051][188188] FPS: 325126.38[0m
[36m[2023-07-03 01:44:59,411][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:44:59,411][188188] Reward + Measures: [[8.81471208 0.64679563 0.33127499 0.34244233 0.63180798 3.97593737]][0m
[37m[1m[2023-07-03 01:44:59,411][188188] Max Reward on eval: 8.814712077169586[0m
[37m[1m[2023-07-03 01:44:59,412][188188] Min Reward on eval: 8.814712077169586[0m
[37m[1m[2023-07-03 01:44:59,412][188188] Mean Reward across all agents: 8.814712077169586[0m
[37m[1m[2023-07-03 01:44:59,412][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:45:04,476][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:45:04,528][188188] Reward + Measures: [[-119.71599899    0.18960001    0.15699999    0.16900001    0.1832
     3.89883971]
 [ 142.5552938     0.42449999    0.0605        0.42980003    0.3867
     3.79271579]
 [  -8.50462947    0.44409999    0.37900001    0.37720001    0.43780002
     3.96507049]
 ...
 [ -49.3338077     0.2455        0.19660001    0.19509999    0.244
     3.93725634]
 [  45.80077727    0.0869        0.1134        0.0598        0.09710001
     3.75561595]
 [ -58.92766517    0.17910001    0.17590001    0.14680001    0.1772
     3.85215831]][0m
[37m[1m[2023-07-03 01:45:04,528][188188] Max Reward on eval: 325.94833941592367[0m
[37m[1m[2023-07-03 01:45:04,529][188188] Min Reward on eval: -404.06375405546277[0m
[37m[1m[2023-07-03 01:45:04,529][188188] Mean Reward across all agents: -32.109372588110546[0m
[37m[1m[2023-07-03 01:45:04,529][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:45:04,530][188188] mean_value=-2619.670284836335, max_value=-6.3572151287235386[0m
[36m[2023-07-03 01:45:04,533][188188] XNES is restarting with a new solution whose measures are [0.80380005 0.43210003 0.81580001 0.38369998 3.99283767] and objective is 51.050360672920945[0m
[36m[2023-07-03 01:45:04,534][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:45:04,536][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:45:04,537][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:45:13,568][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 01:45:13,569][188188] FPS: 425249.53[0m
[36m[2023-07-03 01:45:13,571][188188] itr=125, itrs=2000, Progress: 6.25%[0m
[36m[2023-07-03 01:45:25,342][188188] train() took 11.75 seconds to complete[0m
[36m[2023-07-03 01:45:25,343][188188] FPS: 326737.99[0m
[36m[2023-07-03 01:45:29,724][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:45:29,725][188188] Reward + Measures: [[-42.13462732   0.84879702   0.35979566   0.85528696   0.49772868
    3.9927032 ]][0m
[37m[1m[2023-07-03 01:45:29,725][188188] Max Reward on eval: -42.13462731508749[0m
[37m[1m[2023-07-03 01:45:29,725][188188] Min Reward on eval: -42.13462731508749[0m
[37m[1m[2023-07-03 01:45:29,726][188188] Mean Reward across all agents: -42.13462731508749[0m
[37m[1m[2023-07-03 01:45:29,726][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:45:34,761][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:45:34,766][188188] Reward + Measures: [[-137.79403161    0.80730003    0.0151        0.79730004    0.79090005
     3.90444994]
 [   1.89216551    0.80520004    0.3281        0.80419999    0.4804
     3.98797226]
 [ -66.73343346    0.40959999    0.0551        0.38840002    0.3689
     3.94242978]
 ...
 [-148.11048938    0.67229998    0.0302        0.68380004    0.62849998
     3.78755355]
 [  -9.91328433    0.23280001    0.085         0.21899998    0.1363
     3.70782065]
 [ -40.70820752    0.52969998    0.0376        0.53190005    0.50660002
     3.84567189]][0m
[37m[1m[2023-07-03 01:45:34,767][188188] Max Reward on eval: 510.98748249046037[0m
[37m[1m[2023-07-03 01:45:34,767][188188] Min Reward on eval: -632.023364315927[0m
[37m[1m[2023-07-03 01:45:34,767][188188] Mean Reward across all agents: -47.38925345087639[0m
[37m[1m[2023-07-03 01:45:34,768][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:45:34,771][188188] mean_value=-1017.3858716086523, max_value=151.31639694702824[0m
[37m[1m[2023-07-03 01:45:34,773][188188] New mean coefficients: [[ 0.8476324  -0.77661103  0.03608429 -2.1268222  -0.89585084 -0.64986485]][0m
[37m[1m[2023-07-03 01:45:34,774][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:45:43,839][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 01:45:43,839][188188] FPS: 423700.11[0m
[36m[2023-07-03 01:45:43,841][188188] itr=126, itrs=2000, Progress: 6.30%[0m
[36m[2023-07-03 01:45:55,676][188188] train() took 11.81 seconds to complete[0m
[36m[2023-07-03 01:45:55,676][188188] FPS: 325062.12[0m
[36m[2023-07-03 01:45:59,978][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:45:59,978][188188] Reward + Measures: [[-268.36163479    0.75285435    0.10850134    0.75400764    0.62566102
     3.7585175 ]][0m
[37m[1m[2023-07-03 01:45:59,979][188188] Max Reward on eval: -268.36163479163724[0m
[37m[1m[2023-07-03 01:45:59,979][188188] Min Reward on eval: -268.36163479163724[0m
[37m[1m[2023-07-03 01:45:59,979][188188] Mean Reward across all agents: -268.36163479163724[0m
[37m[1m[2023-07-03 01:45:59,979][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:46:05,006][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:46:05,012][188188] Reward + Measures: [[  66.45717176    0.77990001    0.0265        0.77040005    0.73400003
     3.993119  ]
 [-102.19374827    0.32730004    0.3326        0.42289996    0.2383
     3.97664189]
 [  31.60948023    0.65889996    0.0329        0.60440004    0.62110001
     3.98537254]
 ...
 [  31.32952358    0.63180006    0.035         0.58380002    0.58990002
     3.97756362]
 [ 119.80226614    0.84310001    0.2397        0.59180003    0.77509999
     3.99195075]
 [ -22.60590399    0.54430002    0.41050002    0.1873        0.49779996
     3.84011912]][0m
[37m[1m[2023-07-03 01:46:05,012][188188] Max Reward on eval: 413.36804617671294[0m
[37m[1m[2023-07-03 01:46:05,013][188188] Min Reward on eval: -534.9883213012479[0m
[37m[1m[2023-07-03 01:46:05,013][188188] Mean Reward across all agents: -22.039907603226336[0m
[37m[1m[2023-07-03 01:46:05,013][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:46:05,015][188188] mean_value=-472.5916764177225, max_value=268.54675444858066[0m
[37m[1m[2023-07-03 01:46:05,018][188188] New mean coefficients: [[ 0.44290373 -0.78268796 -0.4347255  -2.3797114  -0.3342982  -0.75848675]][0m
[37m[1m[2023-07-03 01:46:05,019][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:46:14,081][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 01:46:14,082][188188] FPS: 423820.13[0m
[36m[2023-07-03 01:46:14,084][188188] itr=127, itrs=2000, Progress: 6.35%[0m
[36m[2023-07-03 01:46:25,779][188188] train() took 11.68 seconds to complete[0m
[36m[2023-07-03 01:46:25,779][188188] FPS: 328894.14[0m
[36m[2023-07-03 01:46:30,103][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:46:30,103][188188] Reward + Measures: [[-349.64948762    0.67268467    0.16493666    0.6508286     0.51789433
     3.7633698 ]][0m
[37m[1m[2023-07-03 01:46:30,103][188188] Max Reward on eval: -349.6494876238497[0m
[37m[1m[2023-07-03 01:46:30,104][188188] Min Reward on eval: -349.6494876238497[0m
[37m[1m[2023-07-03 01:46:30,104][188188] Mean Reward across all agents: -349.6494876238497[0m
[37m[1m[2023-07-03 01:46:30,104][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:46:35,125][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:46:35,130][188188] Reward + Measures: [[  98.42590241    0.61519998    0.037         0.60350007    0.57310003
     3.99519777]
 [-190.69895555    0.73929995    0.0232        0.72409999    0.68839997
     3.98775363]
 [ 111.32277536    0.8761        0.1031        0.78870004    0.83829993
     3.9984169 ]
 ...
 [  18.3370074     0.71380001    0.0336        0.69080001    0.6473
     3.99401474]
 [ -11.69989841    0.81080008    0.0179        0.79400003    0.77750003
     3.9902761 ]
 [-129.53581148    0.31670001    0.1358        0.30950001    0.19050001
     3.68099093]][0m
[37m[1m[2023-07-03 01:46:35,131][188188] Max Reward on eval: 454.89167594438186[0m
[37m[1m[2023-07-03 01:46:35,131][188188] Min Reward on eval: -409.43926976928486[0m
[37m[1m[2023-07-03 01:46:35,131][188188] Mean Reward across all agents: 18.21373976372391[0m
[37m[1m[2023-07-03 01:46:35,131][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:46:35,134][188188] mean_value=-370.9116152000009, max_value=248.54573818413962[0m
[37m[1m[2023-07-03 01:46:35,136][188188] New mean coefficients: [[ 0.12946042  0.32734638 -0.41744488 -2.9708138  -0.09357573 -0.49577725]][0m
[37m[1m[2023-07-03 01:46:35,137][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:46:44,177][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 01:46:44,177][188188] FPS: 424882.91[0m
[36m[2023-07-03 01:46:44,179][188188] itr=128, itrs=2000, Progress: 6.40%[0m
[36m[2023-07-03 01:46:55,745][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 01:46:55,745][188188] FPS: 332535.89[0m
[36m[2023-07-03 01:46:59,948][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:46:59,948][188188] Reward + Measures: [[143.10261274   0.52917397   0.37339836   0.50937968   0.514189
    3.86947322]][0m
[37m[1m[2023-07-03 01:46:59,948][188188] Max Reward on eval: 143.1026127427706[0m
[37m[1m[2023-07-03 01:46:59,949][188188] Min Reward on eval: 143.1026127427706[0m
[37m[1m[2023-07-03 01:46:59,949][188188] Mean Reward across all agents: 143.1026127427706[0m
[37m[1m[2023-07-03 01:46:59,949][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:47:05,088][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:47:05,094][188188] Reward + Measures: [[   5.26778687    0.55990005    0.0577        0.5363        0.49640003
     3.90777898]
 [-389.46266558    0.81000006    0.0322        0.80000001    0.74150002
     3.78369451]
 [  27.67699542    0.99080002    0.0015        0.94990009    0.89580005
     3.99991345]
 ...
 [-142.94536206    0.62239999    0.0954        0.5485        0.56409997
     3.91209483]
 [ -41.85457847    0.24039999    0.2102        0.21300001    0.2414
     3.9167614 ]
 [-150.71571733    0.6462        0.55750006    0.59180003    0.62330002
     3.72651744]][0m
[37m[1m[2023-07-03 01:47:05,094][188188] Max Reward on eval: 534.9530210410012[0m
[37m[1m[2023-07-03 01:47:05,095][188188] Min Reward on eval: -642.5583839563653[0m
[37m[1m[2023-07-03 01:47:05,095][188188] Mean Reward across all agents: -58.66735955505729[0m
[37m[1m[2023-07-03 01:47:05,095][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:47:05,098][188188] mean_value=-606.4757473331975, max_value=363.7159323375952[0m
[37m[1m[2023-07-03 01:47:05,101][188188] New mean coefficients: [[ 0.31594697  1.0618106   0.09077311 -4.3901763  -0.6173143   0.14923072]][0m
[37m[1m[2023-07-03 01:47:05,102][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:47:14,060][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 01:47:14,061][188188] FPS: 428704.43[0m
[36m[2023-07-03 01:47:14,063][188188] itr=129, itrs=2000, Progress: 6.45%[0m
[36m[2023-07-03 01:47:25,872][188188] train() took 11.79 seconds to complete[0m
[36m[2023-07-03 01:47:25,872][188188] FPS: 325674.63[0m
[36m[2023-07-03 01:47:30,189][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:47:30,189][188188] Reward + Measures: [[-164.27385728    0.57600099    0.06205467    0.61209702    0.56875569
     3.8586719 ]][0m
[37m[1m[2023-07-03 01:47:30,190][188188] Max Reward on eval: -164.27385728001477[0m
[37m[1m[2023-07-03 01:47:30,190][188188] Min Reward on eval: -164.27385728001477[0m
[37m[1m[2023-07-03 01:47:30,190][188188] Mean Reward across all agents: -164.27385728001477[0m
[37m[1m[2023-07-03 01:47:30,190][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:47:35,204][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:47:35,205][188188] Reward + Measures: [[  18.08128526    0.20930003    0.2017        0.13940001    0.22910002
     3.77377367]
 [  12.34700218    0.12660001    0.10770001    0.07390001    0.12800001
     3.89185381]
 [-166.26138737    0.62589997    0.11060001    0.56980002    0.57919997
     3.92349029]
 ...
 [-187.0297508     0.97970003    0.0103        0.95520002    0.90959996
     3.83052897]
 [-151.26459425    0.66499996    0.49120003    0.6534        0.66499996
     3.85399175]
 [-210.67538303    0.34920001    0.33559999    0.30239999    0.0465
     3.87863779]][0m
[37m[1m[2023-07-03 01:47:35,205][188188] Max Reward on eval: 472.7060060759075[0m
[37m[1m[2023-07-03 01:47:35,205][188188] Min Reward on eval: -579.9307470194296[0m
[37m[1m[2023-07-03 01:47:35,206][188188] Mean Reward across all agents: -41.002824442269294[0m
[37m[1m[2023-07-03 01:47:35,206][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:47:35,208][188188] mean_value=-1784.2129568705786, max_value=401.8122912424238[0m
[37m[1m[2023-07-03 01:47:35,211][188188] New mean coefficients: [[ 0.36051872 -0.04296613 -0.24717787 -4.130572   -0.6835582   0.97731364]][0m
[37m[1m[2023-07-03 01:47:35,212][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:47:44,209][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 01:47:44,210][188188] FPS: 426844.53[0m
[36m[2023-07-03 01:47:44,212][188188] itr=130, itrs=2000, Progress: 6.50%[0m
[37m[1m[2023-07-03 01:47:46,700][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000110[0m
[36m[2023-07-03 01:47:58,847][188188] train() took 11.82 seconds to complete[0m
[36m[2023-07-03 01:47:58,847][188188] FPS: 324798.54[0m
[36m[2023-07-03 01:48:03,152][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:48:03,153][188188] Reward + Measures: [[24.09204802  0.60307264  0.13680667  0.49412999  0.54833168  3.85907197]][0m
[37m[1m[2023-07-03 01:48:03,153][188188] Max Reward on eval: 24.092048018346013[0m
[37m[1m[2023-07-03 01:48:03,153][188188] Min Reward on eval: 24.092048018346013[0m
[37m[1m[2023-07-03 01:48:03,153][188188] Mean Reward across all agents: 24.092048018346013[0m
[37m[1m[2023-07-03 01:48:03,154][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:48:08,188][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:48:08,240][188188] Reward + Measures: [[-204.25220299    0.92360002    0.94139999    0.0161        0.94869995
     3.93027735]
 [ -64.24267335    0.32440001    0.10930001    0.37950003    0.36320001
     3.94300151]
 [-294.55528116    0.61899996    0.1081        0.54869998    0.58219999
     3.96274304]
 ...
 [ -18.64055325    0.47389993    0.0963        0.44490001    0.45860001
     3.94135213]
 [  64.06012922    0.4752        0.46240002    0.42529997    0.4605
     3.81847239]
 [-186.01414199    0.35280001    0.23989999    0.41690001    0.20999999
     3.92154241]][0m
[37m[1m[2023-07-03 01:48:08,241][188188] Max Reward on eval: 352.78115939488634[0m
[37m[1m[2023-07-03 01:48:08,241][188188] Min Reward on eval: -453.51490789605305[0m
[37m[1m[2023-07-03 01:48:08,241][188188] Mean Reward across all agents: -51.10841720872077[0m
[37m[1m[2023-07-03 01:48:08,241][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:48:08,244][188188] mean_value=-1134.2437913770223, max_value=242.38409632283833[0m
[37m[1m[2023-07-03 01:48:08,246][188188] New mean coefficients: [[ 0.14009535 -0.13812363  0.32437405 -4.222655   -0.9095176   1.180924  ]][0m
[37m[1m[2023-07-03 01:48:08,247][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:48:17,327][188188] train() took 9.08 seconds to complete[0m
[36m[2023-07-03 01:48:17,328][188188] FPS: 422973.26[0m
[36m[2023-07-03 01:48:17,330][188188] itr=131, itrs=2000, Progress: 6.55%[0m
[36m[2023-07-03 01:48:28,851][188188] train() took 11.50 seconds to complete[0m
[36m[2023-07-03 01:48:28,851][188188] FPS: 333848.08[0m
[36m[2023-07-03 01:48:33,084][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:48:33,084][188188] Reward + Measures: [[62.61957467  0.62505728  0.18214867  0.47072631  0.56318432  3.87242889]][0m
[37m[1m[2023-07-03 01:48:33,084][188188] Max Reward on eval: 62.619574667211545[0m
[37m[1m[2023-07-03 01:48:33,085][188188] Min Reward on eval: 62.619574667211545[0m
[37m[1m[2023-07-03 01:48:33,085][188188] Mean Reward across all agents: 62.619574667211545[0m
[37m[1m[2023-07-03 01:48:33,085][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:48:38,057][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:48:38,058][188188] Reward + Measures: [[-100.96147197    0.0752        0.07910001    0.0975        0.072
     3.77377319]
 [   1.4658937     0.14430001    0.127         0.1251        0.14690001
     3.94928169]
 [ -17.75322607    0.68149996    0.12029999    0.55069995    0.59689999
     3.89301753]
 ...
 [   5.99672605    0.0912        0.101         0.07080001    0.067
     3.85317111]
 [  -9.27771126    0.26199999    0.0593        0.23020001    0.23360001
     3.90481114]
 [ -75.32502758    0.2983        0.2633        0.25830001    0.29620001
     3.96791625]][0m
[37m[1m[2023-07-03 01:48:38,058][188188] Max Reward on eval: 319.7060162426904[0m
[37m[1m[2023-07-03 01:48:38,058][188188] Min Reward on eval: -286.63976432308556[0m
[37m[1m[2023-07-03 01:48:38,059][188188] Mean Reward across all agents: -51.85553158037838[0m
[37m[1m[2023-07-03 01:48:38,059][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:48:38,060][188188] mean_value=-2185.5592931307215, max_value=-48.31440372469913[0m
[36m[2023-07-03 01:48:38,063][188188] XNES is restarting with a new solution whose measures are [0.43980002 0.59919995 0.70190001 0.42899999 3.97935414] and objective is 303.7504253460327[0m
[36m[2023-07-03 01:48:38,064][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:48:38,066][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:48:38,067][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:48:47,009][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 01:48:47,009][188188] FPS: 429496.96[0m
[36m[2023-07-03 01:48:47,011][188188] itr=132, itrs=2000, Progress: 6.60%[0m
[36m[2023-07-03 01:48:58,600][188188] train() took 11.57 seconds to complete[0m
[36m[2023-07-03 01:48:58,600][188188] FPS: 331876.24[0m
[36m[2023-07-03 01:49:02,889][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:49:02,889][188188] Reward + Measures: [[19.78363146  0.17429334  0.45658934  0.38508365  0.46933204  3.92655396]][0m
[37m[1m[2023-07-03 01:49:02,889][188188] Max Reward on eval: 19.78363146403874[0m
[37m[1m[2023-07-03 01:49:02,890][188188] Min Reward on eval: 19.78363146403874[0m
[37m[1m[2023-07-03 01:49:02,890][188188] Mean Reward across all agents: 19.78363146403874[0m
[37m[1m[2023-07-03 01:49:02,890][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:49:08,060][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:49:08,061][188188] Reward + Measures: [[ -20.58915354    0.76679999    0.78590006    0.0277        0.80019999
     3.94749713]
 [ 451.85479117    0.1152        0.61470002    0.50240004    0.62830001
     3.96293569]
 [-139.05266011    0.49860001    0.83759993    0.3346        0.82700008
     3.99441028]
 ...
 [-471.79326922    0.67340004    0.80250007    0.126         0.78980005
     3.98161507]
 [ 338.6758976     0.88800001    0.94869995    0.0149        0.95389998
     3.80418777]
 [-172.65871428    0.58610004    0.9794001     0.38460001    0.98479998
     3.99643302]][0m
[37m[1m[2023-07-03 01:49:08,061][188188] Max Reward on eval: 668.3600311264396[0m
[37m[1m[2023-07-03 01:49:08,061][188188] Min Reward on eval: -623.2302389281801[0m
[37m[1m[2023-07-03 01:49:08,062][188188] Mean Reward across all agents: 16.522109865276033[0m
[37m[1m[2023-07-03 01:49:08,062][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:49:08,064][188188] mean_value=-1182.7194162847109, max_value=324.5247801825777[0m
[37m[1m[2023-07-03 01:49:08,067][188188] New mean coefficients: [[-0.15215155 -0.4866503  -1.7332265  -2.765671   -2.07513     0.04132271]][0m
[37m[1m[2023-07-03 01:49:08,068][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:49:17,180][188188] train() took 9.11 seconds to complete[0m
[36m[2023-07-03 01:49:17,180][188188] FPS: 421478.91[0m
[36m[2023-07-03 01:49:17,183][188188] itr=133, itrs=2000, Progress: 6.65%[0m
[36m[2023-07-03 01:49:28,732][188188] train() took 11.53 seconds to complete[0m
[36m[2023-07-03 01:49:28,732][188188] FPS: 333031.09[0m
[36m[2023-07-03 01:49:32,966][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:49:32,967][188188] Reward + Measures: [[-128.83654136    0.41916865    0.74641502    0.35777399    0.74633634
     3.97766542]][0m
[37m[1m[2023-07-03 01:49:32,967][188188] Max Reward on eval: -128.8365413649945[0m
[37m[1m[2023-07-03 01:49:32,967][188188] Min Reward on eval: -128.8365413649945[0m
[37m[1m[2023-07-03 01:49:32,967][188188] Mean Reward across all agents: -128.8365413649945[0m
[37m[1m[2023-07-03 01:49:32,968][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:49:37,965][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:49:37,965][188188] Reward + Measures: [[ 26.23600547   0.19860001   0.28029999   0.20710002   0.27079999
    3.78328371]
 [231.64557476   0.68519998   0.70649999   0.0294       0.73290002
    3.9425621 ]
 [-83.57201856   0.15519999   0.3818       0.2904       0.34740001
    3.94222426]
 ...
 [ 25.49693181   0.53530002   0.63230002   0.0981       0.59330004
    3.93922114]
 [ 35.95448855   0.25639999   0.20079999   0.1936       0.2595
    3.87011647]
 [ 89.46675648   0.52249998   0.52890003   0.1168       0.55539995
    3.96121669]][0m
[37m[1m[2023-07-03 01:49:37,965][188188] Max Reward on eval: 804.5344085709191[0m
[37m[1m[2023-07-03 01:49:37,966][188188] Min Reward on eval: -566.8750610395334[0m
[37m[1m[2023-07-03 01:49:37,966][188188] Mean Reward across all agents: 59.724288480637334[0m
[37m[1m[2023-07-03 01:49:37,966][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:49:37,969][188188] mean_value=-1064.4331019216431, max_value=380.2602558373567[0m
[37m[1m[2023-07-03 01:49:37,971][188188] New mean coefficients: [[ 0.43513992 -1.2637093  -2.893608   -2.6672904  -2.1158037   0.37809283]][0m
[37m[1m[2023-07-03 01:49:37,972][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:49:47,094][188188] train() took 9.12 seconds to complete[0m
[36m[2023-07-03 01:49:47,094][188188] FPS: 421048.43[0m
[36m[2023-07-03 01:49:47,096][188188] itr=134, itrs=2000, Progress: 6.70%[0m
[36m[2023-07-03 01:49:58,924][188188] train() took 11.81 seconds to complete[0m
[36m[2023-07-03 01:49:58,924][188188] FPS: 325155.62[0m
[36m[2023-07-03 01:50:03,244][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:50:03,244][188188] Reward + Measures: [[86.62619092  0.13930801  0.42945465  0.39002833  0.44534269  3.92002416]][0m
[37m[1m[2023-07-03 01:50:03,244][188188] Max Reward on eval: 86.62619091559787[0m
[37m[1m[2023-07-03 01:50:03,245][188188] Min Reward on eval: 86.62619091559787[0m
[37m[1m[2023-07-03 01:50:03,245][188188] Mean Reward across all agents: 86.62619091559787[0m
[37m[1m[2023-07-03 01:50:03,245][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:50:08,283][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:50:08,284][188188] Reward + Measures: [[-136.09427645    0.22909999    0.20460001    0.0598        0.24949999
     3.79246521]
 [  67.10358436    0.0669        0.1769        0.16140001    0.20650001
     3.82209396]
 [-148.65691082    0.0334        0.46259999    0.46529999    0.56829995
     3.86833501]
 ...
 [ -15.78627642    0.28289998    0.35770002    0.15910001    0.40580001
     3.82838559]
 [ -70.36776951    0.0795        0.1067        0.1055        0.10569999
     3.86739993]
 [ 112.23440494    0.31580004    0.41100001    0.0609        0.43070003
     3.8227005 ]][0m
[37m[1m[2023-07-03 01:50:08,284][188188] Max Reward on eval: 541.5445471017156[0m
[37m[1m[2023-07-03 01:50:08,284][188188] Min Reward on eval: -336.15865900404754[0m
[37m[1m[2023-07-03 01:50:08,285][188188] Mean Reward across all agents: 16.897461492274[0m
[37m[1m[2023-07-03 01:50:08,285][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:50:08,286][188188] mean_value=-2351.144895372793, max_value=12.520084811919844[0m
[37m[1m[2023-07-03 01:50:08,289][188188] New mean coefficients: [[ 0.5740014   0.9596591  -3.1477304  -2.4659822  -1.3614459   0.42767954]][0m
[37m[1m[2023-07-03 01:50:08,290][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:50:17,320][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 01:50:17,320][188188] FPS: 425323.28[0m
[36m[2023-07-03 01:50:17,323][188188] itr=135, itrs=2000, Progress: 6.75%[0m
[36m[2023-07-03 01:50:28,993][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 01:50:28,993][188188] FPS: 329655.70[0m
[36m[2023-07-03 01:50:33,310][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:50:33,311][188188] Reward + Measures: [[13.27950545  0.13300133  0.3357273   0.249185    0.30506933  3.88901305]][0m
[37m[1m[2023-07-03 01:50:33,311][188188] Max Reward on eval: 13.279505449450856[0m
[37m[1m[2023-07-03 01:50:33,311][188188] Min Reward on eval: 13.279505449450856[0m
[37m[1m[2023-07-03 01:50:33,311][188188] Mean Reward across all agents: 13.279505449450856[0m
[37m[1m[2023-07-03 01:50:33,312][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:50:38,332][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:50:38,332][188188] Reward + Measures: [[  4.34594753   0.26700002   0.34419999   0.07320001   0.30700001
    3.69617462]
 [-59.91476691   0.22490001   0.28529999   0.1038       0.2447
    3.92121434]
 [-51.43331434   0.07570001   0.1626       0.12810001   0.1517
    3.80294108]
 ...
 [ -7.16940531   0.53219998   0.55329996   0.0446       0.52940005
    3.89358377]
 [ 17.14019516   0.14390001   0.1585       0.0886       0.1595
    3.62105918]
 [  1.15296921   0.0852       0.0833       0.0836       0.103
    3.74533272]][0m
[37m[1m[2023-07-03 01:50:38,332][188188] Max Reward on eval: 386.4657524092123[0m
[37m[1m[2023-07-03 01:50:38,333][188188] Min Reward on eval: -299.4390800105408[0m
[37m[1m[2023-07-03 01:50:38,333][188188] Mean Reward across all agents: -4.186719251962185[0m
[37m[1m[2023-07-03 01:50:38,333][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:50:38,334][188188] mean_value=-3141.9238058495434, max_value=-142.81351622662285[0m
[36m[2023-07-03 01:50:38,337][188188] XNES is restarting with a new solution whose measures are [0.2784     0.5697     0.81910008 0.83520001 3.98653865] and objective is 172.84153328407555[0m
[36m[2023-07-03 01:50:38,338][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:50:38,340][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:50:38,341][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:50:47,320][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 01:50:47,320][188188] FPS: 427746.63[0m
[36m[2023-07-03 01:50:47,323][188188] itr=136, itrs=2000, Progress: 6.80%[0m
[36m[2023-07-03 01:50:59,113][188188] train() took 11.77 seconds to complete[0m
[36m[2023-07-03 01:50:59,113][188188] FPS: 326269.94[0m
[36m[2023-07-03 01:51:03,355][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:51:03,355][188188] Reward + Measures: [[24.31346549  0.06716633  0.812612    0.85651934  0.8504144   3.99168086]][0m
[37m[1m[2023-07-03 01:51:03,356][188188] Max Reward on eval: 24.31346548875884[0m
[37m[1m[2023-07-03 01:51:03,356][188188] Min Reward on eval: 24.31346548875884[0m
[37m[1m[2023-07-03 01:51:03,356][188188] Mean Reward across all agents: 24.31346548875884[0m
[37m[1m[2023-07-03 01:51:03,356][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:51:08,380][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:51:08,380][188188] Reward + Measures: [[26.3073852   0.0802      0.28640002  0.32160002  0.31009999  3.955405  ]
 [99.65583816  0.0477      0.84359998  0.82200003  0.84089994  3.98280978]
 [81.44190254  0.0018      0.90080005  0.85599995  0.88109988  3.98926544]
 ...
 [85.26116714  0.026       0.87740004  0.8585      0.85319996  3.97643065]
 [34.03168011  0.0706      0.79460001  0.86170006  0.86209995  3.98938727]
 [55.53990087  0.0307      0.90490001  0.87870008  0.87260002  3.98309565]][0m
[37m[1m[2023-07-03 01:51:08,381][188188] Max Reward on eval: 378.4900846473873[0m
[37m[1m[2023-07-03 01:51:08,381][188188] Min Reward on eval: -256.37174039296804[0m
[37m[1m[2023-07-03 01:51:08,381][188188] Mean Reward across all agents: 36.65325506210172[0m
[37m[1m[2023-07-03 01:51:08,381][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:51:08,383][188188] mean_value=-840.0856232382152, max_value=376.85030587774946[0m
[37m[1m[2023-07-03 01:51:08,386][188188] New mean coefficients: [[ 1.285228   -0.14987957 -0.7527298  -1.9134724  -1.3415366  -0.5546907 ]][0m
[37m[1m[2023-07-03 01:51:08,387][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:51:17,414][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 01:51:17,414][188188] FPS: 425477.12[0m
[36m[2023-07-03 01:51:17,416][188188] itr=137, itrs=2000, Progress: 6.85%[0m
[36m[2023-07-03 01:51:29,257][188188] train() took 11.82 seconds to complete[0m
[36m[2023-07-03 01:51:29,258][188188] FPS: 324824.16[0m
[36m[2023-07-03 01:51:33,496][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:51:33,496][188188] Reward + Measures: [[30.6557413   0.01838967  0.82252634  0.7935003   0.80825233  3.98375058]][0m
[37m[1m[2023-07-03 01:51:33,496][188188] Max Reward on eval: 30.655741301866094[0m
[37m[1m[2023-07-03 01:51:33,497][188188] Min Reward on eval: 30.655741301866094[0m
[37m[1m[2023-07-03 01:51:33,497][188188] Mean Reward across all agents: 30.655741301866094[0m
[37m[1m[2023-07-03 01:51:33,497][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:51:38,623][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:51:38,624][188188] Reward + Measures: [[-137.23753835    0.58840001    0.57569999    0.97030002    0.72010005
     3.98977351]
 [-352.03768584    0.39420003    0.57790005    0.95660001    0.95459998
     3.99416423]
 [   0.16822398    0.096         0.18410002    0.16510001    0.19200002
     3.82391024]
 ...
 [   0.35683851    0.15279999    0.76010001    0.81099999    0.70889997
     3.96788144]
 [ 314.05868913    0.19670001    0.50830001    0.4075        0.57770002
     3.98805165]
 [   2.32799688    0.0076        0.79280007    0.81409997    0.80030006
     3.99783635]][0m
[37m[1m[2023-07-03 01:51:38,624][188188] Max Reward on eval: 673.3281105657109[0m
[37m[1m[2023-07-03 01:51:38,624][188188] Min Reward on eval: -472.4084968382493[0m
[37m[1m[2023-07-03 01:51:38,625][188188] Mean Reward across all agents: 9.111582477584848[0m
[37m[1m[2023-07-03 01:51:38,625][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:51:38,628][188188] mean_value=-793.868495022478, max_value=484.40909931855276[0m
[37m[1m[2023-07-03 01:51:38,630][188188] New mean coefficients: [[ 0.84722346 -0.9808243  -0.16004103 -2.404982   -1.8144841  -0.89868045]][0m
[37m[1m[2023-07-03 01:51:38,631][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:51:47,620][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 01:51:47,621][188188] FPS: 427271.37[0m
[36m[2023-07-03 01:51:47,623][188188] itr=138, itrs=2000, Progress: 6.90%[0m
[36m[2023-07-03 01:51:59,139][188188] train() took 11.50 seconds to complete[0m
[36m[2023-07-03 01:51:59,139][188188] FPS: 334030.31[0m
[36m[2023-07-03 01:52:03,380][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:52:03,380][188188] Reward + Measures: [[2.7301168  0.03625133 0.81000429 0.791062   0.79989374 3.98351169]][0m
[37m[1m[2023-07-03 01:52:03,380][188188] Max Reward on eval: 2.7301168000714777[0m
[37m[1m[2023-07-03 01:52:03,381][188188] Min Reward on eval: 2.7301168000714777[0m
[37m[1m[2023-07-03 01:52:03,381][188188] Mean Reward across all agents: 2.7301168000714777[0m
[37m[1m[2023-07-03 01:52:03,381][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:52:08,368][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:52:08,369][188188] Reward + Measures: [[  28.61999062    0.1214        0.51969999    0.55840003    0.38519999
     3.98721862]
 [  16.3780357     0.0052        0.97360003    0.97170001    0.98170006
     3.99907732]
 [ 315.60387944    0.24029998    0.6401        0.73360002    0.7008
     3.95291185]
 ...
 [-196.2359161     0.28959998    0.43459997    0.1947        0.44510004
     3.95481491]
 [-128.02372148    0.09909999    0.88269997    0.9801001     0.98789996
     3.99905634]
 [ -30.97327943    0.10399999    0.89659995    0.86170006    0.7841
     3.98836207]][0m
[37m[1m[2023-07-03 01:52:08,369][188188] Max Reward on eval: 576.813656418171[0m
[37m[1m[2023-07-03 01:52:08,369][188188] Min Reward on eval: -426.93602751521394[0m
[37m[1m[2023-07-03 01:52:08,369][188188] Mean Reward across all agents: 8.20716934611887[0m
[37m[1m[2023-07-03 01:52:08,370][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:52:08,373][188188] mean_value=-709.7794482034849, max_value=526.5975834837184[0m
[37m[1m[2023-07-03 01:52:08,376][188188] New mean coefficients: [[ 1.679775    0.08568364  0.32781705 -3.0254269  -1.4114573  -1.0223256 ]][0m
[37m[1m[2023-07-03 01:52:08,377][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:52:17,327][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 01:52:17,328][188188] FPS: 429094.20[0m
[36m[2023-07-03 01:52:17,330][188188] itr=139, itrs=2000, Progress: 6.95%[0m
[36m[2023-07-03 01:52:29,019][188188] train() took 11.67 seconds to complete[0m
[36m[2023-07-03 01:52:29,019][188188] FPS: 329112.24[0m
[36m[2023-07-03 01:52:33,250][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:52:33,251][188188] Reward + Measures: [[67.37087482  0.026003    0.88730127  0.8871634   0.89241469  3.98873591]][0m
[37m[1m[2023-07-03 01:52:33,251][188188] Max Reward on eval: 67.37087482115392[0m
[37m[1m[2023-07-03 01:52:33,251][188188] Min Reward on eval: 67.37087482115392[0m
[37m[1m[2023-07-03 01:52:33,252][188188] Mean Reward across all agents: 67.37087482115392[0m
[37m[1m[2023-07-03 01:52:33,252][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:52:38,180][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:52:38,185][188188] Reward + Measures: [[ 23.99889949   0.48880002   0.71600002   0.82040006   0.41050002
    3.98845673]
 [-16.52250256   0.10199999   0.7899       0.83820003   0.85809994
    3.97665334]
 [-24.38762438   0.22309999   0.48880002   0.60480005   0.51969999
    3.98449254]
 ...
 [ 13.87609543   0.0132       0.78189999   0.78430003   0.78120005
    3.94748998]
 [116.52336954   0.112        0.68269998   0.70100003   0.57190001
    3.9825573 ]
 [304.84097198   0.0561       0.50960004   0.51569998   0.53480005
    3.94160271]][0m
[37m[1m[2023-07-03 01:52:38,186][188188] Max Reward on eval: 485.9105949657038[0m
[37m[1m[2023-07-03 01:52:38,186][188188] Min Reward on eval: -321.95874482099896[0m
[37m[1m[2023-07-03 01:52:38,186][188188] Mean Reward across all agents: 15.812098731931085[0m
[37m[1m[2023-07-03 01:52:38,186][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:52:38,189][188188] mean_value=-692.3294547164627, max_value=458.3332590629436[0m
[37m[1m[2023-07-03 01:52:38,191][188188] New mean coefficients: [[ 2.264157   -0.13700855  1.1530836  -3.1892095  -1.0669498  -1.1850969 ]][0m
[37m[1m[2023-07-03 01:52:38,192][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:52:47,188][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 01:52:47,188][188188] FPS: 426945.75[0m
[36m[2023-07-03 01:52:47,191][188188] itr=140, itrs=2000, Progress: 7.00%[0m
[37m[1m[2023-07-03 01:52:49,670][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000120[0m
[36m[2023-07-03 01:53:01,877][188188] train() took 11.89 seconds to complete[0m
[36m[2023-07-03 01:53:01,877][188188] FPS: 323011.70[0m
[36m[2023-07-03 01:53:05,697][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:53:05,697][188188] Reward + Measures: [[29.86156873  0.055065    0.867392    0.90145069  0.90131235  3.98726225]][0m
[37m[1m[2023-07-03 01:53:05,697][188188] Max Reward on eval: 29.86156873449955[0m
[37m[1m[2023-07-03 01:53:05,698][188188] Min Reward on eval: 29.86156873449955[0m
[37m[1m[2023-07-03 01:53:05,698][188188] Mean Reward across all agents: 29.86156873449955[0m
[37m[1m[2023-07-03 01:53:05,698][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:53:10,307][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:53:10,307][188188] Reward + Measures: [[ 63.39246436   0.0227       0.52070004   0.5255       0.51640004
    3.93462157]
 [154.05815495   0.0084       0.87940007   0.88239998   0.88640004
    3.98551822]
 [ 70.09633534   0.0066       0.87320006   0.86080009   0.86980003
    3.9909234 ]
 ...
 [ 14.50575546   0.0904       0.52140003   0.59670001   0.51949996
    3.97593045]
 [ 83.05328334   0.0472       0.787        0.79980004   0.82139999
    3.98857689]
 [-23.16785784   0.0666       0.0968       0.09599999   0.09530001
    3.88511086]][0m
[37m[1m[2023-07-03 01:53:10,308][188188] Max Reward on eval: 395.78618380865083[0m
[37m[1m[2023-07-03 01:53:10,308][188188] Min Reward on eval: -395.5898852570448[0m
[37m[1m[2023-07-03 01:53:10,308][188188] Mean Reward across all agents: 13.22702431307114[0m
[37m[1m[2023-07-03 01:53:10,308][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:53:10,310][188188] mean_value=-1025.258855606088, max_value=275.7738091501085[0m
[37m[1m[2023-07-03 01:53:10,312][188188] New mean coefficients: [[ 2.1483715  -1.0182979   2.7453952  -2.6683493  -1.1772513  -0.37943822]][0m
[37m[1m[2023-07-03 01:53:10,313][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:53:19,029][188188] train() took 8.71 seconds to complete[0m
[36m[2023-07-03 01:53:19,029][188188] FPS: 440636.07[0m
[36m[2023-07-03 01:53:19,032][188188] itr=141, itrs=2000, Progress: 7.05%[0m
[36m[2023-07-03 01:53:30,564][188188] train() took 11.51 seconds to complete[0m
[36m[2023-07-03 01:53:30,565][188188] FPS: 333577.97[0m
[36m[2023-07-03 01:53:34,786][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:53:34,786][188188] Reward + Measures: [[-27.93755524   0.05028566   0.8969847    0.9439047    0.94395232
    3.9993186 ]][0m
[37m[1m[2023-07-03 01:53:34,786][188188] Max Reward on eval: -27.937555236546768[0m
[37m[1m[2023-07-03 01:53:34,787][188188] Min Reward on eval: -27.937555236546768[0m
[37m[1m[2023-07-03 01:53:34,787][188188] Mean Reward across all agents: -27.937555236546768[0m
[37m[1m[2023-07-03 01:53:34,787][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:53:39,860][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:53:39,861][188188] Reward + Measures: [[-155.118075      0.39630002    0.97900003    0.9842        0.59619999
     3.99978828]
 [-178.9595913     0.1156        0.7913        0.81679994    0.71740001
     3.97468877]
 [ 563.80563543    0.0302        0.63420004    0.64239997    0.62300003
     3.97959566]
 ...
 [ 354.63856416    0.19149999    0.78239995    0.83190006    0.77410001
     3.65954018]
 [  43.90192447    0.002         0.90569991    0.89560002    0.87520009
     3.90378428]
 [  -9.80490339    0.09820001    0.16230001    0.13860001    0.2332
     3.8862381 ]][0m
[37m[1m[2023-07-03 01:53:39,861][188188] Max Reward on eval: 563.8056354267522[0m
[37m[1m[2023-07-03 01:53:39,861][188188] Min Reward on eval: -424.1209649521858[0m
[37m[1m[2023-07-03 01:53:39,862][188188] Mean Reward across all agents: -6.686190170119439[0m
[37m[1m[2023-07-03 01:53:39,862][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:53:39,864][188188] mean_value=-789.8974774943393, max_value=408.16532360273413[0m
[37m[1m[2023-07-03 01:53:39,867][188188] New mean coefficients: [[ 0.8434783  -0.24357837  2.4886336  -2.970836   -0.5655477  -1.3042372 ]][0m
[37m[1m[2023-07-03 01:53:39,868][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:53:48,804][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 01:53:48,805][188188] FPS: 429762.06[0m
[36m[2023-07-03 01:53:48,807][188188] itr=142, itrs=2000, Progress: 7.10%[0m
[36m[2023-07-03 01:54:00,467][188188] train() took 11.64 seconds to complete[0m
[36m[2023-07-03 01:54:00,468][188188] FPS: 329930.91[0m
[36m[2023-07-03 01:54:04,807][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:54:04,808][188188] Reward + Measures: [[-36.43779951   0.11149166   0.77555096   0.87641925   0.86283737
    3.98484588]][0m
[37m[1m[2023-07-03 01:54:04,808][188188] Max Reward on eval: -36.43779950542841[0m
[37m[1m[2023-07-03 01:54:04,808][188188] Min Reward on eval: -36.43779950542841[0m
[37m[1m[2023-07-03 01:54:04,809][188188] Mean Reward across all agents: -36.43779950542841[0m
[37m[1m[2023-07-03 01:54:04,809][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:54:09,844][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:54:09,849][188188] Reward + Measures: [[-119.20577532    0.35919997    0.95450002    0.96200001    0.61440003
     3.99539733]
 [ -20.89073103    0.12400001    0.39360002    0.4481        0.39669999
     3.86338854]
 [  12.16057679    0.1123        0.7809        0.80530006    0.69239998
     3.83085561]
 ...
 [ -88.59365313    0.25650001    0.63859999    0.69140005    0.4481
     3.72263074]
 [ -55.83657291    0.0923        0.48200002    0.5068        0.52360004
     3.82180595]
 [ -73.40218919    0.19810002    0.653         0.77249998    0.72220004
     3.90221   ]][0m
[37m[1m[2023-07-03 01:54:09,850][188188] Max Reward on eval: 331.65344670163466[0m
[37m[1m[2023-07-03 01:54:09,850][188188] Min Reward on eval: -495.08404512293635[0m
[37m[1m[2023-07-03 01:54:09,850][188188] Mean Reward across all agents: -41.38812215485721[0m
[37m[1m[2023-07-03 01:54:09,851][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:54:09,853][188188] mean_value=-1007.1029031163035, max_value=530.6392001968367[0m
[37m[1m[2023-07-03 01:54:09,856][188188] New mean coefficients: [[ 0.67819667 -0.02151737  2.9280453  -2.0451653  -1.6330345   0.57734966]][0m
[37m[1m[2023-07-03 01:54:09,857][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:54:18,874][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 01:54:18,875][188188] FPS: 425906.65[0m
[36m[2023-07-03 01:54:18,877][188188] itr=143, itrs=2000, Progress: 7.15%[0m
[36m[2023-07-03 01:54:30,636][188188] train() took 11.74 seconds to complete[0m
[36m[2023-07-03 01:54:30,636][188188] FPS: 327155.80[0m
[36m[2023-07-03 01:54:34,961][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:54:34,961][188188] Reward + Measures: [[13.11451526  0.094217    0.73905104  0.79847962  0.79153931  3.97889066]][0m
[37m[1m[2023-07-03 01:54:34,962][188188] Max Reward on eval: 13.114515263446869[0m
[37m[1m[2023-07-03 01:54:34,962][188188] Min Reward on eval: 13.114515263446869[0m
[37m[1m[2023-07-03 01:54:34,962][188188] Mean Reward across all agents: 13.114515263446869[0m
[37m[1m[2023-07-03 01:54:34,962][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:54:39,962][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:54:39,968][188188] Reward + Measures: [[-1.67988732  0.062       0.0954      0.1222      0.0695      3.89797568]
 [11.21076775  0.15989999  0.0542      0.0563      0.1533      3.94215393]
 [19.58858607  0.0221      0.67079997  0.67480004  0.67020005  3.98113632]
 ...
 [-7.14505585  0.0658      0.0865      0.1002      0.0834      3.81784868]
 [16.1697459   0.0538      0.0814      0.0885      0.0774      3.77752185]
 [ 6.55209323  0.0569      0.24340001  0.21440001  0.21500002  3.76413894]][0m
[37m[1m[2023-07-03 01:54:39,968][188188] Max Reward on eval: 257.3424858942628[0m
[37m[1m[2023-07-03 01:54:39,968][188188] Min Reward on eval: -293.47816440844906[0m
[37m[1m[2023-07-03 01:54:39,968][188188] Mean Reward across all agents: -18.729870086558968[0m
[37m[1m[2023-07-03 01:54:39,969][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:54:39,970][188188] mean_value=-2253.3182302229743, max_value=157.63887491190803[0m
[37m[1m[2023-07-03 01:54:39,972][188188] New mean coefficients: [[-1.5775273   0.06195781  1.5595803  -2.2291954  -2.204321    0.57179934]][0m
[37m[1m[2023-07-03 01:54:39,973][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:54:48,930][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 01:54:48,930][188188] FPS: 428806.00[0m
[36m[2023-07-03 01:54:48,932][188188] itr=144, itrs=2000, Progress: 7.20%[0m
[36m[2023-07-03 01:55:00,605][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 01:55:00,605][188188] FPS: 329487.25[0m
[36m[2023-07-03 01:55:04,878][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:55:04,883][188188] Reward + Measures: [[-16.42865505   0.026863     0.77282631   0.77781332   0.78312701
    3.97621179]][0m
[37m[1m[2023-07-03 01:55:04,884][188188] Max Reward on eval: -16.42865504540989[0m
[37m[1m[2023-07-03 01:55:04,884][188188] Min Reward on eval: -16.42865504540989[0m
[37m[1m[2023-07-03 01:55:04,884][188188] Mean Reward across all agents: -16.42865504540989[0m
[37m[1m[2023-07-03 01:55:04,884][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:55:09,928][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:55:09,933][188188] Reward + Measures: [[ -22.57924922    0.0314        0.58999997    0.5844        0.58980006
     3.96968699]
 [ -25.68954883    0.0663        0.1727        0.17879999    0.17330001
     3.91372538]
 [ -40.03686992    0.0071        0.9321        0.88800001    0.92509997
     3.99211001]
 ...
 [   1.99105578    0.0607        0.1012        0.0959        0.1073
     3.80469704]
 [ -28.19491061    0.06149999    0.17510001    0.1707        0.1655
     3.93202972]
 [-268.73308625    0.49419999    0.6821        0.98199999    0.78780001
     3.99950528]][0m
[37m[1m[2023-07-03 01:55:09,934][188188] Max Reward on eval: 151.74636028562674[0m
[37m[1m[2023-07-03 01:55:09,934][188188] Min Reward on eval: -268.73308624625207[0m
[37m[1m[2023-07-03 01:55:09,934][188188] Mean Reward across all agents: -18.49020574320857[0m
[37m[1m[2023-07-03 01:55:09,934][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:55:09,936][188188] mean_value=-1756.5849684982934, max_value=49.77356570725054[0m
[37m[1m[2023-07-03 01:55:09,939][188188] New mean coefficients: [[-0.43961477  1.9332005   1.7637902  -2.2930562  -0.7586558   1.4432302 ]][0m
[37m[1m[2023-07-03 01:55:09,939][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:55:18,917][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 01:55:18,917][188188] FPS: 427833.08[0m
[36m[2023-07-03 01:55:18,919][188188] itr=145, itrs=2000, Progress: 7.25%[0m
[36m[2023-07-03 01:55:30,572][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 01:55:30,572][188188] FPS: 330049.00[0m
[36m[2023-07-03 01:55:34,887][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:55:34,893][188188] Reward + Measures: [[7.17844707 0.03921133 0.49602002 0.50948101 0.50214469 3.95907927]][0m
[37m[1m[2023-07-03 01:55:34,893][188188] Max Reward on eval: 7.178447073114323[0m
[37m[1m[2023-07-03 01:55:34,894][188188] Min Reward on eval: 7.178447073114323[0m
[37m[1m[2023-07-03 01:55:34,894][188188] Mean Reward across all agents: 7.178447073114323[0m
[37m[1m[2023-07-03 01:55:34,894][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:55:40,100][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:55:40,100][188188] Reward + Measures: [[-54.99106716   0.0585       0.66799998   0.68530005   0.67470002
    3.95864677]
 [-23.12957667   0.0973       0.79619998   0.8021       0.7432
    3.98389816]
 [-17.98176556   0.0832       0.13710001   0.1146       0.1631
    3.76640391]
 ...
 [-24.3741217    0.0659       0.0997       0.1014       0.1069
    3.80703545]
 [-39.53138235   0.0716       0.69800007   0.74840003   0.74880004
    3.98795772]
 [-45.58774638   0.0857       0.12159999   0.1123       0.1521
    3.77398992]][0m
[37m[1m[2023-07-03 01:55:40,100][188188] Max Reward on eval: 228.9850750207901[0m
[37m[1m[2023-07-03 01:55:40,101][188188] Min Reward on eval: -187.52240105323727[0m
[37m[1m[2023-07-03 01:55:40,101][188188] Mean Reward across all agents: -8.227363935799563[0m
[37m[1m[2023-07-03 01:55:40,101][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:55:40,102][188188] mean_value=-1874.9076010966992, max_value=117.58512152783948[0m
[37m[1m[2023-07-03 01:55:40,104][188188] New mean coefficients: [[ 0.40707743  3.0258222   1.4737883  -0.8671161  -1.525308    3.294447  ]][0m
[37m[1m[2023-07-03 01:55:40,105][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:55:49,211][188188] train() took 9.10 seconds to complete[0m
[36m[2023-07-03 01:55:49,211][188188] FPS: 421766.80[0m
[36m[2023-07-03 01:55:49,214][188188] itr=146, itrs=2000, Progress: 7.30%[0m
[36m[2023-07-03 01:56:00,985][188188] train() took 11.75 seconds to complete[0m
[36m[2023-07-03 01:56:00,986][188188] FPS: 326803.57[0m
[36m[2023-07-03 01:56:05,351][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:56:05,351][188188] Reward + Measures: [[-13.78296584   0.04006866   0.43966532   0.44540003   0.43806568
    3.92227149]][0m
[37m[1m[2023-07-03 01:56:05,352][188188] Max Reward on eval: -13.782965838384078[0m
[37m[1m[2023-07-03 01:56:05,352][188188] Min Reward on eval: -13.782965838384078[0m
[37m[1m[2023-07-03 01:56:05,352][188188] Mean Reward across all agents: -13.782965838384078[0m
[37m[1m[2023-07-03 01:56:05,353][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:56:10,403][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:56:10,404][188188] Reward + Measures: [[-102.10634269    0.0432        0.42030001    0.44899997    0.4357
     3.91334844]
 [ -11.67117141    0.0332        0.25149998    0.26830003    0.21980003
     3.81351924]
 [-211.20051287    0.16240001    0.43759999    0.41799998    0.2994
     3.88082123]
 ...
 [  50.02331287    0.14500001    0.3211        0.40360004    0.35730001
     3.85770535]
 [ -14.54836252    0.0441        0.28890002    0.30290002    0.26210001
     3.86911464]
 [ -15.0974165     0.0628        0.1181        0.12280001    0.11520001
     3.83267522]][0m
[37m[1m[2023-07-03 01:56:10,404][188188] Max Reward on eval: 270.8295902997255[0m
[37m[1m[2023-07-03 01:56:10,404][188188] Min Reward on eval: -242.25775292422622[0m
[37m[1m[2023-07-03 01:56:10,404][188188] Mean Reward across all agents: -37.46775152856712[0m
[37m[1m[2023-07-03 01:56:10,404][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:56:10,405][188188] mean_value=-1894.916194544558, max_value=241.19978547642705[0m
[37m[1m[2023-07-03 01:56:10,407][188188] New mean coefficients: [[ 1.0878179  1.2740655  1.4202566 -1.3049783 -2.355236   3.1590881]][0m
[37m[1m[2023-07-03 01:56:10,408][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:56:19,455][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 01:56:19,455][188188] FPS: 424542.67[0m
[36m[2023-07-03 01:56:19,457][188188] itr=147, itrs=2000, Progress: 7.35%[0m
[36m[2023-07-03 01:56:31,196][188188] train() took 11.72 seconds to complete[0m
[36m[2023-07-03 01:56:31,196][188188] FPS: 327643.91[0m
[36m[2023-07-03 01:56:35,474][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:56:35,475][188188] Reward + Measures: [[10.03969999  0.036472    0.47408235  0.48535767  0.48073837  3.95355725]][0m
[37m[1m[2023-07-03 01:56:35,475][188188] Max Reward on eval: 10.039699991949698[0m
[37m[1m[2023-07-03 01:56:35,475][188188] Min Reward on eval: 10.039699991949698[0m
[37m[1m[2023-07-03 01:56:35,475][188188] Mean Reward across all agents: 10.039699991949698[0m
[37m[1m[2023-07-03 01:56:35,476][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:56:40,458][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:56:40,459][188188] Reward + Measures: [[ -5.89461344   0.075        0.11130001   0.11900001   0.12660001
    3.79234862]
 [ 34.27253581   0.0957       0.15090001   0.1471       0.18800001
    3.75475883]
 [ 65.20011364   0.0823       0.11650001   0.1128       0.099
    3.93445444]
 ...
 [-79.05084449   0.068        0.2854       0.2599       0.29590002
    3.93295717]
 [-32.78183175   0.06         0.2441       0.25419998   0.24749999
    3.92395759]
 [ -6.81793833   0.06800001   0.10160001   0.0946       0.1105
    3.83973241]][0m
[37m[1m[2023-07-03 01:56:40,459][188188] Max Reward on eval: 512.8503899191506[0m
[37m[1m[2023-07-03 01:56:40,459][188188] Min Reward on eval: -162.9726467492059[0m
[37m[1m[2023-07-03 01:56:40,459][188188] Mean Reward across all agents: 2.0162178463043627[0m
[37m[1m[2023-07-03 01:56:40,460][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:56:40,461][188188] mean_value=-2779.512451405879, max_value=-535.6142459967376[0m
[36m[2023-07-03 01:56:40,463][188188] XNES is restarting with a new solution whose measures are [0.18350001 0.23440002 0.22070001 0.1089     2.44051075] and objective is 4054.2978821208003[0m
[36m[2023-07-03 01:56:40,464][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:56:40,466][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:56:40,467][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:56:49,515][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 01:56:49,515][188188] FPS: 424500.52[0m
[36m[2023-07-03 01:56:49,517][188188] itr=148, itrs=2000, Progress: 7.40%[0m
[36m[2023-07-03 01:57:01,354][188188] train() took 11.82 seconds to complete[0m
[36m[2023-07-03 01:57:01,354][188188] FPS: 324978.88[0m
[36m[2023-07-03 01:57:05,672][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:57:05,673][188188] Reward + Measures: [[2249.39116924    0.144187      0.22389734    0.20794266    0.06904133
     2.43747497]][0m
[37m[1m[2023-07-03 01:57:05,673][188188] Max Reward on eval: 2249.3911692393635[0m
[37m[1m[2023-07-03 01:57:05,673][188188] Min Reward on eval: 2249.3911692393635[0m
[37m[1m[2023-07-03 01:57:05,674][188188] Mean Reward across all agents: 2249.3911692393635[0m
[37m[1m[2023-07-03 01:57:05,674][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:57:10,696][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:57:10,697][188188] Reward + Measures: [[2430.89392094    0.1506        0.2237        0.22230001    0.0709
     2.41521716]
 [1309.32223504    0.10470001    0.1631        0.1383        0.0714
     2.28670645]
 [1610.11725616    0.12490001    0.193         0.16970001    0.077
     2.34667563]
 ...
 [1988.71259308    0.1372        0.21970001    0.20850001    0.0764
     2.40954328]
 [2104.54161069    0.14240001    0.22130001    0.20370002    0.0672
     2.4272306 ]
 [1704.37223051    0.1247        0.18719999    0.1675        0.0737
     2.37521672]][0m
[37m[1m[2023-07-03 01:57:10,697][188188] Max Reward on eval: 2743.4886474698783[0m
[37m[1m[2023-07-03 01:57:10,697][188188] Min Reward on eval: 1028.9422645442187[0m
[37m[1m[2023-07-03 01:57:10,698][188188] Mean Reward across all agents: 1926.1462247047698[0m
[37m[1m[2023-07-03 01:57:10,698][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:57:10,699][188188] mean_value=-1878.3565505762904, max_value=-762.9089534098107[0m
[36m[2023-07-03 01:57:10,701][188188] XNES is restarting with a new solution whose measures are [0.86050004 0.81630003 0.80610001 0.87150002 3.93810272] and objective is 144.99082637033425[0m
[36m[2023-07-03 01:57:10,702][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:57:10,705][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:57:10,706][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:57:19,659][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 01:57:19,665][188188] FPS: 428928.93[0m
[36m[2023-07-03 01:57:19,668][188188] itr=149, itrs=2000, Progress: 7.45%[0m
[36m[2023-07-03 01:57:31,170][188188] train() took 11.48 seconds to complete[0m
[36m[2023-07-03 01:57:31,170][188188] FPS: 334475.25[0m
[36m[2023-07-03 01:57:35,366][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:57:35,367][188188] Reward + Measures: [[26.8704228   0.36108333  0.34105366  0.32572299  0.37825701  3.90092635]][0m
[37m[1m[2023-07-03 01:57:35,367][188188] Max Reward on eval: 26.870422800485606[0m
[37m[1m[2023-07-03 01:57:35,367][188188] Min Reward on eval: 26.870422800485606[0m
[37m[1m[2023-07-03 01:57:35,367][188188] Mean Reward across all agents: 26.870422800485606[0m
[37m[1m[2023-07-03 01:57:35,368][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:57:40,355][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:57:40,355][188188] Reward + Measures: [[  9.3270849    0.14820001   0.1142       0.13869999   0.1486
    3.76876187]
 [ 84.27410821   0.11920001   0.1355       0.1147       0.1268
    3.71591949]
 [125.80652281   0.42790005   0.69810003   0.22000001   0.66709995
    3.91062903]
 ...
 [-11.53284703   0.11979999   0.1487       0.13         0.11980001
    3.71941757]
 [-78.40194083   0.85459995   0.91180003   0.0485       0.90100002
    3.96635985]
 [301.64290255   0.41240001   0.24790001   0.43899998   0.49130002
    3.87506866]][0m
[37m[1m[2023-07-03 01:57:40,355][188188] Max Reward on eval: 703.8845782383578[0m
[37m[1m[2023-07-03 01:57:40,356][188188] Min Reward on eval: -398.50178244067354[0m
[37m[1m[2023-07-03 01:57:40,356][188188] Mean Reward across all agents: 55.4306500410798[0m
[37m[1m[2023-07-03 01:57:40,356][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:57:40,358][188188] mean_value=-2274.072242119758, max_value=101.00256116850505[0m
[37m[1m[2023-07-03 01:57:40,361][188188] New mean coefficients: [[-0.6841316 -2.13591    0.1369834 -0.6957078 -1.6353117 -0.7744297]][0m
[37m[1m[2023-07-03 01:57:40,362][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:57:49,337][188188] train() took 8.97 seconds to complete[0m
[36m[2023-07-03 01:57:49,338][188188] FPS: 427905.87[0m
[36m[2023-07-03 01:57:49,340][188188] itr=150, itrs=2000, Progress: 7.50%[0m
[37m[1m[2023-07-03 01:57:52,013][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000130[0m
[36m[2023-07-03 01:58:04,438][188188] train() took 12.09 seconds to complete[0m
[36m[2023-07-03 01:58:04,439][188188] FPS: 317548.78[0m
[36m[2023-07-03 01:58:08,734][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:58:08,735][188188] Reward + Measures: [[24.80646259  0.35657698  0.34417066  0.33192334  0.37749898  3.88632798]][0m
[37m[1m[2023-07-03 01:58:08,735][188188] Max Reward on eval: 24.80646259058648[0m
[37m[1m[2023-07-03 01:58:08,735][188188] Min Reward on eval: 24.80646259058648[0m
[37m[1m[2023-07-03 01:58:08,735][188188] Mean Reward across all agents: 24.80646259058648[0m
[37m[1m[2023-07-03 01:58:08,736][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:58:13,820][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:58:13,820][188188] Reward + Measures: [[ -14.52772983    0.2441        0.2225        0.25100002    0.24379997
     3.67108536]
 [ 187.00689786    0.29490003    0.5104        0.37700006    0.52770007
     3.78269744]
 [-131.04035093    0.44469997    0.47440001    0.42080003    0.44580004
     3.91061854]
 ...
 [  -2.98460534    0.49060002    0.49639997    0.48000002    0.50790006
     3.88076782]
 [ -70.94296192    0.27950001    0.26720002    0.27270001    0.27540001
     3.88955116]
 [ 199.49900544    0.13260001    0.64320004    0.47530004    0.64600003
     3.91084456]][0m
[37m[1m[2023-07-03 01:58:13,820][188188] Max Reward on eval: 566.0742158968002[0m
[37m[1m[2023-07-03 01:58:13,821][188188] Min Reward on eval: -312.2173576327041[0m
[37m[1m[2023-07-03 01:58:13,821][188188] Mean Reward across all agents: 47.50166527730581[0m
[37m[1m[2023-07-03 01:58:13,821][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:58:13,824][188188] mean_value=-1459.206757615687, max_value=28.911632892205446[0m
[37m[1m[2023-07-03 01:58:13,826][188188] New mean coefficients: [[ 0.599223   -2.2441485   0.29815036  0.24875909 -1.7781774  -0.09664768]][0m
[37m[1m[2023-07-03 01:58:13,827][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:58:22,866][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 01:58:22,866][188188] FPS: 424932.38[0m
[36m[2023-07-03 01:58:22,868][188188] itr=151, itrs=2000, Progress: 7.55%[0m
[36m[2023-07-03 01:58:34,539][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 01:58:34,539][188188] FPS: 329641.77[0m
[36m[2023-07-03 01:58:38,761][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:58:38,761][188188] Reward + Measures: [[47.95310246  0.19708599  0.22291967  0.189311    0.19700767  3.7824707 ]][0m
[37m[1m[2023-07-03 01:58:38,762][188188] Max Reward on eval: 47.95310246430123[0m
[37m[1m[2023-07-03 01:58:38,762][188188] Min Reward on eval: 47.95310246430123[0m
[37m[1m[2023-07-03 01:58:38,762][188188] Mean Reward across all agents: 47.95310246430123[0m
[37m[1m[2023-07-03 01:58:38,762][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:58:43,773][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:58:43,774][188188] Reward + Measures: [[137.27162887   0.1693       0.21229999   0.14570001   0.18169999
    3.74309731]
 [308.74656688   0.0656       0.72490001   0.5517       0.73339999
    3.94811797]
 [ 45.70099336   0.0921       0.0982       0.11329999   0.1068
    3.81733942]
 ...
 [255.744278     0.10030001   0.41879997   0.31390002   0.40210006
    3.84846878]
 [-35.29598978   0.15350001   0.1243       0.126        0.1548
    3.94358373]
 [ -3.30487271   0.08020001   0.1452       0.10539999   0.108
    3.70473027]][0m
[37m[1m[2023-07-03 01:58:43,774][188188] Max Reward on eval: 827.0257033973933[0m
[37m[1m[2023-07-03 01:58:43,774][188188] Min Reward on eval: -138.12877811100333[0m
[37m[1m[2023-07-03 01:58:43,774][188188] Mean Reward across all agents: 101.38751381637883[0m
[37m[1m[2023-07-03 01:58:43,775][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:58:43,776][188188] mean_value=-2375.2273009233245, max_value=115.63857383428297[0m
[37m[1m[2023-07-03 01:58:43,779][188188] New mean coefficients: [[-1.0146348  -2.8926475   1.6497588  -0.5234065  -2.7251632   0.22355169]][0m
[37m[1m[2023-07-03 01:58:43,780][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:58:52,831][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 01:58:52,831][188188] FPS: 424314.22[0m
[36m[2023-07-03 01:58:52,834][188188] itr=152, itrs=2000, Progress: 7.60%[0m
[36m[2023-07-03 01:59:04,393][188188] train() took 11.54 seconds to complete[0m
[36m[2023-07-03 01:59:04,393][188188] FPS: 332740.09[0m
[36m[2023-07-03 01:59:08,726][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:59:08,727][188188] Reward + Measures: [[57.66880037  0.13946134  0.15751399  0.13394766  0.13279033  3.79986835]][0m
[37m[1m[2023-07-03 01:59:08,727][188188] Max Reward on eval: 57.66880037212872[0m
[37m[1m[2023-07-03 01:59:08,727][188188] Min Reward on eval: 57.66880037212872[0m
[37m[1m[2023-07-03 01:59:08,728][188188] Mean Reward across all agents: 57.66880037212872[0m
[37m[1m[2023-07-03 01:59:08,728][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:59:13,784][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:59:13,784][188188] Reward + Measures: [[ 40.82934873   0.1012       0.26410002   0.19509999   0.23079999
    3.90238738]
 [-18.50051393   0.113        0.1328       0.1293       0.1166
    3.65050697]
 [ 47.38121247   0.2753       0.26350001   0.2748       0.25100002
    3.78798366]
 ...
 [ 25.91751265   0.1488       0.14660001   0.1418       0.1455
    3.80382919]
 [ 68.45466929   0.20750001   0.2282       0.1886       0.20009999
    3.7639966 ]
 [228.73790428   0.38900003   0.31400001   0.15710001   0.40470001
    3.7982254 ]][0m
[37m[1m[2023-07-03 01:59:13,784][188188] Max Reward on eval: 464.79789349995553[0m
[37m[1m[2023-07-03 01:59:13,785][188188] Min Reward on eval: -201.4161997004645[0m
[37m[1m[2023-07-03 01:59:13,785][188188] Mean Reward across all agents: 17.73129277518723[0m
[37m[1m[2023-07-03 01:59:13,785][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:59:13,787][188188] mean_value=-2666.3846773712294, max_value=50.14739912212413[0m
[37m[1m[2023-07-03 01:59:13,789][188188] New mean coefficients: [[-2.3867302  -3.5230355  -0.4399047  -0.7034757  -1.7410417  -0.07171798]][0m
[37m[1m[2023-07-03 01:59:13,790][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:59:22,850][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 01:59:22,850][188188] FPS: 423936.18[0m
[36m[2023-07-03 01:59:22,852][188188] itr=153, itrs=2000, Progress: 7.65%[0m
[36m[2023-07-03 01:59:34,659][188188] train() took 11.79 seconds to complete[0m
[36m[2023-07-03 01:59:34,660][188188] FPS: 325749.29[0m
[36m[2023-07-03 01:59:38,973][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:59:38,974][188188] Reward + Measures: [[57.17382271  0.12501533  0.14461666  0.11737233  0.11712667  3.81834054]][0m
[37m[1m[2023-07-03 01:59:38,974][188188] Max Reward on eval: 57.17382271210335[0m
[37m[1m[2023-07-03 01:59:38,974][188188] Min Reward on eval: 57.17382271210335[0m
[37m[1m[2023-07-03 01:59:38,974][188188] Mean Reward across all agents: 57.17382271210335[0m
[37m[1m[2023-07-03 01:59:38,975][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:59:44,025][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 01:59:44,025][188188] Reward + Measures: [[ 22.69810715   0.2728       0.15100001   0.13500001   0.27449998
    3.98059583]
 [ 90.85507412   0.44049999   0.37620002   0.19880001   0.49899998
    3.92268348]
 [ -3.30622764   0.2086       0.24460001   0.0944       0.24240001
    3.92892265]
 ...
 [ 18.77338161   0.3335       0.3439       0.0753       0.3515
    3.92735982]
 [ -7.92002367   0.42930004   0.48160002   0.08050001   0.47890002
    3.88060164]
 [223.99411739   0.33919999   0.31140003   0.31010002   0.4298
    3.87235045]][0m
[37m[1m[2023-07-03 01:59:44,026][188188] Max Reward on eval: 263.2518048383296[0m
[37m[1m[2023-07-03 01:59:44,026][188188] Min Reward on eval: -180.19320724979042[0m
[37m[1m[2023-07-03 01:59:44,026][188188] Mean Reward across all agents: 26.208559116932626[0m
[37m[1m[2023-07-03 01:59:44,026][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 01:59:44,028][188188] mean_value=-3143.145746774076, max_value=-30.576633024872137[0m
[36m[2023-07-03 01:59:44,030][188188] XNES is restarting with a new solution whose measures are [0.43729997 0.53910005 0.62309998 0.24529998 3.89851379] and objective is -55.450012602470814[0m
[36m[2023-07-03 01:59:44,031][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 01:59:44,034][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 01:59:44,034][188188] Moving the mean solution point...[0m
[36m[2023-07-03 01:59:53,019][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 01:59:53,020][188188] FPS: 427456.02[0m
[36m[2023-07-03 01:59:53,022][188188] itr=154, itrs=2000, Progress: 7.70%[0m
[36m[2023-07-03 02:00:04,595][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 02:00:04,595][188188] FPS: 332422.59[0m
[36m[2023-07-03 02:00:08,839][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:00:08,839][188188] Reward + Measures: [[63.2137532   0.17552832  0.87701106  0.883017    0.71866363  3.93831873]][0m
[37m[1m[2023-07-03 02:00:08,839][188188] Max Reward on eval: 63.21375320032732[0m
[37m[1m[2023-07-03 02:00:08,840][188188] Min Reward on eval: 63.21375320032732[0m
[37m[1m[2023-07-03 02:00:08,840][188188] Mean Reward across all agents: 63.21375320032732[0m
[37m[1m[2023-07-03 02:00:08,840][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:00:13,970][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:00:13,971][188188] Reward + Measures: [[ 21.08492546   0.0848       0.10910001   0.1098       0.09060001
    3.88774085]
 [ 23.01256524   0.0021       0.97760004   0.97719997   0.98209995
    3.99641967]
 [ 25.83486177   0.0067       0.89120001   0.89309996   0.89099997
    3.99131632]
 ...
 [ 34.15983226   0.1007       0.95900005   0.95880002   0.8721
    3.9435184 ]
 [ -5.69723408   0.10470001   0.17379999   0.16110002   0.16410001
    3.73579574]
 [-25.12454032   0.1012       0.88080007   0.88510001   0.78170002
    3.98503685]][0m
[37m[1m[2023-07-03 02:00:13,971][188188] Max Reward on eval: 341.80931375399234[0m
[37m[1m[2023-07-03 02:00:13,971][188188] Min Reward on eval: -335.6152248628438[0m
[37m[1m[2023-07-03 02:00:13,971][188188] Mean Reward across all agents: 4.687007915108194[0m
[37m[1m[2023-07-03 02:00:13,972][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:00:13,974][188188] mean_value=-1447.3632693641885, max_value=355.11851734707466[0m
[37m[1m[2023-07-03 02:00:13,976][188188] New mean coefficients: [[ 1.769855   -0.00949466 -1.2422407  -1.2077962  -1.4972521  -1.3943992 ]][0m
[37m[1m[2023-07-03 02:00:13,977][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:00:22,930][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 02:00:22,931][188188] FPS: 428968.63[0m
[36m[2023-07-03 02:00:22,933][188188] itr=155, itrs=2000, Progress: 7.75%[0m
[36m[2023-07-03 02:00:34,584][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 02:00:34,585][188188] FPS: 330085.68[0m
[36m[2023-07-03 02:00:38,892][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:00:38,892][188188] Reward + Measures: [[-59.57351385   0.21201932   0.9183653    0.92813671   0.72283173
    3.99069548]][0m
[37m[1m[2023-07-03 02:00:38,892][188188] Max Reward on eval: -59.573513847840914[0m
[37m[1m[2023-07-03 02:00:38,892][188188] Min Reward on eval: -59.573513847840914[0m
[37m[1m[2023-07-03 02:00:38,892][188188] Mean Reward across all agents: -59.573513847840914[0m
[37m[1m[2023-07-03 02:00:38,893][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:00:43,908][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:00:43,908][188188] Reward + Measures: [[ 161.35212643    0.0007        0.93590003    0.93320006    0.92770004
     3.97902536]
 [-147.81144234    0.0145        0.81160003    0.80390006    0.86210006
     3.84408808]
 [  43.83724382    0.0108        0.89560002    0.88859999    0.89560002
     3.99689364]
 ...
 [-153.15723241    0.26159999    0.87950003    0.92220002    0.68610001
     3.97145271]
 [-113.99355506    0.26939997    0.67189997    0.76950008    0.60820001
     3.93446422]
 [ -50.24709739    0.1987        0.86750001    0.87830001    0.69340003
     3.98954272]][0m
[37m[1m[2023-07-03 02:00:43,909][188188] Max Reward on eval: 253.10999082867056[0m
[37m[1m[2023-07-03 02:00:43,909][188188] Min Reward on eval: -264.00697112325577[0m
[37m[1m[2023-07-03 02:00:43,909][188188] Mean Reward across all agents: -37.910077190185426[0m
[37m[1m[2023-07-03 02:00:43,909][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:00:43,912][188188] mean_value=-1104.3675677226995, max_value=361.9675264996431[0m
[37m[1m[2023-07-03 02:00:43,914][188188] New mean coefficients: [[ 0.6686852  0.9155005 -1.1763868 -1.561348  -0.9444847 -2.149312 ]][0m
[37m[1m[2023-07-03 02:00:43,915][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:00:52,953][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 02:00:52,953][188188] FPS: 424955.20[0m
[36m[2023-07-03 02:00:52,956][188188] itr=156, itrs=2000, Progress: 7.80%[0m
[36m[2023-07-03 02:01:04,908][188188] train() took 11.93 seconds to complete[0m
[36m[2023-07-03 02:01:04,908][188188] FPS: 321773.65[0m
[36m[2023-07-03 02:01:09,238][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:01:09,239][188188] Reward + Measures: [[-15.3636511    0.18134165   0.29756865   0.33281165   0.24470632
    3.89634752]][0m
[37m[1m[2023-07-03 02:01:09,239][188188] Max Reward on eval: -15.36365109602063[0m
[37m[1m[2023-07-03 02:01:09,239][188188] Min Reward on eval: -15.36365109602063[0m
[37m[1m[2023-07-03 02:01:09,240][188188] Mean Reward across all agents: -15.36365109602063[0m
[37m[1m[2023-07-03 02:01:09,240][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:01:14,228][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:01:14,229][188188] Reward + Measures: [[-151.81982496    0.1294        0.1797        0.17169999    0.13509999
     3.87681127]
 [-130.87856952    0.14690001    0.22379997    0.2458        0.16140001
     3.89237452]
 [  19.9306337     0.07039999    0.95979995    0.95860004    0.89359999
     3.83487296]
 ...
 [  14.94665842    0.0164        0.70890003    0.70240003    0.68909997
     3.98969698]
 [  61.15728895    0.101         0.77320004    0.7974        0.69229996
     3.98213172]
 [  45.65134264    0.30159998    0.32049999    0.49320003    0.40170002
     3.77488708]][0m
[37m[1m[2023-07-03 02:01:14,229][188188] Max Reward on eval: 301.865285081882[0m
[37m[1m[2023-07-03 02:01:14,229][188188] Min Reward on eval: -297.3675151756499[0m
[37m[1m[2023-07-03 02:01:14,230][188188] Mean Reward across all agents: -8.386748433560058[0m
[37m[1m[2023-07-03 02:01:14,230][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:01:14,232][188188] mean_value=-1125.01101046027, max_value=314.2528929639912[0m
[37m[1m[2023-07-03 02:01:14,234][188188] New mean coefficients: [[ 1.654367    1.4798989  -0.08906794 -1.4223318  -1.0916448  -2.7181675 ]][0m
[37m[1m[2023-07-03 02:01:14,235][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:01:23,231][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 02:01:23,232][188188] FPS: 426927.12[0m
[36m[2023-07-03 02:01:23,234][188188] itr=157, itrs=2000, Progress: 7.85%[0m
[36m[2023-07-03 02:01:34,828][188188] train() took 11.57 seconds to complete[0m
[36m[2023-07-03 02:01:34,829][188188] FPS: 331770.32[0m
[36m[2023-07-03 02:01:39,123][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:01:39,123][188188] Reward + Measures: [[-42.90083085   0.15018      0.89620703   0.90688705   0.76191503
    3.98704171]][0m
[37m[1m[2023-07-03 02:01:39,124][188188] Max Reward on eval: -42.900830854256014[0m
[37m[1m[2023-07-03 02:01:39,124][188188] Min Reward on eval: -42.900830854256014[0m
[37m[1m[2023-07-03 02:01:39,124][188188] Mean Reward across all agents: -42.900830854256014[0m
[37m[1m[2023-07-03 02:01:39,124][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:01:44,200][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:01:44,200][188188] Reward + Measures: [[-281.43794654    0.59250003    0.87939996    0.89950001    0.29930001
     3.99323392]
 [ -34.19097328    0.0508        0.1856        0.1921        0.14579999
     3.90949249]
 [  16.36675021    0.0621        0.0826        0.0881        0.0699
     3.58633018]
 ...
 [-107.5250826     0.06879999    0.70830005    0.73439997    0.73139995
     3.95382738]
 [ -36.2815983     0.12960002    0.62760001    0.6778        0.60650009
     3.95800328]
 [  -4.92970202    0.2606        0.2757        0.31829998    0.29500002
     3.75726867]][0m
[37m[1m[2023-07-03 02:01:44,201][188188] Max Reward on eval: 262.1492261790903[0m
[37m[1m[2023-07-03 02:01:44,201][188188] Min Reward on eval: -367.7161283206195[0m
[37m[1m[2023-07-03 02:01:44,201][188188] Mean Reward across all agents: -31.073366614217267[0m
[37m[1m[2023-07-03 02:01:44,201][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:01:44,204][188188] mean_value=-1691.3239991775101, max_value=280.52981788679165[0m
[37m[1m[2023-07-03 02:01:44,206][188188] New mean coefficients: [[ 1.2945731   2.2228146  -0.03330804  0.15990222 -1.9058455  -3.2906532 ]][0m
[37m[1m[2023-07-03 02:01:44,207][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:01:53,314][188188] train() took 9.11 seconds to complete[0m
[36m[2023-07-03 02:01:53,314][188188] FPS: 421726.04[0m
[36m[2023-07-03 02:01:53,317][188188] itr=158, itrs=2000, Progress: 7.90%[0m
[36m[2023-07-03 02:02:04,949][188188] train() took 11.61 seconds to complete[0m
[36m[2023-07-03 02:02:04,949][188188] FPS: 330724.43[0m
[36m[2023-07-03 02:02:09,248][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:02:09,249][188188] Reward + Measures: [[0.15119109 0.033802   0.83185238 0.84218538 0.81481373 3.98114252]][0m
[37m[1m[2023-07-03 02:02:09,249][188188] Max Reward on eval: 0.15119108689797575[0m
[37m[1m[2023-07-03 02:02:09,249][188188] Min Reward on eval: 0.15119108689797575[0m
[37m[1m[2023-07-03 02:02:09,250][188188] Mean Reward across all agents: 0.15119108689797575[0m
[37m[1m[2023-07-03 02:02:09,250][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:02:14,422][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:02:14,428][188188] Reward + Measures: [[-64.29619107   0.07610001   0.117        0.0988       0.12159999
    3.74112296]
 [ 34.22704371   0.0955       0.0875       0.0843       0.0845
    3.9225018 ]
 [-73.84391368   0.0711       0.17450002   0.1565       0.1948
    3.82856154]
 ...
 [-92.52120702   0.0887       0.1585       0.15820001   0.1786
    3.75152755]
 [-16.45743148   0.0445       0.55840003   0.57670003   0.58600003
    3.91960979]
 [ 20.04444689   0.0644       0.12050001   0.10649999   0.0803
    3.92865825]][0m
[37m[1m[2023-07-03 02:02:14,428][188188] Max Reward on eval: 225.52708913497628[0m
[37m[1m[2023-07-03 02:02:14,429][188188] Min Reward on eval: -244.78395319096745[0m
[37m[1m[2023-07-03 02:02:14,429][188188] Mean Reward across all agents: -22.948778078088203[0m
[37m[1m[2023-07-03 02:02:14,429][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:02:14,431][188188] mean_value=-1968.8258907273882, max_value=159.847199987468[0m
[37m[1m[2023-07-03 02:02:14,433][188188] New mean coefficients: [[-0.07361042  2.5126112  -0.93771416 -0.7649618  -2.4208155  -1.0410376 ]][0m
[37m[1m[2023-07-03 02:02:14,434][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:02:23,400][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 02:02:23,400][188188] FPS: 428359.63[0m
[36m[2023-07-03 02:02:23,403][188188] itr=159, itrs=2000, Progress: 7.95%[0m
[36m[2023-07-03 02:02:35,079][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 02:02:35,080][188188] FPS: 329475.77[0m
[36m[2023-07-03 02:02:39,306][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:02:39,306][188188] Reward + Measures: [[-133.94333165    0.038272      0.73697168    0.75526869    0.75174493
     3.95941138]][0m
[37m[1m[2023-07-03 02:02:39,307][188188] Max Reward on eval: -133.94333165354067[0m
[37m[1m[2023-07-03 02:02:39,307][188188] Min Reward on eval: -133.94333165354067[0m
[37m[1m[2023-07-03 02:02:39,307][188188] Mean Reward across all agents: -133.94333165354067[0m
[37m[1m[2023-07-03 02:02:39,307][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:02:44,327][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:02:44,328][188188] Reward + Measures: [[ 19.50186758   0.0864       0.15459999   0.1892       0.1384
    3.83639956]
 [ 29.78522611   0.083        0.1033       0.12590002   0.08889999
    3.84180808]
 [  0.628947     0.11979999   0.3716       0.40159997   0.3371
    3.8108139 ]
 ...
 [ -4.19675026   0.07559999   0.0991       0.0905       0.0801
    3.66848183]
 [-76.34448261   0.2313       0.523        0.66530001   0.58770007
    3.92171288]
 [102.21594476   0.18470001   0.76640004   0.79610002   0.60409999
    3.94432616]][0m
[37m[1m[2023-07-03 02:02:44,328][188188] Max Reward on eval: 159.4785766456276[0m
[37m[1m[2023-07-03 02:02:44,328][188188] Min Reward on eval: -281.6289014998823[0m
[37m[1m[2023-07-03 02:02:44,328][188188] Mean Reward across all agents: -34.64389450190773[0m
[37m[1m[2023-07-03 02:02:44,329][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:02:44,330][188188] mean_value=-2031.7300874571902, max_value=86.99010423659232[0m
[37m[1m[2023-07-03 02:02:44,332][188188] New mean coefficients: [[ 2.792355    2.51634    -0.26315612 -0.7110745  -1.2291831  -0.6911346 ]][0m
[37m[1m[2023-07-03 02:02:44,333][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:02:53,359][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 02:02:53,359][188188] FPS: 425537.01[0m
[36m[2023-07-03 02:02:53,361][188188] itr=160, itrs=2000, Progress: 8.00%[0m
[37m[1m[2023-07-03 02:02:55,905][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000140[0m
[36m[2023-07-03 02:03:07,972][188188] train() took 11.74 seconds to complete[0m
[36m[2023-07-03 02:03:07,972][188188] FPS: 327088.33[0m
[36m[2023-07-03 02:03:12,328][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:03:12,328][188188] Reward + Measures: [[-67.02184212   0.08848967   0.56696469   0.61002767   0.59855133
    3.94600844]][0m
[37m[1m[2023-07-03 02:03:12,328][188188] Max Reward on eval: -67.02184211601332[0m
[37m[1m[2023-07-03 02:03:12,329][188188] Min Reward on eval: -67.02184211601332[0m
[37m[1m[2023-07-03 02:03:12,329][188188] Mean Reward across all agents: -67.02184211601332[0m
[37m[1m[2023-07-03 02:03:12,329][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:03:17,425][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:03:17,431][188188] Reward + Measures: [[ -62.97884502    0.0381        0.249         0.25680003    0.22849999
     3.73959541]
 [ -71.61119897    0.0782        0.25139999    0.27160001    0.2538
     3.79003716]
 [ -17.51721949    0.0916        0.48680001    0.54650003    0.49530002
     3.94056129]
 ...
 [  99.7107524     0.0269        0.47570005    0.4585        0.42390004
     3.94178176]
 [-103.66360641    0.19329999    0.45039997    0.49840003    0.3565
     3.92291617]
 [   4.1167507     0.0131        0.8811        0.88929999    0.88070005
     3.99037814]][0m
[37m[1m[2023-07-03 02:03:17,431][188188] Max Reward on eval: 290.11023328304293[0m
[37m[1m[2023-07-03 02:03:17,432][188188] Min Reward on eval: -239.00138710662722[0m
[37m[1m[2023-07-03 02:03:17,432][188188] Mean Reward across all agents: -18.02516432312969[0m
[37m[1m[2023-07-03 02:03:17,432][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:03:17,434][188188] mean_value=-1557.266322251617, max_value=155.4943883697759[0m
[37m[1m[2023-07-03 02:03:17,436][188188] New mean coefficients: [[ 3.594976   1.1436937 -1.3284929 -0.312057  -1.5994763  0.128753 ]][0m
[37m[1m[2023-07-03 02:03:17,437][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:03:26,504][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 02:03:26,504][188188] FPS: 423612.40[0m
[36m[2023-07-03 02:03:26,506][188188] itr=161, itrs=2000, Progress: 8.05%[0m
[36m[2023-07-03 02:03:38,178][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 02:03:38,178][188188] FPS: 329610.30[0m
[36m[2023-07-03 02:03:42,480][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:03:42,480][188188] Reward + Measures: [[-74.55761537   0.09956134   0.53512865   0.57485628   0.55983603
    3.91559601]][0m
[37m[1m[2023-07-03 02:03:42,480][188188] Max Reward on eval: -74.55761537259586[0m
[37m[1m[2023-07-03 02:03:42,481][188188] Min Reward on eval: -74.55761537259586[0m
[37m[1m[2023-07-03 02:03:42,481][188188] Mean Reward across all agents: -74.55761537259586[0m
[37m[1m[2023-07-03 02:03:42,481][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:03:47,472][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:03:47,473][188188] Reward + Measures: [[ -9.32935087   0.06820001   0.094        0.1141       0.07230001
    3.82759666]
 [-35.08688517   0.0743       0.96049994   0.9655       0.90469998
    3.98876452]
 [-77.8520918    0.20469999   0.60100001   0.65530002   0.54659998
    3.93313718]
 ...
 [-21.12560188   0.0714       0.16489999   0.183        0.1619
    3.90283704]
 [-25.52988551   0.07130001   0.15890001   0.2129       0.1832
    3.84278035]
 [-31.11767596   0.0485       0.34800002   0.338        0.3335
    3.96274424]][0m
[37m[1m[2023-07-03 02:03:47,473][188188] Max Reward on eval: 80.1938135731034[0m
[37m[1m[2023-07-03 02:03:47,473][188188] Min Reward on eval: -306.45941832163373[0m
[37m[1m[2023-07-03 02:03:47,474][188188] Mean Reward across all agents: -26.93886848199421[0m
[37m[1m[2023-07-03 02:03:47,474][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:03:47,475][188188] mean_value=-2333.3638810178186, max_value=381.5850586662291[0m
[37m[1m[2023-07-03 02:03:47,478][188188] New mean coefficients: [[ 2.5317037   1.0866519  -2.5456078   0.87221634 -0.33659983 -1.4897511 ]][0m
[37m[1m[2023-07-03 02:03:47,479][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:03:56,508][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 02:03:56,508][188188] FPS: 425350.28[0m
[36m[2023-07-03 02:03:56,511][188188] itr=162, itrs=2000, Progress: 8.10%[0m
[36m[2023-07-03 02:04:08,195][188188] train() took 11.66 seconds to complete[0m
[36m[2023-07-03 02:04:08,195][188188] FPS: 329378.34[0m
[36m[2023-07-03 02:04:12,424][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:04:12,424][188188] Reward + Measures: [[-108.08269953    0.058253      0.85323167    0.86624837    0.82016432
     3.95739698]][0m
[37m[1m[2023-07-03 02:04:12,424][188188] Max Reward on eval: -108.08269953439904[0m
[37m[1m[2023-07-03 02:04:12,424][188188] Min Reward on eval: -108.08269953439904[0m
[37m[1m[2023-07-03 02:04:12,425][188188] Mean Reward across all agents: -108.08269953439904[0m
[37m[1m[2023-07-03 02:04:12,425][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:04:17,519][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:04:17,519][188188] Reward + Measures: [[-44.95343964   0.07399999   0.48189998   0.44479999   0.51910001
    3.91718459]
 [145.3814576    0.7847001    0.78860003   0.84320003   0.84929991
    3.96802592]
 [-95.4825461    0.34640002   0.33500001   0.38060001   0.40120003
    3.86579561]
 ...
 [ 92.24283978   0.64219999   0.63880002   0.69090003   0.66179997
    3.9801724 ]
 [-26.02710629   0.1097       0.13869999   0.22659998   0.14310001
    3.91666269]
 [-31.85460568   0.7202       0.71740001   0.75830001   0.73119998
    3.97771525]][0m
[37m[1m[2023-07-03 02:04:17,520][188188] Max Reward on eval: 303.7167759221978[0m
[37m[1m[2023-07-03 02:04:17,520][188188] Min Reward on eval: -191.69767902344466[0m
[37m[1m[2023-07-03 02:04:17,520][188188] Mean Reward across all agents: -11.594430668364764[0m
[37m[1m[2023-07-03 02:04:17,520][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:04:17,522][188188] mean_value=-980.0257865647637, max_value=19.914567438354624[0m
[37m[1m[2023-07-03 02:04:17,525][188188] New mean coefficients: [[ 3.739852   0.4410116 -3.2451236  1.9315739  0.7198211 -1.73387  ]][0m
[37m[1m[2023-07-03 02:04:17,526][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:04:26,538][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 02:04:26,543][188188] FPS: 426167.85[0m
[36m[2023-07-03 02:04:26,546][188188] itr=163, itrs=2000, Progress: 8.15%[0m
[36m[2023-07-03 02:04:38,126][188188] train() took 11.56 seconds to complete[0m
[36m[2023-07-03 02:04:38,126][188188] FPS: 332164.57[0m
[36m[2023-07-03 02:04:42,405][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:04:42,411][188188] Reward + Measures: [[-103.92617023    0.133637      0.7291193     0.7633307     0.67926598
     3.96228909]][0m
[37m[1m[2023-07-03 02:04:42,411][188188] Max Reward on eval: -103.92617022726716[0m
[37m[1m[2023-07-03 02:04:42,411][188188] Min Reward on eval: -103.92617022726716[0m
[37m[1m[2023-07-03 02:04:42,412][188188] Mean Reward across all agents: -103.92617022726716[0m
[37m[1m[2023-07-03 02:04:42,412][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:04:47,466][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:04:47,472][188188] Reward + Measures: [[-112.34881878    0.26350001    0.66960001    0.74259996    0.53939998
     3.92792678]
 [ -70.43257479    0.16680001    0.1725        0.25079998    0.13109998
     3.77272344]
 [-122.58039421    0.11180001    0.7682001     0.77960008    0.68620002
     3.97060013]
 ...
 [ -21.8160695     0.0663        0.1208        0.17750001    0.175
     3.83559585]
 [ -14.34511153    0.1103        0.1425        0.1559        0.1201
     3.76494646]
 [   9.75698678    0.0619        0.48719999    0.51249999    0.49600002
     3.95032167]][0m
[37m[1m[2023-07-03 02:04:47,472][188188] Max Reward on eval: 117.29235221408308[0m
[37m[1m[2023-07-03 02:04:47,472][188188] Min Reward on eval: -334.2171297080815[0m
[37m[1m[2023-07-03 02:04:47,473][188188] Mean Reward across all agents: -56.28826809884356[0m
[37m[1m[2023-07-03 02:04:47,473][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:04:47,475][188188] mean_value=-1735.7799278007471, max_value=-39.9600917125612[0m
[36m[2023-07-03 02:04:47,477][188188] XNES is restarting with a new solution whose measures are [0.4014     0.81580001 0.85890001 0.47180006 3.9712491 ] and objective is 4.871656478010118[0m
[36m[2023-07-03 02:04:47,478][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 02:04:47,481][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 02:04:47,482][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:04:56,586][188188] train() took 9.10 seconds to complete[0m
[36m[2023-07-03 02:04:56,586][188188] FPS: 421836.67[0m
[36m[2023-07-03 02:04:56,589][188188] itr=164, itrs=2000, Progress: 8.20%[0m
[36m[2023-07-03 02:05:08,170][188188] train() took 11.56 seconds to complete[0m
[36m[2023-07-03 02:05:08,171][188188] FPS: 332180.26[0m
[36m[2023-07-03 02:05:12,405][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:05:12,405][188188] Reward + Measures: [[-15.63255309   0.07418533   0.75758761   0.77188069   0.7155
    3.91726828]][0m
[37m[1m[2023-07-03 02:05:12,406][188188] Max Reward on eval: -15.632553092111328[0m
[37m[1m[2023-07-03 02:05:12,406][188188] Min Reward on eval: -15.632553092111328[0m
[37m[1m[2023-07-03 02:05:12,406][188188] Mean Reward across all agents: -15.632553092111328[0m
[37m[1m[2023-07-03 02:05:12,406][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:05:17,357][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:05:17,358][188188] Reward + Measures: [[ 18.51864132   0.1532       0.59149998   0.611        0.50230002
    3.97148585]
 [  1.63923052   0.0837       0.1471       0.2129       0.19069999
    3.89590502]
 [ 19.80403189   0.0812       0.46759996   0.48899999   0.5352
    3.86000228]
 ...
 [  2.5780658    0.12660001   0.57300001   0.60230005   0.55290002
    3.96211505]
 [-96.37521548   0.0989       0.58050007   0.61669999   0.6358
    3.95335317]
 [  8.76486921   0.0612       0.33899999   0.36760002   0.38820001
    3.85809207]][0m
[37m[1m[2023-07-03 02:05:17,358][188188] Max Reward on eval: 215.34408950968646[0m
[37m[1m[2023-07-03 02:05:17,358][188188] Min Reward on eval: -210.90614551380276[0m
[37m[1m[2023-07-03 02:05:17,359][188188] Mean Reward across all agents: -12.146557676971545[0m
[37m[1m[2023-07-03 02:05:17,359][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:05:17,361][188188] mean_value=-1512.6746083426892, max_value=207.8709562812119[0m
[37m[1m[2023-07-03 02:05:17,363][188188] New mean coefficients: [[ 0.02412608  0.43378258 -1.4905547  -1.8969325  -0.4046278   0.61973345]][0m
[37m[1m[2023-07-03 02:05:17,364][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:05:26,393][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 02:05:26,393][188188] FPS: 425374.41[0m
[36m[2023-07-03 02:05:26,396][188188] itr=165, itrs=2000, Progress: 8.25%[0m
[36m[2023-07-03 02:05:38,030][188188] train() took 11.61 seconds to complete[0m
[36m[2023-07-03 02:05:38,030][188188] FPS: 330680.25[0m
[36m[2023-07-03 02:05:42,280][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:05:42,281][188188] Reward + Measures: [[24.63967632  0.14568733  0.56924736  0.59640902  0.50153965  3.95868802]][0m
[37m[1m[2023-07-03 02:05:42,281][188188] Max Reward on eval: 24.63967631704531[0m
[37m[1m[2023-07-03 02:05:42,281][188188] Min Reward on eval: 24.63967631704531[0m
[37m[1m[2023-07-03 02:05:42,281][188188] Mean Reward across all agents: 24.63967631704531[0m
[37m[1m[2023-07-03 02:05:42,282][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:05:47,267][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:05:47,268][188188] Reward + Measures: [[-90.45624863   0.3057       0.79469997   0.86079997   0.49349999
    3.99695468]
 [-35.37762011   0.0823       0.32960001   0.38200003   0.30879998
    3.92896652]
 [-38.74163127   0.16260001   0.2105       0.14929999   0.17989999
    3.81928754]
 ...
 [ -7.08534949   0.0478       0.56310004   0.57020003   0.60000002
    3.86692929]
 [ 79.28076896   0.19060002   0.68959999   0.70479995   0.53490001
    3.98381877]
 [ 98.60171942   0.1847       0.41690001   0.46079999   0.32940003
    3.9459343 ]][0m
[37m[1m[2023-07-03 02:05:47,268][188188] Max Reward on eval: 132.3564064372331[0m
[37m[1m[2023-07-03 02:05:47,268][188188] Min Reward on eval: -325.532590416912[0m
[37m[1m[2023-07-03 02:05:47,268][188188] Mean Reward across all agents: -20.280707353515513[0m
[37m[1m[2023-07-03 02:05:47,269][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:05:47,270][188188] mean_value=-1843.9395646525975, max_value=303.0495895232075[0m
[37m[1m[2023-07-03 02:05:47,273][188188] New mean coefficients: [[ 0.18452024  1.2692084   0.04793286 -3.8629532  -0.57035965 -0.38725805]][0m
[37m[1m[2023-07-03 02:05:47,274][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:05:56,381][188188] train() took 9.11 seconds to complete[0m
[36m[2023-07-03 02:05:56,381][188188] FPS: 421724.27[0m
[36m[2023-07-03 02:05:56,383][188188] itr=166, itrs=2000, Progress: 8.30%[0m
[36m[2023-07-03 02:06:08,180][188188] train() took 11.78 seconds to complete[0m
[36m[2023-07-03 02:06:08,180][188188] FPS: 326035.61[0m
[36m[2023-07-03 02:06:12,569][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:06:12,569][188188] Reward + Measures: [[-10.64900124   0.09418467   0.284895     0.31975564   0.28813198
    3.95414352]][0m
[37m[1m[2023-07-03 02:06:12,569][188188] Max Reward on eval: -10.649001239180876[0m
[37m[1m[2023-07-03 02:06:12,570][188188] Min Reward on eval: -10.649001239180876[0m
[37m[1m[2023-07-03 02:06:12,570][188188] Mean Reward across all agents: -10.649001239180876[0m
[37m[1m[2023-07-03 02:06:12,570][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:06:17,585][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:06:17,586][188188] Reward + Measures: [[ 40.90453516   0.1053       0.68410009   0.68849999   0.58980006
    3.97118258]
 [-49.10953979   0.07030001   0.06370001   0.073        0.0852
    3.64170384]
 [236.1729732    0.5359       0.53729999   0.74610007   0.25330004
    3.97395945]
 ...
 [195.28081821   0.48759994   0.68559998   0.86919993   0.49359998
    3.98738599]
 [ 58.80327661   0.11440001   0.18100001   0.1382       0.11369999
    3.8415978 ]
 [ -1.40635121   0.06810001   0.26650003   0.29870003   0.25350001
    3.91592836]][0m
[37m[1m[2023-07-03 02:06:17,586][188188] Max Reward on eval: 368.79522390300406[0m
[37m[1m[2023-07-03 02:06:17,586][188188] Min Reward on eval: -296.5953045186587[0m
[37m[1m[2023-07-03 02:06:17,586][188188] Mean Reward across all agents: 18.635838654247053[0m
[37m[1m[2023-07-03 02:06:17,587][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:06:17,590][188188] mean_value=-1526.6898774802455, max_value=502.91483381426383[0m
[37m[1m[2023-07-03 02:06:17,592][188188] New mean coefficients: [[ 0.29805177  3.1668177  -1.0887301  -4.250747   -0.25190493  1.8349128 ]][0m
[37m[1m[2023-07-03 02:06:17,594][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:06:26,649][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 02:06:26,649][188188] FPS: 424152.42[0m
[36m[2023-07-03 02:06:26,651][188188] itr=167, itrs=2000, Progress: 8.35%[0m
[36m[2023-07-03 02:06:38,355][188188] train() took 11.68 seconds to complete[0m
[36m[2023-07-03 02:06:38,355][188188] FPS: 328640.92[0m
[36m[2023-07-03 02:06:42,623][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:06:42,624][188188] Reward + Measures: [[1.52961401 0.087763   0.23152266 0.27035934 0.25064632 3.94450235]][0m
[37m[1m[2023-07-03 02:06:42,624][188188] Max Reward on eval: 1.5296140146698478[0m
[37m[1m[2023-07-03 02:06:42,624][188188] Min Reward on eval: 1.5296140146698478[0m
[37m[1m[2023-07-03 02:06:42,625][188188] Mean Reward across all agents: 1.5296140146698478[0m
[37m[1m[2023-07-03 02:06:42,625][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:06:47,722][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:06:47,722][188188] Reward + Measures: [[-31.67432205   0.0648       0.08400001   0.17479999   0.13970001
    3.88772583]
 [ 83.2852792    0.21040002   0.37889999   0.44639999   0.2418
    3.97669983]
 [-81.2185903    0.063        0.2904       0.33230001   0.35089999
    3.95004344]
 ...
 [ -8.87318376   0.07120001   0.0784       0.0751       0.0823
    3.80643511]
 [ 46.00336168   0.0615       0.0841       0.13789999   0.0602
    3.93484688]
 [ 27.72342105   0.07910001   0.19860001   0.2282       0.1991
    3.90309978]][0m
[37m[1m[2023-07-03 02:06:47,723][188188] Max Reward on eval: 319.1220483662677[0m
[37m[1m[2023-07-03 02:06:47,723][188188] Min Reward on eval: -257.850323658064[0m
[37m[1m[2023-07-03 02:06:47,723][188188] Mean Reward across all agents: 5.256892719245328[0m
[37m[1m[2023-07-03 02:06:47,723][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:06:47,725][188188] mean_value=-2708.517848307273, max_value=241.96751901067864[0m
[37m[1m[2023-07-03 02:06:47,728][188188] New mean coefficients: [[ 2.8061411   1.3847481   0.00608146 -4.4089937   0.20183441  2.8318255 ]][0m
[37m[1m[2023-07-03 02:06:47,729][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:06:56,730][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 02:06:56,730][188188] FPS: 426674.52[0m
[36m[2023-07-03 02:06:56,733][188188] itr=168, itrs=2000, Progress: 8.40%[0m
[36m[2023-07-03 02:07:08,366][188188] train() took 11.61 seconds to complete[0m
[36m[2023-07-03 02:07:08,366][188188] FPS: 330727.78[0m
[36m[2023-07-03 02:07:12,688][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:07:12,688][188188] Reward + Measures: [[28.59911524  0.07763067  0.31216067  0.33369666  0.32355633  3.92125583]][0m
[37m[1m[2023-07-03 02:07:12,688][188188] Max Reward on eval: 28.59911524222954[0m
[37m[1m[2023-07-03 02:07:12,688][188188] Min Reward on eval: 28.59911524222954[0m
[37m[1m[2023-07-03 02:07:12,689][188188] Mean Reward across all agents: 28.59911524222954[0m
[37m[1m[2023-07-03 02:07:12,689][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:07:17,725][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:07:17,725][188188] Reward + Measures: [[ 0.51657516  0.0619      0.1105      0.1523      0.1229      3.83161211]
 [19.53635511  0.07340001  0.0719      0.09679999  0.0859      3.77650142]
 [31.60331842  0.0599      0.0532      0.1472      0.10880001  3.94153666]
 ...
 [79.23580433  0.09550001  0.1014      0.1384      0.08400001  3.94234729]
 [29.99038714  0.0754      0.0653      0.1374      0.12520002  3.91420341]
 [-0.24980591  0.0923      0.24860001  0.27990001  0.29459998  3.92965674]][0m
[37m[1m[2023-07-03 02:07:17,725][188188] Max Reward on eval: 159.70301745808683[0m
[37m[1m[2023-07-03 02:07:17,726][188188] Min Reward on eval: -167.49749420806765[0m
[37m[1m[2023-07-03 02:07:17,726][188188] Mean Reward across all agents: 2.7660030812848895[0m
[37m[1m[2023-07-03 02:07:17,726][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:07:17,728][188188] mean_value=-3547.023088200034, max_value=24.21232814271397[0m
[37m[1m[2023-07-03 02:07:17,730][188188] New mean coefficients: [[ 1.9001064  -1.5305766   0.12610605 -3.750423   -0.3862746   1.1935996 ]][0m
[37m[1m[2023-07-03 02:07:17,731][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:07:26,823][188188] train() took 9.09 seconds to complete[0m
[36m[2023-07-03 02:07:26,823][188188] FPS: 422425.34[0m
[36m[2023-07-03 02:07:26,825][188188] itr=169, itrs=2000, Progress: 8.45%[0m
[36m[2023-07-03 02:07:38,481][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 02:07:38,481][188188] FPS: 330075.39[0m
[36m[2023-07-03 02:07:42,786][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:07:42,787][188188] Reward + Measures: [[-1.68072143  0.06686133  0.17702299  0.20901033  0.21294667  3.77625775]][0m
[37m[1m[2023-07-03 02:07:42,787][188188] Max Reward on eval: -1.6807214258949486[0m
[37m[1m[2023-07-03 02:07:42,787][188188] Min Reward on eval: -1.6807214258949486[0m
[37m[1m[2023-07-03 02:07:42,788][188188] Mean Reward across all agents: -1.6807214258949486[0m
[37m[1m[2023-07-03 02:07:42,788][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:07:47,720][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:07:47,721][188188] Reward + Measures: [[ -18.07989784    0.06460001    0.0953        0.1344        0.13700001
     3.77925277]
 [  15.47001658    0.1031        0.09860001    0.15340002    0.13780001
     3.96541452]
 [  17.62554725    0.0393        0.39050004    0.43899998    0.4341
     3.96410155]
 ...
 [  51.96329642    0.0733        0.0816        0.14479999    0.1392
     3.92809463]
 [-130.59649645    0.0416        0.49860001    0.4797        0.52520001
     3.80126929]
 [ -16.04102248    0.1125        0.2563        0.3233        0.28340003
     3.79951215]][0m
[37m[1m[2023-07-03 02:07:47,721][188188] Max Reward on eval: 597.8428859290667[0m
[37m[1m[2023-07-03 02:07:47,721][188188] Min Reward on eval: -365.3931617588736[0m
[37m[1m[2023-07-03 02:07:47,722][188188] Mean Reward across all agents: -10.548857037150366[0m
[37m[1m[2023-07-03 02:07:47,722][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:07:47,724][188188] mean_value=-2153.4049666903957, max_value=528.569238822721[0m
[37m[1m[2023-07-03 02:07:47,727][188188] New mean coefficients: [[ 3.5882874  -2.1540625   0.13769878 -2.3534212   1.0725794   1.5205104 ]][0m
[37m[1m[2023-07-03 02:07:47,728][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:07:56,789][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 02:07:56,789][188188] FPS: 423866.20[0m
[36m[2023-07-03 02:07:56,791][188188] itr=170, itrs=2000, Progress: 8.50%[0m
[37m[1m[2023-07-03 02:07:59,337][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000150[0m
[36m[2023-07-03 02:08:11,412][188188] train() took 11.75 seconds to complete[0m
[36m[2023-07-03 02:08:11,412][188188] FPS: 326747.97[0m
[36m[2023-07-03 02:08:15,727][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:08:15,733][188188] Reward + Measures: [[0.08778602 0.06044167 0.09877367 0.165419   0.14626598 3.8412292 ]][0m
[37m[1m[2023-07-03 02:08:15,733][188188] Max Reward on eval: 0.08778601881602177[0m
[37m[1m[2023-07-03 02:08:15,734][188188] Min Reward on eval: 0.08778601881602177[0m
[37m[1m[2023-07-03 02:08:15,734][188188] Mean Reward across all agents: 0.08778601881602177[0m
[37m[1m[2023-07-03 02:08:15,734][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:08:20,815][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:08:20,820][188188] Reward + Measures: [[-85.99051732   0.10340001   0.5169       0.5291       0.50330001
    3.96881413]
 [ -7.97895169   0.1788       0.19410001   0.28029999   0.131
    3.86352992]
 [  7.47488611   0.0701       0.33239999   0.34729999   0.34419999
    3.97089577]
 ...
 [-27.2717438    0.0778       0.1533       0.2464       0.18499999
    3.97258759]
 [ -5.30651579   0.0806       0.23699999   0.27509999   0.25729999
    3.82945037]
 [ -2.13350135   0.0983       0.13410001   0.16040002   0.14420001
    3.75354695]][0m
[37m[1m[2023-07-03 02:08:20,821][188188] Max Reward on eval: 119.82470147288404[0m
[37m[1m[2023-07-03 02:08:20,821][188188] Min Reward on eval: -221.51032365951687[0m
[37m[1m[2023-07-03 02:08:20,821][188188] Mean Reward across all agents: 0.7481552074391656[0m
[37m[1m[2023-07-03 02:08:20,821][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:08:20,823][188188] mean_value=-2921.308629713423, max_value=306.0576253176503[0m
[37m[1m[2023-07-03 02:08:20,826][188188] New mean coefficients: [[ 1.9324638  -4.1140866  -0.05937213 -3.0041556   0.0165602   0.55440533]][0m
[37m[1m[2023-07-03 02:08:20,827][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:08:29,918][188188] train() took 9.09 seconds to complete[0m
[36m[2023-07-03 02:08:29,918][188188] FPS: 422463.56[0m
[36m[2023-07-03 02:08:29,920][188188] itr=171, itrs=2000, Progress: 8.55%[0m
[36m[2023-07-03 02:08:41,530][188188] train() took 11.59 seconds to complete[0m
[36m[2023-07-03 02:08:41,531][188188] FPS: 331360.64[0m
[36m[2023-07-03 02:08:45,746][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:08:45,747][188188] Reward + Measures: [[9.17579089 0.07918999 0.13833401 0.16961667 0.16984232 3.75688791]][0m
[37m[1m[2023-07-03 02:08:45,747][188188] Max Reward on eval: 9.175790886060025[0m
[37m[1m[2023-07-03 02:08:45,747][188188] Min Reward on eval: 9.175790886060025[0m
[37m[1m[2023-07-03 02:08:45,748][188188] Mean Reward across all agents: 9.175790886060025[0m
[37m[1m[2023-07-03 02:08:45,748][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:08:50,854][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:08:50,855][188188] Reward + Measures: [[  22.66229948    0.09          0.0621        0.14049999    0.09850001
     3.95586395]
 [-280.95975089    0.79070008    0.76509994    0.78750002    0.0152
     3.99150133]
 [  21.27427995    0.0969        0.2899        0.35279998    0.31500003
     3.76771045]
 ...
 [-172.38106825    0.33330002    0.5772        0.62          0.32070002
     3.97964549]
 [ -32.01304727    0.127         0.23280001    0.26089999    0.20279999
     3.95358086]
 [ -24.14875271    0.0646        0.05689999    0.0999        0.0854
     3.70088816]][0m
[37m[1m[2023-07-03 02:08:50,855][188188] Max Reward on eval: 199.47852423558942[0m
[37m[1m[2023-07-03 02:08:50,855][188188] Min Reward on eval: -390.9978318735026[0m
[37m[1m[2023-07-03 02:08:50,855][188188] Mean Reward across all agents: -14.20428882910736[0m
[37m[1m[2023-07-03 02:08:50,856][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:08:50,857][188188] mean_value=-2437.3889529251296, max_value=470.33133299082516[0m
[37m[1m[2023-07-03 02:08:50,860][188188] New mean coefficients: [[ 3.0187824  -3.5578866  -0.1414599  -4.430133    0.91132194 -1.106825  ]][0m
[37m[1m[2023-07-03 02:08:50,861][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:08:59,900][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 02:08:59,901][188188] FPS: 424879.71[0m
[36m[2023-07-03 02:08:59,903][188188] itr=172, itrs=2000, Progress: 8.60%[0m
[36m[2023-07-03 02:09:11,497][188188] train() took 11.57 seconds to complete[0m
[36m[2023-07-03 02:09:11,497][188188] FPS: 331766.46[0m
[36m[2023-07-03 02:09:15,829][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:09:15,829][188188] Reward + Measures: [[18.55593494  0.06405333  0.08830634  0.14676133  0.13567434  3.85189509]][0m
[37m[1m[2023-07-03 02:09:15,830][188188] Max Reward on eval: 18.555934944235798[0m
[37m[1m[2023-07-03 02:09:15,830][188188] Min Reward on eval: 18.555934944235798[0m
[37m[1m[2023-07-03 02:09:15,830][188188] Mean Reward across all agents: 18.555934944235798[0m
[37m[1m[2023-07-03 02:09:15,830][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:09:20,861][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:09:20,861][188188] Reward + Measures: [[  26.77979828    0.0774        0.071         0.08810001    0.0877
     3.93033528]
 [ -12.03747533    0.0615        0.27310002    0.2985        0.27949998
     3.95553446]
 [ -29.73039593    0.0671        0.12249999    0.17029999    0.1015
     3.95504236]
 ...
 [   5.93432016    0.0681        0.0686        0.08510001    0.07359999
     3.81264877]
 [ -41.2670239     0.07340001    0.1592        0.1859        0.15600002
     3.9087739 ]
 [-216.11857348    0.77270001    0.83240002    0.89090008    0.11040001
     3.9697156 ]][0m
[37m[1m[2023-07-03 02:09:20,862][188188] Max Reward on eval: 100.85897667557[0m
[37m[1m[2023-07-03 02:09:20,862][188188] Min Reward on eval: -356.9653052948415[0m
[37m[1m[2023-07-03 02:09:20,862][188188] Mean Reward across all agents: -15.386286936674463[0m
[37m[1m[2023-07-03 02:09:20,862][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:09:20,864][188188] mean_value=-2479.2173574134285, max_value=-99.93818691801948[0m
[36m[2023-07-03 02:09:20,866][188188] XNES is restarting with a new solution whose measures are [0.20449999 0.35510001 0.2746     0.23369999 2.08462715] and objective is 1741.595870957151[0m
[36m[2023-07-03 02:09:20,867][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 02:09:20,869][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 02:09:20,870][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:09:29,890][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 02:09:29,891][188188] FPS: 425778.93[0m
[36m[2023-07-03 02:09:29,893][188188] itr=173, itrs=2000, Progress: 8.65%[0m
[36m[2023-07-03 02:09:41,891][188188] train() took 11.98 seconds to complete[0m
[36m[2023-07-03 02:09:41,891][188188] FPS: 320632.24[0m
[36m[2023-07-03 02:09:46,200][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:09:46,201][188188] Reward + Measures: [[854.64936189   0.14734167   0.19681466   0.15859233   0.141151
    2.46197033]][0m
[37m[1m[2023-07-03 02:09:46,201][188188] Max Reward on eval: 854.6493618877079[0m
[37m[1m[2023-07-03 02:09:46,201][188188] Min Reward on eval: 854.6493618877079[0m
[37m[1m[2023-07-03 02:09:46,202][188188] Mean Reward across all agents: 854.6493618877079[0m
[37m[1m[2023-07-03 02:09:46,202][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:09:51,252][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:09:51,258][188188] Reward + Measures: [[799.02189929   0.15830001   0.21820001   0.16770001   0.1442
    2.50147605]
 [833.85990622   0.1578       0.20060001   0.1779       0.1548
    2.4562583 ]
 [960.88103106   0.17809999   0.24080001   0.20510001   0.17390001
    2.4821012 ]
 ...
 [756.75353623   0.15000002   0.19240001   0.14830001   0.12909999
    2.44524693]
 [701.1082306    0.1398       0.1912       0.13780001   0.13500001
    2.4560945 ]
 [787.21502778   0.1429       0.17830001   0.1452       0.1258
    2.47109103]][0m
[37m[1m[2023-07-03 02:09:51,258][188188] Max Reward on eval: 1086.1202202206944[0m
[37m[1m[2023-07-03 02:09:51,259][188188] Min Reward on eval: 197.4527678085491[0m
[37m[1m[2023-07-03 02:09:51,259][188188] Mean Reward across all agents: 590.5998098272696[0m
[37m[1m[2023-07-03 02:09:51,259][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:09:51,260][188188] mean_value=-3423.8082086187655, max_value=-2928.2877982253403[0m
[36m[2023-07-03 02:09:51,263][188188] XNES is restarting with a new solution whose measures are [0.49470001 0.49410006 0.61050004 0.0997     3.98147821] and objective is 23.18825687309727[0m
[36m[2023-07-03 02:09:51,264][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 02:09:51,266][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 02:09:51,267][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:10:00,325][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 02:10:00,326][188188] FPS: 424008.19[0m
[36m[2023-07-03 02:10:00,328][188188] itr=174, itrs=2000, Progress: 8.70%[0m
[36m[2023-07-03 02:10:12,019][188188] train() took 11.67 seconds to complete[0m
[36m[2023-07-03 02:10:12,020][188188] FPS: 329057.18[0m
[36m[2023-07-03 02:10:16,307][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:10:16,308][188188] Reward + Measures: [[-8.03346211  0.10588267  0.32990268  0.35135067  0.331025    3.88675523]][0m
[37m[1m[2023-07-03 02:10:16,308][188188] Max Reward on eval: -8.033462110521304[0m
[37m[1m[2023-07-03 02:10:16,308][188188] Min Reward on eval: -8.033462110521304[0m
[37m[1m[2023-07-03 02:10:16,308][188188] Mean Reward across all agents: -8.033462110521304[0m
[37m[1m[2023-07-03 02:10:16,309][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:10:21,289][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:10:21,290][188188] Reward + Measures: [[120.26311818   0.3048       0.24039999   0.34640002   0.1196
    3.91887093]
 [ 46.91691939   0.14469998   0.58810002   0.60400003   0.50500005
    3.98235965]
 [ 43.48016393   0.25460002   0.30350003   0.45570001   0.24889998
    3.91942215]
 ...
 [ -1.59082827   0.13520001   0.33879998   0.3714       0.32820001
    3.88660741]
 [165.83180289   0.60840005   0.4305       0.60540003   0.0478
    3.88297772]
 [194.41814815   0.52430004   0.57280004   0.75089997   0.27799997
    3.97589493]][0m
[37m[1m[2023-07-03 02:10:21,290][188188] Max Reward on eval: 535.4461384046823[0m
[37m[1m[2023-07-03 02:10:21,290][188188] Min Reward on eval: -478.2757260515238[0m
[37m[1m[2023-07-03 02:10:21,290][188188] Mean Reward across all agents: 9.649863740026733[0m
[37m[1m[2023-07-03 02:10:21,291][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:10:21,293][188188] mean_value=-1745.6506897600684, max_value=637.3692710479161[0m
[37m[1m[2023-07-03 02:10:21,296][188188] New mean coefficients: [[ 0.9146925   0.70407176 -1.3628365  -1.2707521  -2.348002    1.7111394 ]][0m
[37m[1m[2023-07-03 02:10:21,297][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:10:30,263][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 02:10:30,264][188188] FPS: 428339.40[0m
[36m[2023-07-03 02:10:30,266][188188] itr=175, itrs=2000, Progress: 8.75%[0m
[36m[2023-07-03 02:10:41,853][188188] train() took 11.57 seconds to complete[0m
[36m[2023-07-03 02:10:41,853][188188] FPS: 331921.47[0m
[36m[2023-07-03 02:10:46,203][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:10:46,204][188188] Reward + Measures: [[-18.11093646   0.078837     0.17342766   0.22178067   0.22016466
    3.93572044]][0m
[37m[1m[2023-07-03 02:10:46,204][188188] Max Reward on eval: -18.110936461195458[0m
[37m[1m[2023-07-03 02:10:46,204][188188] Min Reward on eval: -18.110936461195458[0m
[37m[1m[2023-07-03 02:10:46,204][188188] Mean Reward across all agents: -18.110936461195458[0m
[37m[1m[2023-07-03 02:10:46,205][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:10:51,365][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:10:51,371][188188] Reward + Measures: [[ 94.59292837   0.33759999   0.44549999   0.53060001   0.26610002
    3.96542215]
 [-13.54742464   0.26330003   0.0674       0.30700001   0.25170001
    3.78989005]
 [-63.28877316   0.1673       0.2105       0.3362       0.2791
    3.78778315]
 ...
 [  4.00637728   0.22880001   0.0658       0.34310001   0.2395
    3.97374415]
 [ 51.91169878   0.2314       0.23120001   0.3545       0.37879997
    3.8725698 ]
 [-36.71624816   0.10640001   0.31059998   0.34830001   0.2929
    3.93113136]][0m
[37m[1m[2023-07-03 02:10:51,371][188188] Max Reward on eval: 466.6602659069002[0m
[37m[1m[2023-07-03 02:10:51,371][188188] Min Reward on eval: -367.8986647140235[0m
[37m[1m[2023-07-03 02:10:51,371][188188] Mean Reward across all agents: 17.912706836761735[0m
[37m[1m[2023-07-03 02:10:51,372][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:10:51,374][188188] mean_value=-2834.5992170083623, max_value=436.0003705896079[0m
[37m[1m[2023-07-03 02:10:51,376][188188] New mean coefficients: [[ 2.6578789   0.58046895 -0.21256435 -2.6667762  -2.4098105   0.7613629 ]][0m
[37m[1m[2023-07-03 02:10:51,377][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:11:00,448][188188] train() took 9.07 seconds to complete[0m
[36m[2023-07-03 02:11:00,449][188188] FPS: 423399.26[0m
[36m[2023-07-03 02:11:00,451][188188] itr=176, itrs=2000, Progress: 8.80%[0m
[36m[2023-07-03 02:11:12,186][188188] train() took 11.72 seconds to complete[0m
[36m[2023-07-03 02:11:12,186][188188] FPS: 327745.45[0m
[36m[2023-07-03 02:11:16,431][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:11:16,431][188188] Reward + Measures: [[2.83864544 0.084086   0.15073466 0.19256201 0.18375102 3.80793548]][0m
[37m[1m[2023-07-03 02:11:16,432][188188] Max Reward on eval: 2.838645440813883[0m
[37m[1m[2023-07-03 02:11:16,432][188188] Min Reward on eval: 2.838645440813883[0m
[37m[1m[2023-07-03 02:11:16,432][188188] Mean Reward across all agents: 2.838645440813883[0m
[37m[1m[2023-07-03 02:11:16,432][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:11:21,455][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:11:21,456][188188] Reward + Measures: [[-24.77221281   0.07030001   0.09630001   0.06900001   0.0927
    3.79701304]
 [ 37.71678008   0.29350001   0.0566       0.33399999   0.30720001
    3.77440333]
 [  3.03501879   0.09600001   0.117        0.0902       0.0927
    3.87353897]
 ...
 [-15.86437333   0.07889999   0.0895       0.07310001   0.0813
    3.72096634]
 [-20.34732767   0.27760002   0.32249999   0.34010002   0.13280001
    3.98402405]
 [-10.81532641   0.0693       0.1701       0.2089       0.19580001
    3.66527677]][0m
[37m[1m[2023-07-03 02:11:21,456][188188] Max Reward on eval: 606.4093933317345[0m
[37m[1m[2023-07-03 02:11:21,456][188188] Min Reward on eval: -485.561931598559[0m
[37m[1m[2023-07-03 02:11:21,456][188188] Mean Reward across all agents: -6.841616297825801[0m
[37m[1m[2023-07-03 02:11:21,457][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:11:21,458][188188] mean_value=-3056.4348835972773, max_value=365.64004935405023[0m
[37m[1m[2023-07-03 02:11:21,461][188188] New mean coefficients: [[ 3.688624   -0.54660004  0.40398365 -2.1616275  -0.6636896   1.2855707 ]][0m
[37m[1m[2023-07-03 02:11:21,462][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:11:30,483][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 02:11:30,483][188188] FPS: 425760.47[0m
[36m[2023-07-03 02:11:30,485][188188] itr=177, itrs=2000, Progress: 8.85%[0m
[36m[2023-07-03 02:11:42,055][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 02:11:42,055][188188] FPS: 332497.59[0m
[36m[2023-07-03 02:11:46,438][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:11:46,444][188188] Reward + Measures: [[-39.96763512   0.07793167   0.172352     0.20804533   0.21467765
    3.83563733]][0m
[37m[1m[2023-07-03 02:11:46,444][188188] Max Reward on eval: -39.96763512470714[0m
[37m[1m[2023-07-03 02:11:46,444][188188] Min Reward on eval: -39.96763512470714[0m
[37m[1m[2023-07-03 02:11:46,445][188188] Mean Reward across all agents: -39.96763512470714[0m
[37m[1m[2023-07-03 02:11:46,445][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:11:51,570][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:11:51,576][188188] Reward + Measures: [[ -22.38811012    0.12400001    0.199         0.2402        0.22950001
     3.92279673]
 [-180.00507222    0.52929997    0.69970006    0.81519997    0.40330002
     3.93835616]
 [ -60.04373623    0.54460001    0.0909        0.55870003    0.51950002
     3.96134686]
 ...
 [ -25.9457298     0.15750001    0.15980001    0.16180001    0.16970001
     3.84122157]
 [  24.42274176    0.1164        0.2145        0.24519999    0.2132
     3.96427798]
 [-411.98051262    0.80620003    0.49720001    0.82809991    0.30770001
     3.9230125 ]][0m
[37m[1m[2023-07-03 02:11:51,576][188188] Max Reward on eval: 334.43388679805213[0m
[37m[1m[2023-07-03 02:11:51,576][188188] Min Reward on eval: -591.7890300754458[0m
[37m[1m[2023-07-03 02:11:51,577][188188] Mean Reward across all agents: -94.379059701916[0m
[37m[1m[2023-07-03 02:11:51,577][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:11:51,579][188188] mean_value=-1585.5512171144803, max_value=269.14960282397374[0m
[37m[1m[2023-07-03 02:11:51,582][188188] New mean coefficients: [[ 4.67476    -1.2858481  -0.0722926  -2.911145   -0.17644495  0.89272916]][0m
[37m[1m[2023-07-03 02:11:51,583][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:12:00,698][188188] train() took 9.11 seconds to complete[0m
[36m[2023-07-03 02:12:00,698][188188] FPS: 421367.80[0m
[36m[2023-07-03 02:12:00,700][188188] itr=178, itrs=2000, Progress: 8.90%[0m
[36m[2023-07-03 02:12:12,354][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 02:12:12,355][188188] FPS: 330106.94[0m
[36m[2023-07-03 02:12:16,639][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:12:16,640][188188] Reward + Measures: [[-3.18721347  0.10566366  0.17090534  0.20822868  0.17840366  3.86828566]][0m
[37m[1m[2023-07-03 02:12:16,640][188188] Max Reward on eval: -3.187213473356392[0m
[37m[1m[2023-07-03 02:12:16,640][188188] Min Reward on eval: -3.187213473356392[0m
[37m[1m[2023-07-03 02:12:16,641][188188] Mean Reward across all agents: -3.187213473356392[0m
[37m[1m[2023-07-03 02:12:16,641][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:12:21,697][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:12:21,698][188188] Reward + Measures: [[-129.53314479    0.06280001    0.30070004    0.2933        0.33800003
     3.86427546]
 [ -12.86229471    0.06460001    0.13350001    0.13010001    0.12230001
     3.84451938]
 [  12.85035886    0.0707        0.0638        0.0892        0.09500001
     3.68718386]
 ...
 [ -23.53956979    0.2545        0.3757        0.47929999    0.2464
     3.86808252]
 [ -35.69873032    0.0654        0.27490002    0.21560001    0.36399999
     3.89664721]
 [  94.71480245    0.0542        0.09540001    0.0999        0.0573
     3.90502787]][0m
[37m[1m[2023-07-03 02:12:21,698][188188] Max Reward on eval: 353.22942828368394[0m
[37m[1m[2023-07-03 02:12:21,699][188188] Min Reward on eval: -183.46768494080752[0m
[37m[1m[2023-07-03 02:12:21,699][188188] Mean Reward across all agents: 2.177019586433195[0m
[37m[1m[2023-07-03 02:12:21,699][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:12:21,701][188188] mean_value=-2680.198496662115, max_value=417.1987409363102[0m
[37m[1m[2023-07-03 02:12:21,703][188188] New mean coefficients: [[ 3.6205392  -2.9993396  -0.90311825 -2.8423676   0.7432354   1.4566736 ]][0m
[37m[1m[2023-07-03 02:12:21,704][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:12:30,776][188188] train() took 9.07 seconds to complete[0m
[36m[2023-07-03 02:12:30,782][188188] FPS: 423341.75[0m
[36m[2023-07-03 02:12:30,785][188188] itr=179, itrs=2000, Progress: 8.95%[0m
[36m[2023-07-03 02:12:42,349][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 02:12:42,349][188188] FPS: 332605.99[0m
[36m[2023-07-03 02:12:46,615][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:12:46,620][188188] Reward + Measures: [[-7.64103974  0.07560401  0.14020666  0.18815735  0.17722866  3.87405729]][0m
[37m[1m[2023-07-03 02:12:46,621][188188] Max Reward on eval: -7.641039739081335[0m
[37m[1m[2023-07-03 02:12:46,621][188188] Min Reward on eval: -7.641039739081335[0m
[37m[1m[2023-07-03 02:12:46,621][188188] Mean Reward across all agents: -7.641039739081335[0m
[37m[1m[2023-07-03 02:12:46,622][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:12:51,660][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:12:51,666][188188] Reward + Measures: [[  42.24129826    0.07660001    0.0794        0.11110001    0.10730001
     3.90117192]
 [ -30.18165915    0.13789999    0.13960001    0.1383        0.0819
     3.71171498]
 [ -82.45251858    0.4179        0.41420004    0.43000004    0.0345
     3.88854527]
 ...
 [-263.06155447    0.81290001    0.91449994    0.93810004    0.13950001
     3.98097777]
 [-139.79943405    0.5553        0.51950002    0.58170003    0.0717
     3.9854207 ]
 [ -67.54574633    0.35129997    0.39739999    0.36780003    0.1411
     3.95409513]][0m
[37m[1m[2023-07-03 02:12:51,666][188188] Max Reward on eval: 219.72900491096078[0m
[37m[1m[2023-07-03 02:12:51,667][188188] Min Reward on eval: -263.06155446879563[0m
[37m[1m[2023-07-03 02:12:51,667][188188] Mean Reward across all agents: 3.58551894074602[0m
[37m[1m[2023-07-03 02:12:51,667][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:12:51,669][188188] mean_value=-3327.271543312221, max_value=320.533617210138[0m
[37m[1m[2023-07-03 02:12:51,672][188188] New mean coefficients: [[ 3.7550926 -2.3984628 -3.0904508 -3.0006485  1.776952   1.3588532]][0m
[37m[1m[2023-07-03 02:12:51,673][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:13:00,734][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 02:13:00,734][188188] FPS: 423855.64[0m
[36m[2023-07-03 02:13:00,737][188188] itr=180, itrs=2000, Progress: 9.00%[0m
[37m[1m[2023-07-03 02:13:03,425][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000160[0m
[36m[2023-07-03 02:13:15,487][188188] train() took 11.73 seconds to complete[0m
[36m[2023-07-03 02:13:15,488][188188] FPS: 327432.17[0m
[36m[2023-07-03 02:13:19,718][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:13:19,719][188188] Reward + Measures: [[-31.78068772   0.07547799   0.15609333   0.20004767   0.19975302
    3.89880109]][0m
[37m[1m[2023-07-03 02:13:19,719][188188] Max Reward on eval: -31.78068772329753[0m
[37m[1m[2023-07-03 02:13:19,719][188188] Min Reward on eval: -31.78068772329753[0m
[37m[1m[2023-07-03 02:13:19,720][188188] Mean Reward across all agents: -31.78068772329753[0m
[37m[1m[2023-07-03 02:13:19,720][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:13:24,727][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:13:24,728][188188] Reward + Measures: [[-21.29713685   0.0847       0.0896       0.0857       0.07480001
    3.70882225]
 [  0.70734179   0.09980001   0.16329999   0.14850001   0.15440001
    3.70958972]
 [416.93395139   0.98949999   0.72890002   0.98890013   0.0097
    3.99291468]
 ...
 [  5.97174342   0.1375       0.1373       0.1276       0.1398
    3.89280891]
 [-23.51847557   0.14579999   0.07600001   0.1296       0.1566
    3.90722656]
 [  5.50015644   0.24029998   0.1304       0.25750002   0.23720001
    3.76812363]][0m
[37m[1m[2023-07-03 02:13:24,728][188188] Max Reward on eval: 543.3618030449376[0m
[37m[1m[2023-07-03 02:13:24,728][188188] Min Reward on eval: -387.5352326354012[0m
[37m[1m[2023-07-03 02:13:24,728][188188] Mean Reward across all agents: 26.236826031761353[0m
[37m[1m[2023-07-03 02:13:24,729][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:13:24,731][188188] mean_value=-3009.0474442235954, max_value=522.423415417284[0m
[37m[1m[2023-07-03 02:13:24,734][188188] New mean coefficients: [[ 3.898216  -3.7740164 -3.0261824 -2.945788   1.3304614  2.8752513]][0m
[37m[1m[2023-07-03 02:13:24,734][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:13:33,678][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 02:13:33,678][188188] FPS: 429437.86[0m
[36m[2023-07-03 02:13:33,681][188188] itr=181, itrs=2000, Progress: 9.05%[0m
[36m[2023-07-03 02:13:45,304][188188] train() took 11.60 seconds to complete[0m
[36m[2023-07-03 02:13:45,304][188188] FPS: 330895.24[0m
[36m[2023-07-03 02:13:49,563][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:13:49,564][188188] Reward + Measures: [[-16.99254269   0.06966167   0.11763033   0.16686635   0.15970466
    3.88426232]][0m
[37m[1m[2023-07-03 02:13:49,564][188188] Max Reward on eval: -16.99254269070578[0m
[37m[1m[2023-07-03 02:13:49,564][188188] Min Reward on eval: -16.99254269070578[0m
[37m[1m[2023-07-03 02:13:49,564][188188] Mean Reward across all agents: -16.99254269070578[0m
[37m[1m[2023-07-03 02:13:49,565][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:13:54,566][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:13:54,567][188188] Reward + Measures: [[-76.73077434   0.0596       0.0551       0.0821       0.12
    3.83240676]
 [ 31.43865969   0.43310004   0.48920003   0.4975       0.1234
    3.97432399]
 [  0.01446317   0.0593       0.0993       0.20700002   0.17470001
    3.91782761]
 ...
 [ -9.98652249   0.0557       0.0518       0.17949998   0.13860001
    3.84024739]
 [-16.17898301   0.0825       0.0727       0.0945       0.11059999
    3.84693456]
 [  1.87717591   0.0716       0.07         0.10550001   0.0961
    3.89426661]][0m
[37m[1m[2023-07-03 02:13:54,567][188188] Max Reward on eval: 62.66860628763679[0m
[37m[1m[2023-07-03 02:13:54,567][188188] Min Reward on eval: -374.48847011849284[0m
[37m[1m[2023-07-03 02:13:54,568][188188] Mean Reward across all agents: -27.01569177476212[0m
[37m[1m[2023-07-03 02:13:54,568][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:13:54,569][188188] mean_value=-3437.021422462623, max_value=261.7458114166842[0m
[37m[1m[2023-07-03 02:13:54,571][188188] New mean coefficients: [[ 3.1091907 -1.9498955 -4.8991804 -2.0361958  1.4926187  1.0395242]][0m
[37m[1m[2023-07-03 02:13:54,572][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:14:03,529][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 02:14:03,529][188188] FPS: 428829.70[0m
[36m[2023-07-03 02:14:03,531][188188] itr=182, itrs=2000, Progress: 9.10%[0m
[36m[2023-07-03 02:14:15,132][188188] train() took 11.58 seconds to complete[0m
[36m[2023-07-03 02:14:15,132][188188] FPS: 331540.04[0m
[36m[2023-07-03 02:14:19,350][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:14:19,350][188188] Reward + Measures: [[-22.41262221   0.12948501   0.134104     0.16375667   0.16199033
    3.74965453]][0m
[37m[1m[2023-07-03 02:14:19,350][188188] Max Reward on eval: -22.412622206608695[0m
[37m[1m[2023-07-03 02:14:19,351][188188] Min Reward on eval: -22.412622206608695[0m
[37m[1m[2023-07-03 02:14:19,351][188188] Mean Reward across all agents: -22.412622206608695[0m
[37m[1m[2023-07-03 02:14:19,351][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:14:24,374][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:14:24,375][188188] Reward + Measures: [[ -73.15995829    0.07130001    0.1732        0.20109999    0.19360001
     3.88186312]
 [ -41.21996947    0.078         0.094         0.13970001    0.14579999
     3.73667908]
 [ -41.38410338    0.07720001    0.0596        0.07919999    0.1188
     3.77623749]
 ...
 [  23.61676511    0.08020001    0.08880001    0.1374        0.13060002
     3.91606593]
 [-100.63267087    0.29949999    0.58459997    0.60600007    0.36410001
     3.97085428]
 [   5.08485282    0.1212        0.16160001    0.1128        0.1122
     3.85948038]][0m
[37m[1m[2023-07-03 02:14:24,375][188188] Max Reward on eval: 96.87669162340462[0m
[37m[1m[2023-07-03 02:14:24,375][188188] Min Reward on eval: -342.6730048388243[0m
[37m[1m[2023-07-03 02:14:24,375][188188] Mean Reward across all agents: -19.15871967204422[0m
[37m[1m[2023-07-03 02:14:24,376][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:14:24,377][188188] mean_value=-3343.4658820929885, max_value=169.1052362013192[0m
[37m[1m[2023-07-03 02:14:24,380][188188] New mean coefficients: [[ 2.6241853  -4.3812385  -2.807877   -2.4314096   1.1109669   0.19315183]][0m
[37m[1m[2023-07-03 02:14:24,381][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:14:33,431][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 02:14:33,431][188188] FPS: 424375.74[0m
[36m[2023-07-03 02:14:33,433][188188] itr=183, itrs=2000, Progress: 9.15%[0m
[36m[2023-07-03 02:14:45,085][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 02:14:45,086][188188] FPS: 330079.92[0m
[36m[2023-07-03 02:14:49,423][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:14:49,424][188188] Reward + Measures: [[-18.65544345   0.13102333   0.13263732   0.15940133   0.16017267
    3.76209688]][0m
[37m[1m[2023-07-03 02:14:49,424][188188] Max Reward on eval: -18.655443448184055[0m
[37m[1m[2023-07-03 02:14:49,425][188188] Min Reward on eval: -18.655443448184055[0m
[37m[1m[2023-07-03 02:14:49,425][188188] Mean Reward across all agents: -18.655443448184055[0m
[37m[1m[2023-07-03 02:14:49,425][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:14:54,486][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:14:54,487][188188] Reward + Measures: [[   5.00381979    0.23000002    0.28220001    0.3339        0.16480002
     3.9634366 ]
 [-172.35854336    0.73770005    0.70039999    0.72690004    0.0451
     3.99108815]
 [ -32.48664452    0.08130001    0.0665        0.0803        0.0963
     3.82813883]
 ...
 [ -57.86361016    0.22120002    0.2199        0.22660001    0.0703
     3.77567077]
 [-101.38265339    0.0749        0.41060001    0.43270001    0.44519997
     3.88192153]
 [  98.94842646    0.18350001    0.17709999    0.1892        0.07229999
     3.90584946]][0m
[37m[1m[2023-07-03 02:14:54,487][188188] Max Reward on eval: 193.42773859780283[0m
[37m[1m[2023-07-03 02:14:54,487][188188] Min Reward on eval: -276.42040507346394[0m
[37m[1m[2023-07-03 02:14:54,487][188188] Mean Reward across all agents: -8.13284687267364[0m
[37m[1m[2023-07-03 02:14:54,488][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:14:54,489][188188] mean_value=-3216.4482022023076, max_value=322.82276958164744[0m
[37m[1m[2023-07-03 02:14:54,492][188188] New mean coefficients: [[ 3.396605  -2.2554374 -5.3424606 -1.4038701  2.4622557  0.9505906]][0m
[37m[1m[2023-07-03 02:14:54,493][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:15:03,599][188188] train() took 9.10 seconds to complete[0m
[36m[2023-07-03 02:15:03,599][188188] FPS: 421769.73[0m
[36m[2023-07-03 02:15:03,602][188188] itr=184, itrs=2000, Progress: 9.20%[0m
[36m[2023-07-03 02:15:15,396][188188] train() took 11.77 seconds to complete[0m
[36m[2023-07-03 02:15:15,396][188188] FPS: 326147.75[0m
[36m[2023-07-03 02:15:19,772][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:15:19,772][188188] Reward + Measures: [[-18.68794704   0.13604899   0.13799566   0.16543932   0.164079
    3.75073695]][0m
[37m[1m[2023-07-03 02:15:19,772][188188] Max Reward on eval: -18.687947043638175[0m
[37m[1m[2023-07-03 02:15:19,773][188188] Min Reward on eval: -18.687947043638175[0m
[37m[1m[2023-07-03 02:15:19,773][188188] Mean Reward across all agents: -18.687947043638175[0m
[37m[1m[2023-07-03 02:15:19,773][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:15:24,940][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:15:24,941][188188] Reward + Measures: [[-90.81124568   0.0667       0.10569999   0.1176       0.1954
    3.94213724]
 [-69.91014254   0.0638       0.0683       0.0813       0.12080001
    3.88866615]
 [ 24.02044367   0.105        0.08950001   0.1049       0.085
    3.65471697]
 ...
 [-33.46082634   0.0726       0.0751       0.11720001   0.10830001
    3.60815668]
 [-39.84493613   0.0898       0.1042       0.1212       0.1362
    3.91027904]
 [ -9.13235642   0.13779999   0.1592       0.161        0.15570001
    3.9330337 ]][0m
[37m[1m[2023-07-03 02:15:24,941][188188] Max Reward on eval: 141.56430573286488[0m
[37m[1m[2023-07-03 02:15:24,941][188188] Min Reward on eval: -247.9965930458158[0m
[37m[1m[2023-07-03 02:15:24,941][188188] Mean Reward across all agents: -28.20202672784625[0m
[37m[1m[2023-07-03 02:15:24,942][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:15:24,943][188188] mean_value=-3696.4683617934134, max_value=117.3617740795918[0m
[37m[1m[2023-07-03 02:15:24,945][188188] New mean coefficients: [[ 3.1674895  -3.8455188  -3.62739    -2.6859908   1.1647285   0.02904177]][0m
[37m[1m[2023-07-03 02:15:24,946][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:15:33,986][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 02:15:33,986][188188] FPS: 424851.49[0m
[36m[2023-07-03 02:15:33,989][188188] itr=185, itrs=2000, Progress: 9.25%[0m
[36m[2023-07-03 02:15:45,548][188188] train() took 11.54 seconds to complete[0m
[36m[2023-07-03 02:15:45,549][188188] FPS: 332710.91[0m
[36m[2023-07-03 02:15:49,756][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:15:49,757][188188] Reward + Measures: [[-23.18679509   0.195214     0.20369035   0.23443367   0.22716634
    3.74754715]][0m
[37m[1m[2023-07-03 02:15:49,757][188188] Max Reward on eval: -23.186795087851923[0m
[37m[1m[2023-07-03 02:15:49,757][188188] Min Reward on eval: -23.186795087851923[0m
[37m[1m[2023-07-03 02:15:49,757][188188] Mean Reward across all agents: -23.186795087851923[0m
[37m[1m[2023-07-03 02:15:49,758][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:15:54,708][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:15:54,709][188188] Reward + Measures: [[-192.89151856    0.0512        0.39800006    0.3757        0.41599998
     3.63133216]
 [-114.96346778    0.0649        0.2624        0.29319999    0.382
     3.49469543]
 [  19.10949829    0.1867        0.2422        0.21630001    0.1894
     3.96232677]
 ...
 [ -51.28096102    0.0566        0.14170001    0.141         0.1679
     3.93395782]
 [  28.88478329    0.14940001    0.39270002    0.21170001    0.44120002
     3.74276423]
 [  19.54611388    0.1001        0.21010001    0.19770001    0.28000003
     3.81575751]][0m
[37m[1m[2023-07-03 02:15:54,709][188188] Max Reward on eval: 200.36169435493647[0m
[37m[1m[2023-07-03 02:15:54,709][188188] Min Reward on eval: -288.9076290048659[0m
[37m[1m[2023-07-03 02:15:54,709][188188] Mean Reward across all agents: -33.65920223842954[0m
[37m[1m[2023-07-03 02:15:54,709][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:15:54,711][188188] mean_value=-2278.40611294735, max_value=376.9909816828663[0m
[37m[1m[2023-07-03 02:15:54,714][188188] New mean coefficients: [[ 1.8092949 -3.5507288 -4.407378  -2.090867   2.0182712  0.4241605]][0m
[37m[1m[2023-07-03 02:15:54,715][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:16:03,661][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 02:16:03,661][188188] FPS: 429296.46[0m
[36m[2023-07-03 02:16:03,664][188188] itr=186, itrs=2000, Progress: 9.30%[0m
[36m[2023-07-03 02:16:15,178][188188] train() took 11.50 seconds to complete[0m
[36m[2023-07-03 02:16:15,179][188188] FPS: 334044.74[0m
[36m[2023-07-03 02:16:19,386][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:16:19,386][188188] Reward + Measures: [[-31.10114631   0.19071135   0.21280065   0.25663334   0.24585333
    3.72915435]][0m
[37m[1m[2023-07-03 02:16:19,387][188188] Max Reward on eval: -31.101146308307808[0m
[37m[1m[2023-07-03 02:16:19,387][188188] Min Reward on eval: -31.101146308307808[0m
[37m[1m[2023-07-03 02:16:19,387][188188] Mean Reward across all agents: -31.101146308307808[0m
[37m[1m[2023-07-03 02:16:19,387][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:16:24,348][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:16:24,348][188188] Reward + Measures: [[-67.80454772   0.26279998   0.2538       0.2494       0.0743
    3.94770479]
 [ 49.2250014    0.0774       0.09760001   0.0986       0.10519999
    3.97152257]
 [171.22871399   0.53130007   0.38470003   0.55319995   0.0505
    3.95747828]
 ...
 [ 10.0066015    0.1445       0.10589999   0.1594       0.0905
    3.45596743]
 [217.2022783    0.74630004   0.46919999   0.75440001   0.1305
    3.95679092]
 [-28.72510033   0.09940001   0.097        0.12400001   0.13060001
    3.82287192]][0m
[37m[1m[2023-07-03 02:16:24,349][188188] Max Reward on eval: 603.0941925121472[0m
[37m[1m[2023-07-03 02:16:24,349][188188] Min Reward on eval: -370.6276650336571[0m
[37m[1m[2023-07-03 02:16:24,349][188188] Mean Reward across all agents: -9.473439037580953[0m
[37m[1m[2023-07-03 02:16:24,349][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:16:24,352][188188] mean_value=-1543.884522195384, max_value=419.5053544480196[0m
[37m[1m[2023-07-03 02:16:24,355][188188] New mean coefficients: [[ 2.257155  -1.2651851 -5.920986  -2.1360717  2.795341   1.2264947]][0m
[37m[1m[2023-07-03 02:16:24,356][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:16:33,280][188188] train() took 8.92 seconds to complete[0m
[36m[2023-07-03 02:16:33,281][188188] FPS: 430340.61[0m
[36m[2023-07-03 02:16:33,283][188188] itr=187, itrs=2000, Progress: 9.35%[0m
[36m[2023-07-03 02:16:44,814][188188] train() took 11.51 seconds to complete[0m
[36m[2023-07-03 02:16:44,814][188188] FPS: 333594.77[0m
[36m[2023-07-03 02:16:49,005][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:16:49,006][188188] Reward + Measures: [[-24.00251397   0.15843433   0.17489067   0.22287634   0.21582767
    3.72392893]][0m
[37m[1m[2023-07-03 02:16:49,006][188188] Max Reward on eval: -24.002513970801985[0m
[37m[1m[2023-07-03 02:16:49,006][188188] Min Reward on eval: -24.002513970801985[0m
[37m[1m[2023-07-03 02:16:49,007][188188] Mean Reward across all agents: -24.002513970801985[0m
[37m[1m[2023-07-03 02:16:49,007][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:16:53,989][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:16:53,989][188188] Reward + Measures: [[ 241.93717024    0.5219        0.6498        0.63840002    0.1184
     3.65724635]
 [-220.25836887    0.0537        0.45340005    0.4657        0.48460004
     3.82615328]
 [   1.16850962    0.15959999    0.2218        0.22620001    0.17299999
     3.64769292]
 ...
 [  -2.87538767    0.88199997    0.88780004    0.87369996    0.87460005
     3.99233794]
 [  12.51888299    0.60869998    0.61760002    0.63160002    0.62120003
     3.9363277 ]
 [   7.40597229    0.89379996    0.89359999    0.88910002    0.87999994
     3.99215245]][0m
[37m[1m[2023-07-03 02:16:53,989][188188] Max Reward on eval: 584.9962234527804[0m
[37m[1m[2023-07-03 02:16:53,990][188188] Min Reward on eval: -220.25836886654142[0m
[37m[1m[2023-07-03 02:16:53,990][188188] Mean Reward across all agents: 31.64456073668292[0m
[37m[1m[2023-07-03 02:16:53,990][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:16:53,994][188188] mean_value=-793.8719466951369, max_value=606.2066407786468[0m
[37m[1m[2023-07-03 02:16:53,996][188188] New mean coefficients: [[ 3.4816875  -0.12386107 -7.0102825  -2.5135872   3.228471    1.2082957 ]][0m
[37m[1m[2023-07-03 02:16:53,997][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:17:02,947][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 02:17:02,947][188188] FPS: 429125.00[0m
[36m[2023-07-03 02:17:02,950][188188] itr=188, itrs=2000, Progress: 9.40%[0m
[36m[2023-07-03 02:17:14,508][188188] train() took 11.54 seconds to complete[0m
[36m[2023-07-03 02:17:14,508][188188] FPS: 332750.92[0m
[36m[2023-07-03 02:17:18,797][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:17:18,798][188188] Reward + Measures: [[-21.1297377    0.14531735   0.20225799   0.25539067   0.24785031
    3.71565557]][0m
[37m[1m[2023-07-03 02:17:18,798][188188] Max Reward on eval: -21.129737697864805[0m
[37m[1m[2023-07-03 02:17:18,798][188188] Min Reward on eval: -21.129737697864805[0m
[37m[1m[2023-07-03 02:17:18,799][188188] Mean Reward across all agents: -21.129737697864805[0m
[37m[1m[2023-07-03 02:17:18,799][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:17:23,960][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:17:23,961][188188] Reward + Measures: [[ -19.53334236    0.07910001    0.07300001    0.1851        0.1344
     3.97135425]
 [ -16.31813348    0.13869999    0.0654        0.12910001    0.1247
     3.91606569]
 [  31.04942236    0.0736        0.0528        0.1408        0.09190001
     3.96856093]
 ...
 [-106.54847907    0.2271        0.31369999    0.36539999    0.36939999
     3.75518274]
 [  30.37227645    0.25          0.60839999    0.62990004    0.41709995
     3.98879981]
 [ -31.09070786    0.09330001    0.13340001    0.1947        0.14820002
     3.97264028]][0m
[37m[1m[2023-07-03 02:17:23,961][188188] Max Reward on eval: 167.2029066621326[0m
[37m[1m[2023-07-03 02:17:23,961][188188] Min Reward on eval: -164.3172406676691[0m
[37m[1m[2023-07-03 02:17:23,961][188188] Mean Reward across all agents: -11.4623389490925[0m
[37m[1m[2023-07-03 02:17:23,962][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:17:23,963][188188] mean_value=-2301.0748292056674, max_value=247.50483276446144[0m
[37m[1m[2023-07-03 02:17:23,966][188188] New mean coefficients: [[ 2.5130274  -2.6359842  -6.8594775  -2.201       2.3200157   0.26967877]][0m
[37m[1m[2023-07-03 02:17:23,966][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:17:33,046][188188] train() took 9.08 seconds to complete[0m
[36m[2023-07-03 02:17:33,046][188188] FPS: 423004.95[0m
[36m[2023-07-03 02:17:33,048][188188] itr=189, itrs=2000, Progress: 9.45%[0m
[36m[2023-07-03 02:17:44,788][188188] train() took 11.72 seconds to complete[0m
[36m[2023-07-03 02:17:44,789][188188] FPS: 327608.78[0m
[36m[2023-07-03 02:17:49,093][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:17:49,093][188188] Reward + Measures: [[-29.55887147   0.079384     0.24721199   0.30581367   0.31147233
    3.93960738]][0m
[37m[1m[2023-07-03 02:17:49,093][188188] Max Reward on eval: -29.55887147054007[0m
[37m[1m[2023-07-03 02:17:49,094][188188] Min Reward on eval: -29.55887147054007[0m
[37m[1m[2023-07-03 02:17:49,094][188188] Mean Reward across all agents: -29.55887147054007[0m
[37m[1m[2023-07-03 02:17:49,094][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:17:54,126][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:17:54,127][188188] Reward + Measures: [[-214.90315903    0.78420001    0.72270006    0.68970007    0.19
     3.99061656]
 [-411.64790895    0.85529995    0.87720007    0.8908        0.06060001
     3.99227762]
 [-239.96837356    0.7051        0.76240003    0.80400002    0.0982
     3.98879695]
 ...
 [ -26.99398501    0.20180002    0.22220002    0.38319999    0.2577
     3.86281943]
 [ -70.68040804    0.50279999    0.66680002    0.52540004    0.38270003
     3.91678476]
 [-201.83142922    0.0442        0.46490002    0.4269        0.46529999
     3.87922597]][0m
[37m[1m[2023-07-03 02:17:54,127][188188] Max Reward on eval: 442.591286660824[0m
[37m[1m[2023-07-03 02:17:54,127][188188] Min Reward on eval: -726.3225403164513[0m
[37m[1m[2023-07-03 02:17:54,127][188188] Mean Reward across all agents: -72.43424217522747[0m
[37m[1m[2023-07-03 02:17:54,128][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:17:54,130][188188] mean_value=-1555.8836395518358, max_value=594.7665806762874[0m
[37m[1m[2023-07-03 02:17:54,133][188188] New mean coefficients: [[ 2.92506   -2.896925  -7.4112124 -1.6464918  2.940989   0.9470723]][0m
[37m[1m[2023-07-03 02:17:54,134][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:18:03,104][188188] train() took 8.97 seconds to complete[0m
[36m[2023-07-03 02:18:03,104][188188] FPS: 428193.07[0m
[36m[2023-07-03 02:18:03,106][188188] itr=190, itrs=2000, Progress: 9.50%[0m
[37m[1m[2023-07-03 02:18:05,713][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000170[0m
[36m[2023-07-03 02:18:17,588][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 02:18:17,589][188188] FPS: 332531.98[0m
[36m[2023-07-03 02:18:21,910][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:18:21,911][188188] Reward + Measures: [[-46.26486872   0.26559666   0.31153935   0.35710165   0.37714767
    3.71204185]][0m
[37m[1m[2023-07-03 02:18:21,911][188188] Max Reward on eval: -46.26486871619392[0m
[37m[1m[2023-07-03 02:18:21,911][188188] Min Reward on eval: -46.26486871619392[0m
[37m[1m[2023-07-03 02:18:21,912][188188] Mean Reward across all agents: -46.26486871619392[0m
[37m[1m[2023-07-03 02:18:21,912][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:18:26,979][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:18:26,980][188188] Reward + Measures: [[ -99.6889458     0.0463        0.46160004    0.47780004    0.50060004
     3.94174457]
 [  44.33551655    0.08100001    0.2172        0.16849999    0.22850001
     3.78997421]
 [-145.31465359    0.1935        0.57840002    0.38029999    0.59319997
     3.97694516]
 ...
 [-246.03270574    0.33080003    0.60490006    0.21960001    0.62840003
     3.97502446]
 [ -26.97254897    0.0709        0.0891        0.1158        0.1432
     3.8725884 ]
 [  32.9709045     0.42469999    0.43930003    0.4619        0.45339996
     3.7657001 ]][0m
[37m[1m[2023-07-03 02:18:26,980][188188] Max Reward on eval: 193.26359418034554[0m
[37m[1m[2023-07-03 02:18:26,980][188188] Min Reward on eval: -365.227299414156[0m
[37m[1m[2023-07-03 02:18:26,981][188188] Mean Reward across all agents: -41.48471479339747[0m
[37m[1m[2023-07-03 02:18:26,981][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:18:26,982][188188] mean_value=-2497.79918685146, max_value=-44.19863853958773[0m
[36m[2023-07-03 02:18:26,985][188188] XNES is restarting with a new solution whose measures are [0.58340001 0.2518     0.84180003 0.77100003 3.86803555] and objective is 498.0120291277766[0m
[36m[2023-07-03 02:18:26,986][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 02:18:26,988][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 02:18:26,989][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:18:36,091][188188] train() took 9.10 seconds to complete[0m
[36m[2023-07-03 02:18:36,092][188188] FPS: 421922.46[0m
[36m[2023-07-03 02:18:36,094][188188] itr=191, itrs=2000, Progress: 9.55%[0m
[36m[2023-07-03 02:18:47,768][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 02:18:47,768][188188] FPS: 329478.94[0m
[36m[2023-07-03 02:18:51,984][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:18:51,985][188188] Reward + Measures: [[-197.05018794    0.45214498    0.37705797    0.74108666    0.64681232
     3.92057037]][0m
[37m[1m[2023-07-03 02:18:51,985][188188] Max Reward on eval: -197.050187937667[0m
[37m[1m[2023-07-03 02:18:51,985][188188] Min Reward on eval: -197.050187937667[0m
[37m[1m[2023-07-03 02:18:51,986][188188] Mean Reward across all agents: -197.050187937667[0m
[37m[1m[2023-07-03 02:18:51,986][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:18:56,952][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:18:56,952][188188] Reward + Measures: [[-330.82610274    0.45979998    0.39909998    0.76550001    0.72060007
     3.82544112]
 [  39.35990498    0.1105        0.32269999    0.40580001    0.4355
     3.67637753]
 [-172.16825352    0.30660003    0.76350003    0.47910005    0.71000004
     3.8782711 ]
 ...
 [ -73.80926254    0.1565        0.52609998    0.64880002    0.60549998
     3.97011542]
 [-169.270338      0.20019999    0.70139998    0.91800004    0.89890003
     3.99940276]
 [  56.8071621     0.37980002    0.57629997    0.71869999    0.62300003
     3.85361791]][0m
[37m[1m[2023-07-03 02:18:56,952][188188] Max Reward on eval: 408.2747880679555[0m
[37m[1m[2023-07-03 02:18:56,953][188188] Min Reward on eval: -621.7957038974389[0m
[37m[1m[2023-07-03 02:18:56,953][188188] Mean Reward across all agents: -65.92321091329966[0m
[37m[1m[2023-07-03 02:18:56,953][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:18:56,956][188188] mean_value=-748.1825026203128, max_value=476.02558369474264[0m
[37m[1m[2023-07-03 02:18:56,958][188188] New mean coefficients: [[-0.01352403  0.0154767  -0.12077391 -1.4727973  -0.6839302  -1.1985579 ]][0m
[37m[1m[2023-07-03 02:18:56,959][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:19:05,914][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 02:19:05,914][188188] FPS: 428901.13[0m
[36m[2023-07-03 02:19:05,917][188188] itr=192, itrs=2000, Progress: 9.60%[0m
[36m[2023-07-03 02:19:17,597][188188] train() took 11.66 seconds to complete[0m
[36m[2023-07-03 02:19:17,597][188188] FPS: 329375.57[0m
[36m[2023-07-03 02:19:21,861][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:19:21,862][188188] Reward + Measures: [[101.46013318   0.36322767   0.43761098   0.75684494   0.62722701
    3.96065044]][0m
[37m[1m[2023-07-03 02:19:21,862][188188] Max Reward on eval: 101.46013318105724[0m
[37m[1m[2023-07-03 02:19:21,862][188188] Min Reward on eval: 101.46013318105724[0m
[37m[1m[2023-07-03 02:19:21,862][188188] Mean Reward across all agents: 101.46013318105724[0m
[37m[1m[2023-07-03 02:19:21,863][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:19:26,981][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:19:26,982][188188] Reward + Measures: [[  44.73071267    0.1772        0.59849995    0.68290007    0.63320005
     3.75732422]
 [  85.42488839    0.24870001    0.56960005    0.78820002    0.75680006
     3.85194087]
 [  72.52927519    0.0642        0.75750005    0.80509996    0.78710002
     3.97638822]
 ...
 [-498.59989261    0.98719996    0.16160001    0.97710001    0.74590003
     3.99817896]
 [  96.89098223    0.27320001    0.47170001    0.64570004    0.53120005
     3.97169232]
 [  14.70862469    0.18279999    0.49199995    0.48709998    0.39160001
     3.87326622]][0m
[37m[1m[2023-07-03 02:19:26,982][188188] Max Reward on eval: 386.0574897395447[0m
[37m[1m[2023-07-03 02:19:26,982][188188] Min Reward on eval: -659.7539874706417[0m
[37m[1m[2023-07-03 02:19:26,982][188188] Mean Reward across all agents: -91.14840577592115[0m
[37m[1m[2023-07-03 02:19:26,983][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:19:26,986][188188] mean_value=-660.6558955980507, max_value=135.31799672547908[0m
[37m[1m[2023-07-03 02:19:26,988][188188] New mean coefficients: [[-0.05958273 -1.1675748  -0.44087476 -1.8982735  -1.1314458  -0.5538651 ]][0m
[37m[1m[2023-07-03 02:19:26,989][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:19:35,978][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 02:19:35,978][188188] FPS: 427283.36[0m
[36m[2023-07-03 02:19:35,980][188188] itr=193, itrs=2000, Progress: 9.65%[0m
[36m[2023-07-03 02:19:47,544][188188] train() took 11.54 seconds to complete[0m
[36m[2023-07-03 02:19:47,545][188188] FPS: 332635.95[0m
[36m[2023-07-03 02:19:51,787][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:19:51,787][188188] Reward + Measures: [[71.29961478  0.34743798  0.32290033  0.60317898  0.41290498  3.88755012]][0m
[37m[1m[2023-07-03 02:19:51,788][188188] Max Reward on eval: 71.29961477938502[0m
[37m[1m[2023-07-03 02:19:51,788][188188] Min Reward on eval: 71.29961477938502[0m
[37m[1m[2023-07-03 02:19:51,788][188188] Mean Reward across all agents: 71.29961477938502[0m
[37m[1m[2023-07-03 02:19:51,788][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:19:56,859][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:19:56,860][188188] Reward + Measures: [[  88.9879455     0.14930001    0.4718        0.63480002    0.5165
     3.98085713]
 [-568.54792453    0.8976        0.26180002    0.90710002    0.59070003
     3.99691248]
 [-186.22375841    0.1603        0.45450002    0.54580003    0.52070004
     3.93309784]
 ...
 [  52.80841793    0.27360001    0.4206        0.69860005    0.60640001
     3.97846389]
 [-127.78129517    0.93889999    0.22490001    0.93959999    0.60070002
     3.88077402]
 [  18.49816958    0.2545        0.46539998    0.63129997    0.55140007
     3.65708709]][0m
[37m[1m[2023-07-03 02:19:56,860][188188] Max Reward on eval: 476.106705211848[0m
[37m[1m[2023-07-03 02:19:56,860][188188] Min Reward on eval: -708.3488073317334[0m
[37m[1m[2023-07-03 02:19:56,860][188188] Mean Reward across all agents: -103.36132600570014[0m
[37m[1m[2023-07-03 02:19:56,861][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:19:56,863][188188] mean_value=-636.7448822663612, max_value=120.0305810652286[0m
[37m[1m[2023-07-03 02:19:56,866][188188] New mean coefficients: [[-0.40142292 -0.975883   -1.0385132  -1.9607794  -2.043289   -0.4547925 ]][0m
[37m[1m[2023-07-03 02:19:56,867][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:20:06,029][188188] train() took 9.16 seconds to complete[0m
[36m[2023-07-03 02:20:06,029][188188] FPS: 419190.46[0m
[36m[2023-07-03 02:20:06,032][188188] itr=194, itrs=2000, Progress: 9.70%[0m
[36m[2023-07-03 02:20:17,638][188188] train() took 11.58 seconds to complete[0m
[36m[2023-07-03 02:20:17,638][188188] FPS: 331497.07[0m
[36m[2023-07-03 02:20:21,934][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:20:21,935][188188] Reward + Measures: [[3.41968849 0.41472933 0.37280998 0.77709132 0.69596303 3.98496151]][0m
[37m[1m[2023-07-03 02:20:21,935][188188] Max Reward on eval: 3.4196884896774815[0m
[37m[1m[2023-07-03 02:20:21,935][188188] Min Reward on eval: 3.4196884896774815[0m
[37m[1m[2023-07-03 02:20:21,935][188188] Mean Reward across all agents: 3.4196884896774815[0m
[37m[1m[2023-07-03 02:20:21,936][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:20:26,961][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:20:26,961][188188] Reward + Measures: [[ -42.80124847    0.43220001    0.2947        0.70279998    0.62980002
     3.95740008]
 [-393.6969639     0.76950002    0.3786        0.95839995    0.75959998
     3.99860382]
 [-663.20158769    0.88450003    0.09350001    0.9612999     0.95009995
     3.99516654]
 ...
 [  -9.30312279    0.25999999    0.59970003    0.79960001    0.70419997
     3.95703673]
 [ -18.69245301    0.0219        0.77230006    0.79470003    0.75950003
     3.99360347]
 [ -28.16964072    0.0925        0.54470009    0.63670003    0.59780008
     3.93343091]][0m
[37m[1m[2023-07-03 02:20:26,961][188188] Max Reward on eval: 212.58696962778922[0m
[37m[1m[2023-07-03 02:20:26,962][188188] Min Reward on eval: -722.9373130906606[0m
[37m[1m[2023-07-03 02:20:26,962][188188] Mean Reward across all agents: -181.7369433211356[0m
[37m[1m[2023-07-03 02:20:26,962][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:20:26,964][188188] mean_value=-718.4128066410898, max_value=-60.28101845363392[0m
[36m[2023-07-03 02:20:26,966][188188] XNES is restarting with a new solution whose measures are [0.6706     0.74290001 0.73360008 0.0713     3.79863048] and objective is 317.4038810610655[0m
[36m[2023-07-03 02:20:26,967][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 02:20:26,970][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 02:20:26,971][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:20:36,068][188188] train() took 9.10 seconds to complete[0m
[36m[2023-07-03 02:20:36,068][188188] FPS: 422155.42[0m
[36m[2023-07-03 02:20:36,071][188188] itr=195, itrs=2000, Progress: 9.75%[0m
[36m[2023-07-03 02:20:47,585][188188] train() took 11.49 seconds to complete[0m
[36m[2023-07-03 02:20:47,586][188188] FPS: 334139.14[0m
[36m[2023-07-03 02:20:51,832][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:20:51,837][188188] Reward + Measures: [[359.91965225   0.67508698   0.74706995   0.69506997   0.19867834
    3.64096475]][0m
[37m[1m[2023-07-03 02:20:51,837][188188] Max Reward on eval: 359.9196522486146[0m
[37m[1m[2023-07-03 02:20:51,837][188188] Min Reward on eval: 359.9196522486146[0m
[37m[1m[2023-07-03 02:20:51,838][188188] Mean Reward across all agents: 359.9196522486146[0m
[37m[1m[2023-07-03 02:20:51,838][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:20:56,877][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:20:56,883][188188] Reward + Measures: [[  5.13843684   0.88139993   0.88059998   0.87639999   0.87360001
    3.99715042]
 [ 75.08576257   0.32259998   0.73200005   0.75060004   0.43179998
    3.92174888]
 [ 22.50052483   0.2362       0.26550004   0.23980001   0.12520002
    3.8486836 ]
 ...
 [245.31417081   0.56230003   0.71990007   0.74559999   0.24779999
    3.91448259]
 [  1.91426662   0.95499992   0.95100003   0.9163       0.9113
    3.94988871]
 [ 12.57575065   0.5284       0.61650002   0.65639997   0.1644
    3.91728973]][0m
[37m[1m[2023-07-03 02:20:56,883][188188] Max Reward on eval: 434.57879638327285[0m
[37m[1m[2023-07-03 02:20:56,884][188188] Min Reward on eval: -473.07973758210426[0m
[37m[1m[2023-07-03 02:20:56,884][188188] Mean Reward across all agents: 38.397075872997135[0m
[37m[1m[2023-07-03 02:20:56,884][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:20:56,888][188188] mean_value=-333.38548677210514, max_value=525.0189180016592[0m
[37m[1m[2023-07-03 02:20:56,890][188188] New mean coefficients: [[ 0.44607073 -0.5164289  -1.5478157  -2.5264697  -2.5887268  -0.35347036]][0m
[37m[1m[2023-07-03 02:20:56,891][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:21:05,919][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 02:21:05,919][188188] FPS: 425433.05[0m
[36m[2023-07-03 02:21:05,922][188188] itr=196, itrs=2000, Progress: 9.80%[0m
[36m[2023-07-03 02:21:17,580][188188] train() took 11.64 seconds to complete[0m
[36m[2023-07-03 02:21:17,580][188188] FPS: 329918.91[0m
[36m[2023-07-03 02:21:21,818][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:21:21,819][188188] Reward + Measures: [[41.94582782  0.40050399  0.6192953   0.64403766  0.259453    3.90115809]][0m
[37m[1m[2023-07-03 02:21:21,819][188188] Max Reward on eval: 41.94582782066532[0m
[37m[1m[2023-07-03 02:21:21,819][188188] Min Reward on eval: 41.94582782066532[0m
[37m[1m[2023-07-03 02:21:21,820][188188] Mean Reward across all agents: 41.94582782066532[0m
[37m[1m[2023-07-03 02:21:21,820][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:21:26,853][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:21:26,854][188188] Reward + Measures: [[ -93.03401806    0.0511        0.76240009    0.78200006    0.77310002
     3.86716127]
 [-175.63666152    0.12830001    0.74570006    0.77019995    0.68529999
     3.97045898]
 [ -23.57098573    0.0806        0.34300002    0.36570001    0.35419998
     3.80081153]
 ...
 [ -84.32172669    0.07560001    0.29410002    0.3757        0.3452
     3.94317985]
 [  64.76800152    0.09680001    0.96530002    0.96880001    0.88220006
     3.93693089]
 [ -41.69908354    0.24600004    0.68479997    0.71470004    0.49489999
     3.99352002]][0m
[37m[1m[2023-07-03 02:21:26,854][188188] Max Reward on eval: 452.39371752310547[0m
[37m[1m[2023-07-03 02:21:26,855][188188] Min Reward on eval: -192.7574172543362[0m
[37m[1m[2023-07-03 02:21:26,855][188188] Mean Reward across all agents: 2.4612889051716813[0m
[37m[1m[2023-07-03 02:21:26,855][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:21:26,858][188188] mean_value=-1224.720283945289, max_value=363.3773594009548[0m
[37m[1m[2023-07-03 02:21:26,860][188188] New mean coefficients: [[-0.17606944 -0.54636186 -0.3088132  -3.1678898  -1.3091908  -0.21637353]][0m
[37m[1m[2023-07-03 02:21:26,861][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:21:35,911][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 02:21:35,911][188188] FPS: 424389.43[0m
[36m[2023-07-03 02:21:35,914][188188] itr=197, itrs=2000, Progress: 9.85%[0m
[36m[2023-07-03 02:21:47,603][188188] train() took 11.67 seconds to complete[0m
[36m[2023-07-03 02:21:47,604][188188] FPS: 329045.67[0m
[36m[2023-07-03 02:21:51,987][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:21:51,987][188188] Reward + Measures: [[75.43272201  0.47758198  0.60127634  0.63555402  0.17318633  3.88273478]][0m
[37m[1m[2023-07-03 02:21:51,987][188188] Max Reward on eval: 75.43272200640905[0m
[37m[1m[2023-07-03 02:21:51,988][188188] Min Reward on eval: 75.43272200640905[0m
[37m[1m[2023-07-03 02:21:51,988][188188] Mean Reward across all agents: 75.43272200640905[0m
[37m[1m[2023-07-03 02:21:51,988][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:21:57,206][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:21:57,212][188188] Reward + Measures: [[ 270.31422041    0.65290004    0.75169998    0.78470004    0.1129
     3.80049753]
 [  83.92076089    0.53500003    0.84640008    0.81689996    0.4021
     3.9325521 ]
 [ 188.74860226    0.85880005    0.93740004    0.88760006    0.5661
     3.65369582]
 ...
 [-140.68859756    0.72299999    0.74290001    0.97130007    0.20939998
     3.72418857]
 [ -24.78232795    0.3522        0.34059998    0.38579997    0.3827
     3.80488253]
 [ -41.3325131     0.90760005    0.93419999    0.90269995    0.87550002
     3.90817642]][0m
[37m[1m[2023-07-03 02:21:57,213][188188] Max Reward on eval: 460.263812559098[0m
[37m[1m[2023-07-03 02:21:57,213][188188] Min Reward on eval: -385.9824790928513[0m
[37m[1m[2023-07-03 02:21:57,213][188188] Mean Reward across all agents: 27.055683422447878[0m
[37m[1m[2023-07-03 02:21:57,214][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:21:57,217][188188] mean_value=-1131.7838038698073, max_value=751.9711945718154[0m
[37m[1m[2023-07-03 02:21:57,219][188188] New mean coefficients: [[-0.14582422  0.335374    0.6534217  -2.641493   -1.2818788   0.32508728]][0m
[37m[1m[2023-07-03 02:21:57,220][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:22:06,292][188188] train() took 9.07 seconds to complete[0m
[36m[2023-07-03 02:22:06,292][188188] FPS: 423363.37[0m
[36m[2023-07-03 02:22:06,295][188188] itr=198, itrs=2000, Progress: 9.90%[0m
[36m[2023-07-03 02:22:18,071][188188] train() took 11.76 seconds to complete[0m
[36m[2023-07-03 02:22:18,071][188188] FPS: 326605.27[0m
[36m[2023-07-03 02:22:22,370][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:22:22,371][188188] Reward + Measures: [[32.78595343  0.87084001  0.8745957   0.84167701  0.82311863  3.90128088]][0m
[37m[1m[2023-07-03 02:22:22,371][188188] Max Reward on eval: 32.78595342734573[0m
[37m[1m[2023-07-03 02:22:22,371][188188] Min Reward on eval: 32.78595342734573[0m
[37m[1m[2023-07-03 02:22:22,371][188188] Mean Reward across all agents: 32.78595342734573[0m
[37m[1m[2023-07-03 02:22:22,372][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:22:27,422][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:22:27,423][188188] Reward + Measures: [[-39.27985      0.8592       0.8531       0.81830007   0.80740005
    3.96000528]
 [-87.80814964   0.5758       0.58840001   0.56240004   0.57410008
    3.96975756]
 [  1.49273256   0.70539999   0.71430004   0.68739998   0.68970001
    3.98440337]
 ...
 [ 37.57578437   0.38660002   0.36600003   0.33360001   0.34200001
    3.95859909]
 [  9.08319703   0.87939996   0.85250008   0.81010002   0.79500002
    3.94600844]
 [-16.58773179   0.10770001   0.1337       0.1566       0.1611
    3.7815094 ]][0m
[37m[1m[2023-07-03 02:22:27,423][188188] Max Reward on eval: 485.02570150960236[0m
[37m[1m[2023-07-03 02:22:27,423][188188] Min Reward on eval: -186.100542307843[0m
[37m[1m[2023-07-03 02:22:27,424][188188] Mean Reward across all agents: 10.045797411339342[0m
[37m[1m[2023-07-03 02:22:27,424][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:22:27,427][188188] mean_value=-918.5777534631719, max_value=410.3202784540306[0m
[37m[1m[2023-07-03 02:22:27,429][188188] New mean coefficients: [[ 0.01522177  1.1143913   0.24859926 -1.5266093  -1.4133488  -0.04329437]][0m
[37m[1m[2023-07-03 02:22:27,430][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:22:36,421][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 02:22:36,422][188188] FPS: 427159.64[0m
[36m[2023-07-03 02:22:36,424][188188] itr=199, itrs=2000, Progress: 9.95%[0m
[36m[2023-07-03 02:22:47,962][188188] train() took 11.52 seconds to complete[0m
[36m[2023-07-03 02:22:47,962][188188] FPS: 333365.28[0m
[36m[2023-07-03 02:22:52,311][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:22:52,311][188188] Reward + Measures: [[-14.58774566   0.76239061   0.71160698   0.66253465   0.47166666
    3.81756449]][0m
[37m[1m[2023-07-03 02:22:52,312][188188] Max Reward on eval: -14.587745657207158[0m
[37m[1m[2023-07-03 02:22:52,312][188188] Min Reward on eval: -14.587745657207158[0m
[37m[1m[2023-07-03 02:22:52,312][188188] Mean Reward across all agents: -14.587745657207158[0m
[37m[1m[2023-07-03 02:22:52,312][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:22:57,326][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:22:57,327][188188] Reward + Measures: [[ -13.1521446     0.53119999    0.52870005    0.49859998    0.49830005
     3.95771861]
 [ -22.60800625    0.96139997    0.96399993    0.94889992    0.94430012
     3.97127199]
 [ -36.53311563    0.81960005    0.81830007    0.79640001    0.7974
     3.99200559]
 ...
 [  29.96803882    0.39659998    0.41339999    0.3836        0.18889999
     3.78762507]
 [  33.48232671    0.77290004    0.76809996    0.74670011    0.7511
     3.98563194]
 [-140.26134476    0.53430003    0.60360003    0.59720004    0.44380003
     3.77173924]][0m
[37m[1m[2023-07-03 02:22:57,327][188188] Max Reward on eval: 401.81883809971623[0m
[37m[1m[2023-07-03 02:22:57,327][188188] Min Reward on eval: -251.82637268675026[0m
[37m[1m[2023-07-03 02:22:57,328][188188] Mean Reward across all agents: 15.639001322109525[0m
[37m[1m[2023-07-03 02:22:57,328][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:22:57,332][188188] mean_value=-702.1141261125267, max_value=541.1159853838292[0m
[37m[1m[2023-07-03 02:22:57,334][188188] New mean coefficients: [[ 0.83861727  2.0883448   0.69922215 -1.6045288  -1.0564281   0.30700952]][0m
[37m[1m[2023-07-03 02:22:57,335][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:23:06,321][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 02:23:06,321][188188] FPS: 427428.36[0m
[36m[2023-07-03 02:23:06,323][188188] itr=200, itrs=2000, Progress: 10.00%[0m
[37m[1m[2023-07-03 02:23:08,972][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000180[0m
[36m[2023-07-03 02:23:21,077][188188] train() took 11.78 seconds to complete[0m
[36m[2023-07-03 02:23:21,077][188188] FPS: 326028.36[0m
[36m[2023-07-03 02:23:25,422][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:23:25,423][188188] Reward + Measures: [[-101.03562872    0.848764      0.8174746     0.73273158    0.66737372
     3.74510908]][0m
[37m[1m[2023-07-03 02:23:25,423][188188] Max Reward on eval: -101.03562872362028[0m
[37m[1m[2023-07-03 02:23:25,423][188188] Min Reward on eval: -101.03562872362028[0m
[37m[1m[2023-07-03 02:23:25,424][188188] Mean Reward across all agents: -101.03562872362028[0m
[37m[1m[2023-07-03 02:23:25,424][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:23:30,573][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:23:30,573][188188] Reward + Measures: [[ 78.18609193   0.4835       0.43710002   0.43520004   0.0576
    3.94484496]
 [  3.69076415   0.79329997   0.78809994   0.76259995   0.75209999
    3.98225665]
 [-72.84693912   0.89729995   0.70940006   0.69010001   0.0295
    3.85139775]
 ...
 [-44.77223098   0.70880002   0.787        0.80019999   0.117
    3.99697995]
 [-31.21921714   0.29550001   0.2942       0.31300002   0.1657
    3.81558299]
 [ 56.46686481   0.32020003   0.2911       0.40279999   0.11799999
    3.77427101]][0m
[37m[1m[2023-07-03 02:23:30,574][188188] Max Reward on eval: 661.0733070749557[0m
[37m[1m[2023-07-03 02:23:30,574][188188] Min Reward on eval: -506.9833488628268[0m
[37m[1m[2023-07-03 02:23:30,574][188188] Mean Reward across all agents: 12.528505328701407[0m
[37m[1m[2023-07-03 02:23:30,574][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:23:30,577][188188] mean_value=-961.1745601291493, max_value=718.1569451896473[0m
[37m[1m[2023-07-03 02:23:30,580][188188] New mean coefficients: [[ 0.6867427   1.2134866  -0.27214462 -0.5180137  -1.1578734   1.1465354 ]][0m
[37m[1m[2023-07-03 02:23:30,580][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:23:39,523][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 02:23:39,523][188188] FPS: 429507.67[0m
[36m[2023-07-03 02:23:39,525][188188] itr=201, itrs=2000, Progress: 10.05%[0m
[36m[2023-07-03 02:23:51,047][188188] train() took 11.50 seconds to complete[0m
[36m[2023-07-03 02:23:51,047][188188] FPS: 333871.34[0m
[36m[2023-07-03 02:23:55,310][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:23:55,310][188188] Reward + Measures: [[37.38406084  0.24611434  0.39090568  0.39103332  0.22844467  3.88203454]][0m
[37m[1m[2023-07-03 02:23:55,311][188188] Max Reward on eval: 37.38406083991948[0m
[37m[1m[2023-07-03 02:23:55,311][188188] Min Reward on eval: 37.38406083991948[0m
[37m[1m[2023-07-03 02:23:55,311][188188] Mean Reward across all agents: 37.38406083991948[0m
[37m[1m[2023-07-03 02:23:55,311][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:24:00,296][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:24:00,297][188188] Reward + Measures: [[-135.75166441    0.76090002    0.72589999    0.79870003    0.0677
     3.95042992]
 [ 177.18475534    0.79139996    0.97860003    0.98449993    0.1981
     3.84242678]
 [   2.97615818    0.85990012    0.95979995    0.96609992    0.11679999
     3.99640536]
 ...
 [-259.62592805    0.88980001    0.67070001    0.98299998    0.39039999
     3.99986506]
 [  13.82770753    0.75209999    0.74550003    0.74130005    0.73839998
     3.97900319]
 [ -34.20871366    0.18890001    0.26009998    0.2832        0.1902
     3.8889389 ]][0m
[37m[1m[2023-07-03 02:24:00,297][188188] Max Reward on eval: 501.841803563945[0m
[37m[1m[2023-07-03 02:24:00,297][188188] Min Reward on eval: -506.0523652939126[0m
[37m[1m[2023-07-03 02:24:00,298][188188] Mean Reward across all agents: -36.20655419741237[0m
[37m[1m[2023-07-03 02:24:00,298][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:24:00,301][188188] mean_value=-691.328810184924, max_value=281.34944982737414[0m
[37m[1m[2023-07-03 02:24:00,303][188188] New mean coefficients: [[ 0.43814188  0.5558759  -0.62831074 -0.0236243  -0.74272114  0.46065658]][0m
[37m[1m[2023-07-03 02:24:00,304][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:24:09,326][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 02:24:09,327][188188] FPS: 425692.80[0m
[36m[2023-07-03 02:24:09,329][188188] itr=202, itrs=2000, Progress: 10.10%[0m
[36m[2023-07-03 02:24:20,973][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 02:24:20,973][188188] FPS: 330321.15[0m
[36m[2023-07-03 02:24:25,241][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:24:25,242][188188] Reward + Measures: [[-93.2361406    0.73315662   0.74610364   0.74962175   0.12205966
    3.93047023]][0m
[37m[1m[2023-07-03 02:24:25,242][188188] Max Reward on eval: -93.23614060294945[0m
[37m[1m[2023-07-03 02:24:25,242][188188] Min Reward on eval: -93.23614060294945[0m
[37m[1m[2023-07-03 02:24:25,242][188188] Mean Reward across all agents: -93.23614060294945[0m
[37m[1m[2023-07-03 02:24:25,243][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:24:30,225][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:24:30,225][188188] Reward + Measures: [[  95.15360416    0.76230001    0.80269998    0.82129997    0.0414
     3.7498517 ]
 [-102.08774193    0.38460001    0.37580001    0.3811        0.0573
     3.71674728]
 [  45.41536618    0.77770001    0.97059995    0.97489995    0.19939999
     3.99436998]
 ...
 [  11.02156802    0.55849999    0.67140001    0.65670007    0.24360001
     3.70308924]
 [ 200.69575       0.45830002    0.73250002    0.75599998    0.2983
     3.92632556]
 [  14.12040805    0.77349997    0.86329997    0.87200004    0.1068
     3.99212241]][0m
[37m[1m[2023-07-03 02:24:30,225][188188] Max Reward on eval: 391.18810037295333[0m
[37m[1m[2023-07-03 02:24:30,226][188188] Min Reward on eval: -357.96199707948836[0m
[37m[1m[2023-07-03 02:24:30,226][188188] Mean Reward across all agents: 19.573622205861263[0m
[37m[1m[2023-07-03 02:24:30,226][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:24:30,229][188188] mean_value=-690.7053363387137, max_value=527.6620994454368[0m
[37m[1m[2023-07-03 02:24:30,231][188188] New mean coefficients: [[ 0.8864094   0.02791488 -0.75893515 -0.4076705  -0.6737799   0.746892  ]][0m
[37m[1m[2023-07-03 02:24:30,232][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:24:39,192][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 02:24:39,193][188188] FPS: 428649.26[0m
[36m[2023-07-03 02:24:39,195][188188] itr=203, itrs=2000, Progress: 10.15%[0m
[36m[2023-07-03 02:24:50,876][188188] train() took 11.66 seconds to complete[0m
[36m[2023-07-03 02:24:50,877][188188] FPS: 329348.80[0m
[36m[2023-07-03 02:24:55,175][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:24:55,176][188188] Reward + Measures: [[-42.91957261   0.88825667   0.93425268   0.9439497    0.08594666
    3.97653341]][0m
[37m[1m[2023-07-03 02:24:55,176][188188] Max Reward on eval: -42.91957260695363[0m
[37m[1m[2023-07-03 02:24:55,176][188188] Min Reward on eval: -42.91957260695363[0m
[37m[1m[2023-07-03 02:24:55,176][188188] Mean Reward across all agents: -42.91957260695363[0m
[37m[1m[2023-07-03 02:24:55,177][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:25:00,202][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:25:00,202][188188] Reward + Measures: [[ -74.54641054    0.97690004    0.88700002    0.91230005    0.0011
     3.96185851]
 [ 397.6155793     0.56980002    0.74920005    0.78070003    0.09760001
     3.94764256]
 [ -41.96994578    0.9896        0.95419997    0.9795        0.0032
     3.99974298]
 ...
 [  -1.09147162    0.94369996    0.94320005    0.9375        0.0038
     3.99932027]
 [ 237.0745883     0.49590001    0.6892001     0.69830006    0.17020001
     3.88632751]
 [-153.52060228    0.90650004    0.87639993    0.88269997    0.0138
     3.99919391]][0m
[37m[1m[2023-07-03 02:25:00,202][188188] Max Reward on eval: 473.3105306659825[0m
[37m[1m[2023-07-03 02:25:00,203][188188] Min Reward on eval: -444.0055711934343[0m
[37m[1m[2023-07-03 02:25:00,203][188188] Mean Reward across all agents: 6.5943171939490695[0m
[37m[1m[2023-07-03 02:25:00,203][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:25:00,206][188188] mean_value=-445.4661742085589, max_value=426.51389830961966[0m
[37m[1m[2023-07-03 02:25:00,208][188188] New mean coefficients: [[ 1.039742   -0.50666124 -1.0887728  -0.72013706 -1.3594192   0.05090398]][0m
[37m[1m[2023-07-03 02:25:00,209][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:25:09,165][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 02:25:09,166][188188] FPS: 428831.24[0m
[36m[2023-07-03 02:25:09,168][188188] itr=204, itrs=2000, Progress: 10.20%[0m
[36m[2023-07-03 02:25:21,044][188188] train() took 11.86 seconds to complete[0m
[36m[2023-07-03 02:25:21,044][188188] FPS: 323850.57[0m
[36m[2023-07-03 02:25:25,320][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:25:25,321][188188] Reward + Measures: [[-84.28324246   0.80369133   0.71694398   0.72416794   0.08012533
    3.96381497]][0m
[37m[1m[2023-07-03 02:25:25,321][188188] Max Reward on eval: -84.28324246006399[0m
[37m[1m[2023-07-03 02:25:25,321][188188] Min Reward on eval: -84.28324246006399[0m
[37m[1m[2023-07-03 02:25:25,322][188188] Mean Reward across all agents: -84.28324246006399[0m
[37m[1m[2023-07-03 02:25:25,322][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:25:30,330][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:25:30,331][188188] Reward + Measures: [[  71.02322818    0.75569993    0.86439991    0.85589999    0.19859998
     3.77574992]
 [-147.56376759    0.6494        0.6146        0.67169994    0.0299
     3.98509669]
 [ 259.74286721    0.46759996    0.73740005    0.77150005    0.17900001
     3.95825434]
 ...
 [  94.0592291     0.30629998    0.30609998    0.2814        0.11080001
     3.8030858 ]
 [  -2.10636795    0.49709997    0.67259997    0.64800006    0.2687
     3.78759766]
 [ -55.35505092    0.6365        0.80919999    0.74180001    0.1758
     3.99299693]][0m
[37m[1m[2023-07-03 02:25:30,331][188188] Max Reward on eval: 373.4651660947129[0m
[37m[1m[2023-07-03 02:25:30,331][188188] Min Reward on eval: -260.8610863959417[0m
[37m[1m[2023-07-03 02:25:30,331][188188] Mean Reward across all agents: 9.23264548344562[0m
[37m[1m[2023-07-03 02:25:30,331][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:25:30,334][188188] mean_value=-456.57086632116795, max_value=249.9763069473749[0m
[37m[1m[2023-07-03 02:25:30,336][188188] New mean coefficients: [[ 1.476509   -0.6150363  -1.131549   -1.3721275  -1.7427096   0.02505228]][0m
[37m[1m[2023-07-03 02:25:30,337][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:25:39,348][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 02:25:39,349][188188] FPS: 426215.08[0m
[36m[2023-07-03 02:25:39,351][188188] itr=205, itrs=2000, Progress: 10.25%[0m
[36m[2023-07-03 02:25:51,045][188188] train() took 11.67 seconds to complete[0m
[36m[2023-07-03 02:25:51,045][188188] FPS: 328910.25[0m
[36m[2023-07-03 02:25:55,343][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:25:55,344][188188] Reward + Measures: [[-101.69620824    0.90413296    0.75278497    0.76682794    0.026064
     3.96596193]][0m
[37m[1m[2023-07-03 02:25:55,344][188188] Max Reward on eval: -101.69620824034575[0m
[37m[1m[2023-07-03 02:25:55,344][188188] Min Reward on eval: -101.69620824034575[0m
[37m[1m[2023-07-03 02:25:55,345][188188] Mean Reward across all agents: -101.69620824034575[0m
[37m[1m[2023-07-03 02:25:55,345][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:26:00,470][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:26:00,475][188188] Reward + Measures: [[ -25.28371384    0.98310006    0.97329998    0.97840005    0.0016
     3.9946506 ]
 [ -17.36441567    0.72600001    0.51610005    0.49669996    0.024
     3.90344095]
 [-108.42945902    0.67420006    0.63510001    0.5851        0.1409
     3.72170496]
 ...
 [-128.33024413    0.875         0.88999999    0.92950004    0.0995
     3.9748342 ]
 [ -49.40118838    0.84250003    0.62770003    0.59960002    0.0457
     3.95479059]
 [  13.32260023    0.986         0.95860004    0.97920001    0.0024
     3.99729085]][0m
[37m[1m[2023-07-03 02:26:00,476][188188] Max Reward on eval: 388.66945647250395[0m
[37m[1m[2023-07-03 02:26:00,476][188188] Min Reward on eval: -326.0069124636706[0m
[37m[1m[2023-07-03 02:26:00,476][188188] Mean Reward across all agents: -28.02000283973756[0m
[37m[1m[2023-07-03 02:26:00,477][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:26:00,479][188188] mean_value=-469.89550454665414, max_value=130.54784179118008[0m
[37m[1m[2023-07-03 02:26:00,481][188188] New mean coefficients: [[ 1.5325373  -0.65017295 -1.3733516  -0.6279269  -2.3027902  -1.4591985 ]][0m
[37m[1m[2023-07-03 02:26:00,482][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:26:09,481][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 02:26:09,482][188188] FPS: 426790.04[0m
[36m[2023-07-03 02:26:09,484][188188] itr=206, itrs=2000, Progress: 10.30%[0m
[36m[2023-07-03 02:26:21,134][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 02:26:21,134][188188] FPS: 330179.73[0m
[36m[2023-07-03 02:26:25,464][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:26:25,465][188188] Reward + Measures: [[-41.95481755   0.54390466   0.50826734   0.48222867   0.110893
    3.86640716]][0m
[37m[1m[2023-07-03 02:26:25,465][188188] Max Reward on eval: -41.95481755166556[0m
[37m[1m[2023-07-03 02:26:25,465][188188] Min Reward on eval: -41.95481755166556[0m
[37m[1m[2023-07-03 02:26:25,466][188188] Mean Reward across all agents: -41.95481755166556[0m
[37m[1m[2023-07-03 02:26:25,466][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:26:30,459][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:26:30,459][188188] Reward + Measures: [[ -70.14603375    0.58450001    0.79800004    0.78490001    0.30239999
     3.71305084]
 [ -34.16684138    0.98190004    0.9479        0.96680003    0.0012
     3.98142934]
 [-121.78924561    0.96200001    0.83430004    0.87160009    0.0018
     3.96534085]
 ...
 [  70.58807927    0.1684        0.44369999    0.48769999    0.38729998
     3.98127055]
 [ -77.16793646    0.93889999    0.78049999    0.79619998    0.0018
     3.97390985]
 [-108.48054373    0.86440003    0.85960007    0.87950003    0.10099999
     3.97680211]][0m
[37m[1m[2023-07-03 02:26:30,459][188188] Max Reward on eval: 438.66093444861474[0m
[37m[1m[2023-07-03 02:26:30,460][188188] Min Reward on eval: -391.03298796851885[0m
[37m[1m[2023-07-03 02:26:30,460][188188] Mean Reward across all agents: -15.78151784143652[0m
[37m[1m[2023-07-03 02:26:30,460][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:26:30,462][188188] mean_value=-461.21233551199424, max_value=176.6297263266542[0m
[37m[1m[2023-07-03 02:26:30,465][188188] New mean coefficients: [[ 1.6612015   0.80703545 -0.5688325  -0.5736523  -1.9361075  -0.5064163 ]][0m
[37m[1m[2023-07-03 02:26:30,466][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:26:39,516][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 02:26:39,517][188188] FPS: 424350.51[0m
[36m[2023-07-03 02:26:39,519][188188] itr=207, itrs=2000, Progress: 10.35%[0m
[36m[2023-07-03 02:26:51,167][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 02:26:51,167][188188] FPS: 330220.59[0m
[36m[2023-07-03 02:26:55,494][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:26:55,495][188188] Reward + Measures: [[-25.46668493   0.67791665   0.52573234   0.49561599   0.043506
    3.80079412]][0m
[37m[1m[2023-07-03 02:26:55,495][188188] Max Reward on eval: -25.466684931146403[0m
[37m[1m[2023-07-03 02:26:55,495][188188] Min Reward on eval: -25.466684931146403[0m
[37m[1m[2023-07-03 02:26:55,496][188188] Mean Reward across all agents: -25.466684931146403[0m
[37m[1m[2023-07-03 02:26:55,496][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:27:00,575][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:27:00,576][188188] Reward + Measures: [[-468.1909089     0.90109998    0.0453        0.86770004    0.79310006
     3.94003153]
 [ -15.25527096    0.88359994    0.96540004    0.96859998    0.1001
     3.97783518]
 [ -30.70250801    0.98710006    0.95669997    0.98229998    0.0024
     3.9998517 ]
 ...
 [  -4.03246581    0.94440001    0.6911        0.65849996    0.0036
     3.68539357]
 [  -8.22442446    0.88190001    0.96029997    0.9795        0.1006
     3.99560475]
 [ -97.08505177    0.98670006    0.93920004    0.93899995    0.0031
     3.78041649]][0m
[37m[1m[2023-07-03 02:27:00,576][188188] Max Reward on eval: 432.97047042560297[0m
[37m[1m[2023-07-03 02:27:00,577][188188] Min Reward on eval: -468.19090890102086[0m
[37m[1m[2023-07-03 02:27:00,577][188188] Mean Reward across all agents: 29.551237835173144[0m
[37m[1m[2023-07-03 02:27:00,577][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:27:00,579][188188] mean_value=-495.961241664349, max_value=260.46243352983134[0m
[37m[1m[2023-07-03 02:27:00,581][188188] New mean coefficients: [[ 1.1786057   0.8290461  -1.6225458   0.42455357 -1.4423321   0.95557535]][0m
[37m[1m[2023-07-03 02:27:00,582][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:27:09,699][188188] train() took 9.12 seconds to complete[0m
[36m[2023-07-03 02:27:09,699][188188] FPS: 421272.45[0m
[36m[2023-07-03 02:27:09,702][188188] itr=208, itrs=2000, Progress: 10.40%[0m
[36m[2023-07-03 02:27:21,405][188188] train() took 11.68 seconds to complete[0m
[36m[2023-07-03 02:27:21,405][188188] FPS: 328688.60[0m
[36m[2023-07-03 02:27:25,691][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:27:25,691][188188] Reward + Measures: [[-36.2064068    0.89119464   0.63843429   0.59193528   0.019655
    3.74773502]][0m
[37m[1m[2023-07-03 02:27:25,692][188188] Max Reward on eval: -36.206406799071296[0m
[37m[1m[2023-07-03 02:27:25,692][188188] Min Reward on eval: -36.206406799071296[0m
[37m[1m[2023-07-03 02:27:25,692][188188] Mean Reward across all agents: -36.206406799071296[0m
[37m[1m[2023-07-03 02:27:25,692][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:27:30,716][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:27:30,717][188188] Reward + Measures: [[-24.46748421   0.96000004   0.76019996   0.73750001   0.0033
    3.73930144]
 [-89.35929059   0.9379999    0.75270003   0.77100003   0.0018
    3.95443511]
 [ -4.73898386   0.88560003   0.61040002   0.54269999   0.0052
    3.87151599]
 ...
 [ 17.41462715   0.86370003   0.95030004   0.96110004   0.12539999
    3.9987781 ]
 [ -9.98883176   0.8811       0.65410006   0.60780001   0.0253
    3.79551888]
 [-31.58479037   0.98689997   0.9684       0.98070002   0.0032
    3.99450755]][0m
[37m[1m[2023-07-03 02:27:30,717][188188] Max Reward on eval: 401.7661552630685[0m
[37m[1m[2023-07-03 02:27:30,717][188188] Min Reward on eval: -185.46675514322706[0m
[37m[1m[2023-07-03 02:27:30,718][188188] Mean Reward across all agents: -16.56804960087192[0m
[37m[1m[2023-07-03 02:27:30,718][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:27:30,720][188188] mean_value=-490.2499928864472, max_value=208.66174448558468[0m
[37m[1m[2023-07-03 02:27:30,722][188188] New mean coefficients: [[ 0.964936    0.8407352  -1.184485    0.20002995 -0.5629149   0.8102833 ]][0m
[37m[1m[2023-07-03 02:27:30,723][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:27:39,834][188188] train() took 9.11 seconds to complete[0m
[36m[2023-07-03 02:27:39,834][188188] FPS: 421567.87[0m
[36m[2023-07-03 02:27:39,836][188188] itr=209, itrs=2000, Progress: 10.45%[0m
[36m[2023-07-03 02:27:51,624][188188] train() took 11.77 seconds to complete[0m
[36m[2023-07-03 02:27:51,624][188188] FPS: 326290.50[0m
[36m[2023-07-03 02:27:55,947][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:27:55,947][188188] Reward + Measures: [[-34.68714055   0.84826136   0.61744702   0.59252769   0.023961
    3.80351591]][0m
[37m[1m[2023-07-03 02:27:55,948][188188] Max Reward on eval: -34.687140548329744[0m
[37m[1m[2023-07-03 02:27:55,948][188188] Min Reward on eval: -34.687140548329744[0m
[37m[1m[2023-07-03 02:27:55,948][188188] Mean Reward across all agents: -34.687140548329744[0m
[37m[1m[2023-07-03 02:27:55,948][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:28:01,090][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:28:01,090][188188] Reward + Measures: [[366.9526901    0.588        0.64089996   0.62690002   0.0354
    3.81973767]
 [ 56.20273283   0.87670004   0.85200006   0.84429997   0.0982
    3.74690938]
 [ 31.05339409   0.18269999   0.3177       0.2938       0.2007
    3.92796302]
 ...
 [-18.92880587   0.72149998   0.79730004   0.80830002   0.1353
    3.99647117]
 [-83.6387089    0.8858       0.97049999   0.97699994   0.0993
    3.98636985]
 [-90.10666847   0.92950004   0.69280005   0.7033       0.0035
    3.95532393]][0m
[37m[1m[2023-07-03 02:28:01,090][188188] Max Reward on eval: 400.6897373098647[0m
[37m[1m[2023-07-03 02:28:01,091][188188] Min Reward on eval: -239.28744219364597[0m
[37m[1m[2023-07-03 02:28:01,091][188188] Mean Reward across all agents: -13.092003829790041[0m
[37m[1m[2023-07-03 02:28:01,091][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:28:01,093][188188] mean_value=-513.2332564212185, max_value=107.13632316244701[0m
[37m[1m[2023-07-03 02:28:01,096][188188] New mean coefficients: [[ 0.8370482   0.06091136 -0.13363719  1.3484787   0.07959825  1.3169138 ]][0m
[37m[1m[2023-07-03 02:28:01,097][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:28:10,243][188188] train() took 9.14 seconds to complete[0m
[36m[2023-07-03 02:28:10,243][188188] FPS: 419921.80[0m
[36m[2023-07-03 02:28:10,246][188188] itr=210, itrs=2000, Progress: 10.50%[0m
[37m[1m[2023-07-03 02:28:12,963][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000190[0m
[36m[2023-07-03 02:28:25,126][188188] train() took 11.82 seconds to complete[0m
[36m[2023-07-03 02:28:25,126][188188] FPS: 324789.56[0m
[36m[2023-07-03 02:28:29,431][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:28:29,431][188188] Reward + Measures: [[-56.4486442    0.94097328   0.922171     0.92697769   0.03574466
    3.74383593]][0m
[37m[1m[2023-07-03 02:28:29,432][188188] Max Reward on eval: -56.44864420084331[0m
[37m[1m[2023-07-03 02:28:29,432][188188] Min Reward on eval: -56.44864420084331[0m
[37m[1m[2023-07-03 02:28:29,432][188188] Mean Reward across all agents: -56.44864420084331[0m
[37m[1m[2023-07-03 02:28:29,432][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:28:34,479][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:28:34,479][188188] Reward + Measures: [[   3.77422109    0.69940001    0.7847001     0.86350006    0.12979999
     3.9996326 ]
 [ -33.61506519    0.98999995    0.95950001    0.98030007    0.0032
     3.99999404]
 [-148.94808613    0.99049997    0.96070004    0.98190004    0.0025
     3.99995422]
 ...
 [ -10.71602939    0.60750002    0.50319999    0.53200001    0.0256
     3.64924693]
 [   3.1501465     0.78749996    0.96600002    0.97180003    0.20180002
     3.99956942]
 [ -52.67614529    0.91899997    0.92989999    0.94309998    0.0422
     3.928931  ]][0m
[37m[1m[2023-07-03 02:28:34,479][188188] Max Reward on eval: 349.92720798184166[0m
[37m[1m[2023-07-03 02:28:34,480][188188] Min Reward on eval: -361.7440900815651[0m
[37m[1m[2023-07-03 02:28:34,480][188188] Mean Reward across all agents: -42.30418122059585[0m
[37m[1m[2023-07-03 02:28:34,480][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:28:34,482][188188] mean_value=-474.941792243943, max_value=246.63596117076622[0m
[37m[1m[2023-07-03 02:28:34,484][188188] New mean coefficients: [[ 0.6435483  -0.03184346  0.12231737  0.6793387  -0.44357115  1.0087833 ]][0m
[37m[1m[2023-07-03 02:28:34,485][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:28:43,567][188188] train() took 9.08 seconds to complete[0m
[36m[2023-07-03 02:28:43,567][188188] FPS: 422900.14[0m
[36m[2023-07-03 02:28:43,570][188188] itr=211, itrs=2000, Progress: 10.55%[0m
[36m[2023-07-03 02:28:55,496][188188] train() took 11.90 seconds to complete[0m
[36m[2023-07-03 02:28:55,496][188188] FPS: 322584.69[0m
[36m[2023-07-03 02:28:59,753][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:28:59,753][188188] Reward + Measures: [[-36.29025078   0.90330428   0.94251364   0.961285     0.07055733
    3.98198986]][0m
[37m[1m[2023-07-03 02:28:59,753][188188] Max Reward on eval: -36.29025078370113[0m
[37m[1m[2023-07-03 02:28:59,754][188188] Min Reward on eval: -36.29025078370113[0m
[37m[1m[2023-07-03 02:28:59,754][188188] Mean Reward across all agents: -36.29025078370113[0m
[37m[1m[2023-07-03 02:28:59,754][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:29:04,738][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:29:04,739][188188] Reward + Measures: [[-59.90663867   0.79209995   0.86830008   0.91940004   0.1138
    3.99971008]
 [-12.16577625   0.98540002   0.96960014   0.9806       0.0021
    3.99847484]
 [-26.6856398    0.6904       0.9637       0.98330003   0.2976
    3.99928355]
 ...
 [-16.62416292   0.88840002   0.96889991   0.98380005   0.10089999
    3.99972606]
 [-43.96638364   0.98710006   0.92210007   0.92880005   0.0028
    3.91963506]
 [ 42.15444106   0.8441       0.60519999   0.58939999   0.0104
    3.77187133]][0m
[37m[1m[2023-07-03 02:29:04,739][188188] Max Reward on eval: 430.3316278594313[0m
[37m[1m[2023-07-03 02:29:04,739][188188] Min Reward on eval: -409.6956539017148[0m
[37m[1m[2023-07-03 02:29:04,739][188188] Mean Reward across all agents: -18.5032985064298[0m
[37m[1m[2023-07-03 02:29:04,740][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:29:04,742][188188] mean_value=-376.8249032768714, max_value=155.93580433380637[0m
[37m[1m[2023-07-03 02:29:04,744][188188] New mean coefficients: [[ 1.1344668   0.6348289  -0.7216      1.2079415  -0.98903644  0.99288785]][0m
[37m[1m[2023-07-03 02:29:04,745][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:29:13,861][188188] train() took 9.11 seconds to complete[0m
[36m[2023-07-03 02:29:13,862][188188] FPS: 421285.81[0m
[36m[2023-07-03 02:29:13,864][188188] itr=212, itrs=2000, Progress: 10.60%[0m
[36m[2023-07-03 02:29:25,510][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 02:29:25,510][188188] FPS: 330278.26[0m
[36m[2023-07-03 02:29:29,801][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:29:29,807][188188] Reward + Measures: [[-26.20264218   0.85488898   0.92711997   0.946257     0.106375
    3.99885082]][0m
[37m[1m[2023-07-03 02:29:29,807][188188] Max Reward on eval: -26.202642176204012[0m
[37m[1m[2023-07-03 02:29:29,808][188188] Min Reward on eval: -26.202642176204012[0m
[37m[1m[2023-07-03 02:29:29,808][188188] Mean Reward across all agents: -26.202642176204012[0m
[37m[1m[2023-07-03 02:29:29,808][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:29:34,903][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:29:34,908][188188] Reward + Measures: [[ -44.52908374    0.89790004    0.90140003    0.88260001    0.0617
     3.98789597]
 [-258.48868311    0.72930002    0.78850001    0.80410004    0.1376
     3.99887586]
 [  25.04285661    0.7604        0.85129994    0.86689997    0.14600001
     3.99766326]
 ...
 [ -33.99167434    0.58000004    0.95319998    0.958         0.39839998
     3.99825478]
 [-272.1547368     0.98890001    0.98379993    0.98430008    0.0018
     3.99973845]
 [-158.8703261     0.98439997    0.97130007    0.97920001    0.0022
     3.99995661]][0m
[37m[1m[2023-07-03 02:29:34,909][188188] Max Reward on eval: 362.08113293452186[0m
[37m[1m[2023-07-03 02:29:34,909][188188] Min Reward on eval: -363.83530270941554[0m
[37m[1m[2023-07-03 02:29:34,909][188188] Mean Reward across all agents: -62.42857167039549[0m
[37m[1m[2023-07-03 02:29:34,909][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:29:34,911][188188] mean_value=-402.6447195425874, max_value=90.37693949763522[0m
[37m[1m[2023-07-03 02:29:34,914][188188] New mean coefficients: [[ 0.03856504  2.229436    0.5876526   0.32304835 -1.238338    0.7178561 ]][0m
[37m[1m[2023-07-03 02:29:34,915][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:29:44,052][188188] train() took 9.14 seconds to complete[0m
[36m[2023-07-03 02:29:44,052][188188] FPS: 420330.58[0m
[36m[2023-07-03 02:29:44,055][188188] itr=213, itrs=2000, Progress: 10.65%[0m
[36m[2023-07-03 02:29:55,766][188188] train() took 11.69 seconds to complete[0m
[36m[2023-07-03 02:29:55,766][188188] FPS: 328525.10[0m
[36m[2023-07-03 02:30:00,081][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:30:00,087][188188] Reward + Measures: [[-25.03352277   0.85892558   0.94061798   0.96100032   0.11026633
    3.99976826]][0m
[37m[1m[2023-07-03 02:30:00,087][188188] Max Reward on eval: -25.033522773718516[0m
[37m[1m[2023-07-03 02:30:00,087][188188] Min Reward on eval: -25.033522773718516[0m
[37m[1m[2023-07-03 02:30:00,088][188188] Mean Reward across all agents: -25.033522773718516[0m
[37m[1m[2023-07-03 02:30:00,088][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:30:05,295][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:30:05,302][188188] Reward + Measures: [[-19.36520268   0.97889996   0.96509999   0.97589999   0.0082
    3.99933028]
 [-52.37221373   0.8344       0.90650004   0.9181       0.1085
    3.99179316]
 [ 87.79209533   0.53280002   0.88949996   0.8002001    0.41440001
    3.98016977]
 ...
 [ 57.44782637   0.43590003   0.85950005   0.86470002   0.40640002
    3.92271852]
 [299.9198227    0.58690006   0.61409998   0.65060002   0.0442
    3.94670153]
 [ -4.56908336   0.89219999   0.96750003   0.98000002   0.10229999
    3.99978423]][0m
[37m[1m[2023-07-03 02:30:05,302][188188] Max Reward on eval: 401.0099592178012[0m
[37m[1m[2023-07-03 02:30:05,302][188188] Min Reward on eval: -152.25364572321996[0m
[37m[1m[2023-07-03 02:30:05,303][188188] Mean Reward across all agents: -6.426292128848636[0m
[37m[1m[2023-07-03 02:30:05,303][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:30:05,305][188188] mean_value=-367.2633098973589, max_value=404.0434241537996[0m
[37m[1m[2023-07-03 02:30:05,308][188188] New mean coefficients: [[ 1.4728332   3.0144277   0.6168477  -0.13108844 -2.653447    1.9444003 ]][0m
[37m[1m[2023-07-03 02:30:05,308][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:30:14,449][188188] train() took 9.14 seconds to complete[0m
[36m[2023-07-03 02:30:14,449][188188] FPS: 420179.84[0m
[36m[2023-07-03 02:30:14,451][188188] itr=214, itrs=2000, Progress: 10.70%[0m
[36m[2023-07-03 02:30:26,150][188188] train() took 11.68 seconds to complete[0m
[36m[2023-07-03 02:30:26,151][188188] FPS: 328763.62[0m
[36m[2023-07-03 02:30:30,453][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:30:30,453][188188] Reward + Measures: [[-31.80443919   0.85688132   0.94380695   0.96418464   0.11572633
    3.99659443]][0m
[37m[1m[2023-07-03 02:30:30,453][188188] Max Reward on eval: -31.80443919156801[0m
[37m[1m[2023-07-03 02:30:30,454][188188] Min Reward on eval: -31.80443919156801[0m
[37m[1m[2023-07-03 02:30:30,454][188188] Mean Reward across all agents: -31.80443919156801[0m
[37m[1m[2023-07-03 02:30:30,454][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:30:35,500][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:30:35,501][188188] Reward + Measures: [[-12.15619983   0.82130003   0.8858       0.88290006   0.1087
    3.99956441]
 [-21.01399471   0.98620003   0.96099997   0.97920001   0.0042
    3.99966741]
 [ 11.05067475   0.88840008   0.97440004   0.98030007   0.0997
    3.9996419 ]
 ...
 [ 43.64871324   0.49799997   0.78900003   0.84670001   0.33770001
    3.99917793]
 [ 23.39859787   0.98629999   0.97150004   0.97040004   0.0018
    3.76231742]
 [-46.75280318   0.8950001    0.86470002   0.90399998   0.0182
    3.95608759]][0m
[37m[1m[2023-07-03 02:30:35,501][188188] Max Reward on eval: 209.77808713105043[0m
[37m[1m[2023-07-03 02:30:35,501][188188] Min Reward on eval: -369.5620879837428[0m
[37m[1m[2023-07-03 02:30:35,501][188188] Mean Reward across all agents: -21.843881880353887[0m
[37m[1m[2023-07-03 02:30:35,502][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:30:35,503][188188] mean_value=-376.2040816848272, max_value=59.75833620749992[0m
[37m[1m[2023-07-03 02:30:35,505][188188] New mean coefficients: [[ 1.2748933  1.6509349  0.7344516  1.5147846 -2.1456087  2.278367 ]][0m
[37m[1m[2023-07-03 02:30:35,506][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:30:44,575][188188] train() took 9.07 seconds to complete[0m
[36m[2023-07-03 02:30:44,575][188188] FPS: 423503.24[0m
[36m[2023-07-03 02:30:44,578][188188] itr=215, itrs=2000, Progress: 10.75%[0m
[36m[2023-07-03 02:30:56,240][188188] train() took 11.64 seconds to complete[0m
[36m[2023-07-03 02:30:56,240][188188] FPS: 329843.40[0m
[36m[2023-07-03 02:31:00,483][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:31:00,488][188188] Reward + Measures: [[-53.01797      0.79009366   0.93755758   0.95912004   0.17670599
    3.98636818]][0m
[37m[1m[2023-07-03 02:31:00,489][188188] Max Reward on eval: -53.01796999846593[0m
[37m[1m[2023-07-03 02:31:00,489][188188] Min Reward on eval: -53.01796999846593[0m
[37m[1m[2023-07-03 02:31:00,489][188188] Mean Reward across all agents: -53.01796999846593[0m
[37m[1m[2023-07-03 02:31:00,489][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:31:05,453][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:31:05,454][188188] Reward + Measures: [[  82.87663379    0.49580002    0.9752        0.97589999    0.49640003
     3.9997735 ]
 [ -22.20112204    0.79280001    0.54260004    0.50440001    0.0124
     3.82926798]
 [ -40.05160279    0.89330006    0.86300004    0.91260004    0.0162
     3.99983525]
 ...
 [ -19.84459841    0.5934        0.95879996    0.97829992    0.39700001
     3.99971318]
 [ -30.92195343    0.79090005    0.96940005    0.98049992    0.20059998
     3.99978709]
 [-159.35587955    0.421         0.69590002    0.66410005    0.33810002
     3.99366927]][0m
[37m[1m[2023-07-03 02:31:05,454][188188] Max Reward on eval: 314.78253456130625[0m
[37m[1m[2023-07-03 02:31:05,454][188188] Min Reward on eval: -209.28171799974515[0m
[37m[1m[2023-07-03 02:31:05,454][188188] Mean Reward across all agents: -22.344799554055292[0m
[37m[1m[2023-07-03 02:31:05,455][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:31:05,456][188188] mean_value=-317.58148507645376, max_value=45.92068650899972[0m
[37m[1m[2023-07-03 02:31:05,458][188188] New mean coefficients: [[ 1.6787797   3.0766547   0.22869843  1.2131763  -1.5736966   2.2495553 ]][0m
[37m[1m[2023-07-03 02:31:05,459][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:31:14,432][188188] train() took 8.97 seconds to complete[0m
[36m[2023-07-03 02:31:14,432][188188] FPS: 428037.51[0m
[36m[2023-07-03 02:31:14,434][188188] itr=216, itrs=2000, Progress: 10.80%[0m
[36m[2023-07-03 02:31:26,189][188188] train() took 11.74 seconds to complete[0m
[36m[2023-07-03 02:31:26,190][188188] FPS: 327188.69[0m
[36m[2023-07-03 02:31:30,560][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:31:30,560][188188] Reward + Measures: [[-91.93913537   0.68433303   0.94711328   0.96330094   0.288706
    3.96365619]][0m
[37m[1m[2023-07-03 02:31:30,560][188188] Max Reward on eval: -91.93913537198503[0m
[37m[1m[2023-07-03 02:31:30,561][188188] Min Reward on eval: -91.93913537198503[0m
[37m[1m[2023-07-03 02:31:30,561][188188] Mean Reward across all agents: -91.93913537198503[0m
[37m[1m[2023-07-03 02:31:30,561][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:31:35,590][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:31:35,591][188188] Reward + Measures: [[  1.52904459   0.50320005   0.55030006   0.50749999   0.20700002
    3.77458954]
 [-17.48515304   0.57569999   0.86739999   0.8179       0.36129999
    3.64869761]
 [-37.18710005   0.97299999   0.94029999   0.94950008   0.005
    3.99792528]
 ...
 [ 34.25919265   0.43650004   0.75940001   0.77719998   0.42539999
    3.9750421 ]
 [-84.27126672   0.69220001   0.62559998   0.62099999   0.0436
    3.98497128]
 [ 21.20973473   0.23290001   0.75650001   0.75279999   0.60210007
    3.93385315]][0m
[37m[1m[2023-07-03 02:31:35,591][188188] Max Reward on eval: 304.69806023929266[0m
[37m[1m[2023-07-03 02:31:35,591][188188] Min Reward on eval: -218.1696183854714[0m
[37m[1m[2023-07-03 02:31:35,591][188188] Mean Reward across all agents: -25.854687331159624[0m
[37m[1m[2023-07-03 02:31:35,592][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:31:35,594][188188] mean_value=-408.75968213979667, max_value=60.2678480234674[0m
[37m[1m[2023-07-03 02:31:35,596][188188] New mean coefficients: [[ 0.24614072  2.608714   -0.4238177   0.6375415  -1.103601    2.973642  ]][0m
[37m[1m[2023-07-03 02:31:35,597][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:31:44,686][188188] train() took 9.09 seconds to complete[0m
[36m[2023-07-03 02:31:44,687][188188] FPS: 422545.52[0m
[36m[2023-07-03 02:31:44,689][188188] itr=217, itrs=2000, Progress: 10.85%[0m
[36m[2023-07-03 02:31:56,292][188188] train() took 11.58 seconds to complete[0m
[36m[2023-07-03 02:31:56,293][188188] FPS: 331512.88[0m
[36m[2023-07-03 02:32:00,599][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:32:00,599][188188] Reward + Measures: [[-40.24236657   0.75019068   0.94705069   0.963377     0.22389632
    3.99476504]][0m
[37m[1m[2023-07-03 02:32:00,599][188188] Max Reward on eval: -40.24236657316361[0m
[37m[1m[2023-07-03 02:32:00,599][188188] Min Reward on eval: -40.24236657316361[0m
[37m[1m[2023-07-03 02:32:00,600][188188] Mean Reward across all agents: -40.24236657316361[0m
[37m[1m[2023-07-03 02:32:00,600][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:32:05,739][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:32:05,740][188188] Reward + Measures: [[ -37.00248204    0.68060005    0.90070003    0.87930006    0.27939999
     3.70822382]
 [ 259.29645456    0.2872        0.97360003    0.94999999    0.69100004
     3.96264148]
 [  39.77483075    0.2088        0.1919        0.1719        0.0869
     3.78959966]
 ...
 [   3.61578804    0.77730006    0.86079997    0.83190006    0.1987
     3.63927007]
 [  87.99896513    0.2388        0.76190001    0.69470006    0.58070004
     3.61246419]
 [-102.11185884    0.56870002    0.74810004    0.76469994    0.2723
     3.93354011]][0m
[37m[1m[2023-07-03 02:32:05,740][188188] Max Reward on eval: 398.49193359259516[0m
[37m[1m[2023-07-03 02:32:05,740][188188] Min Reward on eval: -273.26553704347464[0m
[37m[1m[2023-07-03 02:32:05,741][188188] Mean Reward across all agents: 2.4818932760954895[0m
[37m[1m[2023-07-03 02:32:05,741][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:32:05,743][188188] mean_value=-710.9301841907242, max_value=267.57509498873975[0m
[37m[1m[2023-07-03 02:32:05,745][188188] New mean coefficients: [[ 0.43206257  2.3656554  -0.1472618   0.7419315  -0.96743274  2.108735  ]][0m
[37m[1m[2023-07-03 02:32:05,746][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:32:14,769][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 02:32:14,769][188188] FPS: 425630.44[0m
[36m[2023-07-03 02:32:14,772][188188] itr=218, itrs=2000, Progress: 10.90%[0m
[36m[2023-07-03 02:32:26,831][188188] train() took 12.04 seconds to complete[0m
[36m[2023-07-03 02:32:26,832][188188] FPS: 318974.79[0m
[36m[2023-07-03 02:32:31,047][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:32:31,047][188188] Reward + Measures: [[-50.17630668   0.64526874   0.95667332   0.96965665   0.33592266
    3.9935267 ]][0m
[37m[1m[2023-07-03 02:32:31,047][188188] Max Reward on eval: -50.17630668174949[0m
[37m[1m[2023-07-03 02:32:31,047][188188] Min Reward on eval: -50.17630668174949[0m
[37m[1m[2023-07-03 02:32:31,048][188188] Mean Reward across all agents: -50.17630668174949[0m
[37m[1m[2023-07-03 02:32:31,048][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:32:36,035][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:32:36,035][188188] Reward + Measures: [[ -32.07340744    0.59280002    0.96999997    0.97920001    0.39699998
     3.99980164]
 [-201.78924442    0.30469999    0.88459998    0.91320002    0.60820001
     3.85903049]
 [ -37.65342497    0.98830003    0.9601        0.97990006    0.0031
     3.91783762]
 ...
 [ -25.88310253    0.88770002    0.97320002    0.98040003    0.1
     3.98888779]
 [ 308.83023885    0.73040003    0.6728        0.7924        0.17380001
     3.97229767]
 [ -61.57649122    0.49200001    0.96520007    0.96780008    0.49670002
     3.99950528]][0m
[37m[1m[2023-07-03 02:32:36,035][188188] Max Reward on eval: 357.5687379648909[0m
[37m[1m[2023-07-03 02:32:36,036][188188] Min Reward on eval: -335.11590698612855[0m
[37m[1m[2023-07-03 02:32:36,036][188188] Mean Reward across all agents: -14.517970090158268[0m
[37m[1m[2023-07-03 02:32:36,036][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:32:36,039][188188] mean_value=-422.39227294691113, max_value=339.08986889786496[0m
[37m[1m[2023-07-03 02:32:36,041][188188] New mean coefficients: [[ 0.08824855  2.588982   -0.40499175  1.4753922  -1.095154    2.5769062 ]][0m
[37m[1m[2023-07-03 02:32:36,042][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:32:45,021][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 02:32:45,021][188188] FPS: 427739.41[0m
[36m[2023-07-03 02:32:45,024][188188] itr=219, itrs=2000, Progress: 10.95%[0m
[36m[2023-07-03 02:32:56,644][188188] train() took 11.60 seconds to complete[0m
[36m[2023-07-03 02:32:56,644][188188] FPS: 331077.98[0m
[36m[2023-07-03 02:33:00,894][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:33:00,895][188188] Reward + Measures: [[-17.53141738   0.51165599   0.94163299   0.95217139   0.45668164
    3.99808192]][0m
[37m[1m[2023-07-03 02:33:00,895][188188] Max Reward on eval: -17.531417381843106[0m
[37m[1m[2023-07-03 02:33:00,895][188188] Min Reward on eval: -17.531417381843106[0m
[37m[1m[2023-07-03 02:33:00,895][188188] Mean Reward across all agents: -17.531417381843106[0m
[37m[1m[2023-07-03 02:33:00,896][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:33:05,864][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:33:05,865][188188] Reward + Measures: [[ -5.52700815   0.15549999   0.84130001   0.84010011   0.74209994
    3.98739791]
 [-30.22248802   0.1982       0.9781       0.98040009   0.79360002
    3.99953961]
 [-11.22400845   0.51910001   0.94340003   0.9436       0.4522
    3.9914372 ]
 ...
 [ 34.81217433   0.59540004   0.9738       0.98149997   0.39700001
    3.99962616]
 [ 12.59931282   0.31469998   0.77229995   0.75560004   0.5309
    3.90516448]
 [  1.86073416   0.19790001   0.97959995   0.9813       0.79269999
    3.99969363]][0m
[37m[1m[2023-07-03 02:33:05,865][188188] Max Reward on eval: 324.33438112847506[0m
[37m[1m[2023-07-03 02:33:05,865][188188] Min Reward on eval: -346.93483543526384[0m
[37m[1m[2023-07-03 02:33:05,865][188188] Mean Reward across all agents: -19.223883912053548[0m
[37m[1m[2023-07-03 02:33:05,866][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:33:05,868][188188] mean_value=-423.1278495031395, max_value=249.21229151644903[0m
[37m[1m[2023-07-03 02:33:05,870][188188] New mean coefficients: [[ 0.24464807  2.4097555  -0.34529078  2.0198555  -0.94333416  1.9128265 ]][0m
[37m[1m[2023-07-03 02:33:05,871][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:33:14,806][188188] train() took 8.93 seconds to complete[0m
[36m[2023-07-03 02:33:14,806][188188] FPS: 429854.20[0m
[36m[2023-07-03 02:33:14,808][188188] itr=220, itrs=2000, Progress: 11.00%[0m
[37m[1m[2023-07-03 02:33:17,493][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000200[0m
[36m[2023-07-03 02:33:29,562][188188] train() took 11.74 seconds to complete[0m
[36m[2023-07-03 02:33:29,563][188188] FPS: 327023.46[0m
[36m[2023-07-03 02:33:33,883][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:33:33,883][188188] Reward + Measures: [[-13.68511575   0.43160668   0.94354767   0.95347238   0.5364387
    3.99827075]][0m
[37m[1m[2023-07-03 02:33:33,884][188188] Max Reward on eval: -13.685115748749562[0m
[37m[1m[2023-07-03 02:33:33,884][188188] Min Reward on eval: -13.685115748749562[0m
[37m[1m[2023-07-03 02:33:33,884][188188] Mean Reward across all agents: -13.685115748749562[0m
[37m[1m[2023-07-03 02:33:33,885][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:33:38,964][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:33:38,965][188188] Reward + Measures: [[-128.90078829    0.07560001    0.96219999    0.94560003    0.91390002
     3.9684217 ]
 [   9.72801521    0.1948        0.97480005    0.97229999    0.79159999
     3.99906468]
 [  -7.86648712    0.29159999    0.69420004    0.66969997    0.44710001
     3.6875546 ]
 ...
 [  45.20694471    0.37610003    0.79279995    0.77380002    0.48140001
     3.68191719]
 [ -13.15714591    0.19510001    0.87750006    0.85199994    0.72760004
     3.99256492]
 [ -47.02270965    0.69269997    0.96160001    0.97790003    0.29980001
     3.99981809]][0m
[37m[1m[2023-07-03 02:33:38,965][188188] Max Reward on eval: 211.75046351030468[0m
[37m[1m[2023-07-03 02:33:38,965][188188] Min Reward on eval: -229.88767019931691[0m
[37m[1m[2023-07-03 02:33:38,966][188188] Mean Reward across all agents: -8.455671111005397[0m
[37m[1m[2023-07-03 02:33:38,966][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:33:38,967][188188] mean_value=-458.59755614023095, max_value=140.62329163578084[0m
[37m[1m[2023-07-03 02:33:38,970][188188] New mean coefficients: [[ 0.35355815  2.2144012  -0.69945234  2.4780028  -1.2840759   1.2155464 ]][0m
[37m[1m[2023-07-03 02:33:38,971][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:33:47,995][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 02:33:47,995][188188] FPS: 425588.08[0m
[36m[2023-07-03 02:33:47,998][188188] itr=221, itrs=2000, Progress: 11.05%[0m
[36m[2023-07-03 02:33:59,866][188188] train() took 11.85 seconds to complete[0m
[36m[2023-07-03 02:33:59,867][188188] FPS: 324146.60[0m
[36m[2023-07-03 02:34:04,116][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:34:04,116][188188] Reward + Measures: [[-24.20518951   0.3577317    0.9350633    0.94649267   0.5999403
    3.99809432]][0m
[37m[1m[2023-07-03 02:34:04,116][188188] Max Reward on eval: -24.205189511940485[0m
[37m[1m[2023-07-03 02:34:04,117][188188] Min Reward on eval: -24.205189511940485[0m
[37m[1m[2023-07-03 02:34:04,117][188188] Mean Reward across all agents: -24.205189511940485[0m
[37m[1m[2023-07-03 02:34:04,117][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:34:09,261][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:34:09,261][188188] Reward + Measures: [[-52.05720465   0.163        0.80849999   0.77799994   0.70410007
    3.98931885]
 [-40.93961181   0.25710002   0.65009999   0.32010001   0.59109998
    3.86278796]
 [ 58.87733973   0.058        0.73750001   0.65450001   0.80039996
    3.95106173]
 ...
 [ 48.33111675   0.1166       0.56269997   0.41999999   0.57019997
    3.95508456]
 [-59.54941877   0.0188       0.98199999   0.97320002   0.9702
    3.99819446]
 [ -4.36658116   0.39210001   0.96159995   0.96110004   0.59119999
    3.96494341]][0m
[37m[1m[2023-07-03 02:34:09,262][188188] Max Reward on eval: 223.01668466534466[0m
[37m[1m[2023-07-03 02:34:09,262][188188] Min Reward on eval: -275.62480718586596[0m
[37m[1m[2023-07-03 02:34:09,262][188188] Mean Reward across all agents: -8.00673122450293[0m
[37m[1m[2023-07-03 02:34:09,262][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:34:09,264][188188] mean_value=-653.9769315533952, max_value=68.87430846494819[0m
[37m[1m[2023-07-03 02:34:09,266][188188] New mean coefficients: [[ 1.3015189  1.8131864  0.1336897  1.4426911 -1.3852366  1.8877077]][0m
[37m[1m[2023-07-03 02:34:09,267][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:34:18,222][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 02:34:18,222][188188] FPS: 428906.69[0m
[36m[2023-07-03 02:34:18,224][188188] itr=222, itrs=2000, Progress: 11.10%[0m
[36m[2023-07-03 02:34:29,862][188188] train() took 11.62 seconds to complete[0m
[36m[2023-07-03 02:34:29,862][188188] FPS: 330516.91[0m
[36m[2023-07-03 02:34:34,077][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:34:34,078][188188] Reward + Measures: [[-13.70419034   0.3359113    0.93275666   0.94689792   0.61818862
    3.99956965]][0m
[37m[1m[2023-07-03 02:34:34,078][188188] Max Reward on eval: -13.70419033667942[0m
[37m[1m[2023-07-03 02:34:34,078][188188] Min Reward on eval: -13.70419033667942[0m
[37m[1m[2023-07-03 02:34:34,079][188188] Mean Reward across all agents: -13.70419033667942[0m
[37m[1m[2023-07-03 02:34:34,079][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:34:39,091][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:34:39,092][188188] Reward + Measures: [[  59.18377566    0.13870001    0.95220006    0.94859999    0.84060001
     3.99201775]
 [   9.47846105    0.24059999    0.79399997    0.78920001    0.6006
     3.99864197]
 [-106.03991499    0.            0.9795        0.9774        0.98579997
     3.96973681]
 ...
 [  17.60311143    0.27169999    0.97509998    0.97240001    0.71730006
     3.99838614]
 [ -40.92050788    0.41889998    0.60980004    0.58029997    0.2859
     3.82464385]
 [-109.15848031    0.35410005    0.46259999    0.47680002    0.22090001
     3.87206697]][0m
[37m[1m[2023-07-03 02:34:39,092][188188] Max Reward on eval: 351.1628188939765[0m
[37m[1m[2023-07-03 02:34:39,092][188188] Min Reward on eval: -246.6418840408325[0m
[37m[1m[2023-07-03 02:34:39,093][188188] Mean Reward across all agents: -26.327203992145193[0m
[37m[1m[2023-07-03 02:34:39,093][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:34:39,095][188188] mean_value=-499.2340648455499, max_value=262.6928139501699[0m
[37m[1m[2023-07-03 02:34:39,097][188188] New mean coefficients: [[ 1.2282572   2.363645    0.79550457  0.9587387  -1.5983307   1.5066433 ]][0m
[37m[1m[2023-07-03 02:34:39,098][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:34:48,111][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 02:34:48,112][188188] FPS: 426111.87[0m
[36m[2023-07-03 02:34:48,114][188188] itr=223, itrs=2000, Progress: 11.15%[0m
[36m[2023-07-03 02:34:59,710][188188] train() took 11.58 seconds to complete[0m
[36m[2023-07-03 02:34:59,710][188188] FPS: 331724.33[0m
[36m[2023-07-03 02:35:04,070][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:35:04,070][188188] Reward + Measures: [[-181.92428177    0.38188833    0.96121567    0.97028971    0.59778202
     3.92538309]][0m
[37m[1m[2023-07-03 02:35:04,071][188188] Max Reward on eval: -181.92428176782772[0m
[37m[1m[2023-07-03 02:35:04,071][188188] Min Reward on eval: -181.92428176782772[0m
[37m[1m[2023-07-03 02:35:04,071][188188] Mean Reward across all agents: -181.92428176782772[0m
[37m[1m[2023-07-03 02:35:04,071][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:35:09,077][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:35:09,082][188188] Reward + Measures: [[ -52.42278255    0.59400004    0.9806        0.97999996    0.39949998
     3.99965286]
 [  65.97000007    0.85020012    0.95300001    0.96350002    0.132
     3.99835062]
 [  62.8787038     0.58950001    0.98029995    0.9774        0.39420003
     3.99932551]
 ...
 [-150.20651955    0.34119999    0.94610006    0.92379999    0.63190001
     3.95935035]
 [   4.56913758    0.99040002    0.96849996    0.98019999    0.0025
     3.99977374]
 [ 122.1932637     0.45120001    0.56300002    0.5539        0.2027
     3.92678428]][0m
[37m[1m[2023-07-03 02:35:09,083][188188] Max Reward on eval: 372.9741821807344[0m
[37m[1m[2023-07-03 02:35:09,083][188188] Min Reward on eval: -345.07248406577855[0m
[37m[1m[2023-07-03 02:35:09,083][188188] Mean Reward across all agents: -9.284277542961547[0m
[37m[1m[2023-07-03 02:35:09,084][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:35:09,085][188188] mean_value=-368.68583935128606, max_value=86.18814121659398[0m
[37m[1m[2023-07-03 02:35:09,088][188188] New mean coefficients: [[ 1.6313195  2.7740002  1.3630426  0.6701101 -1.5433441  1.1753479]][0m
[37m[1m[2023-07-03 02:35:09,089][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:35:18,122][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 02:35:18,122][188188] FPS: 425186.44[0m
[36m[2023-07-03 02:35:18,124][188188] itr=224, itrs=2000, Progress: 11.20%[0m
[36m[2023-07-03 02:35:29,779][188188] train() took 11.64 seconds to complete[0m
[36m[2023-07-03 02:35:29,779][188188] FPS: 330017.52[0m
[36m[2023-07-03 02:35:34,064][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:35:34,064][188188] Reward + Measures: [[31.55122589  0.62162602  0.89323235  0.87942636  0.31776166  3.73537326]][0m
[37m[1m[2023-07-03 02:35:34,064][188188] Max Reward on eval: 31.55122589156604[0m
[37m[1m[2023-07-03 02:35:34,065][188188] Min Reward on eval: 31.55122589156604[0m
[37m[1m[2023-07-03 02:35:34,065][188188] Mean Reward across all agents: 31.55122589156604[0m
[37m[1m[2023-07-03 02:35:34,065][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:35:39,119][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:35:39,171][188188] Reward + Measures: [[   8.28422848    0.22660001    0.85360003    0.8745001     0.6523
     3.98715448]
 [  78.9610833     0.58810002    0.96859998    0.96730006    0.39560002
     3.97568893]
 [   1.00985223    0.39390001    0.97649997    0.97430003    0.59200001
     3.99929357]
 ...
 [ -12.69036507    0.6814        0.96110004    0.95920002    0.29809999
     3.99625325]
 [-188.59768419    0.38439998    0.95830005    0.94750005    0.59000003
     3.88240743]
 [ -31.23400043    0.68780005    0.97439998    0.97760004    0.29830003
     3.99905086]][0m
[37m[1m[2023-07-03 02:35:39,172][188188] Max Reward on eval: 298.48740895343946[0m
[37m[1m[2023-07-03 02:35:39,172][188188] Min Reward on eval: -328.1980991726741[0m
[37m[1m[2023-07-03 02:35:39,172][188188] Mean Reward across all agents: -4.962381829908078[0m
[37m[1m[2023-07-03 02:35:39,172][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:35:39,174][188188] mean_value=-365.72504687927903, max_value=17.803409387027187[0m
[37m[1m[2023-07-03 02:35:39,177][188188] New mean coefficients: [[ 1.7263218  3.0368874  1.0895314  1.2351129 -1.5903383  1.5942314]][0m
[37m[1m[2023-07-03 02:35:39,178][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:35:48,190][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 02:35:48,190][188188] FPS: 426179.28[0m
[36m[2023-07-03 02:35:48,192][188188] itr=225, itrs=2000, Progress: 11.25%[0m
[36m[2023-07-03 02:35:59,847][188188] train() took 11.64 seconds to complete[0m
[36m[2023-07-03 02:35:59,848][188188] FPS: 330011.42[0m
[36m[2023-07-03 02:36:04,160][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:36:04,160][188188] Reward + Measures: [[-18.51985502   0.41532868   0.96801203   0.9759233    0.56967604
    3.99962568]][0m
[37m[1m[2023-07-03 02:36:04,161][188188] Max Reward on eval: -18.519855018563383[0m
[37m[1m[2023-07-03 02:36:04,161][188188] Min Reward on eval: -18.519855018563383[0m
[37m[1m[2023-07-03 02:36:04,161][188188] Mean Reward across all agents: -18.519855018563383[0m
[37m[1m[2023-07-03 02:36:04,162][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:36:09,132][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:36:09,132][188188] Reward + Measures: [[-42.60714771   0.59060001   0.96540004   0.97600001   0.40050003
    3.99965072]
 [-24.77820694   0.73900002   0.75620002   0.71929997   0.18970001
    3.90473914]
 [ -4.44354763   0.80260003   0.95069999   0.94980001   0.14459999
    3.94714212]
 ...
 [-29.64340389   0.69329995   0.77160001   0.82770008   0.1225
    3.99888968]
 [-13.23610544   0.5941       0.96989995   0.98140001   0.39750001
    3.99960399]
 [-45.08211807   0.77249998   0.83999997   0.83700007   0.1328
    3.8482461 ]][0m
[37m[1m[2023-07-03 02:36:09,133][188188] Max Reward on eval: 374.1852616676595[0m
[37m[1m[2023-07-03 02:36:09,133][188188] Min Reward on eval: -215.28076542967466[0m
[37m[1m[2023-07-03 02:36:09,133][188188] Mean Reward across all agents: -6.4478784131857445[0m
[37m[1m[2023-07-03 02:36:09,133][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:36:09,135][188188] mean_value=-309.0947531077998, max_value=286.68216578242493[0m
[37m[1m[2023-07-03 02:36:09,138][188188] New mean coefficients: [[ 2.356748   2.8717306  0.6814194  0.7857335 -2.4124384  1.8701452]][0m
[37m[1m[2023-07-03 02:36:09,139][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:36:18,145][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 02:36:18,145][188188] FPS: 426451.26[0m
[36m[2023-07-03 02:36:18,147][188188] itr=226, itrs=2000, Progress: 11.30%[0m
[36m[2023-07-03 02:36:29,769][188188] train() took 11.60 seconds to complete[0m
[36m[2023-07-03 02:36:29,769][188188] FPS: 330955.50[0m
[36m[2023-07-03 02:36:34,018][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:36:34,019][188188] Reward + Measures: [[-20.53345749   0.63615      0.95564502   0.96830058   0.34177932
    3.99960279]][0m
[37m[1m[2023-07-03 02:36:34,019][188188] Max Reward on eval: -20.53345749387048[0m
[37m[1m[2023-07-03 02:36:34,019][188188] Min Reward on eval: -20.53345749387048[0m
[37m[1m[2023-07-03 02:36:34,019][188188] Mean Reward across all agents: -20.53345749387048[0m
[37m[1m[2023-07-03 02:36:34,020][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:36:39,169][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:36:39,170][188188] Reward + Measures: [[-13.37045345   0.90080005   0.86590004   0.87670004   0.0047
    3.89227486]
 [-29.26669089   0.69279999   0.87040007   0.92089999   0.20910001
    3.99975276]
 [ -5.89696561   0.68970001   0.97340006   0.97960007   0.29910001
    3.99978256]
 ...
 [-13.53029387   0.98929995   0.97580004   0.97700006   0.0026
    3.99563003]
 [-27.16693168   0.24749999   0.66780007   0.62159997   0.39990002
    3.82065368]
 [ 39.91625562   0.89069998   0.95209998   0.95730001   0.0995
    3.86141014]][0m
[37m[1m[2023-07-03 02:36:39,170][188188] Max Reward on eval: 272.9524272612296[0m
[37m[1m[2023-07-03 02:36:39,171][188188] Min Reward on eval: -330.22139546908437[0m
[37m[1m[2023-07-03 02:36:39,171][188188] Mean Reward across all agents: -19.28148748757059[0m
[37m[1m[2023-07-03 02:36:39,171][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:36:39,172][188188] mean_value=-366.13515239255884, max_value=1.4539492187599308[0m
[37m[1m[2023-07-03 02:36:39,175][188188] New mean coefficients: [[ 0.83221734  2.784755    0.3625545   1.0941583  -3.3329568   1.4615334 ]][0m
[37m[1m[2023-07-03 02:36:39,176][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:36:48,122][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 02:36:48,122][188188] FPS: 429308.45[0m
[36m[2023-07-03 02:36:48,124][188188] itr=227, itrs=2000, Progress: 11.35%[0m
[36m[2023-07-03 02:36:59,689][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 02:36:59,689][188188] FPS: 332585.71[0m
[36m[2023-07-03 02:37:03,963][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:37:03,963][188188] Reward + Measures: [[-31.22430256   0.61055601   0.73329067   0.71447897   0.29411298
    3.87133265]][0m
[37m[1m[2023-07-03 02:37:03,963][188188] Max Reward on eval: -31.224302555938173[0m
[37m[1m[2023-07-03 02:37:03,964][188188] Min Reward on eval: -31.224302555938173[0m
[37m[1m[2023-07-03 02:37:03,964][188188] Mean Reward across all agents: -31.224302555938173[0m
[37m[1m[2023-07-03 02:37:03,964][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:37:08,945][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:37:08,946][188188] Reward + Measures: [[-30.29888751   0.11750001   0.1432       0.17080002   0.17099999
    3.95267606]
 [-97.08038345   0.1072       0.92030001   0.91120005   0.83820003
    3.9331398 ]
 [-76.09374575   0.42200002   0.88940001   0.88210005   0.49710003
    3.99930358]
 ...
 [-58.40741341   0.59619999   0.88410008   0.9393       0.30340001
    3.97545791]
 [ 24.92402973   0.26859999   0.87279999   0.79690003   0.65649998
    3.98943639]
 [-59.28669004   0.27330002   0.95500004   0.93839997   0.70590001
    3.98234677]][0m
[37m[1m[2023-07-03 02:37:08,946][188188] Max Reward on eval: 400.1554019525647[0m
[37m[1m[2023-07-03 02:37:08,946][188188] Min Reward on eval: -206.51601432012393[0m
[37m[1m[2023-07-03 02:37:08,946][188188] Mean Reward across all agents: 6.782331004196721[0m
[37m[1m[2023-07-03 02:37:08,946][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:37:08,948][188188] mean_value=-460.5497847911159, max_value=496.8085348377377[0m
[37m[1m[2023-07-03 02:37:08,951][188188] New mean coefficients: [[ 0.7366378   2.294027    0.32085133  1.2805947  -2.873828    1.9269822 ]][0m
[37m[1m[2023-07-03 02:37:08,952][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:37:17,964][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 02:37:17,965][188188] FPS: 426138.94[0m
[36m[2023-07-03 02:37:17,967][188188] itr=228, itrs=2000, Progress: 11.40%[0m
[36m[2023-07-03 02:37:29,641][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 02:37:29,641][188188] FPS: 329472.55[0m
[36m[2023-07-03 02:37:33,970][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:37:33,971][188188] Reward + Measures: [[-8.85668951  0.62854135  0.95148158  0.96241701  0.33850667  3.79873753]][0m
[37m[1m[2023-07-03 02:37:33,971][188188] Max Reward on eval: -8.85668951323379[0m
[37m[1m[2023-07-03 02:37:33,971][188188] Min Reward on eval: -8.85668951323379[0m
[37m[1m[2023-07-03 02:37:33,972][188188] Mean Reward across all agents: -8.85668951323379[0m
[37m[1m[2023-07-03 02:37:33,972][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:37:38,998][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:37:38,998][188188] Reward + Measures: [[  6.70272801   0.68970007   0.96929997   0.97889996   0.29910001
    3.9998796 ]
 [ 12.56934946   0.78570002   0.97360003   0.97420007   0.19770001
    3.99009299]
 [-42.86022259   0.6013       0.78759998   0.74669999   0.26980001
    3.84193277]
 ...
 [ 58.28206636   0.78570002   0.97850001   0.98430008   0.1982
    3.99989581]
 [-35.7709039    0.88500005   0.95570004   0.97750008   0.1056
    3.9994278 ]
 [-99.59669939   0.75450003   0.60980004   0.65850002   0.045
    3.83061457]][0m
[37m[1m[2023-07-03 02:37:38,999][188188] Max Reward on eval: 313.9451184323058[0m
[37m[1m[2023-07-03 02:37:38,999][188188] Min Reward on eval: -249.61495114825667[0m
[37m[1m[2023-07-03 02:37:38,999][188188] Mean Reward across all agents: -20.327253821359715[0m
[37m[1m[2023-07-03 02:37:38,999][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:37:39,000][188188] mean_value=-322.7797504058404, max_value=-31.74297640070054[0m
[36m[2023-07-03 02:37:39,003][188188] XNES is restarting with a new solution whose measures are [0.67640001 0.80820006 0.80730003 0.14049999 3.6671567 ] and objective is 430.3316278594313[0m
[36m[2023-07-03 02:37:39,004][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 02:37:39,006][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 02:37:39,007][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:37:48,062][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 02:37:48,062][188188] FPS: 424119.92[0m
[36m[2023-07-03 02:37:48,065][188188] itr=229, itrs=2000, Progress: 11.45%[0m
[36m[2023-07-03 02:37:59,820][188188] train() took 11.74 seconds to complete[0m
[36m[2023-07-03 02:37:59,820][188188] FPS: 327218.72[0m
[36m[2023-07-03 02:38:04,203][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:38:04,203][188188] Reward + Measures: [[-44.99881963   0.94223535   0.96129233   0.93679136   0.91881365
    3.98123503]][0m
[37m[1m[2023-07-03 02:38:04,204][188188] Max Reward on eval: -44.99881963471354[0m
[37m[1m[2023-07-03 02:38:04,204][188188] Min Reward on eval: -44.99881963471354[0m
[37m[1m[2023-07-03 02:38:04,204][188188] Mean Reward across all agents: -44.99881963471354[0m
[37m[1m[2023-07-03 02:38:04,204][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:38:09,225][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:38:09,225][188188] Reward + Measures: [[-155.88948847    0.98649997    0.96990007    0.98100007    0.0062
     3.99793863]
 [  39.76087172    0.87169999    0.88150007    0.86370003    0.86059999
     3.99236488]
 [ -76.08191443    0.94660008    0.97240001    0.95030004    0.9235
     3.97907948]
 ...
 [ -75.70761931    0.94340003    0.9601        0.94399995    0.92579997
     3.83211064]
 [ -68.37912022    0.92810005    0.94999999    0.92880005    0.90640002
     3.81192279]
 [ -31.74800968    0.96620005    0.9679001     0.94950008    0.94869995
     3.99321437]][0m
[37m[1m[2023-07-03 02:38:09,225][188188] Max Reward on eval: 331.81836281428116[0m
[37m[1m[2023-07-03 02:38:09,226][188188] Min Reward on eval: -350.09900284558535[0m
[37m[1m[2023-07-03 02:38:09,226][188188] Mean Reward across all agents: -48.354873988994626[0m
[37m[1m[2023-07-03 02:38:09,226][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:38:09,228][188188] mean_value=-310.57654986114176, max_value=461.746401191057[0m
[37m[1m[2023-07-03 02:38:09,230][188188] New mean coefficients: [[-0.33259782 -1.3375022  -1.0003678  -0.88883114 -1.7073102  -1.9506816 ]][0m
[37m[1m[2023-07-03 02:38:09,231][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:38:18,236][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 02:38:18,236][188188] FPS: 426538.78[0m
[36m[2023-07-03 02:38:18,238][188188] itr=230, itrs=2000, Progress: 11.50%[0m
[37m[1m[2023-07-03 02:38:20,956][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000210[0m
[36m[2023-07-03 02:38:32,848][188188] train() took 11.57 seconds to complete[0m
[36m[2023-07-03 02:38:32,849][188188] FPS: 331986.61[0m
[36m[2023-07-03 02:38:37,140][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:38:37,140][188188] Reward + Measures: [[-256.66339846    0.86294496    0.80600762    0.83580929    0.03600967
     3.88865757]][0m
[37m[1m[2023-07-03 02:38:37,141][188188] Max Reward on eval: -256.66339846437666[0m
[37m[1m[2023-07-03 02:38:37,141][188188] Min Reward on eval: -256.66339846437666[0m
[37m[1m[2023-07-03 02:38:37,141][188188] Mean Reward across all agents: -256.66339846437666[0m
[37m[1m[2023-07-03 02:38:37,142][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:38:42,303][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:38:42,355][188188] Reward + Measures: [[ 267.95320295    0.62040001    0.36960003    0.65250003    0.30110002
     3.89499736]
 [  92.28326797    0.8915        0.97010005    0.96969998    0.0992
     3.80703926]
 [  84.62475981    0.2045        0.24200001    0.22159998    0.1468
     3.57840085]
 ...
 [ -21.79123045    0.95890009    0.92449999    0.94460005    0.025
     3.94908834]
 [ -28.04915137    0.29270002    0.89099997    0.9127        0.61210001
     3.94053388]
 [-129.45222823    0.78240001    0.75690001    0.78200001    0.0249
     3.86910486]][0m
[37m[1m[2023-07-03 02:38:42,355][188188] Max Reward on eval: 635.6747315290384[0m
[37m[1m[2023-07-03 02:38:42,356][188188] Min Reward on eval: -333.4087086514337[0m
[37m[1m[2023-07-03 02:38:42,356][188188] Mean Reward across all agents: -1.5401459846076295[0m
[37m[1m[2023-07-03 02:38:42,356][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:38:42,358][188188] mean_value=-437.3940324923341, max_value=246.01103319265002[0m
[37m[1m[2023-07-03 02:38:42,361][188188] New mean coefficients: [[ 0.12988126 -1.1645615  -0.26618236 -0.00309211 -1.6963385  -2.6051323 ]][0m
[37m[1m[2023-07-03 02:38:42,362][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:38:51,366][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 02:38:51,366][188188] FPS: 426547.42[0m
[36m[2023-07-03 02:38:51,369][188188] itr=231, itrs=2000, Progress: 11.55%[0m
[36m[2023-07-03 02:39:02,910][188188] train() took 11.52 seconds to complete[0m
[36m[2023-07-03 02:39:02,910][188188] FPS: 333325.03[0m
[36m[2023-07-03 02:39:07,292][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:39:07,292][188188] Reward + Measures: [[-137.11898602    0.77695799    0.72559798    0.75482094    0.05474666
     3.86389351]][0m
[37m[1m[2023-07-03 02:39:07,293][188188] Max Reward on eval: -137.11898602309267[0m
[37m[1m[2023-07-03 02:39:07,293][188188] Min Reward on eval: -137.11898602309267[0m
[37m[1m[2023-07-03 02:39:07,293][188188] Mean Reward across all agents: -137.11898602309267[0m
[37m[1m[2023-07-03 02:39:07,293][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:39:12,384][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:39:12,384][188188] Reward + Measures: [[-195.34096429    0.76029998    0.71969998    0.74180001    0.0611
     3.90921831]
 [ -39.27541673    0.84969997    0.79400009    0.78940004    0.0253
     3.99076509]
 [  34.61215899    0.84060001    0.77740002    0.79490006    0.0361
     3.9904449 ]
 ...
 [ -48.52250121    0.32180005    0.31580001    0.30130002    0.0783
     3.91190839]
 [ -77.92749958    0.91380006    0.94440001    0.93849993    0.89060003
     3.85290194]
 [  15.12594437    0.96609992    0.97110003    0.96890002    0.97139996
     3.95660758]][0m
[37m[1m[2023-07-03 02:39:12,385][188188] Max Reward on eval: 370.27372503727673[0m
[37m[1m[2023-07-03 02:39:12,385][188188] Min Reward on eval: -318.4645633621141[0m
[37m[1m[2023-07-03 02:39:12,385][188188] Mean Reward across all agents: -59.36470110661173[0m
[37m[1m[2023-07-03 02:39:12,385][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:39:12,387][188188] mean_value=-548.4342095208596, max_value=63.12892522197063[0m
[37m[1m[2023-07-03 02:39:12,389][188188] New mean coefficients: [[ 0.26443365 -0.8100296  -1.3148208   1.1698699   0.41487134 -2.3352284 ]][0m
[37m[1m[2023-07-03 02:39:12,390][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:39:21,533][188188] train() took 9.14 seconds to complete[0m
[36m[2023-07-03 02:39:21,533][188188] FPS: 420104.64[0m
[36m[2023-07-03 02:39:21,535][188188] itr=232, itrs=2000, Progress: 11.60%[0m
[36m[2023-07-03 02:39:33,176][188188] train() took 11.62 seconds to complete[0m
[36m[2023-07-03 02:39:33,176][188188] FPS: 330446.75[0m
[36m[2023-07-03 02:39:37,508][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:39:37,509][188188] Reward + Measures: [[-8.3129758   0.87295431  0.87598801  0.87537766  0.87328464  3.99300289]][0m
[37m[1m[2023-07-03 02:39:37,509][188188] Max Reward on eval: -8.312975795530399[0m
[37m[1m[2023-07-03 02:39:37,509][188188] Min Reward on eval: -8.312975795530399[0m
[37m[1m[2023-07-03 02:39:37,509][188188] Mean Reward across all agents: -8.312975795530399[0m
[37m[1m[2023-07-03 02:39:37,510][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:39:42,498][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:39:42,499][188188] Reward + Measures: [[-13.84139084   0.90810007   0.90640002   0.90009993   0.89200002
    3.99389529]
 [ 36.25695134   0.32750002   0.33750001   0.31800002   0.29029998
    3.76167941]
 [102.04521134   0.44549999   0.8276       0.82429999   0.30930004
    3.97866058]
 ...
 [  4.7331631    0.36950001   0.9224       0.92870009   0.55789995
    3.96760941]
 [-32.55290616   0.72080004   0.72090006   0.70340008   0.69690001
    3.98120165]
 [ 13.52069925   0.27239999   0.21900001   0.1979       0.0588
    3.9561398 ]][0m
[37m[1m[2023-07-03 02:39:42,499][188188] Max Reward on eval: 487.49939057761804[0m
[37m[1m[2023-07-03 02:39:42,499][188188] Min Reward on eval: -295.3038101332728[0m
[37m[1m[2023-07-03 02:39:42,499][188188] Mean Reward across all agents: -7.1382521918557345[0m
[37m[1m[2023-07-03 02:39:42,500][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:39:42,502][188188] mean_value=-705.8130094069927, max_value=105.72727725821494[0m
[37m[1m[2023-07-03 02:39:42,504][188188] New mean coefficients: [[ 0.10627182  0.52399945 -0.42874587  1.6621779   0.41901505 -2.6938415 ]][0m
[37m[1m[2023-07-03 02:39:42,505][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:39:51,483][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 02:39:51,484][188188] FPS: 427773.99[0m
[36m[2023-07-03 02:39:51,486][188188] itr=233, itrs=2000, Progress: 11.65%[0m
[36m[2023-07-03 02:40:03,021][188188] train() took 11.51 seconds to complete[0m
[36m[2023-07-03 02:40:03,021][188188] FPS: 333552.68[0m
[36m[2023-07-03 02:40:07,360][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:40:07,361][188188] Reward + Measures: [[231.5870253    0.81690931   0.69468129   0.78254497   0.29932466
    3.66804814]][0m
[37m[1m[2023-07-03 02:40:07,361][188188] Max Reward on eval: 231.58702530097818[0m
[37m[1m[2023-07-03 02:40:07,361][188188] Min Reward on eval: 231.58702530097818[0m
[37m[1m[2023-07-03 02:40:07,361][188188] Mean Reward across all agents: 231.58702530097818[0m
[37m[1m[2023-07-03 02:40:07,362][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:40:12,434][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:40:12,434][188188] Reward + Measures: [[ -39.53869294    0.89130002    0.787         0.87760001    0.08400001
     3.99675155]
 [ -86.62619207    0.98660004    0.96849996    0.97609997    0.0075
     3.99824882]
 [ -90.50079931    0.79299992    0.59059995    0.77520007    0.18800001
     3.96840334]
 ...
 [ -85.93454984    0.79010004    0.77760005    0.78460002    0.0252
     3.97272086]
 [-296.63116646    0.85480005    0.77880001    0.77700001    0.0183
     3.99312449]
 [   1.48490419    0.98019999    0.9702        0.97489995    0.0103
     3.99821591]][0m
[37m[1m[2023-07-03 02:40:12,434][188188] Max Reward on eval: 440.4626282946207[0m
[37m[1m[2023-07-03 02:40:12,435][188188] Min Reward on eval: -322.2435312151909[0m
[37m[1m[2023-07-03 02:40:12,435][188188] Mean Reward across all agents: -24.90415766999271[0m
[37m[1m[2023-07-03 02:40:12,435][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:40:12,437][188188] mean_value=-562.5333445467588, max_value=180.9147804291655[0m
[37m[1m[2023-07-03 02:40:12,440][188188] New mean coefficients: [[ 0.32312006 -0.7410556  -0.8119104   1.3059156   0.4153693  -1.2135965 ]][0m
[37m[1m[2023-07-03 02:40:12,441][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:40:21,491][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 02:40:21,492][188188] FPS: 424349.73[0m
[36m[2023-07-03 02:40:21,494][188188] itr=234, itrs=2000, Progress: 11.70%[0m
[36m[2023-07-03 02:40:33,315][188188] train() took 11.80 seconds to complete[0m
[36m[2023-07-03 02:40:33,315][188188] FPS: 325402.86[0m
[36m[2023-07-03 02:40:37,589][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:40:37,590][188188] Reward + Measures: [[313.45123493   0.86605197   0.71941096   0.83073968   0.15473199
    3.7024405 ]][0m
[37m[1m[2023-07-03 02:40:37,590][188188] Max Reward on eval: 313.45123493048163[0m
[37m[1m[2023-07-03 02:40:37,590][188188] Min Reward on eval: 313.45123493048163[0m
[37m[1m[2023-07-03 02:40:37,590][188188] Mean Reward across all agents: 313.45123493048163[0m
[37m[1m[2023-07-03 02:40:37,591][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:40:42,674][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:40:42,675][188188] Reward + Measures: [[ 20.13781146   0.13340001   0.21820001   0.22090001   0.27110001
    3.76941228]
 [-84.90000741   0.80310005   0.82089996   0.81570005   0.81560004
    3.94795227]
 [-20.69402742   0.78470004   0.97649997   0.97830003   0.19860001
    3.93461871]
 ...
 [ 34.22696619   0.70850003   0.76899999   0.74540001   0.0953
    3.8843503 ]
 [ 11.66338942   0.0002       0.99220002   0.9835       0.98979998
    3.99668622]
 [ 94.53285439   0.0962       0.98140001   0.96469992   0.89029998
    3.87465477]][0m
[37m[1m[2023-07-03 02:40:42,675][188188] Max Reward on eval: 402.7671470314264[0m
[37m[1m[2023-07-03 02:40:42,675][188188] Min Reward on eval: -311.79167273389174[0m
[37m[1m[2023-07-03 02:40:42,675][188188] Mean Reward across all agents: -1.4343399160558132[0m
[37m[1m[2023-07-03 02:40:42,676][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:40:42,678][188188] mean_value=-656.3716990294033, max_value=387.2921536264436[0m
[37m[1m[2023-07-03 02:40:42,680][188188] New mean coefficients: [[ 0.42517212 -0.5569581  -1.6681957   2.5896192   0.59696996 -0.75579065]][0m
[37m[1m[2023-07-03 02:40:42,681][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:40:51,669][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 02:40:51,669][188188] FPS: 427328.25[0m
[36m[2023-07-03 02:40:51,671][188188] itr=235, itrs=2000, Progress: 11.75%[0m
[36m[2023-07-03 02:41:03,674][188188] train() took 11.98 seconds to complete[0m
[36m[2023-07-03 02:41:03,674][188188] FPS: 320487.14[0m
[36m[2023-07-03 02:41:07,910][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:41:07,911][188188] Reward + Measures: [[-14.48343212   0.92945606   0.93379867   0.93222433   0.9289
    3.9922657 ]][0m
[37m[1m[2023-07-03 02:41:07,911][188188] Max Reward on eval: -14.483432121589901[0m
[37m[1m[2023-07-03 02:41:07,911][188188] Min Reward on eval: -14.483432121589901[0m
[37m[1m[2023-07-03 02:41:07,911][188188] Mean Reward across all agents: -14.483432121589901[0m
[37m[1m[2023-07-03 02:41:07,912][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:41:12,907][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:41:12,908][188188] Reward + Measures: [[  8.05364828   0.97049999   0.93979996   0.9637       0.0229
    3.97038507]
 [-30.26892875   0.91230005   0.91530001   0.92509997   0.93269998
    3.99586678]
 [-60.27718113   0.68260002   0.35399997   0.65630001   0.29260001
    3.97916293]
 ...
 [-53.5049061    0.82250005   0.84089994   0.8351       0.8089
    3.72576904]
 [  5.6134013    0.81849998   0.80270004   0.79040003   0.79350001
    3.99668813]
 [ -6.48383919   0.70409995   0.57410002   0.67570007   0.12639999
    3.82159424]][0m
[37m[1m[2023-07-03 02:41:12,908][188188] Max Reward on eval: 459.39847653773614[0m
[37m[1m[2023-07-03 02:41:12,908][188188] Min Reward on eval: -481.83333253115416[0m
[37m[1m[2023-07-03 02:41:12,909][188188] Mean Reward across all agents: -16.055937345978577[0m
[37m[1m[2023-07-03 02:41:12,909][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:41:12,911][188188] mean_value=-379.7878418577899, max_value=223.68314034770046[0m
[37m[1m[2023-07-03 02:41:12,913][188188] New mean coefficients: [[ 0.9339397 -1.3905697 -1.2851725  1.796452   1.166523  -1.2974391]][0m
[37m[1m[2023-07-03 02:41:12,914][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:41:21,854][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 02:41:21,854][188188] FPS: 429599.71[0m
[36m[2023-07-03 02:41:21,857][188188] itr=236, itrs=2000, Progress: 11.80%[0m
[36m[2023-07-03 02:41:33,409][188188] train() took 11.53 seconds to complete[0m
[36m[2023-07-03 02:41:33,409][188188] FPS: 332986.25[0m
[36m[2023-07-03 02:41:37,635][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:41:37,640][188188] Reward + Measures: [[-10.28576527   0.933164     0.9361847    0.93768734   0.93727326
    3.99061608]][0m
[37m[1m[2023-07-03 02:41:37,641][188188] Max Reward on eval: -10.285765270174956[0m
[37m[1m[2023-07-03 02:41:37,641][188188] Min Reward on eval: -10.285765270174956[0m
[37m[1m[2023-07-03 02:41:37,641][188188] Mean Reward across all agents: -10.285765270174956[0m
[37m[1m[2023-07-03 02:41:37,642][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:41:42,535][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:41:42,536][188188] Reward + Measures: [[ -38.21952511    0.1648        0.60149997    0.47119999    0.60220003
     3.7398355 ]
 [-162.00605396    0.86230004    0.82440007    0.8531        0.023
     3.97977805]
 [ -43.08574944    0.85820001    0.87739992    0.8860001     0.8585
     3.95320487]
 ...
 [  88.61008875    0.18440001    0.28330001    0.31759998    0.2198
     3.78941727]
 [ -40.29839302    0.29370001    0.37059999    0.35279998    0.35350001
     3.51350904]
 [-179.65769506    0.78259999    0.84930003    0.8132        0.85480005
     3.96866226]][0m
[37m[1m[2023-07-03 02:41:42,536][188188] Max Reward on eval: 579.8289752291515[0m
[37m[1m[2023-07-03 02:41:42,536][188188] Min Reward on eval: -442.95580292604865[0m
[37m[1m[2023-07-03 02:41:42,537][188188] Mean Reward across all agents: -40.90780224953183[0m
[37m[1m[2023-07-03 02:41:42,537][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:41:42,539][188188] mean_value=-528.9756631998558, max_value=54.328443092018915[0m
[37m[1m[2023-07-03 02:41:42,542][188188] New mean coefficients: [[ 0.6252682  -1.4372146  -1.5594033   0.36001396  0.44083536 -0.57032543]][0m
[37m[1m[2023-07-03 02:41:42,543][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:41:51,481][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 02:41:51,481][188188] FPS: 429694.90[0m
[36m[2023-07-03 02:41:51,483][188188] itr=237, itrs=2000, Progress: 11.85%[0m
[36m[2023-07-03 02:42:03,031][188188] train() took 11.53 seconds to complete[0m
[36m[2023-07-03 02:42:03,032][188188] FPS: 333067.84[0m
[36m[2023-07-03 02:42:07,313][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:42:07,314][188188] Reward + Measures: [[-12.14756127   0.90120065   0.9034133    0.90691668   0.90779036
    3.99274158]][0m
[37m[1m[2023-07-03 02:42:07,314][188188] Max Reward on eval: -12.147561269974176[0m
[37m[1m[2023-07-03 02:42:07,314][188188] Min Reward on eval: -12.147561269974176[0m
[37m[1m[2023-07-03 02:42:07,315][188188] Mean Reward across all agents: -12.147561269974176[0m
[37m[1m[2023-07-03 02:42:07,315][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:42:12,290][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:42:12,290][188188] Reward + Measures: [[ 195.63667203    0.95979995    0.85260004    0.93050003    0.0201
     3.82811284]
 [ -19.8225518     0.89600003    0.89460003    0.89580005    0.89440006
     3.99279976]
 [   7.97169042    0.28700003    0.77590001    0.42360002    0.79330009
     3.86763167]
 ...
 [ -32.42622139    0.14670001    0.91390002    0.91189998    0.80130005
     3.7577827 ]
 [ -14.77444887    0.88500005    0.90889996    0.90710002    0.89140004
     3.98663592]
 [-181.71067932    0.29910001    0.78220004    0.41319999    0.78260005
     3.99060869]][0m
[37m[1m[2023-07-03 02:42:12,291][188188] Max Reward on eval: 575.3270169685595[0m
[37m[1m[2023-07-03 02:42:12,291][188188] Min Reward on eval: -316.56284286086446[0m
[37m[1m[2023-07-03 02:42:12,291][188188] Mean Reward across all agents: -21.431899518326038[0m
[37m[1m[2023-07-03 02:42:12,291][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:42:12,293][188188] mean_value=-576.8680411977401, max_value=-86.07027111684472[0m
[36m[2023-07-03 02:42:12,295][188188] XNES is restarting with a new solution whose measures are [0.45180002 0.81140006 0.73870003 0.4244     3.7847321 ] and objective is 306.59981199819595[0m
[36m[2023-07-03 02:42:12,296][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 02:42:12,298][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 02:42:12,299][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:42:21,319][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 02:42:21,320][188188] FPS: 425784.06[0m
[36m[2023-07-03 02:42:21,322][188188] itr=238, itrs=2000, Progress: 11.90%[0m
[36m[2023-07-03 02:42:33,078][188188] train() took 11.73 seconds to complete[0m
[36m[2023-07-03 02:42:33,078][188188] FPS: 327230.10[0m
[36m[2023-07-03 02:42:37,329][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:42:37,329][188188] Reward + Measures: [[-142.79087422    0.61948967    0.84119767    0.85213834    0.26195732
     3.99535584]][0m
[37m[1m[2023-07-03 02:42:37,330][188188] Max Reward on eval: -142.7908742209873[0m
[37m[1m[2023-07-03 02:42:37,330][188188] Min Reward on eval: -142.7908742209873[0m
[37m[1m[2023-07-03 02:42:37,330][188188] Mean Reward across all agents: -142.7908742209873[0m
[37m[1m[2023-07-03 02:42:37,330][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:42:42,465][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:42:42,466][188188] Reward + Measures: [[-101.13863628    0.29620001    0.98009998    0.97679996    0.69119996
     3.99939966]
 [-230.12495926    0.72579998    0.70479995    0.75540006    0.0275
     3.98429084]
 [  -4.26464192    0.69550002    0.87930006    0.91610003    0.20640002
     3.99754953]
 ...
 [-221.4628765     0.73329997    0.78130001    0.78969997    0.12639999
     3.99554873]
 [ -69.97961125    0.48199996    0.96210003    0.9655        0.49460003
     3.97985721]
 [-196.0562348     0.60109997    0.6929        0.86650002    0.1168
     3.99798274]][0m
[37m[1m[2023-07-03 02:42:42,466][188188] Max Reward on eval: 519.1914578403346[0m
[37m[1m[2023-07-03 02:42:42,466][188188] Min Reward on eval: -378.9567165532615[0m
[37m[1m[2023-07-03 02:42:42,466][188188] Mean Reward across all agents: -65.4087826631926[0m
[37m[1m[2023-07-03 02:42:42,467][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:42:42,469][188188] mean_value=-459.69917592551985, max_value=194.81433492611205[0m
[37m[1m[2023-07-03 02:42:42,472][188188] New mean coefficients: [[-1.6778696  -0.95301193 -0.2178942  -1.9240571  -1.1660559  -0.9648471 ]][0m
[37m[1m[2023-07-03 02:42:42,473][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:42:51,536][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 02:42:51,536][188188] FPS: 423752.78[0m
[36m[2023-07-03 02:42:51,539][188188] itr=239, itrs=2000, Progress: 11.95%[0m
[36m[2023-07-03 02:43:03,152][188188] train() took 11.59 seconds to complete[0m
[36m[2023-07-03 02:43:03,152][188188] FPS: 331288.41[0m
[36m[2023-07-03 02:43:07,376][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:43:07,376][188188] Reward + Measures: [[-123.19984134    0.54764736    0.87732297    0.88393527    0.36283633
     3.99501562]][0m
[37m[1m[2023-07-03 02:43:07,376][188188] Max Reward on eval: -123.19984134319964[0m
[37m[1m[2023-07-03 02:43:07,377][188188] Min Reward on eval: -123.19984134319964[0m
[37m[1m[2023-07-03 02:43:07,377][188188] Mean Reward across all agents: -123.19984134319964[0m
[37m[1m[2023-07-03 02:43:07,377][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:43:12,333][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:43:12,333][188188] Reward + Measures: [[  1.71780728   0.88759995   0.96870005   0.98219997   0.10109999
    3.99963641]
 [102.69851557   0.44409999   0.71430004   0.75510001   0.30899999
    3.95327926]
 [-19.68096408   0.29640001   0.96970004   0.97770005   0.69390005
    3.99932742]
 ...
 [-80.78184611   0.50029999   0.88380003   0.90929997   0.40279999
    3.99598098]
 [-34.32372124   0.88940001   0.9601       0.98289996   0.10179999
    3.99970865]
 [-15.60790405   0.59120005   0.97620004   0.98120004   0.39610001
    3.99952245]][0m
[37m[1m[2023-07-03 02:43:12,334][188188] Max Reward on eval: 733.2940943328198[0m
[37m[1m[2023-07-03 02:43:12,334][188188] Min Reward on eval: -324.3183717453852[0m
[37m[1m[2023-07-03 02:43:12,334][188188] Mean Reward across all agents: 14.498319775382175[0m
[37m[1m[2023-07-03 02:43:12,334][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:43:12,337][188188] mean_value=-361.8711182586673, max_value=318.6407110674349[0m
[37m[1m[2023-07-03 02:43:12,339][188188] New mean coefficients: [[-1.7312561  -1.3375573   0.24973726 -0.97024536 -1.5540864  -0.995639  ]][0m
[37m[1m[2023-07-03 02:43:12,340][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:43:21,303][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 02:43:21,303][188188] FPS: 428549.77[0m
[36m[2023-07-03 02:43:21,305][188188] itr=240, itrs=2000, Progress: 12.00%[0m
[37m[1m[2023-07-03 02:43:24,029][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000220[0m
[36m[2023-07-03 02:43:36,026][188188] train() took 11.66 seconds to complete[0m
[36m[2023-07-03 02:43:36,027][188188] FPS: 329365.41[0m
[36m[2023-07-03 02:43:40,336][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:43:40,336][188188] Reward + Measures: [[-111.22776069    0.49508867    0.85970068    0.8708443     0.40146631
     3.99731612]][0m
[37m[1m[2023-07-03 02:43:40,337][188188] Max Reward on eval: -111.22776069160093[0m
[37m[1m[2023-07-03 02:43:40,337][188188] Min Reward on eval: -111.22776069160093[0m
[37m[1m[2023-07-03 02:43:40,337][188188] Mean Reward across all agents: -111.22776069160093[0m
[37m[1m[2023-07-03 02:43:40,337][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:43:45,347][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:43:45,348][188188] Reward + Measures: [[ 50.7005748    0.71320003   0.86790001   0.84969997   0.17529999
    3.97485232]
 [ 15.12125166   0.079        0.30850002   0.26809999   0.2647
    3.80229688]
 [ 31.67034738   0.59170002   0.96649998   0.98050004   0.39630002
    3.99920249]
 ...
 [ 95.51892533   0.07870001   0.44910002   0.34619999   0.39340001
    3.90064502]
 [-58.51438679   0.59030002   0.88310003   0.90990001   0.3055
    3.92175913]
 [-61.28800666   0.35729998   0.52790004   0.15860002   0.46869999
    3.77286696]][0m
[37m[1m[2023-07-03 02:43:45,348][188188] Max Reward on eval: 494.09669163396467[0m
[37m[1m[2023-07-03 02:43:45,348][188188] Min Reward on eval: -411.12780853463335[0m
[37m[1m[2023-07-03 02:43:45,348][188188] Mean Reward across all agents: -34.41577441898947[0m
[37m[1m[2023-07-03 02:43:45,349][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:43:45,351][188188] mean_value=-529.9915782080574, max_value=385.6882989902879[0m
[37m[1m[2023-07-03 02:43:45,354][188188] New mean coefficients: [[-2.6818745  -0.43532515  1.0079275  -0.5764234  -1.0649322  -0.917199  ]][0m
[37m[1m[2023-07-03 02:43:45,355][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:43:54,393][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 02:43:54,394][188188] FPS: 424932.59[0m
[36m[2023-07-03 02:43:54,396][188188] itr=241, itrs=2000, Progress: 12.05%[0m
[36m[2023-07-03 02:44:06,117][188188] train() took 11.70 seconds to complete[0m
[36m[2023-07-03 02:44:06,117][188188] FPS: 328187.42[0m
[36m[2023-07-03 02:44:10,375][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:44:10,375][188188] Reward + Measures: [[-78.52467842   0.40694499   0.86702096   0.87406105   0.4963153
    3.99769711]][0m
[37m[1m[2023-07-03 02:44:10,376][188188] Max Reward on eval: -78.52467842164971[0m
[37m[1m[2023-07-03 02:44:10,376][188188] Min Reward on eval: -78.52467842164971[0m
[37m[1m[2023-07-03 02:44:10,376][188188] Mean Reward across all agents: -78.52467842164971[0m
[37m[1m[2023-07-03 02:44:10,376][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:44:15,308][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:44:15,309][188188] Reward + Measures: [[ -54.05723938    0.32570001    0.80790007    0.72530001    0.52049994
     3.93819809]
 [ -82.36518154    0.252         0.79549998    0.78600007    0.6182
     3.99652934]
 [-201.97726851    0.49050003    0.97570008    0.96919996    0.48989996
     3.99421692]
 ...
 [  63.19302631    0.24720001    0.68810004    0.7277        0.51290005
     3.98061609]
 [-284.72220155    0.78329998    0.97469997    0.97760004    0.19950001
     3.99851418]
 [ -55.76203515    0.42160001    0.78730005    0.78060001    0.41019997
     3.97207999]][0m
[37m[1m[2023-07-03 02:44:15,309][188188] Max Reward on eval: 365.0440411737189[0m
[37m[1m[2023-07-03 02:44:15,309][188188] Min Reward on eval: -328.592039081268[0m
[37m[1m[2023-07-03 02:44:15,310][188188] Mean Reward across all agents: 8.712621178605682[0m
[37m[1m[2023-07-03 02:44:15,310][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:44:15,312][188188] mean_value=-509.4243339020053, max_value=582.2586179323225[0m
[37m[1m[2023-07-03 02:44:15,315][188188] New mean coefficients: [[-3.2633638  -0.19217214  1.4689603  -0.66350067 -1.2530851  -0.6758619 ]][0m
[37m[1m[2023-07-03 02:44:15,316][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:44:24,324][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 02:44:24,325][188188] FPS: 426332.12[0m
[36m[2023-07-03 02:44:24,327][188188] itr=242, itrs=2000, Progress: 12.10%[0m
[36m[2023-07-03 02:44:35,851][188188] train() took 11.51 seconds to complete[0m
[36m[2023-07-03 02:44:35,851][188188] FPS: 333758.05[0m
[36m[2023-07-03 02:44:40,135][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:44:40,136][188188] Reward + Measures: [[-88.16318938   0.44598135   0.844028     0.8609609    0.44091365
    3.99791002]][0m
[37m[1m[2023-07-03 02:44:40,136][188188] Max Reward on eval: -88.16318938431355[0m
[37m[1m[2023-07-03 02:44:40,136][188188] Min Reward on eval: -88.16318938431355[0m
[37m[1m[2023-07-03 02:44:40,136][188188] Mean Reward across all agents: -88.16318938431355[0m
[37m[1m[2023-07-03 02:44:40,137][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:44:45,167][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:44:45,167][188188] Reward + Measures: [[  81.20167673    0.30990002    0.43039998    0.4901        0.22000001
     3.8098886 ]
 [-207.21588448    0.55000001    0.68920004    0.72460002    0.23920003
     3.99364138]
 [   3.73191366    0.1267        0.80520004    0.8448        0.72179997
     3.99627757]
 ...
 [  58.49860644    0.5728001     0.67720002    0.68620002    0.20710002
     3.95033765]
 [ 127.07082758    0.29069999    0.85030001    0.85290003    0.60550004
     3.98105669]
 [-102.80780151    0.40170002    0.31560001    0.4835        0.0887
     3.99494624]][0m
[37m[1m[2023-07-03 02:44:45,168][188188] Max Reward on eval: 886.9210433874338[0m
[37m[1m[2023-07-03 02:44:45,168][188188] Min Reward on eval: -374.7816500466317[0m
[37m[1m[2023-07-03 02:44:45,168][188188] Mean Reward across all agents: -16.013534842722827[0m
[37m[1m[2023-07-03 02:44:45,168][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:44:45,171][188188] mean_value=-616.1983161796198, max_value=404.7392159935478[0m
[37m[1m[2023-07-03 02:44:45,173][188188] New mean coefficients: [[-3.675135    0.47196338  1.6020828  -0.76739746 -1.7069261  -0.5144085 ]][0m
[37m[1m[2023-07-03 02:44:45,174][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:44:54,149][188188] train() took 8.97 seconds to complete[0m
[36m[2023-07-03 02:44:54,149][188188] FPS: 427935.28[0m
[36m[2023-07-03 02:44:54,152][188188] itr=243, itrs=2000, Progress: 12.15%[0m
[36m[2023-07-03 02:45:05,777][188188] train() took 11.60 seconds to complete[0m
[36m[2023-07-03 02:45:05,777][188188] FPS: 330912.55[0m
[36m[2023-07-03 02:45:10,139][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:45:10,140][188188] Reward + Measures: [[-98.40800103   0.43488899   0.86248732   0.87422132   0.46815035
    3.99790525]][0m
[37m[1m[2023-07-03 02:45:10,140][188188] Max Reward on eval: -98.40800103220282[0m
[37m[1m[2023-07-03 02:45:10,140][188188] Min Reward on eval: -98.40800103220282[0m
[37m[1m[2023-07-03 02:45:10,140][188188] Mean Reward across all agents: -98.40800103220282[0m
[37m[1m[2023-07-03 02:45:10,141][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:45:15,254][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:45:15,255][188188] Reward + Measures: [[210.59657728   0.41999999   0.72670001   0.51310003   0.46750003
    3.94711494]
 [ 83.35869548   0.25620002   0.7626       0.47890002   0.63900006
    3.85964322]
 [ 61.01153133   0.24609999   0.78240001   0.81019992   0.62169999
    3.70294428]
 ...
 [ 55.53600012   0.1056       0.48319998   0.26860002   0.39820001
    3.78703928]
 [-50.67794592   0.18160002   0.75720006   0.76480001   0.62910002
    3.87883162]
 [-91.63129689   0.68870002   0.96789998   0.96379995   0.30070004
    3.99874949]][0m
[37m[1m[2023-07-03 02:45:15,255][188188] Max Reward on eval: 478.32312346131073[0m
[37m[1m[2023-07-03 02:45:15,256][188188] Min Reward on eval: -416.8826508557424[0m
[37m[1m[2023-07-03 02:45:15,256][188188] Mean Reward across all agents: -12.964370074490835[0m
[37m[1m[2023-07-03 02:45:15,256][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:45:15,258][188188] mean_value=-692.1137810543809, max_value=163.41990738117107[0m
[37m[1m[2023-07-03 02:45:15,261][188188] New mean coefficients: [[-2.5408664   0.6553173   1.5867404  -0.6882577  -2.0654364  -0.56656927]][0m
[37m[1m[2023-07-03 02:45:15,262][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:45:24,273][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 02:45:24,273][188188] FPS: 426215.27[0m
[36m[2023-07-03 02:45:24,276][188188] itr=244, itrs=2000, Progress: 12.20%[0m
[36m[2023-07-03 02:45:35,916][188188] train() took 11.62 seconds to complete[0m
[36m[2023-07-03 02:45:35,917][188188] FPS: 330506.05[0m
[36m[2023-07-03 02:45:40,210][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:45:40,211][188188] Reward + Measures: [[-78.149002     0.41905901   0.84508097   0.85781997   0.47033867
    3.99770236]][0m
[37m[1m[2023-07-03 02:45:40,211][188188] Max Reward on eval: -78.1490020049207[0m
[37m[1m[2023-07-03 02:45:40,211][188188] Min Reward on eval: -78.1490020049207[0m
[37m[1m[2023-07-03 02:45:40,212][188188] Mean Reward across all agents: -78.1490020049207[0m
[37m[1m[2023-07-03 02:45:40,212][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:45:45,206][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:45:45,206][188188] Reward + Measures: [[119.56066676   0.0586       0.73260003   0.60519999   0.70279998
    3.76991343]
 [ -9.26789632   0.51700002   0.87779999   0.87279999   0.40739998
    3.99781489]
 [-44.26577518   0.1714       0.81239998   0.74310005   0.68170005
    3.82768559]
 ...
 [ 15.26757694   0.60060006   0.4896       0.5029       0.0667
    3.98116374]
 [-31.52446078   0.0004       0.98069996   0.97749996   0.98870003
    3.98888326]
 [-63.07524262   0.19600001   0.9835       0.97240001   0.78850001
    3.99904251]][0m
[37m[1m[2023-07-03 02:45:45,206][188188] Max Reward on eval: 481.84521882217376[0m
[37m[1m[2023-07-03 02:45:45,207][188188] Min Reward on eval: -266.56114104250446[0m
[37m[1m[2023-07-03 02:45:45,207][188188] Mean Reward across all agents: 5.592191596645123[0m
[37m[1m[2023-07-03 02:45:45,207][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:45:45,209][188188] mean_value=-537.5163629205358, max_value=118.30450903706412[0m
[37m[1m[2023-07-03 02:45:45,211][188188] New mean coefficients: [[-2.584964    0.76884925  1.7669319  -1.2458519  -1.7504523  -0.67550427]][0m
[37m[1m[2023-07-03 02:45:45,212][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:45:54,218][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 02:45:54,219][188188] FPS: 426459.54[0m
[36m[2023-07-03 02:45:54,221][188188] itr=245, itrs=2000, Progress: 12.25%[0m
[36m[2023-07-03 02:46:05,803][188188] train() took 11.56 seconds to complete[0m
[36m[2023-07-03 02:46:05,803][188188] FPS: 332199.00[0m
[36m[2023-07-03 02:46:10,056][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:46:10,056][188188] Reward + Measures: [[-92.95429167   0.57443166   0.77244765   0.78958267   0.26653633
    3.99683666]][0m
[37m[1m[2023-07-03 02:46:10,057][188188] Max Reward on eval: -92.95429167296214[0m
[37m[1m[2023-07-03 02:46:10,057][188188] Min Reward on eval: -92.95429167296214[0m
[37m[1m[2023-07-03 02:46:10,057][188188] Mean Reward across all agents: -92.95429167296214[0m
[37m[1m[2023-07-03 02:46:10,057][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:46:15,075][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:46:15,075][188188] Reward + Measures: [[-111.43455532    0.91820002    0.86210006    0.86809999    0.0285
     3.99404454]
 [ 206.41758753    0.44829997    0.70679998    0.53509998    0.42880002
     3.96099782]
 [ 355.98977754    0.45520002    0.61750001    0.58859998    0.29260001
     3.93029261]
 ...
 [-180.90089887    0.42390004    0.88640004    0.87980002    0.51089996
     3.98789525]
 [-158.19254645    0.52039999    0.88260001    0.87890005    0.4111
     3.99767542]
 [  59.04166346    0.0431        0.79800004    0.71250004    0.77689999
     3.83953404]][0m
[37m[1m[2023-07-03 02:46:15,075][188188] Max Reward on eval: 516.460136185214[0m
[37m[1m[2023-07-03 02:46:15,076][188188] Min Reward on eval: -358.44254676494745[0m
[37m[1m[2023-07-03 02:46:15,076][188188] Mean Reward across all agents: -10.460941916515624[0m
[37m[1m[2023-07-03 02:46:15,076][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:46:15,078][188188] mean_value=-1279.4057129808818, max_value=176.75463767212415[0m
[37m[1m[2023-07-03 02:46:15,081][188188] New mean coefficients: [[-2.822085    0.88040566  1.0653288  -0.6796339  -2.0803099  -1.4938052 ]][0m
[37m[1m[2023-07-03 02:46:15,082][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:46:24,083][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 02:46:24,083][188188] FPS: 426684.82[0m
[36m[2023-07-03 02:46:24,086][188188] itr=246, itrs=2000, Progress: 12.30%[0m
[36m[2023-07-03 02:46:35,809][188188] train() took 11.70 seconds to complete[0m
[36m[2023-07-03 02:46:35,810][188188] FPS: 328106.32[0m
[36m[2023-07-03 02:46:40,074][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:46:40,075][188188] Reward + Measures: [[-73.17984034   0.42105702   0.79102165   0.81652361   0.42541203
    3.99715948]][0m
[37m[1m[2023-07-03 02:46:40,075][188188] Max Reward on eval: -73.17984034049316[0m
[37m[1m[2023-07-03 02:46:40,075][188188] Min Reward on eval: -73.17984034049316[0m
[37m[1m[2023-07-03 02:46:40,076][188188] Mean Reward across all agents: -73.17984034049316[0m
[37m[1m[2023-07-03 02:46:40,076][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:46:45,040][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:46:45,040][188188] Reward + Measures: [[ 107.19418672    0.87449998    0.55239999    0.87270004    0.2703
     3.8552773 ]
 [ -63.07345376    0.53369999    0.69540006    0.75080007    0.23150001
     3.99576426]
 [-177.59527088    0.62010002    0.87220001    0.8707        0.31079999
     3.99663925]
 ...
 [ 113.69944285    0.40869999    0.7256        0.75310004    0.4138
     3.87106252]
 [ 134.79446064    0.3062        0.58840001    0.63210005    0.38040003
     3.96276641]
 [  47.21762557    0.1164        0.2395        0.29040003    0.21139999
     3.9266603 ]][0m
[37m[1m[2023-07-03 02:46:45,040][188188] Max Reward on eval: 439.6040885626338[0m
[37m[1m[2023-07-03 02:46:45,041][188188] Min Reward on eval: -312.92101674228905[0m
[37m[1m[2023-07-03 02:46:45,041][188188] Mean Reward across all agents: -13.36651934987369[0m
[37m[1m[2023-07-03 02:46:45,041][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:46:45,043][188188] mean_value=-805.3855496011578, max_value=74.43217337394023[0m
[37m[1m[2023-07-03 02:46:45,046][188188] New mean coefficients: [[-3.7477138  0.3521508  1.6840429 -0.5278161 -1.8145556 -1.4719428]][0m
[37m[1m[2023-07-03 02:46:45,046][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:46:54,027][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 02:46:54,027][188188] FPS: 427670.24[0m
[36m[2023-07-03 02:46:54,029][188188] itr=247, itrs=2000, Progress: 12.35%[0m
[36m[2023-07-03 02:47:05,610][188188] train() took 11.56 seconds to complete[0m
[36m[2023-07-03 02:47:05,611][188188] FPS: 332122.67[0m
[36m[2023-07-03 02:47:09,914][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:47:09,920][188188] Reward + Measures: [[-81.79863666   0.39516798   0.81173092   0.82649666   0.47137132
    3.99654365]][0m
[37m[1m[2023-07-03 02:47:09,920][188188] Max Reward on eval: -81.79863666462983[0m
[37m[1m[2023-07-03 02:47:09,921][188188] Min Reward on eval: -81.79863666462983[0m
[37m[1m[2023-07-03 02:47:09,921][188188] Mean Reward across all agents: -81.79863666462983[0m
[37m[1m[2023-07-03 02:47:09,921][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:47:15,013][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:47:15,020][188188] Reward + Measures: [[ -79.96988101    0.28480002    0.69200003    0.67799997    0.52220005
     3.9822948 ]
 [ -72.81774639    0.3567        0.69160002    0.72480005    0.43290001
     3.99615669]
 [ -26.07202725    0.90999997    0.91489995    0.90280002    0.90410006
     3.98835444]
 ...
 [ -76.43089855    0.19630001    0.98299998    0.963         0.7841
     3.96073532]
 [  33.98892182    0.0001        0.98859996    0.97650003    0.98680001
     3.99971318]
 [-212.32972648    0.49260002    0.87709999    0.92999995    0.3998
     3.99639773]][0m
[37m[1m[2023-07-03 02:47:15,021][188188] Max Reward on eval: 670.3000473929569[0m
[37m[1m[2023-07-03 02:47:15,021][188188] Min Reward on eval: -284.62508776029574[0m
[37m[1m[2023-07-03 02:47:15,022][188188] Mean Reward across all agents: -7.647040796102968[0m
[37m[1m[2023-07-03 02:47:15,022][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:47:15,027][188188] mean_value=-683.381613059651, max_value=168.46393857930923[0m
[37m[1m[2023-07-03 02:47:15,031][188188] New mean coefficients: [[-3.8643575   0.02799362  3.3658671   0.08814347 -1.6181333  -1.255134  ]][0m
[37m[1m[2023-07-03 02:47:15,033][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:47:23,985][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 02:47:23,985][188188] FPS: 429052.87[0m
[36m[2023-07-03 02:47:23,988][188188] itr=248, itrs=2000, Progress: 12.40%[0m
[36m[2023-07-03 02:47:35,623][188188] train() took 11.62 seconds to complete[0m
[36m[2023-07-03 02:47:35,624][188188] FPS: 330544.96[0m
[36m[2023-07-03 02:47:39,841][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:47:39,846][188188] Reward + Measures: [[-91.99202443   0.48505664   0.79948264   0.8317287    0.36678165
    3.99701953]][0m
[37m[1m[2023-07-03 02:47:39,846][188188] Max Reward on eval: -91.99202443166081[0m
[37m[1m[2023-07-03 02:47:39,847][188188] Min Reward on eval: -91.99202443166081[0m
[37m[1m[2023-07-03 02:47:39,847][188188] Mean Reward across all agents: -91.99202443166081[0m
[37m[1m[2023-07-03 02:47:39,847][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:47:44,816][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:47:44,816][188188] Reward + Measures: [[  57.98574147    0.0673        0.58670002    0.49310002    0.53820002
     3.87703562]
 [  47.98970986    0.1355        0.7712        0.62159997    0.6735
     3.88260436]
 [  -6.77907285    0.0287        0.89050001    0.87309998    0.89880002
     3.98895621]
 ...
 [  78.56541777    0.0241        0.83249998    0.62350005    0.75029999
     3.66986442]
 [  52.84173987    0.52410001    0.55269998    0.0383        0.51940006
     3.97935605]
 [-134.91700867    0.59110004    0.98280001    0.97839993    0.39269999
     3.99897313]][0m
[37m[1m[2023-07-03 02:47:44,817][188188] Max Reward on eval: 611.4078554287553[0m
[37m[1m[2023-07-03 02:47:44,817][188188] Min Reward on eval: -268.76642053462564[0m
[37m[1m[2023-07-03 02:47:44,817][188188] Mean Reward across all agents: 6.547416509255077[0m
[37m[1m[2023-07-03 02:47:44,817][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:47:44,819][188188] mean_value=-1687.485333121336, max_value=480.87313910871745[0m
[37m[1m[2023-07-03 02:47:44,822][188188] New mean coefficients: [[-2.637609    1.7376342   2.5892742  -0.59133536 -1.8090799  -1.328227  ]][0m
[37m[1m[2023-07-03 02:47:44,823][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:47:53,789][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 02:47:53,789][188188] FPS: 428338.40[0m
[36m[2023-07-03 02:47:53,792][188188] itr=249, itrs=2000, Progress: 12.45%[0m
[36m[2023-07-03 02:48:05,346][188188] train() took 11.54 seconds to complete[0m
[36m[2023-07-03 02:48:05,346][188188] FPS: 332885.62[0m
[36m[2023-07-03 02:48:09,632][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:48:09,633][188188] Reward + Measures: [[-93.3006529    0.49505135   0.80252796   0.84284961   0.35719568
    3.99725103]][0m
[37m[1m[2023-07-03 02:48:09,633][188188] Max Reward on eval: -93.30065289958165[0m
[37m[1m[2023-07-03 02:48:09,633][188188] Min Reward on eval: -93.30065289958165[0m
[37m[1m[2023-07-03 02:48:09,634][188188] Mean Reward across all agents: -93.30065289958165[0m
[37m[1m[2023-07-03 02:48:09,634][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:48:14,657][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:48:14,657][188188] Reward + Measures: [[ 153.75770939    0.1322        0.76609999    0.74830002    0.70740002
     3.93403101]
 [-124.70359072    0.36300001    0.78839999    0.77060002    0.51810002
     3.93043566]
 [ -66.08966637    0.72509998    0.77960002    0.80910009    0.1303
     3.99654746]
 ...
 [ -87.73492391    0.54509997    0.70609999    0.74609995    0.2228
     3.99795318]
 [-129.86324454    0.3378        0.792         0.80000001    0.51859999
     3.99332666]
 [ 127.56369037    0.65499997    0.39670005    0.64199996    0.22819999
     3.97969747]][0m
[37m[1m[2023-07-03 02:48:14,657][188188] Max Reward on eval: 628.0055298790336[0m
[37m[1m[2023-07-03 02:48:14,658][188188] Min Reward on eval: -351.85467099864036[0m
[37m[1m[2023-07-03 02:48:14,658][188188] Mean Reward across all agents: -7.178914402618235[0m
[37m[1m[2023-07-03 02:48:14,658][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:48:14,660][188188] mean_value=-785.5542250459372, max_value=133.67086502866113[0m
[37m[1m[2023-07-03 02:48:14,663][188188] New mean coefficients: [[-2.7072163  1.5790265  2.0468311 -0.5672744 -1.447482  -1.1771967]][0m
[37m[1m[2023-07-03 02:48:14,664][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:48:23,631][188188] train() took 8.97 seconds to complete[0m
[36m[2023-07-03 02:48:23,631][188188] FPS: 428279.86[0m
[36m[2023-07-03 02:48:23,634][188188] itr=250, itrs=2000, Progress: 12.50%[0m
[37m[1m[2023-07-03 02:48:26,365][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000230[0m
[36m[2023-07-03 02:48:38,272][188188] train() took 11.58 seconds to complete[0m
[36m[2023-07-03 02:48:38,272][188188] FPS: 331708.01[0m
[36m[2023-07-03 02:48:42,554][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:48:42,555][188188] Reward + Measures: [[-73.99759192   0.43711597   0.81404734   0.83291566   0.4292447
    3.99683595]][0m
[37m[1m[2023-07-03 02:48:42,555][188188] Max Reward on eval: -73.99759191987894[0m
[37m[1m[2023-07-03 02:48:42,555][188188] Min Reward on eval: -73.99759191987894[0m
[37m[1m[2023-07-03 02:48:42,555][188188] Mean Reward across all agents: -73.99759191987894[0m
[37m[1m[2023-07-03 02:48:42,555][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:48:47,586][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:48:47,586][188188] Reward + Measures: [[-11.52239912   0.0746       0.11600001   0.1078       0.12330001
    3.81859374]
 [141.1632954    0.27620003   0.52310008   0.44470006   0.2052
    3.87993693]
 [ 32.44989562   0.1099       0.1937       0.18799999   0.15390001
    3.91991353]
 ...
 [107.84676017   0.12820001   0.62219995   0.69150001   0.71180004
    3.97188687]
 [ 14.63847611   0.17570001   0.43140003   0.51450002   0.33570001
    3.98734474]
 [-80.30466995   0.0702       0.1358       0.1038       0.0689
    3.45158315]][0m
[37m[1m[2023-07-03 02:48:47,586][188188] Max Reward on eval: 579.4870739377104[0m
[37m[1m[2023-07-03 02:48:47,587][188188] Min Reward on eval: -311.3140497589484[0m
[37m[1m[2023-07-03 02:48:47,587][188188] Mean Reward across all agents: -17.042730898679213[0m
[37m[1m[2023-07-03 02:48:47,587][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:48:47,589][188188] mean_value=-1435.4959652444836, max_value=132.91866021496864[0m
[37m[1m[2023-07-03 02:48:47,591][188188] New mean coefficients: [[-1.9516302   2.1525824   0.5393572  -0.5587588  -1.7800176   0.09433615]][0m
[37m[1m[2023-07-03 02:48:47,592][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:48:56,604][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 02:48:56,605][188188] FPS: 426170.76[0m
[36m[2023-07-03 02:48:56,607][188188] itr=251, itrs=2000, Progress: 12.55%[0m
[36m[2023-07-03 02:49:08,292][188188] train() took 11.66 seconds to complete[0m
[36m[2023-07-03 02:49:08,292][188188] FPS: 329203.34[0m
[36m[2023-07-03 02:49:12,546][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:49:12,547][188188] Reward + Measures: [[-25.91178161   0.33421999   0.84510267   0.85237938   0.553909
    3.99735308]][0m
[37m[1m[2023-07-03 02:49:12,547][188188] Max Reward on eval: -25.911781614804205[0m
[37m[1m[2023-07-03 02:49:12,547][188188] Min Reward on eval: -25.911781614804205[0m
[37m[1m[2023-07-03 02:49:12,547][188188] Mean Reward across all agents: -25.911781614804205[0m
[37m[1m[2023-07-03 02:49:12,548][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:49:17,710][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:49:17,711][188188] Reward + Measures: [[-122.31302842    0.59040004    0.96829998    0.96020001    0.38769999
     3.99755979]
 [  48.41452344    0.68059999    0.68690002    0.68650001    0.0605
     3.87406516]
 [ 123.46888767    0.57339996    0.6925        0.68900007    0.23800002
     3.99106765]
 ...
 [ -15.61384883    0.31980002    0.49450001    0.52700001    0.26250002
     3.97267699]
 [ -46.44989088    0.1007        0.89279997    0.92439997    0.79630005
     3.99915195]
 [  85.32873121    0.30230001    0.73069996    0.74710006    0.45210001
     3.985672  ]][0m
[37m[1m[2023-07-03 02:49:17,711][188188] Max Reward on eval: 552.9694951152953[0m
[37m[1m[2023-07-03 02:49:17,711][188188] Min Reward on eval: -248.33915331205353[0m
[37m[1m[2023-07-03 02:49:17,711][188188] Mean Reward across all agents: -6.195490137044014[0m
[37m[1m[2023-07-03 02:49:17,712][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:49:17,714][188188] mean_value=-753.111812934317, max_value=167.18643345251417[0m
[37m[1m[2023-07-03 02:49:17,716][188188] New mean coefficients: [[-1.9662911   2.5656903   1.0129759  -0.04045898 -2.319851    0.18709219]][0m
[37m[1m[2023-07-03 02:49:17,717][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:49:26,728][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 02:49:26,728][188188] FPS: 426242.33[0m
[36m[2023-07-03 02:49:26,730][188188] itr=252, itrs=2000, Progress: 12.60%[0m
[36m[2023-07-03 02:49:38,504][188188] train() took 11.75 seconds to complete[0m
[36m[2023-07-03 02:49:38,504][188188] FPS: 326680.00[0m
[36m[2023-07-03 02:49:42,806][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:49:42,806][188188] Reward + Measures: [[7.06157323 0.13394    0.93886232 0.93624169 0.8175174  3.9985106 ]][0m
[37m[1m[2023-07-03 02:49:42,807][188188] Max Reward on eval: 7.061573225937939[0m
[37m[1m[2023-07-03 02:49:42,807][188188] Min Reward on eval: 7.061573225937939[0m
[37m[1m[2023-07-03 02:49:42,807][188188] Mean Reward across all agents: 7.061573225937939[0m
[37m[1m[2023-07-03 02:49:42,807][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:49:47,896][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:49:47,897][188188] Reward + Measures: [[ 17.61245119   0.19520001   0.97090006   0.96439999   0.78350002
    3.93720508]
 [ 92.05487171   0.33330002   0.69370002   0.81510001   0.42439994
    3.99892116]
 [ 36.93503275   0.0997       0.9903       0.9788       0.88840002
    3.99884844]
 ...
 [ 18.14398924   0.09170001   0.2227       0.1628       0.20079999
    3.90368581]
 [150.05653373   0.27220002   0.86300004   0.8466       0.61250001
    3.98136783]
 [-13.56524067   0.0002       0.99080002   0.97939998   0.98610002
    3.99881053]][0m
[37m[1m[2023-07-03 02:49:47,897][188188] Max Reward on eval: 261.1351573091466[0m
[37m[1m[2023-07-03 02:49:47,897][188188] Min Reward on eval: -294.9363312714733[0m
[37m[1m[2023-07-03 02:49:47,897][188188] Mean Reward across all agents: 15.899486806579489[0m
[37m[1m[2023-07-03 02:49:47,898][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:49:47,899][188188] mean_value=-914.0389929367221, max_value=130.42469307966368[0m
[37m[1m[2023-07-03 02:49:47,902][188188] New mean coefficients: [[-1.7192036   3.2171383   1.2256093  -0.67586124 -3.0079885  -1.503499  ]][0m
[37m[1m[2023-07-03 02:49:47,903][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:49:56,949][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 02:49:56,949][188188] FPS: 424573.61[0m
[36m[2023-07-03 02:49:56,951][188188] itr=253, itrs=2000, Progress: 12.65%[0m
[36m[2023-07-03 02:50:08,755][188188] train() took 11.78 seconds to complete[0m
[36m[2023-07-03 02:50:08,755][188188] FPS: 325959.69[0m
[36m[2023-07-03 02:50:13,016][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:50:13,017][188188] Reward + Measures: [[15.17707911  0.13694066  0.92214835  0.91652423  0.7997883   3.99720645]][0m
[37m[1m[2023-07-03 02:50:13,017][188188] Max Reward on eval: 15.177079106606504[0m
[37m[1m[2023-07-03 02:50:13,017][188188] Min Reward on eval: 15.177079106606504[0m
[37m[1m[2023-07-03 02:50:13,017][188188] Mean Reward across all agents: 15.177079106606504[0m
[37m[1m[2023-07-03 02:50:13,017][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:50:18,005][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:50:18,006][188188] Reward + Measures: [[  49.33402773    0.0495        0.48630005    0.31120002    0.38689998
     3.87619448]
 [  82.50167989    0.0064        0.8858        0.87460005    0.87690002
     3.99520874]
 [  30.5140422     0.34999999    0.78710002    0.7899        0.52610004
     3.99713516]
 ...
 [  58.64233277    0.31900001    0.88459998    0.87589997    0.60699999
     3.99638987]
 [-112.51420093    0.39470002    0.98110002    0.97100002    0.58570004
     3.99760628]
 [ -14.07565304    0.39130002    0.69069999    0.7392        0.32319999
     3.98409891]][0m
[37m[1m[2023-07-03 02:50:18,006][188188] Max Reward on eval: 401.7941290164599[0m
[37m[1m[2023-07-03 02:50:18,006][188188] Min Reward on eval: -231.54599477272131[0m
[37m[1m[2023-07-03 02:50:18,006][188188] Mean Reward across all agents: -0.08884655402366662[0m
[37m[1m[2023-07-03 02:50:18,007][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:50:18,008][188188] mean_value=-833.7174126200586, max_value=90.75746269092645[0m
[37m[1m[2023-07-03 02:50:18,011][188188] New mean coefficients: [[-1.7596121   3.4158037   2.0112753  -0.80470085 -3.0616999  -2.111403  ]][0m
[37m[1m[2023-07-03 02:50:18,012][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:50:27,045][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 02:50:27,045][188188] FPS: 425151.35[0m
[36m[2023-07-03 02:50:27,048][188188] itr=254, itrs=2000, Progress: 12.70%[0m
[36m[2023-07-03 02:50:38,586][188188] train() took 11.52 seconds to complete[0m
[36m[2023-07-03 02:50:38,586][188188] FPS: 333368.79[0m
[36m[2023-07-03 02:50:42,809][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:50:42,809][188188] Reward + Measures: [[16.77932146  0.07721499  0.9475953   0.94276494  0.87724829  3.99868464]][0m
[37m[1m[2023-07-03 02:50:42,810][188188] Max Reward on eval: 16.779321457321803[0m
[37m[1m[2023-07-03 02:50:42,810][188188] Min Reward on eval: 16.779321457321803[0m
[37m[1m[2023-07-03 02:50:42,810][188188] Mean Reward across all agents: 16.779321457321803[0m
[37m[1m[2023-07-03 02:50:42,810][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:50:47,780][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:50:47,781][188188] Reward + Measures: [[ 81.41582445   0.0647       0.39659998   0.4131       0.3382
    3.89276767]
 [-67.81095717   0.24070001   0.78359997   0.78170007   0.61500001
    3.9962914 ]
 [ 55.47859458   0.038        0.60690004   0.49260002   0.50880003
    3.91471529]
 ...
 [-76.46654494   0.1948       0.98990005   0.97500002   0.7899
    3.99071693]
 [100.86130046   0.1275       0.93050003   0.83360004   0.79000002
    3.9507153 ]
 [ 62.78231538   0.0442       0.39739999   0.3687       0.33390003
    3.94659615]][0m
[37m[1m[2023-07-03 02:50:47,781][188188] Max Reward on eval: 252.94758188510315[0m
[37m[1m[2023-07-03 02:50:47,781][188188] Min Reward on eval: -223.70139598473907[0m
[37m[1m[2023-07-03 02:50:47,781][188188] Mean Reward across all agents: 16.046276912233534[0m
[37m[1m[2023-07-03 02:50:47,782][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:50:47,783][188188] mean_value=-778.4945738273052, max_value=45.60150339123689[0m
[37m[1m[2023-07-03 02:50:47,786][188188] New mean coefficients: [[-1.4694198   3.9512317   1.8719923  -0.68271804 -3.0492537  -3.3157625 ]][0m
[37m[1m[2023-07-03 02:50:47,787][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:50:56,696][188188] train() took 8.91 seconds to complete[0m
[36m[2023-07-03 02:50:56,697][188188] FPS: 431057.23[0m
[36m[2023-07-03 02:50:56,699][188188] itr=255, itrs=2000, Progress: 12.75%[0m
[36m[2023-07-03 02:51:08,272][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 02:51:08,272][188188] FPS: 332357.48[0m
[36m[2023-07-03 02:51:12,541][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:51:12,541][188188] Reward + Measures: [[34.40033808  0.04092067  0.94895965  0.94250494  0.91240704  3.99792123]][0m
[37m[1m[2023-07-03 02:51:12,542][188188] Max Reward on eval: 34.40033807951379[0m
[37m[1m[2023-07-03 02:51:12,542][188188] Min Reward on eval: 34.40033807951379[0m
[37m[1m[2023-07-03 02:51:12,542][188188] Mean Reward across all agents: 34.40033807951379[0m
[37m[1m[2023-07-03 02:51:12,542][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:51:17,704][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:51:17,709][188188] Reward + Measures: [[ -5.00085412   0.0852       0.18249999   0.21830001   0.1684
    3.87128949]
 [ 60.67691968   0.0503       0.53300005   0.44259998   0.47030002
    3.8592155 ]
 [ 11.94085243   0.1155       0.88730001   0.8976       0.787
    3.9947567 ]
 ...
 [ 67.61793685   0.35159999   0.42300001   0.4082       0.17040001
    3.97931337]
 [ 32.00276949   0.11420001   0.38159999   0.3813       0.3899
    3.93423462]
 [149.09870894   0.0023       0.98400003   0.96880001   0.98280001
    3.99840713]][0m
[37m[1m[2023-07-03 02:51:17,710][188188] Max Reward on eval: 394.08246569372716[0m
[37m[1m[2023-07-03 02:51:17,710][188188] Min Reward on eval: -130.21366101847963[0m
[37m[1m[2023-07-03 02:51:17,710][188188] Mean Reward across all agents: 55.89408973042035[0m
[37m[1m[2023-07-03 02:51:17,710][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:51:17,712][188188] mean_value=-1351.0583990764426, max_value=-14.112658906603059[0m
[36m[2023-07-03 02:51:17,715][188188] XNES is restarting with a new solution whose measures are [0.46210003 0.29179999 0.34820002 0.0516     3.68199992] and objective is 70.76954578077421[0m
[36m[2023-07-03 02:51:17,716][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 02:51:17,718][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 02:51:17,719][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:51:26,744][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 02:51:26,744][188188] FPS: 425556.82[0m
[36m[2023-07-03 02:51:26,747][188188] itr=256, itrs=2000, Progress: 12.80%[0m
[36m[2023-07-03 02:51:38,617][188188] train() took 11.85 seconds to complete[0m
[36m[2023-07-03 02:51:38,617][188188] FPS: 324120.22[0m
[36m[2023-07-03 02:51:42,908][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:51:42,909][188188] Reward + Measures: [[-34.53162254   0.57593995   0.22984199   0.51391798   0.28500566
    3.91950583]][0m
[37m[1m[2023-07-03 02:51:42,909][188188] Max Reward on eval: -34.53162254290254[0m
[37m[1m[2023-07-03 02:51:42,909][188188] Min Reward on eval: -34.53162254290254[0m
[37m[1m[2023-07-03 02:51:42,909][188188] Mean Reward across all agents: -34.53162254290254[0m
[37m[1m[2023-07-03 02:51:42,910][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:51:47,931][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:51:47,937][188188] Reward + Measures: [[ 78.56484443   0.4822       0.1481       0.46269998   0.33090001
    3.97015738]
 [180.2577       0.56059998   0.40349999   0.47770005   0.0948
    3.96698642]
 [-37.57262433   0.64829999   0.58420002   0.59680003   0.0866
    3.74083519]
 ...
 [ 74.94980287   0.52320004   0.37009999   0.47100002   0.0642
    3.75664067]
 [120.31910027   0.4948       0.47739998   0.48210001   0.12330001
    3.61519432]
 [-62.64772111   0.60180002   0.49939999   0.51480001   0.0719
    3.74526668]][0m
[37m[1m[2023-07-03 02:51:47,937][188188] Max Reward on eval: 461.5465150940232[0m
[37m[1m[2023-07-03 02:51:47,937][188188] Min Reward on eval: -352.73602868048476[0m
[37m[1m[2023-07-03 02:51:47,938][188188] Mean Reward across all agents: 47.158543875029466[0m
[37m[1m[2023-07-03 02:51:47,938][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:51:47,940][188188] mean_value=-952.534754157652, max_value=277.8336604220691[0m
[37m[1m[2023-07-03 02:51:47,943][188188] New mean coefficients: [[ 1.144302  -0.2161834 -1.2721987 -2.0578814 -1.4704434 -0.6209631]][0m
[37m[1m[2023-07-03 02:51:47,944][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:51:56,967][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 02:51:56,968][188188] FPS: 425622.69[0m
[36m[2023-07-03 02:51:56,970][188188] itr=257, itrs=2000, Progress: 12.85%[0m
[36m[2023-07-03 02:52:08,598][188188] train() took 11.61 seconds to complete[0m
[36m[2023-07-03 02:52:08,599][188188] FPS: 330812.39[0m
[36m[2023-07-03 02:52:12,949][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:52:12,950][188188] Reward + Measures: [[15.50381258  0.53784901  0.36490333  0.47807601  0.12156133  3.89716792]][0m
[37m[1m[2023-07-03 02:52:12,950][188188] Max Reward on eval: 15.503812577481852[0m
[37m[1m[2023-07-03 02:52:12,950][188188] Min Reward on eval: 15.503812577481852[0m
[37m[1m[2023-07-03 02:52:12,950][188188] Mean Reward across all agents: 15.503812577481852[0m
[37m[1m[2023-07-03 02:52:12,951][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:52:17,932][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:52:17,933][188188] Reward + Measures: [[  8.91989376   0.1214       0.0982       0.08870001   0.13340001
    3.70747375]
 [ 80.77570589   0.76840007   0.26340002   0.78249997   0.50450003
    3.95934916]
 [-25.88991674   0.0974       0.0899       0.1025       0.0964
    3.88623309]
 ...
 [ 29.44610119   0.62869996   0.20299999   0.61779994   0.44750005
    3.98040557]
 [ -7.02032136   0.29659998   0.38110003   0.36019999   0.24200001
    3.85236478]
 [187.18492027   0.69929999   0.51200002   0.71550006   0.2599
    3.98670959]][0m
[37m[1m[2023-07-03 02:52:17,933][188188] Max Reward on eval: 372.86194804208355[0m
[37m[1m[2023-07-03 02:52:17,933][188188] Min Reward on eval: -316.23847243972125[0m
[37m[1m[2023-07-03 02:52:17,934][188188] Mean Reward across all agents: 49.69701783199028[0m
[37m[1m[2023-07-03 02:52:17,934][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:52:17,936][188188] mean_value=-1272.9533521252717, max_value=147.84163829254018[0m
[37m[1m[2023-07-03 02:52:17,938][188188] New mean coefficients: [[ 1.7062991  -0.23604581 -0.8493707  -1.9588197  -1.09599    -1.2798274 ]][0m
[37m[1m[2023-07-03 02:52:17,939][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:52:26,925][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 02:52:26,925][188188] FPS: 427432.75[0m
[36m[2023-07-03 02:52:26,927][188188] itr=258, itrs=2000, Progress: 12.90%[0m
[36m[2023-07-03 02:52:38,568][188188] train() took 11.62 seconds to complete[0m
[36m[2023-07-03 02:52:38,568][188188] FPS: 330525.88[0m
[36m[2023-07-03 02:52:42,923][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:52:42,929][188188] Reward + Measures: [[273.04747545   0.81189972   0.47303599   0.8104803    0.35013798
    3.94770122]][0m
[37m[1m[2023-07-03 02:52:42,929][188188] Max Reward on eval: 273.04747544843775[0m
[37m[1m[2023-07-03 02:52:42,929][188188] Min Reward on eval: 273.04747544843775[0m
[37m[1m[2023-07-03 02:52:42,930][188188] Mean Reward across all agents: 273.04747544843775[0m
[37m[1m[2023-07-03 02:52:42,930][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:52:47,977][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:52:47,983][188188] Reward + Measures: [[  39.01588581    0.92760003    0.111         0.91250002    0.79399997
     3.99593425]
 [-301.91683604    0.83070004    0.78589994    0.78709996    0.0377
     3.99469161]
 [  75.23906661    0.1279        0.0902        0.0931        0.0784
     3.60544276]
 ...
 [  90.27084096    0.35089999    0.18260001    0.37550002    0.26890001
     3.95620656]
 [ 364.67647216    0.54070002    0.45179996    0.48100001    0.0382
     3.85891986]
 [  62.41325936    0.79050004    0.46619996    0.7568        0.26139998
     3.89116263]][0m
[37m[1m[2023-07-03 02:52:47,983][188188] Max Reward on eval: 491.38555908009874[0m
[37m[1m[2023-07-03 02:52:47,983][188188] Min Reward on eval: -364.10320378653705[0m
[37m[1m[2023-07-03 02:52:47,984][188188] Mean Reward across all agents: 60.462729969054344[0m
[37m[1m[2023-07-03 02:52:47,984][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:52:47,986][188188] mean_value=-1047.263700281849, max_value=291.8128524596771[0m
[37m[1m[2023-07-03 02:52:47,989][188188] New mean coefficients: [[ 1.1401145 -0.5772531 -1.2357264 -2.2476945 -0.8634139 -1.3350261]][0m
[37m[1m[2023-07-03 02:52:47,990][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:52:57,055][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 02:52:57,056][188188] FPS: 423645.59[0m
[36m[2023-07-03 02:52:57,058][188188] itr=259, itrs=2000, Progress: 12.95%[0m
[36m[2023-07-03 02:53:08,768][188188] train() took 11.69 seconds to complete[0m
[36m[2023-07-03 02:53:08,768][188188] FPS: 328459.42[0m
[36m[2023-07-03 02:53:13,040][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:53:13,040][188188] Reward + Measures: [[-53.4685286    0.53529704   0.26269501   0.47339901   0.207967
    3.91174984]][0m
[37m[1m[2023-07-03 02:53:13,040][188188] Max Reward on eval: -53.46852859703448[0m
[37m[1m[2023-07-03 02:53:13,040][188188] Min Reward on eval: -53.46852859703448[0m
[37m[1m[2023-07-03 02:53:13,041][188188] Mean Reward across all agents: -53.46852859703448[0m
[37m[1m[2023-07-03 02:53:13,041][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:53:18,030][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:53:18,030][188188] Reward + Measures: [[-105.80506803    0.73060006    0.79700005    0.80680001    0.13420001
     3.99654961]
 [  85.84387159    0.54870003    0.32870001    0.52520007    0.1938
     3.90355659]
 [ -17.43561866    0.35960004    0.41510001    0.59370005    0.60790002
     3.90385818]
 ...
 [  29.36501826    0.19319999    0.14390001    0.19310001    0.0932
     3.85330439]
 [  10.98900273    0.59600002    0.25670001    0.50730002    0.24069999
     3.85584188]
 [  -6.81279429    0.16410001    0.17420001    0.2375        0.21949999
     3.70764279]][0m
[37m[1m[2023-07-03 02:53:18,031][188188] Max Reward on eval: 458.6578444027342[0m
[37m[1m[2023-07-03 02:53:18,031][188188] Min Reward on eval: -597.9957711884751[0m
[37m[1m[2023-07-03 02:53:18,031][188188] Mean Reward across all agents: 24.07163866249266[0m
[37m[1m[2023-07-03 02:53:18,031][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:53:18,033][188188] mean_value=-1445.992838418494, max_value=-2.737738953494727[0m
[36m[2023-07-03 02:53:18,036][188188] XNES is restarting with a new solution whose measures are [0.81960005 0.78210002 0.11060001 0.85340005 3.8124299 ] and objective is 370.59309530509637[0m
[36m[2023-07-03 02:53:18,037][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 02:53:18,039][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 02:53:18,040][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:53:27,115][188188] train() took 9.07 seconds to complete[0m
[36m[2023-07-03 02:53:27,115][188188] FPS: 423214.97[0m
[36m[2023-07-03 02:53:27,118][188188] itr=260, itrs=2000, Progress: 13.00%[0m
[37m[1m[2023-07-03 02:53:30,037][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000240[0m
[36m[2023-07-03 02:53:42,128][188188] train() took 11.76 seconds to complete[0m
[36m[2023-07-03 02:53:42,128][188188] FPS: 326408.17[0m
[36m[2023-07-03 02:53:46,400][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:53:46,401][188188] Reward + Measures: [[-20.9710857    0.658068     0.78411895   0.091185     0.80122298
    3.90508914]][0m
[37m[1m[2023-07-03 02:53:46,401][188188] Max Reward on eval: -20.97108570028934[0m
[37m[1m[2023-07-03 02:53:46,401][188188] Min Reward on eval: -20.97108570028934[0m
[37m[1m[2023-07-03 02:53:46,401][188188] Mean Reward across all agents: -20.97108570028934[0m
[37m[1m[2023-07-03 02:53:46,402][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:53:51,400][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:53:51,405][188188] Reward + Measures: [[   9.33643795    0.18780001    0.31440002    0.1146        0.32239997
     3.78759933]
 [  28.79197054    0.1296        0.45199996    0.59450001    0.63800001
     3.8056705 ]
 [-298.87758518    0.0318        0.58430004    0.58740002    0.63120002
     3.70229888]
 ...
 [  -9.60684294    0.0238        0.60140002    0.55000001    0.78110003
     3.81329465]
 [ 215.92159797    0.82190001    0.88210005    0.0198        0.90360004
     3.91497278]
 [ -64.04641822    0.0963        0.47399998    0.61360008    0.66990006
     3.92008829]][0m
[37m[1m[2023-07-03 02:53:51,406][188188] Max Reward on eval: 434.7673377985135[0m
[37m[1m[2023-07-03 02:53:51,406][188188] Min Reward on eval: -702.1766967843287[0m
[37m[1m[2023-07-03 02:53:51,406][188188] Mean Reward across all agents: 10.479750507590586[0m
[37m[1m[2023-07-03 02:53:51,407][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:53:51,409][188188] mean_value=-1161.5785401428204, max_value=83.34926841366735[0m
[37m[1m[2023-07-03 02:53:51,411][188188] New mean coefficients: [[-0.15449125 -1.0115274   0.5672449  -2.6396356  -1.1739999  -0.33458084]][0m
[37m[1m[2023-07-03 02:53:51,412][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:54:00,403][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 02:54:00,403][188188] FPS: 427196.99[0m
[36m[2023-07-03 02:54:00,406][188188] itr=261, itrs=2000, Progress: 13.05%[0m
[36m[2023-07-03 02:54:12,004][188188] train() took 11.58 seconds to complete[0m
[36m[2023-07-03 02:54:12,004][188188] FPS: 331664.62[0m
[36m[2023-07-03 02:54:16,251][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:54:16,252][188188] Reward + Measures: [[-136.49680339    0.20478532    0.47279596    0.28422967    0.50200331
     3.87489486]][0m
[37m[1m[2023-07-03 02:54:16,252][188188] Max Reward on eval: -136.4968033853628[0m
[37m[1m[2023-07-03 02:54:16,252][188188] Min Reward on eval: -136.4968033853628[0m
[37m[1m[2023-07-03 02:54:16,253][188188] Mean Reward across all agents: -136.4968033853628[0m
[37m[1m[2023-07-03 02:54:16,253][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:54:21,183][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:54:21,184][188188] Reward + Measures: [[-30.80702535   0.1073       0.2017       0.1761       0.20990001
    3.81294107]
 [ 85.18491175   0.0181       0.65620005   0.41779995   0.74580002
    3.79874277]
 [209.96597669   0.36860001   0.57520002   0.81599998   0.80419999
    3.96062708]
 ...
 [ 43.52279592   0.0144       0.69519997   0.59930009   0.74060005
    3.93492508]
 [ 24.95492153   0.1214       0.1648       0.1105       0.16720001
    3.56012607]
 [ 56.43579143   0.0146       0.74360001   0.57330006   0.79979998
    3.83582234]][0m
[37m[1m[2023-07-03 02:54:21,184][188188] Max Reward on eval: 439.60548257655466[0m
[37m[1m[2023-07-03 02:54:21,184][188188] Min Reward on eval: -552.8473796658888[0m
[37m[1m[2023-07-03 02:54:21,184][188188] Mean Reward across all agents: 28.982834230570553[0m
[37m[1m[2023-07-03 02:54:21,185][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:54:21,186][188188] mean_value=-1209.949954308276, max_value=145.77704471335295[0m
[37m[1m[2023-07-03 02:54:21,189][188188] New mean coefficients: [[-0.5649081   0.08994853  1.8117543  -0.9954971  -1.3673434   0.75017506]][0m
[37m[1m[2023-07-03 02:54:21,190][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:54:30,105][188188] train() took 8.91 seconds to complete[0m
[36m[2023-07-03 02:54:30,105][188188] FPS: 430833.29[0m
[36m[2023-07-03 02:54:30,107][188188] itr=262, itrs=2000, Progress: 13.10%[0m
[36m[2023-07-03 02:54:41,653][188188] train() took 11.53 seconds to complete[0m
[36m[2023-07-03 02:54:41,654][188188] FPS: 333154.28[0m
[36m[2023-07-03 02:54:45,887][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:54:45,887][188188] Reward + Measures: [[-180.05685533    0.009483      0.59720868    0.63073796    0.78285468
     3.68682098]][0m
[37m[1m[2023-07-03 02:54:45,887][188188] Max Reward on eval: -180.0568553261058[0m
[37m[1m[2023-07-03 02:54:45,888][188188] Min Reward on eval: -180.0568553261058[0m
[37m[1m[2023-07-03 02:54:45,888][188188] Mean Reward across all agents: -180.0568553261058[0m
[37m[1m[2023-07-03 02:54:45,888][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:54:50,917][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:54:50,918][188188] Reward + Measures: [[238.34672878   0.35929999   0.42880002   0.52820003   0.29119998
    3.76427007]
 [289.79160501   0.0332       0.7816       0.69         0.85930008
    3.97450995]
 [111.08425234   0.0022       0.95279998   0.91920006   0.98559999
    3.9864006 ]
 ...
 [ -5.68905081   0.0027       0.97130007   0.95129997   0.98720008
    3.99542785]
 [-24.05423259   0.0803       0.1089       0.0844       0.1152
    3.73174405]
 [-44.55916426   0.0619       0.0975       0.0969       0.1024
    3.73800659]][0m
[37m[1m[2023-07-03 02:54:50,918][188188] Max Reward on eval: 722.808731076587[0m
[37m[1m[2023-07-03 02:54:50,918][188188] Min Reward on eval: -716.653099026531[0m
[37m[1m[2023-07-03 02:54:50,918][188188] Mean Reward across all agents: 38.58845883362474[0m
[37m[1m[2023-07-03 02:54:50,919][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:54:50,921][188188] mean_value=-955.9110827935592, max_value=136.1311486750012[0m
[37m[1m[2023-07-03 02:54:50,923][188188] New mean coefficients: [[-0.944784    0.34383383  1.5918275  -1.5857472  -0.9332589   1.6600533 ]][0m
[37m[1m[2023-07-03 02:54:50,924][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:55:00,003][188188] train() took 9.08 seconds to complete[0m
[36m[2023-07-03 02:55:00,004][188188] FPS: 423037.44[0m
[36m[2023-07-03 02:55:00,006][188188] itr=263, itrs=2000, Progress: 13.15%[0m
[36m[2023-07-03 02:55:11,776][188188] train() took 11.75 seconds to complete[0m
[36m[2023-07-03 02:55:11,776][188188] FPS: 326826.83[0m
[36m[2023-07-03 02:55:16,067][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:55:16,068][188188] Reward + Measures: [[-26.82763694   0.10519434   0.294231     0.20716698   0.40691102
    3.62084055]][0m
[37m[1m[2023-07-03 02:55:16,068][188188] Max Reward on eval: -26.827636938678765[0m
[37m[1m[2023-07-03 02:55:16,068][188188] Min Reward on eval: -26.827636938678765[0m
[37m[1m[2023-07-03 02:55:16,068][188188] Mean Reward across all agents: -26.827636938678765[0m
[37m[1m[2023-07-03 02:55:16,069][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:55:21,033][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:55:21,034][188188] Reward + Measures: [[ 355.05036928    0.93549997    0.97550005    0.0009        0.98619998
     3.94456863]
 [ 358.60232549    0.9325        0.97269994    0.0258        0.96880007
     3.95461082]
 [  10.73843261    0.97729999    0.995         0.            0.99080002
     3.9989624 ]
 ...
 [ -27.67215267    0.3741        0.59750003    0.21960001    0.70180005
     3.93482971]
 [ 190.63018291    0.1245        0.47950003    0.3409        0.52310002
     3.81344271]
 [-448.26388456    0.80790007    0.96200001    0.13150001    0.96130002
     3.99029613]][0m
[37m[1m[2023-07-03 02:55:21,034][188188] Max Reward on eval: 475.54945754949006[0m
[37m[1m[2023-07-03 02:55:21,034][188188] Min Reward on eval: -684.4204330565408[0m
[37m[1m[2023-07-03 02:55:21,034][188188] Mean Reward across all agents: 15.683232134811472[0m
[37m[1m[2023-07-03 02:55:21,035][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:55:21,036][188188] mean_value=-692.7320652896344, max_value=29.02511159811064[0m
[37m[1m[2023-07-03 02:55:21,039][188188] New mean coefficients: [[-0.80505073 -1.1920534   3.2426977  -1.5810916  -0.52961206  2.784227  ]][0m
[37m[1m[2023-07-03 02:55:21,040][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:55:30,009][188188] train() took 8.97 seconds to complete[0m
[36m[2023-07-03 02:55:30,009][188188] FPS: 428180.91[0m
[36m[2023-07-03 02:55:30,012][188188] itr=264, itrs=2000, Progress: 13.20%[0m
[36m[2023-07-03 02:55:41,702][188188] train() took 11.67 seconds to complete[0m
[36m[2023-07-03 02:55:41,702][188188] FPS: 329107.46[0m
[36m[2023-07-03 02:55:45,992][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:55:45,992][188188] Reward + Measures: [[-121.95058094    0.23333       0.45728564    0.217191      0.54753268
     3.80040884]][0m
[37m[1m[2023-07-03 02:55:45,993][188188] Max Reward on eval: -121.9505809410303[0m
[37m[1m[2023-07-03 02:55:45,993][188188] Min Reward on eval: -121.9505809410303[0m
[37m[1m[2023-07-03 02:55:45,993][188188] Mean Reward across all agents: -121.9505809410303[0m
[37m[1m[2023-07-03 02:55:45,993][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:55:51,138][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:55:51,139][188188] Reward + Measures: [[  23.20560866    0.0152        0.5377        0.54510003    0.67629999
     3.80568004]
 [  -8.4668443     0.0844        0.36500001    0.24170001    0.40109998
     3.7651937 ]
 [-312.45108285    0.6182        0.69520003    0.0553        0.6868
     3.91255355]
 ...
 [-115.69086293    0.7863        0.82819998    0.0166        0.85790008
     3.82656527]
 [  94.49356459    0.34550002    0.91610003    0.54369998    0.92490005
     3.91157031]
 [  35.19561386    0.13399999    0.22920001    0.21930002    0.11790001
     3.9027791 ]][0m
[37m[1m[2023-07-03 02:55:51,139][188188] Max Reward on eval: 714.8925628602505[0m
[37m[1m[2023-07-03 02:55:51,139][188188] Min Reward on eval: -706.9372596668079[0m
[37m[1m[2023-07-03 02:55:51,139][188188] Mean Reward across all agents: 29.95302517687197[0m
[37m[1m[2023-07-03 02:55:51,140][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:55:51,141][188188] mean_value=-1117.5354208975646, max_value=54.038303867203126[0m
[37m[1m[2023-07-03 02:55:51,144][188188] New mean coefficients: [[-1.6405499  -1.1934376   4.098201   -0.66854304  0.53958213  3.5426598 ]][0m
[37m[1m[2023-07-03 02:55:51,145][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:56:00,084][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 02:56:00,084][188188] FPS: 429627.93[0m
[36m[2023-07-03 02:56:00,086][188188] itr=265, itrs=2000, Progress: 13.25%[0m
[36m[2023-07-03 02:56:11,873][188188] train() took 11.77 seconds to complete[0m
[36m[2023-07-03 02:56:11,873][188188] FPS: 326336.47[0m
[36m[2023-07-03 02:56:16,163][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:56:16,163][188188] Reward + Measures: [[-245.64206849    0.00841       0.655312      0.693582      0.79720068
     3.82277966]][0m
[37m[1m[2023-07-03 02:56:16,163][188188] Max Reward on eval: -245.64206849047014[0m
[37m[1m[2023-07-03 02:56:16,163][188188] Min Reward on eval: -245.64206849047014[0m
[37m[1m[2023-07-03 02:56:16,164][188188] Mean Reward across all agents: -245.64206849047014[0m
[37m[1m[2023-07-03 02:56:16,164][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:56:21,174][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:56:21,174][188188] Reward + Measures: [[ -41.15680125    0.0956        0.1354        0.1221        0.1327
     3.72984552]
 [  -6.85358988    0.17639999    0.52690005    0.45140001    0.5675
     3.8393209 ]
 [-317.05564673    0.60710001    0.79010004    0.30149999    0.70970005
     3.99804044]
 ...
 [   4.22228389    0.0026        0.89330006    0.90129995    0.89659995
     3.98975039]
 [-189.85309601    0.58130002    0.97970003    0.3863        0.97690004
     3.99848151]
 [ 529.82054899    0.1056        0.87650007    0.77410001    0.90350002
     3.99905062]][0m
[37m[1m[2023-07-03 02:56:21,175][188188] Max Reward on eval: 531.1879901906825[0m
[37m[1m[2023-07-03 02:56:21,175][188188] Min Reward on eval: -724.2304916339228[0m
[37m[1m[2023-07-03 02:56:21,175][188188] Mean Reward across all agents: -10.773631797589802[0m
[37m[1m[2023-07-03 02:56:21,175][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:56:21,177][188188] mean_value=-1178.3297445166463, max_value=-14.871091959894954[0m
[36m[2023-07-03 02:56:21,179][188188] XNES is restarting with a new solution whose measures are [0.47510001 0.69570005 0.56830007 0.43630001 3.94408274] and objective is 239.49889185465872[0m
[36m[2023-07-03 02:56:21,180][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 02:56:21,183][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 02:56:21,184][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:56:30,142][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 02:56:30,142][188188] FPS: 428736.96[0m
[36m[2023-07-03 02:56:30,144][188188] itr=266, itrs=2000, Progress: 13.30%[0m
[36m[2023-07-03 02:56:41,917][188188] train() took 11.75 seconds to complete[0m
[36m[2023-07-03 02:56:41,918][188188] FPS: 326754.30[0m
[36m[2023-07-03 02:56:46,242][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:56:46,243][188188] Reward + Measures: [[199.35139619   0.40973502   0.64541465   0.50301766   0.436373
    3.9252243 ]][0m
[37m[1m[2023-07-03 02:56:46,243][188188] Max Reward on eval: 199.35139618592274[0m
[37m[1m[2023-07-03 02:56:46,243][188188] Min Reward on eval: 199.35139618592274[0m
[37m[1m[2023-07-03 02:56:46,243][188188] Mean Reward across all agents: 199.35139618592274[0m
[37m[1m[2023-07-03 02:56:46,244][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:56:51,259][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:56:51,260][188188] Reward + Measures: [[122.04677974   0.29100001   0.38530001   0.37180001   0.24150001
    3.60644412]
 [ 31.1438255    0.09589999   0.31389999   0.17910001   0.29060003
    3.91467905]
 [ 56.59952923   0.28120002   0.93489999   0.94110006   0.68900001
    3.94029546]
 ...
 [ 55.02038852   0.1445       0.55880004   0.2026       0.50270003
    3.93869019]
 [ 80.0502304    0.54369998   0.7809       0.66639996   0.30669999
    3.77667928]
 [125.16924194   0.21040002   0.57459998   0.51789999   0.45570001
    3.94193244]][0m
[37m[1m[2023-07-03 02:56:51,260][188188] Max Reward on eval: 533.616516084224[0m
[37m[1m[2023-07-03 02:56:51,260][188188] Min Reward on eval: -146.72709232382476[0m
[37m[1m[2023-07-03 02:56:51,261][188188] Mean Reward across all agents: 85.77684569224012[0m
[37m[1m[2023-07-03 02:56:51,261][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:56:51,264][188188] mean_value=-1438.6138760336241, max_value=138.49499837534327[0m
[37m[1m[2023-07-03 02:56:51,266][188188] New mean coefficients: [[-0.83043206 -0.7705394  -0.21479082 -1.7597721  -0.00737393  0.6577103 ]][0m
[37m[1m[2023-07-03 02:56:51,267][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:57:00,326][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 02:57:00,327][188188] FPS: 423949.20[0m
[36m[2023-07-03 02:57:00,329][188188] itr=267, itrs=2000, Progress: 13.35%[0m
[36m[2023-07-03 02:57:12,149][188188] train() took 11.80 seconds to complete[0m
[36m[2023-07-03 02:57:12,149][188188] FPS: 325516.23[0m
[36m[2023-07-03 02:57:16,502][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:57:16,503][188188] Reward + Measures: [[237.08202824   0.48115566   0.61691862   0.54652667   0.35614902
    3.93877053]][0m
[37m[1m[2023-07-03 02:57:16,503][188188] Max Reward on eval: 237.08202823758955[0m
[37m[1m[2023-07-03 02:57:16,503][188188] Min Reward on eval: 237.08202823758955[0m
[37m[1m[2023-07-03 02:57:16,504][188188] Mean Reward across all agents: 237.08202823758955[0m
[37m[1m[2023-07-03 02:57:16,504][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:57:21,500][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:57:21,501][188188] Reward + Measures: [[ 111.46000992    0.63989997    0.62639999    0.65539998    0.1364
     3.89876938]
 [  22.8777918     0.19210002    0.4118        0.22410002    0.3504
     3.93970942]
 [   7.85324034    0.36970001    0.4303        0.37020001    0.39489999
     3.900702  ]
 ...
 [-167.03095168    0.6286        0.77340001    0.79189998    0.2201
     3.9931519 ]
 [  -3.31937718    0.7353        0.963         0.75439996    0.37329999
     3.96631861]
 [ 282.00211311    0.59289998    0.67449999    0.64840001    0.33220002
     3.95256495]][0m
[37m[1m[2023-07-03 02:57:21,501][188188] Max Reward on eval: 707.358882903232[0m
[37m[1m[2023-07-03 02:57:21,501][188188] Min Reward on eval: -341.47922899499537[0m
[37m[1m[2023-07-03 02:57:21,502][188188] Mean Reward across all agents: 73.24095350948922[0m
[37m[1m[2023-07-03 02:57:21,502][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:57:21,505][188188] mean_value=-1445.114568789121, max_value=766.5385835825783[0m
[37m[1m[2023-07-03 02:57:21,508][188188] New mean coefficients: [[-0.6351038 -1.1547043 -1.8644093 -2.3733296 -0.1931977  1.1969872]][0m
[37m[1m[2023-07-03 02:57:21,509][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:57:30,526][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 02:57:30,526][188188] FPS: 425914.75[0m
[36m[2023-07-03 02:57:30,529][188188] itr=268, itrs=2000, Progress: 13.40%[0m
[36m[2023-07-03 02:57:42,220][188188] train() took 11.67 seconds to complete[0m
[36m[2023-07-03 02:57:42,220][188188] FPS: 328998.40[0m
[36m[2023-07-03 02:57:46,551][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:57:46,557][188188] Reward + Measures: [[296.20785445   0.60376734   0.61326367   0.64978969   0.26045164
    3.94506955]][0m
[37m[1m[2023-07-03 02:57:46,557][188188] Max Reward on eval: 296.2078544513759[0m
[37m[1m[2023-07-03 02:57:46,558][188188] Min Reward on eval: 296.2078544513759[0m
[37m[1m[2023-07-03 02:57:46,558][188188] Mean Reward across all agents: 296.2078544513759[0m
[37m[1m[2023-07-03 02:57:46,558][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:57:51,693][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:57:51,699][188188] Reward + Measures: [[ 114.70961996    0.33409998    0.62419999    0.41490003    0.52179998
     3.80898356]
 [ -49.14565603    0.22669999    0.22700003    0.23240001    0.23099999
     3.85181117]
 [ 289.22553445    0.59330004    0.61670005    0.64750004    0.30430001
     3.86431575]
 ...
 [-135.62828492    0.72680002    0.88959998    0.88590002    0.212
     3.99722719]
 [  41.22865544    0.1295        0.3504        0.1591        0.2872
     3.82085347]
 [ 197.11489296    0.5686        0.61930001    0.63140005    0.33220002
     3.94326949]][0m
[37m[1m[2023-07-03 02:57:51,699][188188] Max Reward on eval: 834.6641006529331[0m
[37m[1m[2023-07-03 02:57:51,700][188188] Min Reward on eval: -176.0273369472474[0m
[37m[1m[2023-07-03 02:57:51,700][188188] Mean Reward across all agents: 143.3525538131313[0m
[37m[1m[2023-07-03 02:57:51,700][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:57:51,703][188188] mean_value=-1133.1080837179609, max_value=215.3239490921278[0m
[37m[1m[2023-07-03 02:57:51,706][188188] New mean coefficients: [[-1.9356045  -0.51379764 -1.5984838  -1.5620039  -0.5565407   1.9698187 ]][0m
[37m[1m[2023-07-03 02:57:51,706][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:58:00,752][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 02:58:00,752][188188] FPS: 424589.74[0m
[36m[2023-07-03 02:58:00,755][188188] itr=269, itrs=2000, Progress: 13.45%[0m
[36m[2023-07-03 02:58:12,491][188188] train() took 11.72 seconds to complete[0m
[36m[2023-07-03 02:58:12,492][188188] FPS: 327784.30[0m
[36m[2023-07-03 02:58:16,787][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:58:16,787][188188] Reward + Measures: [[336.0948733    0.6733287    0.605919     0.70411134   0.21593733
    3.96215296]][0m
[37m[1m[2023-07-03 02:58:16,787][188188] Max Reward on eval: 336.09487329822264[0m
[37m[1m[2023-07-03 02:58:16,788][188188] Min Reward on eval: 336.09487329822264[0m
[37m[1m[2023-07-03 02:58:16,788][188188] Mean Reward across all agents: 336.09487329822264[0m
[37m[1m[2023-07-03 02:58:16,788][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:58:21,798][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:58:21,799][188188] Reward + Measures: [[436.71961975   0.96169996   0.62309998   0.93300003   0.0178
    3.9783113 ]
 [ 91.90544793   0.24660003   0.38030002   0.32269999   0.1045
    3.77788711]
 [-29.58048154   0.1164       0.1389       0.1111       0.1365
    3.84385347]
 ...
 [304.36247778   0.64810002   0.62159997   0.66930002   0.20680001
    3.98373222]
 [398.24320031   0.83220005   0.49069998   0.80989999   0.084
    3.94313931]
 [135.78098821   0.75960004   0.75100005   0.77090001   0.6649
    3.58224535]][0m
[37m[1m[2023-07-03 02:58:21,799][188188] Max Reward on eval: 758.6656208192929[0m
[37m[1m[2023-07-03 02:58:21,799][188188] Min Reward on eval: -341.26020148377864[0m
[37m[1m[2023-07-03 02:58:21,799][188188] Mean Reward across all agents: 178.7348619113439[0m
[37m[1m[2023-07-03 02:58:21,800][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:58:21,802][188188] mean_value=-540.2848442793518, max_value=336.04255345269195[0m
[37m[1m[2023-07-03 02:58:21,805][188188] New mean coefficients: [[-2.5255413  -0.33907562 -2.1103852  -0.94389004 -0.97397304  1.7151988 ]][0m
[37m[1m[2023-07-03 02:58:21,806][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:58:30,748][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 02:58:30,748][188188] FPS: 429503.04[0m
[36m[2023-07-03 02:58:30,750][188188] itr=270, itrs=2000, Progress: 13.50%[0m
[37m[1m[2023-07-03 02:58:33,517][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000250[0m
[36m[2023-07-03 02:58:45,558][188188] train() took 11.70 seconds to complete[0m
[36m[2023-07-03 02:58:45,558][188188] FPS: 328092.61[0m
[36m[2023-07-03 02:58:49,786][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:58:49,786][188188] Reward + Measures: [[256.24661186   0.55410266   0.541475     0.60328132   0.27536634
    3.95318151]][0m
[37m[1m[2023-07-03 02:58:49,786][188188] Max Reward on eval: 256.24661185656606[0m
[37m[1m[2023-07-03 02:58:49,787][188188] Min Reward on eval: 256.24661185656606[0m
[37m[1m[2023-07-03 02:58:49,787][188188] Mean Reward across all agents: 256.24661185656606[0m
[37m[1m[2023-07-03 02:58:49,787][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:58:54,734][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:58:54,787][188188] Reward + Measures: [[  58.5997773     0.32119998    0.67140007    0.4844        0.5977
     3.86247802]
 [ 420.51583576    0.77010006    0.50450003    0.65490007    0.0441
     3.72213721]
 [-139.43924696    0.72229999    0.68290001    0.80739993    0.0313
     3.99864507]
 ...
 [ 314.58728788    0.9698        0.72609997    0.96630001    0.0117
     3.99217224]
 [-272.49988034    0.7022        0.68589997    0.85540003    0.0217
     3.99881911]
 [ 375.15675166    0.78610003    0.60659999    0.82370007    0.1365
     3.97084355]][0m
[37m[1m[2023-07-03 02:58:54,789][188188] Max Reward on eval: 648.3781280578114[0m
[37m[1m[2023-07-03 02:58:54,790][188188] Min Reward on eval: -438.4850826334208[0m
[37m[1m[2023-07-03 02:58:54,791][188188] Mean Reward across all agents: 135.6829090997501[0m
[37m[1m[2023-07-03 02:58:54,791][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:58:54,801][188188] mean_value=-1022.9445283192109, max_value=151.75022234040844[0m
[37m[1m[2023-07-03 02:58:54,811][188188] New mean coefficients: [[-2.320676   -0.08351204 -2.3063016  -1.3069487  -0.84628314  1.062608  ]][0m
[37m[1m[2023-07-03 02:58:54,816][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:59:03,803][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 02:59:03,804][188188] FPS: 427460.00[0m
[36m[2023-07-03 02:59:03,806][188188] itr=271, itrs=2000, Progress: 13.55%[0m
[36m[2023-07-03 02:59:15,389][188188] train() took 11.56 seconds to complete[0m
[36m[2023-07-03 02:59:15,389][188188] FPS: 332078.38[0m
[36m[2023-07-03 02:59:19,713][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:59:19,713][188188] Reward + Measures: [[226.38197868   0.51539397   0.55371964   0.55435067   0.29771701
    3.95588017]][0m
[37m[1m[2023-07-03 02:59:19,714][188188] Max Reward on eval: 226.38197867502691[0m
[37m[1m[2023-07-03 02:59:19,714][188188] Min Reward on eval: 226.38197867502691[0m
[37m[1m[2023-07-03 02:59:19,714][188188] Mean Reward across all agents: 226.38197867502691[0m
[37m[1m[2023-07-03 02:59:19,715][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:59:24,734][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:59:24,735][188188] Reward + Measures: [[ 76.89674551   0.4657       0.39359999   0.4883       0.37939999
    3.80639815]
 [  0.52340699   0.1445       0.26970002   0.18920001   0.20969999
    3.59026313]
 [-52.65540431   0.1416       0.15970001   0.1575       0.18060002
    3.73119044]
 ...
 [ 25.67372189   0.1062       0.11140001   0.0918       0.1027
    3.82479286]
 [136.32936483   0.2089       0.53479999   0.49309999   0.34720001
    3.71564102]
 [ 14.57572533   0.13869999   0.34010002   0.2422       0.3263
    3.5727582 ]][0m
[37m[1m[2023-07-03 02:59:24,735][188188] Max Reward on eval: 581.3176231585443[0m
[37m[1m[2023-07-03 02:59:24,735][188188] Min Reward on eval: -294.7120743341278[0m
[37m[1m[2023-07-03 02:59:24,735][188188] Mean Reward across all agents: 68.03836793146715[0m
[37m[1m[2023-07-03 02:59:24,736][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:59:24,737][188188] mean_value=-1825.628172435202, max_value=58.700808792186876[0m
[37m[1m[2023-07-03 02:59:24,740][188188] New mean coefficients: [[-2.7645645  -1.4986321  -1.7619562  -1.5467354  -0.4127346   0.96055275]][0m
[37m[1m[2023-07-03 02:59:24,741][188188] Moving the mean solution point...[0m
[36m[2023-07-03 02:59:33,805][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 02:59:33,805][188188] FPS: 423722.60[0m
[36m[2023-07-03 02:59:33,807][188188] itr=272, itrs=2000, Progress: 13.60%[0m
[36m[2023-07-03 02:59:45,404][188188] train() took 11.58 seconds to complete[0m
[36m[2023-07-03 02:59:45,404][188188] FPS: 331689.23[0m
[36m[2023-07-03 02:59:49,727][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:59:49,727][188188] Reward + Measures: [[340.37698786   0.70028597   0.60146165   0.72917366   0.19901933
    3.97118282]][0m
[37m[1m[2023-07-03 02:59:49,727][188188] Max Reward on eval: 340.37698785678754[0m
[37m[1m[2023-07-03 02:59:49,728][188188] Min Reward on eval: 340.37698785678754[0m
[37m[1m[2023-07-03 02:59:49,728][188188] Mean Reward across all agents: 340.37698785678754[0m
[37m[1m[2023-07-03 02:59:49,728][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:59:54,683][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 02:59:54,684][188188] Reward + Measures: [[125.07557727   0.34349999   0.62170005   0.43280002   0.46039996
    3.94879889]
 [104.13414249   0.22750001   0.52249998   0.35929999   0.4619
    3.77965999]
 [ 12.84553838   0.1177       0.1173       0.12749998   0.1178
    3.91771364]
 ...
 [190.09226421   0.45769998   0.48980004   0.53140002   0.36620003
    3.87028193]
 [ 19.17841309   0.15820001   0.17770001   0.12710001   0.1813
    3.89633942]
 [410.6206627    0.7561       0.64570004   0.79539996   0.16700001
    3.97175288]][0m
[37m[1m[2023-07-03 02:59:54,684][188188] Max Reward on eval: 543.423601857014[0m
[37m[1m[2023-07-03 02:59:54,685][188188] Min Reward on eval: -349.9089537112042[0m
[37m[1m[2023-07-03 02:59:54,685][188188] Mean Reward across all agents: 172.3856003437066[0m
[37m[1m[2023-07-03 02:59:54,685][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 02:59:54,687][188188] mean_value=-853.8674164015157, max_value=26.848988134367175[0m
[37m[1m[2023-07-03 02:59:54,690][188188] New mean coefficients: [[-3.2873013  -1.6136949  -1.5413284  -1.4053253   0.32723847  1.4101095 ]][0m
[37m[1m[2023-07-03 02:59:54,691][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:00:03,743][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 03:00:03,743][188188] FPS: 424292.81[0m
[36m[2023-07-03 03:00:03,746][188188] itr=273, itrs=2000, Progress: 13.65%[0m
[36m[2023-07-03 03:00:15,500][188188] train() took 11.73 seconds to complete[0m
[36m[2023-07-03 03:00:15,500][188188] FPS: 327236.08[0m
[36m[2023-07-03 03:00:19,808][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:00:19,808][188188] Reward + Measures: [[233.05310169   0.52887899   0.62134165   0.60595065   0.34612769
    3.96879554]][0m
[37m[1m[2023-07-03 03:00:19,808][188188] Max Reward on eval: 233.05310168900573[0m
[37m[1m[2023-07-03 03:00:19,809][188188] Min Reward on eval: 233.05310168900573[0m
[37m[1m[2023-07-03 03:00:19,809][188188] Mean Reward across all agents: 233.05310168900573[0m
[37m[1m[2023-07-03 03:00:19,809][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:00:24,968][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:00:24,968][188188] Reward + Measures: [[ 72.31662375   0.1428       0.57120007   0.19999999   0.51310003
    3.93085861]
 [422.00409701   0.84619999   0.66640007   0.85170001   0.1175
    3.97813988]
 [ -6.01243616   0.0902       0.20250002   0.10039999   0.19219999
    3.77054787]
 ...
 [172.07598875   0.45970002   0.55260003   0.51010001   0.33050004
    3.9329145 ]
 [ 39.92465333   0.23940001   0.68489999   0.32660002   0.59260005
    3.95502782]
 [197.8421668    0.56169999   0.79250002   0.71850002   0.42309999
    3.99209094]][0m
[37m[1m[2023-07-03 03:00:24,969][188188] Max Reward on eval: 430.4242390957661[0m
[37m[1m[2023-07-03 03:00:24,969][188188] Min Reward on eval: -168.31685830447822[0m
[37m[1m[2023-07-03 03:00:24,969][188188] Mean Reward across all agents: 133.6286256815211[0m
[37m[1m[2023-07-03 03:00:24,969][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:00:24,972][188188] mean_value=-704.5244028005396, max_value=20.20423794805282[0m
[37m[1m[2023-07-03 03:00:24,974][188188] New mean coefficients: [[-4.313189   -0.9272485  -1.3000335  -1.117831    0.11298478  0.42474544]][0m
[37m[1m[2023-07-03 03:00:24,975][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:00:34,047][188188] train() took 9.07 seconds to complete[0m
[36m[2023-07-03 03:00:34,047][188188] FPS: 423358.13[0m
[36m[2023-07-03 03:00:34,050][188188] itr=274, itrs=2000, Progress: 13.70%[0m
[36m[2023-07-03 03:00:45,770][188188] train() took 11.70 seconds to complete[0m
[36m[2023-07-03 03:00:45,770][188188] FPS: 328224.74[0m
[36m[2023-07-03 03:00:50,081][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:00:50,081][188188] Reward + Measures: [[128.98527054   0.32668668   0.62023735   0.400047     0.488534
    3.95982718]][0m
[37m[1m[2023-07-03 03:00:50,081][188188] Max Reward on eval: 128.98527053624693[0m
[37m[1m[2023-07-03 03:00:50,082][188188] Min Reward on eval: 128.98527053624693[0m
[37m[1m[2023-07-03 03:00:50,082][188188] Mean Reward across all agents: 128.98527053624693[0m
[37m[1m[2023-07-03 03:00:50,082][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:00:55,098][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:00:55,098][188188] Reward + Measures: [[80.66820214  0.31889996  0.67919999  0.3829      0.56240004  3.93837404]
 [51.16373122  0.15679999  0.42880002  0.18120001  0.3779      3.78074765]
 [54.34992963  0.1373      0.1247      0.0898      0.1219      3.88363338]
 ...
 [97.28987821  0.28220001  0.57560003  0.3186      0.44100004  3.93331766]
 [-6.22403964  0.16049999  0.1745      0.0979      0.18359999  3.90341043]
 [ 3.42531187  0.11740001  0.14820001  0.10339999  0.15000001  3.82565737]][0m
[37m[1m[2023-07-03 03:00:55,098][188188] Max Reward on eval: 283.4477032529656[0m
[37m[1m[2023-07-03 03:00:55,099][188188] Min Reward on eval: -164.1659771957173[0m
[37m[1m[2023-07-03 03:00:55,099][188188] Mean Reward across all agents: 45.704603971874626[0m
[37m[1m[2023-07-03 03:00:55,099][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:00:55,101][188188] mean_value=-2372.623322176463, max_value=19.671702194125857[0m
[37m[1m[2023-07-03 03:00:55,103][188188] New mean coefficients: [[-1.781071   -1.3694475  -1.0891752  -1.1751889  -0.94920087  1.9649881 ]][0m
[37m[1m[2023-07-03 03:00:55,104][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:01:04,112][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 03:01:04,113][188188] FPS: 426354.00[0m
[36m[2023-07-03 03:01:04,115][188188] itr=275, itrs=2000, Progress: 13.75%[0m
[36m[2023-07-03 03:01:15,669][188188] train() took 11.53 seconds to complete[0m
[36m[2023-07-03 03:01:15,669][188188] FPS: 332908.08[0m
[36m[2023-07-03 03:01:19,922][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:01:19,923][188188] Reward + Measures: [[224.47679125   0.49089697   0.53645635   0.53437668   0.33076668
    3.9614768 ]][0m
[37m[1m[2023-07-03 03:01:19,923][188188] Max Reward on eval: 224.47679124695222[0m
[37m[1m[2023-07-03 03:01:19,923][188188] Min Reward on eval: 224.47679124695222[0m
[37m[1m[2023-07-03 03:01:19,923][188188] Mean Reward across all agents: 224.47679124695222[0m
[37m[1m[2023-07-03 03:01:19,924][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:01:24,896][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:01:24,896][188188] Reward + Measures: [[427.61190033   0.91720003   0.65010005   0.95230001   0.0208
    3.96924067]
 [157.15935248   0.33390003   0.79050004   0.4808       0.63210005
    3.96141744]
 [ 74.32391186   0.29050002   0.54049999   0.30220002   0.43989998
    3.93424034]
 ...
 [ -0.67337111   0.0674       0.77829999   0.75889999   0.77100003
    3.9669137 ]
 [ 35.41234918   0.35660002   0.50660002   0.41540003   0.36829999
    3.78399634]
 [ 96.40046165   0.27390003   0.4598       0.26769999   0.34800002
    3.8296001 ]][0m
[37m[1m[2023-07-03 03:01:24,897][188188] Max Reward on eval: 506.7804603554308[0m
[37m[1m[2023-07-03 03:01:24,897][188188] Min Reward on eval: -293.8232984513044[0m
[37m[1m[2023-07-03 03:01:24,897][188188] Mean Reward across all agents: 134.02658574467648[0m
[37m[1m[2023-07-03 03:01:24,897][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:01:24,900][188188] mean_value=-737.8157439578337, max_value=200.3279441728447[0m
[37m[1m[2023-07-03 03:01:24,902][188188] New mean coefficients: [[-0.28661466 -1.0678958  -1.3731171  -1.5841312  -0.36330622  3.1875706 ]][0m
[37m[1m[2023-07-03 03:01:24,903][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:01:33,952][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 03:01:33,952][188188] FPS: 424427.19[0m
[36m[2023-07-03 03:01:33,955][188188] itr=276, itrs=2000, Progress: 13.80%[0m
[36m[2023-07-03 03:01:45,558][188188] train() took 11.58 seconds to complete[0m
[36m[2023-07-03 03:01:45,558][188188] FPS: 331595.17[0m
[36m[2023-07-03 03:01:49,917][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:01:49,917][188188] Reward + Measures: [[194.19270823   0.44560501   0.53989232   0.48695433   0.36285135
    3.96277332]][0m
[37m[1m[2023-07-03 03:01:49,918][188188] Max Reward on eval: 194.192708225067[0m
[37m[1m[2023-07-03 03:01:49,918][188188] Min Reward on eval: 194.192708225067[0m
[37m[1m[2023-07-03 03:01:49,918][188188] Mean Reward across all agents: 194.192708225067[0m
[37m[1m[2023-07-03 03:01:49,918][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:01:55,073][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:01:55,073][188188] Reward + Measures: [[ 43.19117694   0.1635       0.60210001   0.25289997   0.54470003
    3.85943604]
 [ 64.23691439   0.25190002   0.42570001   0.26910001   0.3705
    3.83238769]
 [ 62.76051007   0.18620001   0.2421       0.1885       0.22389999
    3.57327056]
 ...
 [ 29.35778374   0.64420003   0.9224       0.21600001   0.90979999
    3.97951102]
 [ 40.73837641   0.2218       0.43340001   0.24250002   0.35240003
    3.89457774]
 [136.25982379   0.29389998   0.52350003   0.31099999   0.42360002
    3.93577957]][0m
[37m[1m[2023-07-03 03:01:55,073][188188] Max Reward on eval: 397.49080944117156[0m
[37m[1m[2023-07-03 03:01:55,074][188188] Min Reward on eval: -466.86283536141275[0m
[37m[1m[2023-07-03 03:01:55,074][188188] Mean Reward across all agents: 81.39617086999468[0m
[37m[1m[2023-07-03 03:01:55,074][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:01:55,076][188188] mean_value=-1250.0579244529188, max_value=68.56984624581801[0m
[37m[1m[2023-07-03 03:01:55,078][188188] New mean coefficients: [[-0.47616583 -0.62265503 -1.6490781  -0.45050883 -0.00159711  2.482882  ]][0m
[37m[1m[2023-07-03 03:01:55,079][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:02:04,142][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 03:02:04,142][188188] FPS: 423775.58[0m
[36m[2023-07-03 03:02:04,145][188188] itr=277, itrs=2000, Progress: 13.85%[0m
[36m[2023-07-03 03:02:15,923][188188] train() took 11.76 seconds to complete[0m
[36m[2023-07-03 03:02:15,923][188188] FPS: 326666.52[0m
[36m[2023-07-03 03:02:20,129][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:02:20,130][188188] Reward + Measures: [[205.48973125   0.48237333   0.5488897    0.55015361   0.34170768
    3.9671073 ]][0m
[37m[1m[2023-07-03 03:02:20,130][188188] Max Reward on eval: 205.48973125418772[0m
[37m[1m[2023-07-03 03:02:20,130][188188] Min Reward on eval: 205.48973125418772[0m
[37m[1m[2023-07-03 03:02:20,130][188188] Mean Reward across all agents: 205.48973125418772[0m
[37m[1m[2023-07-03 03:02:20,131][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:02:25,276][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:02:25,282][188188] Reward + Measures: [[241.89909124   0.47010002   0.68370003   0.64500004   0.40569997
    3.97341418]
 [ 57.74037009   0.12049999   0.38480002   0.16049999   0.35499999
    3.92419553]
 [ 91.44474103   0.1497       0.69590002   0.3152       0.61750001
    3.9686203 ]
 ...
 [ 59.32982423   0.1244       0.92119998   0.45880005   0.86440009
    3.91455007]
 [ 82.78795261   0.20740001   0.56630003   0.25260001   0.49399996
    3.95047355]
 [ 66.63244627   0.20039999   0.37990001   0.2559       0.30169997
    3.85304952]][0m
[37m[1m[2023-07-03 03:02:25,283][188188] Max Reward on eval: 497.8765106111299[0m
[37m[1m[2023-07-03 03:02:25,283][188188] Min Reward on eval: -96.59056478939019[0m
[37m[1m[2023-07-03 03:02:25,283][188188] Mean Reward across all agents: 130.1874919356639[0m
[37m[1m[2023-07-03 03:02:25,283][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:02:25,285][188188] mean_value=-1042.23972334278, max_value=22.484712714805823[0m
[37m[1m[2023-07-03 03:02:25,288][188188] New mean coefficients: [[ 0.6424721   0.81200457 -1.5191095  -1.4508468   1.4757853   1.8649802 ]][0m
[37m[1m[2023-07-03 03:02:25,289][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:02:34,240][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 03:02:34,240][188188] FPS: 429080.94[0m
[36m[2023-07-03 03:02:34,242][188188] itr=278, itrs=2000, Progress: 13.90%[0m
[36m[2023-07-03 03:02:45,806][188188] train() took 11.54 seconds to complete[0m
[36m[2023-07-03 03:02:45,806][188188] FPS: 332633.30[0m
[36m[2023-07-03 03:02:50,091][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:02:50,092][188188] Reward + Measures: [[90.65286524  0.27477202  0.55053765  0.37804332  0.44590363  3.95611882]][0m
[37m[1m[2023-07-03 03:02:50,092][188188] Max Reward on eval: 90.65286523655469[0m
[37m[1m[2023-07-03 03:02:50,092][188188] Min Reward on eval: 90.65286523655469[0m
[37m[1m[2023-07-03 03:02:50,092][188188] Mean Reward across all agents: 90.65286523655469[0m
[37m[1m[2023-07-03 03:02:50,093][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:02:55,077][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:02:55,078][188188] Reward + Measures: [[112.76628045   0.32839999   0.67000002   0.46429998   0.53600001
    3.96379209]
 [175.36312676   0.43129998   0.57650006   0.53230006   0.35999998
    3.97221184]
 [ 78.42429912   0.22089998   0.54970002   0.3267       0.52140003
    3.7436142 ]
 ...
 [ 67.47039721   0.29650003   0.43290001   0.37079999   0.34650001
    3.94642639]
 [ 43.94689654   0.20650001   0.65319997   0.30200002   0.62459999
    3.94631243]
 [ 44.99949942   0.12019999   0.15450001   0.11429999   0.1374
    3.91781688]][0m
[37m[1m[2023-07-03 03:02:55,078][188188] Max Reward on eval: 515.7379646356218[0m
[37m[1m[2023-07-03 03:02:55,078][188188] Min Reward on eval: -336.99803548213094[0m
[37m[1m[2023-07-03 03:02:55,078][188188] Mean Reward across all agents: 110.75045717875895[0m
[37m[1m[2023-07-03 03:02:55,079][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:02:55,081][188188] mean_value=-838.0063903570206, max_value=37.141249360657326[0m
[37m[1m[2023-07-03 03:02:55,083][188188] New mean coefficients: [[ 0.47724417  0.5000911  -3.0334382  -2.0886161   2.1781883   2.8307815 ]][0m
[37m[1m[2023-07-03 03:02:55,084][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:03:04,082][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 03:03:04,083][188188] FPS: 426811.21[0m
[36m[2023-07-03 03:03:04,085][188188] itr=279, itrs=2000, Progress: 13.95%[0m
[36m[2023-07-03 03:03:15,718][188188] train() took 11.61 seconds to complete[0m
[36m[2023-07-03 03:03:15,719][188188] FPS: 330635.89[0m
[36m[2023-07-03 03:03:19,975][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:03:19,975][188188] Reward + Measures: [[93.51391882  0.28386164  0.56102163  0.39383635  0.44349062  3.95951223]][0m
[37m[1m[2023-07-03 03:03:19,976][188188] Max Reward on eval: 93.5139188172608[0m
[37m[1m[2023-07-03 03:03:19,976][188188] Min Reward on eval: 93.5139188172608[0m
[37m[1m[2023-07-03 03:03:19,976][188188] Mean Reward across all agents: 93.5139188172608[0m
[37m[1m[2023-07-03 03:03:19,976][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:03:25,001][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:03:25,002][188188] Reward + Measures: [[ 40.8912737    0.21280001   0.67560005   0.2793       0.55910009
    3.93506861]
 [171.75513387   0.44789997   0.7033       0.61250001   0.4488
    3.97770381]
 [ 90.33564716   0.23099999   0.49760005   0.2929       0.43280002
    3.94127846]
 ...
 [149.66329407   0.20509999   0.3863       0.22719999   0.3213
    3.82025003]
 [ 47.57295848   0.33270001   0.43540001   0.34300002   0.33440003
    3.86259532]
 [ -5.06663165   0.1681       0.40190002   0.18169999   0.32449999
    3.87516761]][0m
[37m[1m[2023-07-03 03:03:25,002][188188] Max Reward on eval: 386.17233851351773[0m
[37m[1m[2023-07-03 03:03:25,002][188188] Min Reward on eval: -48.001894291397186[0m
[37m[1m[2023-07-03 03:03:25,003][188188] Mean Reward across all agents: 88.60557244714633[0m
[37m[1m[2023-07-03 03:03:25,003][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:03:25,004][188188] mean_value=-1114.3281458789843, max_value=-12.757907292635565[0m
[36m[2023-07-03 03:03:25,007][188188] XNES is restarting with a new solution whose measures are [0.44229999 0.56510001 0.58639997 0.2859     3.88779831] and objective is 317.7074129211484[0m
[36m[2023-07-03 03:03:25,008][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 03:03:25,010][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 03:03:25,011][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:03:33,964][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 03:03:33,964][188188] FPS: 428961.21[0m
[36m[2023-07-03 03:03:33,967][188188] itr=280, itrs=2000, Progress: 14.00%[0m
[37m[1m[2023-07-03 03:03:36,770][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000260[0m
[36m[2023-07-03 03:03:48,912][188188] train() took 11.81 seconds to complete[0m
[36m[2023-07-03 03:03:48,912][188188] FPS: 325041.74[0m
[36m[2023-07-03 03:03:53,175][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:03:53,176][188188] Reward + Measures: [[-132.47954653    0.47676599    0.81395996    0.83095491    0.40295702
     3.99766469]][0m
[37m[1m[2023-07-03 03:03:53,176][188188] Max Reward on eval: -132.47954653261772[0m
[37m[1m[2023-07-03 03:03:53,176][188188] Min Reward on eval: -132.47954653261772[0m
[37m[1m[2023-07-03 03:03:53,177][188188] Mean Reward across all agents: -132.47954653261772[0m
[37m[1m[2023-07-03 03:03:53,177][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:03:58,228][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:03:58,229][188188] Reward + Measures: [[ -19.43050896    0.13530001    0.89289999    0.87700003    0.80620003
     3.99309707]
 [  74.83612871    0.21200001    0.6523        0.3369        0.61870003
     3.85695124]
 [ 265.15192272    0.5165        0.65310001    0.58519995    0.36219999
     3.96123433]
 ...
 [-176.37009279    0.42300001    0.78900003    0.80940002    0.41899997
     3.99487042]
 [  86.45504432    0.25009999    0.82639998    0.6814        0.60009998
     3.80365992]
 [ 374.51392935    0.73370004    0.66690004    0.792         0.19240001
     3.97292781]][0m
[37m[1m[2023-07-03 03:03:58,229][188188] Max Reward on eval: 406.5012140467996[0m
[37m[1m[2023-07-03 03:03:58,229][188188] Min Reward on eval: -353.9416977630928[0m
[37m[1m[2023-07-03 03:03:58,229][188188] Mean Reward across all agents: -25.979780036061054[0m
[37m[1m[2023-07-03 03:03:58,230][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:03:58,231][188188] mean_value=-653.9583367725077, max_value=122.56635067780817[0m
[37m[1m[2023-07-03 03:03:58,234][188188] New mean coefficients: [[ 0.8380455  -1.7287076  -1.1897722  -2.0496688  -2.1926045  -0.29621625]][0m
[37m[1m[2023-07-03 03:03:58,235][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:04:07,323][188188] train() took 9.09 seconds to complete[0m
[36m[2023-07-03 03:04:07,324][188188] FPS: 422579.36[0m
[36m[2023-07-03 03:04:07,326][188188] itr=281, itrs=2000, Progress: 14.05%[0m
[36m[2023-07-03 03:04:19,025][188188] train() took 11.68 seconds to complete[0m
[36m[2023-07-03 03:04:19,026][188188] FPS: 328764.42[0m
[36m[2023-07-03 03:04:23,332][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:04:23,332][188188] Reward + Measures: [[-164.97313834    0.56780267    0.83065236    0.86306703    0.31252933
     3.99839163]][0m
[37m[1m[2023-07-03 03:04:23,332][188188] Max Reward on eval: -164.9731383404912[0m
[37m[1m[2023-07-03 03:04:23,333][188188] Min Reward on eval: -164.9731383404912[0m
[37m[1m[2023-07-03 03:04:23,333][188188] Mean Reward across all agents: -164.9731383404912[0m
[37m[1m[2023-07-03 03:04:23,333][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:04:28,441][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:04:28,442][188188] Reward + Measures: [[-244.78805764    0.65200001    0.79019994    0.7802        0.22920001
     3.99760294]
 [ 100.02357717    0.30320001    0.59310001    0.34920001    0.45300004
     3.86936045]
 [  95.9781213     0.18069999    0.46739998    0.2694        0.46290001
     3.87952209]
 ...
 [  44.21900334    0.1939        0.49530002    0.289         0.45840001
     3.80635238]
 [ -77.35713899    0.56260002    0.97480005    0.97250003    0.42090002
     3.99810028]
 [-245.33410191    0.47150001    0.69670004    0.70060003    0.33320001
     3.99289441]][0m
[37m[1m[2023-07-03 03:04:28,442][188188] Max Reward on eval: 469.4121309817303[0m
[37m[1m[2023-07-03 03:04:28,442][188188] Min Reward on eval: -398.5464334502816[0m
[37m[1m[2023-07-03 03:04:28,443][188188] Mean Reward across all agents: -69.91822945507883[0m
[37m[1m[2023-07-03 03:04:28,443][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:04:28,444][188188] mean_value=-781.738448788381, max_value=52.728666375910336[0m
[37m[1m[2023-07-03 03:04:28,447][188188] New mean coefficients: [[ 0.9380871  -2.0017312  -1.1576921  -0.9118173  -1.92983    -0.03236243]][0m
[37m[1m[2023-07-03 03:04:28,448][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:04:37,391][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 03:04:37,392][188188] FPS: 429424.98[0m
[36m[2023-07-03 03:04:37,394][188188] itr=282, itrs=2000, Progress: 14.10%[0m
[36m[2023-07-03 03:04:49,056][188188] train() took 11.64 seconds to complete[0m
[36m[2023-07-03 03:04:49,056][188188] FPS: 329865.39[0m
[36m[2023-07-03 03:04:53,342][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:04:53,343][188188] Reward + Measures: [[-120.36243597    0.46290535    0.82853132    0.85382664    0.41618869
     3.99814296]][0m
[37m[1m[2023-07-03 03:04:53,343][188188] Max Reward on eval: -120.36243596532391[0m
[37m[1m[2023-07-03 03:04:53,343][188188] Min Reward on eval: -120.36243596532391[0m
[37m[1m[2023-07-03 03:04:53,343][188188] Mean Reward across all agents: -120.36243596532391[0m
[37m[1m[2023-07-03 03:04:53,344][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:04:58,336][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:04:58,336][188188] Reward + Measures: [[-216.9005499     0.69130003    0.97980005    0.97670001    0.29609999
     3.99137664]
 [   0.9072682     0.19769999    0.9702        0.96040004    0.78070003
     3.99744487]
 [ -78.72486437    0.2368        0.89589995    0.88260001    0.71090001
     3.99869204]
 ...
 [-117.69376876    0.4923        0.97869998    0.97679996    0.49440002
     3.99938512]
 [  17.7651675     0.71890002    0.625         0.72839999    0.12640001
     3.77248359]
 [ 595.75162781    0.7913        0.69890004    0.77459997    0.1322
     3.9308579 ]][0m
[37m[1m[2023-07-03 03:04:58,336][188188] Max Reward on eval: 595.751627809694[0m
[37m[1m[2023-07-03 03:04:58,337][188188] Min Reward on eval: -385.7346062904224[0m
[37m[1m[2023-07-03 03:04:58,337][188188] Mean Reward across all agents: -49.35010988908491[0m
[37m[1m[2023-07-03 03:04:58,337][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:04:58,339][188188] mean_value=-592.4980652605051, max_value=96.30268995592928[0m
[37m[1m[2023-07-03 03:04:58,341][188188] New mean coefficients: [[ 1.0457081  -2.4930515  -1.2085053  -1.0109379  -1.5986907  -0.05131131]][0m
[37m[1m[2023-07-03 03:04:58,342][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:05:07,330][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 03:05:07,330][188188] FPS: 427304.57[0m
[36m[2023-07-03 03:05:07,333][188188] itr=283, itrs=2000, Progress: 14.15%[0m
[36m[2023-07-03 03:05:18,957][188188] train() took 11.60 seconds to complete[0m
[36m[2023-07-03 03:05:18,957][188188] FPS: 330999.33[0m
[36m[2023-07-03 03:05:23,187][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:05:23,187][188188] Reward + Measures: [[-98.46086999   0.47102433   0.83984637   0.86592233   0.41490033
    3.99835229]][0m
[37m[1m[2023-07-03 03:05:23,187][188188] Max Reward on eval: -98.46086999320795[0m
[37m[1m[2023-07-03 03:05:23,188][188188] Min Reward on eval: -98.46086999320795[0m
[37m[1m[2023-07-03 03:05:23,188][188188] Mean Reward across all agents: -98.46086999320795[0m
[37m[1m[2023-07-03 03:05:23,188][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:05:28,142][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:05:28,142][188188] Reward + Measures: [[ -54.02720553    0.23769999    0.79190004    0.81960005    0.61300004
     3.99474883]
 [  84.94550197    0.3055        0.53470004    0.37090001    0.43579999
     3.8843019 ]
 [-179.42130757    0.88940001    0.82409996    0.87740004    0.058
     3.9942162 ]
 ...
 [-240.32457658    0.69310004    0.88640004    0.93050003    0.20320001
     3.99873519]
 [  49.51030195    0.1626        0.46199998    0.26700002    0.43820006
     3.75773287]
 [  13.5585097     0.2368        0.56940001    0.48799998    0.4323
     3.88193774]][0m
[37m[1m[2023-07-03 03:05:28,143][188188] Max Reward on eval: 320.8192968384363[0m
[37m[1m[2023-07-03 03:05:28,143][188188] Min Reward on eval: -399.9134059236385[0m
[37m[1m[2023-07-03 03:05:28,143][188188] Mean Reward across all agents: -38.05800169358059[0m
[37m[1m[2023-07-03 03:05:28,143][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:05:28,145][188188] mean_value=-860.456380134468, max_value=171.15039860506732[0m
[37m[1m[2023-07-03 03:05:28,147][188188] New mean coefficients: [[ 1.7190506  -2.3499415  -1.6216929  -0.8949519  -1.4717156  -0.16401988]][0m
[37m[1m[2023-07-03 03:05:28,148][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:05:37,117][188188] train() took 8.97 seconds to complete[0m
[36m[2023-07-03 03:05:37,118][188188] FPS: 428219.21[0m
[36m[2023-07-03 03:05:37,120][188188] itr=284, itrs=2000, Progress: 14.20%[0m
[36m[2023-07-03 03:05:48,633][188188] train() took 11.49 seconds to complete[0m
[36m[2023-07-03 03:05:48,633][188188] FPS: 334120.47[0m
[36m[2023-07-03 03:05:52,885][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:05:52,885][188188] Reward + Measures: [[-71.19548627   0.39052659   0.84810269   0.84948337   0.50408965
    3.99324942]][0m
[37m[1m[2023-07-03 03:05:52,885][188188] Max Reward on eval: -71.19548627049895[0m
[37m[1m[2023-07-03 03:05:52,886][188188] Min Reward on eval: -71.19548627049895[0m
[37m[1m[2023-07-03 03:05:52,886][188188] Mean Reward across all agents: -71.19548627049895[0m
[37m[1m[2023-07-03 03:05:52,886][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:05:57,859][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:05:57,859][188188] Reward + Measures: [[ 46.7234353    0.2545       0.79220003   0.47530004   0.65600002
    3.96134949]
 [116.27962039   0.37980005   0.44449997   0.41300002   0.296
    3.91052127]
 [119.57693102   0.38619998   0.75440001   0.55450004   0.54460001
    3.9698441 ]
 ...
 [ 30.73342953   0.3283       0.79320002   0.78369999   0.50949997
    3.99226427]
 [205.73397115   0.4303       0.77999997   0.6268       0.50239998
    3.97118545]
 [ 11.74386788   0.47779998   0.95460004   0.9533       0.50130004
    3.9985714 ]][0m
[37m[1m[2023-07-03 03:05:57,859][188188] Max Reward on eval: 402.38240336664023[0m
[37m[1m[2023-07-03 03:05:57,860][188188] Min Reward on eval: -250.61686982791872[0m
[37m[1m[2023-07-03 03:05:57,860][188188] Mean Reward across all agents: 4.626163428969805[0m
[37m[1m[2023-07-03 03:05:57,860][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:05:57,862][188188] mean_value=-581.7486233851037, max_value=36.94693539523843[0m
[37m[1m[2023-07-03 03:05:57,864][188188] New mean coefficients: [[ 1.8361055  -2.1854823  -1.6831437  -1.0681916  -2.239471    0.01066063]][0m
[37m[1m[2023-07-03 03:05:57,865][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:06:06,883][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 03:06:06,883][188188] FPS: 425891.41[0m
[36m[2023-07-03 03:06:06,886][188188] itr=285, itrs=2000, Progress: 14.25%[0m
[36m[2023-07-03 03:06:18,550][188188] train() took 11.64 seconds to complete[0m
[36m[2023-07-03 03:06:18,550][188188] FPS: 329806.69[0m
[36m[2023-07-03 03:06:22,917][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:06:22,918][188188] Reward + Measures: [[-75.37278867   0.5710597    0.83730769   0.78695995   0.30116567
    3.93634367]][0m
[37m[1m[2023-07-03 03:06:22,918][188188] Max Reward on eval: -75.37278867138623[0m
[37m[1m[2023-07-03 03:06:22,918][188188] Min Reward on eval: -75.37278867138623[0m
[37m[1m[2023-07-03 03:06:22,918][188188] Mean Reward across all agents: -75.37278867138623[0m
[37m[1m[2023-07-03 03:06:22,919][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:06:28,123][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:06:28,123][188188] Reward + Measures: [[  90.68177874    0.20969999    0.39809999    0.1813        0.3599
     3.84854579]
 [  36.29973428    0.16000001    0.56080002    0.26560003    0.55489999
     3.8770082 ]
 [-226.42074839    0.70290005    0.87790006    0.87849998    0.20630001
     3.99751067]
 ...
 [ 102.21245462    0.23050001    0.21900001    0.18170001    0.12410001
     3.92002845]
 [ 271.46514246    0.39210001    0.58249998    0.5521        0.33050004
     3.87469745]
 [  52.50423658    0.1389        0.48070002    0.19829999    0.50029999
     3.87940383]][0m
[37m[1m[2023-07-03 03:06:28,124][188188] Max Reward on eval: 412.4822415985167[0m
[37m[1m[2023-07-03 03:06:28,124][188188] Min Reward on eval: -365.8973092866596[0m
[37m[1m[2023-07-03 03:06:28,124][188188] Mean Reward across all agents: 48.54687293465305[0m
[37m[1m[2023-07-03 03:06:28,124][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:06:28,126][188188] mean_value=-901.6926926393284, max_value=30.59375895734763[0m
[37m[1m[2023-07-03 03:06:28,129][188188] New mean coefficients: [[ 1.4306562  -1.9821064  -1.3384758  -0.48047054 -2.1648314  -0.14371018]][0m
[37m[1m[2023-07-03 03:06:28,130][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:06:37,230][188188] train() took 9.10 seconds to complete[0m
[36m[2023-07-03 03:06:37,230][188188] FPS: 422054.93[0m
[36m[2023-07-03 03:06:37,232][188188] itr=286, itrs=2000, Progress: 14.30%[0m
[36m[2023-07-03 03:06:48,922][188188] train() took 11.67 seconds to complete[0m
[36m[2023-07-03 03:06:48,923][188188] FPS: 329030.09[0m
[36m[2023-07-03 03:06:53,238][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:06:53,238][188188] Reward + Measures: [[-107.45510501    0.45103633    0.86323702    0.89020574    0.45182198
     3.99799347]][0m
[37m[1m[2023-07-03 03:06:53,238][188188] Max Reward on eval: -107.4551050104833[0m
[37m[1m[2023-07-03 03:06:53,239][188188] Min Reward on eval: -107.4551050104833[0m
[37m[1m[2023-07-03 03:06:53,239][188188] Mean Reward across all agents: -107.4551050104833[0m
[37m[1m[2023-07-03 03:06:53,239][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:06:58,324][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:06:58,329][188188] Reward + Measures: [[  78.38467098    0.1258        0.13789999    0.1066        0.11260001
     3.68497896]
 [ 113.5825877     0.37079999    0.88170004    0.83759993    0.56110001
     3.95064163]
 [-162.41604581    0.61519998    0.87989998    0.83239996    0.2897
     3.92137694]
 ...
 [   9.2847287     0.0976        0.14          0.09100001    0.11360001
     3.77537346]
 [ 218.57637356    0.55739993    0.59920007    0.59770006    0.3351
     3.95393181]
 [-131.33288792    0.6207        0.87750006    0.88280004    0.31420001
     3.99739385]][0m
[37m[1m[2023-07-03 03:06:58,330][188188] Max Reward on eval: 421.41065063825806[0m
[37m[1m[2023-07-03 03:06:58,330][188188] Min Reward on eval: -357.0041857197881[0m
[37m[1m[2023-07-03 03:06:58,330][188188] Mean Reward across all agents: -43.09651278202732[0m
[37m[1m[2023-07-03 03:06:58,330][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:06:58,332][188188] mean_value=-860.514451561064, max_value=317.75368563994266[0m
[37m[1m[2023-07-03 03:06:58,335][188188] New mean coefficients: [[ 2.0436282 -1.5648093 -1.4029726 -0.5106476 -2.2712693 -0.5717597]][0m
[37m[1m[2023-07-03 03:06:58,336][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:07:07,395][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 03:07:07,395][188188] FPS: 423946.11[0m
[36m[2023-07-03 03:07:07,398][188188] itr=287, itrs=2000, Progress: 14.35%[0m
[36m[2023-07-03 03:07:18,998][188188] train() took 11.58 seconds to complete[0m
[36m[2023-07-03 03:07:18,998][188188] FPS: 331585.58[0m
[36m[2023-07-03 03:07:23,206][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:07:23,206][188188] Reward + Measures: [[-159.88513676    0.60624766    0.78710598    0.83534729    0.24109098
     3.99794745]][0m
[37m[1m[2023-07-03 03:07:23,206][188188] Max Reward on eval: -159.88513676186778[0m
[37m[1m[2023-07-03 03:07:23,207][188188] Min Reward on eval: -159.88513676186778[0m
[37m[1m[2023-07-03 03:07:23,207][188188] Mean Reward across all agents: -159.88513676186778[0m
[37m[1m[2023-07-03 03:07:23,207][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:07:28,163][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:07:28,163][188188] Reward + Measures: [[-189.65977528    0.64420003    0.78190005    0.77780002    0.22320001
     3.99490142]
 [ 220.5775049     0.6965        0.5133        0.6534        0.0517
     3.65935826]
 [ 106.37982438    0.59610003    0.47050005    0.4851        0.0668
     3.95072865]
 ...
 [ -41.18403584    0.30700001    0.6103        0.64970005    0.4499
     3.99470305]
 [ -42.91360169    0.32500002    0.32179999    0.3671        0.16070001
     3.98654032]
 [-148.755991      0.63169998    0.81950009    0.74369997    0.26890001
     3.7595284 ]][0m
[37m[1m[2023-07-03 03:07:28,163][188188] Max Reward on eval: 458.9193375072442[0m
[37m[1m[2023-07-03 03:07:28,164][188188] Min Reward on eval: -360.74869772234814[0m
[37m[1m[2023-07-03 03:07:28,164][188188] Mean Reward across all agents: -32.821287068572026[0m
[37m[1m[2023-07-03 03:07:28,164][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:07:28,166][188188] mean_value=-700.9665497739541, max_value=195.08764001636655[0m
[37m[1m[2023-07-03 03:07:28,168][188188] New mean coefficients: [[ 2.2864904  -0.9790661  -1.5179086  -0.62276834 -1.842665   -1.3101178 ]][0m
[37m[1m[2023-07-03 03:07:28,169][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:07:37,174][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 03:07:37,174][188188] FPS: 426547.06[0m
[36m[2023-07-03 03:07:37,176][188188] itr=288, itrs=2000, Progress: 14.40%[0m
[36m[2023-07-03 03:07:48,749][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 03:07:48,749][188188] FPS: 332363.96[0m
[36m[2023-07-03 03:07:53,007][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:07:53,008][188188] Reward + Measures: [[-108.89216729    0.48235631    0.78230631    0.83375734    0.35491133
     3.99723291]][0m
[37m[1m[2023-07-03 03:07:53,008][188188] Max Reward on eval: -108.89216728817314[0m
[37m[1m[2023-07-03 03:07:53,008][188188] Min Reward on eval: -108.89216728817314[0m
[37m[1m[2023-07-03 03:07:53,009][188188] Mean Reward across all agents: -108.89216728817314[0m
[37m[1m[2023-07-03 03:07:53,009][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:07:58,007][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:07:58,013][188188] Reward + Measures: [[ -21.95858782    0.23550002    0.59440005    0.64720005    0.48210001
     3.97363591]
 [  27.46281986    0.0015        0.98860008    0.97199994    0.98509997
     3.9941361 ]
 [-182.31889878    0.73680001    0.68919998    0.81110001    0.0318
     3.99851489]
 ...
 [ -14.27221058    0.29839998    0.3405        0.1314        0.30580002
     3.93967795]
 [-236.03032261    0.53920001    0.6882        0.76459998    0.2263
     3.99798131]
 [-327.71056414    0.88950008    0.9641        0.97480005    0.1017
     3.9974556 ]][0m
[37m[1m[2023-07-03 03:07:58,013][188188] Max Reward on eval: 401.84593521570787[0m
[37m[1m[2023-07-03 03:07:58,014][188188] Min Reward on eval: -327.71056414330377[0m
[37m[1m[2023-07-03 03:07:58,014][188188] Mean Reward across all agents: -26.13944297446712[0m
[37m[1m[2023-07-03 03:07:58,014][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:07:58,015][188188] mean_value=-1436.8655157824016, max_value=-33.02643309603991[0m
[36m[2023-07-03 03:07:58,018][188188] XNES is restarting with a new solution whose measures are [0.62650007 0.88830006 0.47390005 0.52530003 3.92436266] and objective is 179.3255739443004[0m
[36m[2023-07-03 03:07:58,019][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 03:07:58,021][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 03:07:58,022][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:08:07,016][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 03:08:07,016][188188] FPS: 427040.39[0m
[36m[2023-07-03 03:08:07,018][188188] itr=289, itrs=2000, Progress: 14.45%[0m
[36m[2023-07-03 03:08:18,716][188188] train() took 11.68 seconds to complete[0m
[36m[2023-07-03 03:08:18,717][188188] FPS: 328843.89[0m
[36m[2023-07-03 03:08:23,027][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:08:23,027][188188] Reward + Measures: [[111.8376858    0.38386261   0.80336326   0.78490764   0.44722399
    3.83023119]][0m
[37m[1m[2023-07-03 03:08:23,028][188188] Max Reward on eval: 111.8376858000751[0m
[37m[1m[2023-07-03 03:08:23,028][188188] Min Reward on eval: 111.8376858000751[0m
[37m[1m[2023-07-03 03:08:23,028][188188] Mean Reward across all agents: 111.8376858000751[0m
[37m[1m[2023-07-03 03:08:23,028][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:08:28,006][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:08:28,007][188188] Reward + Measures: [[  66.63823999    0.35620001    0.597         0.44980001    0.41440001
     3.8876636 ]
 [-182.09191416    0.88549995    0.96320003    0.43339998    0.58149999
     3.98081851]
 [-132.24194717    0.53100002    0.86700004    0.32640001    0.83990002
     3.99241138]
 ...
 [ 149.86817554    0.3946        0.65109998    0.57260007    0.32619998
     3.81535316]
 [ 128.66795495    0.31469998    0.69370002    0.39400002    0.53030002
     3.92111659]
 [  73.99782181    0.72219998    0.86280006    0.1312        0.81639999
     3.91415453]][0m
[37m[1m[2023-07-03 03:08:28,007][188188] Max Reward on eval: 391.0665588743985[0m
[37m[1m[2023-07-03 03:08:28,008][188188] Min Reward on eval: -433.18821856547146[0m
[37m[1m[2023-07-03 03:08:28,008][188188] Mean Reward across all agents: 3.762976407918497[0m
[37m[1m[2023-07-03 03:08:28,008][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:08:28,011][188188] mean_value=-726.434666606956, max_value=497.0222482336686[0m
[37m[1m[2023-07-03 03:08:28,014][188188] New mean coefficients: [[ 0.6355041 -1.6056857 -0.9150992 -2.419384  -2.184023  -1.5631244]][0m
[37m[1m[2023-07-03 03:08:28,015][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:08:37,079][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 03:08:37,079][188188] FPS: 423723.27[0m
[36m[2023-07-03 03:08:37,081][188188] itr=290, itrs=2000, Progress: 14.50%[0m
[37m[1m[2023-07-03 03:08:40,068][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000270[0m
[36m[2023-07-03 03:08:52,045][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 03:08:52,045][188188] FPS: 329692.37[0m
[36m[2023-07-03 03:08:56,238][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:08:56,239][188188] Reward + Measures: [[21.02066211  0.22095302  0.30060264  0.15029567  0.27890602  3.78184295]][0m
[37m[1m[2023-07-03 03:08:56,239][188188] Max Reward on eval: 21.020662110667107[0m
[37m[1m[2023-07-03 03:08:56,239][188188] Min Reward on eval: 21.020662110667107[0m
[37m[1m[2023-07-03 03:08:56,240][188188] Mean Reward across all agents: 21.020662110667107[0m
[37m[1m[2023-07-03 03:08:56,240][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:09:01,218][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:09:01,219][188188] Reward + Measures: [[-244.0777227     0.19399999    0.82559997    0.61479998    0.73350006
     3.89422798]
 [ -45.40538327    0.48179999    0.5424        0.0566        0.58219999
     3.84610605]
 [   7.87496831    0.1033        0.09860001    0.07700001    0.1163
     3.77053499]
 ...
 [ 126.45462509    0.35460001    0.38949999    0.29000002    0.1499
     3.54822922]
 [  -3.18044198    0.17940001    0.16779999    0.171         0.1688
     3.69023061]
 [  22.48819874    0.20070003    0.32540002    0.2172        0.24860001
     3.7595849 ]][0m
[37m[1m[2023-07-03 03:09:01,219][188188] Max Reward on eval: 392.95340065364724[0m
[37m[1m[2023-07-03 03:09:01,219][188188] Min Reward on eval: -429.0236150704324[0m
[37m[1m[2023-07-03 03:09:01,219][188188] Mean Reward across all agents: 4.015855798465113[0m
[37m[1m[2023-07-03 03:09:01,220][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:09:01,223][188188] mean_value=-1315.42450380322, max_value=400.0957052798011[0m
[37m[1m[2023-07-03 03:09:01,225][188188] New mean coefficients: [[-0.09677875 -2.1027985  -0.48670915 -1.4868878  -3.4742708  -1.8914258 ]][0m
[37m[1m[2023-07-03 03:09:01,226][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:09:10,152][188188] train() took 8.92 seconds to complete[0m
[36m[2023-07-03 03:09:10,152][188188] FPS: 430284.40[0m
[36m[2023-07-03 03:09:10,155][188188] itr=291, itrs=2000, Progress: 14.55%[0m
[36m[2023-07-03 03:09:21,773][188188] train() took 11.60 seconds to complete[0m
[36m[2023-07-03 03:09:21,773][188188] FPS: 331067.97[0m
[36m[2023-07-03 03:09:25,977][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:09:25,978][188188] Reward + Measures: [[98.01045207  0.32172331  0.34647468  0.25938135  0.18617399  3.81459641]][0m
[37m[1m[2023-07-03 03:09:25,978][188188] Max Reward on eval: 98.01045206551572[0m
[37m[1m[2023-07-03 03:09:25,978][188188] Min Reward on eval: 98.01045206551572[0m
[37m[1m[2023-07-03 03:09:25,978][188188] Mean Reward across all agents: 98.01045206551572[0m
[37m[1m[2023-07-03 03:09:25,979][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:09:30,913][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:09:30,914][188188] Reward + Measures: [[ 236.70475832    0.48960003    0.54310006    0.17949998    0.44570002
     3.8441956 ]
 [ -97.67256888    0.1398        0.25780001    0.1584        0.28140002
     3.85713363]
 [  14.78007986    0.60170001    0.61210006    0.0955        0.62830001
     3.93733764]
 ...
 [-136.09452099    0.61729997    0.63860005    0.12180001    0.59020001
     3.95277023]
 [  -5.51026521    0.1497        0.24690001    0.15140001    0.23550001
     3.7246964 ]
 [ -72.1558423     0.54390001    0.81870002    0.33410001    0.62540001
     3.96963429]][0m
[37m[1m[2023-07-03 03:09:30,914][188188] Max Reward on eval: 378.61781402938067[0m
[37m[1m[2023-07-03 03:09:30,915][188188] Min Reward on eval: -423.640331149142[0m
[37m[1m[2023-07-03 03:09:30,915][188188] Mean Reward across all agents: 7.122255652301734[0m
[37m[1m[2023-07-03 03:09:30,915][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:09:30,917][188188] mean_value=-1957.9732276447126, max_value=446.20378559739464[0m
[37m[1m[2023-07-03 03:09:30,920][188188] New mean coefficients: [[-1.039602   -0.53630054  0.6854714  -1.5202152  -3.6773493  -1.3807034 ]][0m
[37m[1m[2023-07-03 03:09:30,920][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:09:39,822][188188] train() took 8.90 seconds to complete[0m
[36m[2023-07-03 03:09:39,823][188188] FPS: 431454.25[0m
[36m[2023-07-03 03:09:39,825][188188] itr=292, itrs=2000, Progress: 14.60%[0m
[36m[2023-07-03 03:09:51,393][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 03:09:51,394][188188] FPS: 332498.18[0m
[36m[2023-07-03 03:09:55,677][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:09:55,678][188188] Reward + Measures: [[24.58036175  0.18897466  0.19198999  0.14513633  0.15170734  3.74649024]][0m
[37m[1m[2023-07-03 03:09:55,678][188188] Max Reward on eval: 24.580361753833053[0m
[37m[1m[2023-07-03 03:09:55,678][188188] Min Reward on eval: 24.580361753833053[0m
[37m[1m[2023-07-03 03:09:55,679][188188] Mean Reward across all agents: 24.580361753833053[0m
[37m[1m[2023-07-03 03:09:55,679][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:10:00,776][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:10:00,777][188188] Reward + Measures: [[ 16.13547641   0.122        0.1033       0.0816       0.1204
    3.82263565]
 [396.63705827   0.77469999   0.95880002   0.565        0.55059999
    3.99353027]
 [142.41727915   0.83220005   0.8427       0.57790005   0.29499999
    3.76555634]
 ...
 [145.65868262   0.6663       0.69220001   0.25910005   0.53030002
    3.93784833]
 [ 49.44744466   0.12089999   0.0938       0.092        0.0847
    3.79099727]
 [  6.43958993   0.206        0.17340001   0.11730001   0.17640002
    3.75244713]][0m
[37m[1m[2023-07-03 03:10:00,777][188188] Max Reward on eval: 444.8811130689457[0m
[37m[1m[2023-07-03 03:10:00,777][188188] Min Reward on eval: -335.41768835680557[0m
[37m[1m[2023-07-03 03:10:00,777][188188] Mean Reward across all agents: 68.86832719548995[0m
[37m[1m[2023-07-03 03:10:00,778][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:10:00,781][188188] mean_value=-2501.8978731084007, max_value=395.2609596080857[0m
[37m[1m[2023-07-03 03:10:00,783][188188] New mean coefficients: [[-1.6128043  -1.1214745   2.6118422  -2.4391975  -2.1325674   0.20519304]][0m
[37m[1m[2023-07-03 03:10:00,784][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:10:09,882][188188] train() took 9.10 seconds to complete[0m
[36m[2023-07-03 03:10:09,882][188188] FPS: 422189.27[0m
[36m[2023-07-03 03:10:09,884][188188] itr=293, itrs=2000, Progress: 14.65%[0m
[36m[2023-07-03 03:10:21,680][188188] train() took 11.78 seconds to complete[0m
[36m[2023-07-03 03:10:21,680][188188] FPS: 326080.90[0m
[36m[2023-07-03 03:10:25,989][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:10:25,990][188188] Reward + Measures: [[-37.61643968   0.28974268   0.31003299   0.13650367   0.28446001
    3.81859016]][0m
[37m[1m[2023-07-03 03:10:25,990][188188] Max Reward on eval: -37.61643968238885[0m
[37m[1m[2023-07-03 03:10:25,990][188188] Min Reward on eval: -37.61643968238885[0m
[37m[1m[2023-07-03 03:10:25,991][188188] Mean Reward across all agents: -37.61643968238885[0m
[37m[1m[2023-07-03 03:10:25,991][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:10:31,161][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:10:31,168][188188] Reward + Measures: [[   3.16438013    0.59249997    0.60359997    0.10569999    0.58530009
     3.96880031]
 [-199.59647609    0.68610001    0.7274        0.10829999    0.69950002
     3.98126674]
 [ -45.005923      0.1973        0.43530002    0.20190001    0.39480001
     3.88728523]
 ...
 [   3.9541969     0.13060001    0.11589999    0.0861        0.11489999
     3.83083606]
 [ -52.14979645    0.31060001    0.62190002    0.22410002    0.55489999
     3.93336463]
 [-141.70577624    0.65310001    0.67309999    0.12199999    0.6099
     3.93225646]][0m
[37m[1m[2023-07-03 03:10:31,168][188188] Max Reward on eval: 304.6960354254581[0m
[37m[1m[2023-07-03 03:10:31,168][188188] Min Reward on eval: -352.48454213202933[0m
[37m[1m[2023-07-03 03:10:31,169][188188] Mean Reward across all agents: -8.737722801760299[0m
[37m[1m[2023-07-03 03:10:31,169][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:10:31,171][188188] mean_value=-1669.0148032349518, max_value=76.29627815782885[0m
[37m[1m[2023-07-03 03:10:31,173][188188] New mean coefficients: [[-0.9280333  -0.91687703  2.96773    -3.5475218  -2.7543254  -0.6085456 ]][0m
[37m[1m[2023-07-03 03:10:31,174][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:10:40,207][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 03:10:40,207][188188] FPS: 425188.80[0m
[36m[2023-07-03 03:10:40,209][188188] itr=294, itrs=2000, Progress: 14.70%[0m
[36m[2023-07-03 03:10:51,879][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 03:10:51,879][188188] FPS: 329612.71[0m
[36m[2023-07-03 03:10:56,132][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:10:56,132][188188] Reward + Measures: [[-68.36031652   0.461173     0.49110934   0.12005399   0.46088699
    3.8495183 ]][0m
[37m[1m[2023-07-03 03:10:56,132][188188] Max Reward on eval: -68.3603165248063[0m
[37m[1m[2023-07-03 03:10:56,133][188188] Min Reward on eval: -68.3603165248063[0m
[37m[1m[2023-07-03 03:10:56,133][188188] Mean Reward across all agents: -68.3603165248063[0m
[37m[1m[2023-07-03 03:10:56,133][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:11:01,165][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:11:01,181][188188] Reward + Measures: [[-57.12760012   0.13950001   0.1574       0.10339999   0.16830002
    3.824229  ]
 [ -2.81313822   0.3118       0.4118       0.1521       0.33870003
    3.82443404]
 [-10.35583033   0.10930001   0.14719999   0.1229       0.17110002
    3.86409044]
 ...
 [ 27.84834043   0.36880001   0.40760002   0.1055       0.44800001
    3.93940282]
 [-23.38948942   0.13090001   0.1577       0.1234       0.18269999
    3.79409409]
 [ 38.97672851   0.1168       0.16059999   0.1166       0.1611
    3.83676505]][0m
[37m[1m[2023-07-03 03:11:01,181][188188] Max Reward on eval: 298.9875431060791[0m
[37m[1m[2023-07-03 03:11:01,181][188188] Min Reward on eval: -204.4800705615431[0m
[37m[1m[2023-07-03 03:11:01,181][188188] Mean Reward across all agents: 12.940381581614892[0m
[37m[1m[2023-07-03 03:11:01,182][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:11:01,183][188188] mean_value=-2977.1817981673985, max_value=154.56421690590668[0m
[37m[1m[2023-07-03 03:11:01,185][188188] New mean coefficients: [[-1.5299821  -0.55867755  1.6306896  -1.6663944  -3.5874333   0.7309062 ]][0m
[37m[1m[2023-07-03 03:11:01,186][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:11:10,165][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 03:11:10,166][188188] FPS: 427739.29[0m
[36m[2023-07-03 03:11:10,168][188188] itr=295, itrs=2000, Progress: 14.75%[0m
[36m[2023-07-03 03:11:21,686][188188] train() took 11.50 seconds to complete[0m
[36m[2023-07-03 03:11:21,686][188188] FPS: 333961.03[0m
[36m[2023-07-03 03:11:25,936][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:11:25,936][188188] Reward + Measures: [[-50.93956647   0.6201157    0.63308865   0.12435933   0.597256
    3.95295501]][0m
[37m[1m[2023-07-03 03:11:25,937][188188] Max Reward on eval: -50.939566466512545[0m
[37m[1m[2023-07-03 03:11:25,937][188188] Min Reward on eval: -50.939566466512545[0m
[37m[1m[2023-07-03 03:11:25,937][188188] Mean Reward across all agents: -50.939566466512545[0m
[37m[1m[2023-07-03 03:11:25,937][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:11:30,934][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:11:30,935][188188] Reward + Measures: [[259.2678294    0.50410002   0.78720009   0.37460002   0.64420003
    3.90811229]
 [151.40553143   0.34559998   0.36899999   0.0632       0.40959999
    3.85737228]
 [ 82.32327726   0.20710002   0.21089999   0.13710001   0.1921
    3.82820487]
 ...
 [ 40.22904757   0.14420001   0.35960001   0.2112       0.32249999
    3.81363869]
 [ 57.51858185   0.1346       0.1419       0.1036       0.14509998
    3.82459807]
 [-13.7289236    0.12150001   0.12820001   0.101        0.13259999
    3.85217166]][0m
[37m[1m[2023-07-03 03:11:30,935][188188] Max Reward on eval: 282.6020994481631[0m
[37m[1m[2023-07-03 03:11:30,935][188188] Min Reward on eval: -459.48779665753244[0m
[37m[1m[2023-07-03 03:11:30,936][188188] Mean Reward across all agents: 17.708995745564607[0m
[37m[1m[2023-07-03 03:11:30,936][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:11:30,938][188188] mean_value=-1870.9967363064488, max_value=213.9405727714411[0m
[37m[1m[2023-07-03 03:11:30,940][188188] New mean coefficients: [[-0.01119864 -0.3643877   1.9034736  -1.1906301  -3.9924197   1.0769644 ]][0m
[37m[1m[2023-07-03 03:11:30,941][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:11:39,865][188188] train() took 8.92 seconds to complete[0m
[36m[2023-07-03 03:11:39,865][188188] FPS: 430383.53[0m
[36m[2023-07-03 03:11:39,867][188188] itr=296, itrs=2000, Progress: 14.80%[0m
[36m[2023-07-03 03:11:51,440][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 03:11:51,440][188188] FPS: 332380.38[0m
[36m[2023-07-03 03:11:55,729][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:11:55,730][188188] Reward + Measures: [[20.04062821  0.51847297  0.53804064  0.08772534  0.55030471  3.86189485]][0m
[37m[1m[2023-07-03 03:11:55,730][188188] Max Reward on eval: 20.040628213596502[0m
[37m[1m[2023-07-03 03:11:55,730][188188] Min Reward on eval: 20.040628213596502[0m
[37m[1m[2023-07-03 03:11:55,731][188188] Mean Reward across all agents: 20.040628213596502[0m
[37m[1m[2023-07-03 03:11:55,731][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:12:00,706][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:12:00,706][188188] Reward + Measures: [[   3.44193979    0.0435        0.71130008    0.41069999    0.57389998
     3.83676791]
 [   4.54604515    0.1399        0.1488        0.0986        0.15079999
     3.81016612]
 [  25.94625129    0.12620001    0.11600001    0.085         0.10480001
     3.84615445]
 ...
 [   9.47345272    0.1681        0.15320002    0.12449999    0.10030001
     3.78420949]
 [  -5.89459999    0.0939        0.1086        0.0717        0.0935
     3.8603673 ]
 [-167.86570976    0.56569999    0.61860001    0.05830001    0.64270002
     3.95895648]][0m
[37m[1m[2023-07-03 03:12:00,706][188188] Max Reward on eval: 406.84447482619436[0m
[37m[1m[2023-07-03 03:12:00,707][188188] Min Reward on eval: -335.3268080114882[0m
[37m[1m[2023-07-03 03:12:00,707][188188] Mean Reward across all agents: 4.804778542154599[0m
[37m[1m[2023-07-03 03:12:00,707][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:12:00,709][188188] mean_value=-2153.8585668710816, max_value=209.77760813254454[0m
[37m[1m[2023-07-03 03:12:00,711][188188] New mean coefficients: [[ 0.06975024 -1.5160952   0.9496566  -0.94138825 -5.576874    0.29845357]][0m
[37m[1m[2023-07-03 03:12:00,712][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:12:09,646][188188] train() took 8.93 seconds to complete[0m
[36m[2023-07-03 03:12:09,646][188188] FPS: 429914.18[0m
[36m[2023-07-03 03:12:09,648][188188] itr=297, itrs=2000, Progress: 14.85%[0m
[36m[2023-07-03 03:12:21,276][188188] train() took 11.61 seconds to complete[0m
[36m[2023-07-03 03:12:21,277][188188] FPS: 330821.93[0m
[36m[2023-07-03 03:12:25,511][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:12:25,511][188188] Reward + Measures: [[-121.54305716    0.63933301    0.64900732    0.10513333    0.632083
     3.97459507]][0m
[37m[1m[2023-07-03 03:12:25,512][188188] Max Reward on eval: -121.54305716480606[0m
[37m[1m[2023-07-03 03:12:25,512][188188] Min Reward on eval: -121.54305716480606[0m
[37m[1m[2023-07-03 03:12:25,512][188188] Mean Reward across all agents: -121.54305716480606[0m
[37m[1m[2023-07-03 03:12:25,512][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:12:30,476][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:12:30,488][188188] Reward + Measures: [[ 27.87444516   0.21529999   0.2411       0.0944       0.28330001
    3.85642362]
 [ 36.77507188   0.0935       0.0962       0.0805       0.1035
    3.89320874]
 [-63.56124827   0.69029999   0.70360005   0.1244       0.66759998
    3.98190308]
 ...
 [ -2.04698277   0.14         0.1382       0.0865       0.15000001
    3.91205072]
 [ 67.91074277   0.1156       0.1115       0.08979999   0.11870001
    3.85101008]
 [ 91.68801278   0.39610001   0.44530001   0.0494       0.53029996
    3.85242343]][0m
[37m[1m[2023-07-03 03:12:30,488][188188] Max Reward on eval: 91.6880127842538[0m
[37m[1m[2023-07-03 03:12:30,489][188188] Min Reward on eval: -281.48121454957874[0m
[37m[1m[2023-07-03 03:12:30,489][188188] Mean Reward across all agents: -27.254695900452724[0m
[37m[1m[2023-07-03 03:12:30,489][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:12:30,490][188188] mean_value=-2998.274830119755, max_value=-216.71812955028318[0m
[36m[2023-07-03 03:12:30,493][188188] XNES is restarting with a new solution whose measures are [0.07789999 0.1859     0.30109999 0.17209999 3.93380022] and objective is 19.428084422647952[0m
[36m[2023-07-03 03:12:30,494][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 03:12:30,496][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 03:12:30,497][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:12:39,460][188188] train() took 8.96 seconds to complete[0m
[36m[2023-07-03 03:12:39,460][188188] FPS: 428494.17[0m
[36m[2023-07-03 03:12:39,463][188188] itr=298, itrs=2000, Progress: 14.90%[0m
[36m[2023-07-03 03:12:51,213][188188] train() took 11.73 seconds to complete[0m
[36m[2023-07-03 03:12:51,213][188188] FPS: 327405.10[0m
[36m[2023-07-03 03:12:55,548][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:12:55,548][188188] Reward + Measures: [[13.03090602  0.07490066  0.10614333  0.14235567  0.10908034  3.92116523]][0m
[37m[1m[2023-07-03 03:12:55,549][188188] Max Reward on eval: 13.030906016244403[0m
[37m[1m[2023-07-03 03:12:55,549][188188] Min Reward on eval: 13.030906016244403[0m
[37m[1m[2023-07-03 03:12:55,549][188188] Mean Reward across all agents: 13.030906016244403[0m
[37m[1m[2023-07-03 03:12:55,549][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:13:00,730][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:13:00,730][188188] Reward + Measures: [[ 10.82064278   0.1365       0.1476       0.14390001   0.14600001
    3.82015157]
 [ 20.23634964   0.0769       0.0875       0.0763       0.08459999
    3.90829277]
 [ 23.26608643   0.09550001   0.16339999   0.1286       0.15010001
    3.88914537]
 ...
 [-21.69148643   0.086        0.10120001   0.07120001   0.0803
    3.78695869]
 [ 27.80770188   0.0897       0.11670001   0.09760001   0.1051
    3.72265863]
 [ 33.98314217   0.0981       0.1303       0.0984       0.1177
    3.86679626]][0m
[37m[1m[2023-07-03 03:13:00,731][188188] Max Reward on eval: 243.51807240177877[0m
[37m[1m[2023-07-03 03:13:00,731][188188] Min Reward on eval: -98.48389150942675[0m
[37m[1m[2023-07-03 03:13:00,731][188188] Mean Reward across all agents: 11.110379731872767[0m
[37m[1m[2023-07-03 03:13:00,731][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:13:00,732][188188] mean_value=-3820.733936702606, max_value=-103.2693668079497[0m
[36m[2023-07-03 03:13:00,734][188188] XNES is restarting with a new solution whose measures are [0.23109999 0.50049996 0.72509998 0.3328     3.99330449] and objective is -19.126860891282558[0m
[36m[2023-07-03 03:13:00,735][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 03:13:00,738][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 03:13:00,739][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:13:09,813][188188] train() took 9.07 seconds to complete[0m
[36m[2023-07-03 03:13:09,814][188188] FPS: 423208.05[0m
[36m[2023-07-03 03:13:09,816][188188] itr=299, itrs=2000, Progress: 14.95%[0m
[36m[2023-07-03 03:13:21,553][188188] train() took 11.71 seconds to complete[0m
[36m[2023-07-03 03:13:21,553][188188] FPS: 327816.75[0m
[36m[2023-07-03 03:13:25,857][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:13:25,857][188188] Reward + Measures: [[-116.46365299    0.52793163    0.73638201    0.79890329    0.26160935
     3.99380779]][0m
[37m[1m[2023-07-03 03:13:25,857][188188] Max Reward on eval: -116.46365298609165[0m
[37m[1m[2023-07-03 03:13:25,857][188188] Min Reward on eval: -116.46365298609165[0m
[37m[1m[2023-07-03 03:13:25,858][188188] Mean Reward across all agents: -116.46365298609165[0m
[37m[1m[2023-07-03 03:13:25,858][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:13:30,812][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:13:30,818][188188] Reward + Measures: [[  40.75422733    0.1193        0.33360001    0.2978        0.3346
     3.95521617]
 [-275.72788377    0.6354        0.69940001    0.79870003    0.1296
     3.99713707]
 [-158.46838476    0.81920004    0.86539996    0.87320006    0.11970001
     3.99688315]
 ...
 [-314.12096269    0.82380003    0.8757        0.87580007    0.1217
     3.99862289]
 [-133.30235676    0.58230001    0.97299999    0.96759999    0.39739999
     3.99828029]
 [-236.94130801    0.88969994    0.87360001    0.93029994    0.0086
     3.99896502]][0m
[37m[1m[2023-07-03 03:13:30,818][188188] Max Reward on eval: 342.8051334070042[0m
[37m[1m[2023-07-03 03:13:30,819][188188] Min Reward on eval: -469.6931648466736[0m
[37m[1m[2023-07-03 03:13:30,819][188188] Mean Reward across all agents: -125.2316030515694[0m
[37m[1m[2023-07-03 03:13:30,819][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:13:30,821][188188] mean_value=-1022.8890050873238, max_value=498.37927591165527[0m
[37m[1m[2023-07-03 03:13:30,823][188188] New mean coefficients: [[-0.09867695 -0.81531733 -1.5354872  -1.2893587  -0.78044677 -0.6761186 ]][0m
[37m[1m[2023-07-03 03:13:30,824][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:13:39,769][188188] train() took 8.94 seconds to complete[0m
[36m[2023-07-03 03:13:39,769][188188] FPS: 429388.87[0m
[36m[2023-07-03 03:13:39,771][188188] itr=300, itrs=2000, Progress: 15.00%[0m
[37m[1m[2023-07-03 03:13:42,581][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000280[0m
[36m[2023-07-03 03:13:54,596][188188] train() took 11.68 seconds to complete[0m
[36m[2023-07-03 03:13:54,596][188188] FPS: 328823.05[0m
[36m[2023-07-03 03:13:58,912][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:13:58,918][188188] Reward + Measures: [[-159.39460387    0.67666167    0.80050892    0.86260563    0.16087098
     3.99752188]][0m
[37m[1m[2023-07-03 03:13:58,918][188188] Max Reward on eval: -159.39460387274866[0m
[37m[1m[2023-07-03 03:13:58,918][188188] Min Reward on eval: -159.39460387274866[0m
[37m[1m[2023-07-03 03:13:58,918][188188] Mean Reward across all agents: -159.39460387274866[0m
[37m[1m[2023-07-03 03:13:58,919][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:14:03,975][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:14:03,981][188188] Reward + Measures: [[ -10.14103532    0.34540001    0.66619998    0.6749        0.41730005
     3.91930008]
 [  77.35268793    0.48680001    0.6669001     0.61250001    0.2419
     3.77190399]
 [ 125.9126861     0.31440002    0.30240002    0.17989999    0.16760001
     3.97087288]
 ...
 [ -63.77848964    0.62270004    0.75430006    0.78010005    0.21850002
     3.98099208]
 [-303.02029829    0.91979998    0.88030005    0.88390011    0.0175
     3.99571204]
 [   5.45601103    0.1059        0.17730002    0.1214        0.1305
     3.86121058]][0m
[37m[1m[2023-07-03 03:14:03,981][188188] Max Reward on eval: 358.33628273233774[0m
[37m[1m[2023-07-03 03:14:03,981][188188] Min Reward on eval: -366.3429491363466[0m
[37m[1m[2023-07-03 03:14:03,981][188188] Mean Reward across all agents: -36.620874812924335[0m
[37m[1m[2023-07-03 03:14:03,982][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:14:03,983][188188] mean_value=-1409.6178269818722, max_value=57.144122988766725[0m
[37m[1m[2023-07-03 03:14:03,986][188188] New mean coefficients: [[-0.20134105 -0.77682686 -1.358941   -0.8270825  -1.1342095  -1.0080271 ]][0m
[37m[1m[2023-07-03 03:14:03,987][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:14:13,107][188188] train() took 9.12 seconds to complete[0m
[36m[2023-07-03 03:14:13,107][188188] FPS: 421130.51[0m
[36m[2023-07-03 03:14:13,110][188188] itr=301, itrs=2000, Progress: 15.05%[0m
[36m[2023-07-03 03:14:25,056][188188] train() took 11.93 seconds to complete[0m
[36m[2023-07-03 03:14:25,056][188188] FPS: 322007.43[0m
[36m[2023-07-03 03:14:29,408][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:14:29,409][188188] Reward + Measures: [[132.11581016   0.27977467   0.33590934   0.42022437   0.23541567
    3.95821929]][0m
[37m[1m[2023-07-03 03:14:29,409][188188] Max Reward on eval: 132.11581015767814[0m
[37m[1m[2023-07-03 03:14:29,409][188188] Min Reward on eval: 132.11581015767814[0m
[37m[1m[2023-07-03 03:14:29,409][188188] Mean Reward across all agents: 132.11581015767814[0m
[37m[1m[2023-07-03 03:14:29,410][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:14:34,408][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:14:34,409][188188] Reward + Measures: [[ -32.88123157    0.43199998    0.36149999    0.48739997    0.1693
     3.95146728]
 [-105.61232322    0.88870001    0.96859998    0.96859998    0.0986
     3.91708565]
 [ -86.58659816    0.81459999    0.78940004    0.82680005    0.0168
     3.99646235]
 ...
 [-138.15938162    0.78450006    0.95249999    0.96349996    0.1997
     3.9814744 ]
 [ 173.46394324    0.29790002    0.25549999    0.25030002    0.0847
     3.87342715]
 [ 381.8133544     0.5632        0.64889997    0.65170002    0.2185
     3.90325713]][0m
[37m[1m[2023-07-03 03:14:34,409][188188] Max Reward on eval: 536.240976334922[0m
[37m[1m[2023-07-03 03:14:34,409][188188] Min Reward on eval: -338.5941311839968[0m
[37m[1m[2023-07-03 03:14:34,410][188188] Mean Reward across all agents: 14.241002587502953[0m
[37m[1m[2023-07-03 03:14:34,410][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:14:34,412][188188] mean_value=-1573.9973281304403, max_value=140.2073917083172[0m
[37m[1m[2023-07-03 03:14:34,414][188188] New mean coefficients: [[-1.0457026   0.18438399  0.36536098 -0.39366695 -1.53918    -0.29335904]][0m
[37m[1m[2023-07-03 03:14:34,415][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:14:43,457][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 03:14:43,457][188188] FPS: 424759.94[0m
[36m[2023-07-03 03:14:43,459][188188] itr=302, itrs=2000, Progress: 15.10%[0m
[36m[2023-07-03 03:14:55,211][188188] train() took 11.60 seconds to complete[0m
[36m[2023-07-03 03:14:55,211][188188] FPS: 331165.29[0m
[36m[2023-07-03 03:14:59,519][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:14:59,520][188188] Reward + Measures: [[144.73339634   0.29543465   0.40712231   0.50562102   0.27434367
    3.94781399]][0m
[37m[1m[2023-07-03 03:14:59,520][188188] Max Reward on eval: 144.73339633593363[0m
[37m[1m[2023-07-03 03:14:59,520][188188] Min Reward on eval: 144.73339633593363[0m
[37m[1m[2023-07-03 03:14:59,521][188188] Mean Reward across all agents: 144.73339633593363[0m
[37m[1m[2023-07-03 03:14:59,521][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:15:04,527][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:15:04,527][188188] Reward + Measures: [[-120.40210843    0.72110003    0.69539994    0.78689998    0.0308
     3.99468994]
 [ -30.75217865    0.53740001    0.58380002    0.7087        0.18100001
     3.95524645]
 [  97.00241112    0.14070001    0.16119999    0.2507        0.1157
     3.89867449]
 ...
 [ 101.78402753    0.1076        0.26190001    0.46219999    0.22430001
     3.96743059]
 [  69.92609885    0.1345        0.67199999    0.4975        0.62009996
     3.81015325]
 [ 290.50357146    0.35429999    0.36540002    0.3326        0.0658
     3.95040178]][0m
[37m[1m[2023-07-03 03:15:04,528][188188] Max Reward on eval: 425.54324007332326[0m
[37m[1m[2023-07-03 03:15:04,528][188188] Min Reward on eval: -329.60375974699855[0m
[37m[1m[2023-07-03 03:15:04,528][188188] Mean Reward across all agents: -4.923647316241058[0m
[37m[1m[2023-07-03 03:15:04,528][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:15:04,530][188188] mean_value=-1359.84697120793, max_value=736.3761935627087[0m
[37m[1m[2023-07-03 03:15:04,533][188188] New mean coefficients: [[-1.1978334   0.17747742  0.43517005 -0.4543169  -2.2869163  -1.0765532 ]][0m
[37m[1m[2023-07-03 03:15:04,534][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:15:13,518][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 03:15:13,518][188188] FPS: 427486.73[0m
[36m[2023-07-03 03:15:13,521][188188] itr=303, itrs=2000, Progress: 15.15%[0m
[36m[2023-07-03 03:15:25,375][188188] train() took 11.83 seconds to complete[0m
[36m[2023-07-03 03:15:25,375][188188] FPS: 324473.07[0m
[36m[2023-07-03 03:15:29,775][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:15:29,776][188188] Reward + Measures: [[-188.39092006    0.79992664    0.82187897    0.86764503    0.06609767
     3.99617767]][0m
[37m[1m[2023-07-03 03:15:29,776][188188] Max Reward on eval: -188.39092006011305[0m
[37m[1m[2023-07-03 03:15:29,776][188188] Min Reward on eval: -188.39092006011305[0m
[37m[1m[2023-07-03 03:15:29,776][188188] Mean Reward across all agents: -188.39092006011305[0m
[37m[1m[2023-07-03 03:15:29,777][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:15:34,853][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:15:34,854][188188] Reward + Measures: [[ 112.46812532    0.60879999    0.7299        0.7712        0.24080001
     3.973629  ]
 [ -57.02531435    0.10219999    0.15119998    0.1441        0.1363
     3.79426646]
 [-179.50967466    0.88409996    0.88330001    0.89239997    0.0055
     3.97419357]
 ...
 [-193.06468417    0.68839997    0.79470003    0.85010004    0.1207
     3.99735069]
 [ 151.02975506    0.23710001    0.42799997    0.31080002    0.294
     3.72663999]
 [  50.88127504    0.16419999    0.099         0.2696        0.0856
     3.96035171]][0m
[37m[1m[2023-07-03 03:15:34,854][188188] Max Reward on eval: 566.8179781230167[0m
[37m[1m[2023-07-03 03:15:34,854][188188] Min Reward on eval: -416.2667565122247[0m
[37m[1m[2023-07-03 03:15:34,854][188188] Mean Reward across all agents: -28.604220420592966[0m
[37m[1m[2023-07-03 03:15:34,855][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:15:34,857][188188] mean_value=-1117.6759125063693, max_value=85.17559895423828[0m
[37m[1m[2023-07-03 03:15:34,859][188188] New mean coefficients: [[-1.6697286   0.2314134   0.627014   -0.21463361 -2.3073616  -0.5266571 ]][0m
[37m[1m[2023-07-03 03:15:34,860][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:15:43,925][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 03:15:43,925][188188] FPS: 423691.58[0m
[36m[2023-07-03 03:15:43,927][188188] itr=304, itrs=2000, Progress: 15.20%[0m
[36m[2023-07-03 03:15:55,778][188188] train() took 11.83 seconds to complete[0m
[36m[2023-07-03 03:15:55,779][188188] FPS: 324658.29[0m
[36m[2023-07-03 03:16:00,165][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:16:00,165][188188] Reward + Measures: [[-143.30047724    0.63123727    0.6915437     0.75932062    0.13240866
     3.99544811]][0m
[37m[1m[2023-07-03 03:16:00,165][188188] Max Reward on eval: -143.30047723888003[0m
[37m[1m[2023-07-03 03:16:00,166][188188] Min Reward on eval: -143.30047723888003[0m
[37m[1m[2023-07-03 03:16:00,166][188188] Mean Reward across all agents: -143.30047723888003[0m
[37m[1m[2023-07-03 03:16:00,166][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:16:05,109][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:16:05,110][188188] Reward + Measures: [[  -6.19998991    0.25369999    0.66580003    0.74070007    0.47639999
     3.95906448]
 [  39.2550209     0.08090001    0.25209999    0.18300001    0.20570002
     3.81338763]
 [-191.7809749     0.88999999    0.88810009    0.90700001    0.0038
     3.9979496 ]
 ...
 [-232.52963051    0.52249998    0.78860003    0.84890002    0.31650001
     3.99799323]
 [-165.70148521    0.66650003    0.80540001    0.71399993    0.1885
     3.81053352]
 [  22.56070663    0.88570005    0.88260001    0.89440006    0.0045
     3.99507403]][0m
[37m[1m[2023-07-03 03:16:05,110][188188] Max Reward on eval: 445.1990228059236[0m
[37m[1m[2023-07-03 03:16:05,110][188188] Min Reward on eval: -339.5016880366951[0m
[37m[1m[2023-07-03 03:16:05,111][188188] Mean Reward across all agents: 25.99924536947527[0m
[37m[1m[2023-07-03 03:16:05,111][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:16:05,113][188188] mean_value=-1126.5753954102584, max_value=103.44650527161983[0m
[37m[1m[2023-07-03 03:16:05,115][188188] New mean coefficients: [[-2.137693   -0.68796813  1.1792314  -1.6124951  -2.6196399  -0.7598189 ]][0m
[37m[1m[2023-07-03 03:16:05,116][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:16:14,113][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 03:16:14,114][188188] FPS: 426868.96[0m
[36m[2023-07-03 03:16:14,116][188188] itr=305, itrs=2000, Progress: 15.25%[0m
[36m[2023-07-03 03:16:25,679][188188] train() took 11.54 seconds to complete[0m
[36m[2023-07-03 03:16:25,679][188188] FPS: 332661.12[0m
[36m[2023-07-03 03:16:29,968][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:16:29,968][188188] Reward + Measures: [[-129.86913256    0.61036801    0.75402701    0.80860764    0.19938032
     3.99376035]][0m
[37m[1m[2023-07-03 03:16:29,968][188188] Max Reward on eval: -129.8691325591328[0m
[37m[1m[2023-07-03 03:16:29,969][188188] Min Reward on eval: -129.8691325591328[0m
[37m[1m[2023-07-03 03:16:29,969][188188] Mean Reward across all agents: -129.8691325591328[0m
[37m[1m[2023-07-03 03:16:29,969][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:16:34,950][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:16:34,951][188188] Reward + Measures: [[46.33212025  0.1997      0.574       0.2296      0.51700002  3.91953397]
 [53.19824554  0.0406      0.35390002  0.32790002  0.31440002  3.92550826]
 [38.09877737  0.13700001  0.3452      0.1479      0.29909998  3.91391754]
 ...
 [34.29126311  0.22490001  0.59490007  0.25830001  0.51859999  3.80229735]
 [34.79356914  0.0469      0.46680003  0.39089999  0.39680001  3.87357759]
 [35.15597949  0.1296      0.48629999  0.52570003  0.47370005  3.98405242]][0m
[37m[1m[2023-07-03 03:16:34,951][188188] Max Reward on eval: 265.4576737621799[0m
[37m[1m[2023-07-03 03:16:34,951][188188] Min Reward on eval: -346.16636543590573[0m
[37m[1m[2023-07-03 03:16:34,952][188188] Mean Reward across all agents: 20.940120153407687[0m
[37m[1m[2023-07-03 03:16:34,952][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:16:34,954][188188] mean_value=-1134.6071591244336, max_value=134.57166220729812[0m
[37m[1m[2023-07-03 03:16:34,956][188188] New mean coefficients: [[-1.0424144  -0.7829823   0.93771744 -1.5310059  -2.4193625  -0.54529047]][0m
[37m[1m[2023-07-03 03:16:34,957][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:16:44,008][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 03:16:44,008][188188] FPS: 424354.48[0m
[36m[2023-07-03 03:16:44,010][188188] itr=306, itrs=2000, Progress: 15.30%[0m
[36m[2023-07-03 03:16:55,764][188188] train() took 11.73 seconds to complete[0m
[36m[2023-07-03 03:16:55,765][188188] FPS: 327247.58[0m
[36m[2023-07-03 03:17:00,048][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:17:00,048][188188] Reward + Measures: [[153.92292113   0.23003234   0.68447536   0.61531329   0.51181298
    3.91247773]][0m
[37m[1m[2023-07-03 03:17:00,048][188188] Max Reward on eval: 153.9229211273718[0m
[37m[1m[2023-07-03 03:17:00,049][188188] Min Reward on eval: 153.9229211273718[0m
[37m[1m[2023-07-03 03:17:00,049][188188] Mean Reward across all agents: 153.9229211273718[0m
[37m[1m[2023-07-03 03:17:00,049][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:17:05,170][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:17:05,171][188188] Reward + Measures: [[ 271.08207785    0.88339996    0.87579995    0.94929999    0.10210001
     3.97106218]
 [-148.2118477     0.98380005    0.98439997    0.98850006    0.0018
     3.9958744 ]
 [  52.92875416    0.6433        0.7658        0.76659995    0.2017
     3.97736931]
 ...
 [  34.88114424    0.90070003    0.86399996    0.88959998    0.0143
     3.98014808]
 [  -7.41926245    0.13850001    0.62650001    0.53179997    0.51590002
     3.91199183]
 [   3.00889211    0.18480001    0.2263        0.12329999    0.23630002
     3.85709834]][0m
[37m[1m[2023-07-03 03:17:05,171][188188] Max Reward on eval: 814.3643035842106[0m
[37m[1m[2023-07-03 03:17:05,171][188188] Min Reward on eval: -398.7673110924661[0m
[37m[1m[2023-07-03 03:17:05,172][188188] Mean Reward across all agents: 108.414541033338[0m
[37m[1m[2023-07-03 03:17:05,172][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:17:05,174][188188] mean_value=-729.7830572205963, max_value=645.1323464939612[0m
[37m[1m[2023-07-03 03:17:05,177][188188] New mean coefficients: [[-1.3313458  -1.0543723   1.4492947  -0.90287936 -3.7362752  -0.4714685 ]][0m
[37m[1m[2023-07-03 03:17:05,178][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:17:14,193][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 03:17:14,194][188188] FPS: 426011.85[0m
[36m[2023-07-03 03:17:14,196][188188] itr=307, itrs=2000, Progress: 15.35%[0m
[36m[2023-07-03 03:17:25,845][188188] train() took 11.63 seconds to complete[0m
[36m[2023-07-03 03:17:25,845][188188] FPS: 330299.74[0m
[36m[2023-07-03 03:17:30,058][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:17:30,058][188188] Reward + Measures: [[209.91365852   0.36365035   0.48563233   0.54199499   0.24271801
    3.95803308]][0m
[37m[1m[2023-07-03 03:17:30,058][188188] Max Reward on eval: 209.9136585239401[0m
[37m[1m[2023-07-03 03:17:30,058][188188] Min Reward on eval: 209.9136585239401[0m
[37m[1m[2023-07-03 03:17:30,059][188188] Mean Reward across all agents: 209.9136585239401[0m
[37m[1m[2023-07-03 03:17:30,059][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:17:35,037][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:17:35,037][188188] Reward + Measures: [[ -2.3629056    0.14299999   0.28009999   0.15390001   0.24080001
    3.90487218]
 [ 14.5477458    0.13900001   0.3224       0.1619       0.29190001
    3.92714858]
 [ 92.06330167   0.24260001   0.61980003   0.6415       0.41240001
    3.96479392]
 ...
 [-48.35618314   0.1451       0.24770001   0.16680001   0.20720001
    3.91592407]
 [ 91.13396228   0.0771       0.77080005   0.60720003   0.69010001
    3.83394241]
 [305.07321764   0.93020004   0.86919993   0.84040004   0.0042
    3.82728267]][0m
[37m[1m[2023-07-03 03:17:35,037][188188] Max Reward on eval: 305.07321764328515[0m
[37m[1m[2023-07-03 03:17:35,038][188188] Min Reward on eval: -315.94870640235024[0m
[37m[1m[2023-07-03 03:17:35,038][188188] Mean Reward across all agents: 2.917952317592712[0m
[37m[1m[2023-07-03 03:17:35,038][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:17:35,040][188188] mean_value=-1684.2614892657352, max_value=62.83124097685169[0m
[37m[1m[2023-07-03 03:17:35,042][188188] New mean coefficients: [[-0.9173868  -0.27607703  2.0468457  -1.1039494  -0.8342955  -0.974014  ]][0m
[37m[1m[2023-07-03 03:17:35,043][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:17:44,054][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 03:17:44,055][188188] FPS: 426209.69[0m
[36m[2023-07-03 03:17:44,057][188188] itr=308, itrs=2000, Progress: 15.40%[0m
[36m[2023-07-03 03:17:55,685][188188] train() took 11.61 seconds to complete[0m
[36m[2023-07-03 03:17:55,685][188188] FPS: 330802.59[0m
[36m[2023-07-03 03:18:00,042][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:18:00,043][188188] Reward + Measures: [[206.70273617   0.38695434   0.52961904   0.55458134   0.25959632
    3.9520123 ]][0m
[37m[1m[2023-07-03 03:18:00,043][188188] Max Reward on eval: 206.70273616919192[0m
[37m[1m[2023-07-03 03:18:00,043][188188] Min Reward on eval: 206.70273616919192[0m
[37m[1m[2023-07-03 03:18:00,044][188188] Mean Reward across all agents: 206.70273616919192[0m
[37m[1m[2023-07-03 03:18:00,044][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:18:05,070][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:18:05,071][188188] Reward + Measures: [[  54.46966165    0.1329        0.37100002    0.27770001    0.33460003
     3.82619214]
 [ -58.42436952    0.18170001    0.4147        0.2185        0.36300001
     3.86835098]
 [ -91.11801527    0.4937        0.40229997    0.50030005    0.0683
     3.9942708 ]
 ...
 [  84.8187677     0.22220002    0.2165        0.0409        0.1383
     3.96197772]
 [ 686.94835662    0.92649996    0.69099998    0.84359998    0.0218
     3.9196794 ]
 [-131.36137342    0.68709999    0.86610001    0.93169993    0.20710002
     3.99762774]][0m
[37m[1m[2023-07-03 03:18:05,071][188188] Max Reward on eval: 686.9483566183596[0m
[37m[1m[2023-07-03 03:18:05,071][188188] Min Reward on eval: -373.4737009814009[0m
[37m[1m[2023-07-03 03:18:05,072][188188] Mean Reward across all agents: 24.61124193787366[0m
[37m[1m[2023-07-03 03:18:05,072][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:18:05,074][188188] mean_value=-937.577800877305, max_value=99.59326371832731[0m
[37m[1m[2023-07-03 03:18:05,076][188188] New mean coefficients: [[-1.7735225   0.13471264  2.3414712  -1.0747945  -1.4904697  -0.55126905]][0m
[37m[1m[2023-07-03 03:18:05,077][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:18:14,181][188188] train() took 9.10 seconds to complete[0m
[36m[2023-07-03 03:18:14,182][188188] FPS: 421855.22[0m
[36m[2023-07-03 03:18:14,184][188188] itr=309, itrs=2000, Progress: 15.45%[0m
[36m[2023-07-03 03:18:25,968][188188] train() took 11.76 seconds to complete[0m
[36m[2023-07-03 03:18:25,968][188188] FPS: 326420.91[0m
[36m[2023-07-03 03:18:30,320][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:18:30,326][188188] Reward + Measures: [[446.70745352   0.64457232   0.59133935   0.66654968   0.14366667
    3.94964838]][0m
[37m[1m[2023-07-03 03:18:30,326][188188] Max Reward on eval: 446.70745352233456[0m
[37m[1m[2023-07-03 03:18:30,327][188188] Min Reward on eval: 446.70745352233456[0m
[37m[1m[2023-07-03 03:18:30,327][188188] Mean Reward across all agents: 446.70745352233456[0m
[37m[1m[2023-07-03 03:18:30,327][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:18:35,358][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:18:35,364][188188] Reward + Measures: [[-157.92705152    0.91190004    0.88260001    0.88290006    0.0288
     3.99392319]
 [ 432.95444442    0.57080001    0.40490004    0.54100001    0.0559
     3.95186114]
 [   6.49271866    0.2509        0.66970009    0.40760002    0.60049999
     3.87622619]
 ...
 [ 318.25739906    0.73509997    0.6437        0.74460006    0.12889999
     3.93549275]
 [ 166.20029884    0.07229999    0.81280005    0.67819995    0.77410001
     3.90056229]
 [ 296.82825377    0.61919993    0.6552        0.72259998    0.33570001
     3.9772191 ]][0m
[37m[1m[2023-07-03 03:18:35,364][188188] Max Reward on eval: 786.6343536252156[0m
[37m[1m[2023-07-03 03:18:35,365][188188] Min Reward on eval: -419.842186271213[0m
[37m[1m[2023-07-03 03:18:35,365][188188] Mean Reward across all agents: 86.26474331854234[0m
[37m[1m[2023-07-03 03:18:35,365][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:18:35,368][188188] mean_value=-684.6106867942304, max_value=187.63833071753453[0m
[37m[1m[2023-07-03 03:18:35,370][188188] New mean coefficients: [[-1.7755903  -0.34585118  2.9712677  -1.1521094  -0.39462018  0.6347015 ]][0m
[37m[1m[2023-07-03 03:18:35,371][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:18:44,400][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 03:18:44,400][188188] FPS: 425378.72[0m
[36m[2023-07-03 03:18:44,403][188188] itr=310, itrs=2000, Progress: 15.50%[0m
[37m[1m[2023-07-03 03:18:47,430][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000290[0m
[36m[2023-07-03 03:18:59,414][188188] train() took 11.66 seconds to complete[0m
[36m[2023-07-03 03:18:59,414][188188] FPS: 329460.43[0m
[36m[2023-07-03 03:19:03,652][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:19:03,653][188188] Reward + Measures: [[74.33072782  0.152468    0.65543765  0.51274031  0.57000762  3.85761023]][0m
[37m[1m[2023-07-03 03:19:03,653][188188] Max Reward on eval: 74.33072782000187[0m
[37m[1m[2023-07-03 03:19:03,653][188188] Min Reward on eval: 74.33072782000187[0m
[37m[1m[2023-07-03 03:19:03,653][188188] Mean Reward across all agents: 74.33072782000187[0m
[37m[1m[2023-07-03 03:19:03,654][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:19:08,606][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:19:08,606][188188] Reward + Measures: [[-136.78351832    0.97759992    0.98170006    0.98990005    0.0023
     3.99948239]
 [ -36.17526674    0.80200005    0.77080005    0.85420001    0.0331
     3.98240352]
 [-335.0740719     0.90000004    0.96090001    0.96250004    0.0816
     3.99286509]
 ...
 [  63.13815932    0.47100002    0.72009999    0.64740002    0.34029999
     3.92135167]
 [ 173.38892028    0.70550001    0.60930002    0.56039995    0.0134
     3.68242264]
 [ 106.06853606    0.9266001     0.91359997    0.91850007    0.0079
     3.95078397]][0m
[37m[1m[2023-07-03 03:19:08,607][188188] Max Reward on eval: 776.102344540134[0m
[37m[1m[2023-07-03 03:19:08,607][188188] Min Reward on eval: -335.07407189849766[0m
[37m[1m[2023-07-03 03:19:08,607][188188] Mean Reward across all agents: 8.374375654213308[0m
[37m[1m[2023-07-03 03:19:08,607][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:19:08,609][188188] mean_value=-772.8927537935988, max_value=65.37713775905524[0m
[37m[1m[2023-07-03 03:19:08,611][188188] New mean coefficients: [[-0.88837075 -0.22664484  2.9170341  -1.3550828  -0.15917556  0.16815954]][0m
[37m[1m[2023-07-03 03:19:08,612][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:19:17,610][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 03:19:17,611][188188] FPS: 426825.50[0m
[36m[2023-07-03 03:19:17,613][188188] itr=311, itrs=2000, Progress: 15.55%[0m
[36m[2023-07-03 03:19:29,315][188188] train() took 11.68 seconds to complete[0m
[36m[2023-07-03 03:19:29,315][188188] FPS: 328801.38[0m
[36m[2023-07-03 03:19:33,616][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:19:33,616][188188] Reward + Measures: [[168.05639961   0.31654733   0.60882866   0.58637297   0.39289266
    3.93102264]][0m
[37m[1m[2023-07-03 03:19:33,617][188188] Max Reward on eval: 168.05639961035158[0m
[37m[1m[2023-07-03 03:19:33,617][188188] Min Reward on eval: 168.05639961035158[0m
[37m[1m[2023-07-03 03:19:33,617][188188] Mean Reward across all agents: 168.05639961035158[0m
[37m[1m[2023-07-03 03:19:33,618][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:19:38,522][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:19:38,522][188188] Reward + Measures: [[ 97.32363937   0.3651       0.73560005   0.48630005   0.56249994
    3.93498397]
 [294.21025609   0.78859997   0.89060003   0.95059997   0.1982
    3.94311023]
 [129.7433685    0.55579996   0.66300005   0.69080001   0.111
    3.78618097]
 ...
 [162.43859661   0.50590003   0.55740005   0.6498       0.23050001
    3.92686629]
 [ -4.95794725   0.65320009   0.87610006   0.86739999   0.28009999
    3.94922829]
 [ 91.90870637   0.88660002   0.97360003   0.9695999    0.1023
    3.97475028]][0m
[37m[1m[2023-07-03 03:19:38,523][188188] Max Reward on eval: 754.7124137669801[0m
[37m[1m[2023-07-03 03:19:38,523][188188] Min Reward on eval: -356.60434243378694[0m
[37m[1m[2023-07-03 03:19:38,523][188188] Mean Reward across all agents: 8.162942067670189[0m
[37m[1m[2023-07-03 03:19:38,523][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:19:38,525][188188] mean_value=-822.0121353439697, max_value=392.87544453548577[0m
[37m[1m[2023-07-03 03:19:38,528][188188] New mean coefficients: [[-0.5347796  -0.85117793  2.8687067  -1.6525089  -0.43123955  0.2880536 ]][0m
[37m[1m[2023-07-03 03:19:38,529][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:19:47,525][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 03:19:47,525][188188] FPS: 426914.08[0m
[36m[2023-07-03 03:19:47,528][188188] itr=312, itrs=2000, Progress: 15.60%[0m
[36m[2023-07-03 03:19:59,197][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 03:19:59,197][188188] FPS: 329643.52[0m
[36m[2023-07-03 03:20:03,483][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:20:03,483][188188] Reward + Measures: [[173.10279321   0.334636     0.60398233   0.60690731   0.36075202
    3.94637108]][0m
[37m[1m[2023-07-03 03:20:03,484][188188] Max Reward on eval: 173.10279320675838[0m
[37m[1m[2023-07-03 03:20:03,484][188188] Min Reward on eval: 173.10279320675838[0m
[37m[1m[2023-07-03 03:20:03,484][188188] Mean Reward across all agents: 173.10279320675838[0m
[37m[1m[2023-07-03 03:20:03,484][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:20:08,538][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:20:08,539][188188] Reward + Measures: [[115.23948078   0.1268       0.4197       0.45479998   0.3125
    3.94408202]
 [-43.67305721   0.398        0.87830001   0.92309999   0.49910003
    3.9973495 ]
 [-88.94960024   0.50940001   0.7895       0.83090001   0.3008
    3.99618983]
 ...
 [ 69.68722841   0.2436       0.57300007   0.55320007   0.40030003
    3.92543912]
 [-24.17037401   0.0934       0.1349       0.1006       0.0892
    3.86774373]
 [ 10.57756207   0.14780001   0.2174       0.20870002   0.1622
    3.84029889]][0m
[37m[1m[2023-07-03 03:20:08,539][188188] Max Reward on eval: 763.8022308181971[0m
[37m[1m[2023-07-03 03:20:08,539][188188] Min Reward on eval: -369.58207898512484[0m
[37m[1m[2023-07-03 03:20:08,540][188188] Mean Reward across all agents: 19.474357125515553[0m
[37m[1m[2023-07-03 03:20:08,540][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:20:08,542][188188] mean_value=-1097.16668994838, max_value=167.2833499791936[0m
[37m[1m[2023-07-03 03:20:08,544][188188] New mean coefficients: [[-0.03155583 -0.6880296   3.2434726  -0.6137351  -0.24676941  0.27042064]][0m
[37m[1m[2023-07-03 03:20:08,545][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:20:17,515][188188] train() took 8.97 seconds to complete[0m
[36m[2023-07-03 03:20:17,515][188188] FPS: 428185.57[0m
[36m[2023-07-03 03:20:17,517][188188] itr=313, itrs=2000, Progress: 15.65%[0m
[36m[2023-07-03 03:20:29,198][188188] train() took 11.66 seconds to complete[0m
[36m[2023-07-03 03:20:29,199][188188] FPS: 329283.14[0m
[36m[2023-07-03 03:20:33,435][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:20:33,435][188188] Reward + Measures: [[183.26710937   0.31999967   0.66533202   0.64924204   0.42321366
    3.94135928]][0m
[37m[1m[2023-07-03 03:20:33,435][188188] Max Reward on eval: 183.2671093686474[0m
[37m[1m[2023-07-03 03:20:33,436][188188] Min Reward on eval: 183.2671093686474[0m
[37m[1m[2023-07-03 03:20:33,436][188188] Mean Reward across all agents: 183.2671093686474[0m
[37m[1m[2023-07-03 03:20:33,436][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:20:38,414][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:20:38,414][188188] Reward + Measures: [[-43.8703429    0.1645       0.39720002   0.25050002   0.33919999
    3.8114326 ]
 [ 46.56718014   0.13240001   0.43110004   0.21820001   0.40430003
    3.90376592]
 [  5.22057848   0.1716       0.36000001   0.2189       0.31570002
    3.8564384 ]
 ...
 [202.04485008   0.30899999   0.7615       0.78050005   0.51249999
    3.95693827]
 [125.46276056   0.40900001   0.83719999   0.87200004   0.54089999
    3.95351338]
 [-33.973556     0.1356       0.3926       0.22610001   0.3592
    3.8195169 ]][0m
[37m[1m[2023-07-03 03:20:38,414][188188] Max Reward on eval: 690.2166099375114[0m
[37m[1m[2023-07-03 03:20:38,415][188188] Min Reward on eval: -410.43125342223794[0m
[37m[1m[2023-07-03 03:20:38,415][188188] Mean Reward across all agents: 48.56925107560915[0m
[37m[1m[2023-07-03 03:20:38,415][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:20:38,417][188188] mean_value=-876.0259783280416, max_value=116.03808409560685[0m
[37m[1m[2023-07-03 03:20:38,420][188188] New mean coefficients: [[-0.25894755 -1.5912356   2.746142   -1.0957808  -0.9242343  -0.62238467]][0m
[37m[1m[2023-07-03 03:20:38,421][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:20:47,355][188188] train() took 8.93 seconds to complete[0m
[36m[2023-07-03 03:20:47,355][188188] FPS: 429876.80[0m
[36m[2023-07-03 03:20:47,358][188188] itr=314, itrs=2000, Progress: 15.70%[0m
[36m[2023-07-03 03:20:58,925][188188] train() took 11.55 seconds to complete[0m
[36m[2023-07-03 03:20:58,925][188188] FPS: 332568.41[0m
[36m[2023-07-03 03:21:03,224][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:21:03,224][188188] Reward + Measures: [[25.138479    0.11411832  0.6895873   0.46334636  0.61230832  3.86289048]][0m
[37m[1m[2023-07-03 03:21:03,225][188188] Max Reward on eval: 25.13847900180214[0m
[37m[1m[2023-07-03 03:21:03,225][188188] Min Reward on eval: 25.13847900180214[0m
[37m[1m[2023-07-03 03:21:03,225][188188] Mean Reward across all agents: 25.13847900180214[0m
[37m[1m[2023-07-03 03:21:03,225][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:21:08,290][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:21:08,291][188188] Reward + Measures: [[  43.97392277    0.0826        0.91470003    0.35450003    0.89520007
     3.9842279 ]
 [  -4.87323868    0.11390001    0.16580002    0.13780001    0.15700001
     3.83509612]
 [ -32.94272115    0.1094        0.31240001    0.16320001    0.3008
     3.80098653]
 ...
 [-144.61023656    0.79440004    0.87530005    0.94570011    0.10539999
     3.99510193]
 [  42.42917473    0.08450001    0.91040003    0.3881        0.88730001
     3.98753428]
 [ -97.71976875    0.57019997    0.97110003    0.86000007    0.4808
     3.99772692]][0m
[37m[1m[2023-07-03 03:21:08,291][188188] Max Reward on eval: 518.2288436952047[0m
[37m[1m[2023-07-03 03:21:08,292][188188] Min Reward on eval: -442.06751820147036[0m
[37m[1m[2023-07-03 03:21:08,292][188188] Mean Reward across all agents: -23.30470517085739[0m
[37m[1m[2023-07-03 03:21:08,292][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:21:08,294][188188] mean_value=-1403.1820695077784, max_value=426.81576594310724[0m
[37m[1m[2023-07-03 03:21:08,296][188188] New mean coefficients: [[-0.35758865 -0.39820182  2.9176586  -1.2885559  -1.2246277  -1.2305192 ]][0m
[37m[1m[2023-07-03 03:21:08,297][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:21:17,290][188188] train() took 8.99 seconds to complete[0m
[36m[2023-07-03 03:21:17,290][188188] FPS: 427093.93[0m
[36m[2023-07-03 03:21:17,292][188188] itr=315, itrs=2000, Progress: 15.75%[0m
[36m[2023-07-03 03:21:29,083][188188] train() took 11.77 seconds to complete[0m
[36m[2023-07-03 03:21:29,083][188188] FPS: 326231.40[0m
[36m[2023-07-03 03:21:33,401][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:21:33,402][188188] Reward + Measures: [[34.15973218  0.089701    0.75460237  0.459793    0.70549899  3.83699727]][0m
[37m[1m[2023-07-03 03:21:33,402][188188] Max Reward on eval: 34.15973218460825[0m
[37m[1m[2023-07-03 03:21:33,402][188188] Min Reward on eval: 34.15973218460825[0m
[37m[1m[2023-07-03 03:21:33,403][188188] Mean Reward across all agents: 34.15973218460825[0m
[37m[1m[2023-07-03 03:21:33,403][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:21:38,418][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:21:38,419][188188] Reward + Measures: [[-21.92842445   0.07830001   0.2041       0.1269       0.1824
    3.76253939]
 [ 13.99640813   0.31500003   0.86380005   0.90990001   0.5733
    3.99554443]
 [ 11.26892119   0.1513       0.43080002   0.233        0.39130002
    3.851825  ]
 ...
 [ 41.97951436   0.0801       0.6182       0.30500004   0.58789998
    3.95616984]
 [-39.85420578   0.0867       0.70179999   0.47340003   0.6318
    3.8666923 ]
 [-21.41534104   0.15899999   0.29480001   0.1875       0.2674
    3.89225245]][0m
[37m[1m[2023-07-03 03:21:38,419][188188] Max Reward on eval: 282.0909743336495[0m
[37m[1m[2023-07-03 03:21:38,419][188188] Min Reward on eval: -351.5101790273096[0m
[37m[1m[2023-07-03 03:21:38,419][188188] Mean Reward across all agents: 16.4746410034591[0m
[37m[1m[2023-07-03 03:21:38,420][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:21:38,421][188188] mean_value=-1113.937720731015, max_value=-1.015366947772236[0m
[36m[2023-07-03 03:21:38,423][188188] XNES is restarting with a new solution whose measures are [0.64720005 0.72069997 0.77980006 0.2167     3.92034531] and objective is 495.7125663924962[0m
[36m[2023-07-03 03:21:38,424][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 03:21:38,427][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 03:21:38,427][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:21:47,508][188188] train() took 9.08 seconds to complete[0m
[36m[2023-07-03 03:21:47,508][188188] FPS: 422959.13[0m
[36m[2023-07-03 03:21:47,510][188188] itr=316, itrs=2000, Progress: 15.80%[0m
[36m[2023-07-03 03:21:59,241][188188] train() took 11.71 seconds to complete[0m
[36m[2023-07-03 03:21:59,241][188188] FPS: 327898.38[0m
[36m[2023-07-03 03:22:03,511][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:22:03,512][188188] Reward + Measures: [[-116.92699504    0.67806602    0.79767001    0.823475      0.16921332
     3.988204  ]][0m
[37m[1m[2023-07-03 03:22:03,512][188188] Max Reward on eval: -116.92699503757726[0m
[37m[1m[2023-07-03 03:22:03,512][188188] Min Reward on eval: -116.92699503757726[0m
[37m[1m[2023-07-03 03:22:03,513][188188] Mean Reward across all agents: -116.92699503757726[0m
[37m[1m[2023-07-03 03:22:03,513][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:22:08,547][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:22:08,548][188188] Reward + Measures: [[ -3.84145857   0.1419       0.64810002   0.2536       0.62029999
    3.92194605]
 [ 78.45656663   0.0938       0.27079999   0.27629998   0.22449999
    3.90352893]
 [138.86062251   0.31700003   0.4251       0.55800003   0.2438
    3.9846921 ]
 ...
 [145.679204     0.19509999   0.37450001   0.49250004   0.23239999
    3.95287251]
 [-19.0468595    0.1142       0.3177       0.2362       0.30230004
    3.81438375]
 [186.36835597   0.39559999   0.65640002   0.66800004   0.30860001
    3.97045374]][0m
[37m[1m[2023-07-03 03:22:08,548][188188] Max Reward on eval: 337.87751240711657[0m
[37m[1m[2023-07-03 03:22:08,548][188188] Min Reward on eval: -324.20728494701905[0m
[37m[1m[2023-07-03 03:22:08,548][188188] Mean Reward across all agents: 60.54653277472139[0m
[37m[1m[2023-07-03 03:22:08,549][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:22:08,550][188188] mean_value=-1717.6209282878217, max_value=207.32820125900966[0m
[37m[1m[2023-07-03 03:22:08,553][188188] New mean coefficients: [[ 1.190963   -0.86408746 -0.6971169  -0.6936637  -0.78733563 -0.42023104]][0m
[37m[1m[2023-07-03 03:22:08,554][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:22:17,582][188188] train() took 9.03 seconds to complete[0m
[36m[2023-07-03 03:22:17,582][188188] FPS: 425414.82[0m
[36m[2023-07-03 03:22:17,585][188188] itr=317, itrs=2000, Progress: 15.85%[0m
[36m[2023-07-03 03:22:29,408][188188] train() took 11.80 seconds to complete[0m
[36m[2023-07-03 03:22:29,408][188188] FPS: 325351.77[0m
[36m[2023-07-03 03:22:33,693][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:22:33,694][188188] Reward + Measures: [[-88.98210165   0.64554167   0.76968539   0.7937023    0.17611165
    3.979568  ]][0m
[37m[1m[2023-07-03 03:22:33,694][188188] Max Reward on eval: -88.98210165080215[0m
[37m[1m[2023-07-03 03:22:33,694][188188] Min Reward on eval: -88.98210165080215[0m
[37m[1m[2023-07-03 03:22:33,695][188188] Mean Reward across all agents: -88.98210165080215[0m
[37m[1m[2023-07-03 03:22:33,695][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:22:38,732][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:22:38,738][188188] Reward + Measures: [[-195.7787077     0.72729999    0.78210002    0.80610001    0.1199
     3.99766469]
 [ 162.80124439    0.15820001    0.39809999    0.45820004    0.30740002
     3.94956207]
 [  98.29637412    0.40050003    0.64749998    0.66689998    0.31560001
     3.90221214]
 ...
 [  54.88696443    0.08130001    0.46450001    0.53390002    0.45580003
     3.98141217]
 [-280.0759525     0.92749995    0.97410005    0.97559994    0.0604
     3.99794555]
 [ 113.34378738    0.21359999    0.61159998    0.63490003    0.48500004
     3.95033336]][0m
[37m[1m[2023-07-03 03:22:38,738][188188] Max Reward on eval: 658.3450565415435[0m
[37m[1m[2023-07-03 03:22:38,738][188188] Min Reward on eval: -355.49601033329964[0m
[37m[1m[2023-07-03 03:22:38,738][188188] Mean Reward across all agents: 20.59620325494204[0m
[37m[1m[2023-07-03 03:22:38,739][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:22:38,740][188188] mean_value=-1048.9503893399835, max_value=27.798975371630547[0m
[37m[1m[2023-07-03 03:22:38,743][188188] New mean coefficients: [[ 0.9340144  -1.2652104  -0.55397576 -0.7738279  -0.39774084 -0.812847  ]][0m
[37m[1m[2023-07-03 03:22:38,744][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:22:47,846][188188] train() took 9.10 seconds to complete[0m
[36m[2023-07-03 03:22:47,846][188188] FPS: 421954.07[0m
[36m[2023-07-03 03:22:47,849][188188] itr=318, itrs=2000, Progress: 15.90%[0m
[36m[2023-07-03 03:22:59,953][188188] train() took 12.08 seconds to complete[0m
[36m[2023-07-03 03:22:59,953][188188] FPS: 317831.59[0m
[36m[2023-07-03 03:23:04,242][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:23:04,242][188188] Reward + Measures: [[155.94929182   0.30831999   0.50189829   0.51222628   0.30239832
    3.93926096]][0m
[37m[1m[2023-07-03 03:23:04,243][188188] Max Reward on eval: 155.9492918245554[0m
[37m[1m[2023-07-03 03:23:04,243][188188] Min Reward on eval: 155.9492918245554[0m
[37m[1m[2023-07-03 03:23:04,243][188188] Mean Reward across all agents: 155.9492918245554[0m
[37m[1m[2023-07-03 03:23:04,243][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:23:09,419][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:23:09,419][188188] Reward + Measures: [[   3.4599802     0.60930008    0.67529994    0.69380003    0.1595
     3.98197818]
 [  -1.25760893    0.1382        0.30850002    0.2357        0.3019
     3.86625481]
 [-181.51597119    0.93170005    0.91189998    0.93150008    0.0067
     3.99167323]
 ...
 [ -24.46348094    0.1707        0.32530001    0.29009998    0.30210003
     3.87794876]
 [ -24.91901323    0.0802        0.108         0.1221        0.1018
     3.79379153]
 [ -48.68454571    0.55430001    0.69919997    0.70700002    0.2237
     3.99637985]][0m
[37m[1m[2023-07-03 03:23:09,420][188188] Max Reward on eval: 512.5050154104829[0m
[37m[1m[2023-07-03 03:23:09,420][188188] Min Reward on eval: -339.619450337952[0m
[37m[1m[2023-07-03 03:23:09,420][188188] Mean Reward across all agents: 18.291798571117216[0m
[37m[1m[2023-07-03 03:23:09,420][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:23:09,421][188188] mean_value=-2060.972300029228, max_value=-91.63332213404021[0m
[36m[2023-07-03 03:23:09,424][188188] XNES is restarting with a new solution whose measures are [0.57410002 0.58359998 0.56059998 0.053      3.73355365] and objective is 377.6270618844777[0m
[36m[2023-07-03 03:23:09,425][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 03:23:09,427][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 03:23:09,428][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:23:18,414][188188] train() took 8.98 seconds to complete[0m
[36m[2023-07-03 03:23:18,415][188188] FPS: 427385.58[0m
[36m[2023-07-03 03:23:18,417][188188] itr=319, itrs=2000, Progress: 15.95%[0m
[36m[2023-07-03 03:23:30,252][188188] train() took 11.82 seconds to complete[0m
[36m[2023-07-03 03:23:30,252][188188] FPS: 324991.42[0m
[36m[2023-07-03 03:23:34,565][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:23:34,565][188188] Reward + Measures: [[119.08956149   0.584454     0.59422868   0.33738133   0.32296801
    3.95643282]][0m
[37m[1m[2023-07-03 03:23:34,565][188188] Max Reward on eval: 119.08956148541512[0m
[37m[1m[2023-07-03 03:23:34,566][188188] Min Reward on eval: 119.08956148541512[0m
[37m[1m[2023-07-03 03:23:34,566][188188] Mean Reward across all agents: 119.08956148541512[0m
[37m[1m[2023-07-03 03:23:34,566][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:23:39,610][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:23:39,615][188188] Reward + Measures: [[ 37.74221776   0.2066       0.1866       0.0848       0.24100001
    3.97664952]
 [-33.4613663    0.24420002   0.25710002   0.22520001   0.21519999
    3.89049387]
 [ -1.50557061   0.1525       0.18080001   0.15869999   0.12609999
    3.86710286]
 ...
 [359.11805482   0.68959999   0.66890001   0.4921       0.2246
    3.9650948 ]
 [ 85.70710696   0.95209998   0.96219999   0.13700001   0.84560007
    3.99885535]
 [ 19.86369447   0.5183       0.57919997   0.37380001   0.32860002
    3.86761522]][0m
[37m[1m[2023-07-03 03:23:39,616][188188] Max Reward on eval: 521.965838443255[0m
[37m[1m[2023-07-03 03:23:39,616][188188] Min Reward on eval: -377.3378734648228[0m
[37m[1m[2023-07-03 03:23:39,616][188188] Mean Reward across all agents: 79.13520606806854[0m
[37m[1m[2023-07-03 03:23:39,616][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:23:39,620][188188] mean_value=-1195.5941781580634, max_value=671.6949963092618[0m
[37m[1m[2023-07-03 03:23:39,622][188188] New mean coefficients: [[ 0.22711375  0.04406315 -1.5030826  -3.0080945  -1.4425414   0.49895835]][0m
[37m[1m[2023-07-03 03:23:39,623][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:23:48,681][188188] train() took 9.06 seconds to complete[0m
[36m[2023-07-03 03:23:48,681][188188] FPS: 424016.53[0m
[36m[2023-07-03 03:23:48,684][188188] itr=320, itrs=2000, Progress: 16.00%[0m
[37m[1m[2023-07-03 03:23:51,600][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000300[0m
[36m[2023-07-03 03:24:03,593][188188] train() took 11.66 seconds to complete[0m
[36m[2023-07-03 03:24:03,594][188188] FPS: 329401.14[0m
[36m[2023-07-03 03:24:07,926][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:24:07,932][188188] Reward + Measures: [[181.19622837   0.40514967   0.43560466   0.28543901   0.20912699
    3.93477106]][0m
[37m[1m[2023-07-03 03:24:07,932][188188] Max Reward on eval: 181.19622836662685[0m
[37m[1m[2023-07-03 03:24:07,932][188188] Min Reward on eval: 181.19622836662685[0m
[37m[1m[2023-07-03 03:24:07,933][188188] Mean Reward across all agents: 181.19622836662685[0m
[37m[1m[2023-07-03 03:24:07,933][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:24:12,993][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:24:12,999][188188] Reward + Measures: [[145.50369674   0.60720009   0.92830002   0.48629999   0.63200003
    3.89104342]
 [497.84919163   0.79590005   0.80849999   0.73260003   0.09069999
    3.70275736]
 [-37.67098375   0.35069999   0.35960004   0.13790001   0.2784
    3.97907877]
 ...
 [178.95622386   0.38320002   0.38509998   0.30650002   0.11069999
    3.97257352]
 [-79.14896211   0.40439996   0.59069997   0.09780001   0.5934
    3.76112342]
 [-16.34819601   0.3105       0.3642       0.10439999   0.322
    3.92234349]][0m
[37m[1m[2023-07-03 03:24:13,000][188188] Max Reward on eval: 497.84919163379817[0m
[37m[1m[2023-07-03 03:24:13,000][188188] Min Reward on eval: -515.152625821298[0m
[37m[1m[2023-07-03 03:24:13,001][188188] Mean Reward across all agents: 66.64627651155739[0m
[37m[1m[2023-07-03 03:24:13,001][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:24:13,008][188188] mean_value=-997.366900853422, max_value=508.044196854648[0m
[37m[1m[2023-07-03 03:24:13,012][188188] New mean coefficients: [[ 0.5344548  -1.5290012  -2.3282702  -3.0054138  -0.38614118  1.0625693 ]][0m
[37m[1m[2023-07-03 03:24:13,014][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:24:22,128][188188] train() took 9.11 seconds to complete[0m
[36m[2023-07-03 03:24:22,128][188188] FPS: 421409.74[0m
[36m[2023-07-03 03:24:22,131][188188] itr=321, itrs=2000, Progress: 16.05%[0m
[36m[2023-07-03 03:24:34,000][188188] train() took 11.85 seconds to complete[0m
[36m[2023-07-03 03:24:34,001][188188] FPS: 324151.29[0m
[36m[2023-07-03 03:24:38,267][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:24:38,267][188188] Reward + Measures: [[190.37870067   0.43402895   0.45709166   0.363924     0.14285032
    3.80778742]][0m
[37m[1m[2023-07-03 03:24:38,267][188188] Max Reward on eval: 190.3787006659842[0m
[37m[1m[2023-07-03 03:24:38,268][188188] Min Reward on eval: 190.3787006659842[0m
[37m[1m[2023-07-03 03:24:38,268][188188] Mean Reward across all agents: 190.3787006659842[0m
[37m[1m[2023-07-03 03:24:38,268][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:24:43,253][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:24:43,254][188188] Reward + Measures: [[114.411026     0.0957       0.71990001   0.50120002   0.71420002
    3.91190195]
 [404.91039756   0.89260006   0.82310003   0.65189999   0.2165
    3.99839592]
 [-21.94257026   0.51680005   0.58350003   0.39320001   0.20150001
    3.67834473]
 ...
 [450.68107603   0.51540005   0.89279997   0.78150004   0.41009998
    3.89437366]
 [ 17.18979277   0.68080002   0.67720002   0.58740002   0.15470001
    3.62668991]
 [ 88.98628479   0.0449       0.84670001   0.55619997   0.83059996
    3.85833859]][0m
[37m[1m[2023-07-03 03:24:43,254][188188] Max Reward on eval: 602.4412307512015[0m
[37m[1m[2023-07-03 03:24:43,254][188188] Min Reward on eval: -503.0349616823718[0m
[37m[1m[2023-07-03 03:24:43,255][188188] Mean Reward across all agents: 66.44442385766843[0m
[37m[1m[2023-07-03 03:24:43,255][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:24:43,258][188188] mean_value=-735.2263207652005, max_value=724.2032869171351[0m
[37m[1m[2023-07-03 03:24:43,260][188188] New mean coefficients: [[ 1.754748  -2.1724966 -1.6212904 -2.4605742 -1.2170726  0.9866743]][0m
[37m[1m[2023-07-03 03:24:43,261][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:24:52,168][188188] train() took 8.91 seconds to complete[0m
[36m[2023-07-03 03:24:52,168][188188] FPS: 431214.31[0m
[36m[2023-07-03 03:24:52,170][188188] itr=322, itrs=2000, Progress: 16.10%[0m
[36m[2023-07-03 03:25:03,678][188188] train() took 11.49 seconds to complete[0m
[36m[2023-07-03 03:25:03,678][188188] FPS: 334268.32[0m
[36m[2023-07-03 03:25:07,886][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:25:07,886][188188] Reward + Measures: [[-270.39639046    0.78819001    0.9617793     0.39903036    0.71175933
     3.99752688]][0m
[37m[1m[2023-07-03 03:25:07,887][188188] Max Reward on eval: -270.3963904563979[0m
[37m[1m[2023-07-03 03:25:07,887][188188] Min Reward on eval: -270.3963904563979[0m
[37m[1m[2023-07-03 03:25:07,887][188188] Mean Reward across all agents: -270.3963904563979[0m
[37m[1m[2023-07-03 03:25:07,888][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:25:12,966][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:25:12,967][188188] Reward + Measures: [[ 110.82745467    0.53190005    0.47          0.38779998    0.1962
     3.98662114]
 [ -35.07273289    0.59100002    0.97220004    0.50599998    0.77650005
     3.99798083]
 [ 261.06182789    0.12049999    0.90370005    0.62340003    0.88520002
     3.98967528]
 ...
 [ -88.25550602    0.5887        0.58389997    0.30590001    0.3651
     3.96951747]
 [-314.30505524    0.78929996    0.88830006    0.39080003    0.60190004
     3.99744105]
 [-266.39873606    0.78459996    0.9788        0.38570002    0.76919997
     3.99936533]][0m
[37m[1m[2023-07-03 03:25:12,967][188188] Max Reward on eval: 423.3028446954675[0m
[37m[1m[2023-07-03 03:25:12,967][188188] Min Reward on eval: -662.5386474058032[0m
[37m[1m[2023-07-03 03:25:12,968][188188] Mean Reward across all agents: -147.97401897687473[0m
[37m[1m[2023-07-03 03:25:12,968][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:25:12,970][188188] mean_value=-821.8870850520435, max_value=221.98552063515632[0m
[37m[1m[2023-07-03 03:25:12,972][188188] New mean coefficients: [[ 1.1233146 -1.8804677 -1.6983395 -0.7345923 -2.2914984  1.4454651]][0m
[37m[1m[2023-07-03 03:25:12,973][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:25:21,977][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 03:25:21,978][188188] FPS: 426539.89[0m
[36m[2023-07-03 03:25:21,980][188188] itr=323, itrs=2000, Progress: 16.15%[0m
[36m[2023-07-03 03:25:33,677][188188] train() took 11.68 seconds to complete[0m
[36m[2023-07-03 03:25:33,678][188188] FPS: 328893.92[0m
[36m[2023-07-03 03:25:37,943][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:25:37,943][188188] Reward + Measures: [[-153.12859963    0.81746769    0.95750237    0.55874205    0.51356536
     3.99860382]][0m
[37m[1m[2023-07-03 03:25:37,943][188188] Max Reward on eval: -153.1285996302319[0m
[37m[1m[2023-07-03 03:25:37,943][188188] Min Reward on eval: -153.1285996302319[0m
[37m[1m[2023-07-03 03:25:37,944][188188] Mean Reward across all agents: -153.1285996302319[0m
[37m[1m[2023-07-03 03:25:37,944][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:25:42,907][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:25:42,907][188188] Reward + Measures: [[-67.21148667   0.84750003   0.90369999   0.39629999   0.51690006
    3.99910426]
 [ 56.43634365   0.74980003   0.9018001    0.34560001   0.64959997
    3.99818277]
 [249.66277146   0.6724       0.87460005   0.74580002   0.30270001
    3.99804187]
 ...
 [-81.04556036   0.78190005   0.98000002   0.70230001   0.46800002
    3.99894881]
 [-36.37422754   0.82800007   0.97729999   0.5399       0.55150002
    3.99893355]
 [-52.05511681   0.20440002   0.21180001   0.19289999   0.19500001
    3.8226831 ]][0m
[37m[1m[2023-07-03 03:25:42,908][188188] Max Reward on eval: 410.5602378601208[0m
[37m[1m[2023-07-03 03:25:42,908][188188] Min Reward on eval: -446.3944530446082[0m
[37m[1m[2023-07-03 03:25:42,908][188188] Mean Reward across all agents: -0.3016559109046367[0m
[37m[1m[2023-07-03 03:25:42,908][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:25:42,910][188188] mean_value=-667.4406410403122, max_value=76.08819092850229[0m
[37m[1m[2023-07-03 03:25:42,913][188188] New mean coefficients: [[ 0.31594104 -1.384151   -1.6788589  -1.7513641  -2.224223    0.57786375]][0m
[37m[1m[2023-07-03 03:25:42,914][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:25:51,926][188188] train() took 9.01 seconds to complete[0m
[36m[2023-07-03 03:25:51,926][188188] FPS: 426176.59[0m
[36m[2023-07-03 03:25:51,928][188188] itr=324, itrs=2000, Progress: 16.20%[0m
[36m[2023-07-03 03:26:03,476][188188] train() took 11.53 seconds to complete[0m
[36m[2023-07-03 03:26:03,477][188188] FPS: 333088.17[0m
[36m[2023-07-03 03:26:07,730][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:26:07,731][188188] Reward + Measures: [[-241.72857923    0.83387125    0.97175062    0.37592766    0.69505137
     3.98563218]][0m
[37m[1m[2023-07-03 03:26:07,731][188188] Max Reward on eval: -241.72857923190878[0m
[37m[1m[2023-07-03 03:26:07,731][188188] Min Reward on eval: -241.72857923190878[0m
[37m[1m[2023-07-03 03:26:07,731][188188] Mean Reward across all agents: -241.72857923190878[0m
[37m[1m[2023-07-03 03:26:07,732][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:26:12,710][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:26:12,711][188188] Reward + Measures: [[   6.34386346    0.69370002    0.97630006    0.53170007    0.63090008
     3.90809941]
 [ 146.3017351     0.83960003    0.82920009    0.37349999    0.5165
     3.9953897 ]
 [ -30.34958661    0.1129        0.11110001    0.0956        0.1026
     3.68367648]
 ...
 [  14.91466971    0.13789999    0.12630001    0.1173        0.1181
     3.81875491]
 [-348.1150732     0.64499998    0.98270005    0.37170002    0.8926999
     3.99274063]
 [  51.17330398    0.72360003    0.74669999    0.0298        0.80170006
     3.90231395]][0m
[37m[1m[2023-07-03 03:26:12,711][188188] Max Reward on eval: 564.6488797584549[0m
[37m[1m[2023-07-03 03:26:12,711][188188] Min Reward on eval: -705.4084809411318[0m
[37m[1m[2023-07-03 03:26:12,712][188188] Mean Reward across all agents: 1.0601762396082828[0m
[37m[1m[2023-07-03 03:26:12,712][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:26:12,715][188188] mean_value=-1041.6705901594344, max_value=776.1754487565719[0m
[37m[1m[2023-07-03 03:26:12,718][188188] New mean coefficients: [[-0.72653383 -1.0848296  -2.4396212  -1.1299374  -3.2104115   1.5193753 ]][0m
[37m[1m[2023-07-03 03:26:12,719][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:26:21,741][188188] train() took 9.02 seconds to complete[0m
[36m[2023-07-03 03:26:21,741][188188] FPS: 425683.22[0m
[36m[2023-07-03 03:26:21,743][188188] itr=325, itrs=2000, Progress: 16.25%[0m
[36m[2023-07-03 03:26:33,360][188188] train() took 11.60 seconds to complete[0m
[36m[2023-07-03 03:26:33,360][188188] FPS: 331122.78[0m
[36m[2023-07-03 03:26:37,647][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:26:37,647][188188] Reward + Measures: [[-326.16841109    0.82711327    0.97698438    0.33159798    0.770625
     3.99877405]][0m
[37m[1m[2023-07-03 03:26:37,647][188188] Max Reward on eval: -326.16841108644337[0m
[37m[1m[2023-07-03 03:26:37,648][188188] Min Reward on eval: -326.16841108644337[0m
[37m[1m[2023-07-03 03:26:37,648][188188] Mean Reward across all agents: -326.16841108644337[0m
[37m[1m[2023-07-03 03:26:37,648][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:26:42,700][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:26:42,701][188188] Reward + Measures: [[  30.04220697    0.10750001    0.1168        0.10160001    0.0981
     3.79028511]
 [  64.2889114     0.37510002    0.45660001    0.11299999    0.36549997
     3.91844726]
 [ 282.46988968    0.35660002    0.98040003    0.49100003    0.96460003
     3.99630213]
 ...
 [-568.82548071    0.97439998    0.98639995    0.19599999    0.79420006
     3.99966168]
 [-329.19145787    0.54960001    0.78240001    0.20310001    0.75729996
     3.99147272]
 [  32.6457224     0.0825        0.1           0.09190001    0.0875
     3.76137781]][0m
[37m[1m[2023-07-03 03:26:42,701][188188] Max Reward on eval: 586.366331083735[0m
[37m[1m[2023-07-03 03:26:42,701][188188] Min Reward on eval: -702.4947757515125[0m
[37m[1m[2023-07-03 03:26:42,701][188188] Mean Reward across all agents: -62.86680413792766[0m
[37m[1m[2023-07-03 03:26:42,702][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:26:42,704][188188] mean_value=-1474.7394892191037, max_value=54.22851381161527[0m
[37m[1m[2023-07-03 03:26:42,706][188188] New mean coefficients: [[ 0.7251968 -1.3302295 -3.250589  -2.3938093 -2.7095118  1.767828 ]][0m
[37m[1m[2023-07-03 03:26:42,707][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:26:51,785][188188] train() took 9.08 seconds to complete[0m
[36m[2023-07-03 03:26:51,785][188188] FPS: 423090.33[0m
[36m[2023-07-03 03:26:51,788][188188] itr=326, itrs=2000, Progress: 16.30%[0m
[36m[2023-07-03 03:27:03,550][188188] train() took 11.74 seconds to complete[0m
[36m[2023-07-03 03:27:03,550][188188] FPS: 327133.01[0m
[36m[2023-07-03 03:27:07,801][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:27:07,801][188188] Reward + Measures: [[-323.90087246    0.80643904    0.97077489    0.32525098    0.78112239
     3.99903393]][0m
[37m[1m[2023-07-03 03:27:07,801][188188] Max Reward on eval: -323.9008724614113[0m
[37m[1m[2023-07-03 03:27:07,802][188188] Min Reward on eval: -323.9008724614113[0m
[37m[1m[2023-07-03 03:27:07,802][188188] Mean Reward across all agents: -323.9008724614113[0m
[37m[1m[2023-07-03 03:27:07,802][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:27:12,795][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:27:12,796][188188] Reward + Measures: [[ 56.58201183   0.90020001   0.91260004   0.029        0.92150003
    3.97323847]
 [226.11134504   0.75160003   0.73629999   0.33040002   0.46240002
    3.99289012]
 [-72.2624452    0.86899996   0.87840003   0.0301       0.89109993
    3.98953748]
 ...
 [ 16.2174408    0.8761       0.89839995   0.10960002   0.81879997
    3.99671721]
 [ 63.32164933   0.88729995   0.90679997   0.08400001   0.86970007
    3.98039889]
 [ 42.94706259   0.53640002   0.5751       0.0972       0.56330007
    3.96049666]][0m
[37m[1m[2023-07-03 03:27:12,796][188188] Max Reward on eval: 345.93163081277163[0m
[37m[1m[2023-07-03 03:27:12,796][188188] Min Reward on eval: -625.7769775370136[0m
[37m[1m[2023-07-03 03:27:12,796][188188] Mean Reward across all agents: -51.93521460716506[0m
[37m[1m[2023-07-03 03:27:12,797][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:27:12,798][188188] mean_value=-782.0965295936704, max_value=78.35491685319033[0m
[37m[1m[2023-07-03 03:27:12,800][188188] New mean coefficients: [[ 0.26323283 -1.2016156  -2.7472095  -2.2304819  -2.5027366   1.6698215 ]][0m
[37m[1m[2023-07-03 03:27:12,801][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:27:21,854][188188] train() took 9.05 seconds to complete[0m
[36m[2023-07-03 03:27:21,854][188188] FPS: 424261.24[0m
[36m[2023-07-03 03:27:21,857][188188] itr=327, itrs=2000, Progress: 16.35%[0m
[36m[2023-07-03 03:27:33,543][188188] train() took 11.66 seconds to complete[0m
[36m[2023-07-03 03:27:33,543][188188] FPS: 329240.44[0m
[36m[2023-07-03 03:27:37,932][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:27:37,932][188188] Reward + Measures: [[-347.11598491    0.76223469    0.96045768    0.28715798    0.83753532
     3.99769115]][0m
[37m[1m[2023-07-03 03:27:37,933][188188] Max Reward on eval: -347.1159849087905[0m
[37m[1m[2023-07-03 03:27:37,933][188188] Min Reward on eval: -347.1159849087905[0m
[37m[1m[2023-07-03 03:27:37,933][188188] Mean Reward across all agents: -347.1159849087905[0m
[37m[1m[2023-07-03 03:27:37,933][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:27:43,097][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:27:43,103][188188] Reward + Measures: [[-372.50001143    0.86049998    0.9774        0.17389999    0.89130014
     3.99328923]
 [-148.06201745    0.2326        0.55159998    0.30270001    0.53130001
     3.73562598]
 [-219.56179594    0.87600005    0.98339999    0.3364        0.69440001
     3.94526172]
 ...
 [ -63.13178181    0.40900001    0.47159997    0.05840001    0.51349998
     3.91654062]
 [ 181.72645787    0.13590001    0.86440003    0.57130003    0.81960005
     3.99038315]
 [  85.13339384    0.29359999    0.32880002    0.16849999    0.34940001
     3.75111198]][0m
[37m[1m[2023-07-03 03:27:43,103][188188] Max Reward on eval: 513.7333335440605[0m
[37m[1m[2023-07-03 03:27:43,104][188188] Min Reward on eval: -615.4075889443978[0m
[37m[1m[2023-07-03 03:27:43,104][188188] Mean Reward across all agents: -11.39790271781637[0m
[37m[1m[2023-07-03 03:27:43,104][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:27:43,106][188188] mean_value=-1284.5618106140284, max_value=76.87946372249371[0m
[37m[1m[2023-07-03 03:27:43,108][188188] New mean coefficients: [[ 0.62417156 -0.9405727  -2.5659602  -2.3228974  -1.8660836   2.5814066 ]][0m
[37m[1m[2023-07-03 03:27:43,109][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:27:52,112][188188] train() took 9.00 seconds to complete[0m
[36m[2023-07-03 03:27:52,112][188188] FPS: 426617.47[0m
[36m[2023-07-03 03:27:52,115][188188] itr=328, itrs=2000, Progress: 16.40%[0m
[36m[2023-07-03 03:28:03,789][188188] train() took 11.65 seconds to complete[0m
[36m[2023-07-03 03:28:03,790][188188] FPS: 329513.04[0m
[36m[2023-07-03 03:28:08,143][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:28:08,144][188188] Reward + Measures: [[-326.20169191    0.74435401    0.96558428    0.25523835    0.88899022
     3.99505591]][0m
[37m[1m[2023-07-03 03:28:08,144][188188] Max Reward on eval: -326.201691906491[0m
[37m[1m[2023-07-03 03:28:08,145][188188] Min Reward on eval: -326.201691906491[0m
[37m[1m[2023-07-03 03:28:08,145][188188] Mean Reward across all agents: -326.201691906491[0m
[37m[1m[2023-07-03 03:28:08,145][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:28:13,181][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:28:13,181][188188] Reward + Measures: [[-377.07342531    0.81980002    0.87810004    0.0398        0.88949996
     3.98334765]
 [ -63.38958881    0.48079997    0.87470001    0.33140001    0.83069992
     3.95738411]
 [  -9.29480731    0.49219999    0.51070005    0.139         0.41479999
     3.97607994]
 ...
 [ -32.35327921    0.66100001    0.66820002    0.1287        0.62639999
     3.95501328]
 [  63.79233527    0.37729999    0.78579998    0.84730005    0.96130002
     3.7593956 ]
 [  50.12104141    0.44679999    0.48810002    0.19069999    0.35159999
     3.8183465 ]][0m
[37m[1m[2023-07-03 03:28:13,181][188188] Max Reward on eval: 666.0723934667185[0m
[37m[1m[2023-07-03 03:28:13,182][188188] Min Reward on eval: -694.3716926672263[0m
[37m[1m[2023-07-03 03:28:13,182][188188] Mean Reward across all agents: -102.60826716797875[0m
[37m[1m[2023-07-03 03:28:13,182][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:28:13,183][188188] mean_value=-1386.3513913052516, max_value=-10.714855743096564[0m
[36m[2023-07-03 03:28:13,186][188188] XNES is restarting with a new solution whose measures are [0.47139999 0.85690004 0.62830001 0.43179998 3.67973113] and objective is 224.20328691713513[0m
[36m[2023-07-03 03:28:13,187][188188] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-03 03:28:13,189][188188] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032 -0.664582 ]][0m
[37m[1m[2023-07-03 03:28:13,190][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:28:22,080][188188] train() took 8.89 seconds to complete[0m
[36m[2023-07-03 03:28:22,080][188188] FPS: 431996.20[0m
[36m[2023-07-03 03:28:22,083][188188] itr=329, itrs=2000, Progress: 16.45%[0m
[36m[2023-07-03 03:28:33,682][188188] train() took 11.58 seconds to complete[0m
[36m[2023-07-03 03:28:33,682][188188] FPS: 331734.43[0m
[36m[2023-07-03 03:28:38,059][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:28:38,059][188188] Reward + Measures: [[259.86607457   0.63190329   0.62034333   0.55801898   0.11549732
    3.62914133]][0m
[37m[1m[2023-07-03 03:28:38,060][188188] Max Reward on eval: 259.86607456721805[0m
[37m[1m[2023-07-03 03:28:38,060][188188] Min Reward on eval: 259.86607456721805[0m
[37m[1m[2023-07-03 03:28:38,060][188188] Mean Reward across all agents: 259.86607456721805[0m
[37m[1m[2023-07-03 03:28:38,060][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:28:43,140][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:28:43,141][188188] Reward + Measures: [[ 287.01215313    0.68490005    0.68520004    0.64749998    0.0772
     3.55049014]
 [ 118.83743953    0.49610004    0.53549999    0.27669999    0.32970002
     3.93635941]
 [ 107.25937451    0.7766        0.97780001    0.82950002    0.32549998
     3.99684501]
 ...
 [   1.83435878    0.12490001    0.13730001    0.11040001    0.1043
     3.64881253]
 [ 120.52664152    0.16580001    0.73500001    0.4508        0.64600003
     3.86671758]
 [-229.07974765    0.87579995    0.92909998    0.42340001    0.48569998
     3.58682323]][0m
[37m[1m[2023-07-03 03:28:43,141][188188] Max Reward on eval: 576.4485435318202[0m
[37m[1m[2023-07-03 03:28:43,141][188188] Min Reward on eval: -427.74723149109633[0m
[37m[1m[2023-07-03 03:28:43,142][188188] Mean Reward across all agents: 46.828087228944725[0m
[37m[1m[2023-07-03 03:28:43,142][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:28:43,144][188188] mean_value=-1312.30330881642, max_value=399.8171803731564[0m
[37m[1m[2023-07-03 03:28:43,146][188188] New mean coefficients: [[ 1.0123487  -0.20155418 -1.5935326  -0.86297834 -1.2172687  -0.607863  ]][0m
[37m[1m[2023-07-03 03:28:43,147][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:28:52,287][188188] train() took 9.14 seconds to complete[0m
[36m[2023-07-03 03:28:52,287][188188] FPS: 420209.72[0m
[36m[2023-07-03 03:28:52,290][188188] itr=330, itrs=2000, Progress: 16.50%[0m
[37m[1m[2023-07-03 03:28:55,199][188188] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000310[0m
[36m[2023-07-03 03:29:07,125][188188] train() took 11.59 seconds to complete[0m
[36m[2023-07-03 03:29:07,125][188188] FPS: 331292.14[0m
[36m[2023-07-03 03:29:11,442][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:29:11,443][188188] Reward + Measures: [[300.43896011   0.62380069   0.63602763   0.52617466   0.158108
    3.90669131]][0m
[37m[1m[2023-07-03 03:29:11,443][188188] Max Reward on eval: 300.4389601114427[0m
[37m[1m[2023-07-03 03:29:11,443][188188] Min Reward on eval: 300.4389601114427[0m
[37m[1m[2023-07-03 03:29:11,444][188188] Mean Reward across all agents: 300.4389601114427[0m
[37m[1m[2023-07-03 03:29:11,444][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:29:16,452][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 03:29:16,505][188188] Reward + Measures: [[ -34.28768677    0.35209998    0.36470002    0.35129997    0.329
     3.98208213]
 [-360.41499549    0.6649        0.89869994    0.35610002    0.73299998
     3.99730468]
 [ 237.91259523    0.5783        0.6516        0.29970002    0.41479999
     3.65026665]
 ...
 [ 109.32610766    0.12340001    0.75739998    0.53240001    0.71719998
     3.88165355]
 [ 306.0620682     0.87590009    0.80919999    0.65730006    0.22849999
     3.98523402]
 [ -14.14225223    0.59569997    0.61970001    0.31430003    0.35480002
     3.98396111]][0m
[37m[1m[2023-07-03 03:29:16,505][188188] Max Reward on eval: 528.132818204537[0m
[37m[1m[2023-07-03 03:29:16,505][188188] Min Reward on eval: -402.6849155324511[0m
[37m[1m[2023-07-03 03:29:16,505][188188] Mean Reward across all agents: 76.35084327924653[0m
[37m[1m[2023-07-03 03:29:16,505][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 03:29:16,507][188188] mean_value=-1183.5773393299544, max_value=494.24422199834976[0m
[37m[1m[2023-07-03 03:29:16,510][188188] New mean coefficients: [[ 1.6537373   2.0183249  -1.7443665  -0.2749095  -0.44382447 -0.723793  ]][0m
[37m[1m[2023-07-03 03:29:16,511][188188] Moving the mean solution point...[0m
[36m[2023-07-03 03:29:25,550][188188] train() took 9.04 seconds to complete[0m
[36m[2023-07-03 03:29:25,550][188188] FPS: 424893.68[0m
[36m[2023-07-03 03:29:25,553][188188] itr=331, itrs=2000, Progress: 16.55%[0m
Traceback (most recent call last):
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/icaros/Documents/PPGADev/algorithm/train_ppga.py", line 582, in <module>
    train_ppga(cfg, vec_env)
  File "/home/icaros/Documents/PPGADev/algorithm/train_ppga.py", line 398, in train_ppga
    objs, measures, jacobian, metadata = ppo.train(vec_env=vec_env,
  File "/home/icaros/Documents/PPGADev/RL/ppo.py", line 405, in train
    (pg_loss, v_loss, entropy_loss, old_approx_kl, approx_kl, clipfracs, ratio) = self.batch_update(b_values,
  File "/home/icaros/Documents/PPGADev/RL/ppo.py", line 213, in batch_update
    _, newlogprob, entropy = self.vec_inference.get_action(b_obs[:, mb_inds].reshape(-1, obs_dim),
  File "/home/icaros/Documents/PPGADev/models/vectorized.py", line 163, in get_action
    probs = torch.distributions.Normal(action_mean, action_std)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/torch/distributions/normal.py", line 56, in __init__
    super(Normal, self).__init__(batch_shape, validate_args=validate_args)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/torch/distributions/distribution.py", line 56, in __init__
    raise ValueError(
ValueError: Expected parameter loc (Tensor of shape (48000, 8)) of distribution Normal(loc: torch.Size([48000, 8]), scale: torch.Size([48000, 8])) to satisfy the constraint Real(), but found invalid values:
tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',
       dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>)
Traceback (most recent call last):
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/icaros/Documents/PPGADev/algorithm/train_ppga.py", line 582, in <module>
    train_ppga(cfg, vec_env)
  File "/home/icaros/Documents/PPGADev/algorithm/train_ppga.py", line 398, in train_ppga
    objs, measures, jacobian, metadata = ppo.train(vec_env=vec_env,
  File "/home/icaros/Documents/PPGADev/RL/ppo.py", line 405, in train
    (pg_loss, v_loss, entropy_loss, old_approx_kl, approx_kl, clipfracs, ratio) = self.batch_update(b_values,
  File "/home/icaros/Documents/PPGADev/RL/ppo.py", line 213, in batch_update
    _, newlogprob, entropy = self.vec_inference.get_action(b_obs[:, mb_inds].reshape(-1, obs_dim),
  File "/home/icaros/Documents/PPGADev/models/vectorized.py", line 163, in get_action
    probs = torch.distributions.Normal(action_mean, action_std)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/torch/distributions/normal.py", line 56, in __init__
    super(Normal, self).__init__(batch_shape, validate_args=validate_args)
  File "/home/icaros/anaconda3/envs/ppga/lib/python3.9/site-packages/torch/distributions/distribution.py", line 56, in __init__
    raise ValueError(
ValueError: Expected parameter loc (Tensor of shape (48000, 8)) of distribution Normal(loc: torch.Size([48000, 8]), scale: torch.Size([48000, 8])) to satisfy the constraint Real(), but found invalid values:
tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',
       dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>)
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                                Env step ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñà‚ñÜ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñà‚ñÅ
wandb:                                     FPS ‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÅ‚ñà‚ñÇ‚ñà‚ñÇ‚ñá‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñà
wandb:                                   FPS:  ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ
wandb:                             QD/QD Score ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                  QD/average performance ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                           QD/best score ‚ñÅ‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                         QD/coverage (%) ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                            QD/iteration ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                            QD/max_value ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ
wandb:                  QD/mean_coeff_measure1 ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÜ
wandb:                  QD/mean_coeff_measure2 ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñá‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÖ‚ñÑ
wandb:                  QD/mean_coeff_measure3 ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñá‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÑ
wandb:                  QD/mean_coeff_measure4 ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñá‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ
wandb:                  QD/mean_coeff_measure5 ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÅ‚ñá‚ñÜ‚ñà‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÑ
wandb:                       QD/mean_coeff_obj ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ
wandb:                           QD/mean_value ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñÑ‚ñÇ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÇ‚ñÖ‚ñÜ‚ñÑ‚ñÜ
wandb:                             QD/new_sols ‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                             QD/restarts ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                                  Update ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñà‚ñÜ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñà‚ñÅ
wandb:                             XNES/norm_A ‚ñÖ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:                 charts/actor_avg_logstd ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            charts/average_rew_magnitude ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñà‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÇ
wandb:                             global_step ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñà‚ñÜ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñà‚ñÅ
wandb:                        losses/approx_kl ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÇ
wandb:                         losses/clipfrac ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                          losses/entropy ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:               losses/explained_variance ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñà‚ñà‚ñÉ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñÖ‚ñà‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÅ‚ñà‚ñÇ‚ñà‚ñÖ
wandb: losses/move_mean_agent=False/value_loss ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñÅ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñÇ‚ñá‚ñÇ‚ñÜ‚ñÅ‚ñÜ‚ñÇ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  losses/move_mean_agent=True/value_loss ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                    losses/old_approx_kl ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÇ
wandb:                      losses/policy_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñÅ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                       losses/value_loss ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñà‚ñÅ‚ñá‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ
wandb:                               perf/_fps ‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÅ‚ñà‚ñÇ‚ñà‚ñÇ‚ñá‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñÇ‚ñà‚ñà
wandb:                           train/act_max ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñà
wandb:                           train/act_min ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:                           train/adv_max ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñá‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ
wandb:                          train/adv_mean ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÖ‚ñá‚ñÉ‚ñà‚ñÑ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÅ
wandb:                           train/adv_min ‚ñà‚ñÑ‚ñà‚ñÖ‚ñà‚ñÜ‚ñà‚ñá‚ñà‚ñÑ‚ñà‚ñÖ‚ñá‚ñÜ‚ñà‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñà‚ñá‚ñà‚ñÇ‚ñà‚ñá‚ñà‚ñÅ‚ñà‚ñÜ‚ñà‚ñÜ‚ñà‚ñÜ‚ñà‚ñÜ‚ñÅ
wandb:                           train/adv_std ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñà‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÅ‚ñÖ‚ñÉ‚ñá
wandb:                  train/obs_running_mean ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ
wandb:                   train/obs_running_std ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà‚ñÜ‚ñà‚ñÑ‚ñà‚ñÖ‚ñà‚ñà
wandb:                       train/policy_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñÅ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                         train/ratio_max ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñà‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                         train/ratio_min ‚ñà‚ñà‚ñÜ‚ñá‚ñÇ‚ñÑ‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                             train/value ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÖ
wandb:                        train/value_loss ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñà‚ñÅ‚ñá‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                                Env step 768000
wandb:                                     FPS 324514.02083
wandb:                                   FPS:  424893.68073
wandb:                             QD/QD Score 3598625.85249
wandb:                  QD/average performance 247.04055
wandb:                           QD/best score 4135.25854
wandb:                         QD/coverage (%) 6.1403
wandb:                            QD/iteration 331
wandb:                            QD/max_value 494.24422
wandb:                  QD/mean_coeff_measure1 2.01832
wandb:                  QD/mean_coeff_measure2 -1.74437
wandb:                  QD/mean_coeff_measure3 -0.27491
wandb:                  QD/mean_coeff_measure4 -0.44382
wandb:                  QD/mean_coeff_measure5 -0.72379
wandb:                       QD/mean_coeff_obj 1.65374
wandb:                           QD/mean_value -1183.57734
wandb:                             QD/new_sols 11
wandb:                             QD/restarts 41
wandb:                                  Update 2
wandb:                             XNES/norm_A 7.30358
wandb:                 charts/actor_avg_logstd -4.18107
wandb:            charts/average_rew_magnitude 0.07733
wandb:                             global_step 768000
wandb:                        losses/approx_kl 8321.23047
wandb:                         losses/clipfrac 0.96875
wandb:                          losses/entropy -2.76213
wandb:               losses/explained_variance 0.80684
wandb: losses/move_mean_agent=False/value_loss 0.1334
wandb:  losses/move_mean_agent=True/value_loss 0.00028
wandb:                    losses/old_approx_kl 8322.23047
wandb:                      losses/policy_loss 0.27443
wandb:                       losses/value_loss 0.1334
wandb:                               perf/_fps 324514.02083
wandb:                           train/act_max 71.5625
wandb:                           train/act_min -71.6875
wandb:                           train/adv_max 65.76089
wandb:                          train/adv_mean 17.10948
wandb:                           train/adv_min -15.9382
wandb:                           train/adv_std 19.3856
wandb:                  train/obs_running_mean 0.03169
wandb:                   train/obs_running_std 0.73244
wandb:                       train/policy_loss 0.27443
wandb:                         train/ratio_max 0.0
wandb:                         train/ratio_min 0.0
wandb:                             train/value 0.02003
wandb:                        train/value_loss 0.1334
wandb: 
wandb: üöÄ View run energy_1000_paper_ppga_ant_seed_1111 at: https://wandb.ai/qdrl/PPGA/runs/mx9hiurg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230703_004135-mx9hiurg/logs
is energy measures =  True
feet contact, is_energy_measures =  True
<brax.envs.ant.Ant object at 0x7f12ffa62280>
feet contact and energy
obs_shape
(87,)
action_shape
(8,)
using cvt archive
no kmeans
