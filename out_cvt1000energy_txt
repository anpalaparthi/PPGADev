energy_1000_paper_ppga_ant_seed_1111
wandb: Currently logged in as: anishapv (qdrl). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/icaros/Documents/PPGADev/wandb/run-20230703_004135-mx9hiurg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run energy_1000_paper_ppga_ant_seed_1111
wandb: ‚≠êÔ∏è View project at https://wandb.ai/qdrl/PPGA
wandb: üöÄ View run at https://wandb.ai/qdrl/PPGA/runs/mx9hiurg
[36m[2023-07-03 00:41:38,682][188188] Environment ant, action_dim=8, obs_dim=87[0m
[36m[2023-07-03 00:41:43,211][188188] Created Scheduler for cma_maega with an archive learning rate of 0.1, and add mode batch, using solution dim 28816 and archive dims [7, 7, 7, 7, 7]. Min threshold is -500.0. Restart rule is no_improvement[0m
[36m[2023-07-03 00:42:01,082][188188] train() took 13.90 seconds to complete[0m
[36m[2023-07-03 00:42:01,082][188188] FPS: 276225.97[0m
[36m[2023-07-03 00:42:05,218][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:42:05,219][188188] Reward + Measures: [[0.15953506 0.19033597 0.19046932 0.19079934 0.19051066 2.06406021]][0m
[37m[1m[2023-07-03 00:42:05,219][188188] Max Reward on eval: 0.15953505795116266[0m
[37m[1m[2023-07-03 00:42:05,219][188188] Min Reward on eval: 0.15953505795116266[0m
[37m[1m[2023-07-03 00:42:05,220][188188] Mean Reward across all agents: 0.15953505795116266[0m
[37m[1m[2023-07-03 00:42:05,220][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:42:10,903][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:42:10,904][188188] Reward + Measures: [[-71.04766231   0.24590002   0.20009999   0.2339       0.19679999
    2.25312209]
 [ 43.75833677   0.1525       0.153        0.15530001   0.16110002
    2.18607903]
 [ 44.13899712   0.228        0.21659999   0.23049998   0.226
    2.15890574]
 ...
 [ 35.64034989   0.16440001   0.17870001   0.16590001   0.16660002
    2.12064719]
 [  1.10190596   0.1744       0.17580001   0.17389999   0.1696
    2.15287852]
 [ 69.91767659   0.13689999   0.1397       0.1432       0.1372
    2.21102452]][0m
[37m[1m[2023-07-03 00:42:10,904][188188] Max Reward on eval: 140.23415774982422[0m
[37m[1m[2023-07-03 00:42:10,905][188188] Min Reward on eval: -71.04766231146641[0m
[37m[1m[2023-07-03 00:42:10,905][188188] Mean Reward across all agents: 10.87691528144611[0m
[37m[1m[2023-07-03 00:42:10,905][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:42:10,925][188188] mean_value=468.3633548015203, max_value=613.0481095248833[0m
[37m[1m[2023-07-03 00:42:10,959][188188] New mean coefficients: [[ 2.6749837  -0.5415882  -0.35329807 -1.223468   -1.6744953  -0.6578969 ]][0m
[37m[1m[2023-07-03 00:42:10,960][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:42:19,712][188188] train() took 8.75 seconds to complete[0m
[36m[2023-07-03 00:42:19,712][188188] FPS: 438827.96[0m
[36m[2023-07-03 00:42:19,714][188188] itr=0, itrs=2000, Progress: 0.00%[0m
[36m[2023-07-03 00:42:31,141][188188] train() took 11.41 seconds to complete[0m
[36m[2023-07-03 00:42:31,141][188188] FPS: 336501.57[0m
[36m[2023-07-03 00:42:35,336][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:42:35,336][188188] Reward + Measures: [[145.3270595    0.17726432   0.18277234   0.17729533   0.17002633
    1.9309988 ]][0m
[37m[1m[2023-07-03 00:42:35,336][188188] Max Reward on eval: 145.32705950353284[0m
[37m[1m[2023-07-03 00:42:35,337][188188] Min Reward on eval: 145.32705950353284[0m
[37m[1m[2023-07-03 00:42:35,337][188188] Mean Reward across all agents: 145.32705950353284[0m
[37m[1m[2023-07-03 00:42:35,337][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:42:40,278][188188] Finished Evaluation Step[0m
[37m[1m[2023-07-03 00:42:40,279][188188] Reward + Measures: [[ 65.42957775   0.1496       0.1837       0.1349       0.15119998
    2.1024375 ]
 [ 42.36361246   0.086        0.11700001   0.098        0.1085
    2.28055644]
 [ 85.80933225   0.1244       0.11689999   0.11719999   0.11140001
    2.3262856 ]
 ...
 [208.4845643    0.14470001   0.14570001   0.13739999   0.11799999
    2.14740682]
 [ 89.81088822   0.1592       0.14670001   0.1191       0.1285
    2.24519515]
 [ 97.16148553   0.1073       0.1182       0.0956       0.10910001
    2.16570282]][0m
[37m[1m[2023-07-03 00:42:40,279][188188] Max Reward on eval: 296.2781914835796[0m
[37m[1m[2023-07-03 00:42:40,280][188188] Min Reward on eval: -14.363389656692743[0m
[37m[1m[2023-07-03 00:42:40,280][188188] Mean Reward across all agents: 104.23589749819371[0m
[37m[1m[2023-07-03 00:42:40,280][188188] Average Trajectory Length: 1000.0[0m
[36m[2023-07-03 00:42:40,289][188188] mean_value=79.08689121329839, max_value=270.20929252860344[0m
[37m[1m[2023-07-03 00:42:40,292][188188] New mean coefficients: [[ 4.9211817  -0.93608123  0.2842419  -0.01986301 -0.8909158  -0.20846856]][0m
[37m[1m[2023-07-03 00:42:40,293][188188] Moving the mean solution point...[0m
[36m[2023-07-03 00:42:49,247][188188] train() took 8.95 seconds to complete[0m
[36m[2023-07-03 00:42:49,247][188188] FPS: 428957.00[0m
[36m[2023-07-03 00:42:49,249][188188] itr=1, itrs=2000, Progress: 0.05%[0m
[36m[2023-07-03 00:43:00,881][188188] train() took 11.62 seconds to complete[0m
[36m[2023-07-03 00:43:00,881][188188] FPS: 330577.48[0m
