[36m[2023-07-10 09:07:00,734][227910] Environment ant, action_dim=8, obs_dim=87[0m
[36m[2023-07-10 09:07:25,335][227910] Created Scheduler for cma_maega with an archive learning rate of 0.1, and add mode batch, using solution dim 28816 and archive dims [7, 7, 7, 7]. Min threshold is -500.0. Restart rule is no_improvement[0m
[36m[2023-07-10 09:07:45,376][227910] train() took 16.00 seconds to complete[0m
[36m[2023-07-10 09:07:45,377][227910] FPS: 239951.05[0m
[36m[2023-07-10 09:07:50,041][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:07:50,041][227910] Reward + Measures: [[-573.60597684    0.25882679    0.25834703    0.25986701    0.25982189]][0m
[37m[1m[2023-07-10 09:07:50,041][227910] Max Reward on eval: -573.6059768364302[0m
[37m[1m[2023-07-10 09:07:50,041][227910] Min Reward on eval: -573.6059768364302[0m
[37m[1m[2023-07-10 09:07:50,042][227910] Mean Reward across all agents: -573.6059768364302[0m
[37m[1m[2023-07-10 09:07:50,042][227910] Average Trajectory Length: 537.7506666666667[0m
[36m[2023-07-10 09:07:55,954][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:07:55,954][227910] Reward + Measures: [[-672.45824703    0.29643038    0.25196618    0.26185933    0.24177884]
 [-467.0123201     0.28170019    0.1984387     0.25024009    0.2526696 ]
 [-302.76785266    0.24898437    0.24633418    0.26860115    0.28609216]
 ...
 [-656.60049801    0.26332512    0.18497108    0.24664827    0.19147165]
 [-687.22696347    0.25730357    0.24698606    0.21653903    0.23799896]
 [-441.55012178    0.25304258    0.2492609     0.26455367    0.25124133]][0m
[37m[1m[2023-07-10 09:07:55,954][227910] Max Reward on eval: -209.3619237046456[0m
[37m[1m[2023-07-10 09:07:55,955][227910] Min Reward on eval: -1234.8417915474856[0m
[37m[1m[2023-07-10 09:07:55,955][227910] Mean Reward across all agents: -584.1096266875751[0m
[37m[1m[2023-07-10 09:07:55,955][227910] Average Trajectory Length: 491.769[0m
[36m[2023-07-10 09:07:56,323][227910] mean_value=-84.1096266875751, max_value=290.6380762953544[0m
[37m[1m[2023-07-10 09:07:56,357][227910] New mean coefficients: [[ 0.8586378 -1.5143166 -1.4949265 -3.2007787 -2.328773 ]][0m
[37m[1m[2023-07-10 09:07:56,358][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:08:05,991][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 09:08:05,991][227910] FPS: 398687.76[0m
[36m[2023-07-10 09:08:05,994][227910] itr=0, itrs=2000, Progress: 0.00%[0m
[36m[2023-07-10 09:08:17,302][227910] train() took 11.30 seconds to complete[0m
[36m[2023-07-10 09:08:17,302][227910] FPS: 339961.33[0m
[36m[2023-07-10 09:08:21,943][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:08:21,943][227910] Reward + Measures: [[-245.02047235    0.24007438    0.26410228    0.21528092    0.24309151]][0m
[37m[1m[2023-07-10 09:08:21,943][227910] Max Reward on eval: -245.02047235428333[0m
[37m[1m[2023-07-10 09:08:21,944][227910] Min Reward on eval: -245.02047235428333[0m
[37m[1m[2023-07-10 09:08:21,944][227910] Mean Reward across all agents: -245.02047235428333[0m
[37m[1m[2023-07-10 09:08:21,944][227910] Average Trajectory Length: 284.3623333333333[0m
[36m[2023-07-10 09:08:27,358][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:08:27,358][227910] Reward + Measures: [[-187.06475118    0.21413258    0.30476549    0.12618248    0.30532748]
 [-207.52334533    0.30437431    0.2370895     0.25924283    0.22919033]
 [-268.84280787    0.19324304    0.32148999    0.15555412    0.20301223]
 ...
 [-157.77396013    0.23189926    0.28674084    0.28228435    0.13856958]
 [-315.35409613    0.33056957    0.23371987    0.21761422    0.17603309]
 [-199.00652598    0.19754532    0.30409995    0.1473413     0.19492495]][0m
[37m[1m[2023-07-10 09:08:27,359][227910] Max Reward on eval: -39.66244133436121[0m
[37m[1m[2023-07-10 09:08:27,359][227910] Min Reward on eval: -800.1168775708298[0m
[37m[1m[2023-07-10 09:08:27,359][227910] Mean Reward across all agents: -220.83586384166838[0m
[37m[1m[2023-07-10 09:08:27,359][227910] Average Trajectory Length: 207.99533333333332[0m
[36m[2023-07-10 09:08:27,374][227910] mean_value=264.0537885139634, max_value=460.33755866563877[0m
[37m[1m[2023-07-10 09:08:27,378][227910] New mean coefficients: [[ 1.6327695 -1.8531123 -2.212073  -3.0152774 -3.2597384]][0m
[37m[1m[2023-07-10 09:08:27,379][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:08:37,043][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 09:08:37,043][227910] FPS: 397389.28[0m
[36m[2023-07-10 09:08:37,046][227910] itr=1, itrs=2000, Progress: 0.05%[0m
[36m[2023-07-10 09:08:48,359][227910] train() took 11.30 seconds to complete[0m
[36m[2023-07-10 09:08:48,360][227910] FPS: 339789.90[0m
[36m[2023-07-10 09:08:53,036][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:08:53,036][227910] Reward + Measures: [[-85.43414751   0.23285168   0.2752308    0.20721167   0.23106892]][0m
[37m[1m[2023-07-10 09:08:53,036][227910] Max Reward on eval: -85.43414751363312[0m
[37m[1m[2023-07-10 09:08:53,036][227910] Min Reward on eval: -85.43414751363312[0m
[37m[1m[2023-07-10 09:08:53,037][227910] Mean Reward across all agents: -85.43414751363312[0m
[37m[1m[2023-07-10 09:08:53,037][227910] Average Trajectory Length: 122.35[0m
[36m[2023-07-10 09:08:58,564][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:08:58,565][227910] Reward + Measures: [[-168.73895146    0.22487019    0.3030664     0.16611169    0.20592074]
 [-104.8131794     0.26459184    0.2426838     0.27714315    0.18386793]
 [ -86.75784124    0.22707902    0.2846303     0.18838489    0.20816138]
 ...
 [-196.77193778    0.17558245    0.30118465    0.2200713     0.1946578 ]
 [ -64.9737496     0.28264064    0.25383672    0.28104398    0.19144307]
 [ -54.63463374    0.24953941    0.32128304    0.20494261    0.18705618]][0m
[37m[1m[2023-07-10 09:08:58,565][227910] Max Reward on eval: -21.161234941217117[0m
[37m[1m[2023-07-10 09:08:58,565][227910] Min Reward on eval: -403.40423875798007[0m
[37m[1m[2023-07-10 09:08:58,565][227910] Mean Reward across all agents: -101.87918652256235[0m
[37m[1m[2023-07-10 09:08:58,565][227910] Average Trajectory Length: 112.294[0m
[36m[2023-07-10 09:08:58,576][227910] mean_value=185.28232942400263, max_value=464.4021506188903[0m
[37m[1m[2023-07-10 09:08:58,578][227910] New mean coefficients: [[ 2.0146708 -1.3348383 -1.7335839 -2.8676977 -3.8713245]][0m
[37m[1m[2023-07-10 09:08:58,579][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:09:08,075][227910] train() took 9.49 seconds to complete[0m
[36m[2023-07-10 09:09:08,075][227910] FPS: 404466.41[0m
[36m[2023-07-10 09:09:08,078][227910] itr=2, itrs=2000, Progress: 0.10%[0m
[36m[2023-07-10 09:09:19,483][227910] train() took 11.39 seconds to complete[0m
[36m[2023-07-10 09:09:19,483][227910] FPS: 337053.02[0m
[36m[2023-07-10 09:09:24,261][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:09:24,267][227910] Reward + Measures: [[-30.71850019   0.21427621   0.31181875   0.20306396   0.21102592]][0m
[37m[1m[2023-07-10 09:09:24,267][227910] Max Reward on eval: -30.718500189687553[0m
[37m[1m[2023-07-10 09:09:24,267][227910] Min Reward on eval: -30.718500189687553[0m
[37m[1m[2023-07-10 09:09:24,267][227910] Mean Reward across all agents: -30.718500189687553[0m
[37m[1m[2023-07-10 09:09:24,268][227910] Average Trajectory Length: 67.03666666666666[0m
[36m[2023-07-10 09:09:29,734][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:09:29,740][227910] Reward + Measures: [[ -83.71772784    0.23571931    0.25800544    0.27362013    0.15230031]
 [ -21.38220036    0.27734527    0.23776911    0.28015429    0.11511721]
 [-157.85918258    0.17957278    0.27199435    0.22781877    0.20253633]
 ...
 [ -38.13436316    0.2772927     0.28947234    0.25519234    0.16909091]
 [ -39.50945779    0.22966686    0.27578139    0.20615254    0.21210961]
 [-124.45356343    0.18899082    0.3216871     0.21862049    0.18732157]][0m
[37m[1m[2023-07-10 09:09:29,740][227910] Max Reward on eval: 12.830261258874089[0m
[37m[1m[2023-07-10 09:09:29,740][227910] Min Reward on eval: -284.76919895680624[0m
[37m[1m[2023-07-10 09:09:29,741][227910] Mean Reward across all agents: -62.87431342401753[0m
[37m[1m[2023-07-10 09:09:29,741][227910] Average Trajectory Length: 88.907[0m
[36m[2023-07-10 09:09:29,751][227910] mean_value=145.48591046303048, max_value=504.0641900551738[0m
[37m[1m[2023-07-10 09:09:29,754][227910] New mean coefficients: [[ 1.7041603 -0.7546928 -1.2722524 -2.7864347 -4.5494275]][0m
[37m[1m[2023-07-10 09:09:29,755][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:09:39,520][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 09:09:39,520][227910] FPS: 393308.40[0m
[36m[2023-07-10 09:09:39,523][227910] itr=3, itrs=2000, Progress: 0.15%[0m
[36m[2023-07-10 09:09:51,004][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 09:09:51,005][227910] FPS: 334830.21[0m
[36m[2023-07-10 09:09:55,680][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:09:55,680][227910] Reward + Measures: [[-6.62756233  0.214551    0.35732368  0.21911064  0.1700066 ]][0m
[37m[1m[2023-07-10 09:09:55,680][227910] Max Reward on eval: -6.627562330609071[0m
[37m[1m[2023-07-10 09:09:55,681][227910] Min Reward on eval: -6.627562330609071[0m
[37m[1m[2023-07-10 09:09:55,681][227910] Mean Reward across all agents: -6.627562330609071[0m
[37m[1m[2023-07-10 09:09:55,681][227910] Average Trajectory Length: 43.91[0m
[36m[2023-07-10 09:10:01,094][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:10:01,095][227910] Reward + Measures: [[ -15.01610826    0.32658789    0.25012478    0.32684827    0.09319446]
 [ -18.4209517     0.26704296    0.36311194    0.22603844    0.09192111]
 [ -21.11229332    0.25713336    0.3732751     0.24814585    0.11899707]
 ...
 [  -2.12924111    0.27521017    0.29672346    0.24021943    0.09767102]
 [ -17.76580154    0.21204887    0.31651655    0.20237775    0.22275357]
 [-104.83520781    0.24083352    0.27406493    0.26096433    0.20428191]][0m
[37m[1m[2023-07-10 09:10:01,095][227910] Max Reward on eval: 20.268853283673526[0m
[37m[1m[2023-07-10 09:10:01,096][227910] Min Reward on eval: -257.4386102687684[0m
[37m[1m[2023-07-10 09:10:01,096][227910] Mean Reward across all agents: -34.927316366236745[0m
[37m[1m[2023-07-10 09:10:01,096][227910] Average Trajectory Length: 71.82866666666666[0m
[36m[2023-07-10 09:10:01,105][227910] mean_value=148.08041560111278, max_value=513.1606327772373[0m
[37m[1m[2023-07-10 09:10:01,108][227910] New mean coefficients: [[ 1.4142575  -0.11073935 -0.6997078  -1.9006174  -4.418113  ]][0m
[37m[1m[2023-07-10 09:10:01,109][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:10:10,800][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 09:10:10,801][227910] FPS: 396288.18[0m
[36m[2023-07-10 09:10:10,803][227910] itr=4, itrs=2000, Progress: 0.20%[0m
[36m[2023-07-10 09:10:22,299][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 09:10:22,299][227910] FPS: 334460.31[0m
[36m[2023-07-10 09:10:27,074][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:10:27,074][227910] Reward + Measures: [[6.68809884 0.24350105 0.41381016 0.250783   0.12868166]][0m
[37m[1m[2023-07-10 09:10:27,075][227910] Max Reward on eval: 6.688098839216992[0m
[37m[1m[2023-07-10 09:10:27,075][227910] Min Reward on eval: 6.688098839216992[0m
[37m[1m[2023-07-10 09:10:27,075][227910] Mean Reward across all agents: 6.688098839216992[0m
[37m[1m[2023-07-10 09:10:27,076][227910] Average Trajectory Length: 23.098666666666666[0m
[36m[2023-07-10 09:10:32,754][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:10:32,755][227910] Reward + Measures: [[ -3.38842105   0.21902001   0.31529322   0.23509875   0.16453266]
 [ -8.30361644   0.28069097   0.31829134   0.25002801   0.1290663 ]
 [-62.54818979   0.26537108   0.26382682   0.30272701   0.09860069]
 ...
 [  2.42762872   0.29334113   0.33145332   0.29572996   0.03075758]
 [-29.93634897   0.20396507   0.31392699   0.30508789   0.13217087]
 [  3.31470078   0.28905216   0.30807275   0.30772507   0.05785134]][0m
[37m[1m[2023-07-10 09:10:32,755][227910] Max Reward on eval: 44.8880319849588[0m
[37m[1m[2023-07-10 09:10:32,755][227910] Min Reward on eval: -219.3778735110769[0m
[37m[1m[2023-07-10 09:10:32,755][227910] Mean Reward across all agents: -20.88017495417879[0m
[37m[1m[2023-07-10 09:10:32,756][227910] Average Trajectory Length: 74.378[0m
[36m[2023-07-10 09:10:32,764][227910] mean_value=131.1690816158291, max_value=516.1784237572923[0m
[37m[1m[2023-07-10 09:10:32,767][227910] New mean coefficients: [[ 1.2450166   0.07530965 -0.347792   -0.82814264 -4.201189  ]][0m
[37m[1m[2023-07-10 09:10:32,768][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:10:42,648][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 09:10:42,648][227910] FPS: 388748.10[0m
[36m[2023-07-10 09:10:42,650][227910] itr=5, itrs=2000, Progress: 0.25%[0m
[36m[2023-07-10 09:10:54,174][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 09:10:54,174][227910] FPS: 333611.53[0m
[36m[2023-07-10 09:10:58,934][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:10:58,939][227910] Reward + Measures: [[27.84032463  0.20947649  0.37706962  0.26844099  0.06331309]][0m
[37m[1m[2023-07-10 09:10:58,940][227910] Max Reward on eval: 27.840324625299274[0m
[37m[1m[2023-07-10 09:10:58,940][227910] Min Reward on eval: 27.840324625299274[0m
[37m[1m[2023-07-10 09:10:58,940][227910] Mean Reward across all agents: 27.840324625299274[0m
[37m[1m[2023-07-10 09:10:58,940][227910] Average Trajectory Length: 33.13733333333333[0m
[36m[2023-07-10 09:11:04,483][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:11:04,484][227910] Reward + Measures: [[ 4.38360411  0.18263933  0.36763823  0.31611153  0.08176961]
 [ 7.81221422  0.2646316   0.40697485  0.28309569  0.06868088]
 [ 5.91641135  0.2146095   0.44960004  0.24225612  0.11403078]
 ...
 [14.20213688  0.28002945  0.44434246  0.24443546  0.06942616]
 [ 5.94092117  0.18795244  0.34640822  0.27767229  0.08885764]
 [48.23049134  0.18755633  0.28162035  0.23541446  0.14286591]][0m
[37m[1m[2023-07-10 09:11:04,484][227910] Max Reward on eval: 65.91100014895201[0m
[37m[1m[2023-07-10 09:11:04,484][227910] Min Reward on eval: -105.40399303341401[0m
[37m[1m[2023-07-10 09:11:04,484][227910] Mean Reward across all agents: 12.80351601847355[0m
[37m[1m[2023-07-10 09:11:04,484][227910] Average Trajectory Length: 53.06666666666666[0m
[36m[2023-07-10 09:11:04,492][227910] mean_value=238.30290448273357, max_value=527.3168868654408[0m
[37m[1m[2023-07-10 09:11:04,495][227910] New mean coefficients: [[ 0.37719762 -0.3152187   0.4703518  -0.91434777 -4.439281  ]][0m
[37m[1m[2023-07-10 09:11:04,496][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:11:14,256][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 09:11:14,257][227910] FPS: 393480.68[0m
[36m[2023-07-10 09:11:14,259][227910] itr=6, itrs=2000, Progress: 0.30%[0m
[36m[2023-07-10 09:11:25,787][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 09:11:25,787][227910] FPS: 333475.62[0m
[36m[2023-07-10 09:11:30,526][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:11:30,527][227910] Reward + Measures: [[44.19663093  0.18254453  0.39296016  0.26636642  0.02968905]][0m
[37m[1m[2023-07-10 09:11:30,527][227910] Max Reward on eval: 44.19663093279888[0m
[37m[1m[2023-07-10 09:11:30,527][227910] Min Reward on eval: 44.19663093279888[0m
[37m[1m[2023-07-10 09:11:30,527][227910] Mean Reward across all agents: 44.19663093279888[0m
[37m[1m[2023-07-10 09:11:30,528][227910] Average Trajectory Length: 32.364[0m
[36m[2023-07-10 09:11:36,064][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:11:36,065][227910] Reward + Measures: [[31.08880168  0.22410379  0.49858901  0.24483979  0.02238215]
 [31.37807032  0.2049365   0.39137664  0.25753748  0.06442586]
 [-4.17298471  0.20565131  0.39644265  0.22351196  0.01844404]
 ...
 [49.78008546  0.15013216  0.42156252  0.26221579  0.0634026 ]
 [ 0.4713769   0.26612183  0.31648883  0.26492926  0.03486476]
 [29.37809516  0.23182462  0.4224712   0.20404077  0.08334478]][0m
[37m[1m[2023-07-10 09:11:36,065][227910] Max Reward on eval: 78.34742284390377[0m
[37m[1m[2023-07-10 09:11:36,065][227910] Min Reward on eval: -67.90926386546926[0m
[37m[1m[2023-07-10 09:11:36,066][227910] Mean Reward across all agents: 26.99865699799256[0m
[37m[1m[2023-07-10 09:11:36,066][227910] Average Trajectory Length: 47.565333333333335[0m
[36m[2023-07-10 09:11:36,072][227910] mean_value=55.41600975352416, max_value=531.7755896411836[0m
[37m[1m[2023-07-10 09:11:36,076][227910] New mean coefficients: [[ 0.20970288 -0.63734555  1.663901   -1.2141783  -3.6546743 ]][0m
[37m[1m[2023-07-10 09:11:36,077][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:11:45,879][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 09:11:45,879][227910] FPS: 391809.08[0m
[36m[2023-07-10 09:11:45,882][227910] itr=7, itrs=2000, Progress: 0.35%[0m
[36m[2023-07-10 09:11:57,490][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 09:11:57,490][227910] FPS: 331218.75[0m
[36m[2023-07-10 09:12:02,322][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:12:02,322][227910] Reward + Measures: [[58.79117035  0.17423005  0.44029653  0.24319933  0.02901755]][0m
[37m[1m[2023-07-10 09:12:02,322][227910] Max Reward on eval: 58.791170350026164[0m
[37m[1m[2023-07-10 09:12:02,323][227910] Min Reward on eval: 58.791170350026164[0m
[37m[1m[2023-07-10 09:12:02,323][227910] Mean Reward across all agents: 58.791170350026164[0m
[37m[1m[2023-07-10 09:12:02,323][227910] Average Trajectory Length: 39.681333333333335[0m
[36m[2023-07-10 09:12:07,840][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:12:07,841][227910] Reward + Measures: [[43.21711195  0.2229991   0.47465444  0.24911641  0.00914095]
 [63.73421865  0.18179674  0.35241643  0.22815572  0.04515341]
 [67.10915383  0.14297692  0.45421296  0.24141677  0.04366332]
 ...
 [28.78213987  0.24371922  0.43009397  0.24631563  0.00727273]
 [72.16492121  0.17802644  0.45509753  0.21071315  0.06484886]
 [30.57295641  0.21500568  0.36762163  0.24107471  0.10127902]][0m
[37m[1m[2023-07-10 09:12:07,841][227910] Max Reward on eval: 125.21550909496727[0m
[37m[1m[2023-07-10 09:12:07,841][227910] Min Reward on eval: -12.735877169203013[0m
[37m[1m[2023-07-10 09:12:07,842][227910] Mean Reward across all agents: 41.538104738394445[0m
[37m[1m[2023-07-10 09:12:07,842][227910] Average Trajectory Length: 58.297999999999995[0m
[36m[2023-07-10 09:12:07,847][227910] mean_value=26.912683732334322, max_value=581.9846391977568[0m
[37m[1m[2023-07-10 09:12:07,850][227910] New mean coefficients: [[ 0.84440875 -0.550187    1.6632915  -0.71153355 -2.9304683 ]][0m
[37m[1m[2023-07-10 09:12:07,851][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:12:17,571][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 09:12:17,572][227910] FPS: 395096.08[0m
[36m[2023-07-10 09:12:17,574][227910] itr=8, itrs=2000, Progress: 0.40%[0m
[36m[2023-07-10 09:12:29,079][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 09:12:29,079][227910] FPS: 334153.73[0m
[36m[2023-07-10 09:12:33,967][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:12:33,968][227910] Reward + Measures: [[94.72995492  0.18575242  0.40170881  0.23460416  0.04136755]][0m
[37m[1m[2023-07-10 09:12:33,968][227910] Max Reward on eval: 94.72995492121917[0m
[37m[1m[2023-07-10 09:12:33,968][227910] Min Reward on eval: 94.72995492121917[0m
[37m[1m[2023-07-10 09:12:33,968][227910] Mean Reward across all agents: 94.72995492121917[0m
[37m[1m[2023-07-10 09:12:33,968][227910] Average Trajectory Length: 63.026666666666664[0m
[36m[2023-07-10 09:12:39,610][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:12:39,615][227910] Reward + Measures: [[58.21138468  0.19922133  0.41130993  0.24156676  0.08121043]
 [70.41162381  0.23044685  0.4377391   0.21410513  0.04877296]
 [67.1165891   0.24266516  0.38963914  0.25013152  0.06428359]
 ...
 [73.83678854  0.22509019  0.42483827  0.22472528  0.07824612]
 [19.915045    0.24009275  0.41399392  0.29498109  0.01904762]
 [78.22708488  0.19554232  0.32615516  0.26227474  0.11567821]][0m
[37m[1m[2023-07-10 09:12:39,616][227910] Max Reward on eval: 155.04738134499058[0m
[37m[1m[2023-07-10 09:12:39,616][227910] Min Reward on eval: 0.7287023780983872[0m
[37m[1m[2023-07-10 09:12:39,616][227910] Mean Reward across all agents: 60.99707005890976[0m
[37m[1m[2023-07-10 09:12:39,617][227910] Average Trajectory Length: 61.083[0m
[36m[2023-07-10 09:12:39,621][227910] mean_value=27.515395324020968, max_value=575.4437118860427[0m
[37m[1m[2023-07-10 09:12:39,624][227910] New mean coefficients: [[ 0.7802149  0.1475907  1.6879549 -0.7742837 -2.4558492]][0m
[37m[1m[2023-07-10 09:12:39,625][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:12:49,361][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 09:12:49,362][227910] FPS: 394449.63[0m
[36m[2023-07-10 09:12:49,364][227910] itr=9, itrs=2000, Progress: 0.45%[0m
[36m[2023-07-10 09:13:00,812][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 09:13:00,812][227910] FPS: 335845.78[0m
[36m[2023-07-10 09:13:05,497][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:13:05,498][227910] Reward + Measures: [[120.56866401   0.19133008   0.39877242   0.2371652    0.03880515]][0m
[37m[1m[2023-07-10 09:13:05,498][227910] Max Reward on eval: 120.5686640053794[0m
[37m[1m[2023-07-10 09:13:05,498][227910] Min Reward on eval: 120.5686640053794[0m
[37m[1m[2023-07-10 09:13:05,498][227910] Mean Reward across all agents: 120.5686640053794[0m
[37m[1m[2023-07-10 09:13:05,499][227910] Average Trajectory Length: 71.19866666666667[0m
[36m[2023-07-10 09:13:10,900][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:13:10,901][227910] Reward + Measures: [[123.91701282   0.20358264   0.3730765    0.24852736   0.0556996 ]
 [122.73081265   0.22398613   0.31128725   0.23604444   0.07245549]
 [144.32564028   0.22736077   0.36981839   0.22086327   0.05342224]
 ...
 [ 89.59788582   0.20790108   0.43839407   0.27048618   0.06408074]
 [150.72781108   0.20042871   0.39014694   0.2080684    0.06444971]
 [ 76.63633712   0.22084574   0.34888336   0.244734     0.03505712]][0m
[37m[1m[2023-07-10 09:13:10,901][227910] Max Reward on eval: 204.69991138909828[0m
[37m[1m[2023-07-10 09:13:10,901][227910] Min Reward on eval: 24.879941222723573[0m
[37m[1m[2023-07-10 09:13:10,902][227910] Mean Reward across all agents: 100.42644625410568[0m
[37m[1m[2023-07-10 09:13:10,902][227910] Average Trajectory Length: 83.091[0m
[36m[2023-07-10 09:13:10,907][227910] mean_value=31.59048163952805, max_value=678.1654427755973[0m
[37m[1m[2023-07-10 09:13:10,910][227910] New mean coefficients: [[ 1.5169042   0.23019192  1.4688172   0.11453992 -1.7262288 ]][0m
[37m[1m[2023-07-10 09:13:10,911][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:13:20,579][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 09:13:20,579][227910] FPS: 397274.65[0m
[36m[2023-07-10 09:13:20,581][227910] itr=10, itrs=2000, Progress: 0.50%[0m
[36m[2023-07-10 09:13:33,809][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 09:13:33,809][227910] FPS: 328973.44[0m
[36m[2023-07-10 09:13:38,515][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:13:38,515][227910] Reward + Measures: [[145.14763008   0.1923542    0.39472264   0.2378533    0.04308723]][0m
[37m[1m[2023-07-10 09:13:38,515][227910] Max Reward on eval: 145.1476300807748[0m
[37m[1m[2023-07-10 09:13:38,515][227910] Min Reward on eval: 145.1476300807748[0m
[37m[1m[2023-07-10 09:13:38,516][227910] Mean Reward across all agents: 145.1476300807748[0m
[37m[1m[2023-07-10 09:13:38,516][227910] Average Trajectory Length: 74.44633333333333[0m
[36m[2023-07-10 09:13:43,989][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:13:43,990][227910] Reward + Measures: [[150.93059627   0.17276929   0.3716327    0.22482491   0.0363946 ]
 [143.5548565    0.18038869   0.33454451   0.23094599   0.04737401]
 [ 68.82240379   0.20492195   0.4371298    0.22684944   0.03654384]
 ...
 [116.17826388   0.20744501   0.38972461   0.28821301   0.04768902]
 [145.64556846   0.18582821   0.38561955   0.22625636   0.08442973]
 [164.76822213   0.20032071   0.41045424   0.24187692   0.04773837]][0m
[37m[1m[2023-07-10 09:13:43,990][227910] Max Reward on eval: 305.52698447839356[0m
[37m[1m[2023-07-10 09:13:43,990][227910] Min Reward on eval: 54.58470146406908[0m
[37m[1m[2023-07-10 09:13:43,990][227910] Mean Reward across all agents: 126.09843603851982[0m
[37m[1m[2023-07-10 09:13:43,991][227910] Average Trajectory Length: 96.335[0m
[36m[2023-07-10 09:13:43,994][227910] mean_value=18.449427376880386, max_value=677.9767097347608[0m
[37m[1m[2023-07-10 09:13:43,997][227910] New mean coefficients: [[ 2.1325214   0.7651309   1.2719789   0.21898259 -1.670336  ]][0m
[37m[1m[2023-07-10 09:13:43,998][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:13:53,592][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 09:13:53,592][227910] FPS: 400322.74[0m
[36m[2023-07-10 09:13:53,595][227910] itr=11, itrs=2000, Progress: 0.55%[0m
[36m[2023-07-10 09:14:05,159][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 09:14:05,160][227910] FPS: 332440.66[0m
[36m[2023-07-10 09:14:09,913][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:14:09,914][227910] Reward + Measures: [[173.61666637   0.19724137   0.38285661   0.23986295   0.04608529]][0m
[37m[1m[2023-07-10 09:14:09,914][227910] Max Reward on eval: 173.61666636833615[0m
[37m[1m[2023-07-10 09:14:09,914][227910] Min Reward on eval: 173.61666636833615[0m
[37m[1m[2023-07-10 09:14:09,914][227910] Mean Reward across all agents: 173.61666636833615[0m
[37m[1m[2023-07-10 09:14:09,915][227910] Average Trajectory Length: 84.856[0m
[36m[2023-07-10 09:14:15,464][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:14:15,464][227910] Reward + Measures: [[131.15216514   0.18662347   0.35901681   0.25765014   0.03344074]
 [135.62387642   0.22662178   0.30123574   0.24532261   0.08977612]
 [152.89620708   0.20652536   0.40519175   0.23765798   0.04903405]
 ...
 [170.7596447    0.21816623   0.38291392   0.24693874   0.07214083]
 [126.9168117    0.21479245   0.40405941   0.20988536   0.07726933]
 [130.40254277   0.2165904    0.33177733   0.27768645   0.0676921 ]][0m
[37m[1m[2023-07-10 09:14:15,465][227910] Max Reward on eval: 276.86903211447645[0m
[37m[1m[2023-07-10 09:14:15,465][227910] Min Reward on eval: 43.440718857420144[0m
[37m[1m[2023-07-10 09:14:15,465][227910] Mean Reward across all agents: 147.4265940049007[0m
[37m[1m[2023-07-10 09:14:15,465][227910] Average Trajectory Length: 93.70566666666666[0m
[36m[2023-07-10 09:14:15,469][227910] mean_value=12.327494798526333, max_value=445.0597163809188[0m
[37m[1m[2023-07-10 09:14:15,471][227910] New mean coefficients: [[ 2.580979    1.7285244   1.5820985   0.45960662 -1.4767895 ]][0m
[37m[1m[2023-07-10 09:14:15,472][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:14:25,072][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 09:14:25,072][227910] FPS: 400107.98[0m
[36m[2023-07-10 09:14:25,074][227910] itr=12, itrs=2000, Progress: 0.60%[0m
[36m[2023-07-10 09:14:36,529][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 09:14:36,529][227910] FPS: 335608.01[0m
[36m[2023-07-10 09:14:41,281][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:14:41,281][227910] Reward + Measures: [[222.27266726   0.2015817    0.37229997   0.23482051   0.05143167]][0m
[37m[1m[2023-07-10 09:14:41,281][227910] Max Reward on eval: 222.27266726396084[0m
[37m[1m[2023-07-10 09:14:41,281][227910] Min Reward on eval: 222.27266726396084[0m
[37m[1m[2023-07-10 09:14:41,282][227910] Mean Reward across all agents: 222.27266726396084[0m
[37m[1m[2023-07-10 09:14:41,282][227910] Average Trajectory Length: 103.76966666666667[0m
[36m[2023-07-10 09:14:46,692][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:14:46,693][227910] Reward + Measures: [[261.17019948   0.20906706   0.37049216   0.20749031   0.05822552]
 [220.28644918   0.2117102    0.3426511    0.2535401    0.07953633]
 [244.34162786   0.20231466   0.37915465   0.21921812   0.05296157]
 ...
 [220.05582625   0.20798396   0.34897101   0.26076156   0.08109416]
 [193.05627099   0.17830418   0.35036853   0.20174201   0.0485615 ]
 [234.24412067   0.19316731   0.3569921    0.22643462   0.04776339]][0m
[37m[1m[2023-07-10 09:14:46,693][227910] Max Reward on eval: 388.55054746625535[0m
[37m[1m[2023-07-10 09:14:46,693][227910] Min Reward on eval: 66.59016352635808[0m
[37m[1m[2023-07-10 09:14:46,693][227910] Mean Reward across all agents: 181.13313925887843[0m
[37m[1m[2023-07-10 09:14:46,694][227910] Average Trajectory Length: 119.43466666666666[0m
[36m[2023-07-10 09:14:46,698][227910] mean_value=6.977983881944989, max_value=403.0301348438515[0m
[37m[1m[2023-07-10 09:14:46,701][227910] New mean coefficients: [[ 2.8833396  0.9278171  1.8348098  1.373367  -0.8052749]][0m
[37m[1m[2023-07-10 09:14:46,702][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:14:56,290][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 09:14:56,290][227910] FPS: 400564.79[0m
[36m[2023-07-10 09:14:56,292][227910] itr=13, itrs=2000, Progress: 0.65%[0m
[36m[2023-07-10 09:15:07,713][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 09:15:07,713][227910] FPS: 336634.51[0m
[36m[2023-07-10 09:15:12,523][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:15:12,523][227910] Reward + Measures: [[269.6468512    0.20484786   0.35820028   0.23267052   0.05573076]][0m
[37m[1m[2023-07-10 09:15:12,524][227910] Max Reward on eval: 269.64685120478396[0m
[37m[1m[2023-07-10 09:15:12,524][227910] Min Reward on eval: 269.64685120478396[0m
[37m[1m[2023-07-10 09:15:12,524][227910] Mean Reward across all agents: 269.64685120478396[0m
[37m[1m[2023-07-10 09:15:12,524][227910] Average Trajectory Length: 120.24166666666666[0m
[36m[2023-07-10 09:15:18,011][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:15:18,012][227910] Reward + Measures: [[210.74429526   0.21180956   0.39133736   0.25133556   0.0496233 ]
 [221.69465181   0.21315637   0.41329351   0.20107746   0.04007225]
 [ 92.24089325   0.17779027   0.39394352   0.2385695    0.07119527]
 ...
 [179.98588032   0.19539216   0.28872886   0.2404779    0.04350284]
 [194.10323504   0.20868149   0.3638896    0.25579408   0.07274626]
 [177.52888214   0.21873991   0.41223785   0.24049436   0.0808317 ]][0m
[37m[1m[2023-07-10 09:15:18,012][227910] Max Reward on eval: 421.08767205826007[0m
[37m[1m[2023-07-10 09:15:18,012][227910] Min Reward on eval: 75.65351818664931[0m
[37m[1m[2023-07-10 09:15:18,012][227910] Mean Reward across all agents: 233.25029344904817[0m
[37m[1m[2023-07-10 09:15:18,013][227910] Average Trajectory Length: 147.927[0m
[36m[2023-07-10 09:15:18,017][227910] mean_value=25.684613685669042, max_value=624.8583455785032[0m
[37m[1m[2023-07-10 09:15:18,019][227910] New mean coefficients: [[ 3.2822526  1.3534677  0.7968892  2.291709  -1.1501737]][0m
[37m[1m[2023-07-10 09:15:18,020][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:15:27,722][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 09:15:27,722][227910] FPS: 395871.94[0m
[36m[2023-07-10 09:15:27,725][227910] itr=14, itrs=2000, Progress: 0.70%[0m
[36m[2023-07-10 09:15:39,226][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 09:15:39,226][227910] FPS: 334251.84[0m
[36m[2023-07-10 09:15:43,934][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:15:43,934][227910] Reward + Measures: [[352.22289845   0.2013018    0.34919417   0.23793086   0.0577733 ]][0m
[37m[1m[2023-07-10 09:15:43,934][227910] Max Reward on eval: 352.22289844520157[0m
[37m[1m[2023-07-10 09:15:43,934][227910] Min Reward on eval: 352.22289844520157[0m
[37m[1m[2023-07-10 09:15:43,935][227910] Mean Reward across all agents: 352.22289844520157[0m
[37m[1m[2023-07-10 09:15:43,935][227910] Average Trajectory Length: 152.05166666666665[0m
[36m[2023-07-10 09:15:49,285][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:15:49,286][227910] Reward + Measures: [[165.4979378    0.20246258   0.25229427   0.2349184    0.11218268]
 [277.19962885   0.19966681   0.35222912   0.21958824   0.06288172]
 [263.73680131   0.19030441   0.32432869   0.25643834   0.06632941]
 ...
 [ 97.06860905   0.28524974   0.42446136   0.24603148   0.05368183]
 [184.01733107   0.19872087   0.35501966   0.20741101   0.07322997]
 [257.04736862   0.20328479   0.33108416   0.24020658   0.08512566]][0m
[37m[1m[2023-07-10 09:15:49,286][227910] Max Reward on eval: 446.3115004376974[0m
[37m[1m[2023-07-10 09:15:49,286][227910] Min Reward on eval: 74.78119169324637[0m
[37m[1m[2023-07-10 09:15:49,286][227910] Mean Reward across all agents: 245.5920274579882[0m
[37m[1m[2023-07-10 09:15:49,287][227910] Average Trajectory Length: 157.21633333333332[0m
[36m[2023-07-10 09:15:49,290][227910] mean_value=-3.799170842781599, max_value=944.9108247413766[0m
[37m[1m[2023-07-10 09:15:49,293][227910] New mean coefficients: [[ 3.2949064   1.0179636   0.33840284  1.5620358  -1.318236  ]][0m
[37m[1m[2023-07-10 09:15:49,294][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:15:59,079][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 09:15:59,079][227910] FPS: 392504.28[0m
[36m[2023-07-10 09:15:59,081][227910] itr=15, itrs=2000, Progress: 0.75%[0m
[36m[2023-07-10 09:16:10,734][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 09:16:10,734][227910] FPS: 329894.50[0m
[36m[2023-07-10 09:16:15,536][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:16:15,537][227910] Reward + Measures: [[459.53495266   0.20272493   0.33629757   0.23842879   0.05712872]][0m
[37m[1m[2023-07-10 09:16:15,537][227910] Max Reward on eval: 459.5349526639944[0m
[37m[1m[2023-07-10 09:16:15,537][227910] Min Reward on eval: 459.5349526639944[0m
[37m[1m[2023-07-10 09:16:15,537][227910] Mean Reward across all agents: 459.5349526639944[0m
[37m[1m[2023-07-10 09:16:15,538][227910] Average Trajectory Length: 190.055[0m
[36m[2023-07-10 09:16:21,023][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:16:21,023][227910] Reward + Measures: [[488.46653631   0.19169448   0.27341434   0.17513159   0.07547496]
 [364.72797836   0.20160089   0.37499616   0.23200631   0.06847944]
 [325.39711661   0.22417967   0.30493686   0.24188733   0.060764  ]
 ...
 [577.26285173   0.18371446   0.2910594    0.22809482   0.08040976]
 [195.08178989   0.18606582   0.38117051   0.23035693   0.10285654]
 [227.66028866   0.18836685   0.33396086   0.26553607   0.08189579]][0m
[37m[1m[2023-07-10 09:16:21,024][227910] Max Reward on eval: 644.1450377687812[0m
[37m[1m[2023-07-10 09:16:21,024][227910] Min Reward on eval: 44.65713484156877[0m
[37m[1m[2023-07-10 09:16:21,024][227910] Mean Reward across all agents: 298.0824555949134[0m
[37m[1m[2023-07-10 09:16:21,024][227910] Average Trajectory Length: 186.96[0m
[36m[2023-07-10 09:16:21,028][227910] mean_value=-11.519938907867484, max_value=846.5179912088911[0m
[37m[1m[2023-07-10 09:16:21,030][227910] New mean coefficients: [[ 3.080638    1.6116691   0.5687325   0.54527366 -0.8327223 ]][0m
[37m[1m[2023-07-10 09:16:21,032][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:16:30,785][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 09:16:30,786][227910] FPS: 393754.36[0m
[36m[2023-07-10 09:16:30,788][227910] itr=16, itrs=2000, Progress: 0.80%[0m
[36m[2023-07-10 09:16:42,354][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 09:16:42,355][227910] FPS: 332406.11[0m
[36m[2023-07-10 09:16:47,131][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:16:47,132][227910] Reward + Measures: [[577.26507153   0.2065158    0.32835299   0.24205671   0.05530022]][0m
[37m[1m[2023-07-10 09:16:47,132][227910] Max Reward on eval: 577.2650715307319[0m
[37m[1m[2023-07-10 09:16:47,132][227910] Min Reward on eval: 577.2650715307319[0m
[37m[1m[2023-07-10 09:16:47,132][227910] Mean Reward across all agents: 577.2650715307319[0m
[37m[1m[2023-07-10 09:16:47,133][227910] Average Trajectory Length: 217.94966666666667[0m
[36m[2023-07-10 09:16:52,727][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:16:52,727][227910] Reward + Measures: [[375.89192724   0.20155337   0.33884463   0.22479355   0.07218389]
 [346.64393441   0.23180752   0.35242343   0.22522402   0.08592168]
 [303.00190188   0.19335674   0.2778177    0.20308685   0.08844405]
 ...
 [275.08057576   0.20436011   0.3681567    0.25462654   0.04290957]
 [259.11367796   0.18475585   0.33084437   0.27044412   0.1340453 ]
 [357.63952969   0.22981992   0.35191703   0.2145443    0.0759772 ]][0m
[37m[1m[2023-07-10 09:16:52,728][227910] Max Reward on eval: 884.5264933397644[0m
[37m[1m[2023-07-10 09:16:52,728][227910] Min Reward on eval: 105.27483821241185[0m
[37m[1m[2023-07-10 09:16:52,728][227910] Mean Reward across all agents: 348.4112109091001[0m
[37m[1m[2023-07-10 09:16:52,728][227910] Average Trajectory Length: 187.85033333333334[0m
[36m[2023-07-10 09:16:52,731][227910] mean_value=-35.953155414962715, max_value=803.6256195675991[0m
[37m[1m[2023-07-10 09:16:52,734][227910] New mean coefficients: [[ 1.8171777   1.8053397   1.0947821   0.10803705 -0.79723245]][0m
[37m[1m[2023-07-10 09:16:52,735][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:17:02,413][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 09:17:02,413][227910] FPS: 396863.62[0m
[36m[2023-07-10 09:17:02,415][227910] itr=17, itrs=2000, Progress: 0.85%[0m
[36m[2023-07-10 09:17:13,869][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 09:17:13,870][227910] FPS: 335629.27[0m
[36m[2023-07-10 09:17:18,668][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:17:18,669][227910] Reward + Measures: [[826.56834532   0.20630631   0.31589505   0.24382889   0.0552046 ]][0m
[37m[1m[2023-07-10 09:17:18,669][227910] Max Reward on eval: 826.5683453156369[0m
[37m[1m[2023-07-10 09:17:18,669][227910] Min Reward on eval: 826.5683453156369[0m
[37m[1m[2023-07-10 09:17:18,670][227910] Mean Reward across all agents: 826.5683453156369[0m
[37m[1m[2023-07-10 09:17:18,670][227910] Average Trajectory Length: 304.76[0m
[36m[2023-07-10 09:17:24,049][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:17:24,049][227910] Reward + Measures: [[433.89722207   0.18350695   0.33050388   0.22775336   0.09061616]
 [446.07856532   0.23055911   0.32206008   0.25619659   0.05540565]
 [546.23822434   0.20887013   0.29999861   0.22775488   0.05023307]
 ...
 [565.12413451   0.18095878   0.2701771    0.21140127   0.06646927]
 [343.28345825   0.22251058   0.37813747   0.26186678   0.05994227]
 [677.00122823   0.1967645    0.28848401   0.2048618    0.07886489]][0m
[37m[1m[2023-07-10 09:17:24,049][227910] Max Reward on eval: 1127.9106673406902[0m
[37m[1m[2023-07-10 09:17:24,050][227910] Min Reward on eval: 160.99169054965023[0m
[37m[1m[2023-07-10 09:17:24,050][227910] Mean Reward across all agents: 517.2426244315633[0m
[37m[1m[2023-07-10 09:17:24,050][227910] Average Trajectory Length: 256.221[0m
[36m[2023-07-10 09:17:24,053][227910] mean_value=-5.127009828947921, max_value=1229.3006564589384[0m
[37m[1m[2023-07-10 09:17:24,056][227910] New mean coefficients: [[ 1.3639874   1.6884172   0.93404865 -0.2672612  -0.3429509 ]][0m
[37m[1m[2023-07-10 09:17:24,057][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:17:33,812][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 09:17:33,813][227910] FPS: 393717.99[0m
[36m[2023-07-10 09:17:33,815][227910] itr=18, itrs=2000, Progress: 0.90%[0m
[36m[2023-07-10 09:17:45,290][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 09:17:45,291][227910] FPS: 335048.62[0m
[36m[2023-07-10 09:17:49,997][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:17:49,997][227910] Reward + Measures: [[1083.55413529    0.20973173    0.3149724     0.25161254    0.0498655 ]][0m
[37m[1m[2023-07-10 09:17:49,997][227910] Max Reward on eval: 1083.554135293532[0m
[37m[1m[2023-07-10 09:17:49,998][227910] Min Reward on eval: 1083.554135293532[0m
[37m[1m[2023-07-10 09:17:49,998][227910] Mean Reward across all agents: 1083.554135293532[0m
[37m[1m[2023-07-10 09:17:49,998][227910] Average Trajectory Length: 365.493[0m
[36m[2023-07-10 09:17:55,397][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:17:55,397][227910] Reward + Measures: [[515.58898609   0.22480693   0.29603764   0.25573912   0.04848371]
 [422.10993732   0.21566701   0.30873621   0.22614518   0.06757136]
 [310.62771265   0.19432119   0.3279469    0.22596852   0.07718713]
 ...
 [476.92481671   0.25127405   0.27798828   0.21456237   0.09790278]
 [355.06455015   0.21449223   0.32879317   0.25189143   0.0887116 ]
 [689.76220827   0.21265388   0.32408652   0.27681017   0.05001647]][0m
[37m[1m[2023-07-10 09:17:55,398][227910] Max Reward on eval: 1651.9580154219643[0m
[37m[1m[2023-07-10 09:17:55,398][227910] Min Reward on eval: 157.50092965108342[0m
[37m[1m[2023-07-10 09:17:55,398][227910] Mean Reward across all agents: 629.0519594483856[0m
[37m[1m[2023-07-10 09:17:55,398][227910] Average Trajectory Length: 280.5686666666667[0m
[36m[2023-07-10 09:17:55,401][227910] mean_value=-49.49737287243183, max_value=910.9100728648597[0m
[37m[1m[2023-07-10 09:17:55,404][227910] New mean coefficients: [[ 1.5161754   1.1608961   0.75946134  0.2960906  -0.23077402]][0m
[37m[1m[2023-07-10 09:17:55,405][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:18:05,155][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 09:18:05,155][227910] FPS: 393921.95[0m
[36m[2023-07-10 09:18:05,158][227910] itr=19, itrs=2000, Progress: 0.95%[0m
[36m[2023-07-10 09:18:16,829][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 09:18:16,830][227910] FPS: 329408.11[0m
[36m[2023-07-10 09:18:21,465][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:18:21,466][227910] Reward + Measures: [[1400.53448977    0.21153761    0.30274865    0.25296864    0.0501185 ]][0m
[37m[1m[2023-07-10 09:18:21,466][227910] Max Reward on eval: 1400.534489770919[0m
[37m[1m[2023-07-10 09:18:21,466][227910] Min Reward on eval: 1400.534489770919[0m
[37m[1m[2023-07-10 09:18:21,467][227910] Mean Reward across all agents: 1400.534489770919[0m
[37m[1m[2023-07-10 09:18:21,467][227910] Average Trajectory Length: 451.2223333333333[0m
[36m[2023-07-10 09:18:27,002][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:18:27,002][227910] Reward + Measures: [[802.15534656   0.22400098   0.28392217   0.24221678   0.06337347]
 [821.11108082   0.23099199   0.26099768   0.24278371   0.06412452]
 [466.45691084   0.21376391   0.3358539    0.20338213   0.09114639]
 ...
 [673.53249124   0.2142287    0.32707953   0.24111937   0.08260699]
 [439.8825626    0.21080267   0.30974597   0.25027025   0.04435943]
 [682.28642207   0.18636341   0.29066989   0.19524878   0.07115759]][0m
[37m[1m[2023-07-10 09:18:27,002][227910] Max Reward on eval: 1999.5381494482747[0m
[37m[1m[2023-07-10 09:18:27,003][227910] Min Reward on eval: 276.1700413604325[0m
[37m[1m[2023-07-10 09:18:27,003][227910] Mean Reward across all agents: 808.3253508721665[0m
[37m[1m[2023-07-10 09:18:27,003][227910] Average Trajectory Length: 336.92966666666666[0m
[36m[2023-07-10 09:18:27,006][227910] mean_value=-59.19801530652447, max_value=1251.8514581606976[0m
[37m[1m[2023-07-10 09:18:27,009][227910] New mean coefficients: [[ 0.77895087  1.5077457   0.38127863  0.5593659  -0.33043987]][0m
[37m[1m[2023-07-10 09:18:27,010][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:18:36,759][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 09:18:36,759][227910] FPS: 393955.71[0m
[36m[2023-07-10 09:18:36,762][227910] itr=20, itrs=2000, Progress: 1.00%[0m
[36m[2023-07-10 09:18:49,884][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 09:18:49,884][227910] FPS: 334238.24[0m
[36m[2023-07-10 09:18:54,658][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:18:54,658][227910] Reward + Measures: [[1796.76849123    0.21417671    0.29411307    0.26250461    0.04790689]][0m
[37m[1m[2023-07-10 09:18:54,659][227910] Max Reward on eval: 1796.7684912297016[0m
[37m[1m[2023-07-10 09:18:54,659][227910] Min Reward on eval: 1796.7684912297016[0m
[37m[1m[2023-07-10 09:18:54,659][227910] Mean Reward across all agents: 1796.7684912297016[0m
[37m[1m[2023-07-10 09:18:54,659][227910] Average Trajectory Length: 547.068[0m
[36m[2023-07-10 09:19:00,228][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:19:00,228][227910] Reward + Measures: [[1048.22609505    0.17436795    0.2875205     0.21925502    0.05642698]
 [1470.28791194    0.21466012    0.33560738    0.25848624    0.03947861]
 [ 834.8066856     0.20316446    0.29932418    0.2194932     0.10719273]
 ...
 [ 587.72819122    0.21518557    0.27944332    0.204529      0.08663221]
 [1461.71511108    0.17152195    0.2361228     0.21213873    0.04062402]
 [1768.25887233    0.20308402    0.2972028     0.26046547    0.07013702]][0m
[37m[1m[2023-07-10 09:19:00,229][227910] Max Reward on eval: 2329.1876203039196[0m
[37m[1m[2023-07-10 09:19:00,229][227910] Min Reward on eval: 13.544447332061827[0m
[37m[1m[2023-07-10 09:19:00,229][227910] Mean Reward across all agents: 990.0529082658891[0m
[37m[1m[2023-07-10 09:19:00,229][227910] Average Trajectory Length: 402.06666666666666[0m
[36m[2023-07-10 09:19:00,232][227910] mean_value=-185.1491420590412, max_value=1879.4127297664947[0m
[37m[1m[2023-07-10 09:19:00,235][227910] New mean coefficients: [[ 0.24463713  1.5228192  -0.01331043  0.99814016  0.24788004]][0m
[37m[1m[2023-07-10 09:19:00,236][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:19:09,786][227910] train() took 9.55 seconds to complete[0m
[36m[2023-07-10 09:19:09,786][227910] FPS: 402159.33[0m
[36m[2023-07-10 09:19:09,788][227910] itr=21, itrs=2000, Progress: 1.05%[0m
[36m[2023-07-10 09:19:21,197][227910] train() took 11.40 seconds to complete[0m
[36m[2023-07-10 09:19:21,197][227910] FPS: 336952.80[0m
[36m[2023-07-10 09:19:25,897][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:19:25,897][227910] Reward + Measures: [[2126.69296599    0.22181299    0.28545535    0.26794827    0.04776735]][0m
[37m[1m[2023-07-10 09:19:25,897][227910] Max Reward on eval: 2126.692965994942[0m
[37m[1m[2023-07-10 09:19:25,898][227910] Min Reward on eval: 2126.692965994942[0m
[37m[1m[2023-07-10 09:19:25,898][227910] Mean Reward across all agents: 2126.692965994942[0m
[37m[1m[2023-07-10 09:19:25,898][227910] Average Trajectory Length: 634.3916666666667[0m
[36m[2023-07-10 09:19:31,346][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:19:31,352][227910] Reward + Measures: [[ 563.41189686    0.24327464    0.3209953     0.24837899    0.06219862]
 [ 709.53703329    0.21663718    0.27783909    0.24562608    0.05958534]
 [1723.69740815    0.19813895    0.24420147    0.2202401     0.05097824]
 ...
 [ 392.59203065    0.25050136    0.28288937    0.23491119    0.1021436 ]
 [ 967.67653513    0.20691136    0.2961852     0.2620047     0.06462719]
 [ 255.23768504    0.22304697    0.34073505    0.24089518    0.05068189]][0m
[37m[1m[2023-07-10 09:19:31,352][227910] Max Reward on eval: 2610.2917771296575[0m
[37m[1m[2023-07-10 09:19:31,353][227910] Min Reward on eval: 222.8966297845356[0m
[37m[1m[2023-07-10 09:19:31,353][227910] Mean Reward across all agents: 1169.0655907911237[0m
[37m[1m[2023-07-10 09:19:31,353][227910] Average Trajectory Length: 444.082[0m
[36m[2023-07-10 09:19:31,356][227910] mean_value=-197.29323382939089, max_value=1439.307552722924[0m
[37m[1m[2023-07-10 09:19:31,359][227910] New mean coefficients: [[0.13128838 1.415232   0.02426129 0.91856587 0.25543743]][0m
[37m[1m[2023-07-10 09:19:31,360][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:19:40,644][227910] train() took 9.28 seconds to complete[0m
[36m[2023-07-10 09:19:40,644][227910] FPS: 413690.41[0m
[36m[2023-07-10 09:19:40,646][227910] itr=22, itrs=2000, Progress: 1.10%[0m
[36m[2023-07-10 09:19:52,164][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 09:19:52,164][227910] FPS: 333759.71[0m
[36m[2023-07-10 09:19:56,683][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:19:56,683][227910] Reward + Measures: [[2444.44156637    0.22895072    0.27980173    0.27208418    0.04497963]][0m
[37m[1m[2023-07-10 09:19:56,684][227910] Max Reward on eval: 2444.441566365331[0m
[37m[1m[2023-07-10 09:19:56,684][227910] Min Reward on eval: 2444.441566365331[0m
[37m[1m[2023-07-10 09:19:56,684][227910] Mean Reward across all agents: 2444.441566365331[0m
[37m[1m[2023-07-10 09:19:56,684][227910] Average Trajectory Length: 716.909[0m
[36m[2023-07-10 09:20:02,078][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:20:02,079][227910] Reward + Measures: [[1851.6037232     0.23599033    0.28600761    0.26048002    0.05881408]
 [2407.55231403    0.22717409    0.29108116    0.28464472    0.0502    ]
 [1948.91699454    0.22988616    0.28025246    0.26077744    0.05277467]
 ...
 [2103.26001014    0.20196703    0.27236617    0.2599771     0.05515001]
 [2004.25561669    0.23581706    0.29361135    0.28451806    0.03972261]
 [ 949.23036378    0.21683803    0.32091662    0.24880837    0.04635627]][0m
[37m[1m[2023-07-10 09:20:02,079][227910] Max Reward on eval: 3129.054735284904[0m
[37m[1m[2023-07-10 09:20:02,079][227910] Min Reward on eval: 394.00091429417955[0m
[37m[1m[2023-07-10 09:20:02,080][227910] Mean Reward across all agents: 1497.106775329907[0m
[37m[1m[2023-07-10 09:20:02,080][227910] Average Trajectory Length: 533.793[0m
[36m[2023-07-10 09:20:02,083][227910] mean_value=-263.77369827513536, max_value=1651.2994571448712[0m
[37m[1m[2023-07-10 09:20:02,085][227910] New mean coefficients: [[0.0388069  0.8881031  0.4741791  0.63746184 0.54415977]][0m
[37m[1m[2023-07-10 09:20:02,086][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:20:11,731][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 09:20:11,732][227910] FPS: 398195.97[0m
[36m[2023-07-10 09:20:11,734][227910] itr=23, itrs=2000, Progress: 1.15%[0m
[36m[2023-07-10 09:20:23,169][227910] train() took 11.42 seconds to complete[0m
[36m[2023-07-10 09:20:23,170][227910] FPS: 336173.29[0m
[36m[2023-07-10 09:20:28,009][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:20:28,009][227910] Reward + Measures: [[2605.99725612    0.22879887    0.28667778    0.27980039    0.0453248 ]][0m
[37m[1m[2023-07-10 09:20:28,009][227910] Max Reward on eval: 2605.9972561218547[0m
[37m[1m[2023-07-10 09:20:28,009][227910] Min Reward on eval: 2605.9972561218547[0m
[37m[1m[2023-07-10 09:20:28,010][227910] Mean Reward across all agents: 2605.9972561218547[0m
[37m[1m[2023-07-10 09:20:28,010][227910] Average Trajectory Length: 771.03[0m
[36m[2023-07-10 09:20:33,655][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:20:33,656][227910] Reward + Measures: [[1615.65037355    0.2146275     0.29574323    0.26963922    0.04751143]
 [ 893.05598617    0.18854661    0.25747815    0.20726287    0.05588532]
 [2581.82405061    0.22217777    0.29745686    0.26032549    0.04437778]
 ...
 [2471.71067457    0.21091151    0.28443775    0.27017924    0.0502541 ]
 [1832.10297378    0.23755084    0.30957532    0.27355599    0.05835206]
 [2752.9878913     0.22802292    0.3107163     0.28274068    0.06125491]][0m
[37m[1m[2023-07-10 09:20:33,656][227910] Max Reward on eval: 3061.4545570530927[0m
[37m[1m[2023-07-10 09:20:33,656][227910] Min Reward on eval: 514.1513552694348[0m
[37m[1m[2023-07-10 09:20:33,656][227910] Mean Reward across all agents: 1709.032826149171[0m
[37m[1m[2023-07-10 09:20:33,657][227910] Average Trajectory Length: 592.751[0m
[36m[2023-07-10 09:20:33,659][227910] mean_value=-362.57949229812004, max_value=1708.8729055697604[0m
[37m[1m[2023-07-10 09:20:33,662][227910] New mean coefficients: [[-0.39288813  0.7913321   0.22099352  0.04052496  0.19613376]][0m
[37m[1m[2023-07-10 09:20:33,663][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:20:43,463][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 09:20:43,463][227910] FPS: 391910.07[0m
[36m[2023-07-10 09:20:43,466][227910] itr=24, itrs=2000, Progress: 1.20%[0m
[36m[2023-07-10 09:20:55,162][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 09:20:55,163][227910] FPS: 328709.71[0m
[36m[2023-07-10 09:21:00,025][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:21:00,025][227910] Reward + Measures: [[2360.34572164    0.2431694     0.288533      0.27583924    0.05499062]][0m
[37m[1m[2023-07-10 09:21:00,026][227910] Max Reward on eval: 2360.3457216425954[0m
[37m[1m[2023-07-10 09:21:00,026][227910] Min Reward on eval: 2360.3457216425954[0m
[37m[1m[2023-07-10 09:21:00,026][227910] Mean Reward across all agents: 2360.3457216425954[0m
[37m[1m[2023-07-10 09:21:00,026][227910] Average Trajectory Length: 768.7273333333333[0m
[36m[2023-07-10 09:21:05,488][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:21:05,489][227910] Reward + Measures: [[1805.63665695    0.22999731    0.26032519    0.2736423     0.05634279]
 [ 382.74915065    0.17094091    0.32811415    0.21600464    0.04746027]
 [1447.06722983    0.24903014    0.28903252    0.27411026    0.04874383]
 ...
 [1959.31427236    0.25254768    0.29771382    0.2640717     0.05827777]
 [2084.99718858    0.23643255    0.30201906    0.27468643    0.06272833]
 [1592.82195211    0.24271066    0.28446105    0.27071223    0.07130176]][0m
[37m[1m[2023-07-10 09:21:05,489][227910] Max Reward on eval: 2901.305334628455[0m
[37m[1m[2023-07-10 09:21:05,489][227910] Min Reward on eval: 382.7491506541264[0m
[37m[1m[2023-07-10 09:21:05,490][227910] Mean Reward across all agents: 1632.2695155353317[0m
[37m[1m[2023-07-10 09:21:05,490][227910] Average Trajectory Length: 612.7076666666667[0m
[36m[2023-07-10 09:21:05,492][227910] mean_value=-447.5688205488255, max_value=2326.222117488808[0m
[37m[1m[2023-07-10 09:21:05,495][227910] New mean coefficients: [[-0.69932294  0.16097414  0.44928277  1.0406982  -0.09787014]][0m
[37m[1m[2023-07-10 09:21:05,495][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:21:15,160][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 09:21:15,160][227910] FPS: 397387.27[0m
[36m[2023-07-10 09:21:15,163][227910] itr=25, itrs=2000, Progress: 1.25%[0m
[36m[2023-07-10 09:21:26,794][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 09:21:26,794][227910] FPS: 330567.20[0m
[36m[2023-07-10 09:21:31,440][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:21:31,440][227910] Reward + Measures: [[1873.98549885    0.24228862    0.29967317    0.26575801    0.07105742]][0m
[37m[1m[2023-07-10 09:21:31,440][227910] Max Reward on eval: 1873.9854988453133[0m
[37m[1m[2023-07-10 09:21:31,440][227910] Min Reward on eval: 1873.9854988453133[0m
[37m[1m[2023-07-10 09:21:31,441][227910] Mean Reward across all agents: 1873.9854988453133[0m
[37m[1m[2023-07-10 09:21:31,441][227910] Average Trajectory Length: 706.1376666666666[0m
[36m[2023-07-10 09:21:37,107][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:21:37,107][227910] Reward + Measures: [[ 746.46123095    0.24523604    0.32333821    0.27426881    0.08898765]
 [ 288.1242661     0.2207918     0.35375258    0.23492499    0.06075735]
 [1506.10584708    0.22875421    0.28731036    0.26517347    0.07445522]
 ...
 [2003.49691569    0.24364948    0.28904563    0.2597954     0.07080368]
 [1821.70868721    0.21747418    0.30181795    0.26496431    0.0695832 ]
 [ 904.01848654    0.20951335    0.33494952    0.24312019    0.08955524]][0m
[37m[1m[2023-07-10 09:21:37,108][227910] Max Reward on eval: 2367.670356483571[0m
[37m[1m[2023-07-10 09:21:37,108][227910] Min Reward on eval: 288.1242660963209[0m
[37m[1m[2023-07-10 09:21:37,108][227910] Mean Reward across all agents: 1400.3296522726284[0m
[37m[1m[2023-07-10 09:21:37,108][227910] Average Trajectory Length: 600.9843333333333[0m
[36m[2023-07-10 09:21:37,110][227910] mean_value=-566.392662817867, max_value=1160.1991011700372[0m
[37m[1m[2023-07-10 09:21:37,113][227910] New mean coefficients: [[-0.74807274  0.1675898   0.12954578  1.2100493  -0.15200031]][0m
[37m[1m[2023-07-10 09:21:37,114][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:21:47,078][227910] train() took 9.96 seconds to complete[0m
[36m[2023-07-10 09:21:47,078][227910] FPS: 385433.51[0m
[36m[2023-07-10 09:21:47,081][227910] itr=26, itrs=2000, Progress: 1.30%[0m
[36m[2023-07-10 09:21:58,533][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 09:21:58,533][227910] FPS: 335731.41[0m
[36m[2023-07-10 09:22:03,129][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:22:03,129][227910] Reward + Measures: [[1358.37013088    0.23901631    0.29817957    0.24799225    0.08274852]][0m
[37m[1m[2023-07-10 09:22:03,129][227910] Max Reward on eval: 1358.37013087682[0m
[37m[1m[2023-07-10 09:22:03,130][227910] Min Reward on eval: 1358.37013087682[0m
[37m[1m[2023-07-10 09:22:03,130][227910] Mean Reward across all agents: 1358.37013087682[0m
[37m[1m[2023-07-10 09:22:03,130][227910] Average Trajectory Length: 613.066[0m
[36m[2023-07-10 09:22:08,562][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:22:08,563][227910] Reward + Measures: [[ 721.21746278    0.26076111    0.31352186    0.24100804    0.0865159 ]
 [ 792.70950892    0.2303309     0.29952568    0.26438504    0.06948683]
 [1266.0376022     0.22222941    0.26612267    0.2331028     0.07314543]
 ...
 [1282.98676198    0.26036978    0.33539444    0.27124688    0.10326582]
 [1225.12367728    0.27031943    0.31688651    0.26096115    0.09169203]
 [1194.42366835    0.27479041    0.32142404    0.25468478    0.08820257]][0m
[37m[1m[2023-07-10 09:22:08,563][227910] Max Reward on eval: 1832.0782697279822[0m
[37m[1m[2023-07-10 09:22:08,563][227910] Min Reward on eval: 364.9753001261503[0m
[37m[1m[2023-07-10 09:22:08,564][227910] Mean Reward across all agents: 1065.4973388094188[0m
[37m[1m[2023-07-10 09:22:08,564][227910] Average Trajectory Length: 531.6[0m
[36m[2023-07-10 09:22:08,565][227910] mean_value=-860.4041371423573, max_value=660.932930907418[0m
[37m[1m[2023-07-10 09:22:08,568][227910] New mean coefficients: [[-0.49163312  0.45055956  0.5374384   1.0340594   0.21153125]][0m
[37m[1m[2023-07-10 09:22:08,569][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:22:18,334][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 09:22:18,335][227910] FPS: 393283.27[0m
[36m[2023-07-10 09:22:18,337][227910] itr=27, itrs=2000, Progress: 1.35%[0m
[36m[2023-07-10 09:22:29,941][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 09:22:29,941][227910] FPS: 331327.55[0m
[36m[2023-07-10 09:22:34,796][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:22:34,797][227910] Reward + Measures: [[982.90146855   0.23349878   0.30006436   0.23144647   0.09436174]][0m
[37m[1m[2023-07-10 09:22:34,797][227910] Max Reward on eval: 982.901468551247[0m
[37m[1m[2023-07-10 09:22:34,797][227910] Min Reward on eval: 982.901468551247[0m
[37m[1m[2023-07-10 09:22:34,797][227910] Mean Reward across all agents: 982.901468551247[0m
[37m[1m[2023-07-10 09:22:34,798][227910] Average Trajectory Length: 532.8646666666666[0m
[36m[2023-07-10 09:22:40,182][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:22:40,183][227910] Reward + Measures: [[1214.76873178    0.23997831    0.34903374    0.25616756    0.11482098]
 [ 654.62085814    0.23945411    0.34194919    0.2320303     0.08192603]
 [ 478.1995844     0.2202812     0.32951766    0.24700902    0.10361083]
 ...
 [1177.23281142    0.22359692    0.28327492    0.24104579    0.10368117]
 [ 988.00284991    0.22852977    0.30693892    0.22734061    0.09460601]
 [ 760.41377865    0.20875064    0.31733224    0.27020955    0.09611809]][0m
[37m[1m[2023-07-10 09:22:40,183][227910] Max Reward on eval: 1538.3754257870373[0m
[37m[1m[2023-07-10 09:22:40,183][227910] Min Reward on eval: 309.7024637203664[0m
[37m[1m[2023-07-10 09:22:40,183][227910] Mean Reward across all agents: 889.1838934537567[0m
[37m[1m[2023-07-10 09:22:40,184][227910] Average Trajectory Length: 516.7006666666666[0m
[36m[2023-07-10 09:22:40,185][227910] mean_value=-947.74970880213, max_value=861.2976612211811[0m
[37m[1m[2023-07-10 09:22:40,188][227910] New mean coefficients: [[-0.26623675  0.6534573   0.26744235  0.9494086  -0.02642745]][0m
[37m[1m[2023-07-10 09:22:40,189][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:22:49,831][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 09:22:49,831][227910] FPS: 398330.56[0m
[36m[2023-07-10 09:22:49,833][227910] itr=28, itrs=2000, Progress: 1.40%[0m
[36m[2023-07-10 09:23:01,491][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 09:23:01,492][227910] FPS: 329787.77[0m
[36m[2023-07-10 09:23:06,275][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:23:06,276][227910] Reward + Measures: [[693.34492493   0.22997397   0.29855016   0.21732265   0.10425504]][0m
[37m[1m[2023-07-10 09:23:06,276][227910] Max Reward on eval: 693.3449249252529[0m
[37m[1m[2023-07-10 09:23:06,276][227910] Min Reward on eval: 693.3449249252529[0m
[37m[1m[2023-07-10 09:23:06,277][227910] Mean Reward across all agents: 693.3449249252529[0m
[37m[1m[2023-07-10 09:23:06,277][227910] Average Trajectory Length: 454.97566666666665[0m
[36m[2023-07-10 09:23:11,811][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:23:11,811][227910] Reward + Measures: [[707.61421665   0.19942442   0.2587454    0.22542515   0.13404337]
 [728.20401199   0.25109729   0.30602804   0.22285013   0.09657015]
 [877.85096321   0.24580394   0.29921141   0.23972319   0.11144666]
 ...
 [733.79235587   0.19661076   0.31109703   0.21590625   0.08768076]
 [589.62395545   0.21641226   0.24477553   0.2049311    0.11273748]
 [785.87213353   0.21540694   0.30564567   0.2455122    0.09606954]][0m
[37m[1m[2023-07-10 09:23:11,811][227910] Max Reward on eval: 1383.2050032698316[0m
[37m[1m[2023-07-10 09:23:11,812][227910] Min Reward on eval: 252.09793606936[0m
[37m[1m[2023-07-10 09:23:11,812][227910] Mean Reward across all agents: 742.6095417802114[0m
[37m[1m[2023-07-10 09:23:11,812][227910] Average Trajectory Length: 488.3913333333333[0m
[36m[2023-07-10 09:23:11,814][227910] mean_value=-1100.5613118716344, max_value=1347.1939738668664[0m
[37m[1m[2023-07-10 09:23:11,816][227910] New mean coefficients: [[-0.04769164  0.8587935   0.3793419   0.7702954  -0.14857656]][0m
[37m[1m[2023-07-10 09:23:11,817][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:23:21,636][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 09:23:21,636][227910] FPS: 391151.39[0m
[36m[2023-07-10 09:23:21,639][227910] itr=29, itrs=2000, Progress: 1.45%[0m
[36m[2023-07-10 09:23:33,059][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 09:23:33,059][227910] FPS: 336661.63[0m
[36m[2023-07-10 09:23:37,794][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:23:37,794][227910] Reward + Measures: [[695.39233792   0.23394667   0.30761534   0.22346859   0.10210219]][0m
[37m[1m[2023-07-10 09:23:37,795][227910] Max Reward on eval: 695.3923379192132[0m
[37m[1m[2023-07-10 09:23:37,795][227910] Min Reward on eval: 695.3923379192132[0m
[37m[1m[2023-07-10 09:23:37,795][227910] Mean Reward across all agents: 695.3923379192132[0m
[37m[1m[2023-07-10 09:23:37,796][227910] Average Trajectory Length: 460.50399999999996[0m
[36m[2023-07-10 09:23:43,357][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:23:43,358][227910] Reward + Measures: [[694.89372101   0.24892001   0.31754348   0.24493587   0.108201  ]
 [778.65460112   0.18939002   0.24912865   0.20389608   0.11081465]
 [743.03934362   0.22421272   0.30428815   0.23580198   0.11310568]
 ...
 [517.8902439    0.21323061   0.26860818   0.23131318   0.08884321]
 [618.68122587   0.23745406   0.28026721   0.2091736    0.11740015]
 [578.130554     0.21142733   0.26970273   0.22247401   0.08224368]][0m
[37m[1m[2023-07-10 09:23:43,358][227910] Max Reward on eval: 1275.7289988914622[0m
[37m[1m[2023-07-10 09:23:43,358][227910] Min Reward on eval: 255.94466791553424[0m
[37m[1m[2023-07-10 09:23:43,359][227910] Mean Reward across all agents: 669.835484757579[0m
[37m[1m[2023-07-10 09:23:43,359][227910] Average Trajectory Length: 470.693[0m
[36m[2023-07-10 09:23:43,360][227910] mean_value=-1076.9028424195785, max_value=926.9122977284118[0m
[37m[1m[2023-07-10 09:23:43,363][227910] New mean coefficients: [[0.22706479 0.69835794 0.7073178  0.34863442 0.18098515]][0m
[37m[1m[2023-07-10 09:23:43,364][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:23:52,974][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 09:23:52,974][227910] FPS: 399643.86[0m
[36m[2023-07-10 09:23:52,977][227910] itr=30, itrs=2000, Progress: 1.50%[0m
[37m[1m[2023-07-10 09:23:54,529][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000010[0m
[36m[2023-07-10 09:24:06,246][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 09:24:06,246][227910] FPS: 331823.82[0m
[36m[2023-07-10 09:24:11,127][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:24:11,127][227910] Reward + Measures: [[1003.50919237    0.23599911    0.30974349    0.23647998    0.09410504]][0m
[37m[1m[2023-07-10 09:24:11,127][227910] Max Reward on eval: 1003.5091923697199[0m
[37m[1m[2023-07-10 09:24:11,128][227910] Min Reward on eval: 1003.5091923697199[0m
[37m[1m[2023-07-10 09:24:11,128][227910] Mean Reward across all agents: 1003.5091923697199[0m
[37m[1m[2023-07-10 09:24:11,128][227910] Average Trajectory Length: 572.8193333333334[0m
[36m[2023-07-10 09:24:16,645][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:24:16,698][227910] Reward + Measures: [[951.79975513   0.22562127   0.31718332   0.21181896   0.09210076]
 [826.09902566   0.24679385   0.30025125   0.21975203   0.10504804]
 [578.56252605   0.24931803   0.30250531   0.23990095   0.09807882]
 ...
 [601.35498333   0.2077639    0.26885092   0.1800109    0.09933683]
 [561.80212801   0.23350942   0.30347499   0.18038063   0.12522139]
 [646.14312625   0.27182841   0.34118149   0.2320787    0.11413046]][0m
[37m[1m[2023-07-10 09:24:16,699][227910] Max Reward on eval: 1526.5871641949284[0m
[37m[1m[2023-07-10 09:24:16,700][227910] Min Reward on eval: 412.6165281117195[0m
[37m[1m[2023-07-10 09:24:16,701][227910] Mean Reward across all agents: 871.5462950299793[0m
[37m[1m[2023-07-10 09:24:16,702][227910] Average Trajectory Length: 552.341[0m
[36m[2023-07-10 09:24:16,710][227910] mean_value=-969.3380898026086, max_value=950.8852037801512[0m
[37m[1m[2023-07-10 09:24:16,720][227910] New mean coefficients: [[0.36178195 0.96574235 0.63706076 0.21594037 0.11234014]][0m
[37m[1m[2023-07-10 09:24:16,724][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:24:26,427][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 09:24:26,427][227910] FPS: 395960.36[0m
[36m[2023-07-10 09:24:26,429][227910] itr=31, itrs=2000, Progress: 1.55%[0m
[36m[2023-07-10 09:24:37,984][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 09:24:37,984][227910] FPS: 332729.63[0m
[36m[2023-07-10 09:24:42,846][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:24:42,846][227910] Reward + Measures: [[1288.13786238    0.2399331     0.3121078     0.24898134    0.08726671]][0m
[37m[1m[2023-07-10 09:24:42,847][227910] Max Reward on eval: 1288.13786238048[0m
[37m[1m[2023-07-10 09:24:42,847][227910] Min Reward on eval: 1288.13786238048[0m
[37m[1m[2023-07-10 09:24:42,847][227910] Mean Reward across all agents: 1288.13786238048[0m
[37m[1m[2023-07-10 09:24:42,847][227910] Average Trajectory Length: 642.371[0m
[36m[2023-07-10 09:24:48,300][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:24:48,301][227910] Reward + Measures: [[ 729.62181526    0.21983957    0.26356679    0.23261333    0.08989926]
 [1204.96822006    0.26529363    0.33324423    0.28172034    0.08587313]
 [1353.32907416    0.23045366    0.29800728    0.23169374    0.08404774]
 ...
 [1279.74900527    0.21342269    0.273303      0.22434925    0.07479691]
 [ 864.31271713    0.24082683    0.32075155    0.23464866    0.09043856]
 [1033.00142893    0.21096461    0.28151056    0.2118787     0.08818304]][0m
[37m[1m[2023-07-10 09:24:48,301][227910] Max Reward on eval: 1889.0008721565828[0m
[37m[1m[2023-07-10 09:24:48,301][227910] Min Reward on eval: 438.323660994519[0m
[37m[1m[2023-07-10 09:24:48,301][227910] Mean Reward across all agents: 1122.1956713238335[0m
[37m[1m[2023-07-10 09:24:48,302][227910] Average Trajectory Length: 594.9283333333333[0m
[36m[2023-07-10 09:24:48,303][227910] mean_value=-728.5794338165412, max_value=1182.514059532263[0m
[37m[1m[2023-07-10 09:24:48,306][227910] New mean coefficients: [[0.0433704  1.0184243  0.6646835  0.31662732 0.07369809]][0m
[37m[1m[2023-07-10 09:24:48,307][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:24:58,009][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 09:24:58,009][227910] FPS: 395848.46[0m
[36m[2023-07-10 09:24:58,012][227910] itr=32, itrs=2000, Progress: 1.60%[0m
[36m[2023-07-10 09:25:09,491][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 09:25:09,492][227910] FPS: 334919.24[0m
[36m[2023-07-10 09:25:14,355][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:25:14,355][227910] Reward + Measures: [[1581.25642262    0.24388188    0.31888464    0.25962967    0.08144638]][0m
[37m[1m[2023-07-10 09:25:14,355][227910] Max Reward on eval: 1581.256422624727[0m
[37m[1m[2023-07-10 09:25:14,355][227910] Min Reward on eval: 1581.256422624727[0m
[37m[1m[2023-07-10 09:25:14,356][227910] Mean Reward across all agents: 1581.256422624727[0m
[37m[1m[2023-07-10 09:25:14,356][227910] Average Trajectory Length: 732.8466666666667[0m
[36m[2023-07-10 09:25:19,902][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:25:19,903][227910] Reward + Measures: [[1145.73182856    0.16552892    0.25605798    0.17315669    0.10016594]
 [1103.33631679    0.229174      0.33943984    0.22371197    0.08429521]
 [1561.65884971    0.22325765    0.32041299    0.24609876    0.09061237]
 ...
 [ 879.65795645    0.22467919    0.33082515    0.2429841     0.10605525]
 [1007.18404889    0.2314744     0.30932948    0.21947311    0.09738739]
 [1360.65111034    0.2221889     0.29370064    0.23493817    0.08645092]][0m
[37m[1m[2023-07-10 09:25:19,903][227910] Max Reward on eval: 1980.5186662732624[0m
[37m[1m[2023-07-10 09:25:19,904][227910] Min Reward on eval: 537.489101984573[0m
[37m[1m[2023-07-10 09:25:19,904][227910] Mean Reward across all agents: 1278.437220551169[0m
[37m[1m[2023-07-10 09:25:19,904][227910] Average Trajectory Length: 674.4803333333333[0m
[36m[2023-07-10 09:25:19,906][227910] mean_value=-672.5605910130989, max_value=576.5925140645039[0m
[37m[1m[2023-07-10 09:25:19,909][227910] New mean coefficients: [[-0.10256347  0.8213807   0.58890057  0.16923337 -0.15985066]][0m
[37m[1m[2023-07-10 09:25:19,910][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:25:29,600][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 09:25:29,600][227910] FPS: 396349.29[0m
[36m[2023-07-10 09:25:29,602][227910] itr=33, itrs=2000, Progress: 1.65%[0m
[36m[2023-07-10 09:25:41,268][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 09:25:41,268][227910] FPS: 329582.31[0m
[36m[2023-07-10 09:25:46,126][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:25:46,126][227910] Reward + Measures: [[1582.64834179    0.25040078    0.32948676    0.26127338    0.08118606]][0m
[37m[1m[2023-07-10 09:25:46,126][227910] Max Reward on eval: 1582.6483417858178[0m
[37m[1m[2023-07-10 09:25:46,126][227910] Min Reward on eval: 1582.6483417858178[0m
[37m[1m[2023-07-10 09:25:46,127][227910] Mean Reward across all agents: 1582.6483417858178[0m
[37m[1m[2023-07-10 09:25:46,127][227910] Average Trajectory Length: 769.1443333333333[0m
[36m[2023-07-10 09:25:51,645][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:25:51,646][227910] Reward + Measures: [[1335.41032653    0.25354278    0.33253723    0.25810775    0.08751904]
 [ 897.03899284    0.24748687    0.31884286    0.21573369    0.114735  ]
 [ 855.06169983    0.20764033    0.28397727    0.21415436    0.09485757]
 ...
 [1538.5138368     0.26736951    0.34856907    0.25799853    0.08615744]
 [1656.328025      0.24843834    0.3614791     0.28023553    0.09185495]
 [1031.28125744    0.25768235    0.35380417    0.27004015    0.10129702]][0m
[37m[1m[2023-07-10 09:25:51,646][227910] Max Reward on eval: 2285.659835556429[0m
[37m[1m[2023-07-10 09:25:51,646][227910] Min Reward on eval: 616.7682515207562[0m
[37m[1m[2023-07-10 09:25:51,646][227910] Mean Reward across all agents: 1305.2359188308426[0m
[37m[1m[2023-07-10 09:25:51,647][227910] Average Trajectory Length: 715.2536666666666[0m
[36m[2023-07-10 09:25:51,648][227910] mean_value=-612.2497055445277, max_value=612.8508669743605[0m
[37m[1m[2023-07-10 09:25:51,650][227910] New mean coefficients: [[ 0.06217167  0.36267686  0.5421578   0.49074614 -0.07758944]][0m
[37m[1m[2023-07-10 09:25:51,651][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:26:01,631][227910] train() took 9.98 seconds to complete[0m
[36m[2023-07-10 09:26:01,631][227910] FPS: 384850.71[0m
[36m[2023-07-10 09:26:01,634][227910] itr=34, itrs=2000, Progress: 1.70%[0m
[36m[2023-07-10 09:26:13,271][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 09:26:13,271][227910] FPS: 330380.81[0m
[36m[2023-07-10 09:26:18,084][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:26:18,085][227910] Reward + Measures: [[1801.21225558    0.25015798    0.3359237     0.26969975    0.07634548]][0m
[37m[1m[2023-07-10 09:26:18,085][227910] Max Reward on eval: 1801.2122555760295[0m
[37m[1m[2023-07-10 09:26:18,085][227910] Min Reward on eval: 1801.2122555760295[0m
[37m[1m[2023-07-10 09:26:18,086][227910] Mean Reward across all agents: 1801.2122555760295[0m
[37m[1m[2023-07-10 09:26:18,086][227910] Average Trajectory Length: 822.9246666666667[0m
[36m[2023-07-10 09:26:23,711][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:26:23,712][227910] Reward + Measures: [[1627.23638257    0.20846927    0.31860405    0.25844762    0.07677378]
 [1394.30027336    0.27919567    0.34728795    0.26287913    0.10815614]
 [1646.60935088    0.22940221    0.30787453    0.24663377    0.07754437]
 ...
 [1845.27733678    0.22080091    0.34436768    0.26938474    0.07354355]
 [1814.63476929    0.2535803     0.30834427    0.26848185    0.06502397]
 [1683.42535027    0.24827723    0.37970644    0.28366199    0.09531008]][0m
[37m[1m[2023-07-10 09:26:23,712][227910] Max Reward on eval: 2235.056721474277[0m
[37m[1m[2023-07-10 09:26:23,712][227910] Min Reward on eval: 675.8164367507677[0m
[37m[1m[2023-07-10 09:26:23,713][227910] Mean Reward across all agents: 1537.449744087049[0m
[37m[1m[2023-07-10 09:26:23,713][227910] Average Trajectory Length: 766.9756666666666[0m
[36m[2023-07-10 09:26:23,715][227910] mean_value=-398.6073153056401, max_value=1686.487343018892[0m
[37m[1m[2023-07-10 09:26:23,718][227910] New mean coefficients: [[ 0.04528258  0.1443483   0.44023746  0.51790196 -0.31754082]][0m
[37m[1m[2023-07-10 09:26:23,719][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:26:32,564][227910] train() took 8.84 seconds to complete[0m
[36m[2023-07-10 09:26:32,565][227910] FPS: 434178.87[0m
[36m[2023-07-10 09:26:32,592][227910] itr=35, itrs=2000, Progress: 1.75%[0m
[36m[2023-07-10 09:26:44,888][227910] train() took 12.28 seconds to complete[0m
[36m[2023-07-10 09:26:44,888][227910] FPS: 312815.09[0m
[36m[2023-07-10 09:26:48,731][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:26:48,732][227910] Reward + Measures: [[2054.20572947    0.24492425    0.34121621    0.28755397    0.06256554]][0m
[37m[1m[2023-07-10 09:26:48,732][227910] Max Reward on eval: 2054.205729468483[0m
[37m[1m[2023-07-10 09:26:48,732][227910] Min Reward on eval: 2054.205729468483[0m
[37m[1m[2023-07-10 09:26:48,732][227910] Mean Reward across all agents: 2054.205729468483[0m
[37m[1m[2023-07-10 09:26:48,732][227910] Average Trajectory Length: 866.927[0m
[36m[2023-07-10 09:26:53,397][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:26:53,397][227910] Reward + Measures: [[1625.96902899    0.23221217    0.33127686    0.27649808    0.07744624]
 [2053.09343991    0.26254487    0.32651854    0.30712628    0.07407895]
 [2144.86637258    0.24912679    0.36585578    0.31736469    0.0743794 ]
 ...
 [1644.83789267    0.23798871    0.35845891    0.27235228    0.05949593]
 [1992.83099142    0.23165107    0.34355965    0.28191376    0.06655022]
 [1446.02680676    0.27312651    0.35350251    0.27983209    0.07397483]][0m
[37m[1m[2023-07-10 09:26:53,398][227910] Max Reward on eval: 2478.1938476746786[0m
[37m[1m[2023-07-10 09:26:53,398][227910] Min Reward on eval: 977.13321914525[0m
[37m[1m[2023-07-10 09:26:53,398][227910] Mean Reward across all agents: 1834.893694916914[0m
[37m[1m[2023-07-10 09:26:53,399][227910] Average Trajectory Length: 823.4006666666667[0m
[36m[2023-07-10 09:26:53,401][227910] mean_value=-16.94695178635918, max_value=2509.063424942768[0m
[37m[1m[2023-07-10 09:26:53,404][227910] New mean coefficients: [[ 0.1908336  -0.5117551   0.12475869  0.6706401  -0.5919236 ]][0m
[37m[1m[2023-07-10 09:26:53,405][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:27:02,007][227910] train() took 8.60 seconds to complete[0m
[36m[2023-07-10 09:27:02,007][227910] FPS: 446491.80[0m
[36m[2023-07-10 09:27:02,010][227910] itr=36, itrs=2000, Progress: 1.80%[0m
[36m[2023-07-10 09:27:14,463][227910] train() took 12.44 seconds to complete[0m
[36m[2023-07-10 09:27:14,463][227910] FPS: 308714.34[0m
[36m[2023-07-10 09:27:18,725][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:27:18,725][227910] Reward + Measures: [[2395.73354628    0.24228808    0.34332713    0.30519697    0.05273009]][0m
[37m[1m[2023-07-10 09:27:18,725][227910] Max Reward on eval: 2395.7335462791334[0m
[37m[1m[2023-07-10 09:27:18,726][227910] Min Reward on eval: 2395.7335462791334[0m
[37m[1m[2023-07-10 09:27:18,726][227910] Mean Reward across all agents: 2395.7335462791334[0m
[37m[1m[2023-07-10 09:27:18,726][227910] Average Trajectory Length: 892.1476666666666[0m
[36m[2023-07-10 09:27:23,857][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:27:23,857][227910] Reward + Measures: [[2664.40870467    0.25349998    0.35810003    0.30899999    0.0506    ]
 [1914.58852681    0.24020107    0.36271682    0.32647571    0.05763718]
 [2045.41864991    0.22762752    0.33940005    0.30582649    0.056602  ]
 ...
 [2212.19958247    0.22217536    0.33421257    0.30619046    0.05305141]
 [2007.09235109    0.24251662    0.35804453    0.31268749    0.05151376]
 [2745.12620789    0.2440218     0.33967951    0.32030463    0.04977793]][0m
[37m[1m[2023-07-10 09:27:23,857][227910] Max Reward on eval: 2858.803817318147[0m
[37m[1m[2023-07-10 09:27:23,858][227910] Min Reward on eval: 1014.291542623681[0m
[37m[1m[2023-07-10 09:27:23,858][227910] Mean Reward across all agents: 2120.349320151209[0m
[37m[1m[2023-07-10 09:27:23,858][227910] Average Trajectory Length: 823.775[0m
[36m[2023-07-10 09:27:23,862][227910] mean_value=18.70730546946562, max_value=1355.2805479027952[0m
[37m[1m[2023-07-10 09:27:23,865][227910] New mean coefficients: [[ 0.35742763 -0.5741761   0.28476834  0.6408449  -0.7029876 ]][0m
[37m[1m[2023-07-10 09:27:23,866][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:27:33,110][227910] train() took 9.24 seconds to complete[0m
[36m[2023-07-10 09:27:33,110][227910] FPS: 415482.37[0m
[36m[2023-07-10 09:27:33,112][227910] itr=37, itrs=2000, Progress: 1.85%[0m
[36m[2023-07-10 09:27:45,964][227910] train() took 12.84 seconds to complete[0m
[36m[2023-07-10 09:27:45,964][227910] FPS: 299122.23[0m
[36m[2023-07-10 09:27:50,254][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:27:50,255][227910] Reward + Measures: [[2651.89397388    0.23517546    0.33996263    0.31094718    0.04414366]][0m
[37m[1m[2023-07-10 09:27:50,255][227910] Max Reward on eval: 2651.8939738790245[0m
[37m[1m[2023-07-10 09:27:50,255][227910] Min Reward on eval: 2651.8939738790245[0m
[37m[1m[2023-07-10 09:27:50,256][227910] Mean Reward across all agents: 2651.8939738790245[0m
[37m[1m[2023-07-10 09:27:50,256][227910] Average Trajectory Length: 897.7883333333333[0m
[36m[2023-07-10 09:27:55,394][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:27:55,395][227910] Reward + Measures: [[2254.2606963     0.21573694    0.32947755    0.2896896     0.0497429 ]
 [1864.26216452    0.24352305    0.33796054    0.29092798    0.08279758]
 [2743.66093262    0.24258982    0.35003796    0.31356484    0.05565648]
 ...
 [2630.18463411    0.21387513    0.32559833    0.2931233     0.04457431]
 [2694.03748765    0.24177392    0.35819566    0.30815652    0.05349131]
 [1904.96122328    0.24125819    0.32895851    0.31081635    0.05915139]][0m
[37m[1m[2023-07-10 09:27:55,395][227910] Max Reward on eval: 3047.015366183035[0m
[37m[1m[2023-07-10 09:27:55,396][227910] Min Reward on eval: 1154.3833197227796[0m
[37m[1m[2023-07-10 09:27:55,396][227910] Mean Reward across all agents: 2286.2797375004398[0m
[37m[1m[2023-07-10 09:27:55,396][227910] Average Trajectory Length: 809.3753333333333[0m
[36m[2023-07-10 09:27:55,399][227910] mean_value=-92.36497630854382, max_value=1306.550680446739[0m
[37m[1m[2023-07-10 09:27:55,402][227910] New mean coefficients: [[ 0.1685335   0.06022412  0.73975813  0.5933436  -0.64479345]][0m
[37m[1m[2023-07-10 09:27:55,403][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:28:04,504][227910] train() took 9.10 seconds to complete[0m
[36m[2023-07-10 09:28:04,504][227910] FPS: 421982.47[0m
[36m[2023-07-10 09:28:04,507][227910] itr=38, itrs=2000, Progress: 1.90%[0m
[36m[2023-07-10 09:28:17,225][227910] train() took 12.70 seconds to complete[0m
[36m[2023-07-10 09:28:17,225][227910] FPS: 302289.24[0m
[36m[2023-07-10 09:28:21,520][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:28:21,520][227910] Reward + Measures: [[2885.14644483    0.22900797    0.33850467    0.31913179    0.03903712]][0m
[37m[1m[2023-07-10 09:28:21,520][227910] Max Reward on eval: 2885.1464448323127[0m
[37m[1m[2023-07-10 09:28:21,520][227910] Min Reward on eval: 2885.1464448323127[0m
[37m[1m[2023-07-10 09:28:21,521][227910] Mean Reward across all agents: 2885.1464448323127[0m
[37m[1m[2023-07-10 09:28:21,521][227910] Average Trajectory Length: 910.1273333333334[0m
[36m[2023-07-10 09:28:26,749][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:28:26,750][227910] Reward + Measures: [[2755.30643962    0.21661215    0.33273399    0.30366331    0.03941683]
 [2070.7563797     0.17914291    0.30531785    0.30528626    0.0554826 ]
 [2643.06825128    0.23687473    0.33531103    0.28929779    0.03824615]
 ...
 [3009.25971264    0.20928954    0.33294284    0.31615779    0.03896426]
 [1886.52906667    0.20479393    0.30178472    0.27725568    0.05802583]
 [2439.42623468    0.22937737    0.33695775    0.3156729     0.05617332]][0m
[37m[1m[2023-07-10 09:28:26,750][227910] Max Reward on eval: 3220.194715779647[0m
[37m[1m[2023-07-10 09:28:26,750][227910] Min Reward on eval: 932.945058840327[0m
[37m[1m[2023-07-10 09:28:26,751][227910] Mean Reward across all agents: 2430.5947077518067[0m
[37m[1m[2023-07-10 09:28:26,751][227910] Average Trajectory Length: 831.9143333333333[0m
[36m[2023-07-10 09:28:26,754][227910] mean_value=-20.009543409250714, max_value=3228.9227850506486[0m
[37m[1m[2023-07-10 09:28:26,758][227910] New mean coefficients: [[ 0.24616815 -0.07318226  0.5092032   0.7851521  -0.5681161 ]][0m
[37m[1m[2023-07-10 09:28:26,759][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:28:35,997][227910] train() took 9.24 seconds to complete[0m
[36m[2023-07-10 09:28:36,002][227910] FPS: 415761.78[0m
[36m[2023-07-10 09:28:36,004][227910] itr=39, itrs=2000, Progress: 1.95%[0m
[36m[2023-07-10 09:28:48,873][227910] train() took 12.85 seconds to complete[0m
[36m[2023-07-10 09:28:48,874][227910] FPS: 298743.91[0m
[36m[2023-07-10 09:28:53,198][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:28:53,199][227910] Reward + Measures: [[3076.30824589    0.22478679    0.32987508    0.32196054    0.03463619]][0m
[37m[1m[2023-07-10 09:28:53,199][227910] Max Reward on eval: 3076.3082458925023[0m
[37m[1m[2023-07-10 09:28:53,199][227910] Min Reward on eval: 3076.3082458925023[0m
[37m[1m[2023-07-10 09:28:53,200][227910] Mean Reward across all agents: 3076.3082458925023[0m
[37m[1m[2023-07-10 09:28:53,200][227910] Average Trajectory Length: 912.78[0m
[36m[2023-07-10 09:28:58,276][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:28:58,281][227910] Reward + Measures: [[1529.66785377    0.21091495    0.30636129    0.29033816    0.03428667]
 [2805.1297519     0.20424452    0.30512449    0.32117969    0.04033471]
 [2725.07800807    0.21962118    0.34661785    0.33780304    0.03098067]
 ...
 [1945.34436724    0.22799139    0.33961776    0.3268629     0.06297371]
 [1781.34785964    0.19749761    0.30317435    0.26120654    0.05343494]
 [2539.40002403    0.21659134    0.35478947    0.33533666    0.03364811]][0m
[37m[1m[2023-07-10 09:28:58,281][227910] Max Reward on eval: 3424.745940296166[0m
[37m[1m[2023-07-10 09:28:58,282][227910] Min Reward on eval: 1387.668030059617[0m
[37m[1m[2023-07-10 09:28:58,282][227910] Mean Reward across all agents: 2657.9162386590547[0m
[37m[1m[2023-07-10 09:28:58,282][227910] Average Trajectory Length: 843.261[0m
[36m[2023-07-10 09:28:58,285][227910] mean_value=-4.736275392741445, max_value=2683.4713112961695[0m
[37m[1m[2023-07-10 09:28:58,289][227910] New mean coefficients: [[-0.01703195  0.06830016  0.7613797   1.1021272  -0.43703115]][0m
[37m[1m[2023-07-10 09:28:58,290][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:29:07,585][227910] train() took 9.29 seconds to complete[0m
[36m[2023-07-10 09:29:07,585][227910] FPS: 413226.73[0m
[36m[2023-07-10 09:29:07,587][227910] itr=40, itrs=2000, Progress: 2.00%[0m
[37m[1m[2023-07-10 09:29:09,265][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000020[0m
[36m[2023-07-10 09:29:22,134][227910] train() took 12.71 seconds to complete[0m
[36m[2023-07-10 09:29:22,134][227910] FPS: 302232.52[0m
[36m[2023-07-10 09:29:26,402][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:29:26,403][227910] Reward + Measures: [[3035.12282009    0.22161849    0.35242179    0.34300604    0.03354588]][0m
[37m[1m[2023-07-10 09:29:26,403][227910] Max Reward on eval: 3035.1228200888013[0m
[37m[1m[2023-07-10 09:29:26,403][227910] Min Reward on eval: 3035.1228200888013[0m
[37m[1m[2023-07-10 09:29:26,404][227910] Mean Reward across all agents: 3035.1228200888013[0m
[37m[1m[2023-07-10 09:29:26,404][227910] Average Trajectory Length: 936.5753333333333[0m
[36m[2023-07-10 09:29:31,474][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:29:31,474][227910] Reward + Measures: [[2745.37582196    0.22420616    0.37575629    0.34990865    0.04530933]
 [2358.11392431    0.21256541    0.38322002    0.3390173     0.04482761]
 [2671.18344491    0.20500179    0.39657497    0.37095448    0.05132232]
 ...
 [2899.9146373     0.22412543    0.36430162    0.3425611     0.03352382]
 [2489.98754132    0.21584664    0.38538772    0.35197631    0.06187457]
 [3170.16813523    0.21680002    0.34100005    0.35460001    0.0494    ]][0m
[37m[1m[2023-07-10 09:29:31,474][227910] Max Reward on eval: 3314.3931341242046[0m
[37m[1m[2023-07-10 09:29:31,475][227910] Min Reward on eval: 1396.308964948263[0m
[37m[1m[2023-07-10 09:29:31,475][227910] Mean Reward across all agents: 2612.7409632538597[0m
[37m[1m[2023-07-10 09:29:31,475][227910] Average Trajectory Length: 874.5889999999999[0m
[36m[2023-07-10 09:29:31,478][227910] mean_value=-193.84449826464953, max_value=1630.3363240528665[0m
[37m[1m[2023-07-10 09:29:31,481][227910] New mean coefficients: [[ 0.1179014   0.00578022  0.28842652  1.0360332  -0.18380985]][0m
[37m[1m[2023-07-10 09:29:31,482][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:29:40,737][227910] train() took 9.25 seconds to complete[0m
[36m[2023-07-10 09:29:40,737][227910] FPS: 414992.38[0m
[36m[2023-07-10 09:29:40,739][227910] itr=41, itrs=2000, Progress: 2.05%[0m
[36m[2023-07-10 09:29:53,605][227910] train() took 12.85 seconds to complete[0m
[36m[2023-07-10 09:29:53,605][227910] FPS: 298772.38[0m
[36m[2023-07-10 09:29:57,862][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:29:57,862][227910] Reward + Measures: [[3104.80085813    0.21828625    0.35411787    0.35662618    0.03579062]][0m
[37m[1m[2023-07-10 09:29:57,863][227910] Max Reward on eval: 3104.8008581338436[0m
[37m[1m[2023-07-10 09:29:57,863][227910] Min Reward on eval: 3104.8008581338436[0m
[37m[1m[2023-07-10 09:29:57,863][227910] Mean Reward across all agents: 3104.8008581338436[0m
[37m[1m[2023-07-10 09:29:57,863][227910] Average Trajectory Length: 942.6893333333333[0m
[36m[2023-07-10 09:30:02,888][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:30:02,889][227910] Reward + Measures: [[2678.45015335    0.21340147    0.37880602    0.3193942     0.03709425]
 [2355.1612106     0.2041869     0.3286531     0.36198097    0.0430349 ]
 [2527.78464747    0.21329048    0.38084143    0.3757616     0.04197779]
 ...
 [2468.83193047    0.19991797    0.35458338    0.34271678    0.03914694]
 [2943.32597623    0.22329235    0.36482885    0.35830623    0.04597327]
 [2878.24832677    0.21155719    0.3437764     0.34188637    0.03468303]][0m
[37m[1m[2023-07-10 09:30:02,889][227910] Max Reward on eval: 3492.367642776854[0m
[37m[1m[2023-07-10 09:30:02,889][227910] Min Reward on eval: 1456.5003836854362[0m
[37m[1m[2023-07-10 09:30:02,890][227910] Mean Reward across all agents: 2741.1255935870045[0m
[37m[1m[2023-07-10 09:30:02,890][227910] Average Trajectory Length: 882.6293333333333[0m
[36m[2023-07-10 09:30:02,893][227910] mean_value=-160.47166154314445, max_value=717.8914407311563[0m
[37m[1m[2023-07-10 09:30:02,895][227910] New mean coefficients: [[ 0.33213022 -0.09731847 -0.07612512  0.5085646   0.07433441]][0m
[37m[1m[2023-07-10 09:30:02,896][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:30:14,368][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 09:30:14,368][227910] FPS: 334805.36[0m
[36m[2023-07-10 09:30:14,377][227910] itr=42, itrs=2000, Progress: 2.10%[0m
[36m[2023-07-10 09:30:35,609][227910] train() took 21.20 seconds to complete[0m
[36m[2023-07-10 09:30:35,610][227910] FPS: 181152.48[0m
[36m[2023-07-10 09:30:41,609][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:30:41,611][227910] Reward + Measures: [[3235.5547463     0.21534745    0.34476921    0.35007581    0.03526213]][0m
[37m[1m[2023-07-10 09:30:41,612][227910] Max Reward on eval: 3235.554746303018[0m
[37m[1m[2023-07-10 09:30:41,612][227910] Min Reward on eval: 3235.554746303018[0m
[37m[1m[2023-07-10 09:30:41,613][227910] Mean Reward across all agents: 3235.554746303018[0m
[37m[1m[2023-07-10 09:30:41,613][227910] Average Trajectory Length: 924.3136666666667[0m
[36m[2023-07-10 09:30:48,638][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:30:48,639][227910] Reward + Measures: [[3168.62653947    0.2104305     0.3523623     0.34495506    0.04617988]
 [2578.20180419    0.21047309    0.34937787    0.33290249    0.04252547]
 [2909.82577319    0.20214625    0.33305264    0.34169266    0.04835341]
 ...
 [2893.40329754    0.21897125    0.33707926    0.32274315    0.040359  ]
 [3195.55558944    0.20567672    0.34257522    0.34174213    0.03608405]
 [3178.24150873    0.21213119    0.35016695    0.34212753    0.03413945]][0m
[37m[1m[2023-07-10 09:30:48,639][227910] Max Reward on eval: 3610.2059110373257[0m
[37m[1m[2023-07-10 09:30:48,639][227910] Min Reward on eval: 843.1969917732291[0m
[37m[1m[2023-07-10 09:30:48,640][227910] Mean Reward across all agents: 2912.1422953990736[0m
[37m[1m[2023-07-10 09:30:48,640][227910] Average Trajectory Length: 872.3933333333333[0m
[36m[2023-07-10 09:30:48,660][227910] mean_value=-92.4043804398995, max_value=802.5315966702451[0m
[37m[1m[2023-07-10 09:30:48,666][227910] New mean coefficients: [[ 0.41491985 -0.32799992 -0.00992633  0.37885672 -0.03489994]][0m
[37m[1m[2023-07-10 09:30:48,668][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:31:01,395][227910] train() took 12.72 seconds to complete[0m
[36m[2023-07-10 09:31:01,396][227910] FPS: 301826.17[0m
[36m[2023-07-10 09:31:01,420][227910] itr=43, itrs=2000, Progress: 2.15%[0m
[36m[2023-07-10 09:31:22,417][227910] train() took 20.96 seconds to complete[0m
[36m[2023-07-10 09:31:22,419][227910] FPS: 183199.08[0m
[36m[2023-07-10 09:31:28,258][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:31:28,260][227910] Reward + Measures: [[3368.93541958    0.21013798    0.32465056    0.3378332     0.03333581]][0m
[37m[1m[2023-07-10 09:31:28,261][227910] Max Reward on eval: 3368.9354195794376[0m
[37m[1m[2023-07-10 09:31:28,261][227910] Min Reward on eval: 3368.9354195794376[0m
[37m[1m[2023-07-10 09:31:28,261][227910] Mean Reward across all agents: 3368.9354195794376[0m
[37m[1m[2023-07-10 09:31:28,261][227910] Average Trajectory Length: 908.9979999999999[0m
[36m[2023-07-10 09:31:35,270][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:31:35,271][227910] Reward + Measures: [[3710.45469317    0.21430002    0.3328        0.34699997    0.0383    ]
 [3075.27709081    0.21866143    0.34018865    0.3371242     0.03139124]
 [3696.05128098    0.22123046    0.32168275    0.32900101    0.0319533 ]
 ...
 [3154.20433738    0.20660436    0.3405025     0.34896928    0.03022004]
 [3334.62198346    0.21006766    0.33321953    0.33954939    0.04838419]
 [3380.88933735    0.21301782    0.34889692    0.33626977    0.04027597]][0m
[37m[1m[2023-07-10 09:31:35,271][227910] Max Reward on eval: 3763.771798092872[0m
[37m[1m[2023-07-10 09:31:35,271][227910] Min Reward on eval: 1641.6616570957005[0m
[37m[1m[2023-07-10 09:31:35,272][227910] Mean Reward across all agents: 3056.629360678513[0m
[37m[1m[2023-07-10 09:31:35,272][227910] Average Trajectory Length: 858.9166666666666[0m
[36m[2023-07-10 09:31:35,276][227910] mean_value=-101.2862297632387, max_value=632.482476956372[0m
[37m[1m[2023-07-10 09:31:35,280][227910] New mean coefficients: [[ 0.70418906 -0.23513794 -0.25121373  0.23089243 -0.14643872]][0m
[37m[1m[2023-07-10 09:31:35,281][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:31:48,317][227910] train() took 13.03 seconds to complete[0m
[36m[2023-07-10 09:31:48,318][227910] FPS: 294609.92[0m
[36m[2023-07-10 09:31:48,328][227910] itr=44, itrs=2000, Progress: 2.20%[0m
[36m[2023-07-10 09:32:07,322][227910] train() took 18.96 seconds to complete[0m
[36m[2023-07-10 09:32:07,322][227910] FPS: 202537.40[0m
[36m[2023-07-10 09:32:12,996][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:32:13,003][227910] Reward + Measures: [[3427.10803602    0.2072129     0.31894061    0.32857764    0.03362226]][0m
[37m[1m[2023-07-10 09:32:13,003][227910] Max Reward on eval: 3427.108036019346[0m
[37m[1m[2023-07-10 09:32:13,004][227910] Min Reward on eval: 3427.108036019346[0m
[37m[1m[2023-07-10 09:32:13,004][227910] Mean Reward across all agents: 3427.108036019346[0m
[37m[1m[2023-07-10 09:32:13,004][227910] Average Trajectory Length: 888.4916666666667[0m
[36m[2023-07-10 09:32:21,129][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:32:21,131][227910] Reward + Measures: [[3063.3674221     0.22319756    0.32199979    0.32443354    0.07699307]
 [2390.56069176    0.22487795    0.315173      0.34167859    0.0378182 ]
 [3860.78254096    0.21920002    0.32539999    0.32589999    0.0353    ]
 ...
 [3353.41014298    0.20292346    0.3163715     0.31683627    0.0368855 ]
 [3875.21873895    0.2084        0.3206        0.34699997    0.0343    ]
 [3147.14254074    0.21130271    0.33813706    0.34141037    0.03419282]][0m
[37m[1m[2023-07-10 09:32:21,131][227910] Max Reward on eval: 3944.013275549188[0m
[37m[1m[2023-07-10 09:32:21,131][227910] Min Reward on eval: 1512.9409570697928[0m
[37m[1m[2023-07-10 09:32:21,132][227910] Mean Reward across all agents: 3117.2822173733875[0m
[37m[1m[2023-07-10 09:32:21,132][227910] Average Trajectory Length: 844.7656666666667[0m
[36m[2023-07-10 09:32:21,141][227910] mean_value=-230.6765335184033, max_value=682.2380962524885[0m
[37m[1m[2023-07-10 09:32:21,149][227910] New mean coefficients: [[ 0.56748235 -0.26025063 -0.13882363  0.02075593 -0.3281919 ]][0m
[37m[1m[2023-07-10 09:32:21,152][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:32:33,899][227910] train() took 12.74 seconds to complete[0m
[36m[2023-07-10 09:32:33,900][227910] FPS: 301355.81[0m
[36m[2023-07-10 09:32:33,918][227910] itr=45, itrs=2000, Progress: 2.25%[0m
[36m[2023-07-10 09:32:54,926][227910] train() took 20.97 seconds to complete[0m
[36m[2023-07-10 09:32:54,927][227910] FPS: 183100.44[0m
[36m[2023-07-10 09:33:00,899][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:33:00,899][227910] Reward + Measures: [[3530.19358326    0.20373486    0.31011793    0.32181498    0.03182504]][0m
[37m[1m[2023-07-10 09:33:00,900][227910] Max Reward on eval: 3530.193583257646[0m
[37m[1m[2023-07-10 09:33:00,900][227910] Min Reward on eval: 3530.193583257646[0m
[37m[1m[2023-07-10 09:33:00,900][227910] Mean Reward across all agents: 3530.193583257646[0m
[37m[1m[2023-07-10 09:33:00,900][227910] Average Trajectory Length: 873.6193333333333[0m
[36m[2023-07-10 09:33:08,005][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:33:08,008][227910] Reward + Measures: [[3432.94552182    0.20739639    0.30545875    0.32126796    0.02979247]
 [3470.37408901    0.19295263    0.31887132    0.31829283    0.03639235]
 [3517.69616507    0.19286977    0.28913292    0.31956956    0.03002568]
 ...
 [3142.1311131     0.19640903    0.2927933     0.31127957    0.03419796]
 [2029.46539576    0.21921955    0.30380604    0.30271605    0.03272492]
 [2805.20127051    0.19567069    0.3486256     0.33088452    0.04573325]][0m
[37m[1m[2023-07-10 09:33:08,008][227910] Max Reward on eval: 4054.3293437229468[0m
[37m[1m[2023-07-10 09:33:08,008][227910] Min Reward on eval: 1774.947882951703[0m
[37m[1m[2023-07-10 09:33:08,009][227910] Mean Reward across all agents: 3226.8709114957474[0m
[37m[1m[2023-07-10 09:33:08,009][227910] Average Trajectory Length: 835.136[0m
[36m[2023-07-10 09:33:08,018][227910] mean_value=-278.93943017745465, max_value=644.4178368623875[0m
[37m[1m[2023-07-10 09:33:08,026][227910] New mean coefficients: [[ 0.40033132 -0.64645886 -0.2983005   0.00347889 -0.23433569]][0m
[37m[1m[2023-07-10 09:33:08,029][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:33:21,088][227910] train() took 13.05 seconds to complete[0m
[36m[2023-07-10 09:33:21,089][227910] FPS: 294160.36[0m
[36m[2023-07-10 09:33:21,113][227910] itr=46, itrs=2000, Progress: 2.30%[0m
[36m[2023-07-10 09:33:57,725][227910] train() took 36.58 seconds to complete[0m
[36m[2023-07-10 09:33:57,725][227910] FPS: 104978.61[0m
[36m[2023-07-10 09:34:03,453][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:34:03,455][227910] Reward + Measures: [[3600.48963085    0.20242533    0.30008355    0.3122541     0.02977544]][0m
[37m[1m[2023-07-10 09:34:03,455][227910] Max Reward on eval: 3600.4896308507473[0m
[37m[1m[2023-07-10 09:34:03,456][227910] Min Reward on eval: 3600.4896308507473[0m
[37m[1m[2023-07-10 09:34:03,456][227910] Mean Reward across all agents: 3600.4896308507473[0m
[37m[1m[2023-07-10 09:34:03,456][227910] Average Trajectory Length: 861.615[0m
[36m[2023-07-10 09:34:10,481][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:34:10,485][227910] Reward + Measures: [[3833.00982887    0.1960769     0.27553448    0.29471746    0.03205822]
 [2917.49617946    0.213221      0.32544491    0.30199301    0.03260044]
 [3680.71383486    0.21177702    0.28948975    0.31122282    0.03588643]
 ...
 [1874.44245006    0.19731422    0.30184904    0.31268388    0.03185408]
 [3408.52695372    0.18553315    0.26037294    0.28077734    0.02659116]
 [3605.33389681    0.19181475    0.27789006    0.30177411    0.03249123]][0m
[37m[1m[2023-07-10 09:34:10,486][227910] Max Reward on eval: 4197.622250799276[0m
[37m[1m[2023-07-10 09:34:10,486][227910] Min Reward on eval: 963.2923419389874[0m
[37m[1m[2023-07-10 09:34:10,486][227910] Mean Reward across all agents: 3095.3495991601503[0m
[37m[1m[2023-07-10 09:34:10,486][227910] Average Trajectory Length: 781.7189999999999[0m
[36m[2023-07-10 09:34:10,491][227910] mean_value=-590.4039604124254, max_value=597.8574400298803[0m
[37m[1m[2023-07-10 09:34:10,495][227910] New mean coefficients: [[ 0.28966224 -0.53933686  0.18322954 -0.39828816 -0.0787944 ]][0m
[37m[1m[2023-07-10 09:34:10,498][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:34:23,156][227910] train() took 12.65 seconds to complete[0m
[36m[2023-07-10 09:34:23,157][227910] FPS: 303470.11[0m
[36m[2023-07-10 09:34:23,184][227910] itr=47, itrs=2000, Progress: 2.35%[0m
[36m[2023-07-10 09:34:44,009][227910] train() took 20.78 seconds to complete[0m
[36m[2023-07-10 09:34:44,010][227910] FPS: 184783.48[0m
[36m[2023-07-10 09:34:50,106][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:34:50,108][227910] Reward + Measures: [[3643.50271797    0.19938859    0.30046365    0.3032389     0.02954209]][0m
[37m[1m[2023-07-10 09:34:50,108][227910] Max Reward on eval: 3643.502717969589[0m
[37m[1m[2023-07-10 09:34:50,109][227910] Min Reward on eval: 3643.502717969589[0m
[37m[1m[2023-07-10 09:34:50,109][227910] Mean Reward across all agents: 3643.502717969589[0m
[37m[1m[2023-07-10 09:34:50,109][227910] Average Trajectory Length: 847.5366666666666[0m
[36m[2023-07-10 09:34:57,206][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:34:57,209][227910] Reward + Measures: [[3635.74637053    0.20100193    0.31052589    0.31023055    0.03029022]
 [3857.8149311     0.2103571     0.30045643    0.31396499    0.03914568]
 [3075.08109042    0.19575685    0.27669731    0.26389036    0.03840695]
 ...
 [3483.398395      0.2038803     0.30661479    0.31205851    0.03142633]
 [2911.48491756    0.2066731     0.30647835    0.30636898    0.03142786]
 [1647.29585511    0.18725766    0.29768386    0.27133182    0.03243666]][0m
[37m[1m[2023-07-10 09:34:57,210][227910] Max Reward on eval: 4297.145576554164[0m
[37m[1m[2023-07-10 09:34:57,210][227910] Min Reward on eval: 1546.5421267113184[0m
[37m[1m[2023-07-10 09:34:57,210][227910] Mean Reward across all agents: 3298.6577209411043[0m
[37m[1m[2023-07-10 09:34:57,210][227910] Average Trajectory Length: 801.0743333333334[0m
[36m[2023-07-10 09:34:57,217][227910] mean_value=-497.0900418566564, max_value=491.51543594180566[0m
[37m[1m[2023-07-10 09:34:57,221][227910] New mean coefficients: [[-0.07466418 -0.66090465  0.18473355 -0.62429273 -0.31169695]][0m
[37m[1m[2023-07-10 09:34:57,225][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:35:10,328][227910] train() took 13.10 seconds to complete[0m
[36m[2023-07-10 09:35:10,329][227910] FPS: 293205.41[0m
[36m[2023-07-10 09:35:10,335][227910] itr=48, itrs=2000, Progress: 2.40%[0m
[36m[2023-07-10 09:35:31,292][227910] train() took 20.93 seconds to complete[0m
[36m[2023-07-10 09:35:31,292][227910] FPS: 183496.69[0m
[36m[2023-07-10 09:35:37,224][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:35:37,226][227910] Reward + Measures: [[3633.15625819    0.19834065    0.30691019    0.3030456     0.02831995]][0m
[37m[1m[2023-07-10 09:35:37,227][227910] Max Reward on eval: 3633.156258191599[0m
[37m[1m[2023-07-10 09:35:37,227][227910] Min Reward on eval: 3633.156258191599[0m
[37m[1m[2023-07-10 09:35:37,228][227910] Mean Reward across all agents: 3633.156258191599[0m
[37m[1m[2023-07-10 09:35:37,228][227910] Average Trajectory Length: 848.3376666666667[0m
[36m[2023-07-10 09:35:44,917][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:35:44,918][227910] Reward + Measures: [[1771.6989187     0.19606666    0.32409456    0.30331382    0.03226114]
 [2867.96551865    0.19694093    0.31879431    0.29345131    0.02839468]
 [3499.74349995    0.20273466    0.30216894    0.29969215    0.0285055 ]
 ...
 [2780.35858008    0.19876102    0.30154973    0.29891333    0.03713736]
 [3696.63735948    0.19917305    0.30358765    0.29471236    0.02350899]
 [2110.42436255    0.1945288     0.32942015    0.29568592    0.027761  ]][0m
[37m[1m[2023-07-10 09:35:44,919][227910] Max Reward on eval: 4337.4690252730625[0m
[37m[1m[2023-07-10 09:35:44,919][227910] Min Reward on eval: 1281.1162454683333[0m
[37m[1m[2023-07-10 09:35:44,919][227910] Mean Reward across all agents: 3203.92572755774[0m
[37m[1m[2023-07-10 09:35:44,919][227910] Average Trajectory Length: 777.679[0m
[36m[2023-07-10 09:35:44,922][227910] mean_value=-629.1690050841846, max_value=652.9076568372011[0m
[37m[1m[2023-07-10 09:35:44,925][227910] New mean coefficients: [[ 0.00836112 -0.76184964  0.67734355 -0.6378588  -0.2291781 ]][0m
[37m[1m[2023-07-10 09:35:44,926][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:35:58,210][227910] train() took 13.28 seconds to complete[0m
[36m[2023-07-10 09:35:58,210][227910] FPS: 289123.95[0m
[36m[2023-07-10 09:35:58,220][227910] itr=49, itrs=2000, Progress: 2.45%[0m
[36m[2023-07-10 09:36:19,276][227910] train() took 21.02 seconds to complete[0m
[36m[2023-07-10 09:36:19,278][227910] FPS: 182724.82[0m
[36m[2023-07-10 09:36:25,330][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:36:25,331][227910] Reward + Measures: [[2110.42515882    0.18857381    0.33344933    0.27629882    0.0199555 ]][0m
[37m[1m[2023-07-10 09:36:25,332][227910] Max Reward on eval: 2110.425158820698[0m
[37m[1m[2023-07-10 09:36:25,332][227910] Min Reward on eval: 2110.425158820698[0m
[37m[1m[2023-07-10 09:36:25,332][227910] Mean Reward across all agents: 2110.425158820698[0m
[37m[1m[2023-07-10 09:36:25,332][227910] Average Trajectory Length: 490.267[0m
[36m[2023-07-10 09:36:32,430][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:36:32,433][227910] Reward + Measures: [[2152.37127754    0.18358774    0.33151451    0.28854871    0.0221129 ]
 [2756.07676266    0.19925569    0.34165141    0.30697173    0.02156594]
 [2192.40408713    0.1875435     0.35188875    0.28148857    0.02451324]
 ...
 [2436.35190613    0.20872357    0.33315077    0.29325414    0.05024295]
 [2159.49627007    0.18186784    0.32857999    0.26707149    0.02492428]
 [2361.05668084    0.1867031     0.31944856    0.29076302    0.02975758]][0m
[37m[1m[2023-07-10 09:36:32,433][227910] Max Reward on eval: 3532.5519546753726[0m
[37m[1m[2023-07-10 09:36:32,434][227910] Min Reward on eval: 743.068379120715[0m
[37m[1m[2023-07-10 09:36:32,434][227910] Mean Reward across all agents: 2076.378462096036[0m
[37m[1m[2023-07-10 09:36:32,434][227910] Average Trajectory Length: 500.30266666666665[0m
[36m[2023-07-10 09:36:32,437][227910] mean_value=-1473.8660861980527, max_value=-106.01571672111595[0m
[36m[2023-07-10 09:36:32,444][227910] XNES is restarting with a new solution whose measures are [0.18059285 0.23944612 0.20211101 0.07618968] and objective is 1469.4734429874807[0m
[36m[2023-07-10 09:36:32,447][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 09:36:32,491][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 09:36:32,494][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:36:45,105][227910] train() took 12.61 seconds to complete[0m
[36m[2023-07-10 09:36:45,106][227910] FPS: 304577.52[0m
[36m[2023-07-10 09:36:45,122][227910] itr=50, itrs=2000, Progress: 2.50%[0m
[37m[1m[2023-07-10 09:36:49,770][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000030[0m
[36m[2023-07-10 09:37:02,387][227910] train() took 12.43 seconds to complete[0m
[36m[2023-07-10 09:37:02,388][227910] FPS: 309012.73[0m
[36m[2023-07-10 09:37:06,510][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:37:06,511][227910] Reward + Measures: [[1227.92799121    0.23575699    0.29395881    0.24498303    0.08859514]][0m
[37m[1m[2023-07-10 09:37:06,511][227910] Max Reward on eval: 1227.9279912073196[0m
[37m[1m[2023-07-10 09:37:06,511][227910] Min Reward on eval: 1227.9279912073196[0m
[37m[1m[2023-07-10 09:37:06,512][227910] Mean Reward across all agents: 1227.9279912073196[0m
[37m[1m[2023-07-10 09:37:06,512][227910] Average Trajectory Length: 644.8606666666666[0m
[36m[2023-07-10 09:37:11,470][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:37:11,471][227910] Reward + Measures: [[913.18232795   0.2639043    0.33174345   0.25075004   0.09341905]
 [732.63674885   0.20806257   0.26333702   0.21879506   0.06047051]
 [516.80893925   0.1959397    0.28825402   0.25866076   0.08304152]
 ...
 [911.23214573   0.22394407   0.28941825   0.22790746   0.09082959]
 [372.86503465   0.1827517    0.26314196   0.17396352   0.09937203]
 [606.09592694   0.18829514   0.2653484    0.19044138   0.12480845]][0m
[37m[1m[2023-07-10 09:37:11,471][227910] Max Reward on eval: 1683.3825784969608[0m
[37m[1m[2023-07-10 09:37:11,471][227910] Min Reward on eval: 117.6361804962391[0m
[37m[1m[2023-07-10 09:37:11,472][227910] Mean Reward across all agents: 691.646433498024[0m
[37m[1m[2023-07-10 09:37:11,472][227910] Average Trajectory Length: 501.88366666666667[0m
[36m[2023-07-10 09:37:11,474][227910] mean_value=-1732.7730694880088, max_value=823.1062061288343[0m
[37m[1m[2023-07-10 09:37:11,477][227910] New mean coefficients: [[-0.89909816 -0.55813473 -0.22825813 -2.51198     0.0293144 ]][0m
[37m[1m[2023-07-10 09:37:11,478][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:37:20,486][227910] train() took 9.01 seconds to complete[0m
[36m[2023-07-10 09:37:20,487][227910] FPS: 426326.67[0m
[36m[2023-07-10 09:37:20,499][227910] itr=51, itrs=2000, Progress: 2.55%[0m
[36m[2023-07-10 09:37:32,808][227910] train() took 12.29 seconds to complete[0m
[36m[2023-07-10 09:37:32,808][227910] FPS: 312514.92[0m
[36m[2023-07-10 09:37:36,740][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:37:36,746][227910] Reward + Measures: [[928.06439413   0.22946256   0.29749283   0.23406126   0.0918112 ]][0m
[37m[1m[2023-07-10 09:37:36,746][227910] Max Reward on eval: 928.0643941317188[0m
[37m[1m[2023-07-10 09:37:36,746][227910] Min Reward on eval: 928.0643941317188[0m
[37m[1m[2023-07-10 09:37:36,746][227910] Mean Reward across all agents: 928.0643941317188[0m
[37m[1m[2023-07-10 09:37:36,746][227910] Average Trajectory Length: 564.5996666666666[0m
[36m[2023-07-10 09:37:41,624][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:37:41,624][227910] Reward + Measures: [[208.54630017   0.24639158   0.37264988   0.25599745   0.13440123]
 [635.616994     0.25867131   0.27675626   0.25558081   0.09636532]
 [254.69641805   0.23475848   0.31207749   0.23704226   0.11157256]
 ...
 [472.89139302   0.18627302   0.29065517   0.22393155   0.0790567 ]
 [224.44407813   0.19602357   0.30589676   0.22101517   0.15427117]
 [915.44617964   0.24048845   0.36593401   0.24234977   0.09397513]][0m
[37m[1m[2023-07-10 09:37:41,624][227910] Max Reward on eval: 1469.2203642835607[0m
[37m[1m[2023-07-10 09:37:41,625][227910] Min Reward on eval: 85.56532537294552[0m
[37m[1m[2023-07-10 09:37:41,625][227910] Mean Reward across all agents: 556.783729655523[0m
[37m[1m[2023-07-10 09:37:41,625][227910] Average Trajectory Length: 457.1193333333333[0m
[36m[2023-07-10 09:37:41,627][227910] mean_value=-1512.563011425058, max_value=521.1277737637852[0m
[37m[1m[2023-07-10 09:37:41,630][227910] New mean coefficients: [[-0.7654552  -0.62731653  0.00910872 -2.109858    1.1526775 ]][0m
[37m[1m[2023-07-10 09:37:41,631][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:37:50,593][227910] train() took 8.96 seconds to complete[0m
[36m[2023-07-10 09:37:50,594][227910] FPS: 428524.49[0m
[36m[2023-07-10 09:37:50,620][227910] itr=52, itrs=2000, Progress: 2.60%[0m
[36m[2023-07-10 09:38:03,034][227910] train() took 12.39 seconds to complete[0m
[36m[2023-07-10 09:38:03,035][227910] FPS: 309847.90[0m
[36m[2023-07-10 09:38:07,101][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:38:07,101][227910] Reward + Measures: [[449.36494313   0.2489718    0.36063662   0.24920453   0.06651644]][0m
[37m[1m[2023-07-10 09:38:07,101][227910] Max Reward on eval: 449.3649431314911[0m
[37m[1m[2023-07-10 09:38:07,102][227910] Min Reward on eval: 449.3649431314911[0m
[37m[1m[2023-07-10 09:38:07,102][227910] Mean Reward across all agents: 449.3649431314911[0m
[37m[1m[2023-07-10 09:38:07,102][227910] Average Trajectory Length: 315.303[0m
[36m[2023-07-10 09:38:11,975][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:38:11,976][227910] Reward + Measures: [[500.15114226   0.19595805   0.30928409   0.18760946   0.0988328 ]
 [533.1144421    0.21475756   0.27758136   0.24210525   0.13670692]
 [751.19052076   0.23811637   0.31721932   0.25780764   0.11725654]
 ...
 [433.94664537   0.1912103    0.27043971   0.18546854   0.11362685]
 [570.37170499   0.27705961   0.32003018   0.21320219   0.13821124]
 [495.05918697   0.2490447    0.32547405   0.23329702   0.07972027]][0m
[37m[1m[2023-07-10 09:38:11,976][227910] Max Reward on eval: 915.4033429707749[0m
[37m[1m[2023-07-10 09:38:11,976][227910] Min Reward on eval: 31.58333591017872[0m
[37m[1m[2023-07-10 09:38:11,976][227910] Mean Reward across all agents: 384.7339919521413[0m
[37m[1m[2023-07-10 09:38:11,977][227910] Average Trajectory Length: 379.7416666666667[0m
[36m[2023-07-10 09:38:11,979][227910] mean_value=-1804.8993027245265, max_value=456.65197037433546[0m
[37m[1m[2023-07-10 09:38:11,981][227910] New mean coefficients: [[-0.01172137 -0.8359731  -0.02710558 -2.2114367   2.449421  ]][0m
[37m[1m[2023-07-10 09:38:11,982][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:38:21,027][227910] train() took 9.04 seconds to complete[0m
[36m[2023-07-10 09:38:21,027][227910] FPS: 424632.52[0m
[36m[2023-07-10 09:38:21,029][227910] itr=53, itrs=2000, Progress: 2.65%[0m
[36m[2023-07-10 09:38:33,446][227910] train() took 12.40 seconds to complete[0m
[36m[2023-07-10 09:38:33,446][227910] FPS: 309659.58[0m
[36m[2023-07-10 09:38:37,508][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:38:37,509][227910] Reward + Measures: [[67.67928254  0.29661497  0.43402416  0.30857736  0.01503483]][0m
[37m[1m[2023-07-10 09:38:37,509][227910] Max Reward on eval: 67.67928253606928[0m
[37m[1m[2023-07-10 09:38:37,509][227910] Min Reward on eval: 67.67928253606928[0m
[37m[1m[2023-07-10 09:38:37,510][227910] Mean Reward across all agents: 67.67928253606928[0m
[37m[1m[2023-07-10 09:38:37,510][227910] Average Trajectory Length: 56.730666666666664[0m
[36m[2023-07-10 09:38:42,667][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:38:42,668][227910] Reward + Measures: [[396.07410529   0.226651     0.33426735   0.24497977   0.04382549]
 [  9.47768293   0.3416667    0.47777778   0.3416667    0.        ]
 [270.61650919   0.22217785   0.32936487   0.22172044   0.11770212]
 ...
 [ 80.12692851   0.27621353   0.42413121   0.28321955   0.00898204]
 [  8.6964658    0.33083335   0.39638892   0.33083335   0.        ]
 [345.39148352   0.21799195   0.33637571   0.2278347    0.06018487]][0m
[37m[1m[2023-07-10 09:38:42,668][227910] Max Reward on eval: 573.1792601663852[0m
[37m[1m[2023-07-10 09:38:42,668][227910] Min Reward on eval: 7.676765669533052[0m
[37m[1m[2023-07-10 09:38:42,668][227910] Mean Reward across all agents: 159.08259523772995[0m
[37m[1m[2023-07-10 09:38:42,668][227910] Average Trajectory Length: 194.48833333333332[0m
[36m[2023-07-10 09:38:42,671][227910] mean_value=-1881.0353294866634, max_value=509.7514346228912[0m
[37m[1m[2023-07-10 09:38:42,673][227910] New mean coefficients: [[-0.19999209 -0.7346308   0.21345791 -2.0998995   2.5532475 ]][0m
[37m[1m[2023-07-10 09:38:42,675][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:38:51,774][227910] train() took 9.10 seconds to complete[0m
[36m[2023-07-10 09:38:51,774][227910] FPS: 422088.47[0m
[36m[2023-07-10 09:38:51,778][227910] itr=54, itrs=2000, Progress: 2.70%[0m
[36m[2023-07-10 09:39:04,262][227910] train() took 12.47 seconds to complete[0m
[36m[2023-07-10 09:39:04,262][227910] FPS: 307969.78[0m
[36m[2023-07-10 09:39:08,319][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:39:08,319][227910] Reward + Measures: [[9.9604172  0.26938087 0.40850687 0.3446061  0.00159276]][0m
[37m[1m[2023-07-10 09:39:08,319][227910] Max Reward on eval: 9.960417198964395[0m
[37m[1m[2023-07-10 09:39:08,319][227910] Min Reward on eval: 9.960417198964395[0m
[37m[1m[2023-07-10 09:39:08,320][227910] Mean Reward across all agents: 9.960417198964395[0m
[37m[1m[2023-07-10 09:39:08,320][227910] Average Trajectory Length: 10.065333333333333[0m
[36m[2023-07-10 09:39:13,267][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:39:13,268][227910] Reward + Measures: [[  7.77081418   0.25555557   0.40000001   0.33333334   0.        ]
 [157.94421176   0.24806586   0.31928703   0.20676339   0.09241083]
 [  6.67037469   0.24444444   0.38611114   0.35000002   0.02222222]
 ...
 [ 13.38225653   0.26611111   0.40222222   0.3130556    0.01333333]
 [ 84.97081228   0.24828902   0.3752521    0.25206217   0.03840124]
 [308.37627404   0.22190773   0.27939248   0.19430064   0.11994616]][0m
[37m[1m[2023-07-10 09:39:13,268][227910] Max Reward on eval: 626.5932980468497[0m
[37m[1m[2023-07-10 09:39:13,268][227910] Min Reward on eval: -35.20454406485078[0m
[37m[1m[2023-07-10 09:39:13,268][227910] Mean Reward across all agents: 121.29864278027621[0m
[37m[1m[2023-07-10 09:39:13,269][227910] Average Trajectory Length: 160.78666666666666[0m
[36m[2023-07-10 09:39:13,275][227910] mean_value=-2517.5980645652107, max_value=751.5504051127471[0m
[37m[1m[2023-07-10 09:39:13,278][227910] New mean coefficients: [[ 0.16470897  0.34393108  1.1662881  -3.2804542   1.4392364 ]][0m
[37m[1m[2023-07-10 09:39:13,279][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:39:22,424][227910] train() took 9.14 seconds to complete[0m
[36m[2023-07-10 09:39:22,425][227910] FPS: 419941.88[0m
[36m[2023-07-10 09:39:22,437][227910] itr=55, itrs=2000, Progress: 2.75%[0m
[36m[2023-07-10 09:39:34,815][227910] train() took 12.36 seconds to complete[0m
[36m[2023-07-10 09:39:34,816][227910] FPS: 310738.15[0m
[36m[2023-07-10 09:39:38,915][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:39:38,916][227910] Reward + Measures: [[17.66866976  0.27533358  0.43772823  0.3207024   0.00490554]][0m
[37m[1m[2023-07-10 09:39:38,916][227910] Max Reward on eval: 17.66866975608429[0m
[37m[1m[2023-07-10 09:39:38,916][227910] Min Reward on eval: 17.66866975608429[0m
[37m[1m[2023-07-10 09:39:38,916][227910] Mean Reward across all agents: 17.66866975608429[0m
[37m[1m[2023-07-10 09:39:38,917][227910] Average Trajectory Length: 16.029[0m
[36m[2023-07-10 09:39:43,875][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:39:43,881][227910] Reward + Measures: [[ 34.19900799   0.30601883   0.41032392   0.30852666   0.01630094]
 [  6.88574858   0.30031747   0.32285717   0.33142862   0.03111111]
 [211.08662318   0.21444547   0.31023636   0.21594547   0.11189999]
 ...
 [ 10.78155916   0.28698415   0.37888893   0.32920635   0.        ]
 [113.83273432   0.23669401   0.28724435   0.27161559   0.14485168]
 [ 10.59051158   0.2787374    0.41454551   0.33255053   0.        ]][0m
[37m[1m[2023-07-10 09:39:43,881][227910] Max Reward on eval: 673.8837198628578[0m
[37m[1m[2023-07-10 09:39:43,881][227910] Min Reward on eval: 5.348505874350667[0m
[37m[1m[2023-07-10 09:39:43,881][227910] Mean Reward across all agents: 97.56804623631163[0m
[37m[1m[2023-07-10 09:39:43,882][227910] Average Trajectory Length: 153.68033333333332[0m
[36m[2023-07-10 09:39:43,884][227910] mean_value=-1766.386234581643, max_value=1095.4836947049364[0m
[37m[1m[2023-07-10 09:39:43,887][227910] New mean coefficients: [[-0.3923298   0.16840325  2.3563907  -3.0379481   1.6719741 ]][0m
[37m[1m[2023-07-10 09:39:43,888][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:39:52,766][227910] train() took 8.88 seconds to complete[0m
[36m[2023-07-10 09:39:52,766][227910] FPS: 432580.18[0m
[36m[2023-07-10 09:39:52,769][227910] itr=56, itrs=2000, Progress: 2.80%[0m
[36m[2023-07-10 09:40:04,462][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 09:40:04,463][227910] FPS: 328743.61[0m
[36m[2023-07-10 09:40:08,389][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:40:08,389][227910] Reward + Measures: [[26.20187729  0.26742765  0.46691951  0.30180573  0.00671778]][0m
[37m[1m[2023-07-10 09:40:08,389][227910] Max Reward on eval: 26.201877294950208[0m
[37m[1m[2023-07-10 09:40:08,389][227910] Min Reward on eval: 26.201877294950208[0m
[37m[1m[2023-07-10 09:40:08,390][227910] Mean Reward across all agents: 26.201877294950208[0m
[37m[1m[2023-07-10 09:40:08,390][227910] Average Trajectory Length: 26.147666666666666[0m
[36m[2023-07-10 09:40:13,113][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:40:13,114][227910] Reward + Measures: [[215.19856283   0.204981     0.30972567   0.22358158   0.11338092]
 [ 10.22047538   0.28670943   0.38944444   0.30893165   0.        ]
 [129.35498265   0.24065986   0.40122566   0.22681835   0.07165213]
 ...
 [ 56.60267864   0.28903309   0.42842242   0.2847074    0.01781171]
 [ 72.72627587   0.20356739   0.40742141   0.25939995   0.04869498]
 [503.39460936   0.24487267   0.32829452   0.1951059    0.16295598]][0m
[37m[1m[2023-07-10 09:40:13,114][227910] Max Reward on eval: 538.6973776022089[0m
[37m[1m[2023-07-10 09:40:13,114][227910] Min Reward on eval: 6.821178877353669[0m
[37m[1m[2023-07-10 09:40:13,114][227910] Mean Reward across all agents: 134.17594474495422[0m
[37m[1m[2023-07-10 09:40:13,115][227910] Average Trajectory Length: 183.22833333333332[0m
[36m[2023-07-10 09:40:13,116][227910] mean_value=-1851.9980868603595, max_value=670.9542441113388[0m
[37m[1m[2023-07-10 09:40:13,119][227910] New mean coefficients: [[ 0.6090592  -0.71488523  1.9928811  -2.3124719   2.7878094 ]][0m
[37m[1m[2023-07-10 09:40:13,120][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:40:21,703][227910] train() took 8.58 seconds to complete[0m
[36m[2023-07-10 09:40:21,703][227910] FPS: 447484.14[0m
[36m[2023-07-10 09:40:21,706][227910] itr=57, itrs=2000, Progress: 2.85%[0m
[36m[2023-07-10 09:40:33,864][227910] train() took 12.14 seconds to complete[0m
[36m[2023-07-10 09:40:33,864][227910] FPS: 316212.63[0m
[36m[2023-07-10 09:40:37,825][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:40:37,826][227910] Reward + Measures: [[160.53244286   0.22953135   0.45781669   0.23714724   0.05674978]][0m
[37m[1m[2023-07-10 09:40:37,826][227910] Max Reward on eval: 160.53244285669146[0m
[37m[1m[2023-07-10 09:40:37,826][227910] Min Reward on eval: 160.53244285669146[0m
[37m[1m[2023-07-10 09:40:37,826][227910] Mean Reward across all agents: 160.53244285669146[0m
[37m[1m[2023-07-10 09:40:37,826][227910] Average Trajectory Length: 178.75433333333334[0m
[36m[2023-07-10 09:40:42,487][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:40:42,488][227910] Reward + Measures: [[402.12614285   0.20297031   0.35204583   0.21769328   0.0859372 ]
 [399.62197709   0.20109914   0.36983249   0.22046065   0.15263896]
 [ 45.99021082   0.23759504   0.45758924   0.25744638   0.02353167]
 ...
 [464.48882608   0.20293002   0.35261145   0.22748061   0.1475241 ]
 [141.50374279   0.24005337   0.34957919   0.24413787   0.06926471]
 [404.00613531   0.16932228   0.32703993   0.20461302   0.15534125]][0m
[37m[1m[2023-07-10 09:40:42,488][227910] Max Reward on eval: 789.9958091274835[0m
[37m[1m[2023-07-10 09:40:42,488][227910] Min Reward on eval: 5.99330288246274[0m
[37m[1m[2023-07-10 09:40:42,489][227910] Mean Reward across all agents: 296.644537500624[0m
[37m[1m[2023-07-10 09:40:42,489][227910] Average Trajectory Length: 406.55966666666666[0m
[36m[2023-07-10 09:40:42,492][227910] mean_value=-916.3267841271315, max_value=832.6721504895843[0m
[37m[1m[2023-07-10 09:40:42,495][227910] New mean coefficients: [[-0.3694936  1.1653877  2.8591998 -1.5324519  3.5385873]][0m
[37m[1m[2023-07-10 09:40:42,496][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:40:51,173][227910] train() took 8.68 seconds to complete[0m
[36m[2023-07-10 09:40:51,173][227910] FPS: 442604.17[0m
[36m[2023-07-10 09:40:51,176][227910] itr=58, itrs=2000, Progress: 2.90%[0m
[36m[2023-07-10 09:41:03,311][227910] train() took 12.12 seconds to complete[0m
[36m[2023-07-10 09:41:03,311][227910] FPS: 316828.75[0m
[36m[2023-07-10 09:41:07,584][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:41:07,585][227910] Reward + Measures: [[398.14945752   0.22309741   0.39852759   0.19207662   0.16611831]][0m
[37m[1m[2023-07-10 09:41:07,585][227910] Max Reward on eval: 398.14945752177954[0m
[37m[1m[2023-07-10 09:41:07,585][227910] Min Reward on eval: 398.14945752177954[0m
[37m[1m[2023-07-10 09:41:07,585][227910] Mean Reward across all agents: 398.14945752177954[0m
[37m[1m[2023-07-10 09:41:07,586][227910] Average Trajectory Length: 492.55333333333334[0m
[36m[2023-07-10 09:41:12,883][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:41:12,889][227910] Reward + Measures: [[314.24249196   0.22231136   0.37443763   0.1758129    0.09363886]
 [689.86344698   0.27109793   0.31815639   0.2029331    0.16205239]
 [374.21014352   0.23099852   0.39744064   0.20454486   0.13192306]
 ...
 [355.43148196   0.21907631   0.31767088   0.16427897   0.13026124]
 [367.75541108   0.21487831   0.3972559    0.16493565   0.2525396 ]
 [246.16074521   0.25205252   0.36796975   0.22572453   0.13533942]][0m
[37m[1m[2023-07-10 09:41:12,889][227910] Max Reward on eval: 772.0437078228861[0m
[37m[1m[2023-07-10 09:41:12,889][227910] Min Reward on eval: 39.1136224191403[0m
[37m[1m[2023-07-10 09:41:12,889][227910] Mean Reward across all agents: 338.13797748319206[0m
[37m[1m[2023-07-10 09:41:12,890][227910] Average Trajectory Length: 499.8016666666667[0m
[36m[2023-07-10 09:41:12,892][227910] mean_value=-697.9709203845886, max_value=895.7749670910163[0m
[37m[1m[2023-07-10 09:41:12,895][227910] New mean coefficients: [[ 0.35398102  1.6488259   1.9683448  -0.76818496  3.7198398 ]][0m
[37m[1m[2023-07-10 09:41:12,896][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:41:22,161][227910] train() took 9.26 seconds to complete[0m
[36m[2023-07-10 09:41:22,162][227910] FPS: 414536.69[0m
[36m[2023-07-10 09:41:22,164][227910] itr=59, itrs=2000, Progress: 2.95%[0m
[36m[2023-07-10 09:41:34,873][227910] train() took 12.69 seconds to complete[0m
[36m[2023-07-10 09:41:34,873][227910] FPS: 302495.59[0m
[36m[2023-07-10 09:41:39,124][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:41:39,125][227910] Reward + Measures: [[472.44233005   0.21854553   0.40062165   0.18212651   0.23965293]][0m
[37m[1m[2023-07-10 09:41:39,125][227910] Max Reward on eval: 472.44233005050864[0m
[37m[1m[2023-07-10 09:41:39,125][227910] Min Reward on eval: 472.44233005050864[0m
[37m[1m[2023-07-10 09:41:39,125][227910] Mean Reward across all agents: 472.44233005050864[0m
[37m[1m[2023-07-10 09:41:39,126][227910] Average Trajectory Length: 663.2386666666666[0m
[36m[2023-07-10 09:41:44,247][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:41:44,252][227910] Reward + Measures: [[372.07228486   0.21886538   0.35895306   0.19228521   0.13153858]
 [478.98449402   0.23395996   0.29099956   0.1560798    0.15889484]
 [311.7556206    0.22671579   0.31085286   0.24080336   0.14592861]
 ...
 [166.49129952   0.20926192   0.38184717   0.20205601   0.16226631]
 [ 64.21194002   0.2462946    0.48713252   0.24483812   0.05365352]
 [278.87905544   0.25006434   0.36064225   0.22065273   0.24448434]][0m
[37m[1m[2023-07-10 09:41:44,252][227910] Max Reward on eval: 732.5755892886315[0m
[37m[1m[2023-07-10 09:41:44,253][227910] Min Reward on eval: 53.67653707149439[0m
[37m[1m[2023-07-10 09:41:44,253][227910] Mean Reward across all agents: 346.65557010624485[0m
[37m[1m[2023-07-10 09:41:44,253][227910] Average Trajectory Length: 545.6899999999999[0m
[36m[2023-07-10 09:41:44,256][227910] mean_value=-513.526799270966, max_value=826.0819201037287[0m
[37m[1m[2023-07-10 09:41:44,259][227910] New mean coefficients: [[ 1.2167035  1.5812705  1.8858272 -0.2721281  2.6372335]][0m
[37m[1m[2023-07-10 09:41:44,260][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:41:53,495][227910] train() took 9.23 seconds to complete[0m
[36m[2023-07-10 09:41:53,495][227910] FPS: 415908.48[0m
[36m[2023-07-10 09:41:53,498][227910] itr=60, itrs=2000, Progress: 3.00%[0m
[37m[1m[2023-07-10 09:41:55,145][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000040[0m
[36m[2023-07-10 09:42:08,222][227910] train() took 12.85 seconds to complete[0m
[36m[2023-07-10 09:42:08,223][227910] FPS: 298741.92[0m
[36m[2023-07-10 09:42:12,489][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:42:12,489][227910] Reward + Measures: [[541.9692729    0.21917069   0.41452515   0.19549426   0.26301908]][0m
[37m[1m[2023-07-10 09:42:12,489][227910] Max Reward on eval: 541.9692728957888[0m
[37m[1m[2023-07-10 09:42:12,490][227910] Min Reward on eval: 541.9692728957888[0m
[37m[1m[2023-07-10 09:42:12,490][227910] Mean Reward across all agents: 541.9692728957888[0m
[37m[1m[2023-07-10 09:42:12,490][227910] Average Trajectory Length: 681.7906666666667[0m
[36m[2023-07-10 09:42:17,598][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:42:17,599][227910] Reward + Measures: [[371.42500793   0.17863308   0.25538123   0.18055065   0.14017816]
 [492.37721315   0.23504806   0.4219856    0.19603659   0.21712637]
 [821.81165133   0.19813047   0.29363719   0.16698104   0.14654413]
 ...
 [302.96576662   0.2412058    0.42076549   0.21427529   0.0950623 ]
 [ 81.12445513   0.22902      0.51276052   0.21419182   0.01650678]
 [311.62590974   0.27168259   0.41840848   0.2128129    0.16467123]][0m
[37m[1m[2023-07-10 09:42:17,599][227910] Max Reward on eval: 838.3360703021287[0m
[37m[1m[2023-07-10 09:42:17,599][227910] Min Reward on eval: 46.99291342999786[0m
[37m[1m[2023-07-10 09:42:17,599][227910] Mean Reward across all agents: 404.1462922564974[0m
[37m[1m[2023-07-10 09:42:17,599][227910] Average Trajectory Length: 535.153[0m
[36m[2023-07-10 09:42:17,603][227910] mean_value=-589.6641140398546, max_value=1092.1669854210224[0m
[37m[1m[2023-07-10 09:42:17,605][227910] New mean coefficients: [[ 1.5460418  1.7836974  1.3621876 -0.1979522  1.7885851]][0m
[37m[1m[2023-07-10 09:42:17,606][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:42:26,766][227910] train() took 9.16 seconds to complete[0m
[36m[2023-07-10 09:42:26,766][227910] FPS: 419295.53[0m
[36m[2023-07-10 09:42:26,768][227910] itr=61, itrs=2000, Progress: 3.05%[0m
[36m[2023-07-10 09:42:39,396][227910] train() took 12.61 seconds to complete[0m
[36m[2023-07-10 09:42:39,397][227910] FPS: 304442.80[0m
[36m[2023-07-10 09:42:43,660][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:42:43,661][227910] Reward + Measures: [[727.19347597   0.222066     0.40313932   0.20785062   0.25186905]][0m
[37m[1m[2023-07-10 09:42:43,661][227910] Max Reward on eval: 727.1934759671699[0m
[37m[1m[2023-07-10 09:42:43,661][227910] Min Reward on eval: 727.1934759671699[0m
[37m[1m[2023-07-10 09:42:43,662][227910] Mean Reward across all agents: 727.1934759671699[0m
[37m[1m[2023-07-10 09:42:43,662][227910] Average Trajectory Length: 721.2883333333333[0m
[36m[2023-07-10 09:42:48,791][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:42:48,792][227910] Reward + Measures: [[450.88839577   0.29977804   0.48392421   0.14582859   0.33010218]
 [676.8229021    0.20677686   0.31113315   0.1749187    0.17692967]
 [713.03747886   0.22507857   0.40018511   0.19342586   0.19658008]
 ...
 [371.74364345   0.27848825   0.36177784   0.21178614   0.23025981]
 [266.94341076   0.20402947   0.32490307   0.21196508   0.12376306]
 [927.5637767    0.24386525   0.38553295   0.19174011   0.21806836]][0m
[37m[1m[2023-07-10 09:42:48,792][227910] Max Reward on eval: 1203.2245626508143[0m
[37m[1m[2023-07-10 09:42:48,792][227910] Min Reward on eval: 89.05689483452588[0m
[37m[1m[2023-07-10 09:42:48,792][227910] Mean Reward across all agents: 521.5449921110862[0m
[37m[1m[2023-07-10 09:42:48,793][227910] Average Trajectory Length: 647.5843333333333[0m
[36m[2023-07-10 09:42:48,797][227910] mean_value=-167.05397484566018, max_value=1113.9025937101455[0m
[37m[1m[2023-07-10 09:42:48,801][227910] New mean coefficients: [[ 1.0163563   2.363207    1.1762159  -0.48389536  0.96197426]][0m
[37m[1m[2023-07-10 09:42:48,802][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:42:58,037][227910] train() took 9.23 seconds to complete[0m
[36m[2023-07-10 09:42:58,037][227910] FPS: 415883.27[0m
[36m[2023-07-10 09:42:58,039][227910] itr=62, itrs=2000, Progress: 3.10%[0m
[36m[2023-07-10 09:43:10,573][227910] train() took 12.52 seconds to complete[0m
[36m[2023-07-10 09:43:10,574][227910] FPS: 306736.97[0m
[36m[2023-07-10 09:43:14,806][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:43:14,806][227910] Reward + Measures: [[1031.91058103    0.2453526     0.36670536    0.19980693    0.19051948]][0m
[37m[1m[2023-07-10 09:43:14,807][227910] Max Reward on eval: 1031.9105810292456[0m
[37m[1m[2023-07-10 09:43:14,807][227910] Min Reward on eval: 1031.9105810292456[0m
[37m[1m[2023-07-10 09:43:14,807][227910] Mean Reward across all agents: 1031.9105810292456[0m
[37m[1m[2023-07-10 09:43:14,807][227910] Average Trajectory Length: 744.9193333333333[0m
[36m[2023-07-10 09:43:19,961][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:43:19,967][227910] Reward + Measures: [[976.61219821   0.19164662   0.23146342   0.17897514   0.1280428 ]
 [728.17977748   0.25282645   0.29957649   0.18665972   0.13260953]
 [977.47253434   0.25360084   0.34348831   0.2167865    0.16374111]
 ...
 [709.77272625   0.24148421   0.37131742   0.19935031   0.19866557]
 [512.89840419   0.26199088   0.39655128   0.21815291   0.17231837]
 [584.8902018    0.20681846   0.28052187   0.16261353   0.12517437]][0m
[37m[1m[2023-07-10 09:43:19,967][227910] Max Reward on eval: 1439.3983673044247[0m
[37m[1m[2023-07-10 09:43:19,967][227910] Min Reward on eval: 198.47914275694637[0m
[37m[1m[2023-07-10 09:43:19,967][227910] Mean Reward across all agents: 758.8896227448893[0m
[37m[1m[2023-07-10 09:43:19,968][227910] Average Trajectory Length: 661.2923333333333[0m
[36m[2023-07-10 09:43:19,971][227910] mean_value=-494.3895563619737, max_value=1346.2534586673978[0m
[37m[1m[2023-07-10 09:43:19,975][227910] New mean coefficients: [[ 1.9071615   1.7181826   0.27824557 -0.22805724  0.704898  ]][0m
[37m[1m[2023-07-10 09:43:19,976][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:43:29,209][227910] train() took 9.23 seconds to complete[0m
[36m[2023-07-10 09:43:29,210][227910] FPS: 415938.68[0m
[36m[2023-07-10 09:43:29,212][227910] itr=63, itrs=2000, Progress: 3.15%[0m
[36m[2023-07-10 09:43:42,006][227910] train() took 12.78 seconds to complete[0m
[36m[2023-07-10 09:43:42,006][227910] FPS: 300497.95[0m
[36m[2023-07-10 09:43:46,264][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:43:46,265][227910] Reward + Measures: [[1337.60654751    0.25149739    0.33979657    0.21475166    0.14935917]][0m
[37m[1m[2023-07-10 09:43:46,265][227910] Max Reward on eval: 1337.6065475117734[0m
[37m[1m[2023-07-10 09:43:46,265][227910] Min Reward on eval: 1337.6065475117734[0m
[37m[1m[2023-07-10 09:43:46,265][227910] Mean Reward across all agents: 1337.6065475117734[0m
[37m[1m[2023-07-10 09:43:46,266][227910] Average Trajectory Length: 752.5643333333333[0m
[36m[2023-07-10 09:43:51,421][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:43:51,427][227910] Reward + Measures: [[1274.59414812    0.22991645    0.39442804    0.20418064    0.23360489]
 [1614.62652355    0.27384788    0.32406631    0.25083286    0.12730041]
 [1019.64836676    0.22475727    0.31164652    0.21953242    0.14178264]
 ...
 [ 728.1823369     0.2330178     0.35131997    0.2005754     0.15186945]
 [1520.80562565    0.23227552    0.32163268    0.2118796     0.17319389]
 [1723.27668365    0.26843533    0.33145007    0.23057599    0.12633707]][0m
[37m[1m[2023-07-10 09:43:51,427][227910] Max Reward on eval: 1761.0858539694805[0m
[37m[1m[2023-07-10 09:43:51,427][227910] Min Reward on eval: 308.71182038330005[0m
[37m[1m[2023-07-10 09:43:51,428][227910] Mean Reward across all agents: 888.9488587829793[0m
[37m[1m[2023-07-10 09:43:51,428][227910] Average Trajectory Length: 697.3593333333333[0m
[36m[2023-07-10 09:43:51,431][227910] mean_value=-471.5390392737809, max_value=663.0800656847534[0m
[37m[1m[2023-07-10 09:43:51,433][227910] New mean coefficients: [[ 1.3884313   1.8720471  -0.07302016 -0.22971064  0.90604514]][0m
[37m[1m[2023-07-10 09:43:51,434][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:44:00,670][227910] train() took 9.23 seconds to complete[0m
[36m[2023-07-10 09:44:00,671][227910] FPS: 415835.63[0m
[36m[2023-07-10 09:44:00,673][227910] itr=64, itrs=2000, Progress: 3.20%[0m
[36m[2023-07-10 09:44:19,937][227910] train() took 19.25 seconds to complete[0m
[36m[2023-07-10 09:44:19,938][227910] FPS: 199470.16[0m
[36m[2023-07-10 09:44:26,026][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:44:26,026][227910] Reward + Measures: [[1678.96798892    0.25499111    0.32135972    0.22849759    0.12106597]][0m
[37m[1m[2023-07-10 09:44:26,027][227910] Max Reward on eval: 1678.9679889167323[0m
[37m[1m[2023-07-10 09:44:26,027][227910] Min Reward on eval: 1678.9679889167323[0m
[37m[1m[2023-07-10 09:44:26,027][227910] Mean Reward across all agents: 1678.9679889167323[0m
[37m[1m[2023-07-10 09:44:26,027][227910] Average Trajectory Length: 800.4966666666667[0m
[36m[2023-07-10 09:44:33,073][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:44:33,074][227910] Reward + Measures: [[1108.89922668    0.27478737    0.36791253    0.23837058    0.13694848]
 [1229.96502374    0.25849351    0.38043699    0.22932224    0.16744067]
 [1438.39365002    0.20024268    0.24620152    0.20384768    0.09544938]
 ...
 [1012.11784089    0.21689479    0.23637055    0.2060349     0.09733997]
 [1255.29887198    0.26048622    0.33405319    0.21524127    0.1340297 ]
 [ 967.29529402    0.28841192    0.38071963    0.28198868    0.14614968]][0m
[37m[1m[2023-07-10 09:44:33,074][227910] Max Reward on eval: 1958.4109922922682[0m
[37m[1m[2023-07-10 09:44:33,074][227910] Min Reward on eval: 21.11918826550245[0m
[37m[1m[2023-07-10 09:44:33,075][227910] Mean Reward across all agents: 1217.628146620605[0m
[37m[1m[2023-07-10 09:44:33,075][227910] Average Trajectory Length: 708.6456666666667[0m
[36m[2023-07-10 09:44:33,082][227910] mean_value=-640.1173817375895, max_value=1039.1631311879728[0m
[37m[1m[2023-07-10 09:44:33,090][227910] New mean coefficients: [[0.7667558  1.4440182  0.12802574 0.00472455 0.90045947]][0m
[37m[1m[2023-07-10 09:44:33,093][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:44:42,422][227910] train() took 9.33 seconds to complete[0m
[36m[2023-07-10 09:44:42,422][227910] FPS: 411764.95[0m
[36m[2023-07-10 09:44:42,425][227910] itr=65, itrs=2000, Progress: 3.25%[0m
[36m[2023-07-10 09:44:54,373][227910] train() took 11.93 seconds to complete[0m
[36m[2023-07-10 09:44:54,373][227910] FPS: 321786.20[0m
[36m[2023-07-10 09:44:58,383][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:44:58,383][227910] Reward + Measures: [[1987.56819227    0.25537276    0.31367874    0.24091642    0.10469578]][0m
[37m[1m[2023-07-10 09:44:58,383][227910] Max Reward on eval: 1987.5681922716262[0m
[37m[1m[2023-07-10 09:44:58,383][227910] Min Reward on eval: 1987.5681922716262[0m
[37m[1m[2023-07-10 09:44:58,384][227910] Mean Reward across all agents: 1987.5681922716262[0m
[37m[1m[2023-07-10 09:44:58,384][227910] Average Trajectory Length: 828.0523333333333[0m
[36m[2023-07-10 09:45:03,183][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:45:03,183][227910] Reward + Measures: [[ 864.57953369    0.25635487    0.31537306    0.26299357    0.14276831]
 [1580.20897673    0.23345517    0.27517262    0.18913603    0.09186546]
 [ 815.20529556    0.27625871    0.35817382    0.21312955    0.14290154]
 ...
 [ 641.2833817     0.2412592     0.35359165    0.19668393    0.14223903]
 [ 767.79294217    0.24212901    0.32679969    0.18944235    0.13787548]
 [1467.74612699    0.2429854     0.31132045    0.24235635    0.08668383]][0m
[37m[1m[2023-07-10 09:45:03,184][227910] Max Reward on eval: 2210.4974524232794[0m
[37m[1m[2023-07-10 09:45:03,184][227910] Min Reward on eval: 358.844470785663[0m
[37m[1m[2023-07-10 09:45:03,184][227910] Mean Reward across all agents: 1305.1235105546086[0m
[37m[1m[2023-07-10 09:45:03,184][227910] Average Trajectory Length: 697.5256666666667[0m
[36m[2023-07-10 09:45:03,187][227910] mean_value=-692.2641459018424, max_value=1104.937152975604[0m
[37m[1m[2023-07-10 09:45:03,189][227910] New mean coefficients: [[ 0.19755971  1.152856    0.5698998  -0.0899913   1.0682718 ]][0m
[37m[1m[2023-07-10 09:45:03,190][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:45:12,161][227910] train() took 8.97 seconds to complete[0m
[36m[2023-07-10 09:45:12,162][227910] FPS: 428132.28[0m
[36m[2023-07-10 09:45:12,164][227910] itr=66, itrs=2000, Progress: 3.30%[0m
[36m[2023-07-10 09:45:24,586][227910] train() took 12.40 seconds to complete[0m
[36m[2023-07-10 09:45:24,586][227910] FPS: 309560.68[0m
[36m[2023-07-10 09:45:28,582][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:45:28,583][227910] Reward + Measures: [[2161.1977965     0.26034003    0.31707597    0.25057715    0.09696301]][0m
[37m[1m[2023-07-10 09:45:28,583][227910] Max Reward on eval: 2161.1977965004103[0m
[37m[1m[2023-07-10 09:45:28,583][227910] Min Reward on eval: 2161.1977965004103[0m
[37m[1m[2023-07-10 09:45:28,584][227910] Mean Reward across all agents: 2161.1977965004103[0m
[37m[1m[2023-07-10 09:45:28,584][227910] Average Trajectory Length: 849.612[0m
[36m[2023-07-10 09:45:33,340][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:45:33,341][227910] Reward + Measures: [[1495.83686169    0.26192996    0.32144937    0.22031572    0.12605982]
 [1254.94181663    0.22773361    0.31802303    0.16124426    0.16849379]
 [2670.44832476    0.2538        0.29980001    0.26830003    0.0857    ]
 ...
 [1554.0606186     0.27948746    0.33667871    0.25409779    0.09441004]
 [ 918.37441403    0.24133591    0.3722457     0.21786574    0.16465542]
 [1900.76215399    0.23266558    0.30715251    0.25422737    0.07756406]][0m
[37m[1m[2023-07-10 09:45:33,341][227910] Max Reward on eval: 2670.4483247551134[0m
[37m[1m[2023-07-10 09:45:33,341][227910] Min Reward on eval: 247.7520659230766[0m
[37m[1m[2023-07-10 09:45:33,342][227910] Mean Reward across all agents: 1469.1020171983125[0m
[37m[1m[2023-07-10 09:45:33,342][227910] Average Trajectory Length: 734.2813333333334[0m
[36m[2023-07-10 09:45:33,344][227910] mean_value=-567.5822650444892, max_value=1216.806488090881[0m
[37m[1m[2023-07-10 09:45:33,347][227910] New mean coefficients: [[0.10755745 1.3047895  0.32526612 0.03484666 0.6393814 ]][0m
[37m[1m[2023-07-10 09:45:33,348][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:45:42,072][227910] train() took 8.72 seconds to complete[0m
[36m[2023-07-10 09:45:42,072][227910] FPS: 440229.73[0m
[36m[2023-07-10 09:45:42,075][227910] itr=67, itrs=2000, Progress: 3.35%[0m
[36m[2023-07-10 09:45:54,237][227910] train() took 12.15 seconds to complete[0m
[36m[2023-07-10 09:45:54,237][227910] FPS: 316058.60[0m
[36m[2023-07-10 09:45:58,230][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:45:58,230][227910] Reward + Measures: [[2244.33910128    0.26653728    0.32389992    0.24736558    0.10283822]][0m
[37m[1m[2023-07-10 09:45:58,230][227910] Max Reward on eval: 2244.339101281593[0m
[37m[1m[2023-07-10 09:45:58,230][227910] Min Reward on eval: 2244.339101281593[0m
[37m[1m[2023-07-10 09:45:58,231][227910] Mean Reward across all agents: 2244.339101281593[0m
[37m[1m[2023-07-10 09:45:58,231][227910] Average Trajectory Length: 870.3536666666666[0m
[36m[2023-07-10 09:46:03,161][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:46:03,162][227910] Reward + Measures: [[ 868.97715666    0.28703547    0.37813446    0.17502861    0.14038804]
 [1395.54784325    0.26907593    0.34054169    0.18168139    0.13086149]
 [1001.36327368    0.26629886    0.30032411    0.1894841     0.11487327]
 ...
 [1419.61779513    0.26040745    0.35447401    0.26950964    0.08725113]
 [ 583.53156491    0.23461309    0.33817431    0.17930445    0.12217517]
 [2100.06585365    0.2852475     0.33584699    0.23308519    0.1185562 ]][0m
[37m[1m[2023-07-10 09:46:03,162][227910] Max Reward on eval: 2470.3033249145838[0m
[37m[1m[2023-07-10 09:46:03,162][227910] Min Reward on eval: 335.7789534687996[0m
[37m[1m[2023-07-10 09:46:03,163][227910] Mean Reward across all agents: 1473.6227777157317[0m
[37m[1m[2023-07-10 09:46:03,163][227910] Average Trajectory Length: 739.6563333333334[0m
[36m[2023-07-10 09:46:03,165][227910] mean_value=-491.4728606138712, max_value=966.4102714231569[0m
[37m[1m[2023-07-10 09:46:03,168][227910] New mean coefficients: [[ 0.68237233  1.8522375  -0.33472073  0.02166257  0.9719647 ]][0m
[37m[1m[2023-07-10 09:46:03,169][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:46:11,960][227910] train() took 8.79 seconds to complete[0m
[36m[2023-07-10 09:46:11,960][227910] FPS: 436890.95[0m
[36m[2023-07-10 09:46:11,963][227910] itr=68, itrs=2000, Progress: 3.40%[0m
[36m[2023-07-10 09:46:24,189][227910] train() took 12.21 seconds to complete[0m
[36m[2023-07-10 09:46:24,189][227910] FPS: 314471.81[0m
[36m[2023-07-10 09:46:28,147][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:46:28,147][227910] Reward + Measures: [[2426.94174856    0.26281008    0.31303403    0.25248802    0.09484093]][0m
[37m[1m[2023-07-10 09:46:28,147][227910] Max Reward on eval: 2426.9417485552726[0m
[37m[1m[2023-07-10 09:46:28,148][227910] Min Reward on eval: 2426.9417485552726[0m
[37m[1m[2023-07-10 09:46:28,148][227910] Mean Reward across all agents: 2426.9417485552726[0m
[37m[1m[2023-07-10 09:46:28,148][227910] Average Trajectory Length: 874.8046666666667[0m
[36m[2023-07-10 09:46:32,850][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:46:32,851][227910] Reward + Measures: [[1430.07796439    0.24868713    0.30567014    0.23139957    0.10822996]
 [1917.08697582    0.23575857    0.28445628    0.23704183    0.11345027]
 [1443.18720859    0.27943686    0.34813118    0.19633386    0.14707299]
 ...
 [2305.13924616    0.25716361    0.3085272     0.24155703    0.09694903]
 [1682.29165629    0.25653234    0.37553817    0.24611549    0.15285902]
 [2072.67047808    0.274014      0.34654519    0.25409684    0.10080601]][0m
[37m[1m[2023-07-10 09:46:32,851][227910] Max Reward on eval: 2807.6528093737084[0m
[37m[1m[2023-07-10 09:46:32,852][227910] Min Reward on eval: 201.24993849794845[0m
[37m[1m[2023-07-10 09:46:32,852][227910] Mean Reward across all agents: 1495.1715148994547[0m
[37m[1m[2023-07-10 09:46:32,852][227910] Average Trajectory Length: 721.838[0m
[36m[2023-07-10 09:46:32,854][227910] mean_value=-655.1918128156458, max_value=987.1124289603472[0m
[37m[1m[2023-07-10 09:46:32,857][227910] New mean coefficients: [[ 0.0474388   1.1211255  -0.48315394 -0.31115028  0.5634315 ]][0m
[37m[1m[2023-07-10 09:46:32,858][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:46:41,640][227910] train() took 8.78 seconds to complete[0m
[36m[2023-07-10 09:46:41,640][227910] FPS: 437341.75[0m
[36m[2023-07-10 09:46:41,642][227910] itr=69, itrs=2000, Progress: 3.45%[0m
[36m[2023-07-10 09:46:53,823][227910] train() took 12.16 seconds to complete[0m
[36m[2023-07-10 09:46:53,824][227910] FPS: 315770.58[0m
[36m[2023-07-10 09:46:57,867][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:46:57,872][227910] Reward + Measures: [[2517.26536397    0.27097425    0.31007352    0.25401914    0.09634483]][0m
[37m[1m[2023-07-10 09:46:57,872][227910] Max Reward on eval: 2517.2653639741875[0m
[37m[1m[2023-07-10 09:46:57,873][227910] Min Reward on eval: 2517.2653639741875[0m
[37m[1m[2023-07-10 09:46:57,873][227910] Mean Reward across all agents: 2517.2653639741875[0m
[37m[1m[2023-07-10 09:46:57,873][227910] Average Trajectory Length: 898.1643333333333[0m
[36m[2023-07-10 09:47:02,744][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:47:02,744][227910] Reward + Measures: [[1702.51632122    0.25911781    0.30922899    0.24587403    0.11730336]
 [1181.81699194    0.26043484    0.34678274    0.22405501    0.12138804]
 [1276.23032676    0.24432746    0.34830672    0.22725146    0.09215011]
 ...
 [1041.69369801    0.27049252    0.32528666    0.23781121    0.11257609]
 [1187.93093288    0.25136739    0.3387652     0.21404123    0.12770545]
 [1664.29975392    0.26324666    0.33350322    0.2691085     0.10552291]][0m
[37m[1m[2023-07-10 09:47:02,745][227910] Max Reward on eval: 2800.5150809515735[0m
[37m[1m[2023-07-10 09:47:02,745][227910] Min Reward on eval: 426.91361832632685[0m
[37m[1m[2023-07-10 09:47:02,745][227910] Mean Reward across all agents: 1863.0904324335497[0m
[37m[1m[2023-07-10 09:47:02,745][227910] Average Trajectory Length: 797.4006666666667[0m
[36m[2023-07-10 09:47:02,748][227910] mean_value=-475.48634191584716, max_value=1396.5220944370246[0m
[37m[1m[2023-07-10 09:47:02,750][227910] New mean coefficients: [[ 0.18503961  0.8699559  -0.92968833 -0.49796423  0.4936477 ]][0m
[37m[1m[2023-07-10 09:47:02,751][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:47:11,492][227910] train() took 8.74 seconds to complete[0m
[36m[2023-07-10 09:47:11,493][227910] FPS: 439381.85[0m
[36m[2023-07-10 09:47:11,495][227910] itr=70, itrs=2000, Progress: 3.50%[0m
[37m[1m[2023-07-10 09:47:13,306][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000050[0m
[36m[2023-07-10 09:47:25,884][227910] train() took 12.32 seconds to complete[0m
[36m[2023-07-10 09:47:25,884][227910] FPS: 311791.22[0m
[36m[2023-07-10 09:47:29,848][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:47:29,849][227910] Reward + Measures: [[2598.41784013    0.27431342    0.29788017    0.24923858    0.09156072]][0m
[37m[1m[2023-07-10 09:47:29,849][227910] Max Reward on eval: 2598.417840133804[0m
[37m[1m[2023-07-10 09:47:29,849][227910] Min Reward on eval: 2598.417840133804[0m
[37m[1m[2023-07-10 09:47:29,850][227910] Mean Reward across all agents: 2598.417840133804[0m
[37m[1m[2023-07-10 09:47:29,850][227910] Average Trajectory Length: 893.073[0m
[36m[2023-07-10 09:47:34,715][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:47:34,716][227910] Reward + Measures: [[2279.70318304    0.29046121    0.31838164    0.25011021    0.11468571]
 [2140.66893771    0.26640841    0.30016276    0.23712531    0.0989121 ]
 [1695.36958078    0.29706565    0.34267458    0.22599323    0.12689623]
 ...
 [1877.93478357    0.25862837    0.33981341    0.21911846    0.08642259]
 [2190.99907353    0.24524336    0.277962      0.2481939     0.07905987]
 [2583.63512422    0.25347897    0.29367897    0.25197569    0.08586036]][0m
[37m[1m[2023-07-10 09:47:34,716][227910] Max Reward on eval: 2950.4423118250443[0m
[37m[1m[2023-07-10 09:47:34,716][227910] Min Reward on eval: 555.5966722646729[0m
[37m[1m[2023-07-10 09:47:34,716][227910] Mean Reward across all agents: 2098.877477009984[0m
[37m[1m[2023-07-10 09:47:34,716][227910] Average Trajectory Length: 828.4309999999999[0m
[36m[2023-07-10 09:47:34,719][227910] mean_value=-408.77478359324084, max_value=2312.3591566857426[0m
[37m[1m[2023-07-10 09:47:34,722][227910] New mean coefficients: [[ 0.0970742   1.0992765  -0.32256234 -0.624786    0.160941  ]][0m
[37m[1m[2023-07-10 09:47:34,723][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:47:43,656][227910] train() took 8.93 seconds to complete[0m
[36m[2023-07-10 09:47:43,656][227910] FPS: 429949.83[0m
[36m[2023-07-10 09:47:43,658][227910] itr=71, itrs=2000, Progress: 3.55%[0m
[36m[2023-07-10 09:47:55,928][227910] train() took 12.25 seconds to complete[0m
[36m[2023-07-10 09:47:55,928][227910] FPS: 313408.99[0m
[36m[2023-07-10 09:47:59,960][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:47:59,966][227910] Reward + Measures: [[2567.02904527    0.28140491    0.29526564    0.23148799    0.09277294]][0m
[37m[1m[2023-07-10 09:47:59,966][227910] Max Reward on eval: 2567.0290452656454[0m
[37m[1m[2023-07-10 09:47:59,966][227910] Min Reward on eval: 2567.0290452656454[0m
[37m[1m[2023-07-10 09:47:59,966][227910] Mean Reward across all agents: 2567.0290452656454[0m
[37m[1m[2023-07-10 09:47:59,967][227910] Average Trajectory Length: 883.1536666666666[0m
[36m[2023-07-10 09:48:04,798][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:48:04,799][227910] Reward + Measures: [[1329.78256407    0.22404385    0.36666602    0.19347195    0.23047583]
 [2268.25333759    0.24838237    0.27830774    0.21650413    0.0819011 ]
 [2327.80175805    0.22998428    0.27587882    0.22459231    0.07646253]
 ...
 [1897.28562431    0.26353562    0.29566017    0.22605829    0.10018795]
 [2365.27293265    0.27920732    0.31092969    0.2529113     0.07267966]
 [2057.88243885    0.28894165    0.29620501    0.19607805    0.10346854]][0m
[37m[1m[2023-07-10 09:48:04,799][227910] Max Reward on eval: 3059.7914794475773[0m
[37m[1m[2023-07-10 09:48:04,800][227910] Min Reward on eval: 649.4029424315552[0m
[37m[1m[2023-07-10 09:48:04,800][227910] Mean Reward across all agents: 1948.1804371678961[0m
[37m[1m[2023-07-10 09:48:04,800][227910] Average Trajectory Length: 790.4196666666667[0m
[36m[2023-07-10 09:48:04,803][227910] mean_value=-464.5762653710843, max_value=1867.8558535514615[0m
[37m[1m[2023-07-10 09:48:04,806][227910] New mean coefficients: [[ 0.35580108  1.1703434   0.02893814 -1.2493446   0.03527506]][0m
[37m[1m[2023-07-10 09:48:04,807][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:48:13,702][227910] train() took 8.89 seconds to complete[0m
[36m[2023-07-10 09:48:13,702][227910] FPS: 431781.30[0m
[36m[2023-07-10 09:48:13,715][227910] itr=72, itrs=2000, Progress: 3.60%[0m
[36m[2023-07-10 09:48:25,912][227910] train() took 12.18 seconds to complete[0m
[36m[2023-07-10 09:48:25,912][227910] FPS: 315178.04[0m
[36m[2023-07-10 09:48:29,832][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:48:29,832][227910] Reward + Measures: [[2716.11284264    0.28159887    0.29423121    0.23235266    0.08820062]][0m
[37m[1m[2023-07-10 09:48:29,833][227910] Max Reward on eval: 2716.1128426383466[0m
[37m[1m[2023-07-10 09:48:29,833][227910] Min Reward on eval: 2716.1128426383466[0m
[37m[1m[2023-07-10 09:48:29,833][227910] Mean Reward across all agents: 2716.1128426383466[0m
[37m[1m[2023-07-10 09:48:29,834][227910] Average Trajectory Length: 887.3726666666666[0m
[36m[2023-07-10 09:48:34,855][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:48:34,856][227910] Reward + Measures: [[1550.36840314    0.27056655    0.30491611    0.19292419    0.10301355]
 [2739.31955673    0.2603938     0.29691991    0.22395675    0.09857944]
 [2845.28835714    0.28482133    0.29855844    0.23684628    0.08411549]
 ...
 [1961.52959641    0.27710673    0.275866      0.24657755    0.09171097]
 [1412.59461012    0.22244196    0.231405      0.1770106     0.08128218]
 [1778.54481781    0.27779737    0.27054012    0.20696135    0.11141504]][0m
[37m[1m[2023-07-10 09:48:34,856][227910] Max Reward on eval: 3187.334219092806[0m
[37m[1m[2023-07-10 09:48:34,856][227910] Min Reward on eval: 717.9909526350384[0m
[37m[1m[2023-07-10 09:48:34,857][227910] Mean Reward across all agents: 2143.193911939777[0m
[37m[1m[2023-07-10 09:48:34,857][227910] Average Trajectory Length: 803.3256666666666[0m
[36m[2023-07-10 09:48:34,859][227910] mean_value=-506.33165706555155, max_value=2298.218336612063[0m
[37m[1m[2023-07-10 09:48:34,861][227910] New mean coefficients: [[ 0.25463     1.3365045  -0.24497429 -1.0071859   0.39048958]][0m
[37m[1m[2023-07-10 09:48:34,862][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:48:43,488][227910] train() took 8.62 seconds to complete[0m
[36m[2023-07-10 09:48:43,488][227910] FPS: 445288.34[0m
[36m[2023-07-10 09:48:43,490][227910] itr=73, itrs=2000, Progress: 3.65%[0m
[36m[2023-07-10 09:48:55,839][227910] train() took 12.33 seconds to complete[0m
[36m[2023-07-10 09:48:55,839][227910] FPS: 311331.23[0m
[36m[2023-07-10 09:48:59,894][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:48:59,899][227910] Reward + Measures: [[2761.31279604    0.28476566    0.29093528    0.23089026    0.08467866]][0m
[37m[1m[2023-07-10 09:48:59,899][227910] Max Reward on eval: 2761.3127960433403[0m
[37m[1m[2023-07-10 09:48:59,899][227910] Min Reward on eval: 2761.3127960433403[0m
[37m[1m[2023-07-10 09:48:59,900][227910] Mean Reward across all agents: 2761.3127960433403[0m
[37m[1m[2023-07-10 09:48:59,900][227910] Average Trajectory Length: 873.1596666666667[0m
[36m[2023-07-10 09:49:04,690][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:49:04,691][227910] Reward + Measures: [[2180.24508485    0.28001297    0.25760746    0.19769947    0.09841479]
 [2792.63656213    0.29335943    0.3332448     0.22009312    0.08134299]
 [2952.06565563    0.28894839    0.29699826    0.2309548     0.09103872]
 ...
 [2829.56704854    0.29412982    0.27688119    0.22237387    0.09122451]
 [2588.84410445    0.28582639    0.3039071     0.24524067    0.07837745]
 [2447.85695638    0.2688154     0.29625458    0.23991226    0.08598302]][0m
[37m[1m[2023-07-10 09:49:04,691][227910] Max Reward on eval: 3211.711252788152[0m
[37m[1m[2023-07-10 09:49:04,691][227910] Min Reward on eval: 418.0138106842758[0m
[37m[1m[2023-07-10 09:49:04,692][227910] Mean Reward across all agents: 2144.8839807092445[0m
[37m[1m[2023-07-10 09:49:04,692][227910] Average Trajectory Length: 779.6993333333334[0m
[36m[2023-07-10 09:49:04,694][227910] mean_value=-536.8696472391192, max_value=3309.491344016964[0m
[37m[1m[2023-07-10 09:49:04,697][227910] New mean coefficients: [[ 0.5752677   1.4297892  -0.45927918 -0.5489866   0.6823849 ]][0m
[37m[1m[2023-07-10 09:49:04,698][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:49:13,490][227910] train() took 8.79 seconds to complete[0m
[36m[2023-07-10 09:49:13,490][227910] FPS: 436827.88[0m
[36m[2023-07-10 09:49:13,493][227910] itr=74, itrs=2000, Progress: 3.70%[0m
[36m[2023-07-10 09:49:25,741][227910] train() took 12.23 seconds to complete[0m
[36m[2023-07-10 09:49:25,741][227910] FPS: 314008.34[0m
[36m[2023-07-10 09:49:29,693][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:49:29,694][227910] Reward + Measures: [[2897.46374259    0.28152913    0.28378248    0.23399165    0.07865302]][0m
[37m[1m[2023-07-10 09:49:29,694][227910] Max Reward on eval: 2897.4637425901383[0m
[37m[1m[2023-07-10 09:49:29,694][227910] Min Reward on eval: 2897.4637425901383[0m
[37m[1m[2023-07-10 09:49:29,694][227910] Mean Reward across all agents: 2897.4637425901383[0m
[37m[1m[2023-07-10 09:49:29,694][227910] Average Trajectory Length: 881.1279999999999[0m
[36m[2023-07-10 09:49:34,360][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:49:34,361][227910] Reward + Measures: [[2366.49212597    0.23451285    0.24718194    0.20417471    0.08367132]
 [1907.84892366    0.22763033    0.26560044    0.21813495    0.07171603]
 [2186.6130237     0.26259601    0.28217718    0.24095888    0.08011539]
 ...
 [2490.36487104    0.26681346    0.26658359    0.23246245    0.05740898]
 [2766.32292597    0.30735555    0.29012004    0.23646522    0.06513975]
 [2290.12414039    0.30175504    0.30084348    0.22165631    0.07012711]][0m
[37m[1m[2023-07-10 09:49:34,361][227910] Max Reward on eval: 3369.69014140293[0m
[37m[1m[2023-07-10 09:49:34,361][227910] Min Reward on eval: 588.3899153779727[0m
[37m[1m[2023-07-10 09:49:34,362][227910] Mean Reward across all agents: 2276.3520870308253[0m
[37m[1m[2023-07-10 09:49:34,362][227910] Average Trajectory Length: 807.4183333333333[0m
[36m[2023-07-10 09:49:34,364][227910] mean_value=-444.0418169571241, max_value=2331.257304556652[0m
[37m[1m[2023-07-10 09:49:34,367][227910] New mean coefficients: [[ 0.67092025  1.7901267  -0.42779925 -0.5401126   0.6877729 ]][0m
[37m[1m[2023-07-10 09:49:34,368][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:49:43,289][227910] train() took 8.92 seconds to complete[0m
[36m[2023-07-10 09:49:43,290][227910] FPS: 430508.78[0m
[36m[2023-07-10 09:49:43,302][227910] itr=75, itrs=2000, Progress: 3.75%[0m
[36m[2023-07-10 09:49:55,620][227910] train() took 12.30 seconds to complete[0m
[36m[2023-07-10 09:49:55,621][227910] FPS: 312260.82[0m
[36m[2023-07-10 09:49:59,581][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:49:59,581][227910] Reward + Measures: [[2938.58562802    0.27973363    0.28009474    0.23334631    0.07512914]][0m
[37m[1m[2023-07-10 09:49:59,582][227910] Max Reward on eval: 2938.5856280181333[0m
[37m[1m[2023-07-10 09:49:59,582][227910] Min Reward on eval: 2938.5856280181333[0m
[37m[1m[2023-07-10 09:49:59,582][227910] Mean Reward across all agents: 2938.5856280181333[0m
[37m[1m[2023-07-10 09:49:59,582][227910] Average Trajectory Length: 861.7823333333333[0m
[36m[2023-07-10 09:50:04,410][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:50:04,411][227910] Reward + Measures: [[ 974.12981895    0.25221953    0.29487568    0.1976869     0.09965401]
 [1034.48659923    0.28265443    0.27893391    0.18793464    0.10296356]
 [2966.25989446    0.25799093    0.25914547    0.23126666    0.08255758]
 ...
 [2686.9909867     0.29783592    0.31246156    0.25292048    0.09750257]
 [1263.99566524    0.30012983    0.32220241    0.17837338    0.13916945]
 [2247.4016708     0.26275611    0.30170646    0.25526851    0.10171994]][0m
[37m[1m[2023-07-10 09:50:04,411][227910] Max Reward on eval: 3496.251043563569[0m
[37m[1m[2023-07-10 09:50:04,411][227910] Min Reward on eval: 408.5519096622244[0m
[37m[1m[2023-07-10 09:50:04,411][227910] Mean Reward across all agents: 2041.4894951310384[0m
[37m[1m[2023-07-10 09:50:04,411][227910] Average Trajectory Length: 726.3903333333333[0m
[36m[2023-07-10 09:50:04,414][227910] mean_value=-693.1911895912326, max_value=2275.7327448556102[0m
[37m[1m[2023-07-10 09:50:04,417][227910] New mean coefficients: [[ 0.74331725  1.5006138  -0.0565449  -0.83223736  0.27685606]][0m
[37m[1m[2023-07-10 09:50:04,418][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:50:13,303][227910] train() took 8.88 seconds to complete[0m
[36m[2023-07-10 09:50:13,303][227910] FPS: 432260.95[0m
[36m[2023-07-10 09:50:13,316][227910] itr=76, itrs=2000, Progress: 3.80%[0m
[36m[2023-07-10 09:50:25,696][227910] train() took 12.36 seconds to complete[0m
[36m[2023-07-10 09:50:25,697][227910] FPS: 310697.75[0m
[36m[2023-07-10 09:50:29,580][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:50:29,585][227910] Reward + Measures: [[3113.50403959    0.27379179    0.27476183    0.23839408    0.06852639]][0m
[37m[1m[2023-07-10 09:50:29,585][227910] Max Reward on eval: 3113.504039592494[0m
[37m[1m[2023-07-10 09:50:29,585][227910] Min Reward on eval: 3113.504039592494[0m
[37m[1m[2023-07-10 09:50:29,585][227910] Mean Reward across all agents: 3113.504039592494[0m
[37m[1m[2023-07-10 09:50:29,585][227910] Average Trajectory Length: 877.4889999999999[0m
[36m[2023-07-10 09:50:34,299][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:50:34,300][227910] Reward + Measures: [[1413.0471324     0.27257848    0.25419506    0.19654469    0.0812896 ]
 [1236.27234273    0.22367088    0.25567183    0.1939014     0.09002084]
 [2390.17716886    0.28511477    0.29838568    0.23971668    0.08702099]
 ...
 [2158.22020483    0.26195866    0.26574221    0.2096076     0.09226551]
 [2679.17682616    0.28746587    0.29634169    0.24146307    0.07047554]
 [1852.11915467    0.22175303    0.26679692    0.18059564    0.08398791]][0m
[37m[1m[2023-07-10 09:50:34,300][227910] Max Reward on eval: 3675.6853451480156[0m
[37m[1m[2023-07-10 09:50:34,300][227910] Min Reward on eval: 560.9630600616917[0m
[37m[1m[2023-07-10 09:50:34,301][227910] Mean Reward across all agents: 2317.015622515762[0m
[37m[1m[2023-07-10 09:50:34,301][227910] Average Trajectory Length: 779.117[0m
[36m[2023-07-10 09:50:34,303][227910] mean_value=-650.676271916137, max_value=2212.857878348688[0m
[37m[1m[2023-07-10 09:50:34,306][227910] New mean coefficients: [[ 0.25008333  1.0111498   0.1183711  -0.62984437  0.19370782]][0m
[37m[1m[2023-07-10 09:50:34,307][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:50:42,909][227910] train() took 8.60 seconds to complete[0m
[36m[2023-07-10 09:50:42,909][227910] FPS: 446475.60[0m
[36m[2023-07-10 09:50:42,912][227910] itr=77, itrs=2000, Progress: 3.85%[0m
[36m[2023-07-10 09:50:54,755][227910] train() took 11.83 seconds to complete[0m
[36m[2023-07-10 09:50:54,756][227910] FPS: 324608.69[0m
[36m[2023-07-10 09:50:58,685][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:50:58,691][227910] Reward + Measures: [[3162.50196866    0.2752105     0.28087723    0.2365289     0.06897029]][0m
[37m[1m[2023-07-10 09:50:58,691][227910] Max Reward on eval: 3162.5019686648106[0m
[37m[1m[2023-07-10 09:50:58,691][227910] Min Reward on eval: 3162.5019686648106[0m
[37m[1m[2023-07-10 09:50:58,691][227910] Mean Reward across all agents: 3162.5019686648106[0m
[37m[1m[2023-07-10 09:50:58,692][227910] Average Trajectory Length: 882.9143333333333[0m
[36m[2023-07-10 09:51:03,584][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:51:03,584][227910] Reward + Measures: [[2366.53435275    0.25621048    0.28543445    0.23921728    0.06517854]
 [1792.96270331    0.25746173    0.26520932    0.22983392    0.07116961]
 [2557.76185364    0.2461807     0.2580353     0.22331829    0.06595271]
 ...
 [2871.91836823    0.2825931     0.29934964    0.23651318    0.07015719]
 [2647.09644135    0.29883593    0.34255961    0.25616917    0.1288992 ]
 [2524.67257983    0.30229947    0.30331713    0.22885941    0.07445785]][0m
[37m[1m[2023-07-10 09:51:03,584][227910] Max Reward on eval: 3543.64355161041[0m
[37m[1m[2023-07-10 09:51:03,585][227910] Min Reward on eval: 1058.127341386117[0m
[37m[1m[2023-07-10 09:51:03,585][227910] Mean Reward across all agents: 2697.100875851189[0m
[37m[1m[2023-07-10 09:51:03,585][227910] Average Trajectory Length: 826.0756666666666[0m
[36m[2023-07-10 09:51:03,587][227910] mean_value=-476.3522644892816, max_value=1785.620619638204[0m
[37m[1m[2023-07-10 09:51:03,589][227910] New mean coefficients: [[-0.10675809  1.0124129   0.17105144 -0.6903679   0.69066846]][0m
[37m[1m[2023-07-10 09:51:03,590][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:51:12,277][227910] train() took 8.68 seconds to complete[0m
[36m[2023-07-10 09:51:12,277][227910] FPS: 442149.55[0m
[36m[2023-07-10 09:51:12,280][227910] itr=78, itrs=2000, Progress: 3.90%[0m
[36m[2023-07-10 09:51:24,387][227910] train() took 12.09 seconds to complete[0m
[36m[2023-07-10 09:51:24,387][227910] FPS: 317551.58[0m
[36m[2023-07-10 09:51:28,312][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:51:28,317][227910] Reward + Measures: [[3088.29331518    0.28825581    0.28926086    0.22404134    0.07817624]][0m
[37m[1m[2023-07-10 09:51:28,317][227910] Max Reward on eval: 3088.2933151795332[0m
[37m[1m[2023-07-10 09:51:28,317][227910] Min Reward on eval: 3088.2933151795332[0m
[37m[1m[2023-07-10 09:51:28,318][227910] Mean Reward across all agents: 3088.2933151795332[0m
[37m[1m[2023-07-10 09:51:28,318][227910] Average Trajectory Length: 905.8026666666666[0m
[36m[2023-07-10 09:51:33,062][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:51:33,062][227910] Reward + Measures: [[2941.64851486    0.28007784    0.30020091    0.23426378    0.07804727]
 [3256.94916437    0.30738166    0.3163062     0.20877698    0.08980266]
 [2946.04256607    0.29590145    0.27347961    0.2336248     0.07010243]
 ...
 [2405.89337682    0.24427532    0.26253313    0.19395828    0.06018201]
 [2523.6694132     0.28585711    0.321839      0.2170109     0.1059233 ]
 [3176.05672952    0.26159999    0.26290002    0.2172        0.0583    ]][0m
[37m[1m[2023-07-10 09:51:33,063][227910] Max Reward on eval: 3430.8412621901252[0m
[37m[1m[2023-07-10 09:51:33,063][227910] Min Reward on eval: 1103.538789655629[0m
[37m[1m[2023-07-10 09:51:33,063][227910] Mean Reward across all agents: 2589.1676570035884[0m
[37m[1m[2023-07-10 09:51:33,063][227910] Average Trajectory Length: 832.078[0m
[36m[2023-07-10 09:51:33,065][227910] mean_value=-566.7669322508764, max_value=2090.8795873927993[0m
[37m[1m[2023-07-10 09:51:33,068][227910] New mean coefficients: [[-0.21488547  0.26312077  0.4914054  -0.28530866  0.0428682 ]][0m
[37m[1m[2023-07-10 09:51:33,069][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:51:41,734][227910] train() took 8.66 seconds to complete[0m
[36m[2023-07-10 09:51:41,734][227910] FPS: 443247.65[0m
[36m[2023-07-10 09:51:41,737][227910] itr=79, itrs=2000, Progress: 3.95%[0m
[36m[2023-07-10 09:51:53,621][227910] train() took 11.87 seconds to complete[0m
[36m[2023-07-10 09:51:53,621][227910] FPS: 323462.89[0m
[36m[2023-07-10 09:51:57,438][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:51:57,438][227910] Reward + Measures: [[2867.76951251    0.29888359    0.30889043    0.21165214    0.09428155]][0m
[37m[1m[2023-07-10 09:51:57,438][227910] Max Reward on eval: 2867.7695125120463[0m
[37m[1m[2023-07-10 09:51:57,438][227910] Min Reward on eval: 2867.7695125120463[0m
[37m[1m[2023-07-10 09:51:57,439][227910] Mean Reward across all agents: 2867.7695125120463[0m
[37m[1m[2023-07-10 09:51:57,439][227910] Average Trajectory Length: 916.348[0m
[36m[2023-07-10 09:52:02,023][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:52:02,024][227910] Reward + Measures: [[1727.41663206    0.30897734    0.34833342    0.23603068    0.13720261]
 [2606.14982641    0.31531945    0.37307501    0.21018611    0.15509999]
 [2341.29870789    0.29375097    0.30107257    0.21164706    0.12448235]
 ...
 [1971.98150163    0.29961461    0.32922229    0.21576047    0.0858327 ]
 [1949.20647517    0.29116905    0.31896639    0.1931124     0.11066895]
 [2548.63714843    0.28379855    0.32417917    0.20884278    0.12980483]][0m
[37m[1m[2023-07-10 09:52:02,024][227910] Max Reward on eval: 3269.2259353850504[0m
[37m[1m[2023-07-10 09:52:02,024][227910] Min Reward on eval: 956.334090248309[0m
[37m[1m[2023-07-10 09:52:02,025][227910] Mean Reward across all agents: 2561.890506634627[0m
[37m[1m[2023-07-10 09:52:02,025][227910] Average Trajectory Length: 883.9233333333333[0m
[36m[2023-07-10 09:52:02,027][227910] mean_value=-346.3994896011087, max_value=2249.934809837706[0m
[37m[1m[2023-07-10 09:52:02,030][227910] New mean coefficients: [[-0.37220362  0.7322044   0.10380468 -0.505053    0.21063137]][0m
[37m[1m[2023-07-10 09:52:02,030][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:52:10,524][227910] train() took 8.49 seconds to complete[0m
[36m[2023-07-10 09:52:10,529][227910] FPS: 452190.85[0m
[36m[2023-07-10 09:52:10,532][227910] itr=80, itrs=2000, Progress: 4.00%[0m
[37m[1m[2023-07-10 09:52:12,158][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000060[0m
[36m[2023-07-10 09:52:24,543][227910] train() took 12.15 seconds to complete[0m
[36m[2023-07-10 09:52:24,543][227910] FPS: 315973.70[0m
[36m[2023-07-10 09:52:28,407][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:52:28,408][227910] Reward + Measures: [[2516.06904048    0.30739719    0.32190031    0.19306462    0.11371501]][0m
[37m[1m[2023-07-10 09:52:28,408][227910] Max Reward on eval: 2516.0690404782545[0m
[37m[1m[2023-07-10 09:52:28,408][227910] Min Reward on eval: 2516.0690404782545[0m
[37m[1m[2023-07-10 09:52:28,408][227910] Mean Reward across all agents: 2516.0690404782545[0m
[37m[1m[2023-07-10 09:52:28,409][227910] Average Trajectory Length: 909.2113333333333[0m
[36m[2023-07-10 09:52:33,109][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:52:33,167][227910] Reward + Measures: [[1972.5339038     0.28531119    0.32322893    0.16142228    0.12814699]
 [2466.72562966    0.33732289    0.33939913    0.19254947    0.11345746]
 [2127.87753827    0.28268012    0.31847462    0.18678918    0.10531642]
 ...
 [2630.79237016    0.31799999    0.34240001    0.22590001    0.1513    ]
 [2072.89474905    0.31906065    0.33946547    0.19538327    0.14383094]
 [2603.54370911    0.30780002    0.34882107    0.21359999    0.10789474]][0m
[37m[1m[2023-07-10 09:52:33,167][227910] Max Reward on eval: 3178.4697769123595[0m
[37m[1m[2023-07-10 09:52:33,168][227910] Min Reward on eval: 848.941522416877[0m
[37m[1m[2023-07-10 09:52:33,168][227910] Mean Reward across all agents: 2240.47808767232[0m
[37m[1m[2023-07-10 09:52:33,168][227910] Average Trajectory Length: 882.6553333333333[0m
[36m[2023-07-10 09:52:33,171][227910] mean_value=-292.0673449485131, max_value=2279.4701592981037[0m
[37m[1m[2023-07-10 09:52:33,173][227910] New mean coefficients: [[-0.33113858  1.2885032  -0.4584711  -0.49019486  0.47210702]][0m
[37m[1m[2023-07-10 09:52:33,174][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:52:41,780][227910] train() took 8.60 seconds to complete[0m
[36m[2023-07-10 09:52:41,780][227910] FPS: 446310.30[0m
[36m[2023-07-10 09:52:41,782][227910] itr=81, itrs=2000, Progress: 4.05%[0m
[36m[2023-07-10 09:52:53,718][227910] train() took 11.92 seconds to complete[0m
[36m[2023-07-10 09:52:53,718][227910] FPS: 322076.14[0m
[36m[2023-07-10 09:52:57,599][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:52:57,600][227910] Reward + Measures: [[2087.03502306    0.31709415    0.33211082    0.1720352     0.144676  ]][0m
[37m[1m[2023-07-10 09:52:57,600][227910] Max Reward on eval: 2087.0350230588515[0m
[37m[1m[2023-07-10 09:52:57,600][227910] Min Reward on eval: 2087.0350230588515[0m
[37m[1m[2023-07-10 09:52:57,600][227910] Mean Reward across all agents: 2087.0350230588515[0m
[37m[1m[2023-07-10 09:52:57,600][227910] Average Trajectory Length: 906.307[0m
[36m[2023-07-10 09:53:02,354][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:53:02,355][227910] Reward + Measures: [[2181.62631708    0.30285716    0.3222        0.16814286    0.12231429]
 [2001.8212041     0.3294        0.35100001    0.20639999    0.1795    ]
 [1564.31228497    0.34890658    0.3494986     0.16077377    0.16436079]
 ...
 [1924.16034137    0.3198702     0.3432965     0.17363971    0.1357383 ]
 [1447.51186627    0.23858249    0.28422794    0.173299      0.13283961]
 [1331.50900031    0.33053842    0.34530821    0.15998232    0.18060701]][0m
[37m[1m[2023-07-10 09:53:02,355][227910] Max Reward on eval: 2673.4961615023667[0m
[37m[1m[2023-07-10 09:53:02,355][227910] Min Reward on eval: 680.6556690866826[0m
[37m[1m[2023-07-10 09:53:02,356][227910] Mean Reward across all agents: 1763.7109462752278[0m
[37m[1m[2023-07-10 09:53:02,356][227910] Average Trajectory Length: 862.953[0m
[36m[2023-07-10 09:53:02,360][227910] mean_value=-118.99687162072092, max_value=2553.6396640208877[0m
[37m[1m[2023-07-10 09:53:02,362][227910] New mean coefficients: [[-0.58721495  1.4909823  -0.15186074 -0.07307792  0.6751068 ]][0m
[37m[1m[2023-07-10 09:53:02,363][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:53:10,846][227910] train() took 8.48 seconds to complete[0m
[36m[2023-07-10 09:53:10,847][227910] FPS: 452748.33[0m
[36m[2023-07-10 09:53:10,849][227910] itr=82, itrs=2000, Progress: 4.10%[0m
[36m[2023-07-10 09:53:22,841][227910] train() took 11.98 seconds to complete[0m
[36m[2023-07-10 09:53:22,841][227910] FPS: 320633.33[0m
[36m[2023-07-10 09:53:26,766][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:53:26,767][227910] Reward + Measures: [[1568.25317483    0.34488297    0.38170671    0.16251679    0.21615726]][0m
[37m[1m[2023-07-10 09:53:26,767][227910] Max Reward on eval: 1568.2531748337926[0m
[37m[1m[2023-07-10 09:53:26,767][227910] Min Reward on eval: 1568.2531748337926[0m
[37m[1m[2023-07-10 09:53:26,767][227910] Mean Reward across all agents: 1568.2531748337926[0m
[37m[1m[2023-07-10 09:53:26,767][227910] Average Trajectory Length: 906.9956666666666[0m
[36m[2023-07-10 09:53:31,387][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:53:31,388][227910] Reward + Measures: [[1333.78269127    0.36301896    0.41607624    0.18231268    0.27135572]
 [ 545.80126638    0.32480001    0.63440001    0.21030001    0.57110006]
 [1440.2137852     0.31842768    0.35803485    0.16600589    0.20043989]
 ...
 [1062.12287843    0.32218519    0.42033702    0.11841112    0.28556296]
 [1952.31985871    0.35263672    0.36813745    0.16711631    0.17381702]
 [1387.28784874    0.26411185    0.28438136    0.15607288    0.12902598]][0m
[37m[1m[2023-07-10 09:53:31,388][227910] Max Reward on eval: 1999.3638732250547[0m
[37m[1m[2023-07-10 09:53:31,388][227910] Min Reward on eval: 464.8235501790303[0m
[37m[1m[2023-07-10 09:53:31,389][227910] Mean Reward across all agents: 1302.878805408753[0m
[37m[1m[2023-07-10 09:53:31,389][227910] Average Trajectory Length: 889.685[0m
[36m[2023-07-10 09:53:31,394][227910] mean_value=309.692179305405, max_value=2179.1508286922067[0m
[37m[1m[2023-07-10 09:53:31,397][227910] New mean coefficients: [[-0.69699955  1.8441365  -0.44117376 -0.24529117  0.7590792 ]][0m
[37m[1m[2023-07-10 09:53:31,398][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:53:40,200][227910] train() took 8.80 seconds to complete[0m
[36m[2023-07-10 09:53:40,200][227910] FPS: 436331.69[0m
[36m[2023-07-10 09:53:40,202][227910] itr=83, itrs=2000, Progress: 4.15%[0m
[36m[2023-07-10 09:53:52,389][227910] train() took 12.17 seconds to complete[0m
[36m[2023-07-10 09:53:52,390][227910] FPS: 315428.23[0m
[36m[2023-07-10 09:53:56,293][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:53:56,293][227910] Reward + Measures: [[1072.30735521    0.38729835    0.45489717    0.17284282    0.3259064 ]][0m
[37m[1m[2023-07-10 09:53:56,294][227910] Max Reward on eval: 1072.3073552056755[0m
[37m[1m[2023-07-10 09:53:56,294][227910] Min Reward on eval: 1072.3073552056755[0m
[37m[1m[2023-07-10 09:53:56,294][227910] Mean Reward across all agents: 1072.3073552056755[0m
[37m[1m[2023-07-10 09:53:56,294][227910] Average Trajectory Length: 925.7496666666666[0m
[36m[2023-07-10 09:54:01,043][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:54:01,043][227910] Reward + Measures: [[1286.41926876    0.38547894    0.42127895    0.16875264    0.26252633]
 [ 881.96958804    0.38921914    0.51453316    0.17073043    0.37754184]
 [1094.7142095     0.38087606    0.47385073    0.16889015    0.3168211 ]
 ...
 [ 464.31201268    0.35887682    0.56822789    0.15010934    0.4363597 ]
 [ 973.10608523    0.31650004    0.40780002    0.1567        0.26840001]
 [1191.7250658     0.42840004    0.51450002    0.17209999    0.36830002]][0m
[37m[1m[2023-07-10 09:54:01,043][227910] Max Reward on eval: 1638.40744234951[0m
[37m[1m[2023-07-10 09:54:01,044][227910] Min Reward on eval: 407.80465505575995[0m
[37m[1m[2023-07-10 09:54:01,044][227910] Mean Reward across all agents: 961.2330499135404[0m
[37m[1m[2023-07-10 09:54:01,044][227910] Average Trajectory Length: 933.1863333333333[0m
[36m[2023-07-10 09:54:01,048][227910] mean_value=37.673242298802606, max_value=1570.8466059988364[0m
[37m[1m[2023-07-10 09:54:01,051][227910] New mean coefficients: [[-0.9086755   1.470245   -0.20340094 -0.06054029  0.8649493 ]][0m
[37m[1m[2023-07-10 09:54:01,052][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:54:10,305][227910] train() took 9.25 seconds to complete[0m
[36m[2023-07-10 09:54:10,305][227910] FPS: 415091.94[0m
[36m[2023-07-10 09:54:10,308][227910] itr=84, itrs=2000, Progress: 4.20%[0m
[36m[2023-07-10 09:54:23,099][227910] train() took 12.78 seconds to complete[0m
[36m[2023-07-10 09:54:23,099][227910] FPS: 300586.25[0m
[36m[2023-07-10 09:54:27,347][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:54:27,347][227910] Reward + Measures: [[569.08626568   0.48852006   0.58363843   0.26239872   0.49786478]][0m
[37m[1m[2023-07-10 09:54:27,347][227910] Max Reward on eval: 569.0862656776407[0m
[37m[1m[2023-07-10 09:54:27,348][227910] Min Reward on eval: 569.0862656776407[0m
[37m[1m[2023-07-10 09:54:27,348][227910] Mean Reward across all agents: 569.0862656776407[0m
[37m[1m[2023-07-10 09:54:27,348][227910] Average Trajectory Length: 946.4096666666667[0m
[36m[2023-07-10 09:54:32,473][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:54:32,479][227910] Reward + Measures: [[ 818.1426637     0.47339782    0.52470648    0.21033795    0.38863251]
 [ 498.59673746    0.37920001    0.52990001    0.19350001    0.42659998]
 [ 661.70728793    0.35160002    0.45240003    0.16069999    0.34899998]
 ...
 [ 406.59511343    0.52500004    0.62445003    0.36155       0.51109999]
 [ 897.09101384    0.3745611     0.40981245    0.24424434    0.27173415]
 [1053.56733236    0.43730003    0.49049997    0.17910001    0.35279998]][0m
[37m[1m[2023-07-10 09:54:32,479][227910] Max Reward on eval: 1425.6027159713092[0m
[37m[1m[2023-07-10 09:54:32,479][227910] Min Reward on eval: 237.40564749826444[0m
[37m[1m[2023-07-10 09:54:32,479][227910] Mean Reward across all agents: 683.7526994992068[0m
[37m[1m[2023-07-10 09:54:32,480][227910] Average Trajectory Length: 932.9666666666666[0m
[36m[2023-07-10 09:54:32,487][227910] mean_value=529.4623534970606, max_value=1549.7918648327934[0m
[37m[1m[2023-07-10 09:54:32,490][227910] New mean coefficients: [[-1.056557    1.3501891   0.13226837 -0.06941609  1.1225529 ]][0m
[37m[1m[2023-07-10 09:54:32,492][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:54:41,535][227910] train() took 9.04 seconds to complete[0m
[36m[2023-07-10 09:54:41,535][227910] FPS: 424689.21[0m
[36m[2023-07-10 09:54:41,538][227910] itr=85, itrs=2000, Progress: 4.25%[0m
[36m[2023-07-10 09:54:54,213][227910] train() took 12.66 seconds to complete[0m
[36m[2023-07-10 09:54:54,213][227910] FPS: 303336.01[0m
[36m[2023-07-10 09:54:58,476][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:54:58,476][227910] Reward + Measures: [[282.78352429   0.62685776   0.69134593   0.42913553   0.6356557 ]][0m
[37m[1m[2023-07-10 09:54:58,476][227910] Max Reward on eval: 282.7835242931944[0m
[37m[1m[2023-07-10 09:54:58,476][227910] Min Reward on eval: 282.7835242931944[0m
[37m[1m[2023-07-10 09:54:58,477][227910] Mean Reward across all agents: 282.7835242931944[0m
[37m[1m[2023-07-10 09:54:58,477][227910] Average Trajectory Length: 959.5853333333333[0m
[36m[2023-07-10 09:55:03,624][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:55:03,625][227910] Reward + Measures: [[489.9626586    0.5233826    0.56493622   0.37571159   0.47248983]
 [487.2303644    0.5977       0.65370005   0.39140001   0.57070005]
 [346.70838589   0.65079999   0.71350002   0.44170004   0.65000004]
 ...
 [240.18944919   0.68600005   0.79570001   0.5284       0.73180002]
 [344.77549327   0.57890004   0.63749999   0.4427       0.58310002]
 [187.55541219   0.62705809   0.70958394   0.41707745   0.64412904]][0m
[37m[1m[2023-07-10 09:55:03,625][227910] Max Reward on eval: 837.9179007521482[0m
[37m[1m[2023-07-10 09:55:03,626][227910] Min Reward on eval: 104.66002251771279[0m
[37m[1m[2023-07-10 09:55:03,626][227910] Mean Reward across all agents: 383.4826495456693[0m
[37m[1m[2023-07-10 09:55:03,626][227910] Average Trajectory Length: 944.8913333333333[0m
[36m[2023-07-10 09:55:03,634][227910] mean_value=477.8246810854218, max_value=1111.12552070776[0m
[37m[1m[2023-07-10 09:55:03,637][227910] New mean coefficients: [[-1.5633817   1.5341971   0.51366574 -0.32640862  1.0307809 ]][0m
[37m[1m[2023-07-10 09:55:03,638][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:55:12,917][227910] train() took 9.28 seconds to complete[0m
[36m[2023-07-10 09:55:12,917][227910] FPS: 413897.84[0m
[36m[2023-07-10 09:55:12,920][227910] itr=86, itrs=2000, Progress: 4.30%[0m
[36m[2023-07-10 09:55:25,605][227910] train() took 12.67 seconds to complete[0m
[36m[2023-07-10 09:55:25,605][227910] FPS: 303088.95[0m
[36m[2023-07-10 09:55:29,887][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:55:29,888][227910] Reward + Measures: [[155.81144929   0.70264357   0.74569368   0.51818091   0.70364308]][0m
[37m[1m[2023-07-10 09:55:29,888][227910] Max Reward on eval: 155.81144929414717[0m
[37m[1m[2023-07-10 09:55:29,888][227910] Min Reward on eval: 155.81144929414717[0m
[37m[1m[2023-07-10 09:55:29,888][227910] Mean Reward across all agents: 155.81144929414717[0m
[37m[1m[2023-07-10 09:55:29,889][227910] Average Trajectory Length: 956.756[0m
[36m[2023-07-10 09:55:35,164][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:55:35,169][227910] Reward + Measures: [[200.34482676   0.66580003   0.7518       0.35500002   0.708     ]
 [226.53274977   0.64831263   0.73909843   0.45841262   0.67331499]
 [171.47393103   0.70300001   0.73800004   0.56150001   0.71090007]
 ...
 [253.24569526   0.64050001   0.73800004   0.38439998   0.65869999]
 [482.70950588   0.35260001   0.40360004   0.16650002   0.3608    ]
 [633.41961282   0.41513291   0.50092375   0.19107245   0.3453581 ]][0m
[37m[1m[2023-07-10 09:55:35,169][227910] Max Reward on eval: 970.4336668548407[0m
[37m[1m[2023-07-10 09:55:35,170][227910] Min Reward on eval: 31.694939024792983[0m
[37m[1m[2023-07-10 09:55:35,170][227910] Mean Reward across all agents: 312.7066116378747[0m
[37m[1m[2023-07-10 09:55:35,170][227910] Average Trajectory Length: 945.3403333333333[0m
[36m[2023-07-10 09:55:35,177][227910] mean_value=114.82910438875903, max_value=899.6321418287[0m
[37m[1m[2023-07-10 09:55:35,180][227910] New mean coefficients: [[-0.7220347   1.316827    0.3447019  -0.03294006  0.9824856 ]][0m
[37m[1m[2023-07-10 09:55:35,181][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:55:44,460][227910] train() took 9.28 seconds to complete[0m
[36m[2023-07-10 09:55:44,460][227910] FPS: 413908.76[0m
[36m[2023-07-10 09:55:44,463][227910] itr=87, itrs=2000, Progress: 4.35%[0m
[36m[2023-07-10 09:55:57,137][227910] train() took 12.66 seconds to complete[0m
[36m[2023-07-10 09:55:57,138][227910] FPS: 303325.38[0m
[36m[2023-07-10 09:56:01,406][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:56:01,407][227910] Reward + Measures: [[63.85411111  0.80109626  0.81185824  0.65245289  0.78110886]][0m
[37m[1m[2023-07-10 09:56:01,407][227910] Max Reward on eval: 63.854111105752466[0m
[37m[1m[2023-07-10 09:56:01,407][227910] Min Reward on eval: 63.854111105752466[0m
[37m[1m[2023-07-10 09:56:01,407][227910] Mean Reward across all agents: 63.854111105752466[0m
[37m[1m[2023-07-10 09:56:01,408][227910] Average Trajectory Length: 969.8969999999999[0m
[36m[2023-07-10 09:56:06,562][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:56:06,567][227910] Reward + Measures: [[ 82.30536145   0.61928672   0.67739123   0.45869064   0.56604934]
 [184.70978891   0.60220003   0.75139999   0.35090002   0.7087    ]
 [ 75.84728437   0.66769999   0.77415001   0.49695      0.7101    ]
 ...
 [ 40.24352161   0.88099998   0.88060009   0.84250003   0.84260005]
 [ 98.2146946    0.64443815   0.69009048   0.43852854   0.57508576]
 [ -6.28973574   0.80635655   0.82228261   0.6023435    0.78901738]][0m
[37m[1m[2023-07-10 09:56:06,567][227910] Max Reward on eval: 518.4341480825096[0m
[37m[1m[2023-07-10 09:56:06,567][227910] Min Reward on eval: -83.74987174320268[0m
[37m[1m[2023-07-10 09:56:06,568][227910] Mean Reward across all agents: 113.71360930125971[0m
[37m[1m[2023-07-10 09:56:06,568][227910] Average Trajectory Length: 964.361[0m
[36m[2023-07-10 09:56:06,577][227910] mean_value=300.8281140137838, max_value=783.7678614013712[0m
[37m[1m[2023-07-10 09:56:06,580][227910] New mean coefficients: [[-0.3540229   0.98611116  0.2701182   0.36352992  1.032134  ]][0m
[37m[1m[2023-07-10 09:56:06,582][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:56:15,829][227910] train() took 9.25 seconds to complete[0m
[36m[2023-07-10 09:56:15,829][227910] FPS: 415322.13[0m
[36m[2023-07-10 09:56:15,832][227910] itr=88, itrs=2000, Progress: 4.40%[0m
[36m[2023-07-10 09:56:28,791][227910] train() took 12.94 seconds to complete[0m
[36m[2023-07-10 09:56:28,791][227910] FPS: 296671.83[0m
[36m[2023-07-10 09:56:33,128][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:56:33,129][227910] Reward + Measures: [[-20.55809525   0.85924673   0.85927701   0.75613225   0.83633333]][0m
[37m[1m[2023-07-10 09:56:33,129][227910] Max Reward on eval: -20.558095250265364[0m
[37m[1m[2023-07-10 09:56:33,129][227910] Min Reward on eval: -20.558095250265364[0m
[37m[1m[2023-07-10 09:56:33,129][227910] Mean Reward across all agents: -20.558095250265364[0m
[37m[1m[2023-07-10 09:56:33,129][227910] Average Trajectory Length: 979.303[0m
[36m[2023-07-10 09:56:38,247][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:56:38,247][227910] Reward + Measures: [[ -62.71491604    0.87230009    0.88169998    0.73629999    0.85729998]
 [  -5.54310775    0.80226672    0.82180005    0.72010005    0.75393337]
 [-127.316534      0.91180003    0.90829992    0.8448        0.89670002]
 ...
 [  87.43437893    0.77340001    0.78610003    0.59980005    0.75780004]
 [-103.2187569     0.79430002    0.79689997    0.66110003    0.77939999]
 [ 319.20759188    0.49699998    0.54870003    0.36390001    0.4765    ]][0m
[37m[1m[2023-07-10 09:56:38,248][227910] Max Reward on eval: 426.2894778493792[0m
[37m[1m[2023-07-10 09:56:38,248][227910] Min Reward on eval: -182.79038664016406[0m
[37m[1m[2023-07-10 09:56:38,248][227910] Mean Reward across all agents: 0.8139578340990093[0m
[37m[1m[2023-07-10 09:56:38,248][227910] Average Trajectory Length: 972.4383333333333[0m
[36m[2023-07-10 09:56:38,254][227910] mean_value=162.7797880014822, max_value=606.8695549449185[0m
[37m[1m[2023-07-10 09:56:38,257][227910] New mean coefficients: [[-0.28215113  0.787305    0.28364903  0.46819142  1.0356942 ]][0m
[37m[1m[2023-07-10 09:56:38,258][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:56:47,522][227910] train() took 9.26 seconds to complete[0m
[36m[2023-07-10 09:56:47,523][227910] FPS: 414570.07[0m
[36m[2023-07-10 09:56:47,525][227910] itr=89, itrs=2000, Progress: 4.45%[0m
[36m[2023-07-10 09:57:00,343][227910] train() took 12.80 seconds to complete[0m
[36m[2023-07-10 09:57:00,343][227910] FPS: 299927.39[0m
[36m[2023-07-10 09:57:05,656][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:57:05,657][227910] Reward + Measures: [[-78.14428432   0.88390321   0.88272899   0.82866091   0.8609277 ]][0m
[37m[1m[2023-07-10 09:57:05,657][227910] Max Reward on eval: -78.14428432040916[0m
[37m[1m[2023-07-10 09:57:05,657][227910] Min Reward on eval: -78.14428432040916[0m
[37m[1m[2023-07-10 09:57:05,657][227910] Mean Reward across all agents: -78.14428432040916[0m
[37m[1m[2023-07-10 09:57:05,658][227910] Average Trajectory Length: 989.8773333333334[0m
[36m[2023-07-10 09:57:12,411][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:57:12,414][227910] Reward + Measures: [[ 100.75656886    0.70567274    0.71017277    0.62607276    0.64640003]
 [ 412.62015723    0.40973982    0.47724095    0.3160156     0.39582434]
 [ 142.29966138    0.53104115    0.58667058    0.40695882    0.45110002]
 ...
 [-105.24194254    0.89499998    0.89670002    0.79460001    0.87779999]
 [-127.912754      0.82420009    0.8452        0.77030003    0.82190001]
 [-120.40449016    0.86129999    0.86360008    0.81070006    0.83470005]][0m
[37m[1m[2023-07-10 09:57:12,415][227910] Max Reward on eval: 412.6201572251157[0m
[37m[1m[2023-07-10 09:57:12,415][227910] Min Reward on eval: -220.05085416133517[0m
[37m[1m[2023-07-10 09:57:12,415][227910] Mean Reward across all agents: 29.873976950001996[0m
[37m[1m[2023-07-10 09:57:12,415][227910] Average Trajectory Length: 959.362[0m
[36m[2023-07-10 09:57:12,450][227910] mean_value=129.52153237055626, max_value=851.7604091811576[0m
[37m[1m[2023-07-10 09:57:12,455][227910] New mean coefficients: [[-1.5280292   0.5976761   0.87384963  0.9702163   0.8542075 ]][0m
[37m[1m[2023-07-10 09:57:12,458][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:57:24,267][227910] train() took 11.80 seconds to complete[0m
[36m[2023-07-10 09:57:24,268][227910] FPS: 325376.66[0m
[36m[2023-07-10 09:57:24,292][227910] itr=90, itrs=2000, Progress: 4.50%[0m
[37m[1m[2023-07-10 09:57:29,830][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000070[0m
[36m[2023-07-10 09:57:44,608][227910] train() took 13.98 seconds to complete[0m
[36m[2023-07-10 09:57:44,608][227910] FPS: 274598.27[0m
[36m[2023-07-10 09:57:48,518][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:57:48,519][227910] Reward + Measures: [[-95.77803861   0.88450307   0.88502312   0.83411759   0.86470819]][0m
[37m[1m[2023-07-10 09:57:48,519][227910] Max Reward on eval: -95.77803861462594[0m
[37m[1m[2023-07-10 09:57:48,519][227910] Min Reward on eval: -95.77803861462594[0m
[37m[1m[2023-07-10 09:57:48,520][227910] Mean Reward across all agents: -95.77803861462594[0m
[37m[1m[2023-07-10 09:57:48,520][227910] Average Trajectory Length: 990.5899999999999[0m
[36m[2023-07-10 09:57:53,424][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:57:53,425][227910] Reward + Measures: [[-183.01470109    0.87620002    0.87709999    0.84560007    0.84540004]
 [ 291.39621543    0.59389997    0.65900004    0.40959999    0.56420004]
 [  70.49171196    0.77863342    0.72799999    0.72170001    0.68816668]
 ...
 [  60.8779439     0.81350005    0.88119996    0.5302        0.84980011]
 [  98.37938479    0.62220001    0.78899997    0.22550002    0.70810002]
 [ -12.65386172    0.74650002    0.74150002    0.71000004    0.66510004]][0m
[37m[1m[2023-07-10 09:57:53,425][227910] Max Reward on eval: 620.6278482963098[0m
[37m[1m[2023-07-10 09:57:53,425][227910] Min Reward on eval: -219.5954086608137[0m
[37m[1m[2023-07-10 09:57:53,426][227910] Mean Reward across all agents: 57.40426876480553[0m
[37m[1m[2023-07-10 09:57:53,426][227910] Average Trajectory Length: 973.5816666666666[0m
[36m[2023-07-10 09:57:53,431][227910] mean_value=22.40066382100168, max_value=812.2735722065345[0m
[37m[1m[2023-07-10 09:57:53,434][227910] New mean coefficients: [[-1.8501192  0.7411292  1.4878588  0.9555113  0.8251122]][0m
[37m[1m[2023-07-10 09:57:53,435][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:58:02,182][227910] train() took 8.75 seconds to complete[0m
[36m[2023-07-10 09:58:02,183][227910] FPS: 439063.77[0m
[36m[2023-07-10 09:58:02,185][227910] itr=91, itrs=2000, Progress: 4.55%[0m
[36m[2023-07-10 09:58:14,396][227910] train() took 12.20 seconds to complete[0m
[36m[2023-07-10 09:58:14,396][227910] FPS: 314876.98[0m
[36m[2023-07-10 09:58:18,369][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:58:18,369][227910] Reward + Measures: [[-118.96228822    0.89323264    0.89426845    0.85665625    0.87530971]][0m
[37m[1m[2023-07-10 09:58:18,370][227910] Max Reward on eval: -118.96228822325989[0m
[37m[1m[2023-07-10 09:58:18,370][227910] Min Reward on eval: -118.96228822325989[0m
[37m[1m[2023-07-10 09:58:18,370][227910] Mean Reward across all agents: -118.96228822325989[0m
[37m[1m[2023-07-10 09:58:18,370][227910] Average Trajectory Length: 990.7656666666667[0m
[36m[2023-07-10 09:58:23,119][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:58:23,120][227910] Reward + Measures: [[-117.81459503    0.87389994    0.89209998    0.83090001    0.85769999]
 [ -76.50902104    0.824         0.82600003    0.77109998    0.81110001]
 [ -43.89214764    0.90039998    0.90810007    0.84980005    0.89139998]
 ...
 [-209.05090516    0.87909997    0.89220011    0.84160006    0.86709994]
 [ -67.93903367    0.88810009    0.89580005    0.85339993    0.88249999]
 [   6.56926977    0.8089        0.82120001    0.78690004    0.7991001 ]][0m
[37m[1m[2023-07-10 09:58:23,120][227910] Max Reward on eval: 358.62568349209033[0m
[37m[1m[2023-07-10 09:58:23,120][227910] Min Reward on eval: -340.9850835455116[0m
[37m[1m[2023-07-10 09:58:23,120][227910] Mean Reward across all agents: -65.75071203742246[0m
[37m[1m[2023-07-10 09:58:23,121][227910] Average Trajectory Length: 968.7276666666667[0m
[36m[2023-07-10 09:58:23,124][227910] mean_value=-53.3832223174123, max_value=511.4529831102118[0m
[37m[1m[2023-07-10 09:58:23,126][227910] New mean coefficients: [[-1.7867072   0.59609354  1.0472869   1.4517145   0.62455976]][0m
[37m[1m[2023-07-10 09:58:23,127][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:58:31,847][227910] train() took 8.72 seconds to complete[0m
[36m[2023-07-10 09:58:31,847][227910] FPS: 440493.43[0m
[36m[2023-07-10 09:58:31,849][227910] itr=92, itrs=2000, Progress: 4.60%[0m
[36m[2023-07-10 09:58:44,076][227910] train() took 12.21 seconds to complete[0m
[36m[2023-07-10 09:58:44,076][227910] FPS: 314418.00[0m
[36m[2023-07-10 09:58:47,972][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:58:47,978][227910] Reward + Measures: [[-190.57393586    0.89893734    0.90057218    0.87597573    0.87969553]][0m
[37m[1m[2023-07-10 09:58:47,978][227910] Max Reward on eval: -190.57393585512636[0m
[37m[1m[2023-07-10 09:58:47,978][227910] Min Reward on eval: -190.57393585512636[0m
[37m[1m[2023-07-10 09:58:47,978][227910] Mean Reward across all agents: -190.57393585512636[0m
[37m[1m[2023-07-10 09:58:47,979][227910] Average Trajectory Length: 991.3366666666666[0m
[36m[2023-07-10 09:58:52,682][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:58:52,682][227910] Reward + Measures: [[ -26.81979511    0.70020002    0.6943        0.66640007    0.65600008]
 [ -20.46318367    0.78711814    0.80553037    0.74387276    0.76934856]
 [-207.18334121    0.89909995    0.91359997    0.84960002    0.87860006]
 ...
 [-133.64049422    0.90550005    0.91060001    0.8811        0.88830006]
 [ 135.03282413    0.71250004    0.73879999    0.63740003    0.69980001]
 [-250.44227741    0.91040003    0.91399997    0.88789999    0.89370006]][0m
[37m[1m[2023-07-10 09:58:52,682][227910] Max Reward on eval: 495.76547149774854[0m
[37m[1m[2023-07-10 09:58:52,683][227910] Min Reward on eval: -419.88309761981947[0m
[37m[1m[2023-07-10 09:58:52,683][227910] Mean Reward across all agents: -39.83974032695516[0m
[37m[1m[2023-07-10 09:58:52,683][227910] Average Trajectory Length: 984.7593333333333[0m
[36m[2023-07-10 09:58:52,686][227910] mean_value=-70.91726260064638, max_value=700.989207106343[0m
[37m[1m[2023-07-10 09:58:52,689][227910] New mean coefficients: [[-1.6418766   0.23317707  1.1006123   1.853809    1.0575272 ]][0m
[37m[1m[2023-07-10 09:58:52,690][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:59:01,461][227910] train() took 8.77 seconds to complete[0m
[36m[2023-07-10 09:59:01,461][227910] FPS: 437896.34[0m
[36m[2023-07-10 09:59:01,463][227910] itr=93, itrs=2000, Progress: 4.65%[0m
[36m[2023-07-10 09:59:13,655][227910] train() took 12.18 seconds to complete[0m
[36m[2023-07-10 09:59:13,656][227910] FPS: 315310.28[0m
[36m[2023-07-10 09:59:17,605][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:59:17,610][227910] Reward + Measures: [[-246.29077481    0.90520781    0.9097262     0.8850615     0.88970482]][0m
[37m[1m[2023-07-10 09:59:17,611][227910] Max Reward on eval: -246.2907748089608[0m
[37m[1m[2023-07-10 09:59:17,611][227910] Min Reward on eval: -246.2907748089608[0m
[37m[1m[2023-07-10 09:59:17,611][227910] Mean Reward across all agents: -246.2907748089608[0m
[37m[1m[2023-07-10 09:59:17,611][227910] Average Trajectory Length: 994.4463333333333[0m
[36m[2023-07-10 09:59:22,343][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:59:22,343][227910] Reward + Measures: [[-317.61543794    0.89939994    0.91000003    0.88020003    0.88260001]
 [-315.42499608    0.91909999    0.92049998    0.86770004    0.90609998]
 [-326.82494242    0.92399997    0.92649996    0.90860003    0.90490007]
 ...
 [   3.66269011    0.88240004    0.90070003    0.85689992    0.87309992]
 [-281.12980104    0.90350002    0.91219997    0.88390011    0.89510006]
 [ -30.58356697    0.8955        0.90350002    0.85089999    0.87390006]][0m
[37m[1m[2023-07-10 09:59:22,343][227910] Max Reward on eval: 623.9820887673297[0m
[37m[1m[2023-07-10 09:59:22,344][227910] Min Reward on eval: -596.6298763445578[0m
[37m[1m[2023-07-10 09:59:22,344][227910] Mean Reward across all agents: -93.23162101515703[0m
[37m[1m[2023-07-10 09:59:22,344][227910] Average Trajectory Length: 987.6696666666667[0m
[36m[2023-07-10 09:59:22,348][227910] mean_value=-77.32529270362015, max_value=862.8984334154171[0m
[37m[1m[2023-07-10 09:59:22,351][227910] New mean coefficients: [[-1.6397203   1.7391735   0.65167737  1.1551467   1.900255  ]][0m
[37m[1m[2023-07-10 09:59:22,352][227910] Moving the mean solution point...[0m
[36m[2023-07-10 09:59:31,088][227910] train() took 8.74 seconds to complete[0m
[36m[2023-07-10 09:59:31,089][227910] FPS: 439601.74[0m
[36m[2023-07-10 09:59:31,091][227910] itr=94, itrs=2000, Progress: 4.70%[0m
[36m[2023-07-10 09:59:43,363][227910] train() took 12.26 seconds to complete[0m
[36m[2023-07-10 09:59:43,363][227910] FPS: 313301.28[0m
[36m[2023-07-10 09:59:47,283][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:59:47,283][227910] Reward + Measures: [[-236.27788122    0.90625423    0.91111273    0.8843016     0.89038509]][0m
[37m[1m[2023-07-10 09:59:47,284][227910] Max Reward on eval: -236.27788121601722[0m
[37m[1m[2023-07-10 09:59:47,284][227910] Min Reward on eval: -236.27788121601722[0m
[37m[1m[2023-07-10 09:59:47,284][227910] Mean Reward across all agents: -236.27788121601722[0m
[37m[1m[2023-07-10 09:59:47,284][227910] Average Trajectory Length: 991.2046666666666[0m
[36m[2023-07-10 09:59:52,044][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 09:59:52,044][227910] Reward + Measures: [[-128.13608489    0.86630005    0.82980007    0.82819998    0.81030005]
 [-200.02749135    0.83372223    0.86054438    0.82622594    0.8044073 ]
 [-180.38997812    0.92189997    0.91460001    0.8950001     0.89699996]
 ...
 [-194.86050572    0.83689994    0.84489995    0.80820006    0.82159996]
 [ -87.41413034    0.83700001    0.77850002    0.78070003    0.76200008]
 [-174.34908317    0.81920004    0.81619996    0.78940004    0.79460001]][0m
[37m[1m[2023-07-10 09:59:52,045][227910] Max Reward on eval: 191.76778503847308[0m
[37m[1m[2023-07-10 09:59:52,045][227910] Min Reward on eval: -310.26907138656827[0m
[37m[1m[2023-07-10 09:59:52,045][227910] Mean Reward across all agents: -111.21197648289866[0m
[37m[1m[2023-07-10 09:59:52,045][227910] Average Trajectory Length: 973.4703333333333[0m
[36m[2023-07-10 09:59:52,049][227910] mean_value=-97.13509721639353, max_value=639.2697995570954[0m
[37m[1m[2023-07-10 09:59:52,051][227910] New mean coefficients: [[-1.2898601  2.9031727  0.0499422  0.6559756  2.6827984]][0m
[37m[1m[2023-07-10 09:59:52,052][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:00:00,745][227910] train() took 8.69 seconds to complete[0m
[36m[2023-07-10 10:00:00,745][227910] FPS: 441840.71[0m
[36m[2023-07-10 10:00:00,748][227910] itr=95, itrs=2000, Progress: 4.75%[0m
[36m[2023-07-10 10:00:12,980][227910] train() took 12.22 seconds to complete[0m
[36m[2023-07-10 10:00:12,981][227910] FPS: 314275.64[0m
[36m[2023-07-10 10:00:16,879][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:00:16,884][227910] Reward + Measures: [[-321.47793094    0.9146266     0.91857713    0.89490867    0.89924121]][0m
[37m[1m[2023-07-10 10:00:16,885][227910] Max Reward on eval: -321.4779309445859[0m
[37m[1m[2023-07-10 10:00:16,885][227910] Min Reward on eval: -321.4779309445859[0m
[37m[1m[2023-07-10 10:00:16,885][227910] Mean Reward across all agents: -321.4779309445859[0m
[37m[1m[2023-07-10 10:00:16,885][227910] Average Trajectory Length: 996.6056666666666[0m
[36m[2023-07-10 10:00:21,718][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:00:21,719][227910] Reward + Measures: [[ -44.78921863    0.5948        0.78330004    0.3739        0.74860001]
 [-239.46401435    0.91659993    0.92819995    0.88360006    0.90960008]
 [  24.96737082    0.50021046    0.73143119    0.21455847    0.63204306]
 ...
 [  69.30365192    0.7683        0.8021        0.63669997    0.76290005]
 [  15.94845412    0.72329998    0.78940004    0.52829999    0.76309997]
 [   8.73617304    0.78809994    0.8714        0.64950001    0.82279998]][0m
[37m[1m[2023-07-10 10:00:21,719][227910] Max Reward on eval: 953.0107382390881[0m
[37m[1m[2023-07-10 10:00:21,719][227910] Min Reward on eval: -487.2657705169637[0m
[37m[1m[2023-07-10 10:00:21,720][227910] Mean Reward across all agents: -4.717572846385097[0m
[37m[1m[2023-07-10 10:00:21,720][227910] Average Trajectory Length: 990.0066666666667[0m
[36m[2023-07-10 10:00:21,725][227910] mean_value=-27.68478407842507, max_value=1163.4602013911122[0m
[37m[1m[2023-07-10 10:00:21,727][227910] New mean coefficients: [[-1.4482173   3.558238    0.9385065  -0.11016774  3.4418418 ]][0m
[37m[1m[2023-07-10 10:00:21,728][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:00:30,447][227910] train() took 8.72 seconds to complete[0m
[36m[2023-07-10 10:00:30,447][227910] FPS: 440509.21[0m
[36m[2023-07-10 10:00:30,460][227910] itr=96, itrs=2000, Progress: 4.80%[0m
[36m[2023-07-10 10:00:42,537][227910] train() took 12.05 seconds to complete[0m
[36m[2023-07-10 10:00:42,537][227910] FPS: 318546.75[0m
[36m[2023-07-10 10:00:46,469][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:00:46,474][227910] Reward + Measures: [[-409.1264763     0.92008203    0.92385137    0.90098065    0.90521646]][0m
[37m[1m[2023-07-10 10:00:46,474][227910] Max Reward on eval: -409.12647629905075[0m
[37m[1m[2023-07-10 10:00:46,474][227910] Min Reward on eval: -409.12647629905075[0m
[37m[1m[2023-07-10 10:00:46,475][227910] Mean Reward across all agents: -409.12647629905075[0m
[37m[1m[2023-07-10 10:00:46,475][227910] Average Trajectory Length: 996.7596666666666[0m
[36m[2023-07-10 10:00:51,231][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:00:51,232][227910] Reward + Measures: [[-105.61296507    0.82810003    0.83789998    0.78430003    0.78369999]
 [ 135.95708566    0.55320001    0.66709995    0.44650003    0.54619998]
 [-160.45612163    0.88290006    0.89519995    0.83589995    0.86070007]
 ...
 [ 138.53225569    0.55690002    0.63479996    0.44770002    0.5151    ]
 [ 291.85332916    0.36461431    0.41131431    0.24192858    0.2625905 ]
 [-188.8183152     0.88920003    0.90289992    0.81500006    0.87659997]][0m
[37m[1m[2023-07-10 10:00:51,232][227910] Max Reward on eval: 357.22524811300684[0m
[37m[1m[2023-07-10 10:00:51,232][227910] Min Reward on eval: -549.6841147155035[0m
[37m[1m[2023-07-10 10:00:51,233][227910] Mean Reward across all agents: -72.04296903628301[0m
[37m[1m[2023-07-10 10:00:51,233][227910] Average Trajectory Length: 987.0463333333333[0m
[36m[2023-07-10 10:00:51,236][227910] mean_value=-158.69988248155613, max_value=831.1505492437824[0m
[37m[1m[2023-07-10 10:00:51,238][227910] New mean coefficients: [[-2.6701813   4.460523    1.4853561  -0.26379266  3.9161465 ]][0m
[37m[1m[2023-07-10 10:00:51,239][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:00:59,998][227910] train() took 8.76 seconds to complete[0m
[36m[2023-07-10 10:00:59,999][227910] FPS: 438480.59[0m
[36m[2023-07-10 10:01:00,001][227910] itr=97, itrs=2000, Progress: 4.85%[0m
[36m[2023-07-10 10:01:12,178][227910] train() took 12.16 seconds to complete[0m
[36m[2023-07-10 10:01:12,179][227910] FPS: 315726.38[0m
[36m[2023-07-10 10:01:16,130][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:01:16,131][227910] Reward + Measures: [[-447.85165007    0.92578924    0.92741501    0.90749872    0.9100678 ]][0m
[37m[1m[2023-07-10 10:01:16,131][227910] Max Reward on eval: -447.85165007354357[0m
[37m[1m[2023-07-10 10:01:16,131][227910] Min Reward on eval: -447.85165007354357[0m
[37m[1m[2023-07-10 10:01:16,131][227910] Mean Reward across all agents: -447.85165007354357[0m
[37m[1m[2023-07-10 10:01:16,132][227910] Average Trajectory Length: 999.0459999999999[0m
[36m[2023-07-10 10:01:20,998][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:01:20,998][227910] Reward + Measures: [[-305.69327229    0.87950003    0.88260001    0.84470004    0.84790003]
 [-601.64824525    0.90620005    0.90389997    0.88300002    0.88420004]
 [-254.1627694     0.90939999    0.90459996    0.87379998    0.88000005]
 ...
 [ -83.999458      0.75899851    0.73698539    0.7218942     0.70302504]
 [-282.26153567    0.89019996    0.89740002    0.84289998    0.85949993]
 [-450.86816195    0.87169999    0.87159997    0.83630002    0.84019995]][0m
[37m[1m[2023-07-10 10:01:20,999][227910] Max Reward on eval: 439.0569597034599[0m
[37m[1m[2023-07-10 10:01:20,999][227910] Min Reward on eval: -601.6482452504803[0m
[37m[1m[2023-07-10 10:01:20,999][227910] Mean Reward across all agents: -233.41154130114606[0m
[37m[1m[2023-07-10 10:01:20,999][227910] Average Trajectory Length: 984.6683333333333[0m
[36m[2023-07-10 10:01:21,001][227910] mean_value=-288.26219892940867, max_value=683.5887408696991[0m
[37m[1m[2023-07-10 10:01:21,004][227910] New mean coefficients: [[-2.6530514  3.497517   1.1183109  0.6701243  4.7856674]][0m
[37m[1m[2023-07-10 10:01:21,005][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:01:29,927][227910] train() took 8.92 seconds to complete[0m
[36m[2023-07-10 10:01:29,927][227910] FPS: 430485.12[0m
[36m[2023-07-10 10:01:29,929][227910] itr=98, itrs=2000, Progress: 4.90%[0m
[36m[2023-07-10 10:01:42,103][227910] train() took 12.16 seconds to complete[0m
[36m[2023-07-10 10:01:42,104][227910] FPS: 315818.26[0m
[36m[2023-07-10 10:01:46,113][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:01:46,113][227910] Reward + Measures: [[-511.49375287    0.92841601    0.92998719    0.91160572    0.91151661]][0m
[37m[1m[2023-07-10 10:01:46,113][227910] Max Reward on eval: -511.4937528720543[0m
[37m[1m[2023-07-10 10:01:46,114][227910] Min Reward on eval: -511.4937528720543[0m
[37m[1m[2023-07-10 10:01:46,114][227910] Mean Reward across all agents: -511.4937528720543[0m
[37m[1m[2023-07-10 10:01:46,114][227910] Average Trajectory Length: 996.876[0m
[36m[2023-07-10 10:01:50,823][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:01:50,823][227910] Reward + Measures: [[-102.97553378    0.73460001    0.81209993    0.42609999    0.76550001]
 [-406.82543975    0.81730002    0.85249996    0.66140002    0.81529999]
 [-520.70173195    0.82429999    0.829         0.77130002    0.78950006]
 ...
 [-391.49125037    0.72700006    0.74150002    0.62370002    0.6965    ]
 [-157.32213992    0.78860003    0.84759998    0.41150004    0.7999    ]
 [-203.97177079    0.72419995    0.7274        0.67430001    0.66990006]][0m
[37m[1m[2023-07-10 10:01:50,824][227910] Max Reward on eval: 527.3608428253356[0m
[37m[1m[2023-07-10 10:01:50,824][227910] Min Reward on eval: -903.1577641827287[0m
[37m[1m[2023-07-10 10:01:50,824][227910] Mean Reward across all agents: -325.5146565709575[0m
[37m[1m[2023-07-10 10:01:50,824][227910] Average Trajectory Length: 981.9166666666666[0m
[36m[2023-07-10 10:01:50,828][227910] mean_value=-332.2533643434117, max_value=867.193343045772[0m
[37m[1m[2023-07-10 10:01:50,831][227910] New mean coefficients: [[-3.0818033  2.5460155  3.4765987  1.3519766  4.525133 ]][0m
[37m[1m[2023-07-10 10:01:50,832][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:01:59,526][227910] train() took 8.69 seconds to complete[0m
[36m[2023-07-10 10:01:59,527][227910] FPS: 441725.01[0m
[36m[2023-07-10 10:01:59,529][227910] itr=99, itrs=2000, Progress: 4.95%[0m
[36m[2023-07-10 10:02:11,763][227910] train() took 12.22 seconds to complete[0m
[36m[2023-07-10 10:02:11,764][227910] FPS: 314211.81[0m
[36m[2023-07-10 10:02:15,802][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:02:15,807][227910] Reward + Measures: [[-626.98803053    0.92935121    0.93127376    0.91294837    0.91121721]][0m
[37m[1m[2023-07-10 10:02:15,807][227910] Max Reward on eval: -626.988030525518[0m
[37m[1m[2023-07-10 10:02:15,808][227910] Min Reward on eval: -626.988030525518[0m
[37m[1m[2023-07-10 10:02:15,808][227910] Mean Reward across all agents: -626.988030525518[0m
[37m[1m[2023-07-10 10:02:15,808][227910] Average Trajectory Length: 991.914[0m
[36m[2023-07-10 10:02:20,711][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:02:20,712][227910] Reward + Measures: [[   4.51323378    0.76120001    0.73610002    0.72320002    0.68769997]
 [-137.06338953    0.73980004    0.73769999    0.69400001    0.70990002]
 [ -52.61600255    0.74960005    0.77039999    0.68640006    0.74160004]
 ...
 [-337.35470853    0.83939999    0.82120001    0.79799998    0.80719995]
 [-367.42769819    0.88210005    0.87130004    0.84610003    0.84250003]
 [-336.76854274    0.76020002    0.75710005    0.73170006    0.73720002]][0m
[37m[1m[2023-07-10 10:02:20,712][227910] Max Reward on eval: 498.17095665613886[0m
[37m[1m[2023-07-10 10:02:20,712][227910] Min Reward on eval: -652.2479277341743[0m
[37m[1m[2023-07-10 10:02:20,712][227910] Mean Reward across all agents: -343.52192229298686[0m
[37m[1m[2023-07-10 10:02:20,713][227910] Average Trajectory Length: 994.6546666666667[0m
[36m[2023-07-10 10:02:20,714][227910] mean_value=-361.8103362658578, max_value=848.5538089513778[0m
[37m[1m[2023-07-10 10:02:20,717][227910] New mean coefficients: [[-2.8127253   4.1059084   3.9710712   0.66685736  5.6050014 ]][0m
[37m[1m[2023-07-10 10:02:20,718][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:02:29,483][227910] train() took 8.76 seconds to complete[0m
[36m[2023-07-10 10:02:29,484][227910] FPS: 438159.96[0m
[36m[2023-07-10 10:02:29,486][227910] itr=100, itrs=2000, Progress: 5.00%[0m
[37m[1m[2023-07-10 10:02:31,386][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000080[0m
[36m[2023-07-10 10:02:43,848][227910] train() took 12.22 seconds to complete[0m
[36m[2023-07-10 10:02:43,848][227910] FPS: 314295.20[0m
[36m[2023-07-10 10:02:47,791][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:02:47,791][227910] Reward + Measures: [[-718.16996936    0.9318099     0.93385446    0.91734624    0.91474909]][0m
[37m[1m[2023-07-10 10:02:47,792][227910] Max Reward on eval: -718.1699693574533[0m
[37m[1m[2023-07-10 10:02:47,792][227910] Min Reward on eval: -718.1699693574533[0m
[37m[1m[2023-07-10 10:02:47,792][227910] Mean Reward across all agents: -718.1699693574533[0m
[37m[1m[2023-07-10 10:02:47,792][227910] Average Trajectory Length: 994.5263333333334[0m
[36m[2023-07-10 10:02:52,518][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:02:52,519][227910] Reward + Measures: [[-317.94609171    0.86210006    0.88020003    0.84450001    0.85390007]
 [ 103.80015037    0.71280003    0.77399999    0.65650004    0.7245    ]
 [ -98.56087473    0.82019997    0.83710003    0.79449999    0.79660004]
 ...
 [ 191.0612555     0.6467092     0.69537699    0.53481609    0.63941491]
 [ 294.04947803    0.52680004    0.63940001    0.4118        0.57450002]
 [ 203.61504657    0.54790002    0.61700004    0.44549999    0.56380004]][0m
[37m[1m[2023-07-10 10:02:52,519][227910] Max Reward on eval: 653.6446913616383[0m
[37m[1m[2023-07-10 10:02:52,520][227910] Min Reward on eval: -571.7061934161[0m
[37m[1m[2023-07-10 10:02:52,520][227910] Mean Reward across all agents: 67.08932656326698[0m
[37m[1m[2023-07-10 10:02:52,520][227910] Average Trajectory Length: 986.2716666666666[0m
[36m[2023-07-10 10:02:52,523][227910] mean_value=-114.02174923887448, max_value=980.8513030185946[0m
[37m[1m[2023-07-10 10:02:52,526][227910] New mean coefficients: [[-3.9589505  4.547216   4.9424405  0.832783   5.911969 ]][0m
[37m[1m[2023-07-10 10:02:52,526][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:03:01,256][227910] train() took 8.73 seconds to complete[0m
[36m[2023-07-10 10:03:01,256][227910] FPS: 439994.61[0m
[36m[2023-07-10 10:03:01,258][227910] itr=101, itrs=2000, Progress: 5.05%[0m
[36m[2023-07-10 10:03:13,380][227910] train() took 12.11 seconds to complete[0m
[36m[2023-07-10 10:03:13,380][227910] FPS: 317141.62[0m
[36m[2023-07-10 10:03:17,261][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:03:17,266][227910] Reward + Measures: [[-807.15173019    0.93520939    0.93513012    0.92005289    0.91797894]][0m
[37m[1m[2023-07-10 10:03:17,267][227910] Max Reward on eval: -807.1517301923004[0m
[37m[1m[2023-07-10 10:03:17,267][227910] Min Reward on eval: -807.1517301923004[0m
[37m[1m[2023-07-10 10:03:17,267][227910] Mean Reward across all agents: -807.1517301923004[0m
[37m[1m[2023-07-10 10:03:17,267][227910] Average Trajectory Length: 996.1236666666666[0m
[36m[2023-07-10 10:03:21,999][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:03:22,000][227910] Reward + Measures: [[ 267.43981804    0.68380004    0.71070004    0.55900002    0.64399999]
 [-107.26229866    0.77869999    0.77890009    0.73030001    0.74500006]
 [-482.71055084    0.83999997    0.83470005    0.82569999    0.79479998]
 ...
 [-481.08580145    0.88220006    0.88869995    0.85939997    0.85610002]
 [-442.01607383    0.89390004    0.89410001    0.87120003    0.86930007]
 [  49.72366968    0.64690006    0.6354        0.57629997    0.59569997]][0m
[37m[1m[2023-07-10 10:03:22,000][227910] Max Reward on eval: 635.7625429375679[0m
[37m[1m[2023-07-10 10:03:22,000][227910] Min Reward on eval: -750.0625119035133[0m
[37m[1m[2023-07-10 10:03:22,000][227910] Mean Reward across all agents: -103.20683344130777[0m
[37m[1m[2023-07-10 10:03:22,000][227910] Average Trajectory Length: 995.9983333333333[0m
[36m[2023-07-10 10:03:22,003][227910] mean_value=-192.65509148125483, max_value=739.0944522699679[0m
[37m[1m[2023-07-10 10:03:22,005][227910] New mean coefficients: [[-4.3756366   4.829485    6.8244452   0.53608507  6.102028  ]][0m
[37m[1m[2023-07-10 10:03:22,006][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:03:30,753][227910] train() took 8.75 seconds to complete[0m
[36m[2023-07-10 10:03:30,754][227910] FPS: 439091.46[0m
[36m[2023-07-10 10:03:30,756][227910] itr=102, itrs=2000, Progress: 5.10%[0m
[36m[2023-07-10 10:03:42,909][227910] train() took 12.14 seconds to complete[0m
[36m[2023-07-10 10:03:42,910][227910] FPS: 316299.65[0m
[36m[2023-07-10 10:03:46,845][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:03:46,845][227910] Reward + Measures: [[-841.28967837    0.93450552    0.93428481    0.91850924    0.91510653]][0m
[37m[1m[2023-07-10 10:03:46,845][227910] Max Reward on eval: -841.2896783683784[0m
[37m[1m[2023-07-10 10:03:46,845][227910] Min Reward on eval: -841.2896783683784[0m
[37m[1m[2023-07-10 10:03:46,846][227910] Mean Reward across all agents: -841.2896783683784[0m
[37m[1m[2023-07-10 10:03:46,846][227910] Average Trajectory Length: 994.135[0m
[36m[2023-07-10 10:03:51,525][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:03:51,525][227910] Reward + Measures: [[577.33618447   0.34470001   0.61449999   0.13039999   0.43210003]
 [235.28602023   0.59149998   0.71970004   0.52060002   0.5686    ]
 [170.86595276   0.57779998   0.65900004   0.49170002   0.54190004]
 ...
 [586.98513928   0.40540001   0.62579995   0.30490002   0.39509997]
 [436.84645489   0.28840002   0.6613       0.29889998   0.45919999]
 [684.66539424   0.33370003   0.55530006   0.20810001   0.36919999]][0m
[37m[1m[2023-07-10 10:03:51,526][227910] Max Reward on eval: 704.6503876867122[0m
[37m[1m[2023-07-10 10:03:51,526][227910] Min Reward on eval: -706.8884842969477[0m
[37m[1m[2023-07-10 10:03:51,526][227910] Mean Reward across all agents: 270.1811644599608[0m
[37m[1m[2023-07-10 10:03:51,526][227910] Average Trajectory Length: 999.1186666666666[0m
[36m[2023-07-10 10:03:51,537][227910] mean_value=343.3743101887422, max_value=1195.3682470916071[0m
[37m[1m[2023-07-10 10:03:51,540][227910] New mean coefficients: [[-4.6853127   4.9310617   8.225915    0.97605735  6.453455  ]][0m
[37m[1m[2023-07-10 10:03:51,542][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:04:00,322][227910] train() took 8.78 seconds to complete[0m
[36m[2023-07-10 10:04:00,322][227910] FPS: 437431.84[0m
[36m[2023-07-10 10:04:00,340][227910] itr=103, itrs=2000, Progress: 5.15%[0m
[36m[2023-07-10 10:04:12,511][227910] train() took 12.15 seconds to complete[0m
[36m[2023-07-10 10:04:12,511][227910] FPS: 315989.64[0m
[36m[2023-07-10 10:04:16,456][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:04:16,456][227910] Reward + Measures: [[-908.66761344    0.93580818    0.93763322    0.92120248    0.91909772]][0m
[37m[1m[2023-07-10 10:04:16,457][227910] Max Reward on eval: -908.6676134404164[0m
[37m[1m[2023-07-10 10:04:16,457][227910] Min Reward on eval: -908.6676134404164[0m
[37m[1m[2023-07-10 10:04:16,457][227910] Mean Reward across all agents: -908.6676134404164[0m
[37m[1m[2023-07-10 10:04:16,457][227910] Average Trajectory Length: 992.5066666666667[0m
[36m[2023-07-10 10:04:21,161][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:04:21,161][227910] Reward + Measures: [[-229.95088781    0.51840001    0.59250003    0.48400003    0.55559999]
 [   8.17654577    0.42560002    0.56370002    0.42339998    0.46419999]
 [-944.52279495    0.83539993    0.86440003    0.79330003    0.81729996]
 ...
 [-686.08444641    0.79482377    0.7946831     0.73430675    0.76063222]
 [ 147.9180039     0.34999999    0.50090003    0.3497        0.39000002]
 [  -9.59619802    0.4869        0.5302        0.40640002    0.46140003]][0m
[37m[1m[2023-07-10 10:04:21,162][227910] Max Reward on eval: 368.69789530467244[0m
[37m[1m[2023-07-10 10:04:21,162][227910] Min Reward on eval: -944.5227949518361[0m
[37m[1m[2023-07-10 10:04:21,162][227910] Mean Reward across all agents: -258.5595788727851[0m
[37m[1m[2023-07-10 10:04:21,162][227910] Average Trajectory Length: 979.6663333333333[0m
[36m[2023-07-10 10:04:21,165][227910] mean_value=-318.5013779457996, max_value=689.0837730775762[0m
[37m[1m[2023-07-10 10:04:21,168][227910] New mean coefficients: [[-5.353248    5.753147   10.048815    0.63557917  7.2281914 ]][0m
[37m[1m[2023-07-10 10:04:21,168][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:04:29,889][227910] train() took 8.72 seconds to complete[0m
[36m[2023-07-10 10:04:29,889][227910] FPS: 440447.38[0m
[36m[2023-07-10 10:04:29,891][227910] itr=104, itrs=2000, Progress: 5.20%[0m
[36m[2023-07-10 10:04:42,140][227910] train() took 12.24 seconds to complete[0m
[36m[2023-07-10 10:04:42,140][227910] FPS: 313851.43[0m
[36m[2023-07-10 10:04:46,107][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:04:46,112][227910] Reward + Measures: [[-1000.60389313     0.9355616      0.9379034      0.92083192
      0.91998935]][0m
[37m[1m[2023-07-10 10:04:46,112][227910] Max Reward on eval: -1000.6038931332175[0m
[37m[1m[2023-07-10 10:04:46,112][227910] Min Reward on eval: -1000.6038931332175[0m
[37m[1m[2023-07-10 10:04:46,113][227910] Mean Reward across all agents: -1000.6038931332175[0m
[37m[1m[2023-07-10 10:04:46,113][227910] Average Trajectory Length: 998.0463333333333[0m
[36m[2023-07-10 10:04:50,908][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:04:50,909][227910] Reward + Measures: [[244.63524385   0.39000002   0.50579995   0.31640002   0.3946    ]
 [317.83540339   0.36090001   0.4553       0.2897       0.3434    ]
 [139.10077925   0.40175319   0.42329904   0.31690016   0.3349863 ]
 ...
 [287.84940721   0.419        0.52679998   0.30970001   0.43370005]
 [331.18518868   0.46949998   0.54660004   0.3087       0.47000003]
 [154.5744206    0.29279998   0.3594       0.24230002   0.26809999]][0m
[37m[1m[2023-07-10 10:04:50,909][227910] Max Reward on eval: 492.5724342250731[0m
[37m[1m[2023-07-10 10:04:50,909][227910] Min Reward on eval: -746.8957131866831[0m
[37m[1m[2023-07-10 10:04:50,909][227910] Mean Reward across all agents: 208.78417080461693[0m
[37m[1m[2023-07-10 10:04:50,910][227910] Average Trajectory Length: 982.9333333333333[0m
[36m[2023-07-10 10:04:50,912][227910] mean_value=-270.0742878707933, max_value=922.8258996251726[0m
[37m[1m[2023-07-10 10:04:50,915][227910] New mean coefficients: [[-6.683322   5.275386  10.245342   1.0908326  7.031814 ]][0m
[37m[1m[2023-07-10 10:04:50,916][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:04:59,805][227910] train() took 8.89 seconds to complete[0m
[36m[2023-07-10 10:04:59,805][227910] FPS: 432038.08[0m
[36m[2023-07-10 10:04:59,808][227910] itr=105, itrs=2000, Progress: 5.25%[0m
[36m[2023-07-10 10:05:12,010][227910] train() took 12.19 seconds to complete[0m
[36m[2023-07-10 10:05:12,010][227910] FPS: 315058.77[0m
[36m[2023-07-10 10:05:15,978][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:05:15,983][227910] Reward + Measures: [[-1028.47330349     0.93649852     0.93907672     0.92085677
      0.92134959]][0m
[37m[1m[2023-07-10 10:05:15,983][227910] Max Reward on eval: -1028.4733034936055[0m
[37m[1m[2023-07-10 10:05:15,984][227910] Min Reward on eval: -1028.4733034936055[0m
[37m[1m[2023-07-10 10:05:15,984][227910] Mean Reward across all agents: -1028.4733034936055[0m
[37m[1m[2023-07-10 10:05:15,984][227910] Average Trajectory Length: 998.1936666666667[0m
[36m[2023-07-10 10:05:20,600][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:05:20,601][227910] Reward + Measures: [[633.75782576   0.40910092   0.47879919   0.26190552   0.31090498]
 [476.06819313   0.3863       0.49650002   0.2158       0.34620002]
 [344.53439458   0.32361877   0.39487198   0.22758983   0.25226516]
 ...
 [546.22046419   0.33860001   0.5212       0.2077       0.37999997]
 [738.82129036   0.40189996   0.51960003   0.30350003   0.35160002]
 [335.35758882   0.3795       0.50470001   0.2142       0.3707    ]][0m
[37m[1m[2023-07-10 10:05:20,601][227910] Max Reward on eval: 738.8212903585751[0m
[37m[1m[2023-07-10 10:05:20,601][227910] Min Reward on eval: -30.75931684060488[0m
[37m[1m[2023-07-10 10:05:20,601][227910] Mean Reward across all agents: 385.049628653058[0m
[37m[1m[2023-07-10 10:05:20,602][227910] Average Trajectory Length: 962.2316666666667[0m
[36m[2023-07-10 10:05:20,605][227910] mean_value=-344.2540012407404, max_value=750.3655713940389[0m
[37m[1m[2023-07-10 10:05:20,607][227910] New mean coefficients: [[-5.043097   4.721048   8.983297   0.7356927  6.427778 ]][0m
[37m[1m[2023-07-10 10:05:20,608][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:05:29,333][227910] train() took 8.72 seconds to complete[0m
[36m[2023-07-10 10:05:29,334][227910] FPS: 440194.42[0m
[36m[2023-07-10 10:05:29,336][227910] itr=106, itrs=2000, Progress: 5.30%[0m
[36m[2023-07-10 10:05:41,489][227910] train() took 12.14 seconds to complete[0m
[36m[2023-07-10 10:05:41,490][227910] FPS: 316428.95[0m
[36m[2023-07-10 10:05:45,541][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:05:45,541][227910] Reward + Measures: [[-1147.1876141      0.94245964     0.94348866     0.92657471
      0.92631102]][0m
[37m[1m[2023-07-10 10:05:45,541][227910] Max Reward on eval: -1147.187614102315[0m
[37m[1m[2023-07-10 10:05:45,542][227910] Min Reward on eval: -1147.187614102315[0m
[37m[1m[2023-07-10 10:05:45,542][227910] Mean Reward across all agents: -1147.187614102315[0m
[37m[1m[2023-07-10 10:05:45,542][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:05:50,341][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:05:50,342][227910] Reward + Measures: [[ 119.1863167     0.52819997    0.64080006    0.42589998    0.55370003]
 [  77.99283353    0.51789999    0.5909        0.42300001    0.52860004]
 [ 127.1131919     0.59250003    0.67640007    0.4522        0.58640003]
 ...
 [-421.76904643    0.8125        0.8373        0.77279997    0.78879994]
 [  -8.97256482    0.46489999    0.61449999    0.3739        0.5327    ]
 [  81.65374876    0.36549997    0.47130004    0.29790002    0.398     ]][0m
[37m[1m[2023-07-10 10:05:50,342][227910] Max Reward on eval: 452.9434444539191[0m
[37m[1m[2023-07-10 10:05:50,342][227910] Min Reward on eval: -956.2966477883049[0m
[37m[1m[2023-07-10 10:05:50,343][227910] Mean Reward across all agents: -161.56087996990672[0m
[37m[1m[2023-07-10 10:05:50,343][227910] Average Trajectory Length: 995.034[0m
[36m[2023-07-10 10:05:50,345][227910] mean_value=-382.4895933176241, max_value=548.0613659558469[0m
[37m[1m[2023-07-10 10:05:50,347][227910] New mean coefficients: [[-4.6764984   4.645198   10.725244    0.02111822  6.83828   ]][0m
[37m[1m[2023-07-10 10:05:50,348][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:05:59,100][227910] train() took 8.75 seconds to complete[0m
[36m[2023-07-10 10:05:59,101][227910] FPS: 438836.88[0m
[36m[2023-07-10 10:05:59,103][227910] itr=107, itrs=2000, Progress: 5.35%[0m
[36m[2023-07-10 10:06:11,179][227910] train() took 12.06 seconds to complete[0m
[36m[2023-07-10 10:06:11,179][227910] FPS: 318392.02[0m
[36m[2023-07-10 10:06:14,989][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:06:14,990][227910] Reward + Measures: [[-1269.50227568     0.94066465     0.94236791     0.9259004
      0.92434603]][0m
[37m[1m[2023-07-10 10:06:14,990][227910] Max Reward on eval: -1269.5022756796066[0m
[37m[1m[2023-07-10 10:06:14,990][227910] Min Reward on eval: -1269.5022756796066[0m
[37m[1m[2023-07-10 10:06:14,990][227910] Mean Reward across all agents: -1269.5022756796066[0m
[37m[1m[2023-07-10 10:06:14,991][227910] Average Trajectory Length: 999.0273333333333[0m
[36m[2023-07-10 10:06:19,603][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:06:19,604][227910] Reward + Measures: [[ 233.84246771    0.26519999    0.48009998    0.2016        0.38040003]
 [-267.29792605    0.84180003    0.83149999    0.81849998    0.80019999]
 [ 186.28203246    0.46419999    0.68120003    0.35869998    0.58770007]
 ...
 [  92.36360377    0.36409998    0.53030002    0.28440002    0.46360001]
 [-249.57405633    0.73970002    0.73970002    0.70069999    0.70050001]
 [ 183.80665865    0.3143        0.50749999    0.2045        0.39059997]][0m
[37m[1m[2023-07-10 10:06:19,604][227910] Max Reward on eval: 496.37934589007637[0m
[37m[1m[2023-07-10 10:06:19,605][227910] Min Reward on eval: -793.6689670858789[0m
[37m[1m[2023-07-10 10:06:19,605][227910] Mean Reward across all agents: 41.66025802287647[0m
[37m[1m[2023-07-10 10:06:19,605][227910] Average Trajectory Length: 997.3206666666666[0m
[36m[2023-07-10 10:06:19,608][227910] mean_value=-132.6378905888545, max_value=751.8655462489172[0m
[37m[1m[2023-07-10 10:06:19,611][227910] New mean coefficients: [[-5.0327168   4.038893   10.953551    0.15665674  7.8260674 ]][0m
[37m[1m[2023-07-10 10:06:19,612][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:06:28,175][227910] train() took 8.56 seconds to complete[0m
[36m[2023-07-10 10:06:28,175][227910] FPS: 448519.33[0m
[36m[2023-07-10 10:06:28,178][227910] itr=108, itrs=2000, Progress: 5.40%[0m
[36m[2023-07-10 10:06:40,129][227910] train() took 11.94 seconds to complete[0m
[36m[2023-07-10 10:06:40,129][227910] FPS: 321721.82[0m
[36m[2023-07-10 10:06:43,974][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:06:43,979][227910] Reward + Measures: [[-1325.51754676     0.94188327     0.9437443      0.92874396
      0.92718369]][0m
[37m[1m[2023-07-10 10:06:43,979][227910] Max Reward on eval: -1325.5175467564538[0m
[37m[1m[2023-07-10 10:06:43,979][227910] Min Reward on eval: -1325.5175467564538[0m
[37m[1m[2023-07-10 10:06:43,980][227910] Mean Reward across all agents: -1325.5175467564538[0m
[37m[1m[2023-07-10 10:06:43,980][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:06:48,749][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:06:48,750][227910] Reward + Measures: [[-283.85143334    0.62050003    0.74500006    0.54640001    0.69880003]
 [-709.80562363    0.81140006    0.83700007    0.79189998    0.79680008]
 [-552.55608553    0.75350004    0.82880002    0.73320001    0.76789999]
 ...
 [-372.77655975    0.69209999    0.77299994    0.65939999    0.72359997]
 [-234.0645329     0.63920003    0.65990001    0.62199998    0.60539997]
 [  33.93728496    0.4111        0.61500001    0.31920001    0.53250003]][0m
[37m[1m[2023-07-10 10:06:48,750][227910] Max Reward on eval: 191.1752612367971[0m
[37m[1m[2023-07-10 10:06:48,750][227910] Min Reward on eval: -1053.673110850947[0m
[37m[1m[2023-07-10 10:06:48,750][227910] Mean Reward across all agents: -367.6827849750242[0m
[37m[1m[2023-07-10 10:06:48,751][227910] Average Trajectory Length: 995.6556666666667[0m
[36m[2023-07-10 10:06:48,752][227910] mean_value=-488.9649429663458, max_value=642.0474483754602[0m
[37m[1m[2023-07-10 10:06:48,755][227910] New mean coefficients: [[-5.490316   3.3525214 12.678447   0.433456   8.097807 ]][0m
[37m[1m[2023-07-10 10:06:48,756][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:06:57,379][227910] train() took 8.62 seconds to complete[0m
[36m[2023-07-10 10:06:57,379][227910] FPS: 445411.21[0m
[36m[2023-07-10 10:06:57,381][227910] itr=109, itrs=2000, Progress: 5.45%[0m
[36m[2023-07-10 10:07:09,122][227910] train() took 11.73 seconds to complete[0m
[36m[2023-07-10 10:07:09,122][227910] FPS: 327458.30[0m
[36m[2023-07-10 10:07:13,041][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:07:13,041][227910] Reward + Measures: [[-1395.88384182     0.94453353     0.94652426     0.93193376
      0.93010008]][0m
[37m[1m[2023-07-10 10:07:13,041][227910] Max Reward on eval: -1395.88384182147[0m
[37m[1m[2023-07-10 10:07:13,042][227910] Min Reward on eval: -1395.88384182147[0m
[37m[1m[2023-07-10 10:07:13,042][227910] Mean Reward across all agents: -1395.88384182147[0m
[37m[1m[2023-07-10 10:07:13,042][227910] Average Trajectory Length: 999.0313333333334[0m
[36m[2023-07-10 10:07:17,741][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:07:17,742][227910] Reward + Measures: [[  -0.56535567    0.1952375     0.37320539    0.17338572    0.22566071]
 [  79.87457146    0.3098        0.5309        0.26890001    0.39540002]
 [  41.18529382    0.22449999    0.49300003    0.1877        0.32670003]
 ...
 [-331.83892569    0.67809999    0.7446        0.62930006    0.68330002]
 [-134.1937917     0.20090787    0.44531831    0.2219642     0.31702447]
 [ 246.9600855     0.19090001    0.46359998    0.18970001    0.31679997]][0m
[37m[1m[2023-07-10 10:07:17,742][227910] Max Reward on eval: 360.2275868685509[0m
[37m[1m[2023-07-10 10:07:17,742][227910] Min Reward on eval: -910.7714159010909[0m
[37m[1m[2023-07-10 10:07:17,743][227910] Mean Reward across all agents: 49.089826158582945[0m
[37m[1m[2023-07-10 10:07:17,743][227910] Average Trajectory Length: 958.9943333333333[0m
[36m[2023-07-10 10:07:17,745][227910] mean_value=-416.27365767685694, max_value=769.8167110988172[0m
[37m[1m[2023-07-10 10:07:17,748][227910] New mean coefficients: [[-5.490885   3.2820687 11.349373   1.3177314  7.9936233]][0m
[37m[1m[2023-07-10 10:07:17,749][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:07:26,257][227910] train() took 8.51 seconds to complete[0m
[36m[2023-07-10 10:07:26,262][227910] FPS: 451404.95[0m
[36m[2023-07-10 10:07:26,265][227910] itr=110, itrs=2000, Progress: 5.50%[0m
[37m[1m[2023-07-10 10:07:28,055][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000090[0m
[36m[2023-07-10 10:07:40,428][227910] train() took 12.14 seconds to complete[0m
[36m[2023-07-10 10:07:40,428][227910] FPS: 316372.76[0m
[36m[2023-07-10 10:07:44,326][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:07:44,327][227910] Reward + Measures: [[-1459.10731812     0.94997299     0.9485693      0.93494034
      0.93112367]][0m
[37m[1m[2023-07-10 10:07:44,327][227910] Max Reward on eval: -1459.1073181181396[0m
[37m[1m[2023-07-10 10:07:44,327][227910] Min Reward on eval: -1459.1073181181396[0m
[37m[1m[2023-07-10 10:07:44,327][227910] Mean Reward across all agents: -1459.1073181181396[0m
[37m[1m[2023-07-10 10:07:44,327][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:07:49,023][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:07:49,024][227910] Reward + Measures: [[-93.68358641   0.55290002   0.67939997   0.1402       0.61700004]
 [114.73106781   0.47851858   0.61356258   0.15419382   0.47817451]
 [260.05914536   0.34110001   0.56639999   0.1585       0.42490003]
 ...
 [ 81.1450353    0.34713989   0.55123377   0.1724724    0.42827484]
 [289.67068553   0.34800002   0.57450002   0.22880001   0.43490002]
 [191.78686249   0.2872       0.51500005   0.17739999   0.37480003]][0m
[37m[1m[2023-07-10 10:07:49,024][227910] Max Reward on eval: 569.2346785245696[0m
[37m[1m[2023-07-10 10:07:49,024][227910] Min Reward on eval: -96.74510697451187[0m
[37m[1m[2023-07-10 10:07:49,024][227910] Mean Reward across all agents: 188.49985854493247[0m
[37m[1m[2023-07-10 10:07:49,025][227910] Average Trajectory Length: 976.125[0m
[36m[2023-07-10 10:07:49,029][227910] mean_value=-98.71441019203903, max_value=828.0576915629441[0m
[37m[1m[2023-07-10 10:07:49,032][227910] New mean coefficients: [[-4.397095    3.765511   10.381978    0.68067497  7.8754325 ]][0m
[37m[1m[2023-07-10 10:07:49,033][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:07:57,578][227910] train() took 8.54 seconds to complete[0m
[36m[2023-07-10 10:07:57,578][227910] FPS: 449457.40[0m
[36m[2023-07-10 10:07:57,581][227910] itr=111, itrs=2000, Progress: 5.55%[0m
[36m[2023-07-10 10:08:09,587][227910] train() took 11.99 seconds to complete[0m
[36m[2023-07-10 10:08:09,588][227910] FPS: 320263.34[0m
[36m[2023-07-10 10:08:13,480][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:08:13,481][227910] Reward + Measures: [[-1522.8821595      0.94971973     0.94913131     0.93525469
      0.93180466]][0m
[37m[1m[2023-07-10 10:08:13,481][227910] Max Reward on eval: -1522.882159501506[0m
[37m[1m[2023-07-10 10:08:13,481][227910] Min Reward on eval: -1522.882159501506[0m
[37m[1m[2023-07-10 10:08:13,481][227910] Mean Reward across all agents: -1522.882159501506[0m
[37m[1m[2023-07-10 10:08:13,482][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:08:18,117][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:08:18,118][227910] Reward + Measures: [[ 265.8945445     0.15640001    0.70479995    0.2863        0.62230003]
 [ 299.85667772    0.29481396    0.49024183    0.29084563    0.28970826]
 [ 342.14210907    0.21570002    0.4244        0.24270001    0.33980003]
 ...
 [-480.02769723    0.75890005    0.79319996    0.69510001    0.75419998]
 [ 326.57387683    0.23699999    0.48319998    0.2079        0.36809999]
 [ 230.28283812    0.24270001    0.54580003    0.22230001    0.41050002]][0m
[37m[1m[2023-07-10 10:08:18,118][227910] Max Reward on eval: 439.310586892406[0m
[37m[1m[2023-07-10 10:08:18,118][227910] Min Reward on eval: -692.8018530810834[0m
[37m[1m[2023-07-10 10:08:18,119][227910] Mean Reward across all agents: 242.0034899139138[0m
[37m[1m[2023-07-10 10:08:18,119][227910] Average Trajectory Length: 976.6443333333333[0m
[36m[2023-07-10 10:08:18,127][227910] mean_value=309.8835120112311, max_value=913.4854815580591[0m
[37m[1m[2023-07-10 10:08:18,130][227910] New mean coefficients: [[-3.0812075   3.529157   10.155793   -0.01441079  6.862924  ]][0m
[37m[1m[2023-07-10 10:08:18,131][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:08:26,730][227910] train() took 8.60 seconds to complete[0m
[36m[2023-07-10 10:08:26,730][227910] FPS: 446655.07[0m
[36m[2023-07-10 10:08:26,732][227910] itr=112, itrs=2000, Progress: 5.60%[0m
[36m[2023-07-10 10:08:38,747][227910] train() took 12.00 seconds to complete[0m
[36m[2023-07-10 10:08:38,747][227910] FPS: 320031.37[0m
[36m[2023-07-10 10:08:42,708][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:08:42,709][227910] Reward + Measures: [[-1527.25536826     0.94802082     0.94800192     0.93325669
      0.93005866]][0m
[37m[1m[2023-07-10 10:08:42,709][227910] Max Reward on eval: -1527.2553682612572[0m
[37m[1m[2023-07-10 10:08:42,709][227910] Min Reward on eval: -1527.2553682612572[0m
[37m[1m[2023-07-10 10:08:42,709][227910] Mean Reward across all agents: -1527.2553682612572[0m
[37m[1m[2023-07-10 10:08:42,709][227910] Average Trajectory Length: 999.3553333333333[0m
[36m[2023-07-10 10:08:47,358][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:08:47,359][227910] Reward + Measures: [[-740.99047098    0.79360002    0.80070001    0.7471        0.76450002]
 [  93.21662695    0.35950002    0.52179998    0.3549        0.29110003]
 [-144.80226489    0.67970002    0.713         0.62560004    0.61840004]
 ...
 [ 364.09572208    0.28080004    0.43500003    0.37030002    0.22589998]
 [-185.25613759    0.6688        0.74469995    0.66580003    0.60729998]
 [ 216.54721452    0.55899996    0.61049998    0.5661        0.46490002]][0m
[37m[1m[2023-07-10 10:08:47,359][227910] Max Reward on eval: 656.6990739390719[0m
[37m[1m[2023-07-10 10:08:47,359][227910] Min Reward on eval: -879.034088506832[0m
[37m[1m[2023-07-10 10:08:47,360][227910] Mean Reward across all agents: -54.86894119292934[0m
[37m[1m[2023-07-10 10:08:47,360][227910] Average Trajectory Length: 995.5793333333334[0m
[36m[2023-07-10 10:08:47,365][227910] mean_value=-47.307782566504706, max_value=1106.3288359375788[0m
[37m[1m[2023-07-10 10:08:47,367][227910] New mean coefficients: [[-2.874771    3.7013776  12.383875   -0.66728026  6.8208156 ]][0m
[37m[1m[2023-07-10 10:08:47,369][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:08:55,967][227910] train() took 8.60 seconds to complete[0m
[36m[2023-07-10 10:08:55,967][227910] FPS: 446673.05[0m
[36m[2023-07-10 10:08:55,970][227910] itr=113, itrs=2000, Progress: 5.65%[0m
[36m[2023-07-10 10:09:07,911][227910] train() took 11.93 seconds to complete[0m
[36m[2023-07-10 10:09:07,912][227910] FPS: 321942.67[0m
[36m[2023-07-10 10:09:11,855][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:09:11,856][227910] Reward + Measures: [[-1560.73683716     0.94819272     0.94722134     0.93150991
      0.92831498]][0m
[37m[1m[2023-07-10 10:09:11,856][227910] Max Reward on eval: -1560.7368371605942[0m
[37m[1m[2023-07-10 10:09:11,856][227910] Min Reward on eval: -1560.7368371605942[0m
[37m[1m[2023-07-10 10:09:11,857][227910] Mean Reward across all agents: -1560.7368371605942[0m
[37m[1m[2023-07-10 10:09:11,857][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:09:16,667][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:09:16,668][227910] Reward + Measures: [[413.56486839   0.24833031   0.48780304   0.23044848   0.34741819]
 [293.38062885   0.1962       0.40279999   0.2132       0.28819999]
 [521.93580161   0.23699999   0.53109998   0.24560001   0.33790001]
 ...
 [385.71469604   0.2325       0.47690001   0.2332       0.36489999]
 [451.99848767   0.2491       0.49720001   0.2701       0.32640001]
 [487.84519167   0.27850002   0.49799997   0.28010002   0.33769998]][0m
[37m[1m[2023-07-10 10:09:16,668][227910] Max Reward on eval: 705.3378825969819[0m
[37m[1m[2023-07-10 10:09:16,668][227910] Min Reward on eval: -902.557662006421[0m
[37m[1m[2023-07-10 10:09:16,668][227910] Mean Reward across all agents: 403.826932033983[0m
[37m[1m[2023-07-10 10:09:16,669][227910] Average Trajectory Length: 996.731[0m
[36m[2023-07-10 10:09:16,673][227910] mean_value=81.0591664883055, max_value=988.1361160703934[0m
[37m[1m[2023-07-10 10:09:16,675][227910] New mean coefficients: [[-2.3472404   2.107955   12.169293    0.23536283  6.472562  ]][0m
[37m[1m[2023-07-10 10:09:16,676][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:09:25,217][227910] train() took 8.54 seconds to complete[0m
[36m[2023-07-10 10:09:25,222][227910] FPS: 449677.78[0m
[36m[2023-07-10 10:09:25,225][227910] itr=114, itrs=2000, Progress: 5.70%[0m
[36m[2023-07-10 10:09:37,352][227910] train() took 12.11 seconds to complete[0m
[36m[2023-07-10 10:09:37,352][227910] FPS: 317033.54[0m
[36m[2023-07-10 10:09:41,303][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:09:41,303][227910] Reward + Measures: [[-1596.85687578     0.94941407     0.94953382     0.93537408
      0.93198508]][0m
[37m[1m[2023-07-10 10:09:41,303][227910] Max Reward on eval: -1596.856875782771[0m
[37m[1m[2023-07-10 10:09:41,303][227910] Min Reward on eval: -1596.856875782771[0m
[37m[1m[2023-07-10 10:09:41,304][227910] Mean Reward across all agents: -1596.856875782771[0m
[37m[1m[2023-07-10 10:09:41,304][227910] Average Trajectory Length: 999.6766666666666[0m
[36m[2023-07-10 10:09:45,987][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:09:45,988][227910] Reward + Measures: [[-657.95572587    0.63370001    0.67000002    0.60330003    0.58249998]
 [-275.2824815     0.62270004    0.66140002    0.5952        0.56800002]
 [-140.11606484    0.44682607    0.52105218    0.41466522    0.40440002]
 ...
 [-380.34470628    0.5109188     0.56157929    0.47688118    0.45785093]
 [-886.66996049    0.86420006    0.86660004    0.85290003    0.83999997]
 [-381.17325592    0.66720003    0.72000003    0.63619995    0.62150002]][0m
[37m[1m[2023-07-10 10:09:45,988][227910] Max Reward on eval: 322.6834167217603[0m
[37m[1m[2023-07-10 10:09:45,988][227910] Min Reward on eval: -1128.2823907016077[0m
[37m[1m[2023-07-10 10:09:45,989][227910] Mean Reward across all agents: -370.6728144756266[0m
[37m[1m[2023-07-10 10:09:45,989][227910] Average Trajectory Length: 972.9309999999999[0m
[36m[2023-07-10 10:09:45,990][227910] mean_value=-499.028293993949, max_value=176.58670590143407[0m
[37m[1m[2023-07-10 10:09:45,993][227910] New mean coefficients: [[-2.4726095   3.2969608  12.945429   -0.12169027  6.7759514 ]][0m
[37m[1m[2023-07-10 10:09:45,994][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:09:54,685][227910] train() took 8.69 seconds to complete[0m
[36m[2023-07-10 10:09:54,686][227910] FPS: 441889.48[0m
[36m[2023-07-10 10:09:54,688][227910] itr=115, itrs=2000, Progress: 5.75%[0m
[36m[2023-07-10 10:10:06,629][227910] train() took 11.93 seconds to complete[0m
[36m[2023-07-10 10:10:06,630][227910] FPS: 321971.72[0m
[36m[2023-07-10 10:10:10,508][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:10:10,508][227910] Reward + Measures: [[-1574.83796151     0.95038956     0.94925845     0.93531919
      0.9314028 ]][0m
[37m[1m[2023-07-10 10:10:10,508][227910] Max Reward on eval: -1574.8379615126457[0m
[37m[1m[2023-07-10 10:10:10,509][227910] Min Reward on eval: -1574.8379615126457[0m
[37m[1m[2023-07-10 10:10:10,509][227910] Mean Reward across all agents: -1574.8379615126457[0m
[37m[1m[2023-07-10 10:10:10,509][227910] Average Trajectory Length: 999.0319999999999[0m
[36m[2023-07-10 10:10:15,111][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:10:15,111][227910] Reward + Measures: [[   45.95095876     0.53450006     0.63679999     0.49560004
      0.47960001]
 [  131.95564324     0.4052         0.50950003     0.4226
      0.3576    ]
 [ -212.60867468     0.5589         0.67819995     0.55579996
      0.56639999]
 ...
 [ -325.4241018      0.6354         0.72609997     0.5891
      0.59680003]
 [-1072.12988631     0.88880008     0.90249997     0.84279996
      0.85200006]
 [  238.63356381     0.39317146     0.53323334     0.38878098
      0.38272381]][0m
[37m[1m[2023-07-10 10:10:15,111][227910] Max Reward on eval: 854.5144376260112[0m
[37m[1m[2023-07-10 10:10:15,112][227910] Min Reward on eval: -1095.0745475766016[0m
[37m[1m[2023-07-10 10:10:15,112][227910] Mean Reward across all agents: 96.14108510613649[0m
[37m[1m[2023-07-10 10:10:15,112][227910] Average Trajectory Length: 995.1966666666666[0m
[36m[2023-07-10 10:10:15,116][227910] mean_value=-22.374923132059727, max_value=830.608735608522[0m
[37m[1m[2023-07-10 10:10:15,119][227910] New mean coefficients: [[-0.59265363  2.8179324  13.549603   -0.8108833   5.9461327 ]][0m
[37m[1m[2023-07-10 10:10:15,120][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:10:23,642][227910] train() took 8.52 seconds to complete[0m
[36m[2023-07-10 10:10:23,642][227910] FPS: 450712.02[0m
[36m[2023-07-10 10:10:23,645][227910] itr=116, itrs=2000, Progress: 5.80%[0m
[36m[2023-07-10 10:10:35,604][227910] train() took 11.94 seconds to complete[0m
[36m[2023-07-10 10:10:35,604][227910] FPS: 321505.58[0m
[36m[2023-07-10 10:10:39,453][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:10:39,453][227910] Reward + Measures: [[-1593.86529395     0.94878834     0.94772035     0.93222165
      0.92873436]][0m
[37m[1m[2023-07-10 10:10:39,453][227910] Max Reward on eval: -1593.86529394632[0m
[37m[1m[2023-07-10 10:10:39,454][227910] Min Reward on eval: -1593.86529394632[0m
[37m[1m[2023-07-10 10:10:39,454][227910] Mean Reward across all agents: -1593.86529394632[0m
[37m[1m[2023-07-10 10:10:39,454][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:10:44,107][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:10:44,108][227910] Reward + Measures: [[ 193.89907926    0.47909999    0.60079998    0.28200001    0.53210002]
 [ 311.09765463    0.47389999    0.61750001    0.29449999    0.51400006]
 [-158.76757032    0.56040001    0.72850001    0.35349998    0.66949999]
 ...
 [  80.00616586    0.51194549    0.7000134     0.27923241    0.61573571]
 [-347.20854407    0.6983        0.81650001    0.52740002    0.74030006]
 [ 117.76433362    0.53290004    0.69810003    0.28590003    0.62760001]][0m
[37m[1m[2023-07-10 10:10:44,108][227910] Max Reward on eval: 551.6061199781805[0m
[37m[1m[2023-07-10 10:10:44,108][227910] Min Reward on eval: -1031.0049557304592[0m
[37m[1m[2023-07-10 10:10:44,108][227910] Mean Reward across all agents: 68.78601850560092[0m
[37m[1m[2023-07-10 10:10:44,109][227910] Average Trajectory Length: 982.1606666666667[0m
[36m[2023-07-10 10:10:44,111][227910] mean_value=-296.726540601192, max_value=511.18181751791104[0m
[37m[1m[2023-07-10 10:10:44,113][227910] New mean coefficients: [[-0.45027298  2.8294597  13.557493   -0.52702796  6.1562    ]][0m
[37m[1m[2023-07-10 10:10:44,114][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:10:52,671][227910] train() took 8.56 seconds to complete[0m
[36m[2023-07-10 10:10:52,671][227910] FPS: 448851.87[0m
[36m[2023-07-10 10:10:52,674][227910] itr=117, itrs=2000, Progress: 5.85%[0m
[36m[2023-07-10 10:11:04,765][227910] train() took 12.08 seconds to complete[0m
[36m[2023-07-10 10:11:04,765][227910] FPS: 317979.23[0m
[36m[2023-07-10 10:11:08,647][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:11:08,648][227910] Reward + Measures: [[-1618.16936016     0.95056248     0.94938004     0.93504834
      0.93121713]][0m
[37m[1m[2023-07-10 10:11:08,648][227910] Max Reward on eval: -1618.1693601621862[0m
[37m[1m[2023-07-10 10:11:08,648][227910] Min Reward on eval: -1618.1693601621862[0m
[37m[1m[2023-07-10 10:11:08,648][227910] Mean Reward across all agents: -1618.1693601621862[0m
[37m[1m[2023-07-10 10:11:08,648][227910] Average Trajectory Length: 999.678[0m
[36m[2023-07-10 10:11:13,310][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:11:13,310][227910] Reward + Measures: [[ -19.6253082     0.347         0.45110002    0.33919999    0.3628    ]
 [-333.65782219    0.54479998    0.59499997    0.51949996    0.56440002]
 [  10.43712811    0.35159999    0.48460004    0.3222        0.39789999]
 ...
 [-166.21138598    0.30952242    0.36595502    0.28059933    0.27991581]
 [-112.04082568    0.3818613     0.49001613    0.38301292    0.40281293]
 [ -78.43756756    0.3175        0.4021        0.3048        0.3145    ]][0m
[37m[1m[2023-07-10 10:11:13,311][227910] Max Reward on eval: 360.25571053002494[0m
[37m[1m[2023-07-10 10:11:13,311][227910] Min Reward on eval: -1140.2645162515807[0m
[37m[1m[2023-07-10 10:11:13,311][227910] Mean Reward across all agents: -218.33091239917394[0m
[37m[1m[2023-07-10 10:11:13,311][227910] Average Trajectory Length: 959.967[0m
[36m[2023-07-10 10:11:13,315][227910] mean_value=-277.6375806607517, max_value=590.8616280157833[0m
[37m[1m[2023-07-10 10:11:13,318][227910] New mean coefficients: [[-1.1035552   4.1088586  12.8430395  -0.30032495  7.0067353 ]][0m
[37m[1m[2023-07-10 10:11:13,319][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:11:21,957][227910] train() took 8.64 seconds to complete[0m
[36m[2023-07-10 10:11:21,957][227910] FPS: 444604.60[0m
[36m[2023-07-10 10:11:21,960][227910] itr=118, itrs=2000, Progress: 5.90%[0m
[36m[2023-07-10 10:11:34,073][227910] train() took 12.10 seconds to complete[0m
[36m[2023-07-10 10:11:34,073][227910] FPS: 317439.24[0m
[36m[2023-07-10 10:11:38,124][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:11:38,124][227910] Reward + Measures: [[-1623.49794259     0.95279765     0.95286602     0.93881935
      0.93575132]][0m
[37m[1m[2023-07-10 10:11:38,124][227910] Max Reward on eval: -1623.497942591597[0m
[37m[1m[2023-07-10 10:11:38,124][227910] Min Reward on eval: -1623.497942591597[0m
[37m[1m[2023-07-10 10:11:38,125][227910] Mean Reward across all agents: -1623.497942591597[0m
[37m[1m[2023-07-10 10:11:38,125][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:11:42,727][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:11:42,728][227910] Reward + Measures: [[  16.96766949    0.5316        0.69800007    0.1886        0.67619997]
 [-244.75013113    0.60800004    0.6663        0.50859994    0.6196    ]
 [ 221.50317924    0.42829999    0.54119998    0.22450002    0.49860001]
 ...
 [ 126.37773505    0.42430001    0.60470003    0.33039999    0.58280003]
 [ -19.28977       0.36100003    0.4357        0.1919        0.37450001]
 [ 144.17169864    0.40723178    0.55535871    0.21988253    0.51181591]][0m
[37m[1m[2023-07-10 10:11:42,728][227910] Max Reward on eval: 391.3020638428512[0m
[37m[1m[2023-07-10 10:11:42,729][227910] Min Reward on eval: -565.7792558966554[0m
[37m[1m[2023-07-10 10:11:42,729][227910] Mean Reward across all agents: 82.76397551165384[0m
[37m[1m[2023-07-10 10:11:42,729][227910] Average Trajectory Length: 994.4069999999999[0m
[36m[2023-07-10 10:11:42,733][227910] mean_value=-19.12813261342281, max_value=708.9610250126716[0m
[37m[1m[2023-07-10 10:11:42,736][227910] New mean coefficients: [[-1.9414105  3.8895535 11.801355   0.3066658  7.6359243]][0m
[37m[1m[2023-07-10 10:11:42,737][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:11:51,217][227910] train() took 8.48 seconds to complete[0m
[36m[2023-07-10 10:11:51,218][227910] FPS: 452906.79[0m
[36m[2023-07-10 10:11:51,220][227910] itr=119, itrs=2000, Progress: 5.95%[0m
[36m[2023-07-10 10:12:03,141][227910] train() took 11.90 seconds to complete[0m
[36m[2023-07-10 10:12:03,141][227910] FPS: 322568.99[0m
[36m[2023-07-10 10:12:06,973][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:12:06,973][227910] Reward + Measures: [[-1640.4206582      0.95420629     0.95377797     0.93817627
      0.9351154 ]][0m
[37m[1m[2023-07-10 10:12:06,973][227910] Max Reward on eval: -1640.4206582024278[0m
[37m[1m[2023-07-10 10:12:06,974][227910] Min Reward on eval: -1640.4206582024278[0m
[37m[1m[2023-07-10 10:12:06,974][227910] Mean Reward across all agents: -1640.4206582024278[0m
[37m[1m[2023-07-10 10:12:06,974][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:12:11,529][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:12:11,530][227910] Reward + Measures: [[764.87052159   0.2332       0.40219998   0.35929999   0.22319999]
 [384.87524931   0.39379999   0.48969999   0.2247       0.36770001]
 [424.94433768   0.31660005   0.373        0.34150001   0.16580002]
 ...
 [658.19746723   0.34330001   0.38260001   0.3874       0.1728    ]
 [595.17343398   0.29980001   0.41060001   0.26100001   0.28140002]
 [772.70734184   0.29370001   0.40990004   0.38460001   0.19770001]][0m
[37m[1m[2023-07-10 10:12:11,530][227910] Max Reward on eval: 1109.5481412326335[0m
[37m[1m[2023-07-10 10:12:11,531][227910] Min Reward on eval: -366.90817303726215[0m
[37m[1m[2023-07-10 10:12:11,531][227910] Mean Reward across all agents: 599.4682135310217[0m
[37m[1m[2023-07-10 10:12:11,531][227910] Average Trajectory Length: 996.7393333333333[0m
[36m[2023-07-10 10:12:11,536][227910] mean_value=226.67965773230216, max_value=1351.7273624719571[0m
[37m[1m[2023-07-10 10:12:11,539][227910] New mean coefficients: [[-1.9036769   2.4615922  13.709614    0.32153708  6.479911  ]][0m
[37m[1m[2023-07-10 10:12:11,540][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:12:20,031][227910] train() took 8.49 seconds to complete[0m
[36m[2023-07-10 10:12:20,031][227910] FPS: 452342.47[0m
[36m[2023-07-10 10:12:20,033][227910] itr=120, itrs=2000, Progress: 6.00%[0m
[37m[1m[2023-07-10 10:12:21,863][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000100[0m
[36m[2023-07-10 10:12:34,334][227910] train() took 12.22 seconds to complete[0m
[36m[2023-07-10 10:12:34,334][227910] FPS: 314245.98[0m
[36m[2023-07-10 10:12:38,192][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:12:38,193][227910] Reward + Measures: [[-1687.91153332     0.95183104     0.95202398     0.93642366
      0.93255633]][0m
[37m[1m[2023-07-10 10:12:38,193][227910] Max Reward on eval: -1687.9115333180234[0m
[37m[1m[2023-07-10 10:12:38,193][227910] Min Reward on eval: -1687.9115333180234[0m
[37m[1m[2023-07-10 10:12:38,194][227910] Mean Reward across all agents: -1687.9115333180234[0m
[37m[1m[2023-07-10 10:12:38,194][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:12:42,889][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:12:42,889][227910] Reward + Measures: [[ 169.77680594    0.44479999    0.57170004    0.3026        0.48179999]
 [-107.07124204    0.68489999    0.79280001    0.34440002    0.78579998]
 [ 124.85992515    0.301         0.4061        0.23290001    0.3242    ]
 ...
 [ 572.71440497    0.46470004    0.56560004    0.28779998    0.40840003]
 [ 344.69526809    0.34279999    0.57690001    0.2349        0.45170003]
 [ 206.65249772    0.55849999    0.69920009    0.2818        0.62900001]][0m
[37m[1m[2023-07-10 10:12:42,890][227910] Max Reward on eval: 572.7144049744471[0m
[37m[1m[2023-07-10 10:12:42,890][227910] Min Reward on eval: -469.55136507283896[0m
[37m[1m[2023-07-10 10:12:42,890][227910] Mean Reward across all agents: 178.59430326187507[0m
[37m[1m[2023-07-10 10:12:42,890][227910] Average Trajectory Length: 997.0933333333332[0m
[36m[2023-07-10 10:12:42,894][227910] mean_value=-78.17835747828859, max_value=687.2468546138319[0m
[37m[1m[2023-07-10 10:12:42,896][227910] New mean coefficients: [[-1.5580453  1.5108794 13.321194   1.1749878  6.7424083]][0m
[37m[1m[2023-07-10 10:12:42,897][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:12:51,480][227910] train() took 8.58 seconds to complete[0m
[36m[2023-07-10 10:12:51,480][227910] FPS: 447529.78[0m
[36m[2023-07-10 10:12:51,482][227910] itr=121, itrs=2000, Progress: 6.05%[0m
[36m[2023-07-10 10:13:03,863][227910] train() took 12.36 seconds to complete[0m
[36m[2023-07-10 10:13:03,863][227910] FPS: 310564.35[0m
[36m[2023-07-10 10:13:08,145][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:13:08,145][227910] Reward + Measures: [[-1693.81971334     0.94862449     0.949076       0.93094939
      0.92732126]][0m
[37m[1m[2023-07-10 10:13:08,145][227910] Max Reward on eval: -1693.8197133427616[0m
[37m[1m[2023-07-10 10:13:08,145][227910] Min Reward on eval: -1693.8197133427616[0m
[37m[1m[2023-07-10 10:13:08,146][227910] Mean Reward across all agents: -1693.8197133427616[0m
[37m[1m[2023-07-10 10:13:08,146][227910] Average Trajectory Length: 999.6816666666666[0m
[36m[2023-07-10 10:13:13,308][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:13:13,308][227910] Reward + Measures: [[  -7.9761975     0.51220006    0.63589996    0.51940006    0.49750003]
 [-144.12005646    0.61920005    0.67740005    0.58950007    0.58990002]
 [ 473.66133401    0.3524        0.46269998    0.26989999    0.3743    ]
 ...
 [ 300.45603778    0.31310001    0.49069998    0.30410001    0.31420001]
 [  79.13452324    0.50150007    0.58700007    0.4585        0.4883    ]
 [ 608.70534171    0.28279999    0.40840003    0.2375        0.30410001]][0m
[37m[1m[2023-07-10 10:13:13,308][227910] Max Reward on eval: 693.5250742393139[0m
[37m[1m[2023-07-10 10:13:13,309][227910] Min Reward on eval: -869.4491427923319[0m
[37m[1m[2023-07-10 10:13:13,309][227910] Mean Reward across all agents: 236.09343956864748[0m
[37m[1m[2023-07-10 10:13:13,309][227910] Average Trajectory Length: 998.4196666666667[0m
[36m[2023-07-10 10:13:13,312][227910] mean_value=-207.90940250289083, max_value=637.7799894786906[0m
[37m[1m[2023-07-10 10:13:13,315][227910] New mean coefficients: [[-0.42679858  1.385586   13.381552    0.9155103   6.8272567 ]][0m
[37m[1m[2023-07-10 10:13:13,316][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:13:22,578][227910] train() took 9.26 seconds to complete[0m
[36m[2023-07-10 10:13:22,579][227910] FPS: 414632.61[0m
[36m[2023-07-10 10:13:22,581][227910] itr=122, itrs=2000, Progress: 6.10%[0m
[36m[2023-07-10 10:13:35,401][227910] train() took 12.80 seconds to complete[0m
[36m[2023-07-10 10:13:35,401][227910] FPS: 299926.25[0m
[36m[2023-07-10 10:13:39,700][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:13:39,701][227910] Reward + Measures: [[-1663.13829236     0.945485       0.94550925     0.92668325
      0.92298168]][0m
[37m[1m[2023-07-10 10:13:39,701][227910] Max Reward on eval: -1663.138292356488[0m
[37m[1m[2023-07-10 10:13:39,701][227910] Min Reward on eval: -1663.138292356488[0m
[37m[1m[2023-07-10 10:13:39,702][227910] Mean Reward across all agents: -1663.138292356488[0m
[37m[1m[2023-07-10 10:13:39,702][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:13:44,992][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:13:44,993][227910] Reward + Measures: [[-111.83169097    0.56189996    0.71670002    0.4513        0.63379997]
 [ -92.49322343    0.52950001    0.64850003    0.38790002    0.58770001]
 [-683.0463653     0.76489997    0.8294        0.71649998    0.76130003]
 ...
 [  97.27050455    0.36660001    0.56350005    0.20869999    0.48359999]
 [ 128.7918809     0.38229999    0.62159997    0.25369999    0.50480002]
 [  69.30386952    0.47120005    0.65380001    0.37980002    0.55689996]][0m
[37m[1m[2023-07-10 10:13:44,993][227910] Max Reward on eval: 264.8488896645082[0m
[37m[1m[2023-07-10 10:13:44,993][227910] Min Reward on eval: -1021.162116418191[0m
[37m[1m[2023-07-10 10:13:44,994][227910] Mean Reward across all agents: -290.24879221679345[0m
[37m[1m[2023-07-10 10:13:44,994][227910] Average Trajectory Length: 997.9889999999999[0m
[36m[2023-07-10 10:13:44,995][227910] mean_value=-494.04037464137326, max_value=33.45027711270599[0m
[37m[1m[2023-07-10 10:13:44,998][227910] New mean coefficients: [[-0.3644656   1.1236204  14.7743      0.74457926  6.1723876 ]][0m
[37m[1m[2023-07-10 10:13:44,999][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:13:54,220][227910] train() took 9.22 seconds to complete[0m
[36m[2023-07-10 10:13:54,220][227910] FPS: 416527.19[0m
[36m[2023-07-10 10:13:54,222][227910] itr=123, itrs=2000, Progress: 6.15%[0m
[36m[2023-07-10 10:14:07,045][227910] train() took 12.81 seconds to complete[0m
[36m[2023-07-10 10:14:07,046][227910] FPS: 299782.44[0m
[36m[2023-07-10 10:14:11,383][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:14:11,384][227910] Reward + Measures: [[-1660.17689918     0.94679928     0.94458205     0.92711699
      0.92262769]][0m
[37m[1m[2023-07-10 10:14:11,384][227910] Max Reward on eval: -1660.1768991799215[0m
[37m[1m[2023-07-10 10:14:11,384][227910] Min Reward on eval: -1660.1768991799215[0m
[37m[1m[2023-07-10 10:14:11,385][227910] Mean Reward across all agents: -1660.1768991799215[0m
[37m[1m[2023-07-10 10:14:11,385][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:14:16,508][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:14:16,508][227910] Reward + Measures: [[288.69048623   0.35839999   0.43650004   0.16159999   0.36250001]
 [109.79084824   0.44790003   0.54280001   0.24680002   0.43619999]
 [268.50902497   0.50890005   0.63910002   0.32259998   0.53800005]
 ...
 [332.30248932   0.36450002   0.4409       0.1693       0.391     ]
 [239.48549495   0.30590001   0.3646       0.16060001   0.33810002]
 [322.63096791   0.42300001   0.59450001   0.22580002   0.48060003]][0m
[37m[1m[2023-07-10 10:14:16,509][227910] Max Reward on eval: 623.7879827411031[0m
[37m[1m[2023-07-10 10:14:16,509][227910] Min Reward on eval: -149.6883993527037[0m
[37m[1m[2023-07-10 10:14:16,509][227910] Mean Reward across all agents: 245.3824500185392[0m
[37m[1m[2023-07-10 10:14:16,509][227910] Average Trajectory Length: 999.5993333333333[0m
[36m[2023-07-10 10:14:16,512][227910] mean_value=-325.63019415758606, max_value=903.1059244423942[0m
[37m[1m[2023-07-10 10:14:16,515][227910] New mean coefficients: [[-0.6384737  1.8151114 13.735705   0.6034964  7.0841002]][0m
[37m[1m[2023-07-10 10:14:16,516][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:14:25,825][227910] train() took 9.31 seconds to complete[0m
[36m[2023-07-10 10:14:25,825][227910] FPS: 412596.28[0m
[36m[2023-07-10 10:14:25,827][227910] itr=124, itrs=2000, Progress: 6.20%[0m
[36m[2023-07-10 10:14:38,679][227910] train() took 12.84 seconds to complete[0m
[36m[2023-07-10 10:14:38,679][227910] FPS: 299130.51[0m
[36m[2023-07-10 10:14:42,919][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:14:42,920][227910] Reward + Measures: [[-1641.83428534     0.93814898     0.94100034     0.91921663
      0.91758698]][0m
[37m[1m[2023-07-10 10:14:42,920][227910] Max Reward on eval: -1641.8342853402432[0m
[37m[1m[2023-07-10 10:14:42,920][227910] Min Reward on eval: -1641.8342853402432[0m
[37m[1m[2023-07-10 10:14:42,920][227910] Mean Reward across all agents: -1641.8342853402432[0m
[37m[1m[2023-07-10 10:14:42,921][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:14:47,942][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:14:47,943][227910] Reward + Measures: [[474.67425804   0.28870001   0.46310002   0.3337       0.2552    ]
 [204.00563883   0.25830004   0.52840006   0.41870004   0.30609998]
 [641.1415546    0.29800001   0.49189997   0.45620003   0.2674    ]
 ...
 [307.13781849   0.27760002   0.56299996   0.39210004   0.3497    ]
 [595.26557938   0.28120002   0.45320001   0.40260002   0.20350002]
 [586.47362422   0.31400001   0.50330001   0.31310001   0.30969998]][0m
[37m[1m[2023-07-10 10:14:47,943][227910] Max Reward on eval: 792.6850383327343[0m
[37m[1m[2023-07-10 10:14:47,943][227910] Min Reward on eval: -158.82590578553499[0m
[37m[1m[2023-07-10 10:14:47,943][227910] Mean Reward across all agents: 427.5093705464822[0m
[37m[1m[2023-07-10 10:14:47,944][227910] Average Trajectory Length: 998.5193333333333[0m
[36m[2023-07-10 10:14:47,950][227910] mean_value=357.8328966347677, max_value=1290.3896233437001[0m
[37m[1m[2023-07-10 10:14:47,954][227910] New mean coefficients: [[-0.6473161   2.8682294  13.722282    0.19797516  6.792167  ]][0m
[37m[1m[2023-07-10 10:14:47,955][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:14:57,187][227910] train() took 9.23 seconds to complete[0m
[36m[2023-07-10 10:14:57,188][227910] FPS: 415984.81[0m
[36m[2023-07-10 10:14:57,190][227910] itr=125, itrs=2000, Progress: 6.25%[0m
[36m[2023-07-10 10:15:10,119][227910] train() took 12.91 seconds to complete[0m
[36m[2023-07-10 10:15:10,119][227910] FPS: 297356.63[0m
[36m[2023-07-10 10:15:14,375][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:15:14,375][227910] Reward + Measures: [[-1698.31646399     0.94634193     0.94953758     0.93096501
      0.93004364]][0m
[37m[1m[2023-07-10 10:15:14,376][227910] Max Reward on eval: -1698.3164639900544[0m
[37m[1m[2023-07-10 10:15:14,376][227910] Min Reward on eval: -1698.3164639900544[0m
[37m[1m[2023-07-10 10:15:14,376][227910] Mean Reward across all agents: -1698.3164639900544[0m
[37m[1m[2023-07-10 10:15:14,376][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:15:19,480][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:15:19,481][227910] Reward + Measures: [[-374.58355271    0.80000001    0.8221001     0.76340002    0.7913    ]
 [ 242.57099209    0.3876        0.53860003    0.2757        0.48850003]
 [-261.15027092    0.68050003    0.77380002    0.62580007    0.736     ]
 ...
 [ 459.55397311    0.28169999    0.58100003    0.31580001    0.50209999]
 [ 118.7565815     0.42879996    0.65450001    0.36279997    0.56200004]
 [ 358.42191451    0.45650002    0.6085        0.373         0.57999998]][0m
[37m[1m[2023-07-10 10:15:19,481][227910] Max Reward on eval: 559.5741253083804[0m
[37m[1m[2023-07-10 10:15:19,481][227910] Min Reward on eval: -819.7594672910403[0m
[37m[1m[2023-07-10 10:15:19,482][227910] Mean Reward across all agents: 98.53015379366991[0m
[37m[1m[2023-07-10 10:15:19,482][227910] Average Trajectory Length: 999.4823333333333[0m
[36m[2023-07-10 10:15:19,488][227910] mean_value=194.65430331798922, max_value=1039.300999998802[0m
[37m[1m[2023-07-10 10:15:19,492][227910] New mean coefficients: [[ 1.7188216  3.129801  13.819076  -1.3784866  7.4249673]][0m
[37m[1m[2023-07-10 10:15:19,493][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:15:28,692][227910] train() took 9.20 seconds to complete[0m
[36m[2023-07-10 10:15:28,692][227910] FPS: 417497.43[0m
[36m[2023-07-10 10:15:28,695][227910] itr=126, itrs=2000, Progress: 6.30%[0m
[36m[2023-07-10 10:15:41,571][227910] train() took 12.86 seconds to complete[0m
[36m[2023-07-10 10:15:41,571][227910] FPS: 298573.47[0m
[36m[2023-07-10 10:15:45,854][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:15:45,855][227910] Reward + Measures: [[-1662.31810271     0.94618618     0.94853687     0.93098044
      0.92927986]][0m
[37m[1m[2023-07-10 10:15:45,855][227910] Max Reward on eval: -1662.3181027085514[0m
[37m[1m[2023-07-10 10:15:45,855][227910] Min Reward on eval: -1662.3181027085514[0m
[37m[1m[2023-07-10 10:15:45,855][227910] Mean Reward across all agents: -1662.3181027085514[0m
[37m[1m[2023-07-10 10:15:45,856][227910] Average Trajectory Length: 999.673[0m
[36m[2023-07-10 10:15:51,082][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:15:51,083][227910] Reward + Measures: [[ 25.13451226   0.70539999   0.77789998   0.6304       0.70920002]
 [789.17828801   0.38990003   0.61799997   0.22309999   0.43309999]
 [557.71443668   0.39750001   0.6153       0.24110003   0.46680003]
 ...
 [417.83996558   0.4729       0.63980001   0.273        0.53150004]
 [525.81885037   0.39330003   0.51270002   0.24860001   0.37760001]
 [594.35559368   0.41840002   0.59120005   0.23740001   0.45159999]][0m
[37m[1m[2023-07-10 10:15:51,083][227910] Max Reward on eval: 1089.9145418910775[0m
[37m[1m[2023-07-10 10:15:51,084][227910] Min Reward on eval: -17.35758723668987[0m
[37m[1m[2023-07-10 10:15:51,084][227910] Mean Reward across all agents: 604.4908452579451[0m
[37m[1m[2023-07-10 10:15:51,084][227910] Average Trajectory Length: 998.7613333333333[0m
[36m[2023-07-10 10:15:51,090][227910] mean_value=157.66608772544893, max_value=1088.907383529271[0m
[37m[1m[2023-07-10 10:15:51,093][227910] New mean coefficients: [[ 3.3048277  2.2597477 13.50701   -2.3298054  6.72296  ]][0m
[37m[1m[2023-07-10 10:15:51,094][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:16:01,489][227910] train() took 10.39 seconds to complete[0m
[36m[2023-07-10 10:16:01,489][227910] FPS: 369482.26[0m
[36m[2023-07-10 10:16:01,492][227910] itr=127, itrs=2000, Progress: 6.35%[0m
[36m[2023-07-10 10:16:20,917][227910] train() took 19.41 seconds to complete[0m
[36m[2023-07-10 10:16:20,918][227910] FPS: 197834.12[0m
[36m[2023-07-10 10:16:26,792][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:16:26,795][227910] Reward + Measures: [[-1608.36065624     0.9472363      0.94916749     0.9318701
      0.93041664]][0m
[37m[1m[2023-07-10 10:16:26,795][227910] Max Reward on eval: -1608.3606562434804[0m
[37m[1m[2023-07-10 10:16:26,795][227910] Min Reward on eval: -1608.3606562434804[0m
[37m[1m[2023-07-10 10:16:26,796][227910] Mean Reward across all agents: -1608.3606562434804[0m
[37m[1m[2023-07-10 10:16:26,796][227910] Average Trajectory Length: 999.3623333333333[0m
[36m[2023-07-10 10:16:33,858][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:16:33,859][227910] Reward + Measures: [[ 221.3596461     0.5212        0.72030002    0.52010006    0.56120002]
 [-136.10399335    0.66659999    0.79279995    0.64309996    0.68720001]
 [-445.70669123    0.86779994    0.88990003    0.84850007    0.8448    ]
 ...
 [-231.68023357    0.82300007    0.85950005    0.79380006    0.78320003]
 [ -85.42573401    0.78790003    0.85159999    0.76610005    0.76459998]
 [ 106.35994614    0.42179999    0.69680005    0.45609999    0.54080003]][0m
[37m[1m[2023-07-10 10:16:33,859][227910] Max Reward on eval: 518.0167713392409[0m
[37m[1m[2023-07-10 10:16:33,859][227910] Min Reward on eval: -692.9672669854015[0m
[37m[1m[2023-07-10 10:16:33,860][227910] Mean Reward across all agents: -8.994865040267909[0m
[37m[1m[2023-07-10 10:16:33,860][227910] Average Trajectory Length: 998.7083333333333[0m
[36m[2023-07-10 10:16:33,878][227910] mean_value=-51.892525001204014, max_value=874.6139490704576[0m
[37m[1m[2023-07-10 10:16:33,886][227910] New mean coefficients: [[ 5.474093   2.1319096 13.9551935 -4.1518655  6.6624475]][0m
[37m[1m[2023-07-10 10:16:33,888][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:16:42,742][227910] train() took 8.85 seconds to complete[0m
[36m[2023-07-10 10:16:42,742][227910] FPS: 433841.17[0m
[36m[2023-07-10 10:16:42,745][227910] itr=128, itrs=2000, Progress: 6.40%[0m
[36m[2023-07-10 10:16:54,983][227910] train() took 12.22 seconds to complete[0m
[36m[2023-07-10 10:16:54,983][227910] FPS: 314178.58[0m
[36m[2023-07-10 10:16:58,907][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:16:58,908][227910] Reward + Measures: [[-1415.84722065     0.94428712     0.94712359     0.92760473
      0.92692196]][0m
[37m[1m[2023-07-10 10:16:58,908][227910] Max Reward on eval: -1415.8472206538866[0m
[37m[1m[2023-07-10 10:16:58,908][227910] Min Reward on eval: -1415.8472206538866[0m
[37m[1m[2023-07-10 10:16:58,908][227910] Mean Reward across all agents: -1415.8472206538866[0m
[37m[1m[2023-07-10 10:16:58,909][227910] Average Trajectory Length: 999.6726666666666[0m
[36m[2023-07-10 10:17:03,565][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:17:03,565][227910] Reward + Measures: [[295.02780269   0.37650001   0.6117       0.21870001   0.5413    ]
 [349.85343801   0.43959999   0.59419996   0.45070001   0.28280002]
 [526.9658428    0.38229999   0.59450001   0.26640001   0.41020003]
 ...
 [349.53262758   0.35890004   0.6006       0.18629999   0.48979998]
 [327.15116397   0.38980001   0.64370006   0.1691       0.50120002]
 [330.69335541   0.3829       0.61740005   0.278        0.4628    ]][0m
[37m[1m[2023-07-10 10:17:03,566][227910] Max Reward on eval: 554.733149528224[0m
[37m[1m[2023-07-10 10:17:03,566][227910] Min Reward on eval: -390.86327433357945[0m
[37m[1m[2023-07-10 10:17:03,566][227910] Mean Reward across all agents: 306.8399247694912[0m
[37m[1m[2023-07-10 10:17:03,567][227910] Average Trajectory Length: 999.7616666666667[0m
[36m[2023-07-10 10:17:03,570][227910] mean_value=-42.02110723964033, max_value=939.4185332755209[0m
[37m[1m[2023-07-10 10:17:03,573][227910] New mean coefficients: [[ 5.320366   3.3243523 12.791294  -3.6753669  7.216663 ]][0m
[37m[1m[2023-07-10 10:17:03,574][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:17:12,335][227910] train() took 8.76 seconds to complete[0m
[36m[2023-07-10 10:17:12,336][227910] FPS: 438373.93[0m
[36m[2023-07-10 10:17:12,338][227910] itr=129, itrs=2000, Progress: 6.45%[0m
[36m[2023-07-10 10:17:24,455][227910] train() took 12.10 seconds to complete[0m
[36m[2023-07-10 10:17:24,456][227910] FPS: 317304.57[0m
[36m[2023-07-10 10:17:28,344][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:17:28,345][227910] Reward + Measures: [[-1134.05656002     0.94047523     0.94346952     0.92297328
      0.91943407]][0m
[37m[1m[2023-07-10 10:17:28,345][227910] Max Reward on eval: -1134.0565600179075[0m
[37m[1m[2023-07-10 10:17:28,345][227910] Min Reward on eval: -1134.0565600179075[0m
[37m[1m[2023-07-10 10:17:28,345][227910] Mean Reward across all agents: -1134.0565600179075[0m
[37m[1m[2023-07-10 10:17:28,345][227910] Average Trajectory Length: 997.7136666666667[0m
[36m[2023-07-10 10:17:33,069][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:17:33,070][227910] Reward + Measures: [[ 272.34387665    0.41190001    0.50210005    0.35670003    0.38499999]
 [ 473.85865374    0.34650001    0.49460003    0.21610001    0.37090001]
 [ 461.76168862    0.39639997    0.52870005    0.27130005    0.38229999]
 ...
 [ 234.54633887    0.44499999    0.50760001    0.3653        0.40950003]
 [-287.85379367    0.73360002    0.7944001     0.71400005    0.73780006]
 [ 394.51506946    0.39630002    0.52630007    0.26699999    0.37349999]][0m
[37m[1m[2023-07-10 10:17:33,070][227910] Max Reward on eval: 751.5576244468393[0m
[37m[1m[2023-07-10 10:17:33,070][227910] Min Reward on eval: -791.1748527711956[0m
[37m[1m[2023-07-10 10:17:33,071][227910] Mean Reward across all agents: 207.23705129905682[0m
[37m[1m[2023-07-10 10:17:33,071][227910] Average Trajectory Length: 998.592[0m
[36m[2023-07-10 10:17:33,073][227910] mean_value=-295.13563129998073, max_value=158.48723105439052[0m
[37m[1m[2023-07-10 10:17:33,075][227910] New mean coefficients: [[ 5.2708626  3.385008  12.058937  -3.1347816  7.8658705]][0m
[37m[1m[2023-07-10 10:17:33,076][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:17:41,818][227910] train() took 8.74 seconds to complete[0m
[36m[2023-07-10 10:17:41,819][227910] FPS: 439329.92[0m
[36m[2023-07-10 10:17:41,821][227910] itr=130, itrs=2000, Progress: 6.50%[0m
[37m[1m[2023-07-10 10:17:43,802][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000110[0m
[36m[2023-07-10 10:17:56,262][227910] train() took 12.18 seconds to complete[0m
[36m[2023-07-10 10:17:56,262][227910] FPS: 315357.60[0m
[36m[2023-07-10 10:18:00,220][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:18:00,221][227910] Reward + Measures: [[-621.96037493    0.9394365     0.94296408    0.92027611    0.9178493 ]][0m
[37m[1m[2023-07-10 10:18:00,221][227910] Max Reward on eval: -621.960374926029[0m
[37m[1m[2023-07-10 10:18:00,221][227910] Min Reward on eval: -621.960374926029[0m
[37m[1m[2023-07-10 10:18:00,221][227910] Mean Reward across all agents: -621.960374926029[0m
[37m[1m[2023-07-10 10:18:00,221][227910] Average Trajectory Length: 998.6903333333333[0m
[36m[2023-07-10 10:18:05,003][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:18:05,003][227910] Reward + Measures: [[-327.13271995    0.79090005    0.84490007    0.70199996    0.76920003]
 [  47.46656649    0.52079999    0.65410006    0.38820001    0.56399995]
 [  76.27572461    0.61779994    0.71740001    0.5126        0.64679998]
 ...
 [  34.8519341     0.69279999    0.75369996    0.61919999    0.6814    ]
 [ 186.71327254    0.49849996    0.63740003    0.36879998    0.55559999]
 [  35.25402008    0.69760001    0.71259993    0.6085        0.66670007]][0m
[37m[1m[2023-07-10 10:18:05,004][227910] Max Reward on eval: 482.9679523370811[0m
[37m[1m[2023-07-10 10:18:05,004][227910] Min Reward on eval: -366.106840777304[0m
[37m[1m[2023-07-10 10:18:05,004][227910] Mean Reward across all agents: 120.79334406194553[0m
[37m[1m[2023-07-10 10:18:05,004][227910] Average Trajectory Length: 999.3606666666666[0m
[36m[2023-07-10 10:18:05,006][227910] mean_value=-187.62251550588772, max_value=667.5258629801276[0m
[37m[1m[2023-07-10 10:18:05,009][227910] New mean coefficients: [[ 5.753226  4.107761 11.144344 -3.353067  8.098568]][0m
[37m[1m[2023-07-10 10:18:05,010][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:18:13,791][227910] train() took 8.78 seconds to complete[0m
[36m[2023-07-10 10:18:13,792][227910] FPS: 437380.48[0m
[36m[2023-07-10 10:18:13,794][227910] itr=131, itrs=2000, Progress: 6.55%[0m
[36m[2023-07-10 10:18:26,127][227910] train() took 12.32 seconds to complete[0m
[36m[2023-07-10 10:18:26,127][227910] FPS: 311766.19[0m
[36m[2023-07-10 10:18:30,108][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:18:30,113][227910] Reward + Measures: [[-149.58290206    0.93605417    0.94081444    0.9144032     0.91543329]][0m
[37m[1m[2023-07-10 10:18:30,114][227910] Max Reward on eval: -149.5829020576573[0m
[37m[1m[2023-07-10 10:18:30,114][227910] Min Reward on eval: -149.5829020576573[0m
[37m[1m[2023-07-10 10:18:30,114][227910] Mean Reward across all agents: -149.5829020576573[0m
[37m[1m[2023-07-10 10:18:30,114][227910] Average Trajectory Length: 999.0176666666666[0m
[36m[2023-07-10 10:18:35,010][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:18:35,011][227910] Reward + Measures: [[538.47676633   0.38200003   0.62180007   0.1567       0.4533    ]
 [379.95634948   0.42410001   0.50780004   0.26449999   0.41340002]
 [612.50708176   0.412        0.6433       0.20159999   0.49679995]
 ...
 [542.45090737   0.47989997   0.60170001   0.23440002   0.46680003]
 [512.7146714    0.47000003   0.6455       0.2026       0.52390003]
 [397.19751629   0.4646       0.61379999   0.2318       0.47839999]][0m
[37m[1m[2023-07-10 10:18:35,011][227910] Max Reward on eval: 878.594509609582[0m
[37m[1m[2023-07-10 10:18:35,011][227910] Min Reward on eval: 186.56310773087898[0m
[37m[1m[2023-07-10 10:18:35,011][227910] Mean Reward across all agents: 465.04836524414713[0m
[37m[1m[2023-07-10 10:18:35,011][227910] Average Trajectory Length: 999.7479999999999[0m
[36m[2023-07-10 10:18:35,017][227910] mean_value=130.86428938926596, max_value=1075.9611938460498[0m
[37m[1m[2023-07-10 10:18:35,019][227910] New mean coefficients: [[ 6.1409864  2.536923  10.662242  -3.2087755  7.332045 ]][0m
[37m[1m[2023-07-10 10:18:35,020][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:18:43,894][227910] train() took 8.87 seconds to complete[0m
[36m[2023-07-10 10:18:43,895][227910] FPS: 432811.51[0m
[36m[2023-07-10 10:18:43,897][227910] itr=132, itrs=2000, Progress: 6.60%[0m
[36m[2023-07-10 10:18:56,107][227910] train() took 12.20 seconds to complete[0m
[36m[2023-07-10 10:18:56,108][227910] FPS: 314859.51[0m
[36m[2023-07-10 10:19:00,051][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:19:00,051][227910] Reward + Measures: [[83.79475858  0.9377079   0.94088709  0.91573882  0.91739339]][0m
[37m[1m[2023-07-10 10:19:00,051][227910] Max Reward on eval: 83.79475858130549[0m
[37m[1m[2023-07-10 10:19:00,051][227910] Min Reward on eval: 83.79475858130549[0m
[37m[1m[2023-07-10 10:19:00,051][227910] Mean Reward across all agents: 83.79475858130549[0m
[37m[1m[2023-07-10 10:19:00,052][227910] Average Trajectory Length: 999.673[0m
[36m[2023-07-10 10:19:04,761][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:19:04,762][227910] Reward + Measures: [[448.65121091   0.40310001   0.67800003   0.15360001   0.50489998]
 [553.19042753   0.41470003   0.69309998   0.1184       0.51749998]
 [640.6745598    0.43439999   0.71479994   0.1516       0.52270001]
 ...
 [515.80311781   0.40359998   0.62510008   0.16419999   0.46890002]
 [770.12954091   0.42360002   0.71680003   0.16309999   0.46949998]
 [655.1875143    0.52010006   0.76780003   0.10050001   0.57730001]][0m
[37m[1m[2023-07-10 10:19:04,762][227910] Max Reward on eval: 1172.0761472819372[0m
[37m[1m[2023-07-10 10:19:04,762][227910] Min Reward on eval: 166.53946972003322[0m
[37m[1m[2023-07-10 10:19:04,762][227910] Mean Reward across all agents: 583.1121028238696[0m
[37m[1m[2023-07-10 10:19:04,763][227910] Average Trajectory Length: 999.8706666666666[0m
[36m[2023-07-10 10:19:04,772][227910] mean_value=616.8272465419594, max_value=1431.9788354307414[0m
[37m[1m[2023-07-10 10:19:04,775][227910] New mean coefficients: [[ 6.1182766  1.8868088  9.915453  -3.477373   6.8699713]][0m
[37m[1m[2023-07-10 10:19:04,776][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:19:13,545][227910] train() took 8.77 seconds to complete[0m
[36m[2023-07-10 10:19:13,546][227910] FPS: 437953.74[0m
[36m[2023-07-10 10:19:13,549][227910] itr=133, itrs=2000, Progress: 6.65%[0m
[36m[2023-07-10 10:19:25,746][227910] train() took 12.18 seconds to complete[0m
[36m[2023-07-10 10:19:25,747][227910] FPS: 315217.75[0m
[36m[2023-07-10 10:19:29,692][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:19:29,692][227910] Reward + Measures: [[224.15341459   0.94156027   0.94256479   0.91833091   0.92000699]][0m
[37m[1m[2023-07-10 10:19:29,692][227910] Max Reward on eval: 224.15341458887755[0m
[37m[1m[2023-07-10 10:19:29,692][227910] Min Reward on eval: 224.15341458887755[0m
[37m[1m[2023-07-10 10:19:29,693][227910] Mean Reward across all agents: 224.15341458887755[0m
[37m[1m[2023-07-10 10:19:29,693][227910] Average Trajectory Length: 999.0419999999999[0m
[36m[2023-07-10 10:19:34,475][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:19:34,476][227910] Reward + Measures: [[ 221.31330667    0.50319999    0.69150001    0.41990003    0.57409996]
 [ 126.58029522    0.71680003    0.76190001    0.5431        0.68189996]
 [-159.80486447    0.85830003    0.82450002    0.79520005    0.79310006]
 ...
 [  14.70408059    0.77939999    0.79529995    0.66119999    0.74180001]
 [ 315.25138176    0.36629996    0.57370001    0.23810001    0.48170003]
 [ 308.83031403    0.53669995    0.66400003    0.32870001    0.51850003]][0m
[37m[1m[2023-07-10 10:19:34,476][227910] Max Reward on eval: 514.8308063413249[0m
[37m[1m[2023-07-10 10:19:34,476][227910] Min Reward on eval: -327.55048521913125[0m
[37m[1m[2023-07-10 10:19:34,476][227910] Mean Reward across all agents: 171.29152968019406[0m
[37m[1m[2023-07-10 10:19:34,477][227910] Average Trajectory Length: 999.572[0m
[36m[2023-07-10 10:19:34,480][227910] mean_value=-57.846944728092616, max_value=879.6580700941292[0m
[37m[1m[2023-07-10 10:19:34,482][227910] New mean coefficients: [[ 7.1631413   0.95494896  8.921235   -3.397131    6.914373  ]][0m
[37m[1m[2023-07-10 10:19:34,483][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:19:43,140][227910] train() took 8.66 seconds to complete[0m
[36m[2023-07-10 10:19:43,141][227910] FPS: 443651.35[0m
[36m[2023-07-10 10:19:43,143][227910] itr=134, itrs=2000, Progress: 6.70%[0m
[36m[2023-07-10 10:19:55,299][227910] train() took 12.14 seconds to complete[0m
[36m[2023-07-10 10:19:55,299][227910] FPS: 316332.05[0m
[36m[2023-07-10 10:19:59,234][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:19:59,235][227910] Reward + Measures: [[314.0345303    0.94485658   0.94643879   0.92024225   0.92499316]][0m
[37m[1m[2023-07-10 10:19:59,235][227910] Max Reward on eval: 314.0345303015289[0m
[37m[1m[2023-07-10 10:19:59,235][227910] Min Reward on eval: 314.0345303015289[0m
[37m[1m[2023-07-10 10:19:59,235][227910] Mean Reward across all agents: 314.0345303015289[0m
[37m[1m[2023-07-10 10:19:59,236][227910] Average Trajectory Length: 999.3523333333333[0m
[36m[2023-07-10 10:20:03,952][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:20:03,953][227910] Reward + Measures: [[402.45144247   0.27950001   0.49460003   0.16430001   0.3687    ]
 [286.19710315   0.3087       0.53140002   0.17320001   0.44380003]
 [602.63917215   0.28290004   0.53329998   0.19280002   0.31580001]
 ...
 [509.40560624   0.3215       0.57800001   0.24980001   0.40240002]
 [567.57243906   0.3179       0.63000005   0.1683       0.53190005]
 [460.57781424   0.3626       0.57909995   0.14100002   0.49050003]][0m
[37m[1m[2023-07-10 10:20:03,953][227910] Max Reward on eval: 704.4012357685715[0m
[37m[1m[2023-07-10 10:20:03,953][227910] Min Reward on eval: 105.70725900788094[0m
[37m[1m[2023-07-10 10:20:03,954][227910] Mean Reward across all agents: 442.43595344225787[0m
[37m[1m[2023-07-10 10:20:03,954][227910] Average Trajectory Length: 998.7236666666666[0m
[36m[2023-07-10 10:20:03,958][227910] mean_value=-34.55216173674994, max_value=1031.9360750118387[0m
[37m[1m[2023-07-10 10:20:03,960][227910] New mean coefficients: [[ 5.8027954  0.586866   9.590441  -3.252824   6.310584 ]][0m
[37m[1m[2023-07-10 10:20:03,961][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:20:12,765][227910] train() took 8.80 seconds to complete[0m
[36m[2023-07-10 10:20:12,765][227910] FPS: 436270.66[0m
[36m[2023-07-10 10:20:12,768][227910] itr=135, itrs=2000, Progress: 6.75%[0m
[36m[2023-07-10 10:20:25,040][227910] train() took 12.26 seconds to complete[0m
[36m[2023-07-10 10:20:25,040][227910] FPS: 313290.71[0m
[36m[2023-07-10 10:20:28,889][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:20:28,889][227910] Reward + Measures: [[387.29409831   0.94617152   0.94708115   0.92278391   0.92557323]][0m
[37m[1m[2023-07-10 10:20:28,890][227910] Max Reward on eval: 387.294098310545[0m
[37m[1m[2023-07-10 10:20:28,890][227910] Min Reward on eval: 387.294098310545[0m
[37m[1m[2023-07-10 10:20:28,890][227910] Mean Reward across all agents: 387.294098310545[0m
[37m[1m[2023-07-10 10:20:28,890][227910] Average Trajectory Length: 999.6823333333333[0m
[36m[2023-07-10 10:20:33,761][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:20:33,761][227910] Reward + Measures: [[421.89216698   0.21149997   0.5492       0.27919999   0.44839999]
 [605.21707769   0.205        0.5377       0.28830001   0.43930003]
 [372.49153309   0.43759999   0.73890001   0.1657       0.57320005]
 ...
 [268.88913688   0.15290001   0.65390003   0.28660002   0.63340002]
 [623.4037511    0.22590001   0.58179998   0.25889999   0.40050003]
 [197.55139636   0.64330006   0.74780005   0.49180004   0.65780002]][0m
[37m[1m[2023-07-10 10:20:33,762][227910] Max Reward on eval: 816.4697816277679[0m
[37m[1m[2023-07-10 10:20:33,762][227910] Min Reward on eval: 197.5513963627629[0m
[37m[1m[2023-07-10 10:20:33,762][227910] Mean Reward across all agents: 512.281772261049[0m
[37m[1m[2023-07-10 10:20:33,762][227910] Average Trajectory Length: 999.0456666666666[0m
[36m[2023-07-10 10:20:33,769][227910] mean_value=263.03530865350973, max_value=1162.3108082792721[0m
[37m[1m[2023-07-10 10:20:33,772][227910] New mean coefficients: [[ 5.0159235   0.75413555  9.279478   -2.2870164   6.4466925 ]][0m
[37m[1m[2023-07-10 10:20:33,773][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:20:42,399][227910] train() took 8.62 seconds to complete[0m
[36m[2023-07-10 10:20:42,399][227910] FPS: 445221.54[0m
[36m[2023-07-10 10:20:42,422][227910] itr=136, itrs=2000, Progress: 6.80%[0m
[36m[2023-07-10 10:20:54,597][227910] train() took 12.15 seconds to complete[0m
[36m[2023-07-10 10:20:54,597][227910] FPS: 315931.43[0m
[36m[2023-07-10 10:20:58,533][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:20:58,534][227910] Reward + Measures: [[449.27163262   0.94760722   0.94748938   0.92151654   0.92586541]][0m
[37m[1m[2023-07-10 10:20:58,534][227910] Max Reward on eval: 449.27163261728106[0m
[37m[1m[2023-07-10 10:20:58,534][227910] Min Reward on eval: 449.27163261728106[0m
[37m[1m[2023-07-10 10:20:58,534][227910] Mean Reward across all agents: 449.27163261728106[0m
[37m[1m[2023-07-10 10:20:58,535][227910] Average Trajectory Length: 999.026[0m
[36m[2023-07-10 10:21:03,276][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:21:03,276][227910] Reward + Measures: [[552.15938733   0.60900003   0.62780005   0.32769999   0.61860001]
 [483.01244964   0.54479998   0.67690003   0.1724       0.54659998]
 [584.6710748    0.4391       0.52030003   0.28690001   0.40330002]
 ...
 [604.04125853   0.61579996   0.66149998   0.3012       0.53210002]
 [478.8694437    0.79860002   0.79839998   0.49710003   0.6724    ]
 [349.49968103   0.87190002   0.83859998   0.72620004   0.74860001]][0m
[37m[1m[2023-07-10 10:21:03,277][227910] Max Reward on eval: 689.2601674704231[0m
[37m[1m[2023-07-10 10:21:03,277][227910] Min Reward on eval: 172.6927868333878[0m
[37m[1m[2023-07-10 10:21:03,277][227910] Mean Reward across all agents: 525.4070996043828[0m
[37m[1m[2023-07-10 10:21:03,277][227910] Average Trajectory Length: 999.4923333333332[0m
[36m[2023-07-10 10:21:03,287][227910] mean_value=517.6693262329409, max_value=1127.2442619422218[0m
[37m[1m[2023-07-10 10:21:03,290][227910] New mean coefficients: [[ 3.4417179  1.5138817  8.908069  -1.0919695  7.114813 ]][0m
[37m[1m[2023-07-10 10:21:03,291][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:21:11,995][227910] train() took 8.70 seconds to complete[0m
[36m[2023-07-10 10:21:11,995][227910] FPS: 441252.99[0m
[36m[2023-07-10 10:21:11,997][227910] itr=137, itrs=2000, Progress: 6.85%[0m
[36m[2023-07-10 10:21:24,277][227910] train() took 12.26 seconds to complete[0m
[36m[2023-07-10 10:21:24,277][227910] FPS: 313117.85[0m
[36m[2023-07-10 10:21:28,205][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:21:28,206][227910] Reward + Measures: [[454.9564086    0.94354993   0.9460184    0.91552275   0.92471564]][0m
[37m[1m[2023-07-10 10:21:28,206][227910] Max Reward on eval: 454.95640860365705[0m
[37m[1m[2023-07-10 10:21:28,206][227910] Min Reward on eval: 454.95640860365705[0m
[37m[1m[2023-07-10 10:21:28,206][227910] Mean Reward across all agents: 454.95640860365705[0m
[37m[1m[2023-07-10 10:21:28,206][227910] Average Trajectory Length: 999.7013333333333[0m
[36m[2023-07-10 10:21:33,016][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:21:33,017][227910] Reward + Measures: [[767.2527873    0.5054       0.60120004   0.21699999   0.36210003]
 [595.37778869   0.37990001   0.74299997   0.1375       0.53330004]
 [556.7124939    0.43700001   0.78010005   0.1226       0.64920002]
 ...
 [490.92519507   0.43240005   0.57330006   0.1707       0.43130001]
 [424.02023657   0.55450004   0.76459998   0.25300002   0.68480003]
 [354.27904964   0.59059995   0.74919999   0.35489997   0.65850002]][0m
[37m[1m[2023-07-10 10:21:33,017][227910] Max Reward on eval: 1050.517478441971[0m
[37m[1m[2023-07-10 10:21:33,017][227910] Min Reward on eval: 11.380953813774976[0m
[37m[1m[2023-07-10 10:21:33,017][227910] Mean Reward across all agents: 509.3970774439948[0m
[37m[1m[2023-07-10 10:21:33,018][227910] Average Trajectory Length: 999.7386666666666[0m
[36m[2023-07-10 10:21:33,025][227910] mean_value=254.94674522661197, max_value=1372.6848681382485[0m
[37m[1m[2023-07-10 10:21:33,028][227910] New mean coefficients: [[ 4.185058   1.0125002  8.4903    -1.4277921  6.201844 ]][0m
[37m[1m[2023-07-10 10:21:33,029][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:21:41,767][227910] train() took 8.74 seconds to complete[0m
[36m[2023-07-10 10:21:41,768][227910] FPS: 439544.99[0m
[36m[2023-07-10 10:21:41,770][227910] itr=138, itrs=2000, Progress: 6.90%[0m
[36m[2023-07-10 10:21:54,081][227910] train() took 12.30 seconds to complete[0m
[36m[2023-07-10 10:21:54,081][227910] FPS: 312314.47[0m
[36m[2023-07-10 10:21:58,086][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:21:58,087][227910] Reward + Measures: [[476.01090885   0.94649804   0.94654429   0.91858357   0.92605096]][0m
[37m[1m[2023-07-10 10:21:58,087][227910] Max Reward on eval: 476.0109088491815[0m
[37m[1m[2023-07-10 10:21:58,087][227910] Min Reward on eval: 476.0109088491815[0m
[37m[1m[2023-07-10 10:21:58,088][227910] Mean Reward across all agents: 476.0109088491815[0m
[37m[1m[2023-07-10 10:21:58,088][227910] Average Trajectory Length: 999.0613333333333[0m
[36m[2023-07-10 10:22:02,824][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:22:02,824][227910] Reward + Measures: [[ -77.66772619    0.2606        0.38600001    0.1885        0.3565    ]
 [ 253.36812711    0.32660002    0.61480004    0.17619999    0.56000006]
 [  81.85417581    0.31350002    0.5025        0.18530001    0.46079999]
 ...
 [ 121.35142674    0.25139999    0.46610004    0.1832        0.39739996]
 [-420.74734415    0.20753531    0.27647975    0.1991062     0.26164603]
 [-341.07059788    0.21855751    0.26144755    0.23717752    0.25709006]][0m
[37m[1m[2023-07-10 10:22:02,824][227910] Max Reward on eval: 508.0493904440547[0m
[37m[1m[2023-07-10 10:22:02,825][227910] Min Reward on eval: -783.5466652542178[0m
[37m[1m[2023-07-10 10:22:02,825][227910] Mean Reward across all agents: -109.89388034956433[0m
[37m[1m[2023-07-10 10:22:02,825][227910] Average Trajectory Length: 892.5333333333333[0m
[36m[2023-07-10 10:22:02,828][227910] mean_value=-350.53731515933646, max_value=849.3612633994314[0m
[37m[1m[2023-07-10 10:22:02,830][227910] New mean coefficients: [[ 5.502827   0.6132204  8.746421  -2.4661832  5.0991054]][0m
[37m[1m[2023-07-10 10:22:02,831][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:22:11,591][227910] train() took 8.76 seconds to complete[0m
[36m[2023-07-10 10:22:11,592][227910] FPS: 438423.50[0m
[36m[2023-07-10 10:22:11,594][227910] itr=139, itrs=2000, Progress: 6.95%[0m
[36m[2023-07-10 10:22:23,902][227910] train() took 12.29 seconds to complete[0m
[36m[2023-07-10 10:22:23,902][227910] FPS: 312373.62[0m
[36m[2023-07-10 10:22:27,853][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:22:27,853][227910] Reward + Measures: [[534.488503     0.94925261   0.94845432   0.91771656   0.92900944]][0m
[37m[1m[2023-07-10 10:22:27,854][227910] Max Reward on eval: 534.4885029983727[0m
[37m[1m[2023-07-10 10:22:27,854][227910] Min Reward on eval: 534.4885029983727[0m
[37m[1m[2023-07-10 10:22:27,854][227910] Mean Reward across all agents: 534.4885029983727[0m
[37m[1m[2023-07-10 10:22:27,854][227910] Average Trajectory Length: 999.6746666666667[0m
[36m[2023-07-10 10:22:32,643][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:22:32,644][227910] Reward + Measures: [[-1415.58644028     0.28030989     0.34182426     0.25568965
      0.32672387]
 [-1100.22876019     0.23613806     0.37197813     0.17326419
      0.35026625]
 [-1795.14724221     0.73940003     0.53359997     0.67369998
      0.41430002]
 ...
 [-1247.74798323     0.26809967     0.30752882     0.20014742
      0.26826808]
 [-1241.63055949     0.49709073     0.36996084     0.43539038
      0.35602722]
 [-1203.47054477     0.19606744     0.34911153     0.15575151
      0.26900694]][0m
[37m[1m[2023-07-10 10:22:32,644][227910] Max Reward on eval: -328.92420848069014[0m
[37m[1m[2023-07-10 10:22:32,644][227910] Min Reward on eval: -2286.1256375974745[0m
[37m[1m[2023-07-10 10:22:32,644][227910] Mean Reward across all agents: -1250.0707515987128[0m
[37m[1m[2023-07-10 10:22:32,645][227910] Average Trajectory Length: 897.492[0m
[36m[2023-07-10 10:22:32,647][227910] mean_value=-1329.5607064944925, max_value=-262.3157758664107[0m
[36m[2023-07-10 10:22:32,649][227910] XNES is restarting with a new solution whose measures are [0.23980001 0.56849998 0.4271     0.35840002] and objective is 147.35475947712547[0m
[36m[2023-07-10 10:22:32,650][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 10:22:32,652][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 10:22:32,653][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:22:41,430][227910] train() took 8.78 seconds to complete[0m
[36m[2023-07-10 10:22:41,430][227910] FPS: 437601.00[0m
[36m[2023-07-10 10:22:41,433][227910] itr=140, itrs=2000, Progress: 7.00%[0m
[37m[1m[2023-07-10 10:22:43,548][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000120[0m
[36m[2023-07-10 10:22:55,990][227910] train() took 12.20 seconds to complete[0m
[36m[2023-07-10 10:22:55,991][227910] FPS: 314878.64[0m
[36m[2023-07-10 10:22:59,927][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:22:59,928][227910] Reward + Measures: [[315.57209214   0.26617533   0.48920667   0.40986267   0.24996433]][0m
[37m[1m[2023-07-10 10:22:59,928][227910] Max Reward on eval: 315.5720921429981[0m
[37m[1m[2023-07-10 10:22:59,928][227910] Min Reward on eval: 315.5720921429981[0m
[37m[1m[2023-07-10 10:22:59,928][227910] Mean Reward across all agents: 315.5720921429981[0m
[37m[1m[2023-07-10 10:22:59,928][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:23:04,703][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:23:04,704][227910] Reward + Measures: [[119.52832283   0.29500002   0.5212       0.40030003   0.23179999]
 [145.2472227    0.40040001   0.43829998   0.41750002   0.1769    ]
 [310.1030317    0.21789999   0.45650002   0.35280001   0.30019999]
 ...
 [261.85522937   0.35390002   0.45229998   0.48010001   0.126     ]
 [188.34780691   0.40270004   0.48259997   0.34769997   0.2624    ]
 [-66.49142992   0.2253       0.46700001   0.35689998   0.23699999]][0m
[37m[1m[2023-07-10 10:23:04,704][227910] Max Reward on eval: 736.2517329703551[0m
[37m[1m[2023-07-10 10:23:04,704][227910] Min Reward on eval: -226.83270195217338[0m
[37m[1m[2023-07-10 10:23:04,704][227910] Mean Reward across all agents: 190.7547820321183[0m
[37m[1m[2023-07-10 10:23:04,705][227910] Average Trajectory Length: 999.2276666666667[0m
[36m[2023-07-10 10:23:04,709][227910] mean_value=-19.645671488300092, max_value=1069.8138480066439[0m
[37m[1m[2023-07-10 10:23:04,712][227910] New mean coefficients: [[ 0.8425001  -0.63054305 -0.11363423 -0.94435394 -2.7011657 ]][0m
[37m[1m[2023-07-10 10:23:04,713][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:23:13,342][227910] train() took 8.63 seconds to complete[0m
[36m[2023-07-10 10:23:13,342][227910] FPS: 445075.50[0m
[36m[2023-07-10 10:23:13,345][227910] itr=141, itrs=2000, Progress: 7.05%[0m
[36m[2023-07-10 10:23:25,536][227910] train() took 12.18 seconds to complete[0m
[36m[2023-07-10 10:23:25,536][227910] FPS: 315395.06[0m
[36m[2023-07-10 10:23:29,436][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:23:29,436][227910] Reward + Measures: [[548.38892481   0.29176006   0.45071647   0.37815845   0.18202531]][0m
[37m[1m[2023-07-10 10:23:29,436][227910] Max Reward on eval: 548.3889248102774[0m
[37m[1m[2023-07-10 10:23:29,436][227910] Min Reward on eval: 548.3889248102774[0m
[37m[1m[2023-07-10 10:23:29,437][227910] Mean Reward across all agents: 548.3889248102774[0m
[37m[1m[2023-07-10 10:23:29,437][227910] Average Trajectory Length: 999.8673333333332[0m
[36m[2023-07-10 10:23:34,111][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:23:34,112][227910] Reward + Measures: [[150.1031832    0.29990003   0.49630004   0.4206       0.2106    ]
 [404.9627336    0.31440002   0.42120001   0.37139997   0.12750001]
 [123.52924435   0.3055       0.47639999   0.48559999   0.19410001]
 ...
 [389.37645431   0.27040002   0.34330001   0.41339999   0.1117    ]
 [ 37.52376657   0.41490003   0.39439997   0.46750003   0.1124    ]
 [291.51353628   0.29419997   0.4601       0.45239997   0.18170001]][0m
[37m[1m[2023-07-10 10:23:34,112][227910] Max Reward on eval: 750.7685289436602[0m
[37m[1m[2023-07-10 10:23:34,112][227910] Min Reward on eval: -148.77831163810333[0m
[37m[1m[2023-07-10 10:23:34,113][227910] Mean Reward across all agents: 379.03454548171055[0m
[37m[1m[2023-07-10 10:23:34,113][227910] Average Trajectory Length: 999.2443333333333[0m
[36m[2023-07-10 10:23:34,117][227910] mean_value=37.73449508281483, max_value=1142.6008070579765[0m
[37m[1m[2023-07-10 10:23:34,119][227910] New mean coefficients: [[ 2.0184727  -0.24388927 -0.44466     0.97032213 -3.4726896 ]][0m
[37m[1m[2023-07-10 10:23:34,120][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:23:42,751][227910] train() took 8.63 seconds to complete[0m
[36m[2023-07-10 10:23:42,751][227910] FPS: 445000.86[0m
[36m[2023-07-10 10:23:42,754][227910] itr=142, itrs=2000, Progress: 7.10%[0m
[36m[2023-07-10 10:23:55,014][227910] train() took 12.24 seconds to complete[0m
[36m[2023-07-10 10:23:55,014][227910] FPS: 313604.39[0m
[36m[2023-07-10 10:23:58,925][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:23:58,926][227910] Reward + Measures: [[822.76008118   0.27679488   0.43499967   0.39609459   0.1391508 ]][0m
[37m[1m[2023-07-10 10:23:58,926][227910] Max Reward on eval: 822.7600811833305[0m
[37m[1m[2023-07-10 10:23:58,926][227910] Min Reward on eval: 822.7600811833305[0m
[37m[1m[2023-07-10 10:23:58,927][227910] Mean Reward across all agents: 822.7600811833305[0m
[37m[1m[2023-07-10 10:23:58,927][227910] Average Trajectory Length: 998.4216666666666[0m
[36m[2023-07-10 10:24:03,631][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:24:03,631][227910] Reward + Measures: [[824.93507757   0.29520002   0.42719999   0.40569997   0.1067    ]
 [685.17831963   0.2782       0.34720001   0.46660003   0.16370001]
 [641.13524354   0.31090003   0.42950001   0.45590001   0.142     ]
 ...
 [454.15576636   0.30430004   0.56830001   0.3917       0.2753    ]
 [561.71930141   0.36970001   0.40120003   0.42039999   0.11470001]
 [707.39866509   0.34130001   0.41440001   0.368        0.11919999]][0m
[37m[1m[2023-07-10 10:24:03,632][227910] Max Reward on eval: 928.3017058141995[0m
[37m[1m[2023-07-10 10:24:03,632][227910] Min Reward on eval: 205.12572102794655[0m
[37m[1m[2023-07-10 10:24:03,632][227910] Mean Reward across all agents: 676.1170612795321[0m
[37m[1m[2023-07-10 10:24:03,632][227910] Average Trajectory Length: 996.8593333333333[0m
[36m[2023-07-10 10:24:03,638][227910] mean_value=263.72772458840745, max_value=1138.748920635393[0m
[37m[1m[2023-07-10 10:24:03,640][227910] New mean coefficients: [[ 2.3526704   0.95923775 -0.02776718  0.9358311  -4.85555   ]][0m
[37m[1m[2023-07-10 10:24:03,641][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:24:12,444][227910] train() took 8.80 seconds to complete[0m
[36m[2023-07-10 10:24:12,445][227910] FPS: 436298.14[0m
[36m[2023-07-10 10:24:12,447][227910] itr=143, itrs=2000, Progress: 7.15%[0m
[36m[2023-07-10 10:24:24,655][227910] train() took 12.19 seconds to complete[0m
[36m[2023-07-10 10:24:24,656][227910] FPS: 314949.57[0m
[36m[2023-07-10 10:24:28,620][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:24:28,621][227910] Reward + Measures: [[1004.77010162    0.28680992    0.42751595    0.40363374    0.10455435]][0m
[37m[1m[2023-07-10 10:24:28,621][227910] Max Reward on eval: 1004.7701016229252[0m
[37m[1m[2023-07-10 10:24:28,621][227910] Min Reward on eval: 1004.7701016229252[0m
[37m[1m[2023-07-10 10:24:28,621][227910] Mean Reward across all agents: 1004.7701016229252[0m
[37m[1m[2023-07-10 10:24:28,622][227910] Average Trajectory Length: 998.8489999999999[0m
[36m[2023-07-10 10:24:33,396][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:24:33,397][227910] Reward + Measures: [[395.82197775   0.31310001   0.46370003   0.48800001   0.1053    ]
 [634.92865303   0.34959999   0.37439999   0.40529999   0.099     ]
 [369.17682571   0.3224       0.49770004   0.54930001   0.23210001]
 ...
 [283.38953963   0.41819999   0.3211       0.41050002   0.1186    ]
 [373.16368632   0.38795123   0.32361218   0.38852927   0.08199512]
 [688.49654805   0.33870003   0.44119999   0.39329997   0.1374    ]][0m
[37m[1m[2023-07-10 10:24:33,397][227910] Max Reward on eval: 1036.934053094336[0m
[37m[1m[2023-07-10 10:24:33,397][227910] Min Reward on eval: -223.52876862038391[0m
[37m[1m[2023-07-10 10:24:33,398][227910] Mean Reward across all agents: 593.109366353504[0m
[37m[1m[2023-07-10 10:24:33,398][227910] Average Trajectory Length: 996.2049999999999[0m
[36m[2023-07-10 10:24:33,404][227910] mean_value=166.03724116313217, max_value=1465.6061469572596[0m
[37m[1m[2023-07-10 10:24:33,407][227910] New mean coefficients: [[ 4.0966115  1.390037  -1.5017209  2.0995712 -5.2092595]][0m
[37m[1m[2023-07-10 10:24:33,408][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:24:42,179][227910] train() took 8.77 seconds to complete[0m
[36m[2023-07-10 10:24:42,180][227910] FPS: 437864.39[0m
[36m[2023-07-10 10:24:42,182][227910] itr=144, itrs=2000, Progress: 7.20%[0m
[36m[2023-07-10 10:24:54,315][227910] train() took 12.12 seconds to complete[0m
[36m[2023-07-10 10:24:54,316][227910] FPS: 316900.15[0m
[36m[2023-07-10 10:24:58,300][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:24:58,300][227910] Reward + Measures: [[1200.65584114    0.28652945    0.40815517    0.40169534    0.08213155]][0m
[37m[1m[2023-07-10 10:24:58,300][227910] Max Reward on eval: 1200.6558411406172[0m
[37m[1m[2023-07-10 10:24:58,301][227910] Min Reward on eval: 1200.6558411406172[0m
[37m[1m[2023-07-10 10:24:58,301][227910] Mean Reward across all agents: 1200.6558411406172[0m
[37m[1m[2023-07-10 10:24:58,301][227910] Average Trajectory Length: 998.9653333333333[0m
[36m[2023-07-10 10:25:03,160][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:25:03,161][227910] Reward + Measures: [[ 599.73374573    0.32539999    0.35800001    0.42249998    0.1139    ]
 [ 672.73148466    0.36935702    0.40584525    0.39702687    0.05356847]
 [ 664.64234516    0.31060001    0.4226        0.45860001    0.0674    ]
 ...
 [ 836.21158871    0.28410003    0.38519999    0.42950001    0.13249999]
 [1073.89000967    0.22429998    0.39989999    0.45339999    0.1768    ]
 [ 712.96595782    0.29239997    0.41760001    0.48059997    0.1247    ]][0m
[37m[1m[2023-07-10 10:25:03,161][227910] Max Reward on eval: 1237.07226156838[0m
[37m[1m[2023-07-10 10:25:03,161][227910] Min Reward on eval: 17.056165854458232[0m
[37m[1m[2023-07-10 10:25:03,161][227910] Mean Reward across all agents: 715.2287721598756[0m
[37m[1m[2023-07-10 10:25:03,162][227910] Average Trajectory Length: 997.2963333333333[0m
[36m[2023-07-10 10:25:03,166][227910] mean_value=60.452433186592096, max_value=1573.8900096718687[0m
[37m[1m[2023-07-10 10:25:03,169][227910] New mean coefficients: [[ 4.121867   -0.50333893 -1.5469918   3.3621373  -4.269476  ]][0m
[37m[1m[2023-07-10 10:25:03,170][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:25:12,015][227910] train() took 8.84 seconds to complete[0m
[36m[2023-07-10 10:25:12,015][227910] FPS: 434214.93[0m
[36m[2023-07-10 10:25:12,018][227910] itr=145, itrs=2000, Progress: 7.25%[0m
[36m[2023-07-10 10:25:24,215][227910] train() took 12.18 seconds to complete[0m
[36m[2023-07-10 10:25:24,215][227910] FPS: 315222.95[0m
[36m[2023-07-10 10:25:28,104][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:25:28,104][227910] Reward + Measures: [[1360.91517177    0.27674842    0.39449361    0.40444145    0.0711109 ]][0m
[37m[1m[2023-07-10 10:25:28,105][227910] Max Reward on eval: 1360.9151717724303[0m
[37m[1m[2023-07-10 10:25:28,105][227910] Min Reward on eval: 1360.9151717724303[0m
[37m[1m[2023-07-10 10:25:28,105][227910] Mean Reward across all agents: 1360.9151717724303[0m
[37m[1m[2023-07-10 10:25:28,105][227910] Average Trajectory Length: 998.4513333333333[0m
[36m[2023-07-10 10:25:32,774][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:25:32,780][227910] Reward + Measures: [[1156.02431971    0.19790001    0.42319998    0.47259998    0.13680001]
 [1023.1961048     0.25759998    0.38219997    0.48020002    0.0829    ]
 [ 561.02345448    0.17350002    0.45539999    0.42719999    0.2965    ]
 ...
 [ 901.58126114    0.27739999    0.3448        0.44409999    0.1097    ]
 [ 937.20576041    0.20304707    0.35059497    0.46021768    0.15058571]
 [ 961.41711806    0.25689998    0.34560001    0.4587        0.12810001]][0m
[37m[1m[2023-07-10 10:25:32,780][227910] Max Reward on eval: 1456.8891902565956[0m
[37m[1m[2023-07-10 10:25:32,780][227910] Min Reward on eval: 338.2759437068016[0m
[37m[1m[2023-07-10 10:25:32,780][227910] Mean Reward across all agents: 991.3679684423619[0m
[37m[1m[2023-07-10 10:25:32,781][227910] Average Trajectory Length: 996.1766666666666[0m
[36m[2023-07-10 10:25:32,785][227910] mean_value=-317.36714258183383, max_value=1623.2170564395492[0m
[37m[1m[2023-07-10 10:25:32,788][227910] New mean coefficients: [[ 4.206825  -2.4728365 -1.8647752  4.623275  -4.343763 ]][0m
[37m[1m[2023-07-10 10:25:32,789][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:25:41,511][227910] train() took 8.72 seconds to complete[0m
[36m[2023-07-10 10:25:41,511][227910] FPS: 440332.53[0m
[36m[2023-07-10 10:25:41,514][227910] itr=146, itrs=2000, Progress: 7.30%[0m
[36m[2023-07-10 10:25:53,706][227910] train() took 12.18 seconds to complete[0m
[36m[2023-07-10 10:25:53,706][227910] FPS: 315303.47[0m
[36m[2023-07-10 10:25:57,637][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:25:57,637][227910] Reward + Measures: [[1505.75221671    0.2653982     0.3848379     0.41328922    0.06936428]][0m
[37m[1m[2023-07-10 10:25:57,638][227910] Max Reward on eval: 1505.7522167122047[0m
[37m[1m[2023-07-10 10:25:57,638][227910] Min Reward on eval: 1505.7522167122047[0m
[37m[1m[2023-07-10 10:25:57,638][227910] Mean Reward across all agents: 1505.7522167122047[0m
[37m[1m[2023-07-10 10:25:57,638][227910] Average Trajectory Length: 998.9893333333333[0m
[36m[2023-07-10 10:26:02,374][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:26:02,375][227910] Reward + Measures: [[ 889.36062182    0.2994        0.45619997    0.44070002    0.13500001]
 [1081.61644771    0.21180001    0.4118        0.41700003    0.13880001]
 [1183.27708244    0.20100892    0.4199836     0.46411878    0.16025411]
 ...
 [1452.69717884    0.28800002    0.44759998    0.4436        0.0635    ]
 [ 912.84055809    0.19142719    0.40222248    0.38984522    0.1896783 ]
 [ 971.75588374    0.25699997    0.51520002    0.56300002    0.0993    ]][0m
[37m[1m[2023-07-10 10:26:02,375][227910] Max Reward on eval: 1534.4979328562272[0m
[37m[1m[2023-07-10 10:26:02,375][227910] Min Reward on eval: 386.71164742688416[0m
[37m[1m[2023-07-10 10:26:02,375][227910] Mean Reward across all agents: 1029.0179966096177[0m
[37m[1m[2023-07-10 10:26:02,376][227910] Average Trajectory Length: 998.9683333333332[0m
[36m[2023-07-10 10:26:02,383][227910] mean_value=636.7100574311015, max_value=1831.6491022467148[0m
[37m[1m[2023-07-10 10:26:02,386][227910] New mean coefficients: [[ 4.506846  -3.803605  -1.6635654  5.424058  -3.6849678]][0m
[37m[1m[2023-07-10 10:26:02,387][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:26:11,100][227910] train() took 8.71 seconds to complete[0m
[36m[2023-07-10 10:26:11,101][227910] FPS: 440782.98[0m
[36m[2023-07-10 10:26:11,113][227910] itr=147, itrs=2000, Progress: 7.35%[0m
[36m[2023-07-10 10:26:23,258][227910] train() took 12.12 seconds to complete[0m
[36m[2023-07-10 10:26:23,258][227910] FPS: 316786.76[0m
[36m[2023-07-10 10:26:27,179][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:26:27,179][227910] Reward + Measures: [[1619.69855622    0.25490555    0.37552863    0.42838213    0.06826538]][0m
[37m[1m[2023-07-10 10:26:27,179][227910] Max Reward on eval: 1619.6985562208151[0m
[37m[1m[2023-07-10 10:26:27,180][227910] Min Reward on eval: 1619.6985562208151[0m
[37m[1m[2023-07-10 10:26:27,180][227910] Mean Reward across all agents: 1619.6985562208151[0m
[37m[1m[2023-07-10 10:26:27,180][227910] Average Trajectory Length: 999.6953333333333[0m
[36m[2023-07-10 10:26:31,881][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:26:31,881][227910] Reward + Measures: [[1059.54375897    0.24420002    0.45000002    0.4447        0.14229999]
 [1206.34395964    0.26690003    0.40480003    0.44800001    0.11790001]
 [1081.99272422    0.25580001    0.40279999    0.40720001    0.12679999]
 ...
 [ 779.51176851    0.25229999    0.46029997    0.49110004    0.1714    ]
 [ 809.64840825    0.34780002    0.324         0.33750001    0.06460001]
 [ 950.74137469    0.24870001    0.51340002    0.465         0.21860002]][0m
[37m[1m[2023-07-10 10:26:31,882][227910] Max Reward on eval: 1629.0683125589974[0m
[37m[1m[2023-07-10 10:26:31,882][227910] Min Reward on eval: -75.86237832100596[0m
[37m[1m[2023-07-10 10:26:31,882][227910] Mean Reward across all agents: 1087.2008401733883[0m
[37m[1m[2023-07-10 10:26:31,882][227910] Average Trajectory Length: 997.1746666666667[0m
[36m[2023-07-10 10:26:31,886][227910] mean_value=-1028.9974629764474, max_value=1355.924181786843[0m
[37m[1m[2023-07-10 10:26:31,889][227910] New mean coefficients: [[ 4.962704  -3.7774034 -1.4422711  5.944148  -3.1311264]][0m
[37m[1m[2023-07-10 10:26:31,890][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:26:40,591][227910] train() took 8.70 seconds to complete[0m
[36m[2023-07-10 10:26:40,591][227910] FPS: 441403.39[0m
[36m[2023-07-10 10:26:40,594][227910] itr=148, itrs=2000, Progress: 7.40%[0m
[36m[2023-07-10 10:26:52,730][227910] train() took 12.12 seconds to complete[0m
[36m[2023-07-10 10:26:52,731][227910] FPS: 316785.21[0m
[36m[2023-07-10 10:26:56,708][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:26:56,708][227910] Reward + Measures: [[1775.88546935    0.25480142    0.36843497    0.43094352    0.06426531]][0m
[37m[1m[2023-07-10 10:26:56,708][227910] Max Reward on eval: 1775.885469348431[0m
[37m[1m[2023-07-10 10:26:56,709][227910] Min Reward on eval: 1775.885469348431[0m
[37m[1m[2023-07-10 10:26:56,709][227910] Mean Reward across all agents: 1775.885469348431[0m
[37m[1m[2023-07-10 10:26:56,709][227910] Average Trajectory Length: 999.6859999999999[0m
[36m[2023-07-10 10:27:01,572][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:27:01,573][227910] Reward + Measures: [[1407.85872782    0.22230001    0.4646        0.46429998    0.13720001]
 [1383.65884957    0.28099999    0.37420002    0.45450002    0.0755    ]
 [1290.40744468    0.22070001    0.46810004    0.48179999    0.15680002]
 ...
 [1334.48584171    0.21510001    0.46970001    0.48640004    0.1521    ]
 [1318.90545424    0.21159999    0.42879996    0.43509999    0.16910002]
 [1644.00435947    0.23360001    0.3973        0.42930004    0.10079999]][0m
[37m[1m[2023-07-10 10:27:01,573][227910] Max Reward on eval: 1748.6154516648967[0m
[37m[1m[2023-07-10 10:27:01,573][227910] Min Reward on eval: 681.6910550543311[0m
[37m[1m[2023-07-10 10:27:01,574][227910] Mean Reward across all agents: 1342.1901715847644[0m
[37m[1m[2023-07-10 10:27:01,574][227910] Average Trajectory Length: 998.433[0m
[36m[2023-07-10 10:27:01,579][227910] mean_value=-29.751304619932974, max_value=1531.7518048958113[0m
[37m[1m[2023-07-10 10:27:01,581][227910] New mean coefficients: [[ 4.437635  -3.325105  -0.9554895  7.364629  -2.6871362]][0m
[37m[1m[2023-07-10 10:27:01,582][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:27:10,225][227910] train() took 8.64 seconds to complete[0m
[36m[2023-07-10 10:27:10,225][227910] FPS: 444403.33[0m
[36m[2023-07-10 10:27:10,238][227910] itr=149, itrs=2000, Progress: 7.45%[0m
[36m[2023-07-10 10:27:22,572][227910] train() took 12.31 seconds to complete[0m
[36m[2023-07-10 10:27:22,573][227910] FPS: 311849.79[0m
[36m[2023-07-10 10:27:26,478][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:27:26,478][227910] Reward + Measures: [[1903.07799364    0.24585798    0.36336866    0.4478142     0.06600807]][0m
[37m[1m[2023-07-10 10:27:26,479][227910] Max Reward on eval: 1903.0779936423544[0m
[37m[1m[2023-07-10 10:27:26,479][227910] Min Reward on eval: 1903.0779936423544[0m
[37m[1m[2023-07-10 10:27:26,479][227910] Mean Reward across all agents: 1903.0779936423544[0m
[37m[1m[2023-07-10 10:27:26,479][227910] Average Trajectory Length: 999.4803333333333[0m
[36m[2023-07-10 10:27:31,222][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:27:31,222][227910] Reward + Measures: [[1446.87324248    0.21280001    0.3461        0.42010003    0.1102    ]
 [1747.32333616    0.23900001    0.38180003    0.44299999    0.10610002]
 [1018.10090172    0.23343025    0.3978093     0.48958603    0.15130465]
 ...
 [1731.03359899    0.24849999    0.42010003    0.46610004    0.0582    ]
 [1584.14520526    0.2386        0.38980001    0.44249997    0.11740001]
 [ 293.48235315    0.29367858    0.47096667    0.44773093    0.20223808]][0m
[37m[1m[2023-07-10 10:27:31,223][227910] Max Reward on eval: 1849.7653893759707[0m
[37m[1m[2023-07-10 10:27:31,223][227910] Min Reward on eval: 293.4823531540111[0m
[37m[1m[2023-07-10 10:27:31,223][227910] Mean Reward across all agents: 1268.68107421758[0m
[37m[1m[2023-07-10 10:27:31,223][227910] Average Trajectory Length: 997.183[0m
[36m[2023-07-10 10:27:31,228][227910] mean_value=-26.367231909956075, max_value=1909.347673966505[0m
[37m[1m[2023-07-10 10:27:31,230][227910] New mean coefficients: [[ 4.1523647 -3.2355173 -0.6516048  6.656809  -2.6804686]][0m
[37m[1m[2023-07-10 10:27:31,231][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:27:39,769][227910] train() took 8.54 seconds to complete[0m
[36m[2023-07-10 10:27:39,770][227910] FPS: 449837.81[0m
[36m[2023-07-10 10:27:39,772][227910] itr=150, itrs=2000, Progress: 7.50%[0m
[37m[1m[2023-07-10 10:27:41,717][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000130[0m
[36m[2023-07-10 10:27:53,860][227910] train() took 11.90 seconds to complete[0m
[36m[2023-07-10 10:27:53,860][227910] FPS: 322801.72[0m
[36m[2023-07-10 10:27:57,695][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:27:57,695][227910] Reward + Measures: [[1987.86597777    0.24415684    0.35646304    0.4569895     0.06022867]][0m
[37m[1m[2023-07-10 10:27:57,696][227910] Max Reward on eval: 1987.8659777667706[0m
[37m[1m[2023-07-10 10:27:57,696][227910] Min Reward on eval: 1987.8659777667706[0m
[37m[1m[2023-07-10 10:27:57,696][227910] Mean Reward across all agents: 1987.8659777667706[0m
[37m[1m[2023-07-10 10:27:57,696][227910] Average Trajectory Length: 999.5343333333333[0m
[36m[2023-07-10 10:28:02,281][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:28:02,282][227910] Reward + Measures: [[1050.8226013     0.22650002    0.33319998    0.47720003    0.1515    ]
 [1514.60265101    0.23300003    0.3655        0.4729        0.0618    ]
 [ 998.71643694    0.20990001    0.2471        0.42080003    0.11430001]
 ...
 [ 875.08726444    0.19420001    0.3687        0.40620002    0.14600001]
 [1107.85355016    0.22810002    0.43280002    0.51030004    0.1612    ]
 [1679.10852827    0.2369        0.3626        0.45900002    0.0445    ]][0m
[37m[1m[2023-07-10 10:28:02,282][227910] Max Reward on eval: 1924.7120192131492[0m
[37m[1m[2023-07-10 10:28:02,283][227910] Min Reward on eval: 459.7726540402509[0m
[37m[1m[2023-07-10 10:28:02,283][227910] Mean Reward across all agents: 1204.3965445798488[0m
[37m[1m[2023-07-10 10:28:02,283][227910] Average Trajectory Length: 997.6926666666666[0m
[36m[2023-07-10 10:28:02,287][227910] mean_value=-211.5482377332735, max_value=1987.8894979184913[0m
[37m[1m[2023-07-10 10:28:02,289][227910] New mean coefficients: [[ 4.285602  -3.5750017 -2.5836573  7.456971  -2.6110744]][0m
[37m[1m[2023-07-10 10:28:02,290][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:28:10,772][227910] train() took 8.48 seconds to complete[0m
[36m[2023-07-10 10:28:10,772][227910] FPS: 452820.50[0m
[36m[2023-07-10 10:28:10,775][227910] itr=151, itrs=2000, Progress: 7.55%[0m
[36m[2023-07-10 10:28:22,691][227910] train() took 11.90 seconds to complete[0m
[36m[2023-07-10 10:28:22,691][227910] FPS: 322624.49[0m
[36m[2023-07-10 10:28:26,522][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:28:26,523][227910] Reward + Measures: [[2154.56758427    0.24463844    0.35517654    0.46106124    0.06312534]][0m
[37m[1m[2023-07-10 10:28:26,523][227910] Max Reward on eval: 2154.567584265607[0m
[37m[1m[2023-07-10 10:28:26,523][227910] Min Reward on eval: 2154.567584265607[0m
[37m[1m[2023-07-10 10:28:26,523][227910] Mean Reward across all agents: 2154.567584265607[0m
[37m[1m[2023-07-10 10:28:26,523][227910] Average Trajectory Length: 999.6693333333333[0m
[36m[2023-07-10 10:28:31,087][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:28:31,088][227910] Reward + Measures: [[1751.38339857    0.25190002    0.37260002    0.4673        0.0436    ]
 [ 751.8985957     0.25740001    0.38649997    0.52770007    0.0768    ]
 [1643.13294583    0.26300001    0.40580001    0.45300004    0.0411    ]
 ...
 [1243.70947939    0.26210001    0.36649999    0.51230001    0.0606    ]
 [1714.62220092    0.28649998    0.39580002    0.44870001    0.0508    ]
 [1433.68977257    0.24620001    0.38600001    0.49460003    0.0341    ]][0m
[37m[1m[2023-07-10 10:28:31,088][227910] Max Reward on eval: 1898.6277765132952[0m
[37m[1m[2023-07-10 10:28:31,088][227910] Min Reward on eval: 67.23621668434207[0m
[37m[1m[2023-07-10 10:28:31,089][227910] Mean Reward across all agents: 1199.9587930753967[0m
[37m[1m[2023-07-10 10:28:31,089][227910] Average Trajectory Length: 999.4603333333333[0m
[36m[2023-07-10 10:28:31,091][227910] mean_value=-628.8990692978053, max_value=359.75195540239497[0m
[37m[1m[2023-07-10 10:28:31,094][227910] New mean coefficients: [[ 2.7625856  -2.7659554  -0.24792099  5.710569   -1.4058819 ]][0m
[37m[1m[2023-07-10 10:28:31,095][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:28:39,593][227910] train() took 8.50 seconds to complete[0m
[36m[2023-07-10 10:28:39,594][227910] FPS: 451915.18[0m
[36m[2023-07-10 10:28:39,596][227910] itr=152, itrs=2000, Progress: 7.60%[0m
[36m[2023-07-10 10:28:51,612][227910] train() took 12.00 seconds to complete[0m
[36m[2023-07-10 10:28:51,613][227910] FPS: 319980.71[0m
[36m[2023-07-10 10:28:55,481][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:28:55,481][227910] Reward + Measures: [[2257.03986627    0.24823245    0.35613069    0.46885613    0.05257699]][0m
[37m[1m[2023-07-10 10:28:55,481][227910] Max Reward on eval: 2257.0398662676166[0m
[37m[1m[2023-07-10 10:28:55,482][227910] Min Reward on eval: 2257.0398662676166[0m
[37m[1m[2023-07-10 10:28:55,482][227910] Mean Reward across all agents: 2257.0398662676166[0m
[37m[1m[2023-07-10 10:28:55,482][227910] Average Trajectory Length: 999.8936666666666[0m
[36m[2023-07-10 10:29:00,103][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:29:00,104][227910] Reward + Measures: [[1256.50101256    0.33962885    0.30086145    0.46238151    0.11901223]
 [1997.21305119    0.25330001    0.3743        0.46330005    0.0492    ]
 [1601.94828786    0.28860003    0.37580001    0.45459995    0.0677    ]
 ...
 [2130.82029787    0.25289997    0.33410001    0.49860001    0.0482    ]
 [1127.67847143    0.28400001    0.39019999    0.47699997    0.13970001]
 [1534.21137183    0.30930001    0.2933        0.46830001    0.14570001]][0m
[37m[1m[2023-07-10 10:29:00,104][227910] Max Reward on eval: 2229.5508319218643[0m
[37m[1m[2023-07-10 10:29:00,104][227910] Min Reward on eval: 135.22601590192352[0m
[37m[1m[2023-07-10 10:29:00,105][227910] Mean Reward across all agents: 1548.0660740208452[0m
[37m[1m[2023-07-10 10:29:00,105][227910] Average Trajectory Length: 999.2106666666666[0m
[36m[2023-07-10 10:29:00,109][227910] mean_value=-358.52774421599497, max_value=1783.4414398317458[0m
[37m[1m[2023-07-10 10:29:00,111][227910] New mean coefficients: [[ 3.2993789 -2.4117177 -2.0408938  6.816955  -1.1098632]][0m
[37m[1m[2023-07-10 10:29:00,112][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:29:08,695][227910] train() took 8.58 seconds to complete[0m
[36m[2023-07-10 10:29:08,695][227910] FPS: 447490.59[0m
[36m[2023-07-10 10:29:08,698][227910] itr=153, itrs=2000, Progress: 7.65%[0m
[36m[2023-07-10 10:29:20,774][227910] train() took 12.06 seconds to complete[0m
[36m[2023-07-10 10:29:20,774][227910] FPS: 318420.34[0m
[36m[2023-07-10 10:29:24,672][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:29:24,673][227910] Reward + Measures: [[2366.75901537    0.24514046    0.34639844    0.48447248    0.05492477]][0m
[37m[1m[2023-07-10 10:29:24,673][227910] Max Reward on eval: 2366.7590153661345[0m
[37m[1m[2023-07-10 10:29:24,673][227910] Min Reward on eval: 2366.7590153661345[0m
[37m[1m[2023-07-10 10:29:24,673][227910] Mean Reward across all agents: 2366.7590153661345[0m
[37m[1m[2023-07-10 10:29:24,674][227910] Average Trajectory Length: 999.5506666666666[0m
[36m[2023-07-10 10:29:29,507][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:29:29,512][227910] Reward + Measures: [[2163.33926793    0.24299999    0.3369        0.5126        0.0671    ]
 [1538.21726337    0.23050001    0.39360002    0.53000003    0.1196    ]
 [1668.61851635    0.2422        0.3689        0.49519998    0.0993    ]
 ...
 [1422.71196357    0.24510001    0.40889999    0.5183        0.08759999]
 [1711.02354012    0.2362        0.35370001    0.49860001    0.08020001]
 [1909.95122848    0.23340002    0.37410003    0.51960003    0.081     ]][0m
[37m[1m[2023-07-10 10:29:29,512][227910] Max Reward on eval: 2344.5240309322253[0m
[37m[1m[2023-07-10 10:29:29,513][227910] Min Reward on eval: 514.1036558884196[0m
[37m[1m[2023-07-10 10:29:29,513][227910] Mean Reward across all agents: 1719.5181340123486[0m
[37m[1m[2023-07-10 10:29:29,513][227910] Average Trajectory Length: 999.178[0m
[36m[2023-07-10 10:29:29,517][227910] mean_value=-36.403614919956624, max_value=2495.328662412171[0m
[37m[1m[2023-07-10 10:29:29,519][227910] New mean coefficients: [[ 2.1431427  -1.2354664  -1.4166315   5.96167     0.15484262]][0m
[37m[1m[2023-07-10 10:29:29,520][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:29:38,173][227910] train() took 8.65 seconds to complete[0m
[36m[2023-07-10 10:29:38,173][227910] FPS: 443895.26[0m
[36m[2023-07-10 10:29:38,175][227910] itr=154, itrs=2000, Progress: 7.70%[0m
[36m[2023-07-10 10:29:50,078][227910] train() took 11.89 seconds to complete[0m
[36m[2023-07-10 10:29:50,078][227910] FPS: 322996.08[0m
[36m[2023-07-10 10:29:53,912][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:29:53,912][227910] Reward + Measures: [[2455.3243306     0.24143098    0.33656633    0.50305784    0.05405356]][0m
[37m[1m[2023-07-10 10:29:53,913][227910] Max Reward on eval: 2455.3243305981046[0m
[37m[1m[2023-07-10 10:29:53,913][227910] Min Reward on eval: 2455.3243305981046[0m
[37m[1m[2023-07-10 10:29:53,913][227910] Mean Reward across all agents: 2455.3243305981046[0m
[37m[1m[2023-07-10 10:29:53,913][227910] Average Trajectory Length: 999.8393333333333[0m
[36m[2023-07-10 10:29:58,544][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:29:58,544][227910] Reward + Measures: [[1710.74790338    0.24089999    0.3554        0.51959997    0.08400001]
 [1515.29797713    0.23259997    0.41200003    0.53580004    0.0843    ]
 [1695.72858645    0.23559999    0.3493        0.5499        0.13689999]
 ...
 [1180.15362786    0.3044        0.28550002    0.46250001    0.1278    ]
 [1597.29672066    0.23889999    0.35180002    0.50559998    0.0706    ]
 [1720.14954562    0.22930001    0.38460001    0.45550004    0.08000001]][0m
[37m[1m[2023-07-10 10:29:58,545][227910] Max Reward on eval: 2420.017756563425[0m
[37m[1m[2023-07-10 10:29:58,545][227910] Min Reward on eval: 439.1468051953067[0m
[37m[1m[2023-07-10 10:29:58,545][227910] Mean Reward across all agents: 1633.6727191160398[0m
[37m[1m[2023-07-10 10:29:58,545][227910] Average Trajectory Length: 999.8449999999999[0m
[36m[2023-07-10 10:29:58,549][227910] mean_value=439.8736988103981, max_value=2565.6274729712286[0m
[37m[1m[2023-07-10 10:29:58,552][227910] New mean coefficients: [[ 2.1211472  -2.0043635  -2.051643    4.8808656  -0.25491467]][0m
[37m[1m[2023-07-10 10:29:58,553][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:30:07,029][227910] train() took 8.47 seconds to complete[0m
[36m[2023-07-10 10:30:07,029][227910] FPS: 453135.06[0m
[36m[2023-07-10 10:30:07,032][227910] itr=155, itrs=2000, Progress: 7.75%[0m
[36m[2023-07-10 10:30:19,270][227910] train() took 12.22 seconds to complete[0m
[36m[2023-07-10 10:30:19,270][227910] FPS: 314156.54[0m
[36m[2023-07-10 10:30:23,185][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:30:23,186][227910] Reward + Measures: [[2558.7573786     0.24242887    0.32440206    0.50716585    0.05005393]][0m
[37m[1m[2023-07-10 10:30:23,186][227910] Max Reward on eval: 2558.757378596875[0m
[37m[1m[2023-07-10 10:30:23,187][227910] Min Reward on eval: 2558.757378596875[0m
[37m[1m[2023-07-10 10:30:23,187][227910] Mean Reward across all agents: 2558.757378596875[0m
[37m[1m[2023-07-10 10:30:23,187][227910] Average Trajectory Length: 999.853[0m
[36m[2023-07-10 10:30:27,802][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:30:27,803][227910] Reward + Measures: [[ 343.94017388    0.29800001    0.29710001    0.53730005    0.0753    ]
 [2386.58951237    0.24690001    0.3283        0.49640003    0.0535    ]
 [2178.76511123    0.25080001    0.31110001    0.50229996    0.054     ]
 ...
 [ 992.46068913    0.22939999    0.59009999    0.42080003    0.2737    ]
 [ 895.49021956    0.25729999    0.54820001    0.52540004    0.1901    ]
 [1885.01345181    0.23529999    0.33290002    0.56230003    0.0931    ]][0m
[37m[1m[2023-07-10 10:30:27,803][227910] Max Reward on eval: 2532.7813141740858[0m
[37m[1m[2023-07-10 10:30:27,803][227910] Min Reward on eval: 343.9401738787128[0m
[37m[1m[2023-07-10 10:30:27,803][227910] Mean Reward across all agents: 1770.739433796045[0m
[37m[1m[2023-07-10 10:30:27,804][227910] Average Trajectory Length: 999.7083333333333[0m
[36m[2023-07-10 10:30:27,808][227910] mean_value=158.33888196959776, max_value=2667.9269576179563[0m
[37m[1m[2023-07-10 10:30:27,811][227910] New mean coefficients: [[ 1.9937947 -1.5786324 -2.282676   5.463935  -0.7737025]][0m
[37m[1m[2023-07-10 10:30:27,812][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:30:36,455][227910] train() took 8.64 seconds to complete[0m
[36m[2023-07-10 10:30:36,455][227910] FPS: 444345.84[0m
[36m[2023-07-10 10:30:36,458][227910] itr=156, itrs=2000, Progress: 7.80%[0m
[36m[2023-07-10 10:30:48,499][227910] train() took 12.03 seconds to complete[0m
[36m[2023-07-10 10:30:48,499][227910] FPS: 319286.74[0m
[36m[2023-07-10 10:30:52,415][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:30:52,415][227910] Reward + Measures: [[2627.51456597    0.24028674    0.32408363    0.52575123    0.05114788]][0m
[37m[1m[2023-07-10 10:30:52,415][227910] Max Reward on eval: 2627.5145659685986[0m
[37m[1m[2023-07-10 10:30:52,415][227910] Min Reward on eval: 2627.5145659685986[0m
[37m[1m[2023-07-10 10:30:52,416][227910] Mean Reward across all agents: 2627.5145659685986[0m
[37m[1m[2023-07-10 10:30:52,416][227910] Average Trajectory Length: 999.9793333333333[0m
[36m[2023-07-10 10:30:57,075][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:30:57,076][227910] Reward + Measures: [[2391.62072947    0.25580001    0.28200004    0.5485        0.0843    ]
 [2564.37061941    0.23930001    0.30960003    0.523         0.0592    ]
 [1773.28707356    0.2642        0.3082        0.55680001    0.0897    ]
 ...
 [2104.25325325    0.24390002    0.28720003    0.55730003    0.0788    ]
 [ 892.38613118    0.26949999    0.43260002    0.52759999    0.14350002]
 [1618.30273885    0.25910002    0.32000002    0.57739997    0.07840001]][0m
[37m[1m[2023-07-10 10:30:57,076][227910] Max Reward on eval: 2646.8517809137934[0m
[37m[1m[2023-07-10 10:30:57,076][227910] Min Reward on eval: 641.9927805358893[0m
[37m[1m[2023-07-10 10:30:57,077][227910] Mean Reward across all agents: 1896.5616614075727[0m
[37m[1m[2023-07-10 10:30:57,077][227910] Average Trajectory Length: 998.86[0m
[36m[2023-07-10 10:30:57,081][227910] mean_value=-33.1449872330394, max_value=1941.9645775077788[0m
[37m[1m[2023-07-10 10:30:57,084][227910] New mean coefficients: [[ 1.5395179  -0.23810256 -2.7202437   4.00693    -0.21171957]][0m
[37m[1m[2023-07-10 10:30:57,085][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:31:05,630][227910] train() took 8.54 seconds to complete[0m
[36m[2023-07-10 10:31:05,631][227910] FPS: 449462.63[0m
[36m[2023-07-10 10:31:05,633][227910] itr=157, itrs=2000, Progress: 7.85%[0m
[36m[2023-07-10 10:31:17,580][227910] train() took 11.93 seconds to complete[0m
[36m[2023-07-10 10:31:17,581][227910] FPS: 321834.45[0m
[36m[2023-07-10 10:31:21,429][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:31:21,430][227910] Reward + Measures: [[2760.17329904    0.2401422     0.3158567     0.53846508    0.04753746]][0m
[37m[1m[2023-07-10 10:31:21,430][227910] Max Reward on eval: 2760.1732990446303[0m
[37m[1m[2023-07-10 10:31:21,430][227910] Min Reward on eval: 2760.1732990446303[0m
[37m[1m[2023-07-10 10:31:21,430][227910] Mean Reward across all agents: 2760.1732990446303[0m
[37m[1m[2023-07-10 10:31:21,430][227910] Average Trajectory Length: 999.8466666666666[0m
[36m[2023-07-10 10:31:26,145][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:31:26,145][227910] Reward + Measures: [[2660.16060086    0.24249999    0.32839999    0.53570002    0.0784    ]
 [2560.53303079    0.24389999    0.31140003    0.53899997    0.0497    ]
 [1917.29459264    0.26320001    0.30829999    0.52139997    0.0868    ]
 ...
 [1743.08323178    0.2441        0.33589998    0.47639999    0.0383    ]
 [ 902.50602137    0.30580002    0.3757        0.52840006    0.0418    ]
 [2154.55603378    0.24499999    0.29790002    0.54159999    0.0587    ]][0m
[37m[1m[2023-07-10 10:31:26,146][227910] Max Reward on eval: 2720.119475281984[0m
[37m[1m[2023-07-10 10:31:26,146][227910] Min Reward on eval: -56.715733175206694[0m
[37m[1m[2023-07-10 10:31:26,146][227910] Mean Reward across all agents: 1776.8572506637[0m
[37m[1m[2023-07-10 10:31:26,146][227910] Average Trajectory Length: 999.7906666666667[0m
[36m[2023-07-10 10:31:26,149][227910] mean_value=-655.411214812044, max_value=1109.8895095665794[0m
[37m[1m[2023-07-10 10:31:26,152][227910] New mean coefficients: [[ 0.77895844  0.91568565 -1.554609    1.8967261   0.02681869]][0m
[37m[1m[2023-07-10 10:31:26,153][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:31:34,693][227910] train() took 8.54 seconds to complete[0m
[36m[2023-07-10 10:31:34,694][227910] FPS: 449690.92[0m
[36m[2023-07-10 10:31:34,696][227910] itr=158, itrs=2000, Progress: 7.90%[0m
[36m[2023-07-10 10:31:46,827][227910] train() took 12.12 seconds to complete[0m
[36m[2023-07-10 10:31:46,827][227910] FPS: 316941.36[0m
[36m[2023-07-10 10:31:50,734][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:31:50,739][227910] Reward + Measures: [[2809.81674288    0.23784       0.30400866    0.55895901    0.05042633]][0m
[37m[1m[2023-07-10 10:31:50,739][227910] Max Reward on eval: 2809.816742882217[0m
[37m[1m[2023-07-10 10:31:50,740][227910] Min Reward on eval: 2809.816742882217[0m
[37m[1m[2023-07-10 10:31:50,740][227910] Mean Reward across all agents: 2809.816742882217[0m
[37m[1m[2023-07-10 10:31:50,740][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:31:55,451][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:31:55,451][227910] Reward + Measures: [[2517.82682604    0.24059999    0.3712        0.51809996    0.0557    ]
 [2341.80288676    0.24220002    0.29890001    0.48030001    0.0452    ]
 [1998.39595237    0.2189        0.27999997    0.48769999    0.0778    ]
 ...
 [2814.52393788    0.24430001    0.30840001    0.53320003    0.0407    ]
 [2461.97151683    0.26180002    0.36380002    0.48850003    0.0487    ]
 [2323.85356325    0.25050002    0.29629999    0.4298        0.0533    ]][0m
[37m[1m[2023-07-10 10:31:55,452][227910] Max Reward on eval: 2814.5239378767087[0m
[37m[1m[2023-07-10 10:31:55,452][227910] Min Reward on eval: 194.01806525556603[0m
[37m[1m[2023-07-10 10:31:55,452][227910] Mean Reward across all agents: 2174.405460168102[0m
[37m[1m[2023-07-10 10:31:55,452][227910] Average Trajectory Length: 999.1316666666667[0m
[36m[2023-07-10 10:31:55,457][227910] mean_value=-60.20078403878574, max_value=1891.2685027884552[0m
[37m[1m[2023-07-10 10:31:55,460][227910] New mean coefficients: [[ 1.3657808   0.5072467  -2.543299    2.1236055   0.22081293]][0m
[37m[1m[2023-07-10 10:31:55,461][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:32:04,103][227910] train() took 8.64 seconds to complete[0m
[36m[2023-07-10 10:32:04,104][227910] FPS: 444397.88[0m
[36m[2023-07-10 10:32:04,106][227910] itr=159, itrs=2000, Progress: 7.95%[0m
[36m[2023-07-10 10:32:15,984][227910] train() took 11.86 seconds to complete[0m
[36m[2023-07-10 10:32:15,985][227910] FPS: 323661.33[0m
[36m[2023-07-10 10:32:19,916][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:32:19,921][227910] Reward + Measures: [[2949.07987709    0.23626007    0.30004889    0.55533284    0.04687488]][0m
[37m[1m[2023-07-10 10:32:19,922][227910] Max Reward on eval: 2949.079877086975[0m
[37m[1m[2023-07-10 10:32:19,922][227910] Min Reward on eval: 2949.079877086975[0m
[37m[1m[2023-07-10 10:32:19,922][227910] Mean Reward across all agents: 2949.079877086975[0m
[37m[1m[2023-07-10 10:32:19,922][227910] Average Trajectory Length: 999.245[0m
[36m[2023-07-10 10:32:24,570][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:32:24,571][227910] Reward + Measures: [[1965.69748745    0.226         0.36949998    0.55919999    0.08900001]
 [1577.38428563    0.27330002    0.37329999    0.51659995    0.1076    ]
 [2482.11311126    0.2588        0.35370001    0.4804        0.0755    ]
 ...
 [1120.26805605    0.26429999    0.40760002    0.49590001    0.1053    ]
 [1995.02539495    0.27490002    0.34040001    0.42300001    0.0843    ]
 [1756.89906133    0.22620001    0.39829999    0.56660002    0.13850001]][0m
[37m[1m[2023-07-10 10:32:24,571][227910] Max Reward on eval: 2897.9477426167578[0m
[37m[1m[2023-07-10 10:32:24,572][227910] Min Reward on eval: 931.1290354606463[0m
[37m[1m[2023-07-10 10:32:24,572][227910] Mean Reward across all agents: 2275.2008673459814[0m
[37m[1m[2023-07-10 10:32:24,572][227910] Average Trajectory Length: 998.6833333333333[0m
[36m[2023-07-10 10:32:24,575][227910] mean_value=-179.7557293812502, max_value=1879.9573977839202[0m
[37m[1m[2023-07-10 10:32:24,578][227910] New mean coefficients: [[ 2.2232776  -0.6594364  -2.145635    2.4463327   0.10529014]][0m
[37m[1m[2023-07-10 10:32:24,579][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:32:33,236][227910] train() took 8.66 seconds to complete[0m
[36m[2023-07-10 10:32:33,236][227910] FPS: 443662.14[0m
[36m[2023-07-10 10:32:33,238][227910] itr=160, itrs=2000, Progress: 8.00%[0m
[37m[1m[2023-07-10 10:32:35,240][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000140[0m
[36m[2023-07-10 10:32:47,767][227910] train() took 12.28 seconds to complete[0m
[36m[2023-07-10 10:32:47,767][227910] FPS: 312606.08[0m
[36m[2023-07-10 10:32:51,632][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:32:51,637][227910] Reward + Measures: [[3030.75379124    0.23287719    0.29202989    0.54985684    0.04271833]][0m
[37m[1m[2023-07-10 10:32:51,638][227910] Max Reward on eval: 3030.753791241015[0m
[37m[1m[2023-07-10 10:32:51,638][227910] Min Reward on eval: 3030.753791241015[0m
[37m[1m[2023-07-10 10:32:51,638][227910] Mean Reward across all agents: 3030.753791241015[0m
[37m[1m[2023-07-10 10:32:51,638][227910] Average Trajectory Length: 999.072[0m
[36m[2023-07-10 10:32:56,320][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:32:56,320][227910] Reward + Measures: [[2765.87369223    0.25560004    0.30329999    0.5266        0.0473    ]
 [1744.78400343    0.31930003    0.36759999    0.45140001    0.0757    ]
 [1620.54539749    0.2177        0.4219        0.52280003    0.1488    ]
 ...
 [2439.0713731     0.23280001    0.29699999    0.49919996    0.0645    ]
 [1292.43691298    0.30860001    0.3231        0.42640001    0.0715    ]
 [2661.6914455     0.2402        0.30039999    0.51300001    0.062     ]][0m
[37m[1m[2023-07-10 10:32:56,321][227910] Max Reward on eval: 3039.7162603195757[0m
[37m[1m[2023-07-10 10:32:56,321][227910] Min Reward on eval: 424.8252183076809[0m
[37m[1m[2023-07-10 10:32:56,321][227910] Mean Reward across all agents: 2180.3822619810962[0m
[37m[1m[2023-07-10 10:32:56,322][227910] Average Trajectory Length: 995.1719999999999[0m
[36m[2023-07-10 10:32:56,325][227910] mean_value=-441.07710492272355, max_value=1232.1192599497197[0m
[37m[1m[2023-07-10 10:32:56,327][227910] New mean coefficients: [[ 1.9020612   0.12881392 -1.8306602   2.171558   -0.2454451 ]][0m
[37m[1m[2023-07-10 10:32:56,328][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:33:04,933][227910] train() took 8.60 seconds to complete[0m
[36m[2023-07-10 10:33:04,933][227910] FPS: 446367.88[0m
[36m[2023-07-10 10:33:04,936][227910] itr=161, itrs=2000, Progress: 8.05%[0m
[36m[2023-07-10 10:33:17,076][227910] train() took 12.13 seconds to complete[0m
[36m[2023-07-10 10:33:17,077][227910] FPS: 316658.81[0m
[36m[2023-07-10 10:33:20,978][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:33:20,983][227910] Reward + Measures: [[3160.6724986     0.23111802    0.29301694    0.55300522    0.04032527]][0m
[37m[1m[2023-07-10 10:33:20,983][227910] Max Reward on eval: 3160.6724986036133[0m
[37m[1m[2023-07-10 10:33:20,983][227910] Min Reward on eval: 3160.6724986036133[0m
[37m[1m[2023-07-10 10:33:20,983][227910] Mean Reward across all agents: 3160.6724986036133[0m
[37m[1m[2023-07-10 10:33:20,984][227910] Average Trajectory Length: 999.8206666666666[0m
[36m[2023-07-10 10:33:25,612][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:33:25,613][227910] Reward + Measures: [[2340.9569337     0.21370001    0.30720001    0.48190004    0.0811    ]
 [3002.50389942    0.2369        0.29280001    0.5399        0.0486    ]
 [2304.66861457    0.23140001    0.29229999    0.52300006    0.0952    ]
 ...
 [2039.55429871    0.25209999    0.3017        0.5291        0.0842    ]
 [3070.7338892     0.23410001    0.3012        0.54879999    0.0573    ]
 [2872.96364317    0.2211        0.29189998    0.58399999    0.0557    ]][0m
[37m[1m[2023-07-10 10:33:25,613][227910] Max Reward on eval: 3218.2965378100052[0m
[37m[1m[2023-07-10 10:33:25,614][227910] Min Reward on eval: 1050.5975595200027[0m
[37m[1m[2023-07-10 10:33:25,614][227910] Mean Reward across all agents: 2484.427521147272[0m
[37m[1m[2023-07-10 10:33:25,614][227910] Average Trajectory Length: 998.1613333333333[0m
[36m[2023-07-10 10:33:25,617][227910] mean_value=6.336158382553743, max_value=3567.685417306796[0m
[37m[1m[2023-07-10 10:33:25,620][227910] New mean coefficients: [[ 1.3425386   0.3388064  -0.7561939   2.3964431   0.41942027]][0m
[37m[1m[2023-07-10 10:33:25,621][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:33:34,172][227910] train() took 8.55 seconds to complete[0m
[36m[2023-07-10 10:33:34,172][227910] FPS: 449170.17[0m
[36m[2023-07-10 10:33:34,174][227910] itr=162, itrs=2000, Progress: 8.10%[0m
[36m[2023-07-10 10:33:46,183][227910] train() took 11.99 seconds to complete[0m
[36m[2023-07-10 10:33:46,183][227910] FPS: 320137.87[0m
[36m[2023-07-10 10:33:50,058][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:33:50,058][227910] Reward + Measures: [[3256.69794054    0.23486947    0.29496834    0.55467266    0.03497197]][0m
[37m[1m[2023-07-10 10:33:50,059][227910] Max Reward on eval: 3256.6979405352354[0m
[37m[1m[2023-07-10 10:33:50,059][227910] Min Reward on eval: 3256.6979405352354[0m
[37m[1m[2023-07-10 10:33:50,059][227910] Mean Reward across all agents: 3256.6979405352354[0m
[37m[1m[2023-07-10 10:33:50,059][227910] Average Trajectory Length: 999.3113333333333[0m
[36m[2023-07-10 10:33:54,819][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:33:54,819][227910] Reward + Measures: [[ 844.04332829    0.3716        0.35090002    0.49280006    0.22199999]
 [1759.4172373     0.25500003    0.33070001    0.505         0.1105    ]
 [3026.82513194    0.22909999    0.29790002    0.55159998    0.0408    ]
 ...
 [1709.79378178    0.26460001    0.30109999    0.52399999    0.0911    ]
 [2594.28844906    0.2158        0.26530001    0.47670004    0.0786    ]
 [1146.63839194    0.27710003    0.3123        0.44140002    0.13950001]][0m
[37m[1m[2023-07-10 10:33:54,820][227910] Max Reward on eval: 3190.933564796578[0m
[37m[1m[2023-07-10 10:33:54,820][227910] Min Reward on eval: 826.495209183055[0m
[37m[1m[2023-07-10 10:33:54,820][227910] Mean Reward across all agents: 2510.8406114649138[0m
[37m[1m[2023-07-10 10:33:54,820][227910] Average Trajectory Length: 998.418[0m
[36m[2023-07-10 10:33:54,823][227910] mean_value=-303.622376902295, max_value=1401.8365043249792[0m
[37m[1m[2023-07-10 10:33:54,825][227910] New mean coefficients: [[ 1.2302618   0.583143   -1.4899257   1.7586815   0.47250113]][0m
[37m[1m[2023-07-10 10:33:54,826][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:34:03,413][227910] train() took 8.59 seconds to complete[0m
[36m[2023-07-10 10:34:03,414][227910] FPS: 447268.38[0m
[36m[2023-07-10 10:34:03,416][227910] itr=163, itrs=2000, Progress: 8.15%[0m
[36m[2023-07-10 10:34:15,553][227910] train() took 12.12 seconds to complete[0m
[36m[2023-07-10 10:34:15,553][227910] FPS: 316749.89[0m
[36m[2023-07-10 10:34:19,447][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:34:19,452][227910] Reward + Measures: [[3381.64324578    0.23741183    0.29089013    0.53310084    0.03539823]][0m
[37m[1m[2023-07-10 10:34:19,453][227910] Max Reward on eval: 3381.6432457814453[0m
[37m[1m[2023-07-10 10:34:19,453][227910] Min Reward on eval: 3381.6432457814453[0m
[37m[1m[2023-07-10 10:34:19,453][227910] Mean Reward across all agents: 3381.6432457814453[0m
[37m[1m[2023-07-10 10:34:19,453][227910] Average Trajectory Length: 998.9586666666667[0m
[36m[2023-07-10 10:34:24,139][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:34:24,140][227910] Reward + Measures: [[3011.31851942    0.22620001    0.2744        0.59829998    0.0557    ]
 [2678.28712224    0.23529999    0.28690001    0.56419998    0.0537    ]
 [3138.1568075     0.24330001    0.30829999    0.56339997    0.0491    ]
 ...
 [2162.23416009    0.22360002    0.26830003    0.47049999    0.0592    ]
 [3093.77996516    0.23190001    0.29850003    0.54510003    0.038     ]
 [1751.96646521    0.23699999    0.29300001    0.51520002    0.0402    ]][0m
[37m[1m[2023-07-10 10:34:24,140][227910] Max Reward on eval: 3386.412058119022[0m
[37m[1m[2023-07-10 10:34:24,140][227910] Min Reward on eval: 658.0353316589259[0m
[37m[1m[2023-07-10 10:34:24,140][227910] Mean Reward across all agents: 2596.0427008972156[0m
[37m[1m[2023-07-10 10:34:24,140][227910] Average Trajectory Length: 997.0773333333333[0m
[36m[2023-07-10 10:34:24,143][227910] mean_value=-283.48927620617326, max_value=3420.5560640526946[0m
[37m[1m[2023-07-10 10:34:24,146][227910] New mean coefficients: [[-0.03639126 -0.45680475  0.40970004  1.533393    1.0858706 ]][0m
[37m[1m[2023-07-10 10:34:24,147][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:34:32,881][227910] train() took 8.73 seconds to complete[0m
[36m[2023-07-10 10:34:32,882][227910] FPS: 439725.68[0m
[36m[2023-07-10 10:34:32,884][227910] itr=164, itrs=2000, Progress: 8.20%[0m
[36m[2023-07-10 10:34:44,922][227910] train() took 12.02 seconds to complete[0m
[36m[2023-07-10 10:34:44,922][227910] FPS: 319385.31[0m
[36m[2023-07-10 10:34:48,779][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:34:48,784][227910] Reward + Measures: [[3303.52789502    0.22955529    0.28251544    0.58460194    0.03309818]][0m
[37m[1m[2023-07-10 10:34:48,785][227910] Max Reward on eval: 3303.527895015985[0m
[37m[1m[2023-07-10 10:34:48,785][227910] Min Reward on eval: 3303.527895015985[0m
[37m[1m[2023-07-10 10:34:48,785][227910] Mean Reward across all agents: 3303.527895015985[0m
[37m[1m[2023-07-10 10:34:48,785][227910] Average Trajectory Length: 999.3679999999999[0m
[36m[2023-07-10 10:34:53,388][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:34:53,389][227910] Reward + Measures: [[2689.62416675    0.24240001    0.27830002    0.57509995    0.0545    ]
 [1674.71192739    0.235         0.35130003    0.5183        0.17690001]
 [2583.44451746    0.22409999    0.37330002    0.53940004    0.1069    ]
 ...
 [2698.56660125    0.24169998    0.29650003    0.5693        0.0546    ]
 [2909.59466446    0.23839998    0.2888        0.57010001    0.0448    ]
 [2604.56594165    0.29359999    0.32409999    0.42670003    0.0658    ]][0m
[37m[1m[2023-07-10 10:34:53,389][227910] Max Reward on eval: 3310.738025728101[0m
[37m[1m[2023-07-10 10:34:53,389][227910] Min Reward on eval: 847.0048837515991[0m
[37m[1m[2023-07-10 10:34:53,390][227910] Mean Reward across all agents: 2414.2309061240653[0m
[37m[1m[2023-07-10 10:34:53,390][227910] Average Trajectory Length: 997.9426666666666[0m
[36m[2023-07-10 10:34:53,393][227910] mean_value=-365.22793660055964, max_value=1404.4802890790743[0m
[37m[1m[2023-07-10 10:34:53,396][227910] New mean coefficients: [[-0.9825407   0.01522982  1.355502    0.05061686  2.199102  ]][0m
[37m[1m[2023-07-10 10:34:53,397][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:35:02,023][227910] train() took 8.62 seconds to complete[0m
[36m[2023-07-10 10:35:02,024][227910] FPS: 445232.79[0m
[36m[2023-07-10 10:35:02,026][227910] itr=165, itrs=2000, Progress: 8.25%[0m
[36m[2023-07-10 10:35:14,069][227910] train() took 12.03 seconds to complete[0m
[36m[2023-07-10 10:35:14,070][227910] FPS: 319271.43[0m
[36m[2023-07-10 10:35:17,947][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:35:17,952][227910] Reward + Measures: [[3116.76017488    0.22846159    0.27756453    0.60635102    0.04011505]][0m
[37m[1m[2023-07-10 10:35:17,952][227910] Max Reward on eval: 3116.7601748774605[0m
[37m[1m[2023-07-10 10:35:17,952][227910] Min Reward on eval: 3116.7601748774605[0m
[37m[1m[2023-07-10 10:35:17,953][227910] Mean Reward across all agents: 3116.7601748774605[0m
[37m[1m[2023-07-10 10:35:17,953][227910] Average Trajectory Length: 999.5253333333333[0m
[36m[2023-07-10 10:35:22,598][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:35:22,599][227910] Reward + Measures: [[2323.61294248    0.2349        0.29879999    0.51789999    0.09120001]
 [1734.59275772    0.2297        0.38540003    0.43039998    0.1586    ]
 [3051.81295059    0.23020001    0.271         0.57179999    0.0342    ]
 ...
 [3139.22947147    0.2316        0.2854        0.61250001    0.0478    ]
 [1569.23834479    0.24160002    0.38680002    0.47690001    0.1675    ]
 [2497.32554444    0.23909998    0.33049998    0.47730002    0.048     ]][0m
[37m[1m[2023-07-10 10:35:22,599][227910] Max Reward on eval: 3166.99005632028[0m
[37m[1m[2023-07-10 10:35:22,600][227910] Min Reward on eval: 481.617068038031[0m
[37m[1m[2023-07-10 10:35:22,600][227910] Mean Reward across all agents: 2127.6128161487695[0m
[37m[1m[2023-07-10 10:35:22,600][227910] Average Trajectory Length: 999.533[0m
[36m[2023-07-10 10:35:22,604][227910] mean_value=-144.62571363933398, max_value=3046.14710659197[0m
[37m[1m[2023-07-10 10:35:22,607][227910] New mean coefficients: [[-0.83460957  0.45836288  3.5444913  -0.6646527   2.8080797 ]][0m
[37m[1m[2023-07-10 10:35:22,608][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:35:31,198][227910] train() took 8.59 seconds to complete[0m
[36m[2023-07-10 10:35:31,199][227910] FPS: 447116.49[0m
[36m[2023-07-10 10:35:31,201][227910] itr=166, itrs=2000, Progress: 8.30%[0m
[36m[2023-07-10 10:35:43,382][227910] train() took 12.17 seconds to complete[0m
[36m[2023-07-10 10:35:43,382][227910] FPS: 315635.74[0m
[36m[2023-07-10 10:35:47,273][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:35:47,278][227910] Reward + Measures: [[2915.62777347    0.2313741     0.28606564    0.5897944     0.05001263]][0m
[37m[1m[2023-07-10 10:35:47,279][227910] Max Reward on eval: 2915.6277734745768[0m
[37m[1m[2023-07-10 10:35:47,279][227910] Min Reward on eval: 2915.6277734745768[0m
[37m[1m[2023-07-10 10:35:47,279][227910] Mean Reward across all agents: 2915.6277734745768[0m
[37m[1m[2023-07-10 10:35:47,279][227910] Average Trajectory Length: 999.7819999999999[0m
[36m[2023-07-10 10:35:52,151][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:35:52,152][227910] Reward + Measures: [[ 910.38689452    0.26809999    0.61250001    0.27600002    0.31640002]
 [ 748.45479042    0.25139999    0.59910005    0.28529999    0.37920004]
 [2705.62851808    0.23629999    0.29460001    0.52800006    0.0597    ]
 ...
 [2365.35180544    0.2438        0.3935        0.47250006    0.08110001]
 [2730.7929898     0.22320001    0.30160001    0.58179998    0.0667    ]
 [2102.52405407    0.2326        0.35250002    0.56230003    0.10829999]][0m
[37m[1m[2023-07-10 10:35:52,152][227910] Max Reward on eval: 2921.2283186096[0m
[37m[1m[2023-07-10 10:35:52,153][227910] Min Reward on eval: 441.36794806715335[0m
[37m[1m[2023-07-10 10:35:52,153][227910] Mean Reward across all agents: 2101.6466304904206[0m
[37m[1m[2023-07-10 10:35:52,153][227910] Average Trajectory Length: 999.0136666666666[0m
[36m[2023-07-10 10:35:52,157][227910] mean_value=-166.39700587327377, max_value=1693.2699274108797[0m
[37m[1m[2023-07-10 10:35:52,160][227910] New mean coefficients: [[-2.3032548   0.67680085  6.67072    -1.8839471   3.4805431 ]][0m
[37m[1m[2023-07-10 10:35:52,161][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:36:00,821][227910] train() took 8.66 seconds to complete[0m
[36m[2023-07-10 10:36:00,821][227910] FPS: 443511.05[0m
[36m[2023-07-10 10:36:00,823][227910] itr=167, itrs=2000, Progress: 8.35%[0m
[36m[2023-07-10 10:36:12,974][227910] train() took 12.13 seconds to complete[0m
[36m[2023-07-10 10:36:12,974][227910] FPS: 316446.04[0m
[36m[2023-07-10 10:36:16,892][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:36:16,893][227910] Reward + Measures: [[2783.84620484    0.23364669    0.28995061    0.58724481    0.06151951]][0m
[37m[1m[2023-07-10 10:36:16,893][227910] Max Reward on eval: 2783.846204836201[0m
[37m[1m[2023-07-10 10:36:16,893][227910] Min Reward on eval: 2783.846204836201[0m
[37m[1m[2023-07-10 10:36:16,893][227910] Mean Reward across all agents: 2783.846204836201[0m
[37m[1m[2023-07-10 10:36:16,894][227910] Average Trajectory Length: 999.997[0m
[36m[2023-07-10 10:36:21,530][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:36:21,531][227910] Reward + Measures: [[1081.11434462    0.28510001    0.52829999    0.33739999    0.30350003]
 [2550.13314729    0.23709999    0.29159999    0.59299999    0.0565    ]
 [1888.14347277    0.24949999    0.42640001    0.46799999    0.17290001]
 ...
 [1806.12950351    0.26800001    0.45170003    0.43129998    0.17569999]
 [1758.4541213     0.25150001    0.4427        0.40980002    0.1603    ]
 [1925.01924945    0.2527        0.43889999    0.45560002    0.15460001]][0m
[37m[1m[2023-07-10 10:36:21,531][227910] Max Reward on eval: 2843.0510954778642[0m
[37m[1m[2023-07-10 10:36:21,531][227910] Min Reward on eval: 216.71858868726994[0m
[37m[1m[2023-07-10 10:36:21,531][227910] Mean Reward across all agents: 1808.5875098454767[0m
[37m[1m[2023-07-10 10:36:21,532][227910] Average Trajectory Length: 999.529[0m
[36m[2023-07-10 10:36:21,534][227910] mean_value=-238.96771798343102, max_value=1645.071330814506[0m
[37m[1m[2023-07-10 10:36:21,537][227910] New mean coefficients: [[-3.8957953   0.43328816  7.7549872  -3.0504608   3.602964  ]][0m
[37m[1m[2023-07-10 10:36:21,538][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:36:30,050][227910] train() took 8.51 seconds to complete[0m
[36m[2023-07-10 10:36:30,050][227910] FPS: 451248.07[0m
[36m[2023-07-10 10:36:30,053][227910] itr=168, itrs=2000, Progress: 8.40%[0m
[36m[2023-07-10 10:36:42,057][227910] train() took 11.99 seconds to complete[0m
[36m[2023-07-10 10:36:42,057][227910] FPS: 320280.93[0m
[36m[2023-07-10 10:36:45,894][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:36:45,899][227910] Reward + Measures: [[2611.96030466    0.23506722    0.29449022    0.57857472    0.07576951]][0m
[37m[1m[2023-07-10 10:36:45,899][227910] Max Reward on eval: 2611.960304662314[0m
[37m[1m[2023-07-10 10:36:45,899][227910] Min Reward on eval: 2611.960304662314[0m
[37m[1m[2023-07-10 10:36:45,899][227910] Mean Reward across all agents: 2611.960304662314[0m
[37m[1m[2023-07-10 10:36:45,900][227910] Average Trajectory Length: 999.8386666666667[0m
[36m[2023-07-10 10:36:50,492][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:36:50,493][227910] Reward + Measures: [[1071.27951206    0.19980001    0.52850002    0.3858        0.2775    ]
 [1365.72012865    0.21359999    0.50870001    0.4161        0.24250002]
 [ 509.30507178    0.18099999    0.52030009    0.2868        0.28870001]
 ...
 [1766.69074595    0.2264        0.41240001    0.46819997    0.16440001]
 [1715.82168834    0.2352        0.46649995    0.43490002    0.1645    ]
 [1042.04550829    0.23280001    0.46549997    0.46109995    0.2228    ]][0m
[37m[1m[2023-07-10 10:36:50,493][227910] Max Reward on eval: 2651.6558397469344[0m
[37m[1m[2023-07-10 10:36:50,493][227910] Min Reward on eval: -45.13017747029371[0m
[37m[1m[2023-07-10 10:36:50,493][227910] Mean Reward across all agents: 1494.7802087144053[0m
[37m[1m[2023-07-10 10:36:50,494][227910] Average Trajectory Length: 999.6413333333333[0m
[36m[2023-07-10 10:36:50,496][227910] mean_value=-265.23919343479037, max_value=1098.4610963315943[0m
[37m[1m[2023-07-10 10:36:50,499][227910] New mean coefficients: [[-4.3962216 -0.1323722  8.748454  -3.8781273  4.206343 ]][0m
[37m[1m[2023-07-10 10:36:50,500][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:36:59,016][227910] train() took 8.51 seconds to complete[0m
[36m[2023-07-10 10:36:59,017][227910] FPS: 450974.77[0m
[36m[2023-07-10 10:36:59,019][227910] itr=169, itrs=2000, Progress: 8.45%[0m
[36m[2023-07-10 10:37:10,930][227910] train() took 11.90 seconds to complete[0m
[36m[2023-07-10 10:37:10,930][227910] FPS: 322813.34[0m
[36m[2023-07-10 10:37:14,761][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:37:14,767][227910] Reward + Measures: [[2371.46464169    0.23174554    0.29486942    0.56680018    0.10118641]][0m
[37m[1m[2023-07-10 10:37:14,767][227910] Max Reward on eval: 2371.4646416915284[0m
[37m[1m[2023-07-10 10:37:14,767][227910] Min Reward on eval: 2371.4646416915284[0m
[37m[1m[2023-07-10 10:37:14,767][227910] Mean Reward across all agents: 2371.4646416915284[0m
[37m[1m[2023-07-10 10:37:14,768][227910] Average Trajectory Length: 999.358[0m
[36m[2023-07-10 10:37:19,415][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:37:19,416][227910] Reward + Measures: [[2433.82248651    0.23000002    0.32230002    0.57520002    0.0983    ]
 [ 758.60966649    0.27289999    0.54140002    0.29460001    0.28840002]
 [1208.98151877    0.26610002    0.48179999    0.36970001    0.23580001]
 ...
 [ 540.4886027     0.27020001    0.6031        0.2493        0.35430002]
 [ 389.14029894    0.2484        0.61729997    0.1925        0.43150002]
 [1651.85755154    0.24969999    0.4258        0.49390003    0.14470001]][0m
[37m[1m[2023-07-10 10:37:19,416][227910] Max Reward on eval: 2433.822486513108[0m
[37m[1m[2023-07-10 10:37:19,416][227910] Min Reward on eval: -226.90782779472647[0m
[37m[1m[2023-07-10 10:37:19,417][227910] Mean Reward across all agents: 939.080763632867[0m
[37m[1m[2023-07-10 10:37:19,417][227910] Average Trajectory Length: 990.7403333333333[0m
[36m[2023-07-10 10:37:19,419][227910] mean_value=-318.09871345593314, max_value=907.44981169518[0m
[37m[1m[2023-07-10 10:37:19,422][227910] New mean coefficients: [[-3.473402   1.396097   7.573567  -3.91809    5.0365596]][0m
[37m[1m[2023-07-10 10:37:19,423][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:37:27,937][227910] train() took 8.51 seconds to complete[0m
[36m[2023-07-10 10:37:27,938][227910] FPS: 451075.71[0m
[36m[2023-07-10 10:37:27,940][227910] itr=170, itrs=2000, Progress: 8.50%[0m
[37m[1m[2023-07-10 10:37:29,933][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000150[0m
[36m[2023-07-10 10:37:42,163][227910] train() took 11.98 seconds to complete[0m
[36m[2023-07-10 10:37:42,163][227910] FPS: 320421.53[0m
[36m[2023-07-10 10:37:46,075][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:37:46,076][227910] Reward + Measures: [[2101.38801808    0.23106904    0.30777442    0.55824131    0.12772912]][0m
[37m[1m[2023-07-10 10:37:46,076][227910] Max Reward on eval: 2101.388018076951[0m
[37m[1m[2023-07-10 10:37:46,076][227910] Min Reward on eval: 2101.388018076951[0m
[37m[1m[2023-07-10 10:37:46,077][227910] Mean Reward across all agents: 2101.388018076951[0m
[37m[1m[2023-07-10 10:37:46,077][227910] Average Trajectory Length: 999.7843333333333[0m
[36m[2023-07-10 10:37:50,667][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:37:50,668][227910] Reward + Measures: [[1249.3586566     0.25890002    0.528         0.38209999    0.26030001]
 [1482.31188395    0.26209998    0.49160004    0.4025        0.21069999]
 [ 462.98148585    0.25840002    0.50870007    0.29099998    0.3163    ]
 ...
 [1825.66877557    0.23400001    0.34770003    0.52869999    0.16870001]
 [1518.95875716    0.23339999    0.42799997    0.46059999    0.18910001]
 [2034.85478627    0.25470001    0.3996        0.47060004    0.13959999]][0m
[37m[1m[2023-07-10 10:37:50,668][227910] Max Reward on eval: 2432.389880112279[0m
[37m[1m[2023-07-10 10:37:50,668][227910] Min Reward on eval: 61.72113702134811[0m
[37m[1m[2023-07-10 10:37:50,669][227910] Mean Reward across all agents: 1254.391324321678[0m
[37m[1m[2023-07-10 10:37:50,669][227910] Average Trajectory Length: 999.5873333333333[0m
[36m[2023-07-10 10:37:50,671][227910] mean_value=-275.15714192395245, max_value=744.3805330160636[0m
[37m[1m[2023-07-10 10:37:50,673][227910] New mean coefficients: [[-1.7213889  0.8231019  6.645566  -3.6091595  5.5714397]][0m
[37m[1m[2023-07-10 10:37:50,674][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:37:59,208][227910] train() took 8.53 seconds to complete[0m
[36m[2023-07-10 10:37:59,208][227910] FPS: 450050.68[0m
[36m[2023-07-10 10:37:59,211][227910] itr=171, itrs=2000, Progress: 8.55%[0m
[36m[2023-07-10 10:38:11,322][227910] train() took 12.10 seconds to complete[0m
[36m[2023-07-10 10:38:11,322][227910] FPS: 317434.62[0m
[36m[2023-07-10 10:38:15,181][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:38:15,187][227910] Reward + Measures: [[1870.35256878    0.22989701    0.32954067    0.54565537    0.14773867]][0m
[37m[1m[2023-07-10 10:38:15,187][227910] Max Reward on eval: 1870.3525687755273[0m
[37m[1m[2023-07-10 10:38:15,187][227910] Min Reward on eval: 1870.3525687755273[0m
[37m[1m[2023-07-10 10:38:15,187][227910] Mean Reward across all agents: 1870.3525687755273[0m
[37m[1m[2023-07-10 10:38:15,188][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:38:19,996][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:38:19,996][227910] Reward + Measures: [[ 466.55438969    0.38210002    0.66339999    0.2296        0.45280001]
 [1219.80839563    0.2189        0.37830001    0.44750005    0.20110002]
 [ 284.98844062    0.51660001    0.76000005    0.10320001    0.53940004]
 ...
 [ 213.46757576    0.45749998    0.72030002    0.1068        0.5165    ]
 [ 313.36351244    0.58779997    0.77639997    0.0614        0.63689995]
 [ 100.38378554    0.4901        0.73199999    0.1134        0.51919997]][0m
[37m[1m[2023-07-10 10:38:19,996][227910] Max Reward on eval: 1806.9170620657503[0m
[37m[1m[2023-07-10 10:38:19,997][227910] Min Reward on eval: -117.63261919555953[0m
[37m[1m[2023-07-10 10:38:19,997][227910] Mean Reward across all agents: 705.2001566031164[0m
[37m[1m[2023-07-10 10:38:19,997][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:38:20,000][227910] mean_value=-200.9727839876274, max_value=1137.7845735196997[0m
[37m[1m[2023-07-10 10:38:20,003][227910] New mean coefficients: [[-1.2696252  1.4910719  6.5376987 -3.0278597  5.861274 ]][0m
[37m[1m[2023-07-10 10:38:20,004][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:38:28,582][227910] train() took 8.58 seconds to complete[0m
[36m[2023-07-10 10:38:28,582][227910] FPS: 447755.21[0m
[36m[2023-07-10 10:38:28,584][227910] itr=172, itrs=2000, Progress: 8.60%[0m
[36m[2023-07-10 10:38:40,394][227910] train() took 11.80 seconds to complete[0m
[36m[2023-07-10 10:38:40,394][227910] FPS: 325548.54[0m
[36m[2023-07-10 10:38:44,224][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:38:44,229][227910] Reward + Measures: [[1574.75317159    0.22401156    0.36779618    0.52257591    0.17670214]][0m
[37m[1m[2023-07-10 10:38:44,230][227910] Max Reward on eval: 1574.7531715852456[0m
[37m[1m[2023-07-10 10:38:44,230][227910] Min Reward on eval: 1574.7531715852456[0m
[37m[1m[2023-07-10 10:38:44,230][227910] Mean Reward across all agents: 1574.7531715852456[0m
[37m[1m[2023-07-10 10:38:44,230][227910] Average Trajectory Length: 999.8116666666666[0m
[36m[2023-07-10 10:38:48,803][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:38:48,803][227910] Reward + Measures: [[ 660.92617373    0.2431        0.5614        0.53570002    0.19690001]
 [ 449.30389948    0.2335        0.58680004    0.52490002    0.2174    ]
 [ 595.4612402     0.2124        0.58899999    0.45520002    0.26629999]
 ...
 [1133.25870262    0.23360001    0.49559999    0.41490003    0.24860001]
 [ 697.46123722    0.21470001    0.54730004    0.3143        0.382     ]
 [1089.63475255    0.24460001    0.53490001    0.3802        0.26089999]][0m
[37m[1m[2023-07-10 10:38:48,803][227910] Max Reward on eval: 1781.9013370953849[0m
[37m[1m[2023-07-10 10:38:48,804][227910] Min Reward on eval: -107.46608616965823[0m
[37m[1m[2023-07-10 10:38:48,804][227910] Mean Reward across all agents: 717.6047547886803[0m
[37m[1m[2023-07-10 10:38:48,804][227910] Average Trajectory Length: 999.6773333333333[0m
[36m[2023-07-10 10:38:48,807][227910] mean_value=-269.32259434931535, max_value=1555.4764023919356[0m
[37m[1m[2023-07-10 10:38:48,810][227910] New mean coefficients: [[ 0.84146166  1.6562276   3.5843062  -2.8242652   6.215605  ]][0m
[37m[1m[2023-07-10 10:38:48,811][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:38:57,337][227910] train() took 8.53 seconds to complete[0m
[36m[2023-07-10 10:38:57,338][227910] FPS: 450434.93[0m
[36m[2023-07-10 10:38:57,340][227910] itr=173, itrs=2000, Progress: 8.65%[0m
[36m[2023-07-10 10:39:09,242][227910] train() took 11.89 seconds to complete[0m
[36m[2023-07-10 10:39:09,242][227910] FPS: 323056.35[0m
[36m[2023-07-10 10:39:13,051][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:39:13,052][227910] Reward + Measures: [[1573.1364199     0.22148833    0.38087231    0.50451034    0.196696  ]][0m
[37m[1m[2023-07-10 10:39:13,052][227910] Max Reward on eval: 1573.1364199035065[0m
[37m[1m[2023-07-10 10:39:13,052][227910] Min Reward on eval: 1573.1364199035065[0m
[37m[1m[2023-07-10 10:39:13,052][227910] Mean Reward across all agents: 1573.1364199035065[0m
[37m[1m[2023-07-10 10:39:13,052][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:39:17,676][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:39:17,676][227910] Reward + Measures: [[663.64002718   0.20469999   0.59820002   0.28119999   0.3635    ]
 [746.47100195   0.23510002   0.62459999   0.28509998   0.39860001]
 [826.44386384   0.2203       0.60699999   0.3177       0.35549998]
 ...
 [290.17830817   0.20560001   0.63920003   0.20390001   0.43430001]
 [695.84610227   0.2131       0.58990002   0.2694       0.37830001]
 [785.9212432    0.22849999   0.65350002   0.28119999   0.4269    ]][0m
[37m[1m[2023-07-10 10:39:17,676][227910] Max Reward on eval: 1555.5373760121875[0m
[37m[1m[2023-07-10 10:39:17,677][227910] Min Reward on eval: 290.17830816999776[0m
[37m[1m[2023-07-10 10:39:17,677][227910] Mean Reward across all agents: 831.724042475922[0m
[37m[1m[2023-07-10 10:39:17,677][227910] Average Trajectory Length: 999.7546666666666[0m
[36m[2023-07-10 10:39:17,682][227910] mean_value=242.86420307263813, max_value=1217.4677962996861[0m
[37m[1m[2023-07-10 10:39:17,685][227910] New mean coefficients: [[ 1.2470977  1.8576427  3.4476578 -1.9602215  6.4313197]][0m
[37m[1m[2023-07-10 10:39:17,686][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:39:26,361][227910] train() took 8.67 seconds to complete[0m
[36m[2023-07-10 10:39:26,361][227910] FPS: 442709.88[0m
[36m[2023-07-10 10:39:26,364][227910] itr=174, itrs=2000, Progress: 8.70%[0m
[36m[2023-07-10 10:39:38,509][227910] train() took 12.13 seconds to complete[0m
[36m[2023-07-10 10:39:38,509][227910] FPS: 316628.36[0m
[36m[2023-07-10 10:39:42,336][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:39:42,341][227910] Reward + Measures: [[1693.09743653    0.2165003     0.3931573     0.48427498    0.21091157]][0m
[37m[1m[2023-07-10 10:39:42,341][227910] Max Reward on eval: 1693.0974365278157[0m
[37m[1m[2023-07-10 10:39:42,341][227910] Min Reward on eval: 1693.0974365278157[0m
[37m[1m[2023-07-10 10:39:42,342][227910] Mean Reward across all agents: 1693.0974365278157[0m
[37m[1m[2023-07-10 10:39:42,342][227910] Average Trajectory Length: 999.842[0m
[36m[2023-07-10 10:39:46,979][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:39:46,980][227910] Reward + Measures: [[434.12899119   0.33250001   0.68239999   0.32520002   0.4736    ]
 [237.44403938   0.21359999   0.66570002   0.23800002   0.47960004]
 [481.52521474   0.31380001   0.5844       0.29819998   0.40619999]
 ...
 [925.86411914   0.20770001   0.46650001   0.37550002   0.40000001]
 [674.34597534   0.26799998   0.58740002   0.40900001   0.32790002]
 [941.44293905   0.2414       0.56450003   0.44630003   0.29410002]][0m
[37m[1m[2023-07-10 10:39:46,980][227910] Max Reward on eval: 1736.9488320001633[0m
[37m[1m[2023-07-10 10:39:46,980][227910] Min Reward on eval: 25.29836073293118[0m
[37m[1m[2023-07-10 10:39:46,980][227910] Mean Reward across all agents: 631.5016857994584[0m
[37m[1m[2023-07-10 10:39:46,981][227910] Average Trajectory Length: 999.7216666666666[0m
[36m[2023-07-10 10:39:46,984][227910] mean_value=-102.07683333367497, max_value=2196.4552004076077[0m
[37m[1m[2023-07-10 10:39:46,986][227910] New mean coefficients: [[ 2.771409    1.5669926   0.08493638 -0.578526    5.5885205 ]][0m
[37m[1m[2023-07-10 10:39:46,987][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:39:56,051][227910] train() took 9.06 seconds to complete[0m
[36m[2023-07-10 10:39:56,051][227910] FPS: 423748.16[0m
[36m[2023-07-10 10:39:56,054][227910] itr=175, itrs=2000, Progress: 8.75%[0m
[36m[2023-07-10 10:40:08,913][227910] train() took 12.84 seconds to complete[0m
[36m[2023-07-10 10:40:08,913][227910] FPS: 298997.36[0m
[36m[2023-07-10 10:40:13,222][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:40:13,223][227910] Reward + Measures: [[1880.7796261     0.21663098    0.3756088     0.49215642    0.1985565 ]][0m
[37m[1m[2023-07-10 10:40:13,223][227910] Max Reward on eval: 1880.7796261021963[0m
[37m[1m[2023-07-10 10:40:13,223][227910] Min Reward on eval: 1880.7796261021963[0m
[37m[1m[2023-07-10 10:40:13,223][227910] Mean Reward across all agents: 1880.7796261021963[0m
[37m[1m[2023-07-10 10:40:13,224][227910] Average Trajectory Length: 999.843[0m
[36m[2023-07-10 10:40:18,420][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:40:18,421][227910] Reward + Measures: [[ 859.35278947    0.26730001    0.70590001    0.26760003    0.38400003]
 [1330.41564015    0.22089998    0.48790002    0.4147        0.29380003]
 [1061.12639601    0.1839        0.64300007    0.2791        0.39050004]
 ...
 [1710.55038127    0.2177        0.42869997    0.44450003    0.23310001]
 [1811.28221195    0.21180001    0.3274        0.49710003    0.19690001]
 [ 760.3533114     0.352         0.6717        0.34970003    0.4409    ]][0m
[37m[1m[2023-07-10 10:40:18,421][227910] Max Reward on eval: 1906.1044166225242[0m
[37m[1m[2023-07-10 10:40:18,422][227910] Min Reward on eval: 155.63712191160303[0m
[37m[1m[2023-07-10 10:40:18,422][227910] Mean Reward across all agents: 1294.4393798339468[0m
[37m[1m[2023-07-10 10:40:18,422][227910] Average Trajectory Length: 998.9946666666666[0m
[36m[2023-07-10 10:40:18,428][227910] mean_value=241.84394851639624, max_value=1788.5442023083335[0m
[37m[1m[2023-07-10 10:40:18,431][227910] New mean coefficients: [[ 3.0057452   0.56486094  0.73246133 -0.31618756  5.89932   ]][0m
[37m[1m[2023-07-10 10:40:18,432][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:40:27,658][227910] train() took 9.22 seconds to complete[0m
[36m[2023-07-10 10:40:27,659][227910] FPS: 416285.07[0m
[36m[2023-07-10 10:40:27,661][227910] itr=176, itrs=2000, Progress: 8.80%[0m
[36m[2023-07-10 10:40:40,374][227910] train() took 12.70 seconds to complete[0m
[36m[2023-07-10 10:40:40,374][227910] FPS: 302436.00[0m
[36m[2023-07-10 10:40:44,727][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:40:44,727][227910] Reward + Measures: [[2106.25538423    0.218403      0.35151365    0.50219631    0.18037532]][0m
[37m[1m[2023-07-10 10:40:44,727][227910] Max Reward on eval: 2106.255384231048[0m
[37m[1m[2023-07-10 10:40:44,728][227910] Min Reward on eval: 2106.255384231048[0m
[37m[1m[2023-07-10 10:40:44,728][227910] Mean Reward across all agents: 2106.255384231048[0m
[37m[1m[2023-07-10 10:40:44,728][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 10:40:49,855][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:40:49,860][227910] Reward + Measures: [[ 993.9998068     0.1902        0.5456        0.31780002    0.36360002]
 [ 967.8144879     0.2441        0.38510001    0.43810001    0.32179999]
 [1311.50280432    0.20009999    0.4594        0.34910002    0.30790001]
 ...
 [1792.94786083    0.2095        0.47230002    0.42500001    0.22519998]
 [1658.45200019    0.22359999    0.49270001    0.44979998    0.22070001]
 [ 928.26038985    0.23819999    0.52710003    0.30669999    0.29210001]][0m
[37m[1m[2023-07-10 10:40:49,860][227910] Max Reward on eval: 2035.3358508887002[0m
[37m[1m[2023-07-10 10:40:49,861][227910] Min Reward on eval: 586.7171369424672[0m
[37m[1m[2023-07-10 10:40:49,861][227910] Mean Reward across all agents: 1323.18406069862[0m
[37m[1m[2023-07-10 10:40:49,861][227910] Average Trajectory Length: 999.226[0m
[36m[2023-07-10 10:40:49,866][227910] mean_value=177.65687726491527, max_value=2188.488284473317[0m
[37m[1m[2023-07-10 10:40:49,869][227910] New mean coefficients: [[ 4.6810446  -0.27393168 -1.041259    1.3600619   5.559118  ]][0m
[37m[1m[2023-07-10 10:40:49,870][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:40:59,200][227910] train() took 9.33 seconds to complete[0m
[36m[2023-07-10 10:40:59,200][227910] FPS: 411672.69[0m
[36m[2023-07-10 10:40:59,203][227910] itr=177, itrs=2000, Progress: 8.85%[0m
[36m[2023-07-10 10:41:12,090][227910] train() took 12.87 seconds to complete[0m
[36m[2023-07-10 10:41:12,090][227910] FPS: 298291.66[0m
[36m[2023-07-10 10:41:16,389][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:41:16,390][227910] Reward + Measures: [[2322.83311465    0.22086942    0.33672851    0.51086187    0.15638246]][0m
[37m[1m[2023-07-10 10:41:16,390][227910] Max Reward on eval: 2322.8331146479713[0m
[37m[1m[2023-07-10 10:41:16,390][227910] Min Reward on eval: 2322.8331146479713[0m
[37m[1m[2023-07-10 10:41:16,391][227910] Mean Reward across all agents: 2322.8331146479713[0m
[37m[1m[2023-07-10 10:41:16,391][227910] Average Trajectory Length: 999.9456666666666[0m
[36m[2023-07-10 10:41:21,515][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:41:21,516][227910] Reward + Measures: [[ 912.41658822    0.26030001    0.59970003    0.3224        0.3423    ]
 [1673.23870524    0.23479998    0.46879998    0.44829997    0.2376    ]
 [1762.96988083    0.2079        0.3926        0.4515        0.2182    ]
 ...
 [1926.68266308    0.21470001    0.3369        0.4862        0.16409999]
 [1296.26874399    0.206         0.53029996    0.37549996    0.29150003]
 [1831.01459476    0.2182        0.38830003    0.48810002    0.2       ]][0m
[37m[1m[2023-07-10 10:41:21,516][227910] Max Reward on eval: 2360.368864908721[0m
[37m[1m[2023-07-10 10:41:21,516][227910] Min Reward on eval: 612.4415599818924[0m
[37m[1m[2023-07-10 10:41:21,516][227910] Mean Reward across all agents: 1708.9726378143027[0m
[37m[1m[2023-07-10 10:41:21,517][227910] Average Trajectory Length: 999.9066666666666[0m
[36m[2023-07-10 10:41:21,520][227910] mean_value=-106.33369948394495, max_value=1647.6312607775483[0m
[37m[1m[2023-07-10 10:41:21,523][227910] New mean coefficients: [[ 3.7984133 -1.1235037 -1.1512716  1.1208037  4.330672 ]][0m
[37m[1m[2023-07-10 10:41:21,524][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:41:30,813][227910] train() took 9.29 seconds to complete[0m
[36m[2023-07-10 10:41:30,813][227910] FPS: 413456.04[0m
[36m[2023-07-10 10:41:30,815][227910] itr=178, itrs=2000, Progress: 8.90%[0m
[36m[2023-07-10 10:41:43,809][227910] train() took 12.98 seconds to complete[0m
[36m[2023-07-10 10:41:43,810][227910] FPS: 295858.06[0m
[36m[2023-07-10 10:41:48,077][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:41:48,077][227910] Reward + Measures: [[2533.65350777    0.22461668    0.33491164    0.51506448    0.13783213]][0m
[37m[1m[2023-07-10 10:41:48,078][227910] Max Reward on eval: 2533.653507765058[0m
[37m[1m[2023-07-10 10:41:48,078][227910] Min Reward on eval: 2533.653507765058[0m
[37m[1m[2023-07-10 10:41:48,078][227910] Mean Reward across all agents: 2533.653507765058[0m
[37m[1m[2023-07-10 10:41:48,078][227910] Average Trajectory Length: 999.579[0m
[36m[2023-07-10 10:41:53,161][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:41:53,162][227910] Reward + Measures: [[1718.59961755    0.20149998    0.47119999    0.47729999    0.22519998]
 [2131.17746866    0.20640002    0.3353        0.48480001    0.14860001]
 [2240.16694775    0.22320001    0.4271        0.46670005    0.1719    ]
 ...
 [1997.03353948    0.22060001    0.3901        0.49399996    0.2052    ]
 [1786.11221585    0.2052        0.36970001    0.48289999    0.19850001]
 [1419.39152621    0.1935        0.3524        0.52520007    0.16610001]][0m
[37m[1m[2023-07-10 10:41:53,162][227910] Max Reward on eval: 2482.725522939628[0m
[37m[1m[2023-07-10 10:41:53,162][227910] Min Reward on eval: 538.334304648242[0m
[37m[1m[2023-07-10 10:41:53,163][227910] Mean Reward across all agents: 1761.4289225461264[0m
[37m[1m[2023-07-10 10:41:53,163][227910] Average Trajectory Length: 999.004[0m
[36m[2023-07-10 10:41:53,166][227910] mean_value=-227.21546886286524, max_value=1312.257020623947[0m
[37m[1m[2023-07-10 10:41:53,169][227910] New mean coefficients: [[ 2.0431962  -0.9137995   1.8392246  -0.17064953  4.05295   ]][0m
[37m[1m[2023-07-10 10:41:53,170][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:42:02,464][227910] train() took 9.29 seconds to complete[0m
[36m[2023-07-10 10:42:02,464][227910] FPS: 413266.51[0m
[36m[2023-07-10 10:42:02,467][227910] itr=179, itrs=2000, Progress: 8.95%[0m
[36m[2023-07-10 10:42:15,321][227910] train() took 12.84 seconds to complete[0m
[36m[2023-07-10 10:42:15,322][227910] FPS: 299059.61[0m
[36m[2023-07-10 10:42:19,572][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:42:19,573][227910] Reward + Measures: [[2685.17991134    0.22369459    0.33739096    0.5076434     0.1269301 ]][0m
[37m[1m[2023-07-10 10:42:19,573][227910] Max Reward on eval: 2685.1799113366833[0m
[37m[1m[2023-07-10 10:42:19,573][227910] Min Reward on eval: 2685.1799113366833[0m
[37m[1m[2023-07-10 10:42:19,573][227910] Mean Reward across all agents: 2685.1799113366833[0m
[37m[1m[2023-07-10 10:42:19,574][227910] Average Trajectory Length: 999.6606666666667[0m
[36m[2023-07-10 10:42:24,638][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:42:24,639][227910] Reward + Measures: [[1526.95107319    0.23049998    0.57319999    0.36359999    0.37649998]
 [ 714.83260985    0.2379        0.52689999    0.39780003    0.46680003]
 [ 975.20733687    0.26360002    0.6408        0.28290001    0.40649995]
 ...
 [1887.30187145    0.23149998    0.49709997    0.43529996    0.27070004]
 [1421.80164945    0.21269999    0.56490004    0.3802        0.30380002]
 [1761.80371642    0.21560001    0.45210001    0.44369999    0.23930001]][0m
[37m[1m[2023-07-10 10:42:24,639][227910] Max Reward on eval: 2740.488882580027[0m
[37m[1m[2023-07-10 10:42:24,639][227910] Min Reward on eval: 452.4600223345158[0m
[37m[1m[2023-07-10 10:42:24,640][227910] Mean Reward across all agents: 1746.0595861341687[0m
[37m[1m[2023-07-10 10:42:24,640][227910] Average Trajectory Length: 999.778[0m
[36m[2023-07-10 10:42:24,646][227910] mean_value=118.74383774675142, max_value=1024.8385851494736[0m
[37m[1m[2023-07-10 10:42:24,650][227910] New mean coefficients: [[ 2.2503676  -0.7436315   2.383494    0.27853858  5.173582  ]][0m
[37m[1m[2023-07-10 10:42:24,651][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:42:33,933][227910] train() took 9.28 seconds to complete[0m
[36m[2023-07-10 10:42:33,934][227910] FPS: 413746.34[0m
[36m[2023-07-10 10:42:33,936][227910] itr=180, itrs=2000, Progress: 9.00%[0m
[37m[1m[2023-07-10 10:42:36,113][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000160[0m
[36m[2023-07-10 10:42:49,347][227910] train() took 12.97 seconds to complete[0m
[36m[2023-07-10 10:42:49,347][227910] FPS: 296041.86[0m
[36m[2023-07-10 10:42:54,606][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:42:54,606][227910] Reward + Measures: [[2802.38983665    0.22303818    0.34770125    0.50552309    0.12333877]][0m
[37m[1m[2023-07-10 10:42:54,606][227910] Max Reward on eval: 2802.3898366509006[0m
[37m[1m[2023-07-10 10:42:54,606][227910] Min Reward on eval: 2802.3898366509006[0m
[37m[1m[2023-07-10 10:42:54,607][227910] Mean Reward across all agents: 2802.3898366509006[0m
[37m[1m[2023-07-10 10:42:54,607][227910] Average Trajectory Length: 999.2123333333333[0m
[36m[2023-07-10 10:43:01,220][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:43:01,221][227910] Reward + Measures: [[-474.71714171    0.15192275    0.22550891    0.32673916    0.12604229]
 [1356.50601761    0.21960001    0.60950005    0.3687        0.3143    ]
 [1558.13395134    0.21800001    0.55909997    0.34850001    0.28119999]
 ...
 [ 743.89838499    0.199         0.46630001    0.39649999    0.31920001]
 [1188.07599512    0.26289999    0.55830002    0.33380005    0.29630002]
 [ 746.57892214    0.1961        0.69690007    0.25540003    0.52320004]][0m
[37m[1m[2023-07-10 10:43:01,221][227910] Max Reward on eval: 2609.2791107832454[0m
[37m[1m[2023-07-10 10:43:01,222][227910] Min Reward on eval: -1087.810008606431[0m
[37m[1m[2023-07-10 10:43:01,222][227910] Mean Reward across all agents: 880.3962230776021[0m
[37m[1m[2023-07-10 10:43:01,222][227910] Average Trajectory Length: 963.149[0m
[36m[2023-07-10 10:43:01,246][227910] mean_value=-456.327882272442, max_value=1806.579532927433[0m
[37m[1m[2023-07-10 10:43:01,250][227910] New mean coefficients: [[ 1.8017119 -0.4872323  2.785225  -0.3524062  4.73752  ]][0m
[37m[1m[2023-07-10 10:43:01,253][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:43:14,761][227910] train() took 13.50 seconds to complete[0m
[36m[2023-07-10 10:43:14,761][227910] FPS: 284397.13[0m
[36m[2023-07-10 10:43:14,766][227910] itr=181, itrs=2000, Progress: 9.05%[0m
[36m[2023-07-10 10:43:31,592][227910] train() took 16.79 seconds to complete[0m
[36m[2023-07-10 10:43:31,593][227910] FPS: 228737.47[0m
[36m[2023-07-10 10:43:35,434][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:43:35,435][227910] Reward + Measures: [[2896.13548443    0.22287028    0.35212752    0.49575537    0.11981577]][0m
[37m[1m[2023-07-10 10:43:35,435][227910] Max Reward on eval: 2896.1354844304738[0m
[37m[1m[2023-07-10 10:43:35,435][227910] Min Reward on eval: 2896.1354844304738[0m
[37m[1m[2023-07-10 10:43:35,435][227910] Mean Reward across all agents: 2896.1354844304738[0m
[37m[1m[2023-07-10 10:43:35,435][227910] Average Trajectory Length: 997.8919999999999[0m
[36m[2023-07-10 10:43:40,047][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:43:40,048][227910] Reward + Measures: [[ 776.75773239    0.193         0.52460003    0.38170001    0.23990002]
 [1461.69356839    0.23570001    0.52360004    0.41219997    0.25910002]
 [1637.54446237    0.2375        0.55020005    0.5           0.1442    ]
 ...
 [ 932.98892305    0.29410002    0.60340005    0.31750003    0.36750004]
 [1479.03369638    0.23710001    0.46599999    0.45770001    0.2263    ]
 [1167.2996234     0.26630002    0.41480002    0.49499997    0.141     ]][0m
[37m[1m[2023-07-10 10:43:40,048][227910] Max Reward on eval: 2709.27266093092[0m
[37m[1m[2023-07-10 10:43:40,048][227910] Min Reward on eval: -97.42884985877899[0m
[37m[1m[2023-07-10 10:43:40,049][227910] Mean Reward across all agents: 1106.805978134637[0m
[37m[1m[2023-07-10 10:43:40,049][227910] Average Trajectory Length: 997.8023333333333[0m
[36m[2023-07-10 10:43:40,052][227910] mean_value=-341.6557302584796, max_value=1776.6989136762684[0m
[37m[1m[2023-07-10 10:43:40,055][227910] New mean coefficients: [[ 2.0851526   0.15738043  3.0645144  -0.45489112  4.771295  ]][0m
[37m[1m[2023-07-10 10:43:40,056][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:43:48,826][227910] train() took 8.77 seconds to complete[0m
[36m[2023-07-10 10:43:48,826][227910] FPS: 437950.85[0m
[36m[2023-07-10 10:43:48,829][227910] itr=182, itrs=2000, Progress: 9.10%[0m
[36m[2023-07-10 10:44:00,980][227910] train() took 12.14 seconds to complete[0m
[36m[2023-07-10 10:44:00,980][227910] FPS: 316437.97[0m
[36m[2023-07-10 10:44:04,849][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:44:04,850][227910] Reward + Measures: [[3026.03053559    0.22493364    0.36018258    0.48744768    0.1184904 ]][0m
[37m[1m[2023-07-10 10:44:04,850][227910] Max Reward on eval: 3026.030535593542[0m
[37m[1m[2023-07-10 10:44:04,850][227910] Min Reward on eval: 3026.030535593542[0m
[37m[1m[2023-07-10 10:44:04,850][227910] Mean Reward across all agents: 3026.030535593542[0m
[37m[1m[2023-07-10 10:44:04,850][227910] Average Trajectory Length: 997.3766666666667[0m
[36m[2023-07-10 10:44:09,456][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:44:09,457][227910] Reward + Measures: [[ 954.96902306    0.2132        0.59040004    0.32530001    0.30960003]
 [2726.6223706     0.22807835    0.34463593    0.44225532    0.12894885]
 [2977.45951508    0.22189999    0.38180003    0.49680001    0.12630001]
 ...
 [2765.65702723    0.22570001    0.39480001    0.48590001    0.15890001]
 [1160.54409568    0.21850954    0.61334288    0.3498095     0.3038381 ]
 [1979.11545599    0.2191        0.53660005    0.39120004    0.21780001]][0m
[37m[1m[2023-07-10 10:44:09,457][227910] Max Reward on eval: 2984.5546875679865[0m
[37m[1m[2023-07-10 10:44:09,457][227910] Min Reward on eval: 432.6846745799761[0m
[37m[1m[2023-07-10 10:44:09,458][227910] Mean Reward across all agents: 1923.6033478417326[0m
[37m[1m[2023-07-10 10:44:09,458][227910] Average Trajectory Length: 995.939[0m
[36m[2023-07-10 10:44:09,462][227910] mean_value=71.10623334199806, max_value=1656.0173766201638[0m
[37m[1m[2023-07-10 10:44:09,465][227910] New mean coefficients: [[ 1.192622   1.0118093  3.976039  -2.054292   4.4558616]][0m
[37m[1m[2023-07-10 10:44:09,466][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:44:18,138][227910] train() took 8.67 seconds to complete[0m
[36m[2023-07-10 10:44:18,138][227910] FPS: 442905.04[0m
[36m[2023-07-10 10:44:18,141][227910] itr=183, itrs=2000, Progress: 9.15%[0m
[36m[2023-07-10 10:44:30,244][227910] train() took 12.09 seconds to complete[0m
[36m[2023-07-10 10:44:30,244][227910] FPS: 317678.96[0m
[36m[2023-07-10 10:44:34,202][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:44:34,203][227910] Reward + Measures: [[3059.24746562    0.22346897    0.37681845    0.46537548    0.12178797]][0m
[37m[1m[2023-07-10 10:44:34,203][227910] Max Reward on eval: 3059.2474656192944[0m
[37m[1m[2023-07-10 10:44:34,203][227910] Min Reward on eval: 3059.2474656192944[0m
[37m[1m[2023-07-10 10:44:34,204][227910] Mean Reward across all agents: 3059.2474656192944[0m
[37m[1m[2023-07-10 10:44:34,204][227910] Average Trajectory Length: 996.6966666666666[0m
[36m[2023-07-10 10:44:38,970][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:44:38,970][227910] Reward + Measures: [[2532.34102752    0.2253        0.42590004    0.44390002    0.13599999]
 [1785.51119327    0.24794503    0.44830528    0.35405734    0.15810995]
 [2923.42891859    0.22140001    0.39480004    0.46220002    0.1348    ]
 ...
 [2119.46206572    0.2438        0.42319998    0.39490002    0.15620001]
 [2438.56141641    0.2088        0.35080001    0.43690005    0.11260001]
 [2142.31013689    0.23040001    0.42180005    0.39540002    0.1155    ]][0m
[37m[1m[2023-07-10 10:44:38,971][227910] Max Reward on eval: 2974.0965088150233[0m
[37m[1m[2023-07-10 10:44:38,971][227910] Min Reward on eval: 517.1382386855897[0m
[37m[1m[2023-07-10 10:44:38,971][227910] Mean Reward across all agents: 2135.6202800636593[0m
[37m[1m[2023-07-10 10:44:38,971][227910] Average Trajectory Length: 996.0319999999999[0m
[36m[2023-07-10 10:44:38,974][227910] mean_value=-364.09739755006433, max_value=1204.2337944368514[0m
[37m[1m[2023-07-10 10:44:38,976][227910] New mean coefficients: [[ 1.4891047  0.8217465  3.0015988 -2.4271846  3.575911 ]][0m
[37m[1m[2023-07-10 10:44:38,978][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:44:47,674][227910] train() took 8.69 seconds to complete[0m
[36m[2023-07-10 10:44:47,674][227910] FPS: 441639.18[0m
[36m[2023-07-10 10:44:47,677][227910] itr=184, itrs=2000, Progress: 9.20%[0m
[36m[2023-07-10 10:44:59,958][227910] train() took 12.27 seconds to complete[0m
[36m[2023-07-10 10:44:59,958][227910] FPS: 313078.85[0m
[36m[2023-07-10 10:45:03,827][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:45:03,828][227910] Reward + Measures: [[3136.69233447    0.22546436    0.39101046    0.43907666    0.12807211]][0m
[37m[1m[2023-07-10 10:45:03,828][227910] Max Reward on eval: 3136.6923344744296[0m
[37m[1m[2023-07-10 10:45:03,828][227910] Min Reward on eval: 3136.6923344744296[0m
[37m[1m[2023-07-10 10:45:03,828][227910] Mean Reward across all agents: 3136.6923344744296[0m
[37m[1m[2023-07-10 10:45:03,829][227910] Average Trajectory Length: 993.1873333333333[0m
[36m[2023-07-10 10:45:08,721][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:45:08,722][227910] Reward + Measures: [[1192.94290061    0.1751        0.66769999    0.296         0.38789999]
 [1423.95571211    0.1885        0.6699        0.30520001    0.33129999]
 [2619.96756269    0.22650002    0.43070003    0.41340002    0.16030002]
 ...
 [2114.06099098    0.2211        0.53909999    0.43870002    0.191     ]
 [1252.81816643    0.1656        0.62180001    0.30170003    0.37199998]
 [1242.71212731    0.176         0.67440003    0.29530001    0.37170002]][0m
[37m[1m[2023-07-10 10:45:08,722][227910] Max Reward on eval: 2990.842627103068[0m
[37m[1m[2023-07-10 10:45:08,722][227910] Min Reward on eval: 605.9303389180917[0m
[37m[1m[2023-07-10 10:45:08,722][227910] Mean Reward across all agents: 1837.3073982086041[0m
[37m[1m[2023-07-10 10:45:08,723][227910] Average Trajectory Length: 998.1846666666667[0m
[36m[2023-07-10 10:45:08,728][227910] mean_value=182.47880858626309, max_value=1630.062585094376[0m
[37m[1m[2023-07-10 10:45:08,731][227910] New mean coefficients: [[ 1.0763339  1.5598978  3.402696  -3.0391836  3.3924856]][0m
[37m[1m[2023-07-10 10:45:08,732][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:45:17,473][227910] train() took 8.74 seconds to complete[0m
[36m[2023-07-10 10:45:17,473][227910] FPS: 439406.05[0m
[36m[2023-07-10 10:45:17,476][227910] itr=185, itrs=2000, Progress: 9.25%[0m
[36m[2023-07-10 10:45:29,550][227910] train() took 12.06 seconds to complete[0m
[36m[2023-07-10 10:45:29,550][227910] FPS: 318443.34[0m
[36m[2023-07-10 10:45:33,426][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:45:33,426][227910] Reward + Measures: [[3161.29878767    0.22874865    0.39596567    0.40599614    0.12913534]][0m
[37m[1m[2023-07-10 10:45:33,426][227910] Max Reward on eval: 3161.298787669124[0m
[37m[1m[2023-07-10 10:45:33,426][227910] Min Reward on eval: 3161.298787669124[0m
[37m[1m[2023-07-10 10:45:33,426][227910] Mean Reward across all agents: 3161.298787669124[0m
[37m[1m[2023-07-10 10:45:33,427][227910] Average Trajectory Length: 988.7506666666667[0m
[36m[2023-07-10 10:45:38,065][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:45:38,065][227910] Reward + Measures: [[1704.48641282    0.22341381    0.56719202    0.32141382    0.24459659]
 [2551.10466297    0.21960001    0.44440004    0.36250001    0.17880002]
 [ 365.572929      0.18800001    0.64679998    0.24319999    0.45879999]
 ...
 [1989.56055426    0.23189998    0.52149999    0.34470001    0.21089999]
 [1523.22265211    0.22309999    0.59810001    0.31979999    0.29010001]
 [1185.1397886     0.1953        0.65509999    0.28560004    0.3637    ]][0m
[37m[1m[2023-07-10 10:45:38,066][227910] Max Reward on eval: 3182.6104121638928[0m
[37m[1m[2023-07-10 10:45:38,066][227910] Min Reward on eval: 259.4219102986623[0m
[37m[1m[2023-07-10 10:45:38,066][227910] Mean Reward across all agents: 1853.319347664516[0m
[37m[1m[2023-07-10 10:45:38,066][227910] Average Trajectory Length: 991.6923333333333[0m
[36m[2023-07-10 10:45:38,070][227910] mean_value=-50.32869394181359, max_value=2287.1612571271253[0m
[37m[1m[2023-07-10 10:45:38,072][227910] New mean coefficients: [[ 1.1505361  1.1940556  2.5095863 -2.9337099  2.784008 ]][0m
[37m[1m[2023-07-10 10:45:38,073][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:45:46,693][227910] train() took 8.62 seconds to complete[0m
[36m[2023-07-10 10:45:46,693][227910] FPS: 445569.32[0m
[36m[2023-07-10 10:45:46,696][227910] itr=186, itrs=2000, Progress: 9.30%[0m
[36m[2023-07-10 10:45:58,879][227910] train() took 12.17 seconds to complete[0m
[36m[2023-07-10 10:45:58,879][227910] FPS: 315588.22[0m
[36m[2023-07-10 10:46:02,818][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:46:02,819][227910] Reward + Measures: [[3223.19673744    0.2302006     0.40103084    0.38714108    0.12814404]][0m
[37m[1m[2023-07-10 10:46:02,819][227910] Max Reward on eval: 3223.196737438325[0m
[37m[1m[2023-07-10 10:46:02,819][227910] Min Reward on eval: 3223.196737438325[0m
[37m[1m[2023-07-10 10:46:02,820][227910] Mean Reward across all agents: 3223.196737438325[0m
[37m[1m[2023-07-10 10:46:02,820][227910] Average Trajectory Length: 988.1179999999999[0m
[36m[2023-07-10 10:46:07,503][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:46:07,504][227910] Reward + Measures: [[ -410.97285252     0.11322514     0.30860558     0.2363743
      0.33285141]
 [ -485.6117382      0.16506512     0.41477367     0.18964031
      0.31393799]
 [-1061.71911491     0.15300454     0.30739999     0.19135456
      0.25377271]
 ...
 [ -192.31928233     0.256668       0.39808133     0.27814087
      0.28478405]
 [-1412.76662271     0.23540151     0.28425387     0.30064401
      0.15764566]
 [  822.60559199     0.17200001     0.29720002     0.27729997
      0.14400001]][0m
[37m[1m[2023-07-10 10:46:07,504][227910] Max Reward on eval: 2747.769940142543[0m
[37m[1m[2023-07-10 10:46:07,504][227910] Min Reward on eval: -1668.385849094228[0m
[37m[1m[2023-07-10 10:46:07,505][227910] Mean Reward across all agents: -118.56454272347644[0m
[37m[1m[2023-07-10 10:46:07,505][227910] Average Trajectory Length: 933.1903333333333[0m
[36m[2023-07-10 10:46:07,508][227910] mean_value=-695.9856371605459, max_value=884.6703707698971[0m
[37m[1m[2023-07-10 10:46:07,510][227910] New mean coefficients: [[ 1.3367121  1.5776594  1.6193773 -2.3717453  3.0259397]][0m
[37m[1m[2023-07-10 10:46:07,511][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:46:16,187][227910] train() took 8.67 seconds to complete[0m
[36m[2023-07-10 10:46:16,187][227910] FPS: 442683.05[0m
[36m[2023-07-10 10:46:16,190][227910] itr=187, itrs=2000, Progress: 9.35%[0m
[36m[2023-07-10 10:46:28,360][227910] train() took 12.15 seconds to complete[0m
[36m[2023-07-10 10:46:28,360][227910] FPS: 315933.03[0m
[36m[2023-07-10 10:46:32,285][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:46:32,285][227910] Reward + Measures: [[3308.18481329    0.2345767     0.40874317    0.37238979    0.13004221]][0m
[37m[1m[2023-07-10 10:46:32,286][227910] Max Reward on eval: 3308.1848132867917[0m
[37m[1m[2023-07-10 10:46:32,286][227910] Min Reward on eval: 3308.1848132867917[0m
[37m[1m[2023-07-10 10:46:32,286][227910] Mean Reward across all agents: 3308.1848132867917[0m
[37m[1m[2023-07-10 10:46:32,286][227910] Average Trajectory Length: 989.1916666666666[0m
[36m[2023-07-10 10:46:37,010][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:46:37,011][227910] Reward + Measures: [[ 428.29421284    0.2771        0.50330001    0.32979998    0.34580001]
 [1164.63134059    0.21326819    0.33432272    0.32734093    0.15529548]
 [1411.14261634    0.26430002    0.54590005    0.39489999    0.20419998]
 ...
 [ 598.3155873     0.24468966    0.32582185    0.32313332    0.18636668]
 [ 644.31578073    0.31          0.49770004    0.33859998    0.27490002]
 [1004.48227       0.26319999    0.5492        0.31530002    0.25110003]][0m
[37m[1m[2023-07-10 10:46:37,011][227910] Max Reward on eval: 3457.172097853385[0m
[37m[1m[2023-07-10 10:46:37,011][227910] Min Reward on eval: -372.2946279150201[0m
[37m[1m[2023-07-10 10:46:37,011][227910] Mean Reward across all agents: 1305.094128022565[0m
[37m[1m[2023-07-10 10:46:37,012][227910] Average Trajectory Length: 994.66[0m
[36m[2023-07-10 10:46:37,014][227910] mean_value=-457.6223917314445, max_value=2900.766876553348[0m
[37m[1m[2023-07-10 10:46:37,017][227910] New mean coefficients: [[ 2.0844636  1.5592276  0.7783521 -1.8787999  2.8224137]][0m
[37m[1m[2023-07-10 10:46:37,018][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:46:45,725][227910] train() took 8.71 seconds to complete[0m
[36m[2023-07-10 10:46:45,725][227910] FPS: 441102.36[0m
[36m[2023-07-10 10:46:45,727][227910] itr=188, itrs=2000, Progress: 9.40%[0m
[36m[2023-07-10 10:46:57,852][227910] train() took 12.11 seconds to complete[0m
[36m[2023-07-10 10:46:57,852][227910] FPS: 317115.60[0m
[36m[2023-07-10 10:47:01,723][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:47:01,723][227910] Reward + Measures: [[3387.17583723    0.23530613    0.4063077     0.36759561    0.13021328]][0m
[37m[1m[2023-07-10 10:47:01,723][227910] Max Reward on eval: 3387.175837227722[0m
[37m[1m[2023-07-10 10:47:01,724][227910] Min Reward on eval: 3387.175837227722[0m
[37m[1m[2023-07-10 10:47:01,724][227910] Mean Reward across all agents: 3387.175837227722[0m
[37m[1m[2023-07-10 10:47:01,724][227910] Average Trajectory Length: 990.168[0m
[36m[2023-07-10 10:47:06,630][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:47:06,631][227910] Reward + Measures: [[2336.77997319    0.245         0.51870006    0.32339999    0.18260001]
 [2514.76831836    0.25050002    0.49149999    0.34870002    0.17800002]
 [1604.63150578    0.22239999    0.55840003    0.31479999    0.26530001]
 ...
 [2317.84401694    0.2342        0.51090002    0.37329999    0.1982    ]
 [1914.67008171    0.2586        0.4921        0.30650002    0.24460001]
 [2081.10424577    0.22690001    0.4817        0.37640002    0.1823    ]][0m
[37m[1m[2023-07-10 10:47:06,631][227910] Max Reward on eval: 3179.503654394345[0m
[37m[1m[2023-07-10 10:47:06,631][227910] Min Reward on eval: 348.0032651653979[0m
[37m[1m[2023-07-10 10:47:06,631][227910] Mean Reward across all agents: 1955.7295780019062[0m
[37m[1m[2023-07-10 10:47:06,631][227910] Average Trajectory Length: 996.8793333333333[0m
[36m[2023-07-10 10:47:06,636][227910] mean_value=-33.568516745525244, max_value=2413.908109623816[0m
[37m[1m[2023-07-10 10:47:06,638][227910] New mean coefficients: [[ 2.098862    0.99574345  1.5583065  -2.195634    2.5834565 ]][0m
[37m[1m[2023-07-10 10:47:06,639][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:47:15,322][227910] train() took 8.68 seconds to complete[0m
[36m[2023-07-10 10:47:15,323][227910] FPS: 442316.25[0m
[36m[2023-07-10 10:47:15,335][227910] itr=189, itrs=2000, Progress: 9.45%[0m
[36m[2023-07-10 10:47:27,295][227910] train() took 11.94 seconds to complete[0m
[36m[2023-07-10 10:47:27,295][227910] FPS: 321694.64[0m
[36m[2023-07-10 10:47:31,248][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:47:31,249][227910] Reward + Measures: [[3498.67395425    0.23729071    0.39876318    0.35731962    0.12361852]][0m
[37m[1m[2023-07-10 10:47:31,249][227910] Max Reward on eval: 3498.6739542469836[0m
[37m[1m[2023-07-10 10:47:31,249][227910] Min Reward on eval: 3498.6739542469836[0m
[37m[1m[2023-07-10 10:47:31,249][227910] Mean Reward across all agents: 3498.6739542469836[0m
[37m[1m[2023-07-10 10:47:31,249][227910] Average Trajectory Length: 993.7456666666666[0m
[36m[2023-07-10 10:47:35,990][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:47:35,991][227910] Reward + Measures: [[-652.71377146    0.27028096    0.42893177    0.13795714    0.35218573]
 [-494.47224621    0.20229052    0.24969704    0.29437387    0.28040129]
 [-905.63802977    0.15192597    0.23821476    0.24913131    0.12675592]
 ...
 [-522.66577585    0.1626536     0.33438152    0.31328633    0.25550213]
 [  21.37713483    0.1673        0.27449998    0.23839998    0.1698    ]
 [-207.99429628    0.31516168    0.28833354    0.35309166    0.31895527]][0m
[37m[1m[2023-07-10 10:47:35,991][227910] Max Reward on eval: 2302.700528873992[0m
[37m[1m[2023-07-10 10:47:35,992][227910] Min Reward on eval: -1138.831529754796[0m
[37m[1m[2023-07-10 10:47:35,992][227910] Mean Reward across all agents: -29.195834036929075[0m
[37m[1m[2023-07-10 10:47:35,992][227910] Average Trajectory Length: 899.8653333333333[0m
[36m[2023-07-10 10:47:35,995][227910] mean_value=-723.457991291476, max_value=1432.0807312587626[0m
[37m[1m[2023-07-10 10:47:35,998][227910] New mean coefficients: [[ 2.1107001  0.9865925  1.1005744 -2.48427    2.065355 ]][0m
[37m[1m[2023-07-10 10:47:35,998][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:47:44,611][227910] train() took 8.61 seconds to complete[0m
[36m[2023-07-10 10:47:44,611][227910] FPS: 445970.72[0m
[36m[2023-07-10 10:47:44,613][227910] itr=190, itrs=2000, Progress: 9.50%[0m
[37m[1m[2023-07-10 10:47:46,638][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000170[0m
[36m[2023-07-10 10:47:59,140][227910] train() took 12.24 seconds to complete[0m
[36m[2023-07-10 10:47:59,140][227910] FPS: 313604.00[0m
[36m[2023-07-10 10:48:03,036][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:48:03,036][227910] Reward + Measures: [[3609.77793108    0.23710188    0.39487356    0.3581256     0.11850704]][0m
[37m[1m[2023-07-10 10:48:03,036][227910] Max Reward on eval: 3609.7779310841697[0m
[37m[1m[2023-07-10 10:48:03,037][227910] Min Reward on eval: 3609.7779310841697[0m
[37m[1m[2023-07-10 10:48:03,037][227910] Mean Reward across all agents: 3609.7779310841697[0m
[37m[1m[2023-07-10 10:48:03,037][227910] Average Trajectory Length: 992.0939999999999[0m
[36m[2023-07-10 10:48:07,744][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:48:07,744][227910] Reward + Measures: [[2878.28515475    0.22510003    0.35080001    0.40880004    0.10030001]
 [ 431.32956812    0.28130001    0.35050002    0.3479        0.12789999]
 [3039.58502146    0.22129999    0.42340001    0.3795        0.1244    ]
 ...
 [1281.67940435    0.20310001    0.49500003    0.34869999    0.28990003]
 [-383.58523669    0.17105035    0.46160331    0.32178602    0.36547446]
 [-391.34078011    0.21245365    0.39526942    0.34271535    0.31049544]][0m
[37m[1m[2023-07-10 10:48:07,745][227910] Max Reward on eval: 3631.1669907141477[0m
[37m[1m[2023-07-10 10:48:07,745][227910] Min Reward on eval: -975.5207508365856[0m
[37m[1m[2023-07-10 10:48:07,745][227910] Mean Reward across all agents: 787.6973024290985[0m
[37m[1m[2023-07-10 10:48:07,745][227910] Average Trajectory Length: 894.5046666666666[0m
[36m[2023-07-10 10:48:07,748][227910] mean_value=-560.0961613199908, max_value=2112.9766608489213[0m
[37m[1m[2023-07-10 10:48:07,750][227910] New mean coefficients: [[ 1.924692    0.73778075  0.8638082  -1.7260522   2.1630013 ]][0m
[37m[1m[2023-07-10 10:48:07,751][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:48:16,410][227910] train() took 8.66 seconds to complete[0m
[36m[2023-07-10 10:48:16,410][227910] FPS: 443559.09[0m
[36m[2023-07-10 10:48:16,413][227910] itr=191, itrs=2000, Progress: 9.55%[0m
[36m[2023-07-10 10:48:28,559][227910] train() took 12.13 seconds to complete[0m
[36m[2023-07-10 10:48:28,559][227910] FPS: 316565.38[0m
[36m[2023-07-10 10:48:32,464][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:48:32,465][227910] Reward + Measures: [[3679.01979436    0.23364681    0.40067464    0.36051464    0.12085085]][0m
[37m[1m[2023-07-10 10:48:32,465][227910] Max Reward on eval: 3679.0197943609965[0m
[37m[1m[2023-07-10 10:48:32,465][227910] Min Reward on eval: 3679.0197943609965[0m
[37m[1m[2023-07-10 10:48:32,465][227910] Mean Reward across all agents: 3679.0197943609965[0m
[37m[1m[2023-07-10 10:48:32,466][227910] Average Trajectory Length: 993.098[0m
[36m[2023-07-10 10:48:37,141][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:48:37,141][227910] Reward + Measures: [[1720.89811993    0.2823        0.54560006    0.37170002    0.19599999]
 [ 169.01264445    0.29840001    0.71429998    0.1639        0.5413    ]
 [1931.39948889    0.23          0.47460005    0.38910002    0.097     ]
 ...
 [2175.73997551    0.22830001    0.51539999    0.3698        0.09060001]
 [2973.96272197    0.24990001    0.44679999    0.32539999    0.08090001]
 [ 774.92205105    0.36190003    0.45460001    0.34489998    0.2597    ]][0m
[37m[1m[2023-07-10 10:48:37,142][227910] Max Reward on eval: 3566.5202302578837[0m
[37m[1m[2023-07-10 10:48:37,142][227910] Min Reward on eval: -396.19788563803303[0m
[37m[1m[2023-07-10 10:48:37,142][227910] Mean Reward across all agents: 1927.4835708112114[0m
[37m[1m[2023-07-10 10:48:37,142][227910] Average Trajectory Length: 993.4186666666666[0m
[36m[2023-07-10 10:48:37,145][227910] mean_value=-293.1985664650149, max_value=3216.1301623400645[0m
[37m[1m[2023-07-10 10:48:37,148][227910] New mean coefficients: [[ 1.6589189   0.50050616  1.5542023  -1.1614195   2.1370242 ]][0m
[37m[1m[2023-07-10 10:48:37,149][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:48:45,803][227910] train() took 8.65 seconds to complete[0m
[36m[2023-07-10 10:48:45,803][227910] FPS: 443826.77[0m
[36m[2023-07-10 10:48:45,805][227910] itr=192, itrs=2000, Progress: 9.60%[0m
[36m[2023-07-10 10:48:57,966][227910] train() took 12.15 seconds to complete[0m
[36m[2023-07-10 10:48:57,967][227910] FPS: 316123.83[0m
[36m[2023-07-10 10:49:01,911][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:49:01,911][227910] Reward + Measures: [[3787.36289473    0.23414882    0.39636019    0.35355672    0.11953131]][0m
[37m[1m[2023-07-10 10:49:01,912][227910] Max Reward on eval: 3787.36289473292[0m
[37m[1m[2023-07-10 10:49:01,912][227910] Min Reward on eval: 3787.36289473292[0m
[37m[1m[2023-07-10 10:49:01,912][227910] Mean Reward across all agents: 3787.36289473292[0m
[37m[1m[2023-07-10 10:49:01,912][227910] Average Trajectory Length: 993.4026666666666[0m
[36m[2023-07-10 10:49:06,656][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:49:06,656][227910] Reward + Measures: [[2626.14928374    0.2395        0.48599997    0.32609999    0.1646    ]
 [3447.97011052    0.2385        0.42789999    0.35240003    0.1516    ]
 [3145.59988793    0.24200001    0.44379997    0.3804        0.1274    ]
 ...
 [3164.49896885    0.23900001    0.4391        0.35780001    0.15989999]
 [2391.42591792    0.24197893    0.50678426    0.35888425    0.20016316]
 [2394.74184848    0.24059999    0.42009997    0.39420003    0.16780002]][0m
[37m[1m[2023-07-10 10:49:06,657][227910] Max Reward on eval: 3755.7822613477706[0m
[37m[1m[2023-07-10 10:49:06,657][227910] Min Reward on eval: 1327.4092813118127[0m
[37m[1m[2023-07-10 10:49:06,657][227910] Mean Reward across all agents: 2925.732743813399[0m
[37m[1m[2023-07-10 10:49:06,657][227910] Average Trajectory Length: 994.3346666666666[0m
[36m[2023-07-10 10:49:06,662][227910] mean_value=72.1393765069738, max_value=2060.7840104218194[0m
[37m[1m[2023-07-10 10:49:06,664][227910] New mean coefficients: [[ 1.8000555  0.4238587  1.9374721 -1.7953019  1.3202984]][0m
[37m[1m[2023-07-10 10:49:06,665][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:49:15,253][227910] train() took 8.59 seconds to complete[0m
[36m[2023-07-10 10:49:15,253][227910] FPS: 447246.93[0m
[36m[2023-07-10 10:49:15,266][227910] itr=193, itrs=2000, Progress: 9.65%[0m
[36m[2023-07-10 10:49:27,192][227910] train() took 11.90 seconds to complete[0m
[36m[2023-07-10 10:49:27,192][227910] FPS: 322585.05[0m
[36m[2023-07-10 10:49:31,089][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:49:31,090][227910] Reward + Measures: [[3883.53535212    0.23562011    0.39944801    0.34505355    0.11747899]][0m
[37m[1m[2023-07-10 10:49:31,090][227910] Max Reward on eval: 3883.5353521219604[0m
[37m[1m[2023-07-10 10:49:31,090][227910] Min Reward on eval: 3883.5353521219604[0m
[37m[1m[2023-07-10 10:49:31,090][227910] Mean Reward across all agents: 3883.5353521219604[0m
[37m[1m[2023-07-10 10:49:31,091][227910] Average Trajectory Length: 993.6983333333333[0m
[36m[2023-07-10 10:49:35,936][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:49:35,936][227910] Reward + Measures: [[  657.82873653     0.34780002     0.4305         0.3901
      0.20410001]
 [ -771.14509406     0.27190003     0.32160002     0.16180001
      0.22939999]
 [  546.51325578     0.21180001     0.36620003     0.30539998
      0.1927    ]
 ...
 [ -385.99718838     0.1103         0.55705005     0.35905001
      0.53950006]
 [-1200.05310679     0.25610003     0.2198         0.10860001
      0.16510001]
 [  218.22878786     0.43120003     0.59240001     0.47080001
      0.45070001]][0m
[37m[1m[2023-07-10 10:49:35,937][227910] Max Reward on eval: 3706.655988537241[0m
[37m[1m[2023-07-10 10:49:35,937][227910] Min Reward on eval: -1856.3274046199513[0m
[37m[1m[2023-07-10 10:49:35,937][227910] Mean Reward across all agents: -245.67495702089656[0m
[37m[1m[2023-07-10 10:49:35,937][227910] Average Trajectory Length: 973.255[0m
[36m[2023-07-10 10:49:35,942][227910] mean_value=-961.3308496761632, max_value=1065.4369787512114[0m
[37m[1m[2023-07-10 10:49:35,945][227910] New mean coefficients: [[ 3.0179434  0.3179555 -0.0763979 -2.0529792  0.719    ]][0m
[37m[1m[2023-07-10 10:49:35,946][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:49:44,661][227910] train() took 8.71 seconds to complete[0m
[36m[2023-07-10 10:49:44,662][227910] FPS: 440675.82[0m
[36m[2023-07-10 10:49:44,664][227910] itr=194, itrs=2000, Progress: 9.70%[0m
[36m[2023-07-10 10:49:56,751][227910] train() took 12.07 seconds to complete[0m
[36m[2023-07-10 10:49:56,751][227910] FPS: 318096.63[0m
[36m[2023-07-10 10:50:00,642][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:50:00,642][227910] Reward + Measures: [[3948.59545871    0.23565926    0.39156848    0.3442899     0.11406709]][0m
[37m[1m[2023-07-10 10:50:00,643][227910] Max Reward on eval: 3948.5954587059264[0m
[37m[1m[2023-07-10 10:50:00,643][227910] Min Reward on eval: 3948.5954587059264[0m
[37m[1m[2023-07-10 10:50:00,643][227910] Mean Reward across all agents: 3948.5954587059264[0m
[37m[1m[2023-07-10 10:50:00,643][227910] Average Trajectory Length: 990.197[0m
[36m[2023-07-10 10:50:05,319][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:50:05,320][227910] Reward + Measures: [[1753.85024102    0.23940001    0.56020004    0.31890002    0.1375    ]
 [2258.42241135    0.2155        0.40780002    0.41570002    0.18520002]
 [1544.43415403    0.21330002    0.41560003    0.44239998    0.2309    ]
 ...
 [3448.67095438    0.23540001    0.40799999    0.34190002    0.11440001]
 [1961.17868146    0.2289        0.38160002    0.40980002    0.1833    ]
 [1620.69088414    0.21519999    0.3757        0.30239999    0.13420001]][0m
[37m[1m[2023-07-10 10:50:05,320][227910] Max Reward on eval: 3802.6225495144727[0m
[37m[1m[2023-07-10 10:50:05,320][227910] Min Reward on eval: -482.12058089419736[0m
[37m[1m[2023-07-10 10:50:05,320][227910] Mean Reward across all agents: 1838.4747601998454[0m
[37m[1m[2023-07-10 10:50:05,321][227910] Average Trajectory Length: 976.9813333333333[0m
[36m[2023-07-10 10:50:05,324][227910] mean_value=-371.2874014804291, max_value=2715.1859872414325[0m
[37m[1m[2023-07-10 10:50:05,326][227910] New mean coefficients: [[ 3.0160358   0.15810105  0.6577999  -2.1600833   0.14818078]][0m
[37m[1m[2023-07-10 10:50:05,327][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:50:13,993][227910] train() took 8.66 seconds to complete[0m
[36m[2023-07-10 10:50:13,994][227910] FPS: 443181.43[0m
[36m[2023-07-10 10:50:13,996][227910] itr=195, itrs=2000, Progress: 9.75%[0m
[36m[2023-07-10 10:50:25,935][227910] train() took 11.92 seconds to complete[0m
[36m[2023-07-10 10:50:25,935][227910] FPS: 322077.23[0m
[36m[2023-07-10 10:50:30,718][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:50:30,719][227910] Reward + Measures: [[4040.19361732    0.23635431    0.38968083    0.34028259    0.11377039]][0m
[37m[1m[2023-07-10 10:50:30,719][227910] Max Reward on eval: 4040.193617318262[0m
[37m[1m[2023-07-10 10:50:30,719][227910] Min Reward on eval: 4040.193617318262[0m
[37m[1m[2023-07-10 10:50:30,719][227910] Mean Reward across all agents: 4040.193617318262[0m
[37m[1m[2023-07-10 10:50:30,719][227910] Average Trajectory Length: 992.2896666666667[0m
[36m[2023-07-10 10:50:36,175][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:50:36,175][227910] Reward + Measures: [[-298.48581878    0.20739698    0.4728182     0.21698789    0.34581211]
 [ 140.1482626     0.41391355    0.39471695    0.39350605    0.28513825]
 [ 422.65920683    0.39930001    0.42399999    0.32119998    0.28980002]
 ...
 [2395.05032042    0.25130001    0.45549998    0.3448        0.10830001]
 [ 173.00234448    0.22730289    0.49374944    0.23419313    0.2447429 ]
 [-954.43669303    0.22676086    0.30422607    0.25411305    0.18728696]][0m
[37m[1m[2023-07-10 10:50:36,175][227910] Max Reward on eval: 3724.557386705652[0m
[37m[1m[2023-07-10 10:50:36,176][227910] Min Reward on eval: -1281.1417767316452[0m
[37m[1m[2023-07-10 10:50:36,176][227910] Mean Reward across all agents: 570.886586224259[0m
[37m[1m[2023-07-10 10:50:36,176][227910] Average Trajectory Length: 974.6213333333333[0m
[36m[2023-07-10 10:50:36,179][227910] mean_value=-1161.0883762526094, max_value=1445.744163912251[0m
[37m[1m[2023-07-10 10:50:36,181][227910] New mean coefficients: [[ 2.8025923  -0.22334486  0.73474056 -0.62292016  0.29184836]][0m
[37m[1m[2023-07-10 10:50:36,182][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:50:45,878][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 10:50:45,878][227910] FPS: 396116.41[0m
[36m[2023-07-10 10:50:45,880][227910] itr=196, itrs=2000, Progress: 9.80%[0m
[36m[2023-07-10 10:50:57,446][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 10:50:57,447][227910] FPS: 332405.72[0m
[36m[2023-07-10 10:51:02,259][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:51:02,259][227910] Reward + Measures: [[4155.50943425    0.23649842    0.37899065    0.33769935    0.10022228]][0m
[37m[1m[2023-07-10 10:51:02,260][227910] Max Reward on eval: 4155.509434248685[0m
[37m[1m[2023-07-10 10:51:02,260][227910] Min Reward on eval: 4155.509434248685[0m
[37m[1m[2023-07-10 10:51:02,260][227910] Mean Reward across all agents: 4155.509434248685[0m
[37m[1m[2023-07-10 10:51:02,260][227910] Average Trajectory Length: 993.1909999999999[0m
[36m[2023-07-10 10:51:07,801][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:51:07,802][227910] Reward + Measures: [[1845.61690344    0.29100001    0.39709997    0.32410002    0.0931    ]
 [1945.88786422    0.30169997    0.40219998    0.33070001    0.1173    ]
 [ 814.50892581    0.17480001    0.2782        0.35730001    0.1006    ]
 ...
 [2095.34098977    0.29280001    0.45930001    0.38259998    0.18550001]
 [2087.08792103    0.26570001    0.44119999    0.35600001    0.1384    ]
 [1349.29268038    0.26500002    0.47319999    0.2568        0.1754    ]][0m
[37m[1m[2023-07-10 10:51:07,802][227910] Max Reward on eval: 3503.8814782656264[0m
[37m[1m[2023-07-10 10:51:07,802][227910] Min Reward on eval: -804.1934242143645[0m
[37m[1m[2023-07-10 10:51:07,803][227910] Mean Reward across all agents: 1096.6588751014351[0m
[37m[1m[2023-07-10 10:51:07,803][227910] Average Trajectory Length: 973.6633333333333[0m
[36m[2023-07-10 10:51:07,807][227910] mean_value=-858.9406947813018, max_value=2422.936490227095[0m
[37m[1m[2023-07-10 10:51:07,809][227910] New mean coefficients: [[ 2.7935305  -0.71218824  0.3670122  -1.269689   -0.38631666]][0m
[37m[1m[2023-07-10 10:51:07,810][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:51:17,753][227910] train() took 9.94 seconds to complete[0m
[36m[2023-07-10 10:51:17,754][227910] FPS: 386265.86[0m
[36m[2023-07-10 10:51:17,756][227910] itr=197, itrs=2000, Progress: 9.85%[0m
[36m[2023-07-10 10:51:29,409][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 10:51:29,410][227910] FPS: 329975.83[0m
[36m[2023-07-10 10:51:34,321][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:51:34,321][227910] Reward + Measures: [[4268.23127335    0.23707414    0.37177131    0.33673477    0.09775765]][0m
[37m[1m[2023-07-10 10:51:34,321][227910] Max Reward on eval: 4268.23127335301[0m
[37m[1m[2023-07-10 10:51:34,321][227910] Min Reward on eval: 4268.23127335301[0m
[37m[1m[2023-07-10 10:51:34,322][227910] Mean Reward across all agents: 4268.23127335301[0m
[37m[1m[2023-07-10 10:51:34,322][227910] Average Trajectory Length: 991.4003333333333[0m
[36m[2023-07-10 10:51:39,919][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:51:39,925][227910] Reward + Measures: [[-457.39395735    0.13420001    0.3441        0.2299        0.27790001]
 [ 171.34408204    0.20727435    0.40873471    0.43279243    0.33614901]
 [1001.79009759    0.17040001    0.34530002    0.33309999    0.1151    ]
 ...
 [-251.15287455    0.18240002    0.30360001    0.25050002    0.29380003]
 [ 559.82153564    0.1684        0.49560004    0.3355        0.4007    ]
 [-539.83354825    0.10061865    0.25535762    0.18181865    0.17425933]][0m
[37m[1m[2023-07-10 10:51:39,925][227910] Max Reward on eval: 3729.3119123801125[0m
[37m[1m[2023-07-10 10:51:39,925][227910] Min Reward on eval: -1401.224117991887[0m
[37m[1m[2023-07-10 10:51:39,926][227910] Mean Reward across all agents: 188.00145323251678[0m
[37m[1m[2023-07-10 10:51:39,926][227910] Average Trajectory Length: 925.8293333333334[0m
[36m[2023-07-10 10:51:39,930][227910] mean_value=-855.952886683196, max_value=1269.5726821105104[0m
[37m[1m[2023-07-10 10:51:39,932][227910] New mean coefficients: [[ 3.0865626  -0.4899743  -0.18418294 -0.88680124 -0.692942  ]][0m
[37m[1m[2023-07-10 10:51:39,933][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:51:49,793][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 10:51:49,794][227910] FPS: 389525.33[0m
[36m[2023-07-10 10:51:49,796][227910] itr=198, itrs=2000, Progress: 9.90%[0m
[36m[2023-07-10 10:52:01,468][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 10:52:01,468][227910] FPS: 329396.47[0m
[36m[2023-07-10 10:52:06,333][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:52:06,333][227910] Reward + Measures: [[4373.76359687    0.2365232     0.3614082     0.32860842    0.08468173]][0m
[37m[1m[2023-07-10 10:52:06,333][227910] Max Reward on eval: 4373.763596866652[0m
[37m[1m[2023-07-10 10:52:06,334][227910] Min Reward on eval: 4373.763596866652[0m
[37m[1m[2023-07-10 10:52:06,334][227910] Mean Reward across all agents: 4373.763596866652[0m
[37m[1m[2023-07-10 10:52:06,334][227910] Average Trajectory Length: 992.93[0m
[36m[2023-07-10 10:52:11,745][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:52:11,745][227910] Reward + Measures: [[3105.49674181    0.22570001    0.39430001    0.34540001    0.0728    ]
 [1385.53580202    0.28800002    0.59580004    0.49960002    0.18540001]
 [2355.09086219    0.22309999    0.43630001    0.3994        0.11870001]
 ...
 [3326.86275893    0.24150984    0.36838052    0.34611198    0.08317857]
 [ 198.35686028    0.2897        0.50050002    0.24690001    0.39820001]
 [2242.51372993    0.2516        0.38010001    0.3926        0.1094    ]][0m
[37m[1m[2023-07-10 10:52:11,746][227910] Max Reward on eval: 3906.09218142475[0m
[37m[1m[2023-07-10 10:52:11,746][227910] Min Reward on eval: -1253.5667100873543[0m
[37m[1m[2023-07-10 10:52:11,746][227910] Mean Reward across all agents: 750.8032093520236[0m
[37m[1m[2023-07-10 10:52:11,746][227910] Average Trajectory Length: 925.0593333333333[0m
[36m[2023-07-10 10:52:11,750][227910] mean_value=-1136.5334334819995, max_value=1684.5048375130107[0m
[37m[1m[2023-07-10 10:52:11,753][227910] New mean coefficients: [[ 3.2991745  -0.44009063 -1.349725   -0.17660582  0.42284906]][0m
[37m[1m[2023-07-10 10:52:11,754][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:52:21,554][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 10:52:21,554][227910] FPS: 391912.56[0m
[36m[2023-07-10 10:52:21,557][227910] itr=199, itrs=2000, Progress: 9.95%[0m
[36m[2023-07-10 10:52:33,141][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 10:52:33,146][227910] FPS: 331925.37[0m
[36m[2023-07-10 10:52:38,004][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:52:38,005][227910] Reward + Measures: [[4455.00592671    0.2382337     0.35347208    0.3284657     0.08412878]][0m
[37m[1m[2023-07-10 10:52:38,005][227910] Max Reward on eval: 4455.005926705557[0m
[37m[1m[2023-07-10 10:52:38,005][227910] Min Reward on eval: 4455.005926705557[0m
[37m[1m[2023-07-10 10:52:38,005][227910] Mean Reward across all agents: 4455.005926705557[0m
[37m[1m[2023-07-10 10:52:38,006][227910] Average Trajectory Length: 992.3356666666666[0m
[36m[2023-07-10 10:52:43,561][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:52:43,562][227910] Reward + Measures: [[1474.50729177    0.28410003    0.40109998    0.37830001    0.1353    ]
 [1721.61722233    0.27590001    0.39069998    0.35260001    0.16669999]
 [3133.34538495    0.2617        0.41330001    0.2969        0.0727    ]
 ...
 [2378.11926544    0.24499999    0.38519999    0.40499997    0.0933    ]
 [1562.48165816    0.2507        0.61330003    0.36660001    0.25819999]
 [2308.19140445    0.23840001    0.49200001    0.42140004    0.0938    ]][0m
[37m[1m[2023-07-10 10:52:43,562][227910] Max Reward on eval: 4341.170854392415[0m
[37m[1m[2023-07-10 10:52:43,562][227910] Min Reward on eval: -422.4327637873706[0m
[37m[1m[2023-07-10 10:52:43,563][227910] Mean Reward across all agents: 2433.2380583515546[0m
[37m[1m[2023-07-10 10:52:43,563][227910] Average Trajectory Length: 991.3076666666666[0m
[36m[2023-07-10 10:52:43,566][227910] mean_value=-639.9700437634419, max_value=1810.1723818213716[0m
[37m[1m[2023-07-10 10:52:43,568][227910] New mean coefficients: [[ 2.83422    -0.61484015 -0.08937228 -0.16369003  0.21219414]][0m
[37m[1m[2023-07-10 10:52:43,569][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:52:53,573][227910] train() took 10.00 seconds to complete[0m
[36m[2023-07-10 10:52:53,573][227910] FPS: 383933.51[0m
[36m[2023-07-10 10:52:53,575][227910] itr=200, itrs=2000, Progress: 10.00%[0m
[37m[1m[2023-07-10 10:52:55,635][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000180[0m
[36m[2023-07-10 10:53:08,095][227910] train() took 12.20 seconds to complete[0m
[36m[2023-07-10 10:53:08,096][227910] FPS: 314627.09[0m
[36m[2023-07-10 10:53:12,781][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:53:12,782][227910] Reward + Measures: [[4519.86131579    0.24033536    0.34494922    0.32700667    0.08349489]][0m
[37m[1m[2023-07-10 10:53:12,782][227910] Max Reward on eval: 4519.861315794048[0m
[37m[1m[2023-07-10 10:53:12,782][227910] Min Reward on eval: 4519.861315794048[0m
[37m[1m[2023-07-10 10:53:12,782][227910] Mean Reward across all agents: 4519.861315794048[0m
[37m[1m[2023-07-10 10:53:12,783][227910] Average Trajectory Length: 991.752[0m
[36m[2023-07-10 10:53:18,184][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:53:18,185][227910] Reward + Measures: [[3729.8720577     0.24429999    0.41099998    0.37740001    0.1513    ]
 [ 332.50033474    0.27400002    0.33710003    0.3572        0.24960001]
 [ 137.3615585     0.28454402    0.35344934    0.34936339    0.24885039]
 ...
 [ 217.1266689     0.26280004    0.6767        0.1603        0.48730001]
 [ -37.69068689    0.23840001    0.72559994    0.13780001    0.56760001]
 [-822.23465429    0.30429268    0.57811254    0.26246178    0.54453355]][0m
[37m[1m[2023-07-10 10:53:18,185][227910] Max Reward on eval: 3729.872057702765[0m
[37m[1m[2023-07-10 10:53:18,185][227910] Min Reward on eval: -1033.0325228047557[0m
[37m[1m[2023-07-10 10:53:18,186][227910] Mean Reward across all agents: 562.5645871528868[0m
[37m[1m[2023-07-10 10:53:18,186][227910] Average Trajectory Length: 956.161[0m
[36m[2023-07-10 10:53:18,190][227910] mean_value=-790.3417798802187, max_value=1247.6621054595337[0m
[37m[1m[2023-07-10 10:53:18,193][227910] New mean coefficients: [[ 3.6472924  -0.7583202  -0.9649323   0.15890655  0.5034634 ]][0m
[37m[1m[2023-07-10 10:53:18,194][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:53:27,880][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 10:53:27,880][227910] FPS: 396507.25[0m
[36m[2023-07-10 10:53:27,882][227910] itr=201, itrs=2000, Progress: 10.05%[0m
[36m[2023-07-10 10:53:39,464][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 10:53:39,464][227910] FPS: 331993.83[0m
[36m[2023-07-10 10:53:44,256][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:53:44,256][227910] Reward + Measures: [[4597.39026553    0.23972733    0.33673555    0.32526448    0.07708158]][0m
[37m[1m[2023-07-10 10:53:44,256][227910] Max Reward on eval: 4597.390265531518[0m
[37m[1m[2023-07-10 10:53:44,256][227910] Min Reward on eval: 4597.390265531518[0m
[37m[1m[2023-07-10 10:53:44,257][227910] Mean Reward across all agents: 4597.390265531518[0m
[37m[1m[2023-07-10 10:53:44,257][227910] Average Trajectory Length: 994.4093333333333[0m
[36m[2023-07-10 10:53:49,850][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:53:49,850][227910] Reward + Measures: [[1623.12889261    0.33059999    0.4542        0.3601        0.16940001]
 [1323.16192332    0.2436        0.42299995    0.4808        0.1099    ]
 [-236.70415658    0.30519998    0.40540001    0.48639998    0.24960001]
 ...
 [-542.62364882    0.25310001    0.2899        0.3888        0.14040001]
 [ 829.72212081    0.2297        0.37600002    0.3592        0.13959999]
 [ 760.45081359    0.17550001    0.38460001    0.28800002    0.2088    ]][0m
[37m[1m[2023-07-10 10:53:49,851][227910] Max Reward on eval: 4297.93571769353[0m
[37m[1m[2023-07-10 10:53:49,851][227910] Min Reward on eval: -646.4513359688688[0m
[37m[1m[2023-07-10 10:53:49,851][227910] Mean Reward across all agents: 1101.240702044324[0m
[37m[1m[2023-07-10 10:53:49,851][227910] Average Trajectory Length: 979.8829999999999[0m
[36m[2023-07-10 10:53:49,854][227910] mean_value=-1381.0459346033351, max_value=2266.103985818486[0m
[37m[1m[2023-07-10 10:53:49,856][227910] New mean coefficients: [[ 3.2283216  -0.47645473  0.520175   -0.7495035   0.1790173 ]][0m
[37m[1m[2023-07-10 10:53:49,857][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:53:59,766][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 10:53:59,766][227910] FPS: 387594.37[0m
[36m[2023-07-10 10:53:59,768][227910] itr=202, itrs=2000, Progress: 10.10%[0m
[36m[2023-07-10 10:54:11,221][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 10:54:11,221][227910] FPS: 335794.67[0m
[36m[2023-07-10 10:54:16,005][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:54:16,005][227910] Reward + Measures: [[4654.58193491    0.23909472    0.32958442    0.31457219    0.06923284]][0m
[37m[1m[2023-07-10 10:54:16,005][227910] Max Reward on eval: 4654.58193490821[0m
[37m[1m[2023-07-10 10:54:16,006][227910] Min Reward on eval: 4654.58193490821[0m
[37m[1m[2023-07-10 10:54:16,006][227910] Mean Reward across all agents: 4654.58193490821[0m
[37m[1m[2023-07-10 10:54:16,006][227910] Average Trajectory Length: 992.682[0m
[36m[2023-07-10 10:54:21,669][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:54:21,675][227910] Reward + Measures: [[ 417.33884571    0.24419999    0.35830003    0.2836        0.21409999]
 [ 174.65061947    0.33330002    0.3698        0.43179998    0.34859997]
 [1056.13176189    0.26040003    0.3619        0.39489999    0.23029999]
 ...
 [-915.80717379    0.28257853    0.26976427    0.2444791     0.25266919]
 [ 134.20539177    0.22680001    0.66750002    0.18160002    0.50280005]
 [ -15.18429399    0.2633        0.40900001    0.21190003    0.27900001]][0m
[37m[1m[2023-07-10 10:54:21,675][227910] Max Reward on eval: 3316.1213372699917[0m
[37m[1m[2023-07-10 10:54:21,675][227910] Min Reward on eval: -1390.3890260334417[0m
[37m[1m[2023-07-10 10:54:21,676][227910] Mean Reward across all agents: 579.055600628939[0m
[37m[1m[2023-07-10 10:54:21,676][227910] Average Trajectory Length: 972.0319999999999[0m
[36m[2023-07-10 10:54:21,679][227910] mean_value=-783.2745897527, max_value=2253.265779211232[0m
[37m[1m[2023-07-10 10:54:21,682][227910] New mean coefficients: [[ 2.9931645  -0.5521307   0.4863606  -0.83983666  0.3533159 ]][0m
[37m[1m[2023-07-10 10:54:21,683][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:54:31,497][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 10:54:31,497][227910] FPS: 391336.31[0m
[36m[2023-07-10 10:54:31,499][227910] itr=203, itrs=2000, Progress: 10.15%[0m
[36m[2023-07-10 10:54:43,100][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 10:54:43,100][227910] FPS: 331509.22[0m
[36m[2023-07-10 10:54:47,980][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:54:47,981][227910] Reward + Measures: [[4738.97040057    0.23787639    0.32182959    0.30863449    0.06962094]][0m
[37m[1m[2023-07-10 10:54:47,981][227910] Max Reward on eval: 4738.970400570031[0m
[37m[1m[2023-07-10 10:54:47,981][227910] Min Reward on eval: 4738.970400570031[0m
[37m[1m[2023-07-10 10:54:47,981][227910] Mean Reward across all agents: 4738.970400570031[0m
[37m[1m[2023-07-10 10:54:47,982][227910] Average Trajectory Length: 995.65[0m
[36m[2023-07-10 10:54:53,514][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:54:53,515][227910] Reward + Measures: [[ 170.47880594    0.1725        0.38570005    0.2974        0.2335    ]
 [-742.80443429    0.1037        0.57349998    0.23910001    0.4454    ]
 [ 649.88638497    0.20597421    0.43969998    0.37546453    0.32809353]
 ...
 [ 346.01045923    0.13672338    0.43531504    0.28519562    0.28836882]
 [1330.13932057    0.1715        0.47950003    0.3283        0.22000001]
 [2014.4840779     0.24329999    0.37310001    0.39680001    0.0872    ]][0m
[37m[1m[2023-07-10 10:54:53,515][227910] Max Reward on eval: 2851.9385616697373[0m
[37m[1m[2023-07-10 10:54:53,515][227910] Min Reward on eval: -1086.5480792277492[0m
[37m[1m[2023-07-10 10:54:53,515][227910] Mean Reward across all agents: 403.3836964168122[0m
[37m[1m[2023-07-10 10:54:53,516][227910] Average Trajectory Length: 969.6403333333333[0m
[36m[2023-07-10 10:54:53,520][227910] mean_value=-781.407321577256, max_value=1860.4931436366983[0m
[37m[1m[2023-07-10 10:54:53,523][227910] New mean coefficients: [[ 3.2377782  -0.9770205   0.6909127  -0.46050903  0.54958093]][0m
[37m[1m[2023-07-10 10:54:53,524][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:55:03,408][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 10:55:03,409][227910] FPS: 388551.48[0m
[36m[2023-07-10 10:55:03,411][227910] itr=204, itrs=2000, Progress: 10.20%[0m
[36m[2023-07-10 10:55:15,111][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 10:55:15,112][227910] FPS: 328601.54[0m
[36m[2023-07-10 10:55:19,857][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:55:19,858][227910] Reward + Measures: [[4806.83344208    0.23525143    0.3157379     0.30597636    0.06518021]][0m
[37m[1m[2023-07-10 10:55:19,858][227910] Max Reward on eval: 4806.833442081055[0m
[37m[1m[2023-07-10 10:55:19,858][227910] Min Reward on eval: 4806.833442081055[0m
[37m[1m[2023-07-10 10:55:19,858][227910] Mean Reward across all agents: 4806.833442081055[0m
[37m[1m[2023-07-10 10:55:19,858][227910] Average Trajectory Length: 992.9879999999999[0m
[36m[2023-07-10 10:55:25,514][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:55:25,515][227910] Reward + Measures: [[ 239.96268177    0.63379997    0.26620001    0.58590001    0.41319999]
 [ 664.84729373    0.21245849    0.50351465    0.35534078    0.21141572]
 [ 302.3571135     0.19810823    0.45149213    0.4249948     0.17698531]
 ...
 [ 184.82184327    0.2663568     0.46443588    0.30390733    0.28137583]
 [-358.64954494    0.56840128    0.1352046     0.60663217    0.49913478]
 [ 417.59682575    0.43849999    0.4973        0.41780001    0.23470001]][0m
[37m[1m[2023-07-10 10:55:25,515][227910] Max Reward on eval: 3715.8646007919683[0m
[37m[1m[2023-07-10 10:55:25,515][227910] Min Reward on eval: -1182.508623569447[0m
[37m[1m[2023-07-10 10:55:25,515][227910] Mean Reward across all agents: 367.3280394137753[0m
[37m[1m[2023-07-10 10:55:25,516][227910] Average Trajectory Length: 921.4933333333333[0m
[36m[2023-07-10 10:55:25,522][227910] mean_value=-963.073248206664, max_value=1296.3108396786251[0m
[37m[1m[2023-07-10 10:55:25,525][227910] New mean coefficients: [[ 3.5880876  -1.2225109  -0.65922856  0.52700245  0.40369007]][0m
[37m[1m[2023-07-10 10:55:25,526][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:55:35,257][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 10:55:35,257][227910] FPS: 394680.24[0m
[36m[2023-07-10 10:55:35,259][227910] itr=205, itrs=2000, Progress: 10.25%[0m
[36m[2023-07-10 10:55:46,866][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 10:55:46,866][227910] FPS: 331249.46[0m
[36m[2023-07-10 10:55:51,579][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:55:51,580][227910] Reward + Measures: [[4881.86569951    0.23579055    0.30919909    0.3013725     0.06307198]][0m
[37m[1m[2023-07-10 10:55:51,580][227910] Max Reward on eval: 4881.865699514572[0m
[37m[1m[2023-07-10 10:55:51,580][227910] Min Reward on eval: 4881.865699514572[0m
[37m[1m[2023-07-10 10:55:51,581][227910] Mean Reward across all agents: 4881.865699514572[0m
[37m[1m[2023-07-10 10:55:51,581][227910] Average Trajectory Length: 995.0276666666666[0m
[36m[2023-07-10 10:55:57,031][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:55:57,032][227910] Reward + Measures: [[-57.30661469   0.18975677   0.60897732   0.44807649   0.45278206]
 [821.87512221   0.34269997   0.44769999   0.3687       0.22660001]
 [624.33404805   0.22680001   0.40690002   0.30280003   0.27430001]
 ...
 [895.76637552   0.2665       0.35840002   0.32370001   0.19309999]
 [305.68491021   0.26610002   0.46759996   0.42990002   0.2191    ]
 [782.03823963   0.285        0.52860004   0.41690001   0.44909999]][0m
[37m[1m[2023-07-10 10:55:57,032][227910] Max Reward on eval: 3924.531567911187[0m
[37m[1m[2023-07-10 10:55:57,032][227910] Min Reward on eval: -1204.0514198945602[0m
[37m[1m[2023-07-10 10:55:57,032][227910] Mean Reward across all agents: 516.9751448151225[0m
[37m[1m[2023-07-10 10:55:57,033][227910] Average Trajectory Length: 953.4353333333333[0m
[36m[2023-07-10 10:55:57,037][227910] mean_value=-677.2556141913892, max_value=1128.6653825903545[0m
[37m[1m[2023-07-10 10:55:57,040][227910] New mean coefficients: [[ 3.5473876  -0.35627073 -0.3423956  -0.1192764   0.69669807]][0m
[37m[1m[2023-07-10 10:55:57,041][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:56:06,820][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 10:56:06,820][227910] FPS: 392728.77[0m
[36m[2023-07-10 10:56:06,823][227910] itr=206, itrs=2000, Progress: 10.30%[0m
[36m[2023-07-10 10:56:18,489][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 10:56:18,489][227910] FPS: 329566.31[0m
[36m[2023-07-10 10:56:23,233][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:56:23,234][227910] Reward + Measures: [[3429.49906601    0.30194876    0.34049535    0.40597999    0.06311812]][0m
[37m[1m[2023-07-10 10:56:23,234][227910] Max Reward on eval: 3429.4990660083727[0m
[37m[1m[2023-07-10 10:56:23,234][227910] Min Reward on eval: 3429.4990660083727[0m
[37m[1m[2023-07-10 10:56:23,235][227910] Mean Reward across all agents: 3429.4990660083727[0m
[37m[1m[2023-07-10 10:56:23,235][227910] Average Trajectory Length: 996.9353333333333[0m
[36m[2023-07-10 10:56:28,802][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:56:28,802][227910] Reward + Measures: [[1667.5356111     0.30700001    0.39860001    0.35300002    0.1122    ]
 [1166.46617965    0.4086        0.53249997    0.57680005    0.175     ]
 [  73.47017655    0.26721811    0.44573554    0.36684614    0.36210564]
 ...
 [1395.23773812    0.24095193    0.54866356    0.34673426    0.21722209]
 [ 480.70658784    0.27276215    0.47252971    0.37871894    0.32686758]
 [1511.54385832    0.34109998    0.56610006    0.50120002    0.19590001]][0m
[37m[1m[2023-07-10 10:56:28,802][227910] Max Reward on eval: 3245.2843886447604[0m
[37m[1m[2023-07-10 10:56:28,803][227910] Min Reward on eval: -799.0354698504846[0m
[37m[1m[2023-07-10 10:56:28,803][227910] Mean Reward across all agents: 874.0164100832407[0m
[37m[1m[2023-07-10 10:56:28,803][227910] Average Trajectory Length: 957.466[0m
[36m[2023-07-10 10:56:28,808][227910] mean_value=-586.7432847013059, max_value=2270.773741243554[0m
[37m[1m[2023-07-10 10:56:28,811][227910] New mean coefficients: [[ 4.046941    0.12811905 -0.7925204   0.8603613   0.750196  ]][0m
[37m[1m[2023-07-10 10:56:28,812][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:56:38,614][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 10:56:38,614][227910] FPS: 391833.49[0m
[36m[2023-07-10 10:56:38,617][227910] itr=207, itrs=2000, Progress: 10.35%[0m
[36m[2023-07-10 10:56:50,317][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 10:56:50,318][227910] FPS: 328642.20[0m
[36m[2023-07-10 10:56:55,137][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:56:55,138][227910] Reward + Measures: [[3813.89790208    0.30258811    0.30900571    0.39502665    0.07043967]][0m
[37m[1m[2023-07-10 10:56:55,138][227910] Max Reward on eval: 3813.897902077686[0m
[37m[1m[2023-07-10 10:56:55,138][227910] Min Reward on eval: 3813.897902077686[0m
[37m[1m[2023-07-10 10:56:55,138][227910] Mean Reward across all agents: 3813.897902077686[0m
[37m[1m[2023-07-10 10:56:55,139][227910] Average Trajectory Length: 998.7213333333333[0m
[36m[2023-07-10 10:57:00,654][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:57:00,654][227910] Reward + Measures: [[1279.00188334    0.28099999    0.51060003    0.2696        0.25830001]
 [1497.13707578    0.28952503    0.34044746    0.28471658    0.08563372]
 [ 398.68721997    0.1603        0.58170003    0.17349999    0.45250002]
 ...
 [-784.49696572    0.19097722    0.5281195     0.42007691    0.4494842 ]
 [2946.97337109    0.25279999    0.49000001    0.42150003    0.0824    ]
 [ 447.73848233    0.59240001    0.69090003    0.19880001    0.69259995]][0m
[37m[1m[2023-07-10 10:57:00,655][227910] Max Reward on eval: 3730.3889721041546[0m
[37m[1m[2023-07-10 10:57:00,655][227910] Min Reward on eval: -1598.9277133950266[0m
[37m[1m[2023-07-10 10:57:00,655][227910] Mean Reward across all agents: 572.7255705476741[0m
[37m[1m[2023-07-10 10:57:00,655][227910] Average Trajectory Length: 961.7223333333333[0m
[36m[2023-07-10 10:57:00,660][227910] mean_value=-448.5602979912835, max_value=1921.6647415219272[0m
[37m[1m[2023-07-10 10:57:00,663][227910] New mean coefficients: [[ 4.731516   -0.12500483 -0.67852986  0.37701783  0.47071153]][0m
[37m[1m[2023-07-10 10:57:00,664][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:57:10,350][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 10:57:10,350][227910] FPS: 396504.71[0m
[36m[2023-07-10 10:57:10,353][227910] itr=208, itrs=2000, Progress: 10.40%[0m
[36m[2023-07-10 10:57:22,013][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 10:57:22,013][227910] FPS: 329772.76[0m
[36m[2023-07-10 10:57:26,768][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:57:26,769][227910] Reward + Measures: [[3992.00301569    0.30230367    0.2968601     0.38887       0.07001622]][0m
[37m[1m[2023-07-10 10:57:26,769][227910] Max Reward on eval: 3992.003015687401[0m
[37m[1m[2023-07-10 10:57:26,769][227910] Min Reward on eval: 3992.003015687401[0m
[37m[1m[2023-07-10 10:57:26,770][227910] Mean Reward across all agents: 3992.003015687401[0m
[37m[1m[2023-07-10 10:57:26,770][227910] Average Trajectory Length: 999.8473333333333[0m
[36m[2023-07-10 10:57:32,211][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:57:32,212][227910] Reward + Measures: [[ 962.43706779    0.31540003    0.48129997    0.45959997    0.27589998]
 [-325.66499142    0.2791        0.24510002    0.37760001    0.2131    ]
 [-387.32022073    0.12149268    0.43517867    0.29942235    0.32352039]
 ...
 [2760.57196571    0.28987929    0.32318968    0.39853102    0.1001931 ]
 [ 195.27838982    0.57860005    0.70520002    0.2701        0.53599995]
 [ 880.60991194    0.24648185    0.40108386    0.41048414    0.26815248]][0m
[37m[1m[2023-07-10 10:57:32,212][227910] Max Reward on eval: 3851.3767396211624[0m
[37m[1m[2023-07-10 10:57:32,212][227910] Min Reward on eval: -836.1356179773226[0m
[37m[1m[2023-07-10 10:57:32,213][227910] Mean Reward across all agents: 973.9280625563324[0m
[37m[1m[2023-07-10 10:57:32,213][227910] Average Trajectory Length: 982.7386666666666[0m
[36m[2023-07-10 10:57:32,216][227910] mean_value=-787.5225674334072, max_value=943.4499923693038[0m
[37m[1m[2023-07-10 10:57:32,219][227910] New mean coefficients: [[ 4.770034   -0.21741413 -1.270861   -0.01170015  0.22765614]][0m
[37m[1m[2023-07-10 10:57:32,220][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:57:41,844][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 10:57:41,844][227910] FPS: 399061.11[0m
[36m[2023-07-10 10:57:41,847][227910] itr=209, itrs=2000, Progress: 10.45%[0m
[36m[2023-07-10 10:57:53,422][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 10:57:53,422][227910] FPS: 332158.01[0m
[36m[2023-07-10 10:57:58,326][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:57:58,326][227910] Reward + Measures: [[4175.86810857    0.30411869    0.28342602    0.38370335    0.060824  ]][0m
[37m[1m[2023-07-10 10:57:58,326][227910] Max Reward on eval: 4175.868108572412[0m
[37m[1m[2023-07-10 10:57:58,326][227910] Min Reward on eval: 4175.868108572412[0m
[37m[1m[2023-07-10 10:57:58,327][227910] Mean Reward across all agents: 4175.868108572412[0m
[37m[1m[2023-07-10 10:57:58,327][227910] Average Trajectory Length: 999.9726666666667[0m
[36m[2023-07-10 10:58:03,806][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:58:03,807][227910] Reward + Measures: [[ 307.84027798    0.63180006    0.48120004    0.65720004    0.3678    ]
 [-101.32316927    0.211404      0.31262553    0.21223955    0.14125206]
 [ 327.61447064    0.46599999    0.68919998    0.49790001    0.7622    ]
 ...
 [3329.07769136    0.26989999    0.36280003    0.47350001    0.08270001]
 [ 755.29960876    0.3513        0.38480005    0.33830002    0.1133    ]
 [2000.07808833    0.2775        0.53879994    0.44730002    0.1073    ]][0m
[37m[1m[2023-07-10 10:58:03,807][227910] Max Reward on eval: 3965.3160612020642[0m
[37m[1m[2023-07-10 10:58:03,807][227910] Min Reward on eval: -1382.33230535232[0m
[37m[1m[2023-07-10 10:58:03,807][227910] Mean Reward across all agents: 1237.0479792459719[0m
[37m[1m[2023-07-10 10:58:03,808][227910] Average Trajectory Length: 953.0233333333333[0m
[36m[2023-07-10 10:58:03,813][227910] mean_value=-471.96572665369325, max_value=1990.0161119167228[0m
[37m[1m[2023-07-10 10:58:03,816][227910] New mean coefficients: [[ 4.5007195  -0.00187369 -1.3154105   0.8613782   0.6876542 ]][0m
[37m[1m[2023-07-10 10:58:03,817][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:58:13,510][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 10:58:13,510][227910] FPS: 396260.28[0m
[36m[2023-07-10 10:58:13,512][227910] itr=210, itrs=2000, Progress: 10.50%[0m
[37m[1m[2023-07-10 10:58:15,612][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000190[0m
[36m[2023-07-10 10:58:27,520][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 10:58:27,520][227910] FPS: 329698.78[0m
[36m[2023-07-10 10:58:32,284][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:58:32,285][227910] Reward + Measures: [[2631.76389348    0.25823322    0.37659773    0.4212001     0.17817481]][0m
[37m[1m[2023-07-10 10:58:32,285][227910] Max Reward on eval: 2631.763893478921[0m
[37m[1m[2023-07-10 10:58:32,285][227910] Min Reward on eval: 2631.763893478921[0m
[37m[1m[2023-07-10 10:58:32,285][227910] Mean Reward across all agents: 2631.763893478921[0m
[37m[1m[2023-07-10 10:58:32,286][227910] Average Trajectory Length: 999.8183333333333[0m
[36m[2023-07-10 10:58:37,705][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:58:37,706][227910] Reward + Measures: [[ -47.29438071    0.48719999    0.4711        0.41          0.23910001]
 [ 986.16427938    0.27400002    0.37579998    0.373         0.28730002]
 [ 145.66082785    0.54152668    0.58387512    0.61341888    0.5632515 ]
 ...
 [ 114.33496129    0.31098184    0.42108184    0.36903641    0.25158182]
 [2179.16302464    0.2626        0.30939999    0.43350002    0.1584    ]
 [1129.05968554    0.2985        0.39499998    0.308         0.21920002]][0m
[37m[1m[2023-07-10 10:58:37,706][227910] Max Reward on eval: 2796.253266466921[0m
[37m[1m[2023-07-10 10:58:37,706][227910] Min Reward on eval: -751.2184138853263[0m
[37m[1m[2023-07-10 10:58:37,706][227910] Mean Reward across all agents: 937.7194142395365[0m
[37m[1m[2023-07-10 10:58:37,707][227910] Average Trajectory Length: 967.5033333333333[0m
[36m[2023-07-10 10:58:37,711][227910] mean_value=-632.0566328932792, max_value=2092.7565503760356[0m
[37m[1m[2023-07-10 10:58:37,714][227910] New mean coefficients: [[ 4.8185596  -0.34183508 -0.54730755  0.16096443 -0.26058596]][0m
[37m[1m[2023-07-10 10:58:37,715][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:58:47,363][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 10:58:47,363][227910] FPS: 398055.80[0m
[36m[2023-07-10 10:58:47,366][227910] itr=211, itrs=2000, Progress: 10.55%[0m
[36m[2023-07-10 10:58:58,829][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 10:58:58,829][227910] FPS: 335424.15[0m
[36m[2023-07-10 10:59:03,520][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:59:03,521][227910] Reward + Measures: [[3679.86059449    0.24199514    0.31770566    0.36769164    0.05907214]][0m
[37m[1m[2023-07-10 10:59:03,521][227910] Max Reward on eval: 3679.8605944891897[0m
[37m[1m[2023-07-10 10:59:03,521][227910] Min Reward on eval: 3679.8605944891897[0m
[37m[1m[2023-07-10 10:59:03,521][227910] Mean Reward across all agents: 3679.8605944891897[0m
[37m[1m[2023-07-10 10:59:03,522][227910] Average Trajectory Length: 996.372[0m
[36m[2023-07-10 10:59:08,912][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:59:08,913][227910] Reward + Measures: [[ -39.04516794    0.24983846    0.25926247    0.31193241    0.25148088]
 [ 673.14549577    0.22129802    0.27283069    0.2626594     0.17485742]
 [ 873.71809212    0.30650002    0.55599993    0.25620002    0.4271    ]
 ...
 [-115.89790903    0.40685222    0.19425121    0.45648551    0.37941355]
 [   9.55751185    0.41375467    0.25319585    0.42649278    0.27613816]
 [ 884.8891123     0.25640002    0.5097        0.42880002    0.40559998]][0m
[37m[1m[2023-07-10 10:59:08,913][227910] Max Reward on eval: 2927.642246596422[0m
[37m[1m[2023-07-10 10:59:08,913][227910] Min Reward on eval: -904.0148197255155[0m
[37m[1m[2023-07-10 10:59:08,914][227910] Mean Reward across all agents: 518.2807276392548[0m
[37m[1m[2023-07-10 10:59:08,914][227910] Average Trajectory Length: 882.4886666666666[0m
[36m[2023-07-10 10:59:08,918][227910] mean_value=-857.1600551197479, max_value=1346.1701169928385[0m
[37m[1m[2023-07-10 10:59:08,921][227910] New mean coefficients: [[ 4.707173   -0.72675663 -0.9777147   0.01229875  0.3089205 ]][0m
[37m[1m[2023-07-10 10:59:08,922][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:59:18,601][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 10:59:18,601][227910] FPS: 396798.42[0m
[36m[2023-07-10 10:59:18,603][227910] itr=212, itrs=2000, Progress: 10.60%[0m
[36m[2023-07-10 10:59:30,084][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 10:59:30,084][227910] FPS: 334895.66[0m
[36m[2023-07-10 10:59:34,776][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:59:34,777][227910] Reward + Measures: [[4133.67186507    0.25731856    0.28924888    0.35452139    0.06899676]][0m
[37m[1m[2023-07-10 10:59:34,777][227910] Max Reward on eval: 4133.671865068787[0m
[37m[1m[2023-07-10 10:59:34,777][227910] Min Reward on eval: 4133.671865068787[0m
[37m[1m[2023-07-10 10:59:34,777][227910] Mean Reward across all agents: 4133.671865068787[0m
[37m[1m[2023-07-10 10:59:34,778][227910] Average Trajectory Length: 998.352[0m
[36m[2023-07-10 10:59:40,182][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 10:59:40,183][227910] Reward + Measures: [[3045.95147143    0.24419999    0.42899999    0.303         0.0894    ]
 [2811.11230999    0.23999999    0.34890005    0.35470003    0.09      ]
 [-852.57002509    0.5966        0.75389999    0.0485        0.72520006]
 ...
 [ -17.2476815     0.3496612     0.35427058    0.4151299     0.37257326]
 [ -62.29761995    0.24191044    0.29322988    0.25010449    0.30954924]
 [ 446.05803443    0.60949999    0.37080002    0.51719999    0.0948    ]][0m
[37m[1m[2023-07-10 10:59:40,183][227910] Max Reward on eval: 3773.469487792533[0m
[37m[1m[2023-07-10 10:59:40,183][227910] Min Reward on eval: -1970.3395105262287[0m
[37m[1m[2023-07-10 10:59:40,184][227910] Mean Reward across all agents: 464.81823419496783[0m
[37m[1m[2023-07-10 10:59:40,184][227910] Average Trajectory Length: 965.1293333333333[0m
[36m[2023-07-10 10:59:40,190][227910] mean_value=-399.083349832887, max_value=1513.436587241434[0m
[37m[1m[2023-07-10 10:59:40,192][227910] New mean coefficients: [[ 5.0645375  -1.1062257  -0.6914657   0.05958125 -0.37298995]][0m
[37m[1m[2023-07-10 10:59:40,194][227910] Moving the mean solution point...[0m
[36m[2023-07-10 10:59:49,864][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 10:59:49,865][227910] FPS: 397139.43[0m
[36m[2023-07-10 10:59:49,867][227910] itr=213, itrs=2000, Progress: 10.65%[0m
[36m[2023-07-10 11:00:01,351][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 11:00:01,351][227910] FPS: 334796.67[0m
[36m[2023-07-10 11:00:06,081][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:00:06,081][227910] Reward + Measures: [[4334.33332209    0.25673905    0.27399465    0.33505461    0.06338638]][0m
[37m[1m[2023-07-10 11:00:06,081][227910] Max Reward on eval: 4334.333322089452[0m
[37m[1m[2023-07-10 11:00:06,082][227910] Min Reward on eval: 4334.333322089452[0m
[37m[1m[2023-07-10 11:00:06,082][227910] Mean Reward across all agents: 4334.333322089452[0m
[37m[1m[2023-07-10 11:00:06,082][227910] Average Trajectory Length: 998.5413333333333[0m
[36m[2023-07-10 11:00:11,579][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:00:11,580][227910] Reward + Measures: [[ 156.63303047    0.3452        0.36180001    0.46140003    0.38060004]
 [-343.20023804    0.17289613    0.35604054    0.25111529    0.1025022 ]
 [-536.92349285    0.13378601    0.26105505    0.19024892    0.13237397]
 ...
 [2433.15267545    0.30900002    0.40219998    0.38580003    0.1523    ]
 [2797.42556362    0.3053        0.36550003    0.36629999    0.1186    ]
 [-492.2218628     0.15236412    0.31681478    0.21349417    0.12819028]][0m
[37m[1m[2023-07-10 11:00:11,580][227910] Max Reward on eval: 3964.200848220475[0m
[37m[1m[2023-07-10 11:00:11,580][227910] Min Reward on eval: -1409.677210859838[0m
[37m[1m[2023-07-10 11:00:11,580][227910] Mean Reward across all agents: 401.5226810881254[0m
[37m[1m[2023-07-10 11:00:11,581][227910] Average Trajectory Length: 813.7363333333333[0m
[36m[2023-07-10 11:00:11,583][227910] mean_value=-1392.564230186734, max_value=2314.346467916673[0m
[37m[1m[2023-07-10 11:00:11,585][227910] New mean coefficients: [[ 4.751114   -0.38477713 -0.11763483 -0.16925868  0.23324853]][0m
[37m[1m[2023-07-10 11:00:11,586][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:00:21,405][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 11:00:21,405][227910] FPS: 391165.13[0m
[36m[2023-07-10 11:00:21,407][227910] itr=214, itrs=2000, Progress: 10.70%[0m
[36m[2023-07-10 11:00:32,870][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 11:00:32,870][227910] FPS: 335422.29[0m
[36m[2023-07-10 11:00:37,584][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:00:37,585][227910] Reward + Measures: [[1779.17508684    0.25456774    0.43986782    0.30087391    0.21496303]][0m
[37m[1m[2023-07-10 11:00:37,585][227910] Max Reward on eval: 1779.1750868384975[0m
[37m[1m[2023-07-10 11:00:37,585][227910] Min Reward on eval: 1779.1750868384975[0m
[37m[1m[2023-07-10 11:00:37,586][227910] Mean Reward across all agents: 1779.1750868384975[0m
[37m[1m[2023-07-10 11:00:37,586][227910] Average Trajectory Length: 993.2736666666666[0m
[36m[2023-07-10 11:00:42,952][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:00:42,952][227910] Reward + Measures: [[ 76.19142369   0.21132898   0.42088461   0.20919137   0.30525517]
 [603.33998082   0.34750003   0.46970001   0.35510001   0.28049999]
 [745.06509374   0.27810478   0.28164551   0.30133894   0.18590657]
 ...
 [409.80799105   0.14562307   0.22901164   0.16209185   0.13814585]
 [694.04729611   0.34110004   0.3321       0.31610003   0.18710001]
 [ 70.84549711   0.28130001   0.48850003   0.37800002   0.30929998]][0m
[37m[1m[2023-07-10 11:00:42,952][227910] Max Reward on eval: 2238.5280937059315[0m
[37m[1m[2023-07-10 11:00:42,953][227910] Min Reward on eval: -854.1347000859911[0m
[37m[1m[2023-07-10 11:00:42,953][227910] Mean Reward across all agents: 881.5797194840479[0m
[37m[1m[2023-07-10 11:00:42,953][227910] Average Trajectory Length: 949.802[0m
[36m[2023-07-10 11:00:42,956][227910] mean_value=-704.578681565838, max_value=974.8912207900055[0m
[37m[1m[2023-07-10 11:00:42,959][227910] New mean coefficients: [[ 5.222673   -0.08081323  0.13140915 -0.9136349  -0.2744215 ]][0m
[37m[1m[2023-07-10 11:00:42,960][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:00:52,556][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 11:00:52,556][227910] FPS: 400250.39[0m
[36m[2023-07-10 11:00:52,558][227910] itr=215, itrs=2000, Progress: 10.75%[0m
[36m[2023-07-10 11:01:04,084][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 11:01:04,084][227910] FPS: 333652.81[0m
[36m[2023-07-10 11:01:08,854][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:01:08,855][227910] Reward + Measures: [[2180.44528522    0.24710278    0.41734573    0.30692595    0.1854554 ]][0m
[37m[1m[2023-07-10 11:01:08,855][227910] Max Reward on eval: 2180.4452852206964[0m
[37m[1m[2023-07-10 11:01:08,855][227910] Min Reward on eval: 2180.4452852206964[0m
[37m[1m[2023-07-10 11:01:08,856][227910] Mean Reward across all agents: 2180.4452852206964[0m
[37m[1m[2023-07-10 11:01:08,856][227910] Average Trajectory Length: 993.3623333333333[0m
[36m[2023-07-10 11:01:14,454][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:01:14,455][227910] Reward + Measures: [[ 628.55550487    0.26169997    0.55400002    0.25139999    0.30840001]
 [ 635.9002164     0.17302075    0.35367203    0.24009864    0.29942331]
 [ 917.63427114    0.26610002    0.58570004    0.28200004    0.34200001]
 ...
 [ 193.48917469    0.47300002    0.62120003    0.23420003    0.49400002]
 [  39.45491272    0.28970203    0.24853694    0.37443978    0.21630521]
 [-274.51708244    0.56389999    0.2633        0.43589997    0.39650002]][0m
[37m[1m[2023-07-10 11:01:14,455][227910] Max Reward on eval: 2057.181477402849[0m
[37m[1m[2023-07-10 11:01:14,455][227910] Min Reward on eval: -835.7403391036089[0m
[37m[1m[2023-07-10 11:01:14,456][227910] Mean Reward across all agents: 695.2016891278794[0m
[37m[1m[2023-07-10 11:01:14,456][227910] Average Trajectory Length: 955.813[0m
[36m[2023-07-10 11:01:14,462][227910] mean_value=-227.65234891536014, max_value=1660.7618622395735[0m
[37m[1m[2023-07-10 11:01:14,465][227910] New mean coefficients: [[ 5.956732   -0.46118897 -1.1863183  -0.96796316 -0.05910382]][0m
[37m[1m[2023-07-10 11:01:14,466][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:01:24,039][227910] train() took 9.57 seconds to complete[0m
[36m[2023-07-10 11:01:24,039][227910] FPS: 401210.99[0m
[36m[2023-07-10 11:01:24,042][227910] itr=216, itrs=2000, Progress: 10.80%[0m
[36m[2023-07-10 11:01:35,680][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 11:01:35,680][227910] FPS: 330377.82[0m
[36m[2023-07-10 11:01:40,606][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:01:40,612][227910] Reward + Measures: [[2435.7974002     0.24257934    0.40393814    0.30652443    0.17050056]][0m
[37m[1m[2023-07-10 11:01:40,613][227910] Max Reward on eval: 2435.79740020325[0m
[37m[1m[2023-07-10 11:01:40,613][227910] Min Reward on eval: 2435.79740020325[0m
[37m[1m[2023-07-10 11:01:40,614][227910] Mean Reward across all agents: 2435.79740020325[0m
[37m[1m[2023-07-10 11:01:40,614][227910] Average Trajectory Length: 988.3836666666666[0m
[36m[2023-07-10 11:01:46,169][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:01:46,170][227910] Reward + Measures: [[ -68.24371323    0.42225561    0.37845045    0.37514719    0.25388455]
 [ 639.62167279    0.29499999    0.54770005    0.17490001    0.40780002]
 [2003.0966533     0.24148141    0.35989413    0.29072937    0.18978862]
 ...
 [2432.31625951    0.23189998    0.33930001    0.2915        0.15710001]
 [ 241.69874728    0.21269999    0.60359997    0.1864        0.39910004]
 [ 459.85316141    0.3933        0.67119998    0.103         0.51170003]][0m
[37m[1m[2023-07-10 11:01:46,170][227910] Max Reward on eval: 2557.344054440199[0m
[37m[1m[2023-07-10 11:01:46,170][227910] Min Reward on eval: -551.0907131676853[0m
[37m[1m[2023-07-10 11:01:46,170][227910] Mean Reward across all agents: 781.2911066957944[0m
[37m[1m[2023-07-10 11:01:46,171][227910] Average Trajectory Length: 976.6123333333333[0m
[36m[2023-07-10 11:01:46,176][227910] mean_value=-308.0095208132084, max_value=1926.3983016750647[0m
[37m[1m[2023-07-10 11:01:46,178][227910] New mean coefficients: [[ 5.931182   -0.88879216 -1.1197182  -1.0286766  -0.35226312]][0m
[37m[1m[2023-07-10 11:01:46,179][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:01:55,946][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 11:01:55,946][227910] FPS: 393250.42[0m
[36m[2023-07-10 11:01:55,948][227910] itr=217, itrs=2000, Progress: 10.85%[0m
[36m[2023-07-10 11:02:07,460][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 11:02:07,461][227910] FPS: 334012.46[0m
[36m[2023-07-10 11:02:12,221][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:02:12,221][227910] Reward + Measures: [[2714.56879659    0.23980759    0.39160368    0.30708       0.15559003]][0m
[37m[1m[2023-07-10 11:02:12,221][227910] Max Reward on eval: 2714.5687965895836[0m
[37m[1m[2023-07-10 11:02:12,222][227910] Min Reward on eval: 2714.5687965895836[0m
[37m[1m[2023-07-10 11:02:12,222][227910] Mean Reward across all agents: 2714.5687965895836[0m
[37m[1m[2023-07-10 11:02:12,222][227910] Average Trajectory Length: 988.6073333333333[0m
[36m[2023-07-10 11:02:17,761][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:02:17,762][227910] Reward + Measures: [[-150.83972673    0.33570001    0.4707        0.39649999    0.22090001]
 [  35.09519326    0.3187151     0.4831526     0.36140594    0.25946793]
 [1767.96062217    0.24585783    0.46258107    0.27997476    0.11087032]
 ...
 [ 170.8744932     0.5341        0.52710003    0.32620001    0.34639999]
 [1914.52530033    0.29285502    0.32335722    0.33738342    0.14922428]
 [  95.03428934    0.47919336    0.60691994    0.52152675    0.15434952]][0m
[37m[1m[2023-07-10 11:02:17,762][227910] Max Reward on eval: 2961.7810764092487[0m
[37m[1m[2023-07-10 11:02:17,762][227910] Min Reward on eval: -807.2393137185485[0m
[37m[1m[2023-07-10 11:02:17,762][227910] Mean Reward across all agents: 854.7590080679096[0m
[37m[1m[2023-07-10 11:02:17,763][227910] Average Trajectory Length: 925.0946666666666[0m
[36m[2023-07-10 11:02:17,765][227910] mean_value=-1191.665088711868, max_value=1946.2927435485408[0m
[37m[1m[2023-07-10 11:02:17,768][227910] New mean coefficients: [[ 5.946649  -1.2101624 -1.2761645 -0.1607545 -1.006662 ]][0m
[37m[1m[2023-07-10 11:02:17,769][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:02:27,581][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 11:02:27,581][227910] FPS: 391455.92[0m
[36m[2023-07-10 11:02:27,583][227910] itr=218, itrs=2000, Progress: 10.90%[0m
[36m[2023-07-10 11:02:39,212][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 11:02:39,212][227910] FPS: 330691.42[0m
[36m[2023-07-10 11:02:44,002][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:02:44,002][227910] Reward + Measures: [[3001.32196991    0.24054424    0.39016291    0.30857983    0.13667606]][0m
[37m[1m[2023-07-10 11:02:44,003][227910] Max Reward on eval: 3001.321969912478[0m
[37m[1m[2023-07-10 11:02:44,003][227910] Min Reward on eval: 3001.321969912478[0m
[37m[1m[2023-07-10 11:02:44,003][227910] Mean Reward across all agents: 3001.321969912478[0m
[37m[1m[2023-07-10 11:02:44,003][227910] Average Trajectory Length: 982.3463333333333[0m
[36m[2023-07-10 11:02:49,591][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:02:49,592][227910] Reward + Measures: [[1177.38641115    0.2723        0.5517        0.29539999    0.29650003]
 [ 264.66705038    0.24059999    0.62660003    0.3127        0.49039999]
 [-194.33188015    0.27580002    0.66109997    0.23029999    0.55949998]
 ...
 [1216.48183696    0.27510414    0.44361892    0.34259519    0.2168849 ]
 [ 158.8407816     0.28299999    0.66440004    0.1366        0.52850002]
 [ 318.11492948    0.26089999    0.70970005    0.1822        0.62349999]][0m
[37m[1m[2023-07-10 11:02:49,592][227910] Max Reward on eval: 3057.1781734219753[0m
[37m[1m[2023-07-10 11:02:49,592][227910] Min Reward on eval: -457.5476783977414[0m
[37m[1m[2023-07-10 11:02:49,593][227910] Mean Reward across all agents: 1108.9242207174623[0m
[37m[1m[2023-07-10 11:02:49,593][227910] Average Trajectory Length: 956.5796666666666[0m
[36m[2023-07-10 11:02:49,596][227910] mean_value=-523.1980500168348, max_value=937.1705439745324[0m
[37m[1m[2023-07-10 11:02:49,599][227910] New mean coefficients: [[ 5.8198256  -1.0554873  -2.211601    0.7533837  -0.47463596]][0m
[37m[1m[2023-07-10 11:02:49,600][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:02:59,471][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 11:02:59,471][227910] FPS: 389101.44[0m
[36m[2023-07-10 11:02:59,474][227910] itr=219, itrs=2000, Progress: 10.95%[0m
[36m[2023-07-10 11:03:11,073][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 11:03:11,074][227910] FPS: 331527.77[0m
[36m[2023-07-10 11:03:15,958][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:03:15,959][227910] Reward + Measures: [[3395.56675713    0.22694154    0.39407471    0.34336743    0.10240177]][0m
[37m[1m[2023-07-10 11:03:15,959][227910] Max Reward on eval: 3395.566757134842[0m
[37m[1m[2023-07-10 11:03:15,959][227910] Min Reward on eval: 3395.566757134842[0m
[37m[1m[2023-07-10 11:03:15,959][227910] Mean Reward across all agents: 3395.566757134842[0m
[37m[1m[2023-07-10 11:03:15,960][227910] Average Trajectory Length: 981.752[0m
[36m[2023-07-10 11:03:21,590][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:03:21,590][227910] Reward + Measures: [[458.13961144   0.40549999   0.65439999   0.2599       0.50710005]
 [  8.4259409    0.38920003   0.588        0.3224       0.63059998]
 [206.8666002    0.60699999   0.59749997   0.38700002   0.69419998]
 ...
 [946.04236989   0.33620003   0.56190002   0.31799999   0.29910001]
 [652.52116643   0.26680002   0.43599996   0.43810001   0.36129999]
 [-79.3812463    0.2414       0.48920003   0.33989999   0.33410001]][0m
[37m[1m[2023-07-10 11:03:21,591][227910] Max Reward on eval: 3173.0161967046793[0m
[37m[1m[2023-07-10 11:03:21,591][227910] Min Reward on eval: -1529.2489735689596[0m
[37m[1m[2023-07-10 11:03:21,591][227910] Mean Reward across all agents: 96.68737361285768[0m
[37m[1m[2023-07-10 11:03:21,591][227910] Average Trajectory Length: 979.8813333333333[0m
[36m[2023-07-10 11:03:21,597][227910] mean_value=-376.53977157250694, max_value=1938.1582681526402[0m
[37m[1m[2023-07-10 11:03:21,599][227910] New mean coefficients: [[ 6.1470375  -0.532656   -1.8949738   1.3226216   0.16572773]][0m
[37m[1m[2023-07-10 11:03:21,601][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:03:31,394][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 11:03:31,394][227910] FPS: 392165.99[0m
[36m[2023-07-10 11:03:31,397][227910] itr=220, itrs=2000, Progress: 11.00%[0m
[37m[1m[2023-07-10 11:03:33,558][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000200[0m
[36m[2023-07-10 11:03:45,407][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 11:03:45,407][227910] FPS: 331512.44[0m
[36m[2023-07-10 11:03:50,169][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:03:50,170][227910] Reward + Measures: [[3572.25920638    0.22854085    0.37053767    0.34197119    0.08880927]][0m
[37m[1m[2023-07-10 11:03:50,170][227910] Max Reward on eval: 3572.2592063768407[0m
[37m[1m[2023-07-10 11:03:50,170][227910] Min Reward on eval: 3572.2592063768407[0m
[37m[1m[2023-07-10 11:03:50,170][227910] Mean Reward across all agents: 3572.2592063768407[0m
[37m[1m[2023-07-10 11:03:50,170][227910] Average Trajectory Length: 979.9666666666666[0m
[36m[2023-07-10 11:03:55,706][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:03:55,707][227910] Reward + Measures: [[-322.54638574    0.21010001    0.49609995    0.25839999    0.41240001]
 [2054.20565907    0.24689999    0.55909997    0.30180001    0.14010002]
 [ 109.44961814    0.21440001    0.46450001    0.25120002    0.40700004]
 ...
 [ 986.36823217    0.3204        0.43379998    0.30809999    0.26989999]
 [ 513.88124688    0.38170001    0.51560003    0.28980002    0.40529999]
 [1146.87861712    0.28040001    0.42650005    0.37530002    0.27360001]][0m
[37m[1m[2023-07-10 11:03:55,707][227910] Max Reward on eval: 3087.398857048014[0m
[37m[1m[2023-07-10 11:03:55,707][227910] Min Reward on eval: -1033.611213973118[0m
[37m[1m[2023-07-10 11:03:55,708][227910] Mean Reward across all agents: 633.0362375576373[0m
[37m[1m[2023-07-10 11:03:55,708][227910] Average Trajectory Length: 989.8566666666667[0m
[36m[2023-07-10 11:03:55,711][227910] mean_value=-619.054622585437, max_value=1549.6562092254524[0m
[37m[1m[2023-07-10 11:03:55,714][227910] New mean coefficients: [[ 5.5942216  -0.65356845 -2.6887233   0.9802045   0.50996834]][0m
[37m[1m[2023-07-10 11:03:55,715][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:04:05,444][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 11:04:05,444][227910] FPS: 394751.54[0m
[36m[2023-07-10 11:04:05,447][227910] itr=221, itrs=2000, Progress: 11.05%[0m
[36m[2023-07-10 11:04:17,079][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 11:04:17,079][227910] FPS: 330599.32[0m
[36m[2023-07-10 11:04:21,871][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:04:21,871][227910] Reward + Measures: [[3742.60208265    0.2272688     0.36433703    0.3293193     0.08441283]][0m
[37m[1m[2023-07-10 11:04:21,871][227910] Max Reward on eval: 3742.6020826460967[0m
[37m[1m[2023-07-10 11:04:21,872][227910] Min Reward on eval: 3742.6020826460967[0m
[37m[1m[2023-07-10 11:04:21,872][227910] Mean Reward across all agents: 3742.6020826460967[0m
[37m[1m[2023-07-10 11:04:21,872][227910] Average Trajectory Length: 981.1586666666666[0m
[36m[2023-07-10 11:04:27,370][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:04:27,371][227910] Reward + Measures: [[ 382.26678125    0.20135747    0.35425058    0.32935518    0.32750344]
 [1271.71177156    0.22920001    0.42449999    0.29060003    0.22529998]
 [1031.5121544     0.24920002    0.3705        0.36810002    0.28480002]
 ...
 [2190.36988549    0.24528657    0.4398078     0.35276791    0.13286443]
 [ 929.6565203     0.22309999    0.39260003    0.3231        0.28280002]
 [2184.17549308    0.21539998    0.27090001    0.23830004    0.098     ]][0m
[37m[1m[2023-07-10 11:04:27,371][227910] Max Reward on eval: 3628.04589135875[0m
[37m[1m[2023-07-10 11:04:27,372][227910] Min Reward on eval: -1338.908534970181[0m
[37m[1m[2023-07-10 11:04:27,372][227910] Mean Reward across all agents: 895.0878851856107[0m
[37m[1m[2023-07-10 11:04:27,372][227910] Average Trajectory Length: 966.1013333333333[0m
[36m[2023-07-10 11:04:27,374][227910] mean_value=-681.0039251476569, max_value=951.1368819576539[0m
[37m[1m[2023-07-10 11:04:27,376][227910] New mean coefficients: [[ 5.5190487 -1.2796361 -3.493247   1.1120677  0.7402436]][0m
[37m[1m[2023-07-10 11:04:27,377][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:04:37,093][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 11:04:37,094][227910] FPS: 395296.30[0m
[36m[2023-07-10 11:04:37,096][227910] itr=222, itrs=2000, Progress: 11.10%[0m
[36m[2023-07-10 11:04:48,685][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 11:04:48,685][227910] FPS: 331830.48[0m
[36m[2023-07-10 11:04:53,479][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:04:53,480][227910] Reward + Measures: [[3886.06798846    0.22956981    0.34231794    0.31256309    0.07972962]][0m
[37m[1m[2023-07-10 11:04:53,480][227910] Max Reward on eval: 3886.067988462243[0m
[37m[1m[2023-07-10 11:04:53,480][227910] Min Reward on eval: 3886.067988462243[0m
[37m[1m[2023-07-10 11:04:53,481][227910] Mean Reward across all agents: 3886.067988462243[0m
[37m[1m[2023-07-10 11:04:53,481][227910] Average Trajectory Length: 972.625[0m
[36m[2023-07-10 11:04:58,583][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:04:58,583][227910] Reward + Measures: [[  301.25710873     0.26217774     0.47628003     0.19434656
      0.32086983]
 [ -988.99344724     0.45039091     0.64403641     0.11282728
      0.59727275]
 [   -6.62411102     0.47193828     0.70792788     0.12338344
      0.61811584]
 ...
 [ -369.51436275     0.25952059     0.51916558     0.15257464
      0.38573757]
 [-1288.11723168     0.0959         0.55420005     0.2753
      0.53200001]
 [ -668.27270845     0.1814         0.59140003     0.25219998
      0.55320001]][0m
[37m[1m[2023-07-10 11:04:58,584][227910] Max Reward on eval: 3426.878787535895[0m
[37m[1m[2023-07-10 11:04:58,584][227910] Min Reward on eval: -1666.797467133915[0m
[37m[1m[2023-07-10 11:04:58,584][227910] Mean Reward across all agents: -196.299304954973[0m
[37m[1m[2023-07-10 11:04:58,584][227910] Average Trajectory Length: 842.3696666666666[0m
[36m[2023-07-10 11:04:58,587][227910] mean_value=-851.3234574986124, max_value=1238.4874977197549[0m
[37m[1m[2023-07-10 11:04:58,590][227910] New mean coefficients: [[ 5.0174246  -1.4401867  -2.3185654   0.88000405  0.6561127 ]][0m
[37m[1m[2023-07-10 11:04:58,591][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:05:07,822][227910] train() took 9.23 seconds to complete[0m
[36m[2023-07-10 11:05:07,822][227910] FPS: 416078.22[0m
[36m[2023-07-10 11:05:07,824][227910] itr=223, itrs=2000, Progress: 11.15%[0m
[36m[2023-07-10 11:05:19,260][227910] train() took 11.42 seconds to complete[0m
[36m[2023-07-10 11:05:19,260][227910] FPS: 336215.43[0m
[36m[2023-07-10 11:05:24,044][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:05:24,044][227910] Reward + Measures: [[3803.59261553    0.22226728    0.38644239    0.41064656    0.10052415]][0m
[37m[1m[2023-07-10 11:05:24,045][227910] Max Reward on eval: 3803.5926155325074[0m
[37m[1m[2023-07-10 11:05:24,045][227910] Min Reward on eval: 3803.5926155325074[0m
[37m[1m[2023-07-10 11:05:24,045][227910] Mean Reward across all agents: 3803.5926155325074[0m
[37m[1m[2023-07-10 11:05:24,045][227910] Average Trajectory Length: 985.6653333333333[0m
[36m[2023-07-10 11:05:29,673][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:05:29,673][227910] Reward + Measures: [[ 104.65901061    0.30700001    0.46749997    0.51590002    0.1337    ]
 [ 645.3354466     0.30276656    0.54735941    0.36479855    0.23084866]
 [-466.19552277    0.28654155    0.30531201    0.24857008    0.31404206]
 ...
 [ 965.43875455    0.23683767    0.47927561    0.397219      0.13719802]
 [-773.80898367    0.28191981    0.33851552    0.35773554    0.35817683]
 [-249.99896255    0.3537831     0.40248558    0.36621591    0.34255934]][0m
[37m[1m[2023-07-10 11:05:29,673][227910] Max Reward on eval: 3157.503903898876[0m
[37m[1m[2023-07-10 11:05:29,674][227910] Min Reward on eval: -1043.1781658504624[0m
[37m[1m[2023-07-10 11:05:29,674][227910] Mean Reward across all agents: 429.4912197255624[0m
[37m[1m[2023-07-10 11:05:29,674][227910] Average Trajectory Length: 825.1356666666667[0m
[36m[2023-07-10 11:05:29,676][227910] mean_value=-1455.6964672930837, max_value=739.4723600676298[0m
[37m[1m[2023-07-10 11:05:29,679][227910] New mean coefficients: [[ 5.2492237  -1.8138658  -1.7945757   0.35112286  0.22115588]][0m
[37m[1m[2023-07-10 11:05:29,680][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:05:39,363][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 11:05:39,363][227910] FPS: 396643.72[0m
[36m[2023-07-10 11:05:39,365][227910] itr=224, itrs=2000, Progress: 11.20%[0m
[36m[2023-07-10 11:05:50,825][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 11:05:50,826][227910] FPS: 335501.89[0m
[36m[2023-07-10 11:05:55,597][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:05:55,597][227910] Reward + Measures: [[3792.81386402    0.22964928    0.33154821    0.32752496    0.07755771]][0m
[37m[1m[2023-07-10 11:05:55,598][227910] Max Reward on eval: 3792.8138640220313[0m
[37m[1m[2023-07-10 11:05:55,598][227910] Min Reward on eval: 3792.8138640220313[0m
[37m[1m[2023-07-10 11:05:55,598][227910] Mean Reward across all agents: 3792.8138640220313[0m
[37m[1m[2023-07-10 11:05:55,598][227910] Average Trajectory Length: 958.3973333333333[0m
[36m[2023-07-10 11:06:01,126][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:06:01,127][227910] Reward + Measures: [[ 456.45693398    0.24127157    0.3023971     0.3060796     0.24885713]
 [-243.68360169    0.20417145    0.27147144    0.30858573    0.20157142]
 [-535.06481495    0.3337        0.60700005    0.22920001    0.58050007]
 ...
 [-416.99243994    0.24061723    0.4875457     0.27573895    0.49397117]
 [  32.86805675    0.18408568    0.27849689    0.31930465    0.22473259]
 [ 736.15732509    0.16041158    0.32608798    0.23535371    0.10571139]][0m
[37m[1m[2023-07-10 11:06:01,127][227910] Max Reward on eval: 3276.3946319248525[0m
[37m[1m[2023-07-10 11:06:01,127][227910] Min Reward on eval: -1083.4403682823292[0m
[37m[1m[2023-07-10 11:06:01,128][227910] Mean Reward across all agents: 549.9462489986084[0m
[37m[1m[2023-07-10 11:06:01,128][227910] Average Trajectory Length: 897.6189999999999[0m
[36m[2023-07-10 11:06:01,132][227910] mean_value=-768.8865864219717, max_value=1062.0532054552068[0m
[37m[1m[2023-07-10 11:06:01,135][227910] New mean coefficients: [[ 5.503575   -1.9670663  -2.3597412   0.8933739   0.63942784]][0m
[37m[1m[2023-07-10 11:06:01,136][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:06:10,844][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 11:06:10,844][227910] FPS: 395630.20[0m
[36m[2023-07-10 11:06:10,846][227910] itr=225, itrs=2000, Progress: 11.25%[0m
[36m[2023-07-10 11:06:22,349][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 11:06:22,349][227910] FPS: 334301.65[0m
[36m[2023-07-10 11:06:27,111][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:06:27,112][227910] Reward + Measures: [[4054.11761469    0.22912699    0.3295567     0.32640469    0.06908704]][0m
[37m[1m[2023-07-10 11:06:27,112][227910] Max Reward on eval: 4054.117614689085[0m
[37m[1m[2023-07-10 11:06:27,112][227910] Min Reward on eval: 4054.117614689085[0m
[37m[1m[2023-07-10 11:06:27,113][227910] Mean Reward across all agents: 4054.117614689085[0m
[37m[1m[2023-07-10 11:06:27,113][227910] Average Trajectory Length: 968.6139999999999[0m
[36m[2023-07-10 11:06:32,591][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:06:32,591][227910] Reward + Measures: [[1444.89479025    0.28340003    0.4402        0.42770001    0.15350001]
 [2078.36968069    0.21780001    0.36000001    0.44729996    0.0985    ]
 [1362.12510293    0.23579998    0.5309        0.32800001    0.19569999]
 ...
 [1093.25737136    0.25848854    0.50076312    0.32506666    0.17746611]
 [  87.87466427    0.3143276     0.42388788    0.24912368    0.31957775]
 [ 998.85938861    0.17353132    0.29034957    0.41250095    0.28550962]][0m
[37m[1m[2023-07-10 11:06:32,592][227910] Max Reward on eval: 3839.9728258298246[0m
[37m[1m[2023-07-10 11:06:32,592][227910] Min Reward on eval: -1165.259057196777[0m
[37m[1m[2023-07-10 11:06:32,592][227910] Mean Reward across all agents: 800.8040825303644[0m
[37m[1m[2023-07-10 11:06:32,593][227910] Average Trajectory Length: 884.9513333333333[0m
[36m[2023-07-10 11:06:32,595][227910] mean_value=-1254.7617430498583, max_value=929.3910121260496[0m
[37m[1m[2023-07-10 11:06:32,597][227910] New mean coefficients: [[ 5.167993   -1.917984   -1.7248552   0.65778434  0.46933526]][0m
[37m[1m[2023-07-10 11:06:32,598][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:06:42,428][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 11:06:42,428][227910] FPS: 390716.91[0m
[36m[2023-07-10 11:06:42,430][227910] itr=226, itrs=2000, Progress: 11.30%[0m
[36m[2023-07-10 11:06:54,080][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 11:06:54,080][227910] FPS: 330042.92[0m
[36m[2023-07-10 11:06:58,903][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:06:58,903][227910] Reward + Measures: [[4275.92323433    0.22681612    0.32717898    0.32675004    0.06564519]][0m
[37m[1m[2023-07-10 11:06:58,903][227910] Max Reward on eval: 4275.923234328928[0m
[37m[1m[2023-07-10 11:06:58,903][227910] Min Reward on eval: 4275.923234328928[0m
[37m[1m[2023-07-10 11:06:58,903][227910] Mean Reward across all agents: 4275.923234328928[0m
[37m[1m[2023-07-10 11:06:58,904][227910] Average Trajectory Length: 979.1883333333333[0m
[36m[2023-07-10 11:07:04,354][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:07:04,355][227910] Reward + Measures: [[1546.96849749    0.22679999    0.41560003    0.31160003    0.1549    ]
 [1571.13279432    0.2523424     0.34588945    0.36107072    0.18207471]
 [2792.4876578     0.23999999    0.33010003    0.38929996    0.176     ]
 ...
 [2761.9144253     0.20545316    0.42165318    0.28423166    0.08784557]
 [2294.39007829    0.2256        0.33710003    0.34920001    0.1521    ]
 [1277.740722      0.26072177    0.3960782     0.37039742    0.24570897]][0m
[37m[1m[2023-07-10 11:07:04,355][227910] Max Reward on eval: 4024.0462556285784[0m
[37m[1m[2023-07-10 11:07:04,356][227910] Min Reward on eval: -555.9470478869392[0m
[37m[1m[2023-07-10 11:07:04,356][227910] Mean Reward across all agents: 1638.816684124362[0m
[37m[1m[2023-07-10 11:07:04,356][227910] Average Trajectory Length: 970.649[0m
[36m[2023-07-10 11:07:04,359][227910] mean_value=-697.1280319530157, max_value=1090.9395021711116[0m
[37m[1m[2023-07-10 11:07:04,362][227910] New mean coefficients: [[ 5.664726   -1.756232   -2.2358098   0.17214769  0.53511703]][0m
[37m[1m[2023-07-10 11:07:04,363][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:07:14,106][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 11:07:14,106][227910] FPS: 394189.86[0m
[36m[2023-07-10 11:07:14,109][227910] itr=227, itrs=2000, Progress: 11.35%[0m
[36m[2023-07-10 11:07:25,743][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 11:07:25,743][227910] FPS: 330486.77[0m
[36m[2023-07-10 11:07:30,562][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:07:30,562][227910] Reward + Measures: [[4374.77406487    0.22841203    0.32390299    0.30485559    0.06457802]][0m
[37m[1m[2023-07-10 11:07:30,562][227910] Max Reward on eval: 4374.7740648707595[0m
[37m[1m[2023-07-10 11:07:30,563][227910] Min Reward on eval: 4374.7740648707595[0m
[37m[1m[2023-07-10 11:07:30,563][227910] Mean Reward across all agents: 4374.7740648707595[0m
[37m[1m[2023-07-10 11:07:30,563][227910] Average Trajectory Length: 970.4019999999999[0m
[36m[2023-07-10 11:07:36,069][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:07:36,069][227910] Reward + Measures: [[-126.43796042    0.19423027    0.26164725    0.28625479    0.24969845]
 [ 411.57575775    0.36737058    0.41334119    0.32455292    0.22604707]
 [ 437.73003861    0.25464448    0.36376533    0.28493524    0.2353798 ]
 ...
 [ 320.35426221    0.19897316    0.32351843    0.26844719    0.16278139]
 [-199.25375864    0.26505625    0.44053388    0.29936504    0.27694064]
 [ 562.7169651     0.4021        0.33829999    0.36339998    0.1503    ]][0m
[37m[1m[2023-07-10 11:07:36,069][227910] Max Reward on eval: 3058.6961814821234[0m
[37m[1m[2023-07-10 11:07:36,070][227910] Min Reward on eval: -925.222092601459[0m
[37m[1m[2023-07-10 11:07:36,070][227910] Mean Reward across all agents: 415.05423241982743[0m
[37m[1m[2023-07-10 11:07:36,070][227910] Average Trajectory Length: 861.427[0m
[36m[2023-07-10 11:07:36,072][227910] mean_value=-1272.6773991350833, max_value=1136.8606656531454[0m
[37m[1m[2023-07-10 11:07:36,075][227910] New mean coefficients: [[ 5.5781555  -1.5183045  -1.4361749   0.49044836  0.46697393]][0m
[37m[1m[2023-07-10 11:07:36,076][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:07:45,935][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 11:07:45,935][227910] FPS: 389560.52[0m
[36m[2023-07-10 11:07:45,937][227910] itr=228, itrs=2000, Progress: 11.40%[0m
[36m[2023-07-10 11:07:57,645][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 11:07:57,645][227910] FPS: 328394.51[0m
[36m[2023-07-10 11:08:02,411][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:08:02,412][227910] Reward + Measures: [[4537.2462408     0.23115674    0.31873819    0.29826051    0.06487287]][0m
[37m[1m[2023-07-10 11:08:02,412][227910] Max Reward on eval: 4537.246240802102[0m
[37m[1m[2023-07-10 11:08:02,412][227910] Min Reward on eval: 4537.246240802102[0m
[37m[1m[2023-07-10 11:08:02,412][227910] Mean Reward across all agents: 4537.246240802102[0m
[37m[1m[2023-07-10 11:08:02,412][227910] Average Trajectory Length: 980.5383333333333[0m
[36m[2023-07-10 11:08:07,828][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:08:07,828][227910] Reward + Measures: [[1037.94946688    0.22083722    0.41599712    0.31071526    0.07155677]
 [ 963.19670656    0.18677048    0.31034842    0.26762682    0.13647626]
 [2148.56632854    0.23640768    0.40104514    0.33274773    0.06023866]
 ...
 [   5.5138982     0.21516907    0.24460152    0.26357052    0.1473736 ]
 [ 694.79032508    0.33201444    0.35657054    0.37233123    0.25856626]
 [ 813.08134903    0.22628739    0.45272198    0.33359739    0.30299178]][0m
[37m[1m[2023-07-10 11:08:07,828][227910] Max Reward on eval: 3848.375640141079[0m
[37m[1m[2023-07-10 11:08:07,829][227910] Min Reward on eval: -1110.5548523859063[0m
[37m[1m[2023-07-10 11:08:07,829][227910] Mean Reward across all agents: 821.9455975564804[0m
[37m[1m[2023-07-10 11:08:07,829][227910] Average Trajectory Length: 834.5659999999999[0m
[36m[2023-07-10 11:08:07,832][227910] mean_value=-1168.1286930655906, max_value=1179.1911209445327[0m
[37m[1m[2023-07-10 11:08:07,835][227910] New mean coefficients: [[ 4.873922   -1.6574284  -1.0864632   0.6356131   0.45864564]][0m
[37m[1m[2023-07-10 11:08:07,836][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:08:17,492][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 11:08:17,492][227910] FPS: 397752.17[0m
[36m[2023-07-10 11:08:17,494][227910] itr=229, itrs=2000, Progress: 11.45%[0m
[36m[2023-07-10 11:08:29,030][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 11:08:29,030][227910] FPS: 333323.06[0m
[36m[2023-07-10 11:08:33,667][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:08:33,667][227910] Reward + Measures: [[4651.61962438    0.23345748    0.30738688    0.28876206    0.06321451]][0m
[37m[1m[2023-07-10 11:08:33,668][227910] Max Reward on eval: 4651.619624382032[0m
[37m[1m[2023-07-10 11:08:33,668][227910] Min Reward on eval: 4651.619624382032[0m
[37m[1m[2023-07-10 11:08:33,668][227910] Mean Reward across all agents: 4651.619624382032[0m
[37m[1m[2023-07-10 11:08:33,669][227910] Average Trajectory Length: 982.755[0m
[36m[2023-07-10 11:08:39,149][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:08:39,150][227910] Reward + Measures: [[ 678.26018799    0.1566        0.4508        0.23959999    0.3493    ]
 [-672.6806118     0.31274685    0.26769811    0.28565788    0.37484583]
 [ 987.47681741    0.36740002    0.3802        0.37400001    0.34440002]
 ...
 [   8.59308354    0.19577114    0.28527316    0.18043312    0.28129146]
 [1579.22909404    0.23509999    0.34059998    0.27299997    0.21820001]
 [ -98.2081789     0.19113238    0.34300062    0.23128748    0.20513503]][0m
[37m[1m[2023-07-10 11:08:39,150][227910] Max Reward on eval: 4200.369948471338[0m
[37m[1m[2023-07-10 11:08:39,150][227910] Min Reward on eval: -1347.5089031004113[0m
[37m[1m[2023-07-10 11:08:39,150][227910] Mean Reward across all agents: 563.0016420269104[0m
[37m[1m[2023-07-10 11:08:39,151][227910] Average Trajectory Length: 819.1596666666667[0m
[36m[2023-07-10 11:08:39,156][227910] mean_value=-432.98289477623854, max_value=1795.1742060599781[0m
[37m[1m[2023-07-10 11:08:39,159][227910] New mean coefficients: [[ 4.4000916 -1.2349792 -0.4324559  1.1394657  1.3483162]][0m
[37m[1m[2023-07-10 11:08:39,160][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:08:48,991][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 11:08:48,991][227910] FPS: 390672.94[0m
[36m[2023-07-10 11:08:48,993][227910] itr=230, itrs=2000, Progress: 11.50%[0m
[37m[1m[2023-07-10 11:08:51,193][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000210[0m
[36m[2023-07-10 11:09:03,109][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 11:09:03,109][227910] FPS: 329250.15[0m
[36m[2023-07-10 11:09:07,888][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:09:07,888][227910] Reward + Measures: [[1863.1422933     0.28043926    0.49830428    0.29654044    0.16025153]][0m
[37m[1m[2023-07-10 11:09:07,888][227910] Max Reward on eval: 1863.1422933039194[0m
[37m[1m[2023-07-10 11:09:07,889][227910] Min Reward on eval: 1863.1422933039194[0m
[37m[1m[2023-07-10 11:09:07,889][227910] Mean Reward across all agents: 1863.1422933039194[0m
[37m[1m[2023-07-10 11:09:07,889][227910] Average Trajectory Length: 977.4166666666666[0m
[36m[2023-07-10 11:09:13,375][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:09:13,376][227910] Reward + Measures: [[1693.22848701    0.27940002    0.49620005    0.33610001    0.18990003]
 [1360.44018786    0.21276768    0.43399772    0.29256764    0.18563384]
 [1505.93662227    0.2369        0.46619996    0.42630002    0.2753    ]
 ...
 [2111.89653455    0.2737        0.46360001    0.35280001    0.1646    ]
 [1515.93534357    0.25219998    0.38280001    0.39030001    0.23720002]
 [1824.05461802    0.2538        0.44580004    0.35630003    0.1918    ]][0m
[37m[1m[2023-07-10 11:09:13,376][227910] Max Reward on eval: 2174.938923498895[0m
[37m[1m[2023-07-10 11:09:13,376][227910] Min Reward on eval: 856.754825410829[0m
[37m[1m[2023-07-10 11:09:13,377][227910] Mean Reward across all agents: 1662.821475723393[0m
[37m[1m[2023-07-10 11:09:13,377][227910] Average Trajectory Length: 954.0326666666666[0m
[36m[2023-07-10 11:09:13,379][227910] mean_value=-895.6354038027802, max_value=2249.240158627718[0m
[37m[1m[2023-07-10 11:09:13,381][227910] New mean coefficients: [[ 3.8993354  -0.7280733  -0.23721164  1.0567864   1.3114523 ]][0m
[37m[1m[2023-07-10 11:09:13,382][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:09:23,126][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 11:09:23,126][227910] FPS: 394162.26[0m
[36m[2023-07-10 11:09:23,128][227910] itr=231, itrs=2000, Progress: 11.55%[0m
[36m[2023-07-10 11:09:34,973][227910] train() took 11.83 seconds to complete[0m
[36m[2023-07-10 11:09:34,973][227910] FPS: 324631.44[0m
[36m[2023-07-10 11:09:39,837][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:09:39,838][227910] Reward + Measures: [[2148.76793484    0.27468506    0.47494009    0.30580559    0.1609432 ]][0m
[37m[1m[2023-07-10 11:09:39,838][227910] Max Reward on eval: 2148.7679348438164[0m
[37m[1m[2023-07-10 11:09:39,838][227910] Min Reward on eval: 2148.7679348438164[0m
[37m[1m[2023-07-10 11:09:39,838][227910] Mean Reward across all agents: 2148.7679348438164[0m
[37m[1m[2023-07-10 11:09:39,838][227910] Average Trajectory Length: 986.068[0m
[36m[2023-07-10 11:09:45,163][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:09:45,164][227910] Reward + Measures: [[1644.23429055    0.22979724    0.38460502    0.36255029    0.18724257]
 [ 570.51626289    0.35880002    0.43459997    0.29840001    0.289     ]
 [ 269.39398083    0.27726802    0.31165987    0.30608398    0.1483078 ]
 ...
 [ 937.39884192    0.30560893    0.39328334    0.30568409    0.24285622]
 [2057.52039434    0.30115086    0.42285776    0.31046176    0.20522514]
 [ -92.82675642    0.24439064    0.29175282    0.24478498    0.13229108]][0m
[37m[1m[2023-07-10 11:09:45,164][227910] Max Reward on eval: 2340.6602831347845[0m
[37m[1m[2023-07-10 11:09:45,164][227910] Min Reward on eval: -869.2748264337657[0m
[37m[1m[2023-07-10 11:09:45,165][227910] Mean Reward across all agents: 1107.0602541515311[0m
[37m[1m[2023-07-10 11:09:45,165][227910] Average Trajectory Length: 873.1476666666666[0m
[36m[2023-07-10 11:09:45,167][227910] mean_value=-1545.6707745517754, max_value=1912.0672199697647[0m
[37m[1m[2023-07-10 11:09:45,170][227910] New mean coefficients: [[ 5.1942253  -0.9492997   0.3890683  -0.5907078   0.19890916]][0m
[37m[1m[2023-07-10 11:09:45,170][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:09:54,956][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 11:09:54,956][227910] FPS: 392498.89[0m
[36m[2023-07-10 11:09:54,958][227910] itr=232, itrs=2000, Progress: 11.60%[0m
[36m[2023-07-10 11:10:06,620][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 11:10:06,621][227910] FPS: 329737.80[0m
[36m[2023-07-10 11:10:11,449][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:10:11,449][227910] Reward + Measures: [[2348.73599486    0.27886409    0.45222566    0.30997041    0.1644412 ]][0m
[37m[1m[2023-07-10 11:10:11,450][227910] Max Reward on eval: 2348.735994857389[0m
[37m[1m[2023-07-10 11:10:11,450][227910] Min Reward on eval: 2348.735994857389[0m
[37m[1m[2023-07-10 11:10:11,450][227910] Mean Reward across all agents: 2348.735994857389[0m
[37m[1m[2023-07-10 11:10:11,451][227910] Average Trajectory Length: 988.3723333333332[0m
[36m[2023-07-10 11:10:17,069][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:10:17,069][227910] Reward + Measures: [[1619.06647356    0.2036        0.44250003    0.27790001    0.19960001]
 [1733.88255609    0.30450001    0.35660002    0.35180002    0.23900001]
 [1780.40938953    0.24453752    0.40254822    0.29481107    0.21417592]
 ...
 [1848.35376221    0.2994        0.42460003    0.32600001    0.22290002]
 [1454.44626954    0.33290002    0.44870001    0.31310001    0.23980002]
 [1573.81179509    0.3644        0.46430001    0.2807        0.19209999]][0m
[37m[1m[2023-07-10 11:10:17,070][227910] Max Reward on eval: 2331.6052120430627[0m
[37m[1m[2023-07-10 11:10:17,070][227910] Min Reward on eval: -36.818122440285514[0m
[37m[1m[2023-07-10 11:10:17,070][227910] Mean Reward across all agents: 1601.835253264894[0m
[37m[1m[2023-07-10 11:10:17,070][227910] Average Trajectory Length: 962.8096666666667[0m
[36m[2023-07-10 11:10:17,073][227910] mean_value=-232.3536388572079, max_value=2285.873245890974[0m
[37m[1m[2023-07-10 11:10:17,076][227910] New mean coefficients: [[ 4.2286143  -0.64710855  0.7632523  -0.51848876  0.21044253]][0m
[37m[1m[2023-07-10 11:10:17,077][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:10:26,783][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 11:10:26,784][227910] FPS: 395684.58[0m
[36m[2023-07-10 11:10:26,786][227910] itr=233, itrs=2000, Progress: 11.65%[0m
[36m[2023-07-10 11:10:38,280][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 11:10:38,280][227910] FPS: 334501.79[0m
[36m[2023-07-10 11:10:43,067][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:10:43,067][227910] Reward + Measures: [[2515.29283112    0.28034753    0.45255038    0.3016091     0.15883943]][0m
[37m[1m[2023-07-10 11:10:43,067][227910] Max Reward on eval: 2515.292831120819[0m
[37m[1m[2023-07-10 11:10:43,068][227910] Min Reward on eval: 2515.292831120819[0m
[37m[1m[2023-07-10 11:10:43,068][227910] Mean Reward across all agents: 2515.292831120819[0m
[37m[1m[2023-07-10 11:10:43,068][227910] Average Trajectory Length: 990.846[0m
[36m[2023-07-10 11:10:48,662][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:10:48,662][227910] Reward + Measures: [[1423.2352826     0.31805661    0.38869539    0.24697068    0.18468828]
 [2111.17597411    0.2665        0.36320001    0.2703        0.19070001]
 [-369.5667814     0.35813054    0.54558873    0.10488754    0.46875167]
 ...
 [-133.24542552    0.34462973    0.37673512    0.31782705    0.39161533]
 [2234.29446634    0.24920002    0.41409999    0.3915        0.176     ]
 [-276.15155929    0.21256445    0.23630913    0.18950187    0.25727382]][0m
[37m[1m[2023-07-10 11:10:48,662][227910] Max Reward on eval: 2697.631985256914[0m
[37m[1m[2023-07-10 11:10:48,663][227910] Min Reward on eval: -582.0378133997845[0m
[37m[1m[2023-07-10 11:10:48,663][227910] Mean Reward across all agents: 1104.5867555639413[0m
[37m[1m[2023-07-10 11:10:48,663][227910] Average Trajectory Length: 943.2486666666666[0m
[36m[2023-07-10 11:10:48,666][227910] mean_value=-630.6140400978144, max_value=1049.8286187198478[0m
[37m[1m[2023-07-10 11:10:48,668][227910] New mean coefficients: [[ 4.9432864  -0.7707935   0.01009488  0.4952718   1.0554978 ]][0m
[37m[1m[2023-07-10 11:10:48,669][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:10:58,389][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 11:10:58,389][227910] FPS: 395136.49[0m
[36m[2023-07-10 11:10:58,391][227910] itr=234, itrs=2000, Progress: 11.70%[0m
[36m[2023-07-10 11:11:09,903][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 11:11:09,903][227910] FPS: 333992.13[0m
[36m[2023-07-10 11:11:14,655][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:11:14,655][227910] Reward + Measures: [[3548.81342708    0.2473208     0.33120951    0.31316918    0.11534222]][0m
[37m[1m[2023-07-10 11:11:14,656][227910] Max Reward on eval: 3548.813427084177[0m
[37m[1m[2023-07-10 11:11:14,656][227910] Min Reward on eval: 3548.813427084177[0m
[37m[1m[2023-07-10 11:11:14,656][227910] Mean Reward across all agents: 3548.813427084177[0m
[37m[1m[2023-07-10 11:11:14,657][227910] Average Trajectory Length: 978.1843333333333[0m
[36m[2023-07-10 11:11:20,178][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:11:20,179][227910] Reward + Measures: [[ 461.35872306    0.30719778    0.34924942    0.25203684    0.18066154]
 [ 106.65212011    0.28238955    0.33583263    0.25701883    0.17711303]
 [1759.51056973    0.28040001    0.44169998    0.41879997    0.1245    ]
 ...
 [2341.45629542    0.2323        0.29980001    0.28119999    0.12560001]
 [1893.76495046    0.2318        0.35450003    0.2764        0.1211    ]
 [ 827.44921955    0.29485318    0.30287439    0.26470688    0.16216205]][0m
[37m[1m[2023-07-10 11:11:20,179][227910] Max Reward on eval: 3963.177936429158[0m
[37m[1m[2023-07-10 11:11:20,179][227910] Min Reward on eval: -1026.90247593245[0m
[37m[1m[2023-07-10 11:11:20,179][227910] Mean Reward across all agents: 1388.5990099640671[0m
[37m[1m[2023-07-10 11:11:20,180][227910] Average Trajectory Length: 908.7913333333333[0m
[36m[2023-07-10 11:11:20,182][227910] mean_value=-1240.069296031755, max_value=1439.728585336608[0m
[37m[1m[2023-07-10 11:11:20,185][227910] New mean coefficients: [[ 5.648183   -0.74108106  0.6740303  -0.26709718 -0.20606601]][0m
[37m[1m[2023-07-10 11:11:20,186][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:11:29,951][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 11:11:29,951][227910] FPS: 393320.03[0m
[36m[2023-07-10 11:11:29,953][227910] itr=235, itrs=2000, Progress: 11.75%[0m
[36m[2023-07-10 11:11:41,539][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 11:11:41,539][227910] FPS: 331899.52[0m
[36m[2023-07-10 11:11:46,396][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:11:46,397][227910] Reward + Measures: [[3830.14020987    0.25202411    0.31091452    0.28277814    0.10688487]][0m
[37m[1m[2023-07-10 11:11:46,397][227910] Max Reward on eval: 3830.140209866898[0m
[37m[1m[2023-07-10 11:11:46,397][227910] Min Reward on eval: 3830.140209866898[0m
[37m[1m[2023-07-10 11:11:46,397][227910] Mean Reward across all agents: 3830.140209866898[0m
[37m[1m[2023-07-10 11:11:46,398][227910] Average Trajectory Length: 975.5269999999999[0m
[36m[2023-07-10 11:11:51,920][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:11:51,921][227910] Reward + Measures: [[2780.5372809     0.24124919    0.31640932    0.33802825    0.14281462]
 [2365.24783848    0.23685761    0.27711582    0.26800266    0.11656332]
 [1270.19050361    0.20651388    0.27534091    0.20760059    0.09395761]
 ...
 [2423.33964581    0.23268962    0.3743687     0.39744091    0.12057431]
 [1368.0347824     0.26570779    0.29068828    0.25907001    0.12071649]
 [1104.78319416    0.23321132    0.45283005    0.47821322    0.20798521]][0m
[37m[1m[2023-07-10 11:11:51,921][227910] Max Reward on eval: 4078.2343519458664[0m
[37m[1m[2023-07-10 11:11:51,921][227910] Min Reward on eval: -839.301657223911[0m
[37m[1m[2023-07-10 11:11:51,922][227910] Mean Reward across all agents: 1546.6225998306663[0m
[37m[1m[2023-07-10 11:11:51,922][227910] Average Trajectory Length: 869.3589999999999[0m
[36m[2023-07-10 11:11:51,924][227910] mean_value=-1158.5303017626734, max_value=1538.845803141061[0m
[37m[1m[2023-07-10 11:11:51,927][227910] New mean coefficients: [[ 5.0917306  -0.5535513   1.7913774  -1.1965604  -0.62048966]][0m
[37m[1m[2023-07-10 11:11:51,928][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:12:01,720][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 11:12:01,720][227910] FPS: 392215.59[0m
[36m[2023-07-10 11:12:01,722][227910] itr=236, itrs=2000, Progress: 11.80%[0m
[36m[2023-07-10 11:12:13,278][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 11:12:13,278][227910] FPS: 332791.54[0m
[36m[2023-07-10 11:12:18,155][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:12:18,155][227910] Reward + Measures: [[4099.90219213    0.25749093    0.29925862    0.2632882     0.09418396]][0m
[37m[1m[2023-07-10 11:12:18,155][227910] Max Reward on eval: 4099.9021921315425[0m
[37m[1m[2023-07-10 11:12:18,156][227910] Min Reward on eval: 4099.9021921315425[0m
[37m[1m[2023-07-10 11:12:18,156][227910] Mean Reward across all agents: 4099.9021921315425[0m
[37m[1m[2023-07-10 11:12:18,156][227910] Average Trajectory Length: 974.8639999999999[0m
[36m[2023-07-10 11:12:23,657][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:12:23,657][227910] Reward + Measures: [[ 657.14780761    0.24010323    0.29710659    0.32077926    0.18749872]
 [1327.90270857    0.21632472    0.28712031    0.26279515    0.15131746]
 [ 306.19241282    0.20921312    0.24569507    0.23790042    0.15668671]
 ...
 [1322.95721722    0.27210003    0.45679998    0.2633        0.206     ]
 [ 702.12611386    0.20544349    0.29686522    0.29682755    0.18370871]
 [1459.02494874    0.24997531    0.49134228    0.27186885    0.21044309]][0m
[37m[1m[2023-07-10 11:12:23,658][227910] Max Reward on eval: 4209.9446582574865[0m
[37m[1m[2023-07-10 11:12:23,658][227910] Min Reward on eval: -930.6955827183614[0m
[37m[1m[2023-07-10 11:12:23,658][227910] Mean Reward across all agents: 1833.2202262046371[0m
[37m[1m[2023-07-10 11:12:23,658][227910] Average Trajectory Length: 833.8983333333333[0m
[36m[2023-07-10 11:12:23,661][227910] mean_value=-1096.7530596682363, max_value=1405.2481251304225[0m
[37m[1m[2023-07-10 11:12:23,663][227910] New mean coefficients: [[ 4.5364175  -0.15655744  1.7981436  -0.7001755   0.24568897]][0m
[37m[1m[2023-07-10 11:12:23,664][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:12:33,400][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 11:12:33,405][227910] FPS: 394503.41[0m
[36m[2023-07-10 11:12:33,408][227910] itr=237, itrs=2000, Progress: 11.85%[0m
[36m[2023-07-10 11:12:44,984][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 11:12:44,984][227910] FPS: 332130.36[0m
[36m[2023-07-10 11:12:49,678][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:12:49,678][227910] Reward + Measures: [[4316.60755128    0.2602711     0.29371399    0.25718266    0.08667243]][0m
[37m[1m[2023-07-10 11:12:49,679][227910] Max Reward on eval: 4316.607551281372[0m
[37m[1m[2023-07-10 11:12:49,679][227910] Min Reward on eval: 4316.607551281372[0m
[37m[1m[2023-07-10 11:12:49,679][227910] Mean Reward across all agents: 4316.607551281372[0m
[37m[1m[2023-07-10 11:12:49,679][227910] Average Trajectory Length: 985.5953333333333[0m
[36m[2023-07-10 11:12:55,368][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:12:55,368][227910] Reward + Measures: [[4259.95873128    0.26190004    0.31380001    0.2606        0.0962    ]
 [3635.43085235    0.26656488    0.32499176    0.28761539    0.10393585]
 [4168.25849599    0.25120002    0.29429999    0.24429999    0.08459999]
 ...
 [4227.90245818    0.2554        0.33000001    0.2685        0.0944    ]
 [3503.10428036    0.2199        0.28350002    0.23029999    0.08760001]
 [4040.21377943    0.26140985    0.33163771    0.26293772    0.09368361]][0m
[37m[1m[2023-07-10 11:12:55,369][227910] Max Reward on eval: 4324.293248263746[0m
[37m[1m[2023-07-10 11:12:55,369][227910] Min Reward on eval: 1701.975523004608[0m
[37m[1m[2023-07-10 11:12:55,369][227910] Mean Reward across all agents: 3682.438844625298[0m
[37m[1m[2023-07-10 11:12:55,369][227910] Average Trajectory Length: 964.1283333333333[0m
[36m[2023-07-10 11:12:55,373][227910] mean_value=-45.39719782079737, max_value=1376.4836581099455[0m
[37m[1m[2023-07-10 11:12:55,375][227910] New mean coefficients: [[ 3.8963454  -0.12336582  1.3251265  -0.49221253  0.79939055]][0m
[37m[1m[2023-07-10 11:12:55,376][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:13:05,117][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 11:13:05,118][227910] FPS: 394283.46[0m
[36m[2023-07-10 11:13:05,120][227910] itr=238, itrs=2000, Progress: 11.90%[0m
[36m[2023-07-10 11:13:16,643][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 11:13:16,643][227910] FPS: 333656.39[0m
[36m[2023-07-10 11:13:21,400][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:13:21,400][227910] Reward + Measures: [[4454.92302773    0.26011565    0.28170878    0.24764864    0.07333433]][0m
[37m[1m[2023-07-10 11:13:21,400][227910] Max Reward on eval: 4454.923027725369[0m
[37m[1m[2023-07-10 11:13:21,400][227910] Min Reward on eval: 4454.923027725369[0m
[37m[1m[2023-07-10 11:13:21,400][227910] Mean Reward across all agents: 4454.923027725369[0m
[37m[1m[2023-07-10 11:13:21,401][227910] Average Trajectory Length: 978.4073333333333[0m
[36m[2023-07-10 11:13:26,854][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:13:26,855][227910] Reward + Measures: [[ 372.3275288     0.20451932    0.39726049    0.29821166    0.29183039]
 [1954.44348255    0.24969999    0.33270001    0.39070001    0.1411    ]
 [ 264.63216211    0.2429        0.43150002    0.36960003    0.20630001]
 ...
 [ 425.21001562    0.19226755    0.4217833     0.27264813    0.27784202]
 [1243.88943647    0.25936726    0.3358064     0.39654556    0.17907968]
 [-659.41742048    0.19404212    0.42836627    0.29977915    0.3722865 ]][0m
[37m[1m[2023-07-10 11:13:26,855][227910] Max Reward on eval: 4229.509178451309[0m
[37m[1m[2023-07-10 11:13:26,855][227910] Min Reward on eval: -1800.6184530487633[0m
[37m[1m[2023-07-10 11:13:26,855][227910] Mean Reward across all agents: 677.0471105943126[0m
[37m[1m[2023-07-10 11:13:26,856][227910] Average Trajectory Length: 837.6443333333333[0m
[36m[2023-07-10 11:13:26,858][227910] mean_value=-1088.4192010815352, max_value=1321.143645718947[0m
[37m[1m[2023-07-10 11:13:26,861][227910] New mean coefficients: [[ 3.7826157   0.641691    1.2564342  -0.34226498  1.0853558 ]][0m
[37m[1m[2023-07-10 11:13:26,862][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:13:36,562][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 11:13:36,562][227910] FPS: 395949.38[0m
[36m[2023-07-10 11:13:36,564][227910] itr=239, itrs=2000, Progress: 11.95%[0m
[36m[2023-07-10 11:13:48,149][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 11:13:48,149][227910] FPS: 331876.18[0m
[36m[2023-07-10 11:13:52,810][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:13:52,810][227910] Reward + Measures: [[4614.23714894    0.26031721    0.28094018    0.24317527    0.06513018]][0m
[37m[1m[2023-07-10 11:13:52,811][227910] Max Reward on eval: 4614.23714894231[0m
[37m[1m[2023-07-10 11:13:52,811][227910] Min Reward on eval: 4614.23714894231[0m
[37m[1m[2023-07-10 11:13:52,811][227910] Mean Reward across all agents: 4614.23714894231[0m
[37m[1m[2023-07-10 11:13:52,811][227910] Average Trajectory Length: 982.5533333333333[0m
[36m[2023-07-10 11:13:58,169][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:13:58,170][227910] Reward + Measures: [[1400.13905709    0.20875959    0.31655774    0.26349553    0.10950314]
 [1382.19548881    0.26800001    0.31549999    0.34520003    0.26640001]
 [ 557.32952487    0.26840052    0.39670175    0.33478758    0.10899749]
 ...
 [1608.61961751    0.20396999    0.34284592    0.31590915    0.09494632]
 [3667.25331954    0.2523739     0.25594783    0.23993914    0.10874783]
 [3794.3972702     0.27309591    0.29432604    0.27301231    0.14311369]][0m
[37m[1m[2023-07-10 11:13:58,170][227910] Max Reward on eval: 4511.6884482116675[0m
[37m[1m[2023-07-10 11:13:58,170][227910] Min Reward on eval: -25.995403246354545[0m
[37m[1m[2023-07-10 11:13:58,171][227910] Mean Reward across all agents: 2001.0143618107572[0m
[37m[1m[2023-07-10 11:13:58,171][227910] Average Trajectory Length: 868.0386666666666[0m
[36m[2023-07-10 11:13:58,173][227910] mean_value=-1646.7793146573501, max_value=1178.4620820153777[0m
[37m[1m[2023-07-10 11:13:58,175][227910] New mean coefficients: [[ 2.8978758   0.01753652 -0.4753077   0.34640983  1.7702769 ]][0m
[37m[1m[2023-07-10 11:13:58,176][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:14:07,787][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 11:14:07,787][227910] FPS: 399610.24[0m
[36m[2023-07-10 11:14:07,789][227910] itr=240, itrs=2000, Progress: 12.00%[0m
[37m[1m[2023-07-10 11:14:09,954][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000220[0m
[36m[2023-07-10 11:14:21,817][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 11:14:21,817][227910] FPS: 330965.36[0m
[36m[2023-07-10 11:14:26,635][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:14:26,635][227910] Reward + Measures: [[4747.94690913    0.2607013     0.2739687     0.23796722    0.05796739]][0m
[37m[1m[2023-07-10 11:14:26,635][227910] Max Reward on eval: 4747.946909133647[0m
[37m[1m[2023-07-10 11:14:26,635][227910] Min Reward on eval: 4747.946909133647[0m
[37m[1m[2023-07-10 11:14:26,636][227910] Mean Reward across all agents: 4747.946909133647[0m
[37m[1m[2023-07-10 11:14:26,636][227910] Average Trajectory Length: 984.5186666666666[0m
[36m[2023-07-10 11:14:32,158][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:14:32,159][227910] Reward + Measures: [[1120.01380845    0.22383539    0.46088812    0.27324292    0.15052992]
 [ 643.67187141    0.28404638    0.40798268    0.38967362    0.17674771]
 [3098.73013583    0.30320001    0.31100002    0.2481        0.101     ]
 ...
 [ 639.31073217    0.22533397    0.42791691    0.28206012    0.21943274]
 [1253.60343646    0.23829889    0.45374718    0.26592699    0.35319778]
 [3384.78457322    0.26083651    0.27582875    0.26920202    0.10886228]][0m
[37m[1m[2023-07-10 11:14:32,159][227910] Max Reward on eval: 4596.855389009067[0m
[37m[1m[2023-07-10 11:14:32,159][227910] Min Reward on eval: -786.9533902577357[0m
[37m[1m[2023-07-10 11:14:32,159][227910] Mean Reward across all agents: 1434.9748175668383[0m
[37m[1m[2023-07-10 11:14:32,160][227910] Average Trajectory Length: 896.4473333333333[0m
[36m[2023-07-10 11:14:32,162][227910] mean_value=-1066.2814980579703, max_value=2848.6804457370067[0m
[37m[1m[2023-07-10 11:14:32,165][227910] New mean coefficients: [[2.3714104  0.82651    0.8262142  0.03210181 1.6489731 ]][0m
[37m[1m[2023-07-10 11:14:32,166][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:14:41,971][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 11:14:41,971][227910] FPS: 391714.53[0m
[36m[2023-07-10 11:14:41,973][227910] itr=241, itrs=2000, Progress: 12.05%[0m
[36m[2023-07-10 11:14:53,673][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 11:14:53,673][227910] FPS: 328630.00[0m
[36m[2023-07-10 11:14:58,457][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:14:58,457][227910] Reward + Measures: [[4875.56839214    0.25428915    0.26958936    0.2317041     0.04567962]][0m
[37m[1m[2023-07-10 11:14:58,457][227910] Max Reward on eval: 4875.568392138299[0m
[37m[1m[2023-07-10 11:14:58,458][227910] Min Reward on eval: 4875.568392138299[0m
[37m[1m[2023-07-10 11:14:58,458][227910] Mean Reward across all agents: 4875.568392138299[0m
[37m[1m[2023-07-10 11:14:58,458][227910] Average Trajectory Length: 983.7776666666666[0m
[36m[2023-07-10 11:15:04,094][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:15:04,095][227910] Reward + Measures: [[1685.78156495    0.22098282    0.29569304    0.25768596    0.1567999 ]
 [1199.42374178    0.2046839     0.43213272    0.29457757    0.24210384]
 [ 179.50104103    0.39070001    0.36530003    0.32939997    0.22570002]
 ...
 [  76.85927289    0.22029999    0.4585        0.30320001    0.1382    ]
 [ 189.40728768    0.20691739    0.39269134    0.30041304    0.14333044]
 [-139.60483488    0.18909697    0.41003639    0.2445        0.3458879 ]][0m
[37m[1m[2023-07-10 11:15:04,095][227910] Max Reward on eval: 4435.801268796204[0m
[37m[1m[2023-07-10 11:15:04,095][227910] Min Reward on eval: -879.4672638276592[0m
[37m[1m[2023-07-10 11:15:04,095][227910] Mean Reward across all agents: 573.3010268998664[0m
[37m[1m[2023-07-10 11:15:04,096][227910] Average Trajectory Length: 855.1053333333333[0m
[36m[2023-07-10 11:15:04,098][227910] mean_value=-1611.6136833317303, max_value=1400.9659229118727[0m
[37m[1m[2023-07-10 11:15:04,100][227910] New mean coefficients: [[ 1.8408599   0.35833946 -0.07847875  0.91737044  2.0715723 ]][0m
[37m[1m[2023-07-10 11:15:04,101][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:15:13,864][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 11:15:13,865][227910] FPS: 393367.54[0m
[36m[2023-07-10 11:15:13,867][227910] itr=242, itrs=2000, Progress: 12.10%[0m
[36m[2023-07-10 11:15:25,599][227910] train() took 11.72 seconds to complete[0m
[36m[2023-07-10 11:15:25,599][227910] FPS: 327715.95[0m
[36m[2023-07-10 11:15:30,268][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:15:30,268][227910] Reward + Measures: [[5031.2273562     0.25164273    0.27476937    0.23141654    0.04584084]][0m
[37m[1m[2023-07-10 11:15:30,268][227910] Max Reward on eval: 5031.2273561991215[0m
[37m[1m[2023-07-10 11:15:30,269][227910] Min Reward on eval: 5031.2273561991215[0m
[37m[1m[2023-07-10 11:15:30,269][227910] Mean Reward across all agents: 5031.2273561991215[0m
[37m[1m[2023-07-10 11:15:30,269][227910] Average Trajectory Length: 989.028[0m
[36m[2023-07-10 11:15:35,709][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:15:35,709][227910] Reward + Measures: [[ -18.33389808    0.12793043    0.16951835    0.15787546    0.12564795]
 [1499.83496493    0.19465677    0.25171849    0.2050219     0.11651428]
 [ 993.40747308    0.31640002    0.42320004    0.44510004    0.1463    ]
 ...
 [ 434.15909796    0.17001908    0.27794591    0.20119829    0.10931959]
 [1495.12288076    0.24300002    0.30580002    0.3292        0.14049999]
 [ 905.23824245    0.21635695    0.30015153    0.27392638    0.20504795]][0m
[37m[1m[2023-07-10 11:15:35,710][227910] Max Reward on eval: 4546.986059414781[0m
[37m[1m[2023-07-10 11:15:35,710][227910] Min Reward on eval: -1335.7597123907065[0m
[37m[1m[2023-07-10 11:15:35,710][227910] Mean Reward across all agents: 655.0427981049636[0m
[37m[1m[2023-07-10 11:15:35,710][227910] Average Trajectory Length: 707.4293333333333[0m
[36m[2023-07-10 11:15:35,713][227910] mean_value=-1228.634410619548, max_value=373.61473452545744[0m
[37m[1m[2023-07-10 11:15:35,715][227910] New mean coefficients: [[ 1.8810135   0.59105504 -0.48008347  1.1254057   2.2097743 ]][0m
[37m[1m[2023-07-10 11:15:35,716][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:15:45,361][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 11:15:45,362][227910] FPS: 398181.73[0m
[36m[2023-07-10 11:15:45,364][227910] itr=243, itrs=2000, Progress: 12.15%[0m
[36m[2023-07-10 11:15:56,914][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 11:15:56,914][227910] FPS: 332943.74[0m
[36m[2023-07-10 11:16:01,693][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:16:01,693][227910] Reward + Measures: [[5113.33632015    0.25202349    0.2750555     0.23280951    0.04781054]][0m
[37m[1m[2023-07-10 11:16:01,693][227910] Max Reward on eval: 5113.336320145532[0m
[37m[1m[2023-07-10 11:16:01,693][227910] Min Reward on eval: 5113.336320145532[0m
[37m[1m[2023-07-10 11:16:01,694][227910] Mean Reward across all agents: 5113.336320145532[0m
[37m[1m[2023-07-10 11:16:01,694][227910] Average Trajectory Length: 987.6513333333334[0m
[36m[2023-07-10 11:16:07,182][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:16:07,183][227910] Reward + Measures: [[2887.31743697    0.2441        0.26730001    0.26000002    0.12520002]
 [3391.58843461    0.23440614    0.27813533    0.26638833    0.13207421]
 [3256.76472404    0.22774458    0.32169458    0.24606387    0.06943878]
 ...
 [2224.01554673    0.24370573    0.32548481    0.30246851    0.139245  ]
 [2084.69605948    0.22873683    0.30932632    0.26914737    0.14840527]
 [ 676.78004096    0.24490002    0.26640001    0.19509999    0.13380001]][0m
[37m[1m[2023-07-10 11:16:07,183][227910] Max Reward on eval: 4882.761142247007[0m
[37m[1m[2023-07-10 11:16:07,183][227910] Min Reward on eval: -701.3822167781473[0m
[37m[1m[2023-07-10 11:16:07,183][227910] Mean Reward across all agents: 2162.862782711798[0m
[37m[1m[2023-07-10 11:16:07,184][227910] Average Trajectory Length: 931.6796666666667[0m
[36m[2023-07-10 11:16:07,186][227910] mean_value=-1262.3142208863885, max_value=1649.3725939991066[0m
[37m[1m[2023-07-10 11:16:07,188][227910] New mean coefficients: [[ 1.5924342   0.71719366 -0.16178241  1.0407676   2.2110443 ]][0m
[37m[1m[2023-07-10 11:16:07,189][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:16:16,985][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 11:16:16,985][227910] FPS: 392067.96[0m
[36m[2023-07-10 11:16:16,987][227910] itr=244, itrs=2000, Progress: 12.20%[0m
[36m[2023-07-10 11:16:28,643][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 11:16:28,643][227910] FPS: 329942.65[0m
[36m[2023-07-10 11:16:33,532][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:16:33,532][227910] Reward + Measures: [[5244.97261962    0.2506164     0.28112003    0.232243      0.04503679]][0m
[37m[1m[2023-07-10 11:16:33,533][227910] Max Reward on eval: 5244.972619618218[0m
[37m[1m[2023-07-10 11:16:33,533][227910] Min Reward on eval: 5244.972619618218[0m
[37m[1m[2023-07-10 11:16:33,533][227910] Mean Reward across all agents: 5244.972619618218[0m
[37m[1m[2023-07-10 11:16:33,533][227910] Average Trajectory Length: 992.4586666666667[0m
[36m[2023-07-10 11:16:39,110][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:16:39,111][227910] Reward + Measures: [[2125.05944669    0.3091        0.34320003    0.32600001    0.1895    ]
 [ -51.28447913    0.31094462    0.31741849    0.26133335    0.23472252]
 [3163.16024794    0.22930001    0.4289        0.29480001    0.16680001]
 ...
 [1439.26367346    0.3461        0.50669998    0.34490001    0.24959998]
 [2045.25477773    0.30850002    0.50510001    0.3626        0.19039999]
 [ 635.14353132    0.37490001    0.4535        0.3202        0.24629998]][0m
[37m[1m[2023-07-10 11:16:39,111][227910] Max Reward on eval: 4942.868248102558[0m
[37m[1m[2023-07-10 11:16:39,111][227910] Min Reward on eval: -402.2322831824073[0m
[37m[1m[2023-07-10 11:16:39,112][227910] Mean Reward across all agents: 1775.60176653573[0m
[37m[1m[2023-07-10 11:16:39,112][227910] Average Trajectory Length: 963.1086666666666[0m
[36m[2023-07-10 11:16:39,116][227910] mean_value=-571.2294237029754, max_value=1909.5637574913167[0m
[37m[1m[2023-07-10 11:16:39,119][227910] New mean coefficients: [[ 2.0454297  -0.07573658 -0.99809265  1.231946    2.3242095 ]][0m
[37m[1m[2023-07-10 11:16:39,120][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:16:49,081][227910] train() took 9.96 seconds to complete[0m
[36m[2023-07-10 11:16:49,082][227910] FPS: 385548.12[0m
[36m[2023-07-10 11:16:49,084][227910] itr=245, itrs=2000, Progress: 12.25%[0m
[36m[2023-07-10 11:17:00,792][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 11:17:00,793][227910] FPS: 328438.63[0m
[36m[2023-07-10 11:17:05,625][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:17:05,625][227910] Reward + Measures: [[5319.792193      0.24961075    0.26423794    0.22851022    0.03923277]][0m
[37m[1m[2023-07-10 11:17:05,626][227910] Max Reward on eval: 5319.79219299853[0m
[37m[1m[2023-07-10 11:17:05,626][227910] Min Reward on eval: 5319.79219299853[0m
[37m[1m[2023-07-10 11:17:05,626][227910] Mean Reward across all agents: 5319.79219299853[0m
[37m[1m[2023-07-10 11:17:05,626][227910] Average Trajectory Length: 988.4963333333333[0m
[36m[2023-07-10 11:17:11,212][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:17:11,212][227910] Reward + Measures: [[2436.83378941    0.22280002    0.50120002    0.27079999    0.1428    ]
 [ -94.31007983    0.31176665    0.3127926     0.37617779    0.1888037 ]
 [ 597.04788769    0.25584149    0.40901995    0.31102741    0.32642919]
 ...
 [1300.94726683    0.25242221    0.4609963     0.31205186    0.26383707]
 [  37.29580905    0.33800003    0.49689999    0.20119999    0.40640002]
 [1560.37345922    0.26009402    0.42582723    0.40634471    0.18606406]][0m
[37m[1m[2023-07-10 11:17:11,213][227910] Max Reward on eval: 5263.569813513755[0m
[37m[1m[2023-07-10 11:17:11,213][227910] Min Reward on eval: -1148.947317756631[0m
[37m[1m[2023-07-10 11:17:11,213][227910] Mean Reward across all agents: 799.2924326592721[0m
[37m[1m[2023-07-10 11:17:11,213][227910] Average Trajectory Length: 894.9876666666667[0m
[36m[2023-07-10 11:17:11,215][227910] mean_value=-1135.2773050970304, max_value=908.678104947252[0m
[37m[1m[2023-07-10 11:17:11,218][227910] New mean coefficients: [[ 1.6850215   0.12224746 -0.5797237   1.3641685   2.4564075 ]][0m
[37m[1m[2023-07-10 11:17:11,219][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:17:21,005][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 11:17:21,005][227910] FPS: 392440.72[0m
[36m[2023-07-10 11:17:21,008][227910] itr=246, itrs=2000, Progress: 12.30%[0m
[36m[2023-07-10 11:17:32,765][227910] train() took 11.74 seconds to complete[0m
[36m[2023-07-10 11:17:32,765][227910] FPS: 327029.25[0m
[36m[2023-07-10 11:17:37,577][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:17:37,577][227910] Reward + Measures: [[5397.9177624     0.24605192    0.26237142    0.2272799     0.03430542]][0m
[37m[1m[2023-07-10 11:17:37,577][227910] Max Reward on eval: 5397.917762401562[0m
[37m[1m[2023-07-10 11:17:37,578][227910] Min Reward on eval: 5397.917762401562[0m
[37m[1m[2023-07-10 11:17:37,578][227910] Mean Reward across all agents: 5397.917762401562[0m
[37m[1m[2023-07-10 11:17:37,578][227910] Average Trajectory Length: 984.5413333333333[0m
[36m[2023-07-10 11:17:43,120][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:17:43,120][227910] Reward + Measures: [[ 493.9010355     0.30481645    0.49800131    0.30205065    0.37145573]
 [-482.82606451    0.17486282    0.34534302    0.27782887    0.31425449]
 [1575.3162064     0.22959705    0.23692904    0.25243837    0.11221421]
 ...
 [-256.16148749    0.2081131     0.28519019    0.25103271    0.23919623]
 [ 333.81713634    0.20025234    0.29303023    0.27467346    0.18789773]
 [ 536.42372329    0.25139999    0.45720002    0.26880002    0.32549998]][0m
[37m[1m[2023-07-10 11:17:43,121][227910] Max Reward on eval: 4587.236835653754[0m
[37m[1m[2023-07-10 11:17:43,121][227910] Min Reward on eval: -554.5079129767371[0m
[37m[1m[2023-07-10 11:17:43,121][227910] Mean Reward across all agents: 651.5885833774093[0m
[37m[1m[2023-07-10 11:17:43,121][227910] Average Trajectory Length: 727.822[0m
[36m[2023-07-10 11:17:43,124][227910] mean_value=-1011.164465830686, max_value=1515.467802480186[0m
[37m[1m[2023-07-10 11:17:43,127][227910] New mean coefficients: [[ 1.458766   -0.08694373 -0.29966113  0.8503882   2.2275524 ]][0m
[37m[1m[2023-07-10 11:17:43,128][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:17:52,829][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 11:17:52,830][227910] FPS: 395874.18[0m
[36m[2023-07-10 11:17:52,832][227910] itr=247, itrs=2000, Progress: 12.35%[0m
[36m[2023-07-10 11:18:04,406][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 11:18:04,407][227910] FPS: 332183.79[0m
[36m[2023-07-10 11:18:09,173][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:18:09,174][227910] Reward + Measures: [[5478.19277096    0.24206917    0.25668558    0.2255978     0.03189451]][0m
[37m[1m[2023-07-10 11:18:09,174][227910] Max Reward on eval: 5478.192770963686[0m
[37m[1m[2023-07-10 11:18:09,174][227910] Min Reward on eval: 5478.192770963686[0m
[37m[1m[2023-07-10 11:18:09,174][227910] Mean Reward across all agents: 5478.192770963686[0m
[37m[1m[2023-07-10 11:18:09,175][227910] Average Trajectory Length: 983.7036666666667[0m
[36m[2023-07-10 11:18:14,649][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:18:14,650][227910] Reward + Measures: [[1189.11355027    0.38530001    0.4923        0.23690002    0.43009996]
 [ 931.19572318    0.34029999    0.38010001    0.24150001    0.34819999]
 [1837.53743281    0.23962584    0.35808784    0.21883217    0.15835963]
 ...
 [ 844.17597194    0.29949999    0.46180001    0.24330001    0.45359999]
 [4349.57938268    0.27059576    0.24412309    0.24704957    0.0852923 ]
 [ 432.60032302    0.33179998    0.67140001    0.27169999    0.71350002]][0m
[37m[1m[2023-07-10 11:18:14,650][227910] Max Reward on eval: 5161.997528687864[0m
[37m[1m[2023-07-10 11:18:14,650][227910] Min Reward on eval: -1192.5158556231763[0m
[37m[1m[2023-07-10 11:18:14,650][227910] Mean Reward across all agents: 955.6332027239242[0m
[37m[1m[2023-07-10 11:18:14,650][227910] Average Trajectory Length: 973.87[0m
[36m[2023-07-10 11:18:14,655][227910] mean_value=-216.30961740424652, max_value=1837.4093957614934[0m
[37m[1m[2023-07-10 11:18:14,658][227910] New mean coefficients: [[1.907828   0.17067783 0.53188634 0.5198181  2.1400151 ]][0m
[37m[1m[2023-07-10 11:18:14,659][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:18:24,403][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 11:18:24,404][227910] FPS: 394134.76[0m
[36m[2023-07-10 11:18:24,406][227910] itr=248, itrs=2000, Progress: 12.40%[0m
[36m[2023-07-10 11:18:36,053][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 11:18:36,053][227910] FPS: 330177.10[0m
[36m[2023-07-10 11:18:40,925][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:18:40,940][227910] Reward + Measures: [[5610.91444782    0.2409604     0.25768641    0.22243686    0.02781789]][0m
[37m[1m[2023-07-10 11:18:40,940][227910] Max Reward on eval: 5610.914447815786[0m
[37m[1m[2023-07-10 11:18:40,940][227910] Min Reward on eval: 5610.914447815786[0m
[37m[1m[2023-07-10 11:18:40,940][227910] Mean Reward across all agents: 5610.914447815786[0m
[37m[1m[2023-07-10 11:18:40,941][227910] Average Trajectory Length: 988.8453333333333[0m
[36m[2023-07-10 11:18:46,452][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:18:46,457][227910] Reward + Measures: [[  98.04344603    0.37805268    0.30613685    0.40510526    0.22012632]
 [1633.20838817    0.32184789    0.46453914    0.31306103    0.19602227]
 [  80.71148878    0.14600001    0.48409995    0.3976        0.32049999]
 ...
 [ 130.33626602    0.2583777     0.38456172    0.31093451    0.15293774]
 [2055.97350185    0.23964787    0.39018026    0.34998733    0.11462817]
 [1153.82296872    0.29100645    0.42522904    0.2444258     0.25985485]][0m
[37m[1m[2023-07-10 11:18:46,457][227910] Max Reward on eval: 5289.293255611509[0m
[37m[1m[2023-07-10 11:18:46,457][227910] Min Reward on eval: -622.0041422978277[0m
[37m[1m[2023-07-10 11:18:46,457][227910] Mean Reward across all agents: 617.8094258774472[0m
[37m[1m[2023-07-10 11:18:46,458][227910] Average Trajectory Length: 770.99[0m
[36m[2023-07-10 11:18:46,460][227910] mean_value=-2069.236376018928, max_value=614.4794279605785[0m
[37m[1m[2023-07-10 11:18:46,463][227910] New mean coefficients: [[1.8473042  0.30697837 0.4847258  0.76483303 2.5345843 ]][0m
[37m[1m[2023-07-10 11:18:46,464][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:18:56,208][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 11:18:56,209][227910] FPS: 394133.82[0m
[36m[2023-07-10 11:18:56,211][227910] itr=249, itrs=2000, Progress: 12.45%[0m
[36m[2023-07-10 11:19:07,630][227910] train() took 11.40 seconds to complete[0m
[36m[2023-07-10 11:19:07,631][227910] FPS: 336756.43[0m
[36m[2023-07-10 11:19:12,357][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:19:12,358][227910] Reward + Measures: [[5705.78334368    0.23956823    0.24095702    0.22159392    0.02546102]][0m
[37m[1m[2023-07-10 11:19:12,358][227910] Max Reward on eval: 5705.783343683323[0m
[37m[1m[2023-07-10 11:19:12,358][227910] Min Reward on eval: 5705.783343683323[0m
[37m[1m[2023-07-10 11:19:12,358][227910] Mean Reward across all agents: 5705.783343683323[0m
[37m[1m[2023-07-10 11:19:12,359][227910] Average Trajectory Length: 987.0996666666666[0m
[36m[2023-07-10 11:19:17,762][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:19:17,763][227910] Reward + Measures: [[ 929.46878549    0.24660002    0.48499998    0.36300001    0.23670001]
 [ 359.94484478    0.17569457    0.34371406    0.28742668    0.22473939]
 [1522.27604599    0.25118062    0.50983137    0.31211343    0.20339303]
 ...
 [-318.14356781    0.51429999    0.44400007    0.611         0.184     ]
 [-815.56513567    0.41869998    0.40120003    0.49419999    0.22839999]
 [ 904.57503209    0.26479998    0.48629999    0.46900001    0.27150002]][0m
[37m[1m[2023-07-10 11:19:17,763][227910] Max Reward on eval: 5206.5526578470135[0m
[37m[1m[2023-07-10 11:19:17,763][227910] Min Reward on eval: -1006.7221599948592[0m
[37m[1m[2023-07-10 11:19:17,763][227910] Mean Reward across all agents: 538.0051862740414[0m
[37m[1m[2023-07-10 11:19:17,764][227910] Average Trajectory Length: 948.906[0m
[36m[2023-07-10 11:19:17,766][227910] mean_value=-1776.308006973036, max_value=4791.74274539263[0m
[37m[1m[2023-07-10 11:19:17,768][227910] New mean coefficients: [[ 1.8512627   0.00037867 -0.01445544  0.974669    2.264207  ]][0m
[37m[1m[2023-07-10 11:19:17,769][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:19:27,456][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 11:19:27,457][227910] FPS: 396462.46[0m
[36m[2023-07-10 11:19:27,459][227910] itr=250, itrs=2000, Progress: 12.50%[0m
[37m[1m[2023-07-10 11:19:29,758][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000230[0m
[36m[2023-07-10 11:19:41,566][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 11:19:41,572][227910] FPS: 332750.95[0m
[36m[2023-07-10 11:19:46,250][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:19:46,250][227910] Reward + Measures: [[5776.02200941    0.23955703    0.23405863    0.22098152    0.02589167]][0m
[37m[1m[2023-07-10 11:19:46,250][227910] Max Reward on eval: 5776.022009408271[0m
[37m[1m[2023-07-10 11:19:46,251][227910] Min Reward on eval: 5776.022009408271[0m
[37m[1m[2023-07-10 11:19:46,251][227910] Mean Reward across all agents: 5776.022009408271[0m
[37m[1m[2023-07-10 11:19:46,251][227910] Average Trajectory Length: 985.7923333333333[0m
[36m[2023-07-10 11:19:51,677][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:19:51,683][227910] Reward + Measures: [[ 110.21264409    0.24590333    0.47088489    0.25722751    0.37141615]
 [ 602.83169765    0.22592178    0.35291886    0.27153715    0.1900252 ]
 [  58.50676788    0.18439457    0.30168459    0.19331962    0.17337857]
 ...
 [-248.04080636    0.22694996    0.32263193    0.25889561    0.20112096]
 [  25.08145418    0.22439395    0.40683341    0.307704      0.25517079]
 [-348.45889786    0.22046061    0.34140393    0.27596429    0.16555688]][0m
[37m[1m[2023-07-10 11:19:51,683][227910] Max Reward on eval: 4778.829256347357[0m
[37m[1m[2023-07-10 11:19:51,683][227910] Min Reward on eval: -1392.6488682938275[0m
[37m[1m[2023-07-10 11:19:51,684][227910] Mean Reward across all agents: 592.2890330820086[0m
[37m[1m[2023-07-10 11:19:51,684][227910] Average Trajectory Length: 773.6709999999999[0m
[36m[2023-07-10 11:19:51,686][227910] mean_value=-2008.8226934633985, max_value=942.1505702252769[0m
[37m[1m[2023-07-10 11:19:51,688][227910] New mean coefficients: [[ 2.4470358  -0.24053827  1.0321376   0.4534039   1.1355743 ]][0m
[37m[1m[2023-07-10 11:19:51,689][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:20:01,336][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 11:20:01,336][227910] FPS: 398126.51[0m
[36m[2023-07-10 11:20:01,339][227910] itr=251, itrs=2000, Progress: 12.55%[0m
[36m[2023-07-10 11:20:12,883][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 11:20:12,883][227910] FPS: 333040.88[0m
[36m[2023-07-10 11:20:17,687][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:20:17,688][227910] Reward + Measures: [[5826.70615641    0.23438703    0.23565529    0.21853907    0.0229713 ]][0m
[37m[1m[2023-07-10 11:20:17,688][227910] Max Reward on eval: 5826.706156407062[0m
[37m[1m[2023-07-10 11:20:17,688][227910] Min Reward on eval: 5826.706156407062[0m
[37m[1m[2023-07-10 11:20:17,688][227910] Mean Reward across all agents: 5826.706156407062[0m
[37m[1m[2023-07-10 11:20:17,689][227910] Average Trajectory Length: 981.1146666666666[0m
[36m[2023-07-10 11:20:23,169][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:20:23,170][227910] Reward + Measures: [[-141.60900107    0.19528401    0.33154318    0.19954905    0.26084089]
 [-324.86379745    0.49707684    0.3627466     0.38164389    0.2823019 ]
 [ 704.5413013     0.3864367     0.36908349    0.24586141    0.29171577]
 ...
 [-326.24160084    0.27518117    0.31992948    0.29655385    0.22185837]
 [2863.41646864    0.21374913    0.26336738    0.26728946    0.10418089]
 [-171.05597902    0.25857949    0.33052331    0.2782191     0.1689229 ]][0m
[37m[1m[2023-07-10 11:20:23,170][227910] Max Reward on eval: 4230.306086028926[0m
[37m[1m[2023-07-10 11:20:23,170][227910] Min Reward on eval: -1236.7875592326745[0m
[37m[1m[2023-07-10 11:20:23,171][227910] Mean Reward across all agents: 346.9364020186316[0m
[37m[1m[2023-07-10 11:20:23,171][227910] Average Trajectory Length: 789.68[0m
[36m[2023-07-10 11:20:23,173][227910] mean_value=-1939.4511615782867, max_value=1692.3718972109766[0m
[37m[1m[2023-07-10 11:20:23,175][227910] New mean coefficients: [[ 2.2396054   0.00803681  1.2983613  -0.9084981   0.61561865]][0m
[37m[1m[2023-07-10 11:20:23,176][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:20:33,031][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 11:20:33,031][227910] FPS: 389735.90[0m
[36m[2023-07-10 11:20:33,034][227910] itr=252, itrs=2000, Progress: 12.60%[0m
[36m[2023-07-10 11:20:44,525][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 11:20:44,525][227910] FPS: 334655.95[0m
[36m[2023-07-10 11:20:49,404][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:20:49,404][227910] Reward + Measures: [[5910.27158949    0.22802857    0.23832409    0.21442834    0.02013084]][0m
[37m[1m[2023-07-10 11:20:49,404][227910] Max Reward on eval: 5910.271589486765[0m
[37m[1m[2023-07-10 11:20:49,404][227910] Min Reward on eval: 5910.271589486765[0m
[37m[1m[2023-07-10 11:20:49,405][227910] Mean Reward across all agents: 5910.271589486765[0m
[37m[1m[2023-07-10 11:20:49,405][227910] Average Trajectory Length: 979.8433333333332[0m
[36m[2023-07-10 11:20:54,872][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:20:54,878][227910] Reward + Measures: [[ 876.99345938    0.20143913    0.35251984    0.30175525    0.15970547]
 [ 496.53503672    0.14408706    0.23758967    0.20134409    0.18993233]
 [ 754.80113788    0.34260002    0.39330003    0.29179999    0.1789    ]
 ...
 [ 190.55601796    0.2656357     0.33867809    0.30310026    0.14732198]
 [-101.77368333    0.3002784     0.37036419    0.32312939    0.15600653]
 [ 659.50542092    0.28036591    0.36098018    0.38806286    0.09480249]][0m
[37m[1m[2023-07-10 11:20:54,878][227910] Max Reward on eval: 5051.768073150515[0m
[37m[1m[2023-07-10 11:20:54,879][227910] Min Reward on eval: -937.5128550932728[0m
[37m[1m[2023-07-10 11:20:54,879][227910] Mean Reward across all agents: 604.7076828054323[0m
[37m[1m[2023-07-10 11:20:54,879][227910] Average Trajectory Length: 791.8673333333334[0m
[36m[2023-07-10 11:20:54,881][227910] mean_value=-1930.936837327135, max_value=1103.5702456089348[0m
[37m[1m[2023-07-10 11:20:54,884][227910] New mean coefficients: [[ 1.9957448   0.40973556  0.47680384 -0.2594707   0.69601643]][0m
[37m[1m[2023-07-10 11:20:54,885][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:21:04,535][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 11:21:04,535][227910] FPS: 397973.92[0m
[36m[2023-07-10 11:21:04,538][227910] itr=253, itrs=2000, Progress: 12.65%[0m
[36m[2023-07-10 11:21:16,005][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 11:21:16,005][227910] FPS: 335288.82[0m
[36m[2023-07-10 11:21:20,847][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:21:20,848][227910] Reward + Measures: [[1654.17635098    0.25771621    0.38850257    0.22179882    0.13027893]][0m
[37m[1m[2023-07-10 11:21:20,848][227910] Max Reward on eval: 1654.1763509790214[0m
[37m[1m[2023-07-10 11:21:20,848][227910] Min Reward on eval: 1654.1763509790214[0m
[37m[1m[2023-07-10 11:21:20,848][227910] Mean Reward across all agents: 1654.1763509790214[0m
[37m[1m[2023-07-10 11:21:20,849][227910] Average Trajectory Length: 676.9663333333333[0m
[36m[2023-07-10 11:21:26,516][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:21:26,517][227910] Reward + Measures: [[1968.66455573    0.2969        0.47569999    0.2895        0.1603    ]
 [-171.40968564    0.32426563    0.51090884    0.292779      0.37239835]
 [1128.89612999    0.2439709     0.33563718    0.26543036    0.24341369]
 ...
 [ 824.76868113    0.33845359    0.34431562    0.24271333    0.1535103 ]
 [ 239.0167107     0.23612694    0.50524473    0.31512904    0.36492595]
 [1000.11742526    0.2705        0.58460003    0.29490003    0.23839998]][0m
[37m[1m[2023-07-10 11:21:26,517][227910] Max Reward on eval: 2809.688174735848[0m
[37m[1m[2023-07-10 11:21:26,517][227910] Min Reward on eval: -1204.6505335471127[0m
[37m[1m[2023-07-10 11:21:26,517][227910] Mean Reward across all agents: 427.9720041383462[0m
[37m[1m[2023-07-10 11:21:26,518][227910] Average Trajectory Length: 771.7769999999999[0m
[36m[2023-07-10 11:21:26,520][227910] mean_value=-1895.4916753259279, max_value=895.536200772319[0m
[37m[1m[2023-07-10 11:21:26,522][227910] New mean coefficients: [[ 1.6494267  -0.338018   -0.21146464 -0.02476794  0.776813  ]][0m
[37m[1m[2023-07-10 11:21:26,523][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:21:36,405][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 11:21:36,405][227910] FPS: 388660.56[0m
[36m[2023-07-10 11:21:36,407][227910] itr=254, itrs=2000, Progress: 12.70%[0m
[36m[2023-07-10 11:21:48,007][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 11:21:48,008][227910] FPS: 331452.23[0m
[36m[2023-07-10 11:21:52,776][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:21:52,777][227910] Reward + Measures: [[2334.33748751    0.27282107    0.39745849    0.22772948    0.11634568]][0m
[37m[1m[2023-07-10 11:21:52,777][227910] Max Reward on eval: 2334.337487510512[0m
[37m[1m[2023-07-10 11:21:52,777][227910] Min Reward on eval: 2334.337487510512[0m
[37m[1m[2023-07-10 11:21:52,777][227910] Mean Reward across all agents: 2334.337487510512[0m
[37m[1m[2023-07-10 11:21:52,778][227910] Average Trajectory Length: 812.5316666666666[0m
[36m[2023-07-10 11:21:58,160][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:21:58,160][227910] Reward + Measures: [[1859.22389374    0.23207469    0.27535361    0.21479237    0.13442181]
 [ 223.40087534    0.39290002    0.42139998    0.30089998    0.36039996]
 [ 739.39700297    0.33990002    0.41770002    0.33309999    0.2843    ]
 ...
 [1871.37143054    0.28598729    0.4387171     0.24985161    0.12980324]
 [  25.58841993    0.32170436    0.40726957    0.21530871    0.36272174]
 [-496.43408211    0.1312        0.54519999    0.25400001    0.333     ]][0m
[37m[1m[2023-07-10 11:21:58,160][227910] Max Reward on eval: 2934.82495722553[0m
[37m[1m[2023-07-10 11:21:58,161][227910] Min Reward on eval: -725.8263992991473[0m
[37m[1m[2023-07-10 11:21:58,161][227910] Mean Reward across all agents: 966.3370479882691[0m
[37m[1m[2023-07-10 11:21:58,161][227910] Average Trajectory Length: 902.155[0m
[36m[2023-07-10 11:21:58,163][227910] mean_value=-1319.2690751932435, max_value=859.7388152751521[0m
[37m[1m[2023-07-10 11:21:58,165][227910] New mean coefficients: [[ 1.6836349  -0.38423967 -0.8395812   0.61706907  0.7751499 ]][0m
[37m[1m[2023-07-10 11:21:58,166][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:22:07,862][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 11:22:07,863][227910] FPS: 396101.80[0m
[36m[2023-07-10 11:22:07,865][227910] itr=255, itrs=2000, Progress: 12.75%[0m
[36m[2023-07-10 11:22:19,551][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 11:22:19,551][227910] FPS: 329019.63[0m
[36m[2023-07-10 11:22:24,309][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:22:24,310][227910] Reward + Measures: [[2083.25816431    0.28683886    0.33021837    0.24765186    0.16168071]][0m
[37m[1m[2023-07-10 11:22:24,310][227910] Max Reward on eval: 2083.258164309719[0m
[37m[1m[2023-07-10 11:22:24,310][227910] Min Reward on eval: 2083.258164309719[0m
[37m[1m[2023-07-10 11:22:24,311][227910] Mean Reward across all agents: 2083.258164309719[0m
[37m[1m[2023-07-10 11:22:24,311][227910] Average Trajectory Length: 914.5356666666667[0m
[36m[2023-07-10 11:22:29,906][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:22:29,907][227910] Reward + Measures: [[1928.47098716    0.29998237    0.3242313     0.25722653    0.17823289]
 [1763.36051722    0.29299593    0.27815846    0.22948985    0.14994688]
 [1431.45109797    0.2658        0.44140002    0.26980004    0.2368    ]
 ...
 [1630.17679447    0.28976676    0.38470957    0.27626622    0.22879957]
 [2037.92918792    0.3319484     0.3382223     0.26152477    0.16843306]
 [1856.14337851    0.31182301    0.35371819    0.25993729    0.15980466]][0m
[37m[1m[2023-07-10 11:22:29,907][227910] Max Reward on eval: 2435.131107180566[0m
[37m[1m[2023-07-10 11:22:29,907][227910] Min Reward on eval: 645.5464837102802[0m
[37m[1m[2023-07-10 11:22:29,908][227910] Mean Reward across all agents: 1792.5133464802095[0m
[37m[1m[2023-07-10 11:22:29,908][227910] Average Trajectory Length: 886.762[0m
[36m[2023-07-10 11:22:29,909][227910] mean_value=-1542.3228179906985, max_value=64.42987723745318[0m
[37m[1m[2023-07-10 11:22:29,912][227910] New mean coefficients: [[ 2.5055823  -0.09852424 -0.41090152  0.62536985  0.8233006 ]][0m
[37m[1m[2023-07-10 11:22:29,913][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:22:39,591][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 11:22:39,592][227910] FPS: 396808.40[0m
[36m[2023-07-10 11:22:39,594][227910] itr=256, itrs=2000, Progress: 12.80%[0m
[36m[2023-07-10 11:22:51,207][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 11:22:51,207][227910] FPS: 331143.25[0m
[36m[2023-07-10 11:22:56,053][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:22:56,054][227910] Reward + Measures: [[2282.3207596     0.28618386    0.32312962    0.24562167    0.15039651]][0m
[37m[1m[2023-07-10 11:22:56,054][227910] Max Reward on eval: 2282.320759600673[0m
[37m[1m[2023-07-10 11:22:56,054][227910] Min Reward on eval: 2282.320759600673[0m
[37m[1m[2023-07-10 11:22:56,055][227910] Mean Reward across all agents: 2282.320759600673[0m
[37m[1m[2023-07-10 11:22:56,055][227910] Average Trajectory Length: 916.704[0m
[36m[2023-07-10 11:23:01,421][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:23:01,422][227910] Reward + Measures: [[1067.01189633    0.28400001    0.4147        0.2791        0.23239999]
 [1070.48530229    0.44449997    0.30040002    0.32729998    0.23550001]
 [-384.97351147    0.44174129    0.36862373    0.33098403    0.30067348]
 ...
 [ 628.88081244    0.21331882    0.26180661    0.22063851    0.13002408]
 [  80.01463051    0.38876185    0.33289257    0.32667729    0.26699144]
 [-250.77380767    0.21922588    0.353228      0.30393451    0.28375337]][0m
[37m[1m[2023-07-10 11:23:01,422][227910] Max Reward on eval: 2625.1553069843912[0m
[37m[1m[2023-07-10 11:23:01,422][227910] Min Reward on eval: -1522.4563204304081[0m
[37m[1m[2023-07-10 11:23:01,422][227910] Mean Reward across all agents: 486.0963445950367[0m
[37m[1m[2023-07-10 11:23:01,423][227910] Average Trajectory Length: 857.0896666666666[0m
[36m[2023-07-10 11:23:01,426][227910] mean_value=-1116.4928060490986, max_value=1535.4583288671938[0m
[37m[1m[2023-07-10 11:23:01,429][227910] New mean coefficients: [[ 2.3954103   0.26856497 -0.5150247   0.86896634  1.504895  ]][0m
[37m[1m[2023-07-10 11:23:01,430][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:23:11,190][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 11:23:11,191][227910] FPS: 393491.67[0m
[36m[2023-07-10 11:23:11,193][227910] itr=257, itrs=2000, Progress: 12.85%[0m
[36m[2023-07-10 11:23:22,809][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 11:23:22,809][227910] FPS: 331062.50[0m
[36m[2023-07-10 11:23:27,500][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:23:27,500][227910] Reward + Measures: [[2550.21507424    0.2941179     0.32587972    0.25197265    0.1475254 ]][0m
[37m[1m[2023-07-10 11:23:27,501][227910] Max Reward on eval: 2550.215074235608[0m
[37m[1m[2023-07-10 11:23:27,501][227910] Min Reward on eval: 2550.215074235608[0m
[37m[1m[2023-07-10 11:23:27,501][227910] Mean Reward across all agents: 2550.215074235608[0m
[37m[1m[2023-07-10 11:23:27,502][227910] Average Trajectory Length: 944.2139999999999[0m
[36m[2023-07-10 11:23:32,826][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:23:32,826][227910] Reward + Measures: [[-716.92443654    0.36978307    0.3746514     0.19705793    0.38665465]
 [1059.18299892    0.20235947    0.23707838    0.21252973    0.10986487]
 [2363.57895672    0.29269999    0.30870003    0.25869998    0.1392    ]
 ...
 [-596.75476277    0.67832756    0.56205171    0.37313795    0.64296895]
 [ 484.34065879    0.45460007    0.41050002    0.36490002    0.43140003]
 [-589.29146526    0.72179997    0.35780001    0.43709999    0.65670002]][0m
[37m[1m[2023-07-10 11:23:32,826][227910] Max Reward on eval: 2940.3621813773643[0m
[37m[1m[2023-07-10 11:23:32,827][227910] Min Reward on eval: -721.9568047281471[0m
[37m[1m[2023-07-10 11:23:32,827][227910] Mean Reward across all agents: 859.7746820621103[0m
[37m[1m[2023-07-10 11:23:32,827][227910] Average Trajectory Length: 892.913[0m
[36m[2023-07-10 11:23:32,833][227910] mean_value=-932.1800328506196, max_value=1669.6398712256726[0m
[37m[1m[2023-07-10 11:23:32,836][227910] New mean coefficients: [[ 2.8858283   1.1150061  -0.6092212   0.92122984  1.6802673 ]][0m
[37m[1m[2023-07-10 11:23:32,837][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:23:42,427][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 11:23:42,427][227910] FPS: 400496.95[0m
[36m[2023-07-10 11:23:42,430][227910] itr=258, itrs=2000, Progress: 12.90%[0m
[36m[2023-07-10 11:23:53,886][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 11:23:53,886][227910] FPS: 335677.14[0m
[36m[2023-07-10 11:23:58,733][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:23:58,734][227910] Reward + Measures: [[2782.1208627     0.29111347    0.32121336    0.24952598    0.13312641]][0m
[37m[1m[2023-07-10 11:23:58,734][227910] Max Reward on eval: 2782.1208626962107[0m
[37m[1m[2023-07-10 11:23:58,734][227910] Min Reward on eval: 2782.1208626962107[0m
[37m[1m[2023-07-10 11:23:58,734][227910] Mean Reward across all agents: 2782.1208626962107[0m
[37m[1m[2023-07-10 11:23:58,734][227910] Average Trajectory Length: 945.137[0m
[36m[2023-07-10 11:24:04,361][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:24:04,362][227910] Reward + Measures: [[2757.2755201     0.26860002    0.30879998    0.2418        0.14219999]
 [1524.46533028    0.25690001    0.31310001    0.31440002    0.18670002]
 [-219.06126865    0.288398      0.29658204    0.31861067    0.20144181]
 ...
 [ 617.46196724    0.33138227    0.33180887    0.26458859    0.14698482]
 [ 445.45831397    0.31419998    0.30570003    0.29840001    0.12519999]
 [ 620.22181405    0.5528        0.36960003    0.48699999    0.25740001]][0m
[37m[1m[2023-07-10 11:24:04,362][227910] Max Reward on eval: 2901.225020576804[0m
[37m[1m[2023-07-10 11:24:04,362][227910] Min Reward on eval: -891.4861990719103[0m
[37m[1m[2023-07-10 11:24:04,363][227910] Mean Reward across all agents: 725.3758073294113[0m
[37m[1m[2023-07-10 11:24:04,363][227910] Average Trajectory Length: 877.8516666666667[0m
[36m[2023-07-10 11:24:04,365][227910] mean_value=-1636.4736742179434, max_value=1517.3150195863113[0m
[37m[1m[2023-07-10 11:24:04,368][227910] New mean coefficients: [[3.0124567  1.0225497  0.20597798 0.96530026 1.3954704 ]][0m
[37m[1m[2023-07-10 11:24:04,369][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:24:14,090][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 11:24:14,090][227910] FPS: 395084.32[0m
[36m[2023-07-10 11:24:14,092][227910] itr=259, itrs=2000, Progress: 12.95%[0m
[36m[2023-07-10 11:24:25,664][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 11:24:25,664][227910] FPS: 332253.14[0m
[36m[2023-07-10 11:24:30,540][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:24:30,540][227910] Reward + Measures: [[3072.88765909    0.29425654    0.3169837     0.25009972    0.12370028]][0m
[37m[1m[2023-07-10 11:24:30,540][227910] Max Reward on eval: 3072.887659094386[0m
[37m[1m[2023-07-10 11:24:30,541][227910] Min Reward on eval: 3072.887659094386[0m
[37m[1m[2023-07-10 11:24:30,541][227910] Mean Reward across all agents: 3072.887659094386[0m
[37m[1m[2023-07-10 11:24:30,541][227910] Average Trajectory Length: 953.3396666666666[0m
[36m[2023-07-10 11:24:35,998][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:24:35,998][227910] Reward + Measures: [[1337.22627972    0.29709098    0.25134525    0.25216648    0.10777759]
 [1979.94656682    0.28425387    0.27204686    0.24614947    0.09696467]
 [1974.3644414     0.28340003    0.26349998    0.2414        0.13599999]
 ...
 [ 172.28384084    0.50450009    0.74529999    0.14049999    0.7263    ]
 [ 682.23834401    0.2756978     0.25206608    0.2257612     0.09972055]
 [ 513.569242      0.2195518     0.19327255    0.18095982    0.09337371]][0m
[37m[1m[2023-07-10 11:24:35,998][227910] Max Reward on eval: 3111.9006514786743[0m
[37m[1m[2023-07-10 11:24:35,999][227910] Min Reward on eval: -501.6001608381979[0m
[37m[1m[2023-07-10 11:24:35,999][227910] Mean Reward across all agents: 1001.9348752567117[0m
[37m[1m[2023-07-10 11:24:35,999][227910] Average Trajectory Length: 733.4283333333333[0m
[36m[2023-07-10 11:24:36,002][227910] mean_value=-2338.052548439909, max_value=1188.6107515726005[0m
[37m[1m[2023-07-10 11:24:36,005][227910] New mean coefficients: [[3.4590616  1.2195371  1.9285545  0.80104136 1.3256451 ]][0m
[37m[1m[2023-07-10 11:24:36,006][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:24:45,729][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 11:24:45,729][227910] FPS: 395020.96[0m
[36m[2023-07-10 11:24:45,731][227910] itr=260, itrs=2000, Progress: 13.00%[0m
[37m[1m[2023-07-10 11:24:47,966][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000240[0m
[36m[2023-07-10 11:24:59,690][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 11:24:59,695][227910] FPS: 334892.08[0m
[36m[2023-07-10 11:25:04,468][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:25:04,468][227910] Reward + Measures: [[3337.17987849    0.30101851    0.31510869    0.25372782    0.11649036]][0m
[37m[1m[2023-07-10 11:25:04,469][227910] Max Reward on eval: 3337.179878491548[0m
[37m[1m[2023-07-10 11:25:04,469][227910] Min Reward on eval: 3337.179878491548[0m
[37m[1m[2023-07-10 11:25:04,469][227910] Mean Reward across all agents: 3337.179878491548[0m
[37m[1m[2023-07-10 11:25:04,469][227910] Average Trajectory Length: 967.6316666666667[0m
[36m[2023-07-10 11:25:09,921][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:25:09,922][227910] Reward + Measures: [[2240.31599366    0.28365135    0.3175081     0.23464595    0.14097838]
 [2263.60012066    0.28830001    0.33140001    0.25080001    0.16990001]
 [2264.65323658    0.27346084    0.31162462    0.2426476     0.17874111]
 ...
 [2516.93798004    0.27073389    0.29650879    0.23539709    0.1590478 ]
 [2028.51377677    0.27267915    0.33820161    0.25947991    0.20926987]
 [2682.96590079    0.3021        0.315         0.25410002    0.1408    ]][0m
[37m[1m[2023-07-10 11:25:09,922][227910] Max Reward on eval: 3270.611707049445[0m
[37m[1m[2023-07-10 11:25:09,922][227910] Min Reward on eval: 415.74903194233195[0m
[37m[1m[2023-07-10 11:25:09,923][227910] Mean Reward across all agents: 1944.4297894874417[0m
[37m[1m[2023-07-10 11:25:09,923][227910] Average Trajectory Length: 932.1463333333332[0m
[36m[2023-07-10 11:25:09,925][227910] mean_value=-881.0430611830907, max_value=873.7194307637778[0m
[37m[1m[2023-07-10 11:25:09,928][227910] New mean coefficients: [[3.3118637  1.2567265  1.770369   0.76746887 1.5626813 ]][0m
[37m[1m[2023-07-10 11:25:09,929][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:25:19,680][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 11:25:19,680][227910] FPS: 393884.75[0m
[36m[2023-07-10 11:25:19,682][227910] itr=261, itrs=2000, Progress: 13.05%[0m
[36m[2023-07-10 11:25:31,286][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 11:25:31,286][227910] FPS: 331402.21[0m
[36m[2023-07-10 11:25:36,000][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:25:36,006][227910] Reward + Measures: [[3534.6656647     0.30498803    0.31235632    0.26036167    0.1137308 ]][0m
[37m[1m[2023-07-10 11:25:36,006][227910] Max Reward on eval: 3534.665664702071[0m
[37m[1m[2023-07-10 11:25:36,006][227910] Min Reward on eval: 3534.665664702071[0m
[37m[1m[2023-07-10 11:25:36,007][227910] Mean Reward across all agents: 3534.665664702071[0m
[37m[1m[2023-07-10 11:25:36,007][227910] Average Trajectory Length: 984.0836666666667[0m
[36m[2023-07-10 11:25:41,517][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:25:41,523][227910] Reward + Measures: [[2984.40387159    0.31979999    0.32730001    0.2703        0.15009999]
 [3372.11990711    0.32640001    0.31490001    0.26390001    0.1245    ]
 [2888.13423597    0.27380002    0.27250001    0.24150001    0.12810002]
 ...
 [2698.8824351     0.33048618    0.33297983    0.28002426    0.16719909]
 [3013.18697384    0.29601079    0.30232623    0.24855597    0.11984122]
 [2968.27767434    0.32459998    0.3177        0.2728        0.15010001]][0m
[37m[1m[2023-07-10 11:25:41,523][227910] Max Reward on eval: 3635.976045134105[0m
[37m[1m[2023-07-10 11:25:41,524][227910] Min Reward on eval: 1760.2704358045157[0m
[37m[1m[2023-07-10 11:25:41,524][227910] Mean Reward across all agents: 2864.83790100565[0m
[37m[1m[2023-07-10 11:25:41,524][227910] Average Trajectory Length: 987.5726666666666[0m
[36m[2023-07-10 11:25:41,527][227910] mean_value=81.61813454187991, max_value=960.7410003283562[0m
[37m[1m[2023-07-10 11:25:41,530][227910] New mean coefficients: [[3.280424   0.7839438  1.066333   0.19130707 0.57878095]][0m
[37m[1m[2023-07-10 11:25:41,531][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:25:51,314][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 11:25:51,315][227910] FPS: 392572.21[0m
[36m[2023-07-10 11:25:51,317][227910] itr=262, itrs=2000, Progress: 13.10%[0m
[36m[2023-07-10 11:26:02,997][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 11:26:02,998][227910] FPS: 329238.90[0m
[36m[2023-07-10 11:26:07,776][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:26:07,777][227910] Reward + Measures: [[3730.96905261    0.30039188    0.31313294    0.25836703    0.1060074 ]][0m
[37m[1m[2023-07-10 11:26:07,777][227910] Max Reward on eval: 3730.9690526059007[0m
[37m[1m[2023-07-10 11:26:07,777][227910] Min Reward on eval: 3730.9690526059007[0m
[37m[1m[2023-07-10 11:26:07,778][227910] Mean Reward across all agents: 3730.9690526059007[0m
[37m[1m[2023-07-10 11:26:07,778][227910] Average Trajectory Length: 984.8006666666666[0m
[36m[2023-07-10 11:26:13,348][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:26:13,353][227910] Reward + Measures: [[ 401.66610304    0.2227062     0.50155449    0.31034976    0.29759464]
 [ 159.91327876    0.58079839    0.59761655    0.54056692    0.50283879]
 [2285.60327804    0.26646188    0.36176905    0.27260056    0.19389835]
 ...
 [ 567.80294347    0.21044762    0.31247148    0.23938285    0.12985964]
 [1829.65909985    0.25386688    0.38159734    0.2968291     0.23964854]
 [ 262.12281595    0.45568153    0.5261063     0.43332887    0.43461424]][0m
[37m[1m[2023-07-10 11:26:13,354][227910] Max Reward on eval: 3794.6993191149086[0m
[37m[1m[2023-07-10 11:26:13,354][227910] Min Reward on eval: -1048.686292770668[0m
[37m[1m[2023-07-10 11:26:13,354][227910] Mean Reward across all agents: 1198.1412057975515[0m
[37m[1m[2023-07-10 11:26:13,355][227910] Average Trajectory Length: 810.4563333333333[0m
[36m[2023-07-10 11:26:13,357][227910] mean_value=-1530.3170689166893, max_value=1075.9631836841445[0m
[37m[1m[2023-07-10 11:26:13,359][227910] New mean coefficients: [[3.0834482  0.13900203 0.94435453 0.37069088 0.4676361 ]][0m
[37m[1m[2023-07-10 11:26:13,360][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:26:23,042][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 11:26:23,043][227910] FPS: 396678.22[0m
[36m[2023-07-10 11:26:23,045][227910] itr=263, itrs=2000, Progress: 13.15%[0m
[36m[2023-07-10 11:26:34,620][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 11:26:34,620][227910] FPS: 332226.87[0m
[36m[2023-07-10 11:26:39,285][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:26:39,285][227910] Reward + Measures: [[3943.10510467    0.2984502     0.31514087    0.25715485    0.09833644]][0m
[37m[1m[2023-07-10 11:26:39,285][227910] Max Reward on eval: 3943.1051046748435[0m
[37m[1m[2023-07-10 11:26:39,285][227910] Min Reward on eval: 3943.1051046748435[0m
[37m[1m[2023-07-10 11:26:39,286][227910] Mean Reward across all agents: 3943.1051046748435[0m
[37m[1m[2023-07-10 11:26:39,286][227910] Average Trajectory Length: 988.7636666666666[0m
[36m[2023-07-10 11:26:44,756][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:26:44,756][227910] Reward + Measures: [[ 578.83849183    0.23091181    0.34836727    0.25142789    0.07489152]
 [2942.30403248    0.3414        0.3493        0.2861        0.192     ]
 [ 107.4474489     0.16707455    0.25262889    0.21484995    0.10208191]
 ...
 [  36.25379634    0.16545258    0.26509967    0.20325069    0.09857615]
 [2819.86263462    0.34999999    0.37080002    0.2793        0.20050001]
 [1715.85631691    0.33413562    0.38117385    0.27198145    0.14617006]][0m
[37m[1m[2023-07-10 11:26:44,757][227910] Max Reward on eval: 3942.383084818767[0m
[37m[1m[2023-07-10 11:26:44,757][227910] Min Reward on eval: -325.12496636559956[0m
[37m[1m[2023-07-10 11:26:44,757][227910] Mean Reward across all agents: 1248.686600555071[0m
[37m[1m[2023-07-10 11:26:44,757][227910] Average Trajectory Length: 882.2953333333334[0m
[36m[2023-07-10 11:26:44,761][227910] mean_value=-1240.4781918559042, max_value=975.8130522595029[0m
[37m[1m[2023-07-10 11:26:44,763][227910] New mean coefficients: [[2.6396308  0.00706212 0.6435727  1.7008176  0.8484186 ]][0m
[37m[1m[2023-07-10 11:26:44,764][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:26:54,401][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 11:26:54,401][227910] FPS: 398559.53[0m
[36m[2023-07-10 11:26:54,403][227910] itr=264, itrs=2000, Progress: 13.20%[0m
[36m[2023-07-10 11:27:05,920][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 11:27:05,921][227910] FPS: 333867.64[0m
[36m[2023-07-10 11:27:10,630][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:27:10,631][227910] Reward + Measures: [[4144.46782373    0.29164842    0.31244904    0.25496501    0.08709329]][0m
[37m[1m[2023-07-10 11:27:10,631][227910] Max Reward on eval: 4144.467823728442[0m
[37m[1m[2023-07-10 11:27:10,631][227910] Min Reward on eval: 4144.467823728442[0m
[37m[1m[2023-07-10 11:27:10,631][227910] Mean Reward across all agents: 4144.467823728442[0m
[37m[1m[2023-07-10 11:27:10,631][227910] Average Trajectory Length: 987.3333333333333[0m
[36m[2023-07-10 11:27:16,111][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:27:16,111][227910] Reward + Measures: [[2342.75277052    0.28980002    0.33310002    0.25640002    0.17460001]
 [1763.58618189    0.31649232    0.38266739    0.28437576    0.21505867]
 [2052.65193148    0.23461393    0.31042901    0.2315589     0.12819636]
 ...
 [ -52.37034588    0.37046677    0.28802747    0.38543239    0.26410279]
 [4094.69682432    0.264         0.28839999    0.23580001    0.0701    ]
 [1258.15599749    0.223352      0.25879952    0.21584955    0.15336472]][0m
[37m[1m[2023-07-10 11:27:16,112][227910] Max Reward on eval: 4213.645509980782[0m
[37m[1m[2023-07-10 11:27:16,112][227910] Min Reward on eval: -720.7961964597343[0m
[37m[1m[2023-07-10 11:27:16,112][227910] Mean Reward across all agents: 1164.2647400056205[0m
[37m[1m[2023-07-10 11:27:16,112][227910] Average Trajectory Length: 737.5656666666666[0m
[36m[2023-07-10 11:27:16,114][227910] mean_value=-2348.665608511421, max_value=1201.4212635324984[0m
[37m[1m[2023-07-10 11:27:16,116][227910] New mean coefficients: [[2.79854    0.11526269 0.41919997 1.0404208  0.70006126]][0m
[37m[1m[2023-07-10 11:27:16,117][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:27:25,783][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 11:27:25,788][227910] FPS: 397338.51[0m
[36m[2023-07-10 11:27:25,791][227910] itr=265, itrs=2000, Progress: 13.25%[0m
[36m[2023-07-10 11:27:37,539][227910] train() took 11.73 seconds to complete[0m
[36m[2023-07-10 11:27:37,539][227910] FPS: 327273.72[0m
[36m[2023-07-10 11:27:42,428][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:27:42,429][227910] Reward + Measures: [[4354.18784865    0.2869291     0.31335348    0.25200549    0.07820757]][0m
[37m[1m[2023-07-10 11:27:42,429][227910] Max Reward on eval: 4354.187848647421[0m
[37m[1m[2023-07-10 11:27:42,429][227910] Min Reward on eval: 4354.187848647421[0m
[37m[1m[2023-07-10 11:27:42,429][227910] Mean Reward across all agents: 4354.187848647421[0m
[37m[1m[2023-07-10 11:27:42,429][227910] Average Trajectory Length: 990.8149999999999[0m
[36m[2023-07-10 11:27:47,929][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:27:47,929][227910] Reward + Measures: [[-102.91998745    0.20240234    0.5042249     0.26133013    0.37877434]
 [ 994.94510745    0.49120003    0.56010002    0.41070005    0.42870003]
 [1761.59366851    0.3673        0.32960001    0.32190001    0.1963    ]
 ...
 [ 582.36163171    0.20830467    0.31893021    0.20480485    0.22793622]
 [  89.34821624    0.85059994    0.82349998    0.72399998    0.82390004]
 [-773.32701888    0.83309996    0.61840004    0.76679999    0.81469995]][0m
[37m[1m[2023-07-10 11:27:47,929][227910] Max Reward on eval: 4065.5911500824614[0m
[37m[1m[2023-07-10 11:27:47,930][227910] Min Reward on eval: -984.9058811483206[0m
[37m[1m[2023-07-10 11:27:47,930][227910] Mean Reward across all agents: 1202.3955918315712[0m
[37m[1m[2023-07-10 11:27:47,930][227910] Average Trajectory Length: 818.7603333333333[0m
[36m[2023-07-10 11:27:47,934][227910] mean_value=-989.8910625388341, max_value=863.510494832765[0m
[37m[1m[2023-07-10 11:27:47,936][227910] New mean coefficients: [[ 2.5436468   0.12060305 -0.50243175  1.3646334   1.2590888 ]][0m
[37m[1m[2023-07-10 11:27:47,938][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:27:57,775][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 11:27:57,775][227910] FPS: 390411.03[0m
[36m[2023-07-10 11:27:57,778][227910] itr=266, itrs=2000, Progress: 13.30%[0m
[36m[2023-07-10 11:28:09,411][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 11:28:09,411][227910] FPS: 330528.68[0m
[36m[2023-07-10 11:28:14,111][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:28:14,117][227910] Reward + Measures: [[4566.58733758    0.2816056     0.30734256    0.24898592    0.06573527]][0m
[37m[1m[2023-07-10 11:28:14,117][227910] Max Reward on eval: 4566.587337582941[0m
[37m[1m[2023-07-10 11:28:14,117][227910] Min Reward on eval: 4566.587337582941[0m
[37m[1m[2023-07-10 11:28:14,117][227910] Mean Reward across all agents: 4566.587337582941[0m
[37m[1m[2023-07-10 11:28:14,118][227910] Average Trajectory Length: 991.3276666666667[0m
[36m[2023-07-10 11:28:19,635][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:28:19,636][227910] Reward + Measures: [[ 763.78401283    0.34080002    0.59899998    0.37970001    0.4052    ]
 [ 531.94638539    0.2629618     0.34022686    0.2644777     0.18837684]
 [1587.7244385     0.2981855     0.27115408    0.23282562    0.14389263]
 ...
 [1077.24695205    0.32489178    0.39816195    0.30331895    0.2442416 ]
 [-144.19713834    0.22511171    0.26015836    0.24369004    0.1773714 ]
 [ 591.98723996    0.28361541    0.3151086     0.28004122    0.26771435]][0m
[37m[1m[2023-07-10 11:28:19,636][227910] Max Reward on eval: 4113.873950773082[0m
[37m[1m[2023-07-10 11:28:19,636][227910] Min Reward on eval: -1638.5764418910142[0m
[37m[1m[2023-07-10 11:28:19,636][227910] Mean Reward across all agents: 773.347759478377[0m
[37m[1m[2023-07-10 11:28:19,637][227910] Average Trajectory Length: 772.2553333333333[0m
[36m[2023-07-10 11:28:19,639][227910] mean_value=-1773.4164315693708, max_value=1072.0566814668036[0m
[37m[1m[2023-07-10 11:28:19,641][227910] New mean coefficients: [[ 2.8377185  0.519197  -1.0059183  1.1591222  1.1293867]][0m
[37m[1m[2023-07-10 11:28:19,642][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:28:29,466][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 11:28:29,466][227910] FPS: 390967.76[0m
[36m[2023-07-10 11:28:29,468][227910] itr=267, itrs=2000, Progress: 13.35%[0m
[36m[2023-07-10 11:28:41,121][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 11:28:41,121][227910] FPS: 330010.55[0m
[36m[2023-07-10 11:28:45,832][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:28:45,833][227910] Reward + Measures: [[4722.46895317    0.27144212    0.3016772     0.2434655     0.06141092]][0m
[37m[1m[2023-07-10 11:28:45,833][227910] Max Reward on eval: 4722.468953171472[0m
[37m[1m[2023-07-10 11:28:45,833][227910] Min Reward on eval: 4722.468953171472[0m
[37m[1m[2023-07-10 11:28:45,833][227910] Mean Reward across all agents: 4722.468953171472[0m
[37m[1m[2023-07-10 11:28:45,833][227910] Average Trajectory Length: 987.0143333333333[0m
[36m[2023-07-10 11:28:51,466][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:28:51,467][227910] Reward + Measures: [[ -49.17595968    0.40100002    0.34849998    0.32449999    0.29260001]
 [1009.48703383    0.28555498    0.31249052    0.27255508    0.21537499]
 [1023.31541446    0.27970001    0.35870004    0.29430002    0.16769999]
 ...
 [-134.91678131    0.3321        0.27939996    0.27900001    0.27399999]
 [ 712.31546787    0.4008382     0.30458599    0.27220738    0.18512473]
 [ 400.22835295    0.25276598    0.31753251    0.25934091    0.23185487]][0m
[37m[1m[2023-07-10 11:28:51,467][227910] Max Reward on eval: 4535.280719148437[0m
[37m[1m[2023-07-10 11:28:51,467][227910] Min Reward on eval: -1270.6858238894492[0m
[37m[1m[2023-07-10 11:28:51,467][227910] Mean Reward across all agents: 628.3914221284426[0m
[37m[1m[2023-07-10 11:28:51,468][227910] Average Trajectory Length: 920.1346666666666[0m
[36m[2023-07-10 11:28:51,470][227910] mean_value=-1857.3128773555372, max_value=1402.914346346904[0m
[37m[1m[2023-07-10 11:28:51,472][227910] New mean coefficients: [[ 3.168821    0.96824867 -0.01561606  1.2723227   1.0786453 ]][0m
[37m[1m[2023-07-10 11:28:51,473][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:29:01,192][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 11:29:01,192][227910] FPS: 395190.83[0m
[36m[2023-07-10 11:29:01,194][227910] itr=268, itrs=2000, Progress: 13.40%[0m
[36m[2023-07-10 11:29:12,751][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 11:29:12,751][227910] FPS: 332762.85[0m
[36m[2023-07-10 11:29:17,429][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:29:17,430][227910] Reward + Measures: [[4900.610343      0.26364177    0.28658804    0.23786831    0.05032795]][0m
[37m[1m[2023-07-10 11:29:17,430][227910] Max Reward on eval: 4900.610342998868[0m
[37m[1m[2023-07-10 11:29:17,430][227910] Min Reward on eval: 4900.610342998868[0m
[37m[1m[2023-07-10 11:29:17,430][227910] Mean Reward across all agents: 4900.610342998868[0m
[37m[1m[2023-07-10 11:29:17,431][227910] Average Trajectory Length: 988.5319999999999[0m
[36m[2023-07-10 11:29:22,992][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:29:22,993][227910] Reward + Measures: [[ 389.6782574     0.62057143    0.25706705    0.62007582    0.25128898]
 [-165.03273631    0.454449      0.50005513    0.57347959    0.02916531]
 [ -88.10380282    0.59400004    0.26280001    0.5733        0.20969999]
 ...
 [ -28.73277271    0.16398637    0.2188416     0.18766592    0.11911786]
 [-378.28599691    0.30320916    0.2476716     0.31911671    0.19542228]
 [2129.72069122    0.2482471     0.31562638    0.26749587    0.14854473]][0m
[37m[1m[2023-07-10 11:29:22,993][227910] Max Reward on eval: 4465.633374152705[0m
[37m[1m[2023-07-10 11:29:22,993][227910] Min Reward on eval: -1425.678219328227[0m
[37m[1m[2023-07-10 11:29:22,993][227910] Mean Reward across all agents: 562.8665229176685[0m
[37m[1m[2023-07-10 11:29:22,994][227910] Average Trajectory Length: 845.4433333333333[0m
[36m[2023-07-10 11:29:22,996][227910] mean_value=-1694.939925053202, max_value=1203.1254591229956[0m
[37m[1m[2023-07-10 11:29:22,999][227910] New mean coefficients: [[3.3084612 0.7694473 0.6441164 0.5731795 0.5041564]][0m
[37m[1m[2023-07-10 11:29:23,000][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:29:32,814][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 11:29:32,815][227910] FPS: 391318.71[0m
[36m[2023-07-10 11:29:32,817][227910] itr=269, itrs=2000, Progress: 13.45%[0m
[36m[2023-07-10 11:29:44,487][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 11:29:44,487][227910] FPS: 329478.27[0m
[36m[2023-07-10 11:29:49,345][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:29:49,350][227910] Reward + Measures: [[5038.95444885    0.26124781    0.28032091    0.23432942    0.04005279]][0m
[37m[1m[2023-07-10 11:29:49,351][227910] Max Reward on eval: 5038.954448851396[0m
[37m[1m[2023-07-10 11:29:49,351][227910] Min Reward on eval: 5038.954448851396[0m
[37m[1m[2023-07-10 11:29:49,351][227910] Mean Reward across all agents: 5038.954448851396[0m
[37m[1m[2023-07-10 11:29:49,352][227910] Average Trajectory Length: 988.808[0m
[36m[2023-07-10 11:29:54,830][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:29:54,835][227910] Reward + Measures: [[ 700.33295825    0.22222991    0.29003268    0.21529758    0.15258217]
 [2215.08896481    0.29641578    0.37794209    0.27252379    0.18316761]
 [1788.02302478    0.39199999    0.34330001    0.32640001    0.25419998]
 ...
 [ 801.87023777    0.24094574    0.29347077    0.22098957    0.11192452]
 [ 630.21232044    0.3017        0.47299996    0.27689999    0.34020001]
 [1295.49879746    0.37670001    0.44119999    0.32259998    0.30550003]][0m
[37m[1m[2023-07-10 11:29:54,835][227910] Max Reward on eval: 4076.8913349293407[0m
[37m[1m[2023-07-10 11:29:54,836][227910] Min Reward on eval: -419.96338758377243[0m
[37m[1m[2023-07-10 11:29:54,836][227910] Mean Reward across all agents: 1105.4793442080202[0m
[37m[1m[2023-07-10 11:29:54,836][227910] Average Trajectory Length: 810.2063333333333[0m
[36m[2023-07-10 11:29:54,838][227910] mean_value=-1773.731866287783, max_value=1320.4395044221446[0m
[37m[1m[2023-07-10 11:29:54,841][227910] New mean coefficients: [[ 2.5779154  -0.08588213 -0.193371    1.6428908   0.6280419 ]][0m
[37m[1m[2023-07-10 11:29:54,842][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:30:04,572][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 11:30:04,572][227910] FPS: 394718.88[0m
[36m[2023-07-10 11:30:04,575][227910] itr=270, itrs=2000, Progress: 13.50%[0m
[37m[1m[2023-07-10 11:30:06,855][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000250[0m
[36m[2023-07-10 11:30:18,868][227910] train() took 11.75 seconds to complete[0m
[36m[2023-07-10 11:30:18,868][227910] FPS: 326911.84[0m
[36m[2023-07-10 11:30:23,669][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:30:23,670][227910] Reward + Measures: [[5194.89195848    0.25776961    0.2765308     0.23290917    0.04013341]][0m
[37m[1m[2023-07-10 11:30:23,670][227910] Max Reward on eval: 5194.891958482359[0m
[37m[1m[2023-07-10 11:30:23,670][227910] Min Reward on eval: 5194.891958482359[0m
[37m[1m[2023-07-10 11:30:23,670][227910] Mean Reward across all agents: 5194.891958482359[0m
[37m[1m[2023-07-10 11:30:23,671][227910] Average Trajectory Length: 989.3829999999999[0m
[36m[2023-07-10 11:30:29,136][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:30:29,137][227910] Reward + Measures: [[ 712.63811164    0.32070038    0.31946328    0.32568434    0.19274376]
 [ -98.55113344    0.18851341    0.42834893    0.29455757    0.45009002]
 [ 154.5721407     0.30289999    0.345         0.32319999    0.27040002]
 ...
 [3257.11509234    0.27964458    0.26061019    0.25396782    0.09130155]
 [1436.34292773    0.26359999    0.27840003    0.2577        0.14210001]
 [ 295.45642235    0.32590002    0.43129998    0.24170001    0.33580002]][0m
[37m[1m[2023-07-10 11:30:29,137][227910] Max Reward on eval: 4778.198606488435[0m
[37m[1m[2023-07-10 11:30:29,137][227910] Min Reward on eval: -739.8203087004716[0m
[37m[1m[2023-07-10 11:30:29,138][227910] Mean Reward across all agents: 1178.9514741306562[0m
[37m[1m[2023-07-10 11:30:29,138][227910] Average Trajectory Length: 868.1303333333333[0m
[36m[2023-07-10 11:30:29,140][227910] mean_value=-1914.1933668283461, max_value=1334.5624543913136[0m
[37m[1m[2023-07-10 11:30:29,143][227910] New mean coefficients: [[ 2.8287554  -0.77932644  0.11029279  0.47309363  0.23881617]][0m
[37m[1m[2023-07-10 11:30:29,144][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:30:38,880][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 11:30:38,880][227910] FPS: 394468.50[0m
[36m[2023-07-10 11:30:38,882][227910] itr=271, itrs=2000, Progress: 13.55%[0m
[36m[2023-07-10 11:30:50,492][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 11:30:50,492][227910] FPS: 331180.37[0m
[36m[2023-07-10 11:30:55,360][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:30:55,365][227910] Reward + Measures: [[4096.7888731     0.27605101    0.27871871    0.24055994    0.11168431]][0m
[37m[1m[2023-07-10 11:30:55,366][227910] Max Reward on eval: 4096.7888730952245[0m
[37m[1m[2023-07-10 11:30:55,366][227910] Min Reward on eval: 4096.7888730952245[0m
[37m[1m[2023-07-10 11:30:55,366][227910] Mean Reward across all agents: 4096.7888730952245[0m
[37m[1m[2023-07-10 11:30:55,367][227910] Average Trajectory Length: 948.4893333333333[0m
[36m[2023-07-10 11:31:01,015][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:31:01,021][227910] Reward + Measures: [[2342.55735481    0.25479797    0.33011225    0.23959796    0.1692694 ]
 [1727.7696806     0.3407        0.32619998    0.27650002    0.1365    ]
 [1574.11245195    0.25031692    0.33491415    0.26960501    0.21415937]
 ...
 [2128.19039402    0.28937301    0.30554596    0.24052432    0.10661081]
 [ 465.39191879    0.3113457     0.33375898    0.26147988    0.17706938]
 [ 250.96833572    0.2024916     0.30082151    0.18676074    0.21313739]][0m
[37m[1m[2023-07-10 11:31:01,021][227910] Max Reward on eval: 4440.046259629633[0m
[37m[1m[2023-07-10 11:31:01,021][227910] Min Reward on eval: -721.416715998901[0m
[37m[1m[2023-07-10 11:31:01,021][227910] Mean Reward across all agents: 1491.4926231988065[0m
[37m[1m[2023-07-10 11:31:01,022][227910] Average Trajectory Length: 883.074[0m
[36m[2023-07-10 11:31:01,023][227910] mean_value=-1743.464916435964, max_value=1929.6582177563248[0m
[37m[1m[2023-07-10 11:31:01,026][227910] New mean coefficients: [[ 2.778764   -0.2832734   0.7873443   0.48091674  0.69287425]][0m
[37m[1m[2023-07-10 11:31:01,027][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:31:10,774][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 11:31:10,774][227910] FPS: 394053.46[0m
[36m[2023-07-10 11:31:10,776][227910] itr=272, itrs=2000, Progress: 13.60%[0m
[36m[2023-07-10 11:31:22,390][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 11:31:22,390][227910] FPS: 331137.25[0m
[36m[2023-07-10 11:31:27,180][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:31:27,186][227910] Reward + Measures: [[4473.8165521     0.2777687     0.27347147    0.24343677    0.09959203]][0m
[37m[1m[2023-07-10 11:31:27,186][227910] Max Reward on eval: 4473.816552103847[0m
[37m[1m[2023-07-10 11:31:27,186][227910] Min Reward on eval: 4473.816552103847[0m
[37m[1m[2023-07-10 11:31:27,187][227910] Mean Reward across all agents: 4473.816552103847[0m
[37m[1m[2023-07-10 11:31:27,187][227910] Average Trajectory Length: 973.827[0m
[36m[2023-07-10 11:31:32,665][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:31:32,670][227910] Reward + Measures: [[ 420.11018397    0.36050001    0.35249999    0.32620001    0.21250001]
 [4120.45088529    0.28774855    0.26708141    0.24789818    0.10873592]
 [ 230.53166195    0.22793567    0.29449016    0.15982789    0.11963353]
 ...
 [2279.27292361    0.27080002    0.31740001    0.24559999    0.146     ]
 [   4.72799229    0.19892855    0.2598339     0.15272824    0.11233236]
 [3632.70016693    0.27793163    0.34239745    0.27093163    0.13748462]][0m
[37m[1m[2023-07-10 11:31:32,670][227910] Max Reward on eval: 4711.533309236402[0m
[37m[1m[2023-07-10 11:31:32,670][227910] Min Reward on eval: -396.3387624393217[0m
[37m[1m[2023-07-10 11:31:32,671][227910] Mean Reward across all agents: 1948.1981555598159[0m
[37m[1m[2023-07-10 11:31:32,671][227910] Average Trajectory Length: 821.5699999999999[0m
[36m[2023-07-10 11:31:32,673][227910] mean_value=-1730.7953099102972, max_value=780.2151060606311[0m
[37m[1m[2023-07-10 11:31:32,675][227910] New mean coefficients: [[ 2.5005305  -0.19055673  0.29613918  1.0411384   0.96680623]][0m
[37m[1m[2023-07-10 11:31:32,676][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:31:42,310][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 11:31:42,310][227910] FPS: 398649.71[0m
[36m[2023-07-10 11:31:42,313][227910] itr=273, itrs=2000, Progress: 13.65%[0m
[36m[2023-07-10 11:31:53,915][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 11:31:53,915][227910] FPS: 331385.87[0m
[36m[2023-07-10 11:31:58,752][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:31:58,752][227910] Reward + Measures: [[4750.44279959    0.27750492    0.27151996    0.24428141    0.09077669]][0m
[37m[1m[2023-07-10 11:31:58,753][227910] Max Reward on eval: 4750.442799589028[0m
[37m[1m[2023-07-10 11:31:58,753][227910] Min Reward on eval: 4750.442799589028[0m
[37m[1m[2023-07-10 11:31:58,753][227910] Mean Reward across all agents: 4750.442799589028[0m
[37m[1m[2023-07-10 11:31:58,753][227910] Average Trajectory Length: 987.692[0m
[36m[2023-07-10 11:32:04,271][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:32:04,277][227910] Reward + Measures: [[2803.97373351    0.29377505    0.2927033     0.26485512    0.1361727 ]
 [2740.27896181    0.2196167     0.33428016    0.22991143    0.14207099]
 [2908.01651893    0.27625132    0.29571301    0.25704017    0.12387955]
 ...
 [1799.98176873    0.26655698    0.26629463    0.22010647    0.1129527 ]
 [1172.3024368     0.20585875    0.27386591    0.24922191    0.18999594]
 [ 144.59153021    0.29820001    0.3414        0.31119999    0.27479997]][0m
[37m[1m[2023-07-10 11:32:04,277][227910] Max Reward on eval: 4932.682657912746[0m
[37m[1m[2023-07-10 11:32:04,277][227910] Min Reward on eval: -979.0713990342571[0m
[37m[1m[2023-07-10 11:32:04,277][227910] Mean Reward across all agents: 1248.9187372633598[0m
[37m[1m[2023-07-10 11:32:04,277][227910] Average Trajectory Length: 882.6909999999999[0m
[36m[2023-07-10 11:32:04,279][227910] mean_value=-1591.0270035515653, max_value=1225.5451560453885[0m
[37m[1m[2023-07-10 11:32:04,282][227910] New mean coefficients: [[ 2.6380134  -0.4561574   0.14242108  1.2887051   1.2211058 ]][0m
[37m[1m[2023-07-10 11:32:04,282][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:32:14,106][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 11:32:14,106][227910] FPS: 390963.27[0m
[36m[2023-07-10 11:32:14,109][227910] itr=274, itrs=2000, Progress: 13.70%[0m
[36m[2023-07-10 11:32:25,687][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 11:32:25,687][227910] FPS: 332079.93[0m
[36m[2023-07-10 11:32:30,598][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:32:30,603][227910] Reward + Measures: [[4962.14333834    0.27718881    0.26079148    0.24415824    0.08135817]][0m
[37m[1m[2023-07-10 11:32:30,604][227910] Max Reward on eval: 4962.14333833806[0m
[37m[1m[2023-07-10 11:32:30,604][227910] Min Reward on eval: 4962.14333833806[0m
[37m[1m[2023-07-10 11:32:30,604][227910] Mean Reward across all agents: 4962.14333833806[0m
[37m[1m[2023-07-10 11:32:30,604][227910] Average Trajectory Length: 988.3439999999999[0m
[36m[2023-07-10 11:32:36,099][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:32:36,100][227910] Reward + Measures: [[2052.98793268    0.26407534    0.40386447    0.26659328    0.20847706]
 [ 843.55217806    0.29089999    0.30790001    0.30820003    0.2683    ]
 [ 606.11823521    0.31419301    0.32133785    0.32914367    0.20185988]
 ...
 [1859.029351      0.25224516    0.28045866    0.24223952    0.16288306]
 [-251.37062444    0.31640109    0.34860435    0.32541391    0.20652412]
 [ 439.92910287    0.23232041    0.28906354    0.24712925    0.09848615]][0m
[37m[1m[2023-07-10 11:32:36,100][227910] Max Reward on eval: 4538.842966571508[0m
[37m[1m[2023-07-10 11:32:36,100][227910] Min Reward on eval: -679.8007793836762[0m
[37m[1m[2023-07-10 11:32:36,101][227910] Mean Reward across all agents: 1056.9114794425648[0m
[37m[1m[2023-07-10 11:32:36,101][227910] Average Trajectory Length: 789.7573333333333[0m
[36m[2023-07-10 11:32:36,103][227910] mean_value=-2332.428051585005, max_value=1141.957500590641[0m
[37m[1m[2023-07-10 11:32:36,105][227910] New mean coefficients: [[ 3.188136   -0.54736805  1.2756165   0.43786508  0.39976025]][0m
[37m[1m[2023-07-10 11:32:36,106][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:32:45,866][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 11:32:45,866][227910] FPS: 393538.36[0m
[36m[2023-07-10 11:32:45,868][227910] itr=275, itrs=2000, Progress: 13.75%[0m
[36m[2023-07-10 11:32:57,435][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 11:32:57,435][227910] FPS: 332442.68[0m
[36m[2023-07-10 11:33:02,228][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:33:02,234][227910] Reward + Measures: [[5171.33490662    0.27506149    0.26315239    0.24453418    0.07021013]][0m
[37m[1m[2023-07-10 11:33:02,234][227910] Max Reward on eval: 5171.334906616095[0m
[37m[1m[2023-07-10 11:33:02,234][227910] Min Reward on eval: 5171.334906616095[0m
[37m[1m[2023-07-10 11:33:02,234][227910] Mean Reward across all agents: 5171.334906616095[0m
[37m[1m[2023-07-10 11:33:02,235][227910] Average Trajectory Length: 993.2003333333333[0m
[36m[2023-07-10 11:33:07,908][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:33:07,909][227910] Reward + Measures: [[ 593.07605656    0.22694731    0.28234261    0.27449259    0.18417697]
 [1149.98318231    0.30121413    0.26199168    0.24101849    0.15213346]
 [ 714.69489959    0.31301624    0.28196216    0.28548619    0.14982596]
 ...
 [1714.6373041     0.26441416    0.37550923    0.24473706    0.15003224]
 [1543.95820424    0.261536      0.29160818    0.20991805    0.09594285]
 [2599.88753893    0.23896538    0.36769387    0.29683298    0.06636592]][0m
[37m[1m[2023-07-10 11:33:07,909][227910] Max Reward on eval: 5164.536114479229[0m
[37m[1m[2023-07-10 11:33:07,909][227910] Min Reward on eval: -731.552544758073[0m
[37m[1m[2023-07-10 11:33:07,909][227910] Mean Reward across all agents: 1721.3246907890436[0m
[37m[1m[2023-07-10 11:33:07,910][227910] Average Trajectory Length: 866.424[0m
[36m[2023-07-10 11:33:07,912][227910] mean_value=-1744.4622691329294, max_value=1517.7798193712133[0m
[37m[1m[2023-07-10 11:33:07,914][227910] New mean coefficients: [[ 3.0125334  -0.19812736  0.8865102  -0.3766049  -0.10948622]][0m
[37m[1m[2023-07-10 11:33:07,915][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:33:17,601][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 11:33:17,601][227910] FPS: 396540.55[0m
[36m[2023-07-10 11:33:17,603][227910] itr=276, itrs=2000, Progress: 13.80%[0m
[36m[2023-07-10 11:33:29,068][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 11:33:29,068][227910] FPS: 335370.06[0m
[36m[2023-07-10 11:33:33,779][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:33:33,779][227910] Reward + Measures: [[3951.94261263    0.28300181    0.29879299    0.25181779    0.08980752]][0m
[37m[1m[2023-07-10 11:33:33,779][227910] Max Reward on eval: 3951.9426126322514[0m
[37m[1m[2023-07-10 11:33:33,779][227910] Min Reward on eval: 3951.9426126322514[0m
[37m[1m[2023-07-10 11:33:33,780][227910] Mean Reward across all agents: 3951.9426126322514[0m
[37m[1m[2023-07-10 11:33:33,780][227910] Average Trajectory Length: 989.353[0m
[36m[2023-07-10 11:33:39,311][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:33:39,312][227910] Reward + Measures: [[1446.58643295    0.30672449    0.37195197    0.23308297    0.20749085]
 [ 783.3966579     0.19807883    0.30027294    0.23822759    0.08617454]
 [1129.00155651    0.30508777    0.26421478    0.23094718    0.22128916]
 ...
 [1253.44200631    0.25452074    0.33893782    0.2037437     0.1568507 ]
 [ 765.1918754     0.35717255    0.30096266    0.33128023    0.34088239]
 [1565.30367637    0.27083594    0.3607963     0.2272815     0.1182422 ]][0m
[37m[1m[2023-07-10 11:33:39,312][227910] Max Reward on eval: 4261.829532792571[0m
[37m[1m[2023-07-10 11:33:39,312][227910] Min Reward on eval: -1196.07700082456[0m
[37m[1m[2023-07-10 11:33:39,312][227910] Mean Reward across all agents: 1330.038907457388[0m
[37m[1m[2023-07-10 11:33:39,313][227910] Average Trajectory Length: 812.3133333333333[0m
[36m[2023-07-10 11:33:39,315][227910] mean_value=-1583.8355824754826, max_value=2554.7918467359464[0m
[37m[1m[2023-07-10 11:33:39,317][227910] New mean coefficients: [[ 3.0519855  -0.2118304   0.56549597 -0.27614266  0.76574415]][0m
[37m[1m[2023-07-10 11:33:39,318][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:33:49,105][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 11:33:49,105][227910] FPS: 392430.93[0m
[36m[2023-07-10 11:33:49,107][227910] itr=277, itrs=2000, Progress: 13.85%[0m
[36m[2023-07-10 11:34:00,690][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 11:34:00,691][227910] FPS: 331970.03[0m
[36m[2023-07-10 11:34:05,528][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:34:05,534][227910] Reward + Measures: [[4502.79365907    0.2794089     0.29317313    0.24379781    0.05798685]][0m
[37m[1m[2023-07-10 11:34:05,534][227910] Max Reward on eval: 4502.793659066383[0m
[37m[1m[2023-07-10 11:34:05,534][227910] Min Reward on eval: 4502.793659066383[0m
[37m[1m[2023-07-10 11:34:05,535][227910] Mean Reward across all agents: 4502.793659066383[0m
[37m[1m[2023-07-10 11:34:05,535][227910] Average Trajectory Length: 995.4343333333333[0m
[36m[2023-07-10 11:34:11,022][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:34:11,023][227910] Reward + Measures: [[2007.57653714    0.23621045    0.28102151    0.22380006    0.12715594]
 [ 412.54952777    0.23370922    0.32885787    0.21019129    0.26732984]
 [1205.52548978    0.29269999    0.34829998    0.2509        0.26299998]
 ...
 [2926.823781      0.34163871    0.23776303    0.2516008     0.11870172]
 [1820.66482026    0.25797907    0.28281233    0.28479478    0.15401633]
 [2415.97089401    0.28838307    0.34080157    0.26025483    0.15945052]][0m
[37m[1m[2023-07-10 11:34:11,023][227910] Max Reward on eval: 4296.363853008673[0m
[37m[1m[2023-07-10 11:34:11,023][227910] Min Reward on eval: -591.4105123070069[0m
[37m[1m[2023-07-10 11:34:11,023][227910] Mean Reward across all agents: 1381.3206001162202[0m
[37m[1m[2023-07-10 11:34:11,024][227910] Average Trajectory Length: 896.4766666666667[0m
[36m[2023-07-10 11:34:11,026][227910] mean_value=-1679.5283599651875, max_value=2303.7688356924[0m
[37m[1m[2023-07-10 11:34:11,028][227910] New mean coefficients: [[ 3.0492232   0.38429764  0.37120435 -0.6436249   1.1222454 ]][0m
[37m[1m[2023-07-10 11:34:11,029][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:34:20,959][227910] train() took 9.93 seconds to complete[0m
[36m[2023-07-10 11:34:20,959][227910] FPS: 386783.13[0m
[36m[2023-07-10 11:34:20,962][227910] itr=278, itrs=2000, Progress: 13.90%[0m
[36m[2023-07-10 11:34:32,497][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 11:34:32,497][227910] FPS: 333359.56[0m
[36m[2023-07-10 11:34:37,205][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:34:37,210][227910] Reward + Measures: [[4755.37703163    0.27525601    0.29122394    0.24033055    0.0476769 ]][0m
[37m[1m[2023-07-10 11:34:37,210][227910] Max Reward on eval: 4755.377031629227[0m
[37m[1m[2023-07-10 11:34:37,211][227910] Min Reward on eval: 4755.377031629227[0m
[37m[1m[2023-07-10 11:34:37,211][227910] Mean Reward across all agents: 4755.377031629227[0m
[37m[1m[2023-07-10 11:34:37,211][227910] Average Trajectory Length: 995.9616666666666[0m
[36m[2023-07-10 11:34:42,689][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:34:42,695][227910] Reward + Measures: [[-426.73421056    0.70649999    0.43990001    0.64450002    0.0971    ]
 [ 606.76378687    0.20319417    0.32398051    0.20705231    0.22008629]
 [ 725.80623899    0.27770546    0.35686645    0.17735532    0.20915285]
 ...
 [1913.22527594    0.22597595    0.32959104    0.29244861    0.17309847]
 [ 181.21251875    0.22902369    0.3155385     0.21544886    0.27913192]
 [2105.34094927    0.35848388    0.36418712    0.31640968    0.17448387]][0m
[37m[1m[2023-07-10 11:34:42,695][227910] Max Reward on eval: 4695.903246029467[0m
[37m[1m[2023-07-10 11:34:42,696][227910] Min Reward on eval: -1063.3842134161619[0m
[37m[1m[2023-07-10 11:34:42,696][227910] Mean Reward across all agents: 986.0579387897327[0m
[37m[1m[2023-07-10 11:34:42,696][227910] Average Trajectory Length: 827.2393333333333[0m
[36m[2023-07-10 11:34:42,698][227910] mean_value=-1389.4140050000665, max_value=1580.0193192393576[0m
[37m[1m[2023-07-10 11:34:42,701][227910] New mean coefficients: [[ 3.0518672   0.60842144 -0.8208729  -0.27656993  1.4309752 ]][0m
[37m[1m[2023-07-10 11:34:42,702][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:34:52,505][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 11:34:52,506][227910] FPS: 391768.57[0m
[36m[2023-07-10 11:34:52,508][227910] itr=279, itrs=2000, Progress: 13.95%[0m
[36m[2023-07-10 11:35:04,259][227910] train() took 11.74 seconds to complete[0m
[36m[2023-07-10 11:35:04,260][227910] FPS: 327175.94[0m
[36m[2023-07-10 11:35:09,073][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:35:09,073][227910] Reward + Measures: [[5012.73805817    0.27794087    0.27860209    0.23999849    0.04042716]][0m
[37m[1m[2023-07-10 11:35:09,074][227910] Max Reward on eval: 5012.7380581687[0m
[37m[1m[2023-07-10 11:35:09,074][227910] Min Reward on eval: 5012.7380581687[0m
[37m[1m[2023-07-10 11:35:09,074][227910] Mean Reward across all agents: 5012.7380581687[0m
[37m[1m[2023-07-10 11:35:09,075][227910] Average Trajectory Length: 998.6086666666666[0m
[36m[2023-07-10 11:35:14,601][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:35:14,602][227910] Reward + Measures: [[1856.72169894    0.22168235    0.2806243     0.23796265    0.12888061]
 [ -37.30879307    0.35800001    0.46809998    0.2254        0.32249999]
 [ 454.58737336    0.23940377    0.2787008     0.27088743    0.13845041]
 ...
 [ 669.4499888     0.43930003    0.58430004    0.255         0.37100002]
 [2031.5115612     0.21715203    0.23045801    0.23254156    0.10335872]
 [2780.00830199    0.27181858    0.34373713    0.26636872    0.1426305 ]][0m
[37m[1m[2023-07-10 11:35:14,602][227910] Max Reward on eval: 4789.622270805575[0m
[37m[1m[2023-07-10 11:35:14,602][227910] Min Reward on eval: -1024.1080776541844[0m
[37m[1m[2023-07-10 11:35:14,602][227910] Mean Reward across all agents: 1044.0544224809203[0m
[37m[1m[2023-07-10 11:35:14,603][227910] Average Trajectory Length: 885.2736666666666[0m
[36m[2023-07-10 11:35:14,604][227910] mean_value=-1897.8991536758147, max_value=2083.7067631281807[0m
[37m[1m[2023-07-10 11:35:14,607][227910] New mean coefficients: [[ 3.104508    0.4614971  -0.81810606  0.5808922   1.2841808 ]][0m
[37m[1m[2023-07-10 11:35:14,608][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:35:24,513][227910] train() took 9.90 seconds to complete[0m
[36m[2023-07-10 11:35:24,513][227910] FPS: 387735.82[0m
[36m[2023-07-10 11:35:24,516][227910] itr=280, itrs=2000, Progress: 14.00%[0m
[37m[1m[2023-07-10 11:35:26,928][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000260[0m
[36m[2023-07-10 11:35:38,803][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 11:35:38,803][227910] FPS: 330972.41[0m
[36m[2023-07-10 11:35:43,567][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:35:43,567][227910] Reward + Measures: [[5303.02494383    0.27339971    0.27657261    0.23579606    0.03164382]][0m
[37m[1m[2023-07-10 11:35:43,567][227910] Max Reward on eval: 5303.024943829157[0m
[37m[1m[2023-07-10 11:35:43,568][227910] Min Reward on eval: 5303.024943829157[0m
[37m[1m[2023-07-10 11:35:43,568][227910] Mean Reward across all agents: 5303.024943829157[0m
[37m[1m[2023-07-10 11:35:43,568][227910] Average Trajectory Length: 998.963[0m
[36m[2023-07-10 11:35:49,070][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:35:49,076][227910] Reward + Measures: [[3081.32609151    0.25250003    0.31510001    0.23119998    0.17220001]
 [2371.60223738    0.21042858    0.26815924    0.19565189    0.11920475]
 [1321.45222324    0.33881029    0.36412913    0.28312814    0.20245266]
 ...
 [ 332.84397269    0.18965434    0.29133186    0.21823584    0.21766661]
 [ 235.83680641    0.36059901    0.17888992    0.37113032    0.25077477]
 [ 381.00020441    0.41731045    0.33144099    0.37047866    0.38797897]][0m
[37m[1m[2023-07-10 11:35:49,077][227910] Max Reward on eval: 5085.708390148449[0m
[37m[1m[2023-07-10 11:35:49,078][227910] Min Reward on eval: -1481.8684871889418[0m
[37m[1m[2023-07-10 11:35:49,078][227910] Mean Reward across all agents: 1088.4921570457368[0m
[37m[1m[2023-07-10 11:35:49,079][227910] Average Trajectory Length: 901.2006666666666[0m
[36m[2023-07-10 11:35:49,087][227910] mean_value=-863.8244789537889, max_value=1929.2300129767418[0m
[37m[1m[2023-07-10 11:35:49,091][227910] New mean coefficients: [[ 2.8246632   0.10894585 -0.73636353  1.054826    1.4985886 ]][0m
[37m[1m[2023-07-10 11:35:49,093][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:35:58,817][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 11:35:58,817][227910] FPS: 394990.09[0m
[36m[2023-07-10 11:35:58,819][227910] itr=281, itrs=2000, Progress: 14.05%[0m
[36m[2023-07-10 11:36:10,398][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 11:36:10,399][227910] FPS: 332057.13[0m
[36m[2023-07-10 11:36:15,227][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:36:15,227][227910] Reward + Measures: [[5468.78956977    0.26846835    0.26266679    0.23391072    0.0279454 ]][0m
[37m[1m[2023-07-10 11:36:15,227][227910] Max Reward on eval: 5468.789569773064[0m
[37m[1m[2023-07-10 11:36:15,227][227910] Min Reward on eval: 5468.789569773064[0m
[37m[1m[2023-07-10 11:36:15,227][227910] Mean Reward across all agents: 5468.789569773064[0m
[37m[1m[2023-07-10 11:36:15,228][227910] Average Trajectory Length: 998.6836666666667[0m
[36m[2023-07-10 11:36:20,797][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:36:20,798][227910] Reward + Measures: [[1482.81597597    0.22987042    0.29431328    0.21626925    0.12833607]
 [2501.72628461    0.24960208    0.29586491    0.21606548    0.1123374 ]
 [1432.22087101    0.2438778     0.27508959    0.22476268    0.09796567]
 ...
 [5075.30944544    0.26009998    0.26589999    0.22640002    0.0246    ]
 [2862.32657937    0.27481937    0.25412866    0.23288178    0.06473059]
 [3236.21578856    0.22333741    0.30345598    0.2052023     0.04779674]][0m
[37m[1m[2023-07-10 11:36:20,798][227910] Max Reward on eval: 5230.334036893398[0m
[37m[1m[2023-07-10 11:36:20,798][227910] Min Reward on eval: -721.6353810448549[0m
[37m[1m[2023-07-10 11:36:20,798][227910] Mean Reward across all agents: 1802.8641422212247[0m
[37m[1m[2023-07-10 11:36:20,799][227910] Average Trajectory Length: 755.8776666666666[0m
[36m[2023-07-10 11:36:20,801][227910] mean_value=-2080.901666823342, max_value=2305.334989906682[0m
[37m[1m[2023-07-10 11:36:20,803][227910] New mean coefficients: [[2.6675124  0.20839277 0.34378004 0.5061187  1.3744385 ]][0m
[37m[1m[2023-07-10 11:36:20,804][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:36:30,510][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 11:36:30,510][227910] FPS: 395714.03[0m
[36m[2023-07-10 11:36:30,512][227910] itr=282, itrs=2000, Progress: 14.10%[0m
[36m[2023-07-10 11:36:42,162][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 11:36:42,163][227910] FPS: 330034.33[0m
[36m[2023-07-10 11:36:46,909][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:36:46,909][227910] Reward + Measures: [[5652.80333228    0.26297587    0.25832826    0.23210923    0.02567658]][0m
[37m[1m[2023-07-10 11:36:46,909][227910] Max Reward on eval: 5652.80333227742[0m
[37m[1m[2023-07-10 11:36:46,909][227910] Min Reward on eval: 5652.80333227742[0m
[37m[1m[2023-07-10 11:36:46,910][227910] Mean Reward across all agents: 5652.80333227742[0m
[37m[1m[2023-07-10 11:36:46,910][227910] Average Trajectory Length: 998.1659999999999[0m
[36m[2023-07-10 11:36:52,408][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:36:52,409][227910] Reward + Measures: [[1150.35132892    0.30962247    0.24694668    0.25920391    0.19223122]
 [ 579.27098808    0.2170081     0.27082139    0.22085607    0.25922081]
 [ -65.87260776    0.31132141    0.2918365     0.2420003     0.35545826]
 ...
 [ 224.44868286    0.25773045    0.29914424    0.2720736     0.2141068 ]
 [1447.11157025    0.31095687    0.30725551    0.30313945    0.1557807 ]
 [1564.14991392    0.3064        0.29530001    0.29179999    0.15620001]][0m
[37m[1m[2023-07-10 11:36:52,409][227910] Max Reward on eval: 5303.098427188956[0m
[37m[1m[2023-07-10 11:36:52,409][227910] Min Reward on eval: -1230.235433565313[0m
[37m[1m[2023-07-10 11:36:52,410][227910] Mean Reward across all agents: 739.6319630550795[0m
[37m[1m[2023-07-10 11:36:52,410][227910] Average Trajectory Length: 875.8436666666666[0m
[36m[2023-07-10 11:36:52,414][227910] mean_value=-1033.9835642041849, max_value=1954.6693800841485[0m
[37m[1m[2023-07-10 11:36:52,417][227910] New mean coefficients: [[ 2.347274    0.08432347 -0.44660646  0.58766663  1.509605  ]][0m
[37m[1m[2023-07-10 11:36:52,418][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:37:02,072][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 11:37:02,073][227910] FPS: 397800.82[0m
[36m[2023-07-10 11:37:02,075][227910] itr=283, itrs=2000, Progress: 14.15%[0m
[36m[2023-07-10 11:37:13,533][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 11:37:13,534][227910] FPS: 335568.42[0m
[36m[2023-07-10 11:37:18,338][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:37:18,338][227910] Reward + Measures: [[5739.17593836    0.26982629    0.24410231    0.23351738    0.02569313]][0m
[37m[1m[2023-07-10 11:37:18,339][227910] Max Reward on eval: 5739.1759383621065[0m
[37m[1m[2023-07-10 11:37:18,339][227910] Min Reward on eval: 5739.1759383621065[0m
[37m[1m[2023-07-10 11:37:18,339][227910] Mean Reward across all agents: 5739.1759383621065[0m
[37m[1m[2023-07-10 11:37:18,339][227910] Average Trajectory Length: 997.535[0m
[36m[2023-07-10 11:37:23,930][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:37:23,935][227910] Reward + Measures: [[ 676.22733869    0.24802974    0.29236206    0.29921249    0.18045552]
 [ 240.19051287    0.16791753    0.23396869    0.18642686    0.10887702]
 [ 553.47272382    0.22747336    0.22969981    0.23136716    0.13342731]
 ...
 [  17.02157053    0.70557547    0.1509347     0.66696733    0.53140819]
 [1869.59649665    0.30003157    0.24881403    0.27267545    0.19582048]
 [-601.81844933    0.1534        0.26480004    0.25229999    0.24790001]][0m
[37m[1m[2023-07-10 11:37:23,936][227910] Max Reward on eval: 4461.660662002396[0m
[37m[1m[2023-07-10 11:37:23,936][227910] Min Reward on eval: -1452.5066728946053[0m
[37m[1m[2023-07-10 11:37:23,936][227910] Mean Reward across all agents: 486.0697297827779[0m
[37m[1m[2023-07-10 11:37:23,936][227910] Average Trajectory Length: 884.0193333333333[0m
[36m[2023-07-10 11:37:23,941][227910] mean_value=-1071.6330192424243, max_value=4387.512949176416[0m
[37m[1m[2023-07-10 11:37:23,944][227910] New mean coefficients: [[ 2.820501   -0.08227214 -0.7710922  -0.30615634  1.2528942 ]][0m
[37m[1m[2023-07-10 11:37:23,945][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:37:33,587][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 11:37:33,587][227910] FPS: 398317.28[0m
[36m[2023-07-10 11:37:33,590][227910] itr=284, itrs=2000, Progress: 14.20%[0m
[36m[2023-07-10 11:37:45,287][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 11:37:45,287][227910] FPS: 328766.13[0m
[36m[2023-07-10 11:37:50,185][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:37:50,191][227910] Reward + Measures: [[5859.27008303    0.26464385    0.24474996    0.23256689    0.0234918 ]][0m
[37m[1m[2023-07-10 11:37:50,191][227910] Max Reward on eval: 5859.270083030508[0m
[37m[1m[2023-07-10 11:37:50,191][227910] Min Reward on eval: 5859.270083030508[0m
[37m[1m[2023-07-10 11:37:50,192][227910] Mean Reward across all agents: 5859.270083030508[0m
[37m[1m[2023-07-10 11:37:50,192][227910] Average Trajectory Length: 998.7326666666667[0m
[36m[2023-07-10 11:37:55,671][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:37:55,671][227910] Reward + Measures: [[2038.70009347    0.25061235    0.21772571    0.24890423    0.14503108]
 [2501.80397211    0.26166561    0.20947316    0.26619014    0.12707508]
 [4342.21329793    0.28103051    0.18930946    0.22546051    0.03697846]
 ...
 [ 579.86665564    0.27205399    0.36860353    0.21781173    0.23345616]
 [ 239.56829376    0.26269999    0.30589998    0.1978        0.2282    ]
 [ 516.38566649    0.22282839    0.21689296    0.21860777    0.11184795]][0m
[37m[1m[2023-07-10 11:37:55,672][227910] Max Reward on eval: 4943.91573339384[0m
[37m[1m[2023-07-10 11:37:55,672][227910] Min Reward on eval: -1221.5565120035433[0m
[37m[1m[2023-07-10 11:37:55,672][227910] Mean Reward across all agents: 909.4398571511067[0m
[37m[1m[2023-07-10 11:37:55,672][227910] Average Trajectory Length: 862.4713333333333[0m
[36m[2023-07-10 11:37:55,675][227910] mean_value=-1760.2153048649193, max_value=2518.9231511931575[0m
[37m[1m[2023-07-10 11:37:55,678][227910] New mean coefficients: [[ 2.5049343  -0.06646764 -0.44164088  0.6125706   1.3938082 ]][0m
[37m[1m[2023-07-10 11:37:55,679][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:38:05,467][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 11:38:05,467][227910] FPS: 392383.03[0m
[36m[2023-07-10 11:38:05,470][227910] itr=285, itrs=2000, Progress: 14.25%[0m
[36m[2023-07-10 11:38:17,027][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 11:38:17,027][227910] FPS: 332785.60[0m
[36m[2023-07-10 11:38:21,814][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:38:21,815][227910] Reward + Measures: [[5958.71451477    0.26129699    0.2504251     0.23056492    0.02617387]][0m
[37m[1m[2023-07-10 11:38:21,815][227910] Max Reward on eval: 5958.71451477378[0m
[37m[1m[2023-07-10 11:38:21,815][227910] Min Reward on eval: 5958.71451477378[0m
[37m[1m[2023-07-10 11:38:21,815][227910] Mean Reward across all agents: 5958.71451477378[0m
[37m[1m[2023-07-10 11:38:21,816][227910] Average Trajectory Length: 998.3766666666667[0m
[36m[2023-07-10 11:38:27,252][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:38:27,253][227910] Reward + Measures: [[-593.95685858    0.18166047    0.4523184     0.22847643    0.4055275 ]
 [ 375.74090355    0.2617932     0.26405087    0.18523231    0.18225431]
 [ -93.87524305    0.3193        0.28660002    0.2825        0.2985    ]
 ...
 [3232.34344266    0.3479        0.36039999    0.23539999    0.0653    ]
 [1350.78915328    0.32377896    0.28432974    0.2221012     0.13959207]
 [ 628.79917917    0.32194698    0.30533737    0.23200202    0.20709077]][0m
[37m[1m[2023-07-10 11:38:27,253][227910] Max Reward on eval: 5037.4184500314295[0m
[37m[1m[2023-07-10 11:38:27,253][227910] Min Reward on eval: -1519.9522990465398[0m
[37m[1m[2023-07-10 11:38:27,253][227910] Mean Reward across all agents: 522.2255675874545[0m
[37m[1m[2023-07-10 11:38:27,254][227910] Average Trajectory Length: 881.7566666666667[0m
[36m[2023-07-10 11:38:27,256][227910] mean_value=-1411.5758245876673, max_value=3148.134178375829[0m
[37m[1m[2023-07-10 11:38:27,258][227910] New mean coefficients: [[ 2.5684843  0.4719848 -0.5182775  0.295872   1.2495421]][0m
[37m[1m[2023-07-10 11:38:27,259][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:38:37,018][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 11:38:37,019][227910] FPS: 393521.16[0m
[36m[2023-07-10 11:38:37,021][227910] itr=286, itrs=2000, Progress: 14.30%[0m
[36m[2023-07-10 11:38:48,603][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 11:38:48,603][227910] FPS: 332039.20[0m
[36m[2023-07-10 11:38:53,317][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:38:53,318][227910] Reward + Measures: [[6069.01739489    0.25949374    0.24874751    0.2291919     0.0222505 ]][0m
[37m[1m[2023-07-10 11:38:53,318][227910] Max Reward on eval: 6069.017394888365[0m
[37m[1m[2023-07-10 11:38:53,318][227910] Min Reward on eval: 6069.017394888365[0m
[37m[1m[2023-07-10 11:38:53,318][227910] Mean Reward across all agents: 6069.017394888365[0m
[37m[1m[2023-07-10 11:38:53,318][227910] Average Trajectory Length: 998.5799999999999[0m
[36m[2023-07-10 11:38:58,878][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:38:58,879][227910] Reward + Measures: [[1587.59024994    0.27822539    0.29474258    0.30178288    0.18679284]
 [-396.12154711    0.30518195    0.38491949    0.26967135    0.30316082]
 [-813.33009608    0.22970001    0.4522        0.1979        0.3989    ]
 ...
 [ 146.73318298    0.2545        0.32980001    0.1664        0.2033    ]
 [4743.64872689    0.28239998    0.31689999    0.24220002    0.10470001]
 [ 327.62836957    0.24757104    0.33097866    0.20993602    0.15643826]][0m
[37m[1m[2023-07-10 11:38:58,879][227910] Max Reward on eval: 5381.204866785929[0m
[37m[1m[2023-07-10 11:38:58,879][227910] Min Reward on eval: -1258.439875881275[0m
[37m[1m[2023-07-10 11:38:58,879][227910] Mean Reward across all agents: 233.07794532970556[0m
[37m[1m[2023-07-10 11:38:58,880][227910] Average Trajectory Length: 784.5766666666666[0m
[36m[2023-07-10 11:38:58,882][227910] mean_value=-1503.4268524618622, max_value=773.087355842038[0m
[37m[1m[2023-07-10 11:38:58,884][227910] New mean coefficients: [[ 2.8423393   0.26305255 -0.0707517  -0.17034781  1.3566768 ]][0m
[37m[1m[2023-07-10 11:38:58,885][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:39:08,513][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 11:39:08,513][227910] FPS: 398906.84[0m
[36m[2023-07-10 11:39:08,515][227910] itr=287, itrs=2000, Progress: 14.35%[0m
[36m[2023-07-10 11:39:20,112][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 11:39:20,112][227910] FPS: 331580.72[0m
[36m[2023-07-10 11:39:24,904][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:39:24,909][227910] Reward + Measures: [[6173.47826446    0.25838488    0.24387443    0.22999424    0.02184502]][0m
[37m[1m[2023-07-10 11:39:24,910][227910] Max Reward on eval: 6173.478264460677[0m
[37m[1m[2023-07-10 11:39:24,910][227910] Min Reward on eval: 6173.478264460677[0m
[37m[1m[2023-07-10 11:39:24,910][227910] Mean Reward across all agents: 6173.478264460677[0m
[37m[1m[2023-07-10 11:39:24,910][227910] Average Trajectory Length: 998.7819999999999[0m
[36m[2023-07-10 11:39:30,446][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:39:30,447][227910] Reward + Measures: [[2993.58761126    0.25655881    0.22119887    0.24688354    0.15215547]
 [1444.94384408    0.280898      0.32906938    0.27636012    0.23119059]
 [ 243.30302044    0.4367435     0.24295877    0.34990397    0.2179791 ]
 ...
 [  88.49867862    0.46360001    0.31500003    0.45770001    0.3962    ]
 [ 275.03697917    0.42389998    0.5449        0.3784        0.60000002]
 [ 531.03187503    0.27270001    0.64059997    0.13970001    0.51620001]][0m
[37m[1m[2023-07-10 11:39:30,447][227910] Max Reward on eval: 4644.599237176427[0m
[37m[1m[2023-07-10 11:39:30,447][227910] Min Reward on eval: -1084.254584363976[0m
[37m[1m[2023-07-10 11:39:30,448][227910] Mean Reward across all agents: 515.8378943982781[0m
[37m[1m[2023-07-10 11:39:30,448][227910] Average Trajectory Length: 854.9979999999999[0m
[36m[2023-07-10 11:39:30,452][227910] mean_value=-1340.8690756050005, max_value=2273.817023842152[0m
[37m[1m[2023-07-10 11:39:30,454][227910] New mean coefficients: [[ 3.3400464  -0.18584648  1.1895416  -0.6380044   1.190852  ]][0m
[37m[1m[2023-07-10 11:39:30,455][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:39:40,205][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 11:39:40,205][227910] FPS: 393921.50[0m
[36m[2023-07-10 11:39:40,208][227910] itr=288, itrs=2000, Progress: 14.40%[0m
[36m[2023-07-10 11:39:51,904][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 11:39:51,904][227910] FPS: 328768.80[0m
[36m[2023-07-10 11:39:56,654][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:39:56,654][227910] Reward + Measures: [[6252.57752608    0.2588664     0.23670588    0.23103474    0.0232929 ]][0m
[37m[1m[2023-07-10 11:39:56,654][227910] Max Reward on eval: 6252.577526078438[0m
[37m[1m[2023-07-10 11:39:56,655][227910] Min Reward on eval: 6252.577526078438[0m
[37m[1m[2023-07-10 11:39:56,655][227910] Mean Reward across all agents: 6252.577526078438[0m
[37m[1m[2023-07-10 11:39:56,655][227910] Average Trajectory Length: 998.2199999999999[0m
[36m[2023-07-10 11:40:02,355][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:40:02,361][227910] Reward + Measures: [[  15.4592174     0.59610003    0.7518        0.045         0.69580001]
 [1153.73504742    0.23847532    0.29492423    0.22434919    0.16822293]
 [-314.65528743    0.21378756    0.26457953    0.24252813    0.22063735]
 ...
 [ 829.92922528    0.30280834    0.32631946    0.25089908    0.23164907]
 [ 130.24243881    0.72084469    0.53215039    0.31129691    0.72286463]
 [ 469.24119738    0.27046916    0.39379701    0.16569774    0.37183309]][0m
[37m[1m[2023-07-10 11:40:02,361][227910] Max Reward on eval: 4345.957363517489[0m
[37m[1m[2023-07-10 11:40:02,361][227910] Min Reward on eval: -1247.3867061514873[0m
[37m[1m[2023-07-10 11:40:02,362][227910] Mean Reward across all agents: 365.94495896996335[0m
[37m[1m[2023-07-10 11:40:02,362][227910] Average Trajectory Length: 874.98[0m
[36m[2023-07-10 11:40:02,366][227910] mean_value=-815.651820312136, max_value=1060.2864404364482[0m
[37m[1m[2023-07-10 11:40:02,369][227910] New mean coefficients: [[ 2.8992429  -0.2519033   1.0849211  -0.31597856  1.8396152 ]][0m
[37m[1m[2023-07-10 11:40:02,370][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:40:12,163][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 11:40:12,163][227910] FPS: 392180.30[0m
[36m[2023-07-10 11:40:12,166][227910] itr=289, itrs=2000, Progress: 14.45%[0m
[36m[2023-07-10 11:40:23,773][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 11:40:23,773][227910] FPS: 331259.36[0m
[36m[2023-07-10 11:40:28,546][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:40:28,551][227910] Reward + Measures: [[4469.81644576    0.25498408    0.20945564    0.23298128    0.10763567]][0m
[37m[1m[2023-07-10 11:40:28,552][227910] Max Reward on eval: 4469.816445760012[0m
[37m[1m[2023-07-10 11:40:28,552][227910] Min Reward on eval: 4469.816445760012[0m
[37m[1m[2023-07-10 11:40:28,552][227910] Mean Reward across all agents: 4469.816445760012[0m
[37m[1m[2023-07-10 11:40:28,552][227910] Average Trajectory Length: 942.9986666666666[0m
[36m[2023-07-10 11:40:34,182][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:40:34,182][227910] Reward + Measures: [[ 599.45772856    0.36059999    0.40880004    0.21780001    0.37789997]
 [  -4.087468      0.50459999    0.48479995    0.21620002    0.55900002]
 [1525.97606287    0.26327738    0.31374991    0.27718696    0.23509653]
 ...
 [ 454.99761523    0.29538807    0.3620041     0.26936764    0.38362527]
 [ 419.41514811    0.27430001    0.42560002    0.1948        0.40310001]
 [  41.99149181    0.5097        0.52450001    0.23340002    0.49890003]][0m
[37m[1m[2023-07-10 11:40:34,183][227910] Max Reward on eval: 4080.1819762371947[0m
[37m[1m[2023-07-10 11:40:34,183][227910] Min Reward on eval: -1187.0530085196485[0m
[37m[1m[2023-07-10 11:40:34,183][227910] Mean Reward across all agents: 662.5280838528471[0m
[37m[1m[2023-07-10 11:40:34,183][227910] Average Trajectory Length: 884.85[0m
[36m[2023-07-10 11:40:34,187][227910] mean_value=-964.3421147990048, max_value=1773.670442284582[0m
[37m[1m[2023-07-10 11:40:34,190][227910] New mean coefficients: [[ 2.9906092  -0.2355129   1.8875978  -0.95323396  1.5916716 ]][0m
[37m[1m[2023-07-10 11:40:34,191][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:40:43,984][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 11:40:43,984][227910] FPS: 392200.35[0m
[36m[2023-07-10 11:40:43,986][227910] itr=290, itrs=2000, Progress: 14.50%[0m
[37m[1m[2023-07-10 11:40:46,321][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000270[0m
[36m[2023-07-10 11:40:58,306][227910] train() took 11.73 seconds to complete[0m
[36m[2023-07-10 11:40:58,306][227910] FPS: 327494.88[0m
[36m[2023-07-10 11:41:03,038][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:41:03,043][227910] Reward + Measures: [[4983.73980684    0.25583234    0.20298177    0.23352523    0.08029084]][0m
[37m[1m[2023-07-10 11:41:03,044][227910] Max Reward on eval: 4983.73980684143[0m
[37m[1m[2023-07-10 11:41:03,045][227910] Min Reward on eval: 4983.73980684143[0m
[37m[1m[2023-07-10 11:41:03,045][227910] Mean Reward across all agents: 4983.73980684143[0m
[37m[1m[2023-07-10 11:41:03,046][227910] Average Trajectory Length: 961.4996666666666[0m
[36m[2023-07-10 11:41:08,610][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:41:08,615][227910] Reward + Measures: [[-445.78691455    0.27965006    0.26516798    0.24183047    0.24422768]
 [ 935.33453431    0.2538383     0.33038482    0.23508833    0.13059615]
 [  81.90419454    0.42453367    0.40687075    0.33332062    0.27169487]
 ...
 [ 530.885838      0.31509465    0.49474445    0.2072318     0.30377278]
 [1217.8360133     0.2451438     0.30430239    0.26556095    0.1676929 ]
 [-251.99414924    0.35783148    0.28758857    0.358978      0.44592991]][0m
[37m[1m[2023-07-10 11:41:08,616][227910] Max Reward on eval: 4488.574864457932[0m
[37m[1m[2023-07-10 11:41:08,616][227910] Min Reward on eval: -944.2962465250282[0m
[37m[1m[2023-07-10 11:41:08,616][227910] Mean Reward across all agents: 476.821792142827[0m
[37m[1m[2023-07-10 11:41:08,617][227910] Average Trajectory Length: 683.8389999999999[0m
[36m[2023-07-10 11:41:08,618][227910] mean_value=-2213.1561579340637, max_value=821.6023411244154[0m
[37m[1m[2023-07-10 11:41:08,621][227910] New mean coefficients: [[ 3.4303162  0.2644019  1.4004476 -1.1282824  1.2107618]][0m
[37m[1m[2023-07-10 11:41:08,622][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:41:18,427][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 11:41:18,427][227910] FPS: 391714.51[0m
[36m[2023-07-10 11:41:18,429][227910] itr=291, itrs=2000, Progress: 14.55%[0m
[36m[2023-07-10 11:41:30,017][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 11:41:30,018][227910] FPS: 331830.29[0m
[36m[2023-07-10 11:41:34,916][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:41:34,916][227910] Reward + Measures: [[5371.36768442    0.25188807    0.20054612    0.23328419    0.06051301]][0m
[37m[1m[2023-07-10 11:41:34,916][227910] Max Reward on eval: 5371.36768442322[0m
[37m[1m[2023-07-10 11:41:34,916][227910] Min Reward on eval: 5371.36768442322[0m
[37m[1m[2023-07-10 11:41:34,917][227910] Mean Reward across all agents: 5371.36768442322[0m
[37m[1m[2023-07-10 11:41:34,917][227910] Average Trajectory Length: 979.435[0m
[36m[2023-07-10 11:41:40,404][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:41:40,409][227910] Reward + Measures: [[2539.09049039    0.24319105    0.26625362    0.21874629    0.14697793]
 [3153.29719193    0.27007633    0.31834254    0.23163824    0.07810746]
 [ 868.20110463    0.33029482    0.33330384    0.30919871    0.23854159]
 ...
 [1558.93520379    0.32540002    0.46100003    0.21970001    0.17300001]
 [-407.75962872    0.40809998    0.32350001    0.46820003    0.34830001]
 [-144.46962012    0.50190002    0.27540001    0.42000005    0.28340003]][0m
[37m[1m[2023-07-10 11:41:40,410][227910] Max Reward on eval: 5149.179572882504[0m
[37m[1m[2023-07-10 11:41:40,410][227910] Min Reward on eval: -662.9645531351504[0m
[37m[1m[2023-07-10 11:41:40,410][227910] Mean Reward across all agents: 909.3620191174397[0m
[37m[1m[2023-07-10 11:41:40,411][227910] Average Trajectory Length: 790.3076666666666[0m
[36m[2023-07-10 11:41:40,413][227910] mean_value=-1822.2139092291993, max_value=1040.5963254263343[0m
[37m[1m[2023-07-10 11:41:40,416][227910] New mean coefficients: [[ 3.1897657   0.86262316  1.1543206  -1.3824406   1.1375918 ]][0m
[37m[1m[2023-07-10 11:41:40,417][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:41:50,257][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 11:41:50,257][227910] FPS: 390307.36[0m
[36m[2023-07-10 11:41:50,260][227910] itr=292, itrs=2000, Progress: 14.60%[0m
[36m[2023-07-10 11:42:01,842][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 11:42:01,842][227910] FPS: 331975.99[0m
[36m[2023-07-10 11:42:06,677][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:42:06,677][227910] Reward + Measures: [[5647.79707248    0.24557047    0.19259968    0.2271219     0.03880056]][0m
[37m[1m[2023-07-10 11:42:06,677][227910] Max Reward on eval: 5647.797072476752[0m
[37m[1m[2023-07-10 11:42:06,678][227910] Min Reward on eval: 5647.797072476752[0m
[37m[1m[2023-07-10 11:42:06,678][227910] Mean Reward across all agents: 5647.797072476752[0m
[37m[1m[2023-07-10 11:42:06,678][227910] Average Trajectory Length: 982.1703333333332[0m
[36m[2023-07-10 11:42:12,378][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:42:12,379][227910] Reward + Measures: [[1498.48628357    0.27303022    0.27357659    0.23700093    0.15705869]
 [ 726.50860103    0.2811397     0.44113261    0.39571843    0.40391493]
 [ 464.30362363    0.21462587    0.36616898    0.27989969    0.24880652]
 ...
 [ 165.25364821    0.27906185    0.35045621    0.29804569    0.21810082]
 [ -99.73359464    0.42719999    0.38369998    0.33199999    0.31080002]
 [ 229.07882306    0.52675647    0.35787392    0.45176527    0.11302173]][0m
[37m[1m[2023-07-10 11:42:12,379][227910] Max Reward on eval: 4766.996297163796[0m
[37m[1m[2023-07-10 11:42:12,379][227910] Min Reward on eval: -1129.4273439642275[0m
[37m[1m[2023-07-10 11:42:12,380][227910] Mean Reward across all agents: 481.24395040911077[0m
[37m[1m[2023-07-10 11:42:12,380][227910] Average Trajectory Length: 838.4116666666666[0m
[36m[2023-07-10 11:42:12,383][227910] mean_value=-1621.9547359186524, max_value=931.1970758651617[0m
[37m[1m[2023-07-10 11:42:12,386][227910] New mean coefficients: [[ 2.9934356   1.6378069   0.27114153 -1.7406803   1.2749169 ]][0m
[37m[1m[2023-07-10 11:42:12,387][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:42:22,174][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 11:42:22,174][227910] FPS: 392441.25[0m
[36m[2023-07-10 11:42:22,176][227910] itr=293, itrs=2000, Progress: 14.65%[0m
[36m[2023-07-10 11:42:33,702][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 11:42:33,702][227910] FPS: 333681.72[0m
[36m[2023-07-10 11:42:38,431][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:42:38,431][227910] Reward + Measures: [[5795.03915515    0.25459266    0.21695343    0.23672208    0.04858555]][0m
[37m[1m[2023-07-10 11:42:38,431][227910] Max Reward on eval: 5795.039155154367[0m
[37m[1m[2023-07-10 11:42:38,432][227910] Min Reward on eval: 5795.039155154367[0m
[37m[1m[2023-07-10 11:42:38,432][227910] Mean Reward across all agents: 5795.039155154367[0m
[37m[1m[2023-07-10 11:42:38,432][227910] Average Trajectory Length: 994.847[0m
[36m[2023-07-10 11:42:43,965][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:42:43,966][227910] Reward + Measures: [[  63.87263681    0.22176638    0.52533162    0.23115087    0.45526305]
 [ 638.54436153    0.22138588    0.29260373    0.26061314    0.09021336]
 [ 287.23770969    0.22672851    0.28818202    0.25263101    0.13610324]
 ...
 [1453.4920553     0.26880026    0.36054727    0.25452143    0.09288835]
 [ 623.52200835    0.2153801     0.32876125    0.22239617    0.08716797]
 [ 629.56881753    0.33738586    0.3804571     0.24341944    0.20837815]][0m
[37m[1m[2023-07-10 11:42:43,966][227910] Max Reward on eval: 3830.1451403592246[0m
[37m[1m[2023-07-10 11:42:43,966][227910] Min Reward on eval: -880.2368994063697[0m
[37m[1m[2023-07-10 11:42:43,967][227910] Mean Reward across all agents: 599.3147793827421[0m
[37m[1m[2023-07-10 11:42:43,967][227910] Average Trajectory Length: 748.66[0m
[36m[2023-07-10 11:42:43,969][227910] mean_value=-2658.566095747239, max_value=782.563820810421[0m
[37m[1m[2023-07-10 11:42:43,971][227910] New mean coefficients: [[ 2.4684029   1.0318081   0.48777282 -0.6310092   1.5968236 ]][0m
[37m[1m[2023-07-10 11:42:43,972][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:42:53,695][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 11:42:53,696][227910] FPS: 395000.15[0m
[36m[2023-07-10 11:42:53,698][227910] itr=294, itrs=2000, Progress: 14.70%[0m
[36m[2023-07-10 11:43:05,225][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 11:43:05,226][227910] FPS: 333610.66[0m
[36m[2023-07-10 11:43:10,084][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:43:10,085][227910] Reward + Measures: [[6017.13973376    0.25783426    0.20772535    0.23713551    0.04630609]][0m
[37m[1m[2023-07-10 11:43:10,085][227910] Max Reward on eval: 6017.139733757835[0m
[37m[1m[2023-07-10 11:43:10,085][227910] Min Reward on eval: 6017.139733757835[0m
[37m[1m[2023-07-10 11:43:10,086][227910] Mean Reward across all agents: 6017.139733757835[0m
[37m[1m[2023-07-10 11:43:10,086][227910] Average Trajectory Length: 997.2883333333333[0m
[36m[2023-07-10 11:43:15,553][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:43:15,553][227910] Reward + Measures: [[1966.09284338    0.24243252    0.21935563    0.21597452    0.12439922]
 [ 597.26671149    0.33940002    0.57429999    0.2246        0.36690003]
 [ 376.95680668    0.38771185    0.39440295    0.27395684    0.25978169]
 ...
 [ 134.79606285    0.31142887    0.25340626    0.19322142    0.22120495]
 [1203.76855621    0.27550003    0.39210001    0.2897        0.26750001]
 [  92.99083816    0.33900002    0.5341        0.21710001    0.37499997]][0m
[37m[1m[2023-07-10 11:43:15,553][227910] Max Reward on eval: 4215.786390547361[0m
[37m[1m[2023-07-10 11:43:15,554][227910] Min Reward on eval: -799.4245221243764[0m
[37m[1m[2023-07-10 11:43:15,554][227910] Mean Reward across all agents: 846.3308071245084[0m
[37m[1m[2023-07-10 11:43:15,554][227910] Average Trajectory Length: 927.8333333333333[0m
[36m[2023-07-10 11:43:15,556][227910] mean_value=-1166.0337786957805, max_value=1474.1630544461452[0m
[37m[1m[2023-07-10 11:43:15,559][227910] New mean coefficients: [[ 2.3280852   1.1943032  -0.05426925 -0.01115024  2.05287   ]][0m
[37m[1m[2023-07-10 11:43:15,560][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:43:25,386][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 11:43:25,386][227910] FPS: 390901.62[0m
[36m[2023-07-10 11:43:25,388][227910] itr=295, itrs=2000, Progress: 14.75%[0m
[36m[2023-07-10 11:43:37,006][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 11:43:37,007][227910] FPS: 330978.60[0m
[36m[2023-07-10 11:43:41,788][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:43:41,789][227910] Reward + Measures: [[6197.1229576     0.25353545    0.19927612    0.23425411    0.03461672]][0m
[37m[1m[2023-07-10 11:43:41,789][227910] Max Reward on eval: 6197.1229576036185[0m
[37m[1m[2023-07-10 11:43:41,790][227910] Min Reward on eval: 6197.1229576036185[0m
[37m[1m[2023-07-10 11:43:41,790][227910] Mean Reward across all agents: 6197.1229576036185[0m
[37m[1m[2023-07-10 11:43:41,790][227910] Average Trajectory Length: 995.7713333333332[0m
[36m[2023-07-10 11:43:47,421][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:43:47,422][227910] Reward + Measures: [[ -434.38223026     0.26826352     0.3095063      0.25471261
      0.2982893 ]
 [ 1320.72547508     0.29194838     0.29243898     0.22657815
      0.13109981]
 [ -427.67381901     0.42695719     0.2661154      0.32881251
      0.23038308]
 ...
 [-1344.13468201     0.38415384     0.37756154     0.37087694
      0.38775769]
 [  292.6039437      0.22085559     0.27524984     0.22668381
      0.14692177]
 [  -39.67131238     0.23466276     0.29610452     0.30893487
      0.25055882]][0m
[37m[1m[2023-07-10 11:43:47,422][227910] Max Reward on eval: 4070.38582643877[0m
[37m[1m[2023-07-10 11:43:47,422][227910] Min Reward on eval: -1469.8784210566198[0m
[37m[1m[2023-07-10 11:43:47,423][227910] Mean Reward across all agents: 337.85660713143056[0m
[37m[1m[2023-07-10 11:43:47,423][227910] Average Trajectory Length: 743.6469999999999[0m
[36m[2023-07-10 11:43:47,425][227910] mean_value=-2644.0324257415614, max_value=448.96221032133053[0m
[37m[1m[2023-07-10 11:43:47,427][227910] New mean coefficients: [[ 2.2289352   0.62462556  0.9770556  -0.15024228  1.7320466 ]][0m
[37m[1m[2023-07-10 11:43:47,428][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:43:57,249][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 11:43:57,249][227910] FPS: 391077.79[0m
[36m[2023-07-10 11:43:57,251][227910] itr=296, itrs=2000, Progress: 14.80%[0m
[36m[2023-07-10 11:44:08,726][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 11:44:08,727][227910] FPS: 335081.42[0m
[36m[2023-07-10 11:44:13,460][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:44:13,460][227910] Reward + Measures: [[6308.84693267    0.2489375     0.19521339    0.23061065    0.02645377]][0m
[37m[1m[2023-07-10 11:44:13,461][227910] Max Reward on eval: 6308.846932668076[0m
[37m[1m[2023-07-10 11:44:13,461][227910] Min Reward on eval: 6308.846932668076[0m
[37m[1m[2023-07-10 11:44:13,461][227910] Mean Reward across all agents: 6308.846932668076[0m
[37m[1m[2023-07-10 11:44:13,461][227910] Average Trajectory Length: 993.4443333333332[0m
[36m[2023-07-10 11:44:19,024][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:44:19,024][227910] Reward + Measures: [[-463.35415618    0.1693757     0.306676      0.20271266    0.22873278]
 [-734.29267979    0.82890004    0.8524        0.81259996    0.81540006]
 [  10.13206038    0.23513325    0.40948755    0.22743721    0.09206915]
 ...
 [1970.4726294     0.38221604    0.48999453    0.24709578    0.26341519]
 [ 440.02287906    0.43090564    0.56088275    0.3367193     0.40185696]
 [-517.60356438    0.50956225    0.58505565    0.10666554    0.5514369 ]][0m
[37m[1m[2023-07-10 11:44:19,024][227910] Max Reward on eval: 5633.737426331965[0m
[37m[1m[2023-07-10 11:44:19,025][227910] Min Reward on eval: -1605.7678020769497[0m
[37m[1m[2023-07-10 11:44:19,025][227910] Mean Reward across all agents: 283.3160139797822[0m
[37m[1m[2023-07-10 11:44:19,025][227910] Average Trajectory Length: 729.935[0m
[36m[2023-07-10 11:44:19,028][227910] mean_value=-1959.8504305756558, max_value=1397.6485171706327[0m
[37m[1m[2023-07-10 11:44:19,030][227910] New mean coefficients: [[ 2.405734   -0.24086505  1.6584841   0.25770187  1.3237873 ]][0m
[37m[1m[2023-07-10 11:44:19,031][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:44:28,675][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 11:44:28,675][227910] FPS: 398245.60[0m
[36m[2023-07-10 11:44:28,678][227910] itr=297, itrs=2000, Progress: 14.85%[0m
[36m[2023-07-10 11:44:40,309][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 11:44:40,309][227910] FPS: 330631.64[0m
[36m[2023-07-10 11:44:45,016][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:44:45,017][227910] Reward + Measures: [[6457.02617643    0.24802127    0.1895065     0.22761035    0.02019662]][0m
[37m[1m[2023-07-10 11:44:45,017][227910] Max Reward on eval: 6457.026176426176[0m
[37m[1m[2023-07-10 11:44:45,017][227910] Min Reward on eval: 6457.026176426176[0m
[37m[1m[2023-07-10 11:44:45,017][227910] Mean Reward across all agents: 6457.026176426176[0m
[37m[1m[2023-07-10 11:44:45,018][227910] Average Trajectory Length: 991.6033333333334[0m
[36m[2023-07-10 11:44:50,384][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:44:50,389][227910] Reward + Measures: [[-194.57138266    0.29631376    0.20674132    0.18695569    0.19665809]
 [  85.9246181     0.32509178    0.34007534    0.28502235    0.23766504]
 [ 894.75658041    0.31997058    0.41333023    0.27313629    0.28540197]
 ...
 [-557.41098684    0.74649996    0.0783        0.71290004    0.727     ]
 [ 277.48138987    0.23829947    0.27559868    0.25179955    0.16717266]
 [3206.97874761    0.25585017    0.32922912    0.24266481    0.1627553 ]][0m
[37m[1m[2023-07-10 11:44:50,390][227910] Max Reward on eval: 4885.36504461742[0m
[37m[1m[2023-07-10 11:44:50,390][227910] Min Reward on eval: -1366.5299356492237[0m
[37m[1m[2023-07-10 11:44:50,390][227910] Mean Reward across all agents: 382.0551544738987[0m
[37m[1m[2023-07-10 11:44:50,390][227910] Average Trajectory Length: 768.0763333333333[0m
[36m[2023-07-10 11:44:50,393][227910] mean_value=-1612.1778426316496, max_value=1598.4869764548894[0m
[37m[1m[2023-07-10 11:44:50,395][227910] New mean coefficients: [[ 2.032079   -0.49140808  0.8917505   0.58608174  1.8549228 ]][0m
[37m[1m[2023-07-10 11:44:50,396][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:45:00,050][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 11:45:00,051][227910] FPS: 397817.98[0m
[36m[2023-07-10 11:45:00,053][227910] itr=298, itrs=2000, Progress: 14.90%[0m
[36m[2023-07-10 11:45:11,519][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 11:45:11,519][227910] FPS: 335352.38[0m
[36m[2023-07-10 11:45:16,095][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:45:16,096][227910] Reward + Measures: [[6580.73736385    0.24868676    0.1862485     0.22768547    0.01926235]][0m
[37m[1m[2023-07-10 11:45:16,096][227910] Max Reward on eval: 6580.737363850708[0m
[37m[1m[2023-07-10 11:45:16,096][227910] Min Reward on eval: 6580.737363850708[0m
[37m[1m[2023-07-10 11:45:16,096][227910] Mean Reward across all agents: 6580.737363850708[0m
[37m[1m[2023-07-10 11:45:16,097][227910] Average Trajectory Length: 996.4473333333333[0m
[36m[2023-07-10 11:45:21,616][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:45:21,622][227910] Reward + Measures: [[ 824.77281033    0.23221159    0.26007184    0.22404878    0.12189259]
 [   6.49712013    0.22307396    0.25930747    0.24461429    0.18021373]
 [1233.35251113    0.27449593    0.40670815    0.21126905    0.22204803]
 ...
 [ 483.8900145     0.38945004    0.36211666    0.30315003    0.21856666]
 [3269.49039306    0.27458233    0.31008744    0.21906035    0.13545549]
 [ 399.84465244    0.19731629    0.33548805    0.15908748    0.23914066]][0m
[37m[1m[2023-07-10 11:45:21,622][227910] Max Reward on eval: 3670.448428726755[0m
[37m[1m[2023-07-10 11:45:21,623][227910] Min Reward on eval: -1293.1926097823714[0m
[37m[1m[2023-07-10 11:45:21,623][227910] Mean Reward across all agents: 207.81225214576693[0m
[37m[1m[2023-07-10 11:45:21,623][227910] Average Trajectory Length: 704.334[0m
[36m[2023-07-10 11:45:21,625][227910] mean_value=-1668.497200200341, max_value=657.931105058298[0m
[37m[1m[2023-07-10 11:45:21,628][227910] New mean coefficients: [[ 2.1001487  -0.96819067  1.143952    0.5851065   1.0249895 ]][0m
[37m[1m[2023-07-10 11:45:21,629][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:45:31,375][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 11:45:31,375][227910] FPS: 394084.09[0m
[36m[2023-07-10 11:45:31,377][227910] itr=299, itrs=2000, Progress: 14.95%[0m
[36m[2023-07-10 11:45:42,838][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 11:45:42,838][227910] FPS: 335493.26[0m
[36m[2023-07-10 11:45:47,556][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:45:47,556][227910] Reward + Measures: [[6545.64928902    0.24938694    0.18417203    0.22891527    0.02179927]][0m
[37m[1m[2023-07-10 11:45:47,557][227910] Max Reward on eval: 6545.649289024464[0m
[37m[1m[2023-07-10 11:45:47,557][227910] Min Reward on eval: 6545.649289024464[0m
[37m[1m[2023-07-10 11:45:47,557][227910] Mean Reward across all agents: 6545.649289024464[0m
[37m[1m[2023-07-10 11:45:47,557][227910] Average Trajectory Length: 990.278[0m
[36m[2023-07-10 11:45:53,072][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:45:53,073][227910] Reward + Measures: [[ 842.00198441    0.34730729    0.40888557    0.26319036    0.24905972]
 [1169.39174479    0.29720384    0.37069288    0.26634085    0.25270358]
 [ 473.1765265     0.29228249    0.40161583    0.23880112    0.28581244]
 ...
 [ 286.2397287     0.31184861    0.31787449    0.22990799    0.20270586]
 [2626.5380405     0.28739998    0.29910001    0.25420001    0.17770001]
 [2294.98958187    0.2726        0.33438182    0.23583637    0.12679999]][0m
[37m[1m[2023-07-10 11:45:53,073][227910] Max Reward on eval: 5565.683184865862[0m
[37m[1m[2023-07-10 11:45:53,073][227910] Min Reward on eval: -1005.9730634988169[0m
[37m[1m[2023-07-10 11:45:53,074][227910] Mean Reward across all agents: 776.9885082024819[0m
[37m[1m[2023-07-10 11:45:53,074][227910] Average Trajectory Length: 803.331[0m
[36m[2023-07-10 11:45:53,076][227910] mean_value=-2072.2756758656055, max_value=2031.2853673156192[0m
[37m[1m[2023-07-10 11:45:53,078][227910] New mean coefficients: [[ 2.5262942  -0.38747972  0.72106016 -0.01075947  0.8243124 ]][0m
[37m[1m[2023-07-10 11:45:53,079][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:46:02,807][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 11:46:02,808][227910] FPS: 394804.48[0m
[36m[2023-07-10 11:46:02,810][227910] itr=300, itrs=2000, Progress: 15.00%[0m
[37m[1m[2023-07-10 11:46:05,127][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000280[0m
[36m[2023-07-10 11:46:17,062][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 11:46:17,063][227910] FPS: 329072.67[0m
[36m[2023-07-10 11:46:21,919][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:46:21,919][227910] Reward + Measures: [[6615.36637567    0.25097725    0.18077165    0.22897491    0.02302756]][0m
[37m[1m[2023-07-10 11:46:21,920][227910] Max Reward on eval: 6615.366375667784[0m
[37m[1m[2023-07-10 11:46:21,920][227910] Min Reward on eval: 6615.366375667784[0m
[37m[1m[2023-07-10 11:46:21,920][227910] Mean Reward across all agents: 6615.366375667784[0m
[37m[1m[2023-07-10 11:46:21,920][227910] Average Trajectory Length: 992.2136666666667[0m
[36m[2023-07-10 11:46:27,476][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:46:27,477][227910] Reward + Measures: [[-144.19253615    0.36290002    0.22690001    0.31729999    0.2978    ]
 [ 216.51385802    0.24903528    0.26433295    0.22398941    0.16293275]
 [1141.38796671    0.2793107     0.3647092     0.26689696    0.21865623]
 ...
 [ 447.98295398    0.29329237    0.33719572    0.27211469    0.17474008]
 [1298.97567308    0.2256        0.29100001    0.22480002    0.1772    ]
 [1424.72915419    0.3385269     0.28076607    0.30327356    0.23431234]][0m
[37m[1m[2023-07-10 11:46:27,477][227910] Max Reward on eval: 6129.321824047249[0m
[37m[1m[2023-07-10 11:46:27,477][227910] Min Reward on eval: -906.5875666872365[0m
[37m[1m[2023-07-10 11:46:27,477][227910] Mean Reward across all agents: 1259.70667097614[0m
[37m[1m[2023-07-10 11:46:27,478][227910] Average Trajectory Length: 857.2756666666667[0m
[36m[2023-07-10 11:46:27,479][227910] mean_value=-2117.706559968067, max_value=2141.6838678093713[0m
[37m[1m[2023-07-10 11:46:27,482][227910] New mean coefficients: [[ 2.084804   -0.51804286  1.2575998   0.2773714   1.4625309 ]][0m
[37m[1m[2023-07-10 11:46:27,483][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:46:37,176][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 11:46:37,177][227910] FPS: 396209.02[0m
[36m[2023-07-10 11:46:37,179][227910] itr=301, itrs=2000, Progress: 15.05%[0m
[36m[2023-07-10 11:46:48,755][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 11:46:48,755][227910] FPS: 332144.65[0m
[36m[2023-07-10 11:46:53,589][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:46:53,590][227910] Reward + Measures: [[5675.09088381    0.26548079    0.20937259    0.23211415    0.03153396]][0m
[37m[1m[2023-07-10 11:46:53,590][227910] Max Reward on eval: 5675.090883809246[0m
[37m[1m[2023-07-10 11:46:53,590][227910] Min Reward on eval: 5675.090883809246[0m
[37m[1m[2023-07-10 11:46:53,590][227910] Mean Reward across all agents: 5675.090883809246[0m
[37m[1m[2023-07-10 11:46:53,590][227910] Average Trajectory Length: 965.0943333333333[0m
[36m[2023-07-10 11:46:58,950][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:46:58,951][227910] Reward + Measures: [[-283.36676334    0.37290001    0.52100003    0.2656        0.38210002]
 [ -33.97144626    0.26449999    0.3053        0.23600002    0.21290003]
 [ -48.74589387    0.25875312    0.29942462    0.26348314    0.18354838]
 ...
 [1020.90028272    0.31717768    0.30519655    0.23096672    0.14971423]
 [ -79.19798229    0.35209998    0.37020001    0.2244        0.28910002]
 [-244.4354109     0.22813204    0.30206564    0.23030844    0.30158502]][0m
[37m[1m[2023-07-10 11:46:58,951][227910] Max Reward on eval: 3312.632472669496[0m
[37m[1m[2023-07-10 11:46:58,951][227910] Min Reward on eval: -1658.1510761601617[0m
[37m[1m[2023-07-10 11:46:58,952][227910] Mean Reward across all agents: 143.26483951609896[0m
[37m[1m[2023-07-10 11:46:58,952][227910] Average Trajectory Length: 764.478[0m
[36m[2023-07-10 11:46:58,953][227910] mean_value=-1970.4345273047638, max_value=2274.4587946661386[0m
[37m[1m[2023-07-10 11:46:58,956][227910] New mean coefficients: [[ 2.2571747  -0.6421704   1.0718627   0.10988703  1.1909578 ]][0m
[37m[1m[2023-07-10 11:46:58,957][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:47:08,609][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 11:47:08,610][227910] FPS: 397883.16[0m
[36m[2023-07-10 11:47:08,612][227910] itr=302, itrs=2000, Progress: 15.10%[0m
[36m[2023-07-10 11:47:20,087][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 11:47:20,088][227910] FPS: 335060.90[0m
[36m[2023-07-10 11:47:24,964][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:47:24,964][227910] Reward + Measures: [[5565.55488402    0.2494424     0.20437819    0.22746313    0.04251632]][0m
[37m[1m[2023-07-10 11:47:24,964][227910] Max Reward on eval: 5565.554884017325[0m
[37m[1m[2023-07-10 11:47:24,965][227910] Min Reward on eval: 5565.554884017325[0m
[37m[1m[2023-07-10 11:47:24,965][227910] Mean Reward across all agents: 5565.554884017325[0m
[37m[1m[2023-07-10 11:47:24,965][227910] Average Trajectory Length: 942.8493333333333[0m
[36m[2023-07-10 11:47:30,444][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:47:30,450][227910] Reward + Measures: [[ 112.5551925     0.31745297    0.4738647     0.31208238    0.49767646]
 [ 100.83133932    0.23536125    0.29869092    0.23285337    0.24474309]
 [-254.30039468    0.19136512    0.36973023    0.2053349     0.31080696]
 ...
 [-456.33982809    0.3650412     0.37565079    0.36908236    0.33596081]
 [-432.2204185     0.24885409    0.34735605    0.21182127    0.3179794 ]
 [ 783.03243199    0.28046846    0.28300187    0.19477433    0.1722385 ]][0m
[37m[1m[2023-07-10 11:47:30,450][227910] Max Reward on eval: 2584.6036010609823[0m
[37m[1m[2023-07-10 11:47:30,450][227910] Min Reward on eval: -1439.2622973774792[0m
[37m[1m[2023-07-10 11:47:30,451][227910] Mean Reward across all agents: 267.24687585171733[0m
[37m[1m[2023-07-10 11:47:30,451][227910] Average Trajectory Length: 848.8533333333334[0m
[36m[2023-07-10 11:47:30,453][227910] mean_value=-1416.7483054294041, max_value=466.92273294912945[0m
[37m[1m[2023-07-10 11:47:30,455][227910] New mean coefficients: [[ 1.971607   -0.11612022  1.3187459  -0.02332744  1.4974382 ]][0m
[37m[1m[2023-07-10 11:47:30,456][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:47:40,161][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 11:47:40,161][227910] FPS: 395757.14[0m
[36m[2023-07-10 11:47:40,163][227910] itr=303, itrs=2000, Progress: 15.15%[0m
[36m[2023-07-10 11:47:51,617][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 11:47:51,618][227910] FPS: 335750.09[0m
[36m[2023-07-10 11:47:56,386][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:47:56,386][227910] Reward + Measures: [[2509.86074021    0.29134205    0.310094      0.24994929    0.22652988]][0m
[37m[1m[2023-07-10 11:47:56,386][227910] Max Reward on eval: 2509.8607402109033[0m
[37m[1m[2023-07-10 11:47:56,386][227910] Min Reward on eval: 2509.8607402109033[0m
[37m[1m[2023-07-10 11:47:56,386][227910] Mean Reward across all agents: 2509.8607402109033[0m
[37m[1m[2023-07-10 11:47:56,387][227910] Average Trajectory Length: 948.6196666666666[0m
[36m[2023-07-10 11:48:01,864][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:48:01,864][227910] Reward + Measures: [[1148.95139321    0.2816        0.36579999    0.26359996    0.2915    ]
 [ 878.59817167    0.45240831    0.38269171    0.26921436    0.28572378]
 [1109.08278143    0.29220614    0.2810095     0.23648052    0.2065476 ]
 ...
 [1255.80633385    0.2592102     0.39657345    0.33565375    0.25392994]
 [1212.9020078     0.45390001    0.3443        0.2493        0.25750002]
 [ 267.28840343    0.2309        0.39850003    0.2376        0.34709999]][0m
[37m[1m[2023-07-10 11:48:01,865][227910] Max Reward on eval: 2677.105796488887[0m
[37m[1m[2023-07-10 11:48:01,865][227910] Min Reward on eval: -986.3691857130791[0m
[37m[1m[2023-07-10 11:48:01,865][227910] Mean Reward across all agents: 643.6784932471336[0m
[37m[1m[2023-07-10 11:48:01,865][227910] Average Trajectory Length: 940.8413333333333[0m
[36m[2023-07-10 11:48:01,868][227910] mean_value=-1211.0722175698088, max_value=1001.923850041973[0m
[37m[1m[2023-07-10 11:48:01,870][227910] New mean coefficients: [[1.8341215  0.1350922  0.76850605 0.4316053  1.6933854 ]][0m
[37m[1m[2023-07-10 11:48:01,871][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:48:11,506][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 11:48:11,506][227910] FPS: 398613.19[0m
[36m[2023-07-10 11:48:11,508][227910] itr=304, itrs=2000, Progress: 15.20%[0m
[36m[2023-07-10 11:48:23,003][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 11:48:23,003][227910] FPS: 334499.52[0m
[36m[2023-07-10 11:48:27,797][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:48:27,797][227910] Reward + Measures: [[3050.36886698    0.29177478    0.28682539    0.25028092    0.20709531]][0m
[37m[1m[2023-07-10 11:48:27,797][227910] Max Reward on eval: 3050.3688669770695[0m
[37m[1m[2023-07-10 11:48:27,798][227910] Min Reward on eval: 3050.3688669770695[0m
[37m[1m[2023-07-10 11:48:27,798][227910] Mean Reward across all agents: 3050.3688669770695[0m
[37m[1m[2023-07-10 11:48:27,798][227910] Average Trajectory Length: 938.1956666666666[0m
[36m[2023-07-10 11:48:33,288][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:48:33,288][227910] Reward + Measures: [[-572.79471246    0.24419999    0.26974446    0.21366668    0.23414445]
 [-342.23935861    0.35390002    0.2561        0.28690001    0.3003    ]
 [1577.9678517     0.31270003    0.36390001    0.25659999    0.25500003]
 ...
 [1064.9448318     0.25871107    0.29208887    0.24849732    0.29222441]
 [-496.62325356    0.3846153     0.27977315    0.32902345    0.30890337]
 [ 305.92686799    0.23754787    0.28616425    0.22911684    0.22760414]][0m
[37m[1m[2023-07-10 11:48:33,288][227910] Max Reward on eval: 3239.7600082244257[0m
[37m[1m[2023-07-10 11:48:33,289][227910] Min Reward on eval: -1459.6514904779615[0m
[37m[1m[2023-07-10 11:48:33,289][227910] Mean Reward across all agents: 219.24550315389544[0m
[37m[1m[2023-07-10 11:48:33,289][227910] Average Trajectory Length: 949.3513333333333[0m
[36m[2023-07-10 11:48:33,292][227910] mean_value=-1119.6991234165166, max_value=2008.555078708372[0m
[37m[1m[2023-07-10 11:48:33,294][227910] New mean coefficients: [[ 2.2538934   1.1649172  -0.41791868  0.16969612  1.6483297 ]][0m
[37m[1m[2023-07-10 11:48:33,295][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:48:43,106][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 11:48:43,106][227910] FPS: 391478.02[0m
[36m[2023-07-10 11:48:43,108][227910] itr=305, itrs=2000, Progress: 15.25%[0m
[36m[2023-07-10 11:48:54,592][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 11:48:54,592][227910] FPS: 334905.77[0m
[36m[2023-07-10 11:48:59,318][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:48:59,319][227910] Reward + Measures: [[3599.08346946    0.29084536    0.26947984    0.25183147    0.17645101]][0m
[37m[1m[2023-07-10 11:48:59,319][227910] Max Reward on eval: 3599.0834694635682[0m
[37m[1m[2023-07-10 11:48:59,319][227910] Min Reward on eval: 3599.0834694635682[0m
[37m[1m[2023-07-10 11:48:59,319][227910] Mean Reward across all agents: 3599.0834694635682[0m
[37m[1m[2023-07-10 11:48:59,320][227910] Average Trajectory Length: 910.7083333333333[0m
[36m[2023-07-10 11:49:04,958][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:49:04,963][227910] Reward + Measures: [[ 106.74849814    0.40310001    0.5165        0.16429999    0.32690001]
 [ 230.09602205    0.26970002    0.36960003    0.20289998    0.34550002]
 [-265.80466287    0.38229999    0.40990001    0.2175        0.24779999]
 ...
 [ 572.8813488     0.43989998    0.39970002    0.29320002    0.36380002]
 [ -47.40162574    0.4887        0.69300002    0.0898        0.49020001]
 [ 673.75284016    0.26721677    0.3067489     0.19339602    0.18838146]][0m
[37m[1m[2023-07-10 11:49:04,964][227910] Max Reward on eval: 3275.0472414540127[0m
[37m[1m[2023-07-10 11:49:04,964][227910] Min Reward on eval: -1267.044555799279[0m
[37m[1m[2023-07-10 11:49:04,964][227910] Mean Reward across all agents: 104.02417105098071[0m
[37m[1m[2023-07-10 11:49:04,964][227910] Average Trajectory Length: 936.8123333333333[0m
[36m[2023-07-10 11:49:04,967][227910] mean_value=-1208.1329595710313, max_value=1032.774125139194[0m
[37m[1m[2023-07-10 11:49:04,969][227910] New mean coefficients: [[1.8543271  0.34598386 0.22995353 1.2774773  1.7572011 ]][0m
[37m[1m[2023-07-10 11:49:04,970][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:49:14,677][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 11:49:14,677][227910] FPS: 395678.31[0m
[36m[2023-07-10 11:49:14,679][227910] itr=306, itrs=2000, Progress: 15.30%[0m
[36m[2023-07-10 11:49:26,225][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 11:49:26,225][227910] FPS: 333039.30[0m
[36m[2023-07-10 11:49:30,949][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:49:30,954][227910] Reward + Measures: [[4123.5032585     0.28818959    0.25448045    0.25382882    0.15462036]][0m
[37m[1m[2023-07-10 11:49:30,955][227910] Max Reward on eval: 4123.503258501565[0m
[37m[1m[2023-07-10 11:49:30,955][227910] Min Reward on eval: 4123.503258501565[0m
[37m[1m[2023-07-10 11:49:30,955][227910] Mean Reward across all agents: 4123.503258501565[0m
[37m[1m[2023-07-10 11:49:30,956][227910] Average Trajectory Length: 927.661[0m
[36m[2023-07-10 11:49:36,385][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:49:36,391][227910] Reward + Measures: [[ -52.95039475    0.37129998    0.47279999    0.1767        0.34020001]
 [-137.30078718    0.47929606    0.14813252    0.57764918    0.47209665]
 [ 722.66562528    0.3484        0.39299998    0.20560001    0.28260002]
 ...
 [-663.22299185    0.32350001    0.38280001    0.1595        0.40370002]
 [-847.78415028    0.20648749    0.26696005    0.15211184    0.28787765]
 [  56.1811773     0.32074243    0.44482422    0.14914243    0.37513033]][0m
[37m[1m[2023-07-10 11:49:36,391][227910] Max Reward on eval: 3259.7029500363396[0m
[37m[1m[2023-07-10 11:49:36,392][227910] Min Reward on eval: -1356.3325954817933[0m
[37m[1m[2023-07-10 11:49:36,392][227910] Mean Reward across all agents: -54.9221753580307[0m
[37m[1m[2023-07-10 11:49:36,392][227910] Average Trajectory Length: 919.9166666666666[0m
[36m[2023-07-10 11:49:36,395][227910] mean_value=-993.3046512468583, max_value=1048.1883456766234[0m
[37m[1m[2023-07-10 11:49:36,398][227910] New mean coefficients: [[ 2.2762663   0.01993141 -0.509282    0.99394256  1.2978327 ]][0m
[37m[1m[2023-07-10 11:49:36,399][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:49:46,071][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 11:49:46,071][227910] FPS: 397090.63[0m
[36m[2023-07-10 11:49:46,073][227910] itr=307, itrs=2000, Progress: 15.35%[0m
[36m[2023-07-10 11:49:57,593][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 11:49:57,593][227910] FPS: 333773.23[0m
[36m[2023-07-10 11:50:02,318][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:50:02,319][227910] Reward + Measures: [[4544.83026189    0.28705108    0.24755561    0.2520729     0.13932519]][0m
[37m[1m[2023-07-10 11:50:02,319][227910] Max Reward on eval: 4544.830261885629[0m
[37m[1m[2023-07-10 11:50:02,319][227910] Min Reward on eval: 4544.830261885629[0m
[37m[1m[2023-07-10 11:50:02,319][227910] Mean Reward across all agents: 4544.830261885629[0m
[37m[1m[2023-07-10 11:50:02,320][227910] Average Trajectory Length: 952.3286666666667[0m
[36m[2023-07-10 11:50:07,777][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:50:07,777][227910] Reward + Measures: [[-326.4807392     0.31518722    0.23960209    0.26336125    0.20637536]
 [ 706.87935737    0.26617271    0.28121075    0.18150827    0.26102564]
 [ 384.84949864    0.23991063    0.40035945    0.24020565    0.26359665]
 ...
 [ 919.85675587    0.28480002    0.35690004    0.2809        0.22230001]
 [1352.21830396    0.30626997    0.21303116    0.24641886    0.14228989]
 [2001.15045639    0.3098        0.30590001    0.25870004    0.11980001]][0m
[37m[1m[2023-07-10 11:50:07,778][227910] Max Reward on eval: 4333.340857781572[0m
[37m[1m[2023-07-10 11:50:07,778][227910] Min Reward on eval: -1146.6756571808132[0m
[37m[1m[2023-07-10 11:50:07,778][227910] Mean Reward across all agents: 770.2256458388196[0m
[37m[1m[2023-07-10 11:50:07,778][227910] Average Trajectory Length: 902.0413333333333[0m
[36m[2023-07-10 11:50:07,782][227910] mean_value=-1203.5553560491205, max_value=1658.7226062174668[0m
[37m[1m[2023-07-10 11:50:07,784][227910] New mean coefficients: [[ 2.2524548   0.42954287 -0.63355696  0.22047937  1.3419354 ]][0m
[37m[1m[2023-07-10 11:50:07,785][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:50:17,551][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 11:50:17,551][227910] FPS: 393281.31[0m
[36m[2023-07-10 11:50:17,554][227910] itr=308, itrs=2000, Progress: 15.40%[0m
[36m[2023-07-10 11:50:29,175][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 11:50:29,176][227910] FPS: 330899.04[0m
[36m[2023-07-10 11:50:33,951][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:50:33,951][227910] Reward + Measures: [[4952.06543227    0.28519201    0.23732556    0.24911176    0.10706981]][0m
[37m[1m[2023-07-10 11:50:33,951][227910] Max Reward on eval: 4952.065432265065[0m
[37m[1m[2023-07-10 11:50:33,952][227910] Min Reward on eval: 4952.065432265065[0m
[37m[1m[2023-07-10 11:50:33,952][227910] Mean Reward across all agents: 4952.065432265065[0m
[37m[1m[2023-07-10 11:50:33,952][227910] Average Trajectory Length: 965.7016666666666[0m
[36m[2023-07-10 11:50:39,357][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:50:39,357][227910] Reward + Measures: [[ 279.90699262    0.3053        0.33180001    0.34660003    0.21760002]
 [ 679.92287253    0.32401562    0.25143662    0.20781504    0.12852979]
 [-623.78172264    0.33329999    0.60170001    0.35679999    0.47820002]
 ...
 [ 840.17978305    0.35770544    0.33639455    0.2072991     0.1805059 ]
 [1251.82416116    0.32969999    0.34780002    0.30170003    0.2007    ]
 [ 555.51587787    0.33113009    0.37742516    0.26129422    0.27836609]][0m
[37m[1m[2023-07-10 11:50:39,357][227910] Max Reward on eval: 3984.0060355490423[0m
[37m[1m[2023-07-10 11:50:39,358][227910] Min Reward on eval: -1146.757860709168[0m
[37m[1m[2023-07-10 11:50:39,358][227910] Mean Reward across all agents: 795.2061987072893[0m
[37m[1m[2023-07-10 11:50:39,358][227910] Average Trajectory Length: 929.0366666666666[0m
[36m[2023-07-10 11:50:39,360][227910] mean_value=-1716.262334918473, max_value=991.982552702527[0m
[37m[1m[2023-07-10 11:50:39,363][227910] New mean coefficients: [[ 2.523931    0.9058579  -0.5215719  -0.29728174  1.2582561 ]][0m
[37m[1m[2023-07-10 11:50:39,364][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:50:49,094][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 11:50:49,095][227910] FPS: 394691.41[0m
[36m[2023-07-10 11:50:49,097][227910] itr=309, itrs=2000, Progress: 15.45%[0m
[36m[2023-07-10 11:51:00,589][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 11:51:00,590][227910] FPS: 334589.69[0m
[36m[2023-07-10 11:51:05,448][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:51:05,448][227910] Reward + Measures: [[5121.2073812     0.27924061    0.22670707    0.24391572    0.08706826]][0m
[37m[1m[2023-07-10 11:51:05,448][227910] Max Reward on eval: 5121.207381196941[0m
[37m[1m[2023-07-10 11:51:05,449][227910] Min Reward on eval: 5121.207381196941[0m
[37m[1m[2023-07-10 11:51:05,449][227910] Mean Reward across all agents: 5121.207381196941[0m
[37m[1m[2023-07-10 11:51:05,449][227910] Average Trajectory Length: 962.4456666666666[0m
[36m[2023-07-10 11:51:11,072][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:51:11,072][227910] Reward + Measures: [[1797.8884626     0.35546279    0.39897209    0.23170701    0.12720466]
 [-268.50076806    0.31452319    0.41777372    0.28110847    0.31971174]
 [ 486.54279312    0.29949999    0.45240003    0.1948        0.25560001]
 ...
 [ 618.13127424    0.20119999    0.2158        0.14479999    0.1517    ]
 [  35.59206812    0.27540001    0.54899997    0.0841        0.46069995]
 [ 523.15513751    0.24370001    0.51490003    0.17789999    0.46899995]][0m
[37m[1m[2023-07-10 11:51:11,072][227910] Max Reward on eval: 4538.472393631283[0m
[37m[1m[2023-07-10 11:51:11,073][227910] Min Reward on eval: -984.4311300873057[0m
[37m[1m[2023-07-10 11:51:11,073][227910] Mean Reward across all agents: 492.3193861240015[0m
[37m[1m[2023-07-10 11:51:11,073][227910] Average Trajectory Length: 927.4916666666667[0m
[36m[2023-07-10 11:51:11,077][227910] mean_value=-951.9930386777622, max_value=1092.4300930026657[0m
[37m[1m[2023-07-10 11:51:11,079][227910] New mean coefficients: [[ 2.801031    0.20467061 -0.19824004 -0.51040673  1.2299476 ]][0m
[37m[1m[2023-07-10 11:51:11,080][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:51:20,697][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 11:51:20,698][227910] FPS: 399371.49[0m
[36m[2023-07-10 11:51:20,700][227910] itr=310, itrs=2000, Progress: 15.50%[0m
[37m[1m[2023-07-10 11:51:23,039][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000290[0m
[36m[2023-07-10 11:51:34,991][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 11:51:34,991][227910] FPS: 328568.56[0m
[36m[2023-07-10 11:51:39,798][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:51:39,798][227910] Reward + Measures: [[5419.44062922    0.27785364    0.21897033    0.24478252    0.07500406]][0m
[37m[1m[2023-07-10 11:51:39,798][227910] Max Reward on eval: 5419.440629219504[0m
[37m[1m[2023-07-10 11:51:39,798][227910] Min Reward on eval: 5419.440629219504[0m
[37m[1m[2023-07-10 11:51:39,799][227910] Mean Reward across all agents: 5419.440629219504[0m
[37m[1m[2023-07-10 11:51:39,799][227910] Average Trajectory Length: 979.4903333333333[0m
[36m[2023-07-10 11:51:45,438][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:51:45,439][227910] Reward + Measures: [[342.86955654   0.20465875   0.2872172    0.26460871   0.2201734 ]
 [327.4615911    0.62066835   0.14227907   0.61680412   0.52433378]
 [940.53207089   0.19464557   0.26907668   0.22140351   0.15669261]
 ...
 [662.23019791   0.28777438   0.32474494   0.22870953   0.1364473 ]
 [ 66.65520126   0.22354721   0.23530047   0.19528043   0.28051409]
 [122.56825562   0.41445819   0.26747313   0.38218805   0.34496418]][0m
[37m[1m[2023-07-10 11:51:45,439][227910] Max Reward on eval: 4971.41703357175[0m
[37m[1m[2023-07-10 11:51:45,439][227910] Min Reward on eval: -682.8783011997526[0m
[37m[1m[2023-07-10 11:51:45,440][227910] Mean Reward across all agents: 1188.885126784742[0m
[37m[1m[2023-07-10 11:51:45,440][227910] Average Trajectory Length: 855.1166666666667[0m
[36m[2023-07-10 11:51:45,443][227910] mean_value=-1437.9125252584206, max_value=3110.915198456094[0m
[37m[1m[2023-07-10 11:51:45,446][227910] New mean coefficients: [[ 2.8211396  -0.1707361   0.57491887  0.03840965  1.2060667 ]][0m
[37m[1m[2023-07-10 11:51:45,447][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:51:55,184][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 11:51:55,184][227910] FPS: 394456.76[0m
[36m[2023-07-10 11:51:55,186][227910] itr=311, itrs=2000, Progress: 15.55%[0m
[36m[2023-07-10 11:52:06,828][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 11:52:06,829][227910] FPS: 330270.70[0m
[36m[2023-07-10 11:52:11,647][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:52:11,648][227910] Reward + Measures: [[5633.36632194    0.27630681    0.21127503    0.2436175     0.06438647]][0m
[37m[1m[2023-07-10 11:52:11,648][227910] Max Reward on eval: 5633.366321935637[0m
[37m[1m[2023-07-10 11:52:11,648][227910] Min Reward on eval: 5633.366321935637[0m
[37m[1m[2023-07-10 11:52:11,648][227910] Mean Reward across all agents: 5633.366321935637[0m
[37m[1m[2023-07-10 11:52:11,649][227910] Average Trajectory Length: 986.1519999999999[0m
[36m[2023-07-10 11:52:17,232][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:52:17,232][227910] Reward + Measures: [[-560.61596712    0.40510002    0.33720002    0.34920004    0.12970001]
 [2798.56184511    0.31331691    0.25128809    0.24294339    0.13716829]
 [ 268.87179434    0.18289714    0.19631295    0.11776265    0.08657956]
 ...
 [4375.03037374    0.30939999    0.22810002    0.25310001    0.0697    ]
 [-564.94210646    0.34980002    0.3427        0.25690001    0.3105    ]
 [1293.08523473    0.20726264    0.26907396    0.21127708    0.14631833]][0m
[37m[1m[2023-07-10 11:52:17,232][227910] Max Reward on eval: 5267.470493251644[0m
[37m[1m[2023-07-10 11:52:17,233][227910] Min Reward on eval: -755.7046883517469[0m
[37m[1m[2023-07-10 11:52:17,233][227910] Mean Reward across all agents: 1406.6363950868943[0m
[37m[1m[2023-07-10 11:52:17,233][227910] Average Trajectory Length: 854.1006666666666[0m
[36m[2023-07-10 11:52:17,235][227910] mean_value=-1911.4602496704933, max_value=3403.630646792178[0m
[37m[1m[2023-07-10 11:52:17,238][227910] New mean coefficients: [[ 3.1633906  -0.2670356   0.48919874 -0.20656417  0.93722355]][0m
[37m[1m[2023-07-10 11:52:17,239][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:52:26,958][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 11:52:26,959][227910] FPS: 395146.88[0m
[36m[2023-07-10 11:52:26,961][227910] itr=312, itrs=2000, Progress: 15.60%[0m
[36m[2023-07-10 11:52:38,461][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 11:52:38,461][227910] FPS: 334345.27[0m
[36m[2023-07-10 11:52:43,153][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:52:43,159][227910] Reward + Measures: [[5760.8556613     0.27823502    0.2101955     0.24538346    0.06288364]][0m
[37m[1m[2023-07-10 11:52:43,159][227910] Max Reward on eval: 5760.855661296042[0m
[37m[1m[2023-07-10 11:52:43,159][227910] Min Reward on eval: 5760.855661296042[0m
[37m[1m[2023-07-10 11:52:43,160][227910] Mean Reward across all agents: 5760.855661296042[0m
[37m[1m[2023-07-10 11:52:43,160][227910] Average Trajectory Length: 983.7766666666666[0m
[36m[2023-07-10 11:52:48,601][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:52:48,607][227910] Reward + Measures: [[1436.53861519    0.2545        0.36380002    0.27419999    0.22650002]
 [4171.89577413    0.26670003    0.26149997    0.26809999    0.0988    ]
 [ 766.52706356    0.37540448    0.22597165    0.19227761    0.22324328]
 ...
 [ 250.97662341    0.45220003    0.49490005    0.49649999    0.33919999]
 [1997.78043713    0.26607242    0.27891517    0.24219047    0.07189342]
 [1014.14589311    0.3019        0.3881        0.3224        0.25639999]][0m
[37m[1m[2023-07-10 11:52:48,608][227910] Max Reward on eval: 5750.243040423095[0m
[37m[1m[2023-07-10 11:52:48,609][227910] Min Reward on eval: -1482.0233340449863[0m
[37m[1m[2023-07-10 11:52:48,609][227910] Mean Reward across all agents: 1333.6423027948363[0m
[37m[1m[2023-07-10 11:52:48,610][227910] Average Trajectory Length: 928.9753333333333[0m
[36m[2023-07-10 11:52:48,615][227910] mean_value=-1285.0161187680753, max_value=2963.533077201587[0m
[37m[1m[2023-07-10 11:52:48,620][227910] New mean coefficients: [[ 3.4981     -0.34990713  0.49172735 -0.6127931   0.8329456 ]][0m
[37m[1m[2023-07-10 11:52:48,621][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:52:58,302][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 11:52:58,302][227910] FPS: 396761.54[0m
[36m[2023-07-10 11:52:58,305][227910] itr=313, itrs=2000, Progress: 15.65%[0m
[36m[2023-07-10 11:53:09,937][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 11:53:09,937][227910] FPS: 330623.34[0m
[36m[2023-07-10 11:53:14,816][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:53:14,817][227910] Reward + Measures: [[5949.16408123    0.26516995    0.20247485    0.23800114    0.04073144]][0m
[37m[1m[2023-07-10 11:53:14,817][227910] Max Reward on eval: 5949.164081234066[0m
[37m[1m[2023-07-10 11:53:14,817][227910] Min Reward on eval: 5949.164081234066[0m
[37m[1m[2023-07-10 11:53:14,818][227910] Mean Reward across all agents: 5949.164081234066[0m
[37m[1m[2023-07-10 11:53:14,818][227910] Average Trajectory Length: 986.2453333333333[0m
[36m[2023-07-10 11:53:20,489][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:53:20,490][227910] Reward + Measures: [[-113.88121078    0.2764        0.32370001    0.14459999    0.3143    ]
 [ 541.75575679    0.2794148     0.32441851    0.24027038    0.17977779]
 [1014.45887697    0.32482705    0.24980812    0.20937295    0.22066759]
 ...
 [3596.66287639    0.3128072     0.2606084     0.25013801    0.13975963]
 [ 236.40464494    0.43730003    0.54180002    0.16140001    0.5503    ]
 [ 240.4193974     0.40419513    0.43104878    0.19201463    0.44255853]][0m
[37m[1m[2023-07-10 11:53:20,490][227910] Max Reward on eval: 5555.707338938862[0m
[37m[1m[2023-07-10 11:53:20,490][227910] Min Reward on eval: -1399.569490548689[0m
[37m[1m[2023-07-10 11:53:20,491][227910] Mean Reward across all agents: 1036.4463327573778[0m
[37m[1m[2023-07-10 11:53:20,491][227910] Average Trajectory Length: 878.8246666666666[0m
[36m[2023-07-10 11:53:20,494][227910] mean_value=-1305.1733773459582, max_value=2000.1509060585336[0m
[37m[1m[2023-07-10 11:53:20,496][227910] New mean coefficients: [[ 3.5022142  -0.32937092  0.44349164 -0.40970516  0.88797826]][0m
[37m[1m[2023-07-10 11:53:20,497][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:53:30,138][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 11:53:30,138][227910] FPS: 398389.78[0m
[36m[2023-07-10 11:53:30,140][227910] itr=314, itrs=2000, Progress: 15.70%[0m
[36m[2023-07-10 11:53:41,552][227910] train() took 11.40 seconds to complete[0m
[36m[2023-07-10 11:53:41,552][227910] FPS: 336962.00[0m
[36m[2023-07-10 11:53:46,391][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:53:46,391][227910] Reward + Measures: [[6069.1604485     0.26196286    0.20079744    0.2366744     0.03747253]][0m
[37m[1m[2023-07-10 11:53:46,392][227910] Max Reward on eval: 6069.160448498711[0m
[37m[1m[2023-07-10 11:53:46,392][227910] Min Reward on eval: 6069.160448498711[0m
[37m[1m[2023-07-10 11:53:46,392][227910] Mean Reward across all agents: 6069.160448498711[0m
[37m[1m[2023-07-10 11:53:46,392][227910] Average Trajectory Length: 986.1859999999999[0m
[36m[2023-07-10 11:53:51,835][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:53:51,836][227910] Reward + Measures: [[1569.98591522    0.31370598    0.32341108    0.24132696    0.13664663]
 [1361.56999205    0.35447377    0.32316518    0.25063166    0.18539308]
 [1206.38346225    0.28166524    0.25300869    0.25218698    0.19921306]
 ...
 [1238.28675385    0.31225029    0.32031155    0.27873355    0.29434797]
 [-460.78569512    0.34343553    0.26841977    0.29623628    0.28516155]
 [-204.12546006    0.24042903    0.25203997    0.21519016    0.19303799]][0m
[37m[1m[2023-07-10 11:53:51,836][227910] Max Reward on eval: 4775.269784567366[0m
[37m[1m[2023-07-10 11:53:51,836][227910] Min Reward on eval: -989.9323640976916[0m
[37m[1m[2023-07-10 11:53:51,836][227910] Mean Reward across all agents: 809.879068351383[0m
[37m[1m[2023-07-10 11:53:51,837][227910] Average Trajectory Length: 902.3316666666666[0m
[36m[2023-07-10 11:53:51,839][227910] mean_value=-1604.8572703386376, max_value=1043.3209115140535[0m
[37m[1m[2023-07-10 11:53:51,841][227910] New mean coefficients: [[ 3.5181513   0.47382855  0.00405046 -0.41601503  0.8246486 ]][0m
[37m[1m[2023-07-10 11:53:51,842][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:54:01,564][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 11:54:01,564][227910] FPS: 395061.07[0m
[36m[2023-07-10 11:54:01,566][227910] itr=315, itrs=2000, Progress: 15.75%[0m
[36m[2023-07-10 11:54:13,005][227910] train() took 11.42 seconds to complete[0m
[36m[2023-07-10 11:54:13,005][227910] FPS: 336160.17[0m
[36m[2023-07-10 11:54:17,653][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:54:17,654][227910] Reward + Measures: [[6231.6864429     0.26000601    0.196854      0.23714681    0.03416702]][0m
[37m[1m[2023-07-10 11:54:17,654][227910] Max Reward on eval: 6231.686442903516[0m
[37m[1m[2023-07-10 11:54:17,654][227910] Min Reward on eval: 6231.686442903516[0m
[37m[1m[2023-07-10 11:54:17,654][227910] Mean Reward across all agents: 6231.686442903516[0m
[37m[1m[2023-07-10 11:54:17,655][227910] Average Trajectory Length: 990.9393333333333[0m
[36m[2023-07-10 11:54:23,149][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:54:23,150][227910] Reward + Measures: [[2016.4975646     0.359         0.31650001    0.30810001    0.24909997]
 [1282.85064048    0.30239996    0.22280002    0.20149998    0.14219999]
 [1604.48220025    0.33500001    0.30630001    0.27809998    0.2221    ]
 ...
 [2717.63175894    0.2812019     0.29392788    0.27323177    0.1865844 ]
 [3724.98849568    0.2933        0.27800003    0.2854        0.20840001]
 [1581.48853249    0.38140604    0.32384571    0.23683067    0.207598  ]][0m
[37m[1m[2023-07-10 11:54:23,150][227910] Max Reward on eval: 5337.344465231057[0m
[37m[1m[2023-07-10 11:54:23,150][227910] Min Reward on eval: -694.9733408384957[0m
[37m[1m[2023-07-10 11:54:23,151][227910] Mean Reward across all agents: 1013.5894898385066[0m
[37m[1m[2023-07-10 11:54:23,151][227910] Average Trajectory Length: 919.0503333333334[0m
[36m[2023-07-10 11:54:23,155][227910] mean_value=-916.2089618819643, max_value=2920.0776018380775[0m
[37m[1m[2023-07-10 11:54:23,157][227910] New mean coefficients: [[ 3.7892022   0.55828416  0.24794835 -1.006339    1.0171472 ]][0m
[37m[1m[2023-07-10 11:54:23,158][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:54:32,842][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 11:54:32,843][227910] FPS: 396597.66[0m
[36m[2023-07-10 11:54:32,845][227910] itr=316, itrs=2000, Progress: 15.80%[0m
[36m[2023-07-10 11:54:44,321][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 11:54:44,321][227910] FPS: 335055.33[0m
[36m[2023-07-10 11:54:49,180][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:54:49,180][227910] Reward + Measures: [[6294.72916405    0.25490376    0.19145608    0.23337549    0.03475953]][0m
[37m[1m[2023-07-10 11:54:49,181][227910] Max Reward on eval: 6294.729164048048[0m
[37m[1m[2023-07-10 11:54:49,181][227910] Min Reward on eval: 6294.729164048048[0m
[37m[1m[2023-07-10 11:54:49,181][227910] Mean Reward across all agents: 6294.729164048048[0m
[37m[1m[2023-07-10 11:54:49,181][227910] Average Trajectory Length: 987.5303333333333[0m
[36m[2023-07-10 11:54:54,653][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:54:54,653][227910] Reward + Measures: [[1251.5411031     0.35974833    0.42648903    0.18630077    0.25650299]
 [2409.16934405    0.26872703    0.23501535    0.27261448    0.09345599]
 [ 368.25666003    0.57730132    0.67031252    0.21646814    0.47043616]
 ...
 [2580.56055493    0.24882288    0.21512584    0.234375      0.06834655]
 [2787.27166675    0.24371357    0.21108711    0.21257459    0.09394982]
 [ 669.68361021    0.55805659    0.69911182    0.12619655    0.53266555]][0m
[37m[1m[2023-07-10 11:54:54,653][227910] Max Reward on eval: 5251.8185490617525[0m
[37m[1m[2023-07-10 11:54:54,654][227910] Min Reward on eval: -838.7625550447032[0m
[37m[1m[2023-07-10 11:54:54,654][227910] Mean Reward across all agents: 1260.561239257266[0m
[37m[1m[2023-07-10 11:54:54,654][227910] Average Trajectory Length: 854.8296666666666[0m
[36m[2023-07-10 11:54:54,657][227910] mean_value=-1461.5245545587532, max_value=1698.498624160001[0m
[37m[1m[2023-07-10 11:54:54,660][227910] New mean coefficients: [[ 3.8073163   0.74755156 -0.99936193 -0.98987335  1.0232296 ]][0m
[37m[1m[2023-07-10 11:54:54,661][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:55:04,317][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 11:55:04,317][227910] FPS: 397730.75[0m
[36m[2023-07-10 11:55:04,320][227910] itr=317, itrs=2000, Progress: 15.85%[0m
[36m[2023-07-10 11:55:15,951][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 11:55:15,952][227910] FPS: 330618.06[0m
[36m[2023-07-10 11:55:20,763][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:55:20,763][227910] Reward + Measures: [[6414.64294214    0.2549617     0.18727487    0.23417126    0.03444039]][0m
[37m[1m[2023-07-10 11:55:20,763][227910] Max Reward on eval: 6414.64294213936[0m
[37m[1m[2023-07-10 11:55:20,764][227910] Min Reward on eval: 6414.64294213936[0m
[37m[1m[2023-07-10 11:55:20,764][227910] Mean Reward across all agents: 6414.64294213936[0m
[37m[1m[2023-07-10 11:55:20,764][227910] Average Trajectory Length: 988.9803333333333[0m
[36m[2023-07-10 11:55:26,446][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:55:26,447][227910] Reward + Measures: [[-320.44861367    0.13399999    0.1859        0.1117        0.1123    ]
 [-358.32257873    0.36088943    0.19271085    0.29322946    0.27476832]
 [-429.26016752    0.39998233    0.21432944    0.32542357    0.28242943]
 ...
 [1207.22041174    0.268347      0.404129      0.21226993    0.23369293]
 [1002.0117849     0.4131        0.35640001    0.32000002    0.26660001]
 [-229.17427217    0.50953722    0.59691375    0.45438239    0.53835487]][0m
[37m[1m[2023-07-10 11:55:26,447][227910] Max Reward on eval: 5971.556237458624[0m
[37m[1m[2023-07-10 11:55:26,447][227910] Min Reward on eval: -882.0951008265954[0m
[37m[1m[2023-07-10 11:55:26,447][227910] Mean Reward across all agents: 415.7673717656412[0m
[37m[1m[2023-07-10 11:55:26,448][227910] Average Trajectory Length: 896.5403333333333[0m
[36m[2023-07-10 11:55:26,449][227910] mean_value=-1873.1004742941223, max_value=1034.9883665824946[0m
[37m[1m[2023-07-10 11:55:26,451][227910] New mean coefficients: [[ 3.8920188   1.3691828  -0.10392171 -1.2541479   1.2797623 ]][0m
[37m[1m[2023-07-10 11:55:26,452][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:55:36,214][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 11:55:36,214][227910] FPS: 393464.26[0m
[36m[2023-07-10 11:55:36,216][227910] itr=318, itrs=2000, Progress: 15.90%[0m
[36m[2023-07-10 11:55:47,860][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 11:55:47,861][227910] FPS: 330279.64[0m
[36m[2023-07-10 11:55:52,657][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:55:52,658][227910] Reward + Measures: [[6539.45262168    0.25047329    0.18138826    0.22825681    0.03168211]][0m
[37m[1m[2023-07-10 11:55:52,658][227910] Max Reward on eval: 6539.452621684815[0m
[37m[1m[2023-07-10 11:55:52,658][227910] Min Reward on eval: 6539.452621684815[0m
[37m[1m[2023-07-10 11:55:52,658][227910] Mean Reward across all agents: 6539.452621684815[0m
[37m[1m[2023-07-10 11:55:52,658][227910] Average Trajectory Length: 991.259[0m
[36m[2023-07-10 11:55:58,127][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:55:58,127][227910] Reward + Measures: [[ -70.59888739    0.34412941    0.31389698    0.26995459    0.22636962]
 [2078.90418798    0.43150002    0.39090005    0.25209999    0.18210001]
 [ -65.22444781    0.34279999    0.3283        0.27810001    0.27200001]
 ...
 [ 871.12800479    0.3104237     0.43815514    0.25163838    0.24467495]
 [ 513.15525724    0.37339997    0.50299996    0.2676        0.29250002]
 [ 301.76231646    0.366         0.40970001    0.161         0.26980001]][0m
[37m[1m[2023-07-10 11:55:58,127][227910] Max Reward on eval: 3895.051446463971[0m
[37m[1m[2023-07-10 11:55:58,128][227910] Min Reward on eval: -1194.2942386475158[0m
[37m[1m[2023-07-10 11:55:58,128][227910] Mean Reward across all agents: 523.313821986953[0m
[37m[1m[2023-07-10 11:55:58,128][227910] Average Trajectory Length: 929.9916666666667[0m
[36m[2023-07-10 11:55:58,130][227910] mean_value=-1936.2096658083879, max_value=1287.8797737141792[0m
[37m[1m[2023-07-10 11:55:58,132][227910] New mean coefficients: [[ 3.6762066   0.6334381   0.40487283 -1.4547268   0.8964015 ]][0m
[37m[1m[2023-07-10 11:55:58,133][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:56:07,905][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 11:56:07,905][227910] FPS: 393051.47[0m
[36m[2023-07-10 11:56:07,907][227910] itr=319, itrs=2000, Progress: 15.95%[0m
[36m[2023-07-10 11:56:19,504][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 11:56:19,505][227910] FPS: 331547.34[0m
[36m[2023-07-10 11:56:24,266][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:56:24,267][227910] Reward + Measures: [[6563.22086375    0.24088044    0.17691407    0.22017971    0.02312572]][0m
[37m[1m[2023-07-10 11:56:24,267][227910] Max Reward on eval: 6563.220863751197[0m
[37m[1m[2023-07-10 11:56:24,267][227910] Min Reward on eval: 6563.220863751197[0m
[37m[1m[2023-07-10 11:56:24,267][227910] Mean Reward across all agents: 6563.220863751197[0m
[37m[1m[2023-07-10 11:56:24,267][227910] Average Trajectory Length: 982.8006666666666[0m
[36m[2023-07-10 11:56:29,747][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:56:29,747][227910] Reward + Measures: [[  74.95907969    0.24055152    0.2915535     0.21957521    0.23190723]
 [  60.79594717    0.18827482    0.22093919    0.17445       0.18642037]
 [ 302.35932004    0.26665959    0.36382243    0.21467698    0.26509759]
 ...
 [ -94.92488707    0.2763187     0.33520538    0.20866565    0.34226888]
 [2950.07880789    0.30770001    0.27680001    0.25659999    0.20369999]
 [ 440.50445416    0.3856        0.42469999    0.2256        0.26080003]][0m
[37m[1m[2023-07-10 11:56:29,748][227910] Max Reward on eval: 5652.498266506567[0m
[37m[1m[2023-07-10 11:56:29,748][227910] Min Reward on eval: -1139.8479349921631[0m
[37m[1m[2023-07-10 11:56:29,748][227910] Mean Reward across all agents: 896.520057788511[0m
[37m[1m[2023-07-10 11:56:29,748][227910] Average Trajectory Length: 912.8656666666666[0m
[36m[2023-07-10 11:56:29,750][227910] mean_value=-1400.6093685272385, max_value=970.1003102674566[0m
[37m[1m[2023-07-10 11:56:29,753][227910] New mean coefficients: [[ 3.3654954   0.41083875  0.139824   -1.1068673   1.0498317 ]][0m
[37m[1m[2023-07-10 11:56:29,754][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:56:39,414][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 11:56:39,415][227910] FPS: 397554.19[0m
[36m[2023-07-10 11:56:39,417][227910] itr=320, itrs=2000, Progress: 16.00%[0m
[37m[1m[2023-07-10 11:56:41,785][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000300[0m
[36m[2023-07-10 11:56:53,544][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 11:56:53,544][227910] FPS: 333816.01[0m
[36m[2023-07-10 11:56:58,227][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:56:58,227][227910] Reward + Measures: [[6712.14699725    0.23999287    0.17083319    0.21837522    0.0206135 ]][0m
[37m[1m[2023-07-10 11:56:58,228][227910] Max Reward on eval: 6712.146997247793[0m
[37m[1m[2023-07-10 11:56:58,228][227910] Min Reward on eval: 6712.146997247793[0m
[37m[1m[2023-07-10 11:56:58,228][227910] Mean Reward across all agents: 6712.146997247793[0m
[37m[1m[2023-07-10 11:56:58,228][227910] Average Trajectory Length: 987.221[0m
[36m[2023-07-10 11:57:03,696][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:57:03,701][227910] Reward + Measures: [[2417.87263166    0.22654171    0.27717915    0.21599166    0.11543719]
 [-399.61425819    0.52640003    0.3572        0.45760003    0.31900001]
 [ 130.88342928    0.26139686    0.27987811    0.30225846    0.16422431]
 ...
 [-155.80152803    0.36665905    0.42166868    0.27410123    0.30965301]
 [1841.10568382    0.25291315    0.31886706    0.2404152     0.12065788]
 [ 275.151761      0.34600002    0.43129998    0.31620002    0.29680002]][0m
[37m[1m[2023-07-10 11:57:03,702][227910] Max Reward on eval: 6033.881131046265[0m
[37m[1m[2023-07-10 11:57:03,702][227910] Min Reward on eval: -1215.2767023721476[0m
[37m[1m[2023-07-10 11:57:03,702][227910] Mean Reward across all agents: 678.2498605681114[0m
[37m[1m[2023-07-10 11:57:03,703][227910] Average Trajectory Length: 814.294[0m
[36m[2023-07-10 11:57:03,704][227910] mean_value=-2508.128895282628, max_value=568.0722614348451[0m
[37m[1m[2023-07-10 11:57:03,707][227910] New mean coefficients: [[ 3.5058827   0.13563624 -0.10761578 -1.1155764   0.81990755]][0m
[37m[1m[2023-07-10 11:57:03,708][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:57:13,363][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 11:57:13,364][227910] FPS: 397760.91[0m
[36m[2023-07-10 11:57:13,366][227910] itr=321, itrs=2000, Progress: 16.05%[0m
[36m[2023-07-10 11:57:24,919][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 11:57:24,919][227910] FPS: 332825.90[0m
[36m[2023-07-10 11:57:29,750][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:57:29,750][227910] Reward + Measures: [[6770.75728138    0.25210518    0.17701113    0.22612508    0.02797261]][0m
[37m[1m[2023-07-10 11:57:29,751][227910] Max Reward on eval: 6770.757281382571[0m
[37m[1m[2023-07-10 11:57:29,751][227910] Min Reward on eval: 6770.757281382571[0m
[37m[1m[2023-07-10 11:57:29,751][227910] Mean Reward across all agents: 6770.757281382571[0m
[37m[1m[2023-07-10 11:57:29,751][227910] Average Trajectory Length: 993.1546666666667[0m
[36m[2023-07-10 11:57:35,355][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:57:35,356][227910] Reward + Measures: [[  43.60302164    0.35746849    0.223102      0.33065102    0.20897137]
 [ 631.47056936    0.34076437    0.30444932    0.30010825    0.22869639]
 [ 953.00679413    0.24113981    0.30196625    0.23717272    0.12478962]
 ...
 [ 472.56525476    0.35654655    0.28478035    0.27930745    0.21474352]
 [ 230.53748142    0.30192086    0.28513095    0.2542946     0.24115829]
 [1372.699464      0.20952545    0.22658108    0.16422938    0.09475609]][0m
[37m[1m[2023-07-10 11:57:35,356][227910] Max Reward on eval: 6121.688288712874[0m
[37m[1m[2023-07-10 11:57:35,356][227910] Min Reward on eval: -1347.8310315595359[0m
[37m[1m[2023-07-10 11:57:35,356][227910] Mean Reward across all agents: 613.594535172434[0m
[37m[1m[2023-07-10 11:57:35,357][227910] Average Trajectory Length: 876.9423333333333[0m
[36m[2023-07-10 11:57:35,359][227910] mean_value=-2374.4129478877558, max_value=474.25807439520304[0m
[37m[1m[2023-07-10 11:57:35,361][227910] New mean coefficients: [[ 2.9618807  -0.07771422 -0.5203085  -0.24616373  0.89429593]][0m
[37m[1m[2023-07-10 11:57:35,362][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:57:45,163][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 11:57:45,163][227910] FPS: 391863.92[0m
[36m[2023-07-10 11:57:45,166][227910] itr=322, itrs=2000, Progress: 16.10%[0m
[36m[2023-07-10 11:57:56,821][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 11:57:56,821][227910] FPS: 329967.75[0m
[36m[2023-07-10 11:58:01,524][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:58:01,525][227910] Reward + Measures: [[6881.28315121    0.23847973    0.16980042    0.21756524    0.01956331]][0m
[37m[1m[2023-07-10 11:58:01,525][227910] Max Reward on eval: 6881.283151211705[0m
[37m[1m[2023-07-10 11:58:01,525][227910] Min Reward on eval: 6881.283151211705[0m
[37m[1m[2023-07-10 11:58:01,525][227910] Mean Reward across all agents: 6881.283151211705[0m
[37m[1m[2023-07-10 11:58:01,526][227910] Average Trajectory Length: 991.8856666666667[0m
[36m[2023-07-10 11:58:07,028][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:58:07,029][227910] Reward + Measures: [[ 759.68128897    0.24370001    0.35859999    0.28480002    0.2956    ]
 [ 663.59551111    0.21068476    0.29821604    0.21982424    0.2201488 ]
 [ 775.57675844    0.21970776    0.29176924    0.21612935    0.23393691]
 ...
 [1387.28750182    0.33231542    0.48814616    0.17695385    0.30670768]
 [ 684.93850602    0.3827        0.4937        0.1751        0.32860002]
 [ 112.6439431     0.16760001    0.31110001    0.19580001    0.227     ]][0m
[37m[1m[2023-07-10 11:58:07,029][227910] Max Reward on eval: 5462.253231910328[0m
[37m[1m[2023-07-10 11:58:07,029][227910] Min Reward on eval: -1141.3541064084973[0m
[37m[1m[2023-07-10 11:58:07,029][227910] Mean Reward across all agents: 711.2583017785221[0m
[37m[1m[2023-07-10 11:58:07,030][227910] Average Trajectory Length: 873.9[0m
[36m[2023-07-10 11:58:07,032][227910] mean_value=-1330.733201085451, max_value=586.5526254998698[0m
[37m[1m[2023-07-10 11:58:07,035][227910] New mean coefficients: [[ 3.1592093   0.03352439 -0.54884964 -0.64623785  1.0877745 ]][0m
[37m[1m[2023-07-10 11:58:07,036][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:58:16,779][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 11:58:16,780][227910] FPS: 394177.04[0m
[36m[2023-07-10 11:58:16,782][227910] itr=323, itrs=2000, Progress: 16.15%[0m
[36m[2023-07-10 11:58:28,468][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 11:58:28,469][227910] FPS: 329014.80[0m
[36m[2023-07-10 11:58:33,317][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:58:33,317][227910] Reward + Measures: [[6904.86103861    0.23789889    0.16852377    0.21651697    0.02152615]][0m
[37m[1m[2023-07-10 11:58:33,318][227910] Max Reward on eval: 6904.861038606427[0m
[37m[1m[2023-07-10 11:58:33,318][227910] Min Reward on eval: 6904.861038606427[0m
[37m[1m[2023-07-10 11:58:33,318][227910] Mean Reward across all agents: 6904.861038606427[0m
[37m[1m[2023-07-10 11:58:33,319][227910] Average Trajectory Length: 985.8449999999999[0m
[36m[2023-07-10 11:58:38,817][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:58:38,818][227910] Reward + Measures: [[  39.2547315     0.23529425    0.25341567    0.20939635    0.1949247 ]
 [ 724.44807908    0.27495298    0.46430597    0.34281459    0.34840068]
 [1051.69876116    0.34972468    0.32404971    0.24171217    0.193095  ]
 ...
 [ 352.99271855    0.24354306    0.28289834    0.24887042    0.22166203]
 [ -29.85503318    0.29910126    0.41947499    0.25933346    0.27480745]
 [1102.77509517    0.31133175    0.27489701    0.19836994    0.09462702]][0m
[37m[1m[2023-07-10 11:58:38,818][227910] Max Reward on eval: 4908.717065800639[0m
[37m[1m[2023-07-10 11:58:38,818][227910] Min Reward on eval: -1334.8485656583914[0m
[37m[1m[2023-07-10 11:58:38,818][227910] Mean Reward across all agents: 472.19914820014276[0m
[37m[1m[2023-07-10 11:58:38,819][227910] Average Trajectory Length: 824.1666666666666[0m
[36m[2023-07-10 11:58:38,820][227910] mean_value=-2307.5021643739087, max_value=2868.534686975185[0m
[37m[1m[2023-07-10 11:58:38,822][227910] New mean coefficients: [[ 3.2868705   0.32854915  0.0214138  -1.0331297   0.8833701 ]][0m
[37m[1m[2023-07-10 11:58:38,824][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:58:48,505][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 11:58:48,505][227910] FPS: 396696.71[0m
[36m[2023-07-10 11:58:48,508][227910] itr=324, itrs=2000, Progress: 16.20%[0m
[36m[2023-07-10 11:59:00,032][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 11:59:00,032][227910] FPS: 333701.92[0m
[36m[2023-07-10 11:59:04,835][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:59:04,836][227910] Reward + Measures: [[6829.46978421    0.23302349    0.16566746    0.21760678    0.01896861]][0m
[37m[1m[2023-07-10 11:59:04,836][227910] Max Reward on eval: 6829.469784212393[0m
[37m[1m[2023-07-10 11:59:04,836][227910] Min Reward on eval: 6829.469784212393[0m
[37m[1m[2023-07-10 11:59:04,836][227910] Mean Reward across all agents: 6829.469784212393[0m
[37m[1m[2023-07-10 11:59:04,837][227910] Average Trajectory Length: 985.2213333333333[0m
[36m[2023-07-10 11:59:10,328][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:59:10,381][227910] Reward + Measures: [[1987.89474545    0.2863        0.31780002    0.25670001    0.2208    ]
 [ 546.50137135    0.18201855    0.20387423    0.1658992     0.11852691]
 [ 827.09835897    0.2379        0.4601        0.28509998    0.18919998]
 ...
 [ 300.21221863    0.36709997    0.43340001    0.23430002    0.35570002]
 [2247.71536377    0.26560739    0.33629927    0.23335004    0.17813541]
 [ 201.50024436    0.2994        0.398         0.21069999    0.45690003]][0m
[37m[1m[2023-07-10 11:59:10,381][227910] Max Reward on eval: 5156.068362217024[0m
[37m[1m[2023-07-10 11:59:10,381][227910] Min Reward on eval: -1057.8129161278775[0m
[37m[1m[2023-07-10 11:59:10,381][227910] Mean Reward across all agents: 600.3627162726563[0m
[37m[1m[2023-07-10 11:59:10,382][227910] Average Trajectory Length: 876.6053333333333[0m
[36m[2023-07-10 11:59:10,384][227910] mean_value=-1760.9703091075694, max_value=490.4090029785327[0m
[37m[1m[2023-07-10 11:59:10,387][227910] New mean coefficients: [[ 4.02429    -0.278831    0.7286069  -1.9856217   0.09190917]][0m
[37m[1m[2023-07-10 11:59:10,388][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:59:20,090][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 11:59:20,091][227910] FPS: 395824.58[0m
[36m[2023-07-10 11:59:20,093][227910] itr=325, itrs=2000, Progress: 16.25%[0m
[36m[2023-07-10 11:59:31,640][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 11:59:31,640][227910] FPS: 333037.17[0m
[36m[2023-07-10 11:59:36,479][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:59:36,479][227910] Reward + Measures: [[6877.45580415    0.23072115    0.16497707    0.21513756    0.0165905 ]][0m
[37m[1m[2023-07-10 11:59:36,479][227910] Max Reward on eval: 6877.45580415152[0m
[37m[1m[2023-07-10 11:59:36,480][227910] Min Reward on eval: 6877.45580415152[0m
[37m[1m[2023-07-10 11:59:36,480][227910] Mean Reward across all agents: 6877.45580415152[0m
[37m[1m[2023-07-10 11:59:36,480][227910] Average Trajectory Length: 987.3159999999999[0m
[36m[2023-07-10 11:59:42,010][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 11:59:42,011][227910] Reward + Measures: [[ 175.54410174    0.23739998    0.35870001    0.2211        0.33519998]
 [-486.92247846    0.47451478    0.31696722    0.31613281    0.23062132]
 [ 351.06802894    0.30109832    0.34605041    0.28047103    0.18087764]
 ...
 [ 885.24032534    0.26368278    0.29270831    0.21443544    0.08530595]
 [ 677.5484444     0.34440002    0.3576        0.27880001    0.29370001]
 [ 433.54466136    0.42489997    0.37630001    0.35960001    0.32720003]][0m
[37m[1m[2023-07-10 11:59:42,011][227910] Max Reward on eval: 4397.122224513068[0m
[37m[1m[2023-07-10 11:59:42,011][227910] Min Reward on eval: -1122.0544105864597[0m
[37m[1m[2023-07-10 11:59:42,011][227910] Mean Reward across all agents: 335.4757758711688[0m
[37m[1m[2023-07-10 11:59:42,012][227910] Average Trajectory Length: 916.228[0m
[36m[2023-07-10 11:59:42,013][227910] mean_value=-1863.3393503572465, max_value=487.0421121157298[0m
[37m[1m[2023-07-10 11:59:42,016][227910] New mean coefficients: [[ 4.0460167  -0.4606918   0.8194843  -2.1460655   0.01065386]][0m
[37m[1m[2023-07-10 11:59:42,017][227910] Moving the mean solution point...[0m
[36m[2023-07-10 11:59:51,917][227910] train() took 9.90 seconds to complete[0m
[36m[2023-07-10 11:59:51,917][227910] FPS: 387964.88[0m
[36m[2023-07-10 11:59:51,919][227910] itr=326, itrs=2000, Progress: 16.30%[0m
[36m[2023-07-10 12:00:03,445][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 12:00:03,445][227910] FPS: 333607.37[0m
[36m[2023-07-10 12:00:08,163][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:00:08,163][227910] Reward + Measures: [[7029.15528785    0.23352565    0.16418636    0.21731612    0.01844093]][0m
[37m[1m[2023-07-10 12:00:08,163][227910] Max Reward on eval: 7029.1552878493385[0m
[37m[1m[2023-07-10 12:00:08,164][227910] Min Reward on eval: 7029.1552878493385[0m
[37m[1m[2023-07-10 12:00:08,164][227910] Mean Reward across all agents: 7029.1552878493385[0m
[37m[1m[2023-07-10 12:00:08,164][227910] Average Trajectory Length: 991.3883333333333[0m
[36m[2023-07-10 12:00:13,806][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:00:13,807][227910] Reward + Measures: [[  166.21017155     0.77143228     0.07708065     0.78018278
      0.46837637]
 [ -958.55381678     0.3757         0.10339999     0.44949999
      0.36210001]
 [ 1006.90466722     0.40990001     0.40839997     0.2703
      0.21949999]
 ...
 [  817.60098711     0.26869997     0.34540001     0.2474
      0.22800003]
 [ -940.05355836     0.21510001     0.22940002     0.1184
      0.2096    ]
 [-1386.70404457     0.41640002     0.0422         0.42969999
      0.4068    ]][0m
[37m[1m[2023-07-10 12:00:13,807][227910] Max Reward on eval: 2921.407561408644[0m
[37m[1m[2023-07-10 12:00:13,807][227910] Min Reward on eval: -1515.0251229316927[0m
[37m[1m[2023-07-10 12:00:13,808][227910] Mean Reward across all agents: -24.49429843684675[0m
[37m[1m[2023-07-10 12:00:13,808][227910] Average Trajectory Length: 928.3576666666667[0m
[36m[2023-07-10 12:00:13,812][227910] mean_value=-1357.25087980088, max_value=1197.3465619732965[0m
[37m[1m[2023-07-10 12:00:13,815][227910] New mean coefficients: [[ 4.0088863  -0.98433286  1.0813184  -2.1668098  -0.14841741]][0m
[37m[1m[2023-07-10 12:00:13,816][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:00:23,540][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 12:00:23,540][227910] FPS: 394970.67[0m
[36m[2023-07-10 12:00:23,543][227910] itr=327, itrs=2000, Progress: 16.35%[0m
[36m[2023-07-10 12:00:35,018][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 12:00:35,018][227910] FPS: 335152.14[0m
[36m[2023-07-10 12:00:39,816][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:00:39,822][227910] Reward + Measures: [[1241.54682223    0.2141231     0.46339861    0.2398244     0.24029712]][0m
[37m[1m[2023-07-10 12:00:39,822][227910] Max Reward on eval: 1241.5468222311822[0m
[37m[1m[2023-07-10 12:00:39,823][227910] Min Reward on eval: 1241.5468222311822[0m
[37m[1m[2023-07-10 12:00:39,823][227910] Mean Reward across all agents: 1241.5468222311822[0m
[37m[1m[2023-07-10 12:00:39,823][227910] Average Trajectory Length: 969.4746666666666[0m
[36m[2023-07-10 12:00:45,284][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:00:45,284][227910] Reward + Measures: [[628.99399968   0.28224981   0.4165093    0.18060735   0.2742199 ]
 [-48.29249873   0.30689999   0.29960001   0.1176       0.28130004]
 [ 23.77579638   0.14782898   0.26175189   0.14000426   0.21241891]
 ...
 [850.89993469   0.24419999   0.49570003   0.24779999   0.24000001]
 [438.18711398   0.27763686   0.434268     0.21475415   0.25535297]
 [306.90904638   0.32546535   0.47743517   0.20693715   0.32532907]][0m
[37m[1m[2023-07-10 12:00:45,284][227910] Max Reward on eval: 1568.2643716393504[0m
[37m[1m[2023-07-10 12:00:45,285][227910] Min Reward on eval: -783.8918477870582[0m
[37m[1m[2023-07-10 12:00:45,285][227910] Mean Reward across all agents: 319.271217039079[0m
[37m[1m[2023-07-10 12:00:45,285][227910] Average Trajectory Length: 820.8263333333333[0m
[36m[2023-07-10 12:00:45,287][227910] mean_value=-1100.3335657451826, max_value=578.0867565855933[0m
[37m[1m[2023-07-10 12:00:45,289][227910] New mean coefficients: [[ 3.6219897  -1.2065147   0.93741584 -1.2264789  -0.22802894]][0m
[37m[1m[2023-07-10 12:00:45,290][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:00:55,090][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 12:00:55,091][227910] FPS: 391884.78[0m
[36m[2023-07-10 12:00:55,093][227910] itr=328, itrs=2000, Progress: 16.40%[0m
[36m[2023-07-10 12:01:06,864][227910] train() took 11.75 seconds to complete[0m
[36m[2023-07-10 12:01:06,864][227910] FPS: 326722.96[0m
[36m[2023-07-10 12:01:11,637][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:01:11,637][227910] Reward + Measures: [[1502.28330264    0.22573903    0.32464314    0.21336739    0.10992541]][0m
[37m[1m[2023-07-10 12:01:11,637][227910] Max Reward on eval: 1502.2833026378062[0m
[37m[1m[2023-07-10 12:01:11,638][227910] Min Reward on eval: 1502.2833026378062[0m
[37m[1m[2023-07-10 12:01:11,638][227910] Mean Reward across all agents: 1502.2833026378062[0m
[37m[1m[2023-07-10 12:01:11,638][227910] Average Trajectory Length: 480.00399999999996[0m
[36m[2023-07-10 12:01:17,067][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:01:17,068][227910] Reward + Measures: [[144.21872959   0.37204298   0.38914496   0.28705826   0.23940051]
 [-52.92381321   0.345        0.41869998   0.39320001   0.2299    ]
 [274.22306186   0.2131927    0.36888781   0.19010524   0.1957521 ]
 ...
 [947.57657038   0.34189999   0.42269999   0.2705       0.2261    ]
 [569.70178378   0.25160167   0.22546363   0.22075453   0.21822762]
 [244.16051473   0.29288647   0.30303243   0.2477027    0.28467026]][0m
[37m[1m[2023-07-10 12:01:17,068][227910] Max Reward on eval: 1860.4327702854528[0m
[37m[1m[2023-07-10 12:01:17,068][227910] Min Reward on eval: -942.9911987493048[0m
[37m[1m[2023-07-10 12:01:17,069][227910] Mean Reward across all agents: 468.2441087262963[0m
[37m[1m[2023-07-10 12:01:17,069][227910] Average Trajectory Length: 931.3779999999999[0m
[36m[2023-07-10 12:01:17,070][227910] mean_value=-1981.8873527106089, max_value=99.54121581204589[0m
[37m[1m[2023-07-10 12:01:17,073][227910] New mean coefficients: [[ 2.856497   -1.3506265   0.70110434 -0.18682873  0.3331064 ]][0m
[37m[1m[2023-07-10 12:01:17,074][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:01:26,783][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 12:01:26,783][227910] FPS: 395582.43[0m
[36m[2023-07-10 12:01:26,786][227910] itr=329, itrs=2000, Progress: 16.45%[0m
[36m[2023-07-10 12:01:38,238][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 12:01:38,238][227910] FPS: 335815.17[0m
[36m[2023-07-10 12:01:43,084][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:01:43,085][227910] Reward + Measures: [[1971.41603367    0.27052864    0.33669341    0.24986146    0.14329407]][0m
[37m[1m[2023-07-10 12:01:43,085][227910] Max Reward on eval: 1971.4160336676675[0m
[37m[1m[2023-07-10 12:01:43,085][227910] Min Reward on eval: 1971.4160336676675[0m
[37m[1m[2023-07-10 12:01:43,086][227910] Mean Reward across all agents: 1971.4160336676675[0m
[37m[1m[2023-07-10 12:01:43,086][227910] Average Trajectory Length: 928.0586666666667[0m
[36m[2023-07-10 12:01:48,509][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:01:48,561][227910] Reward + Measures: [[ -67.97481997    0.31000003    0.32120001    0.26180002    0.24790001]
 [ 659.04357088    0.24925061    0.37016186    0.23822041    0.21938157]
 [ 219.27485334    0.2484128     0.32427081    0.2509062     0.24878018]
 ...
 [-163.35917589    0.34937903    0.2033999     0.33961493    0.28687057]
 [-736.87583687    0.3612        0.26159999    0.29630002    0.32170001]
 [   6.80896985    0.21827717    0.25771177    0.18908575    0.17665544]][0m
[37m[1m[2023-07-10 12:01:48,562][227910] Max Reward on eval: 2248.1845565814992[0m
[37m[1m[2023-07-10 12:01:48,562][227910] Min Reward on eval: -823.4583631600486[0m
[37m[1m[2023-07-10 12:01:48,562][227910] Mean Reward across all agents: 434.3844291046321[0m
[37m[1m[2023-07-10 12:01:48,562][227910] Average Trajectory Length: 858.8233333333333[0m
[36m[2023-07-10 12:01:48,564][227910] mean_value=-2495.3793414252964, max_value=323.5569189119968[0m
[37m[1m[2023-07-10 12:01:48,566][227910] New mean coefficients: [[ 2.13787    -1.4972854   0.38322327  0.654892    0.9396639 ]][0m
[37m[1m[2023-07-10 12:01:48,567][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:01:58,481][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 12:01:58,481][227910] FPS: 387399.52[0m
[36m[2023-07-10 12:01:58,483][227910] itr=330, itrs=2000, Progress: 16.50%[0m
[37m[1m[2023-07-10 12:02:00,902][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000310[0m
[36m[2023-07-10 12:02:12,871][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 12:02:12,872][227910] FPS: 328050.36[0m
[36m[2023-07-10 12:02:17,692][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:02:17,693][227910] Reward + Measures: [[2224.15889861    0.26846248    0.32961071    0.24555743    0.12970348]][0m
[37m[1m[2023-07-10 12:02:17,693][227910] Max Reward on eval: 2224.158898613056[0m
[37m[1m[2023-07-10 12:02:17,693][227910] Min Reward on eval: 2224.158898613056[0m
[37m[1m[2023-07-10 12:02:17,693][227910] Mean Reward across all agents: 2224.158898613056[0m
[37m[1m[2023-07-10 12:02:17,693][227910] Average Trajectory Length: 928.1506666666667[0m
[36m[2023-07-10 12:02:23,351][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:02:23,352][227910] Reward + Measures: [[ 447.30313599    0.20050953    0.25715351    0.22736745    0.21569219]
 [1249.73502113    0.25403336    0.3100135     0.2536554     0.18838643]
 [ 845.87613179    0.22837964    0.29600501    0.23271374    0.20309515]
 ...
 [1319.5925607     0.24646516    0.30483907    0.26157457    0.18767133]
 [1284.28041803    0.22351606    0.28260991    0.22283769    0.19555274]
 [ 947.03200835    0.2181311     0.27690345    0.25525972    0.21552496]][0m
[37m[1m[2023-07-10 12:02:23,352][227910] Max Reward on eval: 2499.2475243127556[0m
[37m[1m[2023-07-10 12:02:23,352][227910] Min Reward on eval: 84.63157061735691[0m
[37m[1m[2023-07-10 12:02:23,353][227910] Mean Reward across all agents: 1230.4496328527566[0m
[37m[1m[2023-07-10 12:02:23,353][227910] Average Trajectory Length: 862.9649999999999[0m
[36m[2023-07-10 12:02:23,354][227910] mean_value=-2290.1732471487276, max_value=-196.04125980697347[0m
[36m[2023-07-10 12:02:23,356][227910] XNES is restarting with a new solution whose measures are [0.4608328  0.32525083 0.57715905 0.47518688] and objective is -50.00777465726132[0m
[36m[2023-07-10 12:02:23,357][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 12:02:23,360][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 12:02:23,361][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:02:33,059][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 12:02:33,059][227910] FPS: 396012.08[0m
[36m[2023-07-10 12:02:33,061][227910] itr=331, itrs=2000, Progress: 16.55%[0m
[36m[2023-07-10 12:02:44,613][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 12:02:44,613][227910] FPS: 332884.97[0m
[36m[2023-07-10 12:02:49,544][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:02:49,545][227910] Reward + Measures: [[209.15737722   0.37274975   0.2707893    0.44063166   0.29265749]][0m
[37m[1m[2023-07-10 12:02:49,545][227910] Max Reward on eval: 209.15737721654526[0m
[37m[1m[2023-07-10 12:02:49,545][227910] Min Reward on eval: 209.15737721654526[0m
[37m[1m[2023-07-10 12:02:49,546][227910] Mean Reward across all agents: 209.15737721654526[0m
[37m[1m[2023-07-10 12:02:49,546][227910] Average Trajectory Length: 985.6803333333334[0m
[36m[2023-07-10 12:02:55,070][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:02:55,071][227910] Reward + Measures: [[  162.9591396      0.4677         0.62519997     0.45050001
      0.57199997]
 [-1093.74362215     0.26755235     0.38657942     0.27540785
      0.29824501]
 [  201.95447039     0.23509574     0.35188866     0.29061991
      0.20526008]
 ...
 [ -264.54581167     0.25079998     0.40899998     0.3468
      0.2554    ]
 [-1124.81484306     0.21670401     0.41269514     0.21700005
      0.29398912]
 [-1245.69915152     0.45409414     0.69971174     0.41060591
      0.62792355]][0m
[37m[1m[2023-07-10 12:02:55,071][227910] Max Reward on eval: 862.2370092315483[0m
[37m[1m[2023-07-10 12:02:55,071][227910] Min Reward on eval: -1602.1190332777564[0m
[37m[1m[2023-07-10 12:02:55,071][227910] Mean Reward across all agents: -191.776222291839[0m
[37m[1m[2023-07-10 12:02:55,072][227910] Average Trajectory Length: 953.8026666666666[0m
[36m[2023-07-10 12:02:55,074][227910] mean_value=-1561.397769963159, max_value=646.5427647343295[0m
[37m[1m[2023-07-10 12:02:55,077][227910] New mean coefficients: [[-1.0546758  -0.09283733 -0.32118082 -1.6942624  -2.6147792 ]][0m
[37m[1m[2023-07-10 12:02:55,078][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:03:04,879][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 12:03:04,880][227910] FPS: 391829.12[0m
[36m[2023-07-10 12:03:04,882][227910] itr=332, itrs=2000, Progress: 16.60%[0m
[36m[2023-07-10 12:03:16,615][227910] train() took 11.72 seconds to complete[0m
[36m[2023-07-10 12:03:16,616][227910] FPS: 327691.17[0m
[36m[2023-07-10 12:03:21,383][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:03:21,384][227910] Reward + Measures: [[-207.61195566    0.54136306    0.21530513    0.61130387    0.42151836]][0m
[37m[1m[2023-07-10 12:03:21,384][227910] Max Reward on eval: -207.61195566122979[0m
[37m[1m[2023-07-10 12:03:21,384][227910] Min Reward on eval: -207.61195566122979[0m
[37m[1m[2023-07-10 12:03:21,384][227910] Mean Reward across all agents: -207.61195566122979[0m
[37m[1m[2023-07-10 12:03:21,384][227910] Average Trajectory Length: 982.7416666666667[0m
[36m[2023-07-10 12:03:26,964][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:03:26,964][227910] Reward + Measures: [[ -116.10230329     0.42539999     0.18520001     0.505
      0.27309999]
 [ -774.68023372     0.44524446     0.32427779     0.36059999
      0.35479629]
 [ -328.54013978     0.33705661     0.36003661     0.39796162
      0.31090212]
 ...
 [-1074.99326287     0.15275043     0.36985233     0.24813783
      0.26882038]
 [ -126.0527462      0.35756513     0.29874718     0.4872191
      0.32537016]
 [  -57.80697113     0.21873823     0.44809285     0.30125082
      0.21253982]][0m
[37m[1m[2023-07-10 12:03:26,965][227910] Max Reward on eval: 584.6435845833854[0m
[37m[1m[2023-07-10 12:03:26,965][227910] Min Reward on eval: -1318.637569443113[0m
[37m[1m[2023-07-10 12:03:26,965][227910] Mean Reward across all agents: -395.25334269507255[0m
[37m[1m[2023-07-10 12:03:26,965][227910] Average Trajectory Length: 947.9596666666666[0m
[36m[2023-07-10 12:03:26,970][227910] mean_value=-794.2810257733418, max_value=695.994888403691[0m
[37m[1m[2023-07-10 12:03:26,973][227910] New mean coefficients: [[-1.0296438   0.20545322  0.02876174 -1.7350315  -1.8155148 ]][0m
[37m[1m[2023-07-10 12:03:26,974][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:03:36,786][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 12:03:36,787][227910] FPS: 391396.00[0m
[36m[2023-07-10 12:03:36,789][227910] itr=333, itrs=2000, Progress: 16.65%[0m
[36m[2023-07-10 12:03:48,382][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 12:03:48,382][227910] FPS: 331667.53[0m
[36m[2023-07-10 12:03:53,018][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:03:53,019][227910] Reward + Measures: [[-332.43571534    0.58984107    0.17337699    0.64216053    0.44198406]][0m
[37m[1m[2023-07-10 12:03:53,019][227910] Max Reward on eval: -332.4357153398559[0m
[37m[1m[2023-07-10 12:03:53,019][227910] Min Reward on eval: -332.4357153398559[0m
[37m[1m[2023-07-10 12:03:53,020][227910] Mean Reward across all agents: -332.4357153398559[0m
[37m[1m[2023-07-10 12:03:53,020][227910] Average Trajectory Length: 980.8383333333333[0m
[36m[2023-07-10 12:03:58,309][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:03:58,309][227910] Reward + Measures: [[-1059.03163521     0.59758151     0.62472248     0.64141178
      0.5943082 ]
 [ -712.72428541     0.69597787     0.74256366     0.76750129
      0.67488444]
 [-1187.11506273     0.64686263     0.69046515     0.6467638
      0.66044503]
 ...
 [ -957.53208883     0.35237533     0.39926106     0.33897406
      0.30029091]
 [ -830.04029444     0.30690989     0.32160211     0.28820595
      0.23014556]
 [-1210.1295862      0.75553226     0.76154441     0.74081004
      0.76695824]][0m
[37m[1m[2023-07-10 12:03:58,309][227910] Max Reward on eval: 171.99912693140794[0m
[37m[1m[2023-07-10 12:03:58,310][227910] Min Reward on eval: -1696.2008483547718[0m
[37m[1m[2023-07-10 12:03:58,310][227910] Mean Reward across all agents: -664.0557703424994[0m
[37m[1m[2023-07-10 12:03:58,310][227910] Average Trajectory Length: 915.6506666666667[0m
[36m[2023-07-10 12:03:58,313][227910] mean_value=-571.4866200258042, max_value=632.9918602547039[0m
[37m[1m[2023-07-10 12:03:58,316][227910] New mean coefficients: [[ 0.6071204  -0.19221422 -0.03556148 -0.9883263  -1.0693928 ]][0m
[37m[1m[2023-07-10 12:03:58,317][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:04:08,069][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 12:04:08,070][227910] FPS: 393815.67[0m
[36m[2023-07-10 12:04:08,072][227910] itr=334, itrs=2000, Progress: 16.70%[0m
[36m[2023-07-10 12:04:19,629][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 12:04:19,629][227910] FPS: 332741.25[0m
[36m[2023-07-10 12:04:24,424][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:04:24,424][227910] Reward + Measures: [[-148.34628117    0.53662694    0.33295092    0.55920017    0.13747841]][0m
[37m[1m[2023-07-10 12:04:24,424][227910] Max Reward on eval: -148.34628116769557[0m
[37m[1m[2023-07-10 12:04:24,425][227910] Min Reward on eval: -148.34628116769557[0m
[37m[1m[2023-07-10 12:04:24,425][227910] Mean Reward across all agents: -148.34628116769557[0m
[37m[1m[2023-07-10 12:04:24,425][227910] Average Trajectory Length: 987.0713333333333[0m
[36m[2023-07-10 12:04:30,058][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:04:30,058][227910] Reward + Measures: [[-262.686412      0.41060001    0.22150002    0.44959998    0.2357    ]
 [-151.41402084    0.43869996    0.4073        0.46569997    0.07910001]
 [-892.41148665    0.2033        0.42020002    0.37129998    0.2201    ]
 ...
 [-400.76069443    0.82989997    0.48340002    0.80839998    0.35750002]
 [  26.56746198    0.86269999    0.70290005    0.86779994    0.023     ]
 [  56.72904936    0.3705        0.36549997    0.4364        0.16849999]][0m
[37m[1m[2023-07-10 12:04:30,058][227910] Max Reward on eval: 298.2524184496084[0m
[37m[1m[2023-07-10 12:04:30,059][227910] Min Reward on eval: -1500.3492606818793[0m
[37m[1m[2023-07-10 12:04:30,059][227910] Mean Reward across all agents: -379.1877238334731[0m
[37m[1m[2023-07-10 12:04:30,059][227910] Average Trajectory Length: 964.4493333333334[0m
[36m[2023-07-10 12:04:30,066][227910] mean_value=-310.3947753912866, max_value=794.9164331522828[0m
[37m[1m[2023-07-10 12:04:30,069][227910] New mean coefficients: [[ 1.1265444  -0.19283202  0.13463065 -1.2271218  -0.61150753]][0m
[37m[1m[2023-07-10 12:04:30,070][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:04:39,920][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 12:04:39,920][227910] FPS: 389920.15[0m
[36m[2023-07-10 12:04:39,922][227910] itr=335, itrs=2000, Progress: 16.75%[0m
[36m[2023-07-10 12:04:51,475][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 12:04:51,475][227910] FPS: 332877.25[0m
[36m[2023-07-10 12:04:56,254][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:04:56,255][227910] Reward + Measures: [[-79.67923563   0.57660753   0.36681876   0.585926     0.11709363]][0m
[37m[1m[2023-07-10 12:04:56,255][227910] Max Reward on eval: -79.67923562595287[0m
[37m[1m[2023-07-10 12:04:56,255][227910] Min Reward on eval: -79.67923562595287[0m
[37m[1m[2023-07-10 12:04:56,256][227910] Mean Reward across all agents: -79.67923562595287[0m
[37m[1m[2023-07-10 12:04:56,256][227910] Average Trajectory Length: 981.36[0m
[36m[2023-07-10 12:05:01,667][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:05:01,668][227910] Reward + Measures: [[-350.19606998    0.50800002    0.36399999    0.51969999    0.0301    ]
 [-729.42421893    0.35178921    0.2004108     0.37578377    0.2093865 ]
 [-205.65380174    0.3950159     0.31148216    0.43412682    0.06705648]
 ...
 [-374.85570239    0.20470253    0.24339846    0.35466698    0.09461096]
 [ 152.32553398    0.48494291    0.35183182    0.59403414    0.07926241]
 [ -74.80047403    0.62099999    0.37919998    0.63850003    0.1122    ]][0m
[37m[1m[2023-07-10 12:05:01,668][227910] Max Reward on eval: 354.74778232553507[0m
[37m[1m[2023-07-10 12:05:01,668][227910] Min Reward on eval: -1378.754571328644[0m
[37m[1m[2023-07-10 12:05:01,668][227910] Mean Reward across all agents: -217.42068556576695[0m
[37m[1m[2023-07-10 12:05:01,669][227910] Average Trajectory Length: 930.7846666666667[0m
[36m[2023-07-10 12:05:01,674][227910] mean_value=-971.1922897045483, max_value=780.5687880210287[0m
[37m[1m[2023-07-10 12:05:01,677][227910] New mean coefficients: [[ 0.76886183  0.0157215  -0.10855831 -1.4156585  -1.138536  ]][0m
[37m[1m[2023-07-10 12:05:01,678][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:05:11,257][227910] train() took 9.58 seconds to complete[0m
[36m[2023-07-10 12:05:11,257][227910] FPS: 400924.13[0m
[36m[2023-07-10 12:05:11,260][227910] itr=336, itrs=2000, Progress: 16.80%[0m
[36m[2023-07-10 12:05:22,814][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 12:05:22,814][227910] FPS: 332848.83[0m
[36m[2023-07-10 12:05:27,641][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:05:27,641][227910] Reward + Measures: [[-140.09696309    0.51539069    0.22362833    0.5159452     0.23646024]][0m
[37m[1m[2023-07-10 12:05:27,642][227910] Max Reward on eval: -140.09696309005977[0m
[37m[1m[2023-07-10 12:05:27,642][227910] Min Reward on eval: -140.09696309005977[0m
[37m[1m[2023-07-10 12:05:27,642][227910] Mean Reward across all agents: -140.09696309005977[0m
[37m[1m[2023-07-10 12:05:27,642][227910] Average Trajectory Length: 984.9333333333333[0m
[36m[2023-07-10 12:05:33,179][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:05:33,231][227910] Reward + Measures: [[-420.98307337    0.48794213    0.40007925    0.47210702    0.38707167]
 [-741.37541595    0.39769998    0.40799999    0.38390002    0.31420001]
 [-655.7930201     0.48597321    0.53620237    0.50645423    0.40834042]
 ...
 [-445.04462499    0.48480001    0.25130001    0.43849999    0.2218    ]
 [  62.62414445    0.3184        0.3238        0.36960003    0.2124    ]
 [-404.47604792    0.43970004    0.27719998    0.45340005    0.20040002]][0m
[37m[1m[2023-07-10 12:05:33,232][227910] Max Reward on eval: 695.1993617450469[0m
[37m[1m[2023-07-10 12:05:33,233][227910] Min Reward on eval: -1113.801914629864[0m
[37m[1m[2023-07-10 12:05:33,234][227910] Mean Reward across all agents: -154.94040094156693[0m
[37m[1m[2023-07-10 12:05:33,235][227910] Average Trajectory Length: 966.4473333333333[0m
[36m[2023-07-10 12:05:33,254][227910] mean_value=-234.44673435526653, max_value=930.6060093524086[0m
[37m[1m[2023-07-10 12:05:33,261][227910] New mean coefficients: [[ 1.9463191  -0.44921687 -0.35819155 -0.2360202  -0.35674614]][0m
[37m[1m[2023-07-10 12:05:33,264][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:05:42,987][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 12:05:42,987][227910] FPS: 395050.03[0m
[36m[2023-07-10 12:05:42,989][227910] itr=337, itrs=2000, Progress: 16.85%[0m
[36m[2023-07-10 12:05:54,428][227910] train() took 11.42 seconds to complete[0m
[36m[2023-07-10 12:05:54,428][227910] FPS: 336228.19[0m
[36m[2023-07-10 12:05:59,324][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:05:59,325][227910] Reward + Measures: [[-79.82992144   0.51319391   0.22995137   0.52185625   0.22582793]][0m
[37m[1m[2023-07-10 12:05:59,325][227910] Max Reward on eval: -79.82992143886968[0m
[37m[1m[2023-07-10 12:05:59,325][227910] Min Reward on eval: -79.82992143886968[0m
[37m[1m[2023-07-10 12:05:59,326][227910] Mean Reward across all agents: -79.82992143886968[0m
[37m[1m[2023-07-10 12:05:59,326][227910] Average Trajectory Length: 983.855[0m
[36m[2023-07-10 12:06:04,840][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:06:04,840][227910] Reward + Measures: [[-216.05242945    0.41220132    0.45404348    0.42971802    0.35094222]
 [-140.59131137    0.70410001    0.56760001    0.68430001    0.40869999]
 [-114.00665889    0.58140004    0.57880002    0.49630004    0.46490002]
 ...
 [-304.49020493    0.63780004    0.57530004    0.62350005    0.42399999]
 [-519.92906217    0.51220006    0.20050001    0.50570005    0.27350003]
 [-207.40323882    0.61440003    0.64580005    0.47760001    0.55190003]][0m
[37m[1m[2023-07-10 12:06:04,841][227910] Max Reward on eval: 371.93954962978603[0m
[37m[1m[2023-07-10 12:06:04,841][227910] Min Reward on eval: -1367.7221806022512[0m
[37m[1m[2023-07-10 12:06:04,841][227910] Mean Reward across all agents: -372.4427156830647[0m
[37m[1m[2023-07-10 12:06:04,841][227910] Average Trajectory Length: 965.4636666666667[0m
[36m[2023-07-10 12:06:04,845][227910] mean_value=-634.501112844666, max_value=471.98842935272495[0m
[37m[1m[2023-07-10 12:06:04,848][227910] New mean coefficients: [[ 0.6728529   0.19653168  0.02665141 -0.7449575  -1.2116834 ]][0m
[37m[1m[2023-07-10 12:06:04,849][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:06:14,540][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 12:06:14,541][227910] FPS: 396286.07[0m
[36m[2023-07-10 12:06:14,543][227910] itr=338, itrs=2000, Progress: 16.90%[0m
[36m[2023-07-10 12:06:26,139][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 12:06:26,139][227910] FPS: 331605.15[0m
[36m[2023-07-10 12:06:30,968][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:06:30,969][227910] Reward + Measures: [[-40.33048815   0.43216065   0.25211078   0.45754158   0.18808004]][0m
[37m[1m[2023-07-10 12:06:30,969][227910] Max Reward on eval: -40.33048814805726[0m
[37m[1m[2023-07-10 12:06:30,969][227910] Min Reward on eval: -40.33048814805726[0m
[37m[1m[2023-07-10 12:06:30,969][227910] Mean Reward across all agents: -40.33048814805726[0m
[37m[1m[2023-07-10 12:06:30,969][227910] Average Trajectory Length: 982.2396666666666[0m
[36m[2023-07-10 12:06:36,620][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:06:36,620][227910] Reward + Measures: [[-569.71586539    0.37350002    0.30590001    0.43210003    0.2264    ]
 [-914.75309821    0.28389999    0.3461        0.32540002    0.17020001]
 [-688.24416528    0.38230002    0.32150003    0.44830003    0.23169999]
 ...
 [-336.84950786    0.42260003    0.28290001    0.45140001    0.18340001]
 [-343.04611726    0.40780002    0.25920001    0.42440006    0.21269999]
 [-614.35782309    0.36059999    0.37559998    0.35150003    0.23279999]][0m
[37m[1m[2023-07-10 12:06:36,621][227910] Max Reward on eval: 380.9944378145563[0m
[37m[1m[2023-07-10 12:06:36,621][227910] Min Reward on eval: -1859.5425353588769[0m
[37m[1m[2023-07-10 12:06:36,621][227910] Mean Reward across all agents: -463.94227029023807[0m
[37m[1m[2023-07-10 12:06:36,622][227910] Average Trajectory Length: 946.5686666666667[0m
[36m[2023-07-10 12:06:36,624][227910] mean_value=-1530.1671009878366, max_value=476.64476044933076[0m
[37m[1m[2023-07-10 12:06:36,626][227910] New mean coefficients: [[-0.13161582  0.12012602 -0.39451778 -0.49478826 -1.024611  ]][0m
[37m[1m[2023-07-10 12:06:36,627][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:06:46,539][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 12:06:46,539][227910] FPS: 387500.53[0m
[36m[2023-07-10 12:06:46,541][227910] itr=339, itrs=2000, Progress: 16.95%[0m
[36m[2023-07-10 12:06:58,226][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 12:06:58,226][227910] FPS: 329063.70[0m
[36m[2023-07-10 12:07:03,062][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:07:03,063][227910] Reward + Measures: [[-35.57236001   0.37655053   0.25413111   0.40651146   0.16055748]][0m
[37m[1m[2023-07-10 12:07:03,063][227910] Max Reward on eval: -35.57236000904381[0m
[37m[1m[2023-07-10 12:07:03,063][227910] Min Reward on eval: -35.57236000904381[0m
[37m[1m[2023-07-10 12:07:03,063][227910] Mean Reward across all agents: -35.57236000904381[0m
[37m[1m[2023-07-10 12:07:03,064][227910] Average Trajectory Length: 982.0763333333333[0m
[36m[2023-07-10 12:07:08,531][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:07:08,532][227910] Reward + Measures: [[-181.48499643    0.34990001    0.26890001    0.36880001    0.15220001]
 [-877.05883297    0.80057728    0.79339212    0.756697      0.75980395]
 [ -73.17186766    0.4030827     0.22156894    0.41618085    0.20194045]
 ...
 [  32.07592735    0.20572816    0.26205534    0.28060973    0.1388505 ]
 [ 393.12089707    0.45786381    0.43812567    0.47603041    0.32729051]
 [  57.03720065    0.54689997    0.44039997    0.5363        0.3716    ]][0m
[37m[1m[2023-07-10 12:07:08,532][227910] Max Reward on eval: 630.0011666857769[0m
[37m[1m[2023-07-10 12:07:08,532][227910] Min Reward on eval: -1897.9960640510428[0m
[37m[1m[2023-07-10 12:07:08,532][227910] Mean Reward across all agents: -263.05287830521144[0m
[37m[1m[2023-07-10 12:07:08,533][227910] Average Trajectory Length: 923.151[0m
[36m[2023-07-10 12:07:08,535][227910] mean_value=-1535.9304852839796, max_value=385.7931567651026[0m
[37m[1m[2023-07-10 12:07:08,538][227910] New mean coefficients: [[ 0.3265933   1.0271797  -0.4423489  -0.41375536 -0.6149118 ]][0m
[37m[1m[2023-07-10 12:07:08,539][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:07:18,416][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 12:07:18,417][227910] FPS: 388824.93[0m
[36m[2023-07-10 12:07:18,419][227910] itr=340, itrs=2000, Progress: 17.00%[0m
[37m[1m[2023-07-10 12:07:20,874][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000320[0m
[36m[2023-07-10 12:07:32,726][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 12:07:32,726][227910] FPS: 331450.57[0m
[36m[2023-07-10 12:07:37,543][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:07:37,544][227910] Reward + Measures: [[1.52051359 0.34854504 0.25770906 0.38345996 0.14523199]][0m
[37m[1m[2023-07-10 12:07:37,544][227910] Max Reward on eval: 1.520513591468658[0m
[37m[1m[2023-07-10 12:07:37,544][227910] Min Reward on eval: 1.520513591468658[0m
[37m[1m[2023-07-10 12:07:37,545][227910] Mean Reward across all agents: 1.520513591468658[0m
[37m[1m[2023-07-10 12:07:37,545][227910] Average Trajectory Length: 982.1013333333333[0m
[36m[2023-07-10 12:07:43,076][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:07:43,077][227910] Reward + Measures: [[ -908.0097758      0.81316423     0.09702172     0.79222673
      0.60974473]
 [-1442.95455272     0.67379999     0.15450001     0.68650001
      0.47349998]
 [-1394.02824188     0.77179998     0.10950001     0.77279997
      0.4975    ]
 ...
 [ -243.41701405     0.70830005     0.13090001     0.64700001
      0.40459999]
 [  144.99896609     0.29328898     0.32069084     0.36512634
      0.09750696]
 [  153.05303949     0.21040002     0.2369         0.2647
      0.11189999]][0m
[37m[1m[2023-07-10 12:07:43,077][227910] Max Reward on eval: 486.18171694188493[0m
[37m[1m[2023-07-10 12:07:43,077][227910] Min Reward on eval: -1877.4638946991413[0m
[37m[1m[2023-07-10 12:07:43,078][227910] Mean Reward across all agents: -323.37011343195127[0m
[37m[1m[2023-07-10 12:07:43,078][227910] Average Trajectory Length: 957.7306666666666[0m
[36m[2023-07-10 12:07:43,080][227910] mean_value=-1357.9481196103536, max_value=423.04451366561307[0m
[37m[1m[2023-07-10 12:07:43,083][227910] New mean coefficients: [[ 0.52818555  0.584779   -0.51605976 -0.7758781  -0.8431517 ]][0m
[37m[1m[2023-07-10 12:07:43,084][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:07:52,811][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 12:07:52,811][227910] FPS: 394854.20[0m
[36m[2023-07-10 12:07:52,813][227910] itr=341, itrs=2000, Progress: 17.05%[0m
[36m[2023-07-10 12:08:04,438][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 12:08:04,438][227910] FPS: 330761.82[0m
[36m[2023-07-10 12:08:09,162][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:08:09,163][227910] Reward + Measures: [[59.57193319  0.31045097  0.26599634  0.35331208  0.12801917]][0m
[37m[1m[2023-07-10 12:08:09,163][227910] Max Reward on eval: 59.571933189442035[0m
[37m[1m[2023-07-10 12:08:09,163][227910] Min Reward on eval: 59.571933189442035[0m
[37m[1m[2023-07-10 12:08:09,163][227910] Mean Reward across all agents: 59.571933189442035[0m
[37m[1m[2023-07-10 12:08:09,163][227910] Average Trajectory Length: 981.3383333333333[0m
[36m[2023-07-10 12:08:14,642][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:08:14,643][227910] Reward + Measures: [[  67.58088136    0.2009        0.27150002    0.26480001    0.0851    ]
 [  36.02912017    0.44129997    0.25330001    0.477         0.13630001]
 [-514.99740375    0.71550006    0.35030004    0.64939994    0.61770004]
 ...
 [-419.01209441    0.40291038    0.46558163    0.31428507    0.41206208]
 [ 133.0506474     0.55080003    0.25120002    0.54319996    0.27210003]
 [ 149.13596977    0.57296962    0.40968475    0.56960005    0.09874783]][0m
[37m[1m[2023-07-10 12:08:14,643][227910] Max Reward on eval: 511.22408248514404[0m
[37m[1m[2023-07-10 12:08:14,643][227910] Min Reward on eval: -742.3153967512656[0m
[37m[1m[2023-07-10 12:08:14,643][227910] Mean Reward across all agents: -132.1048094013024[0m
[37m[1m[2023-07-10 12:08:14,644][227910] Average Trajectory Length: 977.1606666666667[0m
[36m[2023-07-10 12:08:14,646][227910] mean_value=-1461.3651205174856, max_value=539.9887261568452[0m
[37m[1m[2023-07-10 12:08:14,649][227910] New mean coefficients: [[ 0.19094008  1.568906    0.1126852  -1.2648333  -1.9760429 ]][0m
[37m[1m[2023-07-10 12:08:14,650][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:08:24,519][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 12:08:24,519][227910] FPS: 389157.61[0m
[36m[2023-07-10 12:08:24,522][227910] itr=342, itrs=2000, Progress: 17.10%[0m
[36m[2023-07-10 12:08:36,113][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 12:08:36,113][227910] FPS: 331839.09[0m
[36m[2023-07-10 12:08:40,827][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:08:40,828][227910] Reward + Measures: [[102.28845266   0.2814644    0.26789281   0.32616293   0.11267007]][0m
[37m[1m[2023-07-10 12:08:40,828][227910] Max Reward on eval: 102.28845265576872[0m
[37m[1m[2023-07-10 12:08:40,828][227910] Min Reward on eval: 102.28845265576872[0m
[37m[1m[2023-07-10 12:08:40,829][227910] Mean Reward across all agents: 102.28845265576872[0m
[37m[1m[2023-07-10 12:08:40,829][227910] Average Trajectory Length: 979.3743333333333[0m
[36m[2023-07-10 12:08:46,443][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:08:46,443][227910] Reward + Measures: [[   7.11507612    0.58777863    0.23512392    0.57704991    0.15342382]
 [ -23.38052348    0.24006502    0.32756558    0.29846558    0.11800054]
 [-148.70500271    0.28607535    0.21470551    0.34678379    0.1309855 ]
 ...
 [-330.24614704    0.4752        0.4632        0.52069998    0.40839997]
 [-176.24540798    0.24532893    0.28237802    0.30439374    0.09322138]
 [-153.99816073    0.24779999    0.28270003    0.2703        0.0761    ]][0m
[37m[1m[2023-07-10 12:08:46,444][227910] Max Reward on eval: 381.7539767060196[0m
[37m[1m[2023-07-10 12:08:46,444][227910] Min Reward on eval: -1126.7577658226305[0m
[37m[1m[2023-07-10 12:08:46,444][227910] Mean Reward across all agents: -241.62701148652607[0m
[37m[1m[2023-07-10 12:08:46,444][227910] Average Trajectory Length: 937.9473333333333[0m
[36m[2023-07-10 12:08:46,446][227910] mean_value=-1749.1391400750451, max_value=353.69461615662095[0m
[37m[1m[2023-07-10 12:08:46,449][227910] New mean coefficients: [[-0.05024798  1.5191272   0.719224   -0.04801953 -2.9672477 ]][0m
[37m[1m[2023-07-10 12:08:46,450][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:08:56,212][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 12:08:56,212][227910] FPS: 393445.61[0m
[36m[2023-07-10 12:08:56,214][227910] itr=343, itrs=2000, Progress: 17.15%[0m
[36m[2023-07-10 12:09:07,760][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 12:09:07,760][227910] FPS: 333020.93[0m
[36m[2023-07-10 12:09:12,467][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:09:12,468][227910] Reward + Measures: [[80.01841573  0.28987679  0.2657885   0.32955849  0.10980929]][0m
[37m[1m[2023-07-10 12:09:12,468][227910] Max Reward on eval: 80.01841573052464[0m
[37m[1m[2023-07-10 12:09:12,468][227910] Min Reward on eval: 80.01841573052464[0m
[37m[1m[2023-07-10 12:09:12,469][227910] Mean Reward across all agents: 80.01841573052464[0m
[37m[1m[2023-07-10 12:09:12,469][227910] Average Trajectory Length: 977.9943333333333[0m
[36m[2023-07-10 12:09:17,975][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:09:17,981][227910] Reward + Measures: [[  -9.63852769    0.23337272    0.27796593    0.29187575    0.09339886]
 [-876.26018828    0.19722202    0.1609184     0.24362834    0.07263664]
 [-416.77974548    0.37392288    0.2494487     0.3962439     0.13293004]
 ...
 [-563.9804543     0.38741821    0.22426479    0.40885672    0.14119886]
 [ 304.16369131    0.241042      0.30376428    0.26065144    0.15954633]
 [ 731.91635595    0.24800412    0.29275155    0.30444437    0.10274743]][0m
[37m[1m[2023-07-10 12:09:17,981][227910] Max Reward on eval: 1141.8769474389555[0m
[37m[1m[2023-07-10 12:09:17,981][227910] Min Reward on eval: -989.747570699011[0m
[37m[1m[2023-07-10 12:09:17,981][227910] Mean Reward across all agents: -196.0043793434785[0m
[37m[1m[2023-07-10 12:09:17,982][227910] Average Trajectory Length: 816.0586666666667[0m
[36m[2023-07-10 12:09:17,983][227910] mean_value=-2782.135252967942, max_value=217.63777684558852[0m
[37m[1m[2023-07-10 12:09:17,986][227910] New mean coefficients: [[-0.19164245  2.8275583   0.5870547   0.4142176  -1.5541852 ]][0m
[37m[1m[2023-07-10 12:09:17,987][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:09:27,659][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 12:09:27,659][227910] FPS: 397089.34[0m
[36m[2023-07-10 12:09:27,661][227910] itr=344, itrs=2000, Progress: 17.20%[0m
[36m[2023-07-10 12:09:39,161][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 12:09:39,161][227910] FPS: 334358.34[0m
[36m[2023-07-10 12:09:43,954][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:09:43,955][227910] Reward + Measures: [[54.16746276  0.35383359  0.27294654  0.38828519  0.10941392]][0m
[37m[1m[2023-07-10 12:09:43,955][227910] Max Reward on eval: 54.16746276390537[0m
[37m[1m[2023-07-10 12:09:43,955][227910] Min Reward on eval: 54.16746276390537[0m
[37m[1m[2023-07-10 12:09:43,955][227910] Mean Reward across all agents: 54.16746276390537[0m
[37m[1m[2023-07-10 12:09:43,956][227910] Average Trajectory Length: 979.0843333333333[0m
[36m[2023-07-10 12:09:49,414][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:09:49,414][227910] Reward + Measures: [[ 140.32435171    0.40570003    0.27760002    0.44449997    0.1409    ]
 [-216.23555026    0.57190001    0.25260001    0.55449998    0.2189    ]
 [-215.2046751     0.17770001    0.26449999    0.21010001    0.0629    ]
 ...
 [-231.36672226    0.3118        0.35320002    0.32979998    0.0503    ]
 [   3.73424391    0.20050001    0.27589998    0.22629999    0.0597    ]
 [ -13.95191062    0.41347027    0.28502449    0.44553438    0.23741062]][0m
[37m[1m[2023-07-10 12:09:49,415][227910] Max Reward on eval: 502.223147186538[0m
[37m[1m[2023-07-10 12:09:49,415][227910] Min Reward on eval: -1221.6012966683425[0m
[37m[1m[2023-07-10 12:09:49,415][227910] Mean Reward across all agents: -114.92254500415103[0m
[37m[1m[2023-07-10 12:09:49,415][227910] Average Trajectory Length: 975.9093333333333[0m
[36m[2023-07-10 12:09:49,419][227910] mean_value=-1372.2251750968976, max_value=642.22017507795[0m
[37m[1m[2023-07-10 12:09:49,421][227910] New mean coefficients: [[-0.57109696  2.5404356  -0.16499668  0.78917205 -2.0928655 ]][0m
[37m[1m[2023-07-10 12:09:49,422][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:09:59,222][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 12:09:59,222][227910] FPS: 391906.03[0m
[36m[2023-07-10 12:09:59,225][227910] itr=345, itrs=2000, Progress: 17.25%[0m
[36m[2023-07-10 12:10:10,850][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 12:10:10,850][227910] FPS: 330739.66[0m
[36m[2023-07-10 12:10:15,569][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:10:15,569][227910] Reward + Measures: [[-11.35181013   0.4205423    0.28084645   0.44260082   0.10686368]][0m
[37m[1m[2023-07-10 12:10:15,569][227910] Max Reward on eval: -11.351810134014949[0m
[37m[1m[2023-07-10 12:10:15,570][227910] Min Reward on eval: -11.351810134014949[0m
[37m[1m[2023-07-10 12:10:15,570][227910] Mean Reward across all agents: -11.351810134014949[0m
[37m[1m[2023-07-10 12:10:15,570][227910] Average Trajectory Length: 981.923[0m
[36m[2023-07-10 12:10:21,057][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:10:21,057][227910] Reward + Measures: [[-769.68160926    0.42410001    0.31540003    0.45619997    0.2498    ]
 [-301.31622813    0.38079998    0.33940002    0.3935        0.15270001]
 [-239.48433663    0.69643253    0.37376747    0.66127676    0.42826509]
 ...
 [-429.55857485    0.37207526    0.2552211     0.48706332    0.29740366]
 [-210.27086978    0.54723638    0.15982728    0.53706366    0.38685459]
 [-334.12123362    0.67469996    0.2024        0.61110002    0.34560001]][0m
[37m[1m[2023-07-10 12:10:21,057][227910] Max Reward on eval: 379.8611472845194[0m
[37m[1m[2023-07-10 12:10:21,058][227910] Min Reward on eval: -1262.286393902125[0m
[37m[1m[2023-07-10 12:10:21,058][227910] Mean Reward across all agents: -278.7039267758195[0m
[37m[1m[2023-07-10 12:10:21,058][227910] Average Trajectory Length: 923.5166666666667[0m
[36m[2023-07-10 12:10:21,061][227910] mean_value=-1441.7423451491625, max_value=443.49555984114994[0m
[37m[1m[2023-07-10 12:10:21,064][227910] New mean coefficients: [[-0.7726426   2.8601718   0.23103702  0.85440975 -0.41833603]][0m
[37m[1m[2023-07-10 12:10:21,065][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:10:30,770][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 12:10:30,770][227910] FPS: 395735.74[0m
[36m[2023-07-10 12:10:30,773][227910] itr=346, itrs=2000, Progress: 17.30%[0m
[36m[2023-07-10 12:10:42,240][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 12:10:42,241][227910] FPS: 335360.74[0m
[36m[2023-07-10 12:10:46,897][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:10:46,898][227910] Reward + Measures: [[-111.8418763     0.53475779    0.29553503    0.54093128    0.10863922]][0m
[37m[1m[2023-07-10 12:10:46,898][227910] Max Reward on eval: -111.84187630340702[0m
[37m[1m[2023-07-10 12:10:46,898][227910] Min Reward on eval: -111.84187630340702[0m
[37m[1m[2023-07-10 12:10:46,898][227910] Mean Reward across all agents: -111.84187630340702[0m
[37m[1m[2023-07-10 12:10:46,898][227910] Average Trajectory Length: 984.6166666666667[0m
[36m[2023-07-10 12:10:52,317][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:10:52,317][227910] Reward + Measures: [[  -9.19514319    0.7626        0.40220004    0.74080002    0.0778    ]
 [-394.19664304    0.59200001    0.1629        0.56660002    0.25259998]
 [-259.5578486     0.64319998    0.2802        0.60970002    0.098     ]
 ...
 [-328.4632558     0.45998517    0.22582963    0.4084259     0.13081113]
 [-834.18738578    0.79520005    0.0872        0.75130004    0.48200002]
 [-186.61513287    0.59630001    0.27250001    0.58590001    0.1305    ]][0m
[37m[1m[2023-07-10 12:10:52,318][227910] Max Reward on eval: 273.1669206726248[0m
[37m[1m[2023-07-10 12:10:52,318][227910] Min Reward on eval: -1409.589624990127[0m
[37m[1m[2023-07-10 12:10:52,318][227910] Mean Reward across all agents: -418.8684603376278[0m
[37m[1m[2023-07-10 12:10:52,318][227910] Average Trajectory Length: 969.524[0m
[36m[2023-07-10 12:10:52,322][227910] mean_value=-392.9620416426394, max_value=423.2448033063374[0m
[37m[1m[2023-07-10 12:10:52,324][227910] New mean coefficients: [[-0.47845623  2.0495899  -0.64393854  2.1825795  -0.8796375 ]][0m
[37m[1m[2023-07-10 12:10:52,325][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:11:01,910][227910] train() took 9.58 seconds to complete[0m
[36m[2023-07-10 12:11:01,910][227910] FPS: 400705.49[0m
[36m[2023-07-10 12:11:01,912][227910] itr=347, itrs=2000, Progress: 17.35%[0m
[36m[2023-07-10 12:11:13,400][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 12:11:13,401][227910] FPS: 334714.44[0m
[36m[2023-07-10 12:11:18,201][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:11:18,201][227910] Reward + Measures: [[-145.98432366    0.61000055    0.33102044    0.60874057    0.09710763]][0m
[37m[1m[2023-07-10 12:11:18,201][227910] Max Reward on eval: -145.98432365576252[0m
[37m[1m[2023-07-10 12:11:18,202][227910] Min Reward on eval: -145.98432365576252[0m
[37m[1m[2023-07-10 12:11:18,202][227910] Mean Reward across all agents: -145.98432365576252[0m
[37m[1m[2023-07-10 12:11:18,202][227910] Average Trajectory Length: 986.7896666666667[0m
[36m[2023-07-10 12:11:23,791][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:11:23,791][227910] Reward + Measures: [[-345.67367783    0.33849999    0.26200002    0.3479        0.1191    ]
 [-360.60669162    0.53530002    0.3414        0.51660001    0.0782    ]
 [-150.59514316    0.28179687    0.27128413    0.31073335    0.10964763]
 ...
 [ -58.18835792    0.61580318    0.31851244    0.61581916    0.11712109]
 [ -11.98032771    0.27541834    0.27241451    0.29984808    0.09577709]
 [  46.65485414    0.19944404    0.29881713    0.24323002    0.07510064]][0m
[37m[1m[2023-07-10 12:11:23,791][227910] Max Reward on eval: 385.6331198722357[0m
[37m[1m[2023-07-10 12:11:23,792][227910] Min Reward on eval: -653.8108138939773[0m
[37m[1m[2023-07-10 12:11:23,792][227910] Mean Reward across all agents: -202.84116792869293[0m
[37m[1m[2023-07-10 12:11:23,792][227910] Average Trajectory Length: 970.1236666666666[0m
[36m[2023-07-10 12:11:23,796][227910] mean_value=-1535.9475694258922, max_value=409.4245128496354[0m
[37m[1m[2023-07-10 12:11:23,799][227910] New mean coefficients: [[ 0.28957835  2.385848   -0.28886428  1.5316933  -0.29639256]][0m
[37m[1m[2023-07-10 12:11:23,800][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:11:33,601][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 12:11:33,601][227910] FPS: 391855.05[0m
[36m[2023-07-10 12:11:33,604][227910] itr=348, itrs=2000, Progress: 17.40%[0m
[36m[2023-07-10 12:11:45,200][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 12:11:45,200][227910] FPS: 331620.47[0m
[36m[2023-07-10 12:11:49,867][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:11:49,868][227910] Reward + Measures: [[-128.23168549    0.68019891    0.39779037    0.66901851    0.07104493]][0m
[37m[1m[2023-07-10 12:11:49,868][227910] Max Reward on eval: -128.23168549211394[0m
[37m[1m[2023-07-10 12:11:49,868][227910] Min Reward on eval: -128.23168549211394[0m
[37m[1m[2023-07-10 12:11:49,869][227910] Mean Reward across all agents: -128.23168549211394[0m
[37m[1m[2023-07-10 12:11:49,869][227910] Average Trajectory Length: 993.841[0m
[36m[2023-07-10 12:11:55,279][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:11:55,280][227910] Reward + Measures: [[-157.40590174    0.45370004    0.29780003    0.46529999    0.09890001]
 [-132.01650418    0.63029999    0.29650003    0.59680003    0.26859999]
 [-184.01672621    0.62173575    0.185         0.6177786     0.26033571]
 ...
 [-354.7918285     0.69220001    0.2386        0.64090002    0.3407    ]
 [ -99.21707707    0.48412201    0.49896154    0.34527913    0.461611  ]
 [ -37.69933183    0.58820003    0.36590001    0.59020007    0.08810001]][0m
[37m[1m[2023-07-10 12:11:55,280][227910] Max Reward on eval: 123.48675565776648[0m
[37m[1m[2023-07-10 12:11:55,280][227910] Min Reward on eval: -746.4932801099261[0m
[37m[1m[2023-07-10 12:11:55,281][227910] Mean Reward across all agents: -197.39184772125625[0m
[37m[1m[2023-07-10 12:11:55,281][227910] Average Trajectory Length: 977.7453333333333[0m
[36m[2023-07-10 12:11:55,286][227910] mean_value=-167.07718912814093, max_value=538.5917254144384[0m
[37m[1m[2023-07-10 12:11:55,288][227910] New mean coefficients: [[-0.05301887  2.4421375  -0.31270665  1.756941   -1.1412046 ]][0m
[37m[1m[2023-07-10 12:11:55,289][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:12:04,948][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 12:12:04,949][227910] FPS: 397624.41[0m
[36m[2023-07-10 12:12:04,951][227910] itr=349, itrs=2000, Progress: 17.45%[0m
[36m[2023-07-10 12:12:16,590][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 12:12:16,591][227910] FPS: 330414.95[0m
[36m[2023-07-10 12:12:21,434][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:12:21,434][227910] Reward + Measures: [[-144.14145721    0.75412923    0.47939798    0.73679399    0.04770735]][0m
[37m[1m[2023-07-10 12:12:21,434][227910] Max Reward on eval: -144.14145721175737[0m
[37m[1m[2023-07-10 12:12:21,435][227910] Min Reward on eval: -144.14145721175737[0m
[37m[1m[2023-07-10 12:12:21,435][227910] Mean Reward across all agents: -144.14145721175737[0m
[37m[1m[2023-07-10 12:12:21,435][227910] Average Trajectory Length: 996.5453333333332[0m
[36m[2023-07-10 12:12:26,919][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:12:26,919][227910] Reward + Measures: [[ -97.69285791    0.74329996    0.40309998    0.72209996    0.09999999]
 [ -97.62721597    0.83649999    0.25530002    0.7944001     0.36750001]
 [-265.43318009    0.76209992    0.43070003    0.70100003    0.47919998]
 ...
 [ -50.67594584    0.81560004    0.31549999    0.79420006    0.2282    ]
 [-219.57973739    0.57639998    0.36160001    0.57590002    0.18889999]
 [-126.38467585    0.6935696     0.31962782    0.63260424    0.34988353]][0m
[37m[1m[2023-07-10 12:12:26,920][227910] Max Reward on eval: 65.0750182840391[0m
[37m[1m[2023-07-10 12:12:26,920][227910] Min Reward on eval: -588.0338361985399[0m
[37m[1m[2023-07-10 12:12:26,920][227910] Mean Reward across all agents: -182.17129168724972[0m
[37m[1m[2023-07-10 12:12:26,920][227910] Average Trajectory Length: 989.0723333333333[0m
[36m[2023-07-10 12:12:26,927][227910] mean_value=92.62608978935886, max_value=563.6112164138001[0m
[37m[1m[2023-07-10 12:12:26,929][227910] New mean coefficients: [[-0.4688928   2.425076    0.04765511  2.7147028  -1.4446906 ]][0m
[37m[1m[2023-07-10 12:12:26,930][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:12:36,764][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 12:12:36,764][227910] FPS: 390569.02[0m
[36m[2023-07-10 12:12:36,766][227910] itr=350, itrs=2000, Progress: 17.50%[0m
[37m[1m[2023-07-10 12:12:39,262][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000330[0m
[36m[2023-07-10 12:12:51,198][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 12:12:51,198][227910] FPS: 328763.12[0m
[36m[2023-07-10 12:12:56,008][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:12:56,009][227910] Reward + Measures: [[-123.5001948     0.78393805    0.53353631    0.76922625    0.03927533]][0m
[37m[1m[2023-07-10 12:12:56,009][227910] Max Reward on eval: -123.5001948011568[0m
[37m[1m[2023-07-10 12:12:56,009][227910] Min Reward on eval: -123.5001948011568[0m
[37m[1m[2023-07-10 12:12:56,009][227910] Mean Reward across all agents: -123.5001948011568[0m
[37m[1m[2023-07-10 12:12:56,010][227910] Average Trajectory Length: 995.608[0m
[36m[2023-07-10 12:13:01,609][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:13:01,610][227910] Reward + Measures: [[-403.54773707    0.63508171    0.35830978    0.59646338    0.10253049]
 [-213.88700419    0.54306644    0.25243723    0.5464555     0.30662847]
 [-122.36527282    0.83809996    0.5187        0.82320005    0.1025    ]
 ...
 [-399.92716837    0.6868        0.14229999    0.68000001    0.36740002]
 [-447.6731942     0.68449998    0.36829999    0.66040003    0.1373    ]
 [-509.20876491    0.6182        0.3996        0.62040001    0.0875    ]][0m
[37m[1m[2023-07-10 12:13:01,610][227910] Max Reward on eval: 87.51322273870464[0m
[37m[1m[2023-07-10 12:13:01,610][227910] Min Reward on eval: -739.0063203154889[0m
[37m[1m[2023-07-10 12:13:01,611][227910] Mean Reward across all agents: -235.49217324294372[0m
[37m[1m[2023-07-10 12:13:01,611][227910] Average Trajectory Length: 988.4326666666666[0m
[36m[2023-07-10 12:13:01,615][227910] mean_value=-53.26118630373636, max_value=554.0634020740737[0m
[37m[1m[2023-07-10 12:13:01,618][227910] New mean coefficients: [[ 0.2181735  2.5417607  0.522141   2.3865924 -1.3305697]][0m
[37m[1m[2023-07-10 12:13:01,619][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:13:11,434][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 12:13:11,434][227910] FPS: 391327.46[0m
[36m[2023-07-10 12:13:11,436][227910] itr=351, itrs=2000, Progress: 17.55%[0m
[36m[2023-07-10 12:13:23,165][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 12:13:23,165][227910] FPS: 327841.59[0m
[36m[2023-07-10 12:13:27,992][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:13:27,993][227910] Reward + Measures: [[-125.82922262    0.80674297    0.62262362    0.80116421    0.02653185]][0m
[37m[1m[2023-07-10 12:13:27,993][227910] Max Reward on eval: -125.82922262058611[0m
[37m[1m[2023-07-10 12:13:27,993][227910] Min Reward on eval: -125.82922262058611[0m
[37m[1m[2023-07-10 12:13:27,994][227910] Mean Reward across all agents: -125.82922262058611[0m
[37m[1m[2023-07-10 12:13:27,994][227910] Average Trajectory Length: 996.7529999999999[0m
[36m[2023-07-10 12:13:33,601][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:13:33,601][227910] Reward + Measures: [[ -78.91116648    0.47810003    0.29679999    0.49829999    0.1096    ]
 [ -85.30334069    0.82440007    0.52419996    0.83030003    0.0671    ]
 [-441.16393703    0.63810003    0.36219999    0.6433        0.08449999]
 ...
 [ -63.02747032    0.72540694    0.19382882    0.62324637    0.3521657 ]
 [-412.75434248    0.7511        0.45830002    0.71780002    0.11270001]
 [-313.60281693    0.68570006    0.44860002    0.66119999    0.36989999]][0m
[37m[1m[2023-07-10 12:13:33,602][227910] Max Reward on eval: 165.17293025107355[0m
[37m[1m[2023-07-10 12:13:33,602][227910] Min Reward on eval: -981.0410324840108[0m
[37m[1m[2023-07-10 12:13:33,602][227910] Mean Reward across all agents: -223.76934809169[0m
[37m[1m[2023-07-10 12:13:33,602][227910] Average Trajectory Length: 990.7886666666666[0m
[36m[2023-07-10 12:13:33,609][227910] mean_value=-72.35053464828339, max_value=511.13970749451545[0m
[37m[1m[2023-07-10 12:13:33,612][227910] New mean coefficients: [[ 0.20069122  2.9472528   0.43896613  1.1366038  -1.6546588 ]][0m
[37m[1m[2023-07-10 12:13:33,613][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:13:43,256][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 12:13:43,256][227910] FPS: 398288.99[0m
[36m[2023-07-10 12:13:43,258][227910] itr=352, itrs=2000, Progress: 17.60%[0m
[36m[2023-07-10 12:13:54,732][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 12:13:54,732][227910] FPS: 335126.81[0m
[36m[2023-07-10 12:13:59,534][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:13:59,535][227910] Reward + Measures: [[-87.03346899   0.8418442    0.68365842   0.84156263   0.02334878]][0m
[37m[1m[2023-07-10 12:13:59,535][227910] Max Reward on eval: -87.03346899338025[0m
[37m[1m[2023-07-10 12:13:59,535][227910] Min Reward on eval: -87.03346899338025[0m
[37m[1m[2023-07-10 12:13:59,535][227910] Mean Reward across all agents: -87.03346899338025[0m
[37m[1m[2023-07-10 12:13:59,536][227910] Average Trajectory Length: 996.7733333333333[0m
[36m[2023-07-10 12:14:05,072][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:14:05,072][227910] Reward + Measures: [[-351.60029253    0.65450001    0.53010005    0.64499998    0.0274    ]
 [-166.66668134    0.80480003    0.31670001    0.78839999    0.14660001]
 [ -75.8922409     0.80060005    0.58750004    0.77669996    0.0298    ]
 ...
 [-611.19241093    0.37780002    0.18179999    0.39089999    0.13640001]
 [-267.44608393    0.78640002    0.39720002    0.76739997    0.09429999]
 [ -45.22707679    0.77689999    0.64340001    0.76910007    0.0245    ]][0m
[37m[1m[2023-07-10 12:14:05,072][227910] Max Reward on eval: 235.25958680980838[0m
[37m[1m[2023-07-10 12:14:05,073][227910] Min Reward on eval: -624.5933429839322[0m
[37m[1m[2023-07-10 12:14:05,073][227910] Mean Reward across all agents: -166.0032938300569[0m
[37m[1m[2023-07-10 12:14:05,073][227910] Average Trajectory Length: 990.4066666666666[0m
[36m[2023-07-10 12:14:05,079][227910] mean_value=12.380055343255615, max_value=675.8249810809386[0m
[37m[1m[2023-07-10 12:14:05,081][227910] New mean coefficients: [[ 0.2560926  2.1620502 -0.8250183  2.1000435 -0.7436532]][0m
[37m[1m[2023-07-10 12:14:05,082][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:14:14,854][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 12:14:14,854][227910] FPS: 393057.37[0m
[36m[2023-07-10 12:14:14,856][227910] itr=353, itrs=2000, Progress: 17.65%[0m
[36m[2023-07-10 12:14:26,319][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 12:14:26,319][227910] FPS: 335526.89[0m
[36m[2023-07-10 12:14:31,052][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:14:31,053][227910] Reward + Measures: [[-119.9522606     0.84421623    0.66307098    0.84317571    0.02290142]][0m
[37m[1m[2023-07-10 12:14:31,053][227910] Max Reward on eval: -119.95226060056531[0m
[37m[1m[2023-07-10 12:14:31,053][227910] Min Reward on eval: -119.95226060056531[0m
[37m[1m[2023-07-10 12:14:31,053][227910] Mean Reward across all agents: -119.95226060056531[0m
[37m[1m[2023-07-10 12:14:31,054][227910] Average Trajectory Length: 997.8843333333333[0m
[36m[2023-07-10 12:14:36,459][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:14:36,459][227910] Reward + Measures: [[-137.77568925    0.89989996    0.30860001    0.86690009    0.0938    ]
 [-414.30701037    0.68991464    0.28897977    0.67694271    0.14237414]
 [  -9.71075146    0.9307        0.73470002    0.92290002    0.0139    ]
 ...
 [-512.44492739    0.62920004    0.153         0.60079998    0.21400002]
 [ -56.23122139    0.89470005    0.37780002    0.86030006    0.1596    ]
 [ -64.69197011    0.90210003    0.0988        0.84200001    0.24950002]][0m
[37m[1m[2023-07-10 12:14:36,459][227910] Max Reward on eval: 88.96290772259235[0m
[37m[1m[2023-07-10 12:14:36,460][227910] Min Reward on eval: -723.16748749658[0m
[37m[1m[2023-07-10 12:14:36,460][227910] Mean Reward across all agents: -152.9269544041157[0m
[37m[1m[2023-07-10 12:14:36,460][227910] Average Trajectory Length: 996.9863333333333[0m
[36m[2023-07-10 12:14:36,465][227910] mean_value=11.63771139797894, max_value=453.1202932281769[0m
[37m[1m[2023-07-10 12:14:36,467][227910] New mean coefficients: [[ 0.47497898  1.9925611  -2.4630346   2.8663511  -1.3407022 ]][0m
[37m[1m[2023-07-10 12:14:36,468][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:14:46,294][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 12:14:46,294][227910] FPS: 390887.52[0m
[36m[2023-07-10 12:14:46,296][227910] itr=354, itrs=2000, Progress: 17.70%[0m
[36m[2023-07-10 12:14:57,856][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 12:14:57,856][227910] FPS: 332650.33[0m
[36m[2023-07-10 12:15:02,622][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:15:02,623][227910] Reward + Measures: [[-173.08240052    0.82608652    0.48280147    0.8134526     0.04300722]][0m
[37m[1m[2023-07-10 12:15:02,623][227910] Max Reward on eval: -173.0824005177131[0m
[37m[1m[2023-07-10 12:15:02,623][227910] Min Reward on eval: -173.0824005177131[0m
[37m[1m[2023-07-10 12:15:02,623][227910] Mean Reward across all agents: -173.0824005177131[0m
[37m[1m[2023-07-10 12:15:02,624][227910] Average Trajectory Length: 997.1223333333332[0m
[36m[2023-07-10 12:15:08,121][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:15:08,122][227910] Reward + Measures: [[ -61.50659876    0.79910004    0.63549995    0.8099001     0.0209    ]
 [ -74.54769473    0.89530003    0.50340003    0.8707        0.0468    ]
 [ -33.42799472    0.89830011    0.57069999    0.88949996    0.0298    ]
 ...
 [-276.47854057    0.78799999    0.46070001    0.7816        0.10649999]
 [-346.56452417    0.75010002    0.47440001    0.74250001    0.0452    ]
 [-342.8291458     0.7123        0.4007        0.68300003    0.0665    ]][0m
[37m[1m[2023-07-10 12:15:08,122][227910] Max Reward on eval: 104.84543094863184[0m
[37m[1m[2023-07-10 12:15:08,122][227910] Min Reward on eval: -629.0127762412187[0m
[37m[1m[2023-07-10 12:15:08,123][227910] Mean Reward across all agents: -206.2048479714592[0m
[37m[1m[2023-07-10 12:15:08,123][227910] Average Trajectory Length: 990.7273333333333[0m
[36m[2023-07-10 12:15:08,127][227910] mean_value=-71.92984757124262, max_value=604.8454309486318[0m
[37m[1m[2023-07-10 12:15:08,130][227910] New mean coefficients: [[ 0.55696917  1.7875305  -1.2043935   3.709613   -0.3715635 ]][0m
[37m[1m[2023-07-10 12:15:08,131][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:15:17,754][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 12:15:17,754][227910] FPS: 399090.44[0m
[36m[2023-07-10 12:15:17,757][227910] itr=355, itrs=2000, Progress: 17.75%[0m
[36m[2023-07-10 12:15:29,210][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 12:15:29,210][227910] FPS: 335721.54[0m
[36m[2023-07-10 12:15:33,927][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:15:33,927][227910] Reward + Measures: [[-146.29654157    0.8460865     0.42008942    0.83132476    0.06080303]][0m
[37m[1m[2023-07-10 12:15:33,927][227910] Max Reward on eval: -146.2965415696883[0m
[37m[1m[2023-07-10 12:15:33,928][227910] Min Reward on eval: -146.2965415696883[0m
[37m[1m[2023-07-10 12:15:33,928][227910] Mean Reward across all agents: -146.2965415696883[0m
[37m[1m[2023-07-10 12:15:33,928][227910] Average Trajectory Length: 998.9699999999999[0m
[36m[2023-07-10 12:15:39,356][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:15:39,356][227910] Reward + Measures: [[  62.29450197    0.85899991    0.30630001    0.81779999    0.28660002]
 [-369.68665413    0.81199998    0.5025        0.79729998    0.0487    ]
 [-120.69340507    0.73430008    0.2782        0.72720003    0.2289    ]
 ...
 [-375.78445881    0.90889996    0.1406        0.89610004    0.25139999]
 [-425.56662595    0.89690012    0.36339998    0.87900001    0.1402    ]
 [-380.84268919    0.88679999    0.36070001    0.87769997    0.1591    ]][0m
[37m[1m[2023-07-10 12:15:39,356][227910] Max Reward on eval: 161.71840961067937[0m
[37m[1m[2023-07-10 12:15:39,357][227910] Min Reward on eval: -1012.8245049375342[0m
[37m[1m[2023-07-10 12:15:39,357][227910] Mean Reward across all agents: -289.028418099568[0m
[37m[1m[2023-07-10 12:15:39,357][227910] Average Trajectory Length: 990.524[0m
[36m[2023-07-10 12:15:39,363][227910] mean_value=-45.897546979661435, max_value=661.7184096106794[0m
[37m[1m[2023-07-10 12:15:39,366][227910] New mean coefficients: [[-0.32557207  1.7280473  -1.3655016   3.1658742  -1.2551486 ]][0m
[37m[1m[2023-07-10 12:15:39,367][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:15:49,062][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 12:15:49,062][227910] FPS: 396152.95[0m
[36m[2023-07-10 12:15:49,064][227910] itr=356, itrs=2000, Progress: 17.80%[0m
[36m[2023-07-10 12:16:00,542][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 12:16:00,543][227910] FPS: 335038.54[0m
[36m[2023-07-10 12:16:05,438][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:16:05,438][227910] Reward + Measures: [[-141.48599329    0.86390626    0.3846803     0.8504647     0.06535804]][0m
[37m[1m[2023-07-10 12:16:05,438][227910] Max Reward on eval: -141.48599329209424[0m
[37m[1m[2023-07-10 12:16:05,439][227910] Min Reward on eval: -141.48599329209424[0m
[37m[1m[2023-07-10 12:16:05,439][227910] Mean Reward across all agents: -141.48599329209424[0m
[37m[1m[2023-07-10 12:16:05,439][227910] Average Trajectory Length: 998.7736666666666[0m
[36m[2023-07-10 12:16:10,944][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:16:10,945][227910] Reward + Measures: [[-190.57669881    0.9073        0.53000003    0.89410001    0.0528    ]
 [ -78.933274      0.86310005    0.50600004    0.81310004    0.44509998]
 [-118.64249188    0.85901308    0.38030872    0.83846962    0.11403479]
 ...
 [   8.05753204    0.91850007    0.39000002    0.90539998    0.177     ]
 [-107.54419274    0.89420003    0.58700001    0.87019998    0.03      ]
 [-223.78180738    0.92840004    0.3784        0.91079998    0.0776    ]][0m
[37m[1m[2023-07-10 12:16:10,945][227910] Max Reward on eval: 84.12592740403488[0m
[37m[1m[2023-07-10 12:16:10,945][227910] Min Reward on eval: -434.4198365501361[0m
[37m[1m[2023-07-10 12:16:10,945][227910] Mean Reward across all agents: -144.02392570319842[0m
[37m[1m[2023-07-10 12:16:10,946][227910] Average Trajectory Length: 995.7403333333333[0m
[36m[2023-07-10 12:16:10,950][227910] mean_value=-11.871061829694675, max_value=537.9702542101965[0m
[37m[1m[2023-07-10 12:16:10,953][227910] New mean coefficients: [[-0.37150788  2.0461903  -0.39426142  2.1285686  -2.6615653 ]][0m
[37m[1m[2023-07-10 12:16:10,954][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:16:20,751][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 12:16:20,752][227910] FPS: 392002.74[0m
[36m[2023-07-10 12:16:20,754][227910] itr=357, itrs=2000, Progress: 17.85%[0m
[36m[2023-07-10 12:16:32,307][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 12:16:32,307][227910] FPS: 332815.04[0m
[36m[2023-07-10 12:16:37,040][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:16:37,041][227910] Reward + Measures: [[-113.38506165    0.8774606     0.48185676    0.86535585    0.03704812]][0m
[37m[1m[2023-07-10 12:16:37,041][227910] Max Reward on eval: -113.38506165293244[0m
[37m[1m[2023-07-10 12:16:37,041][227910] Min Reward on eval: -113.38506165293244[0m
[37m[1m[2023-07-10 12:16:37,041][227910] Mean Reward across all agents: -113.38506165293244[0m
[37m[1m[2023-07-10 12:16:37,042][227910] Average Trajectory Length: 997.901[0m
[36m[2023-07-10 12:16:42,505][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:16:42,505][227910] Reward + Measures: [[-201.21097741    0.85859996    0.51789999    0.83450001    0.0232    ]
 [-386.12868845    0.64340001    0.46000004    0.65900004    0.0736    ]
 [-182.38574141    0.80849999    0.23810001    0.78240001    0.1433    ]
 ...
 [ -66.61664265    0.91610003    0.37689999    0.91050005    0.0865    ]
 [-337.09559679    0.73809999    0.16540001    0.72569996    0.15189999]
 [-325.90638042    0.83059996    0.22309999    0.82080001    0.141     ]][0m
[37m[1m[2023-07-10 12:16:42,506][227910] Max Reward on eval: 236.8721743115806[0m
[37m[1m[2023-07-10 12:16:42,506][227910] Min Reward on eval: -600.6908932086546[0m
[37m[1m[2023-07-10 12:16:42,506][227910] Mean Reward across all agents: -259.53621597962217[0m
[37m[1m[2023-07-10 12:16:42,506][227910] Average Trajectory Length: 991.4486666666667[0m
[36m[2023-07-10 12:16:42,510][227910] mean_value=-65.24007223450805, max_value=655.9926168109756[0m
[37m[1m[2023-07-10 12:16:42,513][227910] New mean coefficients: [[ 0.17486769  2.1069064  -0.44923615  1.5400931  -2.8402529 ]][0m
[37m[1m[2023-07-10 12:16:42,514][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:16:52,306][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 12:16:52,311][227910] FPS: 392219.79[0m
[36m[2023-07-10 12:16:52,314][227910] itr=358, itrs=2000, Progress: 17.90%[0m
[36m[2023-07-10 12:17:03,805][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 12:17:03,806][227910] FPS: 334618.33[0m
[36m[2023-07-10 12:17:08,563][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:17:08,563][227910] Reward + Measures: [[-90.1120397    0.8910619    0.49314183   0.87737936   0.02754147]][0m
[37m[1m[2023-07-10 12:17:08,564][227910] Max Reward on eval: -90.11203970014692[0m
[37m[1m[2023-07-10 12:17:08,564][227910] Min Reward on eval: -90.11203970014692[0m
[37m[1m[2023-07-10 12:17:08,564][227910] Mean Reward across all agents: -90.11203970014692[0m
[37m[1m[2023-07-10 12:17:08,564][227910] Average Trajectory Length: 999.6893333333333[0m
[36m[2023-07-10 12:17:14,044][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:17:14,105][227910] Reward + Measures: [[ -18.30455585    0.84090006    0.62109995    0.81689996    0.014     ]
 [-214.96059343    0.82320005    0.37490001    0.75489998    0.1224    ]
 [   2.20903104    0.91570008    0.67000002    0.89069998    0.0149    ]
 ...
 [ -35.19697156    0.89820004    0.33440003    0.89510006    0.07539999]
 [ -73.21065115    0.90390009    0.46650001    0.89610004    0.0356    ]
 [ -46.11683479    0.91140002    0.4233        0.89880002    0.0409    ]][0m
[37m[1m[2023-07-10 12:17:14,105][227910] Max Reward on eval: 96.93395430139499[0m
[37m[1m[2023-07-10 12:17:14,105][227910] Min Reward on eval: -432.85566251545094[0m
[37m[1m[2023-07-10 12:17:14,106][227910] Mean Reward across all agents: -107.96282272570889[0m
[37m[1m[2023-07-10 12:17:14,106][227910] Average Trajectory Length: 995.947[0m
[36m[2023-07-10 12:17:14,110][227910] mean_value=19.379595268960973, max_value=456.61859413701694[0m
[37m[1m[2023-07-10 12:17:14,113][227910] New mean coefficients: [[ 0.06154157  1.7596819  -0.96022856  3.655949   -1.8463209 ]][0m
[37m[1m[2023-07-10 12:17:14,114][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:17:23,799][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 12:17:23,800][227910] FPS: 396535.56[0m
[36m[2023-07-10 12:17:23,802][227910] itr=359, itrs=2000, Progress: 17.95%[0m
[36m[2023-07-10 12:17:35,305][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 12:17:35,305][227910] FPS: 334281.85[0m
[36m[2023-07-10 12:17:40,072][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:17:40,072][227910] Reward + Measures: [[-125.38118157    0.89961731    0.51640177    0.88534296    0.02097174]][0m
[37m[1m[2023-07-10 12:17:40,072][227910] Max Reward on eval: -125.38118156868043[0m
[37m[1m[2023-07-10 12:17:40,072][227910] Min Reward on eval: -125.38118156868043[0m
[37m[1m[2023-07-10 12:17:40,073][227910] Mean Reward across all agents: -125.38118156868043[0m
[37m[1m[2023-07-10 12:17:40,073][227910] Average Trajectory Length: 999.0996666666666[0m
[36m[2023-07-10 12:17:45,564][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:17:45,565][227910] Reward + Measures: [[-139.42343641    0.92040008    0.50850004    0.9011001     0.0173    ]
 [-278.71045943    0.89750004    0.38270003    0.89209998    0.041     ]
 [-375.73411559    0.9188        0.57340002    0.89780009    0.027     ]
 ...
 [-387.37562257    0.88800001    0.42130002    0.87659997    0.0543    ]
 [-496.48767983    0.82499999    0.39430001    0.79890001    0.0471    ]
 [-214.44005089    0.84159994    0.3581        0.8373        0.06620001]][0m
[37m[1m[2023-07-10 12:17:45,565][227910] Max Reward on eval: 164.71277585297358[0m
[37m[1m[2023-07-10 12:17:45,565][227910] Min Reward on eval: -603.5259944265126[0m
[37m[1m[2023-07-10 12:17:45,566][227910] Mean Reward across all agents: -297.4677400922675[0m
[37m[1m[2023-07-10 12:17:45,566][227910] Average Trajectory Length: 996.175[0m
[36m[2023-07-10 12:17:45,568][227910] mean_value=-231.72398185187268, max_value=374.4016256785486[0m
[37m[1m[2023-07-10 12:17:45,570][227910] New mean coefficients: [[-0.14452524  1.4265584   0.34686685  3.3768919  -0.39454055]][0m
[37m[1m[2023-07-10 12:17:45,571][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:17:55,466][227910] train() took 9.89 seconds to complete[0m
[36m[2023-07-10 12:17:55,467][227910] FPS: 388130.82[0m
[36m[2023-07-10 12:17:55,469][227910] itr=360, itrs=2000, Progress: 18.00%[0m
[37m[1m[2023-07-10 12:17:58,168][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000340[0m
[36m[2023-07-10 12:18:10,172][227910] train() took 11.74 seconds to complete[0m
[36m[2023-07-10 12:18:10,173][227910] FPS: 327160.67[0m
[36m[2023-07-10 12:18:15,074][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:18:15,075][227910] Reward + Measures: [[-60.31688975   0.90613538   0.57705343   0.90088224   0.0165344 ]][0m
[37m[1m[2023-07-10 12:18:15,075][227910] Max Reward on eval: -60.31688974746507[0m
[37m[1m[2023-07-10 12:18:15,075][227910] Min Reward on eval: -60.31688974746507[0m
[37m[1m[2023-07-10 12:18:15,075][227910] Mean Reward across all agents: -60.31688974746507[0m
[37m[1m[2023-07-10 12:18:15,075][227910] Average Trajectory Length: 998.9046666666667[0m
[36m[2023-07-10 12:18:20,440][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:18:20,441][227910] Reward + Measures: [[ -67.13099383    0.84919995    0.56170005    0.83719999    0.0099    ]
 [-302.92237005    0.84380001    0.52500004    0.82620001    0.0193    ]
 [ 110.67868073    0.93949997    0.66590005    0.93029994    0.0086    ]
 ...
 [-100.66432475    0.86629027    0.38957319    0.84900725    0.0668561 ]
 [-185.07205563    0.92950004    0.35300002    0.90290004    0.19560002]
 [ -79.07470992    0.92880005    0.3901        0.91370004    0.0503    ]][0m
[37m[1m[2023-07-10 12:18:20,441][227910] Max Reward on eval: 110.6786807318218[0m
[37m[1m[2023-07-10 12:18:20,441][227910] Min Reward on eval: -481.9753718836117[0m
[37m[1m[2023-07-10 12:18:20,442][227910] Mean Reward across all agents: -108.81803509162978[0m
[37m[1m[2023-07-10 12:18:20,442][227910] Average Trajectory Length: 999.4083333333333[0m
[36m[2023-07-10 12:18:20,446][227910] mean_value=36.85981909716172, max_value=494.0496934636369[0m
[37m[1m[2023-07-10 12:18:20,449][227910] New mean coefficients: [[ 0.07224192  1.5317924  -0.13565645  3.9232402   0.654469  ]][0m
[37m[1m[2023-07-10 12:18:20,450][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:18:30,093][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 12:18:30,093][227910] FPS: 398286.13[0m
[36m[2023-07-10 12:18:30,095][227910] itr=361, itrs=2000, Progress: 18.05%[0m
[36m[2023-07-10 12:18:41,528][227910] train() took 11.42 seconds to complete[0m
[36m[2023-07-10 12:18:41,528][227910] FPS: 336343.77[0m
[36m[2023-07-10 12:18:46,358][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:18:46,358][227910] Reward + Measures: [[47.0761183   0.91825324  0.64973569  0.91661865  0.01526633]][0m
[37m[1m[2023-07-10 12:18:46,359][227910] Max Reward on eval: 47.07611829885771[0m
[37m[1m[2023-07-10 12:18:46,359][227910] Min Reward on eval: 47.07611829885771[0m
[37m[1m[2023-07-10 12:18:46,359][227910] Mean Reward across all agents: 47.07611829885771[0m
[37m[1m[2023-07-10 12:18:46,359][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:18:51,907][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:18:51,913][227910] Reward + Measures: [[ -56.47640199    0.84890002    0.64660001    0.83729994    0.0079    ]
 [ -73.02901382    0.93030006    0.49499997    0.89989996    0.0269    ]
 [ -75.71268757    0.81020004    0.75029999    0.8549        0.79519999]
 ...
 [-307.10832643    0.92119998    0.366         0.88220006    0.09519999]
 [ -98.61375279    0.92760003    0.60470003    0.90360004    0.06820001]
 [  28.022726      0.93120003    0.4786        0.92280006    0.0341    ]][0m
[37m[1m[2023-07-10 12:18:51,913][227910] Max Reward on eval: 190.4667620051652[0m
[37m[1m[2023-07-10 12:18:51,913][227910] Min Reward on eval: -1239.040914658201[0m
[37m[1m[2023-07-10 12:18:51,914][227910] Mean Reward across all agents: -150.74877146563003[0m
[37m[1m[2023-07-10 12:18:51,914][227910] Average Trajectory Length: 998.807[0m
[36m[2023-07-10 12:18:51,919][227910] mean_value=-14.977414991896488, max_value=522.9680166855687[0m
[37m[1m[2023-07-10 12:18:51,922][227910] New mean coefficients: [[-0.8231779   1.9424076   0.18480137  3.2326112   0.3773226 ]][0m
[37m[1m[2023-07-10 12:18:51,923][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:19:01,662][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 12:19:01,662][227910] FPS: 394372.80[0m
[36m[2023-07-10 12:19:01,664][227910] itr=362, itrs=2000, Progress: 18.10%[0m
[36m[2023-07-10 12:19:13,121][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 12:19:13,121][227910] FPS: 335658.55[0m
[36m[2023-07-10 12:19:17,968][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:19:17,974][227910] Reward + Measures: [[16.6656754   0.9232173   0.65546662  0.92353231  0.01469267]][0m
[37m[1m[2023-07-10 12:19:17,974][227910] Max Reward on eval: 16.665675396341918[0m
[37m[1m[2023-07-10 12:19:17,974][227910] Min Reward on eval: 16.665675396341918[0m
[37m[1m[2023-07-10 12:19:17,974][227910] Mean Reward across all agents: 16.665675396341918[0m
[37m[1m[2023-07-10 12:19:17,975][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:19:23,355][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:19:23,360][227910] Reward + Measures: [[204.80207789   0.93120003   0.75529999   0.92830008   0.0091    ]
 [-92.83512324   0.93529999   0.45949998   0.9273001    0.0679    ]
 [  2.10601395   0.91190004   0.40509996   0.88630009   0.1138    ]
 ...
 [-78.22390107   0.89520007   0.0599       0.85529995   0.4305    ]
 [ 55.32288311   0.91670001   0.25280002   0.92350006   0.15320002]
 [ 10.91368375   0.91369992   0.38840002   0.87869996   0.14490001]][0m
[37m[1m[2023-07-10 12:19:23,361][227910] Max Reward on eval: 272.7871441995027[0m
[37m[1m[2023-07-10 12:19:23,361][227910] Min Reward on eval: -238.62846634731395[0m
[37m[1m[2023-07-10 12:19:23,361][227910] Mean Reward across all agents: 50.193476045944934[0m
[37m[1m[2023-07-10 12:19:23,361][227910] Average Trajectory Length: 998.458[0m
[36m[2023-07-10 12:19:23,367][227910] mean_value=102.90080339515237, max_value=600.7392275602165[0m
[37m[1m[2023-07-10 12:19:23,369][227910] New mean coefficients: [[-0.7842306   2.7270427   1.5109783   2.4074807  -0.01732641]][0m
[37m[1m[2023-07-10 12:19:23,370][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:19:33,013][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 12:19:33,013][227910] FPS: 398291.28[0m
[36m[2023-07-10 12:19:33,016][227910] itr=363, itrs=2000, Progress: 18.15%[0m
[36m[2023-07-10 12:19:44,699][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 12:19:44,699][227910] FPS: 329129.39[0m
[36m[2023-07-10 12:19:49,378][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:19:49,378][227910] Reward + Measures: [[-6.92154841  0.92607003  0.75866359  0.93134832  0.01091083]][0m
[37m[1m[2023-07-10 12:19:49,379][227910] Max Reward on eval: -6.921548412545172[0m
[37m[1m[2023-07-10 12:19:49,379][227910] Min Reward on eval: -6.921548412545172[0m
[37m[1m[2023-07-10 12:19:49,379][227910] Mean Reward across all agents: -6.921548412545172[0m
[37m[1m[2023-07-10 12:19:49,379][227910] Average Trajectory Length: 999.6986666666667[0m
[36m[2023-07-10 12:19:54,957][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:19:54,958][227910] Reward + Measures: [[ -11.38453229    0.92620003    0.52570003    0.92199993    0.0399    ]
 [ -20.13559495    0.92589998    0.2289        0.9181        0.1744    ]
 [ -64.64397931    0.92950004    0.31549999    0.92550004    0.1014    ]
 ...
 [ -94.55244003    0.9259001     0.639         0.91030008    0.0123    ]
 [ -65.85350122    0.93059999    0.71390003    0.93870002    0.0174    ]
 [-114.32667674    0.94460005    0.69639999    0.92740005    0.0072    ]][0m
[37m[1m[2023-07-10 12:19:54,958][227910] Max Reward on eval: 189.99864303250797[0m
[37m[1m[2023-07-10 12:19:54,958][227910] Min Reward on eval: -519.3397722780297[0m
[37m[1m[2023-07-10 12:19:54,959][227910] Mean Reward across all agents: -82.61313521552853[0m
[37m[1m[2023-07-10 12:19:54,959][227910] Average Trajectory Length: 998.7583333333333[0m
[36m[2023-07-10 12:19:54,961][227910] mean_value=-110.29957954323366, max_value=392.0570663639577[0m
[37m[1m[2023-07-10 12:19:54,964][227910] New mean coefficients: [[-0.94878066  2.1013806   1.5374285   3.4555924   1.0740249 ]][0m
[37m[1m[2023-07-10 12:19:54,965][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:20:04,821][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 12:20:04,821][227910] FPS: 389673.83[0m
[36m[2023-07-10 12:20:04,824][227910] itr=364, itrs=2000, Progress: 18.20%[0m
[36m[2023-07-10 12:20:16,495][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 12:20:16,496][227910] FPS: 329503.19[0m
[36m[2023-07-10 12:20:21,259][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:20:21,259][227910] Reward + Measures: [[20.77924177  0.93079466  0.80547148  0.93693662  0.00999869]][0m
[37m[1m[2023-07-10 12:20:21,260][227910] Max Reward on eval: 20.779241773341234[0m
[37m[1m[2023-07-10 12:20:21,260][227910] Min Reward on eval: 20.779241773341234[0m
[37m[1m[2023-07-10 12:20:21,260][227910] Mean Reward across all agents: 20.779241773341234[0m
[37m[1m[2023-07-10 12:20:21,260][227910] Average Trajectory Length: 999.8536666666666[0m
[36m[2023-07-10 12:20:26,703][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:20:26,703][227910] Reward + Measures: [[ -97.28727779    0.93510002    0.53250003    0.92319995    0.0178    ]
 [-161.73759743    0.92940009    0.53789997    0.93990004    0.0236    ]
 [ -32.20377833    0.83069992    0.0839        0.82429999    0.26180002]
 ...
 [-601.96303486    0.48330003    0.20830002    0.49589998    0.13109998]
 [-385.22186359    0.56940001    0.1178        0.56540006    0.27509999]
 [-275.68791109    0.82560009    0.24499999    0.82980007    0.098     ]][0m
[37m[1m[2023-07-10 12:20:26,704][227910] Max Reward on eval: 201.91466262976172[0m
[37m[1m[2023-07-10 12:20:26,704][227910] Min Reward on eval: -601.9630348590902[0m
[37m[1m[2023-07-10 12:20:26,704][227910] Mean Reward across all agents: -50.53117347196383[0m
[37m[1m[2023-07-10 12:20:26,704][227910] Average Trajectory Length: 996.7963333333333[0m
[36m[2023-07-10 12:20:26,709][227910] mean_value=12.53968804737137, max_value=617.798415263879[0m
[37m[1m[2023-07-10 12:20:26,712][227910] New mean coefficients: [[-0.98529595  1.9939215   3.517776    4.0962577  -0.49384046]][0m
[37m[1m[2023-07-10 12:20:26,713][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:20:36,503][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 12:20:36,504][227910] FPS: 392286.90[0m
[36m[2023-07-10 12:20:36,506][227910] itr=365, itrs=2000, Progress: 18.25%[0m
[36m[2023-07-10 12:20:47,998][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 12:20:47,998][227910] FPS: 334672.30[0m
[36m[2023-07-10 12:20:52,663][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:20:52,663][227910] Reward + Measures: [[11.39992058  0.92520165  0.83778781  0.93340892  0.0092125 ]][0m
[37m[1m[2023-07-10 12:20:52,663][227910] Max Reward on eval: 11.399920582651808[0m
[37m[1m[2023-07-10 12:20:52,663][227910] Min Reward on eval: 11.399920582651808[0m
[37m[1m[2023-07-10 12:20:52,664][227910] Mean Reward across all agents: 11.399920582651808[0m
[37m[1m[2023-07-10 12:20:52,664][227910] Average Trajectory Length: 999.6953333333333[0m
[36m[2023-07-10 12:20:57,986][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:20:57,987][227910] Reward + Measures: [[  -50.70006135     0.90480006     0.0979         0.83459997
      0.57629997]
 [  -26.99459439     0.9023         0.14099999     0.87819999
      0.34369999]
 [-1040.52007018     0.93500006     0.5133         0.92230004
      0.86280006]
 ...
 [ -164.53011262     0.92059994     0.61320001     0.89960003
      0.0115    ]
 [ -243.22146088     0.83829993     0.1441         0.78859997
      0.5941    ]
 [ -184.24397285     0.8075         0.1327         0.79360002
      0.39890003]][0m
[37m[1m[2023-07-10 12:20:57,987][227910] Max Reward on eval: 107.2032149564242[0m
[37m[1m[2023-07-10 12:20:57,987][227910] Min Reward on eval: -1176.5218285790877[0m
[37m[1m[2023-07-10 12:20:57,988][227910] Mean Reward across all agents: -263.7369917289141[0m
[37m[1m[2023-07-10 12:20:57,988][227910] Average Trajectory Length: 995.164[0m
[36m[2023-07-10 12:20:57,994][227910] mean_value=-3.459158644044413, max_value=562.3835872815363[0m
[37m[1m[2023-07-10 12:20:57,997][227910] New mean coefficients: [[-1.0535383  1.3836446  4.472609   3.996289  -1.7009691]][0m
[37m[1m[2023-07-10 12:20:57,998][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:21:07,532][227910] train() took 9.53 seconds to complete[0m
[36m[2023-07-10 12:21:07,533][227910] FPS: 402821.88[0m
[36m[2023-07-10 12:21:07,535][227910] itr=366, itrs=2000, Progress: 18.30%[0m
[36m[2023-07-10 12:21:19,008][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 12:21:19,008][227910] FPS: 335147.57[0m
[36m[2023-07-10 12:21:23,724][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:21:23,724][227910] Reward + Measures: [[-1.05547115  0.93252766  0.85733527  0.94204932  0.00812533]][0m
[37m[1m[2023-07-10 12:21:23,724][227910] Max Reward on eval: -1.0554711525257106[0m
[37m[1m[2023-07-10 12:21:23,725][227910] Min Reward on eval: -1.0554711525257106[0m
[37m[1m[2023-07-10 12:21:23,725][227910] Mean Reward across all agents: -1.0554711525257106[0m
[37m[1m[2023-07-10 12:21:23,725][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:21:29,260][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:21:29,266][227910] Reward + Measures: [[-252.09274059    0.95310003    0.85480005    0.94570011    0.0095    ]
 [-103.15401527    0.87250006    0.79500002    0.88520002    0.0052    ]
 [-154.5814172     0.95620006    0.89750004    0.95900005    0.0081    ]
 ...
 [-269.73277345    0.96020001    0.87089998    0.95599997    0.0069    ]
 [ -37.22724704    0.94729996    0.87819999    0.95609999    0.0074    ]
 [-167.75930351    0.95190001    0.68740004    0.94099998    0.0317    ]][0m
[37m[1m[2023-07-10 12:21:29,266][227910] Max Reward on eval: 137.76316898546648[0m
[37m[1m[2023-07-10 12:21:29,266][227910] Min Reward on eval: -522.0337151908141[0m
[37m[1m[2023-07-10 12:21:29,266][227910] Mean Reward across all agents: -123.88622875213075[0m
[37m[1m[2023-07-10 12:21:29,267][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:21:29,268][227910] mean_value=-191.92370105991168, max_value=596.7709560503298[0m
[37m[1m[2023-07-10 12:21:29,271][227910] New mean coefficients: [[-0.7233921  1.5011356  3.6603355  2.965767  -2.2621799]][0m
[37m[1m[2023-07-10 12:21:29,272][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:21:39,001][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 12:21:39,001][227910] FPS: 394758.05[0m
[36m[2023-07-10 12:21:39,003][227910] itr=367, itrs=2000, Progress: 18.35%[0m
[36m[2023-07-10 12:21:50,450][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 12:21:50,450][227910] FPS: 335922.20[0m
[36m[2023-07-10 12:21:55,330][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:21:55,330][227910] Reward + Measures: [[32.30463779  0.93407106  0.89561361  0.94379801  0.00673433]][0m
[37m[1m[2023-07-10 12:21:55,331][227910] Max Reward on eval: 32.304637788610876[0m
[37m[1m[2023-07-10 12:21:55,331][227910] Min Reward on eval: 32.304637788610876[0m
[37m[1m[2023-07-10 12:21:55,331][227910] Mean Reward across all agents: 32.304637788610876[0m
[37m[1m[2023-07-10 12:21:55,331][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:22:00,881][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:22:00,887][227910] Reward + Measures: [[   1.13636772    0.94750005    0.861         0.95920008    0.0058    ]
 [ -54.30603231    0.94110006    0.82200003    0.95410007    0.0043    ]
 [  59.56428241    0.95499992    0.8955        0.96449995    0.0057    ]
 ...
 [-730.32538476    0.61400002    0.36470005    0.61750001    0.0974    ]
 [-360.72656458    0.83530426    0.62281162    0.82914585    0.03563354]
 [-324.1092678     0.93650001    0.56949997    0.94099998    0.0417    ]][0m
[37m[1m[2023-07-10 12:22:00,887][227910] Max Reward on eval: 178.49252940930893[0m
[37m[1m[2023-07-10 12:22:00,888][227910] Min Reward on eval: -799.0701780172415[0m
[37m[1m[2023-07-10 12:22:00,888][227910] Mean Reward across all agents: -300.0918620033877[0m
[37m[1m[2023-07-10 12:22:00,888][227910] Average Trajectory Length: 996.1816666666666[0m
[36m[2023-07-10 12:22:00,890][227910] mean_value=-328.3236219013596, max_value=347.59273264407165[0m
[37m[1m[2023-07-10 12:22:00,892][227910] New mean coefficients: [[-0.41711786  1.3401322   2.8585455   3.2972455   0.26270604]][0m
[37m[1m[2023-07-10 12:22:00,893][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:22:10,674][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 12:22:10,674][227910] FPS: 392685.82[0m
[36m[2023-07-10 12:22:10,677][227910] itr=368, itrs=2000, Progress: 18.40%[0m
[36m[2023-07-10 12:22:22,214][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 12:22:22,214][227910] FPS: 333311.42[0m
[36m[2023-07-10 12:22:26,928][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:22:26,933][227910] Reward + Measures: [[49.03566326  0.94459975  0.91693401  0.951796    0.006481  ]][0m
[37m[1m[2023-07-10 12:22:26,934][227910] Max Reward on eval: 49.03566325840001[0m
[37m[1m[2023-07-10 12:22:26,934][227910] Min Reward on eval: 49.03566325840001[0m
[37m[1m[2023-07-10 12:22:26,934][227910] Mean Reward across all agents: 49.03566325840001[0m
[37m[1m[2023-07-10 12:22:26,934][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:22:32,401][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:22:32,401][227910] Reward + Measures: [[  74.65976238    0.93050003    0.6548        0.93090004    0.06870001]
 [-258.09852109    0.91320002    0.82380009    0.88749999    0.86020005]
 [  93.43141995    0.94799995    0.80880004    0.93619996    0.0071    ]
 ...
 [-220.18946136    0.96160001    0.59650004    0.96600002    0.08149999]
 [  37.19923228    0.93759996    0.71150005    0.93290007    0.0065    ]
 [  83.14814741    0.95479995    0.82959998    0.96049994    0.0214    ]][0m
[37m[1m[2023-07-10 12:22:32,401][227910] Max Reward on eval: 258.54521160997683[0m
[37m[1m[2023-07-10 12:22:32,402][227910] Min Reward on eval: -602.1420268606628[0m
[37m[1m[2023-07-10 12:22:32,402][227910] Mean Reward across all agents: -55.77304932042988[0m
[37m[1m[2023-07-10 12:22:32,402][227910] Average Trajectory Length: 999.3906666666667[0m
[36m[2023-07-10 12:22:32,407][227910] mean_value=27.97953817714995, max_value=597.3261099799769[0m
[37m[1m[2023-07-10 12:22:32,410][227910] New mean coefficients: [[-1.0790443   1.5192703   3.5503438   3.4664564  -0.36399037]][0m
[37m[1m[2023-07-10 12:22:32,411][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:22:42,097][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 12:22:42,097][227910] FPS: 396542.32[0m
[36m[2023-07-10 12:22:42,099][227910] itr=369, itrs=2000, Progress: 18.45%[0m
[36m[2023-07-10 12:22:53,604][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 12:22:53,605][227910] FPS: 334207.69[0m
[36m[2023-07-10 12:22:58,499][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:22:58,499][227910] Reward + Measures: [[4.66606052 0.94534636 0.91165024 0.95136303 0.00652867]][0m
[37m[1m[2023-07-10 12:22:58,500][227910] Max Reward on eval: 4.666060515543276[0m
[37m[1m[2023-07-10 12:22:58,500][227910] Min Reward on eval: 4.666060515543276[0m
[37m[1m[2023-07-10 12:22:58,500][227910] Mean Reward across all agents: 4.666060515543276[0m
[37m[1m[2023-07-10 12:22:58,500][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:23:04,078][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:23:04,078][227910] Reward + Measures: [[ -897.50363837     0.68441349     0.06945394     0.70729661
      0.56690115]
 [-1193.4954812      0.58978748     0.07055        0.63714999
      0.4367125 ]
 [ -556.12448267     0.8976         0.0396         0.91240007
      0.76540005]
 ...
 [  -31.09016711     0.93009996     0.0721         0.90630001
      0.37219998]
 [ -563.74213513     0.56861556     0.13004099     0.62235993
      0.45061779]
 [ -990.50913634     0.64372867     0.06854315     0.7311458
      0.52603161]][0m
[37m[1m[2023-07-10 12:23:04,079][227910] Max Reward on eval: 118.33101284126751[0m
[37m[1m[2023-07-10 12:23:04,079][227910] Min Reward on eval: -1642.380871318234[0m
[37m[1m[2023-07-10 12:23:04,079][227910] Mean Reward across all agents: -412.59377754733663[0m
[37m[1m[2023-07-10 12:23:04,079][227910] Average Trajectory Length: 956.2556666666667[0m
[36m[2023-07-10 12:23:04,084][227910] mean_value=-86.2106736078706, max_value=547.6770108743337[0m
[37m[1m[2023-07-10 12:23:04,087][227910] New mean coefficients: [[-0.6608461   2.1905046   3.3973227   3.1705022  -0.36357418]][0m
[37m[1m[2023-07-10 12:23:04,088][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:23:13,924][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 12:23:13,925][227910] FPS: 390490.74[0m
[36m[2023-07-10 12:23:13,927][227910] itr=370, itrs=2000, Progress: 18.50%[0m
[37m[1m[2023-07-10 12:23:16,539][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000350[0m
[36m[2023-07-10 12:23:28,453][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 12:23:28,453][227910] FPS: 329755.13[0m
[36m[2023-07-10 12:23:33,225][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:23:33,225][227910] Reward + Measures: [[-47.02329477   0.94380873   0.91263068   0.95028865   0.006417  ]][0m
[37m[1m[2023-07-10 12:23:33,225][227910] Max Reward on eval: -47.023294765553885[0m
[37m[1m[2023-07-10 12:23:33,226][227910] Min Reward on eval: -47.023294765553885[0m
[37m[1m[2023-07-10 12:23:33,226][227910] Mean Reward across all agents: -47.023294765553885[0m
[37m[1m[2023-07-10 12:23:33,226][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:23:38,665][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:23:38,665][227910] Reward + Measures: [[-308.52535956    0.9443        0.56160003    0.95500004    0.0259    ]
 [-154.66351061    0.94189996    0.1177        0.92979997    0.2194    ]
 [-219.05485265    0.9483        0.86499995    0.954         0.0061    ]
 ...
 [-259.0543692     0.92970002    0.345         0.94050008    0.0579    ]
 [-213.68527269    0.86260003    0.8452        0.87180007    0.0088    ]
 [ -80.31774516    0.95190012    0.87239999    0.95539999    0.0076    ]][0m
[37m[1m[2023-07-10 12:23:38,666][227910] Max Reward on eval: 171.97895835512318[0m
[37m[1m[2023-07-10 12:23:38,666][227910] Min Reward on eval: -612.4403334435192[0m
[37m[1m[2023-07-10 12:23:38,666][227910] Mean Reward across all agents: -212.30384972587487[0m
[37m[1m[2023-07-10 12:23:38,666][227910] Average Trajectory Length: 998.7733333333333[0m
[36m[2023-07-10 12:23:38,668][227910] mean_value=-227.95834686346686, max_value=365.9032309682213[0m
[37m[1m[2023-07-10 12:23:38,671][227910] New mean coefficients: [[-0.36613104  1.9108064   3.0670497   4.472974   -0.4749822 ]][0m
[37m[1m[2023-07-10 12:23:38,672][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:23:48,514][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 12:23:48,515][227910] FPS: 390210.48[0m
[36m[2023-07-10 12:23:48,517][227910] itr=371, itrs=2000, Progress: 18.55%[0m
[36m[2023-07-10 12:24:00,106][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 12:24:00,106][227910] FPS: 331873.00[0m
[36m[2023-07-10 12:24:04,962][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:24:04,962][227910] Reward + Measures: [[-9.34746498  0.94075066  0.91667795  0.94768065  0.006149  ]][0m
[37m[1m[2023-07-10 12:24:04,963][227910] Max Reward on eval: -9.347464980080273[0m
[37m[1m[2023-07-10 12:24:04,963][227910] Min Reward on eval: -9.347464980080273[0m
[37m[1m[2023-07-10 12:24:04,963][227910] Mean Reward across all agents: -9.347464980080273[0m
[37m[1m[2023-07-10 12:24:04,963][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:24:10,580][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:24:10,581][227910] Reward + Measures: [[-380.84166405    0.94940007    0.76520002    0.94679993    0.0085    ]
 [ 109.97552379    0.93549997    0.0503        0.92729998    0.42080003]
 [ -18.61325613    0.94049996    0.17850001    0.9479        0.19589999]
 ...
 [-414.17057953    0.84499997    0.69639999    0.84560007    0.0173    ]
 [-208.27753923    0.94480002    0.91180003    0.95230001    0.0085    ]
 [-400.03948227    0.9479        0.43520004    0.95120001    0.0832    ]][0m
[37m[1m[2023-07-10 12:24:10,581][227910] Max Reward on eval: 229.3674232322839[0m
[37m[1m[2023-07-10 12:24:10,581][227910] Min Reward on eval: -744.636254396825[0m
[37m[1m[2023-07-10 12:24:10,581][227910] Mean Reward across all agents: -229.4831786784741[0m
[37m[1m[2023-07-10 12:24:10,582][227910] Average Trajectory Length: 998.4296666666667[0m
[36m[2023-07-10 12:24:10,585][227910] mean_value=-200.64912779743193, max_value=568.3817261236254[0m
[37m[1m[2023-07-10 12:24:10,588][227910] New mean coefficients: [[-1.3528886  2.6108813  3.4649587  3.3488398 -0.8952005]][0m
[37m[1m[2023-07-10 12:24:10,589][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:24:20,305][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 12:24:20,306][227910] FPS: 395270.06[0m
[36m[2023-07-10 12:24:20,308][227910] itr=372, itrs=2000, Progress: 18.60%[0m
[36m[2023-07-10 12:24:31,882][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 12:24:31,882][227910] FPS: 332228.02[0m
[36m[2023-07-10 12:24:36,860][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:24:36,865][227910] Reward + Measures: [[-95.13042415   0.93958032   0.91622931   0.9467687    0.00640733]][0m
[37m[1m[2023-07-10 12:24:36,865][227910] Max Reward on eval: -95.13042415009815[0m
[37m[1m[2023-07-10 12:24:36,866][227910] Min Reward on eval: -95.13042415009815[0m
[37m[1m[2023-07-10 12:24:36,866][227910] Mean Reward across all agents: -95.13042415009815[0m
[37m[1m[2023-07-10 12:24:36,866][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:24:42,395][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:24:42,396][227910] Reward + Measures: [[  51.36119173    0.95970005    0.91950005    0.96470004    0.0054    ]
 [-350.84737189    0.86020005    0.83430004    0.88669997    0.0094    ]
 [-243.25221766    0.94130003    0.75279999    0.94119996    0.0188    ]
 ...
 [-195.14775525    0.94589996    0.87970001    0.94670004    0.0097    ]
 [-267.30425169    0.95340008    0.83620006    0.9587        0.0175    ]
 [ -82.70822143    0.9479        0.92500001    0.95880002    0.0091    ]][0m
[37m[1m[2023-07-10 12:24:42,396][227910] Max Reward on eval: 77.6533941256348[0m
[37m[1m[2023-07-10 12:24:42,396][227910] Min Reward on eval: -635.2849268026999[0m
[37m[1m[2023-07-10 12:24:42,397][227910] Mean Reward across all agents: -237.19280225714581[0m
[37m[1m[2023-07-10 12:24:42,397][227910] Average Trajectory Length: 999.6859999999999[0m
[36m[2023-07-10 12:24:42,399][227910] mean_value=-308.0385686830541, max_value=239.38844609118038[0m
[37m[1m[2023-07-10 12:24:42,401][227910] New mean coefficients: [[-1.1388178  2.6064286  5.0312357  3.7922099 -1.3806064]][0m
[37m[1m[2023-07-10 12:24:42,402][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:24:52,021][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 12:24:52,021][227910] FPS: 399282.98[0m
[36m[2023-07-10 12:24:52,023][227910] itr=373, itrs=2000, Progress: 18.65%[0m
[36m[2023-07-10 12:25:03,519][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 12:25:03,519][227910] FPS: 334558.72[0m
[36m[2023-07-10 12:25:08,311][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:25:08,312][227910] Reward + Measures: [[-97.27817284   0.93660671   0.91185498   0.94375527   0.00621667]][0m
[37m[1m[2023-07-10 12:25:08,312][227910] Max Reward on eval: -97.2781728447599[0m
[37m[1m[2023-07-10 12:25:08,312][227910] Min Reward on eval: -97.2781728447599[0m
[37m[1m[2023-07-10 12:25:08,312][227910] Mean Reward across all agents: -97.2781728447599[0m
[37m[1m[2023-07-10 12:25:08,313][227910] Average Trajectory Length: 999.6833333333333[0m
[36m[2023-07-10 12:25:13,871][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:25:13,871][227910] Reward + Measures: [[-635.02508779    0.68000001    0.2723        0.66619998    0.1724    ]
 [-310.68891896    0.95469999    0.89870006    0.95959997    0.0084    ]
 [-347.19589942    0.94110006    0.84959996    0.94169998    0.007     ]
 ...
 [-117.81164905    0.95090002    0.90939999    0.95310003    0.0059    ]
 [-459.94389827    0.94490004    0.833         0.94849998    0.0073    ]
 [-103.39191079    0.95769995    0.87789994    0.96330005    0.0042    ]][0m
[37m[1m[2023-07-10 12:25:13,871][227910] Max Reward on eval: 76.75463558332994[0m
[37m[1m[2023-07-10 12:25:13,872][227910] Min Reward on eval: -715.8368475933297[0m
[37m[1m[2023-07-10 12:25:13,872][227910] Mean Reward across all agents: -322.0238079961931[0m
[37m[1m[2023-07-10 12:25:13,872][227910] Average Trajectory Length: 997.5366666666666[0m
[36m[2023-07-10 12:25:13,873][227910] mean_value=-407.5466377333292, max_value=66.25082723440346[0m
[37m[1m[2023-07-10 12:25:13,876][227910] New mean coefficients: [[-0.85588264  2.0077019   2.9674377   2.7975373  -0.8678211 ]][0m
[37m[1m[2023-07-10 12:25:13,877][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:25:23,581][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 12:25:23,581][227910] FPS: 395779.43[0m
[36m[2023-07-10 12:25:23,583][227910] itr=374, itrs=2000, Progress: 18.70%[0m
[36m[2023-07-10 12:25:35,151][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 12:25:35,152][227910] FPS: 332389.34[0m
[36m[2023-07-10 12:25:39,939][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:25:39,940][227910] Reward + Measures: [[-98.02991859   0.94519395   0.9194082    0.95204347   0.00572926]][0m
[37m[1m[2023-07-10 12:25:39,940][227910] Max Reward on eval: -98.02991858738177[0m
[37m[1m[2023-07-10 12:25:39,940][227910] Min Reward on eval: -98.02991858738177[0m
[37m[1m[2023-07-10 12:25:39,940][227910] Mean Reward across all agents: -98.02991858738177[0m
[37m[1m[2023-07-10 12:25:39,941][227910] Average Trajectory Length: 999.3829999999999[0m
[36m[2023-07-10 12:25:45,209][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:25:45,209][227910] Reward + Measures: [[-211.8501306     0.94520009    0.67539996    0.9443        0.066     ]
 [ -46.01791852    0.95360005    0.92469996    0.95950001    0.0088    ]
 [ -32.85221285    0.95929998    0.9224        0.96490002    0.006     ]
 ...
 [ -13.82291264    0.94449997    0.88970006    0.94010001    0.015     ]
 [ -52.42594708    0.96280003    0.94109994    0.97180003    0.0019    ]
 [-166.02694868    0.96050006    0.92930001    0.96990007    0.0031    ]][0m
[37m[1m[2023-07-10 12:25:45,209][227910] Max Reward on eval: 41.11686232992215[0m
[37m[1m[2023-07-10 12:25:45,210][227910] Min Reward on eval: -563.9616452507792[0m
[37m[1m[2023-07-10 12:25:45,210][227910] Mean Reward across all agents: -202.6919645654936[0m
[37m[1m[2023-07-10 12:25:45,210][227910] Average Trajectory Length: 999.2756666666667[0m
[36m[2023-07-10 12:25:45,214][227910] mean_value=-181.17267196059194, max_value=440.017258205777[0m
[37m[1m[2023-07-10 12:25:45,217][227910] New mean coefficients: [[-0.80888355  2.8932087   6.0033617   3.3379154  -0.6835722 ]][0m
[37m[1m[2023-07-10 12:25:45,218][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:25:55,002][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 12:25:55,002][227910] FPS: 392553.86[0m
[36m[2023-07-10 12:25:55,004][227910] itr=375, itrs=2000, Progress: 18.75%[0m
[36m[2023-07-10 12:26:06,699][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 12:26:06,699][227910] FPS: 328862.63[0m
[36m[2023-07-10 12:26:11,444][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:26:11,444][227910] Reward + Measures: [[-71.19420954   0.94647801   0.91897136   0.9530257    0.00558433]][0m
[37m[1m[2023-07-10 12:26:11,444][227910] Max Reward on eval: -71.19420953825623[0m
[37m[1m[2023-07-10 12:26:11,445][227910] Min Reward on eval: -71.19420953825623[0m
[37m[1m[2023-07-10 12:26:11,445][227910] Mean Reward across all agents: -71.19420953825623[0m
[37m[1m[2023-07-10 12:26:11,445][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:26:16,957][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:26:16,957][227910] Reward + Measures: [[ -57.34121262    0.93379992    0.139         0.94470006    0.43130001]
 [-271.22288964    0.94139999    0.70530003    0.94900006    0.0158    ]
 [ -33.25104009    0.95419997    0.92379999    0.96050006    0.0047    ]
 ...
 [-435.58796218    0.81099999    0.1427        0.80130005    0.25600001]
 [  -3.48266535    0.9271        0.23140001    0.93710005    0.39519998]
 [-171.54177773    0.94849998    0.61190003    0.96029997    0.091     ]][0m
[37m[1m[2023-07-10 12:26:16,957][227910] Max Reward on eval: 73.04594236116391[0m
[37m[1m[2023-07-10 12:26:16,958][227910] Min Reward on eval: -588.3005677107255[0m
[37m[1m[2023-07-10 12:26:16,958][227910] Mean Reward across all agents: -208.77237128377215[0m
[37m[1m[2023-07-10 12:26:16,958][227910] Average Trajectory Length: 999.3733333333333[0m
[36m[2023-07-10 12:26:16,961][227910] mean_value=-172.96918707035275, max_value=463.7056795094675[0m
[37m[1m[2023-07-10 12:26:16,964][227910] New mean coefficients: [[-1.5383024  4.114357   7.105009   3.284657   0.2757902]][0m
[37m[1m[2023-07-10 12:26:16,965][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:26:26,832][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 12:26:26,832][227910] FPS: 389244.71[0m
[36m[2023-07-10 12:26:26,834][227910] itr=376, itrs=2000, Progress: 18.80%[0m
[36m[2023-07-10 12:26:38,423][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 12:26:38,424][227910] FPS: 331794.55[0m
[36m[2023-07-10 12:26:43,119][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:26:43,120][227910] Reward + Measures: [[-95.40160031   0.93913567   0.91006696   0.94650495   0.006198  ]][0m
[37m[1m[2023-07-10 12:26:43,120][227910] Max Reward on eval: -95.40160030996307[0m
[37m[1m[2023-07-10 12:26:43,120][227910] Min Reward on eval: -95.40160030996307[0m
[37m[1m[2023-07-10 12:26:43,121][227910] Mean Reward across all agents: -95.40160030996307[0m
[37m[1m[2023-07-10 12:26:43,121][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:26:48,723][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:26:48,729][227910] Reward + Measures: [[ -99.87871873    0.96679991    0.87729996    0.96700001    0.0071    ]
 [ 115.2354805     0.95880002    0.93010008    0.96509999    0.0057    ]
 [-237.58356919    0.92570001    0.56989998    0.89429998    0.0684    ]
 ...
 [ -40.97882697    0.95030004    0.9016        0.95640004    0.0076    ]
 [-232.63016507    0.95050001    0.74450004    0.94469994    0.0272    ]
 [ -63.75191294    0.94539994    0.70139998    0.92379993    0.05269999]][0m
[37m[1m[2023-07-10 12:26:48,729][227910] Max Reward on eval: 204.56449729243758[0m
[37m[1m[2023-07-10 12:26:48,729][227910] Min Reward on eval: -574.1798047053046[0m
[37m[1m[2023-07-10 12:26:48,730][227910] Mean Reward across all agents: -140.48275029742106[0m
[37m[1m[2023-07-10 12:26:48,730][227910] Average Trajectory Length: 999.4786666666666[0m
[36m[2023-07-10 12:26:48,732][227910] mean_value=-243.8128820273886, max_value=205.34020721021227[0m
[37m[1m[2023-07-10 12:26:48,734][227910] New mean coefficients: [[-1.4334105  2.7454543  6.839572   3.145563   0.7655213]][0m
[37m[1m[2023-07-10 12:26:48,735][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:26:58,534][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 12:26:58,534][227910] FPS: 391958.45[0m
[36m[2023-07-10 12:26:58,536][227910] itr=377, itrs=2000, Progress: 18.85%[0m
[36m[2023-07-10 12:27:10,168][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 12:27:10,168][227910] FPS: 330577.21[0m
[36m[2023-07-10 12:27:14,883][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:27:14,883][227910] Reward + Measures: [[-159.08460539    0.94370252    0.91977024    0.95142138    0.00544448]][0m
[37m[1m[2023-07-10 12:27:14,883][227910] Max Reward on eval: -159.0846053889905[0m
[37m[1m[2023-07-10 12:27:14,884][227910] Min Reward on eval: -159.0846053889905[0m
[37m[1m[2023-07-10 12:27:14,884][227910] Mean Reward across all agents: -159.0846053889905[0m
[37m[1m[2023-07-10 12:27:14,884][227910] Average Trajectory Length: 999.6836666666667[0m
[36m[2023-07-10 12:27:20,330][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:27:20,330][227910] Reward + Measures: [[-130.0655479     0.95150006    0.91320002    0.96329993    0.0048    ]
 [-410.33937464    0.83100003    0.70429993    0.82629997    0.0165    ]
 [-299.90075305    0.85350001    0.8398        0.87659997    0.0072    ]
 ...
 [-316.88249058    0.9465        0.81399995    0.94260007    0.0118    ]
 [-288.25951873    0.92580003    0.79560006    0.94210005    0.0066    ]
 [-634.30616614    0.82140011    0.45000002    0.81870002    0.0848    ]][0m
[37m[1m[2023-07-10 12:27:20,331][227910] Max Reward on eval: 96.31016731837299[0m
[37m[1m[2023-07-10 12:27:20,331][227910] Min Reward on eval: -634.306166141713[0m
[37m[1m[2023-07-10 12:27:20,331][227910] Mean Reward across all agents: -296.1352820075959[0m
[37m[1m[2023-07-10 12:27:20,331][227910] Average Trajectory Length: 999.3523333333333[0m
[36m[2023-07-10 12:27:20,333][227910] mean_value=-364.04267980025656, max_value=152.15847081941746[0m
[37m[1m[2023-07-10 12:27:20,335][227910] New mean coefficients: [[-1.129096    3.1200502   6.4904113   2.1107028   0.22761059]][0m
[37m[1m[2023-07-10 12:27:20,336][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:27:30,044][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 12:27:30,045][227910] FPS: 395592.19[0m
[36m[2023-07-10 12:27:30,047][227910] itr=378, itrs=2000, Progress: 18.90%[0m
[36m[2023-07-10 12:27:41,571][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 12:27:41,572][227910] FPS: 333731.25[0m
[36m[2023-07-10 12:27:46,488][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:27:46,489][227910] Reward + Measures: [[-167.02790584    0.94542909    0.92223597    0.95259339    0.00538387]][0m
[37m[1m[2023-07-10 12:27:46,489][227910] Max Reward on eval: -167.02790584413478[0m
[37m[1m[2023-07-10 12:27:46,489][227910] Min Reward on eval: -167.02790584413478[0m
[37m[1m[2023-07-10 12:27:46,489][227910] Mean Reward across all agents: -167.02790584413478[0m
[37m[1m[2023-07-10 12:27:46,490][227910] Average Trajectory Length: 999.6996666666666[0m
[36m[2023-07-10 12:27:52,029][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:27:52,030][227910] Reward + Measures: [[ -98.91214144    0.87080002    0.81290001    0.87120008    0.0071    ]
 [-171.36163323    0.95409995    0.92260009    0.96030009    0.0051    ]
 [ -16.00459805    0.95120001    0.83730012    0.95370007    0.0076    ]
 ...
 [ -66.60577339    0.96149999    0.89209998    0.96609992    0.0046    ]
 [-220.98469397    0.7809        0.70910001    0.7895        0.0108    ]
 [  22.16231646    0.96309996    0.86019993    0.95309991    0.0067    ]][0m
[37m[1m[2023-07-10 12:27:52,030][227910] Max Reward on eval: 282.3698940679198[0m
[37m[1m[2023-07-10 12:27:52,030][227910] Min Reward on eval: -629.5063006785931[0m
[37m[1m[2023-07-10 12:27:52,030][227910] Mean Reward across all agents: -38.03241952385357[0m
[37m[1m[2023-07-10 12:27:52,031][227910] Average Trajectory Length: 999.7023333333333[0m
[36m[2023-07-10 12:27:52,033][227910] mean_value=-167.95176031198002, max_value=185.68299126190203[0m
[37m[1m[2023-07-10 12:27:52,035][227910] New mean coefficients: [[-1.1597781  4.571794   6.3262625  1.919679  -0.3561089]][0m
[37m[1m[2023-07-10 12:27:52,036][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:28:01,932][227910] train() took 9.89 seconds to complete[0m
[36m[2023-07-10 12:28:01,933][227910] FPS: 388093.50[0m
[36m[2023-07-10 12:28:01,935][227910] itr=379, itrs=2000, Progress: 18.95%[0m
[36m[2023-07-10 12:28:13,553][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 12:28:13,554][227910] FPS: 330946.07[0m
[36m[2023-07-10 12:28:18,352][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:28:18,352][227910] Reward + Measures: [[-228.78226566    0.94236231    0.92271501    0.95067561    0.00534167]][0m
[37m[1m[2023-07-10 12:28:18,353][227910] Max Reward on eval: -228.78226565620506[0m
[37m[1m[2023-07-10 12:28:18,353][227910] Min Reward on eval: -228.78226565620506[0m
[37m[1m[2023-07-10 12:28:18,353][227910] Mean Reward across all agents: -228.78226565620506[0m
[37m[1m[2023-07-10 12:28:18,353][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:28:23,799][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:28:23,799][227910] Reward + Measures: [[-211.06797494    0.91790003    0.0427        0.90630001    0.72240001]
 [-226.04704727    0.84289998    0.0522        0.82560009    0.61379999]
 [ -66.54862179    0.94480002    0.1037        0.94450009    0.47490007]
 ...
 [ -92.04756325    0.94669992    0.0921        0.95990002    0.38480002]
 [ -63.34142333    0.93699998    0.1038        0.94260007    0.44890004]
 [-128.68238849    0.92580003    0.0365        0.90219992    0.70620006]][0m
[37m[1m[2023-07-10 12:28:23,800][227910] Max Reward on eval: 9.355220064683817[0m
[37m[1m[2023-07-10 12:28:23,800][227910] Min Reward on eval: -760.5840110741556[0m
[37m[1m[2023-07-10 12:28:23,800][227910] Mean Reward across all agents: -203.13700073666578[0m
[37m[1m[2023-07-10 12:28:23,800][227910] Average Trajectory Length: 999.0786666666667[0m
[36m[2023-07-10 12:28:23,804][227910] mean_value=-89.9540134421153, max_value=435.33647063490935[0m
[37m[1m[2023-07-10 12:28:23,806][227910] New mean coefficients: [[-1.1621085  5.6208067  7.0903053  2.1464021 -1.9079454]][0m
[37m[1m[2023-07-10 12:28:23,807][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:28:33,477][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 12:28:33,477][227910] FPS: 397189.13[0m
[36m[2023-07-10 12:28:33,479][227910] itr=380, itrs=2000, Progress: 19.00%[0m
[37m[1m[2023-07-10 12:28:36,090][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000360[0m
[36m[2023-07-10 12:28:47,950][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 12:28:47,950][227910] FPS: 330899.78[0m
[36m[2023-07-10 12:28:52,725][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:28:52,725][227910] Reward + Measures: [[-231.20216467    0.94155192    0.92111897    0.95005405    0.00522433]][0m
[37m[1m[2023-07-10 12:28:52,725][227910] Max Reward on eval: -231.20216467000006[0m
[37m[1m[2023-07-10 12:28:52,726][227910] Min Reward on eval: -231.20216467000006[0m
[37m[1m[2023-07-10 12:28:52,726][227910] Mean Reward across all agents: -231.20216467000006[0m
[37m[1m[2023-07-10 12:28:52,726][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:28:58,333][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:28:58,334][227910] Reward + Measures: [[-199.68987259    0.86910003    0.72349995    0.84750003    0.0097    ]
 [-651.63648768    0.7726        0.67659998    0.78870004    0.0185    ]
 [-470.02163208    0.94040006    0.83929998    0.95250005    0.0205    ]
 ...
 [-364.74242875    0.94550002    0.8459        0.94999999    0.007     ]
 [-331.51803478    0.87179995    0.83579999    0.88749999    0.0083    ]
 [-245.50596432    0.95739996    0.87700003    0.96270001    0.0078    ]][0m
[37m[1m[2023-07-10 12:28:58,334][227910] Max Reward on eval: 178.3874502605642[0m
[37m[1m[2023-07-10 12:28:58,334][227910] Min Reward on eval: -814.4984773539356[0m
[37m[1m[2023-07-10 12:28:58,335][227910] Mean Reward across all agents: -303.4199690809255[0m
[37m[1m[2023-07-10 12:28:58,335][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:28:58,336][227910] mean_value=-419.3610912098697, max_value=277.04250984773705[0m
[37m[1m[2023-07-10 12:28:58,339][227910] New mean coefficients: [[-1.1346021  3.563799   7.667447   2.446943  -1.2496189]][0m
[37m[1m[2023-07-10 12:28:58,340][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:29:07,999][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 12:29:07,999][227910] FPS: 397606.14[0m
[36m[2023-07-10 12:29:08,001][227910] itr=381, itrs=2000, Progress: 19.05%[0m
[36m[2023-07-10 12:29:19,561][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 12:29:19,561][227910] FPS: 332648.04[0m
[36m[2023-07-10 12:29:24,385][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:29:24,386][227910] Reward + Measures: [[-229.08245521    0.94721305    0.92618334    0.95530492    0.004932  ]][0m
[37m[1m[2023-07-10 12:29:24,386][227910] Max Reward on eval: -229.08245521215008[0m
[37m[1m[2023-07-10 12:29:24,386][227910] Min Reward on eval: -229.08245521215008[0m
[37m[1m[2023-07-10 12:29:24,387][227910] Mean Reward across all agents: -229.08245521215008[0m
[37m[1m[2023-07-10 12:29:24,387][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:29:29,958][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:29:29,958][227910] Reward + Measures: [[-182.98434534    0.94799995    0.83090001    0.95769995    0.0052    ]
 [-335.36257065    0.92810005    0.65930003    0.93959999    0.0152    ]
 [-391.61403071    0.8804        0.77179998    0.89109993    0.0076    ]
 ...
 [-313.13002046    0.95570004    0.85540003    0.96210003    0.0083    ]
 [-377.78778411    0.85140002    0.80240005    0.8653        0.0098    ]
 [-297.17162956    0.94690001    0.81910002    0.95370001    0.0106    ]][0m
[37m[1m[2023-07-10 12:29:29,959][227910] Max Reward on eval: 9.324281645286828[0m
[37m[1m[2023-07-10 12:29:29,959][227910] Min Reward on eval: -847.5257352806627[0m
[37m[1m[2023-07-10 12:29:29,959][227910] Mean Reward across all agents: -344.21519299533713[0m
[37m[1m[2023-07-10 12:29:29,959][227910] Average Trajectory Length: 999.486[0m
[36m[2023-07-10 12:29:29,961][227910] mean_value=-448.5097986712117, max_value=122.83108371673501[0m
[37m[1m[2023-07-10 12:29:29,963][227910] New mean coefficients: [[-0.9899705  2.9280558  6.4181604  3.5262542 -0.4792031]][0m
[37m[1m[2023-07-10 12:29:29,964][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:29:39,834][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 12:29:39,834][227910] FPS: 389142.54[0m
[36m[2023-07-10 12:29:39,836][227910] itr=382, itrs=2000, Progress: 19.10%[0m
[36m[2023-07-10 12:29:51,311][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 12:29:51,311][227910] FPS: 335172.43[0m
[36m[2023-07-10 12:29:55,956][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:29:55,957][227910] Reward + Measures: [[-226.38816737    0.954934      0.93338364    0.96136796    0.00512233]][0m
[37m[1m[2023-07-10 12:29:55,957][227910] Max Reward on eval: -226.38816737139612[0m
[37m[1m[2023-07-10 12:29:55,957][227910] Min Reward on eval: -226.38816737139612[0m
[37m[1m[2023-07-10 12:29:55,957][227910] Mean Reward across all agents: -226.38816737139612[0m
[37m[1m[2023-07-10 12:29:55,957][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:30:01,387][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:30:01,393][227910] Reward + Measures: [[-262.85969025    0.93619996    0.67979997    0.92519999    0.0213    ]
 [-199.35071465    0.94820005    0.60640001    0.95380002    0.0286    ]
 [-285.85476336    0.94300002    0.41359997    0.95139998    0.1158    ]
 ...
 [-297.16127292    0.94910002    0.81140006    0.95389998    0.0109    ]
 [-301.26145081    0.95810002    0.91360009    0.96829998    0.0052    ]
 [-150.11178731    0.92550004    0.34209999    0.94849998    0.20780002]][0m
[37m[1m[2023-07-10 12:30:01,393][227910] Max Reward on eval: 23.78518319777213[0m
[37m[1m[2023-07-10 12:30:01,393][227910] Min Reward on eval: -495.53222945895976[0m
[37m[1m[2023-07-10 12:30:01,394][227910] Mean Reward across all agents: -207.2936299625914[0m
[37m[1m[2023-07-10 12:30:01,394][227910] Average Trajectory Length: 999.3766666666667[0m
[36m[2023-07-10 12:30:01,396][227910] mean_value=-285.8686490550814, max_value=257.32478916992676[0m
[37m[1m[2023-07-10 12:30:01,398][227910] New mean coefficients: [[-0.9476038  4.944758   7.099633   5.614871  -1.3665317]][0m
[37m[1m[2023-07-10 12:30:01,399][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:30:11,047][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 12:30:11,048][227910] FPS: 398055.85[0m
[36m[2023-07-10 12:30:11,050][227910] itr=383, itrs=2000, Progress: 19.15%[0m
[36m[2023-07-10 12:30:22,501][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 12:30:22,501][227910] FPS: 335861.24[0m
[36m[2023-07-10 12:30:27,300][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:30:27,301][227910] Reward + Measures: [[-235.27496148    0.95652497    0.93519092    0.96348697    0.00493633]][0m
[37m[1m[2023-07-10 12:30:27,301][227910] Max Reward on eval: -235.27496147928036[0m
[37m[1m[2023-07-10 12:30:27,301][227910] Min Reward on eval: -235.27496147928036[0m
[37m[1m[2023-07-10 12:30:27,302][227910] Mean Reward across all agents: -235.27496147928036[0m
[37m[1m[2023-07-10 12:30:27,302][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:30:32,824][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:30:32,824][227910] Reward + Measures: [[-479.73446974    0.95550007    0.90819997    0.96149999    0.0068    ]
 [-396.34360535    0.9558        0.94160002    0.96160001    0.0067    ]
 [-602.87515946    0.85970002    0.8901        0.87819999    0.0062    ]
 ...
 [-447.04541109    0.87659997    0.87190002    0.88630003    0.0085    ]
 [-299.5290253     0.9558        0.83789998    0.96070004    0.0113    ]
 [-233.53530652    0.96509999    0.88520002    0.97010005    0.0092    ]][0m
[37m[1m[2023-07-10 12:30:32,824][227910] Max Reward on eval: 80.6314274021308[0m
[37m[1m[2023-07-10 12:30:32,825][227910] Min Reward on eval: -780.8706776149338[0m
[37m[1m[2023-07-10 12:30:32,825][227910] Mean Reward across all agents: -415.22658286280887[0m
[37m[1m[2023-07-10 12:30:32,825][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:30:32,826][227910] mean_value=-512.0479815237467, max_value=184.84475963534788[0m
[37m[1m[2023-07-10 12:30:32,829][227910] New mean coefficients: [[-0.7096211   3.1543872   6.313877    5.620283    0.17277455]][0m
[37m[1m[2023-07-10 12:30:32,830][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:30:42,475][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 12:30:42,476][227910] FPS: 398171.84[0m
[36m[2023-07-10 12:30:42,478][227910] itr=384, itrs=2000, Progress: 19.20%[0m
[36m[2023-07-10 12:30:53,959][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 12:30:53,960][227910] FPS: 334931.08[0m
[36m[2023-07-10 12:30:58,745][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:30:58,745][227910] Reward + Measures: [[-207.95090785    0.95932341    0.93884635    0.96492261    0.00470467]][0m
[37m[1m[2023-07-10 12:30:58,745][227910] Max Reward on eval: -207.95090784649182[0m
[37m[1m[2023-07-10 12:30:58,746][227910] Min Reward on eval: -207.95090784649182[0m
[37m[1m[2023-07-10 12:30:58,746][227910] Mean Reward across all agents: -207.95090784649182[0m
[37m[1m[2023-07-10 12:30:58,746][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:31:04,264][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:31:04,265][227910] Reward + Measures: [[-545.01634049    0.81910002    0.37770003    0.8099001     0.0992    ]
 [-368.86067685    0.92449999    0.71999997    0.912         0.009     ]
 [-400.24221315    0.85130006    0.80769998    0.86140007    0.009     ]
 ...
 [-243.48392732    0.86980003    0.81690007    0.87909997    0.0053    ]
 [-422.37087509    0.85839999    0.6868        0.85089999    0.0099    ]
 [-362.99861356    0.95419997    0.6978001     0.96020001    0.0151    ]][0m
[37m[1m[2023-07-10 12:31:04,265][227910] Max Reward on eval: 8.38703240538016[0m
[37m[1m[2023-07-10 12:31:04,265][227910] Min Reward on eval: -706.0128628192936[0m
[37m[1m[2023-07-10 12:31:04,265][227910] Mean Reward across all agents: -363.0871831969199[0m
[37m[1m[2023-07-10 12:31:04,266][227910] Average Trajectory Length: 999.871[0m
[36m[2023-07-10 12:31:04,267][227910] mean_value=-426.6178313284892, max_value=166.04926078432584[0m
[37m[1m[2023-07-10 12:31:04,269][227910] New mean coefficients: [[-0.29568905  0.86903     5.7423334   4.575459    0.6673791 ]][0m
[37m[1m[2023-07-10 12:31:04,270][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:31:13,975][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 12:31:13,975][227910] FPS: 395766.85[0m
[36m[2023-07-10 12:31:13,977][227910] itr=385, itrs=2000, Progress: 19.25%[0m
[36m[2023-07-10 12:31:25,438][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 12:31:25,439][227910] FPS: 335589.37[0m
[36m[2023-07-10 12:31:30,205][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:31:30,205][227910] Reward + Measures: [[-258.18575052    0.95285934    0.93098956    0.9612236     0.005255  ]][0m
[37m[1m[2023-07-10 12:31:30,206][227910] Max Reward on eval: -258.18575051750446[0m
[37m[1m[2023-07-10 12:31:30,206][227910] Min Reward on eval: -258.18575051750446[0m
[37m[1m[2023-07-10 12:31:30,206][227910] Mean Reward across all agents: -258.18575051750446[0m
[37m[1m[2023-07-10 12:31:30,206][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:31:35,817][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:31:35,822][227910] Reward + Measures: [[-229.92736116    0.95849991    0.70970005    0.96210003    0.0981    ]
 [-159.87456674    0.95830005    0.81850004    0.96689999    0.0599    ]
 [-572.70399779    0.92309999    0.86850005    0.92910004    0.0162    ]
 ...
 [-255.16400884    0.8696        0.87659997    0.88599998    0.0094    ]
 [ -96.15601565    0.9259001     0.2447        0.95760006    0.51719999]
 [-145.58058322    0.92169994    0.676         0.92460006    0.61759996]][0m
[37m[1m[2023-07-10 12:31:35,823][227910] Max Reward on eval: 23.470980750978924[0m
[37m[1m[2023-07-10 12:31:35,823][227910] Min Reward on eval: -856.9752676747041[0m
[37m[1m[2023-07-10 12:31:35,823][227910] Mean Reward across all agents: -280.17372169237444[0m
[37m[1m[2023-07-10 12:31:35,824][227910] Average Trajectory Length: 999.7276666666667[0m
[36m[2023-07-10 12:31:35,827][227910] mean_value=-221.95344999986352, max_value=447.36376222325487[0m
[37m[1m[2023-07-10 12:31:35,830][227910] New mean coefficients: [[-0.5554834   4.891253    6.7573805   4.9013367  -0.39886755]][0m
[37m[1m[2023-07-10 12:31:35,831][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:31:45,701][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 12:31:45,702][227910] FPS: 389107.99[0m
[36m[2023-07-10 12:31:45,704][227910] itr=386, itrs=2000, Progress: 19.30%[0m
[36m[2023-07-10 12:31:57,412][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 12:31:57,412][227910] FPS: 328501.43[0m
[36m[2023-07-10 12:32:02,269][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:32:02,270][227910] Reward + Measures: [[-158.03490721    0.95907372    0.94042796    0.96661562    0.00486567]][0m
[37m[1m[2023-07-10 12:32:02,270][227910] Max Reward on eval: -158.0349072057964[0m
[37m[1m[2023-07-10 12:32:02,270][227910] Min Reward on eval: -158.0349072057964[0m
[37m[1m[2023-07-10 12:32:02,271][227910] Mean Reward across all agents: -158.0349072057964[0m
[37m[1m[2023-07-10 12:32:02,271][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:32:07,768][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:32:07,769][227910] Reward + Measures: [[-546.16165073    0.63679999    0.26640001    0.63820004    0.13090001]
 [-139.6794233     0.92180008    0.42550001    0.9018001     0.16489999]
 [ -66.2958708     0.92480004    0.4298        0.90410006    0.1419    ]
 ...
 [-243.66285186    0.8488        0.56639999    0.86610001    0.07669999]
 [-110.25007127    0.90780002    0.37350002    0.86630005    0.05250001]
 [-623.40599583    0.76789999    0.64560002    0.77840006    0.0205    ]][0m
[37m[1m[2023-07-10 12:32:07,769][227910] Max Reward on eval: 188.93487113766605[0m
[37m[1m[2023-07-10 12:32:07,769][227910] Min Reward on eval: -660.411675049318[0m
[37m[1m[2023-07-10 12:32:07,770][227910] Mean Reward across all agents: -159.3603807824603[0m
[37m[1m[2023-07-10 12:32:07,770][227910] Average Trajectory Length: 999.7043333333334[0m
[36m[2023-07-10 12:32:07,772][227910] mean_value=-214.48107820599844, max_value=266.5141148970171[0m
[37m[1m[2023-07-10 12:32:07,774][227910] New mean coefficients: [[-0.888889   6.859819   7.1541796  6.2772074 -0.4616585]][0m
[37m[1m[2023-07-10 12:32:07,775][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:32:17,464][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 12:32:17,465][227910] FPS: 396376.80[0m
[36m[2023-07-10 12:32:17,467][227910] itr=387, itrs=2000, Progress: 19.35%[0m
[36m[2023-07-10 12:32:28,900][227910] train() took 11.42 seconds to complete[0m
[36m[2023-07-10 12:32:28,901][227910] FPS: 336363.77[0m
[36m[2023-07-10 12:32:33,634][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:32:33,635][227910] Reward + Measures: [[-134.42275376    0.95649236    0.94049537    0.96374601    0.00399067]][0m
[37m[1m[2023-07-10 12:32:33,635][227910] Max Reward on eval: -134.42275376182937[0m
[37m[1m[2023-07-10 12:32:33,635][227910] Min Reward on eval: -134.42275376182937[0m
[37m[1m[2023-07-10 12:32:33,635][227910] Mean Reward across all agents: -134.42275376182937[0m
[37m[1m[2023-07-10 12:32:33,636][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:32:39,072][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:32:39,072][227910] Reward + Measures: [[-490.48550644    0.93110001    0.49899998    0.9497        0.0302    ]
 [-533.77022095    0.93350011    0.48840004    0.94420004    0.03860001]
 [-350.61646916    0.95719999    0.89630002    0.96579999    0.0051    ]
 ...
 [-509.48313818    0.93730003    0.48210001    0.95649999    0.0268    ]
 [-312.01578961    0.90469998    0.15360001    0.89320004    0.18959999]
 [-392.55122187    0.86809999    0.20180002    0.87099999    0.2096    ]][0m
[37m[1m[2023-07-10 12:32:39,072][227910] Max Reward on eval: -37.28801064358559[0m
[37m[1m[2023-07-10 12:32:39,073][227910] Min Reward on eval: -825.1749500214297[0m
[37m[1m[2023-07-10 12:32:39,073][227910] Mean Reward across all agents: -470.5242621316083[0m
[37m[1m[2023-07-10 12:32:39,073][227910] Average Trajectory Length: 999.9876666666667[0m
[36m[2023-07-10 12:32:39,075][227910] mean_value=-521.2949688134645, max_value=81.5076116473926[0m
[37m[1m[2023-07-10 12:32:39,077][227910] New mean coefficients: [[-0.7550341   5.3976445   9.626109    6.9797926  -0.91615963]][0m
[37m[1m[2023-07-10 12:32:39,078][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:32:48,674][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 12:32:48,674][227910] FPS: 400222.98[0m
[36m[2023-07-10 12:32:48,677][227910] itr=388, itrs=2000, Progress: 19.40%[0m
[36m[2023-07-10 12:33:00,120][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 12:33:00,120][227910] FPS: 336062.42[0m
[36m[2023-07-10 12:33:04,839][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:33:04,839][227910] Reward + Measures: [[-125.443411      0.95726627    0.94383836    0.96452004    0.00384667]][0m
[37m[1m[2023-07-10 12:33:04,839][227910] Max Reward on eval: -125.4434109975063[0m
[37m[1m[2023-07-10 12:33:04,840][227910] Min Reward on eval: -125.4434109975063[0m
[37m[1m[2023-07-10 12:33:04,840][227910] Mean Reward across all agents: -125.4434109975063[0m
[37m[1m[2023-07-10 12:33:04,840][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:33:10,210][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:33:10,211][227910] Reward + Measures: [[-511.17573344    0.92859995    0.4032        0.94729996    0.8398    ]
 [-491.99932337    0.80809993    0.1745        0.79350001    0.33329999]
 [-522.33812372    0.95120001    0.33589998    0.96400005    0.84110004]
 ...
 [-284.61301878    0.93349999    0.34770003    0.94080001    0.14209999]
 [-410.79262672    0.93330002    0.15720001    0.9332        0.24969999]
 [-421.0746212     0.86850005    0.2369        0.84799999    0.15939999]][0m
[37m[1m[2023-07-10 12:33:10,211][227910] Max Reward on eval: -19.20192326575052[0m
[37m[1m[2023-07-10 12:33:10,211][227910] Min Reward on eval: -1397.6611595056952[0m
[37m[1m[2023-07-10 12:33:10,212][227910] Mean Reward across all agents: -494.9102460802041[0m
[37m[1m[2023-07-10 12:33:10,212][227910] Average Trajectory Length: 999.0136666666666[0m
[36m[2023-07-10 12:33:10,214][227910] mean_value=-468.7416977262918, max_value=216.6299676446826[0m
[37m[1m[2023-07-10 12:33:10,216][227910] New mean coefficients: [[-0.9160532   4.370928    9.287662    6.34448    -0.47079155]][0m
[37m[1m[2023-07-10 12:33:10,217][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:33:19,851][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 12:33:19,851][227910] FPS: 398687.29[0m
[36m[2023-07-10 12:33:19,853][227910] itr=389, itrs=2000, Progress: 19.45%[0m
[36m[2023-07-10 12:33:31,362][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 12:33:31,362][227910] FPS: 334178.59[0m
[36m[2023-07-10 12:33:36,208][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:33:36,208][227910] Reward + Measures: [[-238.51412338    0.95481706    0.94301194    0.96222931    0.00380233]][0m
[37m[1m[2023-07-10 12:33:36,208][227910] Max Reward on eval: -238.51412337736696[0m
[37m[1m[2023-07-10 12:33:36,208][227910] Min Reward on eval: -238.51412337736696[0m
[37m[1m[2023-07-10 12:33:36,209][227910] Mean Reward across all agents: -238.51412337736696[0m
[37m[1m[2023-07-10 12:33:36,209][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:33:41,778][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:33:41,784][227910] Reward + Measures: [[-595.21619889    0.87509996    0.41879997    0.87540007    0.1104    ]
 [-391.91813267    0.93110001    0.67970002    0.93500006    0.0189    ]
 [-599.71882926    0.74200004    0.44090006    0.75650001    0.07610001]
 ...
 [-545.49927652    0.88239998    0.33629999    0.86079997    0.16849999]
 [-507.67906856    0.90830004    0.56150001    0.91940004    0.05      ]
 [-256.67877231    0.74589998    0.55260003    0.7626        0.0847    ]][0m
[37m[1m[2023-07-10 12:33:41,784][227910] Max Reward on eval: 69.60282039034647[0m
[37m[1m[2023-07-10 12:33:41,785][227910] Min Reward on eval: -789.4877527377103[0m
[37m[1m[2023-07-10 12:33:41,785][227910] Mean Reward across all agents: -493.92698280178774[0m
[37m[1m[2023-07-10 12:33:41,785][227910] Average Trajectory Length: 997.3196666666666[0m
[36m[2023-07-10 12:33:41,787][227910] mean_value=-493.3985269641909, max_value=159.07339098935557[0m
[37m[1m[2023-07-10 12:33:41,790][227910] New mean coefficients: [[-0.8540556  2.2592998  9.878921   6.825738   1.2619542]][0m
[37m[1m[2023-07-10 12:33:41,791][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:33:51,489][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 12:33:51,489][227910] FPS: 396005.23[0m
[36m[2023-07-10 12:33:51,492][227910] itr=390, itrs=2000, Progress: 19.50%[0m
[37m[1m[2023-07-10 12:33:54,127][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000370[0m
[36m[2023-07-10 12:34:06,040][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 12:34:06,040][227910] FPS: 329660.03[0m
[36m[2023-07-10 12:34:10,847][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:34:10,847][227910] Reward + Measures: [[-271.71821389    0.95214695    0.9425047     0.9595536     0.00405467]][0m
[37m[1m[2023-07-10 12:34:10,847][227910] Max Reward on eval: -271.71821389304125[0m
[37m[1m[2023-07-10 12:34:10,847][227910] Min Reward on eval: -271.71821389304125[0m
[37m[1m[2023-07-10 12:34:10,848][227910] Mean Reward across all agents: -271.71821389304125[0m
[37m[1m[2023-07-10 12:34:10,848][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:34:16,293][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:34:16,294][227910] Reward + Measures: [[-464.69104046    0.21123873    0.26211151    0.24307509    0.2733793 ]
 [-411.74620876    0.75130004    0.51730001    0.72640002    0.50000006]
 [-480.1151212     0.21210001    0.30749997    0.24280003    0.31810001]
 ...
 [-407.62158567    0.70139998    0.54560006    0.67650002    0.45270005]
 [-679.01397936    0.14760001    0.2009        0.1373        0.16200002]
 [-581.68561263    0.24290001    0.34730002    0.26150003    0.31170002]][0m
[37m[1m[2023-07-10 12:34:16,294][227910] Max Reward on eval: -127.09407668174245[0m
[37m[1m[2023-07-10 12:34:16,295][227910] Min Reward on eval: -806.6388222341717[0m
[37m[1m[2023-07-10 12:34:16,295][227910] Mean Reward across all agents: -504.8712792511548[0m
[37m[1m[2023-07-10 12:34:16,295][227910] Average Trajectory Length: 982.1836666666667[0m
[36m[2023-07-10 12:34:16,297][227910] mean_value=-1142.5965713241217, max_value=178.2020383857074[0m
[37m[1m[2023-07-10 12:34:16,299][227910] New mean coefficients: [[-0.5759353  1.1056247  6.4579935  5.945084   2.6058326]][0m
[37m[1m[2023-07-10 12:34:16,300][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:34:26,096][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 12:34:26,096][227910] FPS: 392072.09[0m
[36m[2023-07-10 12:34:26,099][227910] itr=391, itrs=2000, Progress: 19.55%[0m
[36m[2023-07-10 12:34:37,706][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 12:34:37,706][227910] FPS: 331275.91[0m
[36m[2023-07-10 12:34:42,607][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:34:42,608][227910] Reward + Measures: [[-264.95234005    0.95502961    0.94184226    0.96250957    0.00420933]][0m
[37m[1m[2023-07-10 12:34:42,608][227910] Max Reward on eval: -264.9523400490813[0m
[37m[1m[2023-07-10 12:34:42,608][227910] Min Reward on eval: -264.9523400490813[0m
[37m[1m[2023-07-10 12:34:42,608][227910] Mean Reward across all agents: -264.9523400490813[0m
[37m[1m[2023-07-10 12:34:42,609][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:34:47,928][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:34:47,929][227910] Reward + Measures: [[-406.84704683    0.87540001    0.81630003    0.88440007    0.0086    ]
 [ -71.43309619    0.95830005    0.3836        0.96079999    0.1945    ]
 [ -40.39143199    0.8627001     0.0447        0.84530014    0.62550002]
 ...
 [-380.85602648    0.84040004    0.85159999    0.84880012    0.0166    ]
 [-351.51823352    0.92749995    0.87410003    0.92589998    0.0208    ]
 [-348.3833061     0.81459999    0.68010002    0.81569999    0.0387    ]][0m
[37m[1m[2023-07-10 12:34:47,929][227910] Max Reward on eval: 172.24475876364158[0m
[37m[1m[2023-07-10 12:34:47,929][227910] Min Reward on eval: -510.3775181078119[0m
[37m[1m[2023-07-10 12:34:47,930][227910] Mean Reward across all agents: -241.9466543523781[0m
[37m[1m[2023-07-10 12:34:47,930][227910] Average Trajectory Length: 999.9736666666666[0m
[36m[2023-07-10 12:34:47,933][227910] mean_value=-257.85861559374047, max_value=595.1327621492755[0m
[37m[1m[2023-07-10 12:34:47,936][227910] New mean coefficients: [[-1.1214225  4.748584   9.4082575  7.2929554  1.4491333]][0m
[37m[1m[2023-07-10 12:34:47,937][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:34:57,740][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 12:34:57,740][227910] FPS: 391785.29[0m
[36m[2023-07-10 12:34:57,742][227910] itr=392, itrs=2000, Progress: 19.60%[0m
[36m[2023-07-10 12:35:09,275][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 12:35:09,275][227910] FPS: 333416.44[0m
[36m[2023-07-10 12:35:14,109][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:35:14,109][227910] Reward + Measures: [[-273.99107776    0.95559466    0.94057029    0.96438134    0.00389433]][0m
[37m[1m[2023-07-10 12:35:14,109][227910] Max Reward on eval: -273.99107775746[0m
[37m[1m[2023-07-10 12:35:14,110][227910] Min Reward on eval: -273.99107775746[0m
[37m[1m[2023-07-10 12:35:14,110][227910] Mean Reward across all agents: -273.99107775746[0m
[37m[1m[2023-07-10 12:35:14,110][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:35:19,804][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:35:19,804][227910] Reward + Measures: [[-242.30593646    0.93920004    0.88360006    0.95130008    0.0137    ]
 [-565.05712358    0.90480006    0.63200003    0.89169997    0.0271    ]
 [-555.00601026    0.87880003    0.65270001    0.85649997    0.0393    ]
 ...
 [-557.71031489    0.92619991    0.72339994    0.89779997    0.0185    ]
 [-509.11733188    0.82019997    0.65199995    0.80489999    0.0243    ]
 [-260.22896499    0.93279994    0.76540005    0.93110001    0.0228    ]][0m
[37m[1m[2023-07-10 12:35:19,804][227910] Max Reward on eval: 79.16832283184631[0m
[37m[1m[2023-07-10 12:35:19,805][227910] Min Reward on eval: -717.1993140092236[0m
[37m[1m[2023-07-10 12:35:19,805][227910] Mean Reward across all agents: -347.42097137248095[0m
[37m[1m[2023-07-10 12:35:19,805][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:35:19,806][227910] mean_value=-487.40162477414947, max_value=65.23129185205113[0m
[37m[1m[2023-07-10 12:35:19,809][227910] New mean coefficients: [[-0.84030634  4.2595673   7.2665224   5.2826557   2.4517374 ]][0m
[37m[1m[2023-07-10 12:35:19,810][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:35:29,637][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 12:35:29,637][227910] FPS: 390825.49[0m
[36m[2023-07-10 12:35:29,639][227910] itr=393, itrs=2000, Progress: 19.65%[0m
[36m[2023-07-10 12:35:41,216][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 12:35:41,216][227910] FPS: 332165.26[0m
[36m[2023-07-10 12:35:45,899][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:35:45,900][227910] Reward + Measures: [[-288.24602258    0.96455222    0.9471463     0.97040135    0.00359733]][0m
[37m[1m[2023-07-10 12:35:45,900][227910] Max Reward on eval: -288.2460225818494[0m
[37m[1m[2023-07-10 12:35:45,900][227910] Min Reward on eval: -288.2460225818494[0m
[37m[1m[2023-07-10 12:35:45,900][227910] Mean Reward across all agents: -288.2460225818494[0m
[37m[1m[2023-07-10 12:35:45,901][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:35:51,320][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:35:51,320][227910] Reward + Measures: [[-272.74090406    0.96589994    0.9429        0.96819991    0.0042    ]
 [-327.41800038    0.85460007    0.80269998    0.84750003    0.0162    ]
 [-384.4645347     0.87130004    0.85280001    0.88380003    0.0054    ]
 ...
 [-561.76858235    0.74779999    0.55290002    0.74940008    0.0262    ]
 [-258.96439455    0.96000004    0.94020003    0.97189999    0.0036    ]
 [-378.96301325    0.95650005    0.88020003    0.95730001    0.0051    ]][0m
[37m[1m[2023-07-10 12:35:51,321][227910] Max Reward on eval: -162.72204779202585[0m
[37m[1m[2023-07-10 12:35:51,321][227910] Min Reward on eval: -663.3109784342407[0m
[37m[1m[2023-07-10 12:35:51,321][227910] Mean Reward across all agents: -368.3201596298386[0m
[37m[1m[2023-07-10 12:35:51,322][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:35:51,322][227910] mean_value=-493.0642132541019, max_value=-8.951693292545713[0m
[36m[2023-07-10 12:35:51,325][227910] XNES is restarting with a new solution whose measures are [0.92229998 0.63000005 0.89670002 0.51450008] and objective is -193.00305217660497[0m
[36m[2023-07-10 12:35:51,326][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 12:35:51,328][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 12:35:51,329][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:36:01,017][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 12:36:01,017][227910] FPS: 396437.13[0m
[36m[2023-07-10 12:36:01,019][227910] itr=394, itrs=2000, Progress: 19.70%[0m
[36m[2023-07-10 12:36:12,564][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 12:36:12,565][227910] FPS: 333134.90[0m
[36m[2023-07-10 12:36:17,278][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:36:17,278][227910] Reward + Measures: [[-679.82717214    0.50494248    0.21320288    0.55075717    0.31574118]][0m
[37m[1m[2023-07-10 12:36:17,278][227910] Max Reward on eval: -679.8271721440793[0m
[37m[1m[2023-07-10 12:36:17,279][227910] Min Reward on eval: -679.8271721440793[0m
[37m[1m[2023-07-10 12:36:17,279][227910] Mean Reward across all agents: -679.8271721440793[0m
[37m[1m[2023-07-10 12:36:17,279][227910] Average Trajectory Length: 968.015[0m
[36m[2023-07-10 12:36:22,745][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:36:22,745][227910] Reward + Measures: [[ -743.44093105     0.21515985     0.19726925     0.26964101
      0.12505125]
 [-1661.56105987     0.41395894     0.39549083     0.39825049
      0.22304146]
 [ -912.56990877     0.30784485     0.32411909     0.3860116
      0.11260026]
 ...
 [ -974.31813379     0.42989999     0.62120003     0.63639998
      0.0003    ]
 [-1821.28771128     0.48870322     0.52805406     0.2513282
      0.44855386]
 [-1297.06597772     0.23860557     0.27986869     0.30223462
      0.21088795]][0m
[37m[1m[2023-07-10 12:36:22,745][227910] Max Reward on eval: -200.1568211173406[0m
[37m[1m[2023-07-10 12:36:22,746][227910] Min Reward on eval: -2083.406212383369[0m
[37m[1m[2023-07-10 12:36:22,746][227910] Mean Reward across all agents: -1071.0503872133156[0m
[37m[1m[2023-07-10 12:36:22,746][227910] Average Trajectory Length: 826.1693333333333[0m
[36m[2023-07-10 12:36:22,747][227910] mean_value=-2344.5878871720556, max_value=-190.9053587106988[0m
[36m[2023-07-10 12:36:22,750][227910] XNES is restarting with a new solution whose measures are [0.67919999 0.3973     0.65359998 0.19220001] and objective is -323.2447395766911[0m
[36m[2023-07-10 12:36:22,751][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 12:36:22,754][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 12:36:22,754][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:36:32,392][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 12:36:32,392][227910] FPS: 398504.56[0m
[36m[2023-07-10 12:36:32,395][227910] itr=395, itrs=2000, Progress: 19.75%[0m
[36m[2023-07-10 12:36:43,946][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 12:36:43,946][227910] FPS: 332906.19[0m
[36m[2023-07-10 12:36:48,723][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:36:48,724][227910] Reward + Measures: [[-672.43544043    0.55701917    0.40979594    0.4747858     0.46557251]][0m
[37m[1m[2023-07-10 12:36:48,724][227910] Max Reward on eval: -672.4354404295104[0m
[37m[1m[2023-07-10 12:36:48,724][227910] Min Reward on eval: -672.4354404295104[0m
[37m[1m[2023-07-10 12:36:48,724][227910] Mean Reward across all agents: -672.4354404295104[0m
[37m[1m[2023-07-10 12:36:48,724][227910] Average Trajectory Length: 856.529[0m
[36m[2023-07-10 12:36:54,173][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:36:54,179][227910] Reward + Measures: [[ -959.58441145     0.28867194     0.38457438     0.32839498
      0.2543495 ]
 [ -801.24323052     0.20752041     0.27883384     0.19684245
      0.20950441]
 [-1009.1935429      0.47511539     0.28743076     0.47066924
      0.41068459]
 ...
 [ -965.70582572     0.49524975     0.26155627     0.54997438
      0.3126168 ]
 [ -747.71611808     0.1892219      0.30678302     0.23269515
      0.25997278]
 [-1703.63004434     0.43754917     0.46664411     0.45072895
      0.40815559]][0m
[37m[1m[2023-07-10 12:36:54,179][227910] Max Reward on eval: -375.2518347410951[0m
[37m[1m[2023-07-10 12:36:54,179][227910] Min Reward on eval: -2068.8664617478616[0m
[37m[1m[2023-07-10 12:36:54,180][227910] Mean Reward across all agents: -1026.5039798907821[0m
[37m[1m[2023-07-10 12:36:54,180][227910] Average Trajectory Length: 820.8923333333333[0m
[36m[2023-07-10 12:36:54,181][227910] mean_value=-1949.5472557779988, max_value=-278.98635612910357[0m
[36m[2023-07-10 12:36:54,183][227910] XNES is restarting with a new solution whose measures are [0.82089996 0.41120002 0.82269996 0.2103    ] and objective is -430.5416077760048[0m
[36m[2023-07-10 12:36:54,185][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 12:36:54,187][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 12:36:54,188][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:37:03,888][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 12:37:03,888][227910] FPS: 395921.18[0m
[36m[2023-07-10 12:37:03,891][227910] itr=396, itrs=2000, Progress: 19.80%[0m
[36m[2023-07-10 12:37:15,669][227910] train() took 11.76 seconds to complete[0m
[36m[2023-07-10 12:37:15,669][227910] FPS: 326531.27[0m
[36m[2023-07-10 12:37:20,380][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:37:20,386][227910] Reward + Measures: [[-668.85712862    0.27603856    0.53521353    0.3390162     0.51044708]][0m
[37m[1m[2023-07-10 12:37:20,386][227910] Max Reward on eval: -668.8571286174558[0m
[37m[1m[2023-07-10 12:37:20,386][227910] Min Reward on eval: -668.8571286174558[0m
[37m[1m[2023-07-10 12:37:20,387][227910] Mean Reward across all agents: -668.8571286174558[0m
[37m[1m[2023-07-10 12:37:20,387][227910] Average Trajectory Length: 962.28[0m
[36m[2023-07-10 12:37:25,670][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:37:25,676][227910] Reward + Measures: [[ -352.73398066     0.6024         0.67900002     0.59750003
      0.6451    ]
 [ -722.1530277      0.18452777     0.34217939     0.24489997
      0.23961817]
 [-1382.00678484     0.17288525     0.37038627     0.38507149
      0.28235024]
 ...
 [-1546.47633286     0.13731717     0.60292667     0.43094212
      0.54376858]
 [ -321.27622626     0.25976938     0.29779783     0.23965657
      0.14558712]
 [-1432.41424265     0.31207761     0.27676854     0.31677273
      0.21733008]][0m
[37m[1m[2023-07-10 12:37:25,676][227910] Max Reward on eval: -38.759149182529654[0m
[37m[1m[2023-07-10 12:37:25,677][227910] Min Reward on eval: -2378.786314531998[0m
[37m[1m[2023-07-10 12:37:25,678][227910] Mean Reward across all agents: -1098.6018365964778[0m
[37m[1m[2023-07-10 12:37:25,678][227910] Average Trajectory Length: 781.1766666666666[0m
[36m[2023-07-10 12:37:25,681][227910] mean_value=-1926.9549875996122, max_value=-212.60448945349117[0m
[36m[2023-07-10 12:37:25,686][227910] XNES is restarting with a new solution whose measures are [0.86969995 0.40060002 0.88990003 0.67369998] and objective is -96.36438100094674[0m
[36m[2023-07-10 12:37:25,688][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 12:37:25,692][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 12:37:25,693][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:37:35,295][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 12:37:35,295][227910] FPS: 399979.22[0m
[36m[2023-07-10 12:37:35,298][227910] itr=397, itrs=2000, Progress: 19.85%[0m
[36m[2023-07-10 12:37:46,736][227910] train() took 11.42 seconds to complete[0m
[36m[2023-07-10 12:37:46,737][227910] FPS: 336153.87[0m
[36m[2023-07-10 12:37:51,493][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:37:51,494][227910] Reward + Measures: [[-701.32172681    0.8051756     0.57648593    0.75838906    0.13266842]][0m
[37m[1m[2023-07-10 12:37:51,494][227910] Max Reward on eval: -701.3217268116347[0m
[37m[1m[2023-07-10 12:37:51,494][227910] Min Reward on eval: -701.3217268116347[0m
[37m[1m[2023-07-10 12:37:51,494][227910] Mean Reward across all agents: -701.3217268116347[0m
[37m[1m[2023-07-10 12:37:51,495][227910] Average Trajectory Length: 999.9936666666666[0m
[36m[2023-07-10 12:37:57,168][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:37:57,169][227910] Reward + Measures: [[-1565.62181563     0.89890003     0.68510002     0.83560002
      0.076     ]
 [ -803.53511384     0.23902853     0.17249893     0.18483305
      0.22653806]
 [ -456.907784       0.43712354     0.21866472     0.45989999
      0.18167059]
 ...
 [ -971.57206664     0.42841345     0.26649794     0.4269467
      0.20290433]
 [ -817.19794667     0.28140554     0.20747618     0.21640866
      0.22466008]
 [-1063.50129474     0.48054892     0.23337589     0.46916237
      0.32316598]][0m
[37m[1m[2023-07-10 12:37:57,169][227910] Max Reward on eval: -52.3449504461023[0m
[37m[1m[2023-07-10 12:37:57,169][227910] Min Reward on eval: -2048.951685648039[0m
[37m[1m[2023-07-10 12:37:57,169][227910] Mean Reward across all agents: -1056.5689348211279[0m
[37m[1m[2023-07-10 12:37:57,170][227910] Average Trajectory Length: 896.3183333333333[0m
[36m[2023-07-10 12:37:57,171][227910] mean_value=-1780.5123399972038, max_value=278.85286745500053[0m
[37m[1m[2023-07-10 12:37:57,174][227910] New mean coefficients: [[ 0.8389523  -0.64590466 -0.55753773 -1.6134269  -1.5446814 ]][0m
[37m[1m[2023-07-10 12:37:57,174][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:38:07,096][227910] train() took 9.92 seconds to complete[0m
[36m[2023-07-10 12:38:07,096][227910] FPS: 387121.21[0m
[36m[2023-07-10 12:38:07,098][227910] itr=398, itrs=2000, Progress: 19.90%[0m
[36m[2023-07-10 12:38:18,752][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 12:38:18,753][227910] FPS: 330014.12[0m
[36m[2023-07-10 12:38:23,627][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:38:23,628][227910] Reward + Measures: [[-1379.13346269     0.79036003     0.77756703     0.72552294
      0.44822264]][0m
[37m[1m[2023-07-10 12:38:23,628][227910] Max Reward on eval: -1379.1334626852006[0m
[37m[1m[2023-07-10 12:38:23,628][227910] Min Reward on eval: -1379.1334626852006[0m
[37m[1m[2023-07-10 12:38:23,628][227910] Mean Reward across all agents: -1379.1334626852006[0m
[37m[1m[2023-07-10 12:38:23,629][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 12:38:29,099][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:38:29,100][227910] Reward + Measures: [[ -882.55971805     0.36622429     0.28985339     0.25377208
      0.34917346]
 [-1556.96974445     0.52950001     0.46750003     0.4941
      0.26980001]
 [-1041.76994379     0.26896602     0.29245645     0.2451252
      0.24893072]
 ...
 [-1286.89487257     0.18544756     0.24144153     0.1844127
      0.23024388]
 [-1257.60335815     0.32469997     0.43759999     0.31650001
      0.39770001]
 [ -891.33541991     0.21313238     0.23657253     0.17577405
      0.21545587]][0m
[37m[1m[2023-07-10 12:38:29,100][227910] Max Reward on eval: -314.4280765529722[0m
[37m[1m[2023-07-10 12:38:29,100][227910] Min Reward on eval: -2819.435751477722[0m
[37m[1m[2023-07-10 12:38:29,100][227910] Mean Reward across all agents: -1154.3414848324683[0m
[37m[1m[2023-07-10 12:38:29,100][227910] Average Trajectory Length: 878.5426666666666[0m
[36m[2023-07-10 12:38:29,102][227910] mean_value=-1869.2608210700362, max_value=-309.2762368063501[0m
[36m[2023-07-10 12:38:29,104][227910] XNES is restarting with a new solution whose measures are [0.17470001 0.65669996 0.26499999 0.44509998] and objective is 554.9756353565026[0m
[36m[2023-07-10 12:38:29,105][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 12:38:29,107][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 12:38:29,108][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:38:38,766][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 12:38:38,767][227910] FPS: 397660.59[0m
[36m[2023-07-10 12:38:38,769][227910] itr=399, itrs=2000, Progress: 19.95%[0m
[36m[2023-07-10 12:38:50,262][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 12:38:50,262][227910] FPS: 334553.80[0m
[36m[2023-07-10 12:38:55,119][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:38:55,120][227910] Reward + Measures: [[603.24613169   0.19548766   0.66291779   0.23348103   0.40902117]][0m
[37m[1m[2023-07-10 12:38:55,120][227910] Max Reward on eval: 603.2461316903702[0m
[37m[1m[2023-07-10 12:38:55,120][227910] Min Reward on eval: 603.2461316903702[0m
[37m[1m[2023-07-10 12:38:55,120][227910] Mean Reward across all agents: 603.2461316903702[0m
[37m[1m[2023-07-10 12:38:55,120][227910] Average Trajectory Length: 999.9633333333333[0m
[36m[2023-07-10 12:39:00,533][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:39:00,534][227910] Reward + Measures: [[   14.59673334     0.19359064     0.47545481     0.2772561
      0.35598755]
 [ -477.38782055     0.21259999     0.30559999     0.17740001
      0.24590002]
 [ -935.16958295     0.36536774     0.39587981     0.35114974
      0.33147556]
 ...
 [ -388.75281536     0.2631         0.44949999     0.22259998
      0.39400002]
 [ -867.2738494      0.26151323     0.3342447      0.20344876
      0.25441381]
 [-1035.54082107     0.40161982     0.53751487     0.38010082
      0.46624628]][0m
[37m[1m[2023-07-10 12:39:00,534][227910] Max Reward on eval: 919.7735151477566[0m
[37m[1m[2023-07-10 12:39:00,534][227910] Min Reward on eval: -1797.9978546531172[0m
[37m[1m[2023-07-10 12:39:00,534][227910] Mean Reward across all agents: -590.6494309209543[0m
[37m[1m[2023-07-10 12:39:00,535][227910] Average Trajectory Length: 922.8953333333333[0m
[36m[2023-07-10 12:39:00,536][227910] mean_value=-1351.932295859415, max_value=778.4040067704638[0m
[37m[1m[2023-07-10 12:39:00,539][227910] New mean coefficients: [[-0.03885669 -0.793501   -1.0848609  -0.3180343  -1.4106805 ]][0m
[37m[1m[2023-07-10 12:39:00,540][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:39:10,321][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 12:39:10,321][227910] FPS: 392651.25[0m
[36m[2023-07-10 12:39:10,324][227910] itr=400, itrs=2000, Progress: 20.00%[0m
[37m[1m[2023-07-10 12:39:12,962][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000380[0m
[36m[2023-07-10 12:39:24,759][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 12:39:24,759][227910] FPS: 333122.57[0m
[36m[2023-07-10 12:39:29,570][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:39:29,571][227910] Reward + Measures: [[663.83433803   0.2522223    0.52231371   0.23369411   0.31397372]][0m
[37m[1m[2023-07-10 12:39:29,571][227910] Max Reward on eval: 663.8343380348178[0m
[37m[1m[2023-07-10 12:39:29,571][227910] Min Reward on eval: 663.8343380348178[0m
[37m[1m[2023-07-10 12:39:29,571][227910] Mean Reward across all agents: 663.8343380348178[0m
[37m[1m[2023-07-10 12:39:29,572][227910] Average Trajectory Length: 996.002[0m
[36m[2023-07-10 12:39:35,197][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:39:35,198][227910] Reward + Measures: [[ -898.73703177     0.19399685     0.33576882     0.33453408
      0.30494151]
 [ -244.69368116     0.24567373     0.37786791     0.21174435
      0.2953082 ]
 [-1189.27898671     0.08204313     0.37789413     0.31991568
      0.32324901]
 ...
 [-1098.9524758      0.1682246      0.22574596     0.1968246
      0.19138609]
 [ -873.5920127      0.19784652     0.26414886     0.23283255
      0.19130698]
 [ -578.40234036     0.1142         0.50920004     0.30749997
      0.433     ]][0m
[37m[1m[2023-07-10 12:39:35,198][227910] Max Reward on eval: 766.8308841225574[0m
[37m[1m[2023-07-10 12:39:35,199][227910] Min Reward on eval: -1641.1655612206437[0m
[37m[1m[2023-07-10 12:39:35,199][227910] Mean Reward across all agents: -423.42291739424223[0m
[37m[1m[2023-07-10 12:39:35,199][227910] Average Trajectory Length: 908.4026666666666[0m
[36m[2023-07-10 12:39:35,200][227910] mean_value=-1574.1785133674575, max_value=88.59455730563604[0m
[37m[1m[2023-07-10 12:39:35,203][227910] New mean coefficients: [[ 0.22858703 -0.28259456 -0.7995918  -0.72382534 -1.9717796 ]][0m
[37m[1m[2023-07-10 12:39:35,204][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:39:44,835][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 12:39:44,835][227910] FPS: 398778.05[0m
[36m[2023-07-10 12:39:44,837][227910] itr=401, itrs=2000, Progress: 20.05%[0m
[36m[2023-07-10 12:39:56,612][227910] train() took 11.76 seconds to complete[0m
[36m[2023-07-10 12:39:56,612][227910] FPS: 326555.56[0m
[36m[2023-07-10 12:40:01,318][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:40:01,319][227910] Reward + Measures: [[678.4386277    0.25112256   0.51373231   0.23496196   0.30263183]][0m
[37m[1m[2023-07-10 12:40:01,319][227910] Max Reward on eval: 678.4386276962857[0m
[37m[1m[2023-07-10 12:40:01,319][227910] Min Reward on eval: 678.4386276962857[0m
[37m[1m[2023-07-10 12:40:01,319][227910] Mean Reward across all agents: 678.4386276962857[0m
[37m[1m[2023-07-10 12:40:01,319][227910] Average Trajectory Length: 995.8943333333333[0m
[36m[2023-07-10 12:40:06,749][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:40:06,756][227910] Reward + Measures: [[ -43.54691178    0.42549559    0.50697261    0.42067608    0.2667062 ]
 [-587.2690104     0.2831063     0.33655247    0.33287892    0.18725298]
 [-133.22201645    0.4059        0.56460005    0.2458        0.45830002]
 ...
 [ 469.75371368    0.28581887    0.55352557    0.2723237     0.34033522]
 [ 100.39950336    0.35448328    0.45178032    0.26191053    0.35768804]
 [-352.94449738    0.26101217    0.41442403    0.2462009     0.28883204]][0m
[37m[1m[2023-07-10 12:40:06,757][227910] Max Reward on eval: 837.0068890024384[0m
[37m[1m[2023-07-10 12:40:06,757][227910] Min Reward on eval: -1725.9826599775813[0m
[37m[1m[2023-07-10 12:40:06,758][227910] Mean Reward across all agents: -379.3164902771208[0m
[37m[1m[2023-07-10 12:40:06,758][227910] Average Trajectory Length: 914.601[0m
[36m[2023-07-10 12:40:06,762][227910] mean_value=-1641.4616848618612, max_value=294.01361573383326[0m
[37m[1m[2023-07-10 12:40:06,767][227910] New mean coefficients: [[ 0.55143917  0.46221942 -0.23096877 -1.3300998  -2.2027216 ]][0m
[37m[1m[2023-07-10 12:40:06,768][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:40:16,532][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 12:40:16,532][227910] FPS: 393395.55[0m
[36m[2023-07-10 12:40:16,534][227910] itr=402, itrs=2000, Progress: 20.10%[0m
[36m[2023-07-10 12:40:28,274][227910] train() took 11.72 seconds to complete[0m
[36m[2023-07-10 12:40:28,274][227910] FPS: 327545.51[0m
[36m[2023-07-10 12:40:33,114][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:40:33,119][227910] Reward + Measures: [[501.37543999   0.26583025   0.51260281   0.22726163   0.27995643]][0m
[37m[1m[2023-07-10 12:40:33,120][227910] Max Reward on eval: 501.3754399894538[0m
[37m[1m[2023-07-10 12:40:33,120][227910] Min Reward on eval: 501.3754399894538[0m
[37m[1m[2023-07-10 12:40:33,120][227910] Mean Reward across all agents: 501.3754399894538[0m
[37m[1m[2023-07-10 12:40:33,120][227910] Average Trajectory Length: 995.0426666666666[0m
[36m[2023-07-10 12:40:38,639][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:40:38,640][227910] Reward + Measures: [[-1083.9817796      0.30281886     0.39906871     0.31413773
      0.33172208]
 [ -460.92698492     0.24102373     0.37623259     0.19815424
      0.26037928]
 [ -539.87919377     0.22812033     0.45293903     0.22365348
      0.32616207]
 ...
 [-1020.83233184     0.23984389     0.30209181     0.21720906
      0.21199422]
 [ -214.13478169     0.21476495     0.42179605     0.21684368
      0.22476196]
 [ -372.73768913     0.23811658     0.40597054     0.20367992
      0.24181953]][0m
[37m[1m[2023-07-10 12:40:38,640][227910] Max Reward on eval: 616.4097148640197[0m
[37m[1m[2023-07-10 12:40:38,640][227910] Min Reward on eval: -1989.7549328373163[0m
[37m[1m[2023-07-10 12:40:38,640][227910] Mean Reward across all agents: -593.767732607043[0m
[37m[1m[2023-07-10 12:40:38,641][227910] Average Trajectory Length: 888.317[0m
[36m[2023-07-10 12:40:38,642][227910] mean_value=-2126.4127462276947, max_value=-59.35324093091765[0m
[36m[2023-07-10 12:40:38,644][227910] XNES is restarting with a new solution whose measures are [0.40186247 0.50612497 0.57447505 0.0346875 ] and objective is 8.909131204988807[0m
[36m[2023-07-10 12:40:38,645][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 12:40:38,648][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 12:40:38,648][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:40:48,300][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 12:40:48,300][227910] FPS: 397922.68[0m
[36m[2023-07-10 12:40:48,302][227910] itr=403, itrs=2000, Progress: 20.15%[0m
[36m[2023-07-10 12:40:59,852][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 12:40:59,852][227910] FPS: 332937.91[0m
[36m[2023-07-10 12:41:04,689][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:41:04,689][227910] Reward + Measures: [[-449.08191509    0.23653279    0.24486509    0.3856332     0.16498354]][0m
[37m[1m[2023-07-10 12:41:04,690][227910] Max Reward on eval: -449.0819150926231[0m
[37m[1m[2023-07-10 12:41:04,690][227910] Min Reward on eval: -449.0819150926231[0m
[37m[1m[2023-07-10 12:41:04,690][227910] Mean Reward across all agents: -449.0819150926231[0m
[37m[1m[2023-07-10 12:41:04,690][227910] Average Trajectory Length: 934.2313333333333[0m
[36m[2023-07-10 12:41:10,159][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:41:10,159][227910] Reward + Measures: [[ -616.92609516     0.2739844      0.64384955     0.1472991
      0.58095968]
 [-1165.57564809     0.21383606     0.28603533     0.20974059
      0.30382198]
 [ -812.52053475     0.33309999     0.1963         0.3348
      0.22710001]
 ...
 [ -478.56854933     0.22412398     0.29958922     0.30040622
      0.24883643]
 [-1418.63607675     0.19510596     0.25412944     0.17382692
      0.23412345]
 [-1152.95260071     0.15120533     0.2200495      0.21581487
      0.18363316]][0m
[37m[1m[2023-07-10 12:41:10,159][227910] Max Reward on eval: -205.2436644159665[0m
[37m[1m[2023-07-10 12:41:10,160][227910] Min Reward on eval: -2006.546216595918[0m
[37m[1m[2023-07-10 12:41:10,160][227910] Mean Reward across all agents: -894.3336361839372[0m
[37m[1m[2023-07-10 12:41:10,160][227910] Average Trajectory Length: 808.4413333333333[0m
[36m[2023-07-10 12:41:10,162][227910] mean_value=-2237.1797399434067, max_value=112.71868646825317[0m
[37m[1m[2023-07-10 12:41:10,164][227910] New mean coefficients: [[ 0.9304664  -1.0374348   0.00758046 -1.8513197  -0.18517113]][0m
[37m[1m[2023-07-10 12:41:10,165][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:41:19,971][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 12:41:19,971][227910] FPS: 391678.97[0m
[36m[2023-07-10 12:41:19,973][227910] itr=404, itrs=2000, Progress: 20.20%[0m
[36m[2023-07-10 12:41:31,481][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 12:41:31,481][227910] FPS: 334155.12[0m
[36m[2023-07-10 12:41:36,246][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:41:36,246][227910] Reward + Measures: [[-566.05080263    0.16685793    0.21713118    0.31731686    0.20389692]][0m
[37m[1m[2023-07-10 12:41:36,246][227910] Max Reward on eval: -566.0508026307406[0m
[37m[1m[2023-07-10 12:41:36,247][227910] Min Reward on eval: -566.0508026307406[0m
[37m[1m[2023-07-10 12:41:36,247][227910] Mean Reward across all agents: -566.0508026307406[0m
[37m[1m[2023-07-10 12:41:36,247][227910] Average Trajectory Length: 955.252[0m
[36m[2023-07-10 12:41:41,663][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:41:41,664][227910] Reward + Measures: [[ -349.73155308     0.21604636     0.33058456     0.30704123
      0.16787685]
 [-1532.77865166     0.18280001     0.1831         0.1822
      0.15400001]
 [-1338.03165538     0.20328391     0.20062391     0.24441636
      0.10952332]
 ...
 [-1512.31625        0.29461429     0.1880143      0.32145715
      0.14504285]
 [-2300.27641563     0.13250001     0.6645         0.30050001
      0.63819999]
 [-1058.54434448     0.27382585     0.19340746     0.2756722
      0.12381821]][0m
[37m[1m[2023-07-10 12:41:41,664][227910] Max Reward on eval: -140.87389517955017[0m
[37m[1m[2023-07-10 12:41:41,664][227910] Min Reward on eval: -2300.2764156275894[0m
[37m[1m[2023-07-10 12:41:41,665][227910] Mean Reward across all agents: -917.6407660606723[0m
[37m[1m[2023-07-10 12:41:41,665][227910] Average Trajectory Length: 795.0306666666667[0m
[36m[2023-07-10 12:41:41,666][227910] mean_value=-2435.124316238178, max_value=-84.95492866906261[0m
[36m[2023-07-10 12:41:41,668][227910] XNES is restarting with a new solution whose measures are [0.77700007 0.77680004 0.73540002 0.77080005] and objective is -47.229399224196094[0m
[36m[2023-07-10 12:41:41,669][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 12:41:41,672][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 12:41:41,672][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:41:51,358][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 12:41:51,358][227910] FPS: 396514.91[0m
[36m[2023-07-10 12:41:51,361][227910] itr=405, itrs=2000, Progress: 20.25%[0m
[36m[2023-07-10 12:42:02,971][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 12:42:02,971][227910] FPS: 331200.57[0m
[36m[2023-07-10 12:42:07,742][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:42:07,742][227910] Reward + Measures: [[57.85510289  0.44294351  0.49405062  0.38105559  0.47284326]][0m
[37m[1m[2023-07-10 12:42:07,742][227910] Max Reward on eval: 57.85510289395971[0m
[37m[1m[2023-07-10 12:42:07,742][227910] Min Reward on eval: 57.85510289395971[0m
[37m[1m[2023-07-10 12:42:07,743][227910] Mean Reward across all agents: 57.85510289395971[0m
[37m[1m[2023-07-10 12:42:07,743][227910] Average Trajectory Length: 991.2123333333333[0m
[36m[2023-07-10 12:42:13,223][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:42:13,223][227910] Reward + Measures: [[  84.10471497    0.39340001    0.47799999    0.28930002    0.37509999]
 [ 114.02497441    0.3838        0.49790001    0.29966667    0.42123333]
 [  38.00100915    0.35927075    0.48150492    0.28405118    0.4301098 ]
 ...
 [ 158.49134145    0.34651294    0.35684893    0.29457983    0.32542157]
 [-121.4760977     0.29275295    0.42429408    0.18661766    0.32132941]
 [ 216.09798249    0.38080001    0.40430003    0.26120001    0.30750003]][0m
[37m[1m[2023-07-10 12:42:13,223][227910] Max Reward on eval: 519.671009207482[0m
[37m[1m[2023-07-10 12:42:13,224][227910] Min Reward on eval: -245.75392117992743[0m
[37m[1m[2023-07-10 12:42:13,224][227910] Mean Reward across all agents: 55.56234805089087[0m
[37m[1m[2023-07-10 12:42:13,224][227910] Average Trajectory Length: 970.746[0m
[36m[2023-07-10 12:42:13,226][227910] mean_value=-841.8785265619911, max_value=535.8057087410767[0m
[37m[1m[2023-07-10 12:42:13,229][227910] New mean coefficients: [[ 0.9368769  -1.0537269  -0.8959394   0.50657713 -1.847117  ]][0m
[37m[1m[2023-07-10 12:42:13,230][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:42:22,936][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 12:42:22,937][227910] FPS: 395671.36[0m
[36m[2023-07-10 12:42:22,939][227910] itr=406, itrs=2000, Progress: 20.30%[0m
[36m[2023-07-10 12:42:34,562][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 12:42:34,562][227910] FPS: 330850.39[0m
[36m[2023-07-10 12:42:39,425][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:42:39,430][227910] Reward + Measures: [[106.55423801   0.40852737   0.47328514   0.35077351   0.42331168]][0m
[37m[1m[2023-07-10 12:42:39,431][227910] Max Reward on eval: 106.55423801400289[0m
[37m[1m[2023-07-10 12:42:39,431][227910] Min Reward on eval: 106.55423801400289[0m
[37m[1m[2023-07-10 12:42:39,431][227910] Mean Reward across all agents: 106.55423801400289[0m
[37m[1m[2023-07-10 12:42:39,431][227910] Average Trajectory Length: 989.2959999999999[0m
[36m[2023-07-10 12:42:44,938][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:42:44,944][227910] Reward + Measures: [[  34.60866185    0.34839997    0.49089995    0.25659999    0.38340002]
 [  23.3474291     0.41770002    0.43729997    0.34199998    0.4271    ]
 [ 186.24551901    0.30602702    0.40314341    0.27357909    0.29019856]
 ...
 [-246.04449061    0.29159999    0.2579        0.2422        0.2465    ]
 [ 329.23164011    0.3611753     0.41388342    0.2629033     0.30832309]
 [ 202.53838627    0.35450003    0.46210003    0.29550001    0.35549998]][0m
[37m[1m[2023-07-10 12:42:44,944][227910] Max Reward on eval: 329.2316401145072[0m
[37m[1m[2023-07-10 12:42:44,945][227910] Min Reward on eval: -309.3116828461505[0m
[37m[1m[2023-07-10 12:42:44,945][227910] Mean Reward across all agents: 6.754958406862708[0m
[37m[1m[2023-07-10 12:42:44,945][227910] Average Trajectory Length: 980.4003333333333[0m
[36m[2023-07-10 12:42:44,947][227910] mean_value=-790.8208700084627, max_value=268.88490776511776[0m
[37m[1m[2023-07-10 12:42:44,949][227910] New mean coefficients: [[ 0.60020113 -0.87925935 -1.2681571   1.5824971  -0.9174236 ]][0m
[37m[1m[2023-07-10 12:42:44,950][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:42:54,637][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 12:42:54,637][227910] FPS: 396466.99[0m
[36m[2023-07-10 12:42:54,640][227910] itr=407, itrs=2000, Progress: 20.35%[0m
[36m[2023-07-10 12:43:06,240][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 12:43:06,240][227910] FPS: 331549.73[0m
[36m[2023-07-10 12:43:11,002][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:43:11,002][227910] Reward + Measures: [[204.36370192   0.3841649    0.44774178   0.33520952   0.38666096]][0m
[37m[1m[2023-07-10 12:43:11,002][227910] Max Reward on eval: 204.3637019164622[0m
[37m[1m[2023-07-10 12:43:11,002][227910] Min Reward on eval: 204.3637019164622[0m
[37m[1m[2023-07-10 12:43:11,003][227910] Mean Reward across all agents: 204.3637019164622[0m
[37m[1m[2023-07-10 12:43:11,003][227910] Average Trajectory Length: 979.6516666666666[0m
[36m[2023-07-10 12:43:16,467][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:43:16,468][227910] Reward + Measures: [[146.41994875   0.35680002   0.46830001   0.29590002   0.38009998]
 [ 70.69085251   0.39649996   0.52560008   0.28059998   0.3725    ]
 [ 10.51598196   0.37600002   0.48809996   0.27989998   0.36680001]
 ...
 [297.54897156   0.33797976   0.37761626   0.32458517   0.33146986]
 [149.36889678   0.34369999   0.43919998   0.33970001   0.4192    ]
 [213.4136548    0.33286101   0.46290809   0.26697847   0.32329693]][0m
[37m[1m[2023-07-10 12:43:16,468][227910] Max Reward on eval: 498.6744179346715[0m
[37m[1m[2023-07-10 12:43:16,469][227910] Min Reward on eval: -282.26303593344056[0m
[37m[1m[2023-07-10 12:43:16,469][227910] Mean Reward across all agents: 171.55103086562406[0m
[37m[1m[2023-07-10 12:43:16,469][227910] Average Trajectory Length: 977.7946666666667[0m
[36m[2023-07-10 12:43:16,470][227910] mean_value=-1061.5340055799393, max_value=101.11036972164541[0m
[37m[1m[2023-07-10 12:43:16,473][227910] New mean coefficients: [[ 0.89493835 -0.03977048 -1.1045296   1.5435052   0.53947616]][0m
[37m[1m[2023-07-10 12:43:16,474][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:43:26,087][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 12:43:26,087][227910] FPS: 399519.92[0m
[36m[2023-07-10 12:43:26,090][227910] itr=408, itrs=2000, Progress: 20.40%[0m
[36m[2023-07-10 12:43:37,575][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 12:43:37,575][227910] FPS: 334797.41[0m
[36m[2023-07-10 12:43:42,313][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:43:42,314][227910] Reward + Measures: [[305.76646941   0.37279046   0.42602867   0.32945338   0.36482131]][0m
[37m[1m[2023-07-10 12:43:42,314][227910] Max Reward on eval: 305.76646941113165[0m
[37m[1m[2023-07-10 12:43:42,314][227910] Min Reward on eval: 305.76646941113165[0m
[37m[1m[2023-07-10 12:43:42,314][227910] Mean Reward across all agents: 305.76646941113165[0m
[37m[1m[2023-07-10 12:43:42,314][227910] Average Trajectory Length: 978.0416666666666[0m
[36m[2023-07-10 12:43:47,881][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:43:47,886][227910] Reward + Measures: [[ 232.12058187    0.3349416     0.37127861    0.29403225    0.31170997]
 [ 143.66424941    0.40925333    0.39157721    0.36845136    0.39161226]
 [-107.0714056     0.51780003    0.61280006    0.42120001    0.55660003]
 ...
 [  33.97707419    0.36660001    0.43179998    0.3125        0.366     ]
 [ 217.75062754    0.37980002    0.51999998    0.30630001    0.43280002]
 [ 589.91429795    0.3574        0.40759999    0.3123        0.33220002]][0m
[37m[1m[2023-07-10 12:43:47,887][227910] Max Reward on eval: 589.914297950943[0m
[37m[1m[2023-07-10 12:43:47,887][227910] Min Reward on eval: -220.35672723799362[0m
[37m[1m[2023-07-10 12:43:47,887][227910] Mean Reward across all agents: 213.23963544276066[0m
[37m[1m[2023-07-10 12:43:47,888][227910] Average Trajectory Length: 983.9743333333333[0m
[36m[2023-07-10 12:43:47,889][227910] mean_value=-856.5684496199436, max_value=364.9078008055552[0m
[37m[1m[2023-07-10 12:43:47,892][227910] New mean coefficients: [[ 0.5368117  1.029224  -2.0567331  2.1457844  1.947595 ]][0m
[37m[1m[2023-07-10 12:43:47,893][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:43:57,710][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 12:43:57,710][227910] FPS: 391215.73[0m
[36m[2023-07-10 12:43:57,712][227910] itr=409, itrs=2000, Progress: 20.45%[0m
[36m[2023-07-10 12:44:09,486][227910] train() took 11.76 seconds to complete[0m
[36m[2023-07-10 12:44:09,486][227910] FPS: 326617.04[0m
[36m[2023-07-10 12:44:14,358][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:44:14,358][227910] Reward + Measures: [[330.4039454    0.39491507   0.40057209   0.35903206   0.36606526]][0m
[37m[1m[2023-07-10 12:44:14,359][227910] Max Reward on eval: 330.40394540477604[0m
[37m[1m[2023-07-10 12:44:14,359][227910] Min Reward on eval: 330.40394540477604[0m
[37m[1m[2023-07-10 12:44:14,359][227910] Mean Reward across all agents: 330.40394540477604[0m
[37m[1m[2023-07-10 12:44:14,359][227910] Average Trajectory Length: 976.1816666666666[0m
[36m[2023-07-10 12:44:20,045][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:44:20,046][227910] Reward + Measures: [[151.40098143   0.55726212   0.37377173   0.48918843   0.46908608]
 [212.2568045    0.39060137   0.49334288   0.30039379   0.37681457]
 [442.00788696   0.39248371   0.46970931   0.36060929   0.40238139]
 ...
 [347.88144148   0.4646       0.39140001   0.40939999   0.3888    ]
 [110.52585365   0.5205       0.43810001   0.4851       0.52670002]
 [-94.73125428   0.32820001   0.33270001   0.28579998   0.3671    ]][0m
[37m[1m[2023-07-10 12:44:20,046][227910] Max Reward on eval: 675.8761588403111[0m
[37m[1m[2023-07-10 12:44:20,046][227910] Min Reward on eval: -356.3160584985977[0m
[37m[1m[2023-07-10 12:44:20,046][227910] Mean Reward across all agents: 220.2620486116978[0m
[37m[1m[2023-07-10 12:44:20,047][227910] Average Trajectory Length: 975.145[0m
[36m[2023-07-10 12:44:20,049][227910] mean_value=-585.1717126099303, max_value=500.8524197613202[0m
[37m[1m[2023-07-10 12:44:20,052][227910] New mean coefficients: [[-0.2984516  1.5361799 -4.360657   2.3071213  2.0996237]][0m
[37m[1m[2023-07-10 12:44:20,053][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:44:29,839][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 12:44:29,839][227910] FPS: 392459.90[0m
[36m[2023-07-10 12:44:29,842][227910] itr=410, itrs=2000, Progress: 20.50%[0m
[37m[1m[2023-07-10 12:44:32,481][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000390[0m
[36m[2023-07-10 12:44:44,288][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 12:44:44,288][227910] FPS: 332462.37[0m
[36m[2023-07-10 12:44:49,034][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:44:49,034][227910] Reward + Measures: [[251.51993538   0.45203158   0.38209382   0.41816193   0.40194157]][0m
[37m[1m[2023-07-10 12:44:49,034][227910] Max Reward on eval: 251.51993537789397[0m
[37m[1m[2023-07-10 12:44:49,035][227910] Min Reward on eval: 251.51993537789397[0m
[37m[1m[2023-07-10 12:44:49,035][227910] Mean Reward across all agents: 251.51993537789397[0m
[37m[1m[2023-07-10 12:44:49,035][227910] Average Trajectory Length: 974.634[0m
[36m[2023-07-10 12:44:54,590][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:44:54,591][227910] Reward + Measures: [[ 15.20609502   0.39484343   0.32044709   0.36401317   0.34780318]
 [-51.29851162   0.3883       0.3608       0.35570002   0.35259998]
 [127.1021599    0.3594       0.35649997   0.31959999   0.31689999]
 ...
 [ 40.97478189   0.373        0.36219999   0.3448       0.34349999]
 [ 38.08383515   0.31970292   0.37714094   0.31345063   0.34542879]
 [ -5.44712992   0.46671826   0.49078265   0.44294906   0.45723325]][0m
[37m[1m[2023-07-10 12:44:54,591][227910] Max Reward on eval: 521.9631388785201[0m
[37m[1m[2023-07-10 12:44:54,592][227910] Min Reward on eval: -263.4832546430087[0m
[37m[1m[2023-07-10 12:44:54,592][227910] Mean Reward across all agents: 92.43019467011095[0m
[37m[1m[2023-07-10 12:44:54,592][227910] Average Trajectory Length: 963.8923333333333[0m
[36m[2023-07-10 12:44:54,594][227910] mean_value=-803.5986249228199, max_value=402.49373051617954[0m
[37m[1m[2023-07-10 12:44:54,596][227910] New mean coefficients: [[ 0.9565832  1.6705722 -3.2783985  2.763133   2.706556 ]][0m
[37m[1m[2023-07-10 12:44:54,597][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:45:04,280][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 12:45:04,280][227910] FPS: 396671.86[0m
[36m[2023-07-10 12:45:04,283][227910] itr=411, itrs=2000, Progress: 20.55%[0m
[36m[2023-07-10 12:45:15,838][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 12:45:15,838][227910] FPS: 332793.55[0m
[36m[2023-07-10 12:45:20,642][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:45:20,643][227910] Reward + Measures: [[246.73848874   0.50116354   0.33450276   0.46929923   0.42746601]][0m
[37m[1m[2023-07-10 12:45:20,643][227910] Max Reward on eval: 246.73848874425042[0m
[37m[1m[2023-07-10 12:45:20,643][227910] Min Reward on eval: 246.73848874425042[0m
[37m[1m[2023-07-10 12:45:20,643][227910] Mean Reward across all agents: 246.73848874425042[0m
[37m[1m[2023-07-10 12:45:20,643][227910] Average Trajectory Length: 978.0416666666666[0m
[36m[2023-07-10 12:45:26,092][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:45:26,093][227910] Reward + Measures: [[347.76112925   0.32867742   0.35872257   0.31197745   0.32533225]
 [ 87.04126902   0.47119999   0.38550001   0.4594       0.47230002]
 [ 45.50038676   0.46599999   0.45460001   0.45409998   0.55500001]
 ...
 [ 94.69243963   0.5729       0.30870003   0.54330003   0.45650002]
 [-46.35317196   0.37820002   0.50620002   0.33770004   0.53560001]
 [ 52.30640836   0.59929997   0.44770002   0.56999999   0.50770003]][0m
[37m[1m[2023-07-10 12:45:26,093][227910] Max Reward on eval: 519.0876315179048[0m
[37m[1m[2023-07-10 12:45:26,093][227910] Min Reward on eval: -223.0944134591322[0m
[37m[1m[2023-07-10 12:45:26,094][227910] Mean Reward across all agents: 141.23412336157637[0m
[37m[1m[2023-07-10 12:45:26,094][227910] Average Trajectory Length: 987.9599999999999[0m
[36m[2023-07-10 12:45:26,097][227910] mean_value=-127.53350728478235, max_value=641.6126762214752[0m
[37m[1m[2023-07-10 12:45:26,100][227910] New mean coefficients: [[ 0.43487203  2.5735643  -2.4208555   1.1162233   3.448829  ]][0m
[37m[1m[2023-07-10 12:45:26,101][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:45:35,823][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 12:45:35,824][227910] FPS: 395039.30[0m
[36m[2023-07-10 12:45:35,826][227910] itr=412, itrs=2000, Progress: 20.60%[0m
[36m[2023-07-10 12:45:47,403][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 12:45:47,403][227910] FPS: 332172.51[0m
[36m[2023-07-10 12:45:52,123][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:45:52,124][227910] Reward + Measures: [[185.19514015   0.54226059   0.29656103   0.51124275   0.46132675]][0m
[37m[1m[2023-07-10 12:45:52,124][227910] Max Reward on eval: 185.19514015076132[0m
[37m[1m[2023-07-10 12:45:52,124][227910] Min Reward on eval: 185.19514015076132[0m
[37m[1m[2023-07-10 12:45:52,125][227910] Mean Reward across all agents: 185.19514015076132[0m
[37m[1m[2023-07-10 12:45:52,125][227910] Average Trajectory Length: 982.2746666666667[0m
[36m[2023-07-10 12:45:57,577][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:45:57,578][227910] Reward + Measures: [[ 55.8194923    0.59928179   0.35034546   0.5613727    0.509     ]
 [272.95670123   0.51859999   0.3107       0.49780002   0.45829996]
 [148.90276041   0.5255       0.296        0.48839998   0.42659998]
 ...
 [197.40390935   0.47709998   0.49689999   0.44580004   0.48730001]
 [130.17090363   0.55179155   0.37828189   0.48478976   0.46076566]
 [327.77335798   0.39709997   0.42120001   0.37180004   0.41529998]][0m
[37m[1m[2023-07-10 12:45:57,578][227910] Max Reward on eval: 518.3930162519682[0m
[37m[1m[2023-07-10 12:45:57,578][227910] Min Reward on eval: -291.3733345572138[0m
[37m[1m[2023-07-10 12:45:57,578][227910] Mean Reward across all agents: 101.06865172397337[0m
[37m[1m[2023-07-10 12:45:57,579][227910] Average Trajectory Length: 994.3366666666666[0m
[36m[2023-07-10 12:45:57,582][227910] mean_value=-201.2105554785159, max_value=476.20521671538484[0m
[37m[1m[2023-07-10 12:45:57,584][227910] New mean coefficients: [[ 0.4112285  1.4091251 -1.7036946  1.5810558  2.6892612]][0m
[37m[1m[2023-07-10 12:45:57,585][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:46:07,267][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 12:46:07,267][227910] FPS: 396696.79[0m
[36m[2023-07-10 12:46:07,269][227910] itr=413, itrs=2000, Progress: 20.65%[0m
[36m[2023-07-10 12:46:18,832][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 12:46:18,833][227910] FPS: 332569.24[0m
[36m[2023-07-10 12:46:23,615][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:46:23,616][227910] Reward + Measures: [[147.18292213   0.58981282   0.26363271   0.56258041   0.50344604]][0m
[37m[1m[2023-07-10 12:46:23,616][227910] Max Reward on eval: 147.18292213016852[0m
[37m[1m[2023-07-10 12:46:23,616][227910] Min Reward on eval: 147.18292213016852[0m
[37m[1m[2023-07-10 12:46:23,616][227910] Mean Reward across all agents: 147.18292213016852[0m
[37m[1m[2023-07-10 12:46:23,616][227910] Average Trajectory Length: 979.2493333333333[0m
[36m[2023-07-10 12:46:29,213][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:46:29,214][227910] Reward + Measures: [[182.76207845   0.75700009   0.2375       0.72180003   0.63079995]
 [129.49638369   0.65540004   0.44600001   0.63440001   0.60540003]
 [ 69.08858869   0.68699998   0.55430001   0.64130008   0.65449995]
 ...
 [ 23.26466971   0.62709999   0.23150001   0.58190006   0.51109999]
 [166.33156433   0.61590004   0.35499999   0.57839996   0.54409999]
 [233.2686598    0.46469998   0.51750004   0.3804       0.48979998]][0m
[37m[1m[2023-07-10 12:46:29,214][227910] Max Reward on eval: 460.32597631141545[0m
[37m[1m[2023-07-10 12:46:29,214][227910] Min Reward on eval: -197.30324845041613[0m
[37m[1m[2023-07-10 12:46:29,214][227910] Mean Reward across all agents: 124.51894744196146[0m
[37m[1m[2023-07-10 12:46:29,215][227910] Average Trajectory Length: 989.026[0m
[36m[2023-07-10 12:46:29,221][227910] mean_value=149.62948378053576, max_value=668.4218535523978[0m
[37m[1m[2023-07-10 12:46:29,224][227910] New mean coefficients: [[ 0.22223583  1.4615457  -1.8161077   1.7668637   2.8662016 ]][0m
[37m[1m[2023-07-10 12:46:29,224][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:46:38,941][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 12:46:38,941][227910] FPS: 395272.36[0m
[36m[2023-07-10 12:46:38,943][227910] itr=414, itrs=2000, Progress: 20.70%[0m
[36m[2023-07-10 12:46:50,437][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 12:46:50,437][227910] FPS: 334555.38[0m
[36m[2023-07-10 12:46:55,242][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:46:55,242][227910] Reward + Measures: [[121.39661496   0.6411975    0.2248022    0.6197474    0.55262643]][0m
[37m[1m[2023-07-10 12:46:55,242][227910] Max Reward on eval: 121.39661496245529[0m
[37m[1m[2023-07-10 12:46:55,242][227910] Min Reward on eval: 121.39661496245529[0m
[37m[1m[2023-07-10 12:46:55,242][227910] Mean Reward across all agents: 121.39661496245529[0m
[37m[1m[2023-07-10 12:46:55,243][227910] Average Trajectory Length: 984.7339999999999[0m
[36m[2023-07-10 12:47:00,630][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:47:00,693][227910] Reward + Measures: [[110.06788228   0.40510002   0.54949999   0.3653       0.5399    ]
 [287.0424487    0.43022805   0.56369734   0.37688851   0.51031798]
 [153.44498604   0.48629999   0.65550005   0.3953       0.63200003]
 ...
 [293.06013395   0.63190001   0.56560004   0.62380004   0.66620004]
 [-30.17560285   0.4612       0.54250002   0.43189999   0.58090001]
 [ 43.10628453   0.55870003   0.17200001   0.55660003   0.47890002]][0m
[37m[1m[2023-07-10 12:47:00,693][227910] Max Reward on eval: 485.4002870214637[0m
[37m[1m[2023-07-10 12:47:00,693][227910] Min Reward on eval: -394.08163945539854[0m
[37m[1m[2023-07-10 12:47:00,693][227910] Mean Reward across all agents: 110.04849338583905[0m
[37m[1m[2023-07-10 12:47:00,694][227910] Average Trajectory Length: 987.8523333333333[0m
[36m[2023-07-10 12:47:00,699][227910] mean_value=101.56495893032921, max_value=653.6792362522799[0m
[37m[1m[2023-07-10 12:47:00,702][227910] New mean coefficients: [[-0.5104643  1.8740411 -1.8395709  2.4791446  2.635761 ]][0m
[37m[1m[2023-07-10 12:47:00,703][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:47:10,419][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 12:47:10,419][227910] FPS: 395292.77[0m
[36m[2023-07-10 12:47:10,421][227910] itr=415, itrs=2000, Progress: 20.75%[0m
[36m[2023-07-10 12:47:21,907][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 12:47:21,907][227910] FPS: 334781.42[0m
[36m[2023-07-10 12:47:26,705][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:47:26,705][227910] Reward + Measures: [[84.08807304  0.68092656  0.1862382   0.66508204  0.5924415 ]][0m
[37m[1m[2023-07-10 12:47:26,705][227910] Max Reward on eval: 84.08807304405121[0m
[37m[1m[2023-07-10 12:47:26,705][227910] Min Reward on eval: 84.08807304405121[0m
[37m[1m[2023-07-10 12:47:26,705][227910] Mean Reward across all agents: 84.08807304405121[0m
[37m[1m[2023-07-10 12:47:26,706][227910] Average Trajectory Length: 988.0523333333333[0m
[36m[2023-07-10 12:47:32,230][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:47:32,231][227910] Reward + Measures: [[-10.15669147   0.70250005   0.45039997   0.66759998   0.6257    ]
 [214.53336958   0.63033456   0.37062103   0.59810632   0.55758113]
 [117.35772632   0.70480007   0.18889999   0.68620008   0.59230006]
 ...
 [  4.98138577   0.81140006   0.1749       0.80260003   0.70680004]
 [120.78530897   0.75260001   0.19289999   0.72200006   0.63850003]
 [ 54.29520948   0.44740587   0.40593919   0.42462751   0.44121766]][0m
[37m[1m[2023-07-10 12:47:32,231][227910] Max Reward on eval: 362.08223440512086[0m
[37m[1m[2023-07-10 12:47:32,231][227910] Min Reward on eval: -115.60091380656232[0m
[37m[1m[2023-07-10 12:47:32,232][227910] Mean Reward across all agents: 79.1450834315364[0m
[37m[1m[2023-07-10 12:47:32,232][227910] Average Trajectory Length: 989.8673333333332[0m
[36m[2023-07-10 12:47:32,237][227910] mean_value=133.58206888737482, max_value=656.0689794049629[0m
[37m[1m[2023-07-10 12:47:32,240][227910] New mean coefficients: [[ 0.39505607  1.1212748  -1.4045086   2.7723777   2.0321474 ]][0m
[37m[1m[2023-07-10 12:47:32,241][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:47:42,124][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 12:47:42,124][227910] FPS: 388614.57[0m
[36m[2023-07-10 12:47:42,127][227910] itr=416, itrs=2000, Progress: 20.80%[0m
[36m[2023-07-10 12:47:53,855][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 12:47:53,855][227910] FPS: 327860.88[0m
[36m[2023-07-10 12:47:58,656][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:47:58,656][227910] Reward + Measures: [[65.95431637  0.73095578  0.18611559  0.71566033  0.65564018]][0m
[37m[1m[2023-07-10 12:47:58,657][227910] Max Reward on eval: 65.95431637216385[0m
[37m[1m[2023-07-10 12:47:58,657][227910] Min Reward on eval: 65.95431637216385[0m
[37m[1m[2023-07-10 12:47:58,657][227910] Mean Reward across all agents: 65.95431637216385[0m
[37m[1m[2023-07-10 12:47:58,657][227910] Average Trajectory Length: 988.8146666666667[0m
[36m[2023-07-10 12:48:04,210][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:48:04,211][227910] Reward + Measures: [[ 54.69232009   0.71799994   0.25569999   0.70539999   0.65180004]
 [-22.33701247   0.74669999   0.27700004   0.71180004   0.64120001]
 [ 91.60883514   0.71910006   0.28579998   0.70889997   0.66099995]
 ...
 [-33.46652602   0.78859997   0.27200001   0.75099999   0.68849999]
 [129.03484967   0.8071       0.17120001   0.78549999   0.68470001]
 [ 65.58466872   0.83630002   0.13620001   0.81560004   0.73050004]][0m
[37m[1m[2023-07-10 12:48:04,211][227910] Max Reward on eval: 440.16009785595816[0m
[37m[1m[2023-07-10 12:48:04,211][227910] Min Reward on eval: -257.35157703720034[0m
[37m[1m[2023-07-10 12:48:04,211][227910] Mean Reward across all agents: 33.868226473335646[0m
[37m[1m[2023-07-10 12:48:04,212][227910] Average Trajectory Length: 990.3919999999999[0m
[36m[2023-07-10 12:48:04,217][227910] mean_value=173.23277320684718, max_value=700.7772387954683[0m
[37m[1m[2023-07-10 12:48:04,220][227910] New mean coefficients: [[ 1.2069751  1.711658  -0.3304249  2.8068984  1.3699892]][0m
[37m[1m[2023-07-10 12:48:04,221][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:48:14,225][227910] train() took 10.00 seconds to complete[0m
[36m[2023-07-10 12:48:14,225][227910] FPS: 383924.67[0m
[36m[2023-07-10 12:48:14,227][227910] itr=417, itrs=2000, Progress: 20.85%[0m
[36m[2023-07-10 12:48:25,746][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 12:48:25,746][227910] FPS: 333837.09[0m
[36m[2023-07-10 12:48:30,516][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:48:30,521][227910] Reward + Measures: [[66.05441292  0.76449609  0.16677198  0.75122815  0.69522965]][0m
[37m[1m[2023-07-10 12:48:30,522][227910] Max Reward on eval: 66.054412922999[0m
[37m[1m[2023-07-10 12:48:30,522][227910] Min Reward on eval: 66.054412922999[0m
[37m[1m[2023-07-10 12:48:30,522][227910] Mean Reward across all agents: 66.054412922999[0m
[37m[1m[2023-07-10 12:48:30,522][227910] Average Trajectory Length: 990.3706666666666[0m
[36m[2023-07-10 12:48:36,018][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:48:36,019][227910] Reward + Measures: [[-138.3135316     0.62930006    0.32460004    0.62119997    0.52490002]
 [ 151.19063653    0.6548        0.28800002    0.63740003    0.57270002]
 [  54.40640777    0.75490004    0.2455        0.73649997    0.67259997]
 ...
 [  77.80488885    0.77310002    0.40820003    0.74529999    0.72039998]
 [-282.30810497    0.66280001    0.54100001    0.64469999    0.63370001]
 [ 100.82370262    0.60710001    0.59840006    0.60170001    0.68400002]][0m
[37m[1m[2023-07-10 12:48:36,019][227910] Max Reward on eval: 433.2957628440898[0m
[37m[1m[2023-07-10 12:48:36,019][227910] Min Reward on eval: -396.6738673281972[0m
[37m[1m[2023-07-10 12:48:36,019][227910] Mean Reward across all agents: 26.66205491220819[0m
[37m[1m[2023-07-10 12:48:36,020][227910] Average Trajectory Length: 993.635[0m
[36m[2023-07-10 12:48:36,025][227910] mean_value=64.7989132293376, max_value=697.0565196127143[0m
[37m[1m[2023-07-10 12:48:36,028][227910] New mean coefficients: [[ 1.6966656   2.178501   -0.18347213  1.9178746   1.521279  ]][0m
[37m[1m[2023-07-10 12:48:36,029][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:48:45,788][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 12:48:45,788][227910] FPS: 393546.40[0m
[36m[2023-07-10 12:48:45,790][227910] itr=418, itrs=2000, Progress: 20.90%[0m
[36m[2023-07-10 12:48:57,251][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 12:48:57,252][227910] FPS: 335503.77[0m
[36m[2023-07-10 12:49:01,971][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:49:01,971][227910] Reward + Measures: [[92.885654    0.77864772  0.19156787  0.76595736  0.71851754]][0m
[37m[1m[2023-07-10 12:49:01,972][227910] Max Reward on eval: 92.88565400235662[0m
[37m[1m[2023-07-10 12:49:01,972][227910] Min Reward on eval: 92.88565400235662[0m
[37m[1m[2023-07-10 12:49:01,972][227910] Mean Reward across all agents: 92.88565400235662[0m
[37m[1m[2023-07-10 12:49:01,972][227910] Average Trajectory Length: 989.2526666666666[0m
[36m[2023-07-10 12:49:07,393][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:49:07,393][227910] Reward + Measures: [[ 69.16545863   0.7432       0.20020001   0.71920002   0.67260003]
 [159.4921028    0.7238102    0.50181228   0.68988985   0.69005513]
 [-24.61876378   0.74010003   0.32980001   0.7342       0.70370001]
 ...
 [-92.23896157   0.75459999   0.50800002   0.73150003   0.7313    ]
 [183.75493318   0.82770008   0.39089999   0.79320002   0.7396    ]
 [ 14.85410524   0.66350001   0.27150002   0.64289999   0.61050004]][0m
[37m[1m[2023-07-10 12:49:07,393][227910] Max Reward on eval: 472.31793194731[0m
[37m[1m[2023-07-10 12:49:07,394][227910] Min Reward on eval: -168.00838029157603[0m
[37m[1m[2023-07-10 12:49:07,394][227910] Mean Reward across all agents: 92.36439117039484[0m
[37m[1m[2023-07-10 12:49:07,394][227910] Average Trajectory Length: 986.4833333333333[0m
[36m[2023-07-10 12:49:07,401][227910] mean_value=116.4314610835807, max_value=612.4850619923789[0m
[37m[1m[2023-07-10 12:49:07,403][227910] New mean coefficients: [[ 1.8234957  2.5204659 -0.6975814  2.7636564  1.8558407]][0m
[37m[1m[2023-07-10 12:49:07,404][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:49:17,122][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 12:49:17,122][227910] FPS: 395238.29[0m
[36m[2023-07-10 12:49:17,124][227910] itr=419, itrs=2000, Progress: 20.95%[0m
[36m[2023-07-10 12:49:28,567][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 12:49:28,567][227910] FPS: 336054.66[0m
[36m[2023-07-10 12:49:33,376][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:49:33,377][227910] Reward + Measures: [[78.5649642   0.80148226  0.18899181  0.79148006  0.74668455]][0m
[37m[1m[2023-07-10 12:49:33,377][227910] Max Reward on eval: 78.56496420359679[0m
[37m[1m[2023-07-10 12:49:33,377][227910] Min Reward on eval: 78.56496420359679[0m
[37m[1m[2023-07-10 12:49:33,378][227910] Mean Reward across all agents: 78.56496420359679[0m
[37m[1m[2023-07-10 12:49:33,378][227910] Average Trajectory Length: 992.569[0m
[36m[2023-07-10 12:49:38,917][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:49:38,917][227910] Reward + Measures: [[  61.49586933    0.85680002    0.41209999    0.84170002    0.79710007]
 [  44.10762246    0.85280001    0.26830003    0.83830005    0.7428    ]
 [  98.35411853    0.86840004    0.4348        0.84860003    0.79699999]
 ...
 [ 104.55358786    0.71900004    0.4549        0.69929999    0.66860002]
 [-127.63912258    0.78590006    0.3012        0.74760002    0.65990007]
 [  35.67943792    0.73900002    0.62279999    0.66689998    0.67039996]][0m
[37m[1m[2023-07-10 12:49:38,918][227910] Max Reward on eval: 345.544116604561[0m
[37m[1m[2023-07-10 12:49:38,918][227910] Min Reward on eval: -293.6371573086246[0m
[37m[1m[2023-07-10 12:49:38,918][227910] Mean Reward across all agents: 40.50680724433298[0m
[37m[1m[2023-07-10 12:49:38,918][227910] Average Trajectory Length: 992.6543333333333[0m
[36m[2023-07-10 12:49:38,924][227910] mean_value=96.88777777636507, max_value=845.5441166045609[0m
[37m[1m[2023-07-10 12:49:38,927][227910] New mean coefficients: [[ 1.909628    2.4964998  -0.37679392  2.6832786   3.2987552 ]][0m
[37m[1m[2023-07-10 12:49:38,928][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:49:48,746][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 12:49:48,746][227910] FPS: 391189.11[0m
[36m[2023-07-10 12:49:48,748][227910] itr=420, itrs=2000, Progress: 21.00%[0m
[37m[1m[2023-07-10 12:49:51,418][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000400[0m
[36m[2023-07-10 12:50:03,402][227910] train() took 11.72 seconds to complete[0m
[36m[2023-07-10 12:50:03,402][227910] FPS: 327723.05[0m
[36m[2023-07-10 12:50:08,190][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:50:08,191][227910] Reward + Measures: [[101.05507022   0.78871036   0.21587533   0.77970785   0.74601299]][0m
[37m[1m[2023-07-10 12:50:08,191][227910] Max Reward on eval: 101.05507022385936[0m
[37m[1m[2023-07-10 12:50:08,191][227910] Min Reward on eval: 101.05507022385936[0m
[37m[1m[2023-07-10 12:50:08,191][227910] Mean Reward across all agents: 101.05507022385936[0m
[37m[1m[2023-07-10 12:50:08,192][227910] Average Trajectory Length: 990.7113333333333[0m
[36m[2023-07-10 12:50:13,608][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:50:13,614][227910] Reward + Measures: [[ -36.08221354    0.75346369    0.24654241    0.73272729    0.7196424 ]
 [ 186.4804297     0.82819998    0.43880001    0.80369997    0.7489    ]
 [-136.51673813    0.79070002    0.79229993    0.76520008    0.7888    ]
 ...
 [ 261.09366072    0.50180006    0.62589997    0.47460005    0.57410002]
 [ 100.55647104    0.75590003    0.68650001    0.74990004    0.74249995]
 [  18.20643447    0.84740001    0.51109999    0.83610004    0.80440009]][0m
[37m[1m[2023-07-10 12:50:13,614][227910] Max Reward on eval: 305.5709104311885[0m
[37m[1m[2023-07-10 12:50:13,614][227910] Min Reward on eval: -212.3950629638508[0m
[37m[1m[2023-07-10 12:50:13,615][227910] Mean Reward across all agents: 65.51967293883315[0m
[37m[1m[2023-07-10 12:50:13,615][227910] Average Trajectory Length: 993.6663333333333[0m
[36m[2023-07-10 12:50:13,621][227910] mean_value=92.63280556362311, max_value=529.3727477114996[0m
[37m[1m[2023-07-10 12:50:13,623][227910] New mean coefficients: [[ 2.3080451   2.615893   -0.03256041  2.181427    2.5617602 ]][0m
[37m[1m[2023-07-10 12:50:13,624][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:50:23,351][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 12:50:23,352][227910] FPS: 394846.97[0m
[36m[2023-07-10 12:50:23,354][227910] itr=421, itrs=2000, Progress: 21.05%[0m
[36m[2023-07-10 12:50:34,995][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 12:50:34,995][227910] FPS: 330314.61[0m
[36m[2023-07-10 12:50:39,815][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:50:39,815][227910] Reward + Measures: [[149.39092285   0.80597776   0.26378936   0.797041     0.77386427]][0m
[37m[1m[2023-07-10 12:50:39,815][227910] Max Reward on eval: 149.39092285437505[0m
[37m[1m[2023-07-10 12:50:39,815][227910] Min Reward on eval: 149.39092285437505[0m
[37m[1m[2023-07-10 12:50:39,816][227910] Mean Reward across all agents: 149.39092285437505[0m
[37m[1m[2023-07-10 12:50:39,816][227910] Average Trajectory Length: 991.8463333333333[0m
[36m[2023-07-10 12:50:45,463][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:50:45,463][227910] Reward + Measures: [[-130.96503311    0.89580005    0.56199998    0.86600012    0.78729999]
 [  76.36023248    0.88480008    0.30710003    0.87          0.77509999]
 [  -4.90773958    0.86197501    0.421875      0.84412497    0.79484999]
 ...
 [ 177.36716572    0.87060016    0.2131        0.84720004    0.78709996]
 [  17.52733113    0.83759993    0.52310002    0.82390004    0.7978    ]
 [  71.44086493    0.84580004    0.3346        0.83400005    0.79320002]][0m
[37m[1m[2023-07-10 12:50:45,464][227910] Max Reward on eval: 438.1659555295395[0m
[37m[1m[2023-07-10 12:50:45,464][227910] Min Reward on eval: -339.89442106325293[0m
[37m[1m[2023-07-10 12:50:45,464][227910] Mean Reward across all agents: 45.89452912466742[0m
[37m[1m[2023-07-10 12:50:45,464][227910] Average Trajectory Length: 995.6083333333333[0m
[36m[2023-07-10 12:50:45,472][227910] mean_value=132.48682721386848, max_value=806.8386971679167[0m
[37m[1m[2023-07-10 12:50:45,475][227910] New mean coefficients: [[ 3.2789066   2.286122   -0.00602613  1.4742879   1.7145047 ]][0m
[37m[1m[2023-07-10 12:50:45,476][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:50:55,177][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 12:50:55,177][227910] FPS: 395877.73[0m
[36m[2023-07-10 12:50:55,180][227910] itr=422, itrs=2000, Progress: 21.10%[0m
[36m[2023-07-10 12:51:06,649][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 12:51:06,650][227910] FPS: 335251.80[0m
[36m[2023-07-10 12:51:11,441][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:51:11,442][227910] Reward + Measures: [[215.91850964   0.77838135   0.32015327   0.7675423    0.74928057]][0m
[37m[1m[2023-07-10 12:51:11,442][227910] Max Reward on eval: 215.91850964291564[0m
[37m[1m[2023-07-10 12:51:11,442][227910] Min Reward on eval: 215.91850964291564[0m
[37m[1m[2023-07-10 12:51:11,442][227910] Mean Reward across all agents: 215.91850964291564[0m
[37m[1m[2023-07-10 12:51:11,443][227910] Average Trajectory Length: 990.7103333333333[0m
[36m[2023-07-10 12:51:17,012][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:51:17,013][227910] Reward + Measures: [[433.1324891    0.70839995   0.2617       0.68009996   0.66520005]
 [250.94739003   0.73800004   0.63959998   0.71880001   0.7428    ]
 [214.49100249   0.70889997   0.52829999   0.65780002   0.6717    ]
 ...
 [ 52.42414408   0.81220001   0.68790001   0.79350007   0.78809994]
 [-76.45436032   0.88850003   0.87010002   0.88910002   0.87019998]
 [302.83969076   0.80190003   0.45539999   0.7687       0.75640005]][0m
[37m[1m[2023-07-10 12:51:17,013][227910] Max Reward on eval: 561.85380166166[0m
[37m[1m[2023-07-10 12:51:17,013][227910] Min Reward on eval: -355.5275202708319[0m
[37m[1m[2023-07-10 12:51:17,014][227910] Mean Reward across all agents: 146.17737867725742[0m
[37m[1m[2023-07-10 12:51:17,014][227910] Average Trajectory Length: 994.5013333333333[0m
[36m[2023-07-10 12:51:17,021][227910] mean_value=139.15477940119868, max_value=780.3596068128594[0m
[37m[1m[2023-07-10 12:51:17,024][227910] New mean coefficients: [[3.239832  2.0035973 0.7065292 0.7208884 1.768095 ]][0m
[37m[1m[2023-07-10 12:51:17,025][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:51:26,814][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 12:51:26,815][227910] FPS: 392339.00[0m
[36m[2023-07-10 12:51:26,817][227910] itr=423, itrs=2000, Progress: 21.15%[0m
[36m[2023-07-10 12:51:38,392][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 12:51:38,393][227910] FPS: 332182.26[0m
[36m[2023-07-10 12:51:43,200][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:51:43,200][227910] Reward + Measures: [[312.86983008   0.77226514   0.31250131   0.76043278   0.7378937 ]][0m
[37m[1m[2023-07-10 12:51:43,201][227910] Max Reward on eval: 312.86983008134695[0m
[37m[1m[2023-07-10 12:51:43,201][227910] Min Reward on eval: 312.86983008134695[0m
[37m[1m[2023-07-10 12:51:43,201][227910] Mean Reward across all agents: 312.86983008134695[0m
[37m[1m[2023-07-10 12:51:43,201][227910] Average Trajectory Length: 989.896[0m
[36m[2023-07-10 12:51:48,701][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:51:48,707][227910] Reward + Measures: [[ 62.33709878   0.86490005   0.58510005   0.8563       0.82919997]
 [199.93618306   0.81269997   0.34890002   0.796        0.77689999]
 [216.46877054   0.81153339   0.51574278   0.81064272   0.79121369]
 ...
 [242.1810461    0.85479993   0.46760002   0.84000009   0.8215    ]
 [117.76385374   0.76719999   0.51010001   0.75780004   0.74860001]
 [245.0682565    0.76010007   0.28740001   0.75570005   0.74049997]][0m
[37m[1m[2023-07-10 12:51:48,708][227910] Max Reward on eval: 578.7091604952002[0m
[37m[1m[2023-07-10 12:51:48,708][227910] Min Reward on eval: -93.47120675178012[0m
[37m[1m[2023-07-10 12:51:48,709][227910] Mean Reward across all agents: 235.3336963872108[0m
[37m[1m[2023-07-10 12:51:48,710][227910] Average Trajectory Length: 990.8[0m
[36m[2023-07-10 12:51:48,723][227910] mean_value=126.41806179987776, max_value=711.2725666507068[0m
[37m[1m[2023-07-10 12:51:48,726][227910] New mean coefficients: [[2.884872  1.9091129 0.4269208 0.9586838 1.7139238]][0m
[37m[1m[2023-07-10 12:51:48,728][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:51:58,615][227910] train() took 9.89 seconds to complete[0m
[36m[2023-07-10 12:51:58,616][227910] FPS: 388441.54[0m
[36m[2023-07-10 12:51:58,618][227910] itr=424, itrs=2000, Progress: 21.20%[0m
[36m[2023-07-10 12:52:10,281][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 12:52:10,281][227910] FPS: 329697.29[0m
[36m[2023-07-10 12:52:15,039][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:52:15,039][227910] Reward + Measures: [[389.54920821   0.74906081   0.33021688   0.73372138   0.71255875]][0m
[37m[1m[2023-07-10 12:52:15,039][227910] Max Reward on eval: 389.54920821370484[0m
[37m[1m[2023-07-10 12:52:15,040][227910] Min Reward on eval: 389.54920821370484[0m
[37m[1m[2023-07-10 12:52:15,040][227910] Mean Reward across all agents: 389.54920821370484[0m
[37m[1m[2023-07-10 12:52:15,040][227910] Average Trajectory Length: 986.6683333333333[0m
[36m[2023-07-10 12:52:20,390][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:52:20,391][227910] Reward + Measures: [[235.42133592   0.68720001   0.27400002   0.6832       0.64970005]
 [433.71440264   0.67720002   0.28059998   0.60600001   0.49639997]
 [535.95746957   0.51270002   0.3091       0.4686       0.52399999]
 ...
 [399.10767383   0.54809999   0.2983       0.50370008   0.39589998]
 [ 70.06770837   0.75489998   0.72670001   0.73970002   0.75909996]
 [401.73893008   0.80849999   0.23179999   0.78940004   0.71489996]][0m
[37m[1m[2023-07-10 12:52:20,391][227910] Max Reward on eval: 695.5101483992592[0m
[37m[1m[2023-07-10 12:52:20,391][227910] Min Reward on eval: -211.92265045879176[0m
[37m[1m[2023-07-10 12:52:20,391][227910] Mean Reward across all agents: 367.61797814969486[0m
[37m[1m[2023-07-10 12:52:20,392][227910] Average Trajectory Length: 986.8796666666666[0m
[36m[2023-07-10 12:52:20,400][227910] mean_value=230.94147438479104, max_value=992.4336056481584[0m
[37m[1m[2023-07-10 12:52:20,402][227910] New mean coefficients: [[3.3436353 0.8441627 0.3006699 0.2363866 1.0302651]][0m
[37m[1m[2023-07-10 12:52:20,403][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:52:30,152][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 12:52:30,153][227910] FPS: 393955.76[0m
[36m[2023-07-10 12:52:30,155][227910] itr=425, itrs=2000, Progress: 21.25%[0m
[36m[2023-07-10 12:52:41,803][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 12:52:41,803][227910] FPS: 330161.85[0m
[36m[2023-07-10 12:52:46,727][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:52:46,727][227910] Reward + Measures: [[447.56706383   0.71344459   0.36187702   0.69492555   0.67458135]][0m
[37m[1m[2023-07-10 12:52:46,727][227910] Max Reward on eval: 447.56706382589215[0m
[37m[1m[2023-07-10 12:52:46,727][227910] Min Reward on eval: 447.56706382589215[0m
[37m[1m[2023-07-10 12:52:46,727][227910] Mean Reward across all agents: 447.56706382589215[0m
[37m[1m[2023-07-10 12:52:46,728][227910] Average Trajectory Length: 982.636[0m
[36m[2023-07-10 12:52:52,360][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:52:52,360][227910] Reward + Measures: [[433.51872766   0.64570004   0.2872       0.63029999   0.6103    ]
 [574.82809228   0.70490003   0.31869999   0.68049997   0.64960003]
 [552.30355408   0.7098       0.22989999   0.71040004   0.66499996]
 ...
 [174.0771738    0.51822501   0.13575      0.50082499   0.46004996]
 [309.32763789   0.71830004   0.39770001   0.69589996   0.66900009]
 [307.13425342   0.60652751   0.12752752   0.60451841   0.55680555]][0m
[37m[1m[2023-07-10 12:52:52,361][227910] Max Reward on eval: 724.5297958715586[0m
[37m[1m[2023-07-10 12:52:52,361][227910] Min Reward on eval: -79.88764576939866[0m
[37m[1m[2023-07-10 12:52:52,361][227910] Mean Reward across all agents: 295.465355717807[0m
[37m[1m[2023-07-10 12:52:52,361][227910] Average Trajectory Length: 980.5419999999999[0m
[36m[2023-07-10 12:52:52,367][227910] mean_value=84.21568993354248, max_value=880.5517086668204[0m
[37m[1m[2023-07-10 12:52:52,370][227910] New mean coefficients: [[ 2.5768697   0.3406505   0.55198294 -0.8153717   1.017359  ]][0m
[37m[1m[2023-07-10 12:52:52,371][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:53:02,058][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 12:53:02,058][227910] FPS: 396495.63[0m
[36m[2023-07-10 12:53:02,060][227910] itr=426, itrs=2000, Progress: 21.30%[0m
[36m[2023-07-10 12:53:13,527][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 12:53:13,527][227910] FPS: 335345.84[0m
[36m[2023-07-10 12:53:18,331][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:53:18,332][227910] Reward + Measures: [[625.76296912   0.66398406   0.29395092   0.64133257   0.61939615]][0m
[37m[1m[2023-07-10 12:53:18,332][227910] Max Reward on eval: 625.7629691209214[0m
[37m[1m[2023-07-10 12:53:18,332][227910] Min Reward on eval: 625.7629691209214[0m
[37m[1m[2023-07-10 12:53:18,332][227910] Mean Reward across all agents: 625.7629691209214[0m
[37m[1m[2023-07-10 12:53:18,332][227910] Average Trajectory Length: 985.1326666666666[0m
[36m[2023-07-10 12:53:23,836][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:53:23,842][227910] Reward + Measures: [[718.93385223   0.61879998   0.5891       0.55940002   0.61119998]
 [512.9024237    0.67309999   0.34940001   0.65960002   0.65200001]
 [686.21218525   0.52020001   0.42659998   0.46409997   0.49060002]
 ...
 [467.81400901   0.76019996   0.1803       0.74070001   0.69410002]
 [736.42940697   0.52985001   0.47615001   0.45860001   0.49645004]
 [702.00806118   0.65487415   0.47604495   0.59324163   0.60652918]][0m
[37m[1m[2023-07-10 12:53:23,842][227910] Max Reward on eval: 977.7641692476697[0m
[37m[1m[2023-07-10 12:53:23,842][227910] Min Reward on eval: -38.793772917683235[0m
[37m[1m[2023-07-10 12:53:23,842][227910] Mean Reward across all agents: 546.8958232752805[0m
[37m[1m[2023-07-10 12:53:23,843][227910] Average Trajectory Length: 974.8106666666666[0m
[36m[2023-07-10 12:53:23,850][227910] mean_value=163.08602824652914, max_value=1019.1646328724823[0m
[37m[1m[2023-07-10 12:53:23,852][227910] New mean coefficients: [[ 2.827324    1.1295996   0.4811991  -0.24493921  1.9121939 ]][0m
[37m[1m[2023-07-10 12:53:23,854][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:53:33,519][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 12:53:33,519][227910] FPS: 397364.85[0m
[36m[2023-07-10 12:53:33,521][227910] itr=427, itrs=2000, Progress: 21.35%[0m
[36m[2023-07-10 12:53:44,944][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 12:53:44,944][227910] FPS: 336649.96[0m
[36m[2023-07-10 12:53:49,652][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:53:49,653][227910] Reward + Measures: [[741.363834     0.61292154   0.2730189    0.58783716   0.56516659]][0m
[37m[1m[2023-07-10 12:53:49,653][227910] Max Reward on eval: 741.3638339956273[0m
[37m[1m[2023-07-10 12:53:49,653][227910] Min Reward on eval: 741.3638339956273[0m
[37m[1m[2023-07-10 12:53:49,653][227910] Mean Reward across all agents: 741.3638339956273[0m
[37m[1m[2023-07-10 12:53:49,654][227910] Average Trajectory Length: 974.7776666666666[0m
[36m[2023-07-10 12:53:55,169][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:53:55,169][227910] Reward + Measures: [[321.75899142   0.52500004   0.48009998   0.4323       0.4138    ]
 [140.22956309   0.8502       0.84930003   0.83610004   0.8344    ]
 [740.93634978   0.55043334   0.32398042   0.51448965   0.49818736]
 ...
 [429.40755532   0.70030004   0.66100001   0.62059999   0.61989999]
 [671.02454384   0.53006154   0.30095384   0.47312307   0.43003845]
 [511.15622455   0.70880854   0.34370339   0.64832371   0.48534408]][0m
[37m[1m[2023-07-10 12:53:55,169][227910] Max Reward on eval: 1095.8166702866554[0m
[37m[1m[2023-07-10 12:53:55,170][227910] Min Reward on eval: -27.436347251589176[0m
[37m[1m[2023-07-10 12:53:55,170][227910] Mean Reward across all agents: 573.1856406358274[0m
[37m[1m[2023-07-10 12:53:55,170][227910] Average Trajectory Length: 976.2843333333333[0m
[36m[2023-07-10 12:53:55,177][227910] mean_value=238.21014528633984, max_value=1296.4305429818226[0m
[37m[1m[2023-07-10 12:53:55,180][227910] New mean coefficients: [[ 3.4386377  2.0759501 -0.405245  -0.0071592  1.6981194]][0m
[37m[1m[2023-07-10 12:53:55,181][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:54:04,883][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 12:54:04,883][227910] FPS: 395868.52[0m
[36m[2023-07-10 12:54:04,886][227910] itr=428, itrs=2000, Progress: 21.40%[0m
[36m[2023-07-10 12:54:16,539][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 12:54:16,539][227910] FPS: 330041.14[0m
[36m[2023-07-10 12:54:21,278][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:54:21,278][227910] Reward + Measures: [[918.07001908   0.55666775   0.29081568   0.52542967   0.50132549]][0m
[37m[1m[2023-07-10 12:54:21,278][227910] Max Reward on eval: 918.0700190829774[0m
[37m[1m[2023-07-10 12:54:21,278][227910] Min Reward on eval: 918.0700190829774[0m
[37m[1m[2023-07-10 12:54:21,278][227910] Mean Reward across all agents: 918.0700190829774[0m
[37m[1m[2023-07-10 12:54:21,279][227910] Average Trajectory Length: 976.5813333333333[0m
[36m[2023-07-10 12:54:26,733][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:54:26,739][227910] Reward + Measures: [[ 622.61688534    0.5228278     0.36800671    0.47707424    0.46815205]
 [ 645.36062793    0.62874597    0.36543784    0.58433068    0.55763245]
 [ 899.38176938    0.45513225    0.41859999    0.38503549    0.38189679]
 ...
 [1155.1760251     0.50770003    0.27449998    0.46259999    0.42610002]
 [1263.01033818    0.47431827    0.40195209    0.42647853    0.38777626]
 [ 909.11821711    0.58670002    0.2949        0.56009996    0.52560002]][0m
[37m[1m[2023-07-10 12:54:26,739][227910] Max Reward on eval: 1335.6279619344277[0m
[37m[1m[2023-07-10 12:54:26,740][227910] Min Reward on eval: 113.02343434036739[0m
[37m[1m[2023-07-10 12:54:26,740][227910] Mean Reward across all agents: 739.3094385394187[0m
[37m[1m[2023-07-10 12:54:26,740][227910] Average Trajectory Length: 979.4163333333333[0m
[36m[2023-07-10 12:54:26,747][227910] mean_value=226.62632697886735, max_value=956.745100008214[0m
[37m[1m[2023-07-10 12:54:26,750][227910] New mean coefficients: [[ 3.366809    1.2309062  -1.2889557  -0.00048644  1.6095431 ]][0m
[37m[1m[2023-07-10 12:54:26,751][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:54:36,590][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 12:54:36,590][227910] FPS: 390368.71[0m
[36m[2023-07-10 12:54:36,592][227910] itr=429, itrs=2000, Progress: 21.45%[0m
[36m[2023-07-10 12:54:48,279][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 12:54:48,280][227910] FPS: 329063.82[0m
[36m[2023-07-10 12:54:53,030][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:54:53,031][227910] Reward + Measures: [[1074.35630136    0.50013566    0.28576061    0.4659082     0.44031212]][0m
[37m[1m[2023-07-10 12:54:53,031][227910] Max Reward on eval: 1074.3563013587357[0m
[37m[1m[2023-07-10 12:54:53,031][227910] Min Reward on eval: 1074.3563013587357[0m
[37m[1m[2023-07-10 12:54:53,031][227910] Mean Reward across all agents: 1074.3563013587357[0m
[37m[1m[2023-07-10 12:54:53,031][227910] Average Trajectory Length: 972.9656666666666[0m
[36m[2023-07-10 12:54:58,431][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:54:58,432][227910] Reward + Measures: [[1198.91510591    0.56770003    0.35530004    0.52290004    0.49880001]
 [1035.16478625    0.52050006    0.37360001    0.47529998    0.45159999]
 [1196.96992806    0.53879452    0.25812212    0.5232532     0.47854859]
 ...
 [ 830.73358948    0.52715713    0.27924284    0.5053429     0.46791431]
 [ 966.89603233    0.53940004    0.31320003    0.50920004    0.454     ]
 [ 729.3898144     0.62309998    0.33670002    0.58238888    0.53033334]][0m
[37m[1m[2023-07-10 12:54:58,432][227910] Max Reward on eval: 1601.8749345658464[0m
[37m[1m[2023-07-10 12:54:58,432][227910] Min Reward on eval: 334.1299770369078[0m
[37m[1m[2023-07-10 12:54:58,432][227910] Mean Reward across all agents: 936.2044327160245[0m
[37m[1m[2023-07-10 12:54:58,433][227910] Average Trajectory Length: 978.3386666666667[0m
[36m[2023-07-10 12:54:58,439][227910] mean_value=226.47290808728914, max_value=1117.6133888037066[0m
[37m[1m[2023-07-10 12:54:58,442][227910] New mean coefficients: [[ 2.8291128   0.8503369  -1.3188668  -0.08881807  1.2005358 ]][0m
[37m[1m[2023-07-10 12:54:58,443][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:55:08,120][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 12:55:08,121][227910] FPS: 396850.00[0m
[36m[2023-07-10 12:55:08,123][227910] itr=430, itrs=2000, Progress: 21.50%[0m
[37m[1m[2023-07-10 12:55:10,902][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000410[0m
[36m[2023-07-10 12:55:22,679][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 12:55:22,679][227910] FPS: 333701.72[0m
[36m[2023-07-10 12:55:27,599][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:55:27,599][227910] Reward + Measures: [[1300.52398763    0.46393016    0.28386897    0.41812256    0.39194176]][0m
[37m[1m[2023-07-10 12:55:27,600][227910] Max Reward on eval: 1300.523987633038[0m
[37m[1m[2023-07-10 12:55:27,600][227910] Min Reward on eval: 1300.523987633038[0m
[37m[1m[2023-07-10 12:55:27,600][227910] Mean Reward across all agents: 1300.523987633038[0m
[37m[1m[2023-07-10 12:55:27,600][227910] Average Trajectory Length: 975.699[0m
[36m[2023-07-10 12:55:33,150][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:55:33,151][227910] Reward + Measures: [[ 968.24972609    0.4858        0.33329999    0.43319997    0.43129998]
 [ 515.53977938    0.43430001    0.1983        0.40299997    0.37639999]
 [ 711.05432103    0.48580003    0.22360002    0.4598        0.43689999]
 ...
 [1312.26649212    0.52766323    0.29437587    0.48643795    0.45196897]
 [ 921.73114477    0.54949999    0.44390002    0.47489998    0.50200003]
 [ 719.94520141    0.57689995    0.23909998    0.54540002    0.53859997]][0m
[37m[1m[2023-07-10 12:55:33,151][227910] Max Reward on eval: 1747.2591614525998[0m
[37m[1m[2023-07-10 12:55:33,151][227910] Min Reward on eval: 3.2608999620540997[0m
[37m[1m[2023-07-10 12:55:33,151][227910] Mean Reward across all agents: 960.2157038902583[0m
[37m[1m[2023-07-10 12:55:33,152][227910] Average Trajectory Length: 971.4283333333333[0m
[36m[2023-07-10 12:55:33,158][227910] mean_value=151.97361072374247, max_value=1115.6376047345304[0m
[37m[1m[2023-07-10 12:55:33,161][227910] New mean coefficients: [[ 2.5045795   0.37799814 -0.9961711  -0.13629371  0.88576496]][0m
[37m[1m[2023-07-10 12:55:33,161][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:55:42,902][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 12:55:42,902][227910] FPS: 394309.05[0m
[36m[2023-07-10 12:55:42,904][227910] itr=431, itrs=2000, Progress: 21.55%[0m
[36m[2023-07-10 12:55:54,344][227910] train() took 11.42 seconds to complete[0m
[36m[2023-07-10 12:55:54,345][227910] FPS: 336190.72[0m
[36m[2023-07-10 12:55:59,130][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:55:59,131][227910] Reward + Measures: [[1503.27774148    0.43417001    0.28251457    0.38314769    0.35128281]][0m
[37m[1m[2023-07-10 12:55:59,131][227910] Max Reward on eval: 1503.277741477253[0m
[37m[1m[2023-07-10 12:55:59,131][227910] Min Reward on eval: 1503.277741477253[0m
[37m[1m[2023-07-10 12:55:59,131][227910] Mean Reward across all agents: 1503.277741477253[0m
[37m[1m[2023-07-10 12:55:59,132][227910] Average Trajectory Length: 974.9399999999999[0m
[36m[2023-07-10 12:56:04,564][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:56:04,565][227910] Reward + Measures: [[1288.88494455    0.48559999    0.42039999    0.3962        0.40939999]
 [ 956.83450631    0.35890001    0.42609999    0.31380001    0.35190001]
 [1380.48803004    0.53820002    0.23029999    0.48410001    0.4368    ]
 ...
 [1060.45464115    0.51531631    0.30981633    0.45979881    0.44721442]
 [1215.44039683    0.55070001    0.19180001    0.52109998    0.45390001]
 [ 785.86025073    0.46981558    0.41102704    0.41419673    0.40265736]][0m
[37m[1m[2023-07-10 12:56:04,565][227910] Max Reward on eval: 2077.792255747295[0m
[37m[1m[2023-07-10 12:56:04,565][227910] Min Reward on eval: 83.44235907218827[0m
[37m[1m[2023-07-10 12:56:04,566][227910] Mean Reward across all agents: 1211.7998089157302[0m
[37m[1m[2023-07-10 12:56:04,566][227910] Average Trajectory Length: 978.935[0m
[36m[2023-07-10 12:56:04,572][227910] mean_value=163.62633365058335, max_value=1021.3515435323937[0m
[37m[1m[2023-07-10 12:56:04,575][227910] New mean coefficients: [[ 2.6162493   0.3020045  -1.4949362  -0.00921321  0.28861248]][0m
[37m[1m[2023-07-10 12:56:04,576][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:56:14,186][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 12:56:14,186][227910] FPS: 399640.45[0m
[36m[2023-07-10 12:56:14,189][227910] itr=432, itrs=2000, Progress: 21.60%[0m
[36m[2023-07-10 12:56:25,701][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 12:56:25,701][227910] FPS: 334035.09[0m
[36m[2023-07-10 12:56:30,588][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:56:30,589][227910] Reward + Measures: [[1710.39698754    0.40317431    0.29318973    0.35152453    0.31418642]][0m
[37m[1m[2023-07-10 12:56:30,589][227910] Max Reward on eval: 1710.396987544146[0m
[37m[1m[2023-07-10 12:56:30,589][227910] Min Reward on eval: 1710.396987544146[0m
[37m[1m[2023-07-10 12:56:30,589][227910] Mean Reward across all agents: 1710.396987544146[0m
[37m[1m[2023-07-10 12:56:30,589][227910] Average Trajectory Length: 976.1909999999999[0m
[36m[2023-07-10 12:56:36,096][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:56:36,097][227910] Reward + Measures: [[1589.41103065    0.38181117    0.3311595     0.29181209    0.26801404]
 [1570.10854036    0.3479        0.30640003    0.27630004    0.22000001]
 [1934.83435585    0.39520454    0.29801175    0.32870218    0.28214225]
 ...
 [1854.54655754    0.41760001    0.33660004    0.33590001    0.28260002]
 [1717.22704064    0.36850002    0.3326        0.25490001    0.23099999]
 [1616.65391214    0.44994003    0.33459416    0.36152002    0.32658732]][0m
[37m[1m[2023-07-10 12:56:36,097][227910] Max Reward on eval: 2084.8183883449647[0m
[37m[1m[2023-07-10 12:56:36,097][227910] Min Reward on eval: 72.65775591928977[0m
[37m[1m[2023-07-10 12:56:36,098][227910] Mean Reward across all agents: 1416.0896186454163[0m
[37m[1m[2023-07-10 12:56:36,098][227910] Average Trajectory Length: 963.2103333333333[0m
[36m[2023-07-10 12:56:36,103][227910] mean_value=-164.7078812267059, max_value=965.6942637287668[0m
[37m[1m[2023-07-10 12:56:36,106][227910] New mean coefficients: [[ 2.087607    0.22697207 -1.5236677   0.590934    0.47368863]][0m
[37m[1m[2023-07-10 12:56:36,107][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:56:45,823][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 12:56:45,824][227910] FPS: 395278.07[0m
[36m[2023-07-10 12:56:45,826][227910] itr=433, itrs=2000, Progress: 21.65%[0m
[36m[2023-07-10 12:56:57,516][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 12:56:57,516][227910] FPS: 328925.52[0m
[36m[2023-07-10 12:57:02,349][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:57:02,350][227910] Reward + Measures: [[1854.11706794    0.38668877    0.29772228    0.32948178    0.29177007]][0m
[37m[1m[2023-07-10 12:57:02,350][227910] Max Reward on eval: 1854.1170679384327[0m
[37m[1m[2023-07-10 12:57:02,350][227910] Min Reward on eval: 1854.1170679384327[0m
[37m[1m[2023-07-10 12:57:02,350][227910] Mean Reward across all agents: 1854.1170679384327[0m
[37m[1m[2023-07-10 12:57:02,351][227910] Average Trajectory Length: 976.901[0m
[36m[2023-07-10 12:57:08,027][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:57:08,028][227910] Reward + Measures: [[2067.67387525    0.31960002    0.3319        0.2764        0.23239999]
 [1634.52351383    0.40190002    0.27210003    0.35680002    0.31320003]
 [1246.16823157    0.33681211    0.23822728    0.30953029    0.26564851]
 ...
 [1464.08738434    0.30472672    0.24290538    0.2794176     0.22309859]
 [1400.95226807    0.45819998    0.35430002    0.36269999    0.33700004]
 [1677.7916725     0.27800003    0.2782        0.2253        0.1948    ]][0m
[37m[1m[2023-07-10 12:57:08,028][227910] Max Reward on eval: 2138.4098978187017[0m
[37m[1m[2023-07-10 12:57:08,028][227910] Min Reward on eval: 516.4069461606675[0m
[37m[1m[2023-07-10 12:57:08,029][227910] Mean Reward across all agents: 1523.7030779512293[0m
[37m[1m[2023-07-10 12:57:08,029][227910] Average Trajectory Length: 972.2926666666666[0m
[36m[2023-07-10 12:57:08,033][227910] mean_value=-170.9052437888606, max_value=1190.3546546939249[0m
[37m[1m[2023-07-10 12:57:08,036][227910] New mean coefficients: [[ 2.4313068   0.7827077  -1.717632    0.26258752  0.5397379 ]][0m
[37m[1m[2023-07-10 12:57:08,037][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:57:17,913][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 12:57:17,913][227910] FPS: 388902.52[0m
[36m[2023-07-10 12:57:17,916][227910] itr=434, itrs=2000, Progress: 21.70%[0m
[36m[2023-07-10 12:57:29,540][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 12:57:29,540][227910] FPS: 330860.57[0m
[36m[2023-07-10 12:57:34,393][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:57:34,393][227910] Reward + Measures: [[1943.20082595    0.37453303    0.2862204     0.32173991    0.27759516]][0m
[37m[1m[2023-07-10 12:57:34,394][227910] Max Reward on eval: 1943.2008259499712[0m
[37m[1m[2023-07-10 12:57:34,394][227910] Min Reward on eval: 1943.2008259499712[0m
[37m[1m[2023-07-10 12:57:34,394][227910] Mean Reward across all agents: 1943.2008259499712[0m
[37m[1m[2023-07-10 12:57:34,394][227910] Average Trajectory Length: 974.707[0m
[36m[2023-07-10 12:57:39,788][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:57:39,789][227910] Reward + Measures: [[1351.6556971     0.42744619    0.30478463    0.36458459    0.30624616]
 [1936.73534202    0.39540002    0.3256        0.32150003    0.29099998]
 [1666.34739404    0.38569021    0.28688571    0.33205184    0.2766116 ]
 ...
 [2050.65429277    0.35266498    0.35999012    0.26172978    0.23724602]
 [1710.84336186    0.36539999    0.36130002    0.30299997    0.27329999]
 [2051.98392405    0.41690001    0.2974        0.36310002    0.30329999]][0m
[37m[1m[2023-07-10 12:57:39,789][227910] Max Reward on eval: 2281.5130806991133[0m
[37m[1m[2023-07-10 12:57:39,789][227910] Min Reward on eval: 429.79128800947217[0m
[37m[1m[2023-07-10 12:57:39,790][227910] Mean Reward across all agents: 1608.3823911496365[0m
[37m[1m[2023-07-10 12:57:39,790][227910] Average Trajectory Length: 978.256[0m
[36m[2023-07-10 12:57:39,793][227910] mean_value=-386.2014513143565, max_value=713.9752871745761[0m
[37m[1m[2023-07-10 12:57:39,796][227910] New mean coefficients: [[ 2.0886343   0.9190964  -0.9298386   0.43218246  1.2806928 ]][0m
[37m[1m[2023-07-10 12:57:39,797][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:57:49,447][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 12:57:49,448][227910] FPS: 397981.12[0m
[36m[2023-07-10 12:57:49,450][227910] itr=435, itrs=2000, Progress: 21.75%[0m
[36m[2023-07-10 12:58:00,907][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 12:58:00,907][227910] FPS: 335659.34[0m
[36m[2023-07-10 12:58:05,601][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:58:05,602][227910] Reward + Measures: [[2091.46198799    0.36870605    0.28045702    0.31012744    0.26238096]][0m
[37m[1m[2023-07-10 12:58:05,602][227910] Max Reward on eval: 2091.4619879869347[0m
[37m[1m[2023-07-10 12:58:05,602][227910] Min Reward on eval: 2091.4619879869347[0m
[37m[1m[2023-07-10 12:58:05,603][227910] Mean Reward across all agents: 2091.4619879869347[0m
[37m[1m[2023-07-10 12:58:05,603][227910] Average Trajectory Length: 969.8136666666667[0m
[36m[2023-07-10 12:58:11,180][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:58:11,181][227910] Reward + Measures: [[1786.64254534    0.38885829    0.30359778    0.33126059    0.29386124]
 [2270.42138028    0.34800002    0.31360003    0.30149999    0.2447    ]
 [1547.02988084    0.42086563    0.32434365    0.34873036    0.31319556]
 ...
 [1838.73905703    0.3479        0.3197        0.30830002    0.26180002]
 [2162.24807994    0.3405        0.32800001    0.28049999    0.22719999]
 [2106.26961613    0.32285205    0.29785785    0.25809339    0.19388182]][0m
[37m[1m[2023-07-10 12:58:11,181][227910] Max Reward on eval: 2528.7817105744266[0m
[37m[1m[2023-07-10 12:58:11,182][227910] Min Reward on eval: 899.5565044822463[0m
[37m[1m[2023-07-10 12:58:11,182][227910] Mean Reward across all agents: 1812.0637104414536[0m
[37m[1m[2023-07-10 12:58:11,182][227910] Average Trajectory Length: 969.8093333333333[0m
[36m[2023-07-10 12:58:11,185][227910] mean_value=-573.3012113234355, max_value=791.5735058781472[0m
[37m[1m[2023-07-10 12:58:11,188][227910] New mean coefficients: [[ 1.7980902   1.3526878  -0.46412343  0.35314476  1.6640681 ]][0m
[37m[1m[2023-07-10 12:58:11,189][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:58:20,903][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 12:58:20,903][227910] FPS: 395395.09[0m
[36m[2023-07-10 12:58:20,905][227910] itr=436, itrs=2000, Progress: 21.80%[0m
[36m[2023-07-10 12:58:32,498][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 12:58:32,498][227910] FPS: 331707.59[0m
[36m[2023-07-10 12:58:37,138][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:58:37,139][227910] Reward + Measures: [[2201.99831221    0.35926887    0.27620673    0.29703119    0.24893707]][0m
[37m[1m[2023-07-10 12:58:37,139][227910] Max Reward on eval: 2201.9983122094604[0m
[37m[1m[2023-07-10 12:58:37,139][227910] Min Reward on eval: 2201.9983122094604[0m
[37m[1m[2023-07-10 12:58:37,140][227910] Mean Reward across all agents: 2201.9983122094604[0m
[37m[1m[2023-07-10 12:58:37,140][227910] Average Trajectory Length: 967.3593333333333[0m
[36m[2023-07-10 12:58:42,445][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:58:42,450][227910] Reward + Measures: [[2219.27480214    0.41109997    0.2854        0.34909999    0.3048    ]
 [1943.139823      0.3917        0.30109999    0.3048        0.2712    ]
 [1955.99964366    0.36179999    0.39049998    0.25920001    0.25400001]
 ...
 [1573.69993796    0.4869611     0.31683192    0.43308029    0.3883439 ]
 [1043.74979381    0.42950001    0.3443        0.3479        0.35280004]
 [1603.69861158    0.41470003    0.37240002    0.35320002    0.34740001]][0m
[37m[1m[2023-07-10 12:58:42,451][227910] Max Reward on eval: 2563.290714614885[0m
[37m[1m[2023-07-10 12:58:42,451][227910] Min Reward on eval: 293.0485009671771[0m
[37m[1m[2023-07-10 12:58:42,451][227910] Mean Reward across all agents: 1662.1861853117873[0m
[37m[1m[2023-07-10 12:58:42,452][227910] Average Trajectory Length: 972.61[0m
[36m[2023-07-10 12:58:42,455][227910] mean_value=-316.15089299627357, max_value=997.9784273508324[0m
[37m[1m[2023-07-10 12:58:42,458][227910] New mean coefficients: [[ 1.5302281  1.5318047 -0.5097683  0.8568515  1.8300087]][0m
[37m[1m[2023-07-10 12:58:42,459][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:58:52,051][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 12:58:52,052][227910] FPS: 400365.04[0m
[36m[2023-07-10 12:58:52,054][227910] itr=437, itrs=2000, Progress: 21.85%[0m
[36m[2023-07-10 12:59:03,543][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 12:59:03,544][227910] FPS: 334764.21[0m
[36m[2023-07-10 12:59:08,370][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:59:08,371][227910] Reward + Measures: [[2378.12968573    0.35105664    0.28438333    0.28790453    0.23699923]][0m
[37m[1m[2023-07-10 12:59:08,371][227910] Max Reward on eval: 2378.129685728304[0m
[37m[1m[2023-07-10 12:59:08,371][227910] Min Reward on eval: 2378.129685728304[0m
[37m[1m[2023-07-10 12:59:08,372][227910] Mean Reward across all agents: 2378.129685728304[0m
[37m[1m[2023-07-10 12:59:08,372][227910] Average Trajectory Length: 974.1036666666666[0m
[36m[2023-07-10 12:59:13,833][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:59:13,833][227910] Reward + Measures: [[1457.914906      0.458         0.3664        0.45240003    0.39540002]
 [2026.22108536    0.35520002    0.27740002    0.3249        0.27340001]
 [1587.51618104    0.24349999    0.24000001    0.22280002    0.18620001]
 ...
 [2358.28801774    0.32453045    0.27932334    0.24596444    0.20078635]
 [1495.75921807    0.44859812    0.2552661     0.4130066     0.36924243]
 [1539.34724226    0.40669999    0.3603        0.35510001    0.30420002]][0m
[37m[1m[2023-07-10 12:59:13,833][227910] Max Reward on eval: 2657.4924991114763[0m
[37m[1m[2023-07-10 12:59:13,834][227910] Min Reward on eval: 799.1388543555047[0m
[37m[1m[2023-07-10 12:59:13,834][227910] Mean Reward across all agents: 1826.640265325356[0m
[37m[1m[2023-07-10 12:59:13,834][227910] Average Trajectory Length: 970.928[0m
[36m[2023-07-10 12:59:13,837][227910] mean_value=-584.2452549980729, max_value=1342.9778761086113[0m
[37m[1m[2023-07-10 12:59:13,839][227910] New mean coefficients: [[ 1.2111322   1.157867   -0.6257743   0.93161213  1.8048253 ]][0m
[37m[1m[2023-07-10 12:59:13,840][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:59:23,651][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 12:59:23,651][227910] FPS: 391469.55[0m
[36m[2023-07-10 12:59:23,654][227910] itr=438, itrs=2000, Progress: 21.90%[0m
[36m[2023-07-10 12:59:35,379][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 12:59:35,379][227910] FPS: 328018.28[0m
[36m[2023-07-10 12:59:40,141][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:59:40,142][227910] Reward + Measures: [[2533.01718543    0.35714903    0.28764364    0.28625515    0.2371015 ]][0m
[37m[1m[2023-07-10 12:59:40,142][227910] Max Reward on eval: 2533.017185425891[0m
[37m[1m[2023-07-10 12:59:40,142][227910] Min Reward on eval: 2533.017185425891[0m
[37m[1m[2023-07-10 12:59:40,143][227910] Mean Reward across all agents: 2533.017185425891[0m
[37m[1m[2023-07-10 12:59:40,143][227910] Average Trajectory Length: 978.232[0m
[36m[2023-07-10 12:59:45,844][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 12:59:45,844][227910] Reward + Measures: [[2309.54244133    0.34392676    0.31007192    0.25596568    0.21749674]
 [1722.0488651     0.37839437    0.32356057    0.32568595    0.2987141 ]
 [2585.47060312    0.3448        0.33799997    0.25970003    0.22390001]
 ...
 [1409.98689809    0.47          0.36220002    0.42630002    0.3964    ]
 [1590.58808416    0.38550001    0.40240002    0.29669997    0.2951    ]
 [2173.62649785    0.38276932    0.27234054    0.30694377    0.24140064]][0m
[37m[1m[2023-07-10 12:59:45,844][227910] Max Reward on eval: 2854.539628114109[0m
[37m[1m[2023-07-10 12:59:45,845][227910] Min Reward on eval: 900.4888225363509[0m
[37m[1m[2023-07-10 12:59:45,845][227910] Mean Reward across all agents: 2078.0432380507477[0m
[37m[1m[2023-07-10 12:59:45,845][227910] Average Trajectory Length: 964.4823333333333[0m
[36m[2023-07-10 12:59:45,848][227910] mean_value=-642.0766487656342, max_value=1421.1088175221653[0m
[37m[1m[2023-07-10 12:59:45,851][227910] New mean coefficients: [[ 0.9665608   0.5396837  -0.30711514  0.5479136   1.6603119 ]][0m
[37m[1m[2023-07-10 12:59:45,852][227910] Moving the mean solution point...[0m
[36m[2023-07-10 12:59:55,570][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 12:59:55,571][227910] FPS: 395178.61[0m
[36m[2023-07-10 12:59:55,573][227910] itr=439, itrs=2000, Progress: 21.95%[0m
[36m[2023-07-10 13:00:07,110][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 13:00:07,111][227910] FPS: 333359.05[0m
[36m[2023-07-10 13:00:11,857][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:00:11,857][227910] Reward + Measures: [[2601.02789417    0.35713422    0.27827862    0.28931612    0.23635446]][0m
[37m[1m[2023-07-10 13:00:11,857][227910] Max Reward on eval: 2601.0278941677825[0m
[37m[1m[2023-07-10 13:00:11,857][227910] Min Reward on eval: 2601.0278941677825[0m
[37m[1m[2023-07-10 13:00:11,858][227910] Mean Reward across all agents: 2601.0278941677825[0m
[37m[1m[2023-07-10 13:00:11,858][227910] Average Trajectory Length: 970.5653333333333[0m
[36m[2023-07-10 13:00:17,250][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:00:17,251][227910] Reward + Measures: [[2680.49109436    0.37979999    0.292         0.31510001    0.2606    ]
 [ 642.26291075    0.46055314    0.2573373     0.44574395    0.35680947]
 [1731.95603302    0.39400002    0.37979999    0.28640002    0.27239999]
 ...
 [1755.58814129    0.38302431    0.25214872    0.30747396    0.25218675]
 [1264.26889185    0.51389998    0.29490003    0.49320003    0.41149998]
 [2209.34876977    0.42700002    0.35740003    0.32210001    0.2705    ]][0m
[37m[1m[2023-07-10 13:00:17,251][227910] Max Reward on eval: 2879.1667651243743[0m
[37m[1m[2023-07-10 13:00:17,251][227910] Min Reward on eval: 584.647634358739[0m
[37m[1m[2023-07-10 13:00:17,252][227910] Mean Reward across all agents: 1984.750407939689[0m
[37m[1m[2023-07-10 13:00:17,252][227910] Average Trajectory Length: 963.5[0m
[36m[2023-07-10 13:00:17,255][227910] mean_value=-527.3813499321075, max_value=1333.76873122986[0m
[37m[1m[2023-07-10 13:00:17,257][227910] New mean coefficients: [[ 1.317481    0.81020904 -0.34819183  1.0891395   2.036626  ]][0m
[37m[1m[2023-07-10 13:00:17,258][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:00:26,816][227910] train() took 9.56 seconds to complete[0m
[36m[2023-07-10 13:00:26,816][227910] FPS: 401862.74[0m
[36m[2023-07-10 13:00:26,818][227910] itr=440, itrs=2000, Progress: 22.00%[0m
[37m[1m[2023-07-10 13:00:29,458][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000420[0m
[36m[2023-07-10 13:00:41,196][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 13:00:41,196][227910] FPS: 334556.49[0m
[36m[2023-07-10 13:00:45,982][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:00:45,988][227910] Reward + Measures: [[2784.95117191    0.35036865    0.28759158    0.27916667    0.22636937]][0m
[37m[1m[2023-07-10 13:00:45,988][227910] Max Reward on eval: 2784.951171912375[0m
[37m[1m[2023-07-10 13:00:45,989][227910] Min Reward on eval: 2784.951171912375[0m
[37m[1m[2023-07-10 13:00:45,989][227910] Mean Reward across all agents: 2784.951171912375[0m
[37m[1m[2023-07-10 13:00:45,989][227910] Average Trajectory Length: 981.0076666666666[0m
[36m[2023-07-10 13:00:51,507][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:00:51,513][227910] Reward + Measures: [[2010.85478195    0.44439998    0.24850002    0.4073        0.34870002]
 [2638.34296813    0.3427        0.27169999    0.26630002    0.2141    ]
 [2155.29600058    0.33736929    0.30215523    0.2826691     0.24378629]
 ...
 [2961.8563102     0.34970003    0.29490003    0.27110001    0.20369999]
 [2982.64957031    0.32710001    0.29730001    0.2622        0.1983    ]
 [2431.53530381    0.39800003    0.29700002    0.34410003    0.28029999]][0m
[37m[1m[2023-07-10 13:00:51,513][227910] Max Reward on eval: 3161.0792804558646[0m
[37m[1m[2023-07-10 13:00:51,514][227910] Min Reward on eval: 847.1393122766109[0m
[37m[1m[2023-07-10 13:00:51,514][227910] Mean Reward across all agents: 2207.1405899625893[0m
[37m[1m[2023-07-10 13:00:51,514][227910] Average Trajectory Length: 957.3463333333333[0m
[36m[2023-07-10 13:00:51,517][227910] mean_value=-725.8443604339677, max_value=1099.3141772911417[0m
[37m[1m[2023-07-10 13:00:51,519][227910] New mean coefficients: [[1.0239731  0.7625078  0.31529608 1.370264   2.3595388 ]][0m
[37m[1m[2023-07-10 13:00:51,520][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:01:01,462][227910] train() took 9.94 seconds to complete[0m
[36m[2023-07-10 13:01:01,462][227910] FPS: 386331.25[0m
[36m[2023-07-10 13:01:01,464][227910] itr=441, itrs=2000, Progress: 22.05%[0m
[36m[2023-07-10 13:01:12,922][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 13:01:12,923][227910] FPS: 335623.40[0m
[36m[2023-07-10 13:01:17,776][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:01:17,776][227910] Reward + Measures: [[2923.59986822    0.34470174    0.28649518    0.27145761    0.21558954]][0m
[37m[1m[2023-07-10 13:01:17,776][227910] Max Reward on eval: 2923.59986821918[0m
[37m[1m[2023-07-10 13:01:17,776][227910] Min Reward on eval: 2923.59986821918[0m
[37m[1m[2023-07-10 13:01:17,777][227910] Mean Reward across all agents: 2923.59986821918[0m
[37m[1m[2023-07-10 13:01:17,777][227910] Average Trajectory Length: 977.439[0m
[36m[2023-07-10 13:01:23,261][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:01:23,262][227910] Reward + Measures: [[1889.11164766    0.39140004    0.4508        0.32329997    0.30700001]
 [2639.99540854    0.35530001    0.34189999    0.26500002    0.20220001]
 [2542.66457431    0.37710002    0.35830003    0.32340002    0.2753    ]
 ...
 [3182.88651393    0.35050002    0.31890002    0.25970003    0.206     ]
 [2513.35040422    0.34029999    0.3874        0.27110001    0.23290001]
 [2001.22662876    0.27315998    0.29748008    0.24107273    0.19590703]][0m
[37m[1m[2023-07-10 13:01:23,262][227910] Max Reward on eval: 3182.886513932515[0m
[37m[1m[2023-07-10 13:01:23,262][227910] Min Reward on eval: 12.96316480586538[0m
[37m[1m[2023-07-10 13:01:23,262][227910] Mean Reward across all agents: 2175.7452092194667[0m
[37m[1m[2023-07-10 13:01:23,262][227910] Average Trajectory Length: 981.8689999999999[0m
[36m[2023-07-10 13:01:23,266][227910] mean_value=-355.8507577564296, max_value=1059.8242091396003[0m
[37m[1m[2023-07-10 13:01:23,269][227910] New mean coefficients: [[0.40886778 1.899603   1.3795364  2.0977082  3.3464837 ]][0m
[37m[1m[2023-07-10 13:01:23,270][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:01:32,866][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 13:01:32,866][227910] FPS: 400231.70[0m
[36m[2023-07-10 13:01:32,868][227910] itr=442, itrs=2000, Progress: 22.10%[0m
[36m[2023-07-10 13:01:44,347][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 13:01:44,347][227910] FPS: 334995.67[0m
[36m[2023-07-10 13:01:48,977][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:01:48,977][227910] Reward + Measures: [[3034.38806964    0.35796994    0.29451546    0.27412942    0.22253528]][0m
[37m[1m[2023-07-10 13:01:48,978][227910] Max Reward on eval: 3034.3880696422652[0m
[37m[1m[2023-07-10 13:01:48,978][227910] Min Reward on eval: 3034.3880696422652[0m
[37m[1m[2023-07-10 13:01:48,978][227910] Mean Reward across all agents: 3034.3880696422652[0m
[37m[1m[2023-07-10 13:01:48,978][227910] Average Trajectory Length: 981.9119999999999[0m
[36m[2023-07-10 13:01:54,554][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:01:54,555][227910] Reward + Measures: [[1813.60899887    0.3021        0.30430004    0.24679999    0.22490001]
 [2188.57876369    0.40169999    0.38330001    0.30019999    0.28040001]
 [2328.33228391    0.35881534    0.26826689    0.31281289    0.26475891]
 ...
 [2445.3646843     0.41020003    0.38859999    0.2868        0.252     ]
 [2118.54872996    0.35910001    0.39389998    0.29890001    0.28889999]
 [1845.34436862    0.44749999    0.42940003    0.33960003    0.32020003]][0m
[37m[1m[2023-07-10 13:01:54,555][227910] Max Reward on eval: 3181.5577168996447[0m
[37m[1m[2023-07-10 13:01:54,555][227910] Min Reward on eval: 118.43478101793444[0m
[37m[1m[2023-07-10 13:01:54,556][227910] Mean Reward across all agents: 1921.2035034124235[0m
[37m[1m[2023-07-10 13:01:54,556][227910] Average Trajectory Length: 992.937[0m
[36m[2023-07-10 13:01:54,559][227910] mean_value=-353.8933699493433, max_value=906.9905953197253[0m
[37m[1m[2023-07-10 13:01:54,562][227910] New mean coefficients: [[0.36877024 2.6099813  0.44009626 3.4548297  3.8727074 ]][0m
[37m[1m[2023-07-10 13:01:54,563][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:02:04,218][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 13:02:04,219][227910] FPS: 397765.66[0m
[36m[2023-07-10 13:02:04,221][227910] itr=443, itrs=2000, Progress: 22.15%[0m
[36m[2023-07-10 13:02:15,664][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 13:02:15,664][227910] FPS: 336032.83[0m
[36m[2023-07-10 13:02:20,533][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:02:20,533][227910] Reward + Measures: [[3058.09817699    0.36548039    0.29224038    0.27916846    0.22858585]][0m
[37m[1m[2023-07-10 13:02:20,534][227910] Max Reward on eval: 3058.098176993899[0m
[37m[1m[2023-07-10 13:02:20,534][227910] Min Reward on eval: 3058.098176993899[0m
[37m[1m[2023-07-10 13:02:20,534][227910] Mean Reward across all agents: 3058.098176993899[0m
[37m[1m[2023-07-10 13:02:20,534][227910] Average Trajectory Length: 986.824[0m
[36m[2023-07-10 13:02:25,862][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:02:25,862][227910] Reward + Measures: [[ 898.34773751    0.60869998    0.58829999    0.54750001    0.53800005]
 [2442.41223469    0.3863        0.36350003    0.2775        0.24450003]
 [1500.70731436    0.5126        0.4488        0.4172        0.37990004]
 ...
 [1738.5607676     0.4941        0.4567        0.46169996    0.4032    ]
 [2462.79487156    0.3998        0.35560003    0.29459998    0.26949999]
 [1881.1267068     0.43929997    0.43310004    0.3511        0.34390002]][0m
[37m[1m[2023-07-10 13:02:25,863][227910] Max Reward on eval: 3054.304736623366[0m
[37m[1m[2023-07-10 13:02:25,863][227910] Min Reward on eval: -402.5960260937689[0m
[37m[1m[2023-07-10 13:02:25,863][227910] Mean Reward across all agents: 1763.1066194723846[0m
[37m[1m[2023-07-10 13:02:25,863][227910] Average Trajectory Length: 986.9083333333333[0m
[36m[2023-07-10 13:02:25,868][227910] mean_value=-129.66675383942803, max_value=914.9464373941414[0m
[37m[1m[2023-07-10 13:02:25,870][227910] New mean coefficients: [[0.10889748 3.1452975  0.7721163  3.795416   4.560482  ]][0m
[37m[1m[2023-07-10 13:02:25,871][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:02:35,441][227910] train() took 9.57 seconds to complete[0m
[36m[2023-07-10 13:02:35,441][227910] FPS: 401342.12[0m
[36m[2023-07-10 13:02:35,443][227910] itr=444, itrs=2000, Progress: 22.20%[0m
[36m[2023-07-10 13:02:47,139][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 13:02:47,140][227910] FPS: 328823.58[0m
[36m[2023-07-10 13:02:51,946][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:02:51,946][227910] Reward + Measures: [[3022.34709444    0.37468919    0.30259013    0.28118199    0.23716003]][0m
[37m[1m[2023-07-10 13:02:51,946][227910] Max Reward on eval: 3022.3470944441337[0m
[37m[1m[2023-07-10 13:02:51,947][227910] Min Reward on eval: 3022.3470944441337[0m
[37m[1m[2023-07-10 13:02:51,947][227910] Mean Reward across all agents: 3022.3470944441337[0m
[37m[1m[2023-07-10 13:02:51,947][227910] Average Trajectory Length: 993.3576666666667[0m
[36m[2023-07-10 13:02:57,495][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:02:57,496][227910] Reward + Measures: [[ 771.2494404     0.6857        0.61329997    0.57670003    0.5618    ]
 [ 567.45021902    0.66989994    0.56010002    0.5855        0.59360003]
 [1301.0517243     0.58700001    0.54460001    0.47839999    0.47620001]
 ...
 [ 829.86502239    0.6512        0.54170007    0.57160002    0.54260004]
 [1635.74981028    0.47839999    0.30270001    0.43330002    0.41339999]
 [2327.51029275    0.4269        0.33340001    0.30429998    0.27580002]][0m
[37m[1m[2023-07-10 13:02:57,496][227910] Max Reward on eval: 2774.6717031341977[0m
[37m[1m[2023-07-10 13:02:57,496][227910] Min Reward on eval: 219.88947331975214[0m
[37m[1m[2023-07-10 13:02:57,497][227910] Mean Reward across all agents: 1243.683241604246[0m
[37m[1m[2023-07-10 13:02:57,497][227910] Average Trajectory Length: 993.2083333333333[0m
[36m[2023-07-10 13:02:57,502][227910] mean_value=136.08093665736524, max_value=1847.9240011365848[0m
[37m[1m[2023-07-10 13:02:57,505][227910] New mean coefficients: [[0.20662317 3.8969083  0.01088971 4.1748376  4.1745315 ]][0m
[37m[1m[2023-07-10 13:02:57,506][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:03:07,311][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 13:03:07,311][227910] FPS: 391704.95[0m
[36m[2023-07-10 13:03:07,313][227910] itr=445, itrs=2000, Progress: 22.25%[0m
[36m[2023-07-10 13:03:18,922][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 13:03:18,923][227910] FPS: 331250.70[0m
[36m[2023-07-10 13:03:23,738][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:03:23,738][227910] Reward + Measures: [[2979.32812352    0.39137045    0.30651447    0.28963649    0.24801329]][0m
[37m[1m[2023-07-10 13:03:23,739][227910] Max Reward on eval: 2979.3281235226805[0m
[37m[1m[2023-07-10 13:03:23,739][227910] Min Reward on eval: 2979.3281235226805[0m
[37m[1m[2023-07-10 13:03:23,739][227910] Mean Reward across all agents: 2979.3281235226805[0m
[37m[1m[2023-07-10 13:03:23,739][227910] Average Trajectory Length: 994.453[0m
[36m[2023-07-10 13:03:29,275][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:03:29,276][227910] Reward + Measures: [[ 191.72655643    0.83789998    0.77069998    0.8028        0.76389998]
 [1441.62483609    0.56639999    0.48979998    0.458         0.43970004]
 [ 262.95893978    0.80120003    0.70220006    0.76279998    0.76109999]
 ...
 [2070.8097056     0.51170003    0.4224        0.37360001    0.34250003]
 [1023.82842878    0.5341        0.50410002    0.44770002    0.4756    ]
 [  75.73765202    0.83879995    0.68500006    0.80389994    0.77950007]][0m
[37m[1m[2023-07-10 13:03:29,276][227910] Max Reward on eval: 2737.19296765679[0m
[37m[1m[2023-07-10 13:03:29,276][227910] Min Reward on eval: -142.40113886160543[0m
[37m[1m[2023-07-10 13:03:29,276][227910] Mean Reward across all agents: 1012.4568568560488[0m
[37m[1m[2023-07-10 13:03:29,277][227910] Average Trajectory Length: 997.1006666666666[0m
[36m[2023-07-10 13:03:29,281][227910] mean_value=35.52870279938367, max_value=876.2035283262546[0m
[37m[1m[2023-07-10 13:03:29,284][227910] New mean coefficients: [[ 0.4829206  3.4809673 -0.514497   4.3720937  3.2538402]][0m
[37m[1m[2023-07-10 13:03:29,285][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:03:38,908][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 13:03:38,908][227910] FPS: 399102.65[0m
[36m[2023-07-10 13:03:38,911][227910] itr=446, itrs=2000, Progress: 22.30%[0m
[36m[2023-07-10 13:03:50,340][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 13:03:50,341][227910] FPS: 336501.85[0m
[36m[2023-07-10 13:03:55,099][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:03:55,099][227910] Reward + Measures: [[2989.08316098    0.40331486    0.29779541    0.29862496    0.24790442]][0m
[37m[1m[2023-07-10 13:03:55,100][227910] Max Reward on eval: 2989.0831609814986[0m
[37m[1m[2023-07-10 13:03:55,100][227910] Min Reward on eval: 2989.0831609814986[0m
[37m[1m[2023-07-10 13:03:55,100][227910] Mean Reward across all agents: 2989.0831609814986[0m
[37m[1m[2023-07-10 13:03:55,100][227910] Average Trajectory Length: 994.218[0m
[36m[2023-07-10 13:04:00,638][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:04:00,644][227910] Reward + Measures: [[2287.68919629    0.4454        0.33909997    0.34230003    0.30389997]
 [2260.82469178    0.44579998    0.38540003    0.31080002    0.28220001]
 [2038.92062392    0.4835        0.36090001    0.37240002    0.33189997]
 ...
 [2470.38100143    0.43390003    0.35479999    0.28770003    0.271     ]
 [1330.44745567    0.56059998    0.54400003    0.46420002    0.4858    ]
 [2024.11935125    0.55430001    0.2951        0.4619        0.41389999]][0m
[37m[1m[2023-07-10 13:04:00,644][227910] Max Reward on eval: 3052.0666575488867[0m
[37m[1m[2023-07-10 13:04:00,644][227910] Min Reward on eval: 583.4014481989376[0m
[37m[1m[2023-07-10 13:04:00,645][227910] Mean Reward across all agents: 2038.9593268769627[0m
[37m[1m[2023-07-10 13:04:00,645][227910] Average Trajectory Length: 992.7936666666666[0m
[36m[2023-07-10 13:04:00,650][227910] mean_value=61.070210330461755, max_value=2359.998035416892[0m
[37m[1m[2023-07-10 13:04:00,653][227910] New mean coefficients: [[ 0.7845074  3.4183805 -0.7104077  4.223057   2.7924964]][0m
[37m[1m[2023-07-10 13:04:00,653][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:04:10,233][227910] train() took 9.58 seconds to complete[0m
[36m[2023-07-10 13:04:10,233][227910] FPS: 400942.72[0m
[36m[2023-07-10 13:04:10,235][227910] itr=447, itrs=2000, Progress: 22.35%[0m
[36m[2023-07-10 13:04:21,788][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 13:04:21,788][227910] FPS: 332843.48[0m
[36m[2023-07-10 13:04:26,582][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:04:26,587][227910] Reward + Measures: [[3135.48849736    0.410375      0.28441936    0.29904842    0.23965421]][0m
[37m[1m[2023-07-10 13:04:26,587][227910] Max Reward on eval: 3135.4884973593694[0m
[37m[1m[2023-07-10 13:04:26,588][227910] Min Reward on eval: 3135.4884973593694[0m
[37m[1m[2023-07-10 13:04:26,588][227910] Mean Reward across all agents: 3135.4884973593694[0m
[37m[1m[2023-07-10 13:04:26,588][227910] Average Trajectory Length: 993.7816666666666[0m
[36m[2023-07-10 13:04:32,094][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:04:32,095][227910] Reward + Measures: [[2060.05402276    0.53380001    0.41210005    0.40450001    0.36109999]
 [1889.3005494     0.51770002    0.43220004    0.38149998    0.36560002]
 [1226.03382346    0.6257        0.54879999    0.53960001    0.50100005]
 ...
 [1989.41116938    0.54829997    0.45960003    0.4395        0.39490002]
 [ -24.00583167    0.91720003    0.87140006    0.89110005    0.87720007]
 [ 816.66634096    0.76120001    0.67869997    0.70839995    0.66890001]][0m
[37m[1m[2023-07-10 13:04:32,095][227910] Max Reward on eval: 3211.483485554275[0m
[37m[1m[2023-07-10 13:04:32,095][227910] Min Reward on eval: -239.8822900508996[0m
[37m[1m[2023-07-10 13:04:32,095][227910] Mean Reward across all agents: 1075.0301451030787[0m
[37m[1m[2023-07-10 13:04:32,095][227910] Average Trajectory Length: 998.8653333333333[0m
[36m[2023-07-10 13:04:32,102][227910] mean_value=119.20212738109224, max_value=1802.0732554431786[0m
[37m[1m[2023-07-10 13:04:32,104][227910] New mean coefficients: [[ 0.8094395   2.8252406  -0.49719894  3.324075    2.000269  ]][0m
[37m[1m[2023-07-10 13:04:32,106][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:04:41,846][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 13:04:41,846][227910] FPS: 394309.28[0m
[36m[2023-07-10 13:04:41,849][227910] itr=448, itrs=2000, Progress: 22.40%[0m
[36m[2023-07-10 13:04:53,276][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 13:04:53,276][227910] FPS: 336511.49[0m
[36m[2023-07-10 13:04:58,097][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:04:58,098][227910] Reward + Measures: [[3198.19142935    0.41213378    0.27187577    0.30432162    0.24015751]][0m
[37m[1m[2023-07-10 13:04:58,098][227910] Max Reward on eval: 3198.1914293464806[0m
[37m[1m[2023-07-10 13:04:58,098][227910] Min Reward on eval: 3198.1914293464806[0m
[37m[1m[2023-07-10 13:04:58,098][227910] Mean Reward across all agents: 3198.1914293464806[0m
[37m[1m[2023-07-10 13:04:58,098][227910] Average Trajectory Length: 990.2043333333334[0m
[36m[2023-07-10 13:05:03,644][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:05:03,645][227910] Reward + Measures: [[ 390.90970507    0.32111636    0.52792311    0.36230618    0.50245744]
 [1775.7325112     0.33427656    0.26258996    0.27807948    0.21660586]
 [ 994.75146196    0.36210001    0.3716        0.34009999    0.34260002]
 ...
 [ 303.75584487    0.36776042    0.30794507    0.33228022    0.28468791]
 [ 697.30140298    0.35054779    0.31409404    0.30265525    0.27403581]
 [2633.13444157    0.42649999    0.2658        0.33110002    0.28889999]][0m
[37m[1m[2023-07-10 13:05:03,645][227910] Max Reward on eval: 3262.7420542406385[0m
[37m[1m[2023-07-10 13:05:03,646][227910] Min Reward on eval: -331.37522355834955[0m
[37m[1m[2023-07-10 13:05:03,646][227910] Mean Reward across all agents: 1385.2192754330035[0m
[37m[1m[2023-07-10 13:05:03,646][227910] Average Trajectory Length: 976.362[0m
[36m[2023-07-10 13:05:03,648][227910] mean_value=-761.0581879831146, max_value=1143.5385892266265[0m
[37m[1m[2023-07-10 13:05:03,651][227910] New mean coefficients: [[1.0261704 2.5742862 0.2940377 3.7364886 2.0049355]][0m
[37m[1m[2023-07-10 13:05:03,652][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:05:13,356][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 13:05:13,356][227910] FPS: 395778.24[0m
[36m[2023-07-10 13:05:13,358][227910] itr=449, itrs=2000, Progress: 22.45%[0m
[36m[2023-07-10 13:05:24,827][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 13:05:24,827][227910] FPS: 335290.98[0m
[36m[2023-07-10 13:05:29,615][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:05:29,615][227910] Reward + Measures: [[3378.67181073    0.40875825    0.26555523    0.29690427    0.22940169]][0m
[37m[1m[2023-07-10 13:05:29,615][227910] Max Reward on eval: 3378.671810733821[0m
[37m[1m[2023-07-10 13:05:29,616][227910] Min Reward on eval: 3378.671810733821[0m
[37m[1m[2023-07-10 13:05:29,616][227910] Mean Reward across all agents: 3378.671810733821[0m
[37m[1m[2023-07-10 13:05:29,616][227910] Average Trajectory Length: 989.603[0m
[36m[2023-07-10 13:05:35,061][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:05:35,062][227910] Reward + Measures: [[1370.3433098     0.4941        0.39340001    0.43590003    0.37670001]
 [2672.37085942    0.46940002    0.29720002    0.36180001    0.30519998]
 [3083.20187125    0.40349999    0.2843        0.28130001    0.22979999]
 ...
 [1611.77429226    0.52740002    0.34260002    0.42490003    0.38990003]
 [1017.15172566    0.55672199    0.40610549    0.49204069    0.42356262]
 [2256.65875778    0.42350003    0.345         0.32430002    0.27959999]][0m
[37m[1m[2023-07-10 13:05:35,062][227910] Max Reward on eval: 3515.519812005013[0m
[37m[1m[2023-07-10 13:05:35,062][227910] Min Reward on eval: -447.6670972499531[0m
[37m[1m[2023-07-10 13:05:35,063][227910] Mean Reward across all agents: 1819.7424366040725[0m
[37m[1m[2023-07-10 13:05:35,063][227910] Average Trajectory Length: 995.3876666666666[0m
[36m[2023-07-10 13:05:35,066][227910] mean_value=-201.06014093304364, max_value=1487.9317559304745[0m
[37m[1m[2023-07-10 13:05:35,068][227910] New mean coefficients: [[0.9395019  3.1157484  0.39967138 3.700048   1.4056567 ]][0m
[37m[1m[2023-07-10 13:05:35,069][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:05:45,009][227910] train() took 9.94 seconds to complete[0m
[36m[2023-07-10 13:05:45,009][227910] FPS: 386414.55[0m
[36m[2023-07-10 13:05:45,011][227910] itr=450, itrs=2000, Progress: 22.50%[0m
[37m[1m[2023-07-10 13:05:47,701][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000430[0m
[36m[2023-07-10 13:05:59,552][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 13:05:59,552][227910] FPS: 331508.92[0m
[36m[2023-07-10 13:06:04,394][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:06:04,395][227910] Reward + Measures: [[3493.91907384    0.41116235    0.26520836    0.29242098    0.21779528]][0m
[37m[1m[2023-07-10 13:06:04,395][227910] Max Reward on eval: 3493.9190738395278[0m
[37m[1m[2023-07-10 13:06:04,395][227910] Min Reward on eval: 3493.9190738395278[0m
[37m[1m[2023-07-10 13:06:04,395][227910] Mean Reward across all agents: 3493.9190738395278[0m
[37m[1m[2023-07-10 13:06:04,396][227910] Average Trajectory Length: 989.366[0m
[36m[2023-07-10 13:06:10,034][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:06:10,034][227910] Reward + Measures: [[1471.11290501    0.4786        0.43000004    0.39200002    0.37259999]
 [2881.87473215    0.44159523    0.25883061    0.35483199    0.26616734]
 [3101.750461      0.43920001    0.29270002    0.32290003    0.26890001]
 ...
 [2494.09127251    0.42050001    0.35929999    0.3091        0.27469999]
 [1955.54771296    0.54710001    0.40090004    0.47019997    0.43099999]
 [3072.52115131    0.40559998    0.29879999    0.33470002    0.24069999]][0m
[37m[1m[2023-07-10 13:06:10,035][227910] Max Reward on eval: 3599.381098730629[0m
[37m[1m[2023-07-10 13:06:10,035][227910] Min Reward on eval: 106.64748035178054[0m
[37m[1m[2023-07-10 13:06:10,035][227910] Mean Reward across all agents: 2068.469148440223[0m
[37m[1m[2023-07-10 13:06:10,035][227910] Average Trajectory Length: 994.5923333333333[0m
[36m[2023-07-10 13:06:10,040][227910] mean_value=-42.39484597949895, max_value=1664.6910532604963[0m
[37m[1m[2023-07-10 13:06:10,042][227910] New mean coefficients: [[1.0094155  2.4523034  0.74721074 3.3164177  0.6725156 ]][0m
[37m[1m[2023-07-10 13:06:10,043][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:06:19,965][227910] train() took 9.92 seconds to complete[0m
[36m[2023-07-10 13:06:19,965][227910] FPS: 387100.54[0m
[36m[2023-07-10 13:06:19,968][227910] itr=451, itrs=2000, Progress: 22.55%[0m
[36m[2023-07-10 13:06:31,559][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 13:06:31,559][227910] FPS: 331771.99[0m
[36m[2023-07-10 13:06:36,252][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:06:36,252][227910] Reward + Measures: [[3631.45438473    0.41007882    0.25063857    0.29404604    0.20778523]][0m
[37m[1m[2023-07-10 13:06:36,253][227910] Max Reward on eval: 3631.4543847334858[0m
[37m[1m[2023-07-10 13:06:36,253][227910] Min Reward on eval: 3631.4543847334858[0m
[37m[1m[2023-07-10 13:06:36,253][227910] Mean Reward across all agents: 3631.4543847334858[0m
[37m[1m[2023-07-10 13:06:36,254][227910] Average Trajectory Length: 986.684[0m
[36m[2023-07-10 13:06:41,684][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:06:41,685][227910] Reward + Measures: [[1936.69880493    0.5399        0.45389995    0.4285        0.3827    ]
 [2481.92491431    0.30519363    0.24149077    0.23377307    0.16564894]
 [2912.38906157    0.3946        0.2592566     0.30143771    0.21659055]
 ...
 [ 350.96208925    0.87710011    0.78380001    0.85109997    0.75949997]
 [ 503.54418478    0.75480002    0.73610002    0.56050009    0.63159996]
 [1894.95740264    0.34150001    0.3836        0.289         0.22389999]][0m
[37m[1m[2023-07-10 13:06:41,685][227910] Max Reward on eval: 3764.042511489382[0m
[37m[1m[2023-07-10 13:06:41,685][227910] Min Reward on eval: 119.12823722935573[0m
[37m[1m[2023-07-10 13:06:41,685][227910] Mean Reward across all agents: 1832.8178302529416[0m
[37m[1m[2023-07-10 13:06:41,685][227910] Average Trajectory Length: 985.8103333333333[0m
[36m[2023-07-10 13:06:41,689][227910] mean_value=-296.77852545556743, max_value=2123.004630401668[0m
[37m[1m[2023-07-10 13:06:41,692][227910] New mean coefficients: [[0.77238417 3.587411   0.9326762  3.4419787  1.7330368 ]][0m
[37m[1m[2023-07-10 13:06:41,693][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:06:51,328][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 13:06:51,328][227910] FPS: 398613.34[0m
[36m[2023-07-10 13:06:51,331][227910] itr=452, itrs=2000, Progress: 22.60%[0m
[36m[2023-07-10 13:07:02,986][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 13:07:02,986][227910] FPS: 329948.96[0m
[36m[2023-07-10 13:07:07,852][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:07:07,852][227910] Reward + Measures: [[3762.28612883    0.42067301    0.25932962    0.28633735    0.20642263]][0m
[37m[1m[2023-07-10 13:07:07,852][227910] Max Reward on eval: 3762.286128827789[0m
[37m[1m[2023-07-10 13:07:07,853][227910] Min Reward on eval: 3762.286128827789[0m
[37m[1m[2023-07-10 13:07:07,853][227910] Mean Reward across all agents: 3762.286128827789[0m
[37m[1m[2023-07-10 13:07:07,853][227910] Average Trajectory Length: 989.9399999999999[0m
[36m[2023-07-10 13:07:13,328][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:07:13,329][227910] Reward + Measures: [[3317.55418892    0.4029386     0.27160317    0.28477797    0.18007952]
 [3514.91433888    0.36699998    0.23909998    0.27040002    0.16410001]
 [2869.51920872    0.47779998    0.3303        0.34260002    0.26609999]
 ...
 [2375.52224278    0.43829998    0.29770002    0.3274        0.2458    ]
 [3133.08478566    0.43880001    0.30000001    0.31539997    0.20900002]
 [3169.32637308    0.42160001    0.27599999    0.2701        0.1963    ]][0m
[37m[1m[2023-07-10 13:07:13,329][227910] Max Reward on eval: 3809.741197144659[0m
[37m[1m[2023-07-10 13:07:13,329][227910] Min Reward on eval: 363.3727714078501[0m
[37m[1m[2023-07-10 13:07:13,329][227910] Mean Reward across all agents: 2552.880895399628[0m
[37m[1m[2023-07-10 13:07:13,330][227910] Average Trajectory Length: 993.2173333333333[0m
[36m[2023-07-10 13:07:13,334][227910] mean_value=235.41067285389096, max_value=3118.056437588311[0m
[37m[1m[2023-07-10 13:07:13,337][227910] New mean coefficients: [[1.0095881 3.2362964 0.6403036 3.1829767 0.4441396]][0m
[37m[1m[2023-07-10 13:07:13,338][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:07:23,156][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 13:07:23,156][227910] FPS: 391183.05[0m
[36m[2023-07-10 13:07:23,159][227910] itr=453, itrs=2000, Progress: 22.65%[0m
[36m[2023-07-10 13:07:34,847][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 13:07:34,847][227910] FPS: 329049.59[0m
[36m[2023-07-10 13:07:39,606][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:07:39,607][227910] Reward + Measures: [[3892.34863234    0.42149633    0.25631723    0.28250781    0.2007443 ]][0m
[37m[1m[2023-07-10 13:07:39,607][227910] Max Reward on eval: 3892.3486323358106[0m
[37m[1m[2023-07-10 13:07:39,607][227910] Min Reward on eval: 3892.3486323358106[0m
[37m[1m[2023-07-10 13:07:39,607][227910] Mean Reward across all agents: 3892.3486323358106[0m
[37m[1m[2023-07-10 13:07:39,607][227910] Average Trajectory Length: 994.0246666666667[0m
[36m[2023-07-10 13:07:45,106][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:07:45,107][227910] Reward + Measures: [[2530.07498544    0.52043325    0.40328604    0.39785942    0.30815065]
 [ 478.1187679     0.82450002    0.74669999    0.81749994    0.74910003]
 [2170.96771704    0.49530002    0.38600001    0.39499998    0.32570001]
 ...
 [1460.6965989     0.67500001    0.59779996    0.61849993    0.5697    ]
 [2761.34105631    0.40760002    0.28819999    0.31540003    0.2323    ]
 [  94.47289369    0.91740006    0.86660004    0.90990001    0.87239993]][0m
[37m[1m[2023-07-10 13:07:45,107][227910] Max Reward on eval: 3979.3069929707794[0m
[37m[1m[2023-07-10 13:07:45,107][227910] Min Reward on eval: -11.40727646254236[0m
[37m[1m[2023-07-10 13:07:45,107][227910] Mean Reward across all agents: 2073.2740483093417[0m
[37m[1m[2023-07-10 13:07:45,108][227910] Average Trajectory Length: 995.134[0m
[36m[2023-07-10 13:07:45,113][227910] mean_value=88.68225383725496, max_value=1607.1597771025529[0m
[37m[1m[2023-07-10 13:07:45,116][227910] New mean coefficients: [[0.6334165  3.1639717  0.80084157 2.9681656  0.7946329 ]][0m
[37m[1m[2023-07-10 13:07:45,117][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:07:54,722][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 13:07:54,723][227910] FPS: 399821.79[0m
[36m[2023-07-10 13:07:54,725][227910] itr=454, itrs=2000, Progress: 22.70%[0m
[36m[2023-07-10 13:08:06,378][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 13:08:06,378][227910] FPS: 330008.82[0m
[36m[2023-07-10 13:08:11,182][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:08:11,182][227910] Reward + Measures: [[4009.01162106    0.42586899    0.25099868    0.28402501    0.19618021]][0m
[37m[1m[2023-07-10 13:08:11,183][227910] Max Reward on eval: 4009.0116210614888[0m
[37m[1m[2023-07-10 13:08:11,183][227910] Min Reward on eval: 4009.0116210614888[0m
[37m[1m[2023-07-10 13:08:11,183][227910] Mean Reward across all agents: 4009.0116210614888[0m
[37m[1m[2023-07-10 13:08:11,183][227910] Average Trajectory Length: 993.15[0m
[36m[2023-07-10 13:08:16,778][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:08:16,779][227910] Reward + Measures: [[1714.08139562    0.46875024    0.31924725    0.37077138    0.30324626]
 [2997.15414484    0.44572973    0.31426761    0.3222892     0.22994594]
 [2954.90445959    0.42659998    0.29010001    0.29409999    0.21160002]
 ...
 [3240.65746199    0.45577726    0.31842273    0.33036366    0.24014091]
 [2784.06799118    0.47940001    0.34150001    0.3405        0.27560002]
 [3614.97589192    0.42969999    0.26320001    0.32370001    0.2172    ]][0m
[37m[1m[2023-07-10 13:08:16,779][227910] Max Reward on eval: 4219.755762325786[0m
[37m[1m[2023-07-10 13:08:16,779][227910] Min Reward on eval: 1011.9083976547815[0m
[37m[1m[2023-07-10 13:08:16,779][227910] Mean Reward across all agents: 2907.873877621528[0m
[37m[1m[2023-07-10 13:08:16,780][227910] Average Trajectory Length: 984.5046666666666[0m
[36m[2023-07-10 13:08:16,782][227910] mean_value=-431.6821876366345, max_value=3724.9910720380954[0m
[37m[1m[2023-07-10 13:08:16,784][227910] New mean coefficients: [[0.4779687  1.9306569  0.05266607 2.104423   0.49577177]][0m
[37m[1m[2023-07-10 13:08:16,785][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:08:26,643][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 13:08:26,643][227910] FPS: 389627.66[0m
[36m[2023-07-10 13:08:26,645][227910] itr=455, itrs=2000, Progress: 22.75%[0m
[36m[2023-07-10 13:08:38,335][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 13:08:38,335][227910] FPS: 329015.25[0m
[36m[2023-07-10 13:08:43,207][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:08:43,207][227910] Reward + Measures: [[4091.62944429    0.42485088    0.24400303    0.28259894    0.1936519 ]][0m
[37m[1m[2023-07-10 13:08:43,208][227910] Max Reward on eval: 4091.6294442852873[0m
[37m[1m[2023-07-10 13:08:43,208][227910] Min Reward on eval: 4091.6294442852873[0m
[37m[1m[2023-07-10 13:08:43,208][227910] Mean Reward across all agents: 4091.6294442852873[0m
[37m[1m[2023-07-10 13:08:43,208][227910] Average Trajectory Length: 992.1963333333333[0m
[36m[2023-07-10 13:08:48,697][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:08:48,697][227910] Reward + Measures: [[3579.0677847     0.45623937    0.30339798    0.29698992    0.21680203]
 [ 676.04368212    0.64429998    0.44119999    0.61289996    0.54630005]
 [1965.03490633    0.43930003    0.44150001    0.37710002    0.34120002]
 ...
 [2009.15056039    0.44499999    0.42399999    0.36269999    0.34049997]
 [3332.31636281    0.42319998    0.31099999    0.26710001    0.23310001]
 [3454.46121458    0.44099998    0.27040002    0.28690001    0.2309    ]][0m
[37m[1m[2023-07-10 13:08:48,698][227910] Max Reward on eval: 4397.528525117552[0m
[37m[1m[2023-07-10 13:08:48,698][227910] Min Reward on eval: 170.74343776998575[0m
[37m[1m[2023-07-10 13:08:48,698][227910] Mean Reward across all agents: 2797.2987236791046[0m
[37m[1m[2023-07-10 13:08:48,698][227910] Average Trajectory Length: 993.3366666666666[0m
[36m[2023-07-10 13:08:48,702][227910] mean_value=-77.9358886603056, max_value=3834.4991157276827[0m
[37m[1m[2023-07-10 13:08:48,705][227910] New mean coefficients: [[ 0.43647015  0.46746898 -0.3733168   0.7893622  -0.26336056]][0m
[37m[1m[2023-07-10 13:08:48,706][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:08:58,452][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 13:08:58,453][227910] FPS: 394046.08[0m
[36m[2023-07-10 13:08:58,455][227910] itr=456, itrs=2000, Progress: 22.80%[0m
[36m[2023-07-10 13:09:10,017][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 13:09:10,018][227910] FPS: 332555.13[0m
[36m[2023-07-10 13:09:14,836][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:09:14,837][227910] Reward + Measures: [[4179.32250739    0.42063355    0.24260332    0.27890787    0.18297885]][0m
[37m[1m[2023-07-10 13:09:14,837][227910] Max Reward on eval: 4179.3225073898575[0m
[37m[1m[2023-07-10 13:09:14,837][227910] Min Reward on eval: 4179.3225073898575[0m
[37m[1m[2023-07-10 13:09:14,838][227910] Mean Reward across all agents: 4179.3225073898575[0m
[37m[1m[2023-07-10 13:09:14,838][227910] Average Trajectory Length: 993.8403333333333[0m
[36m[2023-07-10 13:09:20,258][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:09:20,259][227910] Reward + Measures: [[3787.69071396    0.38049999    0.23969999    0.26480001    0.16530001]
 [3837.66621406    0.43670002    0.26339999    0.2656        0.1971    ]
 [2754.68641168    0.43039998    0.25740001    0.32290003    0.25349998]
 ...
 [4068.99693924    0.4165        0.26530001    0.2757        0.19140001]
 [4039.47990246    0.39110002    0.2626        0.27940002    0.18069999]
 [1397.43524345    0.65820003    0.60820001    0.57980001    0.55580002]][0m
[37m[1m[2023-07-10 13:09:20,259][227910] Max Reward on eval: 4254.205339382612[0m
[37m[1m[2023-07-10 13:09:20,259][227910] Min Reward on eval: 373.9976085780712[0m
[37m[1m[2023-07-10 13:09:20,260][227910] Mean Reward across all agents: 3349.61997808223[0m
[37m[1m[2023-07-10 13:09:20,260][227910] Average Trajectory Length: 986.444[0m
[36m[2023-07-10 13:09:20,263][227910] mean_value=-83.84359578074587, max_value=3939.0650894796145[0m
[37m[1m[2023-07-10 13:09:20,266][227910] New mean coefficients: [[ 0.28703988  0.53735816 -0.5496539   0.23081702 -0.40759635]][0m
[37m[1m[2023-07-10 13:09:20,267][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:09:29,882][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 13:09:29,882][227910] FPS: 399443.32[0m
[36m[2023-07-10 13:09:29,884][227910] itr=457, itrs=2000, Progress: 22.85%[0m
[36m[2023-07-10 13:09:41,327][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 13:09:41,327][227910] FPS: 336030.70[0m
[36m[2023-07-10 13:09:46,016][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:09:46,017][227910] Reward + Measures: [[4312.16159057    0.42097417    0.23470353    0.27836198    0.17426278]][0m
[37m[1m[2023-07-10 13:09:46,017][227910] Max Reward on eval: 4312.161590573812[0m
[37m[1m[2023-07-10 13:09:46,017][227910] Min Reward on eval: 4312.161590573812[0m
[37m[1m[2023-07-10 13:09:46,017][227910] Mean Reward across all agents: 4312.161590573812[0m
[37m[1m[2023-07-10 13:09:46,017][227910] Average Trajectory Length: 992.5623333333333[0m
[36m[2023-07-10 13:09:51,353][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:09:51,354][227910] Reward + Measures: [[ 336.8358596     0.36532989    0.21459441    0.34154126    0.26607829]
 [2617.85527951    0.30899999    0.1761        0.24610002    0.1675    ]
 [ 371.49524612    0.53150362    0.22211854    0.50943923    0.41903177]
 ...
 [4128.73196635    0.40750003    0.2448        0.29679999    0.15949999]
 [1648.83421086    0.53710002    0.36990002    0.43430001    0.35870001]
 [1141.31218179    0.62460005    0.51380002    0.5571        0.49040005]][0m
[37m[1m[2023-07-10 13:09:51,354][227910] Max Reward on eval: 4448.404554353236[0m
[37m[1m[2023-07-10 13:09:51,354][227910] Min Reward on eval: -31.21887783150305[0m
[37m[1m[2023-07-10 13:09:51,354][227910] Mean Reward across all agents: 1756.898293831903[0m
[37m[1m[2023-07-10 13:09:51,355][227910] Average Trajectory Length: 944.6666666666666[0m
[36m[2023-07-10 13:09:51,358][227910] mean_value=-756.9882965970189, max_value=2232.58170788287[0m
[37m[1m[2023-07-10 13:09:51,361][227910] New mean coefficients: [[ 0.5133155   0.12468201 -0.4591542  -0.09441835 -0.22930023]][0m
[37m[1m[2023-07-10 13:09:51,362][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:10:00,987][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 13:10:00,988][227910] FPS: 399009.93[0m
[36m[2023-07-10 13:10:00,990][227910] itr=458, itrs=2000, Progress: 22.90%[0m
[36m[2023-07-10 13:10:12,440][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 13:10:12,441][227910] FPS: 335849.57[0m
[36m[2023-07-10 13:10:17,247][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:10:17,247][227910] Reward + Measures: [[4430.76789137    0.41761628    0.23278345    0.2752355     0.1650061 ]][0m
[37m[1m[2023-07-10 13:10:17,248][227910] Max Reward on eval: 4430.767891371187[0m
[37m[1m[2023-07-10 13:10:17,248][227910] Min Reward on eval: 4430.767891371187[0m
[37m[1m[2023-07-10 13:10:17,248][227910] Mean Reward across all agents: 4430.767891371187[0m
[37m[1m[2023-07-10 13:10:17,248][227910] Average Trajectory Length: 992.6536666666666[0m
[36m[2023-07-10 13:10:22,715][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:10:22,715][227910] Reward + Measures: [[3176.92166506    0.37691697    0.26026398    0.26760632    0.15315129]
 [3940.15367609    0.373         0.22930001    0.2502        0.1573    ]
 [3798.83206031    0.42280003    0.2938        0.30720001    0.1981    ]
 ...
 [2773.76388875    0.38641617    0.29205558    0.2783404     0.20685959]
 [4365.62632966    0.41009998    0.244         0.27079999    0.18179999]
 [4185.53845889    0.41930214    0.24391137    0.27195033    0.17921631]][0m
[37m[1m[2023-07-10 13:10:22,716][227910] Max Reward on eval: 4599.002318184264[0m
[37m[1m[2023-07-10 13:10:22,716][227910] Min Reward on eval: 263.37412719847634[0m
[37m[1m[2023-07-10 13:10:22,716][227910] Mean Reward across all agents: 3690.989027568506[0m
[37m[1m[2023-07-10 13:10:22,716][227910] Average Trajectory Length: 986.7083333333333[0m
[36m[2023-07-10 13:10:22,720][227910] mean_value=-112.61083092551571, max_value=4686.3618512471785[0m
[37m[1m[2023-07-10 13:10:22,722][227910] New mean coefficients: [[ 0.3488919   0.7243125  -0.08524168  0.85552174  0.04343227]][0m
[37m[1m[2023-07-10 13:10:22,723][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:10:32,416][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 13:10:32,417][227910] FPS: 396233.81[0m
[36m[2023-07-10 13:10:32,419][227910] itr=459, itrs=2000, Progress: 22.95%[0m
[36m[2023-07-10 13:10:43,948][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 13:10:43,948][227910] FPS: 333576.81[0m
[36m[2023-07-10 13:10:48,613][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:10:48,613][227910] Reward + Measures: [[4577.60931726    0.40787515    0.22518013    0.27312812    0.15643334]][0m
[37m[1m[2023-07-10 13:10:48,614][227910] Max Reward on eval: 4577.609317264029[0m
[37m[1m[2023-07-10 13:10:48,614][227910] Min Reward on eval: 4577.609317264029[0m
[37m[1m[2023-07-10 13:10:48,614][227910] Mean Reward across all agents: 4577.609317264029[0m
[37m[1m[2023-07-10 13:10:48,614][227910] Average Trajectory Length: 991.7063333333333[0m
[36m[2023-07-10 13:10:54,145][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:10:54,146][227910] Reward + Measures: [[3319.08726094    0.4368        0.34299999    0.29430002    0.2656    ]
 [3801.7590848     0.34480003    0.20290001    0.24510001    0.1122    ]
 [3575.15033734    0.4456        0.31389999    0.28760001    0.23370002]
 ...
 [4337.05539548    0.42930004    0.24389999    0.25509998    0.18730001]
 [ 375.63942168    0.50420541    0.5340963     0.46512839    0.46969739]
 [3459.52274813    0.43849999    0.31479999    0.29260001    0.24240001]][0m
[37m[1m[2023-07-10 13:10:54,146][227910] Max Reward on eval: 4669.410407865257[0m
[37m[1m[2023-07-10 13:10:54,146][227910] Min Reward on eval: -366.1269337019883[0m
[37m[1m[2023-07-10 13:10:54,146][227910] Mean Reward across all agents: 2109.1275237988407[0m
[37m[1m[2023-07-10 13:10:54,147][227910] Average Trajectory Length: 960.8253333333333[0m
[36m[2023-07-10 13:10:54,149][227910] mean_value=-919.4157723301669, max_value=2950.70211097089[0m
[37m[1m[2023-07-10 13:10:54,152][227910] New mean coefficients: [[ 0.45251873  0.72796196 -0.09419027  0.5890155   0.05121087]][0m
[37m[1m[2023-07-10 13:10:54,153][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:11:03,832][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 13:11:03,832][227910] FPS: 396784.67[0m
[36m[2023-07-10 13:11:03,835][227910] itr=460, itrs=2000, Progress: 23.00%[0m
[37m[1m[2023-07-10 13:11:06,522][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000440[0m
[36m[2023-07-10 13:11:18,314][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 13:11:18,314][227910] FPS: 333357.58[0m
[36m[2023-07-10 13:11:23,144][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:11:23,145][227910] Reward + Measures: [[4658.14512261    0.42004564    0.22108313    0.2741906     0.15677054]][0m
[37m[1m[2023-07-10 13:11:23,145][227910] Max Reward on eval: 4658.145122613231[0m
[37m[1m[2023-07-10 13:11:23,145][227910] Min Reward on eval: 4658.145122613231[0m
[37m[1m[2023-07-10 13:11:23,145][227910] Mean Reward across all agents: 4658.145122613231[0m
[37m[1m[2023-07-10 13:11:23,146][227910] Average Trajectory Length: 992.9146666666667[0m
[36m[2023-07-10 13:11:28,635][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:11:28,641][227910] Reward + Measures: [[2945.5297275     0.37590003    0.25419998    0.29099998    0.20009999]
 [1753.2286109     0.435         0.4197        0.37450001    0.31799999]
 [2270.71902741    0.42640001    0.30680001    0.3134        0.23190001]
 ...
 [1524.24650264    0.27822301    0.28258941    0.21435551    0.1775485 ]
 [ 617.31030014    0.3651        0.34140003    0.32140002    0.27420002]
 [ 296.64497975    0.37369999    0.5431        0.3531        0.52539998]][0m
[37m[1m[2023-07-10 13:11:28,642][227910] Max Reward on eval: 4689.901498158183[0m
[37m[1m[2023-07-10 13:11:28,643][227910] Min Reward on eval: -405.78129836304[0m
[37m[1m[2023-07-10 13:11:28,643][227910] Mean Reward across all agents: 2410.500894128307[0m
[37m[1m[2023-07-10 13:11:28,644][227910] Average Trajectory Length: 949.0613333333333[0m
[36m[2023-07-10 13:11:28,649][227910] mean_value=-1026.5318226808586, max_value=3978.109614084808[0m
[37m[1m[2023-07-10 13:11:28,655][227910] New mean coefficients: [[ 0.18585208  0.33348793 -0.11507615  0.14563423  0.24208161]][0m
[37m[1m[2023-07-10 13:11:28,656][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:11:38,312][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 13:11:38,312][227910] FPS: 397774.39[0m
[36m[2023-07-10 13:11:38,315][227910] itr=461, itrs=2000, Progress: 23.05%[0m
[36m[2023-07-10 13:11:49,768][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 13:11:49,768][227910] FPS: 335783.39[0m
[36m[2023-07-10 13:11:54,500][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:11:54,500][227910] Reward + Measures: [[4733.01732654    0.41756666    0.21523555    0.27420643    0.15580446]][0m
[37m[1m[2023-07-10 13:11:54,500][227910] Max Reward on eval: 4733.017326536812[0m
[37m[1m[2023-07-10 13:11:54,501][227910] Min Reward on eval: 4733.017326536812[0m
[37m[1m[2023-07-10 13:11:54,501][227910] Mean Reward across all agents: 4733.017326536812[0m
[37m[1m[2023-07-10 13:11:54,501][227910] Average Trajectory Length: 993.8306666666666[0m
[36m[2023-07-10 13:12:00,061][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:12:00,061][227910] Reward + Measures: [[4636.30795453    0.42719999    0.23010002    0.26830003    0.154     ]
 [4794.74023419    0.42930004    0.2211        0.2692        0.1472    ]
 [4308.86497686    0.42045173    0.21727268    0.2747581     0.16404311]
 ...
 [4942.24443922    0.4258        0.2141        0.27360001    0.15409999]
 [4477.09921036    0.42340001    0.2419        0.26790002    0.177     ]
 [4381.29039678    0.41820002    0.2405        0.26390001    0.1881    ]][0m
[37m[1m[2023-07-10 13:12:00,062][227910] Max Reward on eval: 4961.2625839922575[0m
[37m[1m[2023-07-10 13:12:00,062][227910] Min Reward on eval: 2056.7584809740074[0m
[37m[1m[2023-07-10 13:12:00,062][227910] Mean Reward across all agents: 4234.32944917433[0m
[37m[1m[2023-07-10 13:12:00,062][227910] Average Trajectory Length: 984.1413333333333[0m
[36m[2023-07-10 13:12:00,066][227910] mean_value=201.25909481724236, max_value=3838.0559494854974[0m
[37m[1m[2023-07-10 13:12:00,069][227910] New mean coefficients: [[ 0.11638233  0.97786754  0.02167991 -0.00003585 -0.13280204]][0m
[37m[1m[2023-07-10 13:12:00,070][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:12:09,721][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 13:12:09,721][227910] FPS: 397966.74[0m
[36m[2023-07-10 13:12:09,723][227910] itr=462, itrs=2000, Progress: 23.10%[0m
[36m[2023-07-10 13:12:21,206][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 13:12:21,206][227910] FPS: 334941.99[0m
[36m[2023-07-10 13:12:25,907][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:12:25,907][227910] Reward + Measures: [[4814.15873968    0.42239574    0.20968494    0.27315208    0.14311314]][0m
[37m[1m[2023-07-10 13:12:25,907][227910] Max Reward on eval: 4814.158739681873[0m
[37m[1m[2023-07-10 13:12:25,908][227910] Min Reward on eval: 4814.158739681873[0m
[37m[1m[2023-07-10 13:12:25,908][227910] Mean Reward across all agents: 4814.158739681873[0m
[37m[1m[2023-07-10 13:12:25,908][227910] Average Trajectory Length: 994.0106666666667[0m
[36m[2023-07-10 13:12:31,503][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:12:31,504][227910] Reward + Measures: [[2321.97407102    0.41810003    0.4244        0.38240001    0.3443    ]
 [2725.49401429    0.50060004    0.3567        0.3554        0.27340001]
 [4584.2032456     0.41960001    0.23360001    0.29010001    0.1797    ]
 ...
 [4330.49791766    0.38500002    0.2271        0.26680002    0.14710002]
 [4227.14282658    0.44770455    0.22218697    0.27209413    0.1497492 ]
 [2222.64053346    0.35355705    0.32538247    0.27934778    0.22065496]][0m
[37m[1m[2023-07-10 13:12:31,504][227910] Max Reward on eval: 4913.75195147451[0m
[37m[1m[2023-07-10 13:12:31,504][227910] Min Reward on eval: 295.1427644429059[0m
[37m[1m[2023-07-10 13:12:31,505][227910] Mean Reward across all agents: 3009.3176130552847[0m
[37m[1m[2023-07-10 13:12:31,505][227910] Average Trajectory Length: 929.7339999999999[0m
[36m[2023-07-10 13:12:31,508][227910] mean_value=-542.3809018433038, max_value=5026.440122296661[0m
[37m[1m[2023-07-10 13:12:31,511][227910] New mean coefficients: [[ 0.0655896   0.88948154 -0.02430613  0.01209602  0.11492378]][0m
[37m[1m[2023-07-10 13:12:31,512][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:12:41,198][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 13:12:41,198][227910] FPS: 396518.89[0m
[36m[2023-07-10 13:12:41,200][227910] itr=463, itrs=2000, Progress: 23.15%[0m
[36m[2023-07-10 13:12:52,673][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 13:12:52,673][227910] FPS: 335209.52[0m
[36m[2023-07-10 13:12:57,528][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:12:57,528][227910] Reward + Measures: [[4867.24399752    0.43829304    0.20682116    0.27296072    0.14570959]][0m
[37m[1m[2023-07-10 13:12:57,529][227910] Max Reward on eval: 4867.243997522081[0m
[37m[1m[2023-07-10 13:12:57,529][227910] Min Reward on eval: 4867.243997522081[0m
[37m[1m[2023-07-10 13:12:57,529][227910] Mean Reward across all agents: 4867.243997522081[0m
[37m[1m[2023-07-10 13:12:57,530][227910] Average Trajectory Length: 994.4839999999999[0m
[36m[2023-07-10 13:13:03,003][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:13:03,003][227910] Reward + Measures: [[3589.21891083    0.40089998    0.25910002    0.27400002    0.16410001]
 [3445.95099943    0.39560002    0.23719998    0.23669998    0.16670001]
 [3752.62341644    0.3969        0.24370001    0.26800001    0.1683    ]
 ...
 [3370.51279981    0.38756257    0.25656638    0.27199242    0.17362891]
 [1430.88441431    0.30281672    0.33587056    0.26802832    0.23288821]
 [3603.54509757    0.41160002    0.1928        0.2861        0.18880001]][0m
[37m[1m[2023-07-10 13:13:03,004][227910] Max Reward on eval: 4856.634269296005[0m
[37m[1m[2023-07-10 13:13:03,004][227910] Min Reward on eval: 269.80843581373335[0m
[37m[1m[2023-07-10 13:13:03,004][227910] Mean Reward across all agents: 2829.518247219436[0m
[37m[1m[2023-07-10 13:13:03,004][227910] Average Trajectory Length: 935.6569999999999[0m
[36m[2023-07-10 13:13:03,007][227910] mean_value=-755.3353599435783, max_value=4216.0200218852315[0m
[37m[1m[2023-07-10 13:13:03,009][227910] New mean coefficients: [[0.16345686 1.0022374  0.15261129 0.10596199 0.11145675]][0m
[37m[1m[2023-07-10 13:13:03,010][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:13:12,713][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 13:13:12,713][227910] FPS: 395825.91[0m
[36m[2023-07-10 13:13:12,715][227910] itr=464, itrs=2000, Progress: 23.20%[0m
[36m[2023-07-10 13:13:24,191][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 13:13:24,191][227910] FPS: 335082.88[0m
[36m[2023-07-10 13:13:28,916][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:13:28,916][227910] Reward + Measures: [[2196.92361412    0.44177079    0.24000591    0.40608436    0.34567252]][0m
[37m[1m[2023-07-10 13:13:28,916][227910] Max Reward on eval: 2196.923614124333[0m
[37m[1m[2023-07-10 13:13:28,917][227910] Min Reward on eval: 2196.923614124333[0m
[37m[1m[2023-07-10 13:13:28,917][227910] Mean Reward across all agents: 2196.923614124333[0m
[37m[1m[2023-07-10 13:13:28,917][227910] Average Trajectory Length: 984.4996666666666[0m
[36m[2023-07-10 13:13:34,397][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:13:34,398][227910] Reward + Measures: [[2445.55583596    0.29790002    0.28169999    0.26480001    0.1987    ]
 [2257.11708729    0.38280001    0.36709997    0.34269997    0.29000002]
 [2075.70365056    0.38025212    0.35039577    0.35237032    0.30339074]
 ...
 [1535.41975106    0.30114728    0.21917546    0.26720589    0.20140973]
 [ 828.21601342    0.72329998    0.17620002    0.71580005    0.61379999]
 [1308.36050091    0.32211009    0.2197843     0.31006229    0.23574662]][0m
[37m[1m[2023-07-10 13:13:34,398][227910] Max Reward on eval: 3406.131418662658[0m
[37m[1m[2023-07-10 13:13:34,399][227910] Min Reward on eval: 60.85944030773826[0m
[37m[1m[2023-07-10 13:13:34,399][227910] Mean Reward across all agents: 1592.0049417242803[0m
[37m[1m[2023-07-10 13:13:34,399][227910] Average Trajectory Length: 967.7306666666666[0m
[36m[2023-07-10 13:13:34,403][227910] mean_value=-559.4270853536912, max_value=1262.4418917776698[0m
[37m[1m[2023-07-10 13:13:34,406][227910] New mean coefficients: [[-0.0296711   1.3679838  -0.32705167  0.8301326  -0.07244052]][0m
[37m[1m[2023-07-10 13:13:34,407][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:13:44,170][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 13:13:44,170][227910] FPS: 393406.76[0m
[36m[2023-07-10 13:13:44,172][227910] itr=465, itrs=2000, Progress: 23.25%[0m
[36m[2023-07-10 13:13:55,780][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 13:13:55,780][227910] FPS: 331339.25[0m
[36m[2023-07-10 13:14:00,631][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:14:00,631][227910] Reward + Measures: [[2195.56925432    0.4552561     0.24091095    0.41167122    0.34586743]][0m
[37m[1m[2023-07-10 13:14:00,632][227910] Max Reward on eval: 2195.569254317294[0m
[37m[1m[2023-07-10 13:14:00,632][227910] Min Reward on eval: 2195.569254317294[0m
[37m[1m[2023-07-10 13:14:00,632][227910] Mean Reward across all agents: 2195.569254317294[0m
[37m[1m[2023-07-10 13:14:00,632][227910] Average Trajectory Length: 983.4743333333333[0m
[36m[2023-07-10 13:14:06,141][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:14:06,142][227910] Reward + Measures: [[1808.95279756    0.47650003    0.25659999    0.43150002    0.37529999]
 [2249.08684901    0.42109999    0.26449999    0.36630002    0.31550002]
 [2215.37013785    0.44818574    0.35003573    0.38631788    0.33373928]
 ...
 [1942.19695261    0.3475        0.24690001    0.31169999    0.2705    ]
 [1436.73720784    0.65979999    0.1781        0.64110005    0.54500002]
 [ 748.541631      0.73629999    0.22459999    0.71390003    0.61589998]][0m
[37m[1m[2023-07-10 13:14:06,142][227910] Max Reward on eval: 3092.3511472954297[0m
[37m[1m[2023-07-10 13:14:06,142][227910] Min Reward on eval: 748.5416310038651[0m
[37m[1m[2023-07-10 13:14:06,142][227910] Mean Reward across all agents: 1744.57973692809[0m
[37m[1m[2023-07-10 13:14:06,143][227910] Average Trajectory Length: 983.38[0m
[36m[2023-07-10 13:14:06,147][227910] mean_value=-329.53755968985354, max_value=1271.6269751173986[0m
[37m[1m[2023-07-10 13:14:06,149][227910] New mean coefficients: [[ 0.16477452  1.4316776  -0.12888503  1.135055    0.12707253]][0m
[37m[1m[2023-07-10 13:14:06,150][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:14:16,056][227910] train() took 9.90 seconds to complete[0m
[36m[2023-07-10 13:14:16,056][227910] FPS: 387721.88[0m
[36m[2023-07-10 13:14:16,059][227910] itr=466, itrs=2000, Progress: 23.30%[0m
[36m[2023-07-10 13:14:27,722][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 13:14:27,722][227910] FPS: 329720.20[0m
[36m[2023-07-10 13:14:32,529][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:14:32,530][227910] Reward + Measures: [[2303.3716096     0.48111695    0.21861057    0.4320114     0.36500716]][0m
[37m[1m[2023-07-10 13:14:32,530][227910] Max Reward on eval: 2303.37160959512[0m
[37m[1m[2023-07-10 13:14:32,530][227910] Min Reward on eval: 2303.37160959512[0m
[37m[1m[2023-07-10 13:14:32,531][227910] Mean Reward across all agents: 2303.37160959512[0m
[37m[1m[2023-07-10 13:14:32,531][227910] Average Trajectory Length: 982.722[0m
[36m[2023-07-10 13:14:38,181][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:14:38,181][227910] Reward + Measures: [[2082.1876527     0.51160002    0.19500001    0.4526        0.40240002]
 [1157.43296939    0.57370001    0.51200002    0.54659998    0.59909999]
 [1344.08008277    0.53240001    0.43720004    0.51600003    0.51870006]
 ...
 [1428.89038898    0.51430005    0.41140005    0.47849998    0.50620002]
 [1736.34244319    0.53380001    0.40599999    0.50319999    0.50630003]
 [2427.35417104    0.40890002    0.25690001    0.36170003    0.30609998]][0m
[37m[1m[2023-07-10 13:14:38,182][227910] Max Reward on eval: 2787.772299699963[0m
[37m[1m[2023-07-10 13:14:38,182][227910] Min Reward on eval: 1022.1475597366109[0m
[37m[1m[2023-07-10 13:14:38,182][227910] Mean Reward across all agents: 1848.7123343070516[0m
[37m[1m[2023-07-10 13:14:38,182][227910] Average Trajectory Length: 981.4093333333333[0m
[36m[2023-07-10 13:14:38,187][227910] mean_value=-50.41835281158776, max_value=1645.292507397452[0m
[37m[1m[2023-07-10 13:14:38,190][227910] New mean coefficients: [[ 0.38165677  1.8651289  -0.00670572  1.4396278   0.11741441]][0m
[37m[1m[2023-07-10 13:14:38,191][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:14:48,001][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 13:14:48,001][227910] FPS: 391505.01[0m
[36m[2023-07-10 13:14:48,004][227910] itr=467, itrs=2000, Progress: 23.35%[0m
[36m[2023-07-10 13:14:59,683][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 13:14:59,683][227910] FPS: 329252.54[0m
[36m[2023-07-10 13:15:04,474][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:15:04,474][227910] Reward + Measures: [[2823.12802891    0.44045824    0.22656953    0.38100031    0.30215982]][0m
[37m[1m[2023-07-10 13:15:04,474][227910] Max Reward on eval: 2823.128028910594[0m
[37m[1m[2023-07-10 13:15:04,474][227910] Min Reward on eval: 2823.128028910594[0m
[37m[1m[2023-07-10 13:15:04,475][227910] Mean Reward across all agents: 2823.128028910594[0m
[37m[1m[2023-07-10 13:15:04,475][227910] Average Trajectory Length: 985.0476666666666[0m
[36m[2023-07-10 13:15:09,923][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:15:09,924][227910] Reward + Measures: [[1794.42576921    0.47590002    0.19880001    0.44970003    0.35550004]
 [2227.14335872    0.50330001    0.26390001    0.43619999    0.36170003]
 [2220.13466485    0.5187304     0.19835438    0.47982699    0.38775095]
 ...
 [1722.7983883     0.50929999    0.38610002    0.45070001    0.38719997]
 [1700.86494929    0.39179999    0.24779999    0.3712        0.27290002]
 [1938.70248512    0.51014113    0.28250676    0.46183333    0.37376952]][0m
[37m[1m[2023-07-10 13:15:09,924][227910] Max Reward on eval: 3362.0804803959095[0m
[37m[1m[2023-07-10 13:15:09,925][227910] Min Reward on eval: 405.90061202532377[0m
[37m[1m[2023-07-10 13:15:09,925][227910] Mean Reward across all agents: 2011.7244892343858[0m
[37m[1m[2023-07-10 13:15:09,925][227910] Average Trajectory Length: 969.4879999999999[0m
[36m[2023-07-10 13:15:09,928][227910] mean_value=-687.4329747996824, max_value=1365.1823312665324[0m
[37m[1m[2023-07-10 13:15:09,931][227910] New mean coefficients: [[ 0.02628464  1.4350797  -0.11345343  1.0364045   0.38729006]][0m
[37m[1m[2023-07-10 13:15:09,932][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:15:19,689][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 13:15:19,690][227910] FPS: 393607.83[0m
[36m[2023-07-10 13:15:19,692][227910] itr=468, itrs=2000, Progress: 23.40%[0m
[36m[2023-07-10 13:15:31,334][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 13:15:31,334][227910] FPS: 330354.91[0m
[36m[2023-07-10 13:15:36,080][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:15:36,080][227910] Reward + Measures: [[2796.81915006    0.4614051     0.21850525    0.39742365    0.32354438]][0m
[37m[1m[2023-07-10 13:15:36,081][227910] Max Reward on eval: 2796.8191500562843[0m
[37m[1m[2023-07-10 13:15:36,081][227910] Min Reward on eval: 2796.8191500562843[0m
[37m[1m[2023-07-10 13:15:36,081][227910] Mean Reward across all agents: 2796.8191500562843[0m
[37m[1m[2023-07-10 13:15:36,081][227910] Average Trajectory Length: 990.5833333333333[0m
[36m[2023-07-10 13:15:41,576][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:15:41,577][227910] Reward + Measures: [[3060.70341798    0.4627113     0.29539785    0.36102149    0.29734111]
 [1870.6384175     0.54159999    0.27810001    0.51380008    0.45289999]
 [ 346.10637108    0.80319995    0.52450001    0.77080005    0.71270001]
 ...
 [ 271.13545346    0.85650009    0.81850004    0.82539999    0.80820006]
 [2638.81086567    0.53979999    0.28140002    0.454         0.3829    ]
 [ 282.04994655    0.82530004    0.68919998    0.80080003    0.76809996]][0m
[37m[1m[2023-07-10 13:15:41,577][227910] Max Reward on eval: 3558.801067343168[0m
[37m[1m[2023-07-10 13:15:41,577][227910] Min Reward on eval: -99.34304100724403[0m
[37m[1m[2023-07-10 13:15:41,577][227910] Mean Reward across all agents: 1520.5598881042276[0m
[37m[1m[2023-07-10 13:15:41,578][227910] Average Trajectory Length: 979.5123333333333[0m
[36m[2023-07-10 13:15:41,582][227910] mean_value=-340.85632069856894, max_value=1440.982220519575[0m
[37m[1m[2023-07-10 13:15:41,585][227910] New mean coefficients: [[-0.16649966  0.9070789  -0.43511763  0.32225227  0.6317891 ]][0m
[37m[1m[2023-07-10 13:15:41,586][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:15:51,475][227910] train() took 9.89 seconds to complete[0m
[36m[2023-07-10 13:15:51,475][227910] FPS: 388352.56[0m
[36m[2023-07-10 13:15:51,478][227910] itr=469, itrs=2000, Progress: 23.45%[0m
[36m[2023-07-10 13:16:03,034][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 13:16:03,034][227910] FPS: 332789.87[0m
[36m[2023-07-10 13:16:07,738][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:16:07,744][227910] Reward + Measures: [[2083.14929182    0.55743515    0.18648881    0.50739002    0.44341612]][0m
[37m[1m[2023-07-10 13:16:07,744][227910] Max Reward on eval: 2083.149291816049[0m
[37m[1m[2023-07-10 13:16:07,745][227910] Min Reward on eval: 2083.149291816049[0m
[37m[1m[2023-07-10 13:16:07,745][227910] Mean Reward across all agents: 2083.149291816049[0m
[37m[1m[2023-07-10 13:16:07,745][227910] Average Trajectory Length: 991.2596666666666[0m
[36m[2023-07-10 13:16:13,198][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:16:13,199][227910] Reward + Measures: [[ 612.38375699    0.77499998    0.1013        0.78370005    0.72000003]
 [ 412.38068141    0.86759996    0.1796        0.86059999    0.81560004]
 [2508.5277764     0.45040002    0.21519999    0.37970001    0.27410001]
 ...
 [ 370.96211849    0.70615011    0.14820492    0.69365525    0.53672111]
 [ 230.56230156    0.57989997    0.498         0.55150002    0.49320003]
 [2365.78070746    0.45774078    0.21323486    0.40461275    0.30024534]][0m
[37m[1m[2023-07-10 13:16:13,199][227910] Max Reward on eval: 2961.1536615208606[0m
[37m[1m[2023-07-10 13:16:13,199][227910] Min Reward on eval: -408.81638860560486[0m
[37m[1m[2023-07-10 13:16:13,199][227910] Mean Reward across all agents: 1177.2948728139174[0m
[37m[1m[2023-07-10 13:16:13,199][227910] Average Trajectory Length: 955.021[0m
[36m[2023-07-10 13:16:13,205][227910] mean_value=-384.76140212550445, max_value=1554.4171944561008[0m
[37m[1m[2023-07-10 13:16:13,208][227910] New mean coefficients: [[-0.33214176  0.47255126 -0.5609029   0.26826784  0.3980203 ]][0m
[37m[1m[2023-07-10 13:16:13,208][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:16:22,874][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 13:16:22,874][227910] FPS: 397352.64[0m
[36m[2023-07-10 13:16:22,877][227910] itr=470, itrs=2000, Progress: 23.50%[0m
[37m[1m[2023-07-10 13:16:25,537][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000450[0m
[36m[2023-07-10 13:16:37,295][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 13:16:37,296][227910] FPS: 334035.81[0m
[36m[2023-07-10 13:16:42,094][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:16:42,094][227910] Reward + Measures: [[1167.68174575    0.70252061    0.13593586    0.67276287    0.62380844]][0m
[37m[1m[2023-07-10 13:16:42,095][227910] Max Reward on eval: 1167.6817457539187[0m
[37m[1m[2023-07-10 13:16:42,095][227910] Min Reward on eval: 1167.6817457539187[0m
[37m[1m[2023-07-10 13:16:42,095][227910] Mean Reward across all agents: 1167.6817457539187[0m
[37m[1m[2023-07-10 13:16:42,095][227910] Average Trajectory Length: 994.2326666666667[0m
[36m[2023-07-10 13:16:47,706][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:16:47,712][227910] Reward + Measures: [[ 239.78752216    0.78200001    0.722         0.66830003    0.69540006]
 [ 868.62001321    0.78449994    0.15989999    0.73950005    0.70990002]
 [ 389.13975719    0.63510001    0.2638        0.63950008    0.69110006]
 ...
 [ 366.78279488    0.52910006    0.43560001    0.53680003    0.66960001]
 [1388.53253829    0.52539998    0.3876        0.42440006    0.42089996]
 [ 884.3237185     0.75270003    0.46830001    0.74970001    0.65679997]][0m
[37m[1m[2023-07-10 13:16:47,712][227910] Max Reward on eval: 1908.8256442005513[0m
[37m[1m[2023-07-10 13:16:47,712][227910] Min Reward on eval: -632.8351586898323[0m
[37m[1m[2023-07-10 13:16:47,713][227910] Mean Reward across all agents: 687.1480956532366[0m
[37m[1m[2023-07-10 13:16:47,713][227910] Average Trajectory Length: 990.01[0m
[36m[2023-07-10 13:16:47,721][227910] mean_value=115.2006010858326, max_value=1203.8923657791413[0m
[37m[1m[2023-07-10 13:16:47,723][227910] New mean coefficients: [[-0.49884027  0.60619974 -0.16900662  0.17525794  0.5825389 ]][0m
[37m[1m[2023-07-10 13:16:47,724][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:16:57,424][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 13:16:57,424][227910] FPS: 395965.51[0m
[36m[2023-07-10 13:16:57,427][227910] itr=471, itrs=2000, Progress: 23.55%[0m
[36m[2023-07-10 13:17:08,887][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 13:17:08,888][227910] FPS: 335515.26[0m
[36m[2023-07-10 13:17:13,561][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:17:13,562][227910] Reward + Measures: [[776.3022838    0.79648972   0.09829146   0.7760309    0.73649943]][0m
[37m[1m[2023-07-10 13:17:13,562][227910] Max Reward on eval: 776.3022838019031[0m
[37m[1m[2023-07-10 13:17:13,562][227910] Min Reward on eval: 776.3022838019031[0m
[37m[1m[2023-07-10 13:17:13,562][227910] Mean Reward across all agents: 776.3022838019031[0m
[37m[1m[2023-07-10 13:17:13,563][227910] Average Trajectory Length: 994.7736666666666[0m
[36m[2023-07-10 13:17:19,042][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:17:19,042][227910] Reward + Measures: [[849.48016573   0.79110003   0.1138       0.76570004   0.68220001]
 [491.37247946   0.8653       0.32969999   0.8549       0.7985    ]
 [748.33544003   0.84380007   0.0842       0.83309996   0.78470004]
 ...
 [698.30454973   0.76019996   0.0849       0.73500001   0.68590003]
 [504.19287969   0.66420686   0.20150684   0.65129179   0.58926302]
 [685.4360839    0.82870001   0.13869999   0.80849999   0.759     ]][0m
[37m[1m[2023-07-10 13:17:19,043][227910] Max Reward on eval: 1308.7080738257034[0m
[37m[1m[2023-07-10 13:17:19,043][227910] Min Reward on eval: 238.50003090185345[0m
[37m[1m[2023-07-10 13:17:19,043][227910] Mean Reward across all agents: 701.234285511043[0m
[37m[1m[2023-07-10 13:17:19,043][227910] Average Trajectory Length: 997.0456666666666[0m
[36m[2023-07-10 13:17:19,052][227910] mean_value=302.28358384107736, max_value=1369.4934797793626[0m
[37m[1m[2023-07-10 13:17:19,055][227910] New mean coefficients: [[-0.42082447  0.5735997   0.26525104  0.2250331   0.69832766]][0m
[37m[1m[2023-07-10 13:17:19,056][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:17:28,740][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 13:17:28,741][227910] FPS: 396556.37[0m
[36m[2023-07-10 13:17:28,743][227910] itr=472, itrs=2000, Progress: 23.60%[0m
[36m[2023-07-10 13:17:40,297][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 13:17:40,297][227910] FPS: 332916.51[0m
[36m[2023-07-10 13:17:45,044][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:17:45,044][227910] Reward + Measures: [[558.35551082   0.87841439   0.06568351   0.86530483   0.83640736]][0m
[37m[1m[2023-07-10 13:17:45,044][227910] Max Reward on eval: 558.3555108201673[0m
[37m[1m[2023-07-10 13:17:45,045][227910] Min Reward on eval: 558.3555108201673[0m
[37m[1m[2023-07-10 13:17:45,045][227910] Mean Reward across all agents: 558.3555108201673[0m
[37m[1m[2023-07-10 13:17:45,045][227910] Average Trajectory Length: 997.8756666666667[0m
[36m[2023-07-10 13:17:50,468][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:17:50,474][227910] Reward + Measures: [[169.04682123   0.4673       0.56489998   0.45590001   0.52590001]
 [418.00809226   0.79240006   0.39260003   0.80310005   0.57630002]
 [377.74344579   0.43319997   0.62709999   0.43090001   0.5643    ]
 ...
 [ 91.3727876    0.55979997   0.4533       0.57239997   0.4434    ]
 [374.37699864   0.56720001   0.6552       0.55430001   0.63200003]
 [108.99589857   0.45809999   0.57509995   0.43389997   0.53369999]][0m
[37m[1m[2023-07-10 13:17:50,475][227910] Max Reward on eval: 1387.1979265048867[0m
[37m[1m[2023-07-10 13:17:50,476][227910] Min Reward on eval: -428.11140770891217[0m
[37m[1m[2023-07-10 13:17:50,476][227910] Mean Reward across all agents: 423.14657447063206[0m
[37m[1m[2023-07-10 13:17:50,477][227910] Average Trajectory Length: 997.8009999999999[0m
[36m[2023-07-10 13:17:50,488][227910] mean_value=-18.735395732767792, max_value=1346.421093268766[0m
[37m[1m[2023-07-10 13:17:50,492][227910] New mean coefficients: [[-0.39601505  0.54062474  0.31667954  0.17514776  0.90681946]][0m
[37m[1m[2023-07-10 13:17:50,494][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:18:00,161][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 13:18:00,162][227910] FPS: 397284.56[0m
[36m[2023-07-10 13:18:00,164][227910] itr=473, itrs=2000, Progress: 23.65%[0m
[36m[2023-07-10 13:18:11,630][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 13:18:11,630][227910] FPS: 335360.40[0m
[36m[2023-07-10 13:18:16,464][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:18:16,465][227910] Reward + Measures: [[476.09515656   0.9035598    0.05670832   0.8933264    0.86792678]][0m
[37m[1m[2023-07-10 13:18:16,465][227910] Max Reward on eval: 476.09515655868887[0m
[37m[1m[2023-07-10 13:18:16,465][227910] Min Reward on eval: 476.09515655868887[0m
[37m[1m[2023-07-10 13:18:16,466][227910] Mean Reward across all agents: 476.09515655868887[0m
[37m[1m[2023-07-10 13:18:16,466][227910] Average Trajectory Length: 998.8753333333333[0m
[36m[2023-07-10 13:18:21,908][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:18:21,914][227910] Reward + Measures: [[563.78188252   0.88490009   0.13060001   0.88000005   0.85939997]
 [400.51138006   0.93300003   0.0442       0.92259997   0.88020003]
 [430.70089209   0.90310001   0.0587       0.89799994   0.85509998]
 ...
 [347.91581514   0.76639998   0.64390004   0.6656       0.84650004]
 [407.99361051   0.81660002   0.1267       0.81069994   0.76319999]
 [322.91854786   0.8549       0.5503       0.8283       0.88770002]][0m
[37m[1m[2023-07-10 13:18:21,914][227910] Max Reward on eval: 785.3143705075374[0m
[37m[1m[2023-07-10 13:18:21,915][227910] Min Reward on eval: -427.2904107985727[0m
[37m[1m[2023-07-10 13:18:21,915][227910] Mean Reward across all agents: 399.37136390763885[0m
[37m[1m[2023-07-10 13:18:21,915][227910] Average Trajectory Length: 993.9873333333333[0m
[36m[2023-07-10 13:18:21,921][227910] mean_value=238.7598777199217, max_value=938.2609645373211[0m
[37m[1m[2023-07-10 13:18:21,924][227910] New mean coefficients: [[-0.82370573  0.39639762  0.59353715 -0.22707097  1.0855042 ]][0m
[37m[1m[2023-07-10 13:18:21,925][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:18:31,572][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 13:18:31,572][227910] FPS: 398124.68[0m
[36m[2023-07-10 13:18:31,574][227910] itr=474, itrs=2000, Progress: 23.70%[0m
[36m[2023-07-10 13:18:43,214][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 13:18:43,215][227910] FPS: 330341.30[0m
[36m[2023-07-10 13:18:47,992][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:18:47,993][227910] Reward + Measures: [[387.05642895   0.92943698   0.04613966   0.92237562   0.90401936]][0m
[37m[1m[2023-07-10 13:18:47,993][227910] Max Reward on eval: 387.0564289463696[0m
[37m[1m[2023-07-10 13:18:47,993][227910] Min Reward on eval: 387.0564289463696[0m
[37m[1m[2023-07-10 13:18:47,993][227910] Mean Reward across all agents: 387.0564289463696[0m
[37m[1m[2023-07-10 13:18:47,993][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:18:53,638][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:18:53,643][227910] Reward + Measures: [[400.14357723   0.90469998   0.0826       0.88739997   0.82380003]
 [394.8964547    0.92270005   0.19080003   0.90529996   0.86409998]
 [335.87998543   0.80219996   0.48100001   0.74789995   0.80620003]
 ...
 [387.62058662   0.91259998   0.1305       0.90260011   0.83130008]
 [423.75510933   0.91020006   0.18700002   0.90929997   0.62370002]
 [424.43021342   0.92089999   0.0526       0.91569996   0.8682    ]][0m
[37m[1m[2023-07-10 13:18:53,644][227910] Max Reward on eval: 560.1730762773543[0m
[37m[1m[2023-07-10 13:18:53,644][227910] Min Reward on eval: 233.33463599177776[0m
[37m[1m[2023-07-10 13:18:53,644][227910] Mean Reward across all agents: 378.02767726732236[0m
[37m[1m[2023-07-10 13:18:53,644][227910] Average Trajectory Length: 999.7206666666666[0m
[36m[2023-07-10 13:18:53,648][227910] mean_value=20.69381093699633, max_value=947.3507109600332[0m
[37m[1m[2023-07-10 13:18:53,650][227910] New mean coefficients: [[-1.3422188   0.7199191   0.30051586 -0.19120865  1.336127  ]][0m
[37m[1m[2023-07-10 13:18:53,651][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:19:03,423][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 13:19:03,423][227910] FPS: 393023.26[0m
[36m[2023-07-10 13:19:03,426][227910] itr=475, itrs=2000, Progress: 23.75%[0m
[36m[2023-07-10 13:19:14,973][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 13:19:14,974][227910] FPS: 332998.53[0m
[36m[2023-07-10 13:19:19,771][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:19:19,771][227910] Reward + Measures: [[343.96625979   0.93691099   0.04371528   0.9303562    0.91933942]][0m
[37m[1m[2023-07-10 13:19:19,771][227910] Max Reward on eval: 343.9662597851971[0m
[37m[1m[2023-07-10 13:19:19,772][227910] Min Reward on eval: 343.9662597851971[0m
[37m[1m[2023-07-10 13:19:19,772][227910] Mean Reward across all agents: 343.9662597851971[0m
[37m[1m[2023-07-10 13:19:19,772][227910] Average Trajectory Length: 999.728[0m
[36m[2023-07-10 13:19:25,238][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:19:25,239][227910] Reward + Measures: [[-314.14431356    0.4656857     0.50048572    0.27180001    0.53558576]
 [ 137.80893849    0.93510008    0.0446        0.9307        0.91680002]
 [-672.04133328    0.60589999    0.4587        0.5255        0.44700003]
 ...
 [-397.56138683    0.9314        0.0356        0.92940009    0.81750005]
 [ 628.33151491    0.80229998    0.55730003    0.79280001    0.80669993]
 [  45.91076179    0.95550007    0.0329        0.949         0.93370003]][0m
[37m[1m[2023-07-10 13:19:25,239][227910] Max Reward on eval: 628.3315149140893[0m
[37m[1m[2023-07-10 13:19:25,239][227910] Min Reward on eval: -773.6046460574377[0m
[37m[1m[2023-07-10 13:19:25,240][227910] Mean Reward across all agents: 4.9576842911841545[0m
[37m[1m[2023-07-10 13:19:25,240][227910] Average Trajectory Length: 995.3973333333333[0m
[36m[2023-07-10 13:19:25,246][227910] mean_value=-214.83866879547216, max_value=967.9129093732336[0m
[37m[1m[2023-07-10 13:19:25,248][227910] New mean coefficients: [[-0.79693735  0.6872304   0.92127764 -0.01984498  1.3974538 ]][0m
[37m[1m[2023-07-10 13:19:25,249][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:19:35,191][227910] train() took 9.94 seconds to complete[0m
[36m[2023-07-10 13:19:35,192][227910] FPS: 386305.42[0m
[36m[2023-07-10 13:19:35,194][227910] itr=476, itrs=2000, Progress: 23.80%[0m
[36m[2023-07-10 13:19:46,904][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 13:19:46,904][227910] FPS: 328441.77[0m
[36m[2023-07-10 13:19:51,746][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:19:51,747][227910] Reward + Measures: [[321.53355153   0.94284213   0.04215751   0.93613136   0.92564583]][0m
[37m[1m[2023-07-10 13:19:51,747][227910] Max Reward on eval: 321.5335515281003[0m
[37m[1m[2023-07-10 13:19:51,747][227910] Min Reward on eval: 321.5335515281003[0m
[37m[1m[2023-07-10 13:19:51,748][227910] Mean Reward across all agents: 321.5335515281003[0m
[37m[1m[2023-07-10 13:19:51,748][227910] Average Trajectory Length: 999.4343333333333[0m
[36m[2023-07-10 13:19:57,280][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:19:57,280][227910] Reward + Measures: [[329.91604603   0.9321       0.0488       0.90809995   0.86210006]
 [255.54064501   0.95370001   0.0349       0.94480002   0.93500006]
 [375.57877145   0.93559998   0.0457       0.92480004   0.89820004]
 ...
 [365.43085151   0.778        0.1128       0.76670003   0.58900005]
 [382.52431966   0.92900008   0.0418       0.92080003   0.88950008]
 [340.77425408   0.92060006   0.0548       0.90310001   0.89610004]][0m
[37m[1m[2023-07-10 13:19:57,280][227910] Max Reward on eval: 603.7002953707008[0m
[37m[1m[2023-07-10 13:19:57,281][227910] Min Reward on eval: 88.06117405818077[0m
[37m[1m[2023-07-10 13:19:57,281][227910] Mean Reward across all agents: 333.4833249351474[0m
[37m[1m[2023-07-10 13:19:57,281][227910] Average Trajectory Length: 999.762[0m
[36m[2023-07-10 13:19:57,284][227910] mean_value=-89.3796438582966, max_value=992.6113116794938[0m
[37m[1m[2023-07-10 13:19:57,287][227910] New mean coefficients: [[-0.7318747  1.2580411  2.1004016  0.5601275  1.6442305]][0m
[37m[1m[2023-07-10 13:19:57,288][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:20:06,938][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 13:20:06,938][227910] FPS: 397998.94[0m
[36m[2023-07-10 13:20:06,941][227910] itr=477, itrs=2000, Progress: 23.85%[0m
[36m[2023-07-10 13:20:18,451][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 13:20:18,451][227910] FPS: 334064.55[0m
[36m[2023-07-10 13:20:23,216][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:20:23,216][227910] Reward + Measures: [[278.63207782   0.9474321    0.0388713    0.9417451    0.93588501]][0m
[37m[1m[2023-07-10 13:20:23,216][227910] Max Reward on eval: 278.63207782262424[0m
[37m[1m[2023-07-10 13:20:23,217][227910] Min Reward on eval: 278.63207782262424[0m
[37m[1m[2023-07-10 13:20:23,217][227910] Mean Reward across all agents: 278.63207782262424[0m
[37m[1m[2023-07-10 13:20:23,217][227910] Average Trajectory Length: 999.7049999999999[0m
[36m[2023-07-10 13:20:28,731][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:20:28,736][227910] Reward + Measures: [[ -65.491684      0.87830001    0.88069993    0.87200004    0.90070003]
 [-448.12087941    0.79640001    0.79770005    0.84729999    0.82139999]
 [ 340.36801529    0.95370007    0.0431        0.94750005    0.93440002]
 ...
 [-787.70169784    0.72380006    0.71030009    0.77140009    0.70319998]
 [-614.72629311    0.74990004    0.73460001    0.77930003    0.74609995]
 [ 177.10182989    0.85760003    0.86710006    0.84829998    0.875     ]][0m
[37m[1m[2023-07-10 13:20:28,737][227910] Max Reward on eval: 391.8407937812619[0m
[37m[1m[2023-07-10 13:20:28,737][227910] Min Reward on eval: -899.5311318680294[0m
[37m[1m[2023-07-10 13:20:28,737][227910] Mean Reward across all agents: -167.82671782192068[0m
[37m[1m[2023-07-10 13:20:28,738][227910] Average Trajectory Length: 996.9126666666666[0m
[36m[2023-07-10 13:20:28,741][227910] mean_value=-292.65980296790514, max_value=785.017687561384[0m
[37m[1m[2023-07-10 13:20:28,743][227910] New mean coefficients: [[-0.7306942  0.78771    0.8583075  0.3339504  1.4950379]][0m
[37m[1m[2023-07-10 13:20:28,744][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:20:38,409][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 13:20:38,409][227910] FPS: 397410.21[0m
[36m[2023-07-10 13:20:38,411][227910] itr=478, itrs=2000, Progress: 23.90%[0m
[36m[2023-07-10 13:20:50,037][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 13:20:50,037][227910] FPS: 330744.49[0m
[36m[2023-07-10 13:20:54,908][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:20:54,909][227910] Reward + Measures: [[213.67336051   0.95145863   0.03580767   0.94565171   0.93929595]][0m
[37m[1m[2023-07-10 13:20:54,909][227910] Max Reward on eval: 213.67336051150713[0m
[37m[1m[2023-07-10 13:20:54,909][227910] Min Reward on eval: 213.67336051150713[0m
[37m[1m[2023-07-10 13:20:54,909][227910] Mean Reward across all agents: 213.67336051150713[0m
[37m[1m[2023-07-10 13:20:54,909][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:21:00,317][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:21:00,323][227910] Reward + Measures: [[190.03592505   0.94480002   0.0356       0.94609994   0.92259997]
 [203.27355915   0.94929999   0.03750001   0.94239998   0.9228    ]
 [206.6142851    0.95860004   0.0304       0.95380002   0.94      ]
 ...
 [115.23525911   0.95149994   0.0348       0.94499999   0.93500006]
 [109.01215418   0.95830005   0.0317       0.954        0.93460006]
 [250.27677043   0.94510001   0.0376       0.93949997   0.93079996]][0m
[37m[1m[2023-07-10 13:21:00,323][227910] Max Reward on eval: 424.9140390328481[0m
[37m[1m[2023-07-10 13:21:00,324][227910] Min Reward on eval: 37.68009249062743[0m
[37m[1m[2023-07-10 13:21:00,324][227910] Mean Reward across all agents: 172.3955326258321[0m
[37m[1m[2023-07-10 13:21:00,324][227910] Average Trajectory Length: 999.679[0m
[36m[2023-07-10 13:21:00,326][227910] mean_value=-283.5354737279747, max_value=816.182588230432[0m
[37m[1m[2023-07-10 13:21:00,328][227910] New mean coefficients: [[-0.6184939   0.45796072 -0.36519265  0.48023295  0.48181772]][0m
[37m[1m[2023-07-10 13:21:00,329][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:21:10,106][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 13:21:10,107][227910] FPS: 392817.70[0m
[36m[2023-07-10 13:21:10,109][227910] itr=479, itrs=2000, Progress: 23.95%[0m
[36m[2023-07-10 13:21:21,640][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 13:21:21,641][227910] FPS: 333514.61[0m
[36m[2023-07-10 13:21:26,314][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:21:26,314][227910] Reward + Measures: [[163.91760785   0.9564544    0.03270977   0.95018846   0.94238281]][0m
[37m[1m[2023-07-10 13:21:26,314][227910] Max Reward on eval: 163.91760785280314[0m
[37m[1m[2023-07-10 13:21:26,314][227910] Min Reward on eval: 163.91760785280314[0m
[37m[1m[2023-07-10 13:21:26,315][227910] Mean Reward across all agents: 163.91760785280314[0m
[37m[1m[2023-07-10 13:21:26,315][227910] Average Trajectory Length: 999.6743333333333[0m
[36m[2023-07-10 13:21:31,782][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:21:31,783][227910] Reward + Measures: [[170.27005457   0.9429       0.0472       0.92919999   0.91790003]
 [234.80314234   0.89790004   0.09850001   0.88269997   0.83590001]
 [191.28500272   0.96390003   0.0263       0.95810002   0.92700005]
 ...
 [194.19553571   0.96519995   0.0259       0.95839995   0.95050001]
 [207.63845711   0.93269998   0.049        0.92000002   0.88560003]
 [215.67829382   0.93590003   0.18099999   0.92089999   0.90810007]][0m
[37m[1m[2023-07-10 13:21:31,783][227910] Max Reward on eval: 531.4139541376032[0m
[37m[1m[2023-07-10 13:21:31,783][227910] Min Reward on eval: 38.4115649115789[0m
[37m[1m[2023-07-10 13:21:31,784][227910] Mean Reward across all agents: 186.29480904053656[0m
[37m[1m[2023-07-10 13:21:31,784][227910] Average Trajectory Length: 999.745[0m
[36m[2023-07-10 13:21:31,785][227910] mean_value=-248.14361324112187, max_value=455.5296108837358[0m
[37m[1m[2023-07-10 13:21:31,787][227910] New mean coefficients: [[-0.40211123  0.21798517 -0.60774946  0.20437944  0.68268675]][0m
[37m[1m[2023-07-10 13:21:31,788][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:21:41,297][227910] train() took 9.51 seconds to complete[0m
[36m[2023-07-10 13:21:41,297][227910] FPS: 403893.89[0m
[36m[2023-07-10 13:21:41,300][227910] itr=480, itrs=2000, Progress: 24.00%[0m
[37m[1m[2023-07-10 13:21:43,945][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000460[0m
[36m[2023-07-10 13:21:55,728][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 13:21:55,728][227910] FPS: 333389.91[0m
[36m[2023-07-10 13:22:00,342][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:22:00,343][227910] Reward + Measures: [[114.16133032   0.96193993   0.02876733   0.95606363   0.94878864]][0m
[37m[1m[2023-07-10 13:22:00,343][227910] Max Reward on eval: 114.16133031891094[0m
[37m[1m[2023-07-10 13:22:00,343][227910] Min Reward on eval: 114.16133031891094[0m
[37m[1m[2023-07-10 13:22:00,343][227910] Mean Reward across all agents: 114.16133031891094[0m
[37m[1m[2023-07-10 13:22:00,344][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:22:05,747][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:22:05,748][227910] Reward + Measures: [[  2.76588082   0.96919996   0.11520001   0.96260005   0.94639999]
 [ 52.65295936   0.96090001   0.0286       0.958        0.95459998]
 [ 70.70515087   0.96210003   0.026        0.95749998   0.94139999]
 ...
 [202.09448987   0.92910004   0.038        0.90809995   0.85839999]
 [189.68405469   0.63789999   0.20910001   0.54630005   0.4605    ]
 [175.1899391    0.838        0.0751       0.78250003   0.55919999]][0m
[37m[1m[2023-07-10 13:22:05,748][227910] Max Reward on eval: 337.18130275995935[0m
[37m[1m[2023-07-10 13:22:05,748][227910] Min Reward on eval: -321.90834430723623[0m
[37m[1m[2023-07-10 13:22:05,749][227910] Mean Reward across all agents: 133.46615629596883[0m
[37m[1m[2023-07-10 13:22:05,749][227910] Average Trajectory Length: 999.3743333333333[0m
[36m[2023-07-10 13:22:05,752][227910] mean_value=-145.68493327286757, max_value=769.1413544129199[0m
[37m[1m[2023-07-10 13:22:05,754][227910] New mean coefficients: [[-0.19114687  0.21865526 -0.26553208  0.25523925  0.88740104]][0m
[37m[1m[2023-07-10 13:22:05,755][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:22:15,473][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 13:22:15,473][227910] FPS: 395210.30[0m
[36m[2023-07-10 13:22:15,476][227910] itr=481, itrs=2000, Progress: 24.05%[0m
[36m[2023-07-10 13:22:26,952][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 13:22:26,952][227910] FPS: 335132.34[0m
[36m[2023-07-10 13:22:31,593][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:22:31,593][227910] Reward + Measures: [[122.86087615   0.96514511   0.0278389    0.95927858   0.95293343]][0m
[37m[1m[2023-07-10 13:22:31,593][227910] Max Reward on eval: 122.86087614694127[0m
[37m[1m[2023-07-10 13:22:31,594][227910] Min Reward on eval: 122.86087614694127[0m
[37m[1m[2023-07-10 13:22:31,594][227910] Mean Reward across all agents: 122.86087614694127[0m
[37m[1m[2023-07-10 13:22:31,594][227910] Average Trajectory Length: 999.6803333333334[0m
[36m[2023-07-10 13:22:37,086][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:22:37,087][227910] Reward + Measures: [[620.06196796   0.77960002   0.48129997   0.76180005   0.62819999]
 [171.94546514   0.9636001    0.0237       0.95889997   0.95370001]
 [135.07321167   0.95380002   0.034        0.94890004   0.94880003]
 ...
 [165.75108364   0.9641       0.0333       0.95639992   0.94230002]
 [168.17277617   0.95700008   0.0345       0.95050001   0.95409995]
 [125.72067824   0.96579999   0.0251       0.95929998   0.94849998]][0m
[37m[1m[2023-07-10 13:22:37,087][227910] Max Reward on eval: 715.4421130218543[0m
[37m[1m[2023-07-10 13:22:37,087][227910] Min Reward on eval: 15.092418491723947[0m
[37m[1m[2023-07-10 13:22:37,088][227910] Mean Reward across all agents: 196.68854266966602[0m
[37m[1m[2023-07-10 13:22:37,088][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:22:37,091][227910] mean_value=-161.70256736816896, max_value=992.7808171682991[0m
[37m[1m[2023-07-10 13:22:37,093][227910] New mean coefficients: [[-0.11322131  1.2205868  -0.48541453  0.7548812   1.3112385 ]][0m
[37m[1m[2023-07-10 13:22:37,094][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:22:46,781][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 13:22:46,782][227910] FPS: 396461.76[0m
[36m[2023-07-10 13:22:46,784][227910] itr=482, itrs=2000, Progress: 24.10%[0m
[36m[2023-07-10 13:22:58,352][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 13:22:58,352][227910] FPS: 332477.49[0m
[36m[2023-07-10 13:23:03,147][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:23:03,147][227910] Reward + Measures: [[154.28691311   0.96758103   0.026964     0.96186835   0.95490104]][0m
[37m[1m[2023-07-10 13:23:03,148][227910] Max Reward on eval: 154.28691310542922[0m
[37m[1m[2023-07-10 13:23:03,148][227910] Min Reward on eval: 154.28691310542922[0m
[37m[1m[2023-07-10 13:23:03,148][227910] Mean Reward across all agents: 154.28691310542922[0m
[37m[1m[2023-07-10 13:23:03,148][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:23:08,776][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:23:08,782][227910] Reward + Measures: [[151.4764947    0.96919996   0.0252       0.96270007   0.95669997]
 [113.12635818   0.95850003   0.0741       0.95380002   0.93620008]
 [128.17030112   0.96899998   0.0353       0.96509999   0.93640006]
 ...
 [301.77436518   0.92799991   0.0676       0.91260004   0.88430005]
 [189.81602128   0.94480002   0.1908       0.92870009   0.90970004]
 [153.11230923   0.96160001   0.028        0.95549995   0.9436    ]][0m
[37m[1m[2023-07-10 13:23:08,782][227910] Max Reward on eval: 758.747769025457[0m
[37m[1m[2023-07-10 13:23:08,783][227910] Min Reward on eval: 83.95362047764938[0m
[37m[1m[2023-07-10 13:23:08,783][227910] Mean Reward across all agents: 193.62515217898846[0m
[37m[1m[2023-07-10 13:23:08,783][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:23:08,785][227910] mean_value=-174.44683865334756, max_value=673.8672632848937[0m
[37m[1m[2023-07-10 13:23:08,787][227910] New mean coefficients: [[ 0.28547993  0.05474889 -1.4250942   0.7664093   1.0045158 ]][0m
[37m[1m[2023-07-10 13:23:08,788][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:23:18,603][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 13:23:18,603][227910] FPS: 391328.25[0m
[36m[2023-07-10 13:23:18,605][227910] itr=483, itrs=2000, Progress: 24.15%[0m
[36m[2023-07-10 13:23:30,202][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 13:23:30,203][227910] FPS: 331637.06[0m
[36m[2023-07-10 13:23:34,926][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:23:34,926][227910] Reward + Measures: [[197.73264102   0.97037274   0.02522633   0.96476001   0.95535302]][0m
[37m[1m[2023-07-10 13:23:34,927][227910] Max Reward on eval: 197.73264101744613[0m
[37m[1m[2023-07-10 13:23:34,927][227910] Min Reward on eval: 197.73264101744613[0m
[37m[1m[2023-07-10 13:23:34,927][227910] Mean Reward across all agents: 197.73264101744613[0m
[37m[1m[2023-07-10 13:23:34,927][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:23:40,412][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:23:40,412][227910] Reward + Measures: [[233.73310174   0.96600002   0.0212       0.95960009   0.94189996]
 [263.18005007   0.94309998   0.0411       0.93580002   0.89710009]
 [232.78138313   0.88660002   0.27980003   0.88940001   0.8721    ]
 ...
 [274.1049553    0.95360005   0.0272       0.94620001   0.9188    ]
 [272.31156961   0.95520002   0.0264       0.95100003   0.92570001]
 [226.81438397   0.96499997   0.0289       0.95640004   0.92919999]][0m
[37m[1m[2023-07-10 13:23:40,413][227910] Max Reward on eval: 946.0321061409195[0m
[37m[1m[2023-07-10 13:23:40,413][227910] Min Reward on eval: 77.25098084530327[0m
[37m[1m[2023-07-10 13:23:40,413][227910] Mean Reward across all agents: 245.922037737968[0m
[37m[1m[2023-07-10 13:23:40,413][227910] Average Trajectory Length: 999.3893333333333[0m
[36m[2023-07-10 13:23:40,415][227910] mean_value=-170.49996931700508, max_value=794.1183886684083[0m
[37m[1m[2023-07-10 13:23:40,418][227910] New mean coefficients: [[ 0.05971551 -0.01703236 -0.36828256  0.72756356  1.1374255 ]][0m
[37m[1m[2023-07-10 13:23:40,419][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:23:50,103][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 13:23:50,103][227910] FPS: 396598.61[0m
[36m[2023-07-10 13:23:50,105][227910] itr=484, itrs=2000, Progress: 24.20%[0m
[36m[2023-07-10 13:24:01,579][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 13:24:01,579][227910] FPS: 335181.10[0m
[36m[2023-07-10 13:24:06,421][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:24:06,421][227910] Reward + Measures: [[218.6095438    0.96987516   0.0246771    0.9646253    0.95987254]][0m
[37m[1m[2023-07-10 13:24:06,421][227910] Max Reward on eval: 218.60954380229524[0m
[37m[1m[2023-07-10 13:24:06,422][227910] Min Reward on eval: 218.60954380229524[0m
[37m[1m[2023-07-10 13:24:06,422][227910] Mean Reward across all agents: 218.60954380229524[0m
[37m[1m[2023-07-10 13:24:06,422][227910] Average Trajectory Length: 999.6853333333333[0m
[36m[2023-07-10 13:24:11,890][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:24:11,891][227910] Reward + Measures: [[227.21608937   0.94560003   0.0343       0.94770002   0.92420006]
 [443.64799553   0.77770001   0.1573       0.667        0.5794    ]
 [336.12549375   0.5672524    0.43763334   0.56057143   0.50400478]
 ...
 [477.97220903   0.84890002   0.45310003   0.84209996   0.80590004]
 [ 23.71051637   0.92570001   0.0369       0.9325       0.90090001]
 [172.7188931    0.95979995   0.0258       0.95749998   0.95740002]][0m
[37m[1m[2023-07-10 13:24:11,891][227910] Max Reward on eval: 971.2132851824048[0m
[37m[1m[2023-07-10 13:24:11,891][227910] Min Reward on eval: -180.48609436682892[0m
[37m[1m[2023-07-10 13:24:11,892][227910] Mean Reward across all agents: 236.52195644294878[0m
[37m[1m[2023-07-10 13:24:11,892][227910] Average Trajectory Length: 998.216[0m
[36m[2023-07-10 13:24:11,895][227910] mean_value=-196.52522097695248, max_value=808.3317112282035[0m
[37m[1m[2023-07-10 13:24:11,898][227910] New mean coefficients: [[0.45406967 0.99524593 0.3438118  1.4783912  1.5279322 ]][0m
[37m[1m[2023-07-10 13:24:11,899][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:24:21,745][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 13:24:21,745][227910] FPS: 390066.47[0m
[36m[2023-07-10 13:24:21,748][227910] itr=485, itrs=2000, Progress: 24.25%[0m
[36m[2023-07-10 13:24:33,246][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 13:24:33,246][227910] FPS: 334422.08[0m
[36m[2023-07-10 13:24:37,974][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:24:37,975][227910] Reward + Measures: [[29.4098094   0.83774632  0.037726    0.90093637  0.64817363]][0m
[37m[1m[2023-07-10 13:24:37,975][227910] Max Reward on eval: 29.40980939854408[0m
[37m[1m[2023-07-10 13:24:37,975][227910] Min Reward on eval: 29.40980939854408[0m
[37m[1m[2023-07-10 13:24:37,976][227910] Mean Reward across all agents: 29.40980939854408[0m
[37m[1m[2023-07-10 13:24:37,976][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:24:43,429][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:24:43,481][227910] Reward + Measures: [[87.63287683  0.79430002  0.0408      0.85029995  0.57720006]
 [25.88846128  0.8592      0.045       0.91049999  0.67570001]
 [32.80331619  0.82600003  0.061       0.85900003  0.634     ]
 ...
 [60.6849591   0.80740005  0.0534      0.8641001   0.61800003]
 [20.11789805  0.86870003  0.0374      0.90009993  0.68220001]
 [50.48719686  0.79879999  0.0443      0.83240002  0.5564    ]][0m
[37m[1m[2023-07-10 13:24:43,481][227910] Max Reward on eval: 158.10300288554282[0m
[37m[1m[2023-07-10 13:24:43,481][227910] Min Reward on eval: -102.3051957939344[0m
[37m[1m[2023-07-10 13:24:43,482][227910] Mean Reward across all agents: 51.784617962179624[0m
[37m[1m[2023-07-10 13:24:43,482][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:24:43,485][227910] mean_value=68.02191422163192, max_value=568.2044688704657[0m
[37m[1m[2023-07-10 13:24:43,487][227910] New mean coefficients: [[0.40263844 0.96497464 1.1199214  1.598657   1.0583981 ]][0m
[37m[1m[2023-07-10 13:24:43,488][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:24:53,284][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 13:24:53,284][227910] FPS: 392059.68[0m
[36m[2023-07-10 13:24:53,287][227910] itr=486, itrs=2000, Progress: 24.30%[0m
[36m[2023-07-10 13:25:04,801][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 13:25:04,801][227910] FPS: 333994.68[0m
[36m[2023-07-10 13:25:09,561][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:25:09,561][227910] Reward + Measures: [[103.32981574   0.9129526    0.03296266   0.93290365   0.78979802]][0m
[37m[1m[2023-07-10 13:25:09,561][227910] Max Reward on eval: 103.32981573986847[0m
[37m[1m[2023-07-10 13:25:09,561][227910] Min Reward on eval: 103.32981573986847[0m
[37m[1m[2023-07-10 13:25:09,562][227910] Mean Reward across all agents: 103.32981573986847[0m
[37m[1m[2023-07-10 13:25:09,562][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:25:15,159][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:25:15,160][227910] Reward + Measures: [[153.37816891   0.85789996   0.0523       0.89670002   0.70830005]
 [192.14520903   0.9224       0.0265       0.9325       0.77180004]
 [137.8023501    0.90389997   0.0444       0.90950006   0.77780002]
 ...
 [161.55030586   0.92370003   0.03310001   0.93610001   0.78529996]
 [221.16154486   0.90950006   0.0338       0.92659998   0.75620002]
 [218.47541375   0.90130007   0.0482       0.91180003   0.7529    ]][0m
[37m[1m[2023-07-10 13:25:15,160][227910] Max Reward on eval: 336.5204098642222[0m
[37m[1m[2023-07-10 13:25:15,160][227910] Min Reward on eval: -132.69825240510983[0m
[37m[1m[2023-07-10 13:25:15,160][227910] Mean Reward across all agents: 138.16064192581607[0m
[37m[1m[2023-07-10 13:25:15,161][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:25:15,162][227910] mean_value=-227.52145047429076, max_value=642.4116016073036[0m
[37m[1m[2023-07-10 13:25:15,164][227910] New mean coefficients: [[0.28210026 0.50367856 2.0061255  1.0187355  1.6009489 ]][0m
[37m[1m[2023-07-10 13:25:15,165][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:25:24,888][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 13:25:24,889][227910] FPS: 394990.53[0m
[36m[2023-07-10 13:25:24,891][227910] itr=487, itrs=2000, Progress: 24.35%[0m
[36m[2023-07-10 13:25:36,342][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 13:25:36,342][227910] FPS: 335808.92[0m
[36m[2023-07-10 13:25:41,121][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:25:41,122][227910] Reward + Measures: [[69.00070485  0.9605003   0.027662    0.95536435  0.93739295]][0m
[37m[1m[2023-07-10 13:25:41,122][227910] Max Reward on eval: 69.00070485215689[0m
[37m[1m[2023-07-10 13:25:41,122][227910] Min Reward on eval: 69.00070485215689[0m
[37m[1m[2023-07-10 13:25:41,122][227910] Mean Reward across all agents: 69.00070485215689[0m
[37m[1m[2023-07-10 13:25:41,122][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:25:46,559][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:25:46,559][227910] Reward + Measures: [[-91.52681235   0.94379997   0.0485       0.92770004   0.94509995]
 [ 15.79701159   0.97410005   0.0165       0.96880001   0.95459998]
 [-43.56866985   0.96029997   0.0259       0.95979995   0.95950001]
 ...
 [-64.81142991   0.96680003   0.0241       0.96089995   0.96079999]
 [115.77499391   0.94299996   0.0378       0.93949997   0.90410006]
 [ 50.87816696   0.95620006   0.0369       0.94539994   0.94709998]][0m
[37m[1m[2023-07-10 13:25:46,560][227910] Max Reward on eval: 166.09528629546986[0m
[37m[1m[2023-07-10 13:25:46,560][227910] Min Reward on eval: -164.8510181453894[0m
[37m[1m[2023-07-10 13:25:46,560][227910] Mean Reward across all agents: 22.25589253739205[0m
[37m[1m[2023-07-10 13:25:46,560][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:25:46,561][227910] mean_value=-364.70218622841037, max_value=-143.37146399419336[0m
[36m[2023-07-10 13:25:46,564][227910] XNES is restarting with a new solution whose measures are [0.55743819 0.27980077 0.3320643  0.33383009] and objective is -12.957887884270168[0m
[36m[2023-07-10 13:25:46,565][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 13:25:46,567][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 13:25:46,568][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:25:56,317][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 13:25:56,318][227910] FPS: 393944.92[0m
[36m[2023-07-10 13:25:56,320][227910] itr=488, itrs=2000, Progress: 24.40%[0m
[36m[2023-07-10 13:26:07,955][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 13:26:07,956][227910] FPS: 330579.34[0m
[36m[2023-07-10 13:26:12,691][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:26:12,691][227910] Reward + Measures: [[-653.74577179    0.37534469    0.29769295    0.23717517    0.22731152]][0m
[37m[1m[2023-07-10 13:26:12,692][227910] Max Reward on eval: -653.7457717878491[0m
[37m[1m[2023-07-10 13:26:12,692][227910] Min Reward on eval: -653.7457717878491[0m
[37m[1m[2023-07-10 13:26:12,692][227910] Mean Reward across all agents: -653.7457717878491[0m
[37m[1m[2023-07-10 13:26:12,693][227910] Average Trajectory Length: 784.424[0m
[36m[2023-07-10 13:26:18,251][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:26:18,252][227910] Reward + Measures: [[ -752.60583812     0.37220141     0.29880893     0.24736471
      0.19004922]
 [-1116.03889269     0.33322707     0.24976555     0.31119588
      0.25458044]
 [ -723.03601254     0.33424473     0.28505138     0.22581914
      0.22840504]
 ...
 [ -558.2552127      0.37550917     0.31267524     0.24867944
      0.24735582]
 [-1319.64503514     0.34033266     0.26430267     0.32244354
      0.2587733 ]
 [ -890.55680272     0.72689998     0.64300007     0.71149999
      0.0998    ]][0m
[37m[1m[2023-07-10 13:26:18,252][227910] Max Reward on eval: -246.23559780330396[0m
[37m[1m[2023-07-10 13:26:18,252][227910] Min Reward on eval: -1319.6450351360138[0m
[37m[1m[2023-07-10 13:26:18,252][227910] Mean Reward across all agents: -738.6888976884412[0m
[37m[1m[2023-07-10 13:26:18,253][227910] Average Trajectory Length: 816.2396666666666[0m
[36m[2023-07-10 13:26:18,254][227910] mean_value=-4084.2585817218664, max_value=168.47272393730236[0m
[37m[1m[2023-07-10 13:26:18,257][227910] New mean coefficients: [[-1.808392  -1.3836529 -1.1411366 -2.2971854 -2.381859 ]][0m
[37m[1m[2023-07-10 13:26:18,258][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:26:28,156][227910] train() took 9.90 seconds to complete[0m
[36m[2023-07-10 13:26:28,156][227910] FPS: 388002.32[0m
[36m[2023-07-10 13:26:28,159][227910] itr=489, itrs=2000, Progress: 24.45%[0m
[36m[2023-07-10 13:26:39,744][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 13:26:39,744][227910] FPS: 331950.83[0m
[36m[2023-07-10 13:26:44,445][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:26:44,446][227910] Reward + Measures: [[-780.79211277    0.34621733    0.29334703    0.26895899    0.13237199]][0m
[37m[1m[2023-07-10 13:26:44,446][227910] Max Reward on eval: -780.7921127704155[0m
[37m[1m[2023-07-10 13:26:44,447][227910] Min Reward on eval: -780.7921127704155[0m
[37m[1m[2023-07-10 13:26:44,447][227910] Mean Reward across all agents: -780.7921127704155[0m
[37m[1m[2023-07-10 13:26:44,447][227910] Average Trajectory Length: 716.2343333333333[0m
[36m[2023-07-10 13:26:49,984][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:26:49,984][227910] Reward + Measures: [[ -519.56825424     0.31078175     0.26687661     0.26092538
      0.19235592]
 [ -578.14020375     0.29889822     0.44232312     0.20280938
      0.41237751]
 [-1148.43746577     0.32375821     0.26031443     0.26587996
      0.11183961]
 ...
 [-1650.08746183     0.19097999     0.19130199     0.20765626
      0.14303054]
 [ -875.61678128     0.3218374      0.29022726     0.23788288
      0.14936005]
 [-1162.82365353     0.34031606     0.30655381     0.31636497
      0.13717988]][0m
[37m[1m[2023-07-10 13:26:49,985][227910] Max Reward on eval: -292.03133645353375[0m
[37m[1m[2023-07-10 13:26:49,985][227910] Min Reward on eval: -1820.4184988789027[0m
[37m[1m[2023-07-10 13:26:49,985][227910] Mean Reward across all agents: -783.623985571286[0m
[37m[1m[2023-07-10 13:26:49,985][227910] Average Trajectory Length: 633.4966666666667[0m
[36m[2023-07-10 13:26:49,986][227910] mean_value=-3693.5393702834945, max_value=-310.75404275857244[0m
[36m[2023-07-10 13:26:49,989][227910] XNES is restarting with a new solution whose measures are [0.65539998 0.68790001 0.64799994 0.81259996] and objective is 739.6191621496109[0m
[36m[2023-07-10 13:26:49,990][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 13:26:49,992][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 13:26:49,993][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:26:59,699][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 13:26:59,699][227910] FPS: 395684.55[0m
[36m[2023-07-10 13:26:59,702][227910] itr=490, itrs=2000, Progress: 24.50%[0m
[37m[1m[2023-07-10 13:27:02,417][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000470[0m
[36m[2023-07-10 13:27:14,352][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 13:27:14,352][227910] FPS: 329219.10[0m
[36m[2023-07-10 13:27:19,157][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:27:19,157][227910] Reward + Measures: [[928.92848167   0.56247467   0.54283661   0.54847062   0.63646287]][0m
[37m[1m[2023-07-10 13:27:19,158][227910] Max Reward on eval: 928.9284816687657[0m
[37m[1m[2023-07-10 13:27:19,158][227910] Min Reward on eval: 928.9284816687657[0m
[37m[1m[2023-07-10 13:27:19,158][227910] Mean Reward across all agents: 928.9284816687657[0m
[37m[1m[2023-07-10 13:27:19,158][227910] Average Trajectory Length: 993.0526666666666[0m
[36m[2023-07-10 13:27:24,814][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:27:24,815][227910] Reward + Measures: [[ 104.85105867    0.3775        0.45949998    0.27710003    0.45730001]
 [-840.43685514    0.42945218    0.23607114    0.36730403    0.17028235]
 [1095.90555572    0.3527        0.2906        0.30870003    0.25830004]
 ...
 [ 453.01429085    0.37110177    0.29414967    0.33472154    0.22704367]
 [-930.38321897    0.52393883    0.34770587    0.45143685    0.22064523]
 [-154.06092467    0.43649998    0.50590003    0.33610004    0.52730006]][0m
[37m[1m[2023-07-10 13:27:24,815][227910] Max Reward on eval: 1373.8592495410935[0m
[37m[1m[2023-07-10 13:27:24,815][227910] Min Reward on eval: -1124.9535219643499[0m
[37m[1m[2023-07-10 13:27:24,816][227910] Mean Reward across all agents: 173.71895415639077[0m
[37m[1m[2023-07-10 13:27:24,816][227910] Average Trajectory Length: 950.7553333333333[0m
[36m[2023-07-10 13:27:24,819][227910] mean_value=-1292.5383240796461, max_value=1245.321275623638[0m
[37m[1m[2023-07-10 13:27:24,821][227910] New mean coefficients: [[-0.9462732 -0.4976377 -1.0264618 -1.6485595 -1.0355915]][0m
[37m[1m[2023-07-10 13:27:24,822][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:27:34,600][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 13:27:34,601][227910] FPS: 392780.80[0m
[36m[2023-07-10 13:27:34,603][227910] itr=491, itrs=2000, Progress: 24.55%[0m
[36m[2023-07-10 13:27:46,116][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 13:27:46,117][227910] FPS: 333999.00[0m
[36m[2023-07-10 13:27:50,777][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:27:50,778][227910] Reward + Measures: [[824.46171507   0.55629575   0.50593156   0.5343954    0.59610659]][0m
[37m[1m[2023-07-10 13:27:50,778][227910] Max Reward on eval: 824.4617150723681[0m
[37m[1m[2023-07-10 13:27:50,778][227910] Min Reward on eval: 824.4617150723681[0m
[37m[1m[2023-07-10 13:27:50,778][227910] Mean Reward across all agents: 824.4617150723681[0m
[37m[1m[2023-07-10 13:27:50,779][227910] Average Trajectory Length: 993.8486666666666[0m
[36m[2023-07-10 13:27:56,209][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:27:56,209][227910] Reward + Measures: [[ 487.85495561    0.52570003    0.57569999    0.48660001    0.5535    ]
 [ 423.61082168    0.66160005    0.64420003    0.69469994    0.64019996]
 [ 299.07208316    0.30469999    0.55969995    0.34470004    0.48990002]
 ...
 [ 184.47858819    0.40529999    0.33700001    0.41280004    0.41499996]
 [1655.07023368    0.4585        0.42150003    0.38120002    0.34670001]
 [ 404.63429758    0.46720001    0.70179999    0.47000003    0.68149996]][0m
[37m[1m[2023-07-10 13:27:56,209][227910] Max Reward on eval: 2300.831106032242[0m
[37m[1m[2023-07-10 13:27:56,210][227910] Min Reward on eval: -93.10602613228257[0m
[37m[1m[2023-07-10 13:27:56,210][227910] Mean Reward across all agents: 623.8874021384813[0m
[37m[1m[2023-07-10 13:27:56,210][227910] Average Trajectory Length: 976.7099999999999[0m
[36m[2023-07-10 13:27:56,214][227910] mean_value=-340.6136245293188, max_value=943.3827907190115[0m
[37m[1m[2023-07-10 13:27:56,217][227910] New mean coefficients: [[-0.55820024 -0.95130837 -0.20201397 -2.4148486  -1.3312109 ]][0m
[37m[1m[2023-07-10 13:27:56,218][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:28:05,870][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 13:28:05,870][227910] FPS: 397922.37[0m
[36m[2023-07-10 13:28:05,873][227910] itr=492, itrs=2000, Progress: 24.60%[0m
[36m[2023-07-10 13:28:17,430][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 13:28:17,431][227910] FPS: 332762.93[0m
[36m[2023-07-10 13:28:22,153][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:28:22,154][227910] Reward + Measures: [[701.07012693   0.5326643    0.4889265    0.5024327    0.55823976]][0m
[37m[1m[2023-07-10 13:28:22,154][227910] Max Reward on eval: 701.0701269317642[0m
[37m[1m[2023-07-10 13:28:22,154][227910] Min Reward on eval: 701.0701269317642[0m
[37m[1m[2023-07-10 13:28:22,154][227910] Mean Reward across all agents: 701.0701269317642[0m
[37m[1m[2023-07-10 13:28:22,154][227910] Average Trajectory Length: 995.3396666666666[0m
[36m[2023-07-10 13:28:27,584][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:28:27,585][227910] Reward + Measures: [[ 422.08919656    0.51097393    0.37555408    0.4997611     0.49217781]
 [ 409.51824814    0.67914206    0.52581942    0.66291201    0.65355265]
 [ 273.73746651    0.58891392    0.41304055    0.59247977    0.58281392]
 ...
 [1487.85397179    0.33239999    0.28470001    0.32600001    0.2674    ]
 [1224.59360794    0.36464667    0.2925984     0.38148579    0.28140569]
 [ 726.43715834    0.44670001    0.42379999    0.44280002    0.46739998]][0m
[37m[1m[2023-07-10 13:28:27,585][227910] Max Reward on eval: 1487.8539717865642[0m
[37m[1m[2023-07-10 13:28:27,585][227910] Min Reward on eval: -81.73186435020762[0m
[37m[1m[2023-07-10 13:28:27,585][227910] Mean Reward across all agents: 569.6361177617414[0m
[37m[1m[2023-07-10 13:28:27,586][227910] Average Trajectory Length: 984.6543333333333[0m
[36m[2023-07-10 13:28:27,588][227910] mean_value=-771.2593901058575, max_value=942.521462476125[0m
[37m[1m[2023-07-10 13:28:27,591][227910] New mean coefficients: [[-0.4103614  -1.9351637   0.93590736 -1.2572515  -1.0632997 ]][0m
[37m[1m[2023-07-10 13:28:27,591][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:28:37,313][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 13:28:37,313][227910] FPS: 395065.52[0m
[36m[2023-07-10 13:28:37,316][227910] itr=493, itrs=2000, Progress: 24.65%[0m
[36m[2023-07-10 13:28:48,914][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 13:28:48,915][227910] FPS: 331563.81[0m
[36m[2023-07-10 13:28:53,821][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:28:53,822][227910] Reward + Measures: [[598.17182643   0.46036524   0.47464696   0.42431155   0.4779408 ]][0m
[37m[1m[2023-07-10 13:28:53,822][227910] Max Reward on eval: 598.1718264337369[0m
[37m[1m[2023-07-10 13:28:53,822][227910] Min Reward on eval: 598.1718264337369[0m
[37m[1m[2023-07-10 13:28:53,822][227910] Mean Reward across all agents: 598.1718264337369[0m
[37m[1m[2023-07-10 13:28:53,822][227910] Average Trajectory Length: 993.0103333333333[0m
[36m[2023-07-10 13:28:59,264][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:28:59,264][227910] Reward + Measures: [[528.94055802   0.54570001   0.49400002   0.53310007   0.54450005]
 [287.10769341   0.57100004   0.53420001   0.542        0.57229996]
 [588.12135493   0.49130002   0.43709999   0.45210001   0.51850003]
 ...
 [353.21018218   0.5192436    0.45257822   0.46953565   0.4514277 ]
 [326.51869525   0.31380001   0.32179999   0.29230002   0.31739998]
 [465.25704373   0.4118       0.43920001   0.35699996   0.39030001]][0m
[37m[1m[2023-07-10 13:28:59,265][227910] Max Reward on eval: 1427.518932609877[0m
[37m[1m[2023-07-10 13:28:59,265][227910] Min Reward on eval: -113.95464540058747[0m
[37m[1m[2023-07-10 13:28:59,265][227910] Mean Reward across all agents: 447.23091855058766[0m
[37m[1m[2023-07-10 13:28:59,265][227910] Average Trajectory Length: 989.4373333333333[0m
[36m[2023-07-10 13:28:59,267][227910] mean_value=-943.4567792886938, max_value=579.1953304880255[0m
[37m[1m[2023-07-10 13:28:59,270][227910] New mean coefficients: [[-0.99445826 -1.7153755   1.7854745  -0.39757895  0.62384355]][0m
[37m[1m[2023-07-10 13:28:59,270][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:29:09,011][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 13:29:09,011][227910] FPS: 394301.27[0m
[36m[2023-07-10 13:29:09,013][227910] itr=494, itrs=2000, Progress: 24.70%[0m
[36m[2023-07-10 13:29:20,750][227910] train() took 11.72 seconds to complete[0m
[36m[2023-07-10 13:29:20,750][227910] FPS: 327686.61[0m
[36m[2023-07-10 13:29:25,586][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:29:25,586][227910] Reward + Measures: [[433.55892247   0.45765454   0.49783003   0.42251492   0.4965907 ]][0m
[37m[1m[2023-07-10 13:29:25,586][227910] Max Reward on eval: 433.5589224706938[0m
[37m[1m[2023-07-10 13:29:25,587][227910] Min Reward on eval: 433.5589224706938[0m
[37m[1m[2023-07-10 13:29:25,587][227910] Mean Reward across all agents: 433.5589224706938[0m
[37m[1m[2023-07-10 13:29:25,587][227910] Average Trajectory Length: 992.4733333333332[0m
[36m[2023-07-10 13:29:31,249][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:29:31,249][227910] Reward + Measures: [[  11.43806998    0.56630003    0.3901        0.59100002    0.42500001]
 [-761.9426507     0.36610648    0.42890969    0.40238068    0.43046132]
 [-247.26436966    0.44821748    0.48391753    0.46209708    0.51031536]
 ...
 [ 529.7006662     0.19849999    0.26139998    0.182         0.2263    ]
 [  63.04150226    0.40309927    0.4721992     0.41943517    0.45494089]
 [-349.53081537    0.39990002    0.37670001    0.4172        0.2687    ]][0m
[37m[1m[2023-07-10 13:29:31,250][227910] Max Reward on eval: 1767.2529808905906[0m
[37m[1m[2023-07-10 13:29:31,250][227910] Min Reward on eval: -1013.4595669103088[0m
[37m[1m[2023-07-10 13:29:31,250][227910] Mean Reward across all agents: 99.73745510639941[0m
[37m[1m[2023-07-10 13:29:31,250][227910] Average Trajectory Length: 942.668[0m
[36m[2023-07-10 13:29:31,253][227910] mean_value=-1146.0455913346955, max_value=551.7176957258455[0m
[37m[1m[2023-07-10 13:29:31,255][227910] New mean coefficients: [[ 0.00702482 -1.0503898   1.740091   -1.2048883   0.9804363 ]][0m
[37m[1m[2023-07-10 13:29:31,256][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:29:41,018][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 13:29:41,018][227910] FPS: 393443.86[0m
[36m[2023-07-10 13:29:41,020][227910] itr=495, itrs=2000, Progress: 24.75%[0m
[36m[2023-07-10 13:29:52,544][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 13:29:52,544][227910] FPS: 333671.07[0m
[36m[2023-07-10 13:29:57,224][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:29:57,225][227910] Reward + Measures: [[419.06624951   0.4214415    0.58336806   0.40426305   0.58181059]][0m
[37m[1m[2023-07-10 13:29:57,225][227910] Max Reward on eval: 419.06624950515976[0m
[37m[1m[2023-07-10 13:29:57,225][227910] Min Reward on eval: 419.06624950515976[0m
[37m[1m[2023-07-10 13:29:57,225][227910] Mean Reward across all agents: 419.06624950515976[0m
[37m[1m[2023-07-10 13:29:57,226][227910] Average Trajectory Length: 994.7389999999999[0m
[36m[2023-07-10 13:30:02,632][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:30:02,633][227910] Reward + Measures: [[252.29567151   0.51770002   0.50509995   0.54230005   0.62369996]
 [264.01765144   0.55559999   0.58490002   0.56459999   0.70240003]
 [577.32534219   0.44930002   0.40219998   0.43929997   0.42380005]
 ...
 [508.36993085   0.45229998   0.54820001   0.4443       0.57090002]
 [273.56588654   0.25170001   0.4409       0.29179999   0.40529999]
 [352.42071611   0.3953       0.49829999   0.39560002   0.50019997]][0m
[37m[1m[2023-07-10 13:30:02,633][227910] Max Reward on eval: 1182.9921242829703[0m
[37m[1m[2023-07-10 13:30:02,633][227910] Min Reward on eval: -74.75032352226845[0m
[37m[1m[2023-07-10 13:30:02,634][227910] Mean Reward across all agents: 362.34954345961364[0m
[37m[1m[2023-07-10 13:30:02,634][227910] Average Trajectory Length: 992.6996666666666[0m
[36m[2023-07-10 13:30:02,638][227910] mean_value=-68.7654192399372, max_value=782.177918395678[0m
[37m[1m[2023-07-10 13:30:02,641][227910] New mean coefficients: [[-0.46599066 -0.4944964   2.1201196  -1.6271046   1.976289  ]][0m
[37m[1m[2023-07-10 13:30:02,642][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:30:12,272][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 13:30:12,272][227910] FPS: 398840.63[0m
[36m[2023-07-10 13:30:12,274][227910] itr=496, itrs=2000, Progress: 24.80%[0m
[36m[2023-07-10 13:30:23,754][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 13:30:23,755][227910] FPS: 334951.09[0m
[36m[2023-07-10 13:30:28,554][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:30:28,554][227910] Reward + Measures: [[410.86438498   0.40435368   0.64569044   0.39835864   0.6709286 ]][0m
[37m[1m[2023-07-10 13:30:28,555][227910] Max Reward on eval: 410.864384976242[0m
[37m[1m[2023-07-10 13:30:28,555][227910] Min Reward on eval: 410.864384976242[0m
[37m[1m[2023-07-10 13:30:28,555][227910] Mean Reward across all agents: 410.864384976242[0m
[37m[1m[2023-07-10 13:30:28,555][227910] Average Trajectory Length: 995.2586666666666[0m
[36m[2023-07-10 13:30:34,051][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:30:34,052][227910] Reward + Measures: [[ 227.15148668    0.33720002    0.61089998    0.2304        0.58999997]
 [ 370.27673623    0.29440001    0.442         0.26789999    0.39899999]
 [  71.09108672    0.3524        0.42629996    0.35570002    0.4007    ]
 ...
 [ 501.71965306    0.32212257    0.50491613    0.23201613    0.47458392]
 [-506.84021167    0.37290001    0.29139999    0.32659999    0.29620001]
 [  40.62582358    0.60119998    0.24770002    0.51949996    0.47189999]][0m
[37m[1m[2023-07-10 13:30:34,052][227910] Max Reward on eval: 712.5068977522722[0m
[37m[1m[2023-07-10 13:30:34,052][227910] Min Reward on eval: -1086.3720580575173[0m
[37m[1m[2023-07-10 13:30:34,052][227910] Mean Reward across all agents: -5.036707020676484[0m
[37m[1m[2023-07-10 13:30:34,053][227910] Average Trajectory Length: 980.1803333333334[0m
[36m[2023-07-10 13:30:34,059][227910] mean_value=-766.7679002644538, max_value=878.6543276332872[0m
[37m[1m[2023-07-10 13:30:34,061][227910] New mean coefficients: [[ 0.9287822  -1.3261726   1.8015432  -0.86833763  2.128012  ]][0m
[37m[1m[2023-07-10 13:30:34,062][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:30:43,767][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 13:30:43,767][227910] FPS: 395771.38[0m
[36m[2023-07-10 13:30:43,769][227910] itr=497, itrs=2000, Progress: 24.85%[0m
[36m[2023-07-10 13:30:55,272][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 13:30:55,272][227910] FPS: 334351.93[0m
[36m[2023-07-10 13:31:00,096][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:31:00,096][227910] Reward + Measures: [[419.92481988   0.41856116   0.66083527   0.42220718   0.68975073]][0m
[37m[1m[2023-07-10 13:31:00,097][227910] Max Reward on eval: 419.9248198805624[0m
[37m[1m[2023-07-10 13:31:00,097][227910] Min Reward on eval: 419.9248198805624[0m
[37m[1m[2023-07-10 13:31:00,097][227910] Mean Reward across all agents: 419.9248198805624[0m
[37m[1m[2023-07-10 13:31:00,098][227910] Average Trajectory Length: 997.1246666666666[0m
[36m[2023-07-10 13:31:05,525][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:31:05,526][227910] Reward + Measures: [[194.65745097   0.30070001   0.77059996   0.22749999   0.74000001]
 [420.4439955    0.31890002   0.66790003   0.25730002   0.59089994]
 [170.76315522   0.28910002   0.68480003   0.25709999   0.65510005]
 ...
 [206.56885776   0.32960001   0.65259999   0.27239999   0.64070004]
 [284.72475295   0.28799999   0.63569999   0.28670001   0.6196    ]
 [256.61008634   0.31399998   0.74959999   0.24819998   0.76240003]][0m
[37m[1m[2023-07-10 13:31:05,526][227910] Max Reward on eval: 905.7864709699003[0m
[37m[1m[2023-07-10 13:31:05,526][227910] Min Reward on eval: -398.3537349704129[0m
[37m[1m[2023-07-10 13:31:05,526][227910] Mean Reward across all agents: 340.47558715898504[0m
[37m[1m[2023-07-10 13:31:05,527][227910] Average Trajectory Length: 994.2996666666667[0m
[36m[2023-07-10 13:31:05,532][227910] mean_value=69.18415523217527, max_value=942.8676682922587[0m
[37m[1m[2023-07-10 13:31:05,534][227910] New mean coefficients: [[ 0.10343254 -1.3885442   1.2267926  -1.7851903   2.2581398 ]][0m
[37m[1m[2023-07-10 13:31:05,536][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:31:15,162][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 13:31:15,162][227910] FPS: 398962.57[0m
[36m[2023-07-10 13:31:15,165][227910] itr=498, itrs=2000, Progress: 24.90%[0m
[36m[2023-07-10 13:31:26,610][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 13:31:26,610][227910] FPS: 336008.05[0m
[36m[2023-07-10 13:31:31,339][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:31:31,339][227910] Reward + Measures: [[432.28449665   0.38829216   0.71505076   0.39952779   0.75723398]][0m
[37m[1m[2023-07-10 13:31:31,339][227910] Max Reward on eval: 432.2844966476977[0m
[37m[1m[2023-07-10 13:31:31,340][227910] Min Reward on eval: 432.2844966476977[0m
[37m[1m[2023-07-10 13:31:31,340][227910] Mean Reward across all agents: 432.2844966476977[0m
[37m[1m[2023-07-10 13:31:31,340][227910] Average Trajectory Length: 998.1506666666667[0m
[36m[2023-07-10 13:31:36,718][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:31:36,718][227910] Reward + Measures: [[ 437.65584907    0.42750001    0.75550002    0.45159999    0.81420004]
 [-216.42230586    0.44070002    0.4199        0.38429999    0.38140002]
 [ 304.66131602    0.28569999    0.63440007    0.26050001    0.66049999]
 ...
 [ 292.55538374    0.62260002    0.51390004    0.57920003    0.69410002]
 [ -61.11797974    0.28210002    0.5499        0.28660002    0.56590003]
 [ 127.8167731     0.33266687    0.57670987    0.31640735    0.57009882]][0m
[37m[1m[2023-07-10 13:31:36,718][227910] Max Reward on eval: 681.66443372918[0m
[37m[1m[2023-07-10 13:31:36,719][227910] Min Reward on eval: -836.5627721460012[0m
[37m[1m[2023-07-10 13:31:36,719][227910] Mean Reward across all agents: 94.85566964559384[0m
[37m[1m[2023-07-10 13:31:36,719][227910] Average Trajectory Length: 975.814[0m
[36m[2023-07-10 13:31:36,724][227910] mean_value=-657.9184738522034, max_value=940.2469755139837[0m
[37m[1m[2023-07-10 13:31:36,726][227910] New mean coefficients: [[-0.26291603 -1.4579042   1.3155622  -0.8017264   2.453877  ]][0m
[37m[1m[2023-07-10 13:31:36,728][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:31:46,374][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 13:31:46,374][227910] FPS: 398150.82[0m
[36m[2023-07-10 13:31:46,376][227910] itr=499, itrs=2000, Progress: 24.95%[0m
[36m[2023-07-10 13:31:58,055][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 13:31:58,055][227910] FPS: 329247.83[0m
[36m[2023-07-10 13:32:02,890][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:32:02,890][227910] Reward + Measures: [[424.78999237   0.33387682   0.76671565   0.36298764   0.81110215]][0m
[37m[1m[2023-07-10 13:32:02,891][227910] Max Reward on eval: 424.7899923691691[0m
[37m[1m[2023-07-10 13:32:02,891][227910] Min Reward on eval: 424.7899923691691[0m
[37m[1m[2023-07-10 13:32:02,891][227910] Mean Reward across all agents: 424.7899923691691[0m
[37m[1m[2023-07-10 13:32:02,891][227910] Average Trajectory Length: 999.2313333333333[0m
[36m[2023-07-10 13:32:08,562][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:32:08,563][227910] Reward + Measures: [[242.72960508   0.3585       0.69660002   0.35670003   0.72110003]
 [318.76383644   0.39980003   0.70300001   0.34979999   0.71439999]
 [354.01172257   0.3504       0.6469       0.35970002   0.66070002]
 ...
 [357.92215091   0.3558       0.76169997   0.3116       0.77270001]
 [241.03766408   0.40840003   0.74500006   0.35789999   0.74410003]
 [664.3077024    0.29749998   0.5499       0.2983       0.54229999]][0m
[37m[1m[2023-07-10 13:32:08,563][227910] Max Reward on eval: 664.3077023981198[0m
[37m[1m[2023-07-10 13:32:08,563][227910] Min Reward on eval: -70.73296661678214[0m
[37m[1m[2023-07-10 13:32:08,563][227910] Mean Reward across all agents: 297.6318497513749[0m
[37m[1m[2023-07-10 13:32:08,564][227910] Average Trajectory Length: 998.1026666666667[0m
[36m[2023-07-10 13:32:08,570][227910] mean_value=262.66990804420834, max_value=944.0056967331911[0m
[37m[1m[2023-07-10 13:32:08,573][227910] New mean coefficients: [[ 0.5931606 -1.1778255  1.0101383 -0.6802131  0.9807253]][0m
[37m[1m[2023-07-10 13:32:08,574][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:32:18,345][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 13:32:18,345][227910] FPS: 393057.48[0m
[36m[2023-07-10 13:32:18,348][227910] itr=500, itrs=2000, Progress: 25.00%[0m
[37m[1m[2023-07-10 13:32:21,090][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000480[0m
[36m[2023-07-10 13:32:33,026][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 13:32:33,027][227910] FPS: 328782.69[0m
[36m[2023-07-10 13:32:37,859][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:32:37,860][227910] Reward + Measures: [[426.33491268   0.31316459   0.79844302   0.34235024   0.84214616]][0m
[37m[1m[2023-07-10 13:32:37,860][227910] Max Reward on eval: 426.33491268290487[0m
[37m[1m[2023-07-10 13:32:37,860][227910] Min Reward on eval: 426.33491268290487[0m
[37m[1m[2023-07-10 13:32:37,860][227910] Mean Reward across all agents: 426.33491268290487[0m
[37m[1m[2023-07-10 13:32:37,861][227910] Average Trajectory Length: 998.5806666666666[0m
[36m[2023-07-10 13:32:43,326][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:32:43,327][227910] Reward + Measures: [[376.63585649   0.26990002   0.81949997   0.19340001   0.81170005]
 [336.74229314   0.47119999   0.79860002   0.50760001   0.90869999]
 [557.95746752   0.3382       0.84899998   0.34480003   0.90880007]
 ...
 [340.8281835    0.2379       0.72669995   0.31009999   0.73589998]
 [370.09212106   0.4007       0.73250002   0.43080002   0.74069995]
 [414.08919512   0.49090001   0.73180002   0.50600004   0.83179998]][0m
[37m[1m[2023-07-10 13:32:43,327][227910] Max Reward on eval: 728.0344065562356[0m
[37m[1m[2023-07-10 13:32:43,327][227910] Min Reward on eval: 113.34012032383471[0m
[37m[1m[2023-07-10 13:32:43,328][227910] Mean Reward across all agents: 365.7464179904904[0m
[37m[1m[2023-07-10 13:32:43,328][227910] Average Trajectory Length: 996.887[0m
[36m[2023-07-10 13:32:43,335][227910] mean_value=221.14812042345505, max_value=1057.9574675194629[0m
[37m[1m[2023-07-10 13:32:43,338][227910] New mean coefficients: [[ 0.78569305 -0.6922008  -0.30579185 -0.43935585  1.9229479 ]][0m
[37m[1m[2023-07-10 13:32:43,339][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:32:53,170][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 13:32:53,170][227910] FPS: 390643.32[0m
[36m[2023-07-10 13:32:53,173][227910] itr=501, itrs=2000, Progress: 25.05%[0m
[36m[2023-07-10 13:33:04,730][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 13:33:04,735][227910] FPS: 332775.74[0m
[36m[2023-07-10 13:33:09,492][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:33:09,492][227910] Reward + Measures: [[468.6636515    0.31279123   0.79605144   0.34668747   0.85096842]][0m
[37m[1m[2023-07-10 13:33:09,492][227910] Max Reward on eval: 468.66365149520254[0m
[37m[1m[2023-07-10 13:33:09,493][227910] Min Reward on eval: 468.66365149520254[0m
[37m[1m[2023-07-10 13:33:09,493][227910] Mean Reward across all agents: 468.66365149520254[0m
[37m[1m[2023-07-10 13:33:09,493][227910] Average Trajectory Length: 999.399[0m
[36m[2023-07-10 13:33:14,974][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:33:14,975][227910] Reward + Measures: [[-580.60787807    0.84440005    0.60699999    0.84060001    0.69600004]
 [  95.83921129    0.46060005    0.38370001    0.33649999    0.41560003]
 [-682.13955878    0.57310003    0.65799999    0.62040001    0.59860003]
 ...
 [-125.87486147    0.60769999    0.41149998    0.479         0.47189999]
 [-633.20458638    0.80019999    0.52410001    0.77850002    0.64350003]
 [ 481.68454143    0.33199999    0.82769996    0.36070001    0.88090003]][0m
[37m[1m[2023-07-10 13:33:14,975][227910] Max Reward on eval: 536.9103155481629[0m
[37m[1m[2023-07-10 13:33:14,975][227910] Min Reward on eval: -1065.419188816601[0m
[37m[1m[2023-07-10 13:33:14,975][227910] Mean Reward across all agents: -59.00647584776882[0m
[37m[1m[2023-07-10 13:33:14,976][227910] Average Trajectory Length: 996.7986666666666[0m
[36m[2023-07-10 13:33:14,979][227910] mean_value=-698.1463851963445, max_value=997.3792868041899[0m
[37m[1m[2023-07-10 13:33:14,982][227910] New mean coefficients: [[ 0.524542   -0.79285043  0.04322553 -0.27876216  2.3814518 ]][0m
[37m[1m[2023-07-10 13:33:14,983][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:33:24,491][227910] train() took 9.51 seconds to complete[0m
[36m[2023-07-10 13:33:24,491][227910] FPS: 403954.01[0m
[36m[2023-07-10 13:33:24,493][227910] itr=502, itrs=2000, Progress: 25.10%[0m
[36m[2023-07-10 13:33:35,911][227910] train() took 11.40 seconds to complete[0m
[36m[2023-07-10 13:33:35,911][227910] FPS: 336781.41[0m
[36m[2023-07-10 13:33:40,608][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:33:40,609][227910] Reward + Measures: [[504.17439342   0.28934878   0.8106845    0.34779069   0.86366302]][0m
[37m[1m[2023-07-10 13:33:40,609][227910] Max Reward on eval: 504.17439341654176[0m
[37m[1m[2023-07-10 13:33:40,609][227910] Min Reward on eval: 504.17439341654176[0m
[37m[1m[2023-07-10 13:33:40,610][227910] Mean Reward across all agents: 504.17439341654176[0m
[37m[1m[2023-07-10 13:33:40,610][227910] Average Trajectory Length: 999.0416666666666[0m
[36m[2023-07-10 13:33:46,191][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:33:46,197][227910] Reward + Measures: [[ 408.98167426    0.26019999    0.72490001    0.29299998    0.73110002]
 [-529.71791716    0.34240004    0.27719998    0.29050002    0.29390001]
 [ 195.46143245    0.2543        0.47709998    0.23540001    0.45619997]
 ...
 [ 187.72978953    0.60540003    0.66580003    0.58840007    0.73680001]
 [-577.84796998    0.56190002    0.55260003    0.45360002    0.51359999]
 [-488.83873395    0.54360002    0.47580001    0.38159999    0.47      ]][0m
[37m[1m[2023-07-10 13:33:46,197][227910] Max Reward on eval: 626.4277536693262[0m
[37m[1m[2023-07-10 13:33:46,198][227910] Min Reward on eval: -1177.1273690582486[0m
[37m[1m[2023-07-10 13:33:46,198][227910] Mean Reward across all agents: 7.628924513569464[0m
[37m[1m[2023-07-10 13:33:46,198][227910] Average Trajectory Length: 985.496[0m
[36m[2023-07-10 13:33:46,202][227910] mean_value=-514.0490461920332, max_value=944.3853534786264[0m
[37m[1m[2023-07-10 13:33:46,205][227910] New mean coefficients: [[ 0.3853988  -0.62850225  0.11926252 -0.288023    1.9995759 ]][0m
[37m[1m[2023-07-10 13:33:46,206][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:33:55,853][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 13:33:55,853][227910] FPS: 398117.48[0m
[36m[2023-07-10 13:33:55,856][227910] itr=503, itrs=2000, Progress: 25.15%[0m
[36m[2023-07-10 13:34:07,385][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 13:34:07,385][227910] FPS: 333530.88[0m
[36m[2023-07-10 13:34:12,188][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:34:12,188][227910] Reward + Measures: [[502.40788293   0.26753172   0.83434504   0.34076843   0.89447361]][0m
[37m[1m[2023-07-10 13:34:12,188][227910] Max Reward on eval: 502.4078829299466[0m
[37m[1m[2023-07-10 13:34:12,189][227910] Min Reward on eval: 502.4078829299466[0m
[37m[1m[2023-07-10 13:34:12,189][227910] Mean Reward across all agents: 502.4078829299466[0m
[37m[1m[2023-07-10 13:34:12,189][227910] Average Trajectory Length: 999.4316666666666[0m
[36m[2023-07-10 13:34:17,609][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:34:17,610][227910] Reward + Measures: [[407.71593498   0.39770001   0.75100005   0.4612       0.83820003]
 [524.60908043   0.37069997   0.80269998   0.4461       0.90120012]
 [420.3449094    0.66399997   0.64109999   0.6505       0.79640001]
 ...
 [294.82651149   0.8136999    0.21930002   0.75979996   0.68889999]
 [412.59136219   0.45889997   0.73549998   0.52170002   0.83519995]
 [486.7278057    0.2766       0.67570001   0.3168       0.71730006]][0m
[37m[1m[2023-07-10 13:34:17,610][227910] Max Reward on eval: 1041.5083598083904[0m
[37m[1m[2023-07-10 13:34:17,610][227910] Min Reward on eval: -747.5614312772174[0m
[37m[1m[2023-07-10 13:34:17,610][227910] Mean Reward across all agents: 372.63472174501936[0m
[37m[1m[2023-07-10 13:34:17,611][227910] Average Trajectory Length: 992.2213333333333[0m
[36m[2023-07-10 13:34:17,617][227910] mean_value=-19.099197120320603, max_value=1095.3325853325543[0m
[37m[1m[2023-07-10 13:34:17,620][227910] New mean coefficients: [[ 0.0516935   0.11010396 -0.35009804 -0.13602132  1.7373273 ]][0m
[37m[1m[2023-07-10 13:34:17,621][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:34:27,384][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 13:34:27,384][227910] FPS: 393384.99[0m
[36m[2023-07-10 13:34:27,386][227910] itr=504, itrs=2000, Progress: 25.20%[0m
[36m[2023-07-10 13:34:39,013][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 13:34:39,013][227910] FPS: 330734.89[0m
[36m[2023-07-10 13:34:43,923][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:34:43,924][227910] Reward + Measures: [[492.34079264   0.27184793   0.82784593   0.32816854   0.90639532]][0m
[37m[1m[2023-07-10 13:34:43,924][227910] Max Reward on eval: 492.34079263603587[0m
[37m[1m[2023-07-10 13:34:43,924][227910] Min Reward on eval: 492.34079263603587[0m
[37m[1m[2023-07-10 13:34:43,924][227910] Mean Reward across all agents: 492.34079263603587[0m
[37m[1m[2023-07-10 13:34:43,925][227910] Average Trajectory Length: 999.7723333333333[0m
[36m[2023-07-10 13:34:49,438][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:34:49,438][227910] Reward + Measures: [[408.1123059    0.2184       0.83220005   0.31799999   0.88360006]
 [543.99983002   0.3229       0.81549996   0.3689       0.89600003]
 [363.61302445   0.27079999   0.7665       0.2041       0.82520002]
 ...
 [520.9787823    0.32570001   0.76019996   0.30650002   0.86230004]
 [415.71678464   0.37480003   0.80890006   0.45580003   0.86450005]
 [419.75840955   0.28200001   0.80600005   0.30140001   0.84250003]][0m
[37m[1m[2023-07-10 13:34:49,439][227910] Max Reward on eval: 621.5077962441021[0m
[37m[1m[2023-07-10 13:34:49,439][227910] Min Reward on eval: 205.88319290552172[0m
[37m[1m[2023-07-10 13:34:49,439][227910] Mean Reward across all agents: 455.084287452281[0m
[37m[1m[2023-07-10 13:34:49,439][227910] Average Trajectory Length: 998.7099999999999[0m
[36m[2023-07-10 13:34:49,446][227910] mean_value=404.4342864402271, max_value=1036.9257492470904[0m
[37m[1m[2023-07-10 13:34:49,448][227910] New mean coefficients: [[-0.02224883 -0.4618128  -0.7002671  -0.42495176  1.881152  ]][0m
[37m[1m[2023-07-10 13:34:49,449][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:34:59,119][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 13:34:59,120][227910] FPS: 397164.92[0m
[36m[2023-07-10 13:34:59,122][227910] itr=505, itrs=2000, Progress: 25.25%[0m
[36m[2023-07-10 13:35:10,588][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 13:35:10,588][227910] FPS: 335415.94[0m
[36m[2023-07-10 13:35:15,396][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:35:15,402][227910] Reward + Measures: [[464.56981056   0.25752234   0.81560695   0.32303333   0.91908008]][0m
[37m[1m[2023-07-10 13:35:15,402][227910] Max Reward on eval: 464.56981055522573[0m
[37m[1m[2023-07-10 13:35:15,402][227910] Min Reward on eval: 464.56981055522573[0m
[37m[1m[2023-07-10 13:35:15,402][227910] Mean Reward across all agents: 464.56981055522573[0m
[37m[1m[2023-07-10 13:35:15,403][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:35:20,892][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:35:20,892][227910] Reward + Measures: [[380.21005117   0.60320002   0.62379998   0.56890005   0.84020007]
 [436.39011309   0.27350003   0.7841       0.31660002   0.90230006]
 [443.39587506   0.2915       0.81650013   0.3351       0.89860004]
 ...
 [384.13513147   0.2023       0.72640002   0.2814       0.86949998]
 [504.88802728   0.23200002   0.68680006   0.2881       0.84119999]
 [467.86789554   0.23740001   0.82600003   0.30050001   0.93099993]][0m
[37m[1m[2023-07-10 13:35:20,893][227910] Max Reward on eval: 598.0634820744046[0m
[37m[1m[2023-07-10 13:35:20,893][227910] Min Reward on eval: 196.61884127368685[0m
[37m[1m[2023-07-10 13:35:20,893][227910] Mean Reward across all agents: 431.36026266561964[0m
[37m[1m[2023-07-10 13:35:20,893][227910] Average Trajectory Length: 999.7283333333334[0m
[36m[2023-07-10 13:35:20,898][227910] mean_value=304.0263022833637, max_value=1011.7558975610184[0m
[37m[1m[2023-07-10 13:35:20,901][227910] New mean coefficients: [[-0.19829288 -0.45980722  0.3736682  -0.37708783  1.8064742 ]][0m
[37m[1m[2023-07-10 13:35:20,902][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:35:30,585][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 13:35:30,586][227910] FPS: 396603.98[0m
[36m[2023-07-10 13:35:30,588][227910] itr=506, itrs=2000, Progress: 25.30%[0m
[36m[2023-07-10 13:35:42,322][227910] train() took 11.72 seconds to complete[0m
[36m[2023-07-10 13:35:42,323][227910] FPS: 327680.75[0m
[36m[2023-07-10 13:35:47,140][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:35:47,140][227910] Reward + Measures: [[431.31557445   0.24131799   0.83934397   0.30497667   0.92392701]][0m
[37m[1m[2023-07-10 13:35:47,141][227910] Max Reward on eval: 431.3155744518793[0m
[37m[1m[2023-07-10 13:35:47,141][227910] Min Reward on eval: 431.3155744518793[0m
[37m[1m[2023-07-10 13:35:47,141][227910] Mean Reward across all agents: 431.3155744518793[0m
[37m[1m[2023-07-10 13:35:47,141][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:35:52,828][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:35:52,829][227910] Reward + Measures: [[ 372.02258523    0.29710001    0.79800004    0.39250001    0.91429996]
 [-275.31999185    0.85100001    0.32290003    0.84950012    0.87999994]
 [ 346.74302336    0.27850005    0.72060007    0.2454        0.75150001]
 ...
 [ 383.61970035    0.24769998    0.86730003    0.3012        0.94280005]
 [ 168.12388129    0.18190001    0.77780002    0.2194        0.75629997]
 [ 298.02930785    0.24320002    0.71129996    0.2069        0.70640004]][0m
[37m[1m[2023-07-10 13:35:52,829][227910] Max Reward on eval: 570.352114757261[0m
[37m[1m[2023-07-10 13:35:52,829][227910] Min Reward on eval: -325.55747559518784[0m
[37m[1m[2023-07-10 13:35:52,829][227910] Mean Reward across all agents: 199.3975034501561[0m
[37m[1m[2023-07-10 13:35:52,830][227910] Average Trajectory Length: 999.5146666666666[0m
[36m[2023-07-10 13:35:52,835][227910] mean_value=70.06765670605104, max_value=948.2792399739261[0m
[37m[1m[2023-07-10 13:35:52,838][227910] New mean coefficients: [[ 0.05100788 -0.833518    0.37552118 -0.6846431   2.0204864 ]][0m
[37m[1m[2023-07-10 13:35:52,839][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:36:02,535][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 13:36:02,535][227910] FPS: 396122.14[0m
[36m[2023-07-10 13:36:02,537][227910] itr=507, itrs=2000, Progress: 25.35%[0m
[36m[2023-07-10 13:36:14,243][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 13:36:14,244][227910] FPS: 328549.53[0m
[36m[2023-07-10 13:36:19,057][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:36:19,058][227910] Reward + Measures: [[402.66797657   0.23734099   0.86165565   0.30436632   0.9313677 ]][0m
[37m[1m[2023-07-10 13:36:19,058][227910] Max Reward on eval: 402.66797656726317[0m
[37m[1m[2023-07-10 13:36:19,058][227910] Min Reward on eval: 402.66797656726317[0m
[37m[1m[2023-07-10 13:36:19,058][227910] Mean Reward across all agents: 402.66797656726317[0m
[37m[1m[2023-07-10 13:36:19,059][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:36:24,488][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:36:24,488][227910] Reward + Measures: [[395.80300445   0.27329999   0.83239996   0.31580001   0.92609996]
 [388.32535023   0.30520001   0.86449999   0.29070005   0.94149989]
 [409.39377195   0.49579999   0.82370007   0.52450007   0.93390006]
 ...
 [429.98092211   0.23910001   0.75130004   0.2649       0.87379998]
 [401.27280319   0.60500002   0.78780001   0.5618       0.89979994]
 [417.64182162   0.27340004   0.85149997   0.37600002   0.94279999]][0m
[37m[1m[2023-07-10 13:36:24,488][227910] Max Reward on eval: 559.5653478664229[0m
[37m[1m[2023-07-10 13:36:24,489][227910] Min Reward on eval: 72.90934091213276[0m
[37m[1m[2023-07-10 13:36:24,489][227910] Mean Reward across all agents: 386.9915136009098[0m
[37m[1m[2023-07-10 13:36:24,489][227910] Average Trajectory Length: 999.6806666666666[0m
[36m[2023-07-10 13:36:24,494][227910] mean_value=186.84654399378178, max_value=945.4574098240118[0m
[37m[1m[2023-07-10 13:36:24,497][227910] New mean coefficients: [[ 0.4968235  -0.95723015 -1.4115719  -0.3238081   2.5026052 ]][0m
[37m[1m[2023-07-10 13:36:24,498][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:36:34,225][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 13:36:34,225][227910] FPS: 394855.40[0m
[36m[2023-07-10 13:36:34,227][227910] itr=508, itrs=2000, Progress: 25.40%[0m
[36m[2023-07-10 13:36:45,726][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 13:36:45,726][227910] FPS: 334454.98[0m
[36m[2023-07-10 13:36:50,506][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:36:50,507][227910] Reward + Measures: [[413.5296088    0.20925899   0.83647555   0.30109131   0.93738067]][0m
[37m[1m[2023-07-10 13:36:50,507][227910] Max Reward on eval: 413.52960879787025[0m
[37m[1m[2023-07-10 13:36:50,507][227910] Min Reward on eval: 413.52960879787025[0m
[37m[1m[2023-07-10 13:36:50,507][227910] Mean Reward across all agents: 413.52960879787025[0m
[37m[1m[2023-07-10 13:36:50,507][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:36:56,008][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:36:56,008][227910] Reward + Measures: [[441.32241695   0.2053       0.84330004   0.35179999   0.94490004]
 [416.36126499   0.2599       0.83660001   0.26369998   0.92570001]
 [365.94073617   0.34060001   0.79519999   0.27920005   0.88210005]
 ...
 [449.62853777   0.21110001   0.82139999   0.2687       0.91440004]
 [502.48876885   0.1802       0.82989997   0.28420001   0.92750007]
 [416.54529097   0.25740004   0.8125       0.24660002   0.91380006]][0m
[37m[1m[2023-07-10 13:36:56,009][227910] Max Reward on eval: 520.5566434925422[0m
[37m[1m[2023-07-10 13:36:56,009][227910] Min Reward on eval: 176.31006773870905[0m
[37m[1m[2023-07-10 13:36:56,009][227910] Mean Reward across all agents: 417.56869025910277[0m
[37m[1m[2023-07-10 13:36:56,009][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:36:56,013][227910] mean_value=16.69962349571135, max_value=842.5917168590927[0m
[37m[1m[2023-07-10 13:36:56,015][227910] New mean coefficients: [[ 0.1953645  -0.8571465   0.3219434  -0.29700935  2.335422  ]][0m
[37m[1m[2023-07-10 13:36:56,016][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:37:05,672][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 13:37:05,673][227910] FPS: 397750.85[0m
[36m[2023-07-10 13:37:05,675][227910] itr=509, itrs=2000, Progress: 25.45%[0m
[36m[2023-07-10 13:37:17,141][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 13:37:17,141][227910] FPS: 335353.48[0m
[36m[2023-07-10 13:37:21,987][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:37:21,992][227910] Reward + Measures: [[395.52179722   0.20665732   0.84302068   0.32602465   0.93968993]][0m
[37m[1m[2023-07-10 13:37:21,993][227910] Max Reward on eval: 395.521797222944[0m
[37m[1m[2023-07-10 13:37:21,993][227910] Min Reward on eval: 395.521797222944[0m
[37m[1m[2023-07-10 13:37:21,993][227910] Mean Reward across all agents: 395.521797222944[0m
[37m[1m[2023-07-10 13:37:21,993][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:37:27,495][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:37:27,495][227910] Reward + Measures: [[249.93259692   0.3265       0.81899995   0.4434       0.94470006]
 [519.49414865   0.31689999   0.75060004   0.3256       0.87970001]
 [474.22608056   0.26110002   0.81749994   0.27770004   0.86500007]
 ...
 [359.11392243   0.36740002   0.76499999   0.4003       0.91100007]
 [342.66586755   0.2895       0.76910001   0.28940001   0.88290006]
 [194.93347823   0.50710005   0.79110003   0.59899998   0.91019994]][0m
[37m[1m[2023-07-10 13:37:27,495][227910] Max Reward on eval: 535.9789615789429[0m
[37m[1m[2023-07-10 13:37:27,496][227910] Min Reward on eval: 2.497429058921989[0m
[37m[1m[2023-07-10 13:37:27,496][227910] Mean Reward across all agents: 349.1521707164234[0m
[37m[1m[2023-07-10 13:37:27,496][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:37:27,499][227910] mean_value=16.27540665062234, max_value=818.0497829951833[0m
[37m[1m[2023-07-10 13:37:27,502][227910] New mean coefficients: [[ 0.47117722 -0.9641159   0.42070737  0.22229469  1.781903  ]][0m
[37m[1m[2023-07-10 13:37:27,503][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:37:37,081][227910] train() took 9.58 seconds to complete[0m
[36m[2023-07-10 13:37:37,082][227910] FPS: 400962.20[0m
[36m[2023-07-10 13:37:37,084][227910] itr=510, itrs=2000, Progress: 25.50%[0m
[37m[1m[2023-07-10 13:37:39,804][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000490[0m
[36m[2023-07-10 13:37:51,692][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 13:37:51,692][227910] FPS: 330428.52[0m
[36m[2023-07-10 13:37:56,416][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:37:56,417][227910] Reward + Measures: [[392.99825847   0.15883034   0.874695     0.34707099   0.947469  ]][0m
[37m[1m[2023-07-10 13:37:56,417][227910] Max Reward on eval: 392.9982584680852[0m
[37m[1m[2023-07-10 13:37:56,417][227910] Min Reward on eval: 392.9982584680852[0m
[37m[1m[2023-07-10 13:37:56,418][227910] Mean Reward across all agents: 392.9982584680852[0m
[37m[1m[2023-07-10 13:37:56,418][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:38:02,088][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:38:02,088][227910] Reward + Measures: [[302.53474349   0.57370001   0.6591       0.51920003   0.81269997]
 [378.90694561   0.2309       0.80950004   0.3479       0.86739999]
 [248.39023204   0.51340002   0.62550002   0.5499       0.77150005]
 ...
 [288.9361076    0.24070001   0.55550003   0.23459999   0.61770004]
 [311.96136634   0.1749       0.75950003   0.3019       0.79520005]
 [426.13097725   0.0993       0.85389996   0.32960001   0.92699999]][0m
[37m[1m[2023-07-10 13:38:02,088][227910] Max Reward on eval: 668.4274428599863[0m
[37m[1m[2023-07-10 13:38:02,089][227910] Min Reward on eval: -376.92768252207895[0m
[37m[1m[2023-07-10 13:38:02,089][227910] Mean Reward across all agents: 357.87878577761[0m
[37m[1m[2023-07-10 13:38:02,089][227910] Average Trajectory Length: 982.4336666666667[0m
[36m[2023-07-10 13:38:02,096][227910] mean_value=198.5161564968827, max_value=1141.3551787914826[0m
[37m[1m[2023-07-10 13:38:02,098][227910] New mean coefficients: [[ 0.43328926 -1.1994233   0.02892262  0.19219638  1.4165549 ]][0m
[37m[1m[2023-07-10 13:38:02,099][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:38:11,941][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 13:38:11,942][227910] FPS: 390231.09[0m
[36m[2023-07-10 13:38:11,944][227910] itr=511, itrs=2000, Progress: 25.55%[0m
[36m[2023-07-10 13:38:23,584][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 13:38:23,584][227910] FPS: 330375.06[0m
[36m[2023-07-10 13:38:28,361][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:38:28,361][227910] Reward + Measures: [[399.12771653   0.12180667   0.87455893   0.36879069   0.94677132]][0m
[37m[1m[2023-07-10 13:38:28,362][227910] Max Reward on eval: 399.1277165323184[0m
[37m[1m[2023-07-10 13:38:28,362][227910] Min Reward on eval: 399.1277165323184[0m
[37m[1m[2023-07-10 13:38:28,362][227910] Mean Reward across all agents: 399.1277165323184[0m
[37m[1m[2023-07-10 13:38:28,362][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:38:33,873][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:38:33,874][227910] Reward + Measures: [[425.97917792   0.13309999   0.88309997   0.24590002   0.94319993]
 [357.29344149   0.14860001   0.75949997   0.2723       0.84750003]
 [-64.71494697   0.30195412   0.70066941   0.17955738   0.64515847]
 ...
 [232.15825739   0.57050002   0.77220005   0.61640006   0.85690004]
 [319.27276022   0.36030003   0.80130005   0.1043       0.81309998]
 [440.7304037    0.26440001   0.87130004   0.11800001   0.90700001]][0m
[37m[1m[2023-07-10 13:38:33,874][227910] Max Reward on eval: 528.3984780554077[0m
[37m[1m[2023-07-10 13:38:33,874][227910] Min Reward on eval: -923.662838168093[0m
[37m[1m[2023-07-10 13:38:33,874][227910] Mean Reward across all agents: 191.61661907548546[0m
[37m[1m[2023-07-10 13:38:33,875][227910] Average Trajectory Length: 998.6546666666667[0m
[36m[2023-07-10 13:38:33,881][227910] mean_value=134.46638815491082, max_value=1017.0519363707077[0m
[37m[1m[2023-07-10 13:38:33,883][227910] New mean coefficients: [[ 0.4859547  -0.77889013  0.28488144  0.48762345  2.0685124 ]][0m
[37m[1m[2023-07-10 13:38:33,884][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:38:43,759][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 13:38:43,760][227910] FPS: 388925.18[0m
[36m[2023-07-10 13:38:43,762][227910] itr=512, itrs=2000, Progress: 25.60%[0m
[36m[2023-07-10 13:38:55,367][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 13:38:55,367][227910] FPS: 331428.39[0m
[36m[2023-07-10 13:39:00,233][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:39:00,238][227910] Reward + Measures: [[396.32144821   0.11309633   0.87701732   0.36213532   0.94991332]][0m
[37m[1m[2023-07-10 13:39:00,238][227910] Max Reward on eval: 396.3214482118486[0m
[37m[1m[2023-07-10 13:39:00,238][227910] Min Reward on eval: 396.3214482118486[0m
[37m[1m[2023-07-10 13:39:00,238][227910] Mean Reward across all agents: 396.3214482118486[0m
[37m[1m[2023-07-10 13:39:00,239][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:39:05,751][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:39:05,756][227910] Reward + Measures: [[184.69285288   0.2228       0.83890003   0.41370001   0.91140002]
 [328.82705098   0.16340001   0.83470005   0.4402       0.921     ]
 [412.54523887   0.10320001   0.87300009   0.3608       0.95349997]
 ...
 [355.11654845   0.28459999   0.80070001   0.46350002   0.87589997]
 [425.00788231   0.25650001   0.85470003   0.19330001   0.92070001]
 [370.02935743   0.25210002   0.81980002   0.28280002   0.89640009]][0m
[37m[1m[2023-07-10 13:39:05,757][227910] Max Reward on eval: 521.2102777828579[0m
[37m[1m[2023-07-10 13:39:05,757][227910] Min Reward on eval: 68.52316666919505[0m
[37m[1m[2023-07-10 13:39:05,757][227910] Mean Reward across all agents: 344.5025263729274[0m
[37m[1m[2023-07-10 13:39:05,758][227910] Average Trajectory Length: 999.4143333333333[0m
[36m[2023-07-10 13:39:05,761][227910] mean_value=31.83916498248264, max_value=877.2233646933804[0m
[37m[1m[2023-07-10 13:39:05,763][227910] New mean coefficients: [[ 0.46257383 -0.6681922   0.5983677   0.8544313   2.0979621 ]][0m
[37m[1m[2023-07-10 13:39:05,764][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:39:15,524][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 13:39:15,524][227910] FPS: 393542.51[0m
[36m[2023-07-10 13:39:15,526][227910] itr=513, itrs=2000, Progress: 25.65%[0m
[36m[2023-07-10 13:39:27,155][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 13:39:27,155][227910] FPS: 330673.94[0m
[36m[2023-07-10 13:39:31,966][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:39:31,967][227910] Reward + Measures: [[377.17950159   0.112013     0.8958596    0.415804     0.955028  ]][0m
[37m[1m[2023-07-10 13:39:31,967][227910] Max Reward on eval: 377.1795015943617[0m
[37m[1m[2023-07-10 13:39:31,967][227910] Min Reward on eval: 377.1795015943617[0m
[37m[1m[2023-07-10 13:39:31,967][227910] Mean Reward across all agents: 377.1795015943617[0m
[37m[1m[2023-07-10 13:39:31,968][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:39:37,492][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:39:37,492][227910] Reward + Measures: [[338.55953407   0.12630001   0.8847       0.29070002   0.94480002]
 [260.01817205   0.26110002   0.8757       0.36270002   0.94230002]
 [372.33497337   0.1383       0.88789999   0.2624       0.94999999]
 ...
 [298.61813098   0.2719       0.86580002   0.43199998   0.91650003]
 [361.83228209   0.1231       0.89960003   0.36550003   0.95819998]
 [281.83831054   0.27110001   0.86140007   0.46140003   0.93500006]][0m
[37m[1m[2023-07-10 13:39:37,493][227910] Max Reward on eval: 550.0442529268214[0m
[37m[1m[2023-07-10 13:39:37,493][227910] Min Reward on eval: 104.55653401033487[0m
[37m[1m[2023-07-10 13:39:37,493][227910] Mean Reward across all agents: 342.0812357511578[0m
[37m[1m[2023-07-10 13:39:37,493][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:39:37,497][227910] mean_value=76.25723404423421, max_value=963.4289131880505[0m
[37m[1m[2023-07-10 13:39:37,500][227910] New mean coefficients: [[-0.17663395 -0.46114945  0.6531549   1.0547962   2.4198344 ]][0m
[37m[1m[2023-07-10 13:39:37,501][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:39:47,150][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 13:39:47,150][227910] FPS: 398015.55[0m
[36m[2023-07-10 13:39:47,153][227910] itr=514, itrs=2000, Progress: 25.70%[0m
[36m[2023-07-10 13:39:58,804][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 13:39:58,804][227910] FPS: 330036.47[0m
[36m[2023-07-10 13:40:03,544][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:40:03,544][227910] Reward + Measures: [[379.20464809   0.097586     0.91687208   0.46065766   0.96166301]][0m
[37m[1m[2023-07-10 13:40:03,544][227910] Max Reward on eval: 379.20464808801626[0m
[37m[1m[2023-07-10 13:40:03,544][227910] Min Reward on eval: 379.20464808801626[0m
[37m[1m[2023-07-10 13:40:03,545][227910] Mean Reward across all agents: 379.20464808801626[0m
[37m[1m[2023-07-10 13:40:03,545][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:40:09,115][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:40:09,116][227910] Reward + Measures: [[223.85089881   0.1128       0.89110005   0.35970002   0.93360007]
 [264.50963482   0.25490001   0.87029999   0.45039997   0.9429    ]
 [341.13835233   0.0763       0.8646       0.45469999   0.95039999]
 ...
 [342.64976547   0.20460001   0.91329998   0.4025       0.96259993]
 [320.1958017    0.2658       0.90959996   0.47670004   0.95970005]
 [319.53811262   0.42129999   0.88290006   0.5212       0.94620001]][0m
[37m[1m[2023-07-10 13:40:09,116][227910] Max Reward on eval: 561.2826512228348[0m
[37m[1m[2023-07-10 13:40:09,116][227910] Min Reward on eval: 91.55794179557124[0m
[37m[1m[2023-07-10 13:40:09,116][227910] Mean Reward across all agents: 317.857740752069[0m
[37m[1m[2023-07-10 13:40:09,117][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:40:09,123][227910] mean_value=294.06642960918003, max_value=913.9742563745938[0m
[37m[1m[2023-07-10 13:40:09,126][227910] New mean coefficients: [[0.0897927  0.09423792 1.2242095  0.16142666 2.42589   ]][0m
[37m[1m[2023-07-10 13:40:09,127][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:40:18,928][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 13:40:18,928][227910] FPS: 391843.88[0m
[36m[2023-07-10 13:40:18,931][227910] itr=515, itrs=2000, Progress: 25.75%[0m
[36m[2023-07-10 13:40:30,362][227910] train() took 11.42 seconds to complete[0m
[36m[2023-07-10 13:40:30,362][227910] FPS: 336386.74[0m
[36m[2023-07-10 13:40:35,102][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:40:35,102][227910] Reward + Measures: [[379.04460285   0.11258834   0.92886335   0.47900969   0.96416259]][0m
[37m[1m[2023-07-10 13:40:35,103][227910] Max Reward on eval: 379.0446028502785[0m
[37m[1m[2023-07-10 13:40:35,103][227910] Min Reward on eval: 379.0446028502785[0m
[37m[1m[2023-07-10 13:40:35,103][227910] Mean Reward across all agents: 379.0446028502785[0m
[37m[1m[2023-07-10 13:40:35,103][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:40:40,579][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:40:40,597][227910] Reward + Measures: [[ 369.80613132    0.1031        0.87670004    0.42670003    0.95100003]
 [ 323.74152834    0.82260001    0.3414        0.84539998    0.69200003]
 [ 430.35859405    0.1286        0.91909999    0.44970003    0.9698    ]
 ...
 [ 388.44166301    0.61939996    0.84079999    0.71600002    0.94039994]
 [ 143.94371626    0.67229998    0.48699999    0.67399997    0.56150001]
 [-114.52369195    0.21519999    0.78310001    0.08989999    0.81090003]][0m
[37m[1m[2023-07-10 13:40:40,597][227910] Max Reward on eval: 551.046762242983[0m
[37m[1m[2023-07-10 13:40:40,598][227910] Min Reward on eval: -163.6540270917525[0m
[37m[1m[2023-07-10 13:40:40,598][227910] Mean Reward across all agents: 252.95606541461373[0m
[37m[1m[2023-07-10 13:40:40,598][227910] Average Trajectory Length: 996.204[0m
[36m[2023-07-10 13:40:40,606][227910] mean_value=185.68939379850997, max_value=888.4416630101274[0m
[37m[1m[2023-07-10 13:40:40,609][227910] New mean coefficients: [[0.6330415  0.9238     1.3684211  0.06305347 1.9137566 ]][0m
[37m[1m[2023-07-10 13:40:40,610][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:40:50,356][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 13:40:50,356][227910] FPS: 394078.00[0m
[36m[2023-07-10 13:40:50,359][227910] itr=516, itrs=2000, Progress: 25.80%[0m
[36m[2023-07-10 13:41:02,030][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 13:41:02,030][227910] FPS: 329495.93[0m
[36m[2023-07-10 13:41:06,806][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:41:06,812][227910] Reward + Measures: [[438.57585546   0.15409601   0.93226236   0.43883732   0.96521294]][0m
[37m[1m[2023-07-10 13:41:06,813][227910] Max Reward on eval: 438.5758554572584[0m
[37m[1m[2023-07-10 13:41:06,813][227910] Min Reward on eval: 438.5758554572584[0m
[37m[1m[2023-07-10 13:41:06,813][227910] Mean Reward across all agents: 438.5758554572584[0m
[37m[1m[2023-07-10 13:41:06,814][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:41:12,254][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:41:12,255][227910] Reward + Measures: [[476.57233937   0.3355       0.81980002   0.4664       0.8391    ]
 [350.48607163   0.1445       0.83759993   0.33809999   0.8276    ]
 [372.8983184    0.15840001   0.92929995   0.44980001   0.95699996]
 ...
 [492.26669553   0.25409999   0.91210002   0.4745       0.95339996]
 [473.36493153   0.0949       0.94130003   0.40200001   0.97329998]
 [553.2221727    0.10950001   0.87629998   0.40799999   0.89769995]][0m
[37m[1m[2023-07-10 13:41:12,255][227910] Max Reward on eval: 585.4888829409495[0m
[37m[1m[2023-07-10 13:41:12,255][227910] Min Reward on eval: 166.7770972957369[0m
[37m[1m[2023-07-10 13:41:12,256][227910] Mean Reward across all agents: 438.4615874336114[0m
[37m[1m[2023-07-10 13:41:12,256][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:41:12,263][227910] mean_value=167.43947108998157, max_value=1074.8480051346007[0m
[37m[1m[2023-07-10 13:41:12,265][227910] New mean coefficients: [[ 0.5632329   2.3679795   0.92891526 -0.06900166  1.7391527 ]][0m
[37m[1m[2023-07-10 13:41:12,266][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:41:22,063][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 13:41:22,063][227910] FPS: 392041.05[0m
[36m[2023-07-10 13:41:22,065][227910] itr=517, itrs=2000, Progress: 25.85%[0m
[36m[2023-07-10 13:41:33,770][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 13:41:33,770][227910] FPS: 328529.52[0m
[36m[2023-07-10 13:41:38,548][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:41:38,548][227910] Reward + Measures: [[430.9293908    0.25193      0.91518861   0.40453666   0.96377426]][0m
[37m[1m[2023-07-10 13:41:38,548][227910] Max Reward on eval: 430.929390798188[0m
[37m[1m[2023-07-10 13:41:38,548][227910] Min Reward on eval: 430.929390798188[0m
[37m[1m[2023-07-10 13:41:38,548][227910] Mean Reward across all agents: 430.929390798188[0m
[37m[1m[2023-07-10 13:41:38,549][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:41:43,959][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:41:43,960][227910] Reward + Measures: [[309.97040058   0.48569998   0.59700006   0.29700002   0.66000003]
 [435.86912212   0.29089999   0.83859998   0.4851       0.92530006]
 [239.90663203   0.52660006   0.68470001   0.2472       0.78140002]
 ...
 [568.25854113   0.37280002   0.90760005   0.42050001   0.92980003]
 [317.27797212   0.53517693   0.72176158   0.27395388   0.76378459]
 [514.02504935   0.43600002   0.89960003   0.31750003   0.93260002]][0m
[37m[1m[2023-07-10 13:41:43,960][227910] Max Reward on eval: 1307.419057808735[0m
[37m[1m[2023-07-10 13:41:43,960][227910] Min Reward on eval: 22.612292818853167[0m
[37m[1m[2023-07-10 13:41:43,961][227910] Mean Reward across all agents: 432.70279455239444[0m
[37m[1m[2023-07-10 13:41:43,961][227910] Average Trajectory Length: 992.957[0m
[36m[2023-07-10 13:41:43,971][227910] mean_value=512.3616451032348, max_value=1167.378883308603[0m
[37m[1m[2023-07-10 13:41:43,973][227910] New mean coefficients: [[ 0.7004159   3.0406518   0.99255216 -0.4134249   2.012292  ]][0m
[37m[1m[2023-07-10 13:41:43,974][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:41:53,670][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 13:41:53,670][227910] FPS: 396132.47[0m
[36m[2023-07-10 13:41:53,672][227910] itr=518, itrs=2000, Progress: 25.90%[0m
[36m[2023-07-10 13:42:05,244][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 13:42:05,244][227910] FPS: 332435.35[0m
[36m[2023-07-10 13:42:10,023][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:42:10,029][227910] Reward + Measures: [[383.90709567   0.37925464   0.89840698   0.40726167   0.9602443 ]][0m
[37m[1m[2023-07-10 13:42:10,029][227910] Max Reward on eval: 383.907095669247[0m
[37m[1m[2023-07-10 13:42:10,029][227910] Min Reward on eval: 383.907095669247[0m
[37m[1m[2023-07-10 13:42:10,029][227910] Mean Reward across all agents: 383.907095669247[0m
[37m[1m[2023-07-10 13:42:10,030][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:42:15,531][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:42:15,537][227910] Reward + Measures: [[ 306.47292608    0.32780001    0.91779995    0.4427        0.9691    ]
 [ 117.20330925    0.81590003    0.87830001    0.78300005    0.92620003]
 [ 258.7451422     0.66170007    0.80849999    0.64850003    0.89239997]
 ...
 [ 466.09633485    0.51119995    0.81040001    0.4729        0.95730013]
 [-466.30396684    0.64150006    0.84260005    0.75790006    0.74770004]
 [ 310.30034562    0.2455        0.92840004    0.43989998    0.95830005]][0m
[37m[1m[2023-07-10 13:42:15,537][227910] Max Reward on eval: 616.3555373007432[0m
[37m[1m[2023-07-10 13:42:15,538][227910] Min Reward on eval: -605.0108429058688[0m
[37m[1m[2023-07-10 13:42:15,538][227910] Mean Reward across all agents: 59.1074463393652[0m
[37m[1m[2023-07-10 13:42:15,538][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:42:15,543][227910] mean_value=153.21870449785902, max_value=795.6170203882269[0m
[37m[1m[2023-07-10 13:42:15,546][227910] New mean coefficients: [[ 0.6610491   1.5930307   0.09216386 -0.26123846  1.9082843 ]][0m
[37m[1m[2023-07-10 13:42:15,547][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:42:25,125][227910] train() took 9.58 seconds to complete[0m
[36m[2023-07-10 13:42:25,126][227910] FPS: 400971.40[0m
[36m[2023-07-10 13:42:25,128][227910] itr=519, itrs=2000, Progress: 25.95%[0m
[36m[2023-07-10 13:42:36,722][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 13:42:36,722][227910] FPS: 331737.00[0m
[36m[2023-07-10 13:42:41,433][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:42:41,433][227910] Reward + Measures: [[390.3996403    0.50389284   0.87099618   0.46398517   0.95645505]][0m
[37m[1m[2023-07-10 13:42:41,433][227910] Max Reward on eval: 390.399640304125[0m
[37m[1m[2023-07-10 13:42:41,433][227910] Min Reward on eval: 390.399640304125[0m
[37m[1m[2023-07-10 13:42:41,434][227910] Mean Reward across all agents: 390.399640304125[0m
[37m[1m[2023-07-10 13:42:41,434][227910] Average Trajectory Length: 999.6966666666666[0m
[36m[2023-07-10 13:42:47,037][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:42:47,037][227910] Reward + Measures: [[-555.48555032    0.87069988    0.40300003    0.83740008    0.86850005]
 [  62.22997349    0.6182        0.53879994    0.55470002    0.59939998]
 [ 443.34708062    0.45889997    0.85980004    0.44129997    0.89569998]
 ...
 [  84.69458451    0.58020002    0.60689998    0.48519999    0.64390004]
 [ 447.55240275    0.65440005    0.74419999    0.59600002    0.74250001]
 [-592.67136711    0.85280001    0.23599999    0.83610004    0.81569999]][0m
[37m[1m[2023-07-10 13:42:47,038][227910] Max Reward on eval: 598.7512395595666[0m
[37m[1m[2023-07-10 13:42:47,038][227910] Min Reward on eval: -1454.1930677291007[0m
[37m[1m[2023-07-10 13:42:47,038][227910] Mean Reward across all agents: 2.1391604221343723[0m
[37m[1m[2023-07-10 13:42:47,038][227910] Average Trajectory Length: 999.2076666666667[0m
[36m[2023-07-10 13:42:47,043][227910] mean_value=-295.0200135184445, max_value=819.0036448450992[0m
[37m[1m[2023-07-10 13:42:47,045][227910] New mean coefficients: [[ 0.83233714  1.6809614   0.6537995  -0.41028148  2.3454757 ]][0m
[37m[1m[2023-07-10 13:42:47,046][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:42:56,800][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 13:42:56,801][227910] FPS: 393759.93[0m
[36m[2023-07-10 13:42:56,803][227910] itr=520, itrs=2000, Progress: 26.00%[0m
[37m[1m[2023-07-10 13:42:59,604][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000500[0m
[36m[2023-07-10 13:43:11,353][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 13:43:11,353][227910] FPS: 334593.28[0m
[36m[2023-07-10 13:43:16,139][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:43:16,140][227910] Reward + Measures: [[390.77960838   0.61934799   0.87085569   0.53205264   0.95789832]][0m
[37m[1m[2023-07-10 13:43:16,140][227910] Max Reward on eval: 390.77960838484483[0m
[37m[1m[2023-07-10 13:43:16,140][227910] Min Reward on eval: 390.77960838484483[0m
[37m[1m[2023-07-10 13:43:16,140][227910] Mean Reward across all agents: 390.77960838484483[0m
[37m[1m[2023-07-10 13:43:16,140][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:43:21,584][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:43:21,645][227910] Reward + Measures: [[354.98264467   0.60740006   0.8731001    0.51140004   0.94410002]
 [369.31903375   0.65530002   0.86040002   0.57249999   0.95450002]
 [142.91149548   0.46300003   0.93470001   0.52450001   0.95450002]
 ...
 [351.18605435   0.56610006   0.77740002   0.47610003   0.8854    ]
 [373.90465708   0.69729996   0.88000005   0.58490002   0.93110001]
 [437.04739097   0.60270005   0.82120001   0.55440003   0.95269996]][0m
[37m[1m[2023-07-10 13:43:21,645][227910] Max Reward on eval: 650.6008540072129[0m
[37m[1m[2023-07-10 13:43:21,645][227910] Min Reward on eval: -52.87492396342277[0m
[37m[1m[2023-07-10 13:43:21,646][227910] Mean Reward across all agents: 283.9337782959739[0m
[37m[1m[2023-07-10 13:43:21,646][227910] Average Trajectory Length: 998.8199999999999[0m
[36m[2023-07-10 13:43:21,652][227910] mean_value=374.9806655517012, max_value=939.2497816600487[0m
[37m[1m[2023-07-10 13:43:21,654][227910] New mean coefficients: [[ 1.0611161   1.6040689  -1.0091794  -0.80070937  1.3454342 ]][0m
[37m[1m[2023-07-10 13:43:21,655][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:43:31,281][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 13:43:31,281][227910] FPS: 399018.18[0m
[36m[2023-07-10 13:43:31,283][227910] itr=521, itrs=2000, Progress: 26.05%[0m
[36m[2023-07-10 13:43:42,764][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 13:43:42,764][227910] FPS: 334939.96[0m
[36m[2023-07-10 13:43:47,562][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:43:47,563][227910] Reward + Measures: [[439.47380589   0.61516529   0.81698543   0.48611364   0.95455223]][0m
[37m[1m[2023-07-10 13:43:47,563][227910] Max Reward on eval: 439.4738058883161[0m
[37m[1m[2023-07-10 13:43:47,563][227910] Min Reward on eval: 439.4738058883161[0m
[37m[1m[2023-07-10 13:43:47,563][227910] Mean Reward across all agents: 439.4738058883161[0m
[37m[1m[2023-07-10 13:43:47,564][227910] Average Trajectory Length: 999.6899999999999[0m
[36m[2023-07-10 13:43:53,014][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:43:53,014][227910] Reward + Measures: [[425.22471441   0.60179996   0.86700004   0.45700002   0.95530003]
 [374.21934136   0.59580004   0.91600001   0.5988       0.94410002]
 [349.24155104   0.6006       0.77929997   0.47269997   0.84600002]
 ...
 [364.50698942   0.67450005   0.84250003   0.60640001   0.90869999]
 [474.7138783    0.61154741   0.78889471   0.55822104   0.77001584]
 [321.08544317   0.57129997   0.76280004   0.44369999   0.82649994]][0m
[37m[1m[2023-07-10 13:43:53,014][227910] Max Reward on eval: 608.3487570506928[0m
[37m[1m[2023-07-10 13:43:53,015][227910] Min Reward on eval: 195.40501037203938[0m
[37m[1m[2023-07-10 13:43:53,015][227910] Mean Reward across all agents: 417.6836951052017[0m
[37m[1m[2023-07-10 13:43:53,015][227910] Average Trajectory Length: 998.1213333333333[0m
[36m[2023-07-10 13:43:53,023][227910] mean_value=261.7657586327503, max_value=883.8069460882109[0m
[37m[1m[2023-07-10 13:43:53,026][227910] New mean coefficients: [[ 1.0680401   0.91258156 -0.3076682  -0.50547445  1.734576  ]][0m
[37m[1m[2023-07-10 13:43:53,027][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:44:02,752][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 13:44:02,753][227910] FPS: 394910.57[0m
[36m[2023-07-10 13:44:02,755][227910] itr=522, itrs=2000, Progress: 26.10%[0m
[36m[2023-07-10 13:44:14,318][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 13:44:14,318][227910] FPS: 332593.23[0m
[36m[2023-07-10 13:44:19,028][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:44:19,029][227910] Reward + Measures: [[466.42160778   0.602786     0.79240203   0.44535398   0.95805061]][0m
[37m[1m[2023-07-10 13:44:19,029][227910] Max Reward on eval: 466.42160778277065[0m
[37m[1m[2023-07-10 13:44:19,029][227910] Min Reward on eval: 466.42160778277065[0m
[37m[1m[2023-07-10 13:44:19,029][227910] Mean Reward across all agents: 466.42160778277065[0m
[37m[1m[2023-07-10 13:44:19,030][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:44:24,677][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:44:24,682][227910] Reward + Measures: [[291.30953121   0.88009995   0.92290002   0.84230006   0.95020002]
 [392.70147468   0.79720002   0.7719       0.76010001   0.91530001]
 [387.44949588   0.39180002   0.80330002   0.41100001   0.85799998]
 ...
 [463.23082357   0.55360001   0.78440005   0.4082       0.94950002]
 [439.39199109   0.45229998   0.81669998   0.35820001   0.86709994]
 [375.50018195   0.68830001   0.75529999   0.6577       0.78120005]][0m
[37m[1m[2023-07-10 13:44:24,682][227910] Max Reward on eval: 661.7991948887241[0m
[37m[1m[2023-07-10 13:44:24,683][227910] Min Reward on eval: 10.382256879424677[0m
[37m[1m[2023-07-10 13:44:24,683][227910] Mean Reward across all agents: 359.69377954950374[0m
[37m[1m[2023-07-10 13:44:24,683][227910] Average Trajectory Length: 993.5226666666666[0m
[36m[2023-07-10 13:44:24,691][227910] mean_value=199.64079423511154, max_value=940.4285381603986[0m
[37m[1m[2023-07-10 13:44:24,693][227910] New mean coefficients: [[ 0.43299353  2.09635     0.17500481 -0.19471279  1.5753461 ]][0m
[37m[1m[2023-07-10 13:44:24,694][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:44:34,383][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 13:44:34,383][227910] FPS: 396403.22[0m
[36m[2023-07-10 13:44:34,386][227910] itr=523, itrs=2000, Progress: 26.15%[0m
[36m[2023-07-10 13:44:45,970][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 13:44:45,970][227910] FPS: 332055.56[0m
[36m[2023-07-10 13:44:50,839][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:44:50,840][227910] Reward + Measures: [[450.54078464   0.65576035   0.79793602   0.48650667   0.95805669]][0m
[37m[1m[2023-07-10 13:44:50,840][227910] Max Reward on eval: 450.5407846382729[0m
[37m[1m[2023-07-10 13:44:50,840][227910] Min Reward on eval: 450.5407846382729[0m
[37m[1m[2023-07-10 13:44:50,840][227910] Mean Reward across all agents: 450.5407846382729[0m
[37m[1m[2023-07-10 13:44:50,840][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:44:56,259][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:44:56,260][227910] Reward + Measures: [[302.56711783   0.61720002   0.7554       0.38049999   0.8962    ]
 [243.13390506   0.41160002   0.69349998   0.24770001   0.76290005]
 [344.0495743    0.63270003   0.78570002   0.44060001   0.91890001]
 ...
 [361.19020113   0.47690001   0.74650002   0.28010002   0.8872    ]
 [176.52570448   0.40080005   0.63260001   0.21430002   0.67480004]
 [408.84048702   0.55250001   0.7658       0.37369999   0.88560003]][0m
[37m[1m[2023-07-10 13:44:56,260][227910] Max Reward on eval: 865.5350341092911[0m
[37m[1m[2023-07-10 13:44:56,260][227910] Min Reward on eval: 143.61401102637174[0m
[37m[1m[2023-07-10 13:44:56,261][227910] Mean Reward across all agents: 434.8482954881145[0m
[37m[1m[2023-07-10 13:44:56,261][227910] Average Trajectory Length: 993.7903333333333[0m
[36m[2023-07-10 13:44:56,268][227910] mean_value=245.12307528759382, max_value=969.4494140433205[0m
[37m[1m[2023-07-10 13:44:56,271][227910] New mean coefficients: [[ 0.42199844  2.603294    0.5503771  -0.7803824   1.9517882 ]][0m
[37m[1m[2023-07-10 13:44:56,272][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:45:05,990][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 13:45:05,991][227910] FPS: 395210.33[0m
[36m[2023-07-10 13:45:05,993][227910] itr=524, itrs=2000, Progress: 26.20%[0m
[36m[2023-07-10 13:45:17,601][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 13:45:17,602][227910] FPS: 331246.35[0m
[36m[2023-07-10 13:45:22,296][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:45:22,296][227910] Reward + Measures: [[446.2770014    0.71922231   0.81854934   0.52342433   0.95567966]][0m
[37m[1m[2023-07-10 13:45:22,297][227910] Max Reward on eval: 446.2770014006514[0m
[37m[1m[2023-07-10 13:45:22,297][227910] Min Reward on eval: 446.2770014006514[0m
[37m[1m[2023-07-10 13:45:22,297][227910] Mean Reward across all agents: 446.2770014006514[0m
[37m[1m[2023-07-10 13:45:22,297][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:45:27,721][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:45:27,722][227910] Reward + Measures: [[545.83559612   0.71619999   0.84070009   0.59130001   0.91709995]
 [575.24951449   0.71889997   0.75220007   0.59560007   0.83040011]
 [256.94452215   0.8768       0.87459993   0.83610004   0.90560001]
 ...
 [492.87332758   0.67329997   0.73679996   0.4869       0.90679997]
 [386.39283652   0.79159999   0.90490001   0.67259997   0.96450007]
 [397.80653774   0.76640004   0.89050001   0.68079996   0.93000001]][0m
[37m[1m[2023-07-10 13:45:27,722][227910] Max Reward on eval: 737.6910879977047[0m
[37m[1m[2023-07-10 13:45:27,722][227910] Min Reward on eval: 202.08313414275182[0m
[37m[1m[2023-07-10 13:45:27,722][227910] Mean Reward across all agents: 423.3126215170911[0m
[37m[1m[2023-07-10 13:45:27,723][227910] Average Trajectory Length: 999.0866666666666[0m
[36m[2023-07-10 13:45:27,730][227910] mean_value=277.8080798012695, max_value=970.512470452895[0m
[37m[1m[2023-07-10 13:45:27,733][227910] New mean coefficients: [[ 0.17114785  2.4774075  -0.818754   -1.3754141   2.073214  ]][0m
[37m[1m[2023-07-10 13:45:27,734][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:45:37,370][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 13:45:37,371][227910] FPS: 398546.29[0m
[36m[2023-07-10 13:45:37,373][227910] itr=525, itrs=2000, Progress: 26.25%[0m
[36m[2023-07-10 13:45:49,086][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 13:45:49,086][227910] FPS: 328287.07[0m
[36m[2023-07-10 13:45:53,817][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:45:53,817][227910] Reward + Measures: [[444.98087239   0.7239536    0.78835821   0.49950564   0.95405763]][0m
[37m[1m[2023-07-10 13:45:53,817][227910] Max Reward on eval: 444.9808723907421[0m
[37m[1m[2023-07-10 13:45:53,817][227910] Min Reward on eval: 444.9808723907421[0m
[37m[1m[2023-07-10 13:45:53,818][227910] Mean Reward across all agents: 444.9808723907421[0m
[37m[1m[2023-07-10 13:45:53,818][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:45:59,216][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:45:59,222][227910] Reward + Measures: [[451.65067119   0.75129998   0.79120004   0.55919999   0.96189994]
 [510.54214485   0.61210006   0.8519001    0.50749999   0.90500003]
 [406.60650245   0.75150007   0.85410005   0.55980003   0.96900004]
 ...
 [479.08485409   0.73900002   0.77670002   0.52170002   0.92589998]
 [633.97454988   0.6674       0.77769995   0.5327       0.83610004]
 [481.24252518   0.67000002   0.80059999   0.55690002   0.9479    ]][0m
[37m[1m[2023-07-10 13:45:59,223][227910] Max Reward on eval: 809.3083577473183[0m
[37m[1m[2023-07-10 13:45:59,224][227910] Min Reward on eval: 254.53363353504102[0m
[37m[1m[2023-07-10 13:45:59,224][227910] Mean Reward across all agents: 459.74567846418864[0m
[37m[1m[2023-07-10 13:45:59,225][227910] Average Trajectory Length: 999.2816666666666[0m
[36m[2023-07-10 13:45:59,234][227910] mean_value=56.273000943780524, max_value=1083.7111732559279[0m
[37m[1m[2023-07-10 13:45:59,239][227910] New mean coefficients: [[ 0.6942978   2.2802324  -0.7875961  -0.97359693  0.97691   ]][0m
[37m[1m[2023-07-10 13:45:59,240][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:46:09,098][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 13:46:09,098][227910] FPS: 389634.77[0m
[36m[2023-07-10 13:46:09,100][227910] itr=526, itrs=2000, Progress: 26.30%[0m
[36m[2023-07-10 13:46:20,785][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 13:46:20,785][227910] FPS: 329099.46[0m
[36m[2023-07-10 13:46:25,549][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:46:25,550][227910] Reward + Measures: [[468.45333841   0.73736954   0.76678854   0.49279767   0.95385766]][0m
[37m[1m[2023-07-10 13:46:25,550][227910] Max Reward on eval: 468.45333841201665[0m
[37m[1m[2023-07-10 13:46:25,550][227910] Min Reward on eval: 468.45333841201665[0m
[37m[1m[2023-07-10 13:46:25,550][227910] Mean Reward across all agents: 468.45333841201665[0m
[37m[1m[2023-07-10 13:46:25,550][227910] Average Trajectory Length: 999.4513333333333[0m
[36m[2023-07-10 13:46:31,089][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:46:31,089][227910] Reward + Measures: [[458.29139916   0.77330005   0.81169999   0.59000003   0.95109999]
 [671.02284797   0.78400004   0.77930003   0.70489997   0.84090006]
 [503.32491017   0.73610002   0.77220005   0.55159998   0.93950003]
 ...
 [518.42106247   0.72170001   0.7177       0.48079997   0.91129988]
 [456.42061429   0.736        0.76560003   0.49109998   0.95370001]
 [409.33002833   0.7317       0.76370001   0.54720002   0.95249999]][0m
[37m[1m[2023-07-10 13:46:31,090][227910] Max Reward on eval: 1187.8899527873612[0m
[37m[1m[2023-07-10 13:46:31,090][227910] Min Reward on eval: 326.762278382061[0m
[37m[1m[2023-07-10 13:46:31,090][227910] Mean Reward across all agents: 525.5977872672566[0m
[37m[1m[2023-07-10 13:46:31,090][227910] Average Trajectory Length: 998.6666666666666[0m
[36m[2023-07-10 13:46:31,097][227910] mean_value=229.7944357922697, max_value=1162.4885438858066[0m
[37m[1m[2023-07-10 13:46:31,100][227910] New mean coefficients: [[ 0.57500345  2.3604832  -2.4734836  -0.9641764   0.06627887]][0m
[37m[1m[2023-07-10 13:46:31,101][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:46:40,821][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 13:46:40,821][227910] FPS: 395135.84[0m
[36m[2023-07-10 13:46:40,823][227910] itr=527, itrs=2000, Progress: 26.35%[0m
[36m[2023-07-10 13:46:52,483][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 13:46:52,483][227910] FPS: 329790.43[0m
[36m[2023-07-10 13:46:57,235][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:46:57,235][227910] Reward + Measures: [[628.10475748   0.68093055   0.63500416   0.46871939   0.83348495]][0m
[37m[1m[2023-07-10 13:46:57,235][227910] Max Reward on eval: 628.1047574828544[0m
[37m[1m[2023-07-10 13:46:57,235][227910] Min Reward on eval: 628.1047574828544[0m
[37m[1m[2023-07-10 13:46:57,236][227910] Mean Reward across all agents: 628.1047574828544[0m
[37m[1m[2023-07-10 13:46:57,236][227910] Average Trajectory Length: 995.6016666666667[0m
[36m[2023-07-10 13:47:02,724][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:47:02,724][227910] Reward + Measures: [[745.82971347   0.6577       0.64889997   0.50819999   0.73660004]
 [715.40415707   0.63709998   0.55550003   0.55660003   0.625     ]
 [570.91596296   0.58520001   0.61860001   0.42740002   0.75010002]
 ...
 [587.49927424   0.6645       0.59860003   0.50640005   0.76280004]
 [451.52177113   0.63959998   0.65130001   0.43610001   0.78560001]
 [540.69175918   0.65609998   0.61700004   0.4664       0.79249996]][0m
[37m[1m[2023-07-10 13:47:02,725][227910] Max Reward on eval: 1136.5531530522742[0m
[37m[1m[2023-07-10 13:47:02,725][227910] Min Reward on eval: 159.02601356178056[0m
[37m[1m[2023-07-10 13:47:02,725][227910] Mean Reward across all agents: 630.6219827993883[0m
[37m[1m[2023-07-10 13:47:02,725][227910] Average Trajectory Length: 994.399[0m
[36m[2023-07-10 13:47:02,731][227910] mean_value=346.1077085590588, max_value=1244.9489102928746[0m
[37m[1m[2023-07-10 13:47:02,735][227910] New mean coefficients: [[ 0.5897954   1.1707568  -3.2974167  -1.3314362  -0.68741995]][0m
[37m[1m[2023-07-10 13:47:02,736][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:47:12,477][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 13:47:12,477][227910] FPS: 394269.82[0m
[36m[2023-07-10 13:47:12,479][227910] itr=528, itrs=2000, Progress: 26.40%[0m
[36m[2023-07-10 13:47:24,070][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 13:47:24,070][227910] FPS: 331762.09[0m
[36m[2023-07-10 13:47:28,854][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:47:28,854][227910] Reward + Measures: [[796.62322388   0.62697226   0.55518597   0.44481453   0.75011057]][0m
[37m[1m[2023-07-10 13:47:28,855][227910] Max Reward on eval: 796.6232238837979[0m
[37m[1m[2023-07-10 13:47:28,855][227910] Min Reward on eval: 796.6232238837979[0m
[37m[1m[2023-07-10 13:47:28,855][227910] Mean Reward across all agents: 796.6232238837979[0m
[37m[1m[2023-07-10 13:47:28,855][227910] Average Trajectory Length: 992.2053333333333[0m
[36m[2023-07-10 13:47:34,340][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:47:34,341][227910] Reward + Measures: [[811.50699336   0.65000004   0.53660005   0.53069997   0.64880002]
 [818.89824382   0.60980004   0.51789999   0.4815       0.64390004]
 [837.1826773    0.61470002   0.56940001   0.5582       0.60579997]
 ...
 [969.41019244   0.54840004   0.51120007   0.46510002   0.56779999]
 [791.95859131   0.60849231   0.56362313   0.47834617   0.59196156]
 [611.22173221   0.54290003   0.54320002   0.41510001   0.60409999]][0m
[37m[1m[2023-07-10 13:47:34,341][227910] Max Reward on eval: 1366.997311001236[0m
[37m[1m[2023-07-10 13:47:34,341][227910] Min Reward on eval: 126.40204130813945[0m
[37m[1m[2023-07-10 13:47:34,342][227910] Mean Reward across all agents: 715.4099105508662[0m
[37m[1m[2023-07-10 13:47:34,342][227910] Average Trajectory Length: 979.284[0m
[36m[2023-07-10 13:47:34,348][227910] mean_value=81.0939994196796, max_value=1407.6901080061673[0m
[37m[1m[2023-07-10 13:47:34,351][227910] New mean coefficients: [[ 0.70734376  0.07141805 -3.300187   -1.3821373  -0.54404795]][0m
[37m[1m[2023-07-10 13:47:34,352][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:47:44,034][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 13:47:44,035][227910] FPS: 396676.28[0m
[36m[2023-07-10 13:47:44,037][227910] itr=529, itrs=2000, Progress: 26.45%[0m
[36m[2023-07-10 13:47:55,511][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 13:47:55,511][227910] FPS: 335200.61[0m
[36m[2023-07-10 13:48:00,271][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:48:00,272][227910] Reward + Measures: [[1013.46202206    0.54931468    0.48055252    0.38502869    0.6504367 ]][0m
[37m[1m[2023-07-10 13:48:00,272][227910] Max Reward on eval: 1013.4620220587989[0m
[37m[1m[2023-07-10 13:48:00,272][227910] Min Reward on eval: 1013.4620220587989[0m
[37m[1m[2023-07-10 13:48:00,272][227910] Mean Reward across all agents: 1013.4620220587989[0m
[37m[1m[2023-07-10 13:48:00,273][227910] Average Trajectory Length: 988.2986666666666[0m
[36m[2023-07-10 13:48:05,720][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:48:05,726][227910] Reward + Measures: [[745.02974708   0.55514258   0.32219142   0.56737012   0.49071136]
 [315.74092492   0.44309345   0.23245168   0.42777553   0.34506962]
 [682.8703914    0.42449999   0.69880003   0.44280002   0.67290002]
 ...
 [814.67734565   0.41079074   0.2913909    0.39764047   0.32329828]
 [704.85915949   0.46939999   0.36920002   0.57569999   0.37820002]
 [617.09799951   0.44420001   0.37009999   0.45190001   0.30749997]][0m
[37m[1m[2023-07-10 13:48:05,726][227910] Max Reward on eval: 1351.2479176811873[0m
[37m[1m[2023-07-10 13:48:05,727][227910] Min Reward on eval: 243.16397123542265[0m
[37m[1m[2023-07-10 13:48:05,727][227910] Mean Reward across all agents: 737.3540114481323[0m
[37m[1m[2023-07-10 13:48:05,727][227910] Average Trajectory Length: 979.3206666666666[0m
[36m[2023-07-10 13:48:05,733][227910] mean_value=-204.16238389989437, max_value=1510.4318850488505[0m
[37m[1m[2023-07-10 13:48:05,736][227910] New mean coefficients: [[ 0.7395517   0.16967435 -3.3890138  -1.621301   -0.16385022]][0m
[37m[1m[2023-07-10 13:48:05,737][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:48:15,440][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 13:48:15,440][227910] FPS: 395832.43[0m
[36m[2023-07-10 13:48:15,442][227910] itr=530, itrs=2000, Progress: 26.50%[0m
[37m[1m[2023-07-10 13:48:18,447][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000510[0m
[36m[2023-07-10 13:48:30,294][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 13:48:30,294][227910] FPS: 331270.95[0m
[36m[2023-07-10 13:48:34,982][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:48:34,982][227910] Reward + Measures: [[1432.32368718    0.4902215     0.40139419    0.35129452    0.5359531 ]][0m
[37m[1m[2023-07-10 13:48:34,982][227910] Max Reward on eval: 1432.3236871814254[0m
[37m[1m[2023-07-10 13:48:34,982][227910] Min Reward on eval: 1432.3236871814254[0m
[37m[1m[2023-07-10 13:48:34,983][227910] Mean Reward across all agents: 1432.3236871814254[0m
[37m[1m[2023-07-10 13:48:34,983][227910] Average Trajectory Length: 984.6403333333333[0m
[36m[2023-07-10 13:48:40,361][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:48:40,361][227910] Reward + Measures: [[636.06690056   0.53079998   0.53773338   0.54310006   0.47633335]
 [607.41413278   0.31865457   0.43174544   0.32770908   0.38588184]
 [719.43937479   0.7493       0.68600005   0.72390002   0.71200001]
 ...
 [544.81588139   0.31149998   0.55669999   0.42630002   0.56949997]
 [ 33.21967002   0.74220002   0.5546       0.74280006   0.68110001]
 [423.82460225   0.82369995   0.73970002   0.8057       0.79790002]][0m
[37m[1m[2023-07-10 13:48:40,361][227910] Max Reward on eval: 1792.7667698240607[0m
[37m[1m[2023-07-10 13:48:40,362][227910] Min Reward on eval: -447.6251509369351[0m
[37m[1m[2023-07-10 13:48:40,362][227910] Mean Reward across all agents: 706.2681682140768[0m
[37m[1m[2023-07-10 13:48:40,362][227910] Average Trajectory Length: 986.4966666666667[0m
[36m[2023-07-10 13:48:40,368][227910] mean_value=-107.11888169511272, max_value=1930.6355774305416[0m
[37m[1m[2023-07-10 13:48:40,370][227910] New mean coefficients: [[ 0.9037919   0.19788003 -2.6798692  -1.3100209  -0.27376533]][0m
[37m[1m[2023-07-10 13:48:40,371][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:48:50,115][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 13:48:50,115][227910] FPS: 394172.62[0m
[36m[2023-07-10 13:48:50,118][227910] itr=531, itrs=2000, Progress: 26.55%[0m
[36m[2023-07-10 13:49:01,547][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 13:49:01,547][227910] FPS: 336452.12[0m
[36m[2023-07-10 13:49:06,250][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:49:06,251][227910] Reward + Measures: [[1872.95471589    0.44611168    0.35633415    0.32693547    0.43213248]][0m
[37m[1m[2023-07-10 13:49:06,251][227910] Max Reward on eval: 1872.9547158949567[0m
[37m[1m[2023-07-10 13:49:06,251][227910] Min Reward on eval: 1872.9547158949567[0m
[37m[1m[2023-07-10 13:49:06,252][227910] Mean Reward across all agents: 1872.9547158949567[0m
[37m[1m[2023-07-10 13:49:06,252][227910] Average Trajectory Length: 982.2033333333333[0m
[36m[2023-07-10 13:49:11,711][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:49:11,711][227910] Reward + Measures: [[1439.27216695    0.36701789    0.43966731    0.24757449    0.34609082]
 [1971.97088896    0.4858        0.4237        0.42439994    0.40799999]
 [2163.43132797    0.44409999    0.3556        0.35500002    0.40130001]
 ...
 [1668.6645712     0.54860002    0.45699999    0.49149999    0.48840004]
 [1414.06117568    0.63300002    0.46329999    0.59240001    0.55560005]
 [1865.09890627    0.39820001    0.41530004    0.30720001    0.36129999]][0m
[37m[1m[2023-07-10 13:49:11,711][227910] Max Reward on eval: 2629.1955723100573[0m
[37m[1m[2023-07-10 13:49:11,712][227910] Min Reward on eval: 91.54450374050066[0m
[37m[1m[2023-07-10 13:49:11,712][227910] Mean Reward across all agents: 1450.576699013957[0m
[37m[1m[2023-07-10 13:49:11,712][227910] Average Trajectory Length: 981.6619999999999[0m
[36m[2023-07-10 13:49:11,718][227910] mean_value=-167.16000374288473, max_value=2080.703262394656[0m
[37m[1m[2023-07-10 13:49:11,720][227910] New mean coefficients: [[ 0.75694394  1.1461844  -1.0615064  -0.9913318  -0.10748076]][0m
[37m[1m[2023-07-10 13:49:11,721][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:49:21,531][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 13:49:21,531][227910] FPS: 391533.59[0m
[36m[2023-07-10 13:49:21,533][227910] itr=532, itrs=2000, Progress: 26.60%[0m
[36m[2023-07-10 13:49:33,065][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 13:49:33,066][227910] FPS: 333442.87[0m
[36m[2023-07-10 13:49:37,814][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:49:37,815][227910] Reward + Measures: [[2363.29636474    0.422254      0.3348155     0.313416      0.31026697]][0m
[37m[1m[2023-07-10 13:49:37,815][227910] Max Reward on eval: 2363.296364736131[0m
[37m[1m[2023-07-10 13:49:37,815][227910] Min Reward on eval: 2363.296364736131[0m
[37m[1m[2023-07-10 13:49:37,815][227910] Mean Reward across all agents: 2363.296364736131[0m
[37m[1m[2023-07-10 13:49:37,816][227910] Average Trajectory Length: 983.1193333333333[0m
[36m[2023-07-10 13:49:43,263][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:49:43,264][227910] Reward + Measures: [[1808.06457296    0.39120001    0.40019998    0.27560002    0.36149999]
 [2029.20807374    0.4259823     0.30654189    0.37324634    0.25476381]
 [2035.04122142    0.39825433    0.30987856    0.33040389    0.21208774]
 ...
 [2401.23047826    0.41369945    0.32389364    0.30931434    0.2107573 ]
 [1675.95067991    0.57740003    0.421         0.49769998    0.46750003]
 [1624.82845399    0.52290004    0.37910002    0.43870002    0.42679998]][0m
[37m[1m[2023-07-10 13:49:43,264][227910] Max Reward on eval: 2772.4305933916594[0m
[37m[1m[2023-07-10 13:49:43,264][227910] Min Reward on eval: 404.6110294185695[0m
[37m[1m[2023-07-10 13:49:43,264][227910] Mean Reward across all agents: 1799.4335694511065[0m
[37m[1m[2023-07-10 13:49:43,265][227910] Average Trajectory Length: 977.635[0m
[36m[2023-07-10 13:49:43,269][227910] mean_value=-515.4556979520534, max_value=1712.9513233117978[0m
[37m[1m[2023-07-10 13:49:43,271][227910] New mean coefficients: [[ 0.73017395  0.9012798   1.3785328  -0.6681448   0.91626096]][0m
[37m[1m[2023-07-10 13:49:43,272][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:49:53,074][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 13:49:53,074][227910] FPS: 391835.91[0m
[36m[2023-07-10 13:49:53,077][227910] itr=533, itrs=2000, Progress: 26.65%[0m
[36m[2023-07-10 13:50:04,590][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 13:50:04,590][227910] FPS: 334067.00[0m
[36m[2023-07-10 13:50:09,451][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:50:09,452][227910] Reward + Measures: [[2757.98645287    0.41508472    0.3324573     0.29822028    0.2515907 ]][0m
[37m[1m[2023-07-10 13:50:09,452][227910] Max Reward on eval: 2757.9864528691724[0m
[37m[1m[2023-07-10 13:50:09,452][227910] Min Reward on eval: 2757.9864528691724[0m
[37m[1m[2023-07-10 13:50:09,452][227910] Mean Reward across all agents: 2757.9864528691724[0m
[37m[1m[2023-07-10 13:50:09,453][227910] Average Trajectory Length: 982.111[0m
[36m[2023-07-10 13:50:14,909][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:50:14,910][227910] Reward + Measures: [[1897.48076398    0.373         0.41850004    0.22830001    0.31800002]
 [ 364.53787637    0.69760007    0.76420003    0.6239        0.69679999]
 [2477.21009766    0.36180004    0.3231        0.26840001    0.21890001]
 ...
 [1324.64813825    0.52640003    0.54790002    0.46059999    0.45959997]
 [ 862.70732927    0.66250002    0.71540004    0.60890001    0.65539998]
 [ 510.86927322    0.72819996    0.74940002    0.6753        0.71720004]][0m
[37m[1m[2023-07-10 13:50:14,910][227910] Max Reward on eval: 3231.4411927078154[0m
[37m[1m[2023-07-10 13:50:14,911][227910] Min Reward on eval: -1054.1610665319429[0m
[37m[1m[2023-07-10 13:50:14,911][227910] Mean Reward across all agents: 1315.1981587756406[0m
[37m[1m[2023-07-10 13:50:14,911][227910] Average Trajectory Length: 986.9826666666667[0m
[36m[2023-07-10 13:50:14,916][227910] mean_value=-262.6031498472297, max_value=2027.7597337825416[0m
[37m[1m[2023-07-10 13:50:14,919][227910] New mean coefficients: [[ 0.9450349   0.29075736  2.5805511  -0.4207717   1.2458843 ]][0m
[37m[1m[2023-07-10 13:50:14,920][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:50:24,616][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 13:50:24,616][227910] FPS: 396101.22[0m
[36m[2023-07-10 13:50:24,618][227910] itr=534, itrs=2000, Progress: 26.70%[0m
[36m[2023-07-10 13:50:36,186][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 13:50:36,187][227910] FPS: 332490.27[0m
[36m[2023-07-10 13:50:41,019][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:50:41,019][227910] Reward + Measures: [[3179.44886665    0.41410416    0.33164233    0.2834844     0.22538146]][0m
[37m[1m[2023-07-10 13:50:41,019][227910] Max Reward on eval: 3179.4488666468487[0m
[37m[1m[2023-07-10 13:50:41,019][227910] Min Reward on eval: 3179.4488666468487[0m
[37m[1m[2023-07-10 13:50:41,020][227910] Mean Reward across all agents: 3179.4488666468487[0m
[37m[1m[2023-07-10 13:50:41,020][227910] Average Trajectory Length: 983.7816666666666[0m
[36m[2023-07-10 13:50:46,715][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:50:46,716][227910] Reward + Measures: [[1870.93160872    0.49759999    0.48550001    0.3585        0.42860004]
 [2535.15296734    0.4948        0.46329999    0.36300001    0.34470001]
 [2357.32092402    0.36407325    0.30062029    0.26585385    0.1669738 ]
 ...
 [1983.83060849    0.46700001    0.55989999    0.33430001    0.4289    ]
 [2582.25670084    0.42829999    0.40420005    0.3159        0.26670003]
 [2016.29960552    0.49700004    0.51559997    0.33759999    0.45070001]][0m
[37m[1m[2023-07-10 13:50:46,716][227910] Max Reward on eval: 3518.3568887501488[0m
[37m[1m[2023-07-10 13:50:46,716][227910] Min Reward on eval: 276.10291079382876[0m
[37m[1m[2023-07-10 13:50:46,717][227910] Mean Reward across all agents: 2027.0863609219564[0m
[37m[1m[2023-07-10 13:50:46,717][227910] Average Trajectory Length: 981.2226666666667[0m
[36m[2023-07-10 13:50:46,723][227910] mean_value=14.58278595158064, max_value=2702.461121369496[0m
[37m[1m[2023-07-10 13:50:46,726][227910] New mean coefficients: [[ 0.7147025  -0.04177698  4.216117   -0.31988308  0.93919575]][0m
[37m[1m[2023-07-10 13:50:46,727][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:50:56,405][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 13:50:56,405][227910] FPS: 396853.95[0m
[36m[2023-07-10 13:50:56,407][227910] itr=535, itrs=2000, Progress: 26.75%[0m
[36m[2023-07-10 13:51:07,831][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 13:51:07,832][227910] FPS: 336672.02[0m
[36m[2023-07-10 13:51:12,486][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:51:12,486][227910] Reward + Measures: [[3480.84417934    0.4097473     0.35717604    0.27703059    0.20337977]][0m
[37m[1m[2023-07-10 13:51:12,487][227910] Max Reward on eval: 3480.844179340247[0m
[37m[1m[2023-07-10 13:51:12,487][227910] Min Reward on eval: 3480.844179340247[0m
[37m[1m[2023-07-10 13:51:12,487][227910] Mean Reward across all agents: 3480.844179340247[0m
[37m[1m[2023-07-10 13:51:12,487][227910] Average Trajectory Length: 991.8706666666666[0m
[36m[2023-07-10 13:51:17,902][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:51:17,902][227910] Reward + Measures: [[2971.11264919    0.40839997    0.38079998    0.26860002    0.23190001]
 [2753.11402718    0.36887214    0.40279809    0.26460835    0.2235053 ]
 [1597.03393812    0.433         0.59960002    0.2811        0.48629999]
 ...
 [2357.46570278    0.40114489    0.46347347    0.29762858    0.3463102 ]
 [3304.45198555    0.39149997    0.4025        0.26879999    0.1631    ]
 [2324.39720529    0.39309102    0.44859105    0.2659955     0.25635394]][0m
[37m[1m[2023-07-10 13:51:17,902][227910] Max Reward on eval: 3675.6465312160553[0m
[37m[1m[2023-07-10 13:51:17,903][227910] Min Reward on eval: 449.86784006531815[0m
[37m[1m[2023-07-10 13:51:17,903][227910] Mean Reward across all agents: 2573.0107409845527[0m
[37m[1m[2023-07-10 13:51:17,903][227910] Average Trajectory Length: 981.1893333333333[0m
[36m[2023-07-10 13:51:17,907][227910] mean_value=-72.1364072133509, max_value=2824.2925739620055[0m
[37m[1m[2023-07-10 13:51:17,910][227910] New mean coefficients: [[ 0.23546591  0.20846555  4.4719615  -0.40981734 -0.196599  ]][0m
[37m[1m[2023-07-10 13:51:17,911][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:51:27,618][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 13:51:27,619][227910] FPS: 395644.35[0m
[36m[2023-07-10 13:51:27,621][227910] itr=536, itrs=2000, Progress: 26.80%[0m
[36m[2023-07-10 13:51:39,129][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 13:51:39,129][227910] FPS: 334146.70[0m
[36m[2023-07-10 13:51:43,926][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:51:43,926][227910] Reward + Measures: [[3473.20707528    0.40558055    0.40050781    0.27432415    0.21026461]][0m
[37m[1m[2023-07-10 13:51:43,926][227910] Max Reward on eval: 3473.20707527861[0m
[37m[1m[2023-07-10 13:51:43,926][227910] Min Reward on eval: 3473.20707527861[0m
[37m[1m[2023-07-10 13:51:43,927][227910] Mean Reward across all agents: 3473.20707527861[0m
[37m[1m[2023-07-10 13:51:43,927][227910] Average Trajectory Length: 994.4863333333333[0m
[36m[2023-07-10 13:51:49,388][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:51:49,389][227910] Reward + Measures: [[2944.80529874    0.43260002    0.44730002    0.28889999    0.27700001]
 [1205.59169254    0.49769998    0.62290001    0.36919999    0.49710003]
 [2900.09013768    0.40710002    0.44420001    0.27410001    0.2316    ]
 ...
 [2268.94306656    0.41240001    0.48359999    0.32609999    0.33129999]
 [2767.28479332    0.43590003    0.40190002    0.26679999    0.21610001]
 [1039.65318694    0.56530005    0.62140006    0.45229998    0.51389998]][0m
[37m[1m[2023-07-10 13:51:49,389][227910] Max Reward on eval: 3650.8690037578344[0m
[37m[1m[2023-07-10 13:51:49,390][227910] Min Reward on eval: 706.8494198693428[0m
[37m[1m[2023-07-10 13:51:49,390][227910] Mean Reward across all agents: 2420.6370757597174[0m
[37m[1m[2023-07-10 13:51:49,390][227910] Average Trajectory Length: 994.8106666666666[0m
[36m[2023-07-10 13:51:49,394][227910] mean_value=68.16728051555877, max_value=2654.298337067973[0m
[37m[1m[2023-07-10 13:51:49,397][227910] New mean coefficients: [[-0.03747457 -0.41371033  5.1631875  -0.4854029   0.00822987]][0m
[37m[1m[2023-07-10 13:51:49,398][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:51:59,095][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 13:51:59,096][227910] FPS: 396063.61[0m
[36m[2023-07-10 13:51:59,098][227910] itr=537, itrs=2000, Progress: 26.85%[0m
[36m[2023-07-10 13:52:10,543][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 13:52:10,543][227910] FPS: 335988.16[0m
[36m[2023-07-10 13:52:15,440][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:52:15,440][227910] Reward + Measures: [[552.93390938   0.6863023    0.79104131   0.54366404   0.91097134]][0m
[37m[1m[2023-07-10 13:52:15,441][227910] Max Reward on eval: 552.9339093836569[0m
[37m[1m[2023-07-10 13:52:15,441][227910] Min Reward on eval: 552.9339093836569[0m
[37m[1m[2023-07-10 13:52:15,441][227910] Mean Reward across all agents: 552.9339093836569[0m
[37m[1m[2023-07-10 13:52:15,441][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:52:20,846][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:52:20,847][227910] Reward + Measures: [[437.82139794   0.6318       0.79250002   0.65120006   0.89690012]
 [365.7423417    0.57440001   0.55929995   0.37180004   0.66009998]
 [605.76672986   0.5747       0.83720011   0.51679999   0.90690005]
 ...
 [458.23340567   0.66210002   0.48140001   0.51969999   0.75060004]
 [498.8832896    0.46850005   0.65339994   0.4262       0.74920005]
 [407.82296266   0.57080001   0.73279995   0.4903       0.89000005]][0m
[37m[1m[2023-07-10 13:52:20,847][227910] Max Reward on eval: 635.9851250480627[0m
[37m[1m[2023-07-10 13:52:20,847][227910] Min Reward on eval: -213.8242468264507[0m
[37m[1m[2023-07-10 13:52:20,848][227910] Mean Reward across all agents: 421.29904602328406[0m
[37m[1m[2023-07-10 13:52:20,848][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:52:20,854][227910] mean_value=125.9955090446923, max_value=1029.1056163467933[0m
[37m[1m[2023-07-10 13:52:20,857][227910] New mean coefficients: [[ 0.43351215 -0.7796734   5.8894644  -0.3816752   0.03610922]][0m
[37m[1m[2023-07-10 13:52:20,858][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:52:30,423][227910] train() took 9.56 seconds to complete[0m
[36m[2023-07-10 13:52:30,423][227910] FPS: 401523.09[0m
[36m[2023-07-10 13:52:30,426][227910] itr=538, itrs=2000, Progress: 26.90%[0m
[36m[2023-07-10 13:52:41,976][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 13:52:41,976][227910] FPS: 332917.94[0m
[36m[2023-07-10 13:52:46,827][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:52:46,827][227910] Reward + Measures: [[-29.53611615   0.46942228   0.86227828   0.32122815   0.8277415 ]][0m
[37m[1m[2023-07-10 13:52:46,827][227910] Max Reward on eval: -29.536116154577957[0m
[37m[1m[2023-07-10 13:52:46,827][227910] Min Reward on eval: -29.536116154577957[0m
[37m[1m[2023-07-10 13:52:46,827][227910] Mean Reward across all agents: -29.536116154577957[0m
[37m[1m[2023-07-10 13:52:46,828][227910] Average Trajectory Length: 999.2593333333333[0m
[36m[2023-07-10 13:52:52,411][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:52:52,412][227910] Reward + Measures: [[-122.10808043    0.47620001    0.81090003    0.39120004    0.76910001]
 [-188.02596788    0.44590002    0.93559998    0.2148        0.9483    ]
 [  45.29475848    0.4797        0.77780002    0.4368        0.70679992]
 ...
 [-201.05535558    0.42090002    0.90599996    0.0862        0.86180001]
 [ -94.77153436    0.39400002    0.70469999    0.3488        0.62379998]
 [  73.19765435    0.5201        0.81720001    0.51710004    0.7926001 ]][0m
[37m[1m[2023-07-10 13:52:52,412][227910] Max Reward on eval: 639.4337583958404[0m
[37m[1m[2023-07-10 13:52:52,412][227910] Min Reward on eval: -351.2503579527023[0m
[37m[1m[2023-07-10 13:52:52,412][227910] Mean Reward across all agents: -77.5797332211203[0m
[37m[1m[2023-07-10 13:52:52,412][227910] Average Trajectory Length: 998.4[0m
[36m[2023-07-10 13:52:52,417][227910] mean_value=-71.4025279597161, max_value=705.4801296818598[0m
[37m[1m[2023-07-10 13:52:52,419][227910] New mean coefficients: [[-0.938677   -0.3909359   6.350856   -0.33271655 -0.02167573]][0m
[37m[1m[2023-07-10 13:52:52,420][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:53:02,230][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 13:53:02,230][227910] FPS: 391479.41[0m
[36m[2023-07-10 13:53:02,233][227910] itr=539, itrs=2000, Progress: 26.95%[0m
[36m[2023-07-10 13:53:13,922][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 13:53:13,922][227910] FPS: 328969.91[0m
[36m[2023-07-10 13:53:18,600][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:53:18,600][227910] Reward + Measures: [[-13.2269625    0.43441564   0.78765041   0.33588967   0.76282436]][0m
[37m[1m[2023-07-10 13:53:18,601][227910] Max Reward on eval: -13.226962498335745[0m
[37m[1m[2023-07-10 13:53:18,601][227910] Min Reward on eval: -13.226962498335745[0m
[37m[1m[2023-07-10 13:53:18,601][227910] Mean Reward across all agents: -13.226962498335745[0m
[37m[1m[2023-07-10 13:53:18,601][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:53:24,193][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:53:24,194][227910] Reward + Measures: [[ 124.24439642    0.4553        0.5873        0.46630001    0.55720007]
 [-260.67345399    0.51770002    0.82369995    0.2218        0.75810003]
 [-670.18020883    0.70880002    0.87889999    0.0763        0.80089998]
 ...
 [  15.57882814    0.62670004    0.1987        0.61610001    0.42810002]
 [ 116.40957816    0.54879999    0.7051        0.51319999    0.71350002]
 [ 136.98342172    0.4427        0.66280001    0.44580004    0.67300004]][0m
[37m[1m[2023-07-10 13:53:24,194][227910] Max Reward on eval: 262.42315861196255[0m
[37m[1m[2023-07-10 13:53:24,194][227910] Min Reward on eval: -851.5797646756225[0m
[37m[1m[2023-07-10 13:53:24,195][227910] Mean Reward across all agents: -74.85637835548987[0m
[37m[1m[2023-07-10 13:53:24,195][227910] Average Trajectory Length: 999.3746666666666[0m
[36m[2023-07-10 13:53:24,197][227910] mean_value=-498.8165975520915, max_value=476.7040111913218[0m
[37m[1m[2023-07-10 13:53:24,199][227910] New mean coefficients: [[-1.1331811  -0.59430623  6.1588173  -0.24242607 -0.21421374]][0m
[37m[1m[2023-07-10 13:53:24,200][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:53:33,929][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 13:53:33,929][227910] FPS: 394782.08[0m
[36m[2023-07-10 13:53:33,931][227910] itr=540, itrs=2000, Progress: 27.00%[0m
[37m[1m[2023-07-10 13:53:36,802][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000520[0m
[36m[2023-07-10 13:53:48,538][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 13:53:48,538][227910] FPS: 334929.32[0m
[36m[2023-07-10 13:53:53,358][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:53:53,359][227910] Reward + Measures: [[-629.74384635    0.69235528    0.95732599    0.00466033    0.98892367]][0m
[37m[1m[2023-07-10 13:53:53,359][227910] Max Reward on eval: -629.7438463479388[0m
[37m[1m[2023-07-10 13:53:53,359][227910] Min Reward on eval: -629.7438463479388[0m
[37m[1m[2023-07-10 13:53:53,360][227910] Mean Reward across all agents: -629.7438463479388[0m
[37m[1m[2023-07-10 13:53:53,360][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:53:58,753][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:53:58,754][227910] Reward + Measures: [[-625.0627708     0.54809999    0.87169999    0.0307        0.94390005]
 [-454.66376367    0.49070001    0.92080003    0.1952        0.9084    ]
 [-679.67380962    0.72289997    0.95539999    0.0024        0.98180008]
 ...
 [-453.44886837    0.55919999    0.88910002    0.10930001    0.90430003]
 [-504.41537465    0.46669999    0.83360004    0.1138        0.84460002]
 [-277.1269265     0.21170001    0.63340008    0.2933        0.5205    ]][0m
[37m[1m[2023-07-10 13:53:58,754][227910] Max Reward on eval: -62.87329508641269[0m
[37m[1m[2023-07-10 13:53:58,754][227910] Min Reward on eval: -1059.5657467198557[0m
[37m[1m[2023-07-10 13:53:58,755][227910] Mean Reward across all agents: -551.783255915706[0m
[37m[1m[2023-07-10 13:53:58,755][227910] Average Trajectory Length: 999.4606666666666[0m
[36m[2023-07-10 13:53:58,757][227910] mean_value=-337.6044990748575, max_value=134.82749467109096[0m
[37m[1m[2023-07-10 13:53:58,759][227910] New mean coefficients: [[-1.0015044   0.0029369   7.504078    0.61244404 -0.15694043]][0m
[37m[1m[2023-07-10 13:53:58,760][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:54:08,546][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 13:54:08,546][227910] FPS: 392471.95[0m
[36m[2023-07-10 13:54:08,549][227910] itr=541, itrs=2000, Progress: 27.05%[0m
[36m[2023-07-10 13:54:20,227][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 13:54:20,228][227910] FPS: 329328.44[0m
[36m[2023-07-10 13:54:25,100][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:54:25,100][227910] Reward + Measures: [[-1252.84242117     0.89991331     0.99139166     0.00121033
      0.99329501]][0m
[37m[1m[2023-07-10 13:54:25,100][227910] Max Reward on eval: -1252.8424211678955[0m
[37m[1m[2023-07-10 13:54:25,101][227910] Min Reward on eval: -1252.8424211678955[0m
[37m[1m[2023-07-10 13:54:25,101][227910] Mean Reward across all agents: -1252.8424211678955[0m
[37m[1m[2023-07-10 13:54:25,101][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:54:30,742][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:54:30,743][227910] Reward + Measures: [[-1291.4242442      0.56690001     0.97590011     0.07840001
      0.97469997]
 [-1057.81436364     0.87669992     0.98620003     0.0026
      0.98789996]
 [-1172.88946872     0.75100005     0.97340006     0.0185
      0.97140008]
 ...
 [-1280.24963051     0.6645         0.97550005     0.0482
      0.9698    ]
 [-1250.48554597     0.67680007     0.97930002     0.0616
      0.97819996]
 [ -911.15625839     0.71560001     0.97280008     0.0242
      0.96779996]][0m
[37m[1m[2023-07-10 13:54:30,743][227910] Max Reward on eval: -694.2479872127994[0m
[37m[1m[2023-07-10 13:54:30,743][227910] Min Reward on eval: -1476.9079864334083[0m
[37m[1m[2023-07-10 13:54:30,743][227910] Mean Reward across all agents: -1204.1354744502328[0m
[37m[1m[2023-07-10 13:54:30,744][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:54:30,745][227910] mean_value=-801.4096295494924, max_value=-243.75431427219883[0m
[36m[2023-07-10 13:54:30,747][227910] XNES is restarting with a new solution whose measures are [0.67526364 0.31352732 0.70050001 0.21007274] and objective is 38.58532254728489[0m
[36m[2023-07-10 13:54:30,748][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 13:54:30,752][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 13:54:30,752][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:54:40,556][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 13:54:40,556][227910] FPS: 391746.93[0m
[36m[2023-07-10 13:54:40,559][227910] itr=542, itrs=2000, Progress: 27.10%[0m
[36m[2023-07-10 13:54:52,150][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 13:54:52,151][227910] FPS: 331763.77[0m
[36m[2023-07-10 13:54:56,940][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:54:56,940][227910] Reward + Measures: [[-612.12465853    0.44460684    0.19086054    0.47172734    0.29222354]][0m
[37m[1m[2023-07-10 13:54:56,940][227910] Max Reward on eval: -612.1246585330281[0m
[37m[1m[2023-07-10 13:54:56,941][227910] Min Reward on eval: -612.1246585330281[0m
[37m[1m[2023-07-10 13:54:56,941][227910] Mean Reward across all agents: -612.1246585330281[0m
[37m[1m[2023-07-10 13:54:56,941][227910] Average Trajectory Length: 823.6123333333333[0m
[36m[2023-07-10 13:55:02,444][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:55:02,445][227910] Reward + Measures: [[-295.15676206    0.20689523    0.32468334    0.28196767    0.12812603]
 [-768.08969116    0.24729685    0.19096404    0.20156001    0.10972645]
 [-409.90401195    0.31440002    0.38630003    0.34870002    0.23959999]
 ...
 [-539.19710058    0.39740002    0.35189998    0.40060002    0.20920001]
 [-832.09606806    0.26513845    0.20259337    0.18558435    0.0959685 ]
 [-733.85625216    0.17325708    0.25424194    0.19703679    0.1068057 ]][0m
[37m[1m[2023-07-10 13:55:02,445][227910] Max Reward on eval: -106.525984434376[0m
[37m[1m[2023-07-10 13:55:02,445][227910] Min Reward on eval: -1220.7145382450544[0m
[37m[1m[2023-07-10 13:55:02,446][227910] Mean Reward across all agents: -545.7568600130667[0m
[37m[1m[2023-07-10 13:55:02,446][227910] Average Trajectory Length: 710.6753333333334[0m
[36m[2023-07-10 13:55:02,447][227910] mean_value=-3290.6235382100645, max_value=-186.0928617641743[0m
[36m[2023-07-10 13:55:02,449][227910] XNES is restarting with a new solution whose measures are [0.63389999 0.4585     0.45340005 0.52350003] and objective is 9.541303125675768[0m
[36m[2023-07-10 13:55:02,450][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 13:55:02,453][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 13:55:02,454][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:55:12,146][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 13:55:12,147][227910] FPS: 396230.59[0m
[36m[2023-07-10 13:55:12,149][227910] itr=543, itrs=2000, Progress: 27.15%[0m
[36m[2023-07-10 13:55:23,578][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 13:55:23,578][227910] FPS: 336529.00[0m
[36m[2023-07-10 13:55:28,343][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:55:28,344][227910] Reward + Measures: [[-67.92753773   0.55524802   0.38884032   0.48346135   0.59305435]][0m
[37m[1m[2023-07-10 13:55:28,344][227910] Max Reward on eval: -67.92753772539263[0m
[37m[1m[2023-07-10 13:55:28,344][227910] Min Reward on eval: -67.92753772539263[0m
[37m[1m[2023-07-10 13:55:28,345][227910] Mean Reward across all agents: -67.92753772539263[0m
[37m[1m[2023-07-10 13:55:28,345][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:55:33,903][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:55:33,904][227910] Reward + Measures: [[-1248.21172177     0.57339996     0.43540001     0.57270002
      0.34220001]
 [-1538.47586958     0.57134527     0.72158557     0.32419667
      0.74179429]
 [ -684.55420396     0.16059667     0.24234131     0.17601147
      0.22159532]
 ...
 [-1307.65184679     0.5133         0.45640001     0.52210009
      0.45790002]
 [-1758.86202314     0.31979367     0.39991775     0.15916203
      0.41951138]
 [ -974.68776867     0.23210798     0.28124142     0.24706529
      0.19309573]][0m
[37m[1m[2023-07-10 13:55:33,904][227910] Max Reward on eval: 390.31659942428814[0m
[37m[1m[2023-07-10 13:55:33,904][227910] Min Reward on eval: -2276.7920885446947[0m
[37m[1m[2023-07-10 13:55:33,905][227910] Mean Reward across all agents: -947.0064530793255[0m
[37m[1m[2023-07-10 13:55:33,905][227910] Average Trajectory Length: 919.0736666666667[0m
[36m[2023-07-10 13:55:33,906][227910] mean_value=-1823.7985625428187, max_value=74.57826956437202[0m
[37m[1m[2023-07-10 13:55:33,909][227910] New mean coefficients: [[-0.13379872 -1.0603255  -0.62777007 -1.6395187  -1.1718509 ]][0m
[37m[1m[2023-07-10 13:55:33,910][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:55:43,644][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 13:55:43,645][227910] FPS: 394542.31[0m
[36m[2023-07-10 13:55:43,647][227910] itr=544, itrs=2000, Progress: 27.20%[0m
[36m[2023-07-10 13:55:55,306][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 13:55:55,306][227910] FPS: 329839.60[0m
[36m[2023-07-10 13:56:00,196][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:56:00,201][227910] Reward + Measures: [[-35.96526252   0.64872903   0.49309894   0.54803133   0.66884238]][0m
[37m[1m[2023-07-10 13:56:00,201][227910] Max Reward on eval: -35.965262524624315[0m
[37m[1m[2023-07-10 13:56:00,201][227910] Min Reward on eval: -35.965262524624315[0m
[37m[1m[2023-07-10 13:56:00,202][227910] Mean Reward across all agents: -35.965262524624315[0m
[37m[1m[2023-07-10 13:56:00,202][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:56:05,524][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:56:05,529][227910] Reward + Measures: [[ -654.88800592     0.32284799     0.28261274     0.25412056
      0.17655729]
 [ -987.86107086     0.30212942     0.22717646     0.30652353
      0.1657647 ]
 [ -587.98398039     0.4614         0.37129998     0.39360002
      0.38509998]
 ...
 [-1251.00303787     0.30065805     0.35196391     0.27210954
      0.29490381]
 [ -998.99385889     0.19219956     0.43337756     0.23879643
      0.48071033]
 [ -580.39934681     0.41050932     0.48575816     0.24330698
      0.34455582]][0m
[37m[1m[2023-07-10 13:56:05,529][227910] Max Reward on eval: 863.075047313748[0m
[37m[1m[2023-07-10 13:56:05,529][227910] Min Reward on eval: -1920.0228642354255[0m
[37m[1m[2023-07-10 13:56:05,530][227910] Mean Reward across all agents: -717.4581548891191[0m
[37m[1m[2023-07-10 13:56:05,530][227910] Average Trajectory Length: 915.367[0m
[36m[2023-07-10 13:56:05,531][227910] mean_value=-2265.1196656932525, max_value=280.08240829653914[0m
[37m[1m[2023-07-10 13:56:05,534][227910] New mean coefficients: [[ 0.02389057 -0.43360275 -0.6309888  -0.6496848  -0.4263218 ]][0m
[37m[1m[2023-07-10 13:56:05,535][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:56:15,326][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 13:56:15,326][227910] FPS: 392258.29[0m
[36m[2023-07-10 13:56:15,328][227910] itr=545, itrs=2000, Progress: 27.25%[0m
[36m[2023-07-10 13:56:26,754][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 13:56:26,755][227910] FPS: 336554.71[0m
[36m[2023-07-10 13:56:31,446][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:56:31,446][227910] Reward + Measures: [[25.49719599  0.52514434  0.52132505  0.52803266  0.59794831]][0m
[37m[1m[2023-07-10 13:56:31,446][227910] Max Reward on eval: 25.497195991939755[0m
[37m[1m[2023-07-10 13:56:31,447][227910] Min Reward on eval: 25.497195991939755[0m
[37m[1m[2023-07-10 13:56:31,447][227910] Mean Reward across all agents: 25.497195991939755[0m
[37m[1m[2023-07-10 13:56:31,447][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:56:36,910][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:56:36,911][227910] Reward + Measures: [[ -278.15620144     0.27739999     0.5557         0.31560001
      0.4772    ]
 [ -586.57716195     0.37069997     0.33430001     0.44519997
      0.3915    ]
 [ -328.29973029     0.31599998     0.27449998     0.36679998
      0.31810001]
 ...
 [ -861.37561766     0.3831158      0.40152201     0.34900957
      0.41641435]
 [-1014.72474255     0.24389105     0.31298375     0.23144339
      0.30601856]
 [   -3.76965745     0.32210001     0.50949997     0.36520001
      0.5341    ]][0m
[37m[1m[2023-07-10 13:56:36,911][227910] Max Reward on eval: 173.12144844043505[0m
[37m[1m[2023-07-10 13:56:36,911][227910] Min Reward on eval: -2047.5993667834439[0m
[37m[1m[2023-07-10 13:56:36,911][227910] Mean Reward across all agents: -713.5618742897136[0m
[37m[1m[2023-07-10 13:56:36,911][227910] Average Trajectory Length: 964.927[0m
[36m[2023-07-10 13:56:36,913][227910] mean_value=-1720.0790193374335, max_value=489.50227454335794[0m
[37m[1m[2023-07-10 13:56:36,915][227910] New mean coefficients: [[-0.3491282   0.08603805 -0.69094217  0.28996372 -0.27716678]][0m
[37m[1m[2023-07-10 13:56:36,916][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:56:46,542][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 13:56:46,542][227910] FPS: 399004.67[0m
[36m[2023-07-10 13:56:46,544][227910] itr=546, itrs=2000, Progress: 27.30%[0m
[36m[2023-07-10 13:56:58,089][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 13:56:58,089][227910] FPS: 333093.99[0m
[36m[2023-07-10 13:57:02,901][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:57:02,901][227910] Reward + Measures: [[42.6270764   0.65166301  0.11841801  0.59705466  0.41365567]][0m
[37m[1m[2023-07-10 13:57:02,902][227910] Max Reward on eval: 42.627076399232074[0m
[37m[1m[2023-07-10 13:57:02,902][227910] Min Reward on eval: 42.627076399232074[0m
[37m[1m[2023-07-10 13:57:02,902][227910] Mean Reward across all agents: 42.627076399232074[0m
[37m[1m[2023-07-10 13:57:02,902][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:57:08,442][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:57:08,443][227910] Reward + Measures: [[-1147.09452184     0.48379999     0.4251         0.38410002
      0.5097    ]
 [  -23.77139621     0.66789997     0.49460003     0.53479999
      0.67750001]
 [-1342.63709337     0.27441448     0.22110009     0.21782258
      0.14034095]
 ...
 [ -571.80975524     0.44670001     0.2307         0.44190001
      0.4199    ]
 [ -108.08613954     0.56630009     0.12730001     0.59170002
      0.37209997]
 [ -465.92402118     0.48119998     0.1908         0.46760002
      0.45100003]][0m
[37m[1m[2023-07-10 13:57:08,443][227910] Max Reward on eval: -8.644001929915976[0m
[37m[1m[2023-07-10 13:57:08,443][227910] Min Reward on eval: -2191.0193376445677[0m
[37m[1m[2023-07-10 13:57:08,443][227910] Mean Reward across all agents: -708.405530882733[0m
[37m[1m[2023-07-10 13:57:08,444][227910] Average Trajectory Length: 963.1123333333333[0m
[36m[2023-07-10 13:57:08,446][227910] mean_value=-1545.888514054206, max_value=389.7505317772517[0m
[37m[1m[2023-07-10 13:57:08,449][227910] New mean coefficients: [[-0.24205555 -0.42299604 -0.7031038   0.01755038 -0.68176425]][0m
[37m[1m[2023-07-10 13:57:08,450][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:57:18,113][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 13:57:18,113][227910] FPS: 397445.74[0m
[36m[2023-07-10 13:57:18,116][227910] itr=547, itrs=2000, Progress: 27.35%[0m
[36m[2023-07-10 13:57:29,542][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 13:57:29,542][227910] FPS: 336522.45[0m
[36m[2023-07-10 13:57:34,235][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:57:34,236][227910] Reward + Measures: [[-21.26730783   0.60006589   0.12468386   0.58401865   0.42618534]][0m
[37m[1m[2023-07-10 13:57:34,236][227910] Max Reward on eval: -21.26730782604228[0m
[37m[1m[2023-07-10 13:57:34,236][227910] Min Reward on eval: -21.26730782604228[0m
[37m[1m[2023-07-10 13:57:34,236][227910] Mean Reward across all agents: -21.26730782604228[0m
[37m[1m[2023-07-10 13:57:34,237][227910] Average Trajectory Length: 995.0666666666666[0m
[36m[2023-07-10 13:57:39,758][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:57:39,758][227910] Reward + Measures: [[ -56.25886051    0.37369999    0.21789999    0.44250003    0.26420003]
 [-926.01168628    0.35140255    0.15666375    0.35228938    0.19849329]
 [-314.79722822    0.3108404     0.23997927    0.33940119    0.22017936]
 ...
 [  45.96173756    0.39854419    0.19040582    0.45198375    0.32572213]
 [-750.24943083    0.23041832    0.25715443    0.25876769    0.17014992]
 [-665.34355293    0.2262655     0.18723541    0.27102265    0.17059603]][0m
[37m[1m[2023-07-10 13:57:39,758][227910] Max Reward on eval: 331.21623680891935[0m
[37m[1m[2023-07-10 13:57:39,759][227910] Min Reward on eval: -2167.854486234393[0m
[37m[1m[2023-07-10 13:57:39,759][227910] Mean Reward across all agents: -582.2361177314618[0m
[37m[1m[2023-07-10 13:57:39,759][227910] Average Trajectory Length: 892.9866666666667[0m
[36m[2023-07-10 13:57:39,761][227910] mean_value=-1688.8618184569461, max_value=418.5788275692046[0m
[37m[1m[2023-07-10 13:57:39,764][227910] New mean coefficients: [[-0.06508093  0.16034126 -0.39321497  0.537029   -0.46467978]][0m
[37m[1m[2023-07-10 13:57:39,765][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:57:49,433][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 13:57:49,433][227910] FPS: 397232.86[0m
[36m[2023-07-10 13:57:49,436][227910] itr=548, itrs=2000, Progress: 27.40%[0m
[36m[2023-07-10 13:58:01,016][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 13:58:01,017][227910] FPS: 332077.21[0m
[36m[2023-07-10 13:58:05,856][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:58:05,856][227910] Reward + Measures: [[-253.16726816    0.61667824    0.09643093    0.54553211    0.42140478]][0m
[37m[1m[2023-07-10 13:58:05,856][227910] Max Reward on eval: -253.16726815596988[0m
[37m[1m[2023-07-10 13:58:05,856][227910] Min Reward on eval: -253.16726815596988[0m
[37m[1m[2023-07-10 13:58:05,856][227910] Mean Reward across all agents: -253.16726815596988[0m
[37m[1m[2023-07-10 13:58:05,857][227910] Average Trajectory Length: 991.3376666666667[0m
[36m[2023-07-10 13:58:11,291][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:58:11,292][227910] Reward + Measures: [[ -657.61872887     0.53780001     0.1389         0.54180002
      0.53590006]
 [ -801.61510002     0.52990001     0.32450002     0.4571
      0.58249998]
 [-1491.94527797     0.80670005     0.0939         0.83080006
      0.75029993]
 ...
 [ -630.47109785     0.63169998     0.11830001     0.55199999
      0.39480001]
 [ -915.25592445     0.40970001     0.13200001     0.46020004
      0.2791    ]
 [ -669.87646641     0.68610001     0.0991         0.55960006
      0.42850003]][0m
[37m[1m[2023-07-10 13:58:11,292][227910] Max Reward on eval: 259.12622473620576[0m
[37m[1m[2023-07-10 13:58:11,293][227910] Min Reward on eval: -2002.4010209351545[0m
[37m[1m[2023-07-10 13:58:11,293][227910] Mean Reward across all agents: -670.3697677571137[0m
[37m[1m[2023-07-10 13:58:11,293][227910] Average Trajectory Length: 978.807[0m
[36m[2023-07-10 13:58:11,295][227910] mean_value=-1371.612001543845, max_value=310.3551885973356[0m
[37m[1m[2023-07-10 13:58:11,297][227910] New mean coefficients: [[-1.0399411   0.38963136 -0.42386132  0.6065625  -0.42099583]][0m
[37m[1m[2023-07-10 13:58:11,298][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:58:21,052][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 13:58:21,052][227910] FPS: 393747.15[0m
[36m[2023-07-10 13:58:21,055][227910] itr=549, itrs=2000, Progress: 27.45%[0m
[36m[2023-07-10 13:58:32,676][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 13:58:32,676][227910] FPS: 330886.15[0m
[36m[2023-07-10 13:58:37,529][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:58:37,529][227910] Reward + Measures: [[-276.37587115    0.66393793    0.09041921    0.59347975    0.45250374]][0m
[37m[1m[2023-07-10 13:58:37,530][227910] Max Reward on eval: -276.37587115411185[0m
[37m[1m[2023-07-10 13:58:37,530][227910] Min Reward on eval: -276.37587115411185[0m
[37m[1m[2023-07-10 13:58:37,530][227910] Mean Reward across all agents: -276.37587115411185[0m
[37m[1m[2023-07-10 13:58:37,530][227910] Average Trajectory Length: 997.1523333333333[0m
[36m[2023-07-10 13:58:42,966][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:58:42,967][227910] Reward + Measures: [[ -406.84729482     0.42509446     0.35906288     0.34521979
      0.37586793]
 [ -466.13759669     0.36684123     0.28407657     0.43973199
      0.46033463]
 [ -761.98322114     0.68274248     0.44525656     0.6816293
      0.56919545]
 ...
 [ -641.59852438     0.7748         0.0318         0.64169997
      0.59510005]
 [-1051.3766579      0.63050002     0.32399997     0.62940007
      0.53529996]
 [-1085.77327852     0.5643         0.38250002     0.53200001
      0.5       ]][0m
[37m[1m[2023-07-10 13:58:42,967][227910] Max Reward on eval: 240.16934218604[0m
[37m[1m[2023-07-10 13:58:42,967][227910] Min Reward on eval: -1747.9507592387963[0m
[37m[1m[2023-07-10 13:58:42,968][227910] Mean Reward across all agents: -643.0738647094394[0m
[37m[1m[2023-07-10 13:58:42,968][227910] Average Trajectory Length: 962.663[0m
[36m[2023-07-10 13:58:42,971][227910] mean_value=-1071.8918117385965, max_value=305.7097787694565[0m
[37m[1m[2023-07-10 13:58:42,973][227910] New mean coefficients: [[-1.3080491  -0.04039809 -0.30727077  0.5118701  -0.8289052 ]][0m
[37m[1m[2023-07-10 13:58:42,974][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:58:52,709][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 13:58:52,710][227910] FPS: 394501.45[0m
[36m[2023-07-10 13:58:52,712][227910] itr=550, itrs=2000, Progress: 27.50%[0m
[37m[1m[2023-07-10 13:58:55,743][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000530[0m
[36m[2023-07-10 13:59:07,727][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 13:59:07,727][227910] FPS: 327884.57[0m
[36m[2023-07-10 13:59:12,550][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:59:12,551][227910] Reward + Measures: [[-628.91326442    0.78945464    0.40684032    0.70707959    0.6137656 ]][0m
[37m[1m[2023-07-10 13:59:12,551][227910] Max Reward on eval: -628.9132644164058[0m
[37m[1m[2023-07-10 13:59:12,551][227910] Min Reward on eval: -628.9132644164058[0m
[37m[1m[2023-07-10 13:59:12,551][227910] Mean Reward across all agents: -628.9132644164058[0m
[37m[1m[2023-07-10 13:59:12,552][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 13:59:18,091][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:59:18,092][227910] Reward + Measures: [[-555.52660077    0.70539999    0.3804        0.62600005    0.50400001]
 [-634.523388      0.84139997    0.47349998    0.75819999    0.71939999]
 [-671.79136827    0.54150003    0.25910002    0.42490003    0.37940001]
 ...
 [-844.58064797    0.61269993    0.1811        0.5212        0.48120004]
 [-794.85842401    0.54420006    0.35080001    0.37240002    0.5557    ]
 [-625.83590256    0.52880001    0.27050003    0.44010001    0.30130002]][0m
[37m[1m[2023-07-10 13:59:18,092][227910] Max Reward on eval: -333.520956534869[0m
[37m[1m[2023-07-10 13:59:18,092][227910] Min Reward on eval: -1612.6319434083766[0m
[37m[1m[2023-07-10 13:59:18,093][227910] Mean Reward across all agents: -861.3457394185359[0m
[37m[1m[2023-07-10 13:59:18,093][227910] Average Trajectory Length: 998.3943333333333[0m
[36m[2023-07-10 13:59:18,094][227910] mean_value=-1380.8981805378323, max_value=125.87580066503818[0m
[37m[1m[2023-07-10 13:59:18,097][227910] New mean coefficients: [[ 0.35305595 -0.5375234   0.06340736  0.14213565 -0.9470433 ]][0m
[37m[1m[2023-07-10 13:59:18,098][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:59:27,910][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 13:59:27,910][227910] FPS: 391428.61[0m
[36m[2023-07-10 13:59:27,912][227910] itr=551, itrs=2000, Progress: 27.55%[0m
[36m[2023-07-10 13:59:39,521][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 13:59:39,521][227910] FPS: 331281.19[0m
[36m[2023-07-10 13:59:44,291][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:59:44,291][227910] Reward + Measures: [[-683.52570856    0.75300705    0.38658419    0.70158046    0.59019107]][0m
[37m[1m[2023-07-10 13:59:44,291][227910] Max Reward on eval: -683.5257085639206[0m
[37m[1m[2023-07-10 13:59:44,291][227910] Min Reward on eval: -683.5257085639206[0m
[37m[1m[2023-07-10 13:59:44,291][227910] Mean Reward across all agents: -683.5257085639206[0m
[37m[1m[2023-07-10 13:59:44,292][227910] Average Trajectory Length: 999.689[0m
[36m[2023-07-10 13:59:49,743][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 13:59:49,743][227910] Reward + Measures: [[ -649.10476287     0.78759998     0.41190001     0.736
      0.59390002]
 [-1578.29302283     0.88999999     0.84469998     0.86879998
      0.9066    ]
 [ -673.68608226     0.73080009     0.30680004     0.68330002
      0.60900003]
 ...
 [ -559.10028984     0.69570005     0.30430001     0.63569993
      0.49009997]
 [ -455.4788844      0.65679997     0.1473         0.58660001
      0.4535    ]
 [ -608.96844439     0.69580001     0.30879998     0.57750005
      0.5104    ]][0m
[37m[1m[2023-07-10 13:59:49,744][227910] Max Reward on eval: -415.74958841182524[0m
[37m[1m[2023-07-10 13:59:49,744][227910] Min Reward on eval: -1720.6855209889122[0m
[37m[1m[2023-07-10 13:59:49,744][227910] Mean Reward across all agents: -856.4088818049526[0m
[37m[1m[2023-07-10 13:59:49,744][227910] Average Trajectory Length: 999.6086666666666[0m
[36m[2023-07-10 13:59:49,745][227910] mean_value=-1238.148752929981, max_value=-67.16043668327268[0m
[36m[2023-07-10 13:59:49,748][227910] XNES is restarting with a new solution whose measures are [0.68460006 0.616      0.61630005 0.58319998] and objective is 1133.6651638188982[0m
[36m[2023-07-10 13:59:49,749][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 13:59:49,751][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 13:59:49,752][227910] Moving the mean solution point...[0m
[36m[2023-07-10 13:59:59,496][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 13:59:59,496][227910] FPS: 394148.26[0m
[36m[2023-07-10 13:59:59,498][227910] itr=552, itrs=2000, Progress: 27.60%[0m
[36m[2023-07-10 14:00:11,019][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 14:00:11,020][227910] FPS: 333759.00[0m
[36m[2023-07-10 14:00:15,768][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:00:15,768][227910] Reward + Measures: [[2556.46130489    0.43827277    0.34834132    0.28968176    0.24078475]][0m
[37m[1m[2023-07-10 14:00:15,769][227910] Max Reward on eval: 2556.4613048873975[0m
[37m[1m[2023-07-10 14:00:15,769][227910] Min Reward on eval: 2556.4613048873975[0m
[37m[1m[2023-07-10 14:00:15,769][227910] Mean Reward across all agents: 2556.4613048873975[0m
[37m[1m[2023-07-10 14:00:15,769][227910] Average Trajectory Length: 997.954[0m
[36m[2023-07-10 14:00:21,296][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:00:21,296][227910] Reward + Measures: [[1600.17512418    0.53259999    0.3872        0.42279997    0.32390001]
 [-171.05156084    0.73110002    0.85939997    0.7112        0.83030003]
 [-222.67632197    0.77030003    0.82200003    0.67460006    0.81110001]
 ...
 [-764.41472239    0.6498        0.6031        0.6038        0.58319998]
 [-449.8689135     0.29860002    0.6469        0.2897        0.5625    ]
 [-968.70974373    0.62989998    0.50889999    0.49950001    0.50990003]][0m
[37m[1m[2023-07-10 14:00:21,297][227910] Max Reward on eval: 2756.3192950275725[0m
[37m[1m[2023-07-10 14:00:21,297][227910] Min Reward on eval: -1516.1011880469857[0m
[37m[1m[2023-07-10 14:00:21,297][227910] Mean Reward across all agents: -446.5496918739013[0m
[37m[1m[2023-07-10 14:00:21,297][227910] Average Trajectory Length: 978.2996666666667[0m
[36m[2023-07-10 14:00:21,299][227910] mean_value=-1745.5048117908725, max_value=1562.283450062695[0m
[37m[1m[2023-07-10 14:00:21,302][227910] New mean coefficients: [[ 0.8405435 -0.7825977 -0.6189469 -2.5279438  0.7285149]][0m
[37m[1m[2023-07-10 14:00:21,303][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:00:31,128][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 14:00:31,128][227910] FPS: 390910.26[0m
[36m[2023-07-10 14:00:31,130][227910] itr=553, itrs=2000, Progress: 27.65%[0m
[36m[2023-07-10 14:00:42,607][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 14:00:42,608][227910] FPS: 335048.45[0m
[36m[2023-07-10 14:00:47,303][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:00:47,304][227910] Reward + Measures: [[1188.43192471    0.36286402    0.30520383    0.37545714    0.28790742]][0m
[37m[1m[2023-07-10 14:00:47,304][227910] Max Reward on eval: 1188.4319247102685[0m
[37m[1m[2023-07-10 14:00:47,304][227910] Min Reward on eval: 1188.4319247102685[0m
[37m[1m[2023-07-10 14:00:47,305][227910] Mean Reward across all agents: 1188.4319247102685[0m
[37m[1m[2023-07-10 14:00:47,305][227910] Average Trajectory Length: 997.1416666666667[0m
[36m[2023-07-10 14:00:52,771][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:00:52,772][227910] Reward + Measures: [[-496.90049643    0.23402932    0.23099819    0.2860814     0.26353538]
 [ 153.60794961    0.34544107    0.28368005    0.30757234    0.24784994]
 [-309.59300769    0.27522105    0.26098418    0.28785262    0.22837369]
 ...
 [ 199.72534925    0.52920002    0.50730002    0.40830001    0.46580002]
 [ -57.17279938    0.32786688    0.3180356     0.30788729    0.28455752]
 [-231.97065249    0.2192        0.21360002    0.25240001    0.23310001]][0m
[37m[1m[2023-07-10 14:00:52,772][227910] Max Reward on eval: 1375.7560388707557[0m
[37m[1m[2023-07-10 14:00:52,772][227910] Min Reward on eval: -1137.7233987257816[0m
[37m[1m[2023-07-10 14:00:52,773][227910] Mean Reward across all agents: -18.62988952396215[0m
[37m[1m[2023-07-10 14:00:52,773][227910] Average Trajectory Length: 939.2446666666666[0m
[36m[2023-07-10 14:00:52,775][227910] mean_value=-1593.9427442047063, max_value=608.1651971354398[0m
[37m[1m[2023-07-10 14:00:52,777][227910] New mean coefficients: [[ 1.0537354  -0.12396139  0.36762226 -1.5845814   1.89454   ]][0m
[37m[1m[2023-07-10 14:00:52,778][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:01:02,497][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 14:01:02,497][227910] FPS: 395197.25[0m
[36m[2023-07-10 14:01:02,499][227910] itr=554, itrs=2000, Progress: 27.70%[0m
[36m[2023-07-10 14:01:14,245][227910] train() took 11.73 seconds to complete[0m
[36m[2023-07-10 14:01:14,246][227910] FPS: 327429.86[0m
[36m[2023-07-10 14:01:18,978][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:01:18,978][227910] Reward + Measures: [[1308.55476967    0.36260366    0.29795316    0.36289322    0.27783602]][0m
[37m[1m[2023-07-10 14:01:18,978][227910] Max Reward on eval: 1308.5547696678047[0m
[37m[1m[2023-07-10 14:01:18,979][227910] Min Reward on eval: 1308.5547696678047[0m
[37m[1m[2023-07-10 14:01:18,979][227910] Mean Reward across all agents: 1308.5547696678047[0m
[37m[1m[2023-07-10 14:01:18,979][227910] Average Trajectory Length: 996.9146666666667[0m
[36m[2023-07-10 14:01:24,610][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:01:24,611][227910] Reward + Measures: [[ 145.04522093    0.36630005    0.55249995    0.1883        0.52950001]
 [ -76.71054992    0.49979997    0.51815003    0.46720001    0.58095002]
 [ 495.5748707     0.67940009    0.55760002    0.57019997    0.56680006]
 ...
 [-618.08057447    0.15640187    0.67779112    0.57475823    0.63437033]
 [ 604.7117154     0.435         0.39510003    0.46339998    0.3867    ]
 [  16.93624237    0.29660001    0.67339998    0.37789997    0.65780002]][0m
[37m[1m[2023-07-10 14:01:24,611][227910] Max Reward on eval: 1637.2571641789866[0m
[37m[1m[2023-07-10 14:01:24,611][227910] Min Reward on eval: -1325.6080646820017[0m
[37m[1m[2023-07-10 14:01:24,611][227910] Mean Reward across all agents: 124.70600831287976[0m
[37m[1m[2023-07-10 14:01:24,612][227910] Average Trajectory Length: 965.1646666666667[0m
[36m[2023-07-10 14:01:24,614][227910] mean_value=-954.3681955128454, max_value=871.1409985900364[0m
[37m[1m[2023-07-10 14:01:24,616][227910] New mean coefficients: [[ 1.5518583   0.08010162  1.3200381  -1.4215882   2.0895052 ]][0m
[37m[1m[2023-07-10 14:01:24,617][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:01:34,220][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 14:01:34,220][227910] FPS: 399929.79[0m
[36m[2023-07-10 14:01:34,223][227910] itr=555, itrs=2000, Progress: 27.75%[0m
[36m[2023-07-10 14:01:45,874][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 14:01:45,874][227910] FPS: 330109.94[0m
[36m[2023-07-10 14:01:50,618][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:01:50,618][227910] Reward + Measures: [[1398.6517634     0.35451648    0.28792009    0.3574456     0.26440522]][0m
[37m[1m[2023-07-10 14:01:50,618][227910] Max Reward on eval: 1398.651763401692[0m
[37m[1m[2023-07-10 14:01:50,619][227910] Min Reward on eval: 1398.651763401692[0m
[37m[1m[2023-07-10 14:01:50,619][227910] Mean Reward across all agents: 1398.651763401692[0m
[37m[1m[2023-07-10 14:01:50,619][227910] Average Trajectory Length: 997.177[0m
[36m[2023-07-10 14:01:56,083][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:01:56,088][227910] Reward + Measures: [[   83.17772744     0.59530002     0.3924         0.51700002
      0.35670003]
 [ 1488.44983863     0.331          0.29300001     0.34390002
      0.27660003]
 [  355.79772187     0.49810001     0.41480002     0.47530004
      0.35870001]
 ...
 [  737.6521468      0.32710001     0.32190001     0.3524
      0.33530003]
 [   84.05900292     0.2927278      0.41691437     0.30761296
      0.36590755]
 [-1177.52571358     0.49689999     0.222          0.4499
      0.31809998]][0m
[37m[1m[2023-07-10 14:01:56,089][227910] Max Reward on eval: 1837.0199017584323[0m
[37m[1m[2023-07-10 14:01:56,089][227910] Min Reward on eval: -1496.0643439199835[0m
[37m[1m[2023-07-10 14:01:56,089][227910] Mean Reward across all agents: 577.567319531427[0m
[37m[1m[2023-07-10 14:01:56,089][227910] Average Trajectory Length: 979.5226666666666[0m
[36m[2023-07-10 14:01:56,092][227910] mean_value=-1230.4915661374434, max_value=814.4323798555779[0m
[37m[1m[2023-07-10 14:01:56,094][227910] New mean coefficients: [[ 1.0162152   0.27075145  1.7773361  -1.0751933   3.3529394 ]][0m
[37m[1m[2023-07-10 14:01:56,095][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:02:05,813][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 14:02:05,814][227910] FPS: 395201.38[0m
[36m[2023-07-10 14:02:05,816][227910] itr=556, itrs=2000, Progress: 27.80%[0m
[36m[2023-07-10 14:02:17,348][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 14:02:17,348][227910] FPS: 333554.20[0m
[36m[2023-07-10 14:02:22,079][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:02:22,079][227910] Reward + Measures: [[1511.16641325    0.35648161    0.28878728    0.35338882    0.26348692]][0m
[37m[1m[2023-07-10 14:02:22,079][227910] Max Reward on eval: 1511.166413250062[0m
[37m[1m[2023-07-10 14:02:22,079][227910] Min Reward on eval: 1511.166413250062[0m
[37m[1m[2023-07-10 14:02:22,080][227910] Mean Reward across all agents: 1511.166413250062[0m
[37m[1m[2023-07-10 14:02:22,080][227910] Average Trajectory Length: 998.209[0m
[36m[2023-07-10 14:02:27,554][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:02:27,555][227910] Reward + Measures: [[ 656.97270178    0.40030003    0.2899        0.36129999    0.29819998]
 [-623.27368078    0.69740003    0.63590002    0.72909993    0.56      ]
 [  73.22173153    0.52229995    0.48650002    0.5426001     0.47839999]
 ...
 [1033.31528621    0.38249999    0.35170001    0.42500001    0.3475    ]
 [-117.56547736    0.21870001    0.52789998    0.21700001    0.5176    ]
 [-169.11177251    0.26619998    0.29360002    0.27859998    0.29089999]][0m
[37m[1m[2023-07-10 14:02:27,555][227910] Max Reward on eval: 1547.1849947437179[0m
[37m[1m[2023-07-10 14:02:27,555][227910] Min Reward on eval: -1002.4511691064108[0m
[37m[1m[2023-07-10 14:02:27,556][227910] Mean Reward across all agents: 97.65762535646309[0m
[37m[1m[2023-07-10 14:02:27,556][227910] Average Trajectory Length: 981.9226666666666[0m
[36m[2023-07-10 14:02:27,558][227910] mean_value=-1152.0132847220866, max_value=886.598624448849[0m
[37m[1m[2023-07-10 14:02:27,561][227910] New mean coefficients: [[ 1.3039787   0.30506456  1.6529323  -1.3059106   3.8713648 ]][0m
[37m[1m[2023-07-10 14:02:27,562][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:02:37,190][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 14:02:37,190][227910] FPS: 398903.01[0m
[36m[2023-07-10 14:02:37,193][227910] itr=557, itrs=2000, Progress: 27.85%[0m
[36m[2023-07-10 14:02:48,711][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 14:02:48,711][227910] FPS: 333852.16[0m
[36m[2023-07-10 14:02:53,531][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:02:53,531][227910] Reward + Measures: [[1315.65900843    0.37437645    0.30329329    0.28901541    0.26489025]][0m
[37m[1m[2023-07-10 14:02:53,531][227910] Max Reward on eval: 1315.6590084282957[0m
[37m[1m[2023-07-10 14:02:53,531][227910] Min Reward on eval: 1315.6590084282957[0m
[37m[1m[2023-07-10 14:02:53,532][227910] Mean Reward across all agents: 1315.6590084282957[0m
[37m[1m[2023-07-10 14:02:53,532][227910] Average Trajectory Length: 957.2066666666666[0m
[36m[2023-07-10 14:02:58,934][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:02:58,934][227910] Reward + Measures: [[-170.42103503    0.32049999    0.3567        0.30240002    0.31880003]
 [-635.4993448     0.39135775    0.42891312    0.40139198    0.44642535]
 [-335.47981666    0.41489998    0.33970001    0.49380001    0.33510002]
 ...
 [-351.37723049    0.2852        0.30860004    0.33049998    0.28040001]
 [ 254.91192786    0.29542509    0.28106159    0.27191797    0.2882688 ]
 [-468.04381029    0.36327398    0.31157345    0.42441502    0.28213581]][0m
[37m[1m[2023-07-10 14:02:58,934][227910] Max Reward on eval: 1292.5744639066165[0m
[37m[1m[2023-07-10 14:02:58,935][227910] Min Reward on eval: -1054.2253726083086[0m
[37m[1m[2023-07-10 14:02:58,935][227910] Mean Reward across all agents: -231.8866783911457[0m
[37m[1m[2023-07-10 14:02:58,935][227910] Average Trajectory Length: 921.8383333333333[0m
[36m[2023-07-10 14:02:58,937][227910] mean_value=-1842.8192145108458, max_value=439.24440075977793[0m
[37m[1m[2023-07-10 14:02:58,939][227910] New mean coefficients: [[ 1.9348401  -0.03830022  1.7923374  -0.04112363  3.8226392 ]][0m
[37m[1m[2023-07-10 14:02:58,940][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:03:08,617][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 14:03:08,617][227910] FPS: 396902.69[0m
[36m[2023-07-10 14:03:08,619][227910] itr=558, itrs=2000, Progress: 27.90%[0m
[36m[2023-07-10 14:03:20,247][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 14:03:20,247][227910] FPS: 330733.01[0m
[36m[2023-07-10 14:03:25,036][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:03:25,036][227910] Reward + Measures: [[1461.87020657    0.38677335    0.29947269    0.29091787    0.26187691]][0m
[37m[1m[2023-07-10 14:03:25,036][227910] Max Reward on eval: 1461.8702065746363[0m
[37m[1m[2023-07-10 14:03:25,036][227910] Min Reward on eval: 1461.8702065746363[0m
[37m[1m[2023-07-10 14:03:25,037][227910] Mean Reward across all agents: 1461.8702065746363[0m
[37m[1m[2023-07-10 14:03:25,037][227910] Average Trajectory Length: 959.1016666666667[0m
[36m[2023-07-10 14:03:30,642][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:03:30,648][227910] Reward + Measures: [[-1292.38224574     0.19985925     0.28830558     0.18503159
      0.29695153]
 [   -4.4076413      0.32646313     0.32668895     0.3888106
      0.27749816]
 [ -303.52585837     0.30105716     0.41484284     0.32078567
      0.39500004]
 ...
 [ -772.85656443     0.21949999     0.42150003     0.22490001
      0.37899998]
 [ -503.45023238     0.28764537     0.29681611     0.32393053
      0.22529685]
 [ -956.62015307     0.38144758     0.36981231     0.3108103
      0.25895795]][0m
[37m[1m[2023-07-10 14:03:30,648][227910] Max Reward on eval: 1045.462047570781[0m
[37m[1m[2023-07-10 14:03:30,649][227910] Min Reward on eval: -1448.0795721306[0m
[37m[1m[2023-07-10 14:03:30,649][227910] Mean Reward across all agents: -456.9011668500143[0m
[37m[1m[2023-07-10 14:03:30,649][227910] Average Trajectory Length: 895.7966666666666[0m
[36m[2023-07-10 14:03:30,651][227910] mean_value=-2450.591735966574, max_value=524.2926836025948[0m
[37m[1m[2023-07-10 14:03:30,653][227910] New mean coefficients: [[ 1.747229    0.88229704  1.4579642  -1.0997036   3.913654  ]][0m
[37m[1m[2023-07-10 14:03:30,654][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:03:40,332][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 14:03:40,332][227910] FPS: 396851.46[0m
[36m[2023-07-10 14:03:40,334][227910] itr=559, itrs=2000, Progress: 27.95%[0m
[36m[2023-07-10 14:03:51,819][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 14:03:51,819][227910] FPS: 334863.08[0m
[36m[2023-07-10 14:03:56,538][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:03:56,539][227910] Reward + Measures: [[1593.98610274    0.38782543    0.28876945    0.28622481    0.25144735]][0m
[37m[1m[2023-07-10 14:03:56,539][227910] Max Reward on eval: 1593.9861027428806[0m
[37m[1m[2023-07-10 14:03:56,539][227910] Min Reward on eval: 1593.9861027428806[0m
[37m[1m[2023-07-10 14:03:56,539][227910] Mean Reward across all agents: 1593.9861027428806[0m
[37m[1m[2023-07-10 14:03:56,539][227910] Average Trajectory Length: 965.4653333333333[0m
[36m[2023-07-10 14:04:01,989][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:04:01,989][227910] Reward + Measures: [[-309.64712811    0.46000001    0.2561        0.46410003    0.36749998]
 [ 547.58981732    0.3276        0.2139        0.32609999    0.25279999]
 [ 597.73610928    0.29527569    0.31062374    0.26759872    0.26063624]
 ...
 [-653.48175628    0.18279998    0.51789999    0.19950001    0.4862    ]
 [ 269.11150867    0.44360408    0.39315364    0.3986834     0.38638684]
 [-520.60221842    0.24169998    0.33660001    0.26769999    0.2911    ]][0m
[37m[1m[2023-07-10 14:04:01,989][227910] Max Reward on eval: 1609.2069888798753[0m
[37m[1m[2023-07-10 14:04:01,990][227910] Min Reward on eval: -1262.7445454985834[0m
[37m[1m[2023-07-10 14:04:01,990][227910] Mean Reward across all agents: -49.64785538284888[0m
[37m[1m[2023-07-10 14:04:01,990][227910] Average Trajectory Length: 962.677[0m
[36m[2023-07-10 14:04:01,992][227910] mean_value=-1744.121537890212, max_value=968.5796611032935[0m
[37m[1m[2023-07-10 14:04:01,994][227910] New mean coefficients: [[ 1.6120229  0.5886184  1.193186  -0.5323924  3.3157933]][0m
[37m[1m[2023-07-10 14:04:01,995][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:04:11,736][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 14:04:11,737][227910] FPS: 394263.47[0m
[36m[2023-07-10 14:04:11,739][227910] itr=560, itrs=2000, Progress: 28.00%[0m
[37m[1m[2023-07-10 14:04:14,663][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000540[0m
[36m[2023-07-10 14:04:26,549][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 14:04:26,549][227910] FPS: 330298.43[0m
[36m[2023-07-10 14:04:31,376][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:04:31,376][227910] Reward + Measures: [[1746.25275839    0.39816186    0.28042957    0.2861582     0.24640861]][0m
[37m[1m[2023-07-10 14:04:31,376][227910] Max Reward on eval: 1746.2527583853112[0m
[37m[1m[2023-07-10 14:04:31,376][227910] Min Reward on eval: 1746.2527583853112[0m
[37m[1m[2023-07-10 14:04:31,376][227910] Mean Reward across all agents: 1746.2527583853112[0m
[37m[1m[2023-07-10 14:04:31,377][227910] Average Trajectory Length: 974.1236666666666[0m
[36m[2023-07-10 14:04:36,912][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:04:36,913][227910] Reward + Measures: [[  779.63502391     0.34869751     0.37943992     0.31569558
      0.29391724]
 [    9.07048918     0.33469999     0.34160003     0.3565
      0.36290002]
 [ -492.08435269     0.17482606     0.18492113     0.1931542
      0.12918663]
 ...
 [  207.09342381     0.2696         0.37079999     0.28959998
      0.3276    ]
 [-1174.1909474      0.09570435     0.10604782     0.15015653
      0.11105652]
 [ -934.26836344     0.16780391     0.20227106     0.20840235
      0.20092864]][0m
[37m[1m[2023-07-10 14:04:36,913][227910] Max Reward on eval: 1890.8346576662268[0m
[37m[1m[2023-07-10 14:04:36,914][227910] Min Reward on eval: -1426.255584423896[0m
[37m[1m[2023-07-10 14:04:36,914][227910] Mean Reward across all agents: -13.498581463812595[0m
[37m[1m[2023-07-10 14:04:36,914][227910] Average Trajectory Length: 885.536[0m
[36m[2023-07-10 14:04:36,916][227910] mean_value=-2065.897443648615, max_value=451.7783603262625[0m
[37m[1m[2023-07-10 14:04:36,918][227910] New mean coefficients: [[ 0.95221317  0.67240655  0.60294026 -0.22510782  3.6560094 ]][0m
[37m[1m[2023-07-10 14:04:36,919][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:04:46,704][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 14:04:46,705][227910] FPS: 392488.51[0m
[36m[2023-07-10 14:04:46,707][227910] itr=561, itrs=2000, Progress: 28.05%[0m
[36m[2023-07-10 14:04:58,195][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 14:04:58,196][227910] FPS: 334727.89[0m
[36m[2023-07-10 14:05:02,943][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:05:02,944][227910] Reward + Measures: [[1886.1224152     0.40273982    0.26965487    0.28167099    0.238333  ]][0m
[37m[1m[2023-07-10 14:05:02,944][227910] Max Reward on eval: 1886.122415198684[0m
[37m[1m[2023-07-10 14:05:02,944][227910] Min Reward on eval: 1886.122415198684[0m
[37m[1m[2023-07-10 14:05:02,944][227910] Mean Reward across all agents: 1886.122415198684[0m
[37m[1m[2023-07-10 14:05:02,944][227910] Average Trajectory Length: 975.5913333333333[0m
[36m[2023-07-10 14:05:08,436][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:05:08,437][227910] Reward + Measures: [[ 760.80964508    0.38909999    0.30630001    0.25040001    0.2658    ]
 [ 893.84195133    0.35950002    0.34129998    0.23799999    0.30640003]
 [ 620.72950823    0.43360001    0.34780002    0.30039999    0.32350001]
 ...
 [ 964.6185974     0.34100002    0.34559998    0.25390002    0.32650003]
 [1291.56795495    0.38429999    0.31000003    0.24819998    0.2832    ]
 [1226.53847546    0.33103129    0.26915321    0.32096347    0.20096147]][0m
[37m[1m[2023-07-10 14:05:08,437][227910] Max Reward on eval: 2168.766183044878[0m
[37m[1m[2023-07-10 14:05:08,437][227910] Min Reward on eval: 201.75288055778657[0m
[37m[1m[2023-07-10 14:05:08,438][227910] Mean Reward across all agents: 1254.7626180965844[0m
[37m[1m[2023-07-10 14:05:08,438][227910] Average Trajectory Length: 972.665[0m
[36m[2023-07-10 14:05:08,440][227910] mean_value=-1715.904909940103, max_value=1665.59749523178[0m
[37m[1m[2023-07-10 14:05:08,442][227910] New mean coefficients: [[ 1.4865854  1.2300637  1.4765924 -1.3050147  3.632217 ]][0m
[37m[1m[2023-07-10 14:05:08,443][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:05:18,010][227910] train() took 9.57 seconds to complete[0m
[36m[2023-07-10 14:05:18,010][227910] FPS: 401434.79[0m
[36m[2023-07-10 14:05:18,013][227910] itr=562, itrs=2000, Progress: 28.10%[0m
[36m[2023-07-10 14:05:29,501][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 14:05:29,501][227910] FPS: 334725.14[0m
[36m[2023-07-10 14:05:34,243][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:05:34,244][227910] Reward + Measures: [[1996.05937006    0.40163651    0.26304024    0.27851593    0.2300518 ]][0m
[37m[1m[2023-07-10 14:05:34,244][227910] Max Reward on eval: 1996.0593700576308[0m
[37m[1m[2023-07-10 14:05:34,244][227910] Min Reward on eval: 1996.0593700576308[0m
[37m[1m[2023-07-10 14:05:34,244][227910] Mean Reward across all agents: 1996.0593700576308[0m
[37m[1m[2023-07-10 14:05:34,245][227910] Average Trajectory Length: 975.0976666666667[0m
[36m[2023-07-10 14:05:39,829][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:05:39,835][227910] Reward + Measures: [[-776.47335211    0.25122571    0.20039511    0.21025157    0.21186684]
 [-425.38098107    0.35369182    0.26055509    0.28199264    0.28528452]
 [-379.55746212    0.19229999    0.17629999    0.1858        0.13509999]
 ...
 [-272.13164039    0.41250339    0.3634288     0.36228138    0.35237798]
 [ 753.17591137    0.36166164    0.34950042    0.36636868    0.24982028]
 [ 571.74399863    0.44600001    0.41349998    0.3872        0.35069999]][0m
[37m[1m[2023-07-10 14:05:39,835][227910] Max Reward on eval: 2033.159884397511[0m
[37m[1m[2023-07-10 14:05:39,836][227910] Min Reward on eval: -971.2538401896251[0m
[37m[1m[2023-07-10 14:05:39,836][227910] Mean Reward across all agents: 193.61542213828903[0m
[37m[1m[2023-07-10 14:05:39,836][227910] Average Trajectory Length: 893.5169999999999[0m
[36m[2023-07-10 14:05:39,838][227910] mean_value=-2495.3487256531616, max_value=306.1022700856864[0m
[37m[1m[2023-07-10 14:05:39,840][227910] New mean coefficients: [[ 1.63896     0.94555295  2.0480685  -0.67141646  3.3473077 ]][0m
[37m[1m[2023-07-10 14:05:39,841][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:05:49,547][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 14:05:49,547][227910] FPS: 395719.48[0m
[36m[2023-07-10 14:05:49,549][227910] itr=563, itrs=2000, Progress: 28.15%[0m
[36m[2023-07-10 14:06:01,126][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 14:06:01,126][227910] FPS: 332167.03[0m
[36m[2023-07-10 14:06:05,906][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:06:05,906][227910] Reward + Measures: [[2133.67494913    0.39887664    0.25684232    0.27461323    0.21912061]][0m
[37m[1m[2023-07-10 14:06:05,906][227910] Max Reward on eval: 2133.674949129937[0m
[37m[1m[2023-07-10 14:06:05,907][227910] Min Reward on eval: 2133.674949129937[0m
[37m[1m[2023-07-10 14:06:05,907][227910] Mean Reward across all agents: 2133.674949129937[0m
[37m[1m[2023-07-10 14:06:05,907][227910] Average Trajectory Length: 977.9446666666666[0m
[36m[2023-07-10 14:06:11,318][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:06:11,319][227910] Reward + Measures: [[  198.80044443     0.44889998     0.3725         0.35560003
      0.38320002]
 [  859.56590201     0.35330001     0.3188         0.26069999
      0.27250001]
 [ -783.62874143     0.20320001     0.21759999     0.15089999
      0.22070003]
 ...
 [  257.4841589      0.84339994     0.71090001     0.78649998
      0.72320002]
 [-1002.10844157     0.14053582     0.18495822     0.1328418
      0.17551941]
 [  351.79611408     0.41289997     0.41139999     0.33159998
      0.39570004]][0m
[37m[1m[2023-07-10 14:06:11,319][227910] Max Reward on eval: 2114.785439384822[0m
[37m[1m[2023-07-10 14:06:11,319][227910] Min Reward on eval: -1155.9644438940218[0m
[37m[1m[2023-07-10 14:06:11,320][227910] Mean Reward across all agents: 149.0634460929635[0m
[37m[1m[2023-07-10 14:06:11,320][227910] Average Trajectory Length: 958.7156666666666[0m
[36m[2023-07-10 14:06:11,321][227910] mean_value=-1609.1063587649687, max_value=675.9853760772779[0m
[37m[1m[2023-07-10 14:06:11,324][227910] New mean coefficients: [[2.0693352  1.2223288  1.6338323  0.11166209 3.7886147 ]][0m
[37m[1m[2023-07-10 14:06:11,325][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:06:21,013][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 14:06:21,014][227910] FPS: 396401.97[0m
[36m[2023-07-10 14:06:21,016][227910] itr=564, itrs=2000, Progress: 28.20%[0m
[36m[2023-07-10 14:06:32,463][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 14:06:32,464][227910] FPS: 335999.85[0m
[36m[2023-07-10 14:06:37,237][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:06:37,237][227910] Reward + Measures: [[2217.02455775    0.41019994    0.25048146    0.27232686    0.21484739]][0m
[37m[1m[2023-07-10 14:06:37,238][227910] Max Reward on eval: 2217.024557748854[0m
[37m[1m[2023-07-10 14:06:37,238][227910] Min Reward on eval: 2217.024557748854[0m
[37m[1m[2023-07-10 14:06:37,238][227910] Mean Reward across all agents: 2217.024557748854[0m
[37m[1m[2023-07-10 14:06:37,238][227910] Average Trajectory Length: 976.3206666666666[0m
[36m[2023-07-10 14:06:42,739][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:06:42,739][227910] Reward + Measures: [[1997.52643427    0.41949996    0.26700002    0.2999        0.2254    ]
 [1273.52191857    0.40099177    0.30493307    0.25414047    0.27112728]
 [1764.93940072    0.48730001    0.28940001    0.2949        0.2529    ]
 ...
 [1730.18125829    0.47639999    0.26190004    0.2949        0.2421    ]
 [ 740.38437859    0.63160002    0.43129998    0.4756        0.41760001]
 [1579.10294054    0.46830001    0.28150001    0.31070003    0.21870001]][0m
[37m[1m[2023-07-10 14:06:42,739][227910] Max Reward on eval: 2418.7358822396955[0m
[37m[1m[2023-07-10 14:06:42,740][227910] Min Reward on eval: 43.89815770058194[0m
[37m[1m[2023-07-10 14:06:42,740][227910] Mean Reward across all agents: 1575.3789209688196[0m
[37m[1m[2023-07-10 14:06:42,740][227910] Average Trajectory Length: 990.257[0m
[36m[2023-07-10 14:06:42,742][227910] mean_value=-1651.9859830719997, max_value=1936.864461091684[0m
[37m[1m[2023-07-10 14:06:42,745][227910] New mean coefficients: [[3.2766     1.3451985  2.6814609  0.68036646 4.621492  ]][0m
[37m[1m[2023-07-10 14:06:42,746][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:06:52,428][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 14:06:52,428][227910] FPS: 396665.92[0m
[36m[2023-07-10 14:06:52,430][227910] itr=565, itrs=2000, Progress: 28.25%[0m
[36m[2023-07-10 14:07:03,971][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 14:07:03,972][227910] FPS: 333186.33[0m
[36m[2023-07-10 14:07:08,784][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:07:08,789][227910] Reward + Measures: [[2310.79490207    0.41534284    0.24870674    0.27346623    0.21345934]][0m
[37m[1m[2023-07-10 14:07:08,790][227910] Max Reward on eval: 2310.7949020717356[0m
[37m[1m[2023-07-10 14:07:08,790][227910] Min Reward on eval: 2310.7949020717356[0m
[37m[1m[2023-07-10 14:07:08,790][227910] Mean Reward across all agents: 2310.7949020717356[0m
[37m[1m[2023-07-10 14:07:08,791][227910] Average Trajectory Length: 978.5636666666667[0m
[36m[2023-07-10 14:07:14,345][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:07:14,346][227910] Reward + Measures: [[1985.34315633    0.39239016    0.25298247    0.28828835    0.1983176 ]
 [2081.05963518    0.41479999    0.2383        0.29720002    0.21960001]
 [1657.32996269    0.4197        0.2904        0.3075        0.24350002]
 ...
 [1549.17157288    0.41510001    0.26180002    0.31009999    0.23469999]
 [2043.5696842     0.43729997    0.26890001    0.31170002    0.229     ]
 [1930.4805506     0.40758115    0.25286981    0.27484527    0.21245472]][0m
[37m[1m[2023-07-10 14:07:14,346][227910] Max Reward on eval: 2498.9467176947046[0m
[37m[1m[2023-07-10 14:07:14,346][227910] Min Reward on eval: 907.735755637684[0m
[37m[1m[2023-07-10 14:07:14,346][227910] Mean Reward across all agents: 1748.0720076799685[0m
[37m[1m[2023-07-10 14:07:14,347][227910] Average Trajectory Length: 963.3313333333333[0m
[36m[2023-07-10 14:07:14,348][227910] mean_value=-2446.6678480686196, max_value=79.52635706088995[0m
[37m[1m[2023-07-10 14:07:14,350][227910] New mean coefficients: [[2.055701  0.6225628 1.7726971 0.6667728 3.2043734]][0m
[37m[1m[2023-07-10 14:07:14,351][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:07:24,019][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 14:07:24,019][227910] FPS: 397259.94[0m
[36m[2023-07-10 14:07:24,021][227910] itr=566, itrs=2000, Progress: 28.30%[0m
[36m[2023-07-10 14:07:35,669][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 14:07:35,669][227910] FPS: 330157.88[0m
[36m[2023-07-10 14:07:40,388][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:07:40,389][227910] Reward + Measures: [[2397.12911688    0.41537124    0.2497125     0.27413949    0.21560161]][0m
[37m[1m[2023-07-10 14:07:40,389][227910] Max Reward on eval: 2397.1291168803714[0m
[37m[1m[2023-07-10 14:07:40,389][227910] Min Reward on eval: 2397.1291168803714[0m
[37m[1m[2023-07-10 14:07:40,390][227910] Mean Reward across all agents: 2397.1291168803714[0m
[37m[1m[2023-07-10 14:07:40,390][227910] Average Trajectory Length: 980.4483333333333[0m
[36m[2023-07-10 14:07:45,882][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:07:45,888][227910] Reward + Measures: [[ 659.91982955    0.42283246    0.45390663    0.42444372    0.40740395]
 [ 473.88238046    0.30630001    0.34549999    0.30610001    0.24510001]
 [1327.95444942    0.35160002    0.3382        0.2658        0.26050001]
 ...
 [1912.45268338    0.36310002    0.25570002    0.29000002    0.20100001]
 [1620.88060452    0.46380001    0.29420003    0.3012        0.2651    ]
 [ 213.88083983    0.35389999    0.46970001    0.40920001    0.39700001]][0m
[37m[1m[2023-07-10 14:07:45,888][227910] Max Reward on eval: 2589.764289326733[0m
[37m[1m[2023-07-10 14:07:45,889][227910] Min Reward on eval: -123.0523286984535[0m
[37m[1m[2023-07-10 14:07:45,889][227910] Mean Reward across all agents: 1508.1968519209443[0m
[37m[1m[2023-07-10 14:07:45,889][227910] Average Trajectory Length: 982.3353333333333[0m
[36m[2023-07-10 14:07:45,891][227910] mean_value=-1816.8239648923118, max_value=557.7229805982795[0m
[37m[1m[2023-07-10 14:07:45,894][227910] New mean coefficients: [[2.3191814 0.6518245 2.9165597 1.1561079 4.445295 ]][0m
[37m[1m[2023-07-10 14:07:45,895][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:07:55,635][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 14:07:55,635][227910] FPS: 394309.19[0m
[36m[2023-07-10 14:07:55,637][227910] itr=567, itrs=2000, Progress: 28.35%[0m
[36m[2023-07-10 14:08:07,088][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 14:08:07,088][227910] FPS: 335903.40[0m
[36m[2023-07-10 14:08:11,830][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:08:11,830][227910] Reward + Measures: [[2548.00694054    0.40111569    0.23573487    0.27379635    0.2008442 ]][0m
[37m[1m[2023-07-10 14:08:11,831][227910] Max Reward on eval: 2548.006940535044[0m
[37m[1m[2023-07-10 14:08:11,831][227910] Min Reward on eval: 2548.006940535044[0m
[37m[1m[2023-07-10 14:08:11,831][227910] Mean Reward across all agents: 2548.006940535044[0m
[37m[1m[2023-07-10 14:08:11,831][227910] Average Trajectory Length: 977.8573333333333[0m
[36m[2023-07-10 14:08:17,297][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:08:17,298][227910] Reward + Measures: [[  -76.44299431     0.44292027     0.27984148     0.30907747
      0.29823399]
 [  654.6366274      0.44189999     0.38180003     0.46869999
      0.34999999]
 [  839.11304855     0.4673         0.37740001     0.5007
      0.35639998]
 ...
 [ -913.97107088     0.41184804     0.4030582      0.3990849
      0.41636276]
 [-1046.07885314     0.26243147     0.23989002     0.2606633
      0.21438549]
 [   64.41020926     0.4199         0.36380002     0.44910002
      0.3283    ]][0m
[37m[1m[2023-07-10 14:08:17,298][227910] Max Reward on eval: 2466.6230787892828[0m
[37m[1m[2023-07-10 14:08:17,298][227910] Min Reward on eval: -1079.3627479944146[0m
[37m[1m[2023-07-10 14:08:17,299][227910] Mean Reward across all agents: 472.13140808601787[0m
[37m[1m[2023-07-10 14:08:17,299][227910] Average Trajectory Length: 955.2166666666666[0m
[36m[2023-07-10 14:08:17,301][227910] mean_value=-1438.8372619195175, max_value=1824.025519747403[0m
[37m[1m[2023-07-10 14:08:17,304][227910] New mean coefficients: [[2.6187618 1.2739232 2.3922105 1.6495576 4.859854 ]][0m
[37m[1m[2023-07-10 14:08:17,304][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:08:26,917][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 14:08:26,917][227910] FPS: 399542.71[0m
[36m[2023-07-10 14:08:26,920][227910] itr=568, itrs=2000, Progress: 28.40%[0m
[36m[2023-07-10 14:08:38,380][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 14:08:38,380][227910] FPS: 335543.42[0m
[36m[2023-07-10 14:08:43,119][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:08:43,120][227910] Reward + Measures: [[2686.17433952    0.410344      0.23697864    0.27327877    0.20452408]][0m
[37m[1m[2023-07-10 14:08:43,120][227910] Max Reward on eval: 2686.1743395227354[0m
[37m[1m[2023-07-10 14:08:43,120][227910] Min Reward on eval: 2686.1743395227354[0m
[37m[1m[2023-07-10 14:08:43,120][227910] Mean Reward across all agents: 2686.1743395227354[0m
[37m[1m[2023-07-10 14:08:43,121][227910] Average Trajectory Length: 984.0626666666666[0m
[36m[2023-07-10 14:08:48,511][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:08:48,511][227910] Reward + Measures: [[  532.16760169     0.4014214      0.38978574     0.39747682
      0.38055408]
 [  142.59465971     0.3786         0.35699999     0.3599
      0.37510002]
 [ 2405.16023128     0.43549997     0.2332         0.29500002
      0.23029999]
 ...
 [  -64.7412892      0.37009999     0.4646         0.35489997
      0.46339998]
 [-1004.35372595     0.19032453     0.49375659     0.23115285
      0.450883  ]
 [ 1472.88419465     0.41759998     0.27080002     0.27630001
      0.26800001]][0m
[37m[1m[2023-07-10 14:08:48,512][227910] Max Reward on eval: 2841.7175718661397[0m
[37m[1m[2023-07-10 14:08:48,512][227910] Min Reward on eval: -1257.8070082615595[0m
[37m[1m[2023-07-10 14:08:48,512][227910] Mean Reward across all agents: 388.8562958197911[0m
[37m[1m[2023-07-10 14:08:48,512][227910] Average Trajectory Length: 984.826[0m
[36m[2023-07-10 14:08:48,513][227910] mean_value=-1646.7591988505678, max_value=-229.63203229770875[0m
[36m[2023-07-10 14:08:48,516][227910] XNES is restarting with a new solution whose measures are [0.21230002 0.37670001 0.46090004 0.26030001] and objective is 1198.989592139388[0m
[36m[2023-07-10 14:08:48,517][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 14:08:48,519][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 14:08:48,520][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:08:58,223][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 14:08:58,223][227910] FPS: 395807.89[0m
[36m[2023-07-10 14:08:58,226][227910] itr=569, itrs=2000, Progress: 28.45%[0m
[36m[2023-07-10 14:09:09,759][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 14:09:09,759][227910] FPS: 333492.82[0m
[36m[2023-07-10 14:09:14,437][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:09:14,437][227910] Reward + Measures: [[1234.2735715     0.25708643    0.33502513    0.27844337    0.15196127]][0m
[37m[1m[2023-07-10 14:09:14,437][227910] Max Reward on eval: 1234.2735714993962[0m
[37m[1m[2023-07-10 14:09:14,438][227910] Min Reward on eval: 1234.2735714993962[0m
[37m[1m[2023-07-10 14:09:14,438][227910] Mean Reward across all agents: 1234.2735714993962[0m
[37m[1m[2023-07-10 14:09:14,438][227910] Average Trajectory Length: 917.7046666666666[0m
[36m[2023-07-10 14:09:19,822][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:09:19,828][227910] Reward + Measures: [[ -249.35163811     0.26898241     0.37504953     0.3556684
      0.27412894]
 [-1165.45616805     0.14252611     0.10638893     0.19692795
      0.15641604]
 [ -593.85962126     0.26482534     0.12659742     0.31228703
      0.18134163]
 ...
 [ -515.31948138     0.1504187      0.20519929     0.19107699
      0.17968272]
 [ -798.57894448     0.21115553     0.18793327     0.24607383
      0.18720894]
 [ -847.99275073     0.1541642      0.200385       0.1995822
      0.17138772]][0m
[37m[1m[2023-07-10 14:09:19,828][227910] Max Reward on eval: 746.0223921027267[0m
[37m[1m[2023-07-10 14:09:19,829][227910] Min Reward on eval: -2676.247038484248[0m
[37m[1m[2023-07-10 14:09:19,829][227910] Mean Reward across all agents: -667.3694729810495[0m
[37m[1m[2023-07-10 14:09:19,829][227910] Average Trajectory Length: 663.2236666666666[0m
[36m[2023-07-10 14:09:19,831][227910] mean_value=-1772.1085505291578, max_value=383.2631277085462[0m
[37m[1m[2023-07-10 14:09:19,834][227910] New mean coefficients: [[ 1.0126497  -1.2084715  -1.202864   -3.101262   -0.61654866]][0m
[37m[1m[2023-07-10 14:09:19,835][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:09:29,591][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 14:09:29,592][227910] FPS: 393639.91[0m
[36m[2023-07-10 14:09:29,594][227910] itr=570, itrs=2000, Progress: 28.50%[0m
[37m[1m[2023-07-10 14:09:32,622][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000550[0m
[36m[2023-07-10 14:09:44,619][227910] train() took 11.73 seconds to complete[0m
[36m[2023-07-10 14:09:44,619][227910] FPS: 327277.65[0m
[36m[2023-07-10 14:09:49,380][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:09:49,385][227910] Reward + Measures: [[412.62268165   0.21542814   0.2768822    0.26369473   0.25520784]][0m
[37m[1m[2023-07-10 14:09:49,386][227910] Max Reward on eval: 412.6226816485403[0m
[37m[1m[2023-07-10 14:09:49,386][227910] Min Reward on eval: 412.6226816485403[0m
[37m[1m[2023-07-10 14:09:49,386][227910] Mean Reward across all agents: 412.6226816485403[0m
[37m[1m[2023-07-10 14:09:49,387][227910] Average Trajectory Length: 900.043[0m
[36m[2023-07-10 14:09:54,863][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:09:54,868][227910] Reward + Measures: [[-1114.67116872     0.20227909     0.24605457     0.24989589
      0.23548506]
 [-1870.30069286     0.13880105     0.19708268     0.08527455
      0.14064346]
 [ -625.02205448     0.35290012     0.2989552      0.27952042
      0.2431345 ]
 ...
 [ -929.4476002      0.22143118     0.24869533     0.27654129
      0.13607258]
 [-1286.39997093     0.10755362     0.21072999     0.16665863
      0.16443837]
 [ -655.341403       0.21541598     0.20368019     0.27455911
      0.17333871]][0m
[37m[1m[2023-07-10 14:09:54,869][227910] Max Reward on eval: 279.187975743931[0m
[37m[1m[2023-07-10 14:09:54,869][227910] Min Reward on eval: -2331.189861257421[0m
[37m[1m[2023-07-10 14:09:54,869][227910] Mean Reward across all agents: -917.271068302076[0m
[37m[1m[2023-07-10 14:09:54,870][227910] Average Trajectory Length: 747.9546666666666[0m
[36m[2023-07-10 14:09:54,871][227910] mean_value=-2256.265745544707, max_value=528.1620253263136[0m
[37m[1m[2023-07-10 14:09:54,874][227910] New mean coefficients: [[ 0.13213843 -0.33039117 -1.1682528  -2.3346817  -0.33550277]][0m
[37m[1m[2023-07-10 14:09:54,874][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:10:04,630][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 14:10:04,630][227910] FPS: 393696.65[0m
[36m[2023-07-10 14:10:04,633][227910] itr=571, itrs=2000, Progress: 28.55%[0m
[36m[2023-07-10 14:10:16,264][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 14:10:16,265][227910] FPS: 330671.00[0m
[36m[2023-07-10 14:10:20,994][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:10:20,995][227910] Reward + Measures: [[198.6605327    0.23272054   0.25992212   0.28486046   0.30771166]][0m
[37m[1m[2023-07-10 14:10:20,995][227910] Max Reward on eval: 198.66053270364336[0m
[37m[1m[2023-07-10 14:10:20,995][227910] Min Reward on eval: 198.66053270364336[0m
[37m[1m[2023-07-10 14:10:20,995][227910] Mean Reward across all agents: 198.66053270364336[0m
[37m[1m[2023-07-10 14:10:20,995][227910] Average Trajectory Length: 802.286[0m
[36m[2023-07-10 14:10:26,555][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:10:26,616][227910] Reward + Measures: [[-1072.49430515     0.20479999     0.2757         0.1427
      0.20039999]
 [ -995.13857113     0.29174539     0.2897532      0.24684604
      0.28278056]
 [ -415.7413987      0.23406883     0.2014263      0.23645942
      0.17325325]
 ...
 [-1313.38966208     0.8563         0.81100005     0.83149999
      0.1779    ]
 [ -629.25228973     0.32812604     0.32052502     0.32725033
      0.27603996]
 [ -527.14128134     0.14610514     0.51619744     0.30931282
      0.41392055]][0m
[37m[1m[2023-07-10 14:10:26,616][227910] Max Reward on eval: 663.1798147884198[0m
[37m[1m[2023-07-10 14:10:26,617][227910] Min Reward on eval: -1619.4358716232703[0m
[37m[1m[2023-07-10 14:10:26,617][227910] Mean Reward across all agents: -708.2535795677119[0m
[37m[1m[2023-07-10 14:10:26,617][227910] Average Trajectory Length: 771.1226666666666[0m
[36m[2023-07-10 14:10:26,619][227910] mean_value=-1769.459258958002, max_value=458.30360665700863[0m
[37m[1m[2023-07-10 14:10:26,621][227910] New mean coefficients: [[ 0.3059156  -1.2976847  -0.77153    -1.6738467  -0.40068522]][0m
[37m[1m[2023-07-10 14:10:26,622][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:10:36,502][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 14:10:36,502][227910] FPS: 388728.90[0m
[36m[2023-07-10 14:10:36,504][227910] itr=572, itrs=2000, Progress: 28.60%[0m
[36m[2023-07-10 14:10:48,024][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 14:10:48,024][227910] FPS: 333907.33[0m
[36m[2023-07-10 14:10:52,734][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:10:52,735][227910] Reward + Measures: [[565.95100544   0.24173965   0.32853708   0.33114123   0.25488099]][0m
[37m[1m[2023-07-10 14:10:52,735][227910] Max Reward on eval: 565.9510054378082[0m
[37m[1m[2023-07-10 14:10:52,735][227910] Min Reward on eval: 565.9510054378082[0m
[37m[1m[2023-07-10 14:10:52,735][227910] Mean Reward across all agents: 565.9510054378082[0m
[37m[1m[2023-07-10 14:10:52,736][227910] Average Trajectory Length: 975.5083333333333[0m
[36m[2023-07-10 14:10:58,313][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:10:58,314][227910] Reward + Measures: [[ -828.43118066     0.22177017     0.24611564     0.20352323
      0.24542789]
 [  244.39052434     0.2044367      0.32613125     0.22496843
      0.18223934]
 [ -906.45261816     0.19167595     0.23072112     0.19434263
      0.18046674]
 ...
 [ -845.41724696     0.22570978     0.23461528     0.22496128
      0.12193008]
 [ -847.19469438     0.24306987     0.2303526      0.18199621
      0.17743948]
 [-1002.17462819     0.24611627     0.21827526     0.22185831
      0.1866302 ]][0m
[37m[1m[2023-07-10 14:10:58,314][227910] Max Reward on eval: 464.1051891326788[0m
[37m[1m[2023-07-10 14:10:58,314][227910] Min Reward on eval: -1524.7834745194764[0m
[37m[1m[2023-07-10 14:10:58,315][227910] Mean Reward across all agents: -733.579274375543[0m
[37m[1m[2023-07-10 14:10:58,315][227910] Average Trajectory Length: 759.1763333333333[0m
[36m[2023-07-10 14:10:58,316][227910] mean_value=-3076.7903359997254, max_value=92.94078685919521[0m
[37m[1m[2023-07-10 14:10:58,319][227910] New mean coefficients: [[-0.5462979  -1.4196053   0.06417733 -0.62634516  0.49282262]][0m
[37m[1m[2023-07-10 14:10:58,320][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:11:08,094][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 14:11:08,095][227910] FPS: 392914.88[0m
[36m[2023-07-10 14:11:08,097][227910] itr=573, itrs=2000, Progress: 28.65%[0m
[36m[2023-07-10 14:11:19,589][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 14:11:19,589][227910] FPS: 334602.38[0m
[36m[2023-07-10 14:11:24,364][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:11:24,365][227910] Reward + Measures: [[265.2474175    0.23861971   0.35702541   0.30976254   0.31633094]][0m
[37m[1m[2023-07-10 14:11:24,365][227910] Max Reward on eval: 265.2474174975277[0m
[37m[1m[2023-07-10 14:11:24,365][227910] Min Reward on eval: 265.2474174975277[0m
[37m[1m[2023-07-10 14:11:24,366][227910] Mean Reward across all agents: 265.2474174975277[0m
[37m[1m[2023-07-10 14:11:24,366][227910] Average Trajectory Length: 997.4033333333333[0m
[36m[2023-07-10 14:11:29,835][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:11:29,836][227910] Reward + Measures: [[ -275.97109369     0.27405        0.28125        0.3598125
      0.2043625 ]
 [ -600.02900299     0.32702768     0.36403841     0.20775501
      0.26403284]
 [ -399.88050897     0.193716       0.29768655     0.22594896
      0.22775479]
 ...
 [  -10.43530757     0.27809998     0.34619999     0.44889998
      0.22130001]
 [-1333.59381575     0.24602084     0.22237961     0.25160751
      0.21157876]
 [-1522.16826491     0.18976687     0.19324397     0.19500256
      0.2059484 ]][0m
[37m[1m[2023-07-10 14:11:29,836][227910] Max Reward on eval: 503.29008995381594[0m
[37m[1m[2023-07-10 14:11:29,836][227910] Min Reward on eval: -1810.630909838807[0m
[37m[1m[2023-07-10 14:11:29,837][227910] Mean Reward across all agents: -613.9008801096641[0m
[37m[1m[2023-07-10 14:11:29,837][227910] Average Trajectory Length: 827.5256666666667[0m
[36m[2023-07-10 14:11:29,838][227910] mean_value=-2629.9318999448446, max_value=283.34032608606594[0m
[37m[1m[2023-07-10 14:11:29,841][227910] New mean coefficients: [[-0.74276114 -1.1645432   0.34460413 -0.08392084  0.3435376 ]][0m
[37m[1m[2023-07-10 14:11:29,842][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:11:39,636][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 14:11:39,636][227910] FPS: 392145.91[0m
[36m[2023-07-10 14:11:39,638][227910] itr=574, itrs=2000, Progress: 28.70%[0m
[36m[2023-07-10 14:11:51,180][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 14:11:51,181][227910] FPS: 333242.63[0m
[36m[2023-07-10 14:11:55,828][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:11:55,828][227910] Reward + Measures: [[-267.97448348    0.21786776    0.44070092    0.21278873    0.39388692]][0m
[37m[1m[2023-07-10 14:11:55,828][227910] Max Reward on eval: -267.974483481015[0m
[37m[1m[2023-07-10 14:11:55,829][227910] Min Reward on eval: -267.974483481015[0m
[37m[1m[2023-07-10 14:11:55,829][227910] Mean Reward across all agents: -267.974483481015[0m
[37m[1m[2023-07-10 14:11:55,829][227910] Average Trajectory Length: 995.9086666666666[0m
[36m[2023-07-10 14:12:01,358][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:12:01,359][227910] Reward + Measures: [[-219.19941052    0.2095077     0.2689797     0.22521906    0.14566712]
 [-640.48756838    0.2048292     0.30717835    0.2155066     0.24406457]
 [-734.54307124    0.19693576    0.23937431    0.1998807     0.18362866]
 ...
 [-155.51752512    0.22032802    0.32143614    0.27014977    0.24065213]
 [-534.02885054    0.3012        0.4894        0.11390001    0.40960002]
 [-377.82463376    0.18981187    0.29147291    0.2410913     0.21323395]][0m
[37m[1m[2023-07-10 14:12:01,359][227910] Max Reward on eval: 75.44850113670108[0m
[37m[1m[2023-07-10 14:12:01,359][227910] Min Reward on eval: -1560.1283459570375[0m
[37m[1m[2023-07-10 14:12:01,360][227910] Mean Reward across all agents: -467.922704939938[0m
[37m[1m[2023-07-10 14:12:01,360][227910] Average Trajectory Length: 869.2976666666666[0m
[36m[2023-07-10 14:12:01,361][227910] mean_value=-1434.4419918330448, max_value=211.10268872784394[0m
[37m[1m[2023-07-10 14:12:01,364][227910] New mean coefficients: [[ 0.47076225 -1.165562   -0.62881416 -2.009835    0.8990307 ]][0m
[37m[1m[2023-07-10 14:12:01,365][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:12:11,218][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 14:12:11,218][227910] FPS: 389824.55[0m
[36m[2023-07-10 14:12:11,220][227910] itr=575, itrs=2000, Progress: 28.75%[0m
[36m[2023-07-10 14:12:22,812][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 14:12:22,813][227910] FPS: 331723.19[0m
[36m[2023-07-10 14:12:27,588][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:12:27,589][227910] Reward + Measures: [[-309.96314155    0.20091829    0.47955969    0.1763157     0.41307473]][0m
[37m[1m[2023-07-10 14:12:27,589][227910] Max Reward on eval: -309.963141548828[0m
[37m[1m[2023-07-10 14:12:27,589][227910] Min Reward on eval: -309.963141548828[0m
[37m[1m[2023-07-10 14:12:27,590][227910] Mean Reward across all agents: -309.963141548828[0m
[37m[1m[2023-07-10 14:12:27,590][227910] Average Trajectory Length: 989.5653333333333[0m
[36m[2023-07-10 14:12:33,082][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:12:33,082][227910] Reward + Measures: [[-389.09981435    0.26047879    0.45414543    0.17652123    0.36143944]
 [-256.05520366    0.15603332    0.29999167    0.191         0.17045835]
 [-282.27206785    0.20597403    0.30854186    0.24792612    0.22336932]
 ...
 [-314.45291506    0.21514867    0.34314534    0.19917767    0.24970458]
 [-372.44767276    0.22050583    0.33256212    0.26033047    0.24845171]
 [-583.72670638    0.15970002    0.42800003    0.23440002    0.41      ]][0m
[37m[1m[2023-07-10 14:12:33,083][227910] Max Reward on eval: 97.38372166512127[0m
[37m[1m[2023-07-10 14:12:33,083][227910] Min Reward on eval: -1272.6192447586218[0m
[37m[1m[2023-07-10 14:12:33,083][227910] Mean Reward across all agents: -457.25085727014374[0m
[37m[1m[2023-07-10 14:12:33,084][227910] Average Trajectory Length: 901.2846666666667[0m
[36m[2023-07-10 14:12:33,085][227910] mean_value=-1606.2031095733175, max_value=322.5118683466897[0m
[37m[1m[2023-07-10 14:12:33,088][227910] New mean coefficients: [[-0.48756748 -1.7578173   0.29691362 -0.5447129   0.94228995]][0m
[37m[1m[2023-07-10 14:12:33,089][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:12:42,939][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 14:12:42,939][227910] FPS: 389920.64[0m
[36m[2023-07-10 14:12:42,941][227910] itr=576, itrs=2000, Progress: 28.80%[0m
[36m[2023-07-10 14:12:54,673][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 14:12:54,673][227910] FPS: 327862.00[0m
[36m[2023-07-10 14:12:59,525][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:12:59,526][227910] Reward + Measures: [[-550.88947946    0.19946775    0.52471554    0.20929581    0.47944698]][0m
[37m[1m[2023-07-10 14:12:59,526][227910] Max Reward on eval: -550.8894794633128[0m
[37m[1m[2023-07-10 14:12:59,526][227910] Min Reward on eval: -550.8894794633128[0m
[37m[1m[2023-07-10 14:12:59,527][227910] Mean Reward across all agents: -550.8894794633128[0m
[37m[1m[2023-07-10 14:12:59,527][227910] Average Trajectory Length: 998.3753333333333[0m
[36m[2023-07-10 14:13:04,967][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:13:04,968][227910] Reward + Measures: [[-371.46144965    0.20259999    0.54899997    0.2306        0.46779999]
 [-318.33628346    0.22140001    0.47729999    0.29580003    0.41330001]
 [-380.15299133    0.2138        0.51120007    0.2368        0.41529998]
 ...
 [-829.23286653    0.2163        0.54699999    0.16360001    0.54610008]
 [-450.97338817    0.22679999    0.40540001    0.27970001    0.31650004]
 [-505.57315791    0.2062        0.54470003    0.22319999    0.49349999]][0m
[37m[1m[2023-07-10 14:13:04,968][227910] Max Reward on eval: -246.73400678294712[0m
[37m[1m[2023-07-10 14:13:04,968][227910] Min Reward on eval: -1768.6060752943158[0m
[37m[1m[2023-07-10 14:13:04,968][227910] Mean Reward across all agents: -754.4568202109452[0m
[37m[1m[2023-07-10 14:13:04,969][227910] Average Trajectory Length: 982.1356666666667[0m
[36m[2023-07-10 14:13:04,970][227910] mean_value=-1307.3661572246185, max_value=-265.1500404447306[0m
[36m[2023-07-10 14:13:04,972][227910] XNES is restarting with a new solution whose measures are [0.2309     0.28500003 0.60030001 0.0394    ] and objective is 3166.99005632028[0m
[36m[2023-07-10 14:13:04,973][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 14:13:04,976][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 14:13:04,976][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:13:14,745][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 14:13:14,745][227910] FPS: 393170.22[0m
[36m[2023-07-10 14:13:14,747][227910] itr=577, itrs=2000, Progress: 28.85%[0m
[36m[2023-07-10 14:13:26,426][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 14:13:26,426][227910] FPS: 329345.89[0m
[36m[2023-07-10 14:13:31,173][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:13:31,173][227910] Reward + Measures: [[2324.13856483    0.2241009     0.33589229    0.51507306    0.08590081]][0m
[37m[1m[2023-07-10 14:13:31,173][227910] Max Reward on eval: 2324.13856482791[0m
[37m[1m[2023-07-10 14:13:31,174][227910] Min Reward on eval: 2324.13856482791[0m
[37m[1m[2023-07-10 14:13:31,174][227910] Mean Reward across all agents: 2324.13856482791[0m
[37m[1m[2023-07-10 14:13:31,174][227910] Average Trajectory Length: 999.2719999999999[0m
[36m[2023-07-10 14:13:36,538][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:13:36,544][227910] Reward + Measures: [[1988.77228436    0.29180002    0.37639999    0.546         0.0817    ]
 [ 585.21948632    0.50349998    0.21799998    0.56210005    0.17470001]
 [ 113.34308819    0.46479997    0.21780001    0.42810002    0.2122    ]
 ...
 [2068.53416051    0.27040002    0.36540005    0.51490003    0.08579999]
 [ -23.75451011    0.5183        0.1793        0.51510006    0.17349999]
 [ 179.96139504    0.46680003    0.26710001    0.40430003    0.24720001]][0m
[37m[1m[2023-07-10 14:13:36,544][227910] Max Reward on eval: 2509.244870810583[0m
[37m[1m[2023-07-10 14:13:36,544][227910] Min Reward on eval: -805.6875715322792[0m
[37m[1m[2023-07-10 14:13:36,545][227910] Mean Reward across all agents: 959.4720354117826[0m
[37m[1m[2023-07-10 14:13:36,545][227910] Average Trajectory Length: 995.8426666666667[0m
[36m[2023-07-10 14:13:36,548][227910] mean_value=-879.7613404138568, max_value=1675.2791649693168[0m
[37m[1m[2023-07-10 14:13:36,551][227910] New mean coefficients: [[ 2.2952688   0.12974548 -1.0629455  -2.72221    -1.2661914 ]][0m
[37m[1m[2023-07-10 14:13:36,552][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:13:46,169][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 14:13:46,170][227910] FPS: 399326.96[0m
[36m[2023-07-10 14:13:46,172][227910] itr=578, itrs=2000, Progress: 28.90%[0m
[36m[2023-07-10 14:13:57,927][227910] train() took 11.74 seconds to complete[0m
[36m[2023-07-10 14:13:57,928][227910] FPS: 327142.62[0m
[36m[2023-07-10 14:14:02,742][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:14:02,742][227910] Reward + Measures: [[2657.45816991    0.22813061    0.32047021    0.50064903    0.07598618]][0m
[37m[1m[2023-07-10 14:14:02,742][227910] Max Reward on eval: 2657.458169906566[0m
[37m[1m[2023-07-10 14:14:02,742][227910] Min Reward on eval: 2657.458169906566[0m
[37m[1m[2023-07-10 14:14:02,743][227910] Mean Reward across all agents: 2657.458169906566[0m
[37m[1m[2023-07-10 14:14:02,743][227910] Average Trajectory Length: 998.1646666666667[0m
[36m[2023-07-10 14:14:08,275][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:14:08,275][227910] Reward + Measures: [[-825.25515732    0.2656672     0.28894886    0.26815626    0.28767109]
 [-545.96032904    0.22514598    0.27094334    0.23576848    0.15565917]
 [-357.5481477     0.47750002    0.3924        0.43039998    0.27059999]
 ...
 [ 770.22744059    0.23709999    0.42160007    0.50260001    0.16670001]
 [ -93.66503057    0.36264989    0.34456477    0.33842346    0.26525646]
 [  36.72376173    0.44960004    0.3127        0.47639999    0.21250001]][0m
[37m[1m[2023-07-10 14:14:08,276][227910] Max Reward on eval: 2736.7188962519167[0m
[37m[1m[2023-07-10 14:14:08,276][227910] Min Reward on eval: -1342.6981740778313[0m
[37m[1m[2023-07-10 14:14:08,276][227910] Mean Reward across all agents: 84.64613636862906[0m
[37m[1m[2023-07-10 14:14:08,276][227910] Average Trajectory Length: 931.698[0m
[36m[2023-07-10 14:14:08,278][227910] mean_value=-2293.5009390331834, max_value=15.017474912724577[0m
[37m[1m[2023-07-10 14:14:08,280][227910] New mean coefficients: [[ 2.9853368  -0.23420417  0.790815   -1.7143431  -1.0806189 ]][0m
[37m[1m[2023-07-10 14:14:08,281][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:14:17,895][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 14:14:17,896][227910] FPS: 399476.28[0m
[36m[2023-07-10 14:14:17,898][227910] itr=579, itrs=2000, Progress: 28.95%[0m
[36m[2023-07-10 14:14:29,344][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 14:14:29,345][227910] FPS: 335962.92[0m
[36m[2023-07-10 14:14:34,006][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:14:34,007][227910] Reward + Measures: [[2889.3950905     0.22998124    0.31619376    0.48544428    0.07281425]][0m
[37m[1m[2023-07-10 14:14:34,007][227910] Max Reward on eval: 2889.39509050123[0m
[37m[1m[2023-07-10 14:14:34,007][227910] Min Reward on eval: 2889.39509050123[0m
[37m[1m[2023-07-10 14:14:34,007][227910] Mean Reward across all agents: 2889.39509050123[0m
[37m[1m[2023-07-10 14:14:34,008][227910] Average Trajectory Length: 996.4896666666666[0m
[36m[2023-07-10 14:14:39,604][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:14:39,605][227910] Reward + Measures: [[2183.68148592    0.27442697    0.35441849    0.44600162    0.09266769]
 [2322.75184255    0.2199        0.33019999    0.49349999    0.06260001]
 [2589.48904865    0.23          0.39130002    0.48109999    0.08      ]
 ...
 [1425.46295684    0.32980004    0.44989997    0.42570001    0.2383    ]
 [2122.73386582    0.29660001    0.38530001    0.45719996    0.0811    ]
 [2808.46897062    0.22760001    0.3642        0.51730007    0.0892    ]][0m
[37m[1m[2023-07-10 14:14:39,605][227910] Max Reward on eval: 2978.4397494213654[0m
[37m[1m[2023-07-10 14:14:39,605][227910] Min Reward on eval: 766.8329510335461[0m
[37m[1m[2023-07-10 14:14:39,606][227910] Mean Reward across all agents: 2361.849160459525[0m
[37m[1m[2023-07-10 14:14:39,606][227910] Average Trajectory Length: 996.5123333333333[0m
[36m[2023-07-10 14:14:39,607][227910] mean_value=-901.4678303195691, max_value=72.75533778938029[0m
[37m[1m[2023-07-10 14:14:39,609][227910] New mean coefficients: [[ 3.1829066 -0.4101709  2.008058  -0.8349383 -2.0680423]][0m
[37m[1m[2023-07-10 14:14:39,611][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:14:49,213][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 14:14:49,213][227910] FPS: 399959.45[0m
[36m[2023-07-10 14:14:49,216][227910] itr=580, itrs=2000, Progress: 29.00%[0m
[37m[1m[2023-07-10 14:14:52,067][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000560[0m
[36m[2023-07-10 14:15:03,986][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 14:15:03,986][227910] FPS: 329659.42[0m
[36m[2023-07-10 14:15:08,767][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:15:08,767][227910] Reward + Measures: [[1686.58633986    0.22332944    0.40377003    0.49762255    0.19093518]][0m
[37m[1m[2023-07-10 14:15:08,767][227910] Max Reward on eval: 1686.5863398571257[0m
[37m[1m[2023-07-10 14:15:08,768][227910] Min Reward on eval: 1686.5863398571257[0m
[37m[1m[2023-07-10 14:15:08,768][227910] Mean Reward across all agents: 1686.5863398571257[0m
[37m[1m[2023-07-10 14:15:08,768][227910] Average Trajectory Length: 998.7819999999999[0m
[36m[2023-07-10 14:15:14,312][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:15:14,313][227910] Reward + Measures: [[ 305.15678496    0.63479996    0.63670003    0.62460005    0.55360001]
 [1594.90074479    0.2192        0.44220001    0.48830006    0.2045    ]
 [1242.0988039     0.32049999    0.48990002    0.51440001    0.14389999]
 ...
 [1188.21787585    0.28380001    0.36650002    0.40570003    0.19620001]
 [-206.87659764    0.45930004    0.5528        0.43280002    0.41780001]
 [-696.53951675    0.54444069    0.50657672    0.52521783    0.44228908]][0m
[37m[1m[2023-07-10 14:15:14,313][227910] Max Reward on eval: 2340.096993113891[0m
[37m[1m[2023-07-10 14:15:14,313][227910] Min Reward on eval: -1280.5930847098352[0m
[37m[1m[2023-07-10 14:15:14,313][227910] Mean Reward across all agents: 814.6875404335048[0m
[37m[1m[2023-07-10 14:15:14,314][227910] Average Trajectory Length: 994.8256666666666[0m
[36m[2023-07-10 14:15:14,316][227910] mean_value=-798.4335281203984, max_value=1308.0623375691682[0m
[37m[1m[2023-07-10 14:15:14,319][227910] New mean coefficients: [[ 2.8456674  -0.14668325  2.2612567  -0.17766625 -1.3594627 ]][0m
[37m[1m[2023-07-10 14:15:14,320][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:15:24,193][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 14:15:24,194][227910] FPS: 388987.69[0m
[36m[2023-07-10 14:15:24,196][227910] itr=581, itrs=2000, Progress: 29.05%[0m
[36m[2023-07-10 14:15:35,958][227910] train() took 11.74 seconds to complete[0m
[36m[2023-07-10 14:15:35,959][227910] FPS: 326969.63[0m
[36m[2023-07-10 14:15:40,721][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:15:40,721][227910] Reward + Measures: [[2064.93240665    0.2326355     0.3667604     0.49255213    0.14284997]][0m
[37m[1m[2023-07-10 14:15:40,721][227910] Max Reward on eval: 2064.9324066455797[0m
[37m[1m[2023-07-10 14:15:40,722][227910] Min Reward on eval: 2064.9324066455797[0m
[37m[1m[2023-07-10 14:15:40,722][227910] Mean Reward across all agents: 2064.9324066455797[0m
[37m[1m[2023-07-10 14:15:40,722][227910] Average Trajectory Length: 998.3513333333333[0m
[36m[2023-07-10 14:15:46,365][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:15:46,366][227910] Reward + Measures: [[ 1173.3417941      0.26929998     0.40289998     0.44490004
      0.21169999]
 [ -918.56607323     0.32327867     0.45346856     0.25930133
      0.33553907]
 [-1315.38846812     0.39359999     0.5502001      0.32210001
      0.33020002]
 ...
 [ 1891.45180944     0.22160001     0.41249999     0.45309997
      0.0954    ]
 [  824.01381347     0.3468         0.40190002     0.4585
      0.33680001]
 [  276.41910575     0.2959913      0.56192178     0.4498015
      0.37693191]][0m
[37m[1m[2023-07-10 14:15:46,366][227910] Max Reward on eval: 2141.161639292911[0m
[37m[1m[2023-07-10 14:15:46,366][227910] Min Reward on eval: -1669.601161007816[0m
[37m[1m[2023-07-10 14:15:46,366][227910] Mean Reward across all agents: 260.4884124947435[0m
[37m[1m[2023-07-10 14:15:46,367][227910] Average Trajectory Length: 972.5509999999999[0m
[36m[2023-07-10 14:15:46,369][227910] mean_value=-1538.8910983451144, max_value=1667.619740776089[0m
[37m[1m[2023-07-10 14:15:46,371][227910] New mean coefficients: [[ 3.1380515  -0.0865984   1.2194885   0.11744744  0.00576413]][0m
[37m[1m[2023-07-10 14:15:46,372][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:15:56,230][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 14:15:56,230][227910] FPS: 389617.01[0m
[36m[2023-07-10 14:15:56,232][227910] itr=582, itrs=2000, Progress: 29.10%[0m
[36m[2023-07-10 14:16:07,969][227910] train() took 11.72 seconds to complete[0m
[36m[2023-07-10 14:16:07,969][227910] FPS: 327715.28[0m
[36m[2023-07-10 14:16:12,828][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:16:12,828][227910] Reward + Measures: [[2320.70951728    0.23478539    0.34584916    0.48767114    0.11089382]][0m
[37m[1m[2023-07-10 14:16:12,828][227910] Max Reward on eval: 2320.7095172761665[0m
[37m[1m[2023-07-10 14:16:12,829][227910] Min Reward on eval: 2320.7095172761665[0m
[37m[1m[2023-07-10 14:16:12,829][227910] Mean Reward across all agents: 2320.7095172761665[0m
[37m[1m[2023-07-10 14:16:12,829][227910] Average Trajectory Length: 998.7583333333333[0m
[36m[2023-07-10 14:16:18,230][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:16:18,231][227910] Reward + Measures: [[ 433.67836738    0.30340001    0.52410001    0.3008        0.22379999]
 [1136.78950279    0.31020004    0.32729998    0.38480002    0.16410001]
 [ 225.27153723    0.34020004    0.53350002    0.27380002    0.2538    ]
 ...
 [1678.12397255    0.25350001    0.38470003    0.55310005    0.1622    ]
 [1014.20760798    0.32769999    0.49790001    0.39710003    0.16440001]
 [1891.73121404    0.2375        0.37550002    0.56139994    0.1524    ]][0m
[37m[1m[2023-07-10 14:16:18,231][227910] Max Reward on eval: 2522.52519809911[0m
[37m[1m[2023-07-10 14:16:18,231][227910] Min Reward on eval: -969.7920071668807[0m
[37m[1m[2023-07-10 14:16:18,231][227910] Mean Reward across all agents: 1209.191145399515[0m
[37m[1m[2023-07-10 14:16:18,232][227910] Average Trajectory Length: 993.636[0m
[36m[2023-07-10 14:16:18,233][227910] mean_value=-1440.5196349461537, max_value=1466.1156091179746[0m
[37m[1m[2023-07-10 14:16:18,236][227910] New mean coefficients: [[ 3.6981578   0.7848608   1.1640896  -0.2476508   0.04149842]][0m
[37m[1m[2023-07-10 14:16:18,237][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:16:27,889][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 14:16:27,889][227910] FPS: 397908.14[0m
[36m[2023-07-10 14:16:27,891][227910] itr=583, itrs=2000, Progress: 29.15%[0m
[36m[2023-07-10 14:16:39,484][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 14:16:39,484][227910] FPS: 331780.06[0m
[36m[2023-07-10 14:16:44,308][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:16:44,308][227910] Reward + Measures: [[2706.10834037    0.23581918    0.35861564    0.4646852     0.06227111]][0m
[37m[1m[2023-07-10 14:16:44,308][227910] Max Reward on eval: 2706.1083403656808[0m
[37m[1m[2023-07-10 14:16:44,309][227910] Min Reward on eval: 2706.1083403656808[0m
[37m[1m[2023-07-10 14:16:44,309][227910] Mean Reward across all agents: 2706.1083403656808[0m
[37m[1m[2023-07-10 14:16:44,309][227910] Average Trajectory Length: 996.8616666666667[0m
[36m[2023-07-10 14:16:49,799][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:16:49,799][227910] Reward + Measures: [[-495.36008083    0.4241755     0.27338561    0.40817666    0.19997196]
 [2656.3901511     0.2597        0.36669999    0.46159998    0.03400001]
 [1827.96803616    0.28929999    0.36399999    0.51879996    0.12620001]
 ...
 [-230.13339595    0.49639997    0.28099999    0.47049999    0.18730001]
 [ 526.35754121    0.33570001    0.45180002    0.48979998    0.2447    ]
 [ 493.16646736    0.46040002    0.32119998    0.5           0.2651    ]][0m
[37m[1m[2023-07-10 14:16:49,800][227910] Max Reward on eval: 2781.9634615202435[0m
[37m[1m[2023-07-10 14:16:49,800][227910] Min Reward on eval: -1049.2691382865655[0m
[37m[1m[2023-07-10 14:16:49,800][227910] Mean Reward across all agents: 995.3377565934168[0m
[37m[1m[2023-07-10 14:16:49,800][227910] Average Trajectory Length: 991.6403333333333[0m
[36m[2023-07-10 14:16:49,802][227910] mean_value=-1545.8334079045248, max_value=1461.4383795664621[0m
[37m[1m[2023-07-10 14:16:49,805][227910] New mean coefficients: [[ 3.8324707   1.1557784   0.08224046  0.39137548 -0.12722337]][0m
[37m[1m[2023-07-10 14:16:49,806][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:16:59,540][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 14:16:59,541][227910] FPS: 394524.14[0m
[36m[2023-07-10 14:16:59,543][227910] itr=584, itrs=2000, Progress: 29.20%[0m
[36m[2023-07-10 14:17:11,303][227910] train() took 11.74 seconds to complete[0m
[36m[2023-07-10 14:17:11,303][227910] FPS: 327019.82[0m
[36m[2023-07-10 14:17:16,139][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:17:16,140][227910] Reward + Measures: [[2896.88668752    0.23358455    0.34976661    0.46691275    0.05022882]][0m
[37m[1m[2023-07-10 14:17:16,140][227910] Max Reward on eval: 2896.886687521717[0m
[37m[1m[2023-07-10 14:17:16,140][227910] Min Reward on eval: 2896.886687521717[0m
[37m[1m[2023-07-10 14:17:16,140][227910] Mean Reward across all agents: 2896.886687521717[0m
[37m[1m[2023-07-10 14:17:16,140][227910] Average Trajectory Length: 996.5166666666667[0m
[36m[2023-07-10 14:17:21,681][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:17:21,681][227910] Reward + Measures: [[2082.8308226     0.27159998    0.50209999    0.38159999    0.09190001]
 [ 675.85291488    0.32960001    0.54520005    0.39050001    0.31990001]
 [1916.57527433    0.25310001    0.45649996    0.43610001    0.10680001]
 ...
 [1356.27876182    0.26050001    0.5571        0.51660007    0.1964    ]
 [1232.61065268    0.25920001    0.29910001    0.36320001    0.13689999]
 [  74.3514183     0.39359999    0.57280004    0.23550001    0.4657    ]][0m
[37m[1m[2023-07-10 14:17:21,682][227910] Max Reward on eval: 2838.4160300014714[0m
[37m[1m[2023-07-10 14:17:21,682][227910] Min Reward on eval: -222.5973882057471[0m
[37m[1m[2023-07-10 14:17:21,682][227910] Mean Reward across all agents: 1564.4695181710094[0m
[37m[1m[2023-07-10 14:17:21,682][227910] Average Trajectory Length: 996.5783333333333[0m
[36m[2023-07-10 14:17:21,685][227910] mean_value=-707.8126093478139, max_value=1964.06133268854[0m
[37m[1m[2023-07-10 14:17:21,687][227910] New mean coefficients: [[4.047924   1.2511637  1.1348673  1.0196166  0.40393728]][0m
[37m[1m[2023-07-10 14:17:21,688][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:17:31,349][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 14:17:31,350][227910] FPS: 397551.67[0m
[36m[2023-07-10 14:17:31,352][227910] itr=585, itrs=2000, Progress: 29.25%[0m
[36m[2023-07-10 14:17:42,927][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 14:17:42,928][227910] FPS: 332190.24[0m
[36m[2023-07-10 14:17:47,694][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:17:47,694][227910] Reward + Measures: [[3083.52218018    0.22943644    0.34723571    0.46872598    0.0378124 ]][0m
[37m[1m[2023-07-10 14:17:47,695][227910] Max Reward on eval: 3083.5221801789216[0m
[37m[1m[2023-07-10 14:17:47,695][227910] Min Reward on eval: 3083.5221801789216[0m
[37m[1m[2023-07-10 14:17:47,695][227910] Mean Reward across all agents: 3083.5221801789216[0m
[37m[1m[2023-07-10 14:17:47,695][227910] Average Trajectory Length: 997.8639999999999[0m
[36m[2023-07-10 14:17:53,109][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:17:53,109][227910] Reward + Measures: [[1918.51450069    0.23510002    0.37240002    0.51050001    0.0587    ]
 [1703.96103983    0.33469996    0.41089997    0.38460001    0.1098    ]
 [2568.96604007    0.22040001    0.35730001    0.48920003    0.0462    ]
 ...
 [2199.72933025    0.23910001    0.35120001    0.52890009    0.056     ]
 [2394.32898352    0.20899999    0.35470003    0.4844        0.0478    ]
 [1349.13902592    0.23940001    0.38960001    0.53330004    0.06900001]][0m
[37m[1m[2023-07-10 14:17:53,110][227910] Max Reward on eval: 3063.652625793498[0m
[37m[1m[2023-07-10 14:17:53,110][227910] Min Reward on eval: -322.2610329616931[0m
[37m[1m[2023-07-10 14:17:53,110][227910] Mean Reward across all agents: 1774.1645394678153[0m
[37m[1m[2023-07-10 14:17:53,110][227910] Average Trajectory Length: 996.212[0m
[36m[2023-07-10 14:17:53,112][227910] mean_value=-1021.1845303581046, max_value=1260.6744806700428[0m
[37m[1m[2023-07-10 14:17:53,115][227910] New mean coefficients: [[ 4.6840353   0.9038992  -0.080006    0.54890966  1.0962024 ]][0m
[37m[1m[2023-07-10 14:17:53,116][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:18:02,847][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 14:18:02,848][227910] FPS: 394651.02[0m
[36m[2023-07-10 14:18:02,850][227910] itr=586, itrs=2000, Progress: 29.30%[0m
[36m[2023-07-10 14:18:14,367][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 14:18:14,367][227910] FPS: 333933.04[0m
[36m[2023-07-10 14:18:19,186][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:18:19,187][227910] Reward + Measures: [[3249.20785831    0.22966768    0.35294506    0.46242064    0.02934242]][0m
[37m[1m[2023-07-10 14:18:19,187][227910] Max Reward on eval: 3249.207858312875[0m
[37m[1m[2023-07-10 14:18:19,187][227910] Min Reward on eval: 3249.207858312875[0m
[37m[1m[2023-07-10 14:18:19,188][227910] Mean Reward across all agents: 3249.207858312875[0m
[37m[1m[2023-07-10 14:18:19,188][227910] Average Trajectory Length: 996.413[0m
[36m[2023-07-10 14:18:24,755][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:18:24,755][227910] Reward + Measures: [[  339.89125781     0.1589897      0.36015576     0.27115583
      0.22285584]
 [  -15.32848129     0.21240306     0.36202726     0.20657173
      0.13878183]
 [ 1916.28976871     0.2543         0.42030001     0.41499996
      0.11830001]
 ...
 [  243.01421125     0.26460001     0.40559998     0.33220002
      0.2084    ]
 [ -861.64920735     0.15364204     0.22686796     0.09314058
      0.17762819]
 [-1727.19026663     0.73110002     0.6189         0.71139997
      0.0948    ]][0m
[37m[1m[2023-07-10 14:18:24,756][227910] Max Reward on eval: 2939.7956669497303[0m
[37m[1m[2023-07-10 14:18:24,756][227910] Min Reward on eval: -1727.190266634617[0m
[37m[1m[2023-07-10 14:18:24,756][227910] Mean Reward across all agents: 136.69982108505775[0m
[37m[1m[2023-07-10 14:18:24,756][227910] Average Trajectory Length: 946.4493333333334[0m
[36m[2023-07-10 14:18:24,759][227910] mean_value=-1681.9813888633532, max_value=589.8039213519153[0m
[37m[1m[2023-07-10 14:18:24,761][227910] New mean coefficients: [[ 4.3031936   0.6333909  -0.11653514  1.1176636   0.8377706 ]][0m
[37m[1m[2023-07-10 14:18:24,762][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:18:34,538][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 14:18:34,538][227910] FPS: 392881.15[0m
[36m[2023-07-10 14:18:34,540][227910] itr=587, itrs=2000, Progress: 29.35%[0m
[36m[2023-07-10 14:18:46,076][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 14:18:46,077][227910] FPS: 333334.54[0m
[36m[2023-07-10 14:18:50,922][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:18:50,922][227910] Reward + Measures: [[3387.75851155    0.2254734     0.36034486    0.47610888    0.02519022]][0m
[37m[1m[2023-07-10 14:18:50,923][227910] Max Reward on eval: 3387.7585115533484[0m
[37m[1m[2023-07-10 14:18:50,923][227910] Min Reward on eval: 3387.7585115533484[0m
[37m[1m[2023-07-10 14:18:50,923][227910] Mean Reward across all agents: 3387.7585115533484[0m
[37m[1m[2023-07-10 14:18:50,923][227910] Average Trajectory Length: 999.512[0m
[36m[2023-07-10 14:18:56,361][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:18:56,361][227910] Reward + Measures: [[ 790.57290383    0.2771        0.39660001    0.46180001    0.2271    ]
 [-531.64205632    0.22642401    0.32355875    0.30183718    0.25927436]
 [ -43.89567591    0.18709999    0.35810003    0.31150001    0.17130001]
 ...
 [ 228.8781313     0.55650008    0.56980008    0.3256        0.41289997]
 [ 564.62532974    0.373         0.25920001    0.49330002    0.29290003]
 [-637.74207724    0.27892372    0.34096137    0.31126201    0.28933391]][0m
[37m[1m[2023-07-10 14:18:56,361][227910] Max Reward on eval: 3095.4609846329317[0m
[37m[1m[2023-07-10 14:18:56,362][227910] Min Reward on eval: -1275.6649777704151[0m
[37m[1m[2023-07-10 14:18:56,362][227910] Mean Reward across all agents: 949.8180881562666[0m
[37m[1m[2023-07-10 14:18:56,362][227910] Average Trajectory Length: 984.2679999999999[0m
[36m[2023-07-10 14:18:56,364][227910] mean_value=-1287.4749465359057, max_value=1001.0284663092245[0m
[37m[1m[2023-07-10 14:18:56,367][227910] New mean coefficients: [[ 4.019569  -0.1487518  1.2215832  1.1256475  1.1388946]][0m
[37m[1m[2023-07-10 14:18:56,368][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:19:06,016][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 14:19:06,016][227910] FPS: 398065.03[0m
[36m[2023-07-10 14:19:06,019][227910] itr=588, itrs=2000, Progress: 29.40%[0m
[36m[2023-07-10 14:19:17,546][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 14:19:17,546][227910] FPS: 333609.82[0m
[36m[2023-07-10 14:19:22,319][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:19:22,319][227910] Reward + Measures: [[3519.98794219    0.22113039    0.35992602    0.4828735     0.02212126]][0m
[37m[1m[2023-07-10 14:19:22,320][227910] Max Reward on eval: 3519.9879421856704[0m
[37m[1m[2023-07-10 14:19:22,320][227910] Min Reward on eval: 3519.9879421856704[0m
[37m[1m[2023-07-10 14:19:22,320][227910] Mean Reward across all agents: 3519.9879421856704[0m
[37m[1m[2023-07-10 14:19:22,320][227910] Average Trajectory Length: 998.3673333333332[0m
[36m[2023-07-10 14:19:27,840][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:19:27,841][227910] Reward + Measures: [[ 1924.14720767     0.21500002     0.40760002     0.54100001
      0.206     ]
 [-1090.71793887     0.2155         0.1344         0.24090002
      0.26270002]
 [ -363.71378909     0.37056962     0.46540251     0.32130381
      0.36226711]
 ...
 [ -438.42657888     0.32468173     0.39245376     0.20240323
      0.42489138]
 [-1321.17688768     0.69959998     0.70230001     0.68639994
      0.69849998]
 [  287.56450221     0.50980002     0.46690002     0.3752
      0.59729999]][0m
[37m[1m[2023-07-10 14:19:27,841][227910] Max Reward on eval: 2678.5821852674244[0m
[37m[1m[2023-07-10 14:19:27,841][227910] Min Reward on eval: -2141.708180538134[0m
[37m[1m[2023-07-10 14:19:27,842][227910] Mean Reward across all agents: -150.41384951640902[0m
[37m[1m[2023-07-10 14:19:27,842][227910] Average Trajectory Length: 962.7386666666666[0m
[36m[2023-07-10 14:19:27,844][227910] mean_value=-1711.1161827402466, max_value=1867.3347412979083[0m
[37m[1m[2023-07-10 14:19:27,847][227910] New mean coefficients: [[ 4.117726   -0.21372996  0.97704476  0.6351768   1.480221  ]][0m
[37m[1m[2023-07-10 14:19:27,848][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:19:37,653][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 14:19:37,653][227910] FPS: 391697.50[0m
[36m[2023-07-10 14:19:37,656][227910] itr=589, itrs=2000, Progress: 29.45%[0m
[36m[2023-07-10 14:19:49,383][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 14:19:49,383][227910] FPS: 327931.97[0m
[36m[2023-07-10 14:19:54,064][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:19:54,065][227910] Reward + Measures: [[3652.51870997    0.21768847    0.35687       0.48685318    0.02036534]][0m
[37m[1m[2023-07-10 14:19:54,065][227910] Max Reward on eval: 3652.5187099654177[0m
[37m[1m[2023-07-10 14:19:54,066][227910] Min Reward on eval: 3652.5187099654177[0m
[37m[1m[2023-07-10 14:19:54,066][227910] Mean Reward across all agents: 3652.5187099654177[0m
[37m[1m[2023-07-10 14:19:54,066][227910] Average Trajectory Length: 997.2053333333333[0m
[36m[2023-07-10 14:19:59,554][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:19:59,554][227910] Reward + Measures: [[   3.55589484    0.44600001    0.3371        0.36039999    0.28580001]
 [2754.56150876    0.21560001    0.41440001    0.37990001    0.0521    ]
 [ 638.45486951    0.38930002    0.45310003    0.52389997    0.24390002]
 ...
 [2094.86301173    0.21259999    0.50310004    0.46949998    0.0961    ]
 [  93.07612651    0.35380003    0.3556        0.4052        0.1595    ]
 [-277.59661997    0.50349998    0.38240001    0.41940004    0.30780002]][0m
[37m[1m[2023-07-10 14:19:59,555][227910] Max Reward on eval: 3611.8344801269473[0m
[37m[1m[2023-07-10 14:19:59,555][227910] Min Reward on eval: -629.67475016393[0m
[37m[1m[2023-07-10 14:19:59,555][227910] Mean Reward across all agents: 1224.9908159042402[0m
[37m[1m[2023-07-10 14:19:59,555][227910] Average Trajectory Length: 996.0899999999999[0m
[36m[2023-07-10 14:19:59,557][227910] mean_value=-1365.1989621636158, max_value=2196.2792358135803[0m
[37m[1m[2023-07-10 14:19:59,560][227910] New mean coefficients: [[4.446269   0.14912198 0.17016834 0.962205   1.4771432 ]][0m
[37m[1m[2023-07-10 14:19:59,561][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:20:09,282][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 14:20:09,282][227910] FPS: 395080.90[0m
[36m[2023-07-10 14:20:09,285][227910] itr=590, itrs=2000, Progress: 29.50%[0m
[37m[1m[2023-07-10 14:20:12,318][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000570[0m
[36m[2023-07-10 14:20:24,038][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 14:20:24,039][227910] FPS: 334943.91[0m
[36m[2023-07-10 14:20:28,603][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:20:28,603][227910] Reward + Measures: [[3763.28742131    0.21522819    0.35139737    0.52455825    0.02262356]][0m
[37m[1m[2023-07-10 14:20:28,603][227910] Max Reward on eval: 3763.287421310129[0m
[37m[1m[2023-07-10 14:20:28,604][227910] Min Reward on eval: 3763.287421310129[0m
[37m[1m[2023-07-10 14:20:28,604][227910] Mean Reward across all agents: 3763.287421310129[0m
[37m[1m[2023-07-10 14:20:28,604][227910] Average Trajectory Length: 997.6426666666666[0m
[36m[2023-07-10 14:20:34,096][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:20:34,129][227910] Reward + Measures: [[ 164.97361409    0.37649998    0.38120002    0.2633        0.1778    ]
 [-569.52204799    0.25646421    0.29894862    0.29615471    0.17564772]
 [-397.38310474    0.1986054     0.2987673     0.22125934    0.19780396]
 ...
 [-425.16017875    0.24070732    0.40697297    0.32238561    0.24703307]
 [1312.19240844    0.21680002    0.36030003    0.52890003    0.15970002]
 [ 164.7357879     0.2332        0.36139998    0.30159998    0.19569999]][0m
[37m[1m[2023-07-10 14:20:34,130][227910] Max Reward on eval: 3016.9943922690118[0m
[37m[1m[2023-07-10 14:20:34,130][227910] Min Reward on eval: -1165.2438973895391[0m
[37m[1m[2023-07-10 14:20:34,130][227910] Mean Reward across all agents: 136.1261519892968[0m
[37m[1m[2023-07-10 14:20:34,130][227910] Average Trajectory Length: 867.0989999999999[0m
[36m[2023-07-10 14:20:34,132][227910] mean_value=-2245.5066141380426, max_value=859.5804716285202[0m
[37m[1m[2023-07-10 14:20:34,135][227910] New mean coefficients: [[4.2247567  0.9729482  0.23376231 0.30581158 0.51321137]][0m
[37m[1m[2023-07-10 14:20:34,136][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:20:43,738][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 14:20:43,739][227910] FPS: 399953.20[0m
[36m[2023-07-10 14:20:43,741][227910] itr=591, itrs=2000, Progress: 29.55%[0m
[36m[2023-07-10 14:20:55,306][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 14:20:55,307][227910] FPS: 332489.86[0m
[36m[2023-07-10 14:21:00,142][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:21:00,143][227910] Reward + Measures: [[3886.28105475    0.21580483    0.36153099    0.52770776    0.02488032]][0m
[37m[1m[2023-07-10 14:21:00,143][227910] Max Reward on eval: 3886.2810547487775[0m
[37m[1m[2023-07-10 14:21:00,143][227910] Min Reward on eval: 3886.2810547487775[0m
[37m[1m[2023-07-10 14:21:00,143][227910] Mean Reward across all agents: 3886.2810547487775[0m
[37m[1m[2023-07-10 14:21:00,143][227910] Average Trajectory Length: 998.483[0m
[36m[2023-07-10 14:21:05,553][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:21:05,553][227910] Reward + Measures: [[-214.03991468    0.71160001    0.77569997    0.0535        0.68879998]
 [ 204.24748202    0.46300003    0.62250006    0.1232        0.45029998]
 [2688.85633134    0.21710001    0.39780003    0.48719999    0.0571    ]
 ...
 [-747.2753862     0.18993333    0.31036669    0.30736667    0.13496666]
 [ 198.0745695     0.20290001    0.39420003    0.38729998    0.17749999]
 [ 203.04872844    0.45510003    0.52980006    0.12630001    0.4075    ]][0m
[37m[1m[2023-07-10 14:21:05,553][227910] Max Reward on eval: 3643.5363584578035[0m
[37m[1m[2023-07-10 14:21:05,554][227910] Min Reward on eval: -1118.5980440268293[0m
[37m[1m[2023-07-10 14:21:05,554][227910] Mean Reward across all agents: 743.4260066618788[0m
[37m[1m[2023-07-10 14:21:05,554][227910] Average Trajectory Length: 976.6763333333333[0m
[36m[2023-07-10 14:21:05,556][227910] mean_value=-1647.2630168452717, max_value=1169.2103758107455[0m
[37m[1m[2023-07-10 14:21:05,559][227910] New mean coefficients: [[ 3.968208    0.5467634   1.4476463  -0.44593382  0.3118643 ]][0m
[37m[1m[2023-07-10 14:21:05,560][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:21:15,240][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 14:21:15,241][227910] FPS: 396757.58[0m
[36m[2023-07-10 14:21:15,243][227910] itr=592, itrs=2000, Progress: 29.60%[0m
[36m[2023-07-10 14:21:26,863][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 14:21:26,863][227910] FPS: 330972.25[0m
[36m[2023-07-10 14:21:31,683][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:21:31,683][227910] Reward + Measures: [[628.06244299   0.41294855   0.46970844   0.4992145    0.45679289]][0m
[37m[1m[2023-07-10 14:21:31,684][227910] Max Reward on eval: 628.0624429912099[0m
[37m[1m[2023-07-10 14:21:31,684][227910] Min Reward on eval: 628.0624429912099[0m
[37m[1m[2023-07-10 14:21:31,684][227910] Mean Reward across all agents: 628.0624429912099[0m
[37m[1m[2023-07-10 14:21:31,684][227910] Average Trajectory Length: 999.4466666666666[0m
[36m[2023-07-10 14:21:37,130][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:21:37,131][227910] Reward + Measures: [[-214.98097888    0.0331        0.81870002    0.4082        0.75279999]
 [ -24.86520108    0.2667968     0.40984115    0.31329617    0.30033132]
 [-753.00506039    0.25757191    0.32885501    0.28734747    0.24967027]
 ...
 [-134.03718687    0.30540001    0.48950002    0.45050001    0.42989999]
 [ 274.10716848    0.398         0.34330001    0.57080001    0.26799998]
 [ 501.3166658     0.36580002    0.67000002    0.26120001    0.55320001]][0m
[37m[1m[2023-07-10 14:21:37,131][227910] Max Reward on eval: 851.2393129554839[0m
[37m[1m[2023-07-10 14:21:37,131][227910] Min Reward on eval: -753.0050603857846[0m
[37m[1m[2023-07-10 14:21:37,131][227910] Mean Reward across all agents: 196.44236128326008[0m
[37m[1m[2023-07-10 14:21:37,132][227910] Average Trajectory Length: 950.9006666666667[0m
[36m[2023-07-10 14:21:37,135][227910] mean_value=-1070.3486221086234, max_value=993.0025618731104[0m
[37m[1m[2023-07-10 14:21:37,138][227910] New mean coefficients: [[ 3.5064888  -1.2016689   1.5011721   0.16355824  0.9549227 ]][0m
[37m[1m[2023-07-10 14:21:37,139][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:21:46,920][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 14:21:46,920][227910] FPS: 392675.65[0m
[36m[2023-07-10 14:21:46,922][227910] itr=593, itrs=2000, Progress: 29.65%[0m
[36m[2023-07-10 14:21:58,518][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 14:21:58,518][227910] FPS: 331681.67[0m
[36m[2023-07-10 14:22:03,241][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:22:03,241][227910] Reward + Measures: [[811.67491015   0.31435665   0.55715567   0.46615332   0.39517167]][0m
[37m[1m[2023-07-10 14:22:03,242][227910] Max Reward on eval: 811.6749101513084[0m
[37m[1m[2023-07-10 14:22:03,242][227910] Min Reward on eval: 811.6749101513084[0m
[37m[1m[2023-07-10 14:22:03,242][227910] Mean Reward across all agents: 811.6749101513084[0m
[37m[1m[2023-07-10 14:22:03,242][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:22:08,685][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:22:08,686][227910] Reward + Measures: [[ 988.48183819    0.30810004    0.59190005    0.44309998    0.3272    ]
 [ 742.55938061    0.34800002    0.62230003    0.24879999    0.4443    ]
 [ 754.50435119    0.2588        0.57489997    0.50389999    0.37029997]
 ...
 [1011.81617921    0.28510001    0.54969996    0.4522        0.29099998]
 [ 850.14439231    0.26339999    0.49169999    0.46240002    0.26589999]
 [ 739.38393412    0.21820001    0.55779999    0.50089997    0.36400005]][0m
[37m[1m[2023-07-10 14:22:08,686][227910] Max Reward on eval: 1188.2787252411713[0m
[37m[1m[2023-07-10 14:22:08,686][227910] Min Reward on eval: -387.26518512439213[0m
[37m[1m[2023-07-10 14:22:08,687][227910] Mean Reward across all agents: 791.9164069512599[0m
[37m[1m[2023-07-10 14:22:08,687][227910] Average Trajectory Length: 997.1673333333333[0m
[36m[2023-07-10 14:22:08,691][227910] mean_value=-218.1432632571198, max_value=1384.7243733571609[0m
[37m[1m[2023-07-10 14:22:08,694][227910] New mean coefficients: [[ 3.4121401 -1.1066626  0.4076351  1.7803551  1.3460723]][0m
[37m[1m[2023-07-10 14:22:08,695][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:22:18,531][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 14:22:18,531][227910] FPS: 390456.14[0m
[36m[2023-07-10 14:22:18,534][227910] itr=594, itrs=2000, Progress: 29.70%[0m
[36m[2023-07-10 14:22:30,015][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 14:22:30,015][227910] FPS: 335052.37[0m
[36m[2023-07-10 14:22:34,730][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:22:34,730][227910] Reward + Measures: [[995.28796859   0.26965266   0.58916831   0.47991198   0.34910336]][0m
[37m[1m[2023-07-10 14:22:34,730][227910] Max Reward on eval: 995.2879685885965[0m
[37m[1m[2023-07-10 14:22:34,731][227910] Min Reward on eval: 995.2879685885965[0m
[37m[1m[2023-07-10 14:22:34,731][227910] Mean Reward across all agents: 995.2879685885965[0m
[37m[1m[2023-07-10 14:22:34,731][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:22:40,335][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:22:40,336][227910] Reward + Measures: [[ 623.86055443    0.30700001    0.54450005    0.63660002    0.15790002]
 [ 980.64155028    0.28920004    0.53640002    0.57650006    0.2167    ]
 [ 814.49066934    0.25349998    0.60760003    0.40310001    0.38589999]
 ...
 [ 818.02114256    0.23800002    0.46520001    0.53909999    0.1965    ]
 [-109.66788168    0.3263        0.52999997    0.7191        0.0782    ]
 [ 915.25825498    0.27430001    0.62880003    0.54610008    0.37939999]][0m
[37m[1m[2023-07-10 14:22:40,336][227910] Max Reward on eval: 1117.8105901228496[0m
[37m[1m[2023-07-10 14:22:40,336][227910] Min Reward on eval: -1078.7240016974743[0m
[37m[1m[2023-07-10 14:22:40,337][227910] Mean Reward across all agents: 641.7661331548529[0m
[37m[1m[2023-07-10 14:22:40,337][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:22:40,341][227910] mean_value=-93.62823782343277, max_value=1446.68395841019[0m
[37m[1m[2023-07-10 14:22:40,344][227910] New mean coefficients: [[ 3.3723207  -0.9911142   0.85635364  2.1584818   2.283615  ]][0m
[37m[1m[2023-07-10 14:22:40,345][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:22:49,963][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 14:22:49,964][227910] FPS: 399303.72[0m
[36m[2023-07-10 14:22:49,966][227910] itr=595, itrs=2000, Progress: 29.75%[0m
[36m[2023-07-10 14:23:01,526][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 14:23:01,527][227910] FPS: 332737.86[0m
[36m[2023-07-10 14:23:06,341][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:23:06,341][227910] Reward + Measures: [[1096.19141914    0.26982221    0.58441877    0.4877038     0.34621507]][0m
[37m[1m[2023-07-10 14:23:06,341][227910] Max Reward on eval: 1096.1914191360024[0m
[37m[1m[2023-07-10 14:23:06,342][227910] Min Reward on eval: 1096.1914191360024[0m
[37m[1m[2023-07-10 14:23:06,342][227910] Mean Reward across all agents: 1096.1914191360024[0m
[37m[1m[2023-07-10 14:23:06,342][227910] Average Trajectory Length: 999.6853333333333[0m
[36m[2023-07-10 14:23:11,836][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:23:11,837][227910] Reward + Measures: [[-135.42789107    0.49610001    0.42899999    0.49580002    0.12050001]
 [1206.94087254    0.30039999    0.60180002    0.54449999    0.31430003]
 [ 385.74422513    0.36787853    0.42693144    0.49896279    0.29525539]
 ...
 [ 801.88652491    0.28100002    0.49630004    0.41980001    0.32010001]
 [ 198.40650383    0.37349999    0.3994        0.40000001    0.45879999]
 [1114.55109123    0.22290002    0.57260001    0.44780001    0.27250001]][0m
[37m[1m[2023-07-10 14:23:11,837][227910] Max Reward on eval: 1339.7266958527034[0m
[37m[1m[2023-07-10 14:23:11,837][227910] Min Reward on eval: -1594.4944465290057[0m
[37m[1m[2023-07-10 14:23:11,838][227910] Mean Reward across all agents: 35.65708860956093[0m
[37m[1m[2023-07-10 14:23:11,838][227910] Average Trajectory Length: 971.9776666666667[0m
[36m[2023-07-10 14:23:11,841][227910] mean_value=-1581.0148040660472, max_value=850.5719122344046[0m
[37m[1m[2023-07-10 14:23:11,843][227910] New mean coefficients: [[ 2.6979291  -0.68043035  0.809423    2.406822    0.12918973]][0m
[37m[1m[2023-07-10 14:23:11,844][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:23:21,498][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 14:23:21,498][227910] FPS: 397853.84[0m
[36m[2023-07-10 14:23:21,500][227910] itr=596, itrs=2000, Progress: 29.80%[0m
[36m[2023-07-10 14:23:33,036][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 14:23:33,036][227910] FPS: 333387.17[0m
[36m[2023-07-10 14:23:37,733][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:23:37,733][227910] Reward + Measures: [[1254.74358684    0.25910023    0.57162607    0.50758022    0.29964209]][0m
[37m[1m[2023-07-10 14:23:37,733][227910] Max Reward on eval: 1254.7435868389837[0m
[37m[1m[2023-07-10 14:23:37,733][227910] Min Reward on eval: 1254.7435868389837[0m
[37m[1m[2023-07-10 14:23:37,733][227910] Mean Reward across all agents: 1254.7435868389837[0m
[37m[1m[2023-07-10 14:23:37,734][227910] Average Trajectory Length: 999.4486666666667[0m
[36m[2023-07-10 14:23:43,165][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:23:43,166][227910] Reward + Measures: [[907.70227196   0.23840001   0.55920005   0.51719999   0.39120004]
 [848.93453804   0.26740003   0.61860001   0.38620004   0.45580003]
 [-33.53549701   0.32464343   0.43839392   0.26430508   0.33193231]
 ...
 [279.67076407   0.20039999   0.60429996   0.33199999   0.53150004]
 [467.74392749   0.44369999   0.66460001   0.27270004   0.61269999]
 [994.31424795   0.30560002   0.56590003   0.59689999   0.38690001]][0m
[37m[1m[2023-07-10 14:23:43,166][227910] Max Reward on eval: 1433.4466383376682[0m
[37m[1m[2023-07-10 14:23:43,166][227910] Min Reward on eval: -414.28223730733737[0m
[37m[1m[2023-07-10 14:23:43,166][227910] Mean Reward across all agents: 688.2680789931273[0m
[37m[1m[2023-07-10 14:23:43,167][227910] Average Trajectory Length: 990.7263333333333[0m
[36m[2023-07-10 14:23:43,172][227910] mean_value=100.82625185361894, max_value=1601.2106940938625[0m
[37m[1m[2023-07-10 14:23:43,175][227910] New mean coefficients: [[ 2.8454428  -1.6035473   0.25518352  3.6898956   0.9447794 ]][0m
[37m[1m[2023-07-10 14:23:43,176][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:23:52,806][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 14:23:52,807][227910] FPS: 398794.76[0m
[36m[2023-07-10 14:23:52,809][227910] itr=597, itrs=2000, Progress: 29.85%[0m
[36m[2023-07-10 14:24:04,273][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 14:24:04,274][227910] FPS: 335426.08[0m
[36m[2023-07-10 14:24:09,044][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:24:09,044][227910] Reward + Measures: [[1390.53094244    0.25102028    0.5493204     0.52705246    0.26355127]][0m
[37m[1m[2023-07-10 14:24:09,044][227910] Max Reward on eval: 1390.5309424372506[0m
[37m[1m[2023-07-10 14:24:09,045][227910] Min Reward on eval: 1390.5309424372506[0m
[37m[1m[2023-07-10 14:24:09,045][227910] Mean Reward across all agents: 1390.5309424372506[0m
[37m[1m[2023-07-10 14:24:09,045][227910] Average Trajectory Length: 999.5226666666666[0m
[36m[2023-07-10 14:24:14,514][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:24:14,515][227910] Reward + Measures: [[1290.62167831    0.31329998    0.57120001    0.51459998    0.31140003]
 [ 803.43366849    0.36960003    0.60260004    0.43200001    0.29360002]
 [ 420.21722494    0.33899999    0.5539        0.44520003    0.38700002]
 ...
 [-766.01476684    0.2712        0.27919999    0.4104        0.30130002]
 [ 580.75041592    0.3457        0.62179995    0.39580002    0.3425    ]
 [1077.29288793    0.2775        0.61700004    0.47040001    0.31779999]][0m
[37m[1m[2023-07-10 14:24:14,515][227910] Max Reward on eval: 1440.9435516098863[0m
[37m[1m[2023-07-10 14:24:14,515][227910] Min Reward on eval: -766.0147668435645[0m
[37m[1m[2023-07-10 14:24:14,516][227910] Mean Reward across all agents: 667.2024535568921[0m
[37m[1m[2023-07-10 14:24:14,516][227910] Average Trajectory Length: 991.2343333333333[0m
[36m[2023-07-10 14:24:14,520][227910] mean_value=-284.83150603939276, max_value=1432.9386163796298[0m
[37m[1m[2023-07-10 14:24:14,522][227910] New mean coefficients: [[ 3.6314564  -1.5059811   0.81784195  4.3424296   1.6946347 ]][0m
[37m[1m[2023-07-10 14:24:14,523][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:24:24,183][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 14:24:24,183][227910] FPS: 397613.15[0m
[36m[2023-07-10 14:24:24,185][227910] itr=598, itrs=2000, Progress: 29.90%[0m
[36m[2023-07-10 14:24:35,786][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 14:24:35,786][227910] FPS: 331459.91[0m
[36m[2023-07-10 14:24:40,614][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:24:40,615][227910] Reward + Measures: [[1496.60259969    0.24616647    0.53295082    0.53667235    0.25692362]][0m
[37m[1m[2023-07-10 14:24:40,615][227910] Max Reward on eval: 1496.6025996918409[0m
[37m[1m[2023-07-10 14:24:40,615][227910] Min Reward on eval: 1496.6025996918409[0m
[37m[1m[2023-07-10 14:24:40,615][227910] Mean Reward across all agents: 1496.6025996918409[0m
[37m[1m[2023-07-10 14:24:40,616][227910] Average Trajectory Length: 998.2473333333332[0m
[36m[2023-07-10 14:24:46,189][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:24:46,190][227910] Reward + Measures: [[-365.4590148     0.34010002    0.38690001    0.30430001    0.38750002]
 [ 838.42605099    0.2775        0.61200005    0.4513        0.48190004]
 [-366.21757934    0.35699999    0.37750003    0.26700005    0.40190002]
 ...
 [  89.45549445    0.31750003    0.47650003    0.3723        0.30770001]
 [1033.13442966    0.24990001    0.54839998    0.48230001    0.3953    ]
 [-944.74443215    0.29928824    0.26191345    0.2066832     0.3016437 ]][0m
[37m[1m[2023-07-10 14:24:46,190][227910] Max Reward on eval: 1616.669993431028[0m
[37m[1m[2023-07-10 14:24:46,190][227910] Min Reward on eval: -1266.3880105377175[0m
[37m[1m[2023-07-10 14:24:46,191][227910] Mean Reward across all agents: 429.3882356371467[0m
[37m[1m[2023-07-10 14:24:46,191][227910] Average Trajectory Length: 994.6103333333333[0m
[36m[2023-07-10 14:24:46,194][227910] mean_value=-615.4060216294722, max_value=1265.8479270935443[0m
[37m[1m[2023-07-10 14:24:46,197][227910] New mean coefficients: [[ 2.5641048  -0.8575983   1.1729252   4.0215397   0.07213461]][0m
[37m[1m[2023-07-10 14:24:46,198][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:24:55,909][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 14:24:55,910][227910] FPS: 395489.07[0m
[36m[2023-07-10 14:24:55,912][227910] itr=599, itrs=2000, Progress: 29.95%[0m
[36m[2023-07-10 14:25:07,324][227910] train() took 11.40 seconds to complete[0m
[36m[2023-07-10 14:25:07,325][227910] FPS: 336943.60[0m
[36m[2023-07-10 14:25:12,040][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:25:12,040][227910] Reward + Measures: [[1587.74476121    0.24396338    0.52349895    0.55422968    0.2510207 ]][0m
[37m[1m[2023-07-10 14:25:12,040][227910] Max Reward on eval: 1587.7447612092078[0m
[37m[1m[2023-07-10 14:25:12,040][227910] Min Reward on eval: 1587.7447612092078[0m
[37m[1m[2023-07-10 14:25:12,041][227910] Mean Reward across all agents: 1587.7447612092078[0m
[37m[1m[2023-07-10 14:25:12,041][227910] Average Trajectory Length: 998.8213333333333[0m
[36m[2023-07-10 14:25:17,434][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:25:17,434][227910] Reward + Measures: [[244.62013039   0.2922       0.48230001   0.33239999   0.39489999]
 [324.07437691   0.27790001   0.64539999   0.50470006   0.46779999]
 [786.92298708   0.296        0.46129999   0.55299997   0.2631    ]
 ...
 [283.69095724   0.47589999   0.5061       0.51999998   0.47170001]
 [677.83135828   0.3594       0.4506       0.50050002   0.22129999]
 [115.88896948   0.43260002   0.4021       0.45590001   0.24330001]][0m
[37m[1m[2023-07-10 14:25:17,435][227910] Max Reward on eval: 1582.5523757884628[0m
[37m[1m[2023-07-10 14:25:17,435][227910] Min Reward on eval: -1031.6227847313858[0m
[37m[1m[2023-07-10 14:25:17,435][227910] Mean Reward across all agents: 356.7676201649342[0m
[37m[1m[2023-07-10 14:25:17,435][227910] Average Trajectory Length: 997.342[0m
[36m[2023-07-10 14:25:17,439][227910] mean_value=-558.6442930547016, max_value=1311.2855729503558[0m
[37m[1m[2023-07-10 14:25:17,441][227910] New mean coefficients: [[ 1.5583036  -0.39734346  1.3019205   4.247651    0.21167387]][0m
[37m[1m[2023-07-10 14:25:17,442][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:25:27,147][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 14:25:27,147][227910] FPS: 395740.04[0m
[36m[2023-07-10 14:25:27,150][227910] itr=600, itrs=2000, Progress: 30.00%[0m
[37m[1m[2023-07-10 14:25:30,036][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000580[0m
[36m[2023-07-10 14:25:41,785][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 14:25:41,785][227910] FPS: 334385.71[0m
[36m[2023-07-10 14:25:46,612][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:25:46,612][227910] Reward + Measures: [[1681.98993372    0.24922799    0.52086842    0.57525373    0.24355088]][0m
[37m[1m[2023-07-10 14:25:46,612][227910] Max Reward on eval: 1681.98993371541[0m
[37m[1m[2023-07-10 14:25:46,612][227910] Min Reward on eval: 1681.98993371541[0m
[37m[1m[2023-07-10 14:25:46,613][227910] Mean Reward across all agents: 1681.98993371541[0m
[37m[1m[2023-07-10 14:25:46,613][227910] Average Trajectory Length: 999.9196666666667[0m
[36m[2023-07-10 14:25:52,085][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:25:52,085][227910] Reward + Measures: [[ 656.56516001    0.25299999    0.63870001    0.50959998    0.4869    ]
 [ 793.38932064    0.3177        0.62229997    0.69349998    0.3906    ]
 [ 979.89754046    0.37490001    0.63410002    0.61019993    0.35710001]
 ...
 [1319.90294373    0.27500001    0.54279995    0.60570002    0.30399999]
 [1083.37187998    0.38870001    0.59860003    0.6832        0.31360003]
 [ 340.05260451    0.59210002    0.46480003    0.68380004    0.16469999]][0m
[37m[1m[2023-07-10 14:25:52,085][227910] Max Reward on eval: 1612.8129562842428[0m
[37m[1m[2023-07-10 14:25:52,086][227910] Min Reward on eval: -415.8284658335033[0m
[37m[1m[2023-07-10 14:25:52,086][227910] Mean Reward across all agents: 642.2630907874802[0m
[37m[1m[2023-07-10 14:25:52,086][227910] Average Trajectory Length: 981.1086666666666[0m
[36m[2023-07-10 14:25:52,094][227910] mean_value=577.7851513817327, max_value=1931.2386124917898[0m
[37m[1m[2023-07-10 14:25:52,097][227910] New mean coefficients: [[ 2.053701   -0.3047391   1.5026361   4.113469    0.28431675]][0m
[37m[1m[2023-07-10 14:25:52,098][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:26:01,758][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 14:26:01,758][227910] FPS: 397572.49[0m
[36m[2023-07-10 14:26:01,760][227910] itr=601, itrs=2000, Progress: 30.05%[0m
[36m[2023-07-10 14:26:13,415][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 14:26:13,416][227910] FPS: 329927.80[0m
[36m[2023-07-10 14:26:18,292][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:26:18,292][227910] Reward + Measures: [[1806.47810034    0.24932157    0.504785      0.58783621    0.22555377]][0m
[37m[1m[2023-07-10 14:26:18,293][227910] Max Reward on eval: 1806.4781003350013[0m
[37m[1m[2023-07-10 14:26:18,293][227910] Min Reward on eval: 1806.4781003350013[0m
[37m[1m[2023-07-10 14:26:18,293][227910] Mean Reward across all agents: 1806.4781003350013[0m
[37m[1m[2023-07-10 14:26:18,293][227910] Average Trajectory Length: 999.6646666666667[0m
[36m[2023-07-10 14:26:23,792][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:26:23,798][227910] Reward + Measures: [[ 582.5392654     0.3468        0.51020002    0.42199999    0.25139999]
 [-345.52203494    0.42920002    0.5226        0.61040002    0.13060001]
 [ 384.75803184    0.2845        0.5219        0.40899998    0.21789999]
 ...
 [ -96.22036591    0.31659999    0.41620001    0.3405        0.24969998]
 [1484.27422004    0.2545        0.43190002    0.62049997    0.116     ]
 [1227.61307931    0.3592        0.57630002    0.62699997    0.22160001]][0m
[37m[1m[2023-07-10 14:26:23,799][227910] Max Reward on eval: 1839.7681758210995[0m
[37m[1m[2023-07-10 14:26:23,800][227910] Min Reward on eval: -995.110717116954[0m
[37m[1m[2023-07-10 14:26:23,800][227910] Mean Reward across all agents: 537.8658612314362[0m
[37m[1m[2023-07-10 14:26:23,801][227910] Average Trajectory Length: 987.5423333333333[0m
[36m[2023-07-10 14:26:23,809][227910] mean_value=-595.5391889755219, max_value=1966.6776115460555[0m
[37m[1m[2023-07-10 14:26:23,814][227910] New mean coefficients: [[2.3860059  0.04559359 0.8485454  4.4433513  0.7713797 ]][0m
[37m[1m[2023-07-10 14:26:23,815][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:26:33,621][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 14:26:33,621][227910] FPS: 391679.54[0m
[36m[2023-07-10 14:26:33,623][227910] itr=602, itrs=2000, Progress: 30.10%[0m
[36m[2023-07-10 14:26:45,233][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 14:26:45,233][227910] FPS: 331234.78[0m
[36m[2023-07-10 14:26:49,968][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:26:49,969][227910] Reward + Measures: [[1906.299129      0.24987583    0.49807775    0.59509534    0.2142477 ]][0m
[37m[1m[2023-07-10 14:26:49,969][227910] Max Reward on eval: 1906.299128996358[0m
[37m[1m[2023-07-10 14:26:49,969][227910] Min Reward on eval: 1906.299128996358[0m
[37m[1m[2023-07-10 14:26:49,970][227910] Mean Reward across all agents: 1906.299128996358[0m
[37m[1m[2023-07-10 14:26:49,970][227910] Average Trajectory Length: 999.3126666666666[0m
[36m[2023-07-10 14:26:55,576][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:26:55,577][227910] Reward + Measures: [[1183.07938922    0.23889999    0.40579996    0.6182        0.24169998]
 [ 659.38101732    0.43780002    0.64580005    0.52019995    0.37790003]
 [1673.77717103    0.23199999    0.44730002    0.59230006    0.21689999]
 ...
 [1355.58686213    0.36270002    0.5848        0.60210001    0.27360001]
 [ 772.19071923    0.4831        0.57139999    0.6214        0.39849997]
 [ 143.3825539     0.24589999    0.3732        0.37310001    0.34840003]][0m
[37m[1m[2023-07-10 14:26:55,577][227910] Max Reward on eval: 1928.4950350118802[0m
[37m[1m[2023-07-10 14:26:55,577][227910] Min Reward on eval: -1004.6111630025204[0m
[37m[1m[2023-07-10 14:26:55,577][227910] Mean Reward across all agents: 806.6819577990251[0m
[37m[1m[2023-07-10 14:26:55,578][227910] Average Trajectory Length: 992.1313333333333[0m
[36m[2023-07-10 14:26:55,583][227910] mean_value=69.7942532215898, max_value=2091.0440907460406[0m
[37m[1m[2023-07-10 14:26:55,586][227910] New mean coefficients: [[ 2.245791   -0.12273853  0.3365255   4.126256    0.97296023]][0m
[37m[1m[2023-07-10 14:26:55,587][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:27:05,378][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 14:27:05,378][227910] FPS: 392266.49[0m
[36m[2023-07-10 14:27:05,381][227910] itr=603, itrs=2000, Progress: 30.15%[0m
[36m[2023-07-10 14:27:16,943][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 14:27:16,944][227910] FPS: 332597.62[0m
[36m[2023-07-10 14:27:21,810][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:27:21,810][227910] Reward + Measures: [[2037.41674439    0.24924268    0.4815779     0.59934741    0.19026329]][0m
[37m[1m[2023-07-10 14:27:21,811][227910] Max Reward on eval: 2037.4167443922584[0m
[37m[1m[2023-07-10 14:27:21,811][227910] Min Reward on eval: 2037.4167443922584[0m
[37m[1m[2023-07-10 14:27:21,811][227910] Mean Reward across all agents: 2037.4167443922584[0m
[37m[1m[2023-07-10 14:27:21,811][227910] Average Trajectory Length: 999.9133333333333[0m
[36m[2023-07-10 14:27:27,277][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:27:27,278][227910] Reward + Measures: [[ -694.11378228     0.80289996     0.84499997     0.75349998
      0.86669999]
 [-1724.39994032     0.4664         0.46819997     0.044
      0.498     ]
 [  473.86509499     0.5316         0.67110008     0.48800001
      0.44940001]
 ...
 [  842.93456385     0.44220001     0.5596         0.50500005
      0.39550003]
 [  990.33615555     0.3152         0.45830002     0.4797
      0.294     ]
 [ -467.20647765     0.7949         0.79860002     0.0368
      0.86700004]][0m
[37m[1m[2023-07-10 14:27:27,278][227910] Max Reward on eval: 1666.557306591631[0m
[37m[1m[2023-07-10 14:27:27,278][227910] Min Reward on eval: -1897.6166946218698[0m
[37m[1m[2023-07-10 14:27:27,278][227910] Mean Reward across all agents: 162.30604938680173[0m
[37m[1m[2023-07-10 14:27:27,279][227910] Average Trajectory Length: 993.0316666666666[0m
[36m[2023-07-10 14:27:27,286][227910] mean_value=-284.1706469620462, max_value=1773.6912759473314[0m
[37m[1m[2023-07-10 14:27:27,289][227910] New mean coefficients: [[2.1773949  0.05260538 0.24829465 3.7375436  0.71789545]][0m
[37m[1m[2023-07-10 14:27:27,290][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:27:36,980][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 14:27:36,980][227910] FPS: 396363.98[0m
[36m[2023-07-10 14:27:36,982][227910] itr=604, itrs=2000, Progress: 30.20%[0m
[36m[2023-07-10 14:27:48,461][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 14:27:48,461][227910] FPS: 335019.68[0m
[36m[2023-07-10 14:27:53,218][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:27:53,219][227910] Reward + Measures: [[2140.56383007    0.2563712     0.47193864    0.62263477    0.17866239]][0m
[37m[1m[2023-07-10 14:27:53,219][227910] Max Reward on eval: 2140.5638300726623[0m
[37m[1m[2023-07-10 14:27:53,219][227910] Min Reward on eval: 2140.5638300726623[0m
[37m[1m[2023-07-10 14:27:53,219][227910] Mean Reward across all agents: 2140.5638300726623[0m
[37m[1m[2023-07-10 14:27:53,219][227910] Average Trajectory Length: 999.7383333333333[0m
[36m[2023-07-10 14:27:58,578][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:27:58,578][227910] Reward + Measures: [[ 916.85000719    0.24689999    0.54759997    0.47150001    0.43850002]
 [ 377.83850682    0.3716        0.60180008    0.2719        0.60460001]
 [1420.32131407    0.2458        0.58070004    0.50279999    0.29190001]
 ...
 [ 581.54302567    0.3012        0.5133        0.49309999    0.3141    ]
 [ 291.71228495    0.37199998    0.56209993    0.3434        0.4003    ]
 [1214.22539755    0.2888        0.49509999    0.63050002    0.185     ]][0m
[37m[1m[2023-07-10 14:27:58,579][227910] Max Reward on eval: 1919.3481183517492[0m
[37m[1m[2023-07-10 14:27:58,579][227910] Min Reward on eval: -1264.3372788064182[0m
[37m[1m[2023-07-10 14:27:58,579][227910] Mean Reward across all agents: 737.2954456732317[0m
[37m[1m[2023-07-10 14:27:58,579][227910] Average Trajectory Length: 996.016[0m
[36m[2023-07-10 14:27:58,583][227910] mean_value=-279.25915714326493, max_value=2372.9851749104214[0m
[37m[1m[2023-07-10 14:27:58,586][227910] New mean coefficients: [[ 1.0906477  -0.20267345  0.36689296  3.9647455   0.53722227]][0m
[37m[1m[2023-07-10 14:27:58,587][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:28:08,414][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 14:28:08,415][227910] FPS: 390800.27[0m
[36m[2023-07-10 14:28:08,417][227910] itr=605, itrs=2000, Progress: 30.25%[0m
[36m[2023-07-10 14:28:19,878][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 14:28:19,878][227910] FPS: 335563.29[0m
[36m[2023-07-10 14:28:24,596][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:28:24,597][227910] Reward + Measures: [[2214.43843238    0.25802365    0.46603364    0.63129801    0.17629467]][0m
[37m[1m[2023-07-10 14:28:24,597][227910] Max Reward on eval: 2214.4384323802465[0m
[37m[1m[2023-07-10 14:28:24,597][227910] Min Reward on eval: 2214.4384323802465[0m
[37m[1m[2023-07-10 14:28:24,597][227910] Mean Reward across all agents: 2214.4384323802465[0m
[37m[1m[2023-07-10 14:28:24,598][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:28:30,087][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:28:30,092][227910] Reward + Measures: [[755.38983557   0.44549999   0.47720003   0.56100005   0.44760004]
 [949.28635631   0.35520002   0.6311       0.51240009   0.48880002]
 [533.26157115   0.44759998   0.5851       0.23469999   0.6056    ]
 ...
 [646.31133939   0.36429998   0.43800002   0.45549998   0.3211    ]
 [-25.92718529   0.3378       0.35263333   0.3847       0.29459998]
 [429.35173751   0.71360004   0.39809999   0.65460002   0.58389997]][0m
[37m[1m[2023-07-10 14:28:30,093][227910] Max Reward on eval: 1624.4567490037298[0m
[37m[1m[2023-07-10 14:28:30,093][227910] Min Reward on eval: -1257.5128469489748[0m
[37m[1m[2023-07-10 14:28:30,093][227910] Mean Reward across all agents: 452.07958457134714[0m
[37m[1m[2023-07-10 14:28:30,094][227910] Average Trajectory Length: 986.4853333333333[0m
[36m[2023-07-10 14:28:30,098][227910] mean_value=-370.6975674258776, max_value=1707.1894840135265[0m
[37m[1m[2023-07-10 14:28:30,101][227910] New mean coefficients: [[ 0.68830717 -0.30975294  0.7961805   3.19371     0.00255179]][0m
[37m[1m[2023-07-10 14:28:30,102][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:28:39,780][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 14:28:39,781][227910] FPS: 396843.22[0m
[36m[2023-07-10 14:28:39,783][227910] itr=606, itrs=2000, Progress: 30.30%[0m
[36m[2023-07-10 14:28:51,296][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 14:28:51,297][227910] FPS: 334067.75[0m
[36m[2023-07-10 14:28:56,142][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:28:56,142][227910] Reward + Measures: [[2256.66279229    0.26196772    0.46360591    0.65468216    0.17757483]][0m
[37m[1m[2023-07-10 14:28:56,143][227910] Max Reward on eval: 2256.662792286771[0m
[37m[1m[2023-07-10 14:28:56,143][227910] Min Reward on eval: 2256.662792286771[0m
[37m[1m[2023-07-10 14:28:56,143][227910] Mean Reward across all agents: 2256.662792286771[0m
[37m[1m[2023-07-10 14:28:56,143][227910] Average Trajectory Length: 999.826[0m
[36m[2023-07-10 14:29:01,862][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:29:01,863][227910] Reward + Measures: [[ 195.87215415    0.3479        0.83939999    0.46030003    0.79830009]
 [1742.42515611    0.26770002    0.58340001    0.51500005    0.32029998]
 [1952.37319978    0.2378        0.43710002    0.58770001    0.10820001]
 ...
 [1008.72485303    0.3222        0.63560003    0.53069997    0.53259999]
 [ -12.03263858    0.63990003    0.67070001    0.66090006    0.60159999]
 [ 154.59046684    0.32890001    0.4745        0.40100002    0.42649999]][0m
[37m[1m[2023-07-10 14:29:01,863][227910] Max Reward on eval: 2327.4073888264597[0m
[37m[1m[2023-07-10 14:29:01,863][227910] Min Reward on eval: -576.674049854686[0m
[37m[1m[2023-07-10 14:29:01,863][227910] Mean Reward across all agents: 926.8348412332836[0m
[37m[1m[2023-07-10 14:29:01,863][227910] Average Trajectory Length: 995.756[0m
[36m[2023-07-10 14:29:01,869][227910] mean_value=14.399374957513597, max_value=2279.6209435329665[0m
[37m[1m[2023-07-10 14:29:01,872][227910] New mean coefficients: [[ 1.1705251  -0.05605838  0.24825269  3.2650976   0.10375255]][0m
[37m[1m[2023-07-10 14:29:01,873][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:29:11,854][227910] train() took 9.98 seconds to complete[0m
[36m[2023-07-10 14:29:11,854][227910] FPS: 384789.55[0m
[36m[2023-07-10 14:29:11,856][227910] itr=607, itrs=2000, Progress: 30.35%[0m
[36m[2023-07-10 14:29:23,406][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 14:29:23,407][227910] FPS: 333054.38[0m
[36m[2023-07-10 14:29:28,203][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:29:28,204][227910] Reward + Measures: [[2329.04939379    0.26141566    0.45761377    0.66779149    0.17368962]][0m
[37m[1m[2023-07-10 14:29:28,204][227910] Max Reward on eval: 2329.04939379358[0m
[37m[1m[2023-07-10 14:29:28,204][227910] Min Reward on eval: 2329.04939379358[0m
[37m[1m[2023-07-10 14:29:28,204][227910] Mean Reward across all agents: 2329.04939379358[0m
[37m[1m[2023-07-10 14:29:28,205][227910] Average Trajectory Length: 999.7603333333333[0m
[36m[2023-07-10 14:29:33,602][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:29:33,602][227910] Reward + Measures: [[1122.4036326     0.32350001    0.62149996    0.611         0.43619999]
 [1661.98475303    0.31420001    0.5632        0.65710002    0.34079999]
 [ 367.91211323    0.47550002    0.5151        0.44689998    0.49629998]
 ...
 [1091.54399898    0.34559998    0.53190005    0.64210004    0.1652    ]
 [1077.74268091    0.33000001    0.53689998    0.62360001    0.15159999]
 [1147.8854573     0.32750002    0.55830002    0.7044        0.25250003]][0m
[37m[1m[2023-07-10 14:29:33,603][227910] Max Reward on eval: 2259.6616541916273[0m
[37m[1m[2023-07-10 14:29:33,603][227910] Min Reward on eval: -247.33613790688688[0m
[37m[1m[2023-07-10 14:29:33,603][227910] Mean Reward across all agents: 1038.1378142750775[0m
[37m[1m[2023-07-10 14:29:33,603][227910] Average Trajectory Length: 999.6736666666666[0m
[36m[2023-07-10 14:29:33,609][227910] mean_value=338.2134443314097, max_value=1826.5595684097123[0m
[37m[1m[2023-07-10 14:29:33,612][227910] New mean coefficients: [[ 1.0984834  -0.2955704  -0.190099    4.005584    0.01159452]][0m
[37m[1m[2023-07-10 14:29:33,613][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:29:43,350][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 14:29:43,350][227910] FPS: 394438.93[0m
[36m[2023-07-10 14:29:43,353][227910] itr=608, itrs=2000, Progress: 30.40%[0m
[36m[2023-07-10 14:29:54,914][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 14:29:54,914][227910] FPS: 332658.65[0m
[36m[2023-07-10 14:29:59,666][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:29:59,667][227910] Reward + Measures: [[2358.66647268    0.26516733    0.45180565    0.69550872    0.17007099]][0m
[37m[1m[2023-07-10 14:29:59,667][227910] Max Reward on eval: 2358.666472682758[0m
[37m[1m[2023-07-10 14:29:59,667][227910] Min Reward on eval: 2358.666472682758[0m
[37m[1m[2023-07-10 14:29:59,667][227910] Mean Reward across all agents: 2358.666472682758[0m
[37m[1m[2023-07-10 14:29:59,668][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:30:05,239][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:30:05,240][227910] Reward + Measures: [[1443.34252551    0.24919999    0.5564        0.56389999    0.36229998]
 [1341.9836296     0.28560001    0.56330007    0.6243        0.42980003]
 [1466.5091035     0.36760002    0.55380005    0.70749998    0.21500002]
 ...
 [ 768.79790013    0.4429        0.4513        0.62880003    0.30230001]
 [1300.23874364    0.26849085    0.58648515    0.61098033    0.12522534]
 [1662.88332013    0.33160004    0.54170007    0.73410004    0.20369999]][0m
[37m[1m[2023-07-10 14:30:05,240][227910] Max Reward on eval: 2361.452256706683[0m
[37m[1m[2023-07-10 14:30:05,240][227910] Min Reward on eval: -506.3924420838477[0m
[37m[1m[2023-07-10 14:30:05,240][227910] Mean Reward across all agents: 1062.8391333898044[0m
[37m[1m[2023-07-10 14:30:05,241][227910] Average Trajectory Length: 996.4966666666667[0m
[36m[2023-07-10 14:30:05,248][227910] mean_value=266.4760204118883, max_value=2861.452256706683[0m
[37m[1m[2023-07-10 14:30:05,251][227910] New mean coefficients: [[ 1.3280457   0.38729176 -0.03452948  3.882223   -0.19365086]][0m
[37m[1m[2023-07-10 14:30:05,252][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:30:15,112][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 14:30:15,112][227910] FPS: 389542.83[0m
[36m[2023-07-10 14:30:15,114][227910] itr=609, itrs=2000, Progress: 30.45%[0m
[36m[2023-07-10 14:30:26,786][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 14:30:26,786][227910] FPS: 329543.05[0m
[36m[2023-07-10 14:30:31,630][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:30:31,631][227910] Reward + Measures: [[2503.98349694    0.25977701    0.4335843     0.6907627     0.15454167]][0m
[37m[1m[2023-07-10 14:30:31,631][227910] Max Reward on eval: 2503.9834969383205[0m
[37m[1m[2023-07-10 14:30:31,631][227910] Min Reward on eval: 2503.9834969383205[0m
[37m[1m[2023-07-10 14:30:31,631][227910] Mean Reward across all agents: 2503.9834969383205[0m
[37m[1m[2023-07-10 14:30:31,631][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:30:37,180][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:30:37,181][227910] Reward + Measures: [[ 386.97455609    0.52060002    0.63560003    0.3513        0.54030001]
 [ 728.82772469    0.46040002    0.6024        0.45010003    0.44930002]
 [1104.9097221     0.19250001    0.40470001    0.44259998    0.2251    ]
 ...
 [ 955.50194927    0.30450001    0.46199998    0.62169999    0.0867    ]
 [2159.10099697    0.2983        0.46100003    0.6275        0.1816    ]
 [ 266.06194394    0.70790005    0.86329997    0.89569998    0.6577    ]][0m
[37m[1m[2023-07-10 14:30:37,181][227910] Max Reward on eval: 2290.444757059589[0m
[37m[1m[2023-07-10 14:30:37,181][227910] Min Reward on eval: -1209.784968531644[0m
[37m[1m[2023-07-10 14:30:37,182][227910] Mean Reward across all agents: 827.9215070412332[0m
[37m[1m[2023-07-10 14:30:37,182][227910] Average Trajectory Length: 998.026[0m
[36m[2023-07-10 14:30:37,190][227910] mean_value=152.02289435182246, max_value=1779.9195603528212[0m
[37m[1m[2023-07-10 14:30:37,193][227910] New mean coefficients: [[ 0.9181498   0.82944226  0.14728971  3.4880953  -0.62826675]][0m
[37m[1m[2023-07-10 14:30:37,194][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:30:47,111][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 14:30:47,111][227910] FPS: 387306.74[0m
[36m[2023-07-10 14:30:47,113][227910] itr=610, itrs=2000, Progress: 30.50%[0m
[37m[1m[2023-07-10 14:30:50,351][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000590[0m
[36m[2023-07-10 14:31:02,266][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 14:31:02,266][227910] FPS: 329669.90[0m
[36m[2023-07-10 14:31:07,036][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:31:07,036][227910] Reward + Measures: [[1082.76027701    0.34888232    0.42716667    0.65060997    0.16286367]][0m
[37m[1m[2023-07-10 14:31:07,037][227910] Max Reward on eval: 1082.7602770076421[0m
[37m[1m[2023-07-10 14:31:07,037][227910] Min Reward on eval: 1082.7602770076421[0m
[37m[1m[2023-07-10 14:31:07,037][227910] Mean Reward across all agents: 1082.7602770076421[0m
[37m[1m[2023-07-10 14:31:07,038][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:31:12,577][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:31:12,577][227910] Reward + Measures: [[ 373.99604626    0.34759998    0.7784        0.18540001    0.67460006]
 [ 738.97199576    0.30930001    0.5424        0.58700001    0.28220001]
 [ 322.47782652    0.32850003    0.85120004    0.3251        0.78290004]
 ...
 [-338.56195547    0.62709999    0.9149        0.0267        0.89659995]
 [-341.1681811     0.45909998    0.88360006    0.09429999    0.85790008]
 [ 583.40128238    0.33230001    0.67250001    0.50600004    0.48660001]][0m
[37m[1m[2023-07-10 14:31:12,577][227910] Max Reward on eval: 1397.4012442091712[0m
[37m[1m[2023-07-10 14:31:12,578][227910] Min Reward on eval: -552.7378299311619[0m
[37m[1m[2023-07-10 14:31:12,578][227910] Mean Reward across all agents: 442.9733068759225[0m
[37m[1m[2023-07-10 14:31:12,578][227910] Average Trajectory Length: 994.463[0m
[36m[2023-07-10 14:31:12,585][227910] mean_value=-17.245298204284044, max_value=1562.8107485288288[0m
[37m[1m[2023-07-10 14:31:12,588][227910] New mean coefficients: [[ 0.8722569   0.90018386  0.09297234  3.8566008  -0.7123229 ]][0m
[37m[1m[2023-07-10 14:31:12,589][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:31:22,405][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 14:31:22,405][227910] FPS: 391284.28[0m
[36m[2023-07-10 14:31:22,407][227910] itr=611, itrs=2000, Progress: 30.55%[0m
[36m[2023-07-10 14:31:34,046][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 14:31:34,046][227910] FPS: 330489.51[0m
[36m[2023-07-10 14:31:38,904][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:31:38,905][227910] Reward + Measures: [[1240.58978445    0.34946656    0.42134565    0.67633742    0.14244621]][0m
[37m[1m[2023-07-10 14:31:38,905][227910] Max Reward on eval: 1240.5897844460264[0m
[37m[1m[2023-07-10 14:31:38,905][227910] Min Reward on eval: 1240.5897844460264[0m
[37m[1m[2023-07-10 14:31:38,905][227910] Mean Reward across all agents: 1240.5897844460264[0m
[37m[1m[2023-07-10 14:31:38,905][227910] Average Trajectory Length: 999.823[0m
[36m[2023-07-10 14:31:44,479][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:31:44,479][227910] Reward + Measures: [[-178.769537      0.44679996    0.70699996    0.22019999    0.57419997]
 [-526.50454125    0.37130004    0.55550003    0.1603        0.50089997]
 [-482.09415877    0.33679998    0.5546        0.1873        0.50640005]
 ...
 [-306.11214795    0.6875        0.41890001    0.6735        0.33540002]
 [1000.09529635    0.37830001    0.39320001    0.59850001    0.2911    ]
 [ -99.04240789    0.42320004    0.73929995    0.22780001    0.62379998]][0m
[37m[1m[2023-07-10 14:31:44,479][227910] Max Reward on eval: 1411.7637274417793[0m
[37m[1m[2023-07-10 14:31:44,480][227910] Min Reward on eval: -1127.594364725263[0m
[37m[1m[2023-07-10 14:31:44,480][227910] Mean Reward across all agents: 166.99725525068845[0m
[37m[1m[2023-07-10 14:31:44,480][227910] Average Trajectory Length: 994.2723333333333[0m
[36m[2023-07-10 14:31:44,485][227910] mean_value=-366.5730180199696, max_value=1716.1838450355456[0m
[37m[1m[2023-07-10 14:31:44,487][227910] New mean coefficients: [[ 0.7534701   0.23988664  0.10931219  3.6985853  -0.32313192]][0m
[37m[1m[2023-07-10 14:31:44,488][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:31:54,287][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 14:31:54,287][227910] FPS: 391966.90[0m
[36m[2023-07-10 14:31:54,289][227910] itr=612, itrs=2000, Progress: 30.60%[0m
[36m[2023-07-10 14:32:05,837][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 14:32:05,838][227910] FPS: 332993.59[0m
[36m[2023-07-10 14:32:10,664][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:32:10,664][227910] Reward + Measures: [[1349.25862417    0.34384221    0.42291871    0.69875795    0.12943354]][0m
[37m[1m[2023-07-10 14:32:10,665][227910] Max Reward on eval: 1349.2586241746856[0m
[37m[1m[2023-07-10 14:32:10,665][227910] Min Reward on eval: 1349.2586241746856[0m
[37m[1m[2023-07-10 14:32:10,665][227910] Mean Reward across all agents: 1349.2586241746856[0m
[37m[1m[2023-07-10 14:32:10,665][227910] Average Trajectory Length: 999.872[0m
[36m[2023-07-10 14:32:16,137][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:32:16,138][227910] Reward + Measures: [[ 446.14832424    0.34150001    0.39289999    0.55930001    0.2103    ]
 [1389.27002859    0.25119999    0.49590006    0.5869        0.12780002]
 [ 503.50917017    0.32069999    0.34709999    0.57069999    0.1947    ]
 ...
 [ 865.21251659    0.29652777    0.3988834     0.57611245    0.16870789]
 [1166.89273884    0.31200001    0.50559998    0.63510007    0.14229999]
 [ 814.85997006    0.32950002    0.45120001    0.58829999    0.1068    ]][0m
[37m[1m[2023-07-10 14:32:16,138][227910] Max Reward on eval: 1535.0924551275907[0m
[37m[1m[2023-07-10 14:32:16,138][227910] Min Reward on eval: -526.0889727260161[0m
[37m[1m[2023-07-10 14:32:16,139][227910] Mean Reward across all agents: 836.6549907059768[0m
[37m[1m[2023-07-10 14:32:16,139][227910] Average Trajectory Length: 994.7896666666667[0m
[36m[2023-07-10 14:32:16,143][227910] mean_value=-377.54334498917535, max_value=1951.8304717118153[0m
[37m[1m[2023-07-10 14:32:16,145][227910] New mean coefficients: [[ 1.3453071  -0.36203808  0.01079275  3.8314474  -0.452097  ]][0m
[37m[1m[2023-07-10 14:32:16,146][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:32:25,944][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 14:32:25,944][227910] FPS: 391999.18[0m
[36m[2023-07-10 14:32:25,947][227910] itr=613, itrs=2000, Progress: 30.65%[0m
[36m[2023-07-10 14:32:37,588][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 14:32:37,588][227910] FPS: 330331.56[0m
[36m[2023-07-10 14:32:42,416][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:32:42,417][227910] Reward + Measures: [[1527.298036      0.32562464    0.41754076    0.70608872    0.11508554]][0m
[37m[1m[2023-07-10 14:32:42,417][227910] Max Reward on eval: 1527.2980359950132[0m
[37m[1m[2023-07-10 14:32:42,417][227910] Min Reward on eval: 1527.2980359950132[0m
[37m[1m[2023-07-10 14:32:42,417][227910] Mean Reward across all agents: 1527.2980359950132[0m
[37m[1m[2023-07-10 14:32:42,417][227910] Average Trajectory Length: 999.6573333333333[0m
[36m[2023-07-10 14:32:47,834][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:32:47,835][227910] Reward + Measures: [[ 890.70946768    0.22259998    0.48400003    0.37600002    0.3001    ]
 [ 677.57670563    0.41330004    0.2872        0.64460009    0.1868    ]
 [1133.66940186    0.20444179    0.40419769    0.53861469    0.17543775]
 ...
 [1119.15913485    0.36540002    0.3556        0.64880002    0.2036    ]
 [1163.70825264    0.20299999    0.48230001    0.4729        0.2969    ]
 [ 799.72293166    0.49510002    0.34170002    0.68549997    0.1895    ]][0m
[37m[1m[2023-07-10 14:32:47,835][227910] Max Reward on eval: 1561.5971409535734[0m
[37m[1m[2023-07-10 14:32:47,835][227910] Min Reward on eval: -281.7547173967585[0m
[37m[1m[2023-07-10 14:32:47,835][227910] Mean Reward across all agents: 788.3430925848436[0m
[37m[1m[2023-07-10 14:32:47,836][227910] Average Trajectory Length: 992.1263333333333[0m
[36m[2023-07-10 14:32:47,841][227910] mean_value=-106.19352490097566, max_value=2059.6941303575645[0m
[37m[1m[2023-07-10 14:32:47,844][227910] New mean coefficients: [[ 0.977244    0.2377097   0.11751282  3.8535054  -0.5432092 ]][0m
[37m[1m[2023-07-10 14:32:47,845][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:32:57,537][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 14:32:57,537][227910] FPS: 396276.52[0m
[36m[2023-07-10 14:32:57,540][227910] itr=614, itrs=2000, Progress: 30.70%[0m
[36m[2023-07-10 14:33:08,986][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 14:33:08,986][227910] FPS: 335982.24[0m
[36m[2023-07-10 14:33:13,808][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:33:13,808][227910] Reward + Measures: [[1649.67611591    0.31673881    0.41773936    0.7128073     0.11045101]][0m
[37m[1m[2023-07-10 14:33:13,808][227910] Max Reward on eval: 1649.6761159107318[0m
[37m[1m[2023-07-10 14:33:13,809][227910] Min Reward on eval: 1649.6761159107318[0m
[37m[1m[2023-07-10 14:33:13,809][227910] Mean Reward across all agents: 1649.6761159107318[0m
[37m[1m[2023-07-10 14:33:13,809][227910] Average Trajectory Length: 999.3556666666666[0m
[36m[2023-07-10 14:33:19,440][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:33:19,440][227910] Reward + Measures: [[ 500.91905715    0.37450001    0.47890002    0.37910002    0.32319999]
 [ 380.42162179    0.45720002    0.48270002    0.40700004    0.37200001]
 [ 373.79875958    0.41840002    0.39389998    0.45609999    0.16069999]
 ...
 [ 648.6139375     0.67699999    0.49719998    0.75590003    0.51890004]
 [ 631.61834061    0.38319999    0.48090002    0.611         0.0475    ]
 [1320.95165082    0.30050001    0.42319998    0.74309999    0.108     ]][0m
[37m[1m[2023-07-10 14:33:19,441][227910] Max Reward on eval: 1762.9731558700557[0m
[37m[1m[2023-07-10 14:33:19,441][227910] Min Reward on eval: -181.75633663814224[0m
[37m[1m[2023-07-10 14:33:19,441][227910] Mean Reward across all agents: 846.7683088122486[0m
[37m[1m[2023-07-10 14:33:19,441][227910] Average Trajectory Length: 997.9093333333333[0m
[36m[2023-07-10 14:33:19,447][227910] mean_value=25.145907833552627, max_value=1766.8636111409423[0m
[37m[1m[2023-07-10 14:33:19,449][227910] New mean coefficients: [[ 0.7067877  -0.309555   -0.12071182  4.115273   -0.26926273]][0m
[37m[1m[2023-07-10 14:33:19,450][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:33:29,210][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 14:33:29,210][227910] FPS: 393518.35[0m
[36m[2023-07-10 14:33:29,213][227910] itr=615, itrs=2000, Progress: 30.75%[0m
[36m[2023-07-10 14:33:40,843][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 14:33:40,843][227910] FPS: 330722.38[0m
[36m[2023-07-10 14:33:45,648][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:33:45,648][227910] Reward + Measures: [[1721.99716187    0.30519983    0.41847074    0.73362494    0.10298279]][0m
[37m[1m[2023-07-10 14:33:45,649][227910] Max Reward on eval: 1721.997161868387[0m
[37m[1m[2023-07-10 14:33:45,649][227910] Min Reward on eval: 1721.997161868387[0m
[37m[1m[2023-07-10 14:33:45,649][227910] Mean Reward across all agents: 1721.997161868387[0m
[37m[1m[2023-07-10 14:33:45,649][227910] Average Trajectory Length: 999.0029999999999[0m
[36m[2023-07-10 14:33:51,114][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:33:51,115][227910] Reward + Measures: [[ 671.86718542    0.41910002    0.48540002    0.49449998    0.3757    ]
 [1208.8345672     0.27180001    0.39280003    0.6365        0.18140002]
 [ 921.41497584    0.36980003    0.40019998    0.49180004    0.26350003]
 ...
 [1385.14400292    0.29539999    0.43919998    0.69990009    0.10750001]
 [1238.6510956     0.30990002    0.37310001    0.72229999    0.15269999]
 [ 314.07780168    0.3053        0.2595        0.49980003    0.1349    ]][0m
[37m[1m[2023-07-10 14:33:51,115][227910] Max Reward on eval: 1936.820214550849[0m
[37m[1m[2023-07-10 14:33:51,115][227910] Min Reward on eval: -128.7833300601109[0m
[37m[1m[2023-07-10 14:33:51,115][227910] Mean Reward across all agents: 1107.703449641534[0m
[37m[1m[2023-07-10 14:33:51,116][227910] Average Trajectory Length: 991.1233333333333[0m
[36m[2023-07-10 14:33:51,119][227910] mean_value=-205.93533965706584, max_value=2436.820214550849[0m
[37m[1m[2023-07-10 14:33:51,122][227910] New mean coefficients: [[ 1.0597162  -0.5183464   0.16466595  3.4795408   0.01146078]][0m
[37m[1m[2023-07-10 14:33:51,123][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:34:00,914][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 14:34:00,915][227910] FPS: 392263.61[0m
[36m[2023-07-10 14:34:00,917][227910] itr=616, itrs=2000, Progress: 30.80%[0m
[36m[2023-07-10 14:34:12,539][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 14:34:12,539][227910] FPS: 330930.62[0m
[36m[2023-07-10 14:34:17,370][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:34:17,370][227910] Reward + Measures: [[760.872703     0.19908303   0.29499999   0.36189404   0.10074688]][0m
[37m[1m[2023-07-10 14:34:17,371][227910] Max Reward on eval: 760.8727029964557[0m
[37m[1m[2023-07-10 14:34:17,371][227910] Min Reward on eval: 760.8727029964557[0m
[37m[1m[2023-07-10 14:34:17,371][227910] Mean Reward across all agents: 760.8727029964557[0m
[37m[1m[2023-07-10 14:34:17,371][227910] Average Trajectory Length: 812.447[0m
[36m[2023-07-10 14:34:22,865][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:34:22,865][227910] Reward + Measures: [[  -14.61418319     0.22290002     0.46349999     0.40050003
      0.28970003]
 [-1050.51776074     0.34590003     0.2694         0.37060001
      0.31369999]
 [  326.93047503     0.20830014     0.29793984     0.37320384
      0.13440314]
 ...
 [  -47.52191094     0.25404319     0.28762433     0.387714
      0.21507943]
 [   70.62399405     0.61219996     0.25250003     0.63990003
      0.15340002]
 [  967.93607307     0.2300632      0.29241815     0.46187454
      0.08083989]][0m
[37m[1m[2023-07-10 14:34:22,865][227910] Max Reward on eval: 1313.6417266415897[0m
[37m[1m[2023-07-10 14:34:22,866][227910] Min Reward on eval: -1927.9648426802596[0m
[37m[1m[2023-07-10 14:34:22,866][227910] Mean Reward across all agents: -204.99220969585568[0m
[37m[1m[2023-07-10 14:34:22,866][227910] Average Trajectory Length: 931.9733333333334[0m
[36m[2023-07-10 14:34:22,868][227910] mean_value=-1565.5814522379605, max_value=721.8823868981435[0m
[37m[1m[2023-07-10 14:34:22,871][227910] New mean coefficients: [[ 1.6844203  -0.6400996   0.24175999  2.390493   -0.25587788]][0m
[37m[1m[2023-07-10 14:34:22,871][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:34:32,725][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 14:34:32,725][227910] FPS: 389787.28[0m
[36m[2023-07-10 14:34:32,727][227910] itr=617, itrs=2000, Progress: 30.85%[0m
[36m[2023-07-10 14:34:44,376][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 14:34:44,376][227910] FPS: 330178.98[0m
[36m[2023-07-10 14:34:49,207][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:34:49,207][227910] Reward + Measures: [[953.84641732   0.20825247   0.30028209   0.39723834   0.09759365]][0m
[37m[1m[2023-07-10 14:34:49,207][227910] Max Reward on eval: 953.8464173202387[0m
[37m[1m[2023-07-10 14:34:49,207][227910] Min Reward on eval: 953.8464173202387[0m
[37m[1m[2023-07-10 14:34:49,208][227910] Mean Reward across all agents: 953.8464173202387[0m
[37m[1m[2023-07-10 14:34:49,208][227910] Average Trajectory Length: 887.8449999999999[0m
[36m[2023-07-10 14:34:54,812][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:34:54,812][227910] Reward + Measures: [[ -19.31663254    0.24788237    0.19579542    0.56223297    0.26640081]
 [ -14.25858586    0.38350001    0.29210001    0.35960001    0.06720001]
 [ 925.52674444    0.25479999    0.30603334    0.31933334    0.08213333]
 ...
 [-132.68167987    0.63490003    0.39159998    0.62279999    0.1955    ]
 [-318.91661323    0.3098        0.42630002    0.32200003    0.25710002]
 [ 367.15787659    0.29870003    0.41840002    0.46440002    0.20480001]][0m
[37m[1m[2023-07-10 14:34:54,813][227910] Max Reward on eval: 1238.515227499965[0m
[37m[1m[2023-07-10 14:34:54,813][227910] Min Reward on eval: -666.8428774299973[0m
[37m[1m[2023-07-10 14:34:54,813][227910] Mean Reward across all agents: 283.8202584242258[0m
[37m[1m[2023-07-10 14:34:54,813][227910] Average Trajectory Length: 935.7693333333333[0m
[36m[2023-07-10 14:34:54,818][227910] mean_value=-1413.8526566538976, max_value=1713.277314716077[0m
[37m[1m[2023-07-10 14:34:54,821][227910] New mean coefficients: [[ 1.7133776   0.0028429   0.31271467  2.691144   -0.9792907 ]][0m
[37m[1m[2023-07-10 14:34:54,822][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:35:04,695][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 14:35:04,695][227910] FPS: 388992.79[0m
[36m[2023-07-10 14:35:04,697][227910] itr=618, itrs=2000, Progress: 30.90%[0m
[36m[2023-07-10 14:35:16,153][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 14:35:16,153][227910] FPS: 335733.74[0m
[36m[2023-07-10 14:35:20,933][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:35:20,934][227910] Reward + Measures: [[1139.94653595    0.21980342    0.30935135    0.43432856    0.09653819]][0m
[37m[1m[2023-07-10 14:35:20,934][227910] Max Reward on eval: 1139.9465359536264[0m
[37m[1m[2023-07-10 14:35:20,934][227910] Min Reward on eval: 1139.9465359536264[0m
[37m[1m[2023-07-10 14:35:20,934][227910] Mean Reward across all agents: 1139.9465359536264[0m
[37m[1m[2023-07-10 14:35:20,935][227910] Average Trajectory Length: 931.8986666666666[0m
[36m[2023-07-10 14:35:26,392][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:35:26,392][227910] Reward + Measures: [[387.86537325   0.27740002   0.25780001   0.38080001   0.1576    ]
 [845.08388667   0.37869999   0.3154       0.53420001   0.13790001]
 [234.86784847   0.34530002   0.28819999   0.43700001   0.18539999]
 ...
 [727.94635824   0.31310955   0.40269524   0.48422295   0.0608303 ]
 [ 86.13594187   0.3502       0.32820001   0.4061       0.22330001]
 [232.40776985   0.34419999   0.30850002   0.4258       0.18350001]][0m
[37m[1m[2023-07-10 14:35:26,393][227910] Max Reward on eval: 1568.0310462784023[0m
[37m[1m[2023-07-10 14:35:26,393][227910] Min Reward on eval: -510.44182791786733[0m
[37m[1m[2023-07-10 14:35:26,393][227910] Mean Reward across all agents: 569.7383497171681[0m
[37m[1m[2023-07-10 14:35:26,393][227910] Average Trajectory Length: 971.2283333333334[0m
[36m[2023-07-10 14:35:26,396][227910] mean_value=-1208.7524630350565, max_value=1573.5426432002346[0m
[37m[1m[2023-07-10 14:35:26,398][227910] New mean coefficients: [[ 2.0101793   0.30097717  0.50240064  1.6183202  -1.0423378 ]][0m
[37m[1m[2023-07-10 14:35:26,399][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:35:36,045][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 14:35:36,045][227910] FPS: 398170.07[0m
[36m[2023-07-10 14:35:36,047][227910] itr=619, itrs=2000, Progress: 30.95%[0m
[36m[2023-07-10 14:35:47,495][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 14:35:47,496][227910] FPS: 335932.46[0m
[36m[2023-07-10 14:35:52,355][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:35:52,355][227910] Reward + Measures: [[1318.10338323    0.22834958    0.31887934    0.47530285    0.09035183]][0m
[37m[1m[2023-07-10 14:35:52,356][227910] Max Reward on eval: 1318.1033832269914[0m
[37m[1m[2023-07-10 14:35:52,356][227910] Min Reward on eval: 1318.1033832269914[0m
[37m[1m[2023-07-10 14:35:52,356][227910] Mean Reward across all agents: 1318.1033832269914[0m
[37m[1m[2023-07-10 14:35:52,356][227910] Average Trajectory Length: 951.256[0m
[36m[2023-07-10 14:35:58,021][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:35:58,022][227910] Reward + Measures: [[-136.02341437    0.26679096    0.26480135    0.31178603    0.10977589]
 [ 495.30080819    0.39000002    0.33579999    0.48130003    0.0743    ]
 [-120.14589511    0.37529999    0.3066        0.4192        0.0518    ]
 ...
 [ 289.94311       0.29932758    0.28816897    0.39128968    0.09733448]
 [1009.09146954    0.32000002    0.37250003    0.57849997    0.1688    ]
 [ 658.47466641    0.26088724    0.31124747    0.37755075    0.08809959]][0m
[37m[1m[2023-07-10 14:35:58,022][227910] Max Reward on eval: 1382.541146897059[0m
[37m[1m[2023-07-10 14:35:58,023][227910] Min Reward on eval: -914.5820087663014[0m
[37m[1m[2023-07-10 14:35:58,023][227910] Mean Reward across all agents: 384.54616981037856[0m
[37m[1m[2023-07-10 14:35:58,023][227910] Average Trajectory Length: 962.841[0m
[36m[2023-07-10 14:35:58,025][227910] mean_value=-1846.3052084693227, max_value=1049.8889988109236[0m
[37m[1m[2023-07-10 14:35:58,028][227910] New mean coefficients: [[ 2.1128135   0.18435018  0.3712856   3.3258247  -0.6909702 ]][0m
[37m[1m[2023-07-10 14:35:58,029][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:36:07,798][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 14:36:07,799][227910] FPS: 393135.67[0m
[36m[2023-07-10 14:36:07,801][227910] itr=620, itrs=2000, Progress: 31.00%[0m
[37m[1m[2023-07-10 14:36:10,890][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000600[0m
[36m[2023-07-10 14:36:22,680][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 14:36:22,680][227910] FPS: 333034.43[0m
[36m[2023-07-10 14:36:27,359][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:36:27,360][227910] Reward + Measures: [[1023.76089432    0.27954727    0.35478315    0.53366709    0.11220901]][0m
[37m[1m[2023-07-10 14:36:27,360][227910] Max Reward on eval: 1023.7608943236393[0m
[37m[1m[2023-07-10 14:36:27,360][227910] Min Reward on eval: 1023.7608943236393[0m
[37m[1m[2023-07-10 14:36:27,360][227910] Mean Reward across all agents: 1023.7608943236393[0m
[37m[1m[2023-07-10 14:36:27,361][227910] Average Trajectory Length: 996.332[0m
[36m[2023-07-10 14:36:32,759][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:36:32,765][227910] Reward + Measures: [[ 704.36511575    0.40340003    0.4226        0.6232        0.15110001]
 [ 516.06138245    0.46180001    0.46099997    0.48499998    0.19419999]
 [ 938.07550413    0.333         0.35840002    0.60980004    0.1097    ]
 ...
 [ 560.73324119    0.21887498    0.26819846    0.61850327    0.17291909]
 [ 475.68741293    0.48410001    0.43390003    0.56419998    0.1635    ]
 [1200.54509498    0.26530001    0.33080003    0.574         0.0889    ]][0m
[37m[1m[2023-07-10 14:36:32,765][227910] Max Reward on eval: 1324.221452536201[0m
[37m[1m[2023-07-10 14:36:32,766][227910] Min Reward on eval: -698.3949523892486[0m
[37m[1m[2023-07-10 14:36:32,766][227910] Mean Reward across all agents: 780.9107849480057[0m
[37m[1m[2023-07-10 14:36:32,766][227910] Average Trajectory Length: 990.13[0m
[36m[2023-07-10 14:36:32,770][227910] mean_value=95.16186642050185, max_value=1702.7005395027577[0m
[37m[1m[2023-07-10 14:36:32,773][227910] New mean coefficients: [[ 2.6149411   0.8742704   0.20836665  3.2036173  -0.86153615]][0m
[37m[1m[2023-07-10 14:36:32,774][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:36:42,425][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 14:36:42,425][227910] FPS: 397931.36[0m
[36m[2023-07-10 14:36:42,428][227910] itr=621, itrs=2000, Progress: 31.05%[0m
[36m[2023-07-10 14:36:53,858][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 14:36:53,859][227910] FPS: 336469.22[0m
[36m[2023-07-10 14:36:58,737][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:36:58,738][227910] Reward + Measures: [[1205.95401563    0.26054546    0.3325468     0.54307616    0.08372017]][0m
[37m[1m[2023-07-10 14:36:58,738][227910] Max Reward on eval: 1205.954015629965[0m
[37m[1m[2023-07-10 14:36:58,738][227910] Min Reward on eval: 1205.954015629965[0m
[37m[1m[2023-07-10 14:36:58,739][227910] Mean Reward across all agents: 1205.954015629965[0m
[37m[1m[2023-07-10 14:36:58,739][227910] Average Trajectory Length: 995.0659999999999[0m
[36m[2023-07-10 14:37:04,443][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:37:04,444][227910] Reward + Measures: [[ 507.0556965     0.29720002    0.24689999    0.50490004    0.14060001]
 [ 810.56990902    0.33160001    0.37710002    0.3705        0.1842    ]
 [1044.96274856    0.3272        0.38840002    0.44969997    0.13640001]
 ...
 [ 471.41469262    0.31325164    0.48084193    0.41738364    0.2837151 ]
 [ 462.68416006    0.30477595    0.26820907    0.5311268     0.16738521]
 [ 861.3666528     0.29669997    0.36600003    0.39910004    0.15360001]][0m
[37m[1m[2023-07-10 14:37:04,444][227910] Max Reward on eval: 1242.3093265511911[0m
[37m[1m[2023-07-10 14:37:04,445][227910] Min Reward on eval: -78.64432068188907[0m
[37m[1m[2023-07-10 14:37:04,445][227910] Mean Reward across all agents: 681.6066374387087[0m
[37m[1m[2023-07-10 14:37:04,445][227910] Average Trajectory Length: 982.4716666666666[0m
[36m[2023-07-10 14:37:04,447][227910] mean_value=-1674.8891080984563, max_value=536.0414871518093[0m
[37m[1m[2023-07-10 14:37:04,449][227910] New mean coefficients: [[ 2.820085    1.5650519   0.35098398  3.5575671  -0.6255559 ]][0m
[37m[1m[2023-07-10 14:37:04,450][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:37:14,254][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 14:37:14,255][227910] FPS: 391737.00[0m
[36m[2023-07-10 14:37:14,257][227910] itr=622, itrs=2000, Progress: 31.10%[0m
[36m[2023-07-10 14:37:25,769][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 14:37:25,769][227910] FPS: 334064.00[0m
[36m[2023-07-10 14:37:30,494][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:37:30,494][227910] Reward + Measures: [[1348.74360268    0.24983931    0.32070717    0.57097775    0.0680733 ]][0m
[37m[1m[2023-07-10 14:37:30,494][227910] Max Reward on eval: 1348.7436026796058[0m
[37m[1m[2023-07-10 14:37:30,494][227910] Min Reward on eval: 1348.7436026796058[0m
[37m[1m[2023-07-10 14:37:30,495][227910] Mean Reward across all agents: 1348.7436026796058[0m
[37m[1m[2023-07-10 14:37:30,495][227910] Average Trajectory Length: 995.4433333333333[0m
[36m[2023-07-10 14:37:36,051][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:37:36,051][227910] Reward + Measures: [[ 563.39808765    0.45170003    0.41120002    0.5546        0.22790001]
 [ 962.01902173    0.22119999    0.2969        0.61590004    0.0814    ]
 [ 352.39755251    0.23640001    0.40450001    0.4402        0.2174    ]
 ...
 [1261.99453415    0.27959999    0.28029999    0.58750004    0.06440001]
 [ 697.09425619    0.29100001    0.4368        0.53759998    0.21640001]
 [ 688.70032346    0.28710002    0.4104        0.60909998    0.0801    ]][0m
[37m[1m[2023-07-10 14:37:36,051][227910] Max Reward on eval: 1501.24762243086[0m
[37m[1m[2023-07-10 14:37:36,052][227910] Min Reward on eval: -248.06888892446878[0m
[37m[1m[2023-07-10 14:37:36,052][227910] Mean Reward across all agents: 709.8785382932334[0m
[37m[1m[2023-07-10 14:37:36,052][227910] Average Trajectory Length: 998.3676666666667[0m
[36m[2023-07-10 14:37:36,055][227910] mean_value=-659.9828042452125, max_value=1174.4889647431287[0m
[37m[1m[2023-07-10 14:37:36,058][227910] New mean coefficients: [[3.4505973  1.3646349  0.22809453 3.565235   0.03737682]][0m
[37m[1m[2023-07-10 14:37:36,059][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:37:45,740][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 14:37:45,740][227910] FPS: 396721.80[0m
[36m[2023-07-10 14:37:45,743][227910] itr=623, itrs=2000, Progress: 31.15%[0m
[36m[2023-07-10 14:37:57,424][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 14:37:57,425][227910] FPS: 329257.56[0m
[36m[2023-07-10 14:38:02,304][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:38:02,304][227910] Reward + Measures: [[1782.52661969    0.25788221    0.26487425    0.61800867    0.07195145]][0m
[37m[1m[2023-07-10 14:38:02,305][227910] Max Reward on eval: 1782.526619688176[0m
[37m[1m[2023-07-10 14:38:02,305][227910] Min Reward on eval: 1782.526619688176[0m
[37m[1m[2023-07-10 14:38:02,305][227910] Mean Reward across all agents: 1782.526619688176[0m
[37m[1m[2023-07-10 14:38:02,305][227910] Average Trajectory Length: 998.1659999999999[0m
[36m[2023-07-10 14:38:07,778][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:38:07,779][227910] Reward + Measures: [[ -37.10690544    0.26574358    0.25863302    0.29671419    0.22627096]
 [-155.14376871    0.21859999    0.2314        0.26190001    0.2158    ]
 [ 864.47586721    0.25311065    0.26314101    0.40005109    0.14720058]
 ...
 [-702.07504533    0.29980001    0.3612        0.33849999    0.3299    ]
 [-426.98895018    0.3001        0.3942        0.3811        0.36039999]
 [-734.07758123    0.20836197    0.21193205    0.26996824    0.20017751]][0m
[37m[1m[2023-07-10 14:38:07,779][227910] Max Reward on eval: 1749.570100262668[0m
[37m[1m[2023-07-10 14:38:07,779][227910] Min Reward on eval: -1395.7399599809346[0m
[37m[1m[2023-07-10 14:38:07,779][227910] Mean Reward across all agents: 90.08583281836584[0m
[37m[1m[2023-07-10 14:38:07,780][227910] Average Trajectory Length: 960.5706666666666[0m
[36m[2023-07-10 14:38:07,781][227910] mean_value=-1871.949105101932, max_value=901.8564629368193[0m
[37m[1m[2023-07-10 14:38:07,784][227910] New mean coefficients: [[ 3.306767    1.1908638   0.11202513  3.4183633  -0.19981153]][0m
[37m[1m[2023-07-10 14:38:07,785][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:38:17,492][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 14:38:17,492][227910] FPS: 395651.06[0m
[36m[2023-07-10 14:38:17,494][227910] itr=624, itrs=2000, Progress: 31.20%[0m
[36m[2023-07-10 14:38:29,160][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 14:38:29,161][227910] FPS: 329628.75[0m
[36m[2023-07-10 14:38:34,014][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:38:34,015][227910] Reward + Measures: [[559.38800352   0.51124698   0.31931934   0.65423578   0.22669375]][0m
[37m[1m[2023-07-10 14:38:34,015][227910] Max Reward on eval: 559.3880035235145[0m
[37m[1m[2023-07-10 14:38:34,015][227910] Min Reward on eval: 559.3880035235145[0m
[37m[1m[2023-07-10 14:38:34,015][227910] Mean Reward across all agents: 559.3880035235145[0m
[37m[1m[2023-07-10 14:38:34,015][227910] Average Trajectory Length: 999.2389999999999[0m
[36m[2023-07-10 14:38:39,503][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:38:39,504][227910] Reward + Measures: [[655.98949298   0.47760001   0.30469999   0.62930006   0.19940001]
 [613.55376593   0.43530002   0.27760002   0.58349997   0.20319998]
 [747.26352295   0.40273881   0.29026338   0.57998252   0.17739891]
 ...
 [736.70228578   0.32360002   0.26430002   0.51500005   0.19179998]
 [758.04916386   0.38900003   0.28570005   0.56740004   0.17590001]
 [533.95102011   0.52100003   0.31560001   0.67220002   0.19789998]][0m
[37m[1m[2023-07-10 14:38:39,504][227910] Max Reward on eval: 907.1433473130455[0m
[37m[1m[2023-07-10 14:38:39,504][227910] Min Reward on eval: 193.4025962667074[0m
[37m[1m[2023-07-10 14:38:39,504][227910] Mean Reward across all agents: 632.3742842892827[0m
[37m[1m[2023-07-10 14:38:39,504][227910] Average Trajectory Length: 991.1643333333333[0m
[36m[2023-07-10 14:38:39,508][227910] mean_value=-71.10230783982585, max_value=884.3359235364478[0m
[37m[1m[2023-07-10 14:38:39,510][227910] New mean coefficients: [[ 4.1947427   1.073085    0.01658949  2.7502651  -0.24588513]][0m
[37m[1m[2023-07-10 14:38:39,511][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:38:49,313][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 14:38:49,313][227910] FPS: 391852.19[0m
[36m[2023-07-10 14:38:49,315][227910] itr=625, itrs=2000, Progress: 31.25%[0m
[36m[2023-07-10 14:39:00,931][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 14:39:00,931][227910] FPS: 331165.37[0m
[36m[2023-07-10 14:39:05,691][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:39:05,691][227910] Reward + Measures: [[623.98426546   0.5071708    0.30884644   0.66155946   0.21667533]][0m
[37m[1m[2023-07-10 14:39:05,691][227910] Max Reward on eval: 623.9842654634112[0m
[37m[1m[2023-07-10 14:39:05,691][227910] Min Reward on eval: 623.9842654634112[0m
[37m[1m[2023-07-10 14:39:05,691][227910] Mean Reward across all agents: 623.9842654634112[0m
[37m[1m[2023-07-10 14:39:05,692][227910] Average Trajectory Length: 999.6593333333333[0m
[36m[2023-07-10 14:39:11,055][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:39:11,056][227910] Reward + Measures: [[727.54000249   0.2461462    0.41397601   0.39485964   0.25473863]
 [255.9029665    0.2069       0.28850001   0.27200001   0.1701    ]
 [430.17427222   0.28239998   0.26370001   0.43310004   0.2332    ]
 ...
 [519.63270746   0.25330001   0.32539999   0.36100003   0.24780002]
 [ -5.97923674   0.22656667   0.28944445   0.28018889   0.20335555]
 [584.05974439   0.25650001   0.44330001   0.3637       0.25079998]][0m
[37m[1m[2023-07-10 14:39:11,056][227910] Max Reward on eval: 903.7474210691405[0m
[37m[1m[2023-07-10 14:39:11,056][227910] Min Reward on eval: -742.0620272466738[0m
[37m[1m[2023-07-10 14:39:11,057][227910] Mean Reward across all agents: 364.12137536576535[0m
[37m[1m[2023-07-10 14:39:11,057][227910] Average Trajectory Length: 977.9823333333333[0m
[36m[2023-07-10 14:39:11,059][227910] mean_value=-1702.35826341933, max_value=1348.678121595271[0m
[37m[1m[2023-07-10 14:39:11,061][227910] New mean coefficients: [[ 3.0501354   0.99575466  0.06016742  0.9206778  -0.49932957]][0m
[37m[1m[2023-07-10 14:39:11,062][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:39:20,739][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 14:39:20,739][227910] FPS: 396877.90[0m
[36m[2023-07-10 14:39:20,742][227910] itr=626, itrs=2000, Progress: 31.30%[0m
[36m[2023-07-10 14:39:32,273][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 14:39:32,274][227910] FPS: 333461.02[0m
[36m[2023-07-10 14:39:37,055][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:39:37,055][227910] Reward + Measures: [[729.38590091   0.4807792    0.31795338   0.643493     0.20632486]][0m
[37m[1m[2023-07-10 14:39:37,055][227910] Max Reward on eval: 729.3859009075867[0m
[37m[1m[2023-07-10 14:39:37,056][227910] Min Reward on eval: 729.3859009075867[0m
[37m[1m[2023-07-10 14:39:37,056][227910] Mean Reward across all agents: 729.3859009075867[0m
[37m[1m[2023-07-10 14:39:37,056][227910] Average Trajectory Length: 999.9436666666667[0m
[36m[2023-07-10 14:39:42,732][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:39:42,792][227910] Reward + Measures: [[ 534.96087565    0.62530005    0.47119999    0.73150009    0.38859999]
 [1082.27459946    0.37890002    0.28099999    0.59130001    0.19069999]
 [1057.0705787     0.35499999    0.29239997    0.5765        0.23730002]
 ...
 [ 722.14970261    0.5122        0.30499998    0.65580004    0.1823    ]
 [ 777.23827813    0.4914        0.32010004    0.67210001    0.205     ]
 [ 885.32532675    0.43080002    0.33249998    0.61190003    0.23470001]][0m
[37m[1m[2023-07-10 14:39:42,793][227910] Max Reward on eval: 1229.4631318820873[0m
[37m[1m[2023-07-10 14:39:42,793][227910] Min Reward on eval: 418.0353190041962[0m
[37m[1m[2023-07-10 14:39:42,793][227910] Mean Reward across all agents: 838.8164116017784[0m
[37m[1m[2023-07-10 14:39:42,793][227910] Average Trajectory Length: 998.6076666666667[0m
[36m[2023-07-10 14:39:42,799][227910] mean_value=332.60067156702496, max_value=1468.499097788462[0m
[37m[1m[2023-07-10 14:39:42,802][227910] New mean coefficients: [[ 3.9022472   1.2477441  -0.00260171  2.2095346  -0.28687835]][0m
[37m[1m[2023-07-10 14:39:42,803][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:39:52,490][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 14:39:52,490][227910] FPS: 396477.44[0m
[36m[2023-07-10 14:39:52,492][227910] itr=627, itrs=2000, Progress: 31.35%[0m
[36m[2023-07-10 14:40:03,918][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 14:40:03,919][227910] FPS: 336551.78[0m
[36m[2023-07-10 14:40:08,689][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:40:08,689][227910] Reward + Measures: [[793.99586961   0.46444717   0.31275135   0.63469726   0.20270646]][0m
[37m[1m[2023-07-10 14:40:08,689][227910] Max Reward on eval: 793.9958696096077[0m
[37m[1m[2023-07-10 14:40:08,689][227910] Min Reward on eval: 793.9958696096077[0m
[37m[1m[2023-07-10 14:40:08,690][227910] Mean Reward across all agents: 793.9958696096077[0m
[37m[1m[2023-07-10 14:40:08,690][227910] Average Trajectory Length: 999.7819999999999[0m
[36m[2023-07-10 14:40:14,149][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:40:14,150][227910] Reward + Measures: [[ 990.82765761    0.42230001    0.26830003    0.63510001    0.21870001]
 [ 828.91196275    0.56960005    0.40770003    0.72760004    0.4127    ]
 [1105.90717034    0.41759998    0.301         0.62220001    0.2895    ]
 ...
 [ 812.09438777    0.59939998    0.47950003    0.66990006    0.4639    ]
 [ 913.69969701    0.44549999    0.28889999    0.60190004    0.19469999]
 [ 669.86227971    0.53610003    0.37510002    0.65240002    0.26879999]][0m
[37m[1m[2023-07-10 14:40:14,150][227910] Max Reward on eval: 1314.4421420300846[0m
[37m[1m[2023-07-10 14:40:14,150][227910] Min Reward on eval: 542.4279574216459[0m
[37m[1m[2023-07-10 14:40:14,150][227910] Mean Reward across all agents: 949.2923464981783[0m
[37m[1m[2023-07-10 14:40:14,151][227910] Average Trajectory Length: 998.8086666666667[0m
[36m[2023-07-10 14:40:14,156][227910] mean_value=327.24926883779784, max_value=1549.7757429235237[0m
[37m[1m[2023-07-10 14:40:14,159][227910] New mean coefficients: [[ 5.2210827   1.3488078   0.09027945  3.4623911  -0.06403369]][0m
[37m[1m[2023-07-10 14:40:14,160][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:40:23,879][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 14:40:23,880][227910] FPS: 395155.42[0m
[36m[2023-07-10 14:40:23,882][227910] itr=628, itrs=2000, Progress: 31.40%[0m
[36m[2023-07-10 14:40:35,355][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 14:40:35,355][227910] FPS: 335170.05[0m
[36m[2023-07-10 14:40:40,058][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:40:40,059][227910] Reward + Measures: [[883.67617789   0.44293904   0.30810469   0.62877065   0.19951102]][0m
[37m[1m[2023-07-10 14:40:40,059][227910] Max Reward on eval: 883.6761778942372[0m
[37m[1m[2023-07-10 14:40:40,059][227910] Min Reward on eval: 883.6761778942372[0m
[37m[1m[2023-07-10 14:40:40,059][227910] Mean Reward across all agents: 883.6761778942372[0m
[37m[1m[2023-07-10 14:40:40,060][227910] Average Trajectory Length: 999.6306666666667[0m
[36m[2023-07-10 14:40:45,496][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:40:45,496][227910] Reward + Measures: [[  861.88704335     0.48429999     0.3369         0.58969992
      0.16779999]
 [ -128.10249987     0.82460004     0.6778         0.80849999
      0.66710001]
 [-1978.93415232     0.85120004     0.63409996     0.84799999
      0.59570003]
 ...
 [-1361.5990115      0.77660006     0.50710005     0.76910007
      0.45700002]
 [ -214.93716227     0.78250003     0.6455         0.8071
      0.64560002]
 [-1704.29128057     0.75150007     0.3915         0.68650001
      0.38589999]][0m
[37m[1m[2023-07-10 14:40:45,497][227910] Max Reward on eval: 1203.812208593404[0m
[37m[1m[2023-07-10 14:40:45,497][227910] Min Reward on eval: -2176.4957301635063[0m
[37m[1m[2023-07-10 14:40:45,497][227910] Mean Reward across all agents: -697.7182032168286[0m
[37m[1m[2023-07-10 14:40:45,497][227910] Average Trajectory Length: 995.0169999999999[0m
[36m[2023-07-10 14:40:45,500][227910] mean_value=-752.0526909411515, max_value=702.5098230702123[0m
[37m[1m[2023-07-10 14:40:45,502][227910] New mean coefficients: [[ 4.218732    1.6281025   0.37715095  1.390887   -0.11528248]][0m
[37m[1m[2023-07-10 14:40:45,503][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:40:55,164][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 14:40:55,164][227910] FPS: 397556.64[0m
[36m[2023-07-10 14:40:55,166][227910] itr=629, itrs=2000, Progress: 31.45%[0m
[36m[2023-07-10 14:41:06,788][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 14:41:06,788][227910] FPS: 330881.43[0m
[36m[2023-07-10 14:41:11,578][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:41:11,579][227910] Reward + Measures: [[1031.57442721    0.40474054    0.29676521    0.60272712    0.19402926]][0m
[37m[1m[2023-07-10 14:41:11,579][227910] Max Reward on eval: 1031.574427212162[0m
[37m[1m[2023-07-10 14:41:11,579][227910] Min Reward on eval: 1031.574427212162[0m
[37m[1m[2023-07-10 14:41:11,579][227910] Mean Reward across all agents: 1031.574427212162[0m
[37m[1m[2023-07-10 14:41:11,579][227910] Average Trajectory Length: 999.448[0m
[36m[2023-07-10 14:41:17,047][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:41:17,047][227910] Reward + Measures: [[-234.70354899    0.44086471    0.4009923     0.51081991    0.22717419]
 [ 781.19315073    0.33189997    0.44320002    0.57480001    0.36210001]
 [ 931.02506012    0.41529998    0.35799998    0.61390001    0.1531    ]
 ...
 [ 548.59321193    0.28920004    0.5115        0.51809996    0.41490003]
 [-914.74162175    0.49366474    0.60705984    0.14909808    0.52219558]
 [  51.56013537    0.38000003    0.40459999    0.4506        0.27149999]][0m
[37m[1m[2023-07-10 14:41:17,047][227910] Max Reward on eval: 983.6190694966702[0m
[37m[1m[2023-07-10 14:41:17,048][227910] Min Reward on eval: -1413.5789708604339[0m
[37m[1m[2023-07-10 14:41:17,048][227910] Mean Reward across all agents: -134.31762790588843[0m
[37m[1m[2023-07-10 14:41:17,048][227910] Average Trajectory Length: 936.8276666666667[0m
[36m[2023-07-10 14:41:17,050][227910] mean_value=-1102.2418534990272, max_value=1220.3427421242254[0m
[37m[1m[2023-07-10 14:41:17,053][227910] New mean coefficients: [[ 4.2140517   1.0730661   0.57440513  0.3854853  -0.08981862]][0m
[37m[1m[2023-07-10 14:41:17,054][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:41:26,665][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 14:41:26,665][227910] FPS: 399614.56[0m
[36m[2023-07-10 14:41:26,667][227910] itr=630, itrs=2000, Progress: 31.50%[0m
[37m[1m[2023-07-10 14:41:29,804][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000610[0m
[36m[2023-07-10 14:41:41,672][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 14:41:41,672][227910] FPS: 330990.19[0m
[36m[2023-07-10 14:41:46,511][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:41:46,511][227910] Reward + Measures: [[1213.55030272    0.37021363    0.28700972    0.57819957    0.18348767]][0m
[37m[1m[2023-07-10 14:41:46,511][227910] Max Reward on eval: 1213.5503027228635[0m
[37m[1m[2023-07-10 14:41:46,511][227910] Min Reward on eval: 1213.5503027228635[0m
[37m[1m[2023-07-10 14:41:46,511][227910] Mean Reward across all agents: 1213.5503027228635[0m
[37m[1m[2023-07-10 14:41:46,512][227910] Average Trajectory Length: 999.134[0m
[36m[2023-07-10 14:41:51,992][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:41:52,013][227910] Reward + Measures: [[ 452.01963347    0.52520001    0.29650003    0.63260001    0.1558    ]
 [ 366.67677359    0.542         0.35950002    0.59680003    0.1103    ]
 [1194.60719498    0.30939999    0.31089997    0.58129996    0.20749998]
 ...
 [ 559.65514247    0.31780002    0.33180001    0.43800002    0.2069    ]
 [ 162.62586479    0.46760002    0.27849999    0.50360006    0.1134    ]
 [ 449.59364206    0.33720002    0.34819999    0.45120001    0.28120002]][0m
[37m[1m[2023-07-10 14:41:52,014][227910] Max Reward on eval: 1548.7597024344373[0m
[37m[1m[2023-07-10 14:41:52,015][227910] Min Reward on eval: -743.1235207139631[0m
[37m[1m[2023-07-10 14:41:52,015][227910] Mean Reward across all agents: 516.232022933647[0m
[37m[1m[2023-07-10 14:41:52,016][227910] Average Trajectory Length: 984.9383333333333[0m
[36m[2023-07-10 14:41:52,023][227910] mean_value=-498.47236652148666, max_value=626.915721776227[0m
[37m[1m[2023-07-10 14:41:52,029][227910] New mean coefficients: [[3.5032647  0.831704   0.60312504 1.3356929  0.15933362]][0m
[37m[1m[2023-07-10 14:41:52,031][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:42:01,732][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 14:42:01,732][227910] FPS: 395928.63[0m
[36m[2023-07-10 14:42:01,735][227910] itr=631, itrs=2000, Progress: 31.55%[0m
[36m[2023-07-10 14:42:13,251][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 14:42:13,251][227910] FPS: 334013.45[0m
[36m[2023-07-10 14:42:18,010][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:42:18,011][227910] Reward + Measures: [[1324.23511254    0.35667467    0.27956757    0.57471615    0.17510095]][0m
[37m[1m[2023-07-10 14:42:18,011][227910] Max Reward on eval: 1324.235112540043[0m
[37m[1m[2023-07-10 14:42:18,011][227910] Min Reward on eval: 1324.235112540043[0m
[37m[1m[2023-07-10 14:42:18,012][227910] Mean Reward across all agents: 1324.235112540043[0m
[37m[1m[2023-07-10 14:42:18,012][227910] Average Trajectory Length: 999.7703333333333[0m
[36m[2023-07-10 14:42:23,599][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:42:23,599][227910] Reward + Measures: [[1199.51241526    0.37470001    0.266         0.61199999    0.1583    ]
 [ 693.30270617    0.28270003    0.40310001    0.51570004    0.33839998]
 [1328.71664298    0.34130001    0.2615        0.57130003    0.1787    ]
 ...
 [1118.40834909    0.32619998    0.31300002    0.55190003    0.1937    ]
 [ 268.92880304    0.25420001    0.3831        0.43200001    0.31240001]
 [1212.41490353    0.36719999    0.28380001    0.57520002    0.15450001]][0m
[37m[1m[2023-07-10 14:42:23,599][227910] Max Reward on eval: 1450.2323817528086[0m
[37m[1m[2023-07-10 14:42:23,600][227910] Min Reward on eval: -789.0813192944624[0m
[37m[1m[2023-07-10 14:42:23,600][227910] Mean Reward across all agents: 761.5829385342711[0m
[37m[1m[2023-07-10 14:42:23,600][227910] Average Trajectory Length: 986.1486666666666[0m
[36m[2023-07-10 14:42:23,603][227910] mean_value=-499.17889902302335, max_value=699.4199642889125[0m
[37m[1m[2023-07-10 14:42:23,606][227910] New mean coefficients: [[1.9302833  0.84156257 0.695028   0.0999285  0.11239411]][0m
[37m[1m[2023-07-10 14:42:23,607][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:42:33,406][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 14:42:33,406][227910] FPS: 391941.83[0m
[36m[2023-07-10 14:42:33,408][227910] itr=632, itrs=2000, Progress: 31.60%[0m
[36m[2023-07-10 14:42:44,994][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 14:42:44,995][227910] FPS: 331901.82[0m
[36m[2023-07-10 14:42:49,792][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:42:49,792][227910] Reward + Measures: [[1509.35658558    0.28106415    0.31657344    0.48550403    0.14037538]][0m
[37m[1m[2023-07-10 14:42:49,792][227910] Max Reward on eval: 1509.3565855751062[0m
[37m[1m[2023-07-10 14:42:49,793][227910] Min Reward on eval: 1509.3565855751062[0m
[37m[1m[2023-07-10 14:42:49,793][227910] Mean Reward across all agents: 1509.3565855751062[0m
[37m[1m[2023-07-10 14:42:49,793][227910] Average Trajectory Length: 999.1816666666666[0m
[36m[2023-07-10 14:42:55,244][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:42:55,244][227910] Reward + Measures: [[ 433.96596717    0.33460003    0.31530002    0.4887        0.1444    ]
 [ 635.47442889    0.42629996    0.23380001    0.42989999    0.17870001]
 [ 273.6827074     0.38889998    0.2757        0.43470001    0.22360002]
 ...
 [1572.83138598    0.2956        0.31979999    0.50190002    0.14660001]
 [ 258.0967421     0.39739999    0.32880002    0.55110002    0.1521    ]
 [ 874.58637494    0.36530003    0.32860002    0.43169999    0.15000001]][0m
[37m[1m[2023-07-10 14:42:55,244][227910] Max Reward on eval: 1572.8313859828982[0m
[37m[1m[2023-07-10 14:42:55,245][227910] Min Reward on eval: -460.1473717082292[0m
[37m[1m[2023-07-10 14:42:55,245][227910] Mean Reward across all agents: 902.4912694247001[0m
[37m[1m[2023-07-10 14:42:55,245][227910] Average Trajectory Length: 995.563[0m
[36m[2023-07-10 14:42:55,248][227910] mean_value=-784.8260273404342, max_value=713.7002429070899[0m
[37m[1m[2023-07-10 14:42:55,250][227910] New mean coefficients: [[1.0769569  0.87149674 0.61843026 0.49232763 0.58517146]][0m
[37m[1m[2023-07-10 14:42:55,251][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:43:05,202][227910] train() took 9.95 seconds to complete[0m
[36m[2023-07-10 14:43:05,202][227910] FPS: 385977.11[0m
[36m[2023-07-10 14:43:05,204][227910] itr=633, itrs=2000, Progress: 31.65%[0m
[36m[2023-07-10 14:43:16,847][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 14:43:16,847][227910] FPS: 330380.03[0m
[36m[2023-07-10 14:43:21,620][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:43:21,620][227910] Reward + Measures: [[1648.23120273    0.27432668    0.31858757    0.4845362     0.14150207]][0m
[37m[1m[2023-07-10 14:43:21,621][227910] Max Reward on eval: 1648.23120272748[0m
[37m[1m[2023-07-10 14:43:21,621][227910] Min Reward on eval: 1648.23120272748[0m
[37m[1m[2023-07-10 14:43:21,621][227910] Mean Reward across all agents: 1648.23120272748[0m
[37m[1m[2023-07-10 14:43:21,621][227910] Average Trajectory Length: 999.3273333333333[0m
[36m[2023-07-10 14:43:27,296][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:43:27,297][227910] Reward + Measures: [[1267.93911174    0.44029999    0.42079997    0.55470008    0.32100001]
 [1189.52946784    0.3633        0.35069999    0.47799999    0.25369999]
 [1510.09366651    0.3134        0.31800002    0.49790001    0.1596    ]
 ...
 [1621.07006014    0.29840001    0.31510001    0.51969999    0.1434    ]
 [1579.83029314    0.29970002    0.30679998    0.52020001    0.1506    ]
 [1438.23365771    0.29390001    0.28650001    0.47860003    0.1706    ]][0m
[37m[1m[2023-07-10 14:43:27,297][227910] Max Reward on eval: 1737.2249987208284[0m
[37m[1m[2023-07-10 14:43:27,297][227910] Min Reward on eval: 413.81009729692016[0m
[37m[1m[2023-07-10 14:43:27,297][227910] Mean Reward across all agents: 1402.2756056426977[0m
[37m[1m[2023-07-10 14:43:27,298][227910] Average Trajectory Length: 998.9533333333333[0m
[36m[2023-07-10 14:43:27,301][227910] mean_value=-405.74414860023666, max_value=880.9338380314945[0m
[37m[1m[2023-07-10 14:43:27,304][227910] New mean coefficients: [[ 2.7842617   1.4209833   0.92325264 -0.15195161  0.70000494]][0m
[37m[1m[2023-07-10 14:43:27,305][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:43:36,939][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 14:43:36,940][227910] FPS: 398623.25[0m
[36m[2023-07-10 14:43:36,942][227910] itr=634, itrs=2000, Progress: 31.70%[0m
[36m[2023-07-10 14:43:48,571][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 14:43:48,572][227910] FPS: 330741.75[0m
[36m[2023-07-10 14:43:53,389][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:43:53,389][227910] Reward + Measures: [[1763.91151811    0.27260286    0.32114205    0.48809704    0.13963787]][0m
[37m[1m[2023-07-10 14:43:53,389][227910] Max Reward on eval: 1763.9115181135423[0m
[37m[1m[2023-07-10 14:43:53,390][227910] Min Reward on eval: 1763.9115181135423[0m
[37m[1m[2023-07-10 14:43:53,390][227910] Mean Reward across all agents: 1763.9115181135423[0m
[37m[1m[2023-07-10 14:43:53,390][227910] Average Trajectory Length: 999.1859999999999[0m
[36m[2023-07-10 14:43:58,866][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:43:58,867][227910] Reward + Measures: [[ 603.52974072    0.59370005    0.68909997    0.70480001    0.55740005]
 [ 544.80415394    0.38929999    0.33820003    0.42160001    0.30689999]
 [1306.97374326    0.38920003    0.43220001    0.6821        0.2217    ]
 ...
 [ 472.03658461    0.47709998    0.55039996    0.49390003    0.43129998]
 [ 540.77531714    0.47270003    0.51560003    0.51730007    0.4075    ]
 [ 400.06353966    0.45145717    0.43544289    0.48618576    0.37945715]][0m
[37m[1m[2023-07-10 14:43:58,867][227910] Max Reward on eval: 1953.5673539893469[0m
[37m[1m[2023-07-10 14:43:58,867][227910] Min Reward on eval: 67.96460851559532[0m
[37m[1m[2023-07-10 14:43:58,868][227910] Mean Reward across all agents: 1137.9870124614556[0m
[37m[1m[2023-07-10 14:43:58,868][227910] Average Trajectory Length: 998.3173333333333[0m
[36m[2023-07-10 14:43:58,872][227910] mean_value=-66.47343351491625, max_value=1648.0937809180446[0m
[37m[1m[2023-07-10 14:43:58,875][227910] New mean coefficients: [[2.0107872  1.311263   1.0044489  0.61103576 0.7965465 ]][0m
[37m[1m[2023-07-10 14:43:58,876][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:44:08,758][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 14:44:08,759][227910] FPS: 388650.31[0m
[36m[2023-07-10 14:44:08,761][227910] itr=635, itrs=2000, Progress: 31.75%[0m
[36m[2023-07-10 14:44:20,367][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 14:44:20,367][227910] FPS: 331348.13[0m
[36m[2023-07-10 14:44:25,039][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:44:25,039][227910] Reward + Measures: [[1852.12078168    0.27028275    0.32506776    0.4855082     0.14084606]][0m
[37m[1m[2023-07-10 14:44:25,040][227910] Max Reward on eval: 1852.1207816810354[0m
[37m[1m[2023-07-10 14:44:25,040][227910] Min Reward on eval: 1852.1207816810354[0m
[37m[1m[2023-07-10 14:44:25,040][227910] Mean Reward across all agents: 1852.1207816810354[0m
[37m[1m[2023-07-10 14:44:25,040][227910] Average Trajectory Length: 999.454[0m
[36m[2023-07-10 14:44:30,495][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:44:30,495][227910] Reward + Measures: [[1705.60073521    0.27500001    0.2899        0.53319997    0.1358    ]
 [1882.94982899    0.27410001    0.28050002    0.53050005    0.1453    ]
 [1762.93733972    0.2683        0.29460001    0.50530005    0.13010001]
 ...
 [1621.90377544    0.2755        0.27680001    0.55159998    0.13079999]
 [1368.05740092    0.28850004    0.26869997    0.53900003    0.2088    ]
 [1618.60310514    0.2705        0.26409999    0.59160006    0.1362    ]][0m
[37m[1m[2023-07-10 14:44:30,495][227910] Max Reward on eval: 1935.5555903580273[0m
[37m[1m[2023-07-10 14:44:30,496][227910] Min Reward on eval: 817.5587415770453[0m
[37m[1m[2023-07-10 14:44:30,496][227910] Mean Reward across all agents: 1620.8704067193403[0m
[37m[1m[2023-07-10 14:44:30,496][227910] Average Trajectory Length: 997.9096666666667[0m
[36m[2023-07-10 14:44:30,498][227910] mean_value=-864.1623684077995, max_value=870.2306034374774[0m
[37m[1m[2023-07-10 14:44:30,500][227910] New mean coefficients: [[0.700235  1.247876  0.5630424 0.9033401 1.2142646]][0m
[37m[1m[2023-07-10 14:44:30,501][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:44:40,186][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 14:44:40,186][227910] FPS: 396563.52[0m
[36m[2023-07-10 14:44:40,189][227910] itr=636, itrs=2000, Progress: 31.80%[0m
[36m[2023-07-10 14:44:51,673][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 14:44:51,674][227910] FPS: 334866.90[0m
[36m[2023-07-10 14:44:56,455][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:44:56,455][227910] Reward + Measures: [[1925.25028054    0.27101085    0.32893783    0.48934355    0.13773863]][0m
[37m[1m[2023-07-10 14:44:56,455][227910] Max Reward on eval: 1925.25028053673[0m
[37m[1m[2023-07-10 14:44:56,456][227910] Min Reward on eval: 1925.25028053673[0m
[37m[1m[2023-07-10 14:44:56,456][227910] Mean Reward across all agents: 1925.25028053673[0m
[37m[1m[2023-07-10 14:44:56,456][227910] Average Trajectory Length: 999.283[0m
[36m[2023-07-10 14:45:01,868][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:45:01,869][227910] Reward + Measures: [[2054.9462369     0.2775        0.29449999    0.588         0.1462    ]
 [1564.56155862    0.27430001    0.3145        0.49829999    0.2027    ]
 [1600.7763481     0.27860004    0.3152        0.54150003    0.18959999]
 ...
 [1513.5371467     0.3017        0.30039999    0.50500005    0.21780001]
 [ 975.54861393    0.30147281    0.30837846    0.45601201    0.23264897]
 [1568.59844063    0.31659999    0.29860002    0.55629998    0.20819998]][0m
[37m[1m[2023-07-10 14:45:01,869][227910] Max Reward on eval: 2079.6558933064343[0m
[37m[1m[2023-07-10 14:45:01,869][227910] Min Reward on eval: 599.2868357680039[0m
[37m[1m[2023-07-10 14:45:01,870][227910] Mean Reward across all agents: 1555.3294101140539[0m
[37m[1m[2023-07-10 14:45:01,870][227910] Average Trajectory Length: 995.0683333333333[0m
[36m[2023-07-10 14:45:01,873][227910] mean_value=-292.881286492366, max_value=984.0274866185869[0m
[37m[1m[2023-07-10 14:45:01,876][227910] New mean coefficients: [[1.0945761  1.2889531  0.48017153 0.9727357  1.1100513 ]][0m
[37m[1m[2023-07-10 14:45:01,877][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:45:11,654][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 14:45:11,654][227910] FPS: 392808.22[0m
[36m[2023-07-10 14:45:11,657][227910] itr=637, itrs=2000, Progress: 31.85%[0m
[36m[2023-07-10 14:45:23,218][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 14:45:23,218][227910] FPS: 332698.13[0m
[36m[2023-07-10 14:45:28,050][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:45:28,051][227910] Reward + Measures: [[2019.7119115     0.2725195     0.328677      0.49414623    0.13661127]][0m
[37m[1m[2023-07-10 14:45:28,051][227910] Max Reward on eval: 2019.711911495221[0m
[37m[1m[2023-07-10 14:45:28,051][227910] Min Reward on eval: 2019.711911495221[0m
[37m[1m[2023-07-10 14:45:28,051][227910] Mean Reward across all agents: 2019.711911495221[0m
[37m[1m[2023-07-10 14:45:28,052][227910] Average Trajectory Length: 998.9863333333333[0m
[36m[2023-07-10 14:45:33,606][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:45:33,607][227910] Reward + Measures: [[ 246.79687038    0.45640001    0.18100001    0.45910001    0.152     ]
 [ 295.43853925    0.3369        0.30739999    0.44190001    0.2089    ]
 [ 299.9097934     0.50480002    0.24260001    0.46039996    0.178     ]
 ...
 [ 412.92795316    0.38069999    0.26659998    0.44489995    0.23049998]
 [ 242.6376735     0.3955        0.2306        0.37220001    0.28809997]
 [-485.85928965    0.38009998    0.20410001    0.33810002    0.2599    ]][0m
[37m[1m[2023-07-10 14:45:33,607][227910] Max Reward on eval: 1853.4468000840163[0m
[37m[1m[2023-07-10 14:45:33,607][227910] Min Reward on eval: -957.4128684230615[0m
[37m[1m[2023-07-10 14:45:33,608][227910] Mean Reward across all agents: 343.03593109112796[0m
[37m[1m[2023-07-10 14:45:33,608][227910] Average Trajectory Length: 992.5949999999999[0m
[36m[2023-07-10 14:45:33,610][227910] mean_value=-647.9922033829571, max_value=1248.0578574638307[0m
[37m[1m[2023-07-10 14:45:33,613][227910] New mean coefficients: [[2.2731667  1.3470825  0.35312137 1.7546442  1.1621035 ]][0m
[37m[1m[2023-07-10 14:45:33,614][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:45:43,566][227910] train() took 9.95 seconds to complete[0m
[36m[2023-07-10 14:45:43,566][227910] FPS: 385915.87[0m
[36m[2023-07-10 14:45:43,568][227910] itr=638, itrs=2000, Progress: 31.90%[0m
[36m[2023-07-10 14:45:55,177][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 14:45:55,177][227910] FPS: 331271.67[0m
[36m[2023-07-10 14:45:59,944][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:45:59,944][227910] Reward + Measures: [[2122.39254546    0.27174702    0.32806733    0.49574494    0.13244213]][0m
[37m[1m[2023-07-10 14:45:59,944][227910] Max Reward on eval: 2122.3925454594855[0m
[37m[1m[2023-07-10 14:45:59,945][227910] Min Reward on eval: 2122.3925454594855[0m
[37m[1m[2023-07-10 14:45:59,945][227910] Mean Reward across all agents: 2122.3925454594855[0m
[37m[1m[2023-07-10 14:45:59,945][227910] Average Trajectory Length: 999.8533333333334[0m
[36m[2023-07-10 14:46:05,580][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:46:05,581][227910] Reward + Measures: [[1529.72800511    0.2719        0.28930002    0.42840004    0.14389999]
 [-193.03679359    0.24267094    0.33845529    0.31378981    0.09543707]
 [-737.27746676    0.317         0.31099999    0.36770001    0.0551    ]
 ...
 [-962.96067112    0.46363011    0.48144838    0.52405351    0.0323338 ]
 [-239.64285024    0.26662433    0.33883604    0.4000946     0.07193514]
 [2038.09371615    0.2863        0.3096        0.51710004    0.13930002]][0m
[37m[1m[2023-07-10 14:46:05,581][227910] Max Reward on eval: 2051.492725236676[0m
[37m[1m[2023-07-10 14:46:05,581][227910] Min Reward on eval: -962.960671116563[0m
[37m[1m[2023-07-10 14:46:05,582][227910] Mean Reward across all agents: 958.5679474947765[0m
[37m[1m[2023-07-10 14:46:05,582][227910] Average Trajectory Length: 971.2856666666667[0m
[36m[2023-07-10 14:46:05,583][227910] mean_value=-1694.7144781150464, max_value=371.6041424376022[0m
[37m[1m[2023-07-10 14:46:05,586][227910] New mean coefficients: [[2.333955   0.98039836 0.3433923  0.8397534  1.3412012 ]][0m
[37m[1m[2023-07-10 14:46:05,587][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:46:15,110][227910] train() took 9.52 seconds to complete[0m
[36m[2023-07-10 14:46:15,111][227910] FPS: 403264.94[0m
[36m[2023-07-10 14:46:15,113][227910] itr=639, itrs=2000, Progress: 31.95%[0m
[36m[2023-07-10 14:46:26,577][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 14:46:26,578][227910] FPS: 335427.29[0m
[36m[2023-07-10 14:46:31,407][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:46:31,407][227910] Reward + Measures: [[2206.06859976    0.27105954    0.32441723    0.49830988    0.13134439]][0m
[37m[1m[2023-07-10 14:46:31,408][227910] Max Reward on eval: 2206.0685997628793[0m
[37m[1m[2023-07-10 14:46:31,408][227910] Min Reward on eval: 2206.0685997628793[0m
[37m[1m[2023-07-10 14:46:31,408][227910] Mean Reward across all agents: 2206.0685997628793[0m
[37m[1m[2023-07-10 14:46:31,408][227910] Average Trajectory Length: 999.097[0m
[36m[2023-07-10 14:46:36,843][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:46:36,844][227910] Reward + Measures: [[1850.47821246    0.2554        0.28329998    0.48639998    0.1402    ]
 [2135.32474285    0.28360003    0.3256        0.48460004    0.13389999]
 [2127.97360837    0.2746        0.31799999    0.51590008    0.1329    ]
 ...
 [1983.420429      0.27760002    0.31999999    0.52770001    0.12710001]
 [1677.21671961    0.25760001    0.34300002    0.55610007    0.1479    ]
 [2087.95783506    0.27869999    0.32589999    0.52990001    0.13340001]][0m
[37m[1m[2023-07-10 14:46:36,844][227910] Max Reward on eval: 2258.9644999268467[0m
[37m[1m[2023-07-10 14:46:36,844][227910] Min Reward on eval: 1541.377003523917[0m
[37m[1m[2023-07-10 14:46:36,845][227910] Mean Reward across all agents: 2040.294353593962[0m
[37m[1m[2023-07-10 14:46:36,845][227910] Average Trajectory Length: 998.5676666666666[0m
[36m[2023-07-10 14:46:36,847][227910] mean_value=-349.96228245367894, max_value=585.068332090359[0m
[37m[1m[2023-07-10 14:46:36,849][227910] New mean coefficients: [[1.8723938  0.83512175 0.10836765 1.8314426  1.320325  ]][0m
[37m[1m[2023-07-10 14:46:36,850][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:46:46,500][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 14:46:46,500][227910] FPS: 397988.68[0m
[36m[2023-07-10 14:46:46,502][227910] itr=640, itrs=2000, Progress: 32.00%[0m
[37m[1m[2023-07-10 14:46:49,535][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000620[0m
[36m[2023-07-10 14:47:01,381][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 14:47:01,382][227910] FPS: 331665.68[0m
[36m[2023-07-10 14:47:06,124][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:47:06,125][227910] Reward + Measures: [[2293.18147799    0.27455351    0.32219958    0.49620467    0.12521239]][0m
[37m[1m[2023-07-10 14:47:06,125][227910] Max Reward on eval: 2293.181477986903[0m
[37m[1m[2023-07-10 14:47:06,125][227910] Min Reward on eval: 2293.181477986903[0m
[37m[1m[2023-07-10 14:47:06,125][227910] Mean Reward across all agents: 2293.181477986903[0m
[37m[1m[2023-07-10 14:47:06,126][227910] Average Trajectory Length: 999.4023333333333[0m
[36m[2023-07-10 14:47:11,392][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:47:11,443][227910] Reward + Measures: [[ 407.59807686    0.38509998    0.32570001    0.4745        0.2967    ]
 [1276.55092394    0.44300005    0.40450001    0.66930002    0.3195    ]
 [ 907.07910263    0.32090002    0.25079998    0.54700005    0.20969999]
 ...
 [  68.76395883    0.4039889     0.32360864    0.39957163    0.31108767]
 [1535.15634028    0.31599998    0.31329998    0.52680004    0.17639999]
 [ 825.6198018     0.29560003    0.2863        0.47620001    0.2211    ]][0m
[37m[1m[2023-07-10 14:47:11,444][227910] Max Reward on eval: 2346.107808754966[0m
[37m[1m[2023-07-10 14:47:11,444][227910] Min Reward on eval: -120.5216973989969[0m
[37m[1m[2023-07-10 14:47:11,444][227910] Mean Reward across all agents: 1272.1904119893215[0m
[37m[1m[2023-07-10 14:47:11,445][227910] Average Trajectory Length: 983.2729999999999[0m
[36m[2023-07-10 14:47:11,448][227910] mean_value=-601.0342112418526, max_value=1576.7726435747572[0m
[37m[1m[2023-07-10 14:47:11,450][227910] New mean coefficients: [[2.3748784  0.39125496 0.69655395 2.076705   1.6689115 ]][0m
[37m[1m[2023-07-10 14:47:11,451][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:47:21,260][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 14:47:21,260][227910] FPS: 391558.85[0m
[36m[2023-07-10 14:47:21,263][227910] itr=641, itrs=2000, Progress: 32.05%[0m
[36m[2023-07-10 14:47:32,840][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 14:47:32,840][227910] FPS: 332173.97[0m
[36m[2023-07-10 14:47:37,608][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:47:37,609][227910] Reward + Measures: [[2357.55398109    0.27246776    0.31922728    0.50372028    0.12706107]][0m
[37m[1m[2023-07-10 14:47:37,609][227910] Max Reward on eval: 2357.5539810863343[0m
[37m[1m[2023-07-10 14:47:37,609][227910] Min Reward on eval: 2357.5539810863343[0m
[37m[1m[2023-07-10 14:47:37,609][227910] Mean Reward across all agents: 2357.5539810863343[0m
[37m[1m[2023-07-10 14:47:37,610][227910] Average Trajectory Length: 998.8693333333333[0m
[36m[2023-07-10 14:47:43,270][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:47:43,276][227910] Reward + Measures: [[-461.29519396    0.27374387    0.2345579     0.31572458    0.10002808]
 [  44.83540531    0.32090002    0.33920002    0.4082        0.2307    ]
 [-180.17175459    0.29700002    0.26089999    0.32620001    0.1944    ]
 ...
 [1532.08550807    0.26736733    0.24877451    0.47150525    0.12390196]
 [ 169.06937862    0.43760005    0.45580003    0.50550002    0.40149999]
 [1185.36648277    0.37090001    0.37470001    0.51610005    0.2563    ]][0m
[37m[1m[2023-07-10 14:47:43,276][227910] Max Reward on eval: 2006.523790280579[0m
[37m[1m[2023-07-10 14:47:43,277][227910] Min Reward on eval: -1255.3860676999611[0m
[37m[1m[2023-07-10 14:47:43,277][227910] Mean Reward across all agents: 58.311538961666145[0m
[37m[1m[2023-07-10 14:47:43,277][227910] Average Trajectory Length: 867.1613333333333[0m
[36m[2023-07-10 14:47:43,279][227910] mean_value=-2885.8431672434, max_value=789.6251866310495[0m
[37m[1m[2023-07-10 14:47:43,281][227910] New mean coefficients: [[2.0111756 0.3174542 0.5216078 1.8416269 0.8634794]][0m
[37m[1m[2023-07-10 14:47:43,283][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:47:53,019][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 14:47:53,019][227910] FPS: 394461.35[0m
[36m[2023-07-10 14:47:53,022][227910] itr=642, itrs=2000, Progress: 32.10%[0m
[36m[2023-07-10 14:48:04,536][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 14:48:04,536][227910] FPS: 334019.02[0m
[36m[2023-07-10 14:48:09,358][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:48:09,359][227910] Reward + Measures: [[2446.75619964    0.27036121    0.32246432    0.50856441    0.12123757]][0m
[37m[1m[2023-07-10 14:48:09,359][227910] Max Reward on eval: 2446.7561996444583[0m
[37m[1m[2023-07-10 14:48:09,359][227910] Min Reward on eval: 2446.7561996444583[0m
[37m[1m[2023-07-10 14:48:09,359][227910] Mean Reward across all agents: 2446.7561996444583[0m
[37m[1m[2023-07-10 14:48:09,359][227910] Average Trajectory Length: 998.4939999999999[0m
[36m[2023-07-10 14:48:14,793][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:48:14,794][227910] Reward + Measures: [[-369.28547112    0.24864125    0.22970095    0.27948552    0.2860665 ]
 [ 713.92623307    0.25279999    0.28009999    0.66189998    0.2809    ]
 [ 905.88812415    0.30370003    0.25490001    0.61759996    0.1772    ]
 ...
 [1572.25788121    0.2703        0.2897        0.66280001    0.1479    ]
 [1185.4824162     0.27789998    0.25580001    0.62370002    0.25580001]
 [ 599.71941352    0.28259999    0.17639999    0.65380001    0.1763    ]][0m
[37m[1m[2023-07-10 14:48:14,794][227910] Max Reward on eval: 2473.892106866371[0m
[37m[1m[2023-07-10 14:48:14,794][227910] Min Reward on eval: -661.881185138924[0m
[37m[1m[2023-07-10 14:48:14,795][227910] Mean Reward across all agents: 1013.7141166778184[0m
[37m[1m[2023-07-10 14:48:14,795][227910] Average Trajectory Length: 990.717[0m
[36m[2023-07-10 14:48:14,799][227910] mean_value=-174.2825274451657, max_value=2323.766734627215[0m
[37m[1m[2023-07-10 14:48:14,801][227910] New mean coefficients: [[1.7369483  0.80453813 0.2633008  2.4619398  0.96062213]][0m
[37m[1m[2023-07-10 14:48:14,802][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:48:24,565][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 14:48:24,565][227910] FPS: 393393.04[0m
[36m[2023-07-10 14:48:24,568][227910] itr=643, itrs=2000, Progress: 32.15%[0m
[36m[2023-07-10 14:48:36,193][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 14:48:36,194][227910] FPS: 330766.28[0m
[36m[2023-07-10 14:48:41,031][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:48:41,031][227910] Reward + Measures: [[2527.33369158    0.26998067    0.32081869    0.51455581    0.11823571]][0m
[37m[1m[2023-07-10 14:48:41,031][227910] Max Reward on eval: 2527.333691578771[0m
[37m[1m[2023-07-10 14:48:41,032][227910] Min Reward on eval: 2527.333691578771[0m
[37m[1m[2023-07-10 14:48:41,032][227910] Mean Reward across all agents: 2527.333691578771[0m
[37m[1m[2023-07-10 14:48:41,032][227910] Average Trajectory Length: 997.7466666666667[0m
[36m[2023-07-10 14:48:46,574][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:48:46,574][227910] Reward + Measures: [[1802.20433425    0.2766        0.30840001    0.50000006    0.19560002]
 [ -12.49949549    0.26993334    0.29916665    0.33736667    0.26416665]
 [ 379.69111987    0.28180003    0.34629998    0.40550002    0.33769998]
 ...
 [ 896.3771742     0.28080001    0.4021        0.53490001    0.33170003]
 [ 582.66588031    0.2665        0.41980001    0.51899999    0.45390001]
 [ 480.47939997    0.32519412    0.30605492    0.40337649    0.29657254]][0m
[37m[1m[2023-07-10 14:48:46,575][227910] Max Reward on eval: 2379.20770876006[0m
[37m[1m[2023-07-10 14:48:46,575][227910] Min Reward on eval: -773.2667142317514[0m
[37m[1m[2023-07-10 14:48:46,575][227910] Mean Reward across all agents: 650.5139486712507[0m
[37m[1m[2023-07-10 14:48:46,575][227910] Average Trajectory Length: 981.3996666666667[0m
[36m[2023-07-10 14:48:46,578][227910] mean_value=-840.2507977011267, max_value=1385.1894776237664[0m
[37m[1m[2023-07-10 14:48:46,581][227910] New mean coefficients: [[ 1.3573718   0.49273744 -0.4676669   2.721377    0.71665376]][0m
[37m[1m[2023-07-10 14:48:46,582][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:48:56,518][227910] train() took 9.94 seconds to complete[0m
[36m[2023-07-10 14:48:56,519][227910] FPS: 386502.81[0m
[36m[2023-07-10 14:48:56,521][227910] itr=644, itrs=2000, Progress: 32.20%[0m
[36m[2023-07-10 14:49:08,233][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 14:49:08,233][227910] FPS: 328418.24[0m
[36m[2023-07-10 14:49:13,019][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:49:13,020][227910] Reward + Measures: [[2607.2488439     0.27343652    0.31706318    0.51627642    0.11292005]][0m
[37m[1m[2023-07-10 14:49:13,020][227910] Max Reward on eval: 2607.248843904945[0m
[37m[1m[2023-07-10 14:49:13,020][227910] Min Reward on eval: 2607.248843904945[0m
[37m[1m[2023-07-10 14:49:13,020][227910] Mean Reward across all agents: 2607.248843904945[0m
[37m[1m[2023-07-10 14:49:13,021][227910] Average Trajectory Length: 997.5986666666666[0m
[36m[2023-07-10 14:49:18,437][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:49:18,437][227910] Reward + Measures: [[1612.43101166    0.29230002    0.3258        0.51590008    0.21890001]
 [2178.78698069    0.2696        0.28140002    0.58770001    0.16850002]
 [1307.16205802    0.29429999    0.3488        0.55790007    0.2419    ]
 ...
 [ 331.93341127    0.32076573    0.33714315    0.38663018    0.24499094]
 [1432.98549006    0.30440003    0.39760002    0.55039996    0.3019    ]
 [1134.86321918    0.31850001    0.40260002    0.58700001    0.27959999]][0m
[37m[1m[2023-07-10 14:49:18,438][227910] Max Reward on eval: 2466.607650414342[0m
[37m[1m[2023-07-10 14:49:18,438][227910] Min Reward on eval: -403.84195121335796[0m
[37m[1m[2023-07-10 14:49:18,438][227910] Mean Reward across all agents: 1219.397676139261[0m
[37m[1m[2023-07-10 14:49:18,438][227910] Average Trajectory Length: 984.4293333333333[0m
[36m[2023-07-10 14:49:18,441][227910] mean_value=-448.31713605873625, max_value=1288.0729634368677[0m
[37m[1m[2023-07-10 14:49:18,443][227910] New mean coefficients: [[ 0.86835945  0.620901   -0.24797212  3.429857    0.523933  ]][0m
[37m[1m[2023-07-10 14:49:18,444][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:49:28,175][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 14:49:28,175][227910] FPS: 394702.55[0m
[36m[2023-07-10 14:49:28,177][227910] itr=645, itrs=2000, Progress: 32.25%[0m
[36m[2023-07-10 14:49:39,811][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 14:49:39,811][227910] FPS: 330543.15[0m
[36m[2023-07-10 14:49:44,585][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:49:44,586][227910] Reward + Measures: [[2678.79215957    0.27459681    0.31292477    0.53503042    0.11050096]][0m
[37m[1m[2023-07-10 14:49:44,586][227910] Max Reward on eval: 2678.792159569099[0m
[37m[1m[2023-07-10 14:49:44,586][227910] Min Reward on eval: 2678.792159569099[0m
[37m[1m[2023-07-10 14:49:44,586][227910] Mean Reward across all agents: 2678.792159569099[0m
[37m[1m[2023-07-10 14:49:44,586][227910] Average Trajectory Length: 998.9373333333333[0m
[36m[2023-07-10 14:49:49,938][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:49:49,939][227910] Reward + Measures: [[1135.32938846    0.33720002    0.37890002    0.57440001    0.28490001]
 [ 656.5329301     0.30460003    0.24170001    0.40710002    0.2253    ]
 [ 453.1399553     0.27177054    0.33001646    0.39871898    0.14403532]
 ...
 [1518.48250925    0.32190001    0.32650003    0.53260005    0.2264    ]
 [2101.88388431    0.29480001    0.3973        0.51740003    0.1671    ]
 [2467.10158741    0.273         0.273         0.53720003    0.1204    ]][0m
[37m[1m[2023-07-10 14:49:49,939][227910] Max Reward on eval: 2647.7218328316694[0m
[37m[1m[2023-07-10 14:49:49,939][227910] Min Reward on eval: 95.23736213366792[0m
[37m[1m[2023-07-10 14:49:49,940][227910] Mean Reward across all agents: 1459.745812879528[0m
[37m[1m[2023-07-10 14:49:49,940][227910] Average Trajectory Length: 978.9606666666666[0m
[36m[2023-07-10 14:49:49,943][227910] mean_value=-413.287955884924, max_value=1245.1297345353917[0m
[37m[1m[2023-07-10 14:49:49,945][227910] New mean coefficients: [[ 0.39415997  0.48710388 -0.04658982  3.488051    0.5329385 ]][0m
[37m[1m[2023-07-10 14:49:49,946][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:49:59,579][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 14:49:59,579][227910] FPS: 398701.50[0m
[36m[2023-07-10 14:49:59,581][227910] itr=646, itrs=2000, Progress: 32.30%[0m
[36m[2023-07-10 14:50:11,214][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 14:50:11,215][227910] FPS: 330555.98[0m
[36m[2023-07-10 14:50:16,015][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:50:16,015][227910] Reward + Measures: [[2675.43262667    0.2788558     0.29703298    0.57914323    0.11180177]][0m
[37m[1m[2023-07-10 14:50:16,015][227910] Max Reward on eval: 2675.4326266678745[0m
[37m[1m[2023-07-10 14:50:16,016][227910] Min Reward on eval: 2675.4326266678745[0m
[37m[1m[2023-07-10 14:50:16,016][227910] Mean Reward across all agents: 2675.4326266678745[0m
[37m[1m[2023-07-10 14:50:16,016][227910] Average Trajectory Length: 999.4366666666666[0m
[36m[2023-07-10 14:50:21,686][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:50:21,687][227910] Reward + Measures: [[1452.91384644    0.22750001    0.30990002    0.53829998    0.2368    ]
 [1481.91032096    0.30960003    0.2414        0.57260001    0.1684    ]
 [1307.01470816    0.27069998    0.28169999    0.60480005    0.17700003]
 ...
 [1172.06068958    0.31080002    0.3044        0.63080001    0.16230001]
 [1236.23891407    0.2922        0.26659998    0.59320003    0.15290001]
 [ 885.5408196     0.41439995    0.38909999    0.71630001    0.21759999]][0m
[37m[1m[2023-07-10 14:50:21,687][227910] Max Reward on eval: 2561.9750831956044[0m
[37m[1m[2023-07-10 14:50:21,687][227910] Min Reward on eval: 27.153124507563188[0m
[37m[1m[2023-07-10 14:50:21,688][227910] Mean Reward across all agents: 1444.210065316185[0m
[37m[1m[2023-07-10 14:50:21,688][227910] Average Trajectory Length: 998.1496666666667[0m
[36m[2023-07-10 14:50:21,691][227910] mean_value=-420.356148270711, max_value=1722.1368793658544[0m
[37m[1m[2023-07-10 14:50:21,694][227910] New mean coefficients: [[-0.69489044  0.08753911 -0.49342686  3.6942017   0.39368507]][0m
[37m[1m[2023-07-10 14:50:21,695][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:50:31,613][227910] train() took 9.92 seconds to complete[0m
[36m[2023-07-10 14:50:31,614][227910] FPS: 387208.57[0m
[36m[2023-07-10 14:50:31,616][227910] itr=647, itrs=2000, Progress: 32.35%[0m
[36m[2023-07-10 14:50:43,325][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 14:50:43,326][227910] FPS: 328480.58[0m
[36m[2023-07-10 14:50:48,149][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:50:48,149][227910] Reward + Measures: [[2585.69629181    0.28279942    0.28472999    0.61764401    0.11348914]][0m
[37m[1m[2023-07-10 14:50:48,149][227910] Max Reward on eval: 2585.6962918143013[0m
[37m[1m[2023-07-10 14:50:48,150][227910] Min Reward on eval: 2585.6962918143013[0m
[37m[1m[2023-07-10 14:50:48,150][227910] Mean Reward across all agents: 2585.6962918143013[0m
[37m[1m[2023-07-10 14:50:48,150][227910] Average Trajectory Length: 999.568[0m
[36m[2023-07-10 14:50:53,712][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:50:53,717][227910] Reward + Measures: [[ 959.63032101    0.25299999    0.3021        0.42649999    0.22669999]
 [1159.19477614    0.31669998    0.32089999    0.41570002    0.13469999]
 [1054.0417772     0.36890003    0.32030001    0.51789999    0.16770001]
 ...
 [1436.74798011    0.30089998    0.2811        0.59219998    0.21460001]
 [2150.95747569    0.29010001    0.317         0.46240002    0.14050001]
 [2027.89025999    0.27700001    0.28330001    0.51160002    0.1499    ]][0m
[37m[1m[2023-07-10 14:50:53,718][227910] Max Reward on eval: 2541.13106191922[0m
[37m[1m[2023-07-10 14:50:53,718][227910] Min Reward on eval: -102.87191605157277[0m
[37m[1m[2023-07-10 14:50:53,718][227910] Mean Reward across all agents: 1654.5251028294306[0m
[37m[1m[2023-07-10 14:50:53,719][227910] Average Trajectory Length: 996.7486666666666[0m
[36m[2023-07-10 14:50:53,720][227910] mean_value=-675.0959993878612, max_value=1014.5539487839557[0m
[37m[1m[2023-07-10 14:50:53,723][227910] New mean coefficients: [[-1.032793    0.02821906 -0.45829752  3.288615    0.18951961]][0m
[37m[1m[2023-07-10 14:50:53,724][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:51:03,508][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 14:51:03,508][227910] FPS: 392545.16[0m
[36m[2023-07-10 14:51:03,510][227910] itr=648, itrs=2000, Progress: 32.40%[0m
[36m[2023-07-10 14:51:15,186][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 14:51:15,186][227910] FPS: 329372.20[0m
[36m[2023-07-10 14:51:20,072][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:51:20,077][227910] Reward + Measures: [[2249.91814287    0.29734534    0.27930734    0.66772366    0.115492  ]][0m
[37m[1m[2023-07-10 14:51:20,078][227910] Max Reward on eval: 2249.9181428699594[0m
[37m[1m[2023-07-10 14:51:20,078][227910] Min Reward on eval: 2249.9181428699594[0m
[37m[1m[2023-07-10 14:51:20,078][227910] Mean Reward across all agents: 2249.9181428699594[0m
[37m[1m[2023-07-10 14:51:20,078][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:51:25,520][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:51:25,521][227910] Reward + Measures: [[1960.87107481    0.29980001    0.25969997    0.61019999    0.15030001]
 [1647.7661248     0.32259998    0.26449999    0.5503        0.15910001]
 [1853.82223166    0.32049999    0.30539998    0.60319996    0.134     ]
 ...
 [2063.35109018    0.29890001    0.2651        0.65020007    0.15439999]
 [1909.3733303     0.31810001    0.30389997    0.61129999    0.13689999]
 [2063.53289975    0.30000001    0.27200004    0.6099        0.13700001]][0m
[37m[1m[2023-07-10 14:51:25,521][227910] Max Reward on eval: 2286.4152150491254[0m
[37m[1m[2023-07-10 14:51:25,521][227910] Min Reward on eval: 13.307716540058028[0m
[37m[1m[2023-07-10 14:51:25,521][227910] Mean Reward across all agents: 1875.955175492593[0m
[37m[1m[2023-07-10 14:51:25,522][227910] Average Trajectory Length: 999.7676666666666[0m
[36m[2023-07-10 14:51:25,524][227910] mean_value=-131.64227682409174, max_value=1849.4692294167858[0m
[37m[1m[2023-07-10 14:51:25,527][227910] New mean coefficients: [[-2.1736913   0.0578242  -0.5858307   3.605441    0.09876903]][0m
[37m[1m[2023-07-10 14:51:25,527][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:51:35,222][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 14:51:35,222][227910] FPS: 396185.32[0m
[36m[2023-07-10 14:51:35,224][227910] itr=649, itrs=2000, Progress: 32.45%[0m
[36m[2023-07-10 14:51:46,705][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 14:51:46,705][227910] FPS: 335043.10[0m
[36m[2023-07-10 14:51:51,541][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:51:51,541][227910] Reward + Measures: [[1664.11703964    0.38410529    0.32830399    0.68906802    0.19837199]][0m
[37m[1m[2023-07-10 14:51:51,542][227910] Max Reward on eval: 1664.1170396360842[0m
[37m[1m[2023-07-10 14:51:51,542][227910] Min Reward on eval: 1664.1170396360842[0m
[37m[1m[2023-07-10 14:51:51,542][227910] Mean Reward across all agents: 1664.1170396360842[0m
[37m[1m[2023-07-10 14:51:51,542][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:51:57,042][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:51:57,043][227910] Reward + Measures: [[ 938.06039746    0.57620001    0.48640004    0.75739998    0.44670001]
 [ 657.89689258    0.72760004    0.68660003    0.8398        0.6534    ]
 [1038.88327114    0.61850005    0.58200002    0.81960011    0.51190007]
 ...
 [ 831.69589845    0.74850005    0.72609997    0.85750002    0.66680002]
 [ 659.69653922    0.76809996    0.72310001    0.85769999    0.68940002]
 [1484.51761739    0.44949999    0.39300004    0.76590002    0.29359999]][0m
[37m[1m[2023-07-10 14:51:57,043][227910] Max Reward on eval: 2033.63188212784[0m
[37m[1m[2023-07-10 14:51:57,043][227910] Min Reward on eval: 148.37746546335984[0m
[37m[1m[2023-07-10 14:51:57,044][227910] Mean Reward across all agents: 945.1410515720386[0m
[37m[1m[2023-07-10 14:51:57,044][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:51:57,051][227910] mean_value=955.3218548284848, max_value=1984.5176173915388[0m
[37m[1m[2023-07-10 14:51:57,054][227910] New mean coefficients: [[-1.4180099  -0.34345987 -0.69092894  3.1121652   0.10097343]][0m
[37m[1m[2023-07-10 14:51:57,055][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:52:06,695][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 14:52:06,695][227910] FPS: 398394.16[0m
[36m[2023-07-10 14:52:06,698][227910] itr=650, itrs=2000, Progress: 32.50%[0m
[37m[1m[2023-07-10 14:52:09,872][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000630[0m
[36m[2023-07-10 14:52:21,670][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 14:52:21,670][227910] FPS: 332747.57[0m
[36m[2023-07-10 14:52:26,563][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:52:26,564][227910] Reward + Measures: [[622.23397246   0.75810874   0.62356633   0.82115763   0.64350933]][0m
[37m[1m[2023-07-10 14:52:26,564][227910] Max Reward on eval: 622.2339724644747[0m
[37m[1m[2023-07-10 14:52:26,564][227910] Min Reward on eval: 622.2339724644747[0m
[37m[1m[2023-07-10 14:52:26,564][227910] Mean Reward across all agents: 622.2339724644747[0m
[37m[1m[2023-07-10 14:52:26,565][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:52:32,016][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:52:32,017][227910] Reward + Measures: [[ 619.02512447    0.74779999    0.67290002    0.77910006    0.64790004]
 [1720.99059184    0.2911        0.2552        0.75859994    0.1189    ]
 [1759.81909537    0.31740004    0.28770003    0.77080005    0.09850001]
 ...
 [1685.93111847    0.30140001    0.2529        0.69279999    0.14560001]
 [1520.60697718    0.29460001    0.27590001    0.65269995    0.1601    ]
 [1688.42135317    0.31099999    0.25229999    0.76370001    0.0963    ]][0m
[37m[1m[2023-07-10 14:52:32,017][227910] Max Reward on eval: 1862.3827299900354[0m
[37m[1m[2023-07-10 14:52:32,017][227910] Min Reward on eval: 51.37933803577907[0m
[37m[1m[2023-07-10 14:52:32,018][227910] Mean Reward across all agents: 1314.0013669382326[0m
[37m[1m[2023-07-10 14:52:32,018][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:52:32,023][227910] mean_value=233.10677750703587, max_value=2292.4117226144763[0m
[37m[1m[2023-07-10 14:52:32,026][227910] New mean coefficients: [[-2.1220076  -0.43492585 -1.0278872   3.9666643  -0.1342341 ]][0m
[37m[1m[2023-07-10 14:52:32,027][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:52:41,702][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 14:52:41,703][227910] FPS: 396970.10[0m
[36m[2023-07-10 14:52:41,705][227910] itr=651, itrs=2000, Progress: 32.55%[0m
[36m[2023-07-10 14:52:53,218][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 14:52:53,219][227910] FPS: 333991.93[0m
[36m[2023-07-10 14:52:57,962][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:52:57,963][227910] Reward + Measures: [[358.75915814   0.7840519    0.57602066   0.81812102   0.65943164]][0m
[37m[1m[2023-07-10 14:52:57,963][227910] Max Reward on eval: 358.7591581449979[0m
[37m[1m[2023-07-10 14:52:57,963][227910] Min Reward on eval: 358.7591581449979[0m
[37m[1m[2023-07-10 14:52:57,963][227910] Mean Reward across all agents: 358.7591581449979[0m
[37m[1m[2023-07-10 14:52:57,964][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:53:03,400][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:53:03,406][227910] Reward + Measures: [[ 238.99670158    0.83170003    0.78130001    0.90930003    0.79469997]
 [1507.96477816    0.34510002    0.3263        0.68810004    0.1885    ]
 [1771.98036887    0.33239999    0.2902        0.73050004    0.1602    ]
 ...
 [ 463.99466746    0.69660002    0.58480006    0.83050007    0.62140006]
 [1580.53277423    0.33659998    0.32300001    0.68709993    0.18230002]
 [1622.83971615    0.34080002    0.289         0.74579996    0.1681    ]][0m
[37m[1m[2023-07-10 14:53:03,406][227910] Max Reward on eval: 1969.721406344371[0m
[37m[1m[2023-07-10 14:53:03,407][227910] Min Reward on eval: 86.18657322733198[0m
[37m[1m[2023-07-10 14:53:03,407][227910] Mean Reward across all agents: 1161.319033849198[0m
[37m[1m[2023-07-10 14:53:03,407][227910] Average Trajectory Length: 999.7763333333334[0m
[36m[2023-07-10 14:53:03,412][227910] mean_value=424.23594750320075, max_value=2249.071539559797[0m
[37m[1m[2023-07-10 14:53:03,415][227910] New mean coefficients: [[-2.4663498  -0.47619164 -1.0803211   3.9635928  -0.1733377 ]][0m
[37m[1m[2023-07-10 14:53:03,416][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:53:13,241][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 14:53:13,242][227910] FPS: 390880.79[0m
[36m[2023-07-10 14:53:13,244][227910] itr=652, itrs=2000, Progress: 32.60%[0m
[36m[2023-07-10 14:53:24,773][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 14:53:24,773][227910] FPS: 333588.04[0m
[36m[2023-07-10 14:53:29,645][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:53:29,645][227910] Reward + Measures: [[219.24262528   0.74284762   0.48994896   0.80624968   0.59565765]][0m
[37m[1m[2023-07-10 14:53:29,646][227910] Max Reward on eval: 219.24262527639993[0m
[37m[1m[2023-07-10 14:53:29,646][227910] Min Reward on eval: 219.24262527639993[0m
[37m[1m[2023-07-10 14:53:29,646][227910] Mean Reward across all agents: 219.24262527639993[0m
[37m[1m[2023-07-10 14:53:29,646][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:53:35,124][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:53:35,125][227910] Reward + Measures: [[-86.21192316   0.3429606    0.3563996    0.4051393    0.29795575]
 [155.27498108   0.62940001   0.44300005   0.72370005   0.46989998]
 [ 11.02467957   0.35403892   0.32152694   0.51840538   0.29584369]
 ...
 [-87.65669757   0.30020002   0.3858       0.42950001   0.34720001]
 [ 81.3524252    0.3443       0.2271       0.55660003   0.26460001]
 [218.82956819   0.9641       0.92609996   0.96680003   0.96820003]][0m
[37m[1m[2023-07-10 14:53:35,125][227910] Max Reward on eval: 1954.327088746312[0m
[37m[1m[2023-07-10 14:53:35,125][227910] Min Reward on eval: -494.0308441987145[0m
[37m[1m[2023-07-10 14:53:35,126][227910] Mean Reward across all agents: 138.4747263114006[0m
[37m[1m[2023-07-10 14:53:35,126][227910] Average Trajectory Length: 977.1783333333333[0m
[36m[2023-07-10 14:53:35,130][227910] mean_value=-469.4908114604771, max_value=1143.2591185307363[0m
[37m[1m[2023-07-10 14:53:35,132][227910] New mean coefficients: [[-0.8357761  -0.4490056  -0.62122256  4.3499794   0.53357786]][0m
[37m[1m[2023-07-10 14:53:35,133][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:53:44,939][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 14:53:44,939][227910] FPS: 391660.25[0m
[36m[2023-07-10 14:53:44,941][227910] itr=653, itrs=2000, Progress: 32.65%[0m
[36m[2023-07-10 14:53:56,415][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 14:53:56,415][227910] FPS: 335204.53[0m
[36m[2023-07-10 14:54:01,253][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:54:01,253][227910] Reward + Measures: [[210.00951839   0.71553802   0.483253     0.79173601   0.71940297]][0m
[37m[1m[2023-07-10 14:54:01,254][227910] Max Reward on eval: 210.00951838700396[0m
[37m[1m[2023-07-10 14:54:01,254][227910] Min Reward on eval: 210.00951838700396[0m
[37m[1m[2023-07-10 14:54:01,254][227910] Mean Reward across all agents: 210.00951838700396[0m
[37m[1m[2023-07-10 14:54:01,254][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:54:06,757][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:54:06,763][227910] Reward + Measures: [[574.24451943   0.47589999   0.28640002   0.74630004   0.37859997]
 [330.6490324    0.65880001   0.29239997   0.84860003   0.6196    ]
 [206.30351202   0.71719998   0.35530001   0.86379999   0.7446    ]
 ...
 [206.98611774   0.68370003   0.36199999   0.85270005   0.70160002]
 [161.29535172   0.76569998   0.4664       0.86800003   0.82100004]
 [167.97823247   0.71630001   0.4298       0.84559995   0.73879999]][0m
[37m[1m[2023-07-10 14:54:06,763][227910] Max Reward on eval: 1591.7047259799788[0m
[37m[1m[2023-07-10 14:54:06,763][227910] Min Reward on eval: 119.47447746768594[0m
[37m[1m[2023-07-10 14:54:06,764][227910] Mean Reward across all agents: 317.8736323708137[0m
[37m[1m[2023-07-10 14:54:06,764][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:54:06,771][227910] mean_value=557.7764361817947, max_value=1538.9467717768566[0m
[37m[1m[2023-07-10 14:54:06,774][227910] New mean coefficients: [[ 0.09232557 -0.7712146  -1.3137875   5.1589417   0.12807846]][0m
[37m[1m[2023-07-10 14:54:06,775][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:54:16,499][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 14:54:16,500][227910] FPS: 394933.74[0m
[36m[2023-07-10 14:54:16,502][227910] itr=654, itrs=2000, Progress: 32.70%[0m
[36m[2023-07-10 14:54:28,079][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 14:54:28,079][227910] FPS: 332168.09[0m
[36m[2023-07-10 14:54:32,829][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:54:32,829][227910] Reward + Measures: [[196.56370982   0.74423891   0.34710234   0.76087433   0.57686269]][0m
[37m[1m[2023-07-10 14:54:32,829][227910] Max Reward on eval: 196.56370981632278[0m
[37m[1m[2023-07-10 14:54:32,830][227910] Min Reward on eval: 196.56370981632278[0m
[37m[1m[2023-07-10 14:54:32,830][227910] Mean Reward across all agents: 196.56370981632278[0m
[37m[1m[2023-07-10 14:54:32,830][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:54:38,420][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:54:38,420][227910] Reward + Measures: [[164.31214069   0.75879997   0.40920001   0.69309998   0.60120004]
 [893.49524934   0.4429       0.249        0.60519999   0.30320001]
 [199.77370674   0.6674       0.39040002   0.61360002   0.52410001]
 ...
 [790.02067516   0.53640002   0.24859999   0.62690002   0.3159    ]
 [861.76945664   0.46949998   0.23099999   0.6049       0.31550002]
 [335.81817097   0.69230002   0.25710002   0.75279999   0.49839997]][0m
[37m[1m[2023-07-10 14:54:38,421][227910] Max Reward on eval: 1334.0384920521924[0m
[37m[1m[2023-07-10 14:54:38,421][227910] Min Reward on eval: -373.690648468805[0m
[37m[1m[2023-07-10 14:54:38,421][227910] Mean Reward across all agents: 383.37298187439643[0m
[37m[1m[2023-07-10 14:54:38,421][227910] Average Trajectory Length: 999.6693333333333[0m
[36m[2023-07-10 14:54:38,427][227910] mean_value=-98.21588206527788, max_value=1416.0524736320242[0m
[37m[1m[2023-07-10 14:54:38,429][227910] New mean coefficients: [[ 1.8249344 -0.6042617 -1.3246714  4.748465   0.171918 ]][0m
[37m[1m[2023-07-10 14:54:38,430][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:54:48,072][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 14:54:48,073][227910] FPS: 398329.10[0m
[36m[2023-07-10 14:54:48,075][227910] itr=655, itrs=2000, Progress: 32.75%[0m
[36m[2023-07-10 14:54:59,692][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 14:54:59,692][227910] FPS: 331048.20[0m
[36m[2023-07-10 14:55:04,435][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:55:04,436][227910] Reward + Measures: [[618.76341408   0.55788332   0.25771669   0.74895197   0.38929033]][0m
[37m[1m[2023-07-10 14:55:04,436][227910] Max Reward on eval: 618.7634140835763[0m
[37m[1m[2023-07-10 14:55:04,436][227910] Min Reward on eval: 618.7634140835763[0m
[37m[1m[2023-07-10 14:55:04,437][227910] Mean Reward across all agents: 618.7634140835763[0m
[37m[1m[2023-07-10 14:55:04,437][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:55:09,887][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:55:09,892][227910] Reward + Measures: [[379.3970746    0.6354       0.1996       0.76729995   0.44600001]
 [554.09473805   0.48859999   0.1708       0.78140002   0.3328    ]
 [574.40505611   0.55849999   0.2034       0.77170002   0.36920005]
 ...
 [821.79679621   0.47810003   0.20610002   0.76319999   0.29790002]
 [934.59732361   0.39879999   0.2031       0.79120004   0.23280001]
 [386.4620493    0.60589999   0.16490002   0.76929998   0.43600002]][0m
[37m[1m[2023-07-10 14:55:09,893][227910] Max Reward on eval: 1145.8902431879892[0m
[37m[1m[2023-07-10 14:55:09,893][227910] Min Reward on eval: -94.68359177452513[0m
[37m[1m[2023-07-10 14:55:09,893][227910] Mean Reward across all agents: 567.9872694983308[0m
[37m[1m[2023-07-10 14:55:09,894][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:55:09,899][227910] mean_value=749.9889701741095, max_value=1486.181999396172[0m
[37m[1m[2023-07-10 14:55:09,902][227910] New mean coefficients: [[ 0.3502791  -1.0406779  -1.7770095   4.613836   -0.45447448]][0m
[37m[1m[2023-07-10 14:55:09,903][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:55:19,570][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 14:55:19,570][227910] FPS: 397297.12[0m
[36m[2023-07-10 14:55:19,572][227910] itr=656, itrs=2000, Progress: 32.80%[0m
[36m[2023-07-10 14:55:31,139][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 14:55:31,139][227910] FPS: 332480.58[0m
[36m[2023-07-10 14:55:35,863][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:55:35,864][227910] Reward + Measures: [[1196.20211461    0.380983      0.24505465    0.77675098    0.21556368]][0m
[37m[1m[2023-07-10 14:55:35,864][227910] Max Reward on eval: 1196.2021146106856[0m
[37m[1m[2023-07-10 14:55:35,864][227910] Min Reward on eval: 1196.2021146106856[0m
[37m[1m[2023-07-10 14:55:35,865][227910] Mean Reward across all agents: 1196.2021146106856[0m
[37m[1m[2023-07-10 14:55:35,865][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:55:41,287][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:55:41,287][227910] Reward + Measures: [[1000.07771615    0.3256        0.39410001    0.50780004    0.1796    ]
 [ 952.28120962    0.32969999    0.42380005    0.44169998    0.1816    ]
 [1209.6230439     0.34559998    0.41630003    0.51580006    0.1772    ]
 ...
 [1403.49132247    0.35380003    0.36219999    0.61430001    0.1708    ]
 [1080.0631699     0.35540006    0.39390001    0.54070002    0.17860001]
 [1500.72505591    0.35269997    0.2863        0.6613        0.1569    ]][0m
[37m[1m[2023-07-10 14:55:41,288][227910] Max Reward on eval: 1608.9919278410962[0m
[37m[1m[2023-07-10 14:55:41,288][227910] Min Reward on eval: -30.26479889411712[0m
[37m[1m[2023-07-10 14:55:41,288][227910] Mean Reward across all agents: 1082.8211459803676[0m
[37m[1m[2023-07-10 14:55:41,288][227910] Average Trajectory Length: 996.0926666666667[0m
[36m[2023-07-10 14:55:41,290][227910] mean_value=-848.6283254548936, max_value=1152.185260029335[0m
[37m[1m[2023-07-10 14:55:41,293][227910] New mean coefficients: [[ 1.9425564  -0.28931034 -1.5523747   3.894206   -0.42768228]][0m
[37m[1m[2023-07-10 14:55:41,294][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:55:50,953][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 14:55:50,954][227910] FPS: 397600.53[0m
[36m[2023-07-10 14:55:50,956][227910] itr=657, itrs=2000, Progress: 32.85%[0m
[36m[2023-07-10 14:56:02,586][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 14:56:02,586][227910] FPS: 330740.11[0m
[36m[2023-07-10 14:56:07,379][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:56:07,379][227910] Reward + Measures: [[1623.22465478    0.32044631    0.26265234    0.80463696    0.13957433]][0m
[37m[1m[2023-07-10 14:56:07,379][227910] Max Reward on eval: 1623.2246547772324[0m
[37m[1m[2023-07-10 14:56:07,380][227910] Min Reward on eval: 1623.2246547772324[0m
[37m[1m[2023-07-10 14:56:07,380][227910] Mean Reward across all agents: 1623.2246547772324[0m
[37m[1m[2023-07-10 14:56:07,380][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:56:12,746][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:56:12,746][227910] Reward + Measures: [[ 680.53101538    0.2888        0.38740003    0.44569999    0.28959998]
 [1098.3697016     0.34190002    0.3973        0.59710002    0.26110002]
 [ 950.80432205    0.31890002    0.39260003    0.51980001    0.27210003]
 ...
 [ 822.84704391    0.34479997    0.41409999    0.53900003    0.30029997]
 [1199.42331991    0.32800004    0.38170001    0.61860001    0.25439999]
 [1116.892665      0.34130001    0.37620005    0.63029999    0.2617    ]][0m
[37m[1m[2023-07-10 14:56:12,746][227910] Max Reward on eval: 1621.3532044975786[0m
[37m[1m[2023-07-10 14:56:12,747][227910] Min Reward on eval: -54.64076218723203[0m
[37m[1m[2023-07-10 14:56:12,747][227910] Mean Reward across all agents: 1023.0900573395868[0m
[37m[1m[2023-07-10 14:56:12,747][227910] Average Trajectory Length: 997.8879999999999[0m
[36m[2023-07-10 14:56:12,750][227910] mean_value=-229.13470476891894, max_value=2031.4837475265376[0m
[37m[1m[2023-07-10 14:56:12,753][227910] New mean coefficients: [[ 3.6283085   0.00413001 -0.7413801   3.4205046   0.00855315]][0m
[37m[1m[2023-07-10 14:56:12,754][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:56:22,523][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 14:56:22,523][227910] FPS: 393166.09[0m
[36m[2023-07-10 14:56:22,525][227910] itr=658, itrs=2000, Progress: 32.90%[0m
[36m[2023-07-10 14:56:33,960][227910] train() took 11.42 seconds to complete[0m
[36m[2023-07-10 14:56:33,960][227910] FPS: 336301.91[0m
[36m[2023-07-10 14:56:38,818][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:56:38,818][227910] Reward + Measures: [[1956.22999551    0.30877265    0.27698666    0.80313766    0.11106932]][0m
[37m[1m[2023-07-10 14:56:38,818][227910] Max Reward on eval: 1956.229995513618[0m
[37m[1m[2023-07-10 14:56:38,819][227910] Min Reward on eval: 1956.229995513618[0m
[37m[1m[2023-07-10 14:56:38,819][227910] Mean Reward across all agents: 1956.229995513618[0m
[37m[1m[2023-07-10 14:56:38,819][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:56:44,480][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:56:44,481][227910] Reward + Measures: [[1491.81365806    0.30689999    0.29990003    0.77219999    0.1592    ]
 [1870.59206237    0.31799999    0.2595        0.80149996    0.14210001]
 [1476.85419848    0.29900002    0.29150003    0.78890002    0.15280001]
 ...
 [1781.99352086    0.31830001    0.26150003    0.79840004    0.13829999]
 [1686.89269031    0.3143        0.27069998    0.80489999    0.14820002]
 [1754.64733595    0.3362        0.2674        0.71329999    0.16239999]][0m
[37m[1m[2023-07-10 14:56:44,481][227910] Max Reward on eval: 2058.303582833917[0m
[37m[1m[2023-07-10 14:56:44,482][227910] Min Reward on eval: 544.7570644619293[0m
[37m[1m[2023-07-10 14:56:44,482][227910] Mean Reward across all agents: 1528.2421694568525[0m
[37m[1m[2023-07-10 14:56:44,482][227910] Average Trajectory Length: 999.9686666666666[0m
[36m[2023-07-10 14:56:44,486][227910] mean_value=285.87031040455645, max_value=2358.3656713885953[0m
[37m[1m[2023-07-10 14:56:44,489][227910] New mean coefficients: [[ 3.5491736   0.01917375 -0.95715547  3.1241946  -0.04195458]][0m
[37m[1m[2023-07-10 14:56:44,490][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:56:54,331][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 14:56:54,331][227910] FPS: 390269.17[0m
[36m[2023-07-10 14:56:54,333][227910] itr=659, itrs=2000, Progress: 32.95%[0m
[36m[2023-07-10 14:57:06,042][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 14:57:06,042][227910] FPS: 328489.25[0m
[36m[2023-07-10 14:57:10,809][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:57:10,810][227910] Reward + Measures: [[2132.68975097    0.30297598    0.27328101    0.79772663    0.10317767]][0m
[37m[1m[2023-07-10 14:57:10,810][227910] Max Reward on eval: 2132.6897509686396[0m
[37m[1m[2023-07-10 14:57:10,810][227910] Min Reward on eval: 2132.6897509686396[0m
[37m[1m[2023-07-10 14:57:10,810][227910] Mean Reward across all agents: 2132.6897509686396[0m
[37m[1m[2023-07-10 14:57:10,810][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:57:16,344][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:57:16,345][227910] Reward + Measures: [[ 575.79127673    0.51840001    0.3319        0.61840004    0.37949997]
 [1481.05377497    0.33960006    0.3407        0.72360003    0.17480001]
 [1051.54633822    0.491         0.2906        0.7956        0.28560001]
 ...
 [ 703.50479249    0.3743        0.398         0.49470004    0.28170002]
 [ 779.29504294    0.54430002    0.30960003    0.7137        0.19410001]
 [ 768.11211676    0.55520004    0.30689999    0.69880003    0.16419999]][0m
[37m[1m[2023-07-10 14:57:16,345][227910] Max Reward on eval: 2089.7680520296562[0m
[37m[1m[2023-07-10 14:57:16,345][227910] Min Reward on eval: 71.89608140849887[0m
[37m[1m[2023-07-10 14:57:16,345][227910] Mean Reward across all agents: 939.9994512185206[0m
[37m[1m[2023-07-10 14:57:16,346][227910] Average Trajectory Length: 999.9406666666666[0m
[36m[2023-07-10 14:57:16,351][227910] mean_value=240.8361934146304, max_value=2001.3062262399822[0m
[37m[1m[2023-07-10 14:57:16,354][227910] New mean coefficients: [[ 2.3629298  -0.12928572 -0.89155984  3.7927394  -0.07722081]][0m
[37m[1m[2023-07-10 14:57:16,355][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:57:26,060][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 14:57:26,060][227910] FPS: 395736.28[0m
[36m[2023-07-10 14:57:26,062][227910] itr=660, itrs=2000, Progress: 33.00%[0m
[37m[1m[2023-07-10 14:57:29,109][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000640[0m
[36m[2023-07-10 14:57:40,956][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 14:57:40,957][227910] FPS: 331608.85[0m
[36m[2023-07-10 14:57:45,701][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:57:45,702][227910] Reward + Measures: [[2250.36643159    0.30366799    0.26854101    0.81659764    0.09481332]][0m
[37m[1m[2023-07-10 14:57:45,702][227910] Max Reward on eval: 2250.366431593605[0m
[37m[1m[2023-07-10 14:57:45,702][227910] Min Reward on eval: 2250.366431593605[0m
[37m[1m[2023-07-10 14:57:45,702][227910] Mean Reward across all agents: 2250.366431593605[0m
[37m[1m[2023-07-10 14:57:45,703][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:57:51,047][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:57:51,047][227910] Reward + Measures: [[1718.26907778    0.33260003    0.28380001    0.64230007    0.1237    ]
 [1690.53988905    0.33860001    0.30760002    0.5801        0.1124    ]
 [1358.52889773    0.37400001    0.2507        0.69839996    0.13640001]
 ...
 [1463.87597784    0.33329996    0.33670002    0.62120003    0.17900001]
 [ 821.56160647    0.23779999    0.28239998    0.30340001    0.12110001]
 [1801.40133229    0.32620001    0.27970001    0.67940009    0.11259999]][0m
[37m[1m[2023-07-10 14:57:51,048][227910] Max Reward on eval: 2277.15993284916[0m
[37m[1m[2023-07-10 14:57:51,048][227910] Min Reward on eval: 599.9913314523001[0m
[37m[1m[2023-07-10 14:57:51,048][227910] Mean Reward across all agents: 1725.6165305001728[0m
[37m[1m[2023-07-10 14:57:51,048][227910] Average Trajectory Length: 997.3946666666666[0m
[36m[2023-07-10 14:57:51,053][227910] mean_value=116.4199419610586, max_value=2726.505630315165[0m
[37m[1m[2023-07-10 14:57:51,055][227910] New mean coefficients: [[2.9599357  0.14562342 0.04314733 2.8232362  0.31053448]][0m
[37m[1m[2023-07-10 14:57:51,056][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:58:00,837][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 14:58:00,837][227910] FPS: 392671.25[0m
[36m[2023-07-10 14:58:00,840][227910] itr=661, itrs=2000, Progress: 33.05%[0m
[36m[2023-07-10 14:58:12,308][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 14:58:12,309][227910] FPS: 335295.13[0m
[36m[2023-07-10 14:58:17,062][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:58:17,063][227910] Reward + Measures: [[2385.69149835    0.29479501    0.26484734    0.81081599    0.09042867]][0m
[37m[1m[2023-07-10 14:58:17,063][227910] Max Reward on eval: 2385.691498349768[0m
[37m[1m[2023-07-10 14:58:17,063][227910] Min Reward on eval: 2385.691498349768[0m
[37m[1m[2023-07-10 14:58:17,063][227910] Mean Reward across all agents: 2385.691498349768[0m
[37m[1m[2023-07-10 14:58:17,064][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:58:22,403][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:58:22,404][227910] Reward + Measures: [[1086.90806552    0.30000001    0.39910004    0.56190002    0.2735    ]
 [1157.71855977    0.34880003    0.38990006    0.63000005    0.22150002]
 [ 821.11963776    0.37060004    0.36450002    0.62140006    0.33940002]
 ...
 [ 843.21092769    0.31640002    0.33849999    0.56669998    0.30440003]
 [ 969.79510797    0.29710001    0.3407        0.60580003    0.3089    ]
 [ 806.80797001    0.41479999    0.47499999    0.64889997    0.43309999]][0m
[37m[1m[2023-07-10 14:58:22,404][227910] Max Reward on eval: 2204.2749450523406[0m
[37m[1m[2023-07-10 14:58:22,404][227910] Min Reward on eval: -516.8779474691255[0m
[37m[1m[2023-07-10 14:58:22,405][227910] Mean Reward across all agents: 731.8270928635[0m
[37m[1m[2023-07-10 14:58:22,405][227910] Average Trajectory Length: 946.5036666666666[0m
[36m[2023-07-10 14:58:22,409][227910] mean_value=-497.23408918075546, max_value=2400.795082259504[0m
[37m[1m[2023-07-10 14:58:22,412][227910] New mean coefficients: [[ 2.0665474   0.36860323  0.15234873  2.4957135  -0.0281575 ]][0m
[37m[1m[2023-07-10 14:58:22,413][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:58:32,006][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 14:58:32,006][227910] FPS: 400378.42[0m
[36m[2023-07-10 14:58:32,008][227910] itr=662, itrs=2000, Progress: 33.10%[0m
[36m[2023-07-10 14:58:43,512][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 14:58:43,513][227910] FPS: 334296.57[0m
[36m[2023-07-10 14:58:48,259][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:58:48,259][227910] Reward + Measures: [[2469.94359857    0.29541498    0.25055566    0.82550538    0.092336  ]][0m
[37m[1m[2023-07-10 14:58:48,260][227910] Max Reward on eval: 2469.943598570132[0m
[37m[1m[2023-07-10 14:58:48,260][227910] Min Reward on eval: 2469.943598570132[0m
[37m[1m[2023-07-10 14:58:48,260][227910] Mean Reward across all agents: 2469.943598570132[0m
[37m[1m[2023-07-10 14:58:48,260][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 14:58:53,859][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:58:53,859][227910] Reward + Measures: [[ 971.12896687    0.32409999    0.35410002    0.59730005    0.2965    ]
 [1039.4496898     0.4587        0.34819999    0.65850002    0.20630001]
 [ 788.28716225    0.45874235    0.36824325    0.50018734    0.22961894]
 ...
 [1411.48724414    0.3779        0.37259999    0.65329999    0.1752    ]
 [2494.5494877     0.2951        0.27040002    0.77640003    0.097     ]
 [1036.03750098    0.48059997    0.36480001    0.5837        0.20100002]][0m
[37m[1m[2023-07-10 14:58:53,860][227910] Max Reward on eval: 2494.549487695098[0m
[37m[1m[2023-07-10 14:58:53,860][227910] Min Reward on eval: -474.0912335824396[0m
[37m[1m[2023-07-10 14:58:53,860][227910] Mean Reward across all agents: 1196.685302304844[0m
[37m[1m[2023-07-10 14:58:53,860][227910] Average Trajectory Length: 998.5226666666666[0m
[36m[2023-07-10 14:58:53,864][227910] mean_value=-313.9107809972286, max_value=2115.3965867556585[0m
[37m[1m[2023-07-10 14:58:53,867][227910] New mean coefficients: [[ 1.5230119  -0.07788     0.0890293   2.4752853   0.17339636]][0m
[37m[1m[2023-07-10 14:58:53,868][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:59:03,839][227910] train() took 9.97 seconds to complete[0m
[36m[2023-07-10 14:59:03,839][227910] FPS: 385195.34[0m
[36m[2023-07-10 14:59:03,841][227910] itr=663, itrs=2000, Progress: 33.15%[0m
[36m[2023-07-10 14:59:15,509][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 14:59:15,509][227910] FPS: 329588.31[0m
[36m[2023-07-10 14:59:20,334][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:59:20,334][227910] Reward + Measures: [[1138.42475546    0.27365333    0.42067426    0.48710519    0.29947078]][0m
[37m[1m[2023-07-10 14:59:20,334][227910] Max Reward on eval: 1138.4247554616327[0m
[37m[1m[2023-07-10 14:59:20,334][227910] Min Reward on eval: 1138.4247554616327[0m
[37m[1m[2023-07-10 14:59:20,334][227910] Mean Reward across all agents: 1138.4247554616327[0m
[37m[1m[2023-07-10 14:59:20,335][227910] Average Trajectory Length: 999.317[0m
[36m[2023-07-10 14:59:25,715][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:59:25,716][227910] Reward + Measures: [[1272.29557994    0.27509999    0.39570001    0.4921        0.27210003]
 [1071.78154014    0.27309999    0.43650004    0.48730001    0.30179998]
 [1313.80179175    0.2739        0.3901        0.49400002    0.29449999]
 ...
 [1257.26072048    0.27860004    0.3802        0.49439999    0.29010001]
 [1247.0931605     0.27840003    0.3752        0.51529998    0.30110002]
 [1055.56822701    0.27770001    0.40450001    0.54470003    0.30759999]][0m
[37m[1m[2023-07-10 14:59:25,716][227910] Max Reward on eval: 1396.8717829745729[0m
[37m[1m[2023-07-10 14:59:25,716][227910] Min Reward on eval: 598.908190434426[0m
[37m[1m[2023-07-10 14:59:25,717][227910] Mean Reward across all agents: 1158.1256857459523[0m
[37m[1m[2023-07-10 14:59:25,717][227910] Average Trajectory Length: 999.293[0m
[36m[2023-07-10 14:59:25,718][227910] mean_value=-520.2965011336879, max_value=629.1500273338102[0m
[37m[1m[2023-07-10 14:59:25,720][227910] New mean coefficients: [[ 3.5258458  -0.05920509 -0.16479693  2.7896771  -0.03589614]][0m
[37m[1m[2023-07-10 14:59:25,721][227910] Moving the mean solution point...[0m
[36m[2023-07-10 14:59:35,518][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 14:59:35,519][227910] FPS: 392020.06[0m
[36m[2023-07-10 14:59:35,521][227910] itr=664, itrs=2000, Progress: 33.20%[0m
[36m[2023-07-10 14:59:47,121][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 14:59:47,121][227910] FPS: 331509.49[0m
[36m[2023-07-10 14:59:51,765][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:59:51,766][227910] Reward + Measures: [[1325.88279588    0.2789368     0.41412535    0.49131539    0.28876922]][0m
[37m[1m[2023-07-10 14:59:51,766][227910] Max Reward on eval: 1325.882795882576[0m
[37m[1m[2023-07-10 14:59:51,766][227910] Min Reward on eval: 1325.882795882576[0m
[37m[1m[2023-07-10 14:59:51,766][227910] Mean Reward across all agents: 1325.882795882576[0m
[37m[1m[2023-07-10 14:59:51,767][227910] Average Trajectory Length: 998.555[0m
[36m[2023-07-10 14:59:57,151][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 14:59:57,152][227910] Reward + Measures: [[1400.45211275    0.28749999    0.39579999    0.51130003    0.3184    ]
 [1637.91313585    0.28029999    0.37750003    0.55369997    0.26939997]
 [1680.76716716    0.26810002    0.39970002    0.50559998    0.26190001]
 ...
 [1243.80194469    0.30130002    0.37200001    0.55180001    0.3177    ]
 [1771.46711953    0.2545        0.39129999    0.48810002    0.26660001]
 [1545.41755376    0.27067059    0.40997058    0.5024941     0.25842941]][0m
[37m[1m[2023-07-10 14:59:57,152][227910] Max Reward on eval: 1901.8428492551902[0m
[37m[1m[2023-07-10 14:59:57,152][227910] Min Reward on eval: 72.03001615188259[0m
[37m[1m[2023-07-10 14:59:57,152][227910] Mean Reward across all agents: 1374.782516611812[0m
[37m[1m[2023-07-10 14:59:57,153][227910] Average Trajectory Length: 998.0179999999999[0m
[36m[2023-07-10 14:59:57,155][227910] mean_value=-373.4961189894454, max_value=578.6426454322819[0m
[37m[1m[2023-07-10 14:59:57,157][227910] New mean coefficients: [[ 3.8546069   0.05738707 -0.3403661   2.8225405  -0.1668694 ]][0m
[37m[1m[2023-07-10 14:59:57,158][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:00:06,886][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 15:00:06,887][227910] FPS: 394800.16[0m
[36m[2023-07-10 15:00:06,889][227910] itr=665, itrs=2000, Progress: 33.25%[0m
[36m[2023-07-10 15:00:18,491][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 15:00:18,492][227910] FPS: 331439.37[0m
[36m[2023-07-10 15:00:23,258][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:00:23,259][227910] Reward + Measures: [[1494.55224702    0.26435626    0.44660854    0.48049918    0.25585818]][0m
[37m[1m[2023-07-10 15:00:23,259][227910] Max Reward on eval: 1494.5522470225308[0m
[37m[1m[2023-07-10 15:00:23,259][227910] Min Reward on eval: 1494.5522470225308[0m
[37m[1m[2023-07-10 15:00:23,260][227910] Mean Reward across all agents: 1494.5522470225308[0m
[37m[1m[2023-07-10 15:00:23,260][227910] Average Trajectory Length: 998.778[0m
[36m[2023-07-10 15:00:28,703][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:00:28,704][227910] Reward + Measures: [[1562.41723756    0.2474        0.3678        0.45880005    0.23      ]
 [1780.01005514    0.26069999    0.40200001    0.49600002    0.23330002]
 [1582.14528222    0.25572583    0.40963903    0.48984534    0.24102373]
 ...
 [1710.33991078    0.2481        0.42659998    0.49040005    0.22750001]
 [1530.16449412    0.2832        0.42630002    0.50769997    0.2818    ]
 [1781.2498634     0.25980002    0.40560004    0.4993        0.25190002]][0m
[37m[1m[2023-07-10 15:00:28,704][227910] Max Reward on eval: 1909.995259826607[0m
[37m[1m[2023-07-10 15:00:28,704][227910] Min Reward on eval: 765.4764631750353[0m
[37m[1m[2023-07-10 15:00:28,705][227910] Mean Reward across all agents: 1615.0444847174017[0m
[37m[1m[2023-07-10 15:00:28,705][227910] Average Trajectory Length: 997.5303333333333[0m
[36m[2023-07-10 15:00:28,707][227910] mean_value=-157.6425894566556, max_value=170.83456845902583[0m
[37m[1m[2023-07-10 15:00:28,710][227910] New mean coefficients: [[ 5.2356343  -0.3341942  -0.4179493   2.9670966  -0.31846997]][0m
[37m[1m[2023-07-10 15:00:28,710][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:00:38,510][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 15:00:38,510][227910] FPS: 391937.15[0m
[36m[2023-07-10 15:00:38,512][227910] itr=666, itrs=2000, Progress: 33.30%[0m
[36m[2023-07-10 15:00:50,162][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 15:00:50,162][227910] FPS: 330148.12[0m
[36m[2023-07-10 15:00:54,938][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:00:54,938][227910] Reward + Measures: [[1705.49623155    0.25999281    0.43629435    0.49080345    0.25021994]][0m
[37m[1m[2023-07-10 15:00:54,938][227910] Max Reward on eval: 1705.4962315467217[0m
[37m[1m[2023-07-10 15:00:54,938][227910] Min Reward on eval: 1705.4962315467217[0m
[37m[1m[2023-07-10 15:00:54,939][227910] Mean Reward across all agents: 1705.4962315467217[0m
[37m[1m[2023-07-10 15:00:54,939][227910] Average Trajectory Length: 999.8729999999999[0m
[36m[2023-07-10 15:01:00,539][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:01:00,539][227910] Reward + Measures: [[1661.57231219    0.25569999    0.41209999    0.54140002    0.26430002]
 [1677.90904677    0.24089999    0.37900001    0.56380004    0.2563    ]
 [1565.896435      0.23800002    0.34709999    0.509         0.24980001]
 ...
 [1670.83906224    0.2431        0.36840001    0.56589997    0.25840002]
 [1731.96951358    0.24820001    0.38009998    0.56240004    0.2642    ]
 [1629.20758564    0.2405        0.37830001    0.54269999    0.25120002]][0m
[37m[1m[2023-07-10 15:01:00,539][227910] Max Reward on eval: 1856.9018325269221[0m
[37m[1m[2023-07-10 15:01:00,540][227910] Min Reward on eval: 1267.7045528191725[0m
[37m[1m[2023-07-10 15:01:00,540][227910] Mean Reward across all agents: 1632.563287245628[0m
[37m[1m[2023-07-10 15:01:00,540][227910] Average Trajectory Length: 997.942[0m
[36m[2023-07-10 15:01:00,544][227910] mean_value=62.94807633594898, max_value=1068.845095255694[0m
[37m[1m[2023-07-10 15:01:00,546][227910] New mean coefficients: [[ 6.2832804  -0.17108141 -0.5515419   3.0590997  -0.19910836]][0m
[37m[1m[2023-07-10 15:01:00,547][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:01:10,174][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 15:01:10,174][227910] FPS: 398972.16[0m
[36m[2023-07-10 15:01:10,176][227910] itr=667, itrs=2000, Progress: 33.35%[0m
[36m[2023-07-10 15:01:21,809][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 15:01:21,810][227910] FPS: 330586.01[0m
[36m[2023-07-10 15:01:26,665][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:01:26,666][227910] Reward + Measures: [[1880.57087586    0.26023149    0.42820945    0.5028038     0.24309704]][0m
[37m[1m[2023-07-10 15:01:26,666][227910] Max Reward on eval: 1880.5708758629032[0m
[37m[1m[2023-07-10 15:01:26,666][227910] Min Reward on eval: 1880.5708758629032[0m
[37m[1m[2023-07-10 15:01:26,666][227910] Mean Reward across all agents: 1880.5708758629032[0m
[37m[1m[2023-07-10 15:01:26,667][227910] Average Trajectory Length: 999.9623333333333[0m
[36m[2023-07-10 15:01:32,212][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:01:32,213][227910] Reward + Measures: [[1318.79385647    0.3382        0.47760001    0.47499999    0.27520001]
 [1548.05564886    0.2579        0.45250002    0.40869999    0.24000001]
 [1604.67757242    0.273         0.48100001    0.46960002    0.25620002]
 ...
 [1963.53537571    0.25740001    0.45199999    0.47810003    0.2325    ]
 [1521.41738996    0.3152        0.45289999    0.4869        0.25729999]
 [1863.95454286    0.26750001    0.45220003    0.47340003    0.23410001]][0m
[37m[1m[2023-07-10 15:01:32,213][227910] Max Reward on eval: 2024.503084211005[0m
[37m[1m[2023-07-10 15:01:32,213][227910] Min Reward on eval: -146.12030954840594[0m
[37m[1m[2023-07-10 15:01:32,214][227910] Mean Reward across all agents: 1656.2034096497289[0m
[37m[1m[2023-07-10 15:01:32,214][227910] Average Trajectory Length: 998.5996666666666[0m
[36m[2023-07-10 15:01:32,217][227910] mean_value=-346.49386634468283, max_value=420.1132078701073[0m
[37m[1m[2023-07-10 15:01:32,219][227910] New mean coefficients: [[ 5.7375307  -0.141579   -0.37299225  3.0754776  -0.0712411 ]][0m
[37m[1m[2023-07-10 15:01:32,220][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:01:41,978][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 15:01:41,979][227910] FPS: 393596.31[0m
[36m[2023-07-10 15:01:41,981][227910] itr=668, itrs=2000, Progress: 33.40%[0m
[36m[2023-07-10 15:01:53,643][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 15:01:53,644][227910] FPS: 329744.01[0m
[36m[2023-07-10 15:01:58,407][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:01:58,408][227910] Reward + Measures: [[2028.50088872    0.25899497    0.42221048    0.5165633     0.2377526 ]][0m
[37m[1m[2023-07-10 15:01:58,408][227910] Max Reward on eval: 2028.5008887158415[0m
[37m[1m[2023-07-10 15:01:58,408][227910] Min Reward on eval: 2028.5008887158415[0m
[37m[1m[2023-07-10 15:01:58,408][227910] Mean Reward across all agents: 2028.5008887158415[0m
[37m[1m[2023-07-10 15:01:58,408][227910] Average Trajectory Length: 999.7303333333333[0m
[36m[2023-07-10 15:02:03,855][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:02:03,856][227910] Reward + Measures: [[1598.23119882    0.23109999    0.45380002    0.47269997    0.27010003]
 [1876.36264518    0.2559        0.43540001    0.49670002    0.25710002]
 [1707.22953638    0.26010001    0.375         0.56819999    0.25049999]
 ...
 [1584.57054711    0.20729999    0.42770001    0.46779999    0.25219998]
 [2000.75393474    0.245         0.3928        0.55220002    0.23930001]
 [1809.85549325    0.24390002    0.37850001    0.56560004    0.2642    ]][0m
[37m[1m[2023-07-10 15:02:03,856][227910] Max Reward on eval: 2141.723538708128[0m
[37m[1m[2023-07-10 15:02:03,856][227910] Min Reward on eval: 646.1912461902393[0m
[37m[1m[2023-07-10 15:02:03,857][227910] Mean Reward across all agents: 1708.3186269857242[0m
[37m[1m[2023-07-10 15:02:03,857][227910] Average Trajectory Length: 996.5136666666666[0m
[36m[2023-07-10 15:02:03,860][227910] mean_value=-141.23626527202734, max_value=553.333645355342[0m
[37m[1m[2023-07-10 15:02:03,862][227910] New mean coefficients: [[ 4.8857603  -0.01805961 -0.41608062  3.2397642   0.06438561]][0m
[37m[1m[2023-07-10 15:02:03,863][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:02:13,515][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 15:02:13,515][227910] FPS: 397932.22[0m
[36m[2023-07-10 15:02:13,517][227910] itr=669, itrs=2000, Progress: 33.45%[0m
[36m[2023-07-10 15:02:25,193][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 15:02:25,193][227910] FPS: 329389.77[0m
[36m[2023-07-10 15:02:29,921][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:02:29,921][227910] Reward + Measures: [[1780.82791191    0.24599408    0.34176025    0.50272173    0.25388399]][0m
[37m[1m[2023-07-10 15:02:29,921][227910] Max Reward on eval: 1780.8279119070446[0m
[37m[1m[2023-07-10 15:02:29,922][227910] Min Reward on eval: 1780.8279119070446[0m
[37m[1m[2023-07-10 15:02:29,922][227910] Mean Reward across all agents: 1780.8279119070446[0m
[37m[1m[2023-07-10 15:02:29,922][227910] Average Trajectory Length: 998.9983333333333[0m
[36m[2023-07-10 15:02:35,437][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:02:35,437][227910] Reward + Measures: [[-809.35488831    0.42389998    0.28260002    0.43280002    0.175     ]
 [ 621.98115731    0.20650001    0.2379        0.3242        0.2543    ]
 [-875.51180776    0.43902799    0.32657388    0.48028737    0.21252792]
 ...
 [ 565.22326386    0.24200001    0.30089998    0.3443        0.33140001]
 [ 687.75865562    0.2009        0.26229998    0.3466        0.23140001]
 [ 224.46668537    0.22140001    0.2422        0.2895        0.26840001]][0m
[37m[1m[2023-07-10 15:02:35,437][227910] Max Reward on eval: 2220.772264684923[0m
[37m[1m[2023-07-10 15:02:35,438][227910] Min Reward on eval: -1660.9272078320616[0m
[37m[1m[2023-07-10 15:02:35,438][227910] Mean Reward across all agents: -19.278328440703135[0m
[37m[1m[2023-07-10 15:02:35,438][227910] Average Trajectory Length: 980.824[0m
[36m[2023-07-10 15:02:35,440][227910] mean_value=-1186.4258194129523, max_value=1238.4665515979564[0m
[37m[1m[2023-07-10 15:02:35,442][227910] New mean coefficients: [[ 5.083472    0.2240319  -0.4921047   1.8622704   0.06162669]][0m
[37m[1m[2023-07-10 15:02:35,443][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:02:45,226][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 15:02:45,226][227910] FPS: 392605.62[0m
[36m[2023-07-10 15:02:45,228][227910] itr=670, itrs=2000, Progress: 33.50%[0m
[37m[1m[2023-07-10 15:02:48,379][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000650[0m
[36m[2023-07-10 15:03:00,213][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 15:03:00,213][227910] FPS: 332068.09[0m
[36m[2023-07-10 15:03:04,928][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:03:04,928][227910] Reward + Measures: [[2035.81748866    0.24127015    0.34238255    0.51953822    0.2464515 ]][0m
[37m[1m[2023-07-10 15:03:04,928][227910] Max Reward on eval: 2035.8174886588356[0m
[37m[1m[2023-07-10 15:03:04,929][227910] Min Reward on eval: 2035.8174886588356[0m
[37m[1m[2023-07-10 15:03:04,929][227910] Mean Reward across all agents: 2035.8174886588356[0m
[37m[1m[2023-07-10 15:03:04,929][227910] Average Trajectory Length: 998.5153333333333[0m
[36m[2023-07-10 15:03:10,292][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:03:10,298][227910] Reward + Measures: [[1522.85536237    0.28200004    0.39940003    0.42969999    0.25920001]
 [1737.26788114    0.26320001    0.38030002    0.51520008    0.24459998]
 [1873.45147022    0.3249        0.35460001    0.49869999    0.29119998]
 ...
 [-341.88217076    0.32377204    0.42263103    0.34132981    0.20808473]
 [1943.21317854    0.29860002    0.3355        0.52300006    0.27680001]
 [-119.05145802    0.41107473    0.48685715    0.49936485    0.17147803]][0m
[37m[1m[2023-07-10 15:03:10,298][227910] Max Reward on eval: 2272.5051097862424[0m
[37m[1m[2023-07-10 15:03:10,298][227910] Min Reward on eval: -477.7192722874112[0m
[37m[1m[2023-07-10 15:03:10,298][227910] Mean Reward across all agents: 1397.712353274602[0m
[37m[1m[2023-07-10 15:03:10,299][227910] Average Trajectory Length: 984.9103333333333[0m
[36m[2023-07-10 15:03:10,302][227910] mean_value=-278.70586478985916, max_value=1261.2269794919657[0m
[37m[1m[2023-07-10 15:03:10,305][227910] New mean coefficients: [[ 4.4734893   0.6099983  -0.50760543  1.6033607   0.0435161 ]][0m
[37m[1m[2023-07-10 15:03:10,306][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:03:19,983][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 15:03:19,983][227910] FPS: 396903.56[0m
[36m[2023-07-10 15:03:19,985][227910] itr=671, itrs=2000, Progress: 33.55%[0m
[36m[2023-07-10 15:03:31,612][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 15:03:31,613][227910] FPS: 330806.67[0m
[36m[2023-07-10 15:03:36,087][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:03:36,088][227910] Reward + Measures: [[2221.7413023     0.23883809    0.33392411    0.53615922    0.24220923]][0m
[37m[1m[2023-07-10 15:03:36,088][227910] Max Reward on eval: 2221.7413022957676[0m
[37m[1m[2023-07-10 15:03:36,088][227910] Min Reward on eval: 2221.7413022957676[0m
[37m[1m[2023-07-10 15:03:36,088][227910] Mean Reward across all agents: 2221.7413022957676[0m
[37m[1m[2023-07-10 15:03:36,088][227910] Average Trajectory Length: 998.721[0m
[36m[2023-07-10 15:03:41,172][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:03:41,172][227910] Reward + Measures: [[1967.57425705    0.23629999    0.31429997    0.54439998    0.26190001]
 [1913.86195043    0.26719999    0.31969997    0.6577        0.26620001]
 [2094.44746226    0.26210001    0.36840001    0.5959        0.29650003]
 ...
 [2338.05097426    0.24540003    0.34920001    0.60780001    0.24299999]
 [1894.83643395    0.24499999    0.31669998    0.59440005    0.25729999]
 [2244.82529316    0.24720001    0.32480001    0.58990002    0.27060002]][0m
[37m[1m[2023-07-10 15:03:41,172][227910] Max Reward on eval: 2405.6811305455863[0m
[37m[1m[2023-07-10 15:03:41,173][227910] Min Reward on eval: 1333.4772646497702[0m
[37m[1m[2023-07-10 15:03:41,173][227910] Mean Reward across all agents: 2081.759443034461[0m
[37m[1m[2023-07-10 15:03:41,173][227910] Average Trajectory Length: 998.7456666666666[0m
[36m[2023-07-10 15:03:41,178][227910] mean_value=262.4612122372384, max_value=2100.6573634654883[0m
[37m[1m[2023-07-10 15:03:41,181][227910] New mean coefficients: [[ 3.616086    0.67020124 -0.4824068   2.6221633   0.14448641]][0m
[37m[1m[2023-07-10 15:03:41,181][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:03:50,518][227910] train() took 9.33 seconds to complete[0m
[36m[2023-07-10 15:03:50,518][227910] FPS: 411371.50[0m
[36m[2023-07-10 15:03:50,520][227910] itr=672, itrs=2000, Progress: 33.60%[0m
[36m[2023-07-10 15:04:02,245][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 15:04:02,245][227910] FPS: 327986.92[0m
[36m[2023-07-10 15:04:07,009][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:04:07,009][227910] Reward + Measures: [[2398.54180032    0.23453946    0.32494295    0.55576259    0.23341881]][0m
[37m[1m[2023-07-10 15:04:07,010][227910] Max Reward on eval: 2398.541800316568[0m
[37m[1m[2023-07-10 15:04:07,010][227910] Min Reward on eval: 2398.541800316568[0m
[37m[1m[2023-07-10 15:04:07,010][227910] Mean Reward across all agents: 2398.541800316568[0m
[37m[1m[2023-07-10 15:04:07,010][227910] Average Trajectory Length: 998.8086666666667[0m
[36m[2023-07-10 15:04:12,364][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:04:12,365][227910] Reward + Measures: [[ 561.99035575    0.1574        0.25250003    0.3646        0.24910001]
 [ 237.63609021    0.1847        0.28239998    0.30149999    0.3066    ]
 [1639.97047363    0.21699999    0.30239999    0.51849997    0.28029999]
 ...
 [ 693.11383574    0.20990001    0.3109        0.41319999    0.33340001]
 [1889.24091422    0.23579998    0.31570002    0.52819997    0.2947    ]
 [ 510.79180293    0.14760001    0.24650002    0.3838        0.23629999]][0m
[37m[1m[2023-07-10 15:04:12,365][227910] Max Reward on eval: 2465.596772511909[0m
[37m[1m[2023-07-10 15:04:12,365][227910] Min Reward on eval: -500.2346033030306[0m
[37m[1m[2023-07-10 15:04:12,366][227910] Mean Reward across all agents: 1188.2606652214379[0m
[37m[1m[2023-07-10 15:04:12,366][227910] Average Trajectory Length: 984.5153333333333[0m
[36m[2023-07-10 15:04:12,370][227910] mean_value=224.0695889546748, max_value=2681.0215317152497[0m
[37m[1m[2023-07-10 15:04:12,373][227910] New mean coefficients: [[ 3.026053    0.657026   -0.47874555  2.2148583  -0.01256444]][0m
[37m[1m[2023-07-10 15:04:12,374][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:04:21,845][227910] train() took 9.47 seconds to complete[0m
[36m[2023-07-10 15:04:21,845][227910] FPS: 405523.73[0m
[36m[2023-07-10 15:04:21,847][227910] itr=673, itrs=2000, Progress: 33.65%[0m
[36m[2023-07-10 15:04:33,409][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 15:04:33,410][227910] FPS: 332618.74[0m
[36m[2023-07-10 15:04:38,052][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:04:38,053][227910] Reward + Measures: [[2510.44835795    0.23331815    0.32070467    0.56125659    0.22829537]][0m
[37m[1m[2023-07-10 15:04:38,053][227910] Max Reward on eval: 2510.448357946333[0m
[37m[1m[2023-07-10 15:04:38,053][227910] Min Reward on eval: 2510.448357946333[0m
[37m[1m[2023-07-10 15:04:38,053][227910] Mean Reward across all agents: 2510.448357946333[0m
[37m[1m[2023-07-10 15:04:38,053][227910] Average Trajectory Length: 999.5233333333333[0m
[36m[2023-07-10 15:04:43,370][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:04:43,371][227910] Reward + Measures: [[2088.6717173     0.2102        0.30630001    0.52020001    0.22739999]
 [2460.66887944    0.23140001    0.33129999    0.54759997    0.2385    ]
 [2114.94530008    0.22669999    0.3292        0.59019995    0.28220001]
 ...
 [2422.82692741    0.2287        0.26100001    0.55180007    0.24249999]
 [2361.63385465    0.22410002    0.33610001    0.57260001    0.24770001]
 [1660.36373437    0.1851        0.22400001    0.44010001    0.1864    ]][0m
[37m[1m[2023-07-10 15:04:43,371][227910] Max Reward on eval: 2654.561407819949[0m
[37m[1m[2023-07-10 15:04:43,372][227910] Min Reward on eval: 1510.387020436104[0m
[37m[1m[2023-07-10 15:04:43,372][227910] Mean Reward across all agents: 2309.3506634219175[0m
[37m[1m[2023-07-10 15:04:43,372][227910] Average Trajectory Length: 994.7433333333333[0m
[36m[2023-07-10 15:04:43,376][227910] mean_value=188.661699575313, max_value=1902.136212607644[0m
[37m[1m[2023-07-10 15:04:43,379][227910] New mean coefficients: [[ 3.4942462   0.3090005  -0.5151366   2.4214292   0.37737244]][0m
[37m[1m[2023-07-10 15:04:43,380][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:04:53,062][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 15:04:53,062][227910] FPS: 396692.63[0m
[36m[2023-07-10 15:04:53,065][227910] itr=674, itrs=2000, Progress: 33.70%[0m
[36m[2023-07-10 15:05:04,783][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 15:05:04,783][227910] FPS: 328164.27[0m
[36m[2023-07-10 15:05:09,211][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:05:09,211][227910] Reward + Measures: [[2624.03615418    0.23015936    0.31919134    0.56382674    0.22523804]][0m
[37m[1m[2023-07-10 15:05:09,211][227910] Max Reward on eval: 2624.036154175793[0m
[37m[1m[2023-07-10 15:05:09,211][227910] Min Reward on eval: 2624.036154175793[0m
[37m[1m[2023-07-10 15:05:09,212][227910] Mean Reward across all agents: 2624.036154175793[0m
[37m[1m[2023-07-10 15:05:09,212][227910] Average Trajectory Length: 997.9503333333333[0m
[36m[2023-07-10 15:05:14,557][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:05:14,558][227910] Reward + Measures: [[433.46995958   0.22481652   0.39272004   0.3134633    0.17345177]
 [781.52999279   0.25282618   0.43897697   0.33767405   0.19066551]
 [ 58.36211884   0.29412243   0.40177271   0.37229234   0.23228334]
 ...
 [135.14498128   0.2140805    0.39470538   0.30557725   0.20109613]
 [149.71424284   0.18607783   0.36441416   0.30476388   0.16903774]
 [291.47355394   0.18572102   0.34273371   0.29900095   0.15480199]][0m
[37m[1m[2023-07-10 15:05:14,558][227910] Max Reward on eval: 2635.348646428366[0m
[37m[1m[2023-07-10 15:05:14,558][227910] Min Reward on eval: -662.259008139107[0m
[37m[1m[2023-07-10 15:05:14,559][227910] Mean Reward across all agents: 600.707446056934[0m
[37m[1m[2023-07-10 15:05:14,559][227910] Average Trajectory Length: 757.293[0m
[36m[2023-07-10 15:05:14,560][227910] mean_value=-2007.925073625403, max_value=610.9705873774371[0m
[37m[1m[2023-07-10 15:05:14,562][227910] New mean coefficients: [[ 3.8842318   0.38581842 -0.40479898  1.1837862   0.1174491 ]][0m
[37m[1m[2023-07-10 15:05:14,563][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:05:23,915][227910] train() took 9.35 seconds to complete[0m
[36m[2023-07-10 15:05:23,915][227910] FPS: 410702.74[0m
[36m[2023-07-10 15:05:23,917][227910] itr=675, itrs=2000, Progress: 33.75%[0m
[36m[2023-07-10 15:05:35,435][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 15:05:35,435][227910] FPS: 333859.50[0m
[36m[2023-07-10 15:05:40,115][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:05:40,116][227910] Reward + Measures: [[2765.47941555    0.22750042    0.31939891    0.57371539    0.22114964]][0m
[37m[1m[2023-07-10 15:05:40,116][227910] Max Reward on eval: 2765.4794155507316[0m
[37m[1m[2023-07-10 15:05:40,116][227910] Min Reward on eval: 2765.4794155507316[0m
[37m[1m[2023-07-10 15:05:40,116][227910] Mean Reward across all agents: 2765.4794155507316[0m
[37m[1m[2023-07-10 15:05:40,116][227910] Average Trajectory Length: 999.5469999999999[0m
[36m[2023-07-10 15:05:45,470][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:05:45,470][227910] Reward + Measures: [[2354.17989057    0.2034        0.2816        0.54829997    0.1857    ]
 [1327.39133166    0.23009999    0.30250001    0.55430001    0.23130003]
 [ 964.04379494    0.1647        0.26589999    0.40349999    0.17300001]
 ...
 [ 898.28277931    0.40990001    0.27660003    0.62280005    0.2904    ]
 [ 629.81861694    0.24220002    0.31170002    0.53470004    0.23650001]
 [1644.98564444    0.18789999    0.28170002    0.43509999    0.21350001]][0m
[37m[1m[2023-07-10 15:05:45,471][227910] Max Reward on eval: 2870.1522947472054[0m
[37m[1m[2023-07-10 15:05:45,471][227910] Min Reward on eval: -1080.635290477774[0m
[37m[1m[2023-07-10 15:05:45,471][227910] Mean Reward across all agents: 1161.0002302912983[0m
[37m[1m[2023-07-10 15:05:45,471][227910] Average Trajectory Length: 974.7556666666667[0m
[36m[2023-07-10 15:05:45,475][227910] mean_value=-854.1445324710514, max_value=1898.6925822557496[0m
[37m[1m[2023-07-10 15:05:45,477][227910] New mean coefficients: [[3.7733378  0.4790423  0.0473015  1.1797289  0.11429058]][0m
[37m[1m[2023-07-10 15:05:45,478][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:05:55,096][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 15:05:55,096][227910] FPS: 399323.52[0m
[36m[2023-07-10 15:05:55,099][227910] itr=676, itrs=2000, Progress: 33.80%[0m
[36m[2023-07-10 15:06:06,778][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 15:06:06,778][227910] FPS: 329346.54[0m
[36m[2023-07-10 15:06:11,368][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:06:11,368][227910] Reward + Measures: [[2906.4325129     0.22239827    0.31046304    0.58951706    0.22138231]][0m
[37m[1m[2023-07-10 15:06:11,368][227910] Max Reward on eval: 2906.432512899944[0m
[37m[1m[2023-07-10 15:06:11,369][227910] Min Reward on eval: 2906.432512899944[0m
[37m[1m[2023-07-10 15:06:11,369][227910] Mean Reward across all agents: 2906.432512899944[0m
[37m[1m[2023-07-10 15:06:11,369][227910] Average Trajectory Length: 998.4406666666666[0m
[36m[2023-07-10 15:06:16,619][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:06:16,620][227910] Reward + Measures: [[ 727.41308642    0.17780001    0.2868        0.53470004    0.25260001]
 [1231.24687427    0.17430001    0.3281        0.50200003    0.20549999]
 [ 981.03530368    0.21842502    0.30107501    0.46177503    0.22005001]
 ...
 [ 553.02999458    0.18673609    0.28197774    0.52669829    0.23199697]
 [2037.76359528    0.20350002    0.3337        0.45790002    0.22130001]
 [1161.84846359    0.15657499    0.21352498    0.46772504    0.16585   ]][0m
[37m[1m[2023-07-10 15:06:16,620][227910] Max Reward on eval: 2641.172944859788[0m
[37m[1m[2023-07-10 15:06:16,620][227910] Min Reward on eval: -416.21980397480655[0m
[37m[1m[2023-07-10 15:06:16,620][227910] Mean Reward across all agents: 1200.4200053913776[0m
[37m[1m[2023-07-10 15:06:16,621][227910] Average Trajectory Length: 972.1139999999999[0m
[36m[2023-07-10 15:06:16,623][227910] mean_value=-680.7797657139828, max_value=1932.6312572109769[0m
[37m[1m[2023-07-10 15:06:16,626][227910] New mean coefficients: [[ 3.6061602   0.2003774  -0.22112203  1.1437612   0.00639335]][0m
[37m[1m[2023-07-10 15:06:16,627][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:06:26,212][227910] train() took 9.58 seconds to complete[0m
[36m[2023-07-10 15:06:26,213][227910] FPS: 400660.18[0m
[36m[2023-07-10 15:06:26,215][227910] itr=677, itrs=2000, Progress: 33.85%[0m
[36m[2023-07-10 15:06:37,889][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 15:06:37,890][227910] FPS: 329402.26[0m
[36m[2023-07-10 15:06:42,520][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:06:42,520][227910] Reward + Measures: [[3027.58288165    0.21762797    0.31098574    0.58975971    0.2166983 ]][0m
[37m[1m[2023-07-10 15:06:42,520][227910] Max Reward on eval: 3027.5828816536828[0m
[37m[1m[2023-07-10 15:06:42,520][227910] Min Reward on eval: 3027.5828816536828[0m
[37m[1m[2023-07-10 15:06:42,521][227910] Mean Reward across all agents: 3027.5828816536828[0m
[37m[1m[2023-07-10 15:06:42,521][227910] Average Trajectory Length: 998.8356666666666[0m
[36m[2023-07-10 15:06:47,611][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:06:47,612][227910] Reward + Measures: [[2755.9140899     0.22310002    0.3335        0.5557        0.22390001]
 [2433.50693152    0.22160001    0.34519997    0.50220001    0.23540001]
 [2693.60982568    0.2368        0.34520003    0.55419999    0.2129    ]
 ...
 [2254.7204029     0.2026        0.32809997    0.49849996    0.2026    ]
 [2401.34889095    0.25960001    0.34220001    0.60970002    0.25869998]
 [2889.176247      0.21619999    0.31210002    0.5916        0.21560001]][0m
[37m[1m[2023-07-10 15:06:47,612][227910] Max Reward on eval: 3119.406964363763[0m
[37m[1m[2023-07-10 15:06:47,612][227910] Min Reward on eval: 792.4928102818783[0m
[37m[1m[2023-07-10 15:06:47,612][227910] Mean Reward across all agents: 2624.2993742172634[0m
[37m[1m[2023-07-10 15:06:47,613][227910] Average Trajectory Length: 997.3736666666666[0m
[36m[2023-07-10 15:06:47,617][227910] mean_value=155.89779566609823, max_value=714.6078981457231[0m
[37m[1m[2023-07-10 15:06:47,619][227910] New mean coefficients: [[ 2.7267835   0.11714314 -0.2994702   1.1567436  -0.04769826]][0m
[37m[1m[2023-07-10 15:06:47,620][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:06:57,051][227910] train() took 9.43 seconds to complete[0m
[36m[2023-07-10 15:06:57,052][227910] FPS: 407231.05[0m
[36m[2023-07-10 15:06:57,054][227910] itr=678, itrs=2000, Progress: 33.90%[0m
[36m[2023-07-10 15:07:08,645][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 15:07:08,645][227910] FPS: 331883.41[0m
[36m[2023-07-10 15:07:13,041][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:07:13,041][227910] Reward + Measures: [[3133.91040956    0.21395546    0.30158094    0.59382707    0.2159867 ]][0m
[37m[1m[2023-07-10 15:07:13,041][227910] Max Reward on eval: 3133.910409559358[0m
[37m[1m[2023-07-10 15:07:13,041][227910] Min Reward on eval: 3133.910409559358[0m
[37m[1m[2023-07-10 15:07:13,042][227910] Mean Reward across all agents: 3133.910409559358[0m
[37m[1m[2023-07-10 15:07:13,042][227910] Average Trajectory Length: 999.0423333333333[0m
[36m[2023-07-10 15:07:18,097][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:07:18,098][227910] Reward + Measures: [[1337.3251263     0.26159999    0.44169998    0.51120001    0.29270002]
 [ -17.64841784    0.1401        0.49680001    0.33970001    0.38390002]
 [ 948.04865527    0.29353222    0.42157051    0.41888806    0.20567234]
 ...
 [2666.8924078     0.21900001    0.27830002    0.56730002    0.19930001]
 [2025.9438756     0.21740142    0.29266417    0.60461092    0.18367691]
 [1635.22539938    0.21669999    0.2949        0.40470001    0.2339    ]][0m
[37m[1m[2023-07-10 15:07:18,098][227910] Max Reward on eval: 3141.5394753862174[0m
[37m[1m[2023-07-10 15:07:18,098][227910] Min Reward on eval: -608.9853112782002[0m
[37m[1m[2023-07-10 15:07:18,098][227910] Mean Reward across all agents: 1222.9472439651047[0m
[37m[1m[2023-07-10 15:07:18,099][227910] Average Trajectory Length: 957.7769999999999[0m
[36m[2023-07-10 15:07:18,102][227910] mean_value=-453.92376668639093, max_value=2951.3491392277056[0m
[37m[1m[2023-07-10 15:07:18,104][227910] New mean coefficients: [[ 3.7016292   0.14526087 -0.2661988   0.25026196 -0.11408126]][0m
[37m[1m[2023-07-10 15:07:18,105][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:07:27,282][227910] train() took 9.17 seconds to complete[0m
[36m[2023-07-10 15:07:27,282][227910] FPS: 418530.00[0m
[36m[2023-07-10 15:07:27,284][227910] itr=679, itrs=2000, Progress: 33.95%[0m
[36m[2023-07-10 15:07:38,922][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 15:07:38,922][227910] FPS: 330473.77[0m
[36m[2023-07-10 15:07:43,333][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:07:43,333][227910] Reward + Measures: [[3223.05647037    0.21271512    0.3016606     0.60135382    0.21473031]][0m
[37m[1m[2023-07-10 15:07:43,334][227910] Max Reward on eval: 3223.0564703723776[0m
[37m[1m[2023-07-10 15:07:43,334][227910] Min Reward on eval: 3223.0564703723776[0m
[37m[1m[2023-07-10 15:07:43,334][227910] Mean Reward across all agents: 3223.0564703723776[0m
[37m[1m[2023-07-10 15:07:43,334][227910] Average Trajectory Length: 997.7493333333333[0m
[36m[2023-07-10 15:07:48,492][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:07:48,493][227910] Reward + Measures: [[2513.85639017    0.23640001    0.38210002    0.52640003    0.18859999]
 [1926.00434978    0.23436156    0.31536594    0.41752195    0.20493846]
 [  58.73776587    0.40799999    0.3522        0.4436        0.25420001]
 ...
 [2829.9333367     0.22360002    0.3603        0.54770005    0.189     ]
 [2498.69733487    0.23009999    0.28920001    0.4973        0.21429999]
 [1196.72836984    0.21222115    0.29816121    0.3607249     0.19715932]][0m
[37m[1m[2023-07-10 15:07:48,493][227910] Max Reward on eval: 3260.043587867543[0m
[37m[1m[2023-07-10 15:07:48,494][227910] Min Reward on eval: -414.6686182559817[0m
[37m[1m[2023-07-10 15:07:48,494][227910] Mean Reward across all agents: 1846.0100637244104[0m
[37m[1m[2023-07-10 15:07:48,494][227910] Average Trajectory Length: 937.293[0m
[36m[2023-07-10 15:07:48,497][227910] mean_value=-600.157078350927, max_value=909.6154035816053[0m
[37m[1m[2023-07-10 15:07:48,499][227910] New mean coefficients: [[ 2.9987934   0.08016197 -0.38513127  0.5114388   0.14360416]][0m
[37m[1m[2023-07-10 15:07:48,500][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:07:57,839][227910] train() took 9.34 seconds to complete[0m
[36m[2023-07-10 15:07:57,849][227910] FPS: 411264.62[0m
[36m[2023-07-10 15:07:57,856][227910] itr=680, itrs=2000, Progress: 34.00%[0m
[37m[1m[2023-07-10 15:08:00,928][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000660[0m
[36m[2023-07-10 15:08:12,748][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 15:08:12,748][227910] FPS: 332248.88[0m
[36m[2023-07-10 15:08:17,122][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:08:17,122][227910] Reward + Measures: [[3340.09030043    0.20718274    0.3037459     0.59766251    0.21292682]][0m
[37m[1m[2023-07-10 15:08:17,122][227910] Max Reward on eval: 3340.0903004254315[0m
[37m[1m[2023-07-10 15:08:17,122][227910] Min Reward on eval: 3340.0903004254315[0m
[37m[1m[2023-07-10 15:08:17,123][227910] Mean Reward across all agents: 3340.0903004254315[0m
[37m[1m[2023-07-10 15:08:17,123][227910] Average Trajectory Length: 999.0563333333333[0m
[36m[2023-07-10 15:08:22,271][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:08:22,272][227910] Reward + Measures: [[2861.82965666    0.22330001    0.29179999    0.53249997    0.22070001]
 [3316.22928358    0.21870001    0.3184        0.58780003    0.21519999]
 [3086.01389385    0.20419998    0.30890003    0.60190004    0.21369998]
 ...
 [3282.8801848     0.2112        0.32410002    0.60600001    0.2053    ]
 [2836.50888106    0.23580001    0.31850001    0.62799996    0.24130002]
 [2825.95777323    0.20630001    0.31200001    0.56240004    0.2106    ]][0m
[37m[1m[2023-07-10 15:08:22,272][227910] Max Reward on eval: 3390.1915495017543[0m
[37m[1m[2023-07-10 15:08:22,272][227910] Min Reward on eval: 1107.7760734189155[0m
[37m[1m[2023-07-10 15:08:22,272][227910] Mean Reward across all agents: 2907.4127702166024[0m
[37m[1m[2023-07-10 15:08:22,273][227910] Average Trajectory Length: 995.944[0m
[36m[2023-07-10 15:08:22,276][227910] mean_value=-34.22760408433045, max_value=2059.386221661313[0m
[37m[1m[2023-07-10 15:08:22,278][227910] New mean coefficients: [[ 2.7400925   0.2333955  -0.19070338 -0.2491278  -0.08731365]][0m
[37m[1m[2023-07-10 15:08:22,279][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:08:31,419][227910] train() took 9.14 seconds to complete[0m
[36m[2023-07-10 15:08:31,420][227910] FPS: 420212.27[0m
[36m[2023-07-10 15:08:31,422][227910] itr=681, itrs=2000, Progress: 34.05%[0m
[36m[2023-07-10 15:08:43,064][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 15:08:43,064][227910] FPS: 330299.37[0m
[36m[2023-07-10 15:08:47,315][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:08:47,316][227910] Reward + Measures: [[3460.09418846    0.20506716    0.29046312    0.59761417    0.21008508]][0m
[37m[1m[2023-07-10 15:08:47,316][227910] Max Reward on eval: 3460.094188460166[0m
[37m[1m[2023-07-10 15:08:47,316][227910] Min Reward on eval: 3460.094188460166[0m
[37m[1m[2023-07-10 15:08:47,316][227910] Mean Reward across all agents: 3460.094188460166[0m
[37m[1m[2023-07-10 15:08:47,316][227910] Average Trajectory Length: 996.5709999999999[0m
[36m[2023-07-10 15:08:52,525][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:08:52,526][227910] Reward + Measures: [[2775.91811042    0.24130002    0.35510001    0.47170001    0.2106    ]
 [ 141.47646634    0.23748913    0.37377906    0.30710527    0.21100275]
 [2094.59840221    0.2141        0.22389999    0.42160001    0.21879999]
 ...
 [1913.88206453    0.24159999    0.39949998    0.43009996    0.17419998]
 [3071.19302289    0.21269999    0.345         0.55699998    0.19160001]
 [1489.1975706     0.24320002    0.39960003    0.45609999    0.19059999]][0m
[37m[1m[2023-07-10 15:08:52,526][227910] Max Reward on eval: 3464.9988301615695[0m
[37m[1m[2023-07-10 15:08:52,526][227910] Min Reward on eval: -192.7478269327956[0m
[37m[1m[2023-07-10 15:08:52,527][227910] Mean Reward across all agents: 1775.5386973286772[0m
[37m[1m[2023-07-10 15:08:52,527][227910] Average Trajectory Length: 983.8913333333333[0m
[36m[2023-07-10 15:08:52,530][227910] mean_value=-660.2452282857098, max_value=1536.5278018456124[0m
[37m[1m[2023-07-10 15:08:52,532][227910] New mean coefficients: [[ 2.0679855  -0.12719826 -0.11301115 -0.6298897  -0.19803801]][0m
[37m[1m[2023-07-10 15:08:52,533][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:09:01,697][227910] train() took 9.16 seconds to complete[0m
[36m[2023-07-10 15:09:01,697][227910] FPS: 419105.04[0m
[36m[2023-07-10 15:09:01,699][227910] itr=682, itrs=2000, Progress: 34.10%[0m
[36m[2023-07-10 15:09:13,306][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 15:09:13,306][227910] FPS: 331313.59[0m
[36m[2023-07-10 15:09:17,564][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:09:17,564][227910] Reward + Measures: [[3561.40380068    0.19465938    0.27965161    0.59228671    0.20453116]][0m
[37m[1m[2023-07-10 15:09:17,564][227910] Max Reward on eval: 3561.403800678473[0m
[37m[1m[2023-07-10 15:09:17,564][227910] Min Reward on eval: 3561.403800678473[0m
[37m[1m[2023-07-10 15:09:17,565][227910] Mean Reward across all agents: 3561.403800678473[0m
[37m[1m[2023-07-10 15:09:17,565][227910] Average Trajectory Length: 994.9356666666666[0m
[36m[2023-07-10 15:09:22,639][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:09:22,640][227910] Reward + Measures: [[2130.60760822    0.20110002    0.27770004    0.46650001    0.21420002]
 [1146.54141121    0.21570002    0.27720001    0.37169999    0.23269999]
 [2702.41379969    0.23435402    0.36397573    0.47533417    0.15574658]
 ...
 [3084.59769352    0.20650001    0.2976        0.50209999    0.2086    ]
 [2363.95273453    0.24442632    0.33937368    0.57683688    0.18730001]
 [ 297.29586927    0.46019998    0.47230002    0.53960001    0.14600001]][0m
[37m[1m[2023-07-10 15:09:22,640][227910] Max Reward on eval: 3464.8159113521224[0m
[37m[1m[2023-07-10 15:09:22,640][227910] Min Reward on eval: -1074.6922924080748[0m
[37m[1m[2023-07-10 15:09:22,641][227910] Mean Reward across all agents: 1684.1319235275937[0m
[37m[1m[2023-07-10 15:09:22,641][227910] Average Trajectory Length: 978.6113333333333[0m
[36m[2023-07-10 15:09:22,644][227910] mean_value=-483.6059965348793, max_value=3122.2117211087775[0m
[37m[1m[2023-07-10 15:09:22,647][227910] New mean coefficients: [[ 2.1088843  -0.3026966  -0.17912664 -0.44893128 -0.18167204]][0m
[37m[1m[2023-07-10 15:09:22,648][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:09:31,744][227910] train() took 9.09 seconds to complete[0m
[36m[2023-07-10 15:09:31,744][227910] FPS: 422261.48[0m
[36m[2023-07-10 15:09:31,746][227910] itr=683, itrs=2000, Progress: 34.15%[0m
[36m[2023-07-10 15:09:43,367][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 15:09:43,367][227910] FPS: 330923.15[0m
[36m[2023-07-10 15:09:47,872][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:09:47,873][227910] Reward + Measures: [[3665.73722095    0.19215654    0.26942125    0.59479833    0.20196912]][0m
[37m[1m[2023-07-10 15:09:47,873][227910] Max Reward on eval: 3665.737220950432[0m
[37m[1m[2023-07-10 15:09:47,873][227910] Min Reward on eval: 3665.737220950432[0m
[37m[1m[2023-07-10 15:09:47,873][227910] Mean Reward across all agents: 3665.737220950432[0m
[37m[1m[2023-07-10 15:09:47,873][227910] Average Trajectory Length: 993.7983333333333[0m
[36m[2023-07-10 15:09:52,983][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:09:52,984][227910] Reward + Measures: [[3076.42720362    0.1956        0.36110005    0.5535        0.19399999]
 [1876.48014688    0.22068496    0.28235799    0.43419024    0.19591348]
 [2886.43701433    0.19190001    0.29689997    0.53960001    0.1955    ]
 ...
 [2500.33671493    0.21756902    0.29050285    0.55889577    0.21321832]
 [3482.70190166    0.2069        0.31960002    0.60210001    0.20719998]
 [3537.08836209    0.1947        0.3283        0.57599998    0.2041    ]][0m
[37m[1m[2023-07-10 15:09:52,984][227910] Max Reward on eval: 3701.7275178474374[0m
[37m[1m[2023-07-10 15:09:52,984][227910] Min Reward on eval: 935.1239720963465[0m
[37m[1m[2023-07-10 15:09:52,984][227910] Mean Reward across all agents: 2680.535510031738[0m
[37m[1m[2023-07-10 15:09:52,985][227910] Average Trajectory Length: 989.8073333333333[0m
[36m[2023-07-10 15:09:52,989][227910] mean_value=225.5462306682875, max_value=2466.0249284546967[0m
[37m[1m[2023-07-10 15:09:52,991][227910] New mean coefficients: [[ 2.0642896  -0.08916423  0.42403823 -0.803643   -0.10707285]][0m
[37m[1m[2023-07-10 15:09:52,992][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:10:02,384][227910] train() took 9.39 seconds to complete[0m
[36m[2023-07-10 15:10:02,384][227910] FPS: 408959.47[0m
[36m[2023-07-10 15:10:02,386][227910] itr=684, itrs=2000, Progress: 34.20%[0m
[36m[2023-07-10 15:10:14,108][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 15:10:14,108][227910] FPS: 328149.79[0m
[36m[2023-07-10 15:10:18,452][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:10:18,452][227910] Reward + Measures: [[3736.51469709    0.18815091    0.25688159    0.58353221    0.20258133]][0m
[37m[1m[2023-07-10 15:10:18,452][227910] Max Reward on eval: 3736.514697093924[0m
[37m[1m[2023-07-10 15:10:18,452][227910] Min Reward on eval: 3736.514697093924[0m
[37m[1m[2023-07-10 15:10:18,453][227910] Mean Reward across all agents: 3736.514697093924[0m
[37m[1m[2023-07-10 15:10:18,453][227910] Average Trajectory Length: 990.4223333333333[0m
[36m[2023-07-10 15:10:23,642][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:10:23,643][227910] Reward + Measures: [[2666.38084025    0.19530001    0.26910001    0.60710001    0.18520001]
 [ 803.53037361    0.27119008    0.36639816    0.29015341    0.23146334]
 [1277.10679065    0.21700001    0.30050001    0.3168        0.22420001]
 ...
 [1458.80635444    0.1782271     0.27496073    0.32564902    0.16333374]
 [2780.54906988    0.22710001    0.25650001    0.48020002    0.2158    ]
 [2917.0576253     0.19351904    0.25666189    0.59439045    0.18127619]][0m
[37m[1m[2023-07-10 15:10:23,643][227910] Max Reward on eval: 3716.12664099019[0m
[37m[1m[2023-07-10 15:10:23,643][227910] Min Reward on eval: -711.253571602286[0m
[37m[1m[2023-07-10 15:10:23,643][227910] Mean Reward across all agents: 1541.8171861757824[0m
[37m[1m[2023-07-10 15:10:23,644][227910] Average Trajectory Length: 967.9266666666666[0m
[36m[2023-07-10 15:10:23,646][227910] mean_value=-991.2401037766606, max_value=723.4501023015382[0m
[37m[1m[2023-07-10 15:10:23,649][227910] New mean coefficients: [[ 1.8501719  -0.16239336  0.21192622 -0.20582855 -0.05385533]][0m
[37m[1m[2023-07-10 15:10:23,650][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:10:32,881][227910] train() took 9.23 seconds to complete[0m
[36m[2023-07-10 15:10:32,882][227910] FPS: 416032.72[0m
[36m[2023-07-10 15:10:32,884][227910] itr=685, itrs=2000, Progress: 34.25%[0m
[36m[2023-07-10 15:10:44,520][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 15:10:44,520][227910] FPS: 330483.03[0m
[36m[2023-07-10 15:10:48,914][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:10:48,914][227910] Reward + Measures: [[3839.21637531    0.18233274    0.25539342    0.56860161    0.19929032]][0m
[37m[1m[2023-07-10 15:10:48,914][227910] Max Reward on eval: 3839.216375305809[0m
[37m[1m[2023-07-10 15:10:48,914][227910] Min Reward on eval: 3839.216375305809[0m
[37m[1m[2023-07-10 15:10:48,915][227910] Mean Reward across all agents: 3839.216375305809[0m
[37m[1m[2023-07-10 15:10:48,915][227910] Average Trajectory Length: 984.5406666666667[0m
[36m[2023-07-10 15:10:53,991][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:10:53,992][227910] Reward + Measures: [[3433.63908248    0.1925        0.34990001    0.523         0.19719999]
 [2768.56980026    0.21209998    0.22979999    0.51920003    0.19320002]
 [ 993.95565201    0.33610001    0.3856        0.28060004    0.26440001]
 ...
 [2188.1214272     0.2368        0.31119999    0.48699999    0.2404    ]
 [2665.41723993    0.22130001    0.25530002    0.49690005    0.20089999]
 [1701.14563017    0.25369999    0.2818        0.49670002    0.23639999]][0m
[37m[1m[2023-07-10 15:10:53,992][227910] Max Reward on eval: 3826.274101080373[0m
[37m[1m[2023-07-10 15:10:53,992][227910] Min Reward on eval: 45.99336617263034[0m
[37m[1m[2023-07-10 15:10:53,993][227910] Mean Reward across all agents: 2556.7696932934473[0m
[37m[1m[2023-07-10 15:10:53,993][227910] Average Trajectory Length: 983.726[0m
[36m[2023-07-10 15:10:53,996][227910] mean_value=-376.2655856007986, max_value=1945.5999071908068[0m
[37m[1m[2023-07-10 15:10:53,998][227910] New mean coefficients: [[ 1.3464432  -0.14890377 -0.06160602 -0.30782896 -0.08802221]][0m
[37m[1m[2023-07-10 15:10:53,999][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:11:03,198][227910] train() took 9.20 seconds to complete[0m
[36m[2023-07-10 15:11:03,199][227910] FPS: 417505.81[0m
[36m[2023-07-10 15:11:03,201][227910] itr=686, itrs=2000, Progress: 34.30%[0m
[36m[2023-07-10 15:11:14,859][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 15:11:14,859][227910] FPS: 329939.08[0m
[36m[2023-07-10 15:11:19,244][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:11:19,245][227910] Reward + Measures: [[3970.71553981    0.18109398    0.26671022    0.56610805    0.19415478]][0m
[37m[1m[2023-07-10 15:11:19,245][227910] Max Reward on eval: 3970.7155398084924[0m
[37m[1m[2023-07-10 15:11:19,245][227910] Min Reward on eval: 3970.7155398084924[0m
[37m[1m[2023-07-10 15:11:19,246][227910] Mean Reward across all agents: 3970.7155398084924[0m
[37m[1m[2023-07-10 15:11:19,246][227910] Average Trajectory Length: 990.7199999999999[0m
[36m[2023-07-10 15:11:24,449][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:11:24,449][227910] Reward + Measures: [[2714.78967566    0.21519999    0.38910004    0.47639999    0.23850003]
 [3228.70523375    0.20030001    0.3098        0.52559996    0.2234    ]
 [1410.29284174    0.36726788    0.43415889    0.38152975    0.23246408]
 ...
 [2283.48823441    0.20289998    0.21069999    0.528         0.20990001]
 [ 217.60039426    0.20124757    0.17220117    0.33470485    0.18891369]
 [3383.07306157    0.20560001    0.2502        0.61260003    0.21510001]][0m
[37m[1m[2023-07-10 15:11:24,450][227910] Max Reward on eval: 4001.0716343209147[0m
[37m[1m[2023-07-10 15:11:24,450][227910] Min Reward on eval: 195.158927551948[0m
[37m[1m[2023-07-10 15:11:24,450][227910] Mean Reward across all agents: 2514.208003553917[0m
[37m[1m[2023-07-10 15:11:24,450][227910] Average Trajectory Length: 968.168[0m
[36m[2023-07-10 15:11:24,454][227910] mean_value=-397.1674489386359, max_value=2610.099430593267[0m
[37m[1m[2023-07-10 15:11:24,457][227910] New mean coefficients: [[ 1.7289743  -0.26795897  0.09623933 -0.21230505 -0.01561639]][0m
[37m[1m[2023-07-10 15:11:24,458][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:11:33,721][227910] train() took 9.26 seconds to complete[0m
[36m[2023-07-10 15:11:33,722][227910] FPS: 414602.40[0m
[36m[2023-07-10 15:11:33,724][227910] itr=687, itrs=2000, Progress: 34.35%[0m
[36m[2023-07-10 15:11:45,459][227910] train() took 11.72 seconds to complete[0m
[36m[2023-07-10 15:11:45,460][227910] FPS: 327673.41[0m
[36m[2023-07-10 15:11:49,837][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:11:49,837][227910] Reward + Measures: [[4041.73446495    0.17797388    0.26774952    0.56131178    0.18771578]][0m
[37m[1m[2023-07-10 15:11:49,837][227910] Max Reward on eval: 4041.734464949197[0m
[37m[1m[2023-07-10 15:11:49,838][227910] Min Reward on eval: 4041.734464949197[0m
[37m[1m[2023-07-10 15:11:49,838][227910] Mean Reward across all agents: 4041.734464949197[0m
[37m[1m[2023-07-10 15:11:49,838][227910] Average Trajectory Length: 987.6243333333333[0m
[36m[2023-07-10 15:11:54,911][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:11:54,925][227910] Reward + Measures: [[3140.21399227    0.1829105     0.29734945    0.50771374    0.17757225]
 [3712.61191327    0.18110001    0.2665        0.60780001    0.176     ]
 [3987.55512102    0.1882        0.26540002    0.55350006    0.19930001]
 ...
 [3397.52568525    0.16440001    0.27695003    0.55284995    0.17704999]
 [3426.9675714     0.18938938    0.2659865     0.57361966    0.19294004]
 [3502.25864765    0.1979        0.31560001    0.5248        0.18020001]][0m
[37m[1m[2023-07-10 15:11:54,925][227910] Max Reward on eval: 4066.7269764263183[0m
[37m[1m[2023-07-10 15:11:54,925][227910] Min Reward on eval: 257.6129846403084[0m
[37m[1m[2023-07-10 15:11:54,926][227910] Mean Reward across all agents: 2751.895572351059[0m
[37m[1m[2023-07-10 15:11:54,926][227910] Average Trajectory Length: 976.0946666666666[0m
[36m[2023-07-10 15:11:54,929][227910] mean_value=-113.76227291593226, max_value=4003.3011322658417[0m
[37m[1m[2023-07-10 15:11:54,932][227910] New mean coefficients: [[ 1.9701015  -0.05263513  0.24074608 -0.35180157 -0.03373392]][0m
[37m[1m[2023-07-10 15:11:54,933][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:12:04,160][227910] train() took 9.23 seconds to complete[0m
[36m[2023-07-10 15:12:04,160][227910] FPS: 416247.56[0m
[36m[2023-07-10 15:12:04,162][227910] itr=688, itrs=2000, Progress: 34.40%[0m
[36m[2023-07-10 15:12:15,727][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 15:12:15,727][227910] FPS: 332553.42[0m
[36m[2023-07-10 15:12:20,555][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:12:20,555][227910] Reward + Measures: [[4154.54501393    0.1753833     0.2751011     0.55479175    0.18364444]][0m
[37m[1m[2023-07-10 15:12:20,556][227910] Max Reward on eval: 4154.545013932313[0m
[37m[1m[2023-07-10 15:12:20,556][227910] Min Reward on eval: 4154.545013932313[0m
[37m[1m[2023-07-10 15:12:20,556][227910] Mean Reward across all agents: 4154.545013932313[0m
[37m[1m[2023-07-10 15:12:20,556][227910] Average Trajectory Length: 987.4433333333333[0m
[36m[2023-07-10 15:12:25,966][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:12:25,966][227910] Reward + Measures: [[2132.14667421    0.20816568    0.31213042    0.48173079    0.19974124]
 [ 951.79371161    0.30538592    0.29928407    0.43484733    0.20605205]
 [1991.29192651    0.17619462    0.30247399    0.35917309    0.17211883]
 ...
 [2227.46279725    0.20999999    0.28400001    0.54519999    0.21900001]
 [3167.74794591    0.20121832    0.28835246    0.52663678    0.18387337]
 [1914.51439394    0.18816666    0.26794168    0.4442583     0.22449167]][0m
[37m[1m[2023-07-10 15:12:25,966][227910] Max Reward on eval: 4167.742310683522[0m
[37m[1m[2023-07-10 15:12:25,967][227910] Min Reward on eval: 374.59142060304583[0m
[37m[1m[2023-07-10 15:12:25,967][227910] Mean Reward across all agents: 2443.4900199332797[0m
[37m[1m[2023-07-10 15:12:25,967][227910] Average Trajectory Length: 922.069[0m
[36m[2023-07-10 15:12:25,970][227910] mean_value=-544.6632978220572, max_value=2093.945638114893[0m
[37m[1m[2023-07-10 15:12:25,973][227910] New mean coefficients: [[ 1.9201099   0.09226881  0.22664678 -0.16807598  0.10214332]][0m
[37m[1m[2023-07-10 15:12:25,974][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:12:35,717][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 15:12:35,717][227910] FPS: 394174.91[0m
[36m[2023-07-10 15:12:35,720][227910] itr=689, itrs=2000, Progress: 34.45%[0m
[36m[2023-07-10 15:12:47,190][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 15:12:47,190][227910] FPS: 335356.48[0m
[36m[2023-07-10 15:12:52,033][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:12:52,034][227910] Reward + Measures: [[4250.91138597    0.17278942    0.26465172    0.55045664    0.18092488]][0m
[37m[1m[2023-07-10 15:12:52,034][227910] Max Reward on eval: 4250.911385967123[0m
[37m[1m[2023-07-10 15:12:52,035][227910] Min Reward on eval: 4250.911385967123[0m
[37m[1m[2023-07-10 15:12:52,035][227910] Mean Reward across all agents: 4250.911385967123[0m
[37m[1m[2023-07-10 15:12:52,035][227910] Average Trajectory Length: 982.6816666666666[0m
[36m[2023-07-10 15:12:57,451][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:12:57,457][227910] Reward + Measures: [[4154.00055025    0.17961311    0.26019263    0.56660652    0.18586721]
 [4134.94393983    0.175         0.29229999    0.57080001    0.18730001]
 [3888.66464062    0.20280002    0.28330001    0.54839998    0.18240002]
 ...
 [3110.66549248    0.1961989     0.27377138    0.52403748    0.1902989 ]
 [2748.18759228    0.21650787    0.27738521    0.56137598    0.20520821]
 [4105.53779279    0.18480001    0.27620003    0.54580003    0.1962    ]][0m
[37m[1m[2023-07-10 15:12:57,457][227910] Max Reward on eval: 4382.615197154647[0m
[37m[1m[2023-07-10 15:12:57,457][227910] Min Reward on eval: 987.9165030673961[0m
[37m[1m[2023-07-10 15:12:57,457][227910] Mean Reward across all agents: 3530.5344163692766[0m
[37m[1m[2023-07-10 15:12:57,458][227910] Average Trajectory Length: 978.2143333333333[0m
[36m[2023-07-10 15:12:57,461][227910] mean_value=-121.45977944139537, max_value=3034.7616399445515[0m
[37m[1m[2023-07-10 15:12:57,463][227910] New mean coefficients: [[ 1.7730473   0.17472121  0.11418825 -0.07125214  0.16790044]][0m
[37m[1m[2023-07-10 15:12:57,465][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:13:07,203][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 15:13:07,204][227910] FPS: 394367.74[0m
[36m[2023-07-10 15:13:07,206][227910] itr=690, itrs=2000, Progress: 34.50%[0m
[37m[1m[2023-07-10 15:13:10,434][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000670[0m
[36m[2023-07-10 15:13:22,321][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 15:13:22,321][227910] FPS: 329127.87[0m
[36m[2023-07-10 15:13:27,059][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:13:27,060][227910] Reward + Measures: [[4386.28377486    0.17055675    0.26510072    0.55327344    0.17779779]][0m
[37m[1m[2023-07-10 15:13:27,060][227910] Max Reward on eval: 4386.283774856583[0m
[37m[1m[2023-07-10 15:13:27,060][227910] Min Reward on eval: 4386.283774856583[0m
[37m[1m[2023-07-10 15:13:27,061][227910] Mean Reward across all agents: 4386.283774856583[0m
[37m[1m[2023-07-10 15:13:27,061][227910] Average Trajectory Length: 986.216[0m
[36m[2023-07-10 15:13:32,552][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:13:32,552][227910] Reward + Measures: [[3411.80792203    0.1926        0.1961        0.61310005    0.17749999]
 [1642.03835119    0.13193987    0.27878499    0.34762409    0.18320818]
 [4198.72943863    0.17380002    0.3321        0.5158        0.16849999]
 ...
 [2950.41130874    0.2084        0.4048        0.47480002    0.24660002]
 [1516.65905875    0.17633757    0.23840478    0.4728553     0.19573158]
 [3732.37830151    0.21985812    0.22072907    0.61170352    0.17017905]][0m
[37m[1m[2023-07-10 15:13:32,553][227910] Max Reward on eval: 4407.815352199413[0m
[37m[1m[2023-07-10 15:13:32,553][227910] Min Reward on eval: 26.42661635208351[0m
[37m[1m[2023-07-10 15:13:32,553][227910] Mean Reward across all agents: 2688.8474975285735[0m
[37m[1m[2023-07-10 15:13:32,553][227910] Average Trajectory Length: 942.3266666666666[0m
[36m[2023-07-10 15:13:32,557][227910] mean_value=-443.26547019328495, max_value=2337.999346981012[0m
[37m[1m[2023-07-10 15:13:32,560][227910] New mean coefficients: [[1.8208696  0.05420747 0.12358078 0.14092267 0.21418294]][0m
[37m[1m[2023-07-10 15:13:32,561][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:13:42,336][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 15:13:42,337][227910] FPS: 392882.33[0m
[36m[2023-07-10 15:13:42,339][227910] itr=691, itrs=2000, Progress: 34.55%[0m
[36m[2023-07-10 15:13:53,861][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 15:13:53,861][227910] FPS: 333846.48[0m
[36m[2023-07-10 15:13:58,601][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:13:58,602][227910] Reward + Measures: [[4475.70250659    0.16833097    0.2723394     0.54654264    0.17751397]][0m
[37m[1m[2023-07-10 15:13:58,602][227910] Max Reward on eval: 4475.702506587564[0m
[37m[1m[2023-07-10 15:13:58,602][227910] Min Reward on eval: 4475.702506587564[0m
[37m[1m[2023-07-10 15:13:58,602][227910] Mean Reward across all agents: 4475.702506587564[0m
[37m[1m[2023-07-10 15:13:58,602][227910] Average Trajectory Length: 989.0056666666667[0m
[36m[2023-07-10 15:14:04,088][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:14:04,088][227910] Reward + Measures: [[3990.1754132     0.16921221    0.25635529    0.52292359    0.16106668]
 [1706.36618173    0.20899999    0.35859999    0.29750001    0.26899999]
 [3349.09955242    0.19258392    0.36090517    0.494414      0.19584148]
 ...
 [4106.98761838    0.17331702    0.26968557    0.57544583    0.18119955]
 [2907.47730881    0.19088767    0.34311357    0.60382843    0.19921854]
 [2847.29063368    0.24770001    0.26929998    0.55730003    0.17830001]][0m
[37m[1m[2023-07-10 15:14:04,089][227910] Max Reward on eval: 4512.672366215289[0m
[37m[1m[2023-07-10 15:14:04,089][227910] Min Reward on eval: 1180.550070732835[0m
[37m[1m[2023-07-10 15:14:04,089][227910] Mean Reward across all agents: 3455.4226713749636[0m
[37m[1m[2023-07-10 15:14:04,089][227910] Average Trajectory Length: 970.087[0m
[36m[2023-07-10 15:14:04,093][227910] mean_value=-178.00659328171025, max_value=1458.2994252110757[0m
[37m[1m[2023-07-10 15:14:04,095][227910] New mean coefficients: [[1.610838   0.13034049 0.07503845 0.25029248 0.10537302]][0m
[37m[1m[2023-07-10 15:14:04,096][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:14:13,885][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 15:14:13,886][227910] FPS: 392343.81[0m
[36m[2023-07-10 15:14:13,888][227910] itr=692, itrs=2000, Progress: 34.60%[0m
[36m[2023-07-10 15:14:25,610][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 15:14:25,610][227910] FPS: 328133.67[0m
[36m[2023-07-10 15:14:30,348][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:14:30,348][227910] Reward + Measures: [[4642.60135332    0.16480005    0.26787263    0.55082786    0.17258699]][0m
[37m[1m[2023-07-10 15:14:30,348][227910] Max Reward on eval: 4642.601353324825[0m
[37m[1m[2023-07-10 15:14:30,349][227910] Min Reward on eval: 4642.601353324825[0m
[37m[1m[2023-07-10 15:14:30,349][227910] Mean Reward across all agents: 4642.601353324825[0m
[37m[1m[2023-07-10 15:14:30,349][227910] Average Trajectory Length: 991.9263333333333[0m
[36m[2023-07-10 15:14:35,765][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:14:35,771][227910] Reward + Measures: [[4233.82115985    0.19600001    0.30090004    0.51209998    0.19589999]
 [3920.03061487    0.18870001    0.29449999    0.51980001    0.2076    ]
 [1565.58772683    0.210096      0.41364977    0.45812264    0.13648394]
 ...
 [2436.97706542    0.25229999    0.30930001    0.59580004    0.14310001]
 [4126.34360789    0.1997        0.34710002    0.59119999    0.19069999]
 [3463.51780644    0.22260001    0.35180002    0.50220001    0.2306    ]][0m
[37m[1m[2023-07-10 15:14:35,772][227910] Max Reward on eval: 4744.242216978036[0m
[37m[1m[2023-07-10 15:14:35,772][227910] Min Reward on eval: 406.65285318273527[0m
[37m[1m[2023-07-10 15:14:35,772][227910] Mean Reward across all agents: 3235.103770853132[0m
[37m[1m[2023-07-10 15:14:35,772][227910] Average Trajectory Length: 949.1336666666666[0m
[36m[2023-07-10 15:14:35,776][227910] mean_value=-67.28074140975122, max_value=2493.4469113875593[0m
[37m[1m[2023-07-10 15:14:35,779][227910] New mean coefficients: [[1.485633   0.24330676 0.15088221 0.16190696 0.01054683]][0m
[37m[1m[2023-07-10 15:14:35,780][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:14:45,467][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 15:14:45,467][227910] FPS: 396489.49[0m
[36m[2023-07-10 15:14:45,469][227910] itr=693, itrs=2000, Progress: 34.65%[0m
[36m[2023-07-10 15:14:57,003][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 15:14:57,003][227910] FPS: 333464.93[0m
[36m[2023-07-10 15:15:01,894][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:15:01,894][227910] Reward + Measures: [[4779.48931363    0.16582       0.2728667     0.54044783    0.17082462]][0m
[37m[1m[2023-07-10 15:15:01,894][227910] Max Reward on eval: 4779.48931363237[0m
[37m[1m[2023-07-10 15:15:01,894][227910] Min Reward on eval: 4779.48931363237[0m
[37m[1m[2023-07-10 15:15:01,895][227910] Mean Reward across all agents: 4779.48931363237[0m
[37m[1m[2023-07-10 15:15:01,895][227910] Average Trajectory Length: 993.4923333333332[0m
[36m[2023-07-10 15:15:07,569][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:15:07,569][227910] Reward + Measures: [[2516.35685794    0.2168        0.34760001    0.41280004    0.2386    ]
 [ 809.30375933    0.1473        0.229         0.245         0.1018    ]
 [ 402.31239353    0.14836238    0.23726955    0.21904023    0.09761601]
 ...
 [4313.43109832    0.1753        0.26320001    0.52969998    0.18719999]
 [2627.10234695    0.20910001    0.3193        0.38850001    0.1592    ]
 [2700.8985339     0.2174        0.26519999    0.48079997    0.17119999]][0m
[37m[1m[2023-07-10 15:15:07,570][227910] Max Reward on eval: 4791.959750336408[0m
[37m[1m[2023-07-10 15:15:07,570][227910] Min Reward on eval: -557.0849401830725[0m
[37m[1m[2023-07-10 15:15:07,570][227910] Mean Reward across all agents: 2507.052451523081[0m
[37m[1m[2023-07-10 15:15:07,570][227910] Average Trajectory Length: 964.6206666666666[0m
[36m[2023-07-10 15:15:07,573][227910] mean_value=-611.8581159594273, max_value=1859.7264063148186[0m
[37m[1m[2023-07-10 15:15:07,575][227910] New mean coefficients: [[ 1.114434    0.17406029  0.10677011 -0.02422187  0.02024122]][0m
[37m[1m[2023-07-10 15:15:07,576][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:15:17,293][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 15:15:17,293][227910] FPS: 395259.83[0m
[36m[2023-07-10 15:15:17,296][227910] itr=694, itrs=2000, Progress: 34.70%[0m
[36m[2023-07-10 15:15:28,901][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 15:15:28,901][227910] FPS: 331361.54[0m
[36m[2023-07-10 15:15:33,730][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:15:33,731][227910] Reward + Measures: [[4871.73543048    0.16444932    0.27951199    0.52121967    0.16670772]][0m
[37m[1m[2023-07-10 15:15:33,731][227910] Max Reward on eval: 4871.735430480739[0m
[37m[1m[2023-07-10 15:15:33,731][227910] Min Reward on eval: 4871.735430480739[0m
[37m[1m[2023-07-10 15:15:33,732][227910] Mean Reward across all agents: 4871.735430480739[0m
[37m[1m[2023-07-10 15:15:33,732][227910] Average Trajectory Length: 990.5643333333333[0m
[36m[2023-07-10 15:15:39,205][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:15:39,205][227910] Reward + Measures: [[3175.32548026    0.20250002    0.39350006    0.49959999    0.17190002]
 [3240.87013063    0.21140002    0.32820001    0.52410001    0.19090001]
 [4836.768325      0.16960001    0.25420001    0.54640001    0.16599999]
 ...
 [1737.48606927    0.18977658    0.38423228    0.41251689    0.16505305]
 [3576.06287345    0.18869999    0.30149999    0.52459997    0.18080001]
 [2362.33399394    0.18009302    0.25180778    0.42682034    0.18511157]][0m
[37m[1m[2023-07-10 15:15:39,206][227910] Max Reward on eval: 4858.149930087477[0m
[37m[1m[2023-07-10 15:15:39,206][227910] Min Reward on eval: 835.7580727795372[0m
[37m[1m[2023-07-10 15:15:39,206][227910] Mean Reward across all agents: 3439.202429555038[0m
[37m[1m[2023-07-10 15:15:39,206][227910] Average Trajectory Length: 929.2533333333333[0m
[36m[2023-07-10 15:15:39,209][227910] mean_value=-205.35350988087575, max_value=3451.1145385733034[0m
[37m[1m[2023-07-10 15:15:39,212][227910] New mean coefficients: [[ 1.1900383   0.1860316   0.2950005  -0.19178654  0.09285594]][0m
[37m[1m[2023-07-10 15:15:39,213][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:15:49,093][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 15:15:49,093][227910] FPS: 388723.62[0m
[36m[2023-07-10 15:15:49,096][227910] itr=695, itrs=2000, Progress: 34.75%[0m
[36m[2023-07-10 15:16:00,690][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 15:16:00,690][227910] FPS: 331760.24[0m
[36m[2023-07-10 15:16:05,357][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:16:05,357][227910] Reward + Measures: [[4981.78712193    0.16255879    0.28319943    0.51020247    0.16537201]][0m
[37m[1m[2023-07-10 15:16:05,357][227910] Max Reward on eval: 4981.7871219317685[0m
[37m[1m[2023-07-10 15:16:05,358][227910] Min Reward on eval: 4981.7871219317685[0m
[37m[1m[2023-07-10 15:16:05,358][227910] Mean Reward across all agents: 4981.7871219317685[0m
[37m[1m[2023-07-10 15:16:05,358][227910] Average Trajectory Length: 990.7813333333334[0m
[36m[2023-07-10 15:16:10,882][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:16:10,882][227910] Reward + Measures: [[2465.49221909    0.27980003    0.28990003    0.49189997    0.16950002]
 [2793.32304817    0.264         0.3003        0.49359998    0.17590001]
 [2921.15541987    0.17759462    0.35073009    0.38108531    0.21070747]
 ...
 [3896.14656104    0.16230001    0.28869998    0.56380004    0.1763    ]
 [4296.52234125    0.17990001    0.28670001    0.62169999    0.1698    ]
 [3534.63713681    0.1996        0.32640001    0.51350003    0.1743    ]][0m
[37m[1m[2023-07-10 15:16:10,883][227910] Max Reward on eval: 5002.668619078398[0m
[37m[1m[2023-07-10 15:16:10,883][227910] Min Reward on eval: 1539.4646380051563[0m
[37m[1m[2023-07-10 15:16:10,883][227910] Mean Reward across all agents: 3528.394457469042[0m
[37m[1m[2023-07-10 15:16:10,883][227910] Average Trajectory Length: 969.557[0m
[36m[2023-07-10 15:16:10,886][227910] mean_value=-271.40768913391616, max_value=2359.4870761954253[0m
[37m[1m[2023-07-10 15:16:10,889][227910] New mean coefficients: [[1.0030413  0.15765645 0.09156705 0.08337502 0.02986195]][0m
[37m[1m[2023-07-10 15:16:10,890][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:16:20,716][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 15:16:20,716][227910] FPS: 390870.01[0m
[36m[2023-07-10 15:16:20,718][227910] itr=696, itrs=2000, Progress: 34.80%[0m
[36m[2023-07-10 15:16:32,172][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 15:16:32,172][227910] FPS: 335777.38[0m
[36m[2023-07-10 15:16:36,931][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:16:36,931][227910] Reward + Measures: [[5087.05266294    0.16208564    0.28247553    0.50460112    0.16176382]][0m
[37m[1m[2023-07-10 15:16:36,931][227910] Max Reward on eval: 5087.05266294298[0m
[37m[1m[2023-07-10 15:16:36,932][227910] Min Reward on eval: 5087.05266294298[0m
[37m[1m[2023-07-10 15:16:36,932][227910] Mean Reward across all agents: 5087.05266294298[0m
[37m[1m[2023-07-10 15:16:36,932][227910] Average Trajectory Length: 991.5319999999999[0m
[36m[2023-07-10 15:16:42,428][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:16:42,428][227910] Reward + Measures: [[2494.42623848    0.16624169    0.30626211    0.45800638    0.19490556]
 [3902.28936181    0.1674        0.32080001    0.43309999    0.18010001]
 [3440.01140148    0.21340001    0.33329999    0.4355        0.18499999]
 ...
 [1296.28845892    0.43689701    0.50164551    0.53035158    0.16729698]
 [4984.17569908    0.15709999    0.26099998    0.51120007    0.1673    ]
 [3298.4181823     0.22809999    0.2608        0.41079998    0.21519999]][0m
[37m[1m[2023-07-10 15:16:42,428][227910] Max Reward on eval: 4984.175699080503[0m
[37m[1m[2023-07-10 15:16:42,429][227910] Min Reward on eval: -22.184185919415903[0m
[37m[1m[2023-07-10 15:16:42,429][227910] Mean Reward across all agents: 3343.168109861623[0m
[37m[1m[2023-07-10 15:16:42,429][227910] Average Trajectory Length: 960.4606666666666[0m
[36m[2023-07-10 15:16:42,433][227910] mean_value=-213.78794962266747, max_value=2193.24346743867[0m
[37m[1m[2023-07-10 15:16:42,436][227910] New mean coefficients: [[0.98959684 0.2222077  0.08214425 0.35299128 0.06509458]][0m
[37m[1m[2023-07-10 15:16:42,437][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:16:52,132][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 15:16:52,132][227910] FPS: 396154.86[0m
[36m[2023-07-10 15:16:52,134][227910] itr=697, itrs=2000, Progress: 34.85%[0m
[36m[2023-07-10 15:17:03,661][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 15:17:03,661][227910] FPS: 333711.43[0m
[36m[2023-07-10 15:17:08,456][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:17:08,457][227910] Reward + Measures: [[5126.58612649    0.15826939    0.27380124    0.51842588    0.1601757 ]][0m
[37m[1m[2023-07-10 15:17:08,457][227910] Max Reward on eval: 5126.586126494894[0m
[37m[1m[2023-07-10 15:17:08,457][227910] Min Reward on eval: 5126.586126494894[0m
[37m[1m[2023-07-10 15:17:08,458][227910] Mean Reward across all agents: 5126.586126494894[0m
[37m[1m[2023-07-10 15:17:08,458][227910] Average Trajectory Length: 985.3276666666667[0m
[36m[2023-07-10 15:17:13,826][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:17:13,879][227910] Reward + Measures: [[4148.27561518    0.1804        0.35800004    0.5402        0.1533    ]
 [3011.34735212    0.24610002    0.31389999    0.53740007    0.21530001]
 [4061.08888302    0.1839        0.3026        0.51239997    0.2027    ]
 ...
 [2983.62587605    0.2014        0.31490001    0.45889997    0.2323    ]
 [3647.63850024    0.18789999    0.30289999    0.49960002    0.2307    ]
 [3714.83845082    0.18740001    0.32980001    0.51370001    0.21820001]][0m
[37m[1m[2023-07-10 15:17:13,879][227910] Max Reward on eval: 5185.964035737724[0m
[37m[1m[2023-07-10 15:17:13,879][227910] Min Reward on eval: 1885.9956074598595[0m
[37m[1m[2023-07-10 15:17:13,879][227910] Mean Reward across all agents: 3837.7600442477697[0m
[37m[1m[2023-07-10 15:17:13,880][227910] Average Trajectory Length: 972.0543333333333[0m
[36m[2023-07-10 15:17:13,882][227910] mean_value=-385.11681084803337, max_value=1644.1133976218362[0m
[37m[1m[2023-07-10 15:17:13,885][227910] New mean coefficients: [[ 0.61570483  0.21121672  0.23129994 -0.04113618  0.02584617]][0m
[37m[1m[2023-07-10 15:17:13,886][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:17:23,454][227910] train() took 9.57 seconds to complete[0m
[36m[2023-07-10 15:17:23,454][227910] FPS: 401407.74[0m
[36m[2023-07-10 15:17:23,457][227910] itr=698, itrs=2000, Progress: 34.90%[0m
[36m[2023-07-10 15:17:34,947][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 15:17:34,947][227910] FPS: 334720.36[0m
[36m[2023-07-10 15:17:39,820][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:17:39,820][227910] Reward + Measures: [[4412.59684547    0.15520671    0.25723726    0.47440991    0.16589211]][0m
[37m[1m[2023-07-10 15:17:39,820][227910] Max Reward on eval: 4412.596845466222[0m
[37m[1m[2023-07-10 15:17:39,820][227910] Min Reward on eval: 4412.596845466222[0m
[37m[1m[2023-07-10 15:17:39,821][227910] Mean Reward across all agents: 4412.596845466222[0m
[37m[1m[2023-07-10 15:17:39,821][227910] Average Trajectory Length: 939.9[0m
[36m[2023-07-10 15:17:45,530][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:17:45,530][227910] Reward + Measures: [[3703.1295232     0.18709998    0.27320001    0.47729999    0.2115    ]
 [2723.61092975    0.19960001    0.39990002    0.4145        0.20510001]
 [2230.65382413    0.15105106    0.33402976    0.48268136    0.18048666]
 ...
 [3901.69293159    0.18679999    0.26549998    0.48240003    0.19660001]
 [3118.12252931    0.19490001    0.25690004    0.5399        0.2052    ]
 [3281.1337328     0.18709999    0.30130002    0.42910004    0.21140002]][0m
[37m[1m[2023-07-10 15:17:45,531][227910] Max Reward on eval: 4680.3801093816755[0m
[37m[1m[2023-07-10 15:17:45,531][227910] Min Reward on eval: 1583.8403226248454[0m
[37m[1m[2023-07-10 15:17:45,531][227910] Mean Reward across all agents: 3178.127229783008[0m
[37m[1m[2023-07-10 15:17:45,531][227910] Average Trajectory Length: 955.3106666666666[0m
[36m[2023-07-10 15:17:45,534][227910] mean_value=-697.8097533388494, max_value=1782.0750560885301[0m
[37m[1m[2023-07-10 15:17:45,536][227910] New mean coefficients: [[0.35403454 0.24511375 0.1989832  0.0313592  0.09082501]][0m
[37m[1m[2023-07-10 15:17:45,537][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:17:55,456][227910] train() took 9.92 seconds to complete[0m
[36m[2023-07-10 15:17:55,456][227910] FPS: 387208.40[0m
[36m[2023-07-10 15:17:55,459][227910] itr=699, itrs=2000, Progress: 34.95%[0m
[36m[2023-07-10 15:18:07,161][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 15:18:07,162][227910] FPS: 328678.24[0m
[36m[2023-07-10 15:18:11,974][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:18:11,974][227910] Reward + Measures: [[4960.86134448    0.15635623    0.27827582    0.4728097     0.15771951]][0m
[37m[1m[2023-07-10 15:18:11,975][227910] Max Reward on eval: 4960.861344475437[0m
[37m[1m[2023-07-10 15:18:11,975][227910] Min Reward on eval: 4960.861344475437[0m
[37m[1m[2023-07-10 15:18:11,975][227910] Mean Reward across all agents: 4960.861344475437[0m
[37m[1m[2023-07-10 15:18:11,976][227910] Average Trajectory Length: 986.001[0m
[36m[2023-07-10 15:18:17,489][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:18:17,489][227910] Reward + Measures: [[3404.89112593    0.2086        0.28570002    0.53900003    0.18100001]
 [2690.61489938    0.1914541     0.29272243    0.46290165    0.21691146]
 [4515.168564      0.15231483    0.26104102    0.47814819    0.14949971]
 ...
 [4767.80182065    0.16610001    0.29350001    0.48289999    0.15989999]
 [3751.42718125    0.20650001    0.28331253    0.52583754    0.17721668]
 [3091.55446449    0.20050001    0.31280002    0.46170002    0.2244    ]][0m
[37m[1m[2023-07-10 15:18:17,489][227910] Max Reward on eval: 5175.36752277147[0m
[37m[1m[2023-07-10 15:18:17,490][227910] Min Reward on eval: 1422.6988645317615[0m
[37m[1m[2023-07-10 15:18:17,490][227910] Mean Reward across all agents: 3865.529129435988[0m
[37m[1m[2023-07-10 15:18:17,490][227910] Average Trajectory Length: 978.2673333333333[0m
[36m[2023-07-10 15:18:17,493][227910] mean_value=-585.4094241251563, max_value=2782.986087965704[0m
[37m[1m[2023-07-10 15:18:17,495][227910] New mean coefficients: [[0.404602   0.2529888  0.14091107 0.01704254 0.00977469]][0m
[37m[1m[2023-07-10 15:18:17,496][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:18:27,287][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 15:18:27,287][227910] FPS: 392267.98[0m
[36m[2023-07-10 15:18:27,290][227910] itr=700, itrs=2000, Progress: 35.00%[0m
[37m[1m[2023-07-10 15:18:30,304][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000680[0m
[36m[2023-07-10 15:18:42,072][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 15:18:42,072][227910] FPS: 332546.36[0m
[36m[2023-07-10 15:18:46,933][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:18:46,934][227910] Reward + Measures: [[5018.97255488    0.15677746    0.28104234    0.47472498    0.15653265]][0m
[37m[1m[2023-07-10 15:18:46,934][227910] Max Reward on eval: 5018.972554876844[0m
[37m[1m[2023-07-10 15:18:46,934][227910] Min Reward on eval: 5018.972554876844[0m
[37m[1m[2023-07-10 15:18:46,934][227910] Mean Reward across all agents: 5018.972554876844[0m
[37m[1m[2023-07-10 15:18:46,934][227910] Average Trajectory Length: 987.774[0m
[36m[2023-07-10 15:18:52,341][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:18:52,342][227910] Reward + Measures: [[4875.61658507    0.16697429    0.27638608    0.43644834    0.16273455]
 [5141.08357497    0.15989999    0.27680001    0.46779999    0.1559    ]
 [3897.99555677    0.17353837    0.29559538    0.41007978    0.1600166 ]
 ...
 [4602.27589407    0.1607309     0.2702553     0.44395691    0.16486342]
 [5126.07063549    0.15940002    0.27250001    0.47089997    0.15840001]
 [3779.6100351     0.17015155    0.30367884    0.44281626    0.16385022]][0m
[37m[1m[2023-07-10 15:18:52,342][227910] Max Reward on eval: 5156.958156772889[0m
[37m[1m[2023-07-10 15:18:52,342][227910] Min Reward on eval: 2615.028384174034[0m
[37m[1m[2023-07-10 15:18:52,342][227910] Mean Reward across all agents: 4392.33123046088[0m
[37m[1m[2023-07-10 15:18:52,343][227910] Average Trajectory Length: 949.7223333333333[0m
[36m[2023-07-10 15:18:52,345][227910] mean_value=-119.89538630725, max_value=1807.5586114499802[0m
[37m[1m[2023-07-10 15:18:52,348][227910] New mean coefficients: [[-0.11232764  0.23124844  0.13585508 -0.34967127 -0.08614589]][0m
[37m[1m[2023-07-10 15:18:52,349][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:19:01,982][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 15:19:01,982][227910] FPS: 398705.45[0m
[36m[2023-07-10 15:19:01,984][227910] itr=701, itrs=2000, Progress: 35.05%[0m
[36m[2023-07-10 15:19:13,630][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 15:19:13,631][227910] FPS: 330189.81[0m
[36m[2023-07-10 15:19:18,395][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:19:18,396][227910] Reward + Measures: [[4964.00471235    0.16295886    0.29100406    0.46720383    0.1610446 ]][0m
[37m[1m[2023-07-10 15:19:18,396][227910] Max Reward on eval: 4964.004712354795[0m
[37m[1m[2023-07-10 15:19:18,396][227910] Min Reward on eval: 4964.004712354795[0m
[37m[1m[2023-07-10 15:19:18,396][227910] Mean Reward across all agents: 4964.004712354795[0m
[37m[1m[2023-07-10 15:19:18,397][227910] Average Trajectory Length: 994.7643333333333[0m
[36m[2023-07-10 15:19:23,848][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:19:23,849][227910] Reward + Measures: [[5041.26789979    0.16329999    0.28420001    0.4598        0.16199999]
 [5048.45759468    0.15800001    0.2886        0.47209999    0.1586    ]
 [4993.05693332    0.16410001    0.27879998    0.46370003    0.16140001]
 ...
 [5054.68744899    0.15350001    0.29789999    0.4657        0.1514    ]
 [4796.58481839    0.1675        0.27870002    0.4639        0.1605    ]
 [4994.33658656    0.16419999    0.28660002    0.45959997    0.16180001]][0m
[37m[1m[2023-07-10 15:19:23,849][227910] Max Reward on eval: 5173.586949983798[0m
[37m[1m[2023-07-10 15:19:23,849][227910] Min Reward on eval: 3714.09740447253[0m
[37m[1m[2023-07-10 15:19:23,850][227910] Mean Reward across all agents: 4738.83778194787[0m
[37m[1m[2023-07-10 15:19:23,850][227910] Average Trajectory Length: 984.2853333333333[0m
[36m[2023-07-10 15:19:23,852][227910] mean_value=-177.31618293698506, max_value=799.8749732318624[0m
[37m[1m[2023-07-10 15:19:23,854][227910] New mean coefficients: [[-0.23979066  0.13841549  0.12397516 -0.2817011   0.01162482]][0m
[37m[1m[2023-07-10 15:19:23,855][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:19:33,473][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 15:19:33,473][227910] FPS: 399325.40[0m
[36m[2023-07-10 15:19:33,476][227910] itr=702, itrs=2000, Progress: 35.10%[0m
[36m[2023-07-10 15:19:44,945][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 15:19:44,945][227910] FPS: 335284.27[0m
[36m[2023-07-10 15:19:49,790][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:19:49,790][227910] Reward + Measures: [[3152.33878063    0.14605589    0.2851603     0.43486133    0.15538317]][0m
[37m[1m[2023-07-10 15:19:49,790][227910] Max Reward on eval: 3152.338780634923[0m
[37m[1m[2023-07-10 15:19:49,791][227910] Min Reward on eval: 3152.338780634923[0m
[37m[1m[2023-07-10 15:19:49,791][227910] Mean Reward across all agents: 3152.338780634923[0m
[37m[1m[2023-07-10 15:19:49,791][227910] Average Trajectory Length: 710.179[0m
[36m[2023-07-10 15:19:55,213][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:19:55,213][227910] Reward + Measures: [[1827.15324429    0.14468797    0.28609324    0.47016382    0.16260952]
 [2432.84283462    0.14443873    0.28835979    0.4326534     0.15724543]
 [2956.58522773    0.15535763    0.28998718    0.44268274    0.16455521]
 ...
 [3628.01875419    0.14891596    0.2947593     0.42300043    0.1564867 ]
 [2843.87524692    0.14989215    0.28966495    0.3968839     0.1520223 ]
 [2989.55276451    0.14792238    0.29800892    0.4181779     0.16076656]][0m
[37m[1m[2023-07-10 15:19:55,213][227910] Max Reward on eval: 4305.468817685917[0m
[37m[1m[2023-07-10 15:19:55,214][227910] Min Reward on eval: 1798.0629961809143[0m
[37m[1m[2023-07-10 15:19:55,214][227910] Mean Reward across all agents: 3194.7082696122943[0m
[37m[1m[2023-07-10 15:19:55,214][227910] Average Trajectory Length: 745.067[0m
[36m[2023-07-10 15:19:55,215][227910] mean_value=-1405.8849351831748, max_value=-116.91396456569692[0m
[36m[2023-07-10 15:19:55,218][227910] XNES is restarting with a new solution whose measures are [0.1816     0.74610007 0.30909997 0.5952    ] and objective is -99.3498421370401[0m
[36m[2023-07-10 15:19:55,219][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 15:19:55,221][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 15:19:55,222][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:20:04,902][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 15:20:04,902][227910] FPS: 396751.37[0m
[36m[2023-07-10 15:20:04,905][227910] itr=703, itrs=2000, Progress: 35.15%[0m
[36m[2023-07-10 15:20:16,328][227910] train() took 11.40 seconds to complete[0m
[36m[2023-07-10 15:20:16,329][227910] FPS: 336708.64[0m
[36m[2023-07-10 15:20:21,111][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:20:21,111][227910] Reward + Measures: [[-50.91039771   0.20569234   0.68870401   0.32467434   0.52395564]][0m
[37m[1m[2023-07-10 15:20:21,111][227910] Max Reward on eval: -50.91039770895945[0m
[37m[1m[2023-07-10 15:20:21,112][227910] Min Reward on eval: -50.91039770895945[0m
[37m[1m[2023-07-10 15:20:21,112][227910] Mean Reward across all agents: -50.91039770895945[0m
[37m[1m[2023-07-10 15:20:21,112][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:20:26,545][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:20:26,546][227910] Reward + Measures: [[ -97.4275848     0.2667        0.70420003    0.32609996    0.57680005]
 [ -59.11089612    0.19400001    0.62709999    0.30950001    0.49510002]
 [-163.40284358    0.14919999    0.67159998    0.25680003    0.52430004]
 ...
 [ -22.18705222    0.28330001    0.5887        0.34639999    0.43650004]
 [ -46.20429974    0.18969999    0.64029998    0.28100002    0.52890003]
 [-169.90304596    0.28980002    0.64170003    0.32520002    0.51639998]][0m
[37m[1m[2023-07-10 15:20:26,546][227910] Max Reward on eval: 55.23173368921416[0m
[37m[1m[2023-07-10 15:20:26,546][227910] Min Reward on eval: -361.44184655481365[0m
[37m[1m[2023-07-10 15:20:26,546][227910] Mean Reward across all agents: -110.9801020749289[0m
[37m[1m[2023-07-10 15:20:26,547][227910] Average Trajectory Length: 998.9693333333333[0m
[36m[2023-07-10 15:20:26,548][227910] mean_value=-657.6144642808872, max_value=357.7903272196639[0m
[37m[1m[2023-07-10 15:20:26,551][227910] New mean coefficients: [[ 0.16533577 -1.423124   -0.4258712  -1.7234486   1.8866436 ]][0m
[37m[1m[2023-07-10 15:20:26,552][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:20:36,311][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 15:20:36,311][227910] FPS: 393550.89[0m
[36m[2023-07-10 15:20:36,313][227910] itr=704, itrs=2000, Progress: 35.20%[0m
[36m[2023-07-10 15:20:47,771][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 15:20:47,772][227910] FPS: 335625.70[0m
[36m[2023-07-10 15:20:52,556][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:20:52,557][227910] Reward + Measures: [[-28.76612558   0.190319     0.71080232   0.31177735   0.56034833]][0m
[37m[1m[2023-07-10 15:20:52,557][227910] Max Reward on eval: -28.766125581405834[0m
[37m[1m[2023-07-10 15:20:52,557][227910] Min Reward on eval: -28.766125581405834[0m
[37m[1m[2023-07-10 15:20:52,557][227910] Mean Reward across all agents: -28.766125581405834[0m
[37m[1m[2023-07-10 15:20:52,558][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:20:57,972][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:20:57,973][227910] Reward + Measures: [[-144.92517476    0.1498        0.72260004    0.28529999    0.58450001]
 [  -0.94531633    0.22719999    0.64320004    0.38590002    0.5007    ]
 [-195.96486051    0.15270001    0.72119999    0.25149998    0.62490004]
 ...
 [ -24.10872939    0.1716        0.70700002    0.24340001    0.65889996]
 [ 126.83778805    0.41780001    0.63330001    0.36469999    0.46890002]
 [-106.22356244    0.17920001    0.74590003    0.27540001    0.61920005]][0m
[37m[1m[2023-07-10 15:20:57,973][227910] Max Reward on eval: 135.57125750264385[0m
[37m[1m[2023-07-10 15:20:57,973][227910] Min Reward on eval: -329.6389449623181[0m
[37m[1m[2023-07-10 15:20:57,974][227910] Mean Reward across all agents: -104.50373125868289[0m
[37m[1m[2023-07-10 15:20:57,974][227910] Average Trajectory Length: 999.8526666666667[0m
[36m[2023-07-10 15:20:57,977][227910] mean_value=-177.9659839975717, max_value=412.7439470431389[0m
[37m[1m[2023-07-10 15:20:57,979][227910] New mean coefficients: [[-1.362822   -3.3974895  -0.81270957 -2.356729    2.7924607 ]][0m
[37m[1m[2023-07-10 15:20:57,980][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:21:07,640][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 15:21:07,640][227910] FPS: 397606.28[0m
[36m[2023-07-10 15:21:07,643][227910] itr=705, itrs=2000, Progress: 35.25%[0m
[36m[2023-07-10 15:21:19,134][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 15:21:19,135][227910] FPS: 334713.19[0m
[36m[2023-07-10 15:21:23,804][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:21:23,804][227910] Reward + Measures: [[-121.11918045    0.15830441    0.74219376    0.28109282    0.61083388]][0m
[37m[1m[2023-07-10 15:21:23,804][227910] Max Reward on eval: -121.11918045147338[0m
[37m[1m[2023-07-10 15:21:23,805][227910] Min Reward on eval: -121.11918045147338[0m
[37m[1m[2023-07-10 15:21:23,805][227910] Mean Reward across all agents: -121.11918045147338[0m
[37m[1m[2023-07-10 15:21:23,805][227910] Average Trajectory Length: 999.9913333333333[0m
[36m[2023-07-10 15:21:29,188][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:21:29,189][227910] Reward + Measures: [[-172.37841979    0.19410001    0.69190001    0.30730003    0.59250003]
 [-216.23426532    0.09890001    0.52859998    0.32790002    0.44720003]
 [-184.54607033    0.13803104    0.60117596    0.33600795    0.51503152]
 ...
 [ -79.07539229    0.14430001    0.77969998    0.204         0.60420001]
 [ -43.62917453    0.1839        0.8057        0.24589999    0.68520004]
 [-177.64244534    0.11169999    0.65950006    0.331         0.53750002]][0m
[37m[1m[2023-07-10 15:21:29,189][227910] Max Reward on eval: -43.62917452596012[0m
[37m[1m[2023-07-10 15:21:29,189][227910] Min Reward on eval: -440.63113200207664[0m
[37m[1m[2023-07-10 15:21:29,189][227910] Mean Reward across all agents: -170.3161100866346[0m
[37m[1m[2023-07-10 15:21:29,190][227910] Average Trajectory Length: 999.7873333333333[0m
[36m[2023-07-10 15:21:29,193][227910] mean_value=-115.9543394989978, max_value=420.7027091298066[0m
[37m[1m[2023-07-10 15:21:29,195][227910] New mean coefficients: [[-1.2715094 -5.4230947 -1.0442913 -2.0075848  2.5285115]][0m
[37m[1m[2023-07-10 15:21:29,196][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:21:38,817][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 15:21:38,818][227910] FPS: 399191.54[0m
[36m[2023-07-10 15:21:38,820][227910] itr=706, itrs=2000, Progress: 35.30%[0m
[36m[2023-07-10 15:21:50,458][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 15:21:50,458][227910] FPS: 330467.93[0m
[36m[2023-07-10 15:21:55,231][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:21:55,231][227910] Reward + Measures: [[-170.44031044    0.149251      0.75920802    0.27643734    0.64885098]][0m
[37m[1m[2023-07-10 15:21:55,232][227910] Max Reward on eval: -170.44031043850228[0m
[37m[1m[2023-07-10 15:21:55,232][227910] Min Reward on eval: -170.44031043850228[0m
[37m[1m[2023-07-10 15:21:55,232][227910] Mean Reward across all agents: -170.44031043850228[0m
[37m[1m[2023-07-10 15:21:55,232][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:22:00,875][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:22:00,876][227910] Reward + Measures: [[-225.53968092    0.0863        0.65950006    0.35640001    0.56520003]
 [ -82.88767992    0.0696        0.64469999    0.38139999    0.52960002]
 [-211.76713977    0.0748        0.49670002    0.41560003    0.44940001]
 ...
 [-147.96005995    0.12673119    0.74073237    0.30765009    0.60459542]
 [ -32.63824643    0.0558        0.56599998    0.44210002    0.44319996]
 [-128.19635636    0.15970002    0.76210004    0.29170001    0.67160004]][0m
[37m[1m[2023-07-10 15:22:00,876][227910] Max Reward on eval: 181.2098993499647[0m
[37m[1m[2023-07-10 15:22:00,876][227910] Min Reward on eval: -268.2296141084109[0m
[37m[1m[2023-07-10 15:22:00,876][227910] Mean Reward across all agents: -73.64073233983211[0m
[37m[1m[2023-07-10 15:22:00,877][227910] Average Trajectory Length: 999.153[0m
[36m[2023-07-10 15:22:00,881][227910] mean_value=27.978022328159, max_value=681.2098993499646[0m
[37m[1m[2023-07-10 15:22:00,884][227910] New mean coefficients: [[-2.6756155 -8.291127  -1.9262002 -2.5185137  5.0510244]][0m
[37m[1m[2023-07-10 15:22:00,885][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:22:10,631][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 15:22:10,632][227910] FPS: 394050.07[0m
[36m[2023-07-10 15:22:10,634][227910] itr=707, itrs=2000, Progress: 35.35%[0m
[36m[2023-07-10 15:22:22,139][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 15:22:22,139][227910] FPS: 334351.00[0m
[36m[2023-07-10 15:22:26,921][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:22:26,921][227910] Reward + Measures: [[-222.57806682    0.13730592    0.77392292    0.26656583    0.6786288 ]][0m
[37m[1m[2023-07-10 15:22:26,922][227910] Max Reward on eval: -222.5780668150187[0m
[37m[1m[2023-07-10 15:22:26,922][227910] Min Reward on eval: -222.5780668150187[0m
[37m[1m[2023-07-10 15:22:26,922][227910] Mean Reward across all agents: -222.5780668150187[0m
[37m[1m[2023-07-10 15:22:26,922][227910] Average Trajectory Length: 999.9399999999999[0m
[36m[2023-07-10 15:22:32,307][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:22:32,308][227910] Reward + Measures: [[-377.78988667    0.0671        0.4646        0.33819997    0.4382    ]
 [-406.08980761    0.17830001    0.64230007    0.27620003    0.54629999]
 [-591.11854411    0.1419        0.55680001    0.30590001    0.47810003]
 ...
 [-460.46958111    0.1184        0.5927        0.37869999    0.52640003]
 [-337.32618096    0.0588        0.50930005    0.35120001    0.50270003]
 [-466.12086986    0.1304        0.49980003    0.28790003    0.46040002]][0m
[37m[1m[2023-07-10 15:22:32,308][227910] Max Reward on eval: -100.43556088277838[0m
[37m[1m[2023-07-10 15:22:32,308][227910] Min Reward on eval: -694.5906410633819[0m
[37m[1m[2023-07-10 15:22:32,309][227910] Mean Reward across all agents: -379.6609513356364[0m
[37m[1m[2023-07-10 15:22:32,309][227910] Average Trajectory Length: 992.6379999999999[0m
[36m[2023-07-10 15:22:32,311][227910] mean_value=-313.59108378476003, max_value=326.116339123277[0m
[37m[1m[2023-07-10 15:22:32,314][227910] New mean coefficients: [[-3.3627992 -8.98562   -4.848528  -2.6135337  5.4875555]][0m
[37m[1m[2023-07-10 15:22:32,315][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:22:41,938][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 15:22:41,938][227910] FPS: 399100.33[0m
[36m[2023-07-10 15:22:41,940][227910] itr=708, itrs=2000, Progress: 35.40%[0m
[36m[2023-07-10 15:22:53,496][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 15:22:53,497][227910] FPS: 332799.47[0m
[36m[2023-07-10 15:22:58,292][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:22:58,292][227910] Reward + Measures: [[-289.69856616    0.12278125    0.7703864     0.26331404    0.6934551 ]][0m
[37m[1m[2023-07-10 15:22:58,292][227910] Max Reward on eval: -289.6985661562852[0m
[37m[1m[2023-07-10 15:22:58,293][227910] Min Reward on eval: -289.6985661562852[0m
[37m[1m[2023-07-10 15:22:58,293][227910] Mean Reward across all agents: -289.6985661562852[0m
[37m[1m[2023-07-10 15:22:58,293][227910] Average Trajectory Length: 999.698[0m
[36m[2023-07-10 15:23:03,737][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:23:03,737][227910] Reward + Measures: [[-601.38297223    0.0299        0.61630005    0.39519998    0.64239997]
 [-620.37346336    0.1024        0.56779999    0.36269999    0.49600002]
 [-701.19305253    0.0811        0.5474        0.34250003    0.52650005]
 ...
 [-587.19706275    0.0842        0.63320005    0.3163        0.5844    ]
 [-447.34464238    0.0462        0.59019995    0.32580003    0.59330004]
 [-527.12721333    0.0734        0.56560004    0.31620002    0.52999997]][0m
[37m[1m[2023-07-10 15:23:03,737][227910] Max Reward on eval: -22.771151082636788[0m
[37m[1m[2023-07-10 15:23:03,738][227910] Min Reward on eval: -762.5230536361283[0m
[37m[1m[2023-07-10 15:23:03,738][227910] Mean Reward across all agents: -436.5649781120934[0m
[37m[1m[2023-07-10 15:23:03,738][227910] Average Trajectory Length: 996.813[0m
[36m[2023-07-10 15:23:03,740][227910] mean_value=-221.44297241330196, max_value=223.16007131815738[0m
[37m[1m[2023-07-10 15:23:03,742][227910] New mean coefficients: [[-2.0340633 -8.640017  -3.7149878 -4.6660004  4.552221 ]][0m
[37m[1m[2023-07-10 15:23:03,743][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:23:13,504][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 15:23:13,504][227910] FPS: 393494.43[0m
[36m[2023-07-10 15:23:13,506][227910] itr=709, itrs=2000, Progress: 35.45%[0m
[36m[2023-07-10 15:23:25,116][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 15:23:25,116][227910] FPS: 331291.28[0m
[36m[2023-07-10 15:23:29,956][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:23:29,957][227910] Reward + Measures: [[-311.26700349    0.11569966    0.77059001    0.25094366    0.71299803]][0m
[37m[1m[2023-07-10 15:23:29,957][227910] Max Reward on eval: -311.26700348580994[0m
[37m[1m[2023-07-10 15:23:29,957][227910] Min Reward on eval: -311.26700348580994[0m
[37m[1m[2023-07-10 15:23:29,957][227910] Mean Reward across all agents: -311.26700348580994[0m
[37m[1m[2023-07-10 15:23:29,958][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:23:35,400][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:23:35,401][227910] Reward + Measures: [[-323.05527189    0.0884        0.60860002    0.32870001    0.58960003]
 [-121.05682585    0.0575        0.62950003    0.39210001    0.65090001]
 [-266.55141841    0.1754        0.64940006    0.4118        0.66850007]
 ...
 [-324.7857051     0.14560001    0.68620002    0.34590003    0.67880005]
 [-258.76810809    0.1098        0.57889998    0.44889998    0.63480002]
 [-353.16735076    0.14650001    0.6573        0.34920001    0.64849997]][0m
[37m[1m[2023-07-10 15:23:35,401][227910] Max Reward on eval: 52.85209605732816[0m
[37m[1m[2023-07-10 15:23:35,401][227910] Min Reward on eval: -444.901223310607[0m
[37m[1m[2023-07-10 15:23:35,402][227910] Mean Reward across all agents: -234.8699974579741[0m
[37m[1m[2023-07-10 15:23:35,402][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:23:35,405][227910] mean_value=-110.93661144217488, max_value=526.1673983705463[0m
[37m[1m[2023-07-10 15:23:35,408][227910] New mean coefficients: [[ -1.1145201 -12.164166   -5.6029263  -3.6876972   3.8469763]][0m
[37m[1m[2023-07-10 15:23:35,409][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:23:45,158][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 15:23:45,158][227910] FPS: 393957.89[0m
[36m[2023-07-10 15:23:45,160][227910] itr=710, itrs=2000, Progress: 35.50%[0m
[37m[1m[2023-07-10 15:23:48,483][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000690[0m
[36m[2023-07-10 15:24:00,349][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 15:24:00,349][227910] FPS: 330872.48[0m
[36m[2023-07-10 15:24:05,111][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:24:05,111][227910] Reward + Measures: [[-333.80663737    0.09937365    0.78821564    0.24374364    0.740861  ]][0m
[37m[1m[2023-07-10 15:24:05,111][227910] Max Reward on eval: -333.80663736770225[0m
[37m[1m[2023-07-10 15:24:05,112][227910] Min Reward on eval: -333.80663736770225[0m
[37m[1m[2023-07-10 15:24:05,112][227910] Mean Reward across all agents: -333.80663736770225[0m
[37m[1m[2023-07-10 15:24:05,112][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:24:10,605][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:24:10,606][227910] Reward + Measures: [[-431.41155071    0.21879999    0.48330003    0.26980001    0.46059999]
 [-460.56997119    0.19030002    0.56090003    0.29760003    0.51529998]
 [-333.65625833    0.292         0.48880002    0.28650001    0.48289999]
 ...
 [-447.55725073    0.21510001    0.523         0.28909999    0.47469997]
 [-447.5619293     0.24730001    0.49969998    0.29700002    0.41219997]
 [-407.60432399    0.2105        0.55910003    0.2976        0.50700003]][0m
[37m[1m[2023-07-10 15:24:10,606][227910] Max Reward on eval: -333.65625833320667[0m
[37m[1m[2023-07-10 15:24:10,606][227910] Min Reward on eval: -646.9627107951761[0m
[37m[1m[2023-07-10 15:24:10,607][227910] Mean Reward across all agents: -488.9406803409453[0m
[37m[1m[2023-07-10 15:24:10,607][227910] Average Trajectory Length: 993.5226666666666[0m
[36m[2023-07-10 15:24:10,608][227910] mean_value=-994.8942810852178, max_value=77.81780571729178[0m
[37m[1m[2023-07-10 15:24:10,611][227910] New mean coefficients: [[ -0.45569563 -10.637975    -3.8046646   -3.441977     3.5732088 ]][0m
[37m[1m[2023-07-10 15:24:10,612][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:24:20,274][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 15:24:20,274][227910] FPS: 397492.68[0m
[36m[2023-07-10 15:24:20,276][227910] itr=711, itrs=2000, Progress: 35.55%[0m
[36m[2023-07-10 15:24:31,733][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 15:24:31,733][227910] FPS: 335661.91[0m
[36m[2023-07-10 15:24:36,457][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:24:36,458][227910] Reward + Measures: [[-332.72623833    0.08890016    0.79065818    0.24016167    0.74627531]][0m
[37m[1m[2023-07-10 15:24:36,458][227910] Max Reward on eval: -332.72623832669933[0m
[37m[1m[2023-07-10 15:24:36,458][227910] Min Reward on eval: -332.72623832669933[0m
[37m[1m[2023-07-10 15:24:36,458][227910] Mean Reward across all agents: -332.72623832669933[0m
[37m[1m[2023-07-10 15:24:36,458][227910] Average Trajectory Length: 999.7126666666667[0m
[36m[2023-07-10 15:24:41,831][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:24:41,831][227910] Reward + Measures: [[-226.36496536    0.10550001    0.64480001    0.3558        0.65560001]
 [-306.23460969    0.1346        0.52710003    0.29910001    0.58850002]
 [-158.8931624     0.0701        0.59100002    0.36590001    0.68400002]
 ...
 [-232.55025491    0.0689        0.67579997    0.34610003    0.67189997]
 [-268.33515685    0.0913        0.56580001    0.29959998    0.67360002]
 [-277.85707598    0.1193        0.66149998    0.23369999    0.6681    ]][0m
[37m[1m[2023-07-10 15:24:41,832][227910] Max Reward on eval: -135.52220750917985[0m
[37m[1m[2023-07-10 15:24:41,832][227910] Min Reward on eval: -496.8903402726399[0m
[37m[1m[2023-07-10 15:24:41,832][227910] Mean Reward across all agents: -260.25333605771846[0m
[37m[1m[2023-07-10 15:24:41,832][227910] Average Trajectory Length: 999.7083333333333[0m
[36m[2023-07-10 15:24:41,835][227910] mean_value=-128.31531490099883, max_value=307.84040027741577[0m
[37m[1m[2023-07-10 15:24:41,837][227910] New mean coefficients: [[ -0.0442751 -11.965383   -5.518949   -3.216045    5.7816315]][0m
[37m[1m[2023-07-10 15:24:41,838][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:24:51,446][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 15:24:51,447][227910] FPS: 399722.36[0m
[36m[2023-07-10 15:24:51,449][227910] itr=712, itrs=2000, Progress: 35.60%[0m
[36m[2023-07-10 15:25:03,067][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 15:25:03,067][227910] FPS: 331026.08[0m
[36m[2023-07-10 15:25:07,851][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:25:07,852][227910] Reward + Measures: [[-309.32653662    0.08047079    0.80036366    0.24089241    0.76396132]][0m
[37m[1m[2023-07-10 15:25:07,852][227910] Max Reward on eval: -309.3265366214854[0m
[37m[1m[2023-07-10 15:25:07,852][227910] Min Reward on eval: -309.3265366214854[0m
[37m[1m[2023-07-10 15:25:07,852][227910] Mean Reward across all agents: -309.3265366214854[0m
[37m[1m[2023-07-10 15:25:07,852][227910] Average Trajectory Length: 999.7056666666666[0m
[36m[2023-07-10 15:25:13,313][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:25:13,313][227910] Reward + Measures: [[ -93.52773051    0.0385        0.50500005    0.35600001    0.5654    ]
 [-170.87958161    0.06110001    0.6142        0.29930001    0.60880005]
 [-228.04827203    0.0709        0.61370003    0.30930004    0.61840004]
 ...
 [-214.28700773    0.0326        0.51310003    0.368         0.53330004]
 [-171.65682674    0.0402        0.45660004    0.35119998    0.49490005]
 [-237.8139961     0.0552        0.55369997    0.43270001    0.5395    ]][0m
[37m[1m[2023-07-10 15:25:13,313][227910] Max Reward on eval: -47.214789841359014[0m
[37m[1m[2023-07-10 15:25:13,314][227910] Min Reward on eval: -414.3041191048804[0m
[37m[1m[2023-07-10 15:25:13,314][227910] Mean Reward across all agents: -207.6284056755187[0m
[37m[1m[2023-07-10 15:25:13,314][227910] Average Trajectory Length: 999.0219999999999[0m
[36m[2023-07-10 15:25:13,318][227910] mean_value=72.03480047510665, max_value=415.48607108525465[0m
[37m[1m[2023-07-10 15:25:13,321][227910] New mean coefficients: [[  1.2421856 -12.928298   -4.2945366  -3.997837    5.633071 ]][0m
[37m[1m[2023-07-10 15:25:13,322][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:25:23,080][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 15:25:23,080][227910] FPS: 393591.33[0m
[36m[2023-07-10 15:25:23,082][227910] itr=713, itrs=2000, Progress: 35.65%[0m
[36m[2023-07-10 15:25:34,574][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 15:25:34,574][227910] FPS: 334686.40[0m
[36m[2023-07-10 15:25:39,348][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:25:39,348][227910] Reward + Measures: [[-266.23960413    0.06984933    0.8116684     0.23314133    0.78462988]][0m
[37m[1m[2023-07-10 15:25:39,349][227910] Max Reward on eval: -266.23960413421185[0m
[37m[1m[2023-07-10 15:25:39,349][227910] Min Reward on eval: -266.23960413421185[0m
[37m[1m[2023-07-10 15:25:39,349][227910] Mean Reward across all agents: -266.23960413421185[0m
[37m[1m[2023-07-10 15:25:39,349][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:25:44,892][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:25:44,893][227910] Reward + Measures: [[ -41.08497825    0.09398105    0.39814201    0.33770818    0.37397692]
 [-239.35904162    0.0588        0.57450002    0.3558        0.52630001]
 [-231.62654219    0.08220001    0.4357        0.32520005    0.42350003]
 ...
 [-362.57299114    0.1104        0.42389998    0.32340002    0.43689999]
 [-354.41806744    0.0654        0.60409999    0.3346        0.5467    ]
 [-377.22997104    0.0688        0.68120003    0.29090002    0.60760003]][0m
[37m[1m[2023-07-10 15:25:44,893][227910] Max Reward on eval: 99.71288178453688[0m
[37m[1m[2023-07-10 15:25:44,893][227910] Min Reward on eval: -530.4274338337593[0m
[37m[1m[2023-07-10 15:25:44,894][227910] Mean Reward across all agents: -251.38620782150684[0m
[37m[1m[2023-07-10 15:25:44,894][227910] Average Trajectory Length: 991.5079999999999[0m
[36m[2023-07-10 15:25:44,897][227910] mean_value=-85.40093348557667, max_value=579.0369096410495[0m
[37m[1m[2023-07-10 15:25:44,899][227910] New mean coefficients: [[  0.57046485 -13.869144    -5.1945205   -2.3977304    8.586987  ]][0m
[37m[1m[2023-07-10 15:25:44,900][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:25:54,665][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 15:25:54,666][227910] FPS: 393304.68[0m
[36m[2023-07-10 15:25:54,668][227910] itr=714, itrs=2000, Progress: 35.70%[0m
[36m[2023-07-10 15:26:06,286][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 15:26:06,286][227910] FPS: 330992.71[0m
[36m[2023-07-10 15:26:11,077][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:26:11,078][227910] Reward + Measures: [[-224.17174795    0.06020833    0.8364017     0.23085134    0.81968832]][0m
[37m[1m[2023-07-10 15:26:11,078][227910] Max Reward on eval: -224.1717479475712[0m
[37m[1m[2023-07-10 15:26:11,078][227910] Min Reward on eval: -224.1717479475712[0m
[37m[1m[2023-07-10 15:26:11,078][227910] Mean Reward across all agents: -224.1717479475712[0m
[37m[1m[2023-07-10 15:26:11,079][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:26:16,659][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:26:16,660][227910] Reward + Measures: [[-207.33856043    0.0521        0.66379994    0.37540001    0.68550003]
 [-211.57465089    0.13420001    0.45120001    0.3743        0.51539999]
 [-206.07824553    0.0046        0.61090004    0.56420004    0.70419997]
 ...
 [-353.33196968    0.0394        0.57929999    0.3994        0.69419998]
 [-311.73060978    0.008         0.56910002    0.50790006    0.6591    ]
 [-181.54728597    0.0278        0.52039999    0.41940004    0.60360003]][0m
[37m[1m[2023-07-10 15:26:16,660][227910] Max Reward on eval: -6.845474706892856[0m
[37m[1m[2023-07-10 15:26:16,660][227910] Min Reward on eval: -484.6513978050789[0m
[37m[1m[2023-07-10 15:26:16,661][227910] Mean Reward across all agents: -224.63621396730255[0m
[37m[1m[2023-07-10 15:26:16,661][227910] Average Trajectory Length: 998.8433333333332[0m
[36m[2023-07-10 15:26:16,665][227910] mean_value=21.08896047268783, max_value=450.45868371461984[0m
[37m[1m[2023-07-10 15:26:16,668][227910] New mean coefficients: [[  0.95159245 -13.566837    -4.878941    -2.1583686    9.981523  ]][0m
[37m[1m[2023-07-10 15:26:16,669][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:26:26,447][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 15:26:26,447][227910] FPS: 392784.83[0m
[36m[2023-07-10 15:26:26,449][227910] itr=715, itrs=2000, Progress: 35.75%[0m
[36m[2023-07-10 15:26:37,983][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 15:26:37,983][227910] FPS: 333407.88[0m
[36m[2023-07-10 15:26:42,755][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:26:42,761][227910] Reward + Measures: [[-174.64973848    0.050271      0.8615703     0.23655567    0.85505223]][0m
[37m[1m[2023-07-10 15:26:42,761][227910] Max Reward on eval: -174.64973847983796[0m
[37m[1m[2023-07-10 15:26:42,761][227910] Min Reward on eval: -174.64973847983796[0m
[37m[1m[2023-07-10 15:26:42,761][227910] Mean Reward across all agents: -174.64973847983796[0m
[37m[1m[2023-07-10 15:26:42,762][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:26:48,218][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:26:48,219][227910] Reward + Measures: [[-612.31467694    0.09730001    0.63009995    0.25600001    0.58240002]
 [-221.24137911    0.20209999    0.66790003    0.2694        0.60799998]
 [-553.29413008    0.11260001    0.69030005    0.25229999    0.6408    ]
 ...
 [-288.03350488    0.1157        0.76700002    0.2192        0.736     ]
 [-461.24372093    0.0955        0.74560004    0.31479999    0.74340004]
 [-497.12374835    0.10510001    0.71320003    0.27149999    0.67200005]][0m
[37m[1m[2023-07-10 15:26:48,219][227910] Max Reward on eval: -136.80585738682421[0m
[37m[1m[2023-07-10 15:26:48,219][227910] Min Reward on eval: -743.5146289252676[0m
[37m[1m[2023-07-10 15:26:48,219][227910] Mean Reward across all agents: -447.92470411922756[0m
[37m[1m[2023-07-10 15:26:48,220][227910] Average Trajectory Length: 999.702[0m
[36m[2023-07-10 15:26:48,221][227910] mean_value=-390.7243852587359, max_value=363.1941426131758[0m
[37m[1m[2023-07-10 15:26:48,224][227910] New mean coefficients: [[  1.2798129 -10.041771   -3.502043   -3.5843182   8.189286 ]][0m
[37m[1m[2023-07-10 15:26:48,225][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:26:58,116][227910] train() took 9.89 seconds to complete[0m
[36m[2023-07-10 15:26:58,121][227910] FPS: 388306.94[0m
[36m[2023-07-10 15:26:58,124][227910] itr=716, itrs=2000, Progress: 35.80%[0m
[36m[2023-07-10 15:27:09,745][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 15:27:09,745][227910] FPS: 330962.22[0m
[36m[2023-07-10 15:27:14,520][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:27:14,520][227910] Reward + Measures: [[-142.41538912    0.04365467    0.869434      0.23852901    0.87349868]][0m
[37m[1m[2023-07-10 15:27:14,521][227910] Max Reward on eval: -142.41538912431403[0m
[37m[1m[2023-07-10 15:27:14,521][227910] Min Reward on eval: -142.41538912431403[0m
[37m[1m[2023-07-10 15:27:14,521][227910] Mean Reward across all agents: -142.41538912431403[0m
[37m[1m[2023-07-10 15:27:14,521][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:27:20,000][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:27:20,000][227910] Reward + Measures: [[-313.25354284    0.0428        0.68950003    0.31870002    0.65789998]
 [-405.55342545    0.0407        0.6024        0.42640001    0.56470001]
 [-384.79663141    0.0455        0.61910003    0.3723        0.60610002]
 ...
 [-316.8853724     0.0492        0.61639994    0.37510005    0.6117    ]
 [-195.44100791    0.03580001    0.7098        0.3635        0.70480007]
 [-271.61842892    0.0417        0.64750004    0.36859998    0.62719995]][0m
[37m[1m[2023-07-10 15:27:20,000][227910] Max Reward on eval: 28.941376354102978[0m
[37m[1m[2023-07-10 15:27:20,001][227910] Min Reward on eval: -579.4623687441111[0m
[37m[1m[2023-07-10 15:27:20,001][227910] Mean Reward across all agents: -248.77853649077116[0m
[37m[1m[2023-07-10 15:27:20,001][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:27:20,004][227910] mean_value=-42.22923519877516, max_value=432.8321818564906[0m
[37m[1m[2023-07-10 15:27:20,007][227910] New mean coefficients: [[ 1.7056655 -6.203194  -2.030693  -3.8525681  6.1644735]][0m
[37m[1m[2023-07-10 15:27:20,008][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:27:29,684][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 15:27:29,684][227910] FPS: 396901.78[0m
[36m[2023-07-10 15:27:29,687][227910] itr=717, itrs=2000, Progress: 35.85%[0m
[36m[2023-07-10 15:27:41,216][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 15:27:41,216][227910] FPS: 333582.69[0m
[36m[2023-07-10 15:27:45,942][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:27:45,942][227910] Reward + Measures: [[-81.04986595   0.03622933   0.88208932   0.248679     0.8933146 ]][0m
[37m[1m[2023-07-10 15:27:45,943][227910] Max Reward on eval: -81.04986594578024[0m
[37m[1m[2023-07-10 15:27:45,943][227910] Min Reward on eval: -81.04986594578024[0m
[37m[1m[2023-07-10 15:27:45,943][227910] Mean Reward across all agents: -81.04986594578024[0m
[37m[1m[2023-07-10 15:27:45,943][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:27:51,386][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:27:51,387][227910] Reward + Measures: [[ -24.11071419    0.08290001    0.74450004    0.2404        0.74659997]
 [-106.76808067    0.0377        0.85859996    0.31330001    0.85720009]
 [ -66.51429811    0.0702        0.67189997    0.26820001    0.68669999]
 ...
 [-145.89967052    0.08400001    0.65430003    0.26480001    0.69850004]
 [ -50.75528024    0.053         0.88899994    0.24419999    0.8858    ]
 [ -97.20182489    0.044         0.87419999    0.24949999    0.87419999]][0m
[37m[1m[2023-07-10 15:27:51,387][227910] Max Reward on eval: 55.45977293414762[0m
[37m[1m[2023-07-10 15:27:51,387][227910] Min Reward on eval: -361.1381484357174[0m
[37m[1m[2023-07-10 15:27:51,387][227910] Mean Reward across all agents: -65.6271775934237[0m
[37m[1m[2023-07-10 15:27:51,388][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:27:51,393][227910] mean_value=103.55612617290706, max_value=453.1842027364569[0m
[37m[1m[2023-07-10 15:27:51,396][227910] New mean coefficients: [[ 2.2631893 -5.4682674 -2.6129892 -3.5917082  6.0083265]][0m
[37m[1m[2023-07-10 15:27:51,397][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:28:01,058][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 15:28:01,059][227910] FPS: 397513.19[0m
[36m[2023-07-10 15:28:01,061][227910] itr=718, itrs=2000, Progress: 35.90%[0m
[36m[2023-07-10 15:28:12,670][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 15:28:12,670][227910] FPS: 331255.79[0m
[36m[2023-07-10 15:28:17,496][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:28:17,502][227910] Reward + Measures: [[-8.60447973  0.03328133  0.89395672  0.24347633  0.91085798]][0m
[37m[1m[2023-07-10 15:28:17,502][227910] Max Reward on eval: -8.604479727880962[0m
[37m[1m[2023-07-10 15:28:17,502][227910] Min Reward on eval: -8.604479727880962[0m
[37m[1m[2023-07-10 15:28:17,502][227910] Mean Reward across all agents: -8.604479727880962[0m
[37m[1m[2023-07-10 15:28:17,503][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:28:23,151][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:28:23,157][227910] Reward + Measures: [[-177.0185205     0.2359        0.66350001    0.33650002    0.58389997]
 [ -44.39597442    0.08530001    0.86049998    0.2253        0.85000002]
 [  48.67814549    0.0475        0.89369994    0.22330001    0.90539998]
 ...
 [ -38.05601401    0.08810001    0.77810001    0.31780002    0.76789999]
 [ -78.3150522     0.07210001    0.84820002    0.23179999    0.83599997]
 [ -54.81478275    0.0764        0.84060001    0.2527        0.82039994]][0m
[37m[1m[2023-07-10 15:28:23,157][227910] Max Reward on eval: 112.19760245096404[0m
[37m[1m[2023-07-10 15:28:23,157][227910] Min Reward on eval: -305.6653341836703[0m
[37m[1m[2023-07-10 15:28:23,158][227910] Mean Reward across all agents: -68.31001107046686[0m
[37m[1m[2023-07-10 15:28:23,158][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:28:23,161][227910] mean_value=18.984997347690477, max_value=432.60880316939404[0m
[37m[1m[2023-07-10 15:28:23,164][227910] New mean coefficients: [[ 1.764447  -2.0030484 -1.4203227 -3.9025574  4.4929757]][0m
[37m[1m[2023-07-10 15:28:23,165][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:28:32,916][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 15:28:32,916][227910] FPS: 393872.65[0m
[36m[2023-07-10 15:28:32,918][227910] itr=719, itrs=2000, Progress: 35.95%[0m
[36m[2023-07-10 15:28:44,501][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 15:28:44,501][227910] FPS: 332009.96[0m
[36m[2023-07-10 15:28:49,293][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:28:49,293][227910] Reward + Measures: [[26.7628828   0.036847    0.90105796  0.21847834  0.92102605]][0m
[37m[1m[2023-07-10 15:28:49,293][227910] Max Reward on eval: 26.76288279839019[0m
[37m[1m[2023-07-10 15:28:49,294][227910] Min Reward on eval: 26.76288279839019[0m
[37m[1m[2023-07-10 15:28:49,294][227910] Mean Reward across all agents: 26.76288279839019[0m
[37m[1m[2023-07-10 15:28:49,294][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:28:54,849][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:28:54,849][227910] Reward + Measures: [[  99.36494387    0.0868        0.83920002    0.16790001    0.87989998]
 [ 185.1384546     0.09100001    0.81750005    0.1837        0.84710008]
 [ -27.2138908     0.0147        0.89160007    0.3867        0.89020008]
 ...
 [  49.49743714    0.0193        0.91090006    0.30140001    0.90529996]
 [ -49.29695037    0.011         0.77569997    0.46700001    0.81829995]
 [-204.90198912    0.0214        0.78310007    0.35089999    0.76789999]][0m
[37m[1m[2023-07-10 15:28:54,849][227910] Max Reward on eval: 217.13122927838702[0m
[37m[1m[2023-07-10 15:28:54,850][227910] Min Reward on eval: -264.53939780787914[0m
[37m[1m[2023-07-10 15:28:54,850][227910] Mean Reward across all agents: 47.76862591243358[0m
[37m[1m[2023-07-10 15:28:54,850][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:28:54,855][227910] mean_value=112.5881826769625, max_value=688.3012840473442[0m
[37m[1m[2023-07-10 15:28:54,858][227910] New mean coefficients: [[ 3.7457185   0.27635598  0.17338872 -4.9474907   3.6956491 ]][0m
[37m[1m[2023-07-10 15:28:54,859][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:29:04,545][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 15:29:04,545][227910] FPS: 396493.10[0m
[36m[2023-07-10 15:29:04,548][227910] itr=720, itrs=2000, Progress: 36.00%[0m
[37m[1m[2023-07-10 15:29:07,677][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000700[0m
[36m[2023-07-10 15:29:19,513][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 15:29:19,513][227910] FPS: 331712.17[0m
[36m[2023-07-10 15:29:24,289][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:29:24,289][227910] Reward + Measures: [[98.1797617   0.047931    0.91110301  0.189796    0.92828465]][0m
[37m[1m[2023-07-10 15:29:24,290][227910] Max Reward on eval: 98.17976169914566[0m
[37m[1m[2023-07-10 15:29:24,290][227910] Min Reward on eval: 98.17976169914566[0m
[37m[1m[2023-07-10 15:29:24,290][227910] Mean Reward across all agents: 98.17976169914566[0m
[37m[1m[2023-07-10 15:29:24,290][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:29:29,759][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:29:29,765][227910] Reward + Measures: [[ 48.0010067    0.01         0.78649998   0.41170001   0.76820004]
 [ 23.12137143   0.0456       0.84580004   0.26269999   0.83459997]
 [129.92850312   0.0161       0.74069995   0.41879997   0.69880003]
 ...
 [131.2143236    0.0315       0.92129993   0.23600002   0.93730003]
 [151.35168765   0.0729       0.81510001   0.25240001   0.82250005]
 [178.49838922   0.0226       0.86429995   0.3197       0.8768    ]][0m
[37m[1m[2023-07-10 15:29:29,765][227910] Max Reward on eval: 259.10863883218553[0m
[37m[1m[2023-07-10 15:29:29,765][227910] Min Reward on eval: -146.38498720675707[0m
[37m[1m[2023-07-10 15:29:29,766][227910] Mean Reward across all agents: 110.60643623436064[0m
[37m[1m[2023-07-10 15:29:29,766][227910] Average Trajectory Length: 999.939[0m
[36m[2023-07-10 15:29:29,770][227910] mean_value=108.22872448597703, max_value=759.1086388321855[0m
[37m[1m[2023-07-10 15:29:29,773][227910] New mean coefficients: [[ 5.0808887  -0.93535626 -0.45430827 -5.243169    5.450689  ]][0m
[37m[1m[2023-07-10 15:29:29,774][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:29:39,609][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 15:29:39,610][227910] FPS: 390477.61[0m
[36m[2023-07-10 15:29:39,612][227910] itr=721, itrs=2000, Progress: 36.05%[0m
[36m[2023-07-10 15:29:51,475][227910] train() took 11.85 seconds to complete[0m
[36m[2023-07-10 15:29:51,475][227910] FPS: 324153.57[0m
[36m[2023-07-10 15:29:56,148][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:29:56,148][227910] Reward + Measures: [[186.85151572   0.046321     0.91625667   0.180356     0.93484002]][0m
[37m[1m[2023-07-10 15:29:56,149][227910] Max Reward on eval: 186.8515157214117[0m
[37m[1m[2023-07-10 15:29:56,149][227910] Min Reward on eval: 186.8515157214117[0m
[37m[1m[2023-07-10 15:29:56,149][227910] Mean Reward across all agents: 186.8515157214117[0m
[37m[1m[2023-07-10 15:29:56,149][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:30:01,867][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:30:01,868][227910] Reward + Measures: [[203.84108065   0.0532       0.86809999   0.23450001   0.90590012]
 [238.17093023   0.0534       0.90450001   0.2067       0.93660003]
 [194.63354926   0.0506       0.89470005   0.2031       0.93050003]
 ...
 [124.94765033   0.042        0.90599996   0.2378       0.93190002]
 [ 67.07318484   0.0336       0.89729995   0.26519999   0.91440004]
 [107.30654033   0.0446       0.90439999   0.2218       0.91210002]][0m
[37m[1m[2023-07-10 15:30:01,868][227910] Max Reward on eval: 269.22447641460457[0m
[37m[1m[2023-07-10 15:30:01,868][227910] Min Reward on eval: -409.0130729488563[0m
[37m[1m[2023-07-10 15:30:01,868][227910] Mean Reward across all agents: 92.12675337297496[0m
[37m[1m[2023-07-10 15:30:01,869][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:30:01,871][227910] mean_value=12.127797504429703, max_value=525.5134187229463[0m
[37m[1m[2023-07-10 15:30:01,874][227910] New mean coefficients: [[ 5.810359    0.7749523  -0.40995508 -4.2332244   5.253816  ]][0m
[37m[1m[2023-07-10 15:30:01,875][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:30:11,543][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 15:30:11,544][227910] FPS: 397228.94[0m
[36m[2023-07-10 15:30:11,546][227910] itr=722, itrs=2000, Progress: 36.10%[0m
[36m[2023-07-10 15:30:22,996][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 15:30:22,996][227910] FPS: 335930.71[0m
[36m[2023-07-10 15:30:27,776][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:30:27,776][227910] Reward + Measures: [[267.85876002   0.05506666   0.92086637   0.16436133   0.93739533]][0m
[37m[1m[2023-07-10 15:30:27,777][227910] Max Reward on eval: 267.8587600234082[0m
[37m[1m[2023-07-10 15:30:27,777][227910] Min Reward on eval: 267.8587600234082[0m
[37m[1m[2023-07-10 15:30:27,777][227910] Mean Reward across all agents: 267.8587600234082[0m
[37m[1m[2023-07-10 15:30:27,778][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:30:33,156][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:30:33,157][227910] Reward + Measures: [[315.48861712   0.0509       0.93149996   0.1811       0.93260002]
 [-21.59790837   0.0585       0.87760001   0.19690001   0.8563    ]
 [  8.43794186   0.0624       0.9174       0.18450001   0.88840002]
 ...
 [ 91.48669048   0.0498       0.92179996   0.25729999   0.90649998]
 [176.42950872   0.09600001   0.87010002   0.16240001   0.8976    ]
 [292.87437222   0.05930001   0.7906       0.2861       0.84320003]][0m
[37m[1m[2023-07-10 15:30:33,157][227910] Max Reward on eval: 436.66920270141566[0m
[37m[1m[2023-07-10 15:30:33,157][227910] Min Reward on eval: -357.685530501639[0m
[37m[1m[2023-07-10 15:30:33,158][227910] Mean Reward across all agents: 91.85937657779797[0m
[37m[1m[2023-07-10 15:30:33,158][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:30:33,161][227910] mean_value=-37.162875809111924, max_value=730.0314004629734[0m
[37m[1m[2023-07-10 15:30:33,164][227910] New mean coefficients: [[ 5.0717955 -1.5051035 -1.5574462 -4.0358586  6.620267 ]][0m
[37m[1m[2023-07-10 15:30:33,165][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:30:42,793][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 15:30:42,793][227910] FPS: 398910.18[0m
[36m[2023-07-10 15:30:42,795][227910] itr=723, itrs=2000, Progress: 36.15%[0m
[36m[2023-07-10 15:30:54,384][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 15:30:54,385][227910] FPS: 331898.54[0m
[36m[2023-07-10 15:30:59,156][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:30:59,156][227910] Reward + Measures: [[317.35445415   0.05982833   0.93071306   0.15452799   0.94625473]][0m
[37m[1m[2023-07-10 15:30:59,156][227910] Max Reward on eval: 317.3544541469491[0m
[37m[1m[2023-07-10 15:30:59,157][227910] Min Reward on eval: 317.3544541469491[0m
[37m[1m[2023-07-10 15:30:59,157][227910] Mean Reward across all agents: 317.3544541469491[0m
[37m[1m[2023-07-10 15:30:59,157][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:31:04,552][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:31:04,557][227910] Reward + Measures: [[262.1306675    0.0381       0.9073       0.26569998   0.92000002]
 [259.85899606   0.0575       0.88199997   0.259        0.89429998]
 [ 44.28776276   0.08580001   0.87299997   0.18139999   0.87339991]
 ...
 [329.1283487    0.13950001   0.76669997   0.24940002   0.78190005]
 [162.22934328   0.0556       0.88269997   0.2422       0.88740009]
 [181.59985359   0.0444       0.83090001   0.26890001   0.81549996]][0m
[37m[1m[2023-07-10 15:31:04,558][227910] Max Reward on eval: 404.57012545404433[0m
[37m[1m[2023-07-10 15:31:04,558][227910] Min Reward on eval: -102.26407720327843[0m
[37m[1m[2023-07-10 15:31:04,558][227910] Mean Reward across all agents: 257.3980465244156[0m
[37m[1m[2023-07-10 15:31:04,559][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:31:04,562][227910] mean_value=9.43581192457555, max_value=447.89634665878407[0m
[37m[1m[2023-07-10 15:31:04,565][227910] New mean coefficients: [[ 6.757147  -1.1850893 -0.5395533 -4.475389   5.130759 ]][0m
[37m[1m[2023-07-10 15:31:04,566][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:31:14,333][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 15:31:14,334][227910] FPS: 393196.23[0m
[36m[2023-07-10 15:31:14,336][227910] itr=724, itrs=2000, Progress: 36.20%[0m
[36m[2023-07-10 15:31:25,825][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 15:31:25,826][227910] FPS: 334697.32[0m
[36m[2023-07-10 15:31:30,541][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:31:30,541][227910] Reward + Measures: [[392.09036755   0.062544     0.93687099   0.14545433   0.95033836]][0m
[37m[1m[2023-07-10 15:31:30,542][227910] Max Reward on eval: 392.0903675549448[0m
[37m[1m[2023-07-10 15:31:30,542][227910] Min Reward on eval: 392.0903675549448[0m
[37m[1m[2023-07-10 15:31:30,542][227910] Mean Reward across all agents: 392.0903675549448[0m
[37m[1m[2023-07-10 15:31:30,542][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:31:35,943][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:31:35,949][227910] Reward + Measures: [[122.22368154   0.0908       0.88859999   0.1956       0.86390001]
 [225.33100435   0.10639999   0.90469998   0.21109998   0.90360004]
 [302.27960474   0.0606       0.91559994   0.23530002   0.89120007]
 ...
 [377.72835672   0.0794       0.91820002   0.1504       0.92430001]
 [162.57551567   0.0603       0.93950003   0.20130001   0.92159998]
 [165.3309892    0.1028       0.8962       0.19230001   0.89600003]][0m
[37m[1m[2023-07-10 15:31:35,949][227910] Max Reward on eval: 451.74678502407625[0m
[37m[1m[2023-07-10 15:31:35,950][227910] Min Reward on eval: -108.10242664880352[0m
[37m[1m[2023-07-10 15:31:35,950][227910] Mean Reward across all agents: 223.48635419314718[0m
[37m[1m[2023-07-10 15:31:35,950][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:31:35,954][227910] mean_value=15.048610802471073, max_value=844.6275140402606[0m
[37m[1m[2023-07-10 15:31:35,956][227910] New mean coefficients: [[ 6.625122  -3.311852  -1.6618853 -4.426278   5.606701 ]][0m
[37m[1m[2023-07-10 15:31:35,958][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:31:45,803][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 15:31:45,804][227910] FPS: 390088.58[0m
[36m[2023-07-10 15:31:45,806][227910] itr=725, itrs=2000, Progress: 36.25%[0m
[36m[2023-07-10 15:31:57,257][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 15:31:57,257][227910] FPS: 335901.51[0m
[36m[2023-07-10 15:32:02,104][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:32:02,105][227910] Reward + Measures: [[490.53267264   0.06397      0.93383527   0.13844267   0.94795132]][0m
[37m[1m[2023-07-10 15:32:02,105][227910] Max Reward on eval: 490.53267263897044[0m
[37m[1m[2023-07-10 15:32:02,105][227910] Min Reward on eval: 490.53267263897044[0m
[37m[1m[2023-07-10 15:32:02,105][227910] Mean Reward across all agents: 490.53267263897044[0m
[37m[1m[2023-07-10 15:32:02,106][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:32:07,589][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:32:07,595][227910] Reward + Measures: [[263.38381955   0.0418       0.68840003   0.41060001   0.62779999]
 [333.67052973   0.0266       0.84180003   0.39030004   0.83230001]
 [339.82699569   0.0374       0.93590003   0.2227       0.93370003]
 ...
 [537.60965565   0.1427       0.87099999   0.23150001   0.89660007]
 [144.50173183   0.0887       0.89300007   0.19340001   0.86469996]
 [396.54012522   0.0765       0.87919998   0.2484       0.87700003]][0m
[37m[1m[2023-07-10 15:32:07,596][227910] Max Reward on eval: 537.609655650251[0m
[37m[1m[2023-07-10 15:32:07,596][227910] Min Reward on eval: -52.9140631554299[0m
[37m[1m[2023-07-10 15:32:07,597][227910] Mean Reward across all agents: 282.645091528822[0m
[37m[1m[2023-07-10 15:32:07,598][227910] Average Trajectory Length: 999.7033333333333[0m
[36m[2023-07-10 15:32:07,606][227910] mean_value=48.61691871492013, max_value=934.0658462122898[0m
[37m[1m[2023-07-10 15:32:07,611][227910] New mean coefficients: [[ 7.1155615 -3.4969754 -2.0463653 -3.674066   5.196057 ]][0m
[37m[1m[2023-07-10 15:32:07,612][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:32:17,289][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 15:32:17,289][227910] FPS: 396907.26[0m
[36m[2023-07-10 15:32:17,291][227910] itr=726, itrs=2000, Progress: 36.30%[0m
[36m[2023-07-10 15:32:28,746][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 15:32:28,746][227910] FPS: 335734.46[0m
[36m[2023-07-10 15:32:33,387][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:32:33,387][227910] Reward + Measures: [[588.78372426   0.07230367   0.92294908   0.13774467   0.94398504]][0m
[37m[1m[2023-07-10 15:32:33,387][227910] Max Reward on eval: 588.7837242624976[0m
[37m[1m[2023-07-10 15:32:33,388][227910] Min Reward on eval: 588.7837242624976[0m
[37m[1m[2023-07-10 15:32:33,388][227910] Mean Reward across all agents: 588.7837242624976[0m
[37m[1m[2023-07-10 15:32:33,388][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:32:38,888][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:32:38,889][227910] Reward + Measures: [[333.3511083    0.0819       0.8391       0.27630004   0.85680002]
 [370.1842427    0.1383       0.75370002   0.3091       0.75519997]
 [380.77444209   0.06860001   0.85170001   0.26449999   0.88809997]
 ...
 [450.67675644   0.13780001   0.76330006   0.27850002   0.77719998]
 [565.50206384   0.13520001   0.84400004   0.2313       0.86199999]
 [514.43505983   0.13920002   0.74009997   0.32510003   0.75530005]][0m
[37m[1m[2023-07-10 15:32:38,889][227910] Max Reward on eval: 605.9361846037907[0m
[37m[1m[2023-07-10 15:32:38,889][227910] Min Reward on eval: -167.9622179202619[0m
[37m[1m[2023-07-10 15:32:38,889][227910] Mean Reward across all agents: 441.5726240655212[0m
[37m[1m[2023-07-10 15:32:38,889][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:32:38,896][227910] mean_value=118.71526104055141, max_value=890.6710237647638[0m
[37m[1m[2023-07-10 15:32:38,899][227910] New mean coefficients: [[ 7.331435  -3.4380338 -1.9423573 -3.5289984  4.534864 ]][0m
[37m[1m[2023-07-10 15:32:38,900][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:32:48,474][227910] train() took 9.57 seconds to complete[0m
[36m[2023-07-10 15:32:48,474][227910] FPS: 401147.63[0m
[36m[2023-07-10 15:32:48,476][227910] itr=727, itrs=2000, Progress: 36.35%[0m
[36m[2023-07-10 15:32:59,898][227910] train() took 11.40 seconds to complete[0m
[36m[2023-07-10 15:32:59,898][227910] FPS: 336700.85[0m
[36m[2023-07-10 15:33:04,661][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:33:04,662][227910] Reward + Measures: [[645.29081109   0.06801467   0.92171091   0.141583     0.94485193]][0m
[37m[1m[2023-07-10 15:33:04,662][227910] Max Reward on eval: 645.2908110916778[0m
[37m[1m[2023-07-10 15:33:04,662][227910] Min Reward on eval: 645.2908110916778[0m
[37m[1m[2023-07-10 15:33:04,662][227910] Mean Reward across all agents: 645.2908110916778[0m
[37m[1m[2023-07-10 15:33:04,663][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:33:10,158][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:33:10,226][227910] Reward + Measures: [[490.45861064   0.0798       0.86199999   0.2494       0.89089996]
 [412.84666687   0.0413       0.83570004   0.4578       0.88920003]
 [368.93499079   0.1062       0.87029999   0.1538       0.86020005]
 ...
 [370.6129454    0.063        0.87480003   0.2472       0.88950008]
 [429.47967174   0.0231       0.92480004   0.31359997   0.93020004]
 [355.11289045   0.0697       0.8452       0.2325       0.86020005]][0m
[37m[1m[2023-07-10 15:33:10,227][227910] Max Reward on eval: 616.3095826768579[0m
[37m[1m[2023-07-10 15:33:10,227][227910] Min Reward on eval: 68.79734211930773[0m
[37m[1m[2023-07-10 15:33:10,227][227910] Mean Reward across all agents: 431.4020206858801[0m
[37m[1m[2023-07-10 15:33:10,228][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:33:10,235][227910] mean_value=137.8402304755748, max_value=930.68403770593[0m
[37m[1m[2023-07-10 15:33:10,238][227910] New mean coefficients: [[ 7.2938876 -3.382362  -1.6397372 -3.4362884  5.524435 ]][0m
[37m[1m[2023-07-10 15:33:10,239][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:33:19,944][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 15:33:19,944][227910] FPS: 395746.60[0m
[36m[2023-07-10 15:33:19,947][227910] itr=728, itrs=2000, Progress: 36.40%[0m
[36m[2023-07-10 15:33:31,497][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 15:33:31,497][227910] FPS: 332948.20[0m
[36m[2023-07-10 15:33:36,289][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:33:36,289][227910] Reward + Measures: [[685.61986823   0.06104133   0.92729396   0.13625199   0.95089835]][0m
[37m[1m[2023-07-10 15:33:36,289][227910] Max Reward on eval: 685.6198682319897[0m
[37m[1m[2023-07-10 15:33:36,290][227910] Min Reward on eval: 685.6198682319897[0m
[37m[1m[2023-07-10 15:33:36,290][227910] Mean Reward across all agents: 685.6198682319897[0m
[37m[1m[2023-07-10 15:33:36,290][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:33:41,775][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:33:41,781][227910] Reward + Measures: [[265.24957595   0.0118       0.93479997   0.40120003   0.92789996]
 [ 91.88459037   0.27990001   0.73190004   0.31729999   0.70139998]
 [330.9225789    0.0197       0.95139998   0.36190003   0.94169998]
 ...
 [414.46328229   0.1195       0.92119998   0.2106       0.92799997]
 [297.22572537   0.0623       0.93310004   0.1894       0.93280011]
 [ 32.13533537   0.0471       0.71520007   0.42950001   0.58529997]][0m
[37m[1m[2023-07-10 15:33:41,781][227910] Max Reward on eval: 667.248398314137[0m
[37m[1m[2023-07-10 15:33:41,782][227910] Min Reward on eval: -67.46800615637912[0m
[37m[1m[2023-07-10 15:33:41,782][227910] Mean Reward across all agents: 331.196149132321[0m
[37m[1m[2023-07-10 15:33:41,782][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:33:41,785][227910] mean_value=-70.06228838933335, max_value=765.4529712377464[0m
[37m[1m[2023-07-10 15:33:41,788][227910] New mean coefficients: [[ 6.851863  -2.5123544 -1.0723822 -2.6706333  4.4285   ]][0m
[37m[1m[2023-07-10 15:33:41,789][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:33:51,606][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 15:33:51,606][227910] FPS: 391255.71[0m
[36m[2023-07-10 15:33:51,608][227910] itr=729, itrs=2000, Progress: 36.45%[0m
[36m[2023-07-10 15:34:03,176][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 15:34:03,177][227910] FPS: 332423.52[0m
[36m[2023-07-10 15:34:07,971][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:34:07,971][227910] Reward + Measures: [[749.64683834   0.060151     0.93283802   0.13308233   0.95614761]][0m
[37m[1m[2023-07-10 15:34:07,971][227910] Max Reward on eval: 749.646838343741[0m
[37m[1m[2023-07-10 15:34:07,971][227910] Min Reward on eval: 749.646838343741[0m
[37m[1m[2023-07-10 15:34:07,972][227910] Mean Reward across all agents: 749.646838343741[0m
[37m[1m[2023-07-10 15:34:07,972][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:34:13,476][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:34:13,476][227910] Reward + Measures: [[436.46032459   0.037        0.91730005   0.40200004   0.91680002]
 [ 11.20441983   0.1714       0.51550001   0.4578       0.66150004]
 [213.0292942    0.1321       0.60880005   0.2929       0.75530005]
 ...
 [418.12485979   0.1432       0.80189991   0.26969999   0.85830003]
 [358.1988115    0.0786       0.78180003   0.2579       0.87349999]
 [410.72788354   0.0708       0.85039997   0.2418       0.90140003]][0m
[37m[1m[2023-07-10 15:34:13,476][227910] Max Reward on eval: 753.2363445761497[0m
[37m[1m[2023-07-10 15:34:13,477][227910] Min Reward on eval: 11.204419828433311[0m
[37m[1m[2023-07-10 15:34:13,477][227910] Mean Reward across all agents: 418.87790337119[0m
[37m[1m[2023-07-10 15:34:13,477][227910] Average Trajectory Length: 999.737[0m
[36m[2023-07-10 15:34:13,484][227910] mean_value=224.27247927302096, max_value=902.4474106383161[0m
[37m[1m[2023-07-10 15:34:13,487][227910] New mean coefficients: [[ 7.3439546 -5.8283167 -2.77171   -2.1599174  4.909975 ]][0m
[37m[1m[2023-07-10 15:34:13,488][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:34:23,145][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 15:34:23,145][227910] FPS: 397720.53[0m
[36m[2023-07-10 15:34:23,148][227910] itr=730, itrs=2000, Progress: 36.50%[0m
[37m[1m[2023-07-10 15:34:26,433][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000710[0m
[36m[2023-07-10 15:34:38,266][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 15:34:38,267][227910] FPS: 331428.51[0m
[36m[2023-07-10 15:34:43,015][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:34:43,015][227910] Reward + Measures: [[791.87052131   0.058543     0.92762238   0.14354733   0.95665461]][0m
[37m[1m[2023-07-10 15:34:43,016][227910] Max Reward on eval: 791.8705213130215[0m
[37m[1m[2023-07-10 15:34:43,016][227910] Min Reward on eval: 791.8705213130215[0m
[37m[1m[2023-07-10 15:34:43,016][227910] Mean Reward across all agents: 791.8705213130215[0m
[37m[1m[2023-07-10 15:34:43,016][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:34:48,516][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:34:48,517][227910] Reward + Measures: [[-987.41565437    0.32925454    0.4048546     0.09693636    0.37108183]
 [ 256.8256843     0.1497        0.74779999    0.1402        0.76470006]
 [-803.7183608     0.35587728    0.43261456    0.12603687    0.43042207]
 ...
 [ 374.66328474    0.28150001    0.85890001    0.0477        0.85519999]
 [-654.96568735    0.37780002    0.4219        0.12019999    0.4982    ]
 [ 145.0958806     0.27940002    0.78619999    0.0608        0.73950005]][0m
[37m[1m[2023-07-10 15:34:48,517][227910] Max Reward on eval: 787.3115803534863[0m
[37m[1m[2023-07-10 15:34:48,517][227910] Min Reward on eval: -2094.3187003341272[0m
[37m[1m[2023-07-10 15:34:48,517][227910] Mean Reward across all agents: -222.9135366618345[0m
[37m[1m[2023-07-10 15:34:48,518][227910] Average Trajectory Length: 988.1406666666667[0m
[36m[2023-07-10 15:34:48,525][227910] mean_value=-46.08773310392062, max_value=1107.2895259903976[0m
[37m[1m[2023-07-10 15:34:48,527][227910] New mean coefficients: [[ 7.062294  -5.354038  -2.5995855 -0.9479042  4.603997 ]][0m
[37m[1m[2023-07-10 15:34:48,528][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:34:58,263][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 15:34:58,263][227910] FPS: 394535.67[0m
[36m[2023-07-10 15:34:58,266][227910] itr=731, itrs=2000, Progress: 36.55%[0m
[36m[2023-07-10 15:35:09,777][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 15:35:09,777][227910] FPS: 334151.63[0m
[36m[2023-07-10 15:35:14,609][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:35:14,609][227910] Reward + Measures: [[835.73921238   0.04503367   0.91409636   0.16389701   0.95016164]][0m
[37m[1m[2023-07-10 15:35:14,609][227910] Max Reward on eval: 835.7392123794774[0m
[37m[1m[2023-07-10 15:35:14,610][227910] Min Reward on eval: 835.7392123794774[0m
[37m[1m[2023-07-10 15:35:14,610][227910] Mean Reward across all agents: 835.7392123794774[0m
[37m[1m[2023-07-10 15:35:14,610][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:35:20,049][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:35:20,050][227910] Reward + Measures: [[-1691.24901173     0.148          0.1885         0.14530002
      0.16859999]
 [-1078.79717062     0.3488         0.414          0.26269999
      0.32360002]
 [ -669.70691528     0.27340001     0.43420002     0.24690004
      0.40959999]
 ...
 [-1596.11067518     0.19230001     0.28639999     0.206
      0.22160001]
 [ -831.64454805     0.4726873      0.37936431     0.35154387
      0.36827937]
 [-1599.13723804     0.28450003     0.25840002     0.1768
      0.1497    ]][0m
[37m[1m[2023-07-10 15:35:20,050][227910] Max Reward on eval: 784.1830458988552[0m
[37m[1m[2023-07-10 15:35:20,050][227910] Min Reward on eval: -2077.3765160399025[0m
[37m[1m[2023-07-10 15:35:20,051][227910] Mean Reward across all agents: -764.9421213274184[0m
[37m[1m[2023-07-10 15:35:20,051][227910] Average Trajectory Length: 978.6949999999999[0m
[36m[2023-07-10 15:35:20,054][227910] mean_value=-1843.6787563543387, max_value=1155.5301762203103[0m
[37m[1m[2023-07-10 15:35:20,057][227910] New mean coefficients: [[ 6.782582   -0.6286125  -0.59639835 -1.6571462   3.1452093 ]][0m
[37m[1m[2023-07-10 15:35:20,058][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:35:29,867][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 15:35:29,867][227910] FPS: 391555.90[0m
[36m[2023-07-10 15:35:29,869][227910] itr=732, itrs=2000, Progress: 36.60%[0m
[36m[2023-07-10 15:35:41,372][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 15:35:41,372][227910] FPS: 334348.76[0m
[36m[2023-07-10 15:35:46,182][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:35:46,182][227910] Reward + Measures: [[867.1045903    0.04327834   0.92277735   0.16326566   0.95502734]][0m
[37m[1m[2023-07-10 15:35:46,182][227910] Max Reward on eval: 867.1045903015038[0m
[37m[1m[2023-07-10 15:35:46,183][227910] Min Reward on eval: 867.1045903015038[0m
[37m[1m[2023-07-10 15:35:46,183][227910] Mean Reward across all agents: 867.1045903015038[0m
[37m[1m[2023-07-10 15:35:46,183][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:35:51,428][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:35:51,428][227910] Reward + Measures: [[759.78423338   0.19840001   0.57690001   0.53360003   0.50120002]
 [679.00049007   0.1393       0.83550006   0.35400003   0.79750007]
 [844.22762489   0.2163       0.88059998   0.2467       0.89480001]
 ...
 [614.82436715   0.1028       0.84329998   0.31         0.82639998]
 [676.35501966   0.51020002   0.5262       0.48090002   0.53680003]
 [552.93029527   0.11250001   0.64610004   0.37250003   0.54260004]][0m
[37m[1m[2023-07-10 15:35:51,428][227910] Max Reward on eval: 854.1174885848304[0m
[37m[1m[2023-07-10 15:35:51,429][227910] Min Reward on eval: 203.32561845151358[0m
[37m[1m[2023-07-10 15:35:51,429][227910] Mean Reward across all agents: 646.8808330643208[0m
[37m[1m[2023-07-10 15:35:51,429][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:35:51,437][227910] mean_value=192.92746034317207, max_value=1155.2049568258105[0m
[37m[1m[2023-07-10 15:35:51,440][227910] New mean coefficients: [[ 6.4201226  -0.6185445  -0.64760774  0.10755217  2.5903797 ]][0m
[37m[1m[2023-07-10 15:35:51,441][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:36:01,190][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 15:36:01,190][227910] FPS: 393955.21[0m
[36m[2023-07-10 15:36:01,193][227910] itr=733, itrs=2000, Progress: 36.65%[0m
[36m[2023-07-10 15:36:12,908][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 15:36:12,909][227910] FPS: 328263.28[0m
[36m[2023-07-10 15:36:17,706][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:36:17,707][227910] Reward + Measures: [[885.43133875   0.13970634   0.86030996   0.18697701   0.90951401]][0m
[37m[1m[2023-07-10 15:36:17,707][227910] Max Reward on eval: 885.4313387487175[0m
[37m[1m[2023-07-10 15:36:17,707][227910] Min Reward on eval: 885.4313387487175[0m
[37m[1m[2023-07-10 15:36:17,707][227910] Mean Reward across all agents: 885.4313387487175[0m
[37m[1m[2023-07-10 15:36:17,708][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:36:23,398][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:36:23,399][227910] Reward + Measures: [[  653.91876059     0.1152         0.83750004     0.30099997
      0.94630003]
 [  797.56231053     0.51539999     0.80610001     0.1882
      0.76809996]
 [  666.50258234     0.78970003     0.75299996     0.64660007
      0.37820002]
 ...
 [-1347.67789567     0.17652325     0.4106724      0.19343956
      0.38880324]
 [  447.41655932     0.22059999     0.57020003     0.39610001
      0.72830003]
 [  -45.78198882     0.36320001     0.722          0.25330004
      0.75760001]][0m
[37m[1m[2023-07-10 15:36:23,399][227910] Max Reward on eval: 863.8529699142557[0m
[37m[1m[2023-07-10 15:36:23,399][227910] Min Reward on eval: -2210.235670304531[0m
[37m[1m[2023-07-10 15:36:23,399][227910] Mean Reward across all agents: -439.1266223651869[0m
[37m[1m[2023-07-10 15:36:23,400][227910] Average Trajectory Length: 984.211[0m
[36m[2023-07-10 15:36:23,407][227910] mean_value=-427.3507542493017, max_value=1363.8529699142557[0m
[37m[1m[2023-07-10 15:36:23,410][227910] New mean coefficients: [[ 6.756717    0.1017068  -0.27069706 -0.36363277  3.2774882 ]][0m
[37m[1m[2023-07-10 15:36:23,411][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:36:33,086][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 15:36:33,086][227910] FPS: 397000.17[0m
[36m[2023-07-10 15:36:33,088][227910] itr=734, itrs=2000, Progress: 36.70%[0m
[36m[2023-07-10 15:36:44,554][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 15:36:44,554][227910] FPS: 335412.17[0m
[36m[2023-07-10 15:36:49,309][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:36:49,309][227910] Reward + Measures: [[940.43355545   0.08412199   0.8780033    0.18087      0.89300162]][0m
[37m[1m[2023-07-10 15:36:49,310][227910] Max Reward on eval: 940.4335554517154[0m
[37m[1m[2023-07-10 15:36:49,310][227910] Min Reward on eval: 940.4335554517154[0m
[37m[1m[2023-07-10 15:36:49,310][227910] Mean Reward across all agents: 940.4335554517154[0m
[37m[1m[2023-07-10 15:36:49,310][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:36:54,750][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:36:54,756][227910] Reward + Measures: [[  629.83628278     0.64440006     0.63490003     0.46370003
      0.73450005]
 [  557.14411589     0.71309996     0.0807         0.82860005
      0.57929999]
 [ -980.83607063     0.24270001     0.3188         0.2411
      0.29049999]
 ...
 [-1377.12430206     0.19779229     0.19697611     0.2473437
      0.2335887 ]
 [ -931.8263806      0.26360002     0.68809998     0.30679998
      0.7026    ]
 [ -611.39730599     0.21620002     0.33389997     0.228
      0.33750001]][0m
[37m[1m[2023-07-10 15:36:54,756][227910] Max Reward on eval: 988.0448579895077[0m
[37m[1m[2023-07-10 15:36:54,756][227910] Min Reward on eval: -1990.8131154514383[0m
[37m[1m[2023-07-10 15:36:54,757][227910] Mean Reward across all agents: -311.87049751887776[0m
[37m[1m[2023-07-10 15:36:54,757][227910] Average Trajectory Length: 997.0426666666666[0m
[36m[2023-07-10 15:36:54,762][227910] mean_value=-655.9630303444904, max_value=1324.8324124776082[0m
[37m[1m[2023-07-10 15:36:54,765][227910] New mean coefficients: [[ 6.8750978 -1.3593268 -0.6783819 -0.1640735  2.514891 ]][0m
[37m[1m[2023-07-10 15:36:54,766][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:37:04,422][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 15:37:04,423][227910] FPS: 397746.96[0m
[36m[2023-07-10 15:37:04,425][227910] itr=735, itrs=2000, Progress: 36.75%[0m
[36m[2023-07-10 15:37:16,030][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 15:37:16,030][227910] FPS: 331384.26[0m
[36m[2023-07-10 15:37:20,848][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:37:20,849][227910] Reward + Measures: [[976.59309756   0.06983333   0.88064533   0.17552465   0.89202571]][0m
[37m[1m[2023-07-10 15:37:20,849][227910] Max Reward on eval: 976.5930975567971[0m
[37m[1m[2023-07-10 15:37:20,849][227910] Min Reward on eval: 976.5930975567971[0m
[37m[1m[2023-07-10 15:37:20,849][227910] Mean Reward across all agents: 976.5930975567971[0m
[37m[1m[2023-07-10 15:37:20,849][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:37:26,225][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:37:26,226][227910] Reward + Measures: [[-935.46500761    0.13950288    0.23895505    0.192983      0.2434928 ]
 [-518.54142808    0.11120899    0.43037185    0.29304275    0.42637378]
 [-894.14714841    0.37892669    0.36019614    0.44034705    0.4253791 ]
 ...
 [ 243.33398332    0.59369999    0.56970006    0.68520004    0.32350001]
 [-935.28553753    0.44980001    0.46080002    0.46890002    0.41850004]
 [-479.0620915     0.0352        0.79650003    0.72609997    0.83680004]][0m
[37m[1m[2023-07-10 15:37:26,226][227910] Max Reward on eval: 874.5333469902631[0m
[37m[1m[2023-07-10 15:37:26,226][227910] Min Reward on eval: -1548.623005118093[0m
[37m[1m[2023-07-10 15:37:26,227][227910] Mean Reward across all agents: -128.42957131940412[0m
[37m[1m[2023-07-10 15:37:26,227][227910] Average Trajectory Length: 988.5833333333333[0m
[36m[2023-07-10 15:37:26,233][227910] mean_value=-315.2402650585487, max_value=1348.5411113059731[0m
[37m[1m[2023-07-10 15:37:26,236][227910] New mean coefficients: [[ 6.7339716  -0.9023927  -0.40727058  0.7092289   2.1873372 ]][0m
[37m[1m[2023-07-10 15:37:26,237][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:37:35,941][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 15:37:35,942][227910] FPS: 395767.42[0m
[36m[2023-07-10 15:37:35,944][227910] itr=736, itrs=2000, Progress: 36.80%[0m
[36m[2023-07-10 15:37:47,575][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 15:37:47,575][227910] FPS: 330707.11[0m
[36m[2023-07-10 15:37:52,361][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:37:52,362][227910] Reward + Measures: [[1021.19362377    0.07018233    0.880503      0.18506998    0.8777793 ]][0m
[37m[1m[2023-07-10 15:37:52,362][227910] Max Reward on eval: 1021.1936237725355[0m
[37m[1m[2023-07-10 15:37:52,362][227910] Min Reward on eval: 1021.1936237725355[0m
[37m[1m[2023-07-10 15:37:52,362][227910] Mean Reward across all agents: 1021.1936237725355[0m
[37m[1m[2023-07-10 15:37:52,362][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:37:57,771][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:37:57,772][227910] Reward + Measures: [[ -610.4428879      0.3028         0.55040002     0.61230004
      0.65319997]
 [-1058.62694737     0.34700003     0.69510001     0.1496
      0.65450001]
 [  806.60495866     0.0412         0.9417001      0.3872
      0.84250003]
 ...
 [ -821.55390155     0.44241816     0.40572444     0.33469591
      0.39542004]
 [-1606.47872925     0.2723493      0.28717089     0.24363756
      0.25416243]
 [  726.36964665     0.36980003     0.65410006     0.37459999
      0.56800002]][0m
[37m[1m[2023-07-10 15:37:57,772][227910] Max Reward on eval: 1040.6125896233716[0m
[37m[1m[2023-07-10 15:37:57,772][227910] Min Reward on eval: -1647.6969068526291[0m
[37m[1m[2023-07-10 15:37:57,773][227910] Mean Reward across all agents: 34.27827282271965[0m
[37m[1m[2023-07-10 15:37:57,773][227910] Average Trajectory Length: 988.6373333333333[0m
[36m[2023-07-10 15:37:57,779][227910] mean_value=-585.9764167637735, max_value=1424.436000554543[0m
[37m[1m[2023-07-10 15:37:57,782][227910] New mean coefficients: [[7.09306    0.21326    0.26716557 0.2906198  1.7247036 ]][0m
[37m[1m[2023-07-10 15:37:57,783][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:38:07,582][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 15:38:07,583][227910] FPS: 391931.87[0m
[36m[2023-07-10 15:38:07,585][227910] itr=737, itrs=2000, Progress: 36.85%[0m
[36m[2023-07-10 15:38:19,171][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 15:38:19,171][227910] FPS: 331912.97[0m
[36m[2023-07-10 15:38:23,974][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:38:23,975][227910] Reward + Measures: [[1074.02113479    0.06578399    0.89473599    0.18027668    0.85639697]][0m
[37m[1m[2023-07-10 15:38:23,975][227910] Max Reward on eval: 1074.0211347921445[0m
[37m[1m[2023-07-10 15:38:23,975][227910] Min Reward on eval: 1074.0211347921445[0m
[37m[1m[2023-07-10 15:38:23,975][227910] Mean Reward across all agents: 1074.0211347921445[0m
[37m[1m[2023-07-10 15:38:23,975][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:38:29,512][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:38:29,512][227910] Reward + Measures: [[  819.79980975     0.50690001     0.62440002     0.61189997
      0.42550001]
 [ -520.35189761     0.23469999     0.36170003     0.29879999
      0.43439999]
 [-1408.65779467     0.44840002     0.25709999     0.3152
      0.28550002]
 ...
 [  620.91224767     0.10820001     0.87279999     0.24249999
      0.79540002]
 [ -593.08650217     0.38009998     0.68739998     0.5205
      0.45720002]
 [  258.1610036      0.76849997     0.78999996     0.86210006
      0.45879999]][0m
[37m[1m[2023-07-10 15:38:29,513][227910] Max Reward on eval: 1025.1104616159341[0m
[37m[1m[2023-07-10 15:38:29,513][227910] Min Reward on eval: -1718.5095845998846[0m
[37m[1m[2023-07-10 15:38:29,513][227910] Mean Reward across all agents: 78.43100175104584[0m
[37m[1m[2023-07-10 15:38:29,513][227910] Average Trajectory Length: 996.103[0m
[36m[2023-07-10 15:38:29,520][227910] mean_value=-434.2301339586042, max_value=1508.4526180882008[0m
[37m[1m[2023-07-10 15:38:29,522][227910] New mean coefficients: [[6.890632   0.74999756 0.5407009  0.753361   1.6105059 ]][0m
[37m[1m[2023-07-10 15:38:29,523][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:38:39,359][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 15:38:39,359][227910] FPS: 390490.91[0m
[36m[2023-07-10 15:38:39,361][227910] itr=738, itrs=2000, Progress: 36.90%[0m
[36m[2023-07-10 15:38:50,871][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 15:38:50,872][227910] FPS: 334202.80[0m
[36m[2023-07-10 15:38:55,655][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:38:55,655][227910] Reward + Measures: [[1147.84518608    0.072209      0.91189665    0.189264      0.78981525]][0m
[37m[1m[2023-07-10 15:38:55,655][227910] Max Reward on eval: 1147.8451860786524[0m
[37m[1m[2023-07-10 15:38:55,656][227910] Min Reward on eval: 1147.8451860786524[0m
[37m[1m[2023-07-10 15:38:55,656][227910] Mean Reward across all agents: 1147.8451860786524[0m
[37m[1m[2023-07-10 15:38:55,656][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:39:01,300][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:39:01,301][227910] Reward + Measures: [[ 328.22488192    0.85100001    0.1758        0.84090006    0.75139999]
 [1040.14236983    0.15970002    0.91779995    0.29249999    0.79520005]
 [ 795.42955842    0.2714        0.53380001    0.35460004    0.43979999]
 ...
 [-114.70515215    0.7841        0.7913        0.81370002    0.79459995]
 [ 491.72903781    0.74910003    0.35499999    0.76750004    0.69690001]
 [-381.25090649    0.59580004    0.71600002    0.52240002    0.8193    ]][0m
[37m[1m[2023-07-10 15:39:01,301][227910] Max Reward on eval: 1211.6081946921302[0m
[37m[1m[2023-07-10 15:39:01,301][227910] Min Reward on eval: -1793.9092198774683[0m
[37m[1m[2023-07-10 15:39:01,301][227910] Mean Reward across all agents: 431.22105513994705[0m
[37m[1m[2023-07-10 15:39:01,302][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:39:01,310][227910] mean_value=155.7245921521734, max_value=1653.2941780676133[0m
[37m[1m[2023-07-10 15:39:01,313][227910] New mean coefficients: [[7.567339   0.95528215 0.7981932  0.03052711 0.4681338 ]][0m
[37m[1m[2023-07-10 15:39:01,314][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:39:11,086][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 15:39:11,086][227910] FPS: 393029.54[0m
[36m[2023-07-10 15:39:11,088][227910] itr=739, itrs=2000, Progress: 36.95%[0m
[36m[2023-07-10 15:39:22,753][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 15:39:22,753][227910] FPS: 329672.75[0m
[36m[2023-07-10 15:39:27,553][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:39:27,553][227910] Reward + Measures: [[1244.88068566    0.10306667    0.9162246     0.18481934    0.70613891]][0m
[37m[1m[2023-07-10 15:39:27,553][227910] Max Reward on eval: 1244.880685656603[0m
[37m[1m[2023-07-10 15:39:27,554][227910] Min Reward on eval: 1244.880685656603[0m
[37m[1m[2023-07-10 15:39:27,554][227910] Mean Reward across all agents: 1244.880685656603[0m
[37m[1m[2023-07-10 15:39:27,554][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:39:33,058][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:39:33,063][227910] Reward + Measures: [[1088.5009003     0.32409999    0.81540006    0.28959998    0.61680001]
 [ 555.96339923    0.69430012    0.80459994    0.82779998    0.7141    ]
 [ 541.54353727    0.40140006    0.58120006    0.33309999    0.48609996]
 ...
 [ 596.55152932    0.24270001    0.80200005    0.55470002    0.76290005]
 [ 830.58306515    0.51800007    0.4217        0.59280002    0.39810002]
 [-623.1296686     0.1832        0.69670004    0.40580001    0.67300004]][0m
[37m[1m[2023-07-10 15:39:33,064][227910] Max Reward on eval: 1224.4895017656497[0m
[37m[1m[2023-07-10 15:39:33,064][227910] Min Reward on eval: -1163.1080573148793[0m
[37m[1m[2023-07-10 15:39:33,064][227910] Mean Reward across all agents: 273.37905836637344[0m
[37m[1m[2023-07-10 15:39:33,065][227910] Average Trajectory Length: 987.8026666666666[0m
[36m[2023-07-10 15:39:33,072][227910] mean_value=-126.14592834025423, max_value=1713.2073150387034[0m
[37m[1m[2023-07-10 15:39:33,075][227910] New mean coefficients: [[7.3861055  1.2942278  0.65380615 0.32013205 0.939174  ]][0m
[37m[1m[2023-07-10 15:39:33,076][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:39:42,867][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 15:39:42,868][227910] FPS: 392232.60[0m
[36m[2023-07-10 15:39:42,870][227910] itr=740, itrs=2000, Progress: 37.00%[0m
[37m[1m[2023-07-10 15:39:46,127][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000720[0m
[36m[2023-07-10 15:39:57,993][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 15:39:57,994][227910] FPS: 330716.15[0m
[36m[2023-07-10 15:40:02,762][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:40:02,762][227910] Reward + Measures: [[1350.74803542    0.09807999    0.93379802    0.16810566    0.66874397]][0m
[37m[1m[2023-07-10 15:40:02,763][227910] Max Reward on eval: 1350.7480354179652[0m
[37m[1m[2023-07-10 15:40:02,763][227910] Min Reward on eval: 1350.7480354179652[0m
[37m[1m[2023-07-10 15:40:02,763][227910] Mean Reward across all agents: 1350.7480354179652[0m
[37m[1m[2023-07-10 15:40:02,763][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:40:08,260][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:40:08,261][227910] Reward + Measures: [[  220.4337682      0.51590008     0.57139999     0.26930001
      0.54460001]
 [ -209.76775583     0.40937534     0.45683333     0.22829631
      0.35667655]
 [-1011.25037491     0.45930001     0.94249994     0.0553
      0.92370003]
 ...
 [  865.55848071     0.60960001     0.49860007     0.59030002
      0.55760002]
 [   36.34999229     0.12119999     0.76389998     0.24519999
      0.72069997]
 [  906.39935166     0.40200001     0.93310004     0.32239997
      0.89729995]][0m
[37m[1m[2023-07-10 15:40:08,261][227910] Max Reward on eval: 1149.2676177521237[0m
[37m[1m[2023-07-10 15:40:08,261][227910] Min Reward on eval: -2102.294251360721[0m
[37m[1m[2023-07-10 15:40:08,261][227910] Mean Reward across all agents: 355.0923328813157[0m
[37m[1m[2023-07-10 15:40:08,262][227910] Average Trajectory Length: 995.9683333333332[0m
[36m[2023-07-10 15:40:08,269][227910] mean_value=-133.43033563670267, max_value=1546.3944091157289[0m
[37m[1m[2023-07-10 15:40:08,272][227910] New mean coefficients: [[6.914356   0.5895848  0.44576415 0.4888109  1.5153992 ]][0m
[37m[1m[2023-07-10 15:40:08,273][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:40:17,979][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 15:40:17,980][227910] FPS: 395662.18[0m
[36m[2023-07-10 15:40:17,982][227910] itr=741, itrs=2000, Progress: 37.05%[0m
[36m[2023-07-10 15:40:29,445][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 15:40:29,445][227910] FPS: 335502.03[0m
[36m[2023-07-10 15:40:34,150][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:40:34,150][227910] Reward + Measures: [[1458.56282888    0.08500133    0.9471364     0.17478165    0.64237499]][0m
[37m[1m[2023-07-10 15:40:34,150][227910] Max Reward on eval: 1458.5628288756484[0m
[37m[1m[2023-07-10 15:40:34,151][227910] Min Reward on eval: 1458.5628288756484[0m
[37m[1m[2023-07-10 15:40:34,151][227910] Mean Reward across all agents: 1458.5628288756484[0m
[37m[1m[2023-07-10 15:40:34,151][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:40:39,775][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:40:39,775][227910] Reward + Measures: [[  856.6120505      0.14329998     0.81779999     0.44619998
      0.80610007]
 [ 1162.77779259     0.25409999     0.7245         0.50760001
      0.38569999]
 [-1975.56380486     0.81350005     0.80529994     0.69949996
      0.72310001]
 ...
 [  646.4462721      0.27250001     0.91350001     0.56660002
      0.75920004]
 [  794.86313065     0.35829997     0.70410001     0.14320001
      0.67180008]
 [  -65.99532538     0.54560006     0.7184         0.44490001
      0.55410004]][0m
[37m[1m[2023-07-10 15:40:39,775][227910] Max Reward on eval: 1410.8789974994957[0m
[37m[1m[2023-07-10 15:40:39,776][227910] Min Reward on eval: -2416.585926809814[0m
[37m[1m[2023-07-10 15:40:39,776][227910] Mean Reward across all agents: -84.46385269761905[0m
[37m[1m[2023-07-10 15:40:39,776][227910] Average Trajectory Length: 994.7213333333333[0m
[36m[2023-07-10 15:40:39,784][227910] mean_value=-319.014926676689, max_value=1910.8789974994957[0m
[37m[1m[2023-07-10 15:40:39,786][227910] New mean coefficients: [[ 6.8280897  -0.19052225  0.392608    0.73912853  1.8968861 ]][0m
[37m[1m[2023-07-10 15:40:39,787][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:40:49,435][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 15:40:49,436][227910] FPS: 398082.24[0m
[36m[2023-07-10 15:40:49,438][227910] itr=742, itrs=2000, Progress: 37.10%[0m
[36m[2023-07-10 15:41:01,086][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 15:41:01,086][227910] FPS: 330234.35[0m
[36m[2023-07-10 15:41:05,727][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:41:05,727][227910] Reward + Measures: [[1562.99432357    0.074224      0.94843537    0.20230168    0.609878  ]][0m
[37m[1m[2023-07-10 15:41:05,727][227910] Max Reward on eval: 1562.9943235693102[0m
[37m[1m[2023-07-10 15:41:05,727][227910] Min Reward on eval: 1562.9943235693102[0m
[37m[1m[2023-07-10 15:41:05,728][227910] Mean Reward across all agents: 1562.9943235693102[0m
[37m[1m[2023-07-10 15:41:05,728][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:41:11,065][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:41:11,065][227910] Reward + Measures: [[ -416.49455864     0.52030003     0.55770004     0.26279998
      0.48920003]
 [-1331.62711938     0.29340002     0.53109998     0.54100001
      0.50159997]
 [  326.55073854     0.3468         0.56619996     0.2218
      0.4368    ]
 ...
 [  816.31758826     0.5110001      0.81830007     0.15190001
      0.82120001]
 [-1232.16685375     0.33049998     0.80809993     0.29599997
      0.7608    ]
 [  614.63080977     0.45240003     0.4488         0.45930001
      0.34800002]][0m
[37m[1m[2023-07-10 15:41:11,065][227910] Max Reward on eval: 1314.3678840129637[0m
[37m[1m[2023-07-10 15:41:11,066][227910] Min Reward on eval: -1562.6828005749383[0m
[37m[1m[2023-07-10 15:41:11,066][227910] Mean Reward across all agents: 122.19318664258782[0m
[37m[1m[2023-07-10 15:41:11,066][227910] Average Trajectory Length: 995.9146666666667[0m
[36m[2023-07-10 15:41:11,072][227910] mean_value=-513.6996669226871, max_value=1523.110044652701[0m
[37m[1m[2023-07-10 15:41:11,074][227910] New mean coefficients: [[ 6.6638064  -0.15164064  0.5792997   0.7721372   0.6914897 ]][0m
[37m[1m[2023-07-10 15:41:11,075][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:41:20,739][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 15:41:20,739][227910] FPS: 397445.33[0m
[36m[2023-07-10 15:41:20,741][227910] itr=743, itrs=2000, Progress: 37.15%[0m
[36m[2023-07-10 15:41:32,255][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 15:41:32,255][227910] FPS: 334008.17[0m
[36m[2023-07-10 15:41:36,918][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:41:36,918][227910] Reward + Measures: [[1702.51951179    0.05644567    0.954732      0.23018301    0.57918733]][0m
[37m[1m[2023-07-10 15:41:36,918][227910] Max Reward on eval: 1702.5195117892704[0m
[37m[1m[2023-07-10 15:41:36,918][227910] Min Reward on eval: 1702.5195117892704[0m
[37m[1m[2023-07-10 15:41:36,919][227910] Mean Reward across all agents: 1702.5195117892704[0m
[37m[1m[2023-07-10 15:41:36,919][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:41:42,320][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:41:42,321][227910] Reward + Measures: [[  206.57450908     0.4659         0.29060003     0.42220002
      0.37180001]
 [  978.33851883     0.39680001     0.65939999     0.3655
      0.55699998]
 [ -539.18055185     0.67979997     0.53730005     0.71990001
      0.55490005]
 ...
 [  441.71160457     0.37450001     0.76439995     0.38960001
      0.76289999]
 [-1042.27865936     0.14229999     0.37380001     0.3098
      0.3276    ]
 [-1281.32638399     0.4298304      0.20968576     0.50629717
      0.32571194]][0m
[37m[1m[2023-07-10 15:41:42,321][227910] Max Reward on eval: 1558.4424777507782[0m
[37m[1m[2023-07-10 15:41:42,321][227910] Min Reward on eval: -1704.389680640091[0m
[37m[1m[2023-07-10 15:41:42,322][227910] Mean Reward across all agents: 76.19905018429625[0m
[37m[1m[2023-07-10 15:41:42,322][227910] Average Trajectory Length: 990.4663333333333[0m
[36m[2023-07-10 15:41:42,328][227910] mean_value=-420.52709475633884, max_value=1996.1956962317229[0m
[37m[1m[2023-07-10 15:41:42,331][227910] New mean coefficients: [[6.2801867  0.33997345 1.1765895  0.2723758  0.1580419 ]][0m
[37m[1m[2023-07-10 15:41:42,332][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:41:52,025][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 15:41:52,025][227910] FPS: 396216.39[0m
[36m[2023-07-10 15:41:52,028][227910] itr=744, itrs=2000, Progress: 37.20%[0m
[36m[2023-07-10 15:42:03,635][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 15:42:03,635][227910] FPS: 331306.15[0m
[36m[2023-07-10 15:42:08,452][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:42:08,453][227910] Reward + Measures: [[1832.39097646    0.05758234    0.95694023    0.21964099    0.54914331]][0m
[37m[1m[2023-07-10 15:42:08,453][227910] Max Reward on eval: 1832.3909764583477[0m
[37m[1m[2023-07-10 15:42:08,453][227910] Min Reward on eval: 1832.3909764583477[0m
[37m[1m[2023-07-10 15:42:08,453][227910] Mean Reward across all agents: 1832.3909764583477[0m
[37m[1m[2023-07-10 15:42:08,453][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:42:13,971][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:42:13,972][227910] Reward + Measures: [[ 578.66094614    0.0262        0.73390001    0.57429999    0.64539999]
 [ 122.78582464    0.0042        0.96689999    0.68540001    0.96709996]
 [  36.24072082    0.64650005    0.47629997    0.65350002    0.60570002]
 ...
 [ 924.40458813    0.28459999    0.61689997    0.28060001    0.58429998]
 [-941.20675656    0.0442        0.88420004    0.70420009    0.89490002]
 [ 205.62913405    0.0931        0.72259998    0.46529999    0.61989999]][0m
[37m[1m[2023-07-10 15:42:13,972][227910] Max Reward on eval: 1278.9354109296576[0m
[37m[1m[2023-07-10 15:42:13,972][227910] Min Reward on eval: -1521.5817024186138[0m
[37m[1m[2023-07-10 15:42:13,972][227910] Mean Reward across all agents: -20.73171824937386[0m
[37m[1m[2023-07-10 15:42:13,973][227910] Average Trajectory Length: 985.9549999999999[0m
[36m[2023-07-10 15:42:13,981][227910] mean_value=-234.29001884425006, max_value=1763.979208792746[0m
[37m[1m[2023-07-10 15:42:13,984][227910] New mean coefficients: [[ 5.825917   -0.05432856  1.1638285   0.22036348  0.48391375]][0m
[37m[1m[2023-07-10 15:42:13,985][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:42:23,835][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 15:42:23,835][227910] FPS: 389907.21[0m
[36m[2023-07-10 15:42:23,837][227910] itr=745, itrs=2000, Progress: 37.25%[0m
[36m[2023-07-10 15:42:35,507][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 15:42:35,507][227910] FPS: 329553.27[0m
[36m[2023-07-10 15:42:40,263][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:42:40,263][227910] Reward + Measures: [[1949.06535753    0.05191766    0.96017462    0.22264099    0.5200606 ]][0m
[37m[1m[2023-07-10 15:42:40,263][227910] Max Reward on eval: 1949.0653575269841[0m
[37m[1m[2023-07-10 15:42:40,264][227910] Min Reward on eval: 1949.0653575269841[0m
[37m[1m[2023-07-10 15:42:40,264][227910] Mean Reward across all agents: 1949.0653575269841[0m
[37m[1m[2023-07-10 15:42:40,264][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:42:45,921][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:42:45,922][227910] Reward + Measures: [[ 937.57826031    0.32910001    0.49020004    0.37729999    0.41799998]
 [  59.49313173    0.77930003    0.77900004    0.82520002    0.80299997]
 [-200.78690805    0.67939997    0.46619996    0.68360001    0.5456    ]
 ...
 [ 167.5418406     0.59689999    0.48429999    0.53060001    0.244     ]
 [-393.6632358     0.25489998    0.70329994    0.20190001    0.81220007]
 [ 248.87892505    0.31979999    0.77320004    0.1603        0.71520007]][0m
[37m[1m[2023-07-10 15:42:45,922][227910] Max Reward on eval: 1350.0844640929251[0m
[37m[1m[2023-07-10 15:42:45,922][227910] Min Reward on eval: -2019.3246847170406[0m
[37m[1m[2023-07-10 15:42:45,923][227910] Mean Reward across all agents: -20.51431009555617[0m
[37m[1m[2023-07-10 15:42:45,923][227910] Average Trajectory Length: 985.8853333333333[0m
[36m[2023-07-10 15:42:45,930][227910] mean_value=-498.13073015567966, max_value=1850.0844640929251[0m
[37m[1m[2023-07-10 15:42:45,933][227910] New mean coefficients: [[ 5.6788864  -0.2208863   1.0558841  -0.17192267  0.4431839 ]][0m
[37m[1m[2023-07-10 15:42:45,934][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:42:55,740][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 15:42:55,740][227910] FPS: 391649.12[0m
[36m[2023-07-10 15:42:55,743][227910] itr=746, itrs=2000, Progress: 37.30%[0m
[36m[2023-07-10 15:43:07,537][227910] train() took 11.77 seconds to complete[0m
[36m[2023-07-10 15:43:07,537][227910] FPS: 326135.31[0m
[36m[2023-07-10 15:43:12,427][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:43:12,427][227910] Reward + Measures: [[1814.54322709    0.08888701    0.94930738    0.22472866    0.56526297]][0m
[37m[1m[2023-07-10 15:43:12,428][227910] Max Reward on eval: 1814.5432270878305[0m
[37m[1m[2023-07-10 15:43:12,428][227910] Min Reward on eval: 1814.5432270878305[0m
[37m[1m[2023-07-10 15:43:12,428][227910] Mean Reward across all agents: 1814.5432270878305[0m
[37m[1m[2023-07-10 15:43:12,429][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:43:17,892][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:43:17,892][227910] Reward + Measures: [[ 663.85721946    0.58719999    0.73260003    0.75190002    0.4061    ]
 [ 365.32630963    0.27290002    0.75870001    0.15720001    0.72970003]
 [-218.6697341     0.88150007    0.86940002    0.92919999    0.17479999]
 ...
 [ 139.05143878    0.8023001     0.23029999    0.75239998    0.1603    ]
 [ 415.5087422     0.24010001    0.54640001    0.1895        0.51190007]
 [ 752.52918972    0.92880005    0.97510004    0.92799997    0.96950006]][0m
[37m[1m[2023-07-10 15:43:17,893][227910] Max Reward on eval: 1579.5152181722224[0m
[37m[1m[2023-07-10 15:43:17,893][227910] Min Reward on eval: -1319.9089042943901[0m
[37m[1m[2023-07-10 15:43:17,893][227910] Mean Reward across all agents: 345.61732963642066[0m
[37m[1m[2023-07-10 15:43:17,893][227910] Average Trajectory Length: 985.8543333333333[0m
[36m[2023-07-10 15:43:17,902][227910] mean_value=-351.28158004912075, max_value=2079.5152181722224[0m
[37m[1m[2023-07-10 15:43:17,905][227910] New mean coefficients: [[ 6.0613747   0.3313747   1.621629   -0.20608428 -0.3657776 ]][0m
[37m[1m[2023-07-10 15:43:17,906][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:43:27,820][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 15:43:27,820][227910] FPS: 387407.20[0m
[36m[2023-07-10 15:43:27,822][227910] itr=747, itrs=2000, Progress: 37.35%[0m
[36m[2023-07-10 15:43:39,438][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 15:43:39,438][227910] FPS: 331116.80[0m
[36m[2023-07-10 15:43:44,112][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:43:44,112][227910] Reward + Measures: [[2086.62945094    0.04305767    0.9534663     0.23088965    0.49134368]][0m
[37m[1m[2023-07-10 15:43:44,112][227910] Max Reward on eval: 2086.6294509359145[0m
[37m[1m[2023-07-10 15:43:44,113][227910] Min Reward on eval: 2086.6294509359145[0m
[37m[1m[2023-07-10 15:43:44,113][227910] Mean Reward across all agents: 2086.6294509359145[0m
[37m[1m[2023-07-10 15:43:44,113][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:43:49,467][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:43:49,467][227910] Reward + Measures: [[  62.82309245    0.81300002    0.89480001    0.93699998    0.57089996]
 [ 198.04155215    0.0679        0.4808        0.29280001    0.43309999]
 [ 704.84600145    0.0277        0.94250005    0.4578        0.93470001]
 ...
 [-313.03227914    0.21669999    0.58719999    0.3583        0.44800001]
 [-381.26880492    0.80620003    0.84980005    0.84310001    0.80229998]
 [ 754.93276301    0.66619998    0.83770001    0.65469998    0.79730004]][0m
[37m[1m[2023-07-10 15:43:49,467][227910] Max Reward on eval: 1798.589665041119[0m
[37m[1m[2023-07-10 15:43:49,468][227910] Min Reward on eval: -2064.212983169337[0m
[37m[1m[2023-07-10 15:43:49,468][227910] Mean Reward across all agents: 157.30478978352664[0m
[37m[1m[2023-07-10 15:43:49,468][227910] Average Trajectory Length: 968.019[0m
[36m[2023-07-10 15:43:49,476][227910] mean_value=-199.31448578198066, max_value=2298.5896650411187[0m
[37m[1m[2023-07-10 15:43:49,478][227910] New mean coefficients: [[ 5.580217   -0.28262222  1.2598362  -0.06270981 -0.29177988]][0m
[37m[1m[2023-07-10 15:43:49,479][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:43:59,261][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 15:43:59,261][227910] FPS: 392658.26[0m
[36m[2023-07-10 15:43:59,263][227910] itr=748, itrs=2000, Progress: 37.40%[0m
[36m[2023-07-10 15:44:10,991][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 15:44:10,991][227910] FPS: 327918.92[0m
[36m[2023-07-10 15:44:15,790][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:44:15,790][227910] Reward + Measures: [[2121.06966506    0.054761      0.9274776     0.19453332    0.47879699]][0m
[37m[1m[2023-07-10 15:44:15,791][227910] Max Reward on eval: 2121.0696650648806[0m
[37m[1m[2023-07-10 15:44:15,791][227910] Min Reward on eval: 2121.0696650648806[0m
[37m[1m[2023-07-10 15:44:15,791][227910] Mean Reward across all agents: 2121.0696650648806[0m
[37m[1m[2023-07-10 15:44:15,791][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:44:21,367][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:44:21,367][227910] Reward + Measures: [[1262.92385446    0.23660003    0.8136        0.28779998    0.50270003]
 [ 481.09614979    0.1177        0.68830001    0.50520003    0.55239999]
 [-208.49414529    0.33430001    0.31580001    0.31690001    0.25920001]
 ...
 [ 531.08642617    0.11849999    0.70699996    0.34279999    0.74739999]
 [ 759.16937257    0.38100001    0.36680001    0.50500005    0.30940005]
 [ 690.31211607    0.45550004    0.44440004    0.5176        0.38050005]][0m
[37m[1m[2023-07-10 15:44:21,368][227910] Max Reward on eval: 1292.9839822590352[0m
[37m[1m[2023-07-10 15:44:21,368][227910] Min Reward on eval: -1311.5086923913448[0m
[37m[1m[2023-07-10 15:44:21,368][227910] Mean Reward across all agents: 358.50034496711567[0m
[37m[1m[2023-07-10 15:44:21,368][227910] Average Trajectory Length: 995.6786666666667[0m
[36m[2023-07-10 15:44:21,376][227910] mean_value=-267.2253273419118, max_value=1792.9839822590352[0m
[37m[1m[2023-07-10 15:44:21,379][227910] New mean coefficients: [[ 5.3018274  -0.38501942  1.1789306   0.4871223   0.08925283]][0m
[37m[1m[2023-07-10 15:44:21,380][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:44:31,288][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 15:44:31,289][227910] FPS: 387600.25[0m
[36m[2023-07-10 15:44:31,291][227910] itr=749, itrs=2000, Progress: 37.45%[0m
[36m[2023-07-10 15:44:43,003][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 15:44:43,003][227910] FPS: 328468.77[0m
[36m[2023-07-10 15:44:47,820][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:44:47,820][227910] Reward + Measures: [[1827.7165469     0.024342      0.89932996    0.28708735    0.50148535]][0m
[37m[1m[2023-07-10 15:44:47,821][227910] Max Reward on eval: 1827.71654690072[0m
[37m[1m[2023-07-10 15:44:47,821][227910] Min Reward on eval: 1827.71654690072[0m
[37m[1m[2023-07-10 15:44:47,821][227910] Mean Reward across all agents: 1827.71654690072[0m
[37m[1m[2023-07-10 15:44:47,822][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:44:53,272][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:44:53,273][227910] Reward + Measures: [[1066.67981756    0.0239        0.88659996    0.33759999    0.65709996]
 [1238.83763332    0.1855        0.8125        0.2746        0.54750001]
 [ 818.83371885    0.17810002    0.86339998    0.2624        0.71280003]
 ...
 [-617.27580099    0.54769999    0.36250001    0.51809996    0.76500005]
 [-523.10558541    0.1391        0.50730002    0.24990001    0.4822    ]
 [  90.72230545    0.8398        0.82070011    0.83310002    0.14039999]][0m
[37m[1m[2023-07-10 15:44:53,273][227910] Max Reward on eval: 1641.9664919684176[0m
[37m[1m[2023-07-10 15:44:53,273][227910] Min Reward on eval: -1103.0398771367968[0m
[37m[1m[2023-07-10 15:44:53,273][227910] Mean Reward across all agents: 518.4232998718009[0m
[37m[1m[2023-07-10 15:44:53,274][227910] Average Trajectory Length: 998.6053333333333[0m
[36m[2023-07-10 15:44:53,282][227910] mean_value=133.97496910653348, max_value=2141.9664919684174[0m
[37m[1m[2023-07-10 15:44:53,284][227910] New mean coefficients: [[ 4.3025327  -0.09460682  1.3021946   0.25836617  0.35709757]][0m
[37m[1m[2023-07-10 15:44:53,285][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:45:03,037][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 15:45:03,043][227910] FPS: 393835.77[0m
[36m[2023-07-10 15:45:03,046][227910] itr=750, itrs=2000, Progress: 37.50%[0m
[37m[1m[2023-07-10 15:45:06,528][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000730[0m
[36m[2023-07-10 15:45:18,423][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 15:45:18,423][227910] FPS: 330094.66[0m
[36m[2023-07-10 15:45:23,294][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:45:23,294][227910] Reward + Measures: [[926.73061061   0.23648067   0.70186603   0.32676634   0.65456599]][0m
[37m[1m[2023-07-10 15:45:23,295][227910] Max Reward on eval: 926.7306106107536[0m
[37m[1m[2023-07-10 15:45:23,295][227910] Min Reward on eval: 926.7306106107536[0m
[37m[1m[2023-07-10 15:45:23,295][227910] Mean Reward across all agents: 926.7306106107536[0m
[37m[1m[2023-07-10 15:45:23,295][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:45:28,837][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:45:28,838][227910] Reward + Measures: [[ -85.68753748    0.28870001    0.83080006    0.19699998    0.81849998]
 [-249.28814195    0.34806591    0.39418167    0.27824202    0.31311202]
 [ 166.56044898    0.84680003    0.95209998    0.0127        0.94960004]
 ...
 [-300.94385462    0.67320913    0.17010497    0.59615946    0.39282894]
 [-366.17922004    0.53680003    0.83480006    0.53170007    0.66780007]
 [ 729.51499658    0.59569997    0.90580004    0.65420002    0.87880003]][0m
[37m[1m[2023-07-10 15:45:28,838][227910] Max Reward on eval: 1196.4279952833429[0m
[37m[1m[2023-07-10 15:45:28,839][227910] Min Reward on eval: -1185.366660529026[0m
[37m[1m[2023-07-10 15:45:28,839][227910] Mean Reward across all agents: 83.74107274819863[0m
[37m[1m[2023-07-10 15:45:28,839][227910] Average Trajectory Length: 972.6036666666666[0m
[36m[2023-07-10 15:45:28,845][227910] mean_value=-512.3195776056264, max_value=1339.3681908844271[0m
[37m[1m[2023-07-10 15:45:28,847][227910] New mean coefficients: [[3.950171   0.27748957 0.99776894 0.51831603 0.4355602 ]][0m
[37m[1m[2023-07-10 15:45:28,848][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:45:38,614][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 15:45:38,615][227910] FPS: 393264.70[0m
[36m[2023-07-10 15:45:38,617][227910] itr=751, itrs=2000, Progress: 37.55%[0m
[36m[2023-07-10 15:45:50,299][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 15:45:50,299][227910] FPS: 329207.41[0m
[36m[2023-07-10 15:45:55,138][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:45:55,138][227910] Reward + Measures: [[1238.89144121    0.240219      0.65727466    0.29959133    0.46491462]][0m
[37m[1m[2023-07-10 15:45:55,139][227910] Max Reward on eval: 1238.8914412087126[0m
[37m[1m[2023-07-10 15:45:55,139][227910] Min Reward on eval: 1238.8914412087126[0m
[37m[1m[2023-07-10 15:45:55,139][227910] Mean Reward across all agents: 1238.8914412087126[0m
[37m[1m[2023-07-10 15:45:55,139][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:46:00,579][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:46:00,579][227910] Reward + Measures: [[-208.06673384    0.42879996    0.83080006    0.60330003    0.77890003]
 [ 441.73669647    0.42309999    0.70150006    0.2023        0.64040005]
 [ 318.71863026    0.18069999    0.69679993    0.26370001    0.33880001]
 ...
 [ 782.45930208    0.22160001    0.70310003    0.29089999    0.41190001]
 [ 248.04850636    0.24989998    0.79280001    0.44829997    0.72819996]
 [-120.83820669    0.59720004    0.35749999    0.57550001    0.29449999]][0m
[37m[1m[2023-07-10 15:46:00,579][227910] Max Reward on eval: 1267.062445551157[0m
[37m[1m[2023-07-10 15:46:00,580][227910] Min Reward on eval: -1588.1563938600593[0m
[37m[1m[2023-07-10 15:46:00,580][227910] Mean Reward across all agents: -56.069298380787195[0m
[37m[1m[2023-07-10 15:46:00,580][227910] Average Trajectory Length: 993.3043333333333[0m
[36m[2023-07-10 15:46:00,585][227910] mean_value=-561.4025507365272, max_value=1392.0062476170528[0m
[37m[1m[2023-07-10 15:46:00,588][227910] New mean coefficients: [[ 4.084561   -0.18178228  0.592814    0.47278228  0.92273176]][0m
[37m[1m[2023-07-10 15:46:00,589][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:46:10,335][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 15:46:10,335][227910] FPS: 394069.20[0m
[36m[2023-07-10 15:46:10,338][227910] itr=752, itrs=2000, Progress: 37.60%[0m
[36m[2023-07-10 15:46:21,866][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 15:46:21,866][227910] FPS: 333680.82[0m
[36m[2023-07-10 15:46:26,636][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:46:26,636][227910] Reward + Measures: [[1321.92882331    0.24003901    0.65686935    0.30416533    0.44015536]][0m
[37m[1m[2023-07-10 15:46:26,636][227910] Max Reward on eval: 1321.9288233093303[0m
[37m[1m[2023-07-10 15:46:26,636][227910] Min Reward on eval: 1321.9288233093303[0m
[37m[1m[2023-07-10 15:46:26,636][227910] Mean Reward across all agents: 1321.9288233093303[0m
[37m[1m[2023-07-10 15:46:26,637][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:46:32,285][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:46:32,286][227910] Reward + Measures: [[-648.88608038    0.39801371    0.50608504    0.14302693    0.50125784]
 [ 459.6292321     0.29200003    0.79259998    0.36740002    0.77060002]
 [-204.96473309    0.52420002    0.71359998    0.55330002    0.62860006]
 ...
 [ 160.26219326    0.18660001    0.68239999    0.37940001    0.66520005]
 [ 745.2952431     0.22750001    0.40150005    0.29429999    0.48789999]
 [ 599.53012234    0.1331        0.61549997    0.37750003    0.55050004]][0m
[37m[1m[2023-07-10 15:46:32,286][227910] Max Reward on eval: 1253.2328965353313[0m
[37m[1m[2023-07-10 15:46:32,286][227910] Min Reward on eval: -893.7663439814409[0m
[37m[1m[2023-07-10 15:46:32,287][227910] Mean Reward across all agents: 408.4705878677881[0m
[37m[1m[2023-07-10 15:46:32,287][227910] Average Trajectory Length: 990.0543333333333[0m
[36m[2023-07-10 15:46:32,293][227910] mean_value=-276.6519809011077, max_value=1680.0736619949807[0m
[37m[1m[2023-07-10 15:46:32,295][227910] New mean coefficients: [[3.6752005  0.1332283  1.0995917  0.359245   0.48671308]][0m
[37m[1m[2023-07-10 15:46:32,296][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:46:41,988][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 15:46:41,988][227910] FPS: 396290.83[0m
[36m[2023-07-10 15:46:41,990][227910] itr=753, itrs=2000, Progress: 37.65%[0m
[36m[2023-07-10 15:46:53,448][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 15:46:53,448][227910] FPS: 335658.95[0m
[36m[2023-07-10 15:46:58,112][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:46:58,112][227910] Reward + Measures: [[1312.00892334    0.31290367    0.57378435    0.33748034    0.33002499]][0m
[37m[1m[2023-07-10 15:46:58,112][227910] Max Reward on eval: 1312.0089233368788[0m
[37m[1m[2023-07-10 15:46:58,112][227910] Min Reward on eval: 1312.0089233368788[0m
[37m[1m[2023-07-10 15:46:58,113][227910] Mean Reward across all agents: 1312.0089233368788[0m
[37m[1m[2023-07-10 15:46:58,113][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:47:03,507][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:47:03,507][227910] Reward + Measures: [[566.56858849   0.37129998   0.57270002   0.49200001   0.61059999]
 [-77.49653226   0.6706       0.61330003   0.62649995   0.58740002]
 [512.5876744    0.42790005   0.39330003   0.41920003   0.46199998]
 ...
 [358.28440098   0.84280008   0.82250005   0.84450001   0.84870005]
 [439.28182781   0.58689994   0.47119999   0.66750002   0.64050001]
 [340.3228373    0.64790004   0.29529998   0.50749999   0.55129999]][0m
[37m[1m[2023-07-10 15:47:03,507][227910] Max Reward on eval: 1478.3798430514057[0m
[37m[1m[2023-07-10 15:47:03,508][227910] Min Reward on eval: -1806.0057644023443[0m
[37m[1m[2023-07-10 15:47:03,508][227910] Mean Reward across all agents: 311.790818030546[0m
[37m[1m[2023-07-10 15:47:03,508][227910] Average Trajectory Length: 995.077[0m
[36m[2023-07-10 15:47:03,514][227910] mean_value=-262.2866009104762, max_value=1600.53696638649[0m
[37m[1m[2023-07-10 15:47:03,517][227910] New mean coefficients: [[ 4.031586   -0.36610907  0.45380652  0.08429924  0.61591375]][0m
[37m[1m[2023-07-10 15:47:03,518][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:47:13,255][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 15:47:13,255][227910] FPS: 394431.15[0m
[36m[2023-07-10 15:47:13,257][227910] itr=754, itrs=2000, Progress: 37.70%[0m
[36m[2023-07-10 15:47:24,737][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 15:47:24,737][227910] FPS: 335043.76[0m
[36m[2023-07-10 15:47:29,537][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:47:29,537][227910] Reward + Measures: [[1401.64889932    0.29880077    0.58526129    0.3450608     0.32254374]][0m
[37m[1m[2023-07-10 15:47:29,537][227910] Max Reward on eval: 1401.6488993200235[0m
[37m[1m[2023-07-10 15:47:29,537][227910] Min Reward on eval: 1401.6488993200235[0m
[37m[1m[2023-07-10 15:47:29,538][227910] Mean Reward across all agents: 1401.6488993200235[0m
[37m[1m[2023-07-10 15:47:29,538][227910] Average Trajectory Length: 999.9413333333333[0m
[36m[2023-07-10 15:47:35,138][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:47:35,139][227910] Reward + Measures: [[ 275.87683494    0.63059998    0.1295        0.7705        0.49070001]
 [ 543.50262335    0.24730001    0.5801        0.31289998    0.45520002]
 [-183.82799337    0.31510004    0.3748        0.44860002    0.33170003]
 ...
 [  12.91381701    0.35859999    0.37779999    0.48359999    0.37349999]
 [ -81.20293761    0.36840001    0.44520003    0.31830001    0.43050003]
 [-132.60674515    0.26120001    0.3272        0.37440005    0.32870004]][0m
[37m[1m[2023-07-10 15:47:35,139][227910] Max Reward on eval: 1450.2064654984977[0m
[37m[1m[2023-07-10 15:47:35,139][227910] Min Reward on eval: -1098.7777218904812[0m
[37m[1m[2023-07-10 15:47:35,140][227910] Mean Reward across all agents: 300.24434823050126[0m
[37m[1m[2023-07-10 15:47:35,140][227910] Average Trajectory Length: 993.982[0m
[36m[2023-07-10 15:47:35,145][227910] mean_value=-574.1474607055454, max_value=1255.8089864176052[0m
[37m[1m[2023-07-10 15:47:35,148][227910] New mean coefficients: [[ 4.0388036   0.21596295  1.0363984  -0.2223106   0.46260113]][0m
[37m[1m[2023-07-10 15:47:35,149][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:47:45,046][227910] train() took 9.90 seconds to complete[0m
[36m[2023-07-10 15:47:45,047][227910] FPS: 388049.77[0m
[36m[2023-07-10 15:47:45,049][227910] itr=755, itrs=2000, Progress: 37.75%[0m
[36m[2023-07-10 15:47:56,544][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 15:47:56,544][227910] FPS: 334655.50[0m
[36m[2023-07-10 15:48:01,231][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:48:01,232][227910] Reward + Measures: [[1489.4945134     0.28664869    0.60030264    0.34053963    0.32230732]][0m
[37m[1m[2023-07-10 15:48:01,232][227910] Max Reward on eval: 1489.4945134037234[0m
[37m[1m[2023-07-10 15:48:01,232][227910] Min Reward on eval: 1489.4945134037234[0m
[37m[1m[2023-07-10 15:48:01,232][227910] Mean Reward across all agents: 1489.4945134037234[0m
[37m[1m[2023-07-10 15:48:01,233][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:48:06,734][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:48:06,734][227910] Reward + Measures: [[ 275.05406818    0.2392        0.4804        0.25870001    0.39690003]
 [ 787.98792051    0.4341        0.55010003    0.20570002    0.50060004]
 [ 694.6080323     0.2352        0.55150002    0.31459999    0.41019997]
 ...
 [ 590.99753159    0.43400002    0.60620004    0.17189999    0.63009995]
 [-578.60273299    0.74949998    0.23339999    0.68739998    0.73700005]
 [ 134.39285064    0.40130001    0.61210001    0.1123        0.5546    ]][0m
[37m[1m[2023-07-10 15:48:06,735][227910] Max Reward on eval: 1482.3249548277818[0m
[37m[1m[2023-07-10 15:48:06,735][227910] Min Reward on eval: -1267.3681921388138[0m
[37m[1m[2023-07-10 15:48:06,735][227910] Mean Reward across all agents: 242.48631848940272[0m
[37m[1m[2023-07-10 15:48:06,736][227910] Average Trajectory Length: 998.0136666666666[0m
[36m[2023-07-10 15:48:06,741][227910] mean_value=-448.92276075685766, max_value=1520.6938899430725[0m
[37m[1m[2023-07-10 15:48:06,744][227910] New mean coefficients: [[ 3.966308   -0.23127985  0.7456814   0.04891893  1.152561  ]][0m
[37m[1m[2023-07-10 15:48:06,745][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:48:16,565][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 15:48:16,565][227910] FPS: 391107.17[0m
[36m[2023-07-10 15:48:16,567][227910] itr=756, itrs=2000, Progress: 37.80%[0m
[36m[2023-07-10 15:48:28,179][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 15:48:28,180][227910] FPS: 331263.49[0m
[36m[2023-07-10 15:48:33,032][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:48:33,032][227910] Reward + Measures: [[1556.0911688     0.27728632    0.60974133    0.34546602    0.319556  ]][0m
[37m[1m[2023-07-10 15:48:33,033][227910] Max Reward on eval: 1556.091168799471[0m
[37m[1m[2023-07-10 15:48:33,033][227910] Min Reward on eval: 1556.091168799471[0m
[37m[1m[2023-07-10 15:48:33,033][227910] Mean Reward across all agents: 1556.091168799471[0m
[37m[1m[2023-07-10 15:48:33,033][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:48:38,455][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:48:38,455][227910] Reward + Measures: [[ 853.05648199    0.21900001    0.60350001    0.22479999    0.41410002]
 [ 651.52797476    0.16180001    0.39960003    0.2753        0.32839999]
 [-274.01927864    0.72990006    0.06590001    0.75159997    0.4578    ]
 ...
 [-297.11763685    0.34180003    0.66250002    0.66420001    0.71309996]
 [ 389.41026711    0.22670002    0.45670006    0.32020003    0.32200003]
 [ 625.01814462    0.44409999    0.30599999    0.43050003    0.22490001]][0m
[37m[1m[2023-07-10 15:48:38,455][227910] Max Reward on eval: 1447.6523307251744[0m
[37m[1m[2023-07-10 15:48:38,456][227910] Min Reward on eval: -1092.1376612405875[0m
[37m[1m[2023-07-10 15:48:38,456][227910] Mean Reward across all agents: 362.69586134245657[0m
[37m[1m[2023-07-10 15:48:38,456][227910] Average Trajectory Length: 997.14[0m
[36m[2023-07-10 15:48:38,460][227910] mean_value=-831.5512479988464, max_value=1018.7979270296404[0m
[37m[1m[2023-07-10 15:48:38,463][227910] New mean coefficients: [[ 3.6881385  -0.2232463   0.10407454  0.32318723  0.7101655 ]][0m
[37m[1m[2023-07-10 15:48:38,464][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:48:48,297][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 15:48:48,298][227910] FPS: 390555.16[0m
[36m[2023-07-10 15:48:48,300][227910] itr=757, itrs=2000, Progress: 37.85%[0m
[36m[2023-07-10 15:48:59,928][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 15:48:59,929][227910] FPS: 330734.14[0m
[36m[2023-07-10 15:49:04,677][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:49:04,677][227910] Reward + Measures: [[1631.30590352    0.27093133    0.606534      0.3514117     0.31283733]][0m
[37m[1m[2023-07-10 15:49:04,678][227910] Max Reward on eval: 1631.3059035224937[0m
[37m[1m[2023-07-10 15:49:04,678][227910] Min Reward on eval: 1631.3059035224937[0m
[37m[1m[2023-07-10 15:49:04,678][227910] Mean Reward across all agents: 1631.3059035224937[0m
[37m[1m[2023-07-10 15:49:04,678][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:49:10,276][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:49:10,277][227910] Reward + Measures: [[ 820.31425174    0.1209        0.74040002    0.2079        0.5456    ]
 [ 547.06986098    0.4436        0.3809        0.43430001    0.30040002]
 [-790.88792642    0.34380931    0.3539426     0.35983655    0.37651175]
 ...
 [1195.58388996    0.32880002    0.52530003    0.50159997    0.31040001]
 [ 766.11671467    0.54910004    0.24060002    0.41260001    0.26720002]
 [ -92.15242444    0.23729999    0.53780001    0.23559999    0.4269    ]][0m
[37m[1m[2023-07-10 15:49:10,277][227910] Max Reward on eval: 1745.6951234193052[0m
[37m[1m[2023-07-10 15:49:10,277][227910] Min Reward on eval: -1394.3295513021294[0m
[37m[1m[2023-07-10 15:49:10,278][227910] Mean Reward across all agents: 452.37980799984985[0m
[37m[1m[2023-07-10 15:49:10,278][227910] Average Trajectory Length: 994.139[0m
[36m[2023-07-10 15:49:10,282][227910] mean_value=-687.8545750728194, max_value=1685.4444719806313[0m
[37m[1m[2023-07-10 15:49:10,284][227910] New mean coefficients: [[4.2753897  0.0149851  1.024398   0.00875556 0.5043762 ]][0m
[37m[1m[2023-07-10 15:49:10,285][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:49:19,927][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 15:49:19,927][227910] FPS: 398326.05[0m
[36m[2023-07-10 15:49:19,930][227910] itr=758, itrs=2000, Progress: 37.90%[0m
[36m[2023-07-10 15:49:31,429][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 15:49:31,429][227910] FPS: 334423.01[0m
[36m[2023-07-10 15:49:36,151][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:49:36,151][227910] Reward + Measures: [[1076.48429232    0.25046232    0.71773601    0.37143233    0.42571634]][0m
[37m[1m[2023-07-10 15:49:36,152][227910] Max Reward on eval: 1076.4842923248505[0m
[37m[1m[2023-07-10 15:49:36,152][227910] Min Reward on eval: 1076.4842923248505[0m
[37m[1m[2023-07-10 15:49:36,152][227910] Mean Reward across all agents: 1076.4842923248505[0m
[37m[1m[2023-07-10 15:49:36,152][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:49:41,509][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:49:41,510][227910] Reward + Measures: [[520.91516747   0.51930004   0.65979999   0.54210001   0.45860001]
 [252.56205076   0.30950004   0.57159996   0.20600002   0.36570001]
 [-63.02799657   0.0048       0.99049997   0.64650005   0.98889989]
 ...
 [946.67087967   0.1772       0.60249996   0.38429999   0.37670001]
 [552.77747281   0.50450003   0.3635       0.60289997   0.61120003]
 [528.02249797   0.13610001   0.5571       0.3299       0.33360001]][0m
[37m[1m[2023-07-10 15:49:41,510][227910] Max Reward on eval: 1274.4131775861374[0m
[37m[1m[2023-07-10 15:49:41,510][227910] Min Reward on eval: -1258.0164257387164[0m
[37m[1m[2023-07-10 15:49:41,511][227910] Mean Reward across all agents: 396.4363699364948[0m
[37m[1m[2023-07-10 15:49:41,511][227910] Average Trajectory Length: 995.7226666666667[0m
[36m[2023-07-10 15:49:41,516][227910] mean_value=-85.62339735404268, max_value=1251.052440917673[0m
[37m[1m[2023-07-10 15:49:41,519][227910] New mean coefficients: [[ 3.960325    0.10841697  1.3233901  -0.2645076  -0.02568674]][0m
[37m[1m[2023-07-10 15:49:41,520][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:49:51,180][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 15:49:51,181][227910] FPS: 397560.30[0m
[36m[2023-07-10 15:49:51,183][227910] itr=759, itrs=2000, Progress: 37.95%[0m
[36m[2023-07-10 15:50:02,625][227910] train() took 11.42 seconds to complete[0m
[36m[2023-07-10 15:50:02,626][227910] FPS: 336196.04[0m
[36m[2023-07-10 15:50:07,417][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:50:07,418][227910] Reward + Measures: [[1218.95745903    0.21863399    0.71020293    0.36745563    0.40508333]][0m
[37m[1m[2023-07-10 15:50:07,418][227910] Max Reward on eval: 1218.9574590290365[0m
[37m[1m[2023-07-10 15:50:07,418][227910] Min Reward on eval: 1218.9574590290365[0m
[37m[1m[2023-07-10 15:50:07,418][227910] Mean Reward across all agents: 1218.9574590290365[0m
[37m[1m[2023-07-10 15:50:07,419][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:50:12,838][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:50:12,838][227910] Reward + Measures: [[ 903.21426374    0.23769999    0.70419997    0.4073        0.48330003]
 [ 867.51033389    0.3028        0.68660003    0.40690002    0.49630004]
 [ 782.95074235    0.20289998    0.65580004    0.36250001    0.43780002]
 ...
 [1044.76061755    0.3037        0.69709998    0.32889998    0.35249999]
 [1168.67941853    0.16170001    0.76260006    0.35240003    0.42220002]
 [ 867.16915438    0.13830002    0.7701        0.44720003    0.50439996]][0m
[37m[1m[2023-07-10 15:50:12,838][227910] Max Reward on eval: 1183.0386281577871[0m
[37m[1m[2023-07-10 15:50:12,839][227910] Min Reward on eval: -551.7465731948381[0m
[37m[1m[2023-07-10 15:50:12,839][227910] Mean Reward across all agents: 684.5654433771508[0m
[37m[1m[2023-07-10 15:50:12,839][227910] Average Trajectory Length: 999.4906666666666[0m
[36m[2023-07-10 15:50:12,846][227910] mean_value=130.2989283431543, max_value=1433.8449769952663[0m
[37m[1m[2023-07-10 15:50:12,849][227910] New mean coefficients: [[3.7983341 0.5145968 1.5530874 0.1188454 0.3932537]][0m
[37m[1m[2023-07-10 15:50:12,850][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:50:22,456][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 15:50:22,456][227910] FPS: 399819.53[0m
[36m[2023-07-10 15:50:22,458][227910] itr=760, itrs=2000, Progress: 38.00%[0m
[37m[1m[2023-07-10 15:50:25,758][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000740[0m
[36m[2023-07-10 15:50:37,916][227910] train() took 11.91 seconds to complete[0m
[36m[2023-07-10 15:50:37,917][227910] FPS: 322499.77[0m
[36m[2023-07-10 15:50:42,757][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:50:42,757][227910] Reward + Measures: [[1086.35346753    0.25381568    0.54348731    0.38628769    0.28705367]][0m
[37m[1m[2023-07-10 15:50:42,758][227910] Max Reward on eval: 1086.3534675295934[0m
[37m[1m[2023-07-10 15:50:42,758][227910] Min Reward on eval: 1086.3534675295934[0m
[37m[1m[2023-07-10 15:50:42,758][227910] Mean Reward across all agents: 1086.3534675295934[0m
[37m[1m[2023-07-10 15:50:42,759][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:50:48,496][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:50:48,497][227910] Reward + Measures: [[ 152.50631753    0.2378        0.50270003    0.27830002    0.43710002]
 [ 431.49249215    0.37129998    0.40400001    0.44870001    0.29029998]
 [ 237.70495131    0.1015        0.67610002    0.40009999    0.62510002]
 ...
 [ 624.90689393    0.24360001    0.53170002    0.33870003    0.34470001]
 [ 790.79376509    0.1098        0.71020001    0.31280002    0.41770002]
 [-526.02245835    0.28909999    0.7263        0.0918        0.42820001]][0m
[37m[1m[2023-07-10 15:50:48,497][227910] Max Reward on eval: 1075.7308575366042[0m
[37m[1m[2023-07-10 15:50:48,497][227910] Min Reward on eval: -1370.3499184074813[0m
[37m[1m[2023-07-10 15:50:48,498][227910] Mean Reward across all agents: 221.48201576826668[0m
[37m[1m[2023-07-10 15:50:48,498][227910] Average Trajectory Length: 999.4266666666666[0m
[36m[2023-07-10 15:50:48,501][227910] mean_value=-625.4209349182486, max_value=1052.6883456162673[0m
[37m[1m[2023-07-10 15:50:48,504][227910] New mean coefficients: [[4.578548   0.6171936  1.2671727  0.5163943  0.13578302]][0m
[37m[1m[2023-07-10 15:50:48,505][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:50:58,313][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 15:50:58,313][227910] FPS: 391585.88[0m
[36m[2023-07-10 15:50:58,315][227910] itr=761, itrs=2000, Progress: 38.05%[0m
[36m[2023-07-10 15:51:09,857][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 15:51:09,857][227910] FPS: 333226.58[0m
[36m[2023-07-10 15:51:14,660][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:51:14,660][227910] Reward + Measures: [[1190.44566429    0.25418568    0.54953295    0.38495865    0.26796365]][0m
[37m[1m[2023-07-10 15:51:14,660][227910] Max Reward on eval: 1190.4456642891594[0m
[37m[1m[2023-07-10 15:51:14,660][227910] Min Reward on eval: 1190.4456642891594[0m
[37m[1m[2023-07-10 15:51:14,661][227910] Mean Reward across all agents: 1190.4456642891594[0m
[37m[1m[2023-07-10 15:51:14,661][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:51:20,075][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:51:20,076][227910] Reward + Measures: [[ 742.51370219    0.32080001    0.50629997    0.42780003    0.20120001]
 [ 588.91003477    0.1135        0.71550006    0.6401        0.53930002]
 [1085.21969472    0.2782        0.47079998    0.2714        0.2481    ]
 ...
 [ 947.58213295    0.2947        0.53820002    0.37290001    0.25560001]
 [ -89.50745581    0.088         0.73810005    0.69910002    0.67329997]
 [ 367.95419959    0.57740003    0.53380001    0.51230001    0.13139999]][0m
[37m[1m[2023-07-10 15:51:20,076][227910] Max Reward on eval: 1201.4036280110013[0m
[37m[1m[2023-07-10 15:51:20,076][227910] Min Reward on eval: -1374.5378429621924[0m
[37m[1m[2023-07-10 15:51:20,076][227910] Mean Reward across all agents: 262.46982354026846[0m
[37m[1m[2023-07-10 15:51:20,076][227910] Average Trajectory Length: 994.7479999999999[0m
[36m[2023-07-10 15:51:20,080][227910] mean_value=-1187.6991019767313, max_value=1238.9158189084264[0m
[37m[1m[2023-07-10 15:51:20,083][227910] New mean coefficients: [[4.8031826  0.005943   0.29332644 1.0050569  0.32822898]][0m
[37m[1m[2023-07-10 15:51:20,084][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:51:29,786][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 15:51:29,787][227910] FPS: 395847.54[0m
[36m[2023-07-10 15:51:29,789][227910] itr=762, itrs=2000, Progress: 38.10%[0m
[36m[2023-07-10 15:51:41,319][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 15:51:41,319][227910] FPS: 333542.87[0m
[36m[2023-07-10 15:51:46,034][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:51:46,040][227910] Reward + Measures: [[368.07928732   0.31995699   0.72237533   0.38951734   0.621912  ]][0m
[37m[1m[2023-07-10 15:51:46,040][227910] Max Reward on eval: 368.0792873193015[0m
[37m[1m[2023-07-10 15:51:46,040][227910] Min Reward on eval: 368.0792873193015[0m
[37m[1m[2023-07-10 15:51:46,041][227910] Mean Reward across all agents: 368.0792873193015[0m
[37m[1m[2023-07-10 15:51:46,041][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:51:51,575][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:51:51,581][227910] Reward + Measures: [[ -82.28377011    0.3242        0.47699997    0.3098        0.2369    ]
 [  43.40935877    0.15210001    0.89370006    0.34230003    0.86509991]
 [-226.42088828    0.30039999    0.52250004    0.28870001    0.44819999]
 ...
 [ 114.25547498    0.32230002    0.69400007    0.625         0.58700001]
 [-285.73055484    0.61390001    0.24860001    0.5478        0.40829998]
 [  56.98668198    0.27320001    0.36120003    0.24650002    0.19939999]][0m
[37m[1m[2023-07-10 15:51:51,581][227910] Max Reward on eval: 978.5683721642475[0m
[37m[1m[2023-07-10 15:51:51,582][227910] Min Reward on eval: -1043.022269697339[0m
[37m[1m[2023-07-10 15:51:51,582][227910] Mean Reward across all agents: 114.17544015685283[0m
[37m[1m[2023-07-10 15:51:51,582][227910] Average Trajectory Length: 998.177[0m
[36m[2023-07-10 15:51:51,587][227910] mean_value=-891.038826572221, max_value=1286.8242486457107[0m
[37m[1m[2023-07-10 15:51:51,589][227910] New mean coefficients: [[5.2973742  0.24064717 0.2607532  1.0596238  0.4714002 ]][0m
[37m[1m[2023-07-10 15:51:51,590][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:52:01,427][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 15:52:01,427][227910] FPS: 390429.38[0m
[36m[2023-07-10 15:52:01,430][227910] itr=763, itrs=2000, Progress: 38.15%[0m
[36m[2023-07-10 15:52:12,906][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 15:52:12,907][227910] FPS: 335187.88[0m
[36m[2023-07-10 15:52:17,661][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:52:17,662][227910] Reward + Measures: [[424.30407643   0.36495167   0.68432635   0.32795033   0.54795331]][0m
[37m[1m[2023-07-10 15:52:17,662][227910] Max Reward on eval: 424.30407643212675[0m
[37m[1m[2023-07-10 15:52:17,662][227910] Min Reward on eval: 424.30407643212675[0m
[37m[1m[2023-07-10 15:52:17,662][227910] Mean Reward across all agents: 424.30407643212675[0m
[37m[1m[2023-07-10 15:52:17,663][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:52:23,125][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:52:23,126][227910] Reward + Measures: [[ -44.75651806    0.38150001    0.70500004    0.42560002    0.63160002]
 [ 309.49691884    0.44640002    0.65250009    0.37649998    0.47679996]
 [-262.51992408    0.0459        0.94859999    0.77500004    0.91580003]
 ...
 [ -49.97812161    0.53130001    0.72240001    0.45660001    0.61019993]
 [  72.96951992    0.32949996    0.47019997    0.27000001    0.33220002]
 [-318.96049825    0.30290002    0.38990003    0.26650003    0.34990001]][0m
[37m[1m[2023-07-10 15:52:23,126][227910] Max Reward on eval: 818.2373916074982[0m
[37m[1m[2023-07-10 15:52:23,126][227910] Min Reward on eval: -1252.1140517380322[0m
[37m[1m[2023-07-10 15:52:23,126][227910] Mean Reward across all agents: 24.033130283049445[0m
[37m[1m[2023-07-10 15:52:23,127][227910] Average Trajectory Length: 999.4096666666667[0m
[36m[2023-07-10 15:52:23,128][227910] mean_value=-969.172962793619, max_value=722.6178636725375[0m
[37m[1m[2023-07-10 15:52:23,131][227910] New mean coefficients: [[5.045563   0.32718557 0.37245983 0.5466835  0.6114966 ]][0m
[37m[1m[2023-07-10 15:52:23,132][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:52:32,902][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 15:52:32,902][227910] FPS: 393095.84[0m
[36m[2023-07-10 15:52:32,904][227910] itr=764, itrs=2000, Progress: 38.20%[0m
[36m[2023-07-10 15:52:44,505][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 15:52:44,505][227910] FPS: 331596.83[0m
[36m[2023-07-10 15:52:49,348][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:52:49,349][227910] Reward + Measures: [[1138.39021247    0.23678333    0.59306663    0.28702667    0.29771501]][0m
[37m[1m[2023-07-10 15:52:49,349][227910] Max Reward on eval: 1138.3902124695226[0m
[37m[1m[2023-07-10 15:52:49,349][227910] Min Reward on eval: 1138.3902124695226[0m
[37m[1m[2023-07-10 15:52:49,349][227910] Mean Reward across all agents: 1138.3902124695226[0m
[37m[1m[2023-07-10 15:52:49,350][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:52:55,011][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:52:55,011][227910] Reward + Measures: [[ 137.80811745    0.12459999    0.77409995    0.32930002    0.86490005]
 [ -39.16954711    0.41009998    0.61200005    0.42669997    0.49470001]
 [ -79.29483384    0.71100003    0.30140004    0.72859997    0.49780002]
 ...
 [-300.70289219    0.20480001    0.59619999    0.47589999    0.44110003]
 [ 441.68330298    0.2494        0.48130003    0.22730003    0.38649997]
 [  29.23889453    0.27129999    0.48629999    0.25289997    0.33220002]][0m
[37m[1m[2023-07-10 15:52:55,012][227910] Max Reward on eval: 1226.244743323204[0m
[37m[1m[2023-07-10 15:52:55,012][227910] Min Reward on eval: -1089.1575576365692[0m
[37m[1m[2023-07-10 15:52:55,012][227910] Mean Reward across all agents: 267.39836824774693[0m
[37m[1m[2023-07-10 15:52:55,012][227910] Average Trajectory Length: 995.5476666666666[0m
[36m[2023-07-10 15:52:55,015][227910] mean_value=-796.9157000334278, max_value=852.1586250067464[0m
[37m[1m[2023-07-10 15:52:55,018][227910] New mean coefficients: [[ 5.588596    0.50754184  0.91054094  0.7775842  -0.3974114 ]][0m
[37m[1m[2023-07-10 15:52:55,019][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:53:04,951][227910] train() took 9.93 seconds to complete[0m
[36m[2023-07-10 15:53:04,952][227910] FPS: 386687.42[0m
[36m[2023-07-10 15:53:04,954][227910] itr=765, itrs=2000, Progress: 38.25%[0m
[36m[2023-07-10 15:53:16,550][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 15:53:16,550][227910] FPS: 331640.24[0m
[36m[2023-07-10 15:53:21,347][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:53:21,348][227910] Reward + Measures: [[1328.84197583    0.24228665    0.56638038    0.29093635    0.22937301]][0m
[37m[1m[2023-07-10 15:53:21,348][227910] Max Reward on eval: 1328.8419758274824[0m
[37m[1m[2023-07-10 15:53:21,348][227910] Min Reward on eval: 1328.8419758274824[0m
[37m[1m[2023-07-10 15:53:21,348][227910] Mean Reward across all agents: 1328.8419758274824[0m
[37m[1m[2023-07-10 15:53:21,349][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:53:26,881][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:53:26,882][227910] Reward + Measures: [[ 134.49280491    0.31210002    0.70030004    0.23450001    0.50260001]
 [ 258.30130612    0.54259998    0.54820001    0.3653        0.32179999]
 [1019.79451431    0.28850001    0.54770005    0.29100001    0.3197    ]
 ...
 [-729.86952986    0.75879997    0.74769998    0.28630003    0.81910002]
 [ -93.25861023    0.38410002    0.81759995    0.89560002    0.82849997]
 [-244.19835332    0.51030004    0.38500002    0.50580001    0.57589996]][0m
[37m[1m[2023-07-10 15:53:26,882][227910] Max Reward on eval: 1484.7845317637198[0m
[37m[1m[2023-07-10 15:53:26,882][227910] Min Reward on eval: -1194.032538313954[0m
[37m[1m[2023-07-10 15:53:26,882][227910] Mean Reward across all agents: 172.93339214112882[0m
[37m[1m[2023-07-10 15:53:26,883][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:53:26,888][227910] mean_value=-530.4235756231514, max_value=1170.7590214861325[0m
[37m[1m[2023-07-10 15:53:26,890][227910] New mean coefficients: [[ 6.452072    0.8196267   0.83021617  0.86001384 -0.52237874]][0m
[37m[1m[2023-07-10 15:53:26,891][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:53:36,690][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 15:53:36,691][227910] FPS: 391949.15[0m
[36m[2023-07-10 15:53:36,693][227910] itr=766, itrs=2000, Progress: 38.30%[0m
[36m[2023-07-10 15:53:48,529][227910] train() took 11.82 seconds to complete[0m
[36m[2023-07-10 15:53:48,529][227910] FPS: 324953.51[0m
[36m[2023-07-10 15:53:53,352][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:53:53,352][227910] Reward + Measures: [[1476.48403841    0.23785034    0.56161702    0.29418433    0.20526367]][0m
[37m[1m[2023-07-10 15:53:53,353][227910] Max Reward on eval: 1476.4840384100812[0m
[37m[1m[2023-07-10 15:53:53,353][227910] Min Reward on eval: 1476.4840384100812[0m
[37m[1m[2023-07-10 15:53:53,353][227910] Mean Reward across all agents: 1476.4840384100812[0m
[37m[1m[2023-07-10 15:53:53,353][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:53:58,762][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:53:58,768][227910] Reward + Measures: [[ 123.3282945     0.49590001    0.69659996    0.39180002    0.61849999]
 [-350.00555581    0.68539995    0.6649        0.7123        0.64090008]
 [-119.99642562    0.33369997    0.54489994    0.5377        0.50160003]
 ...
 [ 144.36551048    0.27950001    0.43179998    0.34950003    0.3328    ]
 [ 872.39218615    0.29050002    0.53689998    0.33000001    0.3335    ]
 [ 318.46010729    0.2282        0.39820001    0.39559999    0.30689999]][0m
[37m[1m[2023-07-10 15:53:58,769][227910] Max Reward on eval: 1520.5837634822353[0m
[37m[1m[2023-07-10 15:53:58,769][227910] Min Reward on eval: -750.6374516254989[0m
[37m[1m[2023-07-10 15:53:58,769][227910] Mean Reward across all agents: 443.746880271047[0m
[37m[1m[2023-07-10 15:53:58,769][227910] Average Trajectory Length: 998.8133333333333[0m
[36m[2023-07-10 15:53:58,771][227910] mean_value=-1049.779772689513, max_value=506.4153550777388[0m
[37m[1m[2023-07-10 15:53:58,774][227910] New mean coefficients: [[ 6.5553904   0.3345082   0.8452463   0.641228   -0.69492185]][0m
[37m[1m[2023-07-10 15:53:58,775][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:54:08,595][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 15:54:08,596][227910] FPS: 391084.49[0m
[36m[2023-07-10 15:54:08,598][227910] itr=767, itrs=2000, Progress: 38.35%[0m
[36m[2023-07-10 15:54:20,069][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 15:54:20,070][227910] FPS: 335344.59[0m
[36m[2023-07-10 15:54:24,803][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:54:24,809][227910] Reward + Measures: [[1247.60818387    0.33439869    0.62809604    0.37187698    0.22770832]][0m
[37m[1m[2023-07-10 15:54:24,809][227910] Max Reward on eval: 1247.6081838672167[0m
[37m[1m[2023-07-10 15:54:24,809][227910] Min Reward on eval: 1247.6081838672167[0m
[37m[1m[2023-07-10 15:54:24,810][227910] Mean Reward across all agents: 1247.6081838672167[0m
[37m[1m[2023-07-10 15:54:24,810][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:54:30,243][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:54:30,243][227910] Reward + Measures: [[610.74732626   0.43570003   0.70730001   0.37090001   0.4822    ]
 [570.65914885   0.23740001   0.68370003   0.2766       0.53189999]
 [250.20242733   0.25220001   0.55339998   0.22130001   0.48100004]
 ...
 [400.31138286   0.29099998   0.4578       0.27199998   0.3599    ]
 [-21.49635312   0.59079999   0.37360001   0.54840004   0.3091    ]
 [ 81.22399875   0.29389998   0.52689999   0.22909999   0.46900001]][0m
[37m[1m[2023-07-10 15:54:30,243][227910] Max Reward on eval: 1406.448777528957[0m
[37m[1m[2023-07-10 15:54:30,244][227910] Min Reward on eval: -399.66244856015544[0m
[37m[1m[2023-07-10 15:54:30,244][227910] Mean Reward across all agents: 585.7641544799026[0m
[37m[1m[2023-07-10 15:54:30,244][227910] Average Trajectory Length: 997.622[0m
[36m[2023-07-10 15:54:30,247][227910] mean_value=-570.393669213377, max_value=1315.1592474286622[0m
[37m[1m[2023-07-10 15:54:30,249][227910] New mean coefficients: [[ 5.871147   -0.0874168   0.578079    0.8669979  -0.06930572]][0m
[37m[1m[2023-07-10 15:54:30,250][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:54:39,870][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 15:54:39,871][227910] FPS: 399231.10[0m
[36m[2023-07-10 15:54:39,873][227910] itr=768, itrs=2000, Progress: 38.40%[0m
[36m[2023-07-10 15:54:51,489][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 15:54:51,489][227910] FPS: 331078.71[0m
[36m[2023-07-10 15:54:56,257][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:54:56,258][227910] Reward + Measures: [[674.43285796   0.37994164   0.58753866   0.32462999   0.38846037]][0m
[37m[1m[2023-07-10 15:54:56,258][227910] Max Reward on eval: 674.4328579599675[0m
[37m[1m[2023-07-10 15:54:56,258][227910] Min Reward on eval: 674.4328579599675[0m
[37m[1m[2023-07-10 15:54:56,259][227910] Mean Reward across all agents: 674.4328579599675[0m
[37m[1m[2023-07-10 15:54:56,259][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:55:01,782][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:55:01,788][227910] Reward + Measures: [[-367.38180332    0.65230006    0.62889999    0.52719998    0.54370004]
 [-827.1460091     0.75470001    0.73260003    0.31650001    0.55920005]
 [-295.5996275     0.55930001    0.59390002    0.27430001    0.44720003]
 ...
 [ 599.61745942    0.50599998    0.40769997    0.5194        0.56839997]
 [  81.55711784    0.29700002    0.7682001     0.52360004    0.73439997]
 [-304.76479358    0.63020003    0.61190003    0.47460005    0.40650001]][0m
[37m[1m[2023-07-10 15:55:01,788][227910] Max Reward on eval: 1026.2000368691195[0m
[37m[1m[2023-07-10 15:55:01,789][227910] Min Reward on eval: -1560.3334967705887[0m
[37m[1m[2023-07-10 15:55:01,789][227910] Mean Reward across all agents: 129.9192516263131[0m
[37m[1m[2023-07-10 15:55:01,789][227910] Average Trajectory Length: 997.5953333333333[0m
[36m[2023-07-10 15:55:01,794][227910] mean_value=-569.2706509047421, max_value=1312.0480932572227[0m
[37m[1m[2023-07-10 15:55:01,797][227910] New mean coefficients: [[ 5.926519   -0.2108655  -0.02927172  1.0533415   0.5207499 ]][0m
[37m[1m[2023-07-10 15:55:01,798][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:55:11,477][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 15:55:11,477][227910] FPS: 396813.27[0m
[36m[2023-07-10 15:55:11,479][227910] itr=769, itrs=2000, Progress: 38.45%[0m
[36m[2023-07-10 15:55:22,933][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 15:55:22,933][227910] FPS: 335818.12[0m
[36m[2023-07-10 15:55:27,863][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:55:27,863][227910] Reward + Measures: [[899.91466453   0.34150332   0.52845502   0.37231568   0.26088968]][0m
[37m[1m[2023-07-10 15:55:27,864][227910] Max Reward on eval: 899.9146645263288[0m
[37m[1m[2023-07-10 15:55:27,864][227910] Min Reward on eval: 899.9146645263288[0m
[37m[1m[2023-07-10 15:55:27,864][227910] Mean Reward across all agents: 899.9146645263288[0m
[37m[1m[2023-07-10 15:55:27,864][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:55:33,502][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:55:33,503][227910] Reward + Measures: [[871.14434907   0.33049998   0.48389998   0.34779999   0.21530001]
 [292.66433187   0.44449997   0.39540002   0.53310001   0.49950004]
 [227.16455833   0.15280001   0.52679998   0.47569999   0.53289998]
 ...
 [701.25534625   0.31900001   0.4921       0.3766       0.23120001]
 [487.18832124   0.34489998   0.4797       0.40230003   0.22950001]
 [507.3609038    0.30270001   0.49160001   0.34540001   0.26539999]][0m
[37m[1m[2023-07-10 15:55:33,503][227910] Max Reward on eval: 1034.7140420519397[0m
[37m[1m[2023-07-10 15:55:33,503][227910] Min Reward on eval: -601.7389765498577[0m
[37m[1m[2023-07-10 15:55:33,503][227910] Mean Reward across all agents: 385.9433709988997[0m
[37m[1m[2023-07-10 15:55:33,504][227910] Average Trajectory Length: 999.7363333333333[0m
[36m[2023-07-10 15:55:33,507][227910] mean_value=-1119.4012085490003, max_value=910.7249665562634[0m
[37m[1m[2023-07-10 15:55:33,510][227910] New mean coefficients: [[ 6.53268     0.3744763   0.59768575  0.8449497  -0.33800697]][0m
[37m[1m[2023-07-10 15:55:33,510][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:55:43,186][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 15:55:43,186][227910] FPS: 396944.40[0m
[36m[2023-07-10 15:55:43,189][227910] itr=770, itrs=2000, Progress: 38.50%[0m
[37m[1m[2023-07-10 15:55:46,520][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000750[0m
[36m[2023-07-10 15:55:58,322][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 15:55:58,322][227910] FPS: 332633.16[0m
[36m[2023-07-10 15:56:03,100][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:56:03,101][227910] Reward + Measures: [[1031.95311133    0.340608      0.52727932    0.37828499    0.23632801]][0m
[37m[1m[2023-07-10 15:56:03,101][227910] Max Reward on eval: 1031.9531113292257[0m
[37m[1m[2023-07-10 15:56:03,101][227910] Min Reward on eval: 1031.9531113292257[0m
[37m[1m[2023-07-10 15:56:03,102][227910] Mean Reward across all agents: 1031.9531113292257[0m
[37m[1m[2023-07-10 15:56:03,102][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:56:08,550][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:56:08,551][227910] Reward + Measures: [[ 193.58057001    0.81549996    0.91849995    0.0342        0.84790003]
 [ 204.4253257     0.45790005    0.60690004    0.1451        0.64840001]
 [ 199.74408562    0.7062        0.82010001    0.108         0.75639999]
 ...
 [-122.92152919    0.73980004    0.75299996    0.1105        0.6904    ]
 [ 246.55477801    0.79759997    0.95270008    0.0248        0.83649999]
 [  37.85902363    0.30889997    0.69060004    0.18079999    0.76210004]][0m
[37m[1m[2023-07-10 15:56:08,551][227910] Max Reward on eval: 1024.1424578081235[0m
[37m[1m[2023-07-10 15:56:08,551][227910] Min Reward on eval: -456.73835084039604[0m
[37m[1m[2023-07-10 15:56:08,551][227910] Mean Reward across all agents: 359.5789306156354[0m
[37m[1m[2023-07-10 15:56:08,552][227910] Average Trajectory Length: 999.0786666666667[0m
[36m[2023-07-10 15:56:08,557][227910] mean_value=-299.91440748057596, max_value=965.3453945946846[0m
[37m[1m[2023-07-10 15:56:08,560][227910] New mean coefficients: [[ 6.710184    0.45655388  1.042201    0.8356846  -0.7040845 ]][0m
[37m[1m[2023-07-10 15:56:08,561][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:56:18,244][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 15:56:18,244][227910] FPS: 396654.47[0m
[36m[2023-07-10 15:56:18,246][227910] itr=771, itrs=2000, Progress: 38.55%[0m
[36m[2023-07-10 15:56:29,858][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 15:56:29,858][227910] FPS: 331288.80[0m
[36m[2023-07-10 15:56:34,775][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:56:34,781][227910] Reward + Measures: [[1155.81679575    0.33463466    0.53045601    0.3839803     0.22201966]][0m
[37m[1m[2023-07-10 15:56:34,781][227910] Max Reward on eval: 1155.816795749271[0m
[37m[1m[2023-07-10 15:56:34,782][227910] Min Reward on eval: 1155.816795749271[0m
[37m[1m[2023-07-10 15:56:34,782][227910] Mean Reward across all agents: 1155.816795749271[0m
[37m[1m[2023-07-10 15:56:34,783][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:56:40,451][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:56:40,452][227910] Reward + Measures: [[1169.82030887    0.3827        0.41980001    0.4436        0.1163    ]
 [ 445.60556599    0.40830001    0.40279999    0.34590003    0.32339999]
 [ -63.8853747     0.51049995    0.36680001    0.57170004    0.37670001]
 ...
 [1029.27147689    0.37969962    0.41269192    0.40726718    0.16783375]
 [ 809.21538979    0.27590004    0.3942        0.29190001    0.1619    ]
 [1272.0116455     0.30689999    0.50800008    0.36939999    0.17840001]][0m
[37m[1m[2023-07-10 15:56:40,452][227910] Max Reward on eval: 1517.7920934619383[0m
[37m[1m[2023-07-10 15:56:40,452][227910] Min Reward on eval: -396.39604424946594[0m
[37m[1m[2023-07-10 15:56:40,452][227910] Mean Reward across all agents: 953.3538260851292[0m
[37m[1m[2023-07-10 15:56:40,453][227910] Average Trajectory Length: 990.4689999999999[0m
[36m[2023-07-10 15:56:40,454][227910] mean_value=-1161.8042481748955, max_value=958.9091354751785[0m
[37m[1m[2023-07-10 15:56:40,457][227910] New mean coefficients: [[ 6.8363175   0.34777832  0.07781971  0.810932   -0.4907619 ]][0m
[37m[1m[2023-07-10 15:56:40,458][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:56:50,162][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 15:56:50,163][227910] FPS: 395756.76[0m
[36m[2023-07-10 15:56:50,165][227910] itr=772, itrs=2000, Progress: 38.60%[0m
[36m[2023-07-10 15:57:01,577][227910] train() took 11.39 seconds to complete[0m
[36m[2023-07-10 15:57:01,577][227910] FPS: 336990.90[0m
[36m[2023-07-10 15:57:06,417][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:57:06,417][227910] Reward + Measures: [[1269.57237836    0.33855167    0.52755201    0.38391435    0.20784833]][0m
[37m[1m[2023-07-10 15:57:06,418][227910] Max Reward on eval: 1269.5723783581223[0m
[37m[1m[2023-07-10 15:57:06,418][227910] Min Reward on eval: 1269.5723783581223[0m
[37m[1m[2023-07-10 15:57:06,418][227910] Mean Reward across all agents: 1269.5723783581223[0m
[37m[1m[2023-07-10 15:57:06,418][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:57:11,931][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:57:11,931][227910] Reward + Measures: [[  91.37596543    0.4206        0.38860002    0.41330001    0.25120002]
 [ 999.54022349    0.35120001    0.50830001    0.33330002    0.2392    ]
 [ 436.12237751    0.34870002    0.43099999    0.32840002    0.2352    ]
 ...
 [1019.0721164     0.33899999    0.42120001    0.31819996    0.17560001]
 [ 818.16155817    0.43120003    0.44549999    0.3231        0.2467    ]
 [ 777.72442431    0.31730002    0.36540002    0.2994        0.18889999]][0m
[37m[1m[2023-07-10 15:57:11,932][227910] Max Reward on eval: 1434.379138462618[0m
[37m[1m[2023-07-10 15:57:11,932][227910] Min Reward on eval: -520.6833478202112[0m
[37m[1m[2023-07-10 15:57:11,932][227910] Mean Reward across all agents: 765.640572345976[0m
[37m[1m[2023-07-10 15:57:11,932][227910] Average Trajectory Length: 998.8536666666666[0m
[36m[2023-07-10 15:57:11,934][227910] mean_value=-1327.2396907876782, max_value=541.0299946277024[0m
[37m[1m[2023-07-10 15:57:11,937][227910] New mean coefficients: [[ 6.500464    0.51150614  0.8735294   0.38151732 -0.55992526]][0m
[37m[1m[2023-07-10 15:57:11,938][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:57:21,762][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 15:57:21,763][227910] FPS: 390918.33[0m
[36m[2023-07-10 15:57:21,765][227910] itr=773, itrs=2000, Progress: 38.65%[0m
[36m[2023-07-10 15:57:33,348][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 15:57:33,348][227910] FPS: 332019.56[0m
[36m[2023-07-10 15:57:38,062][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:57:38,062][227910] Reward + Measures: [[1378.52524276    0.33974832    0.51928866    0.38660201    0.19079167]][0m
[37m[1m[2023-07-10 15:57:38,062][227910] Max Reward on eval: 1378.5252427593632[0m
[37m[1m[2023-07-10 15:57:38,062][227910] Min Reward on eval: 1378.5252427593632[0m
[37m[1m[2023-07-10 15:57:38,063][227910] Mean Reward across all agents: 1378.5252427593632[0m
[37m[1m[2023-07-10 15:57:38,063][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:57:43,507][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:57:43,508][227910] Reward + Measures: [[893.05908339   0.50760001   0.46700001   0.47049999   0.35100001]
 [451.43399667   0.20209999   0.79769999   0.39510003   0.75200003]
 [327.31886721   0.44180003   0.46970001   0.35390002   0.29599997]
 ...
 [ 45.28487599   0.48879996   0.52939999   0.42570001   0.4745    ]
 [886.75942231   0.31210002   0.52349997   0.41249999   0.244     ]
 [895.35567998   0.53599995   0.71120006   0.24730003   0.50749999]][0m
[37m[1m[2023-07-10 15:57:43,508][227910] Max Reward on eval: 1315.0524830750887[0m
[37m[1m[2023-07-10 15:57:43,508][227910] Min Reward on eval: -190.56366685134125[0m
[37m[1m[2023-07-10 15:57:43,508][227910] Mean Reward across all agents: 694.8173181633356[0m
[37m[1m[2023-07-10 15:57:43,509][227910] Average Trajectory Length: 998.6809999999999[0m
[36m[2023-07-10 15:57:43,512][227910] mean_value=-542.9116280291673, max_value=1162.654968741809[0m
[37m[1m[2023-07-10 15:57:43,515][227910] New mean coefficients: [[ 7.0255446   0.53551686  0.53351945  0.61276174 -0.39694214]][0m
[37m[1m[2023-07-10 15:57:43,516][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:57:53,282][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 15:57:53,282][227910] FPS: 393257.80[0m
[36m[2023-07-10 15:57:53,284][227910] itr=774, itrs=2000, Progress: 38.70%[0m
[36m[2023-07-10 15:58:04,847][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 15:58:04,847][227910] FPS: 332625.48[0m
[36m[2023-07-10 15:58:09,693][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:58:09,694][227910] Reward + Measures: [[1492.88828516    0.34231201    0.51816696    0.38168067    0.18105133]][0m
[37m[1m[2023-07-10 15:58:09,694][227910] Max Reward on eval: 1492.888285157375[0m
[37m[1m[2023-07-10 15:58:09,694][227910] Min Reward on eval: 1492.888285157375[0m
[37m[1m[2023-07-10 15:58:09,695][227910] Mean Reward across all agents: 1492.888285157375[0m
[37m[1m[2023-07-10 15:58:09,695][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:58:15,083][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:58:15,084][227910] Reward + Measures: [[730.18723499   0.138        0.61230004   0.34640002   0.33090001]
 [ 60.74614047   0.24190001   0.4138       0.30650002   0.24099998]
 [124.97101266   0.3355       0.3795       0.29750001   0.25659999]
 ...
 [737.24813646   0.2811       0.47240001   0.34600002   0.24689999]
 [227.29193477   0.17215811   0.43360433   0.28347275   0.2077581 ]
 [617.57436917   0.61740005   0.56790006   0.49149999   0.3946    ]][0m
[37m[1m[2023-07-10 15:58:15,084][227910] Max Reward on eval: 1360.2927403491456[0m
[37m[1m[2023-07-10 15:58:15,084][227910] Min Reward on eval: -402.1972195591079[0m
[37m[1m[2023-07-10 15:58:15,084][227910] Mean Reward across all agents: 502.31642936904865[0m
[37m[1m[2023-07-10 15:58:15,085][227910] Average Trajectory Length: 995.6146666666666[0m
[36m[2023-07-10 15:58:15,089][227910] mean_value=-1237.3742618491606, max_value=1379.9859730124704[0m
[37m[1m[2023-07-10 15:58:15,092][227910] New mean coefficients: [[6.9055376  0.39585274 0.41737098 0.5359204  0.41343325]][0m
[37m[1m[2023-07-10 15:58:15,093][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:58:24,859][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 15:58:24,859][227910] FPS: 393267.36[0m
[36m[2023-07-10 15:58:24,862][227910] itr=775, itrs=2000, Progress: 38.75%[0m
[36m[2023-07-10 15:58:36,494][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 15:58:36,494][227910] FPS: 330617.17[0m
[36m[2023-07-10 15:58:41,356][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:58:41,357][227910] Reward + Measures: [[1613.79702904    0.33527422    0.5108943     0.37351131    0.17225793]][0m
[37m[1m[2023-07-10 15:58:41,357][227910] Max Reward on eval: 1613.7970290404817[0m
[37m[1m[2023-07-10 15:58:41,357][227910] Min Reward on eval: 1613.7970290404817[0m
[37m[1m[2023-07-10 15:58:41,357][227910] Mean Reward across all agents: 1613.7970290404817[0m
[37m[1m[2023-07-10 15:58:41,358][227910] Average Trajectory Length: 999.8936666666666[0m
[36m[2023-07-10 15:58:46,884][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:58:46,890][227910] Reward + Measures: [[1410.24490841    0.29090002    0.49850002    0.3213        0.2218    ]
 [1077.82044497    0.249         0.5438        0.2879        0.32430002]
 [ 982.75041374    0.40599999    0.4887        0.45210001    0.39220002]
 ...
 [1589.53955816    0.29949999    0.52200001    0.33769998    0.20870002]
 [1341.24949145    0.28999999    0.55100006    0.34990001    0.22049999]
 [ 804.44338852    0.39659998    0.57380003    0.46510002    0.41560003]][0m
[37m[1m[2023-07-10 15:58:46,890][227910] Max Reward on eval: 1881.9975765878103[0m
[37m[1m[2023-07-10 15:58:46,891][227910] Min Reward on eval: -599.7016405876726[0m
[37m[1m[2023-07-10 15:58:46,891][227910] Mean Reward across all agents: 915.5859452929352[0m
[37m[1m[2023-07-10 15:58:46,891][227910] Average Trajectory Length: 998.468[0m
[36m[2023-07-10 15:58:46,894][227910] mean_value=-471.33674487694174, max_value=1073.6418837007136[0m
[37m[1m[2023-07-10 15:58:46,897][227910] New mean coefficients: [[7.667341   0.00567746 0.64856184 0.7822144  0.12498721]][0m
[37m[1m[2023-07-10 15:58:46,898][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:58:56,643][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 15:58:56,643][227910] FPS: 394124.13[0m
[36m[2023-07-10 15:58:56,645][227910] itr=776, itrs=2000, Progress: 38.80%[0m
[36m[2023-07-10 15:59:08,153][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 15:59:08,153][227910] FPS: 334197.19[0m
[36m[2023-07-10 15:59:12,861][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:59:12,866][227910] Reward + Measures: [[1714.60316409    0.33410302    0.50290567    0.37000096    0.15811767]][0m
[37m[1m[2023-07-10 15:59:12,867][227910] Max Reward on eval: 1714.6031640903748[0m
[37m[1m[2023-07-10 15:59:12,868][227910] Min Reward on eval: 1714.6031640903748[0m
[37m[1m[2023-07-10 15:59:12,868][227910] Mean Reward across all agents: 1714.6031640903748[0m
[37m[1m[2023-07-10 15:59:12,869][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:59:18,549][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:59:18,555][227910] Reward + Measures: [[1587.92792156    0.3127        0.4664        0.3177        0.15989999]
 [ 531.45222914    0.57350004    0.66479999    0.43689999    0.54870003]
 [ 741.5113941     0.33740002    0.58029997    0.3928        0.3752    ]
 ...
 [1279.16321182    0.3348        0.52750003    0.3809        0.22040001]
 [1047.6908332     0.35409999    0.50580001    0.34830001    0.22849999]
 [ 883.08482802    0.34200001    0.50940001    0.40830001    0.26250002]][0m
[37m[1m[2023-07-10 15:59:18,555][227910] Max Reward on eval: 1790.4146442663507[0m
[37m[1m[2023-07-10 15:59:18,555][227910] Min Reward on eval: 90.50930443223623[0m
[37m[1m[2023-07-10 15:59:18,556][227910] Mean Reward across all agents: 973.1027113362912[0m
[37m[1m[2023-07-10 15:59:18,556][227910] Average Trajectory Length: 999.1056666666666[0m
[36m[2023-07-10 15:59:18,559][227910] mean_value=-676.6498587954384, max_value=808.8031133233404[0m
[37m[1m[2023-07-10 15:59:18,561][227910] New mean coefficients: [[ 8.107165    0.07350554  0.71701014  1.0186856  -0.48761925]][0m
[37m[1m[2023-07-10 15:59:18,562][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:59:28,394][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 15:59:28,394][227910] FPS: 390641.95[0m
[36m[2023-07-10 15:59:28,396][227910] itr=777, itrs=2000, Progress: 38.85%[0m
[36m[2023-07-10 15:59:39,874][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 15:59:39,874][227910] FPS: 335065.40[0m
[36m[2023-07-10 15:59:44,571][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:59:44,572][227910] Reward + Measures: [[1824.89657848    0.33158633    0.49805632    0.36720264    0.147293  ]][0m
[37m[1m[2023-07-10 15:59:44,572][227910] Max Reward on eval: 1824.8965784799973[0m
[37m[1m[2023-07-10 15:59:44,572][227910] Min Reward on eval: 1824.8965784799973[0m
[37m[1m[2023-07-10 15:59:44,572][227910] Mean Reward across all agents: 1824.8965784799973[0m
[37m[1m[2023-07-10 15:59:44,573][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 15:59:49,899][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 15:59:49,900][227910] Reward + Measures: [[ 705.75208734    0.58570004    0.1044        0.7554        0.47440001]
 [1200.15703189    0.40640002    0.51069999    0.38150001    0.22719999]
 [ 919.9899113     0.25920001    0.61549997    0.41370001    0.35170004]
 ...
 [1480.44227727    0.3335        0.5           0.35279998    0.17650001]
 [1033.6277532     0.40079999    0.49400002    0.4034        0.20940001]
 [ 971.24135905    0.6239        0.15620001    0.69379997    0.34060001]][0m
[37m[1m[2023-07-10 15:59:49,900][227910] Max Reward on eval: 1893.158678755071[0m
[37m[1m[2023-07-10 15:59:49,900][227910] Min Reward on eval: 314.77401344406826[0m
[37m[1m[2023-07-10 15:59:49,901][227910] Mean Reward across all agents: 1306.3594423836805[0m
[37m[1m[2023-07-10 15:59:49,901][227910] Average Trajectory Length: 998.7186666666666[0m
[36m[2023-07-10 15:59:49,905][227910] mean_value=-343.3484462328863, max_value=1440.3556152372696[0m
[37m[1m[2023-07-10 15:59:49,907][227910] New mean coefficients: [[7.938009   0.3754928  0.3685627  1.2461141  0.12257823]][0m
[37m[1m[2023-07-10 15:59:49,908][227910] Moving the mean solution point...[0m
[36m[2023-07-10 15:59:59,649][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 15:59:59,649][227910] FPS: 394298.73[0m
[36m[2023-07-10 15:59:59,651][227910] itr=778, itrs=2000, Progress: 38.90%[0m
[36m[2023-07-10 16:00:11,241][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 16:00:11,241][227910] FPS: 331819.10[0m
[36m[2023-07-10 16:00:15,992][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:00:15,998][227910] Reward + Measures: [[1951.73088102    0.3251622     0.48837838    0.36240676    0.13422802]][0m
[37m[1m[2023-07-10 16:00:15,998][227910] Max Reward on eval: 1951.7308810217096[0m
[37m[1m[2023-07-10 16:00:15,998][227910] Min Reward on eval: 1951.7308810217096[0m
[37m[1m[2023-07-10 16:00:15,998][227910] Mean Reward across all agents: 1951.7308810217096[0m
[37m[1m[2023-07-10 16:00:15,999][227910] Average Trajectory Length: 999.6953333333333[0m
[36m[2023-07-10 16:00:21,509][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:00:21,514][227910] Reward + Measures: [[731.13412183   0.41290003   0.33759999   0.4578       0.22210002]
 [906.36557541   0.22830001   0.46129999   0.43970004   0.24010001]
 [939.20606914   0.2052       0.56         0.56209999   0.4032    ]
 ...
 [566.71477453   0.28780004   0.48109999   0.29630002   0.34590003]
 [261.5684907    0.15320002   0.80839998   0.71550006   0.76249999]
 [421.22241235   0.72310001   0.33559999   0.62210006   0.0904    ]][0m
[37m[1m[2023-07-10 16:00:21,515][227910] Max Reward on eval: 1936.0931557823903[0m
[37m[1m[2023-07-10 16:00:21,515][227910] Min Reward on eval: -914.1891314317472[0m
[37m[1m[2023-07-10 16:00:21,515][227910] Mean Reward across all agents: 800.5836704802475[0m
[37m[1m[2023-07-10 16:00:21,515][227910] Average Trajectory Length: 995.6513333333334[0m
[36m[2023-07-10 16:00:21,520][227910] mean_value=-574.0127992089801, max_value=1691.3426782866998[0m
[37m[1m[2023-07-10 16:00:21,523][227910] New mean coefficients: [[ 8.806041    0.48319384 -0.7603607   1.2224097   0.15840265]][0m
[37m[1m[2023-07-10 16:00:21,524][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:00:31,286][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 16:00:31,286][227910] FPS: 393451.18[0m
[36m[2023-07-10 16:00:31,288][227910] itr=779, itrs=2000, Progress: 38.95%[0m
[36m[2023-07-10 16:00:42,844][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 16:00:42,844][227910] FPS: 332810.85[0m
[36m[2023-07-10 16:00:47,635][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:00:47,635][227910] Reward + Measures: [[2060.31381628    0.32155499    0.48381767    0.35490838    0.12627867]][0m
[37m[1m[2023-07-10 16:00:47,636][227910] Max Reward on eval: 2060.3138162803575[0m
[37m[1m[2023-07-10 16:00:47,636][227910] Min Reward on eval: 2060.3138162803575[0m
[37m[1m[2023-07-10 16:00:47,636][227910] Mean Reward across all agents: 2060.3138162803575[0m
[37m[1m[2023-07-10 16:00:47,636][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 16:00:53,073][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:00:53,073][227910] Reward + Measures: [[-199.81857958    0.66650003    0.21259999    0.73110002    0.71160001]
 [ 470.32560888    0.48769999    0.45389995    0.48219997    0.41020003]
 [1625.88470615    0.34          0.44329998    0.36870003    0.1459    ]
 ...
 [ 957.71325216    0.39139229    0.32016927    0.38194618    0.18566921]
 [1658.36893642    0.301         0.45299998    0.34340003    0.1514    ]
 [1704.68111254    0.32550001    0.45040002    0.35699996    0.1567    ]][0m
[37m[1m[2023-07-10 16:00:53,074][227910] Max Reward on eval: 1896.9794299827422[0m
[37m[1m[2023-07-10 16:00:53,074][227910] Min Reward on eval: -933.1440497190458[0m
[37m[1m[2023-07-10 16:00:53,074][227910] Mean Reward across all agents: 819.5625429400839[0m
[37m[1m[2023-07-10 16:00:53,074][227910] Average Trajectory Length: 977.2669999999999[0m
[36m[2023-07-10 16:00:53,077][227910] mean_value=-1091.5620908920218, max_value=761.6430024612642[0m
[37m[1m[2023-07-10 16:00:53,079][227910] New mean coefficients: [[ 7.9665384  -0.18197325 -0.7538724   1.2763846  -0.2761391 ]][0m
[37m[1m[2023-07-10 16:00:53,080][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:01:02,817][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 16:01:02,817][227910] FPS: 394451.68[0m
[36m[2023-07-10 16:01:02,820][227910] itr=780, itrs=2000, Progress: 39.00%[0m
[37m[1m[2023-07-10 16:01:06,378][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000760[0m
[36m[2023-07-10 16:01:18,241][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 16:01:18,241][227910] FPS: 330765.94[0m
[36m[2023-07-10 16:01:22,949][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:01:22,949][227910] Reward + Measures: [[2181.30291562    0.31803203    0.47888857    0.34820715    0.11574908]][0m
[37m[1m[2023-07-10 16:01:22,950][227910] Max Reward on eval: 2181.3029156217367[0m
[37m[1m[2023-07-10 16:01:22,950][227910] Min Reward on eval: 2181.3029156217367[0m
[37m[1m[2023-07-10 16:01:22,950][227910] Mean Reward across all agents: 2181.3029156217367[0m
[37m[1m[2023-07-10 16:01:22,950][227910] Average Trajectory Length: 999.9556666666666[0m
[36m[2023-07-10 16:01:28,315][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:01:28,316][227910] Reward + Measures: [[ 762.6892325     0.28530002    0.59569997    0.45830002    0.5061    ]
 [ 110.64182094    0.37540004    0.79510003    0.22280002    0.7403    ]
 [-257.93610921    0.37099999    0.92670006    0.0935        0.90249997]
 ...
 [ 303.43878962    0.26250002    0.59089994    0.27120003    0.60280001]
 [-935.24484746    0.35170004    0.94729996    0.0901        0.92970002]
 [1165.68096233    0.38550001    0.49289998    0.37890002    0.2392    ]][0m
[37m[1m[2023-07-10 16:01:28,316][227910] Max Reward on eval: 2062.4930370647694[0m
[37m[1m[2023-07-10 16:01:28,316][227910] Min Reward on eval: -1690.4162664485164[0m
[37m[1m[2023-07-10 16:01:28,316][227910] Mean Reward across all agents: 486.0014491942134[0m
[37m[1m[2023-07-10 16:01:28,316][227910] Average Trajectory Length: 996.3306666666666[0m
[36m[2023-07-10 16:01:28,319][227910] mean_value=-763.719055656669, max_value=858.0946377773435[0m
[37m[1m[2023-07-10 16:01:28,322][227910] New mean coefficients: [[ 7.9931197  -0.73459566 -0.62388337  1.1445295  -0.7977327 ]][0m
[37m[1m[2023-07-10 16:01:28,323][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:01:38,094][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 16:01:38,094][227910] FPS: 393044.00[0m
[36m[2023-07-10 16:01:38,097][227910] itr=781, itrs=2000, Progress: 39.05%[0m
[36m[2023-07-10 16:01:49,610][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 16:01:49,610][227910] FPS: 334043.80[0m
[36m[2023-07-10 16:01:54,221][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:01:54,221][227910] Reward + Measures: [[2296.34139469    0.31291154    0.47383374    0.34446865    0.10377415]][0m
[37m[1m[2023-07-10 16:01:54,222][227910] Max Reward on eval: 2296.3413946869164[0m
[37m[1m[2023-07-10 16:01:54,222][227910] Min Reward on eval: 2296.3413946869164[0m
[37m[1m[2023-07-10 16:01:54,222][227910] Mean Reward across all agents: 2296.3413946869164[0m
[37m[1m[2023-07-10 16:01:54,222][227910] Average Trajectory Length: 999.7506666666667[0m
[36m[2023-07-10 16:01:59,657][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:01:59,658][227910] Reward + Measures: [[1078.81962471    0.31786484    0.48611337    0.23731522    0.36613572]
 [1371.1907872     0.34073704    0.3966988     0.40762845    0.1501037 ]
 [ 166.15108691    0.35970002    0.36020002    0.3847        0.2667    ]
 ...
 [1113.93359165    0.35053506    0.36257625    0.38036034    0.16941482]
 [ 940.52020188    0.3409        0.43610001    0.31710002    0.30870003]
 [1620.16730778    0.352         0.44400001    0.35699999    0.1698    ]][0m
[37m[1m[2023-07-10 16:01:59,658][227910] Max Reward on eval: 2143.692357635626[0m
[37m[1m[2023-07-10 16:01:59,658][227910] Min Reward on eval: -674.6808254592936[0m
[37m[1m[2023-07-10 16:01:59,659][227910] Mean Reward across all agents: 1015.5076553724411[0m
[37m[1m[2023-07-10 16:01:59,659][227910] Average Trajectory Length: 991.5383333333333[0m
[36m[2023-07-10 16:01:59,661][227910] mean_value=-1071.918789428822, max_value=1690.08388507287[0m
[37m[1m[2023-07-10 16:01:59,664][227910] New mean coefficients: [[ 7.418934   -0.7108061   0.1501742   1.2653711  -0.61898696]][0m
[37m[1m[2023-07-10 16:01:59,665][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:02:09,420][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 16:02:09,421][227910] FPS: 393671.01[0m
[36m[2023-07-10 16:02:09,423][227910] itr=782, itrs=2000, Progress: 39.10%[0m
[36m[2023-07-10 16:02:21,088][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 16:02:21,088][227910] FPS: 329671.71[0m
[36m[2023-07-10 16:02:25,726][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:02:25,726][227910] Reward + Measures: [[1471.08009315    0.34554467    0.51558268    0.34906331    0.18750468]][0m
[37m[1m[2023-07-10 16:02:25,727][227910] Max Reward on eval: 1471.0800931472759[0m
[37m[1m[2023-07-10 16:02:25,727][227910] Min Reward on eval: 1471.0800931472759[0m
[37m[1m[2023-07-10 16:02:25,727][227910] Mean Reward across all agents: 1471.0800931472759[0m
[37m[1m[2023-07-10 16:02:25,727][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 16:02:31,381][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:02:31,382][227910] Reward + Measures: [[ 532.70080921    0.2604        0.39790002    0.28459999    0.25460002]
 [1085.23872961    0.26419997    0.4729        0.32600001    0.2472    ]
 [ 907.8280484     0.22409999    0.38389999    0.25690001    0.1913    ]
 ...
 [ 571.45924721    0.2538        0.44979998    0.2976        0.2419    ]
 [ 650.05145495    0.27970272    0.43385679    0.28085405    0.25621083]
 [1569.61457199    0.3035        0.5108        0.29819998    0.1961    ]][0m
[37m[1m[2023-07-10 16:02:31,382][227910] Max Reward on eval: 1745.4298626885634[0m
[37m[1m[2023-07-10 16:02:31,382][227910] Min Reward on eval: -249.21129558113753[0m
[37m[1m[2023-07-10 16:02:31,383][227910] Mean Reward across all agents: 1076.4907343351952[0m
[37m[1m[2023-07-10 16:02:31,383][227910] Average Trajectory Length: 998.3349999999999[0m
[36m[2023-07-10 16:02:31,384][227910] mean_value=-1465.5206667040009, max_value=155.71614630596446[0m
[37m[1m[2023-07-10 16:02:31,387][227910] New mean coefficients: [[ 7.6686144  -0.61923814 -0.6442372   1.2051744   0.0336886 ]][0m
[37m[1m[2023-07-10 16:02:31,388][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:02:41,140][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 16:02:41,140][227910] FPS: 393850.63[0m
[36m[2023-07-10 16:02:41,142][227910] itr=783, itrs=2000, Progress: 39.15%[0m
[36m[2023-07-10 16:02:52,714][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 16:02:52,714][227910] FPS: 332351.49[0m
[36m[2023-07-10 16:02:57,463][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:02:57,463][227910] Reward + Measures: [[1621.23724739    0.33938333    0.507976      0.3421503     0.176238  ]][0m
[37m[1m[2023-07-10 16:02:57,463][227910] Max Reward on eval: 1621.2372473938742[0m
[37m[1m[2023-07-10 16:02:57,464][227910] Min Reward on eval: 1621.2372473938742[0m
[37m[1m[2023-07-10 16:02:57,464][227910] Mean Reward across all agents: 1621.2372473938742[0m
[37m[1m[2023-07-10 16:02:57,464][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 16:03:02,931][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:03:02,937][227910] Reward + Measures: [[ -11.77931094    0.52572423    0.36212227    0.49305877    0.29969808]
 [1210.51077767    0.33510002    0.43619999    0.4567        0.13340001]
 [ -44.69178874    0.51155424    0.38785782    0.34673974    0.34549278]
 ...
 [ 583.85585115    0.58199996    0.2192        0.61390001    0.36339998]
 [ 580.93067601    0.47490001    0.3136        0.49340001    0.22839999]
 [ 849.30329739    0.39390001    0.36249998    0.4831        0.25250003]][0m
[37m[1m[2023-07-10 16:03:02,937][227910] Max Reward on eval: 1644.0869795994834[0m
[37m[1m[2023-07-10 16:03:02,937][227910] Min Reward on eval: -721.3367284667969[0m
[37m[1m[2023-07-10 16:03:02,938][227910] Mean Reward across all agents: 573.4180844948904[0m
[37m[1m[2023-07-10 16:03:02,938][227910] Average Trajectory Length: 988.0506666666666[0m
[36m[2023-07-10 16:03:02,941][227910] mean_value=-803.7277967108437, max_value=667.6909729205579[0m
[37m[1m[2023-07-10 16:03:02,944][227910] New mean coefficients: [[ 7.7999     -0.5179502  -1.2692173   1.6065375   0.31808108]][0m
[37m[1m[2023-07-10 16:03:02,945][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:03:12,717][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 16:03:12,717][227910] FPS: 393007.85[0m
[36m[2023-07-10 16:03:12,720][227910] itr=784, itrs=2000, Progress: 39.20%[0m
[36m[2023-07-10 16:03:24,348][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 16:03:24,348][227910] FPS: 330719.64[0m
[36m[2023-07-10 16:03:29,137][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:03:29,138][227910] Reward + Measures: [[1747.51804285    0.33334872    0.50017887    0.33399862    0.16825685]][0m
[37m[1m[2023-07-10 16:03:29,138][227910] Max Reward on eval: 1747.5180428543692[0m
[37m[1m[2023-07-10 16:03:29,138][227910] Min Reward on eval: 1747.5180428543692[0m
[37m[1m[2023-07-10 16:03:29,139][227910] Mean Reward across all agents: 1747.5180428543692[0m
[37m[1m[2023-07-10 16:03:29,139][227910] Average Trajectory Length: 999.746[0m
[36m[2023-07-10 16:03:34,533][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:03:34,534][227910] Reward + Measures: [[ 312.24365643    0.35330001    0.51630002    0.33600003    0.46660003]
 [ -78.71766944    0.20920001    0.27680001    0.15220001    0.21089999]
 [ 649.15103752    0.1744        0.61269999    0.31740001    0.38399997]
 ...
 [ 426.33517996    0.21360002    0.41840002    0.21610001    0.26540002]
 [-100.54536937    0.26370001    0.3608        0.17400001    0.38429999]
 [ 924.11523614    0.28339997    0.5334        0.24609999    0.23210001]][0m
[37m[1m[2023-07-10 16:03:34,534][227910] Max Reward on eval: 1628.5035434345132[0m
[37m[1m[2023-07-10 16:03:34,534][227910] Min Reward on eval: -699.1837824780902[0m
[37m[1m[2023-07-10 16:03:34,534][227910] Mean Reward across all agents: 535.2260886058156[0m
[37m[1m[2023-07-10 16:03:34,535][227910] Average Trajectory Length: 991.8626666666667[0m
[36m[2023-07-10 16:03:34,537][227910] mean_value=-745.0478596368624, max_value=1577.1879943493382[0m
[37m[1m[2023-07-10 16:03:34,539][227910] New mean coefficients: [[ 8.110945  -0.377159  -1.4005531  1.7735424  0.3552348]][0m
[37m[1m[2023-07-10 16:03:34,540][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:03:44,257][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 16:03:44,257][227910] FPS: 395271.07[0m
[36m[2023-07-10 16:03:44,259][227910] itr=785, itrs=2000, Progress: 39.25%[0m
[36m[2023-07-10 16:03:55,895][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 16:03:55,895][227910] FPS: 330602.18[0m
[36m[2023-07-10 16:04:00,536][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:04:00,536][227910] Reward + Measures: [[1896.62590693    0.32305866    0.48637334    0.34074834    0.15171501]][0m
[37m[1m[2023-07-10 16:04:00,536][227910] Max Reward on eval: 1896.6259069288435[0m
[37m[1m[2023-07-10 16:04:00,537][227910] Min Reward on eval: 1896.6259069288435[0m
[37m[1m[2023-07-10 16:04:00,537][227910] Mean Reward across all agents: 1896.6259069288435[0m
[37m[1m[2023-07-10 16:04:00,537][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 16:04:05,874][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:04:05,875][227910] Reward + Measures: [[ 814.12322364    0.4341        0.47580001    0.2823        0.25190002]
 [ 294.91635994    0.34620002    0.41090003    0.29449999    0.24790001]
 [-173.79398979    0.61390001    0.80859995    0.0805        0.71139997]
 ...
 [ 357.59896727    0.31430003    0.435         0.19460002    0.2411    ]
 [-236.73561076    0.35139999    0.31010002    0.24879999    0.2105    ]
 [ 320.95155735    0.4199        0.43290001    0.35010001    0.25630003]][0m
[37m[1m[2023-07-10 16:04:05,875][227910] Max Reward on eval: 1797.5933640146163[0m
[37m[1m[2023-07-10 16:04:05,875][227910] Min Reward on eval: -1018.3825629571104[0m
[37m[1m[2023-07-10 16:04:05,875][227910] Mean Reward across all agents: 389.4493253871648[0m
[37m[1m[2023-07-10 16:04:05,876][227910] Average Trajectory Length: 992.2616666666667[0m
[36m[2023-07-10 16:04:05,878][227910] mean_value=-1157.631607367414, max_value=659.9636000509856[0m
[37m[1m[2023-07-10 16:04:05,880][227910] New mean coefficients: [[ 7.9527802  -0.6837107  -0.80603856  1.804548    0.29334027]][0m
[37m[1m[2023-07-10 16:04:05,882][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:04:15,548][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 16:04:15,548][227910] FPS: 397330.47[0m
[36m[2023-07-10 16:04:15,550][227910] itr=786, itrs=2000, Progress: 39.30%[0m
[36m[2023-07-10 16:04:27,011][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 16:04:27,011][227910] FPS: 335570.93[0m
[36m[2023-07-10 16:04:31,802][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:04:31,802][227910] Reward + Measures: [[2065.93214212    0.31223947    0.46906209    0.3408556     0.13739899]][0m
[37m[1m[2023-07-10 16:04:31,802][227910] Max Reward on eval: 2065.9321421220297[0m
[37m[1m[2023-07-10 16:04:31,803][227910] Min Reward on eval: 2065.9321421220297[0m
[37m[1m[2023-07-10 16:04:31,803][227910] Mean Reward across all agents: 2065.9321421220297[0m
[37m[1m[2023-07-10 16:04:31,803][227910] Average Trajectory Length: 999.6896666666667[0m
[36m[2023-07-10 16:04:37,267][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:04:37,268][227910] Reward + Measures: [[1337.41498429    0.17019999    0.50290006    0.35699999    0.21140002]
 [1766.83526317    0.28459999    0.40780002    0.37719998    0.1402    ]
 [-194.46807921    0.37734768    0.37326714    0.40254474    0.34507138]
 ...
 [1454.01312776    0.27612951    0.43385902    0.34050819    0.15377542]
 [ 570.42665693    0.18910001    0.57440007    0.36719999    0.2613    ]
 [ 490.4397406     0.3626        0.38650003    0.39120004    0.2692    ]][0m
[37m[1m[2023-07-10 16:04:37,268][227910] Max Reward on eval: 2140.820572209917[0m
[37m[1m[2023-07-10 16:04:37,268][227910] Min Reward on eval: -356.06129323316856[0m
[37m[1m[2023-07-10 16:04:37,269][227910] Mean Reward across all agents: 1314.5297450322107[0m
[37m[1m[2023-07-10 16:04:37,269][227910] Average Trajectory Length: 995.755[0m
[36m[2023-07-10 16:04:37,271][227910] mean_value=-1490.2834266422162, max_value=1559.6120668393676[0m
[37m[1m[2023-07-10 16:04:37,273][227910] New mean coefficients: [[ 8.339453   -0.5564778  -0.47317564  1.4951853  -0.2696122 ]][0m
[37m[1m[2023-07-10 16:04:37,274][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:04:46,955][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 16:04:46,955][227910] FPS: 396731.40[0m
[36m[2023-07-10 16:04:46,957][227910] itr=787, itrs=2000, Progress: 39.35%[0m
[36m[2023-07-10 16:04:58,403][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 16:04:58,403][227910] FPS: 336001.95[0m
[36m[2023-07-10 16:05:03,204][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:05:03,204][227910] Reward + Measures: [[2199.95959021    0.30725154    0.45374241    0.35004979    0.12164404]][0m
[37m[1m[2023-07-10 16:05:03,204][227910] Max Reward on eval: 2199.959590211705[0m
[37m[1m[2023-07-10 16:05:03,204][227910] Min Reward on eval: 2199.959590211705[0m
[37m[1m[2023-07-10 16:05:03,205][227910] Mean Reward across all agents: 2199.959590211705[0m
[37m[1m[2023-07-10 16:05:03,205][227910] Average Trajectory Length: 999.8043333333333[0m
[36m[2023-07-10 16:05:08,812][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:05:08,812][227910] Reward + Measures: [[ 518.98801863    0.2622        0.31299999    0.1865        0.1441    ]
 [ 120.65936688    0.19993775    0.21726952    0.11247718    0.10158465]
 [-275.29968373    0.22834103    0.16955385    0.09931283    0.12357692]
 ...
 [-660.52840185    0.25939998    0.21540001    0.16320001    0.20710002]
 [1028.00405772    0.33090001    0.40050003    0.22750001    0.19840001]
 [1343.41175868    0.35209998    0.42340001    0.39340001    0.14460002]][0m
[37m[1m[2023-07-10 16:05:08,813][227910] Max Reward on eval: 2036.875229128357[0m
[37m[1m[2023-07-10 16:05:08,813][227910] Min Reward on eval: -1056.8345481641882[0m
[37m[1m[2023-07-10 16:05:08,813][227910] Mean Reward across all agents: 545.5587910228683[0m
[37m[1m[2023-07-10 16:05:08,813][227910] Average Trajectory Length: 984.721[0m
[36m[2023-07-10 16:05:08,816][227910] mean_value=-1545.4651472004746, max_value=2017.2345954167286[0m
[37m[1m[2023-07-10 16:05:08,819][227910] New mean coefficients: [[ 8.423149   -0.23242003 -0.45904607  1.1369169   0.27373433]][0m
[37m[1m[2023-07-10 16:05:08,820][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:05:18,464][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 16:05:18,465][227910] FPS: 398232.88[0m
[36m[2023-07-10 16:05:18,467][227910] itr=788, itrs=2000, Progress: 39.40%[0m
[36m[2023-07-10 16:05:29,968][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 16:05:29,969][227910] FPS: 334407.24[0m
[36m[2023-07-10 16:05:34,706][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:05:34,707][227910] Reward + Measures: [[2320.99339936    0.30120134    0.44803622    0.34748721    0.11575782]][0m
[37m[1m[2023-07-10 16:05:34,707][227910] Max Reward on eval: 2320.9933993590653[0m
[37m[1m[2023-07-10 16:05:34,707][227910] Min Reward on eval: 2320.9933993590653[0m
[37m[1m[2023-07-10 16:05:34,707][227910] Mean Reward across all agents: 2320.9933993590653[0m
[37m[1m[2023-07-10 16:05:34,707][227910] Average Trajectory Length: 999.8823333333333[0m
[36m[2023-07-10 16:05:40,169][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:05:40,169][227910] Reward + Measures: [[1214.40162782    0.2631        0.47540003    0.3251        0.16440001]
 [ 939.8638456     0.29440001    0.42550001    0.41780001    0.16989999]
 [2164.08335018    0.2791        0.45790002    0.34040001    0.1346    ]
 ...
 [1381.03329192    0.31130001    0.49449998    0.4763        0.1269    ]
 [1679.97687345    0.30950001    0.48390004    0.40950003    0.13770001]
 [ 842.84271499    0.3346        0.42910001    0.50830001    0.1303    ]][0m
[37m[1m[2023-07-10 16:05:40,170][227910] Max Reward on eval: 2164.0833501769[0m
[37m[1m[2023-07-10 16:05:40,170][227910] Min Reward on eval: 278.03784343993175[0m
[37m[1m[2023-07-10 16:05:40,170][227910] Mean Reward across all agents: 1322.7239432108283[0m
[37m[1m[2023-07-10 16:05:40,170][227910] Average Trajectory Length: 998.7196666666666[0m
[36m[2023-07-10 16:05:40,172][227910] mean_value=-1026.446877275476, max_value=734.8559711146761[0m
[37m[1m[2023-07-10 16:05:40,175][227910] New mean coefficients: [[ 7.5888247  -0.41306227  0.03138584  0.64138377 -0.2676217 ]][0m
[37m[1m[2023-07-10 16:05:40,176][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:05:50,112][227910] train() took 9.93 seconds to complete[0m
[36m[2023-07-10 16:05:50,112][227910] FPS: 386540.19[0m
[36m[2023-07-10 16:05:50,114][227910] itr=789, itrs=2000, Progress: 39.45%[0m
[36m[2023-07-10 16:06:01,628][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 16:06:01,628][227910] FPS: 334110.17[0m
[36m[2023-07-10 16:06:06,275][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:06:06,275][227910] Reward + Measures: [[2477.12035546    0.29728469    0.44164386    0.34597576    0.10286448]][0m
[37m[1m[2023-07-10 16:06:06,275][227910] Max Reward on eval: 2477.1203554573[0m
[37m[1m[2023-07-10 16:06:06,276][227910] Min Reward on eval: 2477.1203554573[0m
[37m[1m[2023-07-10 16:06:06,276][227910] Mean Reward across all agents: 2477.1203554573[0m
[37m[1m[2023-07-10 16:06:06,276][227910] Average Trajectory Length: 999.948[0m
[36m[2023-07-10 16:06:11,662][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:06:11,663][227910] Reward + Measures: [[ 311.03128744    0.14729999    0.71160001    0.37150002    0.50690001]
 [1619.91417253    0.33410001    0.38300002    0.37030002    0.153     ]
 [2028.05192644    0.30000001    0.39790002    0.31350002    0.14289999]
 ...
 [1412.39891261    0.42090002    0.42650005    0.4183        0.13399999]
 [1031.2033876     0.3416        0.4481        0.3531        0.27350003]
 [  97.50355478    0.35560003    0.44460002    0.40380001    0.33419999]][0m
[37m[1m[2023-07-10 16:06:11,663][227910] Max Reward on eval: 2370.426663785406[0m
[37m[1m[2023-07-10 16:06:11,663][227910] Min Reward on eval: -777.781918411667[0m
[37m[1m[2023-07-10 16:06:11,664][227910] Mean Reward across all agents: 967.0632016608291[0m
[37m[1m[2023-07-10 16:06:11,664][227910] Average Trajectory Length: 994.3433333333332[0m
[36m[2023-07-10 16:06:11,666][227910] mean_value=-954.3566672606822, max_value=1125.5788900920174[0m
[37m[1m[2023-07-10 16:06:11,669][227910] New mean coefficients: [[7.61362    0.13240075 0.49024644 0.66980743 0.01395056]][0m
[37m[1m[2023-07-10 16:06:11,670][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:06:21,303][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 16:06:21,304][227910] FPS: 398756.56[0m
[36m[2023-07-10 16:06:21,306][227910] itr=790, itrs=2000, Progress: 39.50%[0m
[37m[1m[2023-07-10 16:06:24,655][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000770[0m
[36m[2023-07-10 16:06:36,491][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 16:06:36,491][227910] FPS: 331428.42[0m
[36m[2023-07-10 16:06:41,270][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:06:41,270][227910] Reward + Measures: [[2632.06976938    0.29113662    0.43916252    0.33868685    0.09736536]][0m
[37m[1m[2023-07-10 16:06:41,270][227910] Max Reward on eval: 2632.0697693763045[0m
[37m[1m[2023-07-10 16:06:41,271][227910] Min Reward on eval: 2632.0697693763045[0m
[37m[1m[2023-07-10 16:06:41,271][227910] Mean Reward across all agents: 2632.0697693763045[0m
[37m[1m[2023-07-10 16:06:41,271][227910] Average Trajectory Length: 999.8486666666666[0m
[36m[2023-07-10 16:06:46,922][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:06:46,922][227910] Reward + Measures: [[1381.86782811    0.32469997    0.51729995    0.34549999    0.21229999]
 [2045.99122955    0.27879998    0.47119999    0.38119999    0.14819999]
 [2045.64653144    0.29449999    0.44190001    0.40180001    0.08940001]
 ...
 [1383.0331005     0.23720002    0.41700003    0.34339997    0.2089    ]
 [1450.76146585    0.35090002    0.45000002    0.41089997    0.1514    ]
 [ 699.79941263    0.29249999    0.49470001    0.41569996    0.2543    ]][0m
[37m[1m[2023-07-10 16:06:46,923][227910] Max Reward on eval: 2584.4362839053733[0m
[37m[1m[2023-07-10 16:06:46,923][227910] Min Reward on eval: -149.65263422738062[0m
[37m[1m[2023-07-10 16:06:46,923][227910] Mean Reward across all agents: 1299.4430083696116[0m
[37m[1m[2023-07-10 16:06:46,923][227910] Average Trajectory Length: 999.7679999999999[0m
[36m[2023-07-10 16:06:46,926][227910] mean_value=-665.0238745694998, max_value=1113.9022473300224[0m
[37m[1m[2023-07-10 16:06:46,928][227910] New mean coefficients: [[ 6.9731383   0.00583903  1.1949826   0.463482   -0.10173267]][0m
[37m[1m[2023-07-10 16:06:46,929][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:06:56,533][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 16:06:56,533][227910] FPS: 399917.75[0m
[36m[2023-07-10 16:06:56,535][227910] itr=791, itrs=2000, Progress: 39.55%[0m
[36m[2023-07-10 16:07:08,188][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 16:07:08,189][227910] FPS: 330019.30[0m
[36m[2023-07-10 16:07:12,947][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:07:12,947][227910] Reward + Measures: [[2802.10253293    0.28533003    0.43815848    0.35208166    0.0833431 ]][0m
[37m[1m[2023-07-10 16:07:12,947][227910] Max Reward on eval: 2802.102532930304[0m
[37m[1m[2023-07-10 16:07:12,947][227910] Min Reward on eval: 2802.102532930304[0m
[37m[1m[2023-07-10 16:07:12,948][227910] Mean Reward across all agents: 2802.102532930304[0m
[37m[1m[2023-07-10 16:07:12,948][227910] Average Trajectory Length: 998.9473333333333[0m
[36m[2023-07-10 16:07:18,443][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:07:18,443][227910] Reward + Measures: [[1977.65192538    0.32049999    0.47940001    0.34020001    0.1505    ]
 [1189.44067708    0.22990003    0.50430006    0.234         0.24090002]
 [2262.71768223    0.3012        0.4752        0.3154        0.14170001]
 ...
 [1587.0248355     0.27430001    0.54010004    0.37420002    0.22420001]
 [1620.01072099    0.31200001    0.51859999    0.4251        0.1499    ]
 [1646.89208065    0.31029999    0.5           0.44120002    0.14570001]][0m
[37m[1m[2023-07-10 16:07:18,443][227910] Max Reward on eval: 2687.745649877563[0m
[37m[1m[2023-07-10 16:07:18,444][227910] Min Reward on eval: -90.38230425703804[0m
[37m[1m[2023-07-10 16:07:18,444][227910] Mean Reward across all agents: 1501.7980310073465[0m
[37m[1m[2023-07-10 16:07:18,444][227910] Average Trajectory Length: 999.0536666666667[0m
[36m[2023-07-10 16:07:18,447][227910] mean_value=-461.07389741162314, max_value=1602.1121469928169[0m
[37m[1m[2023-07-10 16:07:18,449][227910] New mean coefficients: [[ 6.538792    0.38644123  1.531147   -0.00643778 -0.594911  ]][0m
[37m[1m[2023-07-10 16:07:18,451][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:07:28,188][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 16:07:28,188][227910] FPS: 394424.61[0m
[36m[2023-07-10 16:07:28,191][227910] itr=792, itrs=2000, Progress: 39.60%[0m
[36m[2023-07-10 16:07:39,889][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 16:07:39,889][227910] FPS: 328799.22[0m
[36m[2023-07-10 16:07:44,598][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:07:44,598][227910] Reward + Measures: [[2933.81023202    0.27971265    0.43588883    0.35309464    0.07596792]][0m
[37m[1m[2023-07-10 16:07:44,598][227910] Max Reward on eval: 2933.8102320191606[0m
[37m[1m[2023-07-10 16:07:44,599][227910] Min Reward on eval: 2933.8102320191606[0m
[37m[1m[2023-07-10 16:07:44,599][227910] Mean Reward across all agents: 2933.8102320191606[0m
[37m[1m[2023-07-10 16:07:44,599][227910] Average Trajectory Length: 999.5783333333333[0m
[36m[2023-07-10 16:07:50,048][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:07:50,049][227910] Reward + Measures: [[ 388.16045358    0.3651        0.36560002    0.47200003    0.20470002]
 [ 584.70246379    0.3626        0.52060002    0.51569998    0.13380001]
 [2507.984154      0.29589999    0.44150001    0.39270002    0.0992    ]
 ...
 [1671.19396835    0.29920003    0.4461        0.43710002    0.13780001]
 [1852.69383828    0.30889997    0.4337        0.43790004    0.12049999]
 [1799.95714079    0.29949999    0.4025        0.3409        0.1586    ]][0m
[37m[1m[2023-07-10 16:07:50,049][227910] Max Reward on eval: 2909.30412365963[0m
[37m[1m[2023-07-10 16:07:50,049][227910] Min Reward on eval: -789.6317273843218[0m
[37m[1m[2023-07-10 16:07:50,050][227910] Mean Reward across all agents: 1417.9894326554547[0m
[37m[1m[2023-07-10 16:07:50,050][227910] Average Trajectory Length: 999.0023333333334[0m
[36m[2023-07-10 16:07:50,051][227910] mean_value=-1209.2184480963058, max_value=666.2591757590428[0m
[37m[1m[2023-07-10 16:07:50,054][227910] New mean coefficients: [[ 7.035703    0.6582776   0.9947799   0.04212286 -0.31661814]][0m
[37m[1m[2023-07-10 16:07:50,055][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:07:59,697][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 16:07:59,698][227910] FPS: 398294.57[0m
[36m[2023-07-10 16:07:59,700][227910] itr=793, itrs=2000, Progress: 39.65%[0m
[36m[2023-07-10 16:08:11,130][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 16:08:11,130][227910] FPS: 336473.53[0m
[36m[2023-07-10 16:08:15,876][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:08:15,877][227910] Reward + Measures: [[3073.08686053    0.2782779     0.44151366    0.35312548    0.06765666]][0m
[37m[1m[2023-07-10 16:08:15,877][227910] Max Reward on eval: 3073.086860532531[0m
[37m[1m[2023-07-10 16:08:15,877][227910] Min Reward on eval: 3073.086860532531[0m
[37m[1m[2023-07-10 16:08:15,877][227910] Mean Reward across all agents: 3073.086860532531[0m
[37m[1m[2023-07-10 16:08:15,878][227910] Average Trajectory Length: 998.6353333333333[0m
[36m[2023-07-10 16:08:21,262][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:08:21,263][227910] Reward + Measures: [[ 716.77533368    0.42219996    0.3732        0.37080002    0.19149999]
 [1643.77447844    0.27739999    0.41339999    0.37310001    0.1717    ]
 [ -88.55940997    0.61680001    0.12909999    0.63670003    0.51450008]
 ...
 [1133.28508609    0.27280003    0.55910003    0.45269999    0.28270003]
 [ 586.85514922    0.41170001    0.45039997    0.2818        0.28130004]
 [1905.57273333    0.3364        0.41820002    0.35249996    0.1181    ]][0m
[37m[1m[2023-07-10 16:08:21,263][227910] Max Reward on eval: 2830.7197764708194[0m
[37m[1m[2023-07-10 16:08:21,263][227910] Min Reward on eval: -698.5475645418977[0m
[37m[1m[2023-07-10 16:08:21,263][227910] Mean Reward across all agents: 1101.0152267088442[0m
[37m[1m[2023-07-10 16:08:21,264][227910] Average Trajectory Length: 995.2796666666667[0m
[36m[2023-07-10 16:08:21,266][227910] mean_value=-902.3752719969187, max_value=849.3490916079085[0m
[37m[1m[2023-07-10 16:08:21,269][227910] New mean coefficients: [[ 6.304823    0.2516737   0.82434094 -0.08145402  0.06468698]][0m
[37m[1m[2023-07-10 16:08:21,270][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:08:30,990][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 16:08:30,990][227910] FPS: 395153.06[0m
[36m[2023-07-10 16:08:30,992][227910] itr=794, itrs=2000, Progress: 39.70%[0m
[36m[2023-07-10 16:08:42,577][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 16:08:42,577][227910] FPS: 332051.85[0m
[36m[2023-07-10 16:08:47,361][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:08:47,361][227910] Reward + Measures: [[2059.13399451    0.27952713    0.43079922    0.29243875    0.16456489]][0m
[37m[1m[2023-07-10 16:08:47,361][227910] Max Reward on eval: 2059.1339945050086[0m
[37m[1m[2023-07-10 16:08:47,361][227910] Min Reward on eval: 2059.1339945050086[0m
[37m[1m[2023-07-10 16:08:47,362][227910] Mean Reward across all agents: 2059.1339945050086[0m
[37m[1m[2023-07-10 16:08:47,362][227910] Average Trajectory Length: 998.414[0m
[36m[2023-07-10 16:08:52,919][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:08:52,920][227910] Reward + Measures: [[1728.16629857    0.34380004    0.43560001    0.29500002    0.20810001]
 [ -17.94588876    0.1071        0.81140006    0.52560008    0.7863    ]
 [1120.74242327    0.329         0.41060001    0.4082        0.30330002]
 ...
 [ 707.98494218    0.26139998    0.4842        0.34699997    0.2665    ]
 [1182.00652364    0.3114        0.34960002    0.23930001    0.23269999]
 [1066.10142266    0.22319999    0.44749999    0.29550001    0.14240001]][0m
[37m[1m[2023-07-10 16:08:52,920][227910] Max Reward on eval: 2158.7315637310967[0m
[37m[1m[2023-07-10 16:08:52,920][227910] Min Reward on eval: -529.9924974368769[0m
[37m[1m[2023-07-10 16:08:52,920][227910] Mean Reward across all agents: 963.2679304375301[0m
[37m[1m[2023-07-10 16:08:52,921][227910] Average Trajectory Length: 995.0656666666666[0m
[36m[2023-07-10 16:08:52,924][227910] mean_value=-783.3073087827516, max_value=1142.6806089861084[0m
[37m[1m[2023-07-10 16:08:52,927][227910] New mean coefficients: [[ 6.103649    0.5659516   1.2592072  -0.2640866   0.18664895]][0m
[37m[1m[2023-07-10 16:08:52,928][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:09:02,662][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 16:09:02,662][227910] FPS: 394547.63[0m
[36m[2023-07-10 16:09:02,665][227910] itr=795, itrs=2000, Progress: 39.75%[0m
[36m[2023-07-10 16:09:14,302][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 16:09:14,303][227910] FPS: 330497.72[0m
[36m[2023-07-10 16:09:18,964][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:09:18,965][227910] Reward + Measures: [[2178.36297002    0.2867339     0.41820362    0.2914291     0.1603985 ]][0m
[37m[1m[2023-07-10 16:09:18,965][227910] Max Reward on eval: 2178.362970019215[0m
[37m[1m[2023-07-10 16:09:18,965][227910] Min Reward on eval: 2178.362970019215[0m
[37m[1m[2023-07-10 16:09:18,965][227910] Mean Reward across all agents: 2178.362970019215[0m
[37m[1m[2023-07-10 16:09:18,965][227910] Average Trajectory Length: 998.371[0m
[36m[2023-07-10 16:09:24,385][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:09:24,386][227910] Reward + Measures: [[-599.2936017     0.1276        0.84100002    0.58970004    0.81890005]
 [1789.51722078    0.31810004    0.4364        0.29990003    0.1365    ]
 [1807.57312058    0.30019999    0.45770001    0.29170001    0.2404    ]
 ...
 [1204.96394204    0.31560001    0.5632        0.27880001    0.30070001]
 [ 173.50713835    0.44799995    0.41580001    0.67940003    0.5966    ]
 [ 372.38488557    0.354         0.56510001    0.57820004    0.51470006]][0m
[37m[1m[2023-07-10 16:09:24,386][227910] Max Reward on eval: 2233.778797495016[0m
[37m[1m[2023-07-10 16:09:24,386][227910] Min Reward on eval: -1160.775564219663[0m
[37m[1m[2023-07-10 16:09:24,387][227910] Mean Reward across all agents: 416.818920910424[0m
[37m[1m[2023-07-10 16:09:24,387][227910] Average Trajectory Length: 991.1916666666666[0m
[36m[2023-07-10 16:09:24,390][227910] mean_value=-526.4045406910735, max_value=1119.476454688378[0m
[37m[1m[2023-07-10 16:09:24,393][227910] New mean coefficients: [[ 6.04358     0.5590814   1.3152244  -0.26883814  0.35847554]][0m
[37m[1m[2023-07-10 16:09:24,394][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:09:34,136][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 16:09:34,136][227910] FPS: 394240.62[0m
[36m[2023-07-10 16:09:34,139][227910] itr=796, itrs=2000, Progress: 39.80%[0m
[36m[2023-07-10 16:09:45,614][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 16:09:45,614][227910] FPS: 335219.75[0m
[36m[2023-07-10 16:09:50,416][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:09:50,417][227910] Reward + Measures: [[2349.11216123    0.27935791    0.40338629    0.29094905    0.14281093]][0m
[37m[1m[2023-07-10 16:09:50,417][227910] Max Reward on eval: 2349.1121612317315[0m
[37m[1m[2023-07-10 16:09:50,417][227910] Min Reward on eval: 2349.1121612317315[0m
[37m[1m[2023-07-10 16:09:50,417][227910] Mean Reward across all agents: 2349.1121612317315[0m
[37m[1m[2023-07-10 16:09:50,417][227910] Average Trajectory Length: 996.7533333333333[0m
[36m[2023-07-10 16:09:55,876][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:09:55,876][227910] Reward + Measures: [[ 488.63530526    0.36270002    0.4736        0.26470003    0.35429999]
 [1481.92958408    0.33320004    0.40380001    0.35870001    0.1864    ]
 [-377.78731664    0.8387        0.18820001    0.82120001    0.42510006]
 ...
 [1836.22613608    0.30930001    0.48550001    0.36479998    0.2651    ]
 [ 764.4034254     0.35116038    0.55679816    0.27621508    0.30313206]
 [1222.73196517    0.25158811    0.46901995    0.28695595    0.32103345]][0m
[37m[1m[2023-07-10 16:09:55,877][227910] Max Reward on eval: 2370.488853595592[0m
[37m[1m[2023-07-10 16:09:55,877][227910] Min Reward on eval: -534.6474834952387[0m
[37m[1m[2023-07-10 16:09:55,877][227910] Mean Reward across all agents: 804.5062516770017[0m
[37m[1m[2023-07-10 16:09:55,877][227910] Average Trajectory Length: 997.9453333333333[0m
[36m[2023-07-10 16:09:55,882][227910] mean_value=-588.8584698493834, max_value=1051.4445756331875[0m
[37m[1m[2023-07-10 16:09:55,885][227910] New mean coefficients: [[ 6.057975    0.75060725  1.5042434  -0.64009386  0.4969582 ]][0m
[37m[1m[2023-07-10 16:09:55,886][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:10:05,775][227910] train() took 9.89 seconds to complete[0m
[36m[2023-07-10 16:10:05,775][227910] FPS: 388370.40[0m
[36m[2023-07-10 16:10:05,778][227910] itr=797, itrs=2000, Progress: 39.85%[0m
[36m[2023-07-10 16:10:17,419][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 16:10:17,419][227910] FPS: 330390.61[0m
[36m[2023-07-10 16:10:22,203][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:10:22,204][227910] Reward + Measures: [[2501.52938867    0.2812888     0.4037011     0.29595914    0.1354569 ]][0m
[37m[1m[2023-07-10 16:10:22,204][227910] Max Reward on eval: 2501.5293886667996[0m
[37m[1m[2023-07-10 16:10:22,204][227910] Min Reward on eval: 2501.5293886667996[0m
[37m[1m[2023-07-10 16:10:22,205][227910] Mean Reward across all agents: 2501.5293886667996[0m
[37m[1m[2023-07-10 16:10:22,205][227910] Average Trajectory Length: 996.87[0m
[36m[2023-07-10 16:10:27,639][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:10:27,640][227910] Reward + Measures: [[ 517.95228142    0.21929999    0.41279998    0.25770003    0.23630002]
 [ 975.84539732    0.2994        0.50349998    0.46099997    0.1481    ]
 [1668.67720502    0.28910002    0.35840002    0.26730001    0.19419999]
 ...
 [ 268.85506864    0.2271        0.55260003    0.333         0.35460001]
 [ 396.87284605    0.36250001    0.4673        0.537         0.53149998]
 [1103.64259919    0.24319999    0.57749999    0.31720003    0.28430003]][0m
[37m[1m[2023-07-10 16:10:27,640][227910] Max Reward on eval: 2669.1198016096373[0m
[37m[1m[2023-07-10 16:10:27,640][227910] Min Reward on eval: -839.985239134566[0m
[37m[1m[2023-07-10 16:10:27,640][227910] Mean Reward across all agents: 746.3416076403907[0m
[37m[1m[2023-07-10 16:10:27,641][227910] Average Trajectory Length: 996.5566666666666[0m
[36m[2023-07-10 16:10:27,644][227910] mean_value=-817.5231612828899, max_value=882.5894250007161[0m
[37m[1m[2023-07-10 16:10:27,646][227910] New mean coefficients: [[ 6.25457     0.95554537  0.99005747 -0.2516215   0.94350666]][0m
[37m[1m[2023-07-10 16:10:27,648][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:10:37,327][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 16:10:37,328][227910] FPS: 396774.36[0m
[36m[2023-07-10 16:10:37,330][227910] itr=798, itrs=2000, Progress: 39.90%[0m
[36m[2023-07-10 16:10:48,846][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 16:10:48,847][227910] FPS: 333989.52[0m
[36m[2023-07-10 16:10:53,599][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:10:53,599][227910] Reward + Measures: [[2669.11410113    0.28283194    0.39478824    0.29935563    0.1230083 ]][0m
[37m[1m[2023-07-10 16:10:53,600][227910] Max Reward on eval: 2669.114101132283[0m
[37m[1m[2023-07-10 16:10:53,600][227910] Min Reward on eval: 2669.114101132283[0m
[37m[1m[2023-07-10 16:10:53,600][227910] Mean Reward across all agents: 2669.114101132283[0m
[37m[1m[2023-07-10 16:10:53,600][227910] Average Trajectory Length: 996.6379999999999[0m
[36m[2023-07-10 16:10:59,058][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:10:59,058][227910] Reward + Measures: [[1693.75358871    0.3272        0.37750003    0.36470002    0.24429999]
 [ 998.33882106    0.58020002    0.41279998    0.43379998    0.34410003]
 [ 740.2737027     0.51180005    0.35480002    0.4797        0.36489999]
 ...
 [ 378.23276645    0.6182        0.24730001    0.5959        0.50810003]
 [ 844.56350075    0.44029999    0.37729999    0.44299999    0.36269999]
 [-396.72400935    0.12890001    0.23340002    0.199         0.12120002]][0m
[37m[1m[2023-07-10 16:10:59,058][227910] Max Reward on eval: 2440.840037792339[0m
[37m[1m[2023-07-10 16:10:59,059][227910] Min Reward on eval: -1006.4513716042042[0m
[37m[1m[2023-07-10 16:10:59,059][227910] Mean Reward across all agents: 401.7175195732414[0m
[37m[1m[2023-07-10 16:10:59,059][227910] Average Trajectory Length: 990.0306666666667[0m
[36m[2023-07-10 16:10:59,061][227910] mean_value=-1277.646753479593, max_value=930.4966155193513[0m
[37m[1m[2023-07-10 16:10:59,064][227910] New mean coefficients: [[ 6.295482    0.61301136  1.0531214  -0.14653018  0.79780614]][0m
[37m[1m[2023-07-10 16:10:59,065][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:11:08,779][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 16:11:08,779][227910] FPS: 395369.07[0m
[36m[2023-07-10 16:11:08,781][227910] itr=799, itrs=2000, Progress: 39.95%[0m
[36m[2023-07-10 16:11:20,248][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 16:11:20,248][227910] FPS: 335390.29[0m
[36m[2023-07-10 16:11:25,047][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:11:25,048][227910] Reward + Measures: [[2846.00500005    0.27674362    0.38917261    0.30069685    0.1126041 ]][0m
[37m[1m[2023-07-10 16:11:25,048][227910] Max Reward on eval: 2846.0050000531264[0m
[37m[1m[2023-07-10 16:11:25,048][227910] Min Reward on eval: 2846.0050000531264[0m
[37m[1m[2023-07-10 16:11:25,049][227910] Mean Reward across all agents: 2846.0050000531264[0m
[37m[1m[2023-07-10 16:11:25,049][227910] Average Trajectory Length: 997.437[0m
[36m[2023-07-10 16:11:30,684][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:11:30,684][227910] Reward + Measures: [[1427.09869608    0.33980003    0.57299995    0.32640001    0.2323    ]
 [ 873.83544758    0.22080003    0.3716        0.20990001    0.189     ]
 [1653.20585435    0.3646        0.52899998    0.331         0.19520001]
 ...
 [ 156.01306619    0.34480003    0.33189997    0.33109999    0.2816    ]
 [ 648.81224281    0.4271        0.454         0.44360003    0.30810001]
 [ 991.45736986    0.36290002    0.5061        0.30060002    0.2642    ]][0m
[37m[1m[2023-07-10 16:11:30,685][227910] Max Reward on eval: 2833.678566366364[0m
[37m[1m[2023-07-10 16:11:30,685][227910] Min Reward on eval: -1027.8080555462045[0m
[37m[1m[2023-07-10 16:11:30,685][227910] Mean Reward across all agents: 1050.747596064034[0m
[37m[1m[2023-07-10 16:11:30,685][227910] Average Trajectory Length: 997.067[0m
[36m[2023-07-10 16:11:30,688][227910] mean_value=-1014.143909214627, max_value=1082.2074448256367[0m
[37m[1m[2023-07-10 16:11:30,691][227910] New mean coefficients: [[5.9384394  0.3447218  1.3488536  0.16779822 0.24141467]][0m
[37m[1m[2023-07-10 16:11:30,692][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:11:40,348][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 16:11:40,348][227910] FPS: 397722.71[0m
[36m[2023-07-10 16:11:40,351][227910] itr=800, itrs=2000, Progress: 40.00%[0m
[37m[1m[2023-07-10 16:11:43,690][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000780[0m
[36m[2023-07-10 16:11:55,429][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 16:11:55,430][227910] FPS: 334521.26[0m
[36m[2023-07-10 16:12:00,145][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:12:00,146][227910] Reward + Measures: [[1665.2817857     0.52412957    0.53172725    0.24974109    0.27682978]][0m
[37m[1m[2023-07-10 16:12:00,146][227910] Max Reward on eval: 1665.2817856959896[0m
[37m[1m[2023-07-10 16:12:00,146][227910] Min Reward on eval: 1665.2817856959896[0m
[37m[1m[2023-07-10 16:12:00,146][227910] Mean Reward across all agents: 1665.2817856959896[0m
[37m[1m[2023-07-10 16:12:00,147][227910] Average Trajectory Length: 999.8446666666666[0m
[36m[2023-07-10 16:12:05,598][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:12:05,598][227910] Reward + Measures: [[1238.04804848    0.4752        0.55940002    0.24949999    0.33250004]
 [1406.72883868    0.45240003    0.5442        0.222         0.271     ]
 [1093.3950795     0.56565255    0.46103549    0.33085641    0.40238562]
 ...
 [ 836.28301465    0.43239999    0.5851        0.4357        0.51899999]
 [ 933.0066082     0.42249998    0.50630003    0.26500002    0.3457    ]
 [1198.88316521    0.47909999    0.52850002    0.24840002    0.32860002]][0m
[37m[1m[2023-07-10 16:12:05,598][227910] Max Reward on eval: 1823.9164836355717[0m
[37m[1m[2023-07-10 16:12:05,599][227910] Min Reward on eval: -770.0553141047712[0m
[37m[1m[2023-07-10 16:12:05,599][227910] Mean Reward across all agents: 884.7618282570564[0m
[37m[1m[2023-07-10 16:12:05,599][227910] Average Trajectory Length: 997.87[0m
[36m[2023-07-10 16:12:05,604][227910] mean_value=-76.72653312913442, max_value=2233.532062386442[0m
[37m[1m[2023-07-10 16:12:05,607][227910] New mean coefficients: [[6.478201   0.38915408 0.5390955  0.08694933 0.54023296]][0m
[37m[1m[2023-07-10 16:12:05,608][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:12:15,374][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 16:12:15,375][227910] FPS: 393244.72[0m
[36m[2023-07-10 16:12:15,377][227910] itr=801, itrs=2000, Progress: 40.05%[0m
[36m[2023-07-10 16:12:26,874][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 16:12:26,875][227910] FPS: 334576.50[0m
[36m[2023-07-10 16:12:31,702][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:12:31,703][227910] Reward + Measures: [[1847.00152573    0.515055      0.54482329    0.24022       0.26108199]][0m
[37m[1m[2023-07-10 16:12:31,703][227910] Max Reward on eval: 1847.0015257347095[0m
[37m[1m[2023-07-10 16:12:31,703][227910] Min Reward on eval: 1847.0015257347095[0m
[37m[1m[2023-07-10 16:12:31,703][227910] Mean Reward across all agents: 1847.0015257347095[0m
[37m[1m[2023-07-10 16:12:31,704][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 16:12:37,373][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:12:37,373][227910] Reward + Measures: [[1653.84335877    0.58270001    0.52170002    0.29640001    0.22230001]
 [1252.1705877     0.41309997    0.212         0.23629999    0.14600001]
 [1388.31667432    0.47009999    0.55610001    0.29569998    0.26390001]
 ...
 [1723.5907574     0.51990002    0.43170005    0.34639999    0.19629999]
 [1780.51505921    0.55680001    0.26809999    0.26449999    0.18379998]
 [ 845.69345452    0.45499998    0.50999999    0.2527        0.3215    ]][0m
[37m[1m[2023-07-10 16:12:37,373][227910] Max Reward on eval: 2268.000811294187[0m
[37m[1m[2023-07-10 16:12:37,374][227910] Min Reward on eval: -342.9683905019425[0m
[37m[1m[2023-07-10 16:12:37,374][227910] Mean Reward across all agents: 1254.0084008036772[0m
[37m[1m[2023-07-10 16:12:37,374][227910] Average Trajectory Length: 990.486[0m
[36m[2023-07-10 16:12:37,379][227910] mean_value=61.53486745468855, max_value=2462.239544118964[0m
[37m[1m[2023-07-10 16:12:37,382][227910] New mean coefficients: [[6.4565997  0.11750746 0.3734392  0.17837015 0.14398354]][0m
[37m[1m[2023-07-10 16:12:37,383][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:12:47,042][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 16:12:47,042][227910] FPS: 397624.58[0m
[36m[2023-07-10 16:12:47,045][227910] itr=802, itrs=2000, Progress: 40.10%[0m
[36m[2023-07-10 16:12:58,475][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 16:12:58,475][227910] FPS: 336544.01[0m
[36m[2023-07-10 16:13:03,237][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:13:03,237][227910] Reward + Measures: [[2011.31941354    0.50695449    0.55571997    0.23147391    0.24860767]][0m
[37m[1m[2023-07-10 16:13:03,237][227910] Max Reward on eval: 2011.3194135400222[0m
[37m[1m[2023-07-10 16:13:03,237][227910] Min Reward on eval: 2011.3194135400222[0m
[37m[1m[2023-07-10 16:13:03,238][227910] Mean Reward across all agents: 2011.3194135400222[0m
[37m[1m[2023-07-10 16:13:03,238][227910] Average Trajectory Length: 999.877[0m
[36m[2023-07-10 16:13:08,607][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:13:08,607][227910] Reward + Measures: [[ 518.41451837    0.45860001    0.2192        0.37980002    0.36210001]
 [  27.48100848    0.47480002    0.17839999    0.36540002    0.38499999]
 [1297.32783359    0.40889999    0.5029        0.28509998    0.30310002]
 ...
 [ 595.7241153     0.2726        0.44169998    0.28380001    0.33319998]
 [1146.63009144    0.52880001    0.27149999    0.32969999    0.30170003]
 [1114.06260358    0.5011        0.42539999    0.31730005    0.35310003]][0m
[37m[1m[2023-07-10 16:13:08,607][227910] Max Reward on eval: 2020.4732810505666[0m
[37m[1m[2023-07-10 16:13:08,608][227910] Min Reward on eval: -399.10712311184614[0m
[37m[1m[2023-07-10 16:13:08,608][227910] Mean Reward across all agents: 1227.800604166345[0m
[37m[1m[2023-07-10 16:13:08,608][227910] Average Trajectory Length: 993.8[0m
[36m[2023-07-10 16:13:08,612][227910] mean_value=-415.92724275436, max_value=2155.579962725332[0m
[37m[1m[2023-07-10 16:13:08,615][227910] New mean coefficients: [[ 6.2666035   0.19475159  0.24661846 -0.12018779  0.11651728]][0m
[37m[1m[2023-07-10 16:13:08,616][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:13:18,129][227910] train() took 9.51 seconds to complete[0m
[36m[2023-07-10 16:13:18,129][227910] FPS: 403717.04[0m
[36m[2023-07-10 16:13:18,132][227910] itr=803, itrs=2000, Progress: 40.15%[0m
[36m[2023-07-10 16:13:29,603][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 16:13:29,604][227910] FPS: 335332.66[0m
[36m[2023-07-10 16:13:34,343][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:13:34,343][227910] Reward + Measures: [[2202.17130065    0.50693202    0.55544299    0.22238134    0.23311366]][0m
[37m[1m[2023-07-10 16:13:34,343][227910] Max Reward on eval: 2202.17130064502[0m
[37m[1m[2023-07-10 16:13:34,343][227910] Min Reward on eval: 2202.17130064502[0m
[37m[1m[2023-07-10 16:13:34,344][227910] Mean Reward across all agents: 2202.17130064502[0m
[37m[1m[2023-07-10 16:13:34,344][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 16:13:39,858][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:13:39,858][227910] Reward + Measures: [[1785.71291763    0.52090001    0.57230002    0.23029999    0.2739    ]
 [1623.22168715    0.44439998    0.61309999    0.24920002    0.2924    ]
 [1540.39800788    0.50439996    0.5643        0.22830001    0.25420001]
 ...
 [1547.13360177    0.50340003    0.45649996    0.29070002    0.29250002]
 [1342.26251546    0.49530002    0.45770001    0.24169998    0.2579    ]
 [1756.68856234    0.49850002    0.5582        0.2167        0.24089999]][0m
[37m[1m[2023-07-10 16:13:39,858][227910] Max Reward on eval: 2209.149008382438[0m
[37m[1m[2023-07-10 16:13:39,859][227910] Min Reward on eval: -120.0619560712017[0m
[37m[1m[2023-07-10 16:13:39,859][227910] Mean Reward across all agents: 1564.8703062951133[0m
[37m[1m[2023-07-10 16:13:39,859][227910] Average Trajectory Length: 999.736[0m
[36m[2023-07-10 16:13:39,864][227910] mean_value=397.0353831199305, max_value=2615.66274108151[0m
[37m[1m[2023-07-10 16:13:39,867][227910] New mean coefficients: [[ 5.170557    0.07832325  0.62152386 -0.2233786  -0.07075685]][0m
[37m[1m[2023-07-10 16:13:39,868][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:13:49,547][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 16:13:49,548][227910] FPS: 396796.44[0m
[36m[2023-07-10 16:13:49,550][227910] itr=804, itrs=2000, Progress: 40.20%[0m
[36m[2023-07-10 16:14:01,018][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 16:14:01,018][227910] FPS: 335385.96[0m
[36m[2023-07-10 16:14:05,762][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:14:05,762][227910] Reward + Measures: [[1799.02618799    0.30365747    0.48386911    0.21977462    0.20967318]][0m
[37m[1m[2023-07-10 16:14:05,762][227910] Max Reward on eval: 1799.0261879917455[0m
[37m[1m[2023-07-10 16:14:05,763][227910] Min Reward on eval: 1799.0261879917455[0m
[37m[1m[2023-07-10 16:14:05,763][227910] Mean Reward across all agents: 1799.0261879917455[0m
[37m[1m[2023-07-10 16:14:05,763][227910] Average Trajectory Length: 992.7903333333333[0m
[36m[2023-07-10 16:14:11,132][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:14:11,133][227910] Reward + Measures: [[1385.22427547    0.2499107     0.40394232    0.2240047     0.18851592]
 [1851.66355501    0.33185717    0.46063334    0.20802855    0.20022856]
 [1391.64500491    0.29389998    0.4206        0.27770001    0.205     ]
 ...
 [1223.82799149    0.24890001    0.4226        0.2378        0.22620001]
 [1323.26099903    0.24530001    0.4887        0.22979999    0.20439999]
 [ 888.74843033    0.40200001    0.49650002    0.34539998    0.34390002]][0m
[37m[1m[2023-07-10 16:14:11,133][227910] Max Reward on eval: 2419.604817304993[0m
[37m[1m[2023-07-10 16:14:11,133][227910] Min Reward on eval: 114.06162767878996[0m
[37m[1m[2023-07-10 16:14:11,133][227910] Mean Reward across all agents: 1430.608706354221[0m
[37m[1m[2023-07-10 16:14:11,134][227910] Average Trajectory Length: 982.0269999999999[0m
[36m[2023-07-10 16:14:11,136][227910] mean_value=-685.6416580504665, max_value=1282.0255938068399[0m
[37m[1m[2023-07-10 16:14:11,138][227910] New mean coefficients: [[ 5.0406556  -0.10692634  0.5889117   0.2052055  -0.08828109]][0m
[37m[1m[2023-07-10 16:14:11,139][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:14:20,878][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 16:14:20,879][227910] FPS: 394354.27[0m
[36m[2023-07-10 16:14:20,881][227910] itr=805, itrs=2000, Progress: 40.25%[0m
[36m[2023-07-10 16:14:32,369][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 16:14:32,369][227910] FPS: 334764.76[0m
[36m[2023-07-10 16:14:37,085][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:14:37,085][227910] Reward + Measures: [[1871.87877254    0.17625588    0.44872996    0.30235091    0.19863245]][0m
[37m[1m[2023-07-10 16:14:37,086][227910] Max Reward on eval: 1871.8787725365444[0m
[37m[1m[2023-07-10 16:14:37,086][227910] Min Reward on eval: 1871.8787725365444[0m
[37m[1m[2023-07-10 16:14:37,086][227910] Mean Reward across all agents: 1871.8787725365444[0m
[37m[1m[2023-07-10 16:14:37,087][227910] Average Trajectory Length: 979.5003333333333[0m
[36m[2023-07-10 16:14:42,455][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:14:42,456][227910] Reward + Measures: [[1516.67500792    0.17320001    0.41209999    0.2784        0.1842    ]
 [ 404.00727934    0.14526311    0.28961945    0.22898391    0.14430337]
 [1513.57636112    0.19160001    0.5043        0.25690004    0.22400001]
 ...
 [ 844.05982129    0.1435        0.25530002    0.21570002    0.1487    ]
 [1971.65830226    0.1814        0.5183        0.27620003    0.22280002]
 [ 143.16277907    0.1583        0.31430003    0.25099999    0.17029999]][0m
[37m[1m[2023-07-10 16:14:42,456][227910] Max Reward on eval: 2079.2168841537787[0m
[37m[1m[2023-07-10 16:14:42,456][227910] Min Reward on eval: -321.9832676319638[0m
[37m[1m[2023-07-10 16:14:42,456][227910] Mean Reward across all agents: 1056.4498242653617[0m
[37m[1m[2023-07-10 16:14:42,456][227910] Average Trajectory Length: 979.4556666666666[0m
[36m[2023-07-10 16:14:42,458][227910] mean_value=-1272.2263863852088, max_value=1215.2694013284874[0m
[37m[1m[2023-07-10 16:14:42,461][227910] New mean coefficients: [[ 5.9887013  -0.0211046   0.09767401  0.2670399   0.09985555]][0m
[37m[1m[2023-07-10 16:14:42,462][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:14:52,151][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 16:14:52,152][227910] FPS: 396354.48[0m
[36m[2023-07-10 16:14:52,154][227910] itr=806, itrs=2000, Progress: 40.30%[0m
[36m[2023-07-10 16:15:03,680][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 16:15:03,680][227910] FPS: 333688.24[0m
[36m[2023-07-10 16:15:08,372][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:15:08,372][227910] Reward + Measures: [[2005.65415794    0.17555009    0.42067215    0.28984973    0.18252312]][0m
[37m[1m[2023-07-10 16:15:08,372][227910] Max Reward on eval: 2005.654157943896[0m
[37m[1m[2023-07-10 16:15:08,373][227910] Min Reward on eval: 2005.654157943896[0m
[37m[1m[2023-07-10 16:15:08,373][227910] Mean Reward across all agents: 2005.654157943896[0m
[37m[1m[2023-07-10 16:15:08,373][227910] Average Trajectory Length: 971.2026666666667[0m
[36m[2023-07-10 16:15:13,930][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:15:13,930][227910] Reward + Measures: [[1587.55533858    0.17839999    0.5212        0.24059999    0.2402    ]
 [1934.292777      0.15592904    0.50126117    0.29723892    0.2034694 ]
 [1715.59401129    0.2251        0.45580003    0.33829999    0.22280002]
 ...
 [1654.1746885     0.1735        0.53500003    0.34290001    0.24959998]
 [1928.99967822    0.1706        0.44250003    0.29320002    0.20710002]
 [1288.52114647    0.21983901    0.3878729     0.2410339     0.18786441]][0m
[37m[1m[2023-07-10 16:15:13,930][227910] Max Reward on eval: 2337.379966424452[0m
[37m[1m[2023-07-10 16:15:13,931][227910] Min Reward on eval: -28.200622714252678[0m
[37m[1m[2023-07-10 16:15:13,931][227910] Mean Reward across all agents: 1512.6143870679282[0m
[37m[1m[2023-07-10 16:15:13,931][227910] Average Trajectory Length: 973.6163333333333[0m
[36m[2023-07-10 16:15:13,933][227910] mean_value=-660.7910629373301, max_value=1156.3919109036874[0m
[37m[1m[2023-07-10 16:15:13,936][227910] New mean coefficients: [[ 6.5543947   0.23920596 -0.00333253  0.23997647 -0.04145833]][0m
[37m[1m[2023-07-10 16:15:13,937][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:15:23,721][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 16:15:23,721][227910] FPS: 392534.45[0m
[36m[2023-07-10 16:15:23,724][227910] itr=807, itrs=2000, Progress: 40.35%[0m
[36m[2023-07-10 16:15:35,276][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 16:15:35,276][227910] FPS: 332955.69[0m
[36m[2023-07-10 16:15:40,084][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:15:40,084][227910] Reward + Measures: [[2154.74136733    0.17532538    0.41226998    0.28986686    0.17207293]][0m
[37m[1m[2023-07-10 16:15:40,084][227910] Max Reward on eval: 2154.7413673253163[0m
[37m[1m[2023-07-10 16:15:40,085][227910] Min Reward on eval: 2154.7413673253163[0m
[37m[1m[2023-07-10 16:15:40,085][227910] Mean Reward across all agents: 2154.7413673253163[0m
[37m[1m[2023-07-10 16:15:40,085][227910] Average Trajectory Length: 951.6783333333333[0m
[36m[2023-07-10 16:15:45,546][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:15:45,546][227910] Reward + Measures: [[1416.70227762    0.17888461    0.38055387    0.26190004    0.18407694]
 [1428.83242806    0.147         0.37289998    0.23630002    0.16770001]
 [2014.44385247    0.17066666    0.47850463    0.30493909    0.21213448]
 ...
 [1510.58954655    0.18429999    0.39340001    0.27210003    0.17900001]
 [1257.49355867    0.11570896    0.30045521    0.19864926    0.12704776]
 [1871.15046831    0.1753        0.39129999    0.29729998    0.17290001]][0m
[37m[1m[2023-07-10 16:15:45,546][227910] Max Reward on eval: 2482.9955277317667[0m
[37m[1m[2023-07-10 16:15:45,547][227910] Min Reward on eval: 662.4891030884901[0m
[37m[1m[2023-07-10 16:15:45,547][227910] Mean Reward across all agents: 1535.626018172595[0m
[37m[1m[2023-07-10 16:15:45,547][227910] Average Trajectory Length: 971.8066666666666[0m
[36m[2023-07-10 16:15:45,549][227910] mean_value=-900.0976048710422, max_value=1557.4288733064943[0m
[37m[1m[2023-07-10 16:15:45,552][227910] New mean coefficients: [[ 6.613214    0.2450475   0.2763501   0.14540228 -0.05472905]][0m
[37m[1m[2023-07-10 16:15:45,553][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:15:55,478][227910] train() took 9.92 seconds to complete[0m
[36m[2023-07-10 16:15:55,478][227910] FPS: 386958.91[0m
[36m[2023-07-10 16:15:55,481][227910] itr=808, itrs=2000, Progress: 40.40%[0m
[36m[2023-07-10 16:16:07,176][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 16:16:07,176][227910] FPS: 328881.66[0m
[36m[2023-07-10 16:16:12,105][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:16:12,105][227910] Reward + Measures: [[2370.6124372     0.17394379    0.42168736    0.28990924    0.17154504]][0m
[37m[1m[2023-07-10 16:16:12,105][227910] Max Reward on eval: 2370.612437203579[0m
[37m[1m[2023-07-10 16:16:12,106][227910] Min Reward on eval: 2370.612437203579[0m
[37m[1m[2023-07-10 16:16:12,106][227910] Mean Reward across all agents: 2370.612437203579[0m
[37m[1m[2023-07-10 16:16:12,106][227910] Average Trajectory Length: 960.9693333333333[0m
[36m[2023-07-10 16:16:17,572][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:16:17,573][227910] Reward + Measures: [[1992.35059542    0.15653406    0.43912563    0.29177144    0.18794762]
 [1939.42698485    0.25619999    0.42719999    0.2789        0.16599999]
 [ 563.42734348    0.09779286    0.4509742     0.25911409    0.21982253]
 ...
 [1082.20916915    0.14561951    0.43854371    0.26868567    0.20036447]
 [ 473.04742118    0.0973        0.38659999    0.2278        0.18959999]
 [1632.90297913    0.22248189    0.42103806    0.25421423    0.16697569]][0m
[37m[1m[2023-07-10 16:16:17,573][227910] Max Reward on eval: 2425.787319741177[0m
[37m[1m[2023-07-10 16:16:17,574][227910] Min Reward on eval: -287.91275287323225[0m
[37m[1m[2023-07-10 16:16:17,574][227910] Mean Reward across all agents: 1199.7760606836455[0m
[37m[1m[2023-07-10 16:16:17,574][227910] Average Trajectory Length: 952.2909999999999[0m
[36m[2023-07-10 16:16:17,577][227910] mean_value=-572.980574597968, max_value=1988.1198174460035[0m
[37m[1m[2023-07-10 16:16:17,579][227910] New mean coefficients: [[ 6.099772    0.13876243  0.6569521   0.51861525 -0.36579177]][0m
[37m[1m[2023-07-10 16:16:17,580][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:16:27,367][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 16:16:27,367][227910] FPS: 392437.96[0m
[36m[2023-07-10 16:16:27,369][227910] itr=809, itrs=2000, Progress: 40.45%[0m
[36m[2023-07-10 16:16:39,024][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 16:16:39,025][227910] FPS: 329973.66[0m
[36m[2023-07-10 16:16:43,795][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:16:43,795][227910] Reward + Measures: [[2518.08501255    0.17283525    0.42219791    0.28767049    0.16586317]][0m
[37m[1m[2023-07-10 16:16:43,795][227910] Max Reward on eval: 2518.085012554526[0m
[37m[1m[2023-07-10 16:16:43,795][227910] Min Reward on eval: 2518.085012554526[0m
[37m[1m[2023-07-10 16:16:43,796][227910] Mean Reward across all agents: 2518.085012554526[0m
[37m[1m[2023-07-10 16:16:43,796][227910] Average Trajectory Length: 951.953[0m
[36m[2023-07-10 16:16:49,365][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:16:49,366][227910] Reward + Measures: [[ 818.49977925    0.17809999    0.63889998    0.3908        0.32300004]
 [ 718.93988552    0.18700002    0.47150001    0.29410002    0.36549997]
 [1453.80589937    0.18880001    0.54930001    0.25820002    0.20750001]
 ...
 [ 890.81229693    0.329         0.4831        0.29770002    0.48900005]
 [ 829.51324438    0.30810001    0.54369998    0.38820001    0.38909999]
 [ 861.58436451    0.18599999    0.6408        0.3989        0.31300002]][0m
[37m[1m[2023-07-10 16:16:49,366][227910] Max Reward on eval: 2591.357662842888[0m
[37m[1m[2023-07-10 16:16:49,366][227910] Min Reward on eval: -44.21389494417817[0m
[37m[1m[2023-07-10 16:16:49,367][227910] Mean Reward across all agents: 1162.714282422754[0m
[37m[1m[2023-07-10 16:16:49,367][227910] Average Trajectory Length: 984.3063333333333[0m
[36m[2023-07-10 16:16:49,369][227910] mean_value=-635.4268918131673, max_value=1353.987324212457[0m
[37m[1m[2023-07-10 16:16:49,372][227910] New mean coefficients: [[ 5.856951    0.3597332   0.59855217  0.29457322 -0.21028998]][0m
[37m[1m[2023-07-10 16:16:49,373][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:16:59,196][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 16:16:59,196][227910] FPS: 390986.41[0m
[36m[2023-07-10 16:16:59,199][227910] itr=810, itrs=2000, Progress: 40.50%[0m
[37m[1m[2023-07-10 16:17:02,835][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000790[0m
[36m[2023-07-10 16:17:14,591][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 16:17:14,591][227910] FPS: 333886.81[0m
[36m[2023-07-10 16:17:19,464][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:17:19,464][227910] Reward + Measures: [[2694.66228275    0.17435595    0.42834967    0.28512555    0.16460432]][0m
[37m[1m[2023-07-10 16:17:19,464][227910] Max Reward on eval: 2694.6622827516767[0m
[37m[1m[2023-07-10 16:17:19,465][227910] Min Reward on eval: 2694.6622827516767[0m
[37m[1m[2023-07-10 16:17:19,465][227910] Mean Reward across all agents: 2694.6622827516767[0m
[37m[1m[2023-07-10 16:17:19,465][227910] Average Trajectory Length: 949.5943333333333[0m
[36m[2023-07-10 16:17:24,946][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:17:24,946][227910] Reward + Measures: [[1914.5708154     0.1671        0.458         0.3317        0.22619998]
 [1843.07635757    0.18539999    0.44320002    0.3001        0.22669999]
 [1382.8764922     0.15750001    0.4535        0.34209999    0.25100002]
 ...
 [1761.57586492    0.17309999    0.43400002    0.33050001    0.2244    ]
 [1253.25848052    0.14497519    0.41847801    0.33330709    0.23158582]
 [2402.77521799    0.18700001    0.43179998    0.255         0.17389999]][0m
[37m[1m[2023-07-10 16:17:24,947][227910] Max Reward on eval: 2868.375101758982[0m
[37m[1m[2023-07-10 16:17:24,947][227910] Min Reward on eval: 192.4533151343756[0m
[37m[1m[2023-07-10 16:17:24,947][227910] Mean Reward across all agents: 1911.950957270226[0m
[37m[1m[2023-07-10 16:17:24,947][227910] Average Trajectory Length: 980.534[0m
[36m[2023-07-10 16:17:24,950][227910] mean_value=-761.8712014031116, max_value=1080.4541429524131[0m
[37m[1m[2023-07-10 16:17:24,952][227910] New mean coefficients: [[ 6.9056053   0.46906495  0.18817091  0.40016574 -0.10963289]][0m
[37m[1m[2023-07-10 16:17:24,953][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:17:34,681][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 16:17:34,681][227910] FPS: 394815.96[0m
[36m[2023-07-10 16:17:34,683][227910] itr=811, itrs=2000, Progress: 40.55%[0m
[36m[2023-07-10 16:17:46,247][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 16:17:46,247][227910] FPS: 332571.18[0m
[36m[2023-07-10 16:17:51,113][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:17:51,113][227910] Reward + Measures: [[2851.38145805    0.17520843    0.41579971    0.28735077    0.15839256]][0m
[37m[1m[2023-07-10 16:17:51,114][227910] Max Reward on eval: 2851.3814580479884[0m
[37m[1m[2023-07-10 16:17:51,114][227910] Min Reward on eval: 2851.3814580479884[0m
[37m[1m[2023-07-10 16:17:51,114][227910] Mean Reward across all agents: 2851.3814580479884[0m
[37m[1m[2023-07-10 16:17:51,115][227910] Average Trajectory Length: 951.3346666666666[0m
[36m[2023-07-10 16:17:56,677][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:17:56,678][227910] Reward + Measures: [[2248.84324892    0.20460001    0.31370002    0.26100001    0.1234    ]
 [2647.03716204    0.20292149    0.38647383    0.35541776    0.17287852]
 [1755.38080774    0.17119999    0.46720001    0.35370001    0.21949999]
 ...
 [2586.99202973    0.20120001    0.3955        0.33399999    0.1815    ]
 [2424.04612035    0.1718        0.3838        0.34520003    0.18170001]
 [1492.58461592    0.32479998    0.41009998    0.3048        0.25170001]][0m
[37m[1m[2023-07-10 16:17:56,678][227910] Max Reward on eval: 2949.2193284353243[0m
[37m[1m[2023-07-10 16:17:56,678][227910] Min Reward on eval: 837.6422923168896[0m
[37m[1m[2023-07-10 16:17:56,679][227910] Mean Reward across all agents: 2232.5305830745287[0m
[37m[1m[2023-07-10 16:17:56,679][227910] Average Trajectory Length: 981.4889999999999[0m
[36m[2023-07-10 16:17:56,680][227910] mean_value=-954.8946920846977, max_value=310.88172462966804[0m
[37m[1m[2023-07-10 16:17:56,683][227910] New mean coefficients: [[ 6.182851    0.26812977  0.16988032  0.18978406 -0.03446798]][0m
[37m[1m[2023-07-10 16:17:56,684][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:18:06,617][227910] train() took 9.93 seconds to complete[0m
[36m[2023-07-10 16:18:06,617][227910] FPS: 386647.08[0m
[36m[2023-07-10 16:18:06,620][227910] itr=812, itrs=2000, Progress: 40.60%[0m
[36m[2023-07-10 16:18:18,147][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 16:18:18,147][227910] FPS: 333629.05[0m
[36m[2023-07-10 16:18:22,900][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:18:22,900][227910] Reward + Measures: [[3005.30215001    0.17825274    0.41504183    0.28530392    0.15460125]][0m
[37m[1m[2023-07-10 16:18:22,901][227910] Max Reward on eval: 3005.3021500095174[0m
[37m[1m[2023-07-10 16:18:22,901][227910] Min Reward on eval: 3005.3021500095174[0m
[37m[1m[2023-07-10 16:18:22,901][227910] Mean Reward across all agents: 3005.3021500095174[0m
[37m[1m[2023-07-10 16:18:22,902][227910] Average Trajectory Length: 947.6419999999999[0m
[36m[2023-07-10 16:18:28,535][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:18:28,536][227910] Reward + Measures: [[2925.29791984    0.1731941     0.48483345    0.29935682    0.16483222]
 [3048.29202058    0.18279998    0.45699999    0.30329999    0.15900001]
 [3027.2944128     0.17709999    0.40400001    0.29340002    0.14299999]
 ...
 [2736.64114429    0.19284724    0.41884145    0.2958073     0.14445876]
 [3007.49621075    0.18789999    0.46219999    0.29519999    0.15300001]
 [3214.53698824    0.184         0.42539999    0.30010003    0.14910001]][0m
[37m[1m[2023-07-10 16:18:28,536][227910] Max Reward on eval: 3313.807996149501[0m
[37m[1m[2023-07-10 16:18:28,536][227910] Min Reward on eval: 2251.6340417578817[0m
[37m[1m[2023-07-10 16:18:28,537][227910] Mean Reward across all agents: 2935.247071191675[0m
[37m[1m[2023-07-10 16:18:28,537][227910] Average Trajectory Length: 983.876[0m
[36m[2023-07-10 16:18:28,539][227910] mean_value=-68.17030875112609, max_value=1052.0380738261258[0m
[37m[1m[2023-07-10 16:18:28,542][227910] New mean coefficients: [[ 5.9693747  -0.2554075   0.25481218  0.5450315  -0.11549717]][0m
[37m[1m[2023-07-10 16:18:28,543][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:18:38,401][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 16:18:38,401][227910] FPS: 389593.35[0m
[36m[2023-07-10 16:18:38,404][227910] itr=813, itrs=2000, Progress: 40.65%[0m
[36m[2023-07-10 16:18:50,338][227910] train() took 11.91 seconds to complete[0m
[36m[2023-07-10 16:18:50,338][227910] FPS: 322316.49[0m
[36m[2023-07-10 16:18:55,035][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:18:55,035][227910] Reward + Measures: [[3130.73662916    0.17816825    0.41459274    0.28431487    0.15555272]][0m
[37m[1m[2023-07-10 16:18:55,035][227910] Max Reward on eval: 3130.736629162974[0m
[37m[1m[2023-07-10 16:18:55,036][227910] Min Reward on eval: 3130.736629162974[0m
[37m[1m[2023-07-10 16:18:55,036][227910] Mean Reward across all agents: 3130.736629162974[0m
[37m[1m[2023-07-10 16:18:55,036][227910] Average Trajectory Length: 957.5183333333333[0m
[36m[2023-07-10 16:19:00,446][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:19:00,446][227910] Reward + Measures: [[2777.78802267    0.17859344    0.42196232    0.25746885    0.16045411]
 [2983.07270559    0.193         0.45360002    0.2626        0.16689999]
 [2149.89764864    0.17870164    0.39433429    0.23505794    0.14904489]
 ...
 [2933.55882351    0.18177833    0.43625665    0.27287412    0.17278042]
 [2978.155051      0.1857        0.46520001    0.26989999    0.1758    ]
 [3050.41984158    0.1778        0.4355        0.26079997    0.16159999]][0m
[37m[1m[2023-07-10 16:19:00,446][227910] Max Reward on eval: 3406.308145412989[0m
[37m[1m[2023-07-10 16:19:00,447][227910] Min Reward on eval: 2062.659837931441[0m
[37m[1m[2023-07-10 16:19:00,447][227910] Mean Reward across all agents: 2980.738264436716[0m
[37m[1m[2023-07-10 16:19:00,447][227910] Average Trajectory Length: 964.607[0m
[36m[2023-07-10 16:19:00,450][227910] mean_value=-39.89644895551634, max_value=1020.6287602550251[0m
[37m[1m[2023-07-10 16:19:00,453][227910] New mean coefficients: [[ 6.0377817  -0.22330251  0.2164045   0.29647833 -0.15437558]][0m
[37m[1m[2023-07-10 16:19:00,454][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:19:10,181][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 16:19:10,181][227910] FPS: 394851.57[0m
[36m[2023-07-10 16:19:10,183][227910] itr=814, itrs=2000, Progress: 40.70%[0m
[36m[2023-07-10 16:19:21,662][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 16:19:21,662][227910] FPS: 335141.75[0m
[36m[2023-07-10 16:19:26,405][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:19:26,405][227910] Reward + Measures: [[3277.36050197    0.17990787    0.41556582    0.28138018    0.15463632]][0m
[37m[1m[2023-07-10 16:19:26,405][227910] Max Reward on eval: 3277.360501968492[0m
[37m[1m[2023-07-10 16:19:26,406][227910] Min Reward on eval: 3277.360501968492[0m
[37m[1m[2023-07-10 16:19:26,406][227910] Mean Reward across all agents: 3277.360501968492[0m
[37m[1m[2023-07-10 16:19:26,406][227910] Average Trajectory Length: 958.1416666666667[0m
[36m[2023-07-10 16:19:31,892][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:19:31,893][227910] Reward + Measures: [[ 989.49518813    0.17811525    0.42473975    0.26260188    0.18470602]
 [1713.88731672    0.2192        0.40330002    0.32030001    0.16109999]
 [1769.82850066    0.1749659     0.38291574    0.22686592    0.1363904 ]
 ...
 [1632.02888729    0.24969999    0.4052        0.28959998    0.2254    ]
 [2568.87778749    0.17096677    0.44057927    0.25762519    0.1417108 ]
 [ 663.92658937    0.3213        0.41669998    0.3594        0.31469998]][0m
[37m[1m[2023-07-10 16:19:31,893][227910] Max Reward on eval: 3293.9963694987587[0m
[37m[1m[2023-07-10 16:19:31,893][227910] Min Reward on eval: 561.7647290323687[0m
[37m[1m[2023-07-10 16:19:31,894][227910] Mean Reward across all agents: 1952.4912729994758[0m
[37m[1m[2023-07-10 16:19:31,894][227910] Average Trajectory Length: 872.726[0m
[36m[2023-07-10 16:19:31,896][227910] mean_value=-670.4598669714428, max_value=2290.6229679346993[0m
[37m[1m[2023-07-10 16:19:31,899][227910] New mean coefficients: [[ 6.103957   -0.03035314 -0.23274177  0.3941052   0.23524493]][0m
[37m[1m[2023-07-10 16:19:31,900][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:19:41,616][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 16:19:41,617][227910] FPS: 395280.34[0m
[36m[2023-07-10 16:19:41,619][227910] itr=815, itrs=2000, Progress: 40.75%[0m
[36m[2023-07-10 16:19:53,084][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 16:19:53,085][227910] FPS: 335448.83[0m
[36m[2023-07-10 16:19:57,788][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:19:57,789][227910] Reward + Measures: [[3324.4745111     0.18018746    0.39799643    0.27785468    0.15196255]][0m
[37m[1m[2023-07-10 16:19:57,789][227910] Max Reward on eval: 3324.474511095618[0m
[37m[1m[2023-07-10 16:19:57,789][227910] Min Reward on eval: 3324.474511095618[0m
[37m[1m[2023-07-10 16:19:57,790][227910] Mean Reward across all agents: 3324.474511095618[0m
[37m[1m[2023-07-10 16:19:57,790][227910] Average Trajectory Length: 942.5353333333333[0m
[36m[2023-07-10 16:20:03,148][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:20:03,149][227910] Reward + Measures: [[3528.57644756    0.1786        0.38839999    0.28619999    0.1476    ]
 [3334.63764781    0.17997904    0.37635228    0.28929046    0.14399068]
 [3114.47031719    0.18930802    0.38566098    0.28179881    0.14778796]
 ...
 [3029.94745926    0.18437989    0.38358638    0.2864556     0.14631526]
 [3231.02675771    0.1851        0.38940001    0.28439999    0.1566    ]
 [3137.26095588    0.18399112    0.37840483    0.2828711     0.14691611]][0m
[37m[1m[2023-07-10 16:20:03,149][227910] Max Reward on eval: 3591.74783773222[0m
[37m[1m[2023-07-10 16:20:03,149][227910] Min Reward on eval: 2158.952824399172[0m
[37m[1m[2023-07-10 16:20:03,150][227910] Mean Reward across all agents: 3003.7015918135585[0m
[37m[1m[2023-07-10 16:20:03,150][227910] Average Trajectory Length: 916.3973333333333[0m
[36m[2023-07-10 16:20:03,152][227910] mean_value=-55.74503485301301, max_value=1041.4776915521256[0m
[37m[1m[2023-07-10 16:20:03,155][227910] New mean coefficients: [[ 5.639075   -0.14909504 -0.2799592   0.31481588  0.19150123]][0m
[37m[1m[2023-07-10 16:20:03,156][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:20:13,109][227910] train() took 9.95 seconds to complete[0m
[36m[2023-07-10 16:20:13,110][227910] FPS: 385842.12[0m
[36m[2023-07-10 16:20:13,112][227910] itr=816, itrs=2000, Progress: 40.80%[0m
[36m[2023-07-10 16:20:24,786][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 16:20:24,786][227910] FPS: 329519.53[0m
[36m[2023-07-10 16:20:29,699][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:20:29,700][227910] Reward + Measures: [[3464.24120019    0.180848      0.4018926     0.2742711     0.15241976]][0m
[37m[1m[2023-07-10 16:20:29,700][227910] Max Reward on eval: 3464.241200194033[0m
[37m[1m[2023-07-10 16:20:29,700][227910] Min Reward on eval: 3464.241200194033[0m
[37m[1m[2023-07-10 16:20:29,700][227910] Mean Reward across all agents: 3464.241200194033[0m
[37m[1m[2023-07-10 16:20:29,701][227910] Average Trajectory Length: 961.1143333333333[0m
[36m[2023-07-10 16:20:35,157][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:20:35,157][227910] Reward + Measures: [[2386.09467208    0.19745421    0.42401871    0.26770806    0.11061261]
 [2025.84897579    0.2448        0.4303        0.27950001    0.17190002]
 [3345.39142905    0.19200002    0.41359997    0.29100001    0.13370001]
 ...
 [2879.9788581     0.20799999    0.3989        0.29340002    0.1508    ]
 [3050.52951556    0.19083095    0.40704527    0.29805002    0.14766906]
 [2597.0351272     0.1729095     0.39686894    0.24124236    0.12402918]][0m
[37m[1m[2023-07-10 16:20:35,157][227910] Max Reward on eval: 3657.364570425451[0m
[37m[1m[2023-07-10 16:20:35,158][227910] Min Reward on eval: 1207.8675705288188[0m
[37m[1m[2023-07-10 16:20:35,158][227910] Mean Reward across all agents: 2691.425342541319[0m
[37m[1m[2023-07-10 16:20:35,158][227910] Average Trajectory Length: 901.5273333333333[0m
[36m[2023-07-10 16:20:35,160][227910] mean_value=-614.0243664553342, max_value=1912.3130482295287[0m
[37m[1m[2023-07-10 16:20:35,162][227910] New mean coefficients: [[ 4.898655   -0.15757181  0.17276391  0.24536687  0.11499381]][0m
[37m[1m[2023-07-10 16:20:35,163][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:20:44,765][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 16:20:44,765][227910] FPS: 399994.85[0m
[36m[2023-07-10 16:20:44,768][227910] itr=817, itrs=2000, Progress: 40.85%[0m
[36m[2023-07-10 16:20:56,294][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 16:20:56,294][227910] FPS: 333707.71[0m
[36m[2023-07-10 16:21:01,123][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:21:01,123][227910] Reward + Measures: [[3583.1123083     0.17757991    0.39448088    0.2841112     0.15449551]][0m
[37m[1m[2023-07-10 16:21:01,123][227910] Max Reward on eval: 3583.1123082966965[0m
[37m[1m[2023-07-10 16:21:01,124][227910] Min Reward on eval: 3583.1123082966965[0m
[37m[1m[2023-07-10 16:21:01,124][227910] Mean Reward across all agents: 3583.1123082966965[0m
[37m[1m[2023-07-10 16:21:01,124][227910] Average Trajectory Length: 965.389[0m
[36m[2023-07-10 16:21:06,745][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:21:06,746][227910] Reward + Measures: [[2963.98193716    0.19269998    0.38280001    0.273         0.13150001]
 [ 965.57638969    0.1886407     0.32793102    0.29697466    0.17946592]
 [2062.21758179    0.19849415    0.35525659    0.26322517    0.14845441]
 ...
 [3756.20078618    0.18370001    0.38680002    0.29179999    0.1445    ]
 [1418.95772338    0.2165        0.39379999    0.22119999    0.1736    ]
 [1310.55253113    0.20820001    0.31380001    0.2361        0.2084    ]][0m
[37m[1m[2023-07-10 16:21:06,746][227910] Max Reward on eval: 3756.20078617651[0m
[37m[1m[2023-07-10 16:21:06,746][227910] Min Reward on eval: 398.98530653791533[0m
[37m[1m[2023-07-10 16:21:06,746][227910] Mean Reward across all agents: 2252.1888367901543[0m
[37m[1m[2023-07-10 16:21:06,747][227910] Average Trajectory Length: 930.2266666666667[0m
[36m[2023-07-10 16:21:06,748][227910] mean_value=-957.7564956846253, max_value=584.0492210385523[0m
[37m[1m[2023-07-10 16:21:06,751][227910] New mean coefficients: [[ 4.66437    -0.1411286   0.18711388  0.22726548  0.10380721]][0m
[37m[1m[2023-07-10 16:21:06,752][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:21:16,513][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 16:21:16,513][227910] FPS: 393473.14[0m
[36m[2023-07-10 16:21:16,515][227910] itr=818, itrs=2000, Progress: 40.90%[0m
[36m[2023-07-10 16:21:28,146][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 16:21:28,146][227910] FPS: 330677.59[0m
[36m[2023-07-10 16:21:32,909][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:21:32,914][227910] Reward + Measures: [[3692.10227038    0.17912988    0.39535847    0.28212562    0.15508682]][0m
[37m[1m[2023-07-10 16:21:32,915][227910] Max Reward on eval: 3692.1022703810013[0m
[37m[1m[2023-07-10 16:21:32,915][227910] Min Reward on eval: 3692.1022703810013[0m
[37m[1m[2023-07-10 16:21:32,915][227910] Mean Reward across all agents: 3692.1022703810013[0m
[37m[1m[2023-07-10 16:21:32,916][227910] Average Trajectory Length: 969.9943333333333[0m
[36m[2023-07-10 16:21:38,330][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:21:38,331][227910] Reward + Measures: [[3142.11241229    0.1978        0.32279998    0.28100002    0.1516    ]
 [1349.84916292    0.23470001    0.31029999    0.22309999    0.16059999]
 [2582.55747808    0.1772        0.37670001    0.26980001    0.1541    ]
 ...
 [2765.92239459    0.18200001    0.32229999    0.3057        0.1323    ]
 [3136.04277222    0.19479196    0.44411907    0.2992402     0.16259246]
 [3053.32334685    0.1973        0.46600005    0.27860004    0.1453    ]][0m
[37m[1m[2023-07-10 16:21:38,331][227910] Max Reward on eval: 3855.7639776226133[0m
[37m[1m[2023-07-10 16:21:38,331][227910] Min Reward on eval: 371.5857931067876[0m
[37m[1m[2023-07-10 16:21:38,332][227910] Mean Reward across all agents: 2848.1013875667472[0m
[37m[1m[2023-07-10 16:21:38,332][227910] Average Trajectory Length: 971.12[0m
[36m[2023-07-10 16:21:38,334][227910] mean_value=-637.1511913670115, max_value=599.4930630874992[0m
[37m[1m[2023-07-10 16:21:38,337][227910] New mean coefficients: [[ 3.9744196  -0.19784606  0.12415145  0.23687175 -0.05465427]][0m
[37m[1m[2023-07-10 16:21:38,338][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:21:48,029][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 16:21:48,029][227910] FPS: 396302.57[0m
[36m[2023-07-10 16:21:48,031][227910] itr=819, itrs=2000, Progress: 40.95%[0m
[36m[2023-07-10 16:21:59,489][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 16:21:59,489][227910] FPS: 335644.47[0m
[36m[2023-07-10 16:22:04,167][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:22:04,167][227910] Reward + Measures: [[3743.67724429    0.17803779    0.38668644    0.28231722    0.15793049]][0m
[37m[1m[2023-07-10 16:22:04,168][227910] Max Reward on eval: 3743.67724428718[0m
[37m[1m[2023-07-10 16:22:04,168][227910] Min Reward on eval: 3743.67724428718[0m
[37m[1m[2023-07-10 16:22:04,168][227910] Mean Reward across all agents: 3743.67724428718[0m
[37m[1m[2023-07-10 16:22:04,168][227910] Average Trajectory Length: 967.381[0m
[36m[2023-07-10 16:22:09,617][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:22:09,618][227910] Reward + Measures: [[2569.92725681    0.20388246    0.44137269    0.23731069    0.13954309]
 [3390.79442026    0.18547276    0.36327267    0.27035591    0.15588181]
 [2174.25086277    0.15325998    0.3080793     0.21581577    0.13930127]
 ...
 [3322.63570264    0.19240001    0.42630002    0.30229998    0.1409    ]
 [1701.63438483    0.15642002    0.33709392    0.24223709    0.16622309]
 [ 834.11423147    0.31420001    0.57590002    0.32130003    0.3655    ]][0m
[37m[1m[2023-07-10 16:22:09,618][227910] Max Reward on eval: 3873.0886164130643[0m
[37m[1m[2023-07-10 16:22:09,618][227910] Min Reward on eval: 818.8390909885172[0m
[37m[1m[2023-07-10 16:22:09,619][227910] Mean Reward across all agents: 2620.720749530691[0m
[37m[1m[2023-07-10 16:22:09,619][227910] Average Trajectory Length: 933.28[0m
[36m[2023-07-10 16:22:09,621][227910] mean_value=-836.6134708624074, max_value=668.3593425945032[0m
[37m[1m[2023-07-10 16:22:09,623][227910] New mean coefficients: [[ 3.7695026  -0.23595327  0.20014903  0.07421219 -0.30156684]][0m
[37m[1m[2023-07-10 16:22:09,624][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:22:19,355][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 16:22:19,355][227910] FPS: 394688.42[0m
[36m[2023-07-10 16:22:19,358][227910] itr=820, itrs=2000, Progress: 41.00%[0m
[37m[1m[2023-07-10 16:22:22,746][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000800[0m
[36m[2023-07-10 16:22:34,488][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 16:22:34,489][227910] FPS: 334664.05[0m
[36m[2023-07-10 16:22:39,276][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:22:39,276][227910] Reward + Measures: [[3843.41087337    0.18003254    0.38853207    0.27862483    0.15832126]][0m
[37m[1m[2023-07-10 16:22:39,276][227910] Max Reward on eval: 3843.410873373051[0m
[37m[1m[2023-07-10 16:22:39,277][227910] Min Reward on eval: 3843.410873373051[0m
[37m[1m[2023-07-10 16:22:39,277][227910] Mean Reward across all agents: 3843.410873373051[0m
[37m[1m[2023-07-10 16:22:39,277][227910] Average Trajectory Length: 970.1949999999999[0m
[36m[2023-07-10 16:22:44,902][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:22:44,902][227910] Reward + Measures: [[3960.10368523    0.17999999    0.39300001    0.27169999    0.16080001]
 [3723.84389695    0.18390757    0.40161654    0.28345096    0.14268571]
 [3922.8771543     0.18402402    0.39050058    0.28942069    0.15307207]
 ...
 [3876.58310701    0.18450001    0.39820004    0.2773        0.14600001]
 [3566.25478753    0.1908        0.39120001    0.26799998    0.13410001]
 [3780.1000193     0.18672553    0.39836812    0.27118644    0.15370075]][0m
[37m[1m[2023-07-10 16:22:44,903][227910] Max Reward on eval: 4021.1627226790415[0m
[37m[1m[2023-07-10 16:22:44,903][227910] Min Reward on eval: 2574.2262487959115[0m
[37m[1m[2023-07-10 16:22:44,903][227910] Mean Reward across all agents: 3707.0035081497876[0m
[37m[1m[2023-07-10 16:22:44,903][227910] Average Trajectory Length: 960.1836666666667[0m
[36m[2023-07-10 16:22:44,906][227910] mean_value=-9.302236524094964, max_value=441.5262888471698[0m
[37m[1m[2023-07-10 16:22:44,909][227910] New mean coefficients: [[ 2.415636   -0.26765805  0.19446522 -0.10703151 -0.311361  ]][0m
[37m[1m[2023-07-10 16:22:44,910][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:22:54,589][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 16:22:54,590][227910] FPS: 396784.54[0m
[36m[2023-07-10 16:22:54,592][227910] itr=821, itrs=2000, Progress: 41.05%[0m
[36m[2023-07-10 16:23:06,345][227910] train() took 11.74 seconds to complete[0m
[36m[2023-07-10 16:23:06,345][227910] FPS: 327214.15[0m
[36m[2023-07-10 16:23:11,268][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:23:11,269][227910] Reward + Measures: [[3881.79742297    0.17591529    0.38687772    0.28369913    0.15800953]][0m
[37m[1m[2023-07-10 16:23:11,269][227910] Max Reward on eval: 3881.7974229689457[0m
[37m[1m[2023-07-10 16:23:11,269][227910] Min Reward on eval: 3881.7974229689457[0m
[37m[1m[2023-07-10 16:23:11,269][227910] Mean Reward across all agents: 3881.7974229689457[0m
[37m[1m[2023-07-10 16:23:11,270][227910] Average Trajectory Length: 969.1333333333333[0m
[36m[2023-07-10 16:23:16,858][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:23:16,859][227910] Reward + Measures: [[3990.17965769    0.18260001    0.3888        0.27330002    0.16170001]
 [4065.22088579    0.17900001    0.39050001    0.28299999    0.15790001]
 [3682.30270102    0.17478046    0.38324362    0.28359517    0.15943089]
 ...
 [3520.1972952     0.16800001    0.36249998    0.27519998    0.1322    ]
 [3913.73646691    0.17478037    0.39084762    0.29402319    0.16087516]
 [3673.19244256    0.17558236    0.40182945    0.28303531    0.15254705]][0m
[37m[1m[2023-07-10 16:23:16,859][227910] Max Reward on eval: 4074.1207812767475[0m
[37m[1m[2023-07-10 16:23:16,859][227910] Min Reward on eval: 2798.3747353035315[0m
[37m[1m[2023-07-10 16:23:16,860][227910] Mean Reward across all agents: 3768.229106663241[0m
[37m[1m[2023-07-10 16:23:16,860][227910] Average Trajectory Length: 957.8209999999999[0m
[36m[2023-07-10 16:23:16,862][227910] mean_value=-77.16237385101935, max_value=255.70447503570085[0m
[37m[1m[2023-07-10 16:23:16,865][227910] New mean coefficients: [[ 1.8575585  -0.24624453  0.03905161 -0.22664455 -0.36842626]][0m
[37m[1m[2023-07-10 16:23:16,866][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:23:26,663][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 16:23:26,663][227910] FPS: 392036.97[0m
[36m[2023-07-10 16:23:26,665][227910] itr=822, itrs=2000, Progress: 41.10%[0m
[36m[2023-07-10 16:23:38,261][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 16:23:38,261][227910] FPS: 331661.73[0m
[36m[2023-07-10 16:23:42,991][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:23:42,991][227910] Reward + Measures: [[3958.61757138    0.17991056    0.38611817    0.28164893    0.15587395]][0m
[37m[1m[2023-07-10 16:23:42,991][227910] Max Reward on eval: 3958.6175713794705[0m
[37m[1m[2023-07-10 16:23:42,992][227910] Min Reward on eval: 3958.6175713794705[0m
[37m[1m[2023-07-10 16:23:42,992][227910] Mean Reward across all agents: 3958.6175713794705[0m
[37m[1m[2023-07-10 16:23:42,992][227910] Average Trajectory Length: 970.3[0m
[36m[2023-07-10 16:23:48,518][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:23:48,519][227910] Reward + Measures: [[4043.34188511    0.1716771     0.40631676    0.2807743     0.1545894 ]
 [3454.11213895    0.17464577    0.41725072    0.2768127     0.15536915]
 [2245.83949952    0.16793722    0.49989834    0.24652477    0.18849465]
 ...
 [3875.15498484    0.18730001    0.4244        0.26069999    0.15580001]
 [3793.60399146    0.16808502    0.36968681    0.26541671    0.14649473]
 [3556.56299243    0.17787063    0.40604424    0.27926466    0.16100973]][0m
[37m[1m[2023-07-10 16:23:48,519][227910] Max Reward on eval: 4131.491063846741[0m
[37m[1m[2023-07-10 16:23:48,519][227910] Min Reward on eval: 1542.0362693619331[0m
[37m[1m[2023-07-10 16:23:48,519][227910] Mean Reward across all agents: 3139.4255690593272[0m
[37m[1m[2023-07-10 16:23:48,520][227910] Average Trajectory Length: 893.8929999999999[0m
[36m[2023-07-10 16:23:48,522][227910] mean_value=-535.7804716011094, max_value=538.2265499133196[0m
[37m[1m[2023-07-10 16:23:48,524][227910] New mean coefficients: [[ 1.9371452  -0.2421997   0.10163282 -0.24721782 -0.42122385]][0m
[37m[1m[2023-07-10 16:23:48,525][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:23:58,387][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 16:23:58,387][227910] FPS: 389434.45[0m
[36m[2023-07-10 16:23:58,390][227910] itr=823, itrs=2000, Progress: 41.15%[0m
[36m[2023-07-10 16:24:10,015][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 16:24:10,016][227910] FPS: 330931.24[0m
[36m[2023-07-10 16:24:14,771][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:24:14,772][227910] Reward + Measures: [[4000.46607566    0.17884983    0.38700911    0.28714535    0.15691207]][0m
[37m[1m[2023-07-10 16:24:14,772][227910] Max Reward on eval: 4000.466075660416[0m
[37m[1m[2023-07-10 16:24:14,772][227910] Min Reward on eval: 4000.466075660416[0m
[37m[1m[2023-07-10 16:24:14,772][227910] Mean Reward across all agents: 4000.466075660416[0m
[37m[1m[2023-07-10 16:24:14,773][227910] Average Trajectory Length: 965.9626666666667[0m
[36m[2023-07-10 16:24:20,171][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:24:20,172][227910] Reward + Measures: [[3286.95735908    0.17677172    0.47332221    0.2930828     0.1752404 ]
 [2731.51764479    0.20914522    0.34187403    0.25861105    0.13574162]
 [3688.59510133    0.19426361    0.33453178    0.25900191    0.14086819]
 ...
 [3986.22537243    0.18520001    0.35700002    0.27500001    0.1573    ]
 [3962.60721142    0.16940002    0.38750002    0.30270001    0.1591    ]
 [3291.0001464     0.20774999    0.31398335    0.24221669    0.13286667]][0m
[37m[1m[2023-07-10 16:24:20,172][227910] Max Reward on eval: 4152.091096018557[0m
[37m[1m[2023-07-10 16:24:20,172][227910] Min Reward on eval: 1622.9824280748376[0m
[37m[1m[2023-07-10 16:24:20,172][227910] Mean Reward across all agents: 3388.6524400762173[0m
[37m[1m[2023-07-10 16:24:20,173][227910] Average Trajectory Length: 929.933[0m
[36m[2023-07-10 16:24:20,174][227910] mean_value=-563.7765625462425, max_value=482.9476415826339[0m
[37m[1m[2023-07-10 16:24:20,177][227910] New mean coefficients: [[ 1.6975633  -0.25233066  0.01453171 -0.04636978 -0.42740333]][0m
[37m[1m[2023-07-10 16:24:20,178][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:24:29,948][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 16:24:29,949][227910] FPS: 393081.90[0m
[36m[2023-07-10 16:24:29,951][227910] itr=824, itrs=2000, Progress: 41.20%[0m
[36m[2023-07-10 16:24:41,554][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 16:24:41,554][227910] FPS: 331443.41[0m
[36m[2023-07-10 16:24:46,328][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:24:46,328][227910] Reward + Measures: [[4020.09612794    0.18061109    0.38174471    0.28343081    0.15346444]][0m
[37m[1m[2023-07-10 16:24:46,328][227910] Max Reward on eval: 4020.0961279437975[0m
[37m[1m[2023-07-10 16:24:46,329][227910] Min Reward on eval: 4020.0961279437975[0m
[37m[1m[2023-07-10 16:24:46,329][227910] Mean Reward across all agents: 4020.0961279437975[0m
[37m[1m[2023-07-10 16:24:46,329][227910] Average Trajectory Length: 959.1936666666667[0m
[36m[2023-07-10 16:24:51,970][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:24:51,971][227910] Reward + Measures: [[4190.21450081    0.17640001    0.37929997    0.28639999    0.1574    ]
 [2784.73462351    0.16635649    0.39290926    0.23964246    0.15552339]
 [ 799.71307611    0.20441008    0.41066179    0.20367442    0.19271493]
 ...
 [3776.27919122    0.1725        0.32360002    0.26280001    0.1441    ]
 [2935.86463116    0.17596887    0.39395371    0.27590501    0.15627252]
 [3695.78400431    0.1745216     0.38933966    0.27711374    0.15548426]][0m
[37m[1m[2023-07-10 16:24:51,976][227910] Max Reward on eval: 4211.073036145326[0m
[37m[1m[2023-07-10 16:24:51,976][227910] Min Reward on eval: 799.7130761134089[0m
[37m[1m[2023-07-10 16:24:51,977][227910] Mean Reward across all agents: 2708.0021930839284[0m
[37m[1m[2023-07-10 16:24:51,977][227910] Average Trajectory Length: 858.016[0m
[36m[2023-07-10 16:24:51,979][227910] mean_value=-774.4734016050265, max_value=1012.8755791318026[0m
[37m[1m[2023-07-10 16:24:51,981][227910] New mean coefficients: [[ 1.6188968  -0.31434983 -0.07331072 -0.24791396 -0.29088622]][0m
[37m[1m[2023-07-10 16:24:51,982][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:25:01,615][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 16:25:01,616][227910] FPS: 398689.03[0m
[36m[2023-07-10 16:25:01,618][227910] itr=825, itrs=2000, Progress: 41.25%[0m
[36m[2023-07-10 16:25:13,305][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 16:25:13,306][227910] FPS: 329117.57[0m
[36m[2023-07-10 16:25:18,012][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:25:18,013][227910] Reward + Measures: [[4128.1724477     0.1839221     0.38624072    0.27701655    0.15272816]][0m
[37m[1m[2023-07-10 16:25:18,013][227910] Max Reward on eval: 4128.172447700456[0m
[37m[1m[2023-07-10 16:25:18,013][227910] Min Reward on eval: 4128.172447700456[0m
[37m[1m[2023-07-10 16:25:18,013][227910] Mean Reward across all agents: 4128.172447700456[0m
[37m[1m[2023-07-10 16:25:18,014][227910] Average Trajectory Length: 969.2893333333333[0m
[36m[2023-07-10 16:25:23,436][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:25:23,437][227910] Reward + Measures: [[3768.06206041    0.17740002    0.37709999    0.27850005    0.1499    ]
 [4058.00804956    0.19100001    0.40270001    0.26990002    0.1603    ]
 [3251.29612713    0.17305222    0.40269765    0.27952057    0.15681633]
 ...
 [4047.63444696    0.18215075    0.38737166    0.28641194    0.15201195]
 [4156.6568378     0.17650001    0.38320002    0.27390003    0.1464    ]
 [4290.53414316    0.18470001    0.37960002    0.28700003    0.14590001]][0m
[37m[1m[2023-07-10 16:25:23,437][227910] Max Reward on eval: 4326.5188153483905[0m
[37m[1m[2023-07-10 16:25:23,437][227910] Min Reward on eval: 2500.335292181745[0m
[37m[1m[2023-07-10 16:25:23,438][227910] Mean Reward across all agents: 3867.741761139972[0m
[37m[1m[2023-07-10 16:25:23,438][227910] Average Trajectory Length: 943.1246666666666[0m
[36m[2023-07-10 16:25:23,440][227910] mean_value=-225.46237416673893, max_value=351.9240517999124[0m
[37m[1m[2023-07-10 16:25:23,443][227910] New mean coefficients: [[ 0.94331735 -0.4567148  -0.23729764 -0.13331977 -0.27853507]][0m
[37m[1m[2023-07-10 16:25:23,444][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:25:33,112][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 16:25:33,112][227910] FPS: 397253.62[0m
[36m[2023-07-10 16:25:33,114][227910] itr=826, itrs=2000, Progress: 41.30%[0m
[36m[2023-07-10 16:25:44,684][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 16:25:44,684][227910] FPS: 332394.99[0m
[36m[2023-07-10 16:25:49,515][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:25:49,516][227910] Reward + Measures: [[4133.40139016    0.18326634    0.37925324    0.2779257     0.14935154]][0m
[37m[1m[2023-07-10 16:25:49,516][227910] Max Reward on eval: 4133.401390162137[0m
[37m[1m[2023-07-10 16:25:49,516][227910] Min Reward on eval: 4133.401390162137[0m
[37m[1m[2023-07-10 16:25:49,516][227910] Mean Reward across all agents: 4133.401390162137[0m
[37m[1m[2023-07-10 16:25:49,516][227910] Average Trajectory Length: 963.904[0m
[36m[2023-07-10 16:25:55,065][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:25:55,066][227910] Reward + Measures: [[2745.47314738    0.2332        0.42220002    0.2933        0.21810003]
 [3525.90281861    0.20560001    0.38569999    0.26719999    0.16229999]
 [4014.62237609    0.18102083    0.39321902    0.27257448    0.14971767]
 ...
 [3861.30267063    0.19023806    0.36499205    0.26444072    0.14275222]
 [3802.90022364    0.18167007    0.38942215    0.28594247    0.15489472]
 [3506.12777163    0.17296921    0.41526365    0.2896255     0.14330624]][0m
[37m[1m[2023-07-10 16:25:55,066][227910] Max Reward on eval: 4361.2906398424875[0m
[37m[1m[2023-07-10 16:25:55,066][227910] Min Reward on eval: 157.15500986877595[0m
[37m[1m[2023-07-10 16:25:55,067][227910] Mean Reward across all agents: 3507.0037961830526[0m
[37m[1m[2023-07-10 16:25:55,067][227910] Average Trajectory Length: 923.617[0m
[36m[2023-07-10 16:25:55,068][227910] mean_value=-606.5525222758163, max_value=544.4881030276892[0m
[37m[1m[2023-07-10 16:25:55,071][227910] New mean coefficients: [[ 1.0610075  -0.3635983  -0.07472692 -0.23128283 -0.26591477]][0m
[37m[1m[2023-07-10 16:25:55,072][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:26:04,989][227910] train() took 9.92 seconds to complete[0m
[36m[2023-07-10 16:26:04,989][227910] FPS: 387266.20[0m
[36m[2023-07-10 16:26:04,992][227910] itr=827, itrs=2000, Progress: 41.35%[0m
[36m[2023-07-10 16:26:16,679][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 16:26:16,679][227910] FPS: 329058.93[0m
[36m[2023-07-10 16:26:21,424][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:26:21,424][227910] Reward + Measures: [[4139.56972117    0.18425521    0.37820956    0.27920061    0.14776607]][0m
[37m[1m[2023-07-10 16:26:21,424][227910] Max Reward on eval: 4139.569721169512[0m
[37m[1m[2023-07-10 16:26:21,424][227910] Min Reward on eval: 4139.569721169512[0m
[37m[1m[2023-07-10 16:26:21,424][227910] Mean Reward across all agents: 4139.569721169512[0m
[37m[1m[2023-07-10 16:26:21,425][227910] Average Trajectory Length: 953.7773333333333[0m
[36m[2023-07-10 16:26:26,926][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:26:26,932][227910] Reward + Measures: [[4386.76485733    0.19060002    0.37750003    0.28029999    0.1429    ]
 [4043.88732495    0.18548183    0.38008788    0.27177677    0.15304749]
 [4362.49883452    0.191         0.3757        0.2737        0.14400001]
 ...
 [3532.99550306    0.18791674    0.388165      0.28395751    0.15883219]
 [3995.34996875    0.1858875     0.39347503    0.28132501    0.13882498]
 [3790.47869676    0.19124524    0.3852571     0.26797864    0.13579983]][0m
[37m[1m[2023-07-10 16:26:26,932][227910] Max Reward on eval: 4438.907020656019[0m
[37m[1m[2023-07-10 16:26:26,932][227910] Min Reward on eval: 2583.4041733374006[0m
[37m[1m[2023-07-10 16:26:26,933][227910] Mean Reward across all agents: 4030.997069380926[0m
[37m[1m[2023-07-10 16:26:26,933][227910] Average Trajectory Length: 941.432[0m
[36m[2023-07-10 16:26:26,935][227910] mean_value=-206.8999557253015, max_value=355.1674694323301[0m
[37m[1m[2023-07-10 16:26:26,938][227910] New mean coefficients: [[ 0.5309217  -0.31151304 -0.25163445 -0.3948956  -0.1628468 ]][0m
[37m[1m[2023-07-10 16:26:26,938][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:26:36,735][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 16:26:36,735][227910] FPS: 392048.47[0m
[36m[2023-07-10 16:26:36,737][227910] itr=828, itrs=2000, Progress: 41.40%[0m
[36m[2023-07-10 16:26:48,239][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 16:26:48,240][227910] FPS: 334348.22[0m
[36m[2023-07-10 16:26:53,049][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:26:53,050][227910] Reward + Measures: [[2470.78944802    0.18959537    0.41283566    0.22382869    0.17427173]][0m
[37m[1m[2023-07-10 16:26:53,050][227910] Max Reward on eval: 2470.78944802049[0m
[37m[1m[2023-07-10 16:26:53,050][227910] Min Reward on eval: 2470.78944802049[0m
[37m[1m[2023-07-10 16:26:53,050][227910] Mean Reward across all agents: 2470.78944802049[0m
[37m[1m[2023-07-10 16:26:53,051][227910] Average Trajectory Length: 981.6186666666666[0m
[36m[2023-07-10 16:26:58,485][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:26:58,485][227910] Reward + Measures: [[2703.63490716    0.25929999    0.41920003    0.20720001    0.17309999]
 [2704.3753656     0.22281039    0.38043982    0.23758356    0.14538784]
 [2802.51863687    0.24770001    0.39250001    0.22290002    0.15880001]
 ...
 [2329.22833196    0.2818        0.39340001    0.198         0.1736    ]
 [1671.09269714    0.15900001    0.45539999    0.2586        0.1899    ]
 [2258.66585281    0.30700001    0.40089998    0.21250001    0.1735    ]][0m
[37m[1m[2023-07-10 16:26:58,485][227910] Max Reward on eval: 3079.124598124856[0m
[37m[1m[2023-07-10 16:26:58,486][227910] Min Reward on eval: 648.0565883889037[0m
[37m[1m[2023-07-10 16:26:58,486][227910] Mean Reward across all agents: 2258.77268680746[0m
[37m[1m[2023-07-10 16:26:58,486][227910] Average Trajectory Length: 952.708[0m
[36m[2023-07-10 16:26:58,489][227910] mean_value=-959.0593757736236, max_value=1449.5698000321374[0m
[37m[1m[2023-07-10 16:26:58,491][227910] New mean coefficients: [[ 0.86282367  0.1554837  -0.23268007 -0.72402084 -0.07672931]][0m
[37m[1m[2023-07-10 16:26:58,492][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:27:08,175][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 16:27:08,176][227910] FPS: 396633.02[0m
[36m[2023-07-10 16:27:08,178][227910] itr=829, itrs=2000, Progress: 41.45%[0m
[36m[2023-07-10 16:27:19,708][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 16:27:19,708][227910] FPS: 333539.35[0m
[36m[2023-07-10 16:27:24,461][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:27:24,461][227910] Reward + Measures: [[2739.36567703    0.19504198    0.39595348    0.21703306    0.17373194]][0m
[37m[1m[2023-07-10 16:27:24,462][227910] Max Reward on eval: 2739.365677025941[0m
[37m[1m[2023-07-10 16:27:24,462][227910] Min Reward on eval: 2739.365677025941[0m
[37m[1m[2023-07-10 16:27:24,462][227910] Mean Reward across all agents: 2739.365677025941[0m
[37m[1m[2023-07-10 16:27:24,462][227910] Average Trajectory Length: 984.4886666666666[0m
[36m[2023-07-10 16:27:30,053][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:27:30,058][227910] Reward + Measures: [[1080.8769128     0.23294778    0.35892639    0.20831156    0.1391211 ]
 [1679.5918009     0.27577293    0.35630193    0.23351896    0.14991455]
 [1834.83831279    0.27960935    0.3893652     0.24455233    0.17311995]
 ...
 [2470.59860853    0.22066028    0.36729595    0.20888737    0.16190635]
 [2901.1408672     0.20249999    0.38930002    0.21529999    0.1689    ]
 [1059.55685644    0.30811992    0.37052965    0.25274298    0.13831727]][0m
[37m[1m[2023-07-10 16:27:30,059][227910] Max Reward on eval: 2967.816993281199[0m
[37m[1m[2023-07-10 16:27:30,059][227910] Min Reward on eval: -534.9934345767484[0m
[37m[1m[2023-07-10 16:27:30,059][227910] Mean Reward across all agents: 2042.2326355192884[0m
[37m[1m[2023-07-10 16:27:30,059][227910] Average Trajectory Length: 880.6013333333333[0m
[36m[2023-07-10 16:27:30,061][227910] mean_value=-1428.5087231684104, max_value=939.5976985204222[0m
[37m[1m[2023-07-10 16:27:30,064][227910] New mean coefficients: [[ 0.6609399  -0.03553689 -0.19169429 -0.19742233 -0.08772225]][0m
[37m[1m[2023-07-10 16:27:30,065][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:27:39,620][227910] train() took 9.55 seconds to complete[0m
[36m[2023-07-10 16:27:39,620][227910] FPS: 401954.13[0m
[36m[2023-07-10 16:27:39,622][227910] itr=830, itrs=2000, Progress: 41.50%[0m
[37m[1m[2023-07-10 16:27:43,004][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000810[0m
[36m[2023-07-10 16:27:54,810][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 16:27:54,810][227910] FPS: 332639.89[0m
[36m[2023-07-10 16:27:59,475][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:27:59,476][227910] Reward + Measures: [[3011.21486467    0.20228219    0.38630432    0.21286169    0.17495243]][0m
[37m[1m[2023-07-10 16:27:59,476][227910] Max Reward on eval: 3011.214864667265[0m
[37m[1m[2023-07-10 16:27:59,476][227910] Min Reward on eval: 3011.214864667265[0m
[37m[1m[2023-07-10 16:27:59,477][227910] Mean Reward across all agents: 3011.214864667265[0m
[37m[1m[2023-07-10 16:27:59,477][227910] Average Trajectory Length: 987.7976666666666[0m
[36m[2023-07-10 16:28:04,888][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:28:04,889][227910] Reward + Measures: [[2373.04728939    0.21070002    0.35840002    0.2132        0.18570001]
 [2667.76961354    0.20110002    0.37890002    0.2181        0.17220001]
 [2906.96914733    0.19271468    0.3781206     0.21844342    0.17033462]
 ...
 [2997.54764848    0.193         0.37279996    0.2234        0.17030001]
 [2042.20189863    0.23860002    0.3450942     0.22967683    0.20026521]
 [3061.74012807    0.21420002    0.3811        0.2201        0.17290001]][0m
[37m[1m[2023-07-10 16:28:04,889][227910] Max Reward on eval: 3224.7125293787567[0m
[37m[1m[2023-07-10 16:28:04,889][227910] Min Reward on eval: 1331.7488525412277[0m
[37m[1m[2023-07-10 16:28:04,890][227910] Mean Reward across all agents: 2775.03710084566[0m
[37m[1m[2023-07-10 16:28:04,890][227910] Average Trajectory Length: 969.1443333333333[0m
[36m[2023-07-10 16:28:04,893][227910] mean_value=-171.4186694386021, max_value=544.5056160073959[0m
[37m[1m[2023-07-10 16:28:04,896][227910] New mean coefficients: [[ 1.9711118  -0.06326842  0.11341625 -0.2905507  -0.1743527 ]][0m
[37m[1m[2023-07-10 16:28:04,897][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:28:14,531][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 16:28:14,531][227910] FPS: 398664.74[0m
[36m[2023-07-10 16:28:14,533][227910] itr=831, itrs=2000, Progress: 41.55%[0m
[36m[2023-07-10 16:28:26,094][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 16:28:26,095][227910] FPS: 332678.39[0m
[36m[2023-07-10 16:28:30,909][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:28:30,909][227910] Reward + Measures: [[3203.12784938    0.20015499    0.38966796    0.21172886    0.17725335]][0m
[37m[1m[2023-07-10 16:28:30,909][227910] Max Reward on eval: 3203.1278493804857[0m
[37m[1m[2023-07-10 16:28:30,909][227910] Min Reward on eval: 3203.1278493804857[0m
[37m[1m[2023-07-10 16:28:30,910][227910] Mean Reward across all agents: 3203.1278493804857[0m
[37m[1m[2023-07-10 16:28:30,910][227910] Average Trajectory Length: 989.3209999999999[0m
[36m[2023-07-10 16:28:36,611][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:28:36,611][227910] Reward + Measures: [[3249.92756015    0.20610002    0.40310001    0.2093        0.17560001]
 [3121.14955627    0.20539999    0.37760001    0.2132        0.18049999]
 [2955.17011238    0.1962        0.41980001    0.23319998    0.1772    ]
 ...
 [2499.72791931    0.19350001    0.3845        0.25440001    0.18160002]
 [3018.43135839    0.20469999    0.3743        0.20180002    0.1778    ]
 [3009.50502802    0.19140001    0.35440001    0.20210002    0.18040001]][0m
[37m[1m[2023-07-10 16:28:36,611][227910] Max Reward on eval: 3316.792401383072[0m
[37m[1m[2023-07-10 16:28:36,612][227910] Min Reward on eval: 1729.4593031616764[0m
[37m[1m[2023-07-10 16:28:36,612][227910] Mean Reward across all agents: 2940.3689482172267[0m
[37m[1m[2023-07-10 16:28:36,612][227910] Average Trajectory Length: 990.4536666666667[0m
[36m[2023-07-10 16:28:36,615][227910] mean_value=-251.7023390544479, max_value=1351.985137189351[0m
[37m[1m[2023-07-10 16:28:36,618][227910] New mean coefficients: [[ 1.6268673  -0.10339473  0.32055098 -0.02948269 -0.23533593]][0m
[37m[1m[2023-07-10 16:28:36,619][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:28:46,514][227910] train() took 9.89 seconds to complete[0m
[36m[2023-07-10 16:28:46,515][227910] FPS: 388105.65[0m
[36m[2023-07-10 16:28:46,517][227910] itr=832, itrs=2000, Progress: 41.60%[0m
[36m[2023-07-10 16:28:58,101][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 16:28:58,101][227910] FPS: 332086.92[0m
[36m[2023-07-10 16:29:02,837][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:29:02,843][227910] Reward + Measures: [[3443.76310777    0.19745398    0.38742119    0.21778975    0.17857458]][0m
[37m[1m[2023-07-10 16:29:02,843][227910] Max Reward on eval: 3443.7631077706988[0m
[37m[1m[2023-07-10 16:29:02,843][227910] Min Reward on eval: 3443.7631077706988[0m
[37m[1m[2023-07-10 16:29:02,843][227910] Mean Reward across all agents: 3443.7631077706988[0m
[37m[1m[2023-07-10 16:29:02,844][227910] Average Trajectory Length: 980.351[0m
[36m[2023-07-10 16:29:08,277][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:29:08,282][227910] Reward + Measures: [[2871.78816179    0.1723        0.33879995    0.21809998    0.15619999]
 [2138.89384067    0.2467        0.49450001    0.22260001    0.25169998]
 [3525.24585915    0.19579999    0.37620002    0.22620001    0.17780001]
 ...
 [2510.49267228    0.24159999    0.47860003    0.2165        0.223     ]
 [2222.49212129    0.16806217    0.30811563    0.251147      0.16753374]
 [2419.77506874    0.2289        0.47269997    0.22239999    0.22570001]][0m
[37m[1m[2023-07-10 16:29:08,283][227910] Max Reward on eval: 3579.7243321187793[0m
[37m[1m[2023-07-10 16:29:08,283][227910] Min Reward on eval: 1028.5042590891244[0m
[37m[1m[2023-07-10 16:29:08,283][227910] Mean Reward across all agents: 2841.0379019821485[0m
[37m[1m[2023-07-10 16:29:08,283][227910] Average Trajectory Length: 963.4913333333333[0m
[36m[2023-07-10 16:29:08,287][227910] mean_value=-224.74898602283176, max_value=1014.1939896097945[0m
[37m[1m[2023-07-10 16:29:08,289][227910] New mean coefficients: [[ 1.2580192  -0.01288141  0.7412752  -0.17314272 -0.41690218]][0m
[37m[1m[2023-07-10 16:29:08,290][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:29:18,223][227910] train() took 9.93 seconds to complete[0m
[36m[2023-07-10 16:29:18,223][227910] FPS: 386673.86[0m
[36m[2023-07-10 16:29:18,225][227910] itr=833, itrs=2000, Progress: 41.65%[0m
[36m[2023-07-10 16:29:29,902][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 16:29:29,903][227910] FPS: 329342.03[0m
[36m[2023-07-10 16:29:34,641][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:29:34,641][227910] Reward + Measures: [[3581.20989162    0.19873179    0.3841674     0.21442939    0.17838971]][0m
[37m[1m[2023-07-10 16:29:34,641][227910] Max Reward on eval: 3581.2098916157333[0m
[37m[1m[2023-07-10 16:29:34,642][227910] Min Reward on eval: 3581.2098916157333[0m
[37m[1m[2023-07-10 16:29:34,642][227910] Mean Reward across all agents: 3581.2098916157333[0m
[37m[1m[2023-07-10 16:29:34,642][227910] Average Trajectory Length: 983.5649999999999[0m
[36m[2023-07-10 16:29:40,044][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:29:40,049][227910] Reward + Measures: [[3432.69353383    0.19219999    0.42799997    0.21510001    0.17909999]
 [3443.91525478    0.1952        0.39560005    0.21170001    0.1758    ]
 [3086.80441079    0.19353805    0.43481499    0.21191378    0.17912833]
 ...
 [3560.95022425    0.20650001    0.3888        0.205         0.1753    ]
 [3293.99292497    0.19149999    0.4147        0.1947        0.1772    ]
 [2939.53868999    0.18209998    0.41520005    0.207         0.17620002]][0m
[37m[1m[2023-07-10 16:29:40,050][227910] Max Reward on eval: 3758.5122982164844[0m
[37m[1m[2023-07-10 16:29:40,050][227910] Min Reward on eval: 2540.234319023718[0m
[37m[1m[2023-07-10 16:29:40,050][227910] Mean Reward across all agents: 3379.866210504971[0m
[37m[1m[2023-07-10 16:29:40,050][227910] Average Trajectory Length: 977.3153333333333[0m
[36m[2023-07-10 16:29:40,054][227910] mean_value=86.64181928644425, max_value=1246.1801076028623[0m
[37m[1m[2023-07-10 16:29:40,057][227910] New mean coefficients: [[ 1.6739868  -0.2075944   0.8594325   0.13438283 -0.33882153]][0m
[37m[1m[2023-07-10 16:29:40,058][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:29:49,807][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 16:29:49,807][227910] FPS: 393962.63[0m
[36m[2023-07-10 16:29:49,809][227910] itr=834, itrs=2000, Progress: 41.70%[0m
[36m[2023-07-10 16:30:01,438][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 16:30:01,438][227910] FPS: 330759.18[0m
[36m[2023-07-10 16:30:05,946][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:30:05,946][227910] Reward + Measures: [[3704.67821331    0.20005926    0.38650391    0.21811767    0.17830729]][0m
[37m[1m[2023-07-10 16:30:05,946][227910] Max Reward on eval: 3704.6782133054808[0m
[37m[1m[2023-07-10 16:30:05,946][227910] Min Reward on eval: 3704.6782133054808[0m
[37m[1m[2023-07-10 16:30:05,947][227910] Mean Reward across all agents: 3704.6782133054808[0m
[37m[1m[2023-07-10 16:30:05,947][227910] Average Trajectory Length: 986.001[0m
[36m[2023-07-10 16:30:11,040][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:30:11,040][227910] Reward + Measures: [[3756.68177477    0.19360001    0.40219998    0.21630001    0.17920001]
 [2837.5479353     0.1596        0.33399999    0.21620002    0.18100001]
 [3400.78964638    0.19724058    0.40080148    0.22508696    0.17971449]
 ...
 [3577.78182367    0.2043        0.39809999    0.2227        0.177     ]
 [3124.44104389    0.18239999    0.37450001    0.20970002    0.1831    ]
 [3795.30362161    0.206         0.375         0.223         0.1794    ]][0m
[37m[1m[2023-07-10 16:30:11,040][227910] Max Reward on eval: 3842.239082794264[0m
[37m[1m[2023-07-10 16:30:11,041][227910] Min Reward on eval: 1627.7179438913358[0m
[37m[1m[2023-07-10 16:30:11,041][227910] Mean Reward across all agents: 3581.640560051843[0m
[37m[1m[2023-07-10 16:30:11,041][227910] Average Trajectory Length: 985.8439999999999[0m
[36m[2023-07-10 16:30:11,045][227910] mean_value=5.916875267664582, max_value=1381.5096975438871[0m
[37m[1m[2023-07-10 16:30:11,048][227910] New mean coefficients: [[ 1.7288336  -0.10639491  0.8810268   0.13780166 -0.32417452]][0m
[37m[1m[2023-07-10 16:30:11,049][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:30:20,519][227910] train() took 9.47 seconds to complete[0m
[36m[2023-07-10 16:30:20,520][227910] FPS: 405523.33[0m
[36m[2023-07-10 16:30:20,522][227910] itr=835, itrs=2000, Progress: 41.75%[0m
[36m[2023-07-10 16:30:32,032][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 16:30:32,032][227910] FPS: 334164.78[0m
[36m[2023-07-10 16:30:36,723][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:30:36,729][227910] Reward + Measures: [[3816.31748718    0.19999859    0.38867477    0.22082971    0.17789778]][0m
[37m[1m[2023-07-10 16:30:36,729][227910] Max Reward on eval: 3816.317487183083[0m
[37m[1m[2023-07-10 16:30:36,729][227910] Min Reward on eval: 3816.317487183083[0m
[37m[1m[2023-07-10 16:30:36,729][227910] Mean Reward across all agents: 3816.317487183083[0m
[37m[1m[2023-07-10 16:30:36,730][227910] Average Trajectory Length: 989.401[0m
[36m[2023-07-10 16:30:42,213][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:30:42,213][227910] Reward + Measures: [[2252.16447603    0.21973835    0.33810949    0.21953213    0.17552049]
 [2273.38942191    0.17869647    0.34670335    0.22267811    0.17450257]
 [2804.31490177    0.20210002    0.42339998    0.28490001    0.20250002]
 ...
 [1663.29421841    0.22947584    0.31419325    0.22349386    0.18289322]
 [1242.02018755    0.14639033    0.29054505    0.23844276    0.16810164]
 [3466.02166815    0.21177931    0.32056552    0.21290345    0.1744276 ]][0m
[37m[1m[2023-07-10 16:30:42,213][227910] Max Reward on eval: 3864.495724717062[0m
[37m[1m[2023-07-10 16:30:42,214][227910] Min Reward on eval: 121.49044418435078[0m
[37m[1m[2023-07-10 16:30:42,214][227910] Mean Reward across all agents: 2145.8617098812742[0m
[37m[1m[2023-07-10 16:30:42,214][227910] Average Trajectory Length: 875.8853333333333[0m
[36m[2023-07-10 16:30:42,216][227910] mean_value=-1591.8449530850003, max_value=2802.584169564582[0m
[37m[1m[2023-07-10 16:30:42,219][227910] New mean coefficients: [[ 1.5042714   0.11020619  0.73033214 -0.30839252 -0.03638035]][0m
[37m[1m[2023-07-10 16:30:42,220][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:30:51,954][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 16:30:51,954][227910] FPS: 394556.86[0m
[36m[2023-07-10 16:30:51,956][227910] itr=836, itrs=2000, Progress: 41.80%[0m
[36m[2023-07-10 16:31:03,453][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 16:31:03,453][227910] FPS: 334513.58[0m
[36m[2023-07-10 16:31:08,107][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:31:08,107][227910] Reward + Measures: [[3865.99927508    0.19796816    0.38150528    0.22052795    0.18042754]][0m
[37m[1m[2023-07-10 16:31:08,107][227910] Max Reward on eval: 3865.999275081747[0m
[37m[1m[2023-07-10 16:31:08,108][227910] Min Reward on eval: 3865.999275081747[0m
[37m[1m[2023-07-10 16:31:08,108][227910] Mean Reward across all agents: 3865.999275081747[0m
[37m[1m[2023-07-10 16:31:08,108][227910] Average Trajectory Length: 983.083[0m
[36m[2023-07-10 16:31:13,613][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:31:13,665][227910] Reward + Measures: [[3777.44357141    0.21139999    0.4233        0.2326        0.17849998]
 [3830.35692709    0.20009999    0.41809997    0.2124        0.1803    ]
 [3481.22437895    0.22620001    0.49950001    0.24590002    0.18870001]
 ...
 [3513.97725342    0.22320001    0.46809998    0.2369        0.18380001]
 [3278.14118287    0.18381503    0.42200765    0.21784432    0.17848241]
 [2544.85589554    0.17644785    0.34079808    0.22178975    0.1730818 ]][0m
[37m[1m[2023-07-10 16:31:13,666][227910] Max Reward on eval: 3992.70699404981[0m
[37m[1m[2023-07-10 16:31:13,666][227910] Min Reward on eval: 1370.0234433718492[0m
[37m[1m[2023-07-10 16:31:13,666][227910] Mean Reward across all agents: 3294.5605736986913[0m
[37m[1m[2023-07-10 16:31:13,666][227910] Average Trajectory Length: 982.6813333333333[0m
[36m[2023-07-10 16:31:13,669][227910] mean_value=-305.8144032253103, max_value=1072.7348509747558[0m
[37m[1m[2023-07-10 16:31:13,672][227910] New mean coefficients: [[ 1.1439221   0.13423717  0.8888173  -0.55806464 -0.06166996]][0m
[37m[1m[2023-07-10 16:31:13,673][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:31:23,290][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 16:31:23,290][227910] FPS: 399341.74[0m
[36m[2023-07-10 16:31:23,293][227910] itr=837, itrs=2000, Progress: 41.85%[0m
[36m[2023-07-10 16:31:35,018][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 16:31:35,019][227910] FPS: 328092.04[0m
[36m[2023-07-10 16:31:39,782][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:31:39,782][227910] Reward + Measures: [[3960.0291319     0.19772066    0.38790151    0.22047162    0.17957194]][0m
[37m[1m[2023-07-10 16:31:39,783][227910] Max Reward on eval: 3960.029131895353[0m
[37m[1m[2023-07-10 16:31:39,783][227910] Min Reward on eval: 3960.029131895353[0m
[37m[1m[2023-07-10 16:31:39,783][227910] Mean Reward across all agents: 3960.029131895353[0m
[37m[1m[2023-07-10 16:31:39,783][227910] Average Trajectory Length: 986.3343333333333[0m
[36m[2023-07-10 16:31:45,164][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:31:45,165][227910] Reward + Measures: [[2229.80993764    0.1662        0.3204        0.21660002    0.15470001]
 [1656.16845789    0.1469        0.27379999    0.1936        0.1269    ]
 [3535.14864248    0.20159999    0.41020003    0.23459999    0.17      ]
 ...
 [2740.71046722    0.17120001    0.3348        0.2349        0.15250002]
 [3613.48048299    0.1869        0.49759999    0.2395        0.2       ]
 [3099.11541897    0.2247        0.34820002    0.24789999    0.19399999]][0m
[37m[1m[2023-07-10 16:31:45,165][227910] Max Reward on eval: 3946.6944210443644[0m
[37m[1m[2023-07-10 16:31:45,165][227910] Min Reward on eval: 668.1092144465714[0m
[37m[1m[2023-07-10 16:31:45,166][227910] Mean Reward across all agents: 2731.3450364263913[0m
[37m[1m[2023-07-10 16:31:45,166][227910] Average Trajectory Length: 979.0023333333334[0m
[36m[2023-07-10 16:31:45,168][227910] mean_value=-911.1690335775611, max_value=2017.7413392697174[0m
[37m[1m[2023-07-10 16:31:45,171][227910] New mean coefficients: [[ 0.7698792  -0.07330607  0.72777224 -0.42067236 -0.06447426]][0m
[37m[1m[2023-07-10 16:31:45,172][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:31:55,089][227910] train() took 9.92 seconds to complete[0m
[36m[2023-07-10 16:31:55,090][227910] FPS: 387250.96[0m
[36m[2023-07-10 16:31:55,092][227910] itr=838, itrs=2000, Progress: 41.90%[0m
[36m[2023-07-10 16:32:06,740][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 16:32:06,740][227910] FPS: 330178.64[0m
[36m[2023-07-10 16:32:11,519][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:32:11,520][227910] Reward + Measures: [[4000.01070369    0.19699654    0.38518667    0.22325389    0.17878196]][0m
[37m[1m[2023-07-10 16:32:11,520][227910] Max Reward on eval: 4000.0107036868417[0m
[37m[1m[2023-07-10 16:32:11,520][227910] Min Reward on eval: 4000.0107036868417[0m
[37m[1m[2023-07-10 16:32:11,520][227910] Mean Reward across all agents: 4000.0107036868417[0m
[37m[1m[2023-07-10 16:32:11,521][227910] Average Trajectory Length: 978.3793333333333[0m
[36m[2023-07-10 16:32:16,964][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:32:16,965][227910] Reward + Measures: [[3921.20208566    0.18359999    0.3761        0.2422        0.18270001]
 [3881.49206974    0.2287        0.38610002    0.19509999    0.1714    ]
 [3999.55981701    0.2079        0.38779998    0.24039999    0.17330001]
 ...
 [2941.62730881    0.1805381     0.35825711    0.24501906    0.1815    ]
 [4030.96250303    0.20190001    0.40570003    0.23020001    0.17910001]
 [3819.92181816    0.19968967    0.4021897     0.25139308    0.17176206]][0m
[37m[1m[2023-07-10 16:32:16,965][227910] Max Reward on eval: 4142.037926290743[0m
[37m[1m[2023-07-10 16:32:16,965][227910] Min Reward on eval: 1079.147108914703[0m
[37m[1m[2023-07-10 16:32:16,966][227910] Mean Reward across all agents: 3677.0772942768767[0m
[37m[1m[2023-07-10 16:32:16,966][227910] Average Trajectory Length: 986.795[0m
[36m[2023-07-10 16:32:16,969][227910] mean_value=-246.38550970824713, max_value=1202.8339434497511[0m
[37m[1m[2023-07-10 16:32:16,971][227910] New mean coefficients: [[ 0.98959047  0.00095683  0.29450485 -0.68285906  0.05765964]][0m
[37m[1m[2023-07-10 16:32:16,972][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:32:26,597][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 16:32:26,597][227910] FPS: 399020.66[0m
[36m[2023-07-10 16:32:26,600][227910] itr=839, itrs=2000, Progress: 41.95%[0m
[36m[2023-07-10 16:32:38,218][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 16:32:38,218][227910] FPS: 330999.37[0m
[36m[2023-07-10 16:32:43,005][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:32:43,011][227910] Reward + Measures: [[4048.19111746    0.19482784    0.37804541    0.22585143    0.17799689]][0m
[37m[1m[2023-07-10 16:32:43,011][227910] Max Reward on eval: 4048.191117458079[0m
[37m[1m[2023-07-10 16:32:43,011][227910] Min Reward on eval: 4048.191117458079[0m
[37m[1m[2023-07-10 16:32:43,011][227910] Mean Reward across all agents: 4048.191117458079[0m
[37m[1m[2023-07-10 16:32:43,012][227910] Average Trajectory Length: 969.9993333333333[0m
[36m[2023-07-10 16:32:48,469][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:32:48,474][227910] Reward + Measures: [[1664.70337672    0.20044146    0.26084453    0.21081086    0.13195801]
 [1831.01860441    0.23038435    0.41439638    0.1933759     0.1841964 ]
 [4143.64757757    0.20610002    0.39320001    0.2168        0.1744    ]
 ...
 [2315.67068521    0.22205415    0.39784688    0.20756249    0.17686258]
 [1773.36046222    0.25028226    0.43438587    0.19812404    0.19248688]
 [3301.51977935    0.18714106    0.39145988    0.2066257     0.17092477]][0m
[37m[1m[2023-07-10 16:32:48,475][227910] Max Reward on eval: 4217.482582954503[0m
[37m[1m[2023-07-10 16:32:48,475][227910] Min Reward on eval: 824.6713173432042[0m
[37m[1m[2023-07-10 16:32:48,475][227910] Mean Reward across all agents: 3088.3647250510076[0m
[37m[1m[2023-07-10 16:32:48,475][227910] Average Trajectory Length: 920.053[0m
[36m[2023-07-10 16:32:48,477][227910] mean_value=-905.262033189888, max_value=891.2265527024219[0m
[37m[1m[2023-07-10 16:32:48,480][227910] New mean coefficients: [[ 0.8282012   0.11650945  0.2525073  -0.6895293   0.05915976]][0m
[37m[1m[2023-07-10 16:32:48,481][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:32:58,220][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 16:32:58,221][227910] FPS: 394345.14[0m
[36m[2023-07-10 16:32:58,223][227910] itr=840, itrs=2000, Progress: 42.00%[0m
[37m[1m[2023-07-10 16:33:01,767][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000820[0m
[36m[2023-07-10 16:33:13,765][227910] train() took 11.73 seconds to complete[0m
[36m[2023-07-10 16:33:13,765][227910] FPS: 327338.16[0m
[36m[2023-07-10 16:33:18,549][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:33:18,549][227910] Reward + Measures: [[4160.95808194    0.19586654    0.38774604    0.22705537    0.17585929]][0m
[37m[1m[2023-07-10 16:33:18,549][227910] Max Reward on eval: 4160.958081940323[0m
[37m[1m[2023-07-10 16:33:18,549][227910] Min Reward on eval: 4160.958081940323[0m
[37m[1m[2023-07-10 16:33:18,550][227910] Mean Reward across all agents: 4160.958081940323[0m
[37m[1m[2023-07-10 16:33:18,550][227910] Average Trajectory Length: 977.7703333333333[0m
[36m[2023-07-10 16:33:23,927][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:33:23,933][227910] Reward + Measures: [[4215.3768585     0.20250002    0.39309999    0.2296        0.17709999]
 [1482.28390396    0.14773333    0.230875      0.232825      0.16270415]
 [3481.29178611    0.17760001    0.3804        0.2184        0.17460001]
 ...
 [2431.99435117    0.18690354    0.27860495    0.22844115    0.16803475]
 [3195.04584552    0.22912116    0.32121068    0.21268408    0.16570373]
 [4164.18056317    0.2086        0.39989999    0.23080002    0.17620002]][0m
[37m[1m[2023-07-10 16:33:23,933][227910] Max Reward on eval: 4291.486851118505[0m
[37m[1m[2023-07-10 16:33:23,933][227910] Min Reward on eval: 512.2770771656214[0m
[37m[1m[2023-07-10 16:33:23,933][227910] Mean Reward across all agents: 3338.7546431693495[0m
[37m[1m[2023-07-10 16:33:23,934][227910] Average Trajectory Length: 966.384[0m
[36m[2023-07-10 16:33:23,936][227910] mean_value=-600.4717408659398, max_value=941.9697958861343[0m
[37m[1m[2023-07-10 16:33:23,939][227910] New mean coefficients: [[ 0.6121189   0.07843229  0.25604415 -0.76293695  0.26166877]][0m
[37m[1m[2023-07-10 16:33:23,940][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:33:33,737][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 16:33:33,738][227910] FPS: 391982.88[0m
[36m[2023-07-10 16:33:33,740][227910] itr=841, itrs=2000, Progress: 42.05%[0m
[36m[2023-07-10 16:33:45,231][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 16:33:45,231][227910] FPS: 334791.94[0m
[36m[2023-07-10 16:33:49,936][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:33:49,936][227910] Reward + Measures: [[4250.95301637    0.19722013    0.39109823    0.22890183    0.17559139]][0m
[37m[1m[2023-07-10 16:33:49,936][227910] Max Reward on eval: 4250.953016367696[0m
[37m[1m[2023-07-10 16:33:49,937][227910] Min Reward on eval: 4250.953016367696[0m
[37m[1m[2023-07-10 16:33:49,937][227910] Mean Reward across all agents: 4250.953016367696[0m
[37m[1m[2023-07-10 16:33:49,937][227910] Average Trajectory Length: 979.0583333333333[0m
[36m[2023-07-10 16:33:55,292][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:33:55,293][227910] Reward + Measures: [[1424.88668368    0.25816715    0.28804868    0.20714393    0.17666173]
 [1734.48976779    0.28072926    0.27524135    0.16363595    0.19833902]
 [3109.7683917     0.26402006    0.36851904    0.20009434    0.1892378 ]
 ...
 [3367.599758      0.2371989     0.39556393    0.21719728    0.19780438]
 [2778.88688998    0.2825        0.31780002    0.18969999    0.1724    ]
 [1734.4458319     0.21739529    0.29130995    0.18983535    0.17796274]][0m
[37m[1m[2023-07-10 16:33:55,293][227910] Max Reward on eval: 4368.483788478375[0m
[37m[1m[2023-07-10 16:33:55,293][227910] Min Reward on eval: 811.2740792258264[0m
[37m[1m[2023-07-10 16:33:55,294][227910] Mean Reward across all agents: 2861.5407110386573[0m
[37m[1m[2023-07-10 16:33:55,294][227910] Average Trajectory Length: 958.5926666666667[0m
[36m[2023-07-10 16:33:55,297][227910] mean_value=-570.8025228827755, max_value=2792.7706805920457[0m
[37m[1m[2023-07-10 16:33:55,300][227910] New mean coefficients: [[ 0.75657755 -0.09006952  0.65762776 -0.5965249   0.10630408]][0m
[37m[1m[2023-07-10 16:33:55,301][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:34:04,975][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 16:34:04,976][227910] FPS: 396972.61[0m
[36m[2023-07-10 16:34:04,978][227910] itr=842, itrs=2000, Progress: 42.10%[0m
[36m[2023-07-10 16:34:16,549][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 16:34:16,549][227910] FPS: 332447.09[0m
[36m[2023-07-10 16:34:21,334][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:34:21,334][227910] Reward + Measures: [[4321.17263422    0.19539426    0.39943179    0.22625598    0.17597103]][0m
[37m[1m[2023-07-10 16:34:21,334][227910] Max Reward on eval: 4321.17263421965[0m
[37m[1m[2023-07-10 16:34:21,334][227910] Min Reward on eval: 4321.17263421965[0m
[37m[1m[2023-07-10 16:34:21,335][227910] Mean Reward across all agents: 4321.17263421965[0m
[37m[1m[2023-07-10 16:34:21,335][227910] Average Trajectory Length: 977.7076666666667[0m
[36m[2023-07-10 16:34:27,048][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:34:27,053][227910] Reward + Measures: [[4108.93489234    0.1921        0.43990001    0.29909998    0.17290001]
 [2095.49339268    0.26599649    0.34677371    0.25255176    0.14351587]
 [3751.65247959    0.2113        0.42940003    0.26190004    0.17760001]
 ...
 [3705.64136537    0.19860001    0.4542        0.29889998    0.17279999]
 [3854.65470262    0.1839        0.43420002    0.27519998    0.1767    ]
 [2453.10935366    0.1734        0.46689996    0.38970003    0.2167    ]][0m
[37m[1m[2023-07-10 16:34:27,054][227910] Max Reward on eval: 4508.57163658468[0m
[37m[1m[2023-07-10 16:34:27,054][227910] Min Reward on eval: 488.45811566103947[0m
[37m[1m[2023-07-10 16:34:27,054][227910] Mean Reward across all agents: 3116.806375709311[0m
[37m[1m[2023-07-10 16:34:27,055][227910] Average Trajectory Length: 976.8166666666666[0m
[36m[2023-07-10 16:34:27,057][227910] mean_value=-910.1253490473326, max_value=939.7489764958054[0m
[37m[1m[2023-07-10 16:34:27,059][227910] New mean coefficients: [[ 0.7746458  -0.19195198  0.69303966 -0.64013696  0.14635198]][0m
[37m[1m[2023-07-10 16:34:27,060][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:34:36,794][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 16:34:36,794][227910] FPS: 394585.67[0m
[36m[2023-07-10 16:34:36,796][227910] itr=843, itrs=2000, Progress: 42.15%[0m
[36m[2023-07-10 16:34:48,410][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 16:34:48,410][227910] FPS: 331171.38[0m
[36m[2023-07-10 16:34:53,049][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:34:53,050][227910] Reward + Measures: [[4373.66914957    0.19514421    0.40391958    0.22771376    0.17572503]][0m
[37m[1m[2023-07-10 16:34:53,050][227910] Max Reward on eval: 4373.669149568131[0m
[37m[1m[2023-07-10 16:34:53,050][227910] Min Reward on eval: 4373.669149568131[0m
[37m[1m[2023-07-10 16:34:53,050][227910] Mean Reward across all agents: 4373.669149568131[0m
[37m[1m[2023-07-10 16:34:53,050][227910] Average Trajectory Length: 976.6619999999999[0m
[36m[2023-07-10 16:34:58,398][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:34:58,399][227910] Reward + Measures: [[2169.00658905    0.17960425    0.44203421    0.18767126    0.17580321]
 [1947.77675722    0.18561904    0.47902185    0.21180081    0.202191  ]
 [ 984.47736921    0.13959999    0.56100005    0.20650001    0.38890001]
 ...
 [1863.97480886    0.22239418    0.49818459    0.19144845    0.22120419]
 [3613.22614167    0.21900001    0.4436        0.2467        0.1814    ]
 [2685.54640753    0.21440001    0.56690007    0.26360002    0.23169999]][0m
[37m[1m[2023-07-10 16:34:58,399][227910] Max Reward on eval: 4332.490411867574[0m
[37m[1m[2023-07-10 16:34:58,400][227910] Min Reward on eval: -27.444370830547996[0m
[37m[1m[2023-07-10 16:34:58,400][227910] Mean Reward across all agents: 2178.847493878753[0m
[37m[1m[2023-07-10 16:34:58,400][227910] Average Trajectory Length: 970.2326666666667[0m
[36m[2023-07-10 16:34:58,405][227910] mean_value=-115.06300693936815, max_value=2830.3553108122846[0m
[37m[1m[2023-07-10 16:34:58,407][227910] New mean coefficients: [[ 0.5084535  -0.30683416  0.50829387 -0.91083264  0.11477839]][0m
[37m[1m[2023-07-10 16:34:58,408][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:35:08,149][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 16:35:08,149][227910] FPS: 394307.72[0m
[36m[2023-07-10 16:35:08,151][227910] itr=844, itrs=2000, Progress: 42.20%[0m
[36m[2023-07-10 16:35:19,759][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 16:35:19,760][227910] FPS: 331305.13[0m
[36m[2023-07-10 16:35:24,210][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:35:24,211][227910] Reward + Measures: [[4490.34534424    0.19357836    0.41219795    0.22767366    0.17422836]][0m
[37m[1m[2023-07-10 16:35:24,211][227910] Max Reward on eval: 4490.3453442387345[0m
[37m[1m[2023-07-10 16:35:24,211][227910] Min Reward on eval: 4490.3453442387345[0m
[37m[1m[2023-07-10 16:35:24,211][227910] Mean Reward across all agents: 4490.3453442387345[0m
[37m[1m[2023-07-10 16:35:24,211][227910] Average Trajectory Length: 987.0266666666666[0m
[36m[2023-07-10 16:35:29,337][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:35:29,337][227910] Reward + Measures: [[1914.85198455    0.19319868    0.288719      0.16927221    0.15671615]
 [1751.9883299     0.23176531    0.31977245    0.16214871    0.16552891]
 [3309.04922244    0.17984763    0.32554457    0.18562727    0.17546539]
 ...
 [4096.27245227    0.21872067    0.38794321    0.21918562    0.16817515]
 [2676.34141661    0.2168        0.35600001    0.1699        0.1638    ]
 [2200.53269995    0.2313087     0.38063294    0.17335775    0.17770623]][0m
[37m[1m[2023-07-10 16:35:29,337][227910] Max Reward on eval: 4567.2622898461295[0m
[37m[1m[2023-07-10 16:35:29,338][227910] Min Reward on eval: 19.863356242419105[0m
[37m[1m[2023-07-10 16:35:29,338][227910] Mean Reward across all agents: 2851.427712230258[0m
[37m[1m[2023-07-10 16:35:29,338][227910] Average Trajectory Length: 889.5966666666667[0m
[36m[2023-07-10 16:35:29,341][227910] mean_value=-879.3007296402836, max_value=989.7192784829274[0m
[37m[1m[2023-07-10 16:35:29,343][227910] New mean coefficients: [[ 0.05479276 -0.26514804  0.17602149 -1.0310626   0.18513599]][0m
[37m[1m[2023-07-10 16:35:29,344][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:35:38,586][227910] train() took 9.24 seconds to complete[0m
[36m[2023-07-10 16:35:38,586][227910] FPS: 415592.87[0m
[36m[2023-07-10 16:35:38,588][227910] itr=845, itrs=2000, Progress: 42.25%[0m
[36m[2023-07-10 16:35:50,076][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 16:35:50,076][227910] FPS: 334819.49[0m
[36m[2023-07-10 16:35:54,802][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:35:54,803][227910] Reward + Measures: [[4520.55009235    0.19477905    0.42009807    0.22073784    0.17489001]][0m
[37m[1m[2023-07-10 16:35:54,803][227910] Max Reward on eval: 4520.550092346619[0m
[37m[1m[2023-07-10 16:35:54,803][227910] Min Reward on eval: 4520.550092346619[0m
[37m[1m[2023-07-10 16:35:54,804][227910] Mean Reward across all agents: 4520.550092346619[0m
[37m[1m[2023-07-10 16:35:54,804][227910] Average Trajectory Length: 987.9549999999999[0m
[36m[2023-07-10 16:36:00,247][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:36:00,248][227910] Reward + Measures: [[4294.6013711     0.20246871    0.41435716    0.2173789     0.16994695]
 [3950.06405996    0.18990003    0.39490002    0.2622        0.19819999]
 [3460.65736702    0.1911        0.49429998    0.2568        0.22860001]
 ...
 [3948.69019964    0.20519999    0.41560003    0.24800001    0.18789999]
 [3456.35715394    0.22264956    0.32122067    0.19192719    0.15963209]
 [3920.19976093    0.21140002    0.39120004    0.2263        0.15410002]][0m
[37m[1m[2023-07-10 16:36:00,248][227910] Max Reward on eval: 4589.917057216912[0m
[37m[1m[2023-07-10 16:36:00,248][227910] Min Reward on eval: 655.6810875653551[0m
[37m[1m[2023-07-10 16:36:00,248][227910] Mean Reward across all agents: 3056.240990287188[0m
[37m[1m[2023-07-10 16:36:00,249][227910] Average Trajectory Length: 959.5713333333333[0m
[36m[2023-07-10 16:36:00,251][227910] mean_value=-601.7905358298086, max_value=1906.411576473592[0m
[37m[1m[2023-07-10 16:36:00,254][227910] New mean coefficients: [[ 0.31991723 -0.2532645   0.3065586  -1.0324905   0.39268833]][0m
[37m[1m[2023-07-10 16:36:00,255][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:36:09,937][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 16:36:09,938][227910] FPS: 396661.93[0m
[36m[2023-07-10 16:36:09,940][227910] itr=846, itrs=2000, Progress: 42.30%[0m
[36m[2023-07-10 16:36:21,623][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 16:36:21,623][227910] FPS: 329162.40[0m
[36m[2023-07-10 16:36:26,531][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:36:26,532][227910] Reward + Measures: [[4557.89661305    0.19177832    0.42364344    0.20893398    0.17798474]][0m
[37m[1m[2023-07-10 16:36:26,532][227910] Max Reward on eval: 4557.896613046355[0m
[37m[1m[2023-07-10 16:36:26,532][227910] Min Reward on eval: 4557.896613046355[0m
[37m[1m[2023-07-10 16:36:26,532][227910] Mean Reward across all agents: 4557.896613046355[0m
[37m[1m[2023-07-10 16:36:26,532][227910] Average Trajectory Length: 986.6933333333333[0m
[36m[2023-07-10 16:36:32,106][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:36:32,107][227910] Reward + Measures: [[3360.2022782     0.22459999    0.37399998    0.23650001    0.17580001]
 [2876.45397437    0.2150379     0.3236075     0.2194802     0.17010075]
 [2833.61955138    0.20320001    0.37620002    0.24679999    0.18410002]
 ...
 [2139.08110921    0.18923011    0.3296093     0.24323049    0.16155057]
 [4057.85632711    0.20809999    0.36299998    0.22830001    0.1716    ]
 [1680.74975906    0.17969422    0.27033937    0.22739828    0.15361218]][0m
[37m[1m[2023-07-10 16:36:32,107][227910] Max Reward on eval: 4522.280023685843[0m
[37m[1m[2023-07-10 16:36:32,107][227910] Min Reward on eval: 69.5408101898909[0m
[37m[1m[2023-07-10 16:36:32,107][227910] Mean Reward across all agents: 2173.061545054754[0m
[37m[1m[2023-07-10 16:36:32,108][227910] Average Trajectory Length: 952.0803333333333[0m
[36m[2023-07-10 16:36:32,109][227910] mean_value=-1691.2777370778651, max_value=661.4403190616985[0m
[37m[1m[2023-07-10 16:36:32,112][227910] New mean coefficients: [[ 0.48632568 -0.20979045  0.20164055 -0.6404203   0.31978983]][0m
[37m[1m[2023-07-10 16:36:32,112][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:36:42,010][227910] train() took 9.90 seconds to complete[0m
[36m[2023-07-10 16:36:42,011][227910] FPS: 388023.27[0m
[36m[2023-07-10 16:36:42,013][227910] itr=847, itrs=2000, Progress: 42.35%[0m
[36m[2023-07-10 16:36:53,785][227910] train() took 11.75 seconds to complete[0m
[36m[2023-07-10 16:36:53,785][227910] FPS: 326725.97[0m
[36m[2023-07-10 16:36:58,541][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:36:58,541][227910] Reward + Measures: [[4557.71363864    0.19138402    0.42646959    0.20400374    0.17697829]][0m
[37m[1m[2023-07-10 16:36:58,541][227910] Max Reward on eval: 4557.713638644598[0m
[37m[1m[2023-07-10 16:36:58,542][227910] Min Reward on eval: 4557.713638644598[0m
[37m[1m[2023-07-10 16:36:58,542][227910] Mean Reward across all agents: 4557.713638644598[0m
[37m[1m[2023-07-10 16:36:58,542][227910] Average Trajectory Length: 981.6926666666666[0m
[36m[2023-07-10 16:37:03,951][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:37:03,951][227910] Reward + Measures: [[2732.48995393    0.18772832    0.42022586    0.22199397    0.18231784]
 [3359.60552413    0.19300614    0.4212527     0.22133501    0.17248224]
 [4147.37099698    0.1933627     0.40762982    0.22056738    0.17046848]
 ...
 [1804.12873144    0.1786        0.47729999    0.28979999    0.20469999]
 [2415.48148829    0.15032116    0.33664843    0.21900988    0.17078748]
 [1751.53063917    0.17667337    0.39874417    0.2275888     0.17748401]][0m
[37m[1m[2023-07-10 16:37:03,952][227910] Max Reward on eval: 4714.262320001935[0m
[37m[1m[2023-07-10 16:37:03,952][227910] Min Reward on eval: 738.9176761059862[0m
[37m[1m[2023-07-10 16:37:03,952][227910] Mean Reward across all agents: 3418.2490156327062[0m
[37m[1m[2023-07-10 16:37:03,952][227910] Average Trajectory Length: 892.495[0m
[36m[2023-07-10 16:37:03,955][227910] mean_value=-859.8682436326598, max_value=452.52160905760957[0m
[37m[1m[2023-07-10 16:37:03,957][227910] New mean coefficients: [[ 0.7282349  -0.28749934  0.3283623  -0.17422158  0.15688387]][0m
[37m[1m[2023-07-10 16:37:03,958][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:37:13,730][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 16:37:13,730][227910] FPS: 393032.77[0m
[36m[2023-07-10 16:37:13,733][227910] itr=848, itrs=2000, Progress: 42.40%[0m
[36m[2023-07-10 16:37:25,380][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 16:37:25,380][227910] FPS: 330178.70[0m
[36m[2023-07-10 16:37:30,137][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:37:30,138][227910] Reward + Measures: [[4626.46198359    0.19124125    0.42346716    0.20857251    0.17868054]][0m
[37m[1m[2023-07-10 16:37:30,138][227910] Max Reward on eval: 4626.461983593972[0m
[37m[1m[2023-07-10 16:37:30,138][227910] Min Reward on eval: 4626.461983593972[0m
[37m[1m[2023-07-10 16:37:30,138][227910] Mean Reward across all agents: 4626.461983593972[0m
[37m[1m[2023-07-10 16:37:30,138][227910] Average Trajectory Length: 988.0356666666667[0m
[36m[2023-07-10 16:37:35,565][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:37:35,566][227910] Reward + Measures: [[2694.47805874    0.1743        0.52899998    0.30710003    0.22120002]
 [3993.76190863    0.2152317     0.38831052    0.21370617    0.17825443]
 [1307.73725108    0.09411844    0.17866649    0.16785868    0.19455306]
 ...
 [4270.77014716    0.20250002    0.46059999    0.20480001    0.1825    ]
 [2813.8338053     0.17449999    0.55520004    0.2861        0.20820001]
 [2330.63623532    0.23239999    0.34540007    0.17129999    0.2201    ]][0m
[37m[1m[2023-07-10 16:37:35,566][227910] Max Reward on eval: 4621.724523959495[0m
[37m[1m[2023-07-10 16:37:35,566][227910] Min Reward on eval: 133.8986194736295[0m
[37m[1m[2023-07-10 16:37:35,567][227910] Mean Reward across all agents: 2813.73695489465[0m
[37m[1m[2023-07-10 16:37:35,567][227910] Average Trajectory Length: 957.7036666666667[0m
[36m[2023-07-10 16:37:35,570][227910] mean_value=-648.1840751550251, max_value=2781.8541386132592[0m
[37m[1m[2023-07-10 16:37:35,572][227910] New mean coefficients: [[ 0.987497   -0.38477173  0.4065706   0.07414255  0.18163985]][0m
[37m[1m[2023-07-10 16:37:35,573][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:37:45,204][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 16:37:45,204][227910] FPS: 398796.41[0m
[36m[2023-07-10 16:37:45,206][227910] itr=849, itrs=2000, Progress: 42.45%[0m
[36m[2023-07-10 16:37:56,765][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 16:37:56,765][227910] FPS: 332710.87[0m
[36m[2023-07-10 16:38:01,570][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:38:01,570][227910] Reward + Measures: [[4645.65257856    0.18606572    0.42489812    0.20025094    0.18061222]][0m
[37m[1m[2023-07-10 16:38:01,571][227910] Max Reward on eval: 4645.652578559972[0m
[37m[1m[2023-07-10 16:38:01,571][227910] Min Reward on eval: 4645.652578559972[0m
[37m[1m[2023-07-10 16:38:01,571][227910] Mean Reward across all agents: 4645.652578559972[0m
[37m[1m[2023-07-10 16:38:01,572][227910] Average Trajectory Length: 984.901[0m
[36m[2023-07-10 16:38:07,013][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:38:07,014][227910] Reward + Measures: [[3635.62576154    0.21059573    0.48669916    0.22797692    0.143747  ]
 [4099.18788515    0.21064691    0.4021062     0.20415309    0.18856543]
 [3742.50899223    0.198         0.39159998    0.1946        0.17639999]
 ...
 [3734.47900917    0.20555103    0.39284527    0.21264063    0.18533584]
 [4426.38502378    0.204         0.45129997    0.22190002    0.1655    ]
 [4349.16419901    0.19845484    0.45682237    0.21090274    0.16030955]][0m
[37m[1m[2023-07-10 16:38:07,014][227910] Max Reward on eval: 4773.088571520569[0m
[37m[1m[2023-07-10 16:38:07,014][227910] Min Reward on eval: 1325.1297710166705[0m
[37m[1m[2023-07-10 16:38:07,015][227910] Mean Reward across all agents: 3760.772191115299[0m
[37m[1m[2023-07-10 16:38:07,015][227910] Average Trajectory Length: 958.1446666666666[0m
[36m[2023-07-10 16:38:07,017][227910] mean_value=-633.1187843245909, max_value=903.443576743879[0m
[37m[1m[2023-07-10 16:38:07,019][227910] New mean coefficients: [[ 0.93964046 -0.34638232  0.3013122   0.07996048  0.01289627]][0m
[37m[1m[2023-07-10 16:38:07,020][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:38:16,719][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 16:38:16,720][227910] FPS: 395984.22[0m
[36m[2023-07-10 16:38:16,722][227910] itr=850, itrs=2000, Progress: 42.50%[0m
[37m[1m[2023-07-10 16:38:20,121][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000830[0m
[36m[2023-07-10 16:38:31,947][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 16:38:31,947][227910] FPS: 331976.12[0m
[36m[2023-07-10 16:38:36,653][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:38:36,654][227910] Reward + Measures: [[4686.32901396    0.18697068    0.42002878    0.20046903    0.17947762]][0m
[37m[1m[2023-07-10 16:38:36,654][227910] Max Reward on eval: 4686.329013961925[0m
[37m[1m[2023-07-10 16:38:36,654][227910] Min Reward on eval: 4686.329013961925[0m
[37m[1m[2023-07-10 16:38:36,654][227910] Mean Reward across all agents: 4686.329013961925[0m
[37m[1m[2023-07-10 16:38:36,654][227910] Average Trajectory Length: 979.5746666666666[0m
[36m[2023-07-10 16:38:42,275][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:38:42,275][227910] Reward + Measures: [[4226.10095443    0.19291607    0.38735914    0.21497254    0.17380546]
 [3935.79853503    0.19641277    0.39173105    0.23114172    0.1750953 ]
 [4340.74004516    0.18118873    0.4248831     0.20946479    0.18082254]
 ...
 [3252.99454471    0.20694546    0.44080219    0.23607603    0.21304142]
 [4355.00944812    0.19069281    0.39701656    0.22932158    0.17348346]
 [4181.96703383    0.18593907    0.37880278    0.21939917    0.16325903]][0m
[37m[1m[2023-07-10 16:38:42,275][227910] Max Reward on eval: 4841.649637858756[0m
[37m[1m[2023-07-10 16:38:42,276][227910] Min Reward on eval: 2640.9149965896736[0m
[37m[1m[2023-07-10 16:38:42,276][227910] Mean Reward across all agents: 4298.218233616973[0m
[37m[1m[2023-07-10 16:38:42,276][227910] Average Trajectory Length: 958.9033333333333[0m
[36m[2023-07-10 16:38:42,279][227910] mean_value=-227.14437971762706, max_value=556.3392734000345[0m
[37m[1m[2023-07-10 16:38:42,282][227910] New mean coefficients: [[ 0.42374504 -0.4176283   0.16142605  0.0197961   0.1724435 ]][0m
[37m[1m[2023-07-10 16:38:42,283][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:38:51,992][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 16:38:51,993][227910] FPS: 395553.76[0m
[36m[2023-07-10 16:38:51,995][227910] itr=851, itrs=2000, Progress: 42.55%[0m
[36m[2023-07-10 16:39:03,511][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 16:39:03,511][227910] FPS: 333950.98[0m
[36m[2023-07-10 16:39:08,212][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:39:08,212][227910] Reward + Measures: [[4756.29519599    0.18592317    0.41838032    0.20607114    0.18121073]][0m
[37m[1m[2023-07-10 16:39:08,212][227910] Max Reward on eval: 4756.2951959859565[0m
[37m[1m[2023-07-10 16:39:08,212][227910] Min Reward on eval: 4756.2951959859565[0m
[37m[1m[2023-07-10 16:39:08,213][227910] Mean Reward across all agents: 4756.2951959859565[0m
[37m[1m[2023-07-10 16:39:08,213][227910] Average Trajectory Length: 986.1663333333333[0m
[36m[2023-07-10 16:39:13,632][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:39:13,633][227910] Reward + Measures: [[1852.76427317    0.18752597    0.42973739    0.19310708    0.19206193]
 [2762.06847387    0.17239361    0.41062242    0.18737783    0.18083461]
 [3287.58235774    0.17164846    0.44346485    0.20511012    0.19320475]
 ...
 [1197.3189481     0.1908744     0.36784199    0.21170965    0.20363931]
 [2441.76849363    0.17767306    0.36940488    0.28107172    0.16775683]
 [1948.24797128    0.18036659    0.38421643    0.20804088    0.19988428]][0m
[37m[1m[2023-07-10 16:39:13,633][227910] Max Reward on eval: 4720.6988654540855[0m
[37m[1m[2023-07-10 16:39:13,633][227910] Min Reward on eval: 135.13096810810967[0m
[37m[1m[2023-07-10 16:39:13,634][227910] Mean Reward across all agents: 2690.562460442655[0m
[37m[1m[2023-07-10 16:39:13,634][227910] Average Trajectory Length: 821.8969999999999[0m
[36m[2023-07-10 16:39:13,636][227910] mean_value=-1559.6749218925077, max_value=2244.714928135447[0m
[37m[1m[2023-07-10 16:39:13,638][227910] New mean coefficients: [[ 0.43854204 -0.31029522  0.38401708 -0.14267555  0.18659517]][0m
[37m[1m[2023-07-10 16:39:13,639][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:39:23,298][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 16:39:23,298][227910] FPS: 397642.78[0m
[36m[2023-07-10 16:39:23,300][227910] itr=852, itrs=2000, Progress: 42.60%[0m
[36m[2023-07-10 16:39:34,949][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 16:39:34,949][227910] FPS: 330167.66[0m
[36m[2023-07-10 16:39:39,743][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:39:39,743][227910] Reward + Measures: [[4810.87127295    0.18534957    0.42625347    0.20081542    0.18497139]][0m
[37m[1m[2023-07-10 16:39:39,744][227910] Max Reward on eval: 4810.87127295188[0m
[37m[1m[2023-07-10 16:39:39,744][227910] Min Reward on eval: 4810.87127295188[0m
[37m[1m[2023-07-10 16:39:39,744][227910] Mean Reward across all agents: 4810.87127295188[0m
[37m[1m[2023-07-10 16:39:39,744][227910] Average Trajectory Length: 992.199[0m
[36m[2023-07-10 16:39:45,208][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:39:45,208][227910] Reward + Measures: [[2348.33709771    0.13547352    0.28963304    0.26721656    0.19120514]
 [2645.04762626    0.16758832    0.30004272    0.26533905    0.14146383]
 [ 896.46039612    0.08786666    0.15634055    0.17112975    0.13692072]
 ...
 [ 641.33044309    0.13165377    0.24563439    0.17462657    0.0979936 ]
 [1685.97862496    0.18190001    0.30040002    0.19700001    0.21230002]
 [3781.12330306    0.20349999    0.42749998    0.23699999    0.20039999]][0m
[37m[1m[2023-07-10 16:39:45,209][227910] Max Reward on eval: 4735.277381099016[0m
[37m[1m[2023-07-10 16:39:45,209][227910] Min Reward on eval: 47.89709032069368[0m
[37m[1m[2023-07-10 16:39:45,209][227910] Mean Reward across all agents: 2169.974047297076[0m
[37m[1m[2023-07-10 16:39:45,209][227910] Average Trajectory Length: 915.6073333333333[0m
[36m[2023-07-10 16:39:45,212][227910] mean_value=-918.5446164553896, max_value=1381.787379669055[0m
[37m[1m[2023-07-10 16:39:45,215][227910] New mean coefficients: [[ 0.4130465  -0.46918184  0.38018775 -0.15588507  0.20255467]][0m
[37m[1m[2023-07-10 16:39:45,216][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:39:54,978][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 16:39:54,979][227910] FPS: 393414.35[0m
[36m[2023-07-10 16:39:54,981][227910] itr=853, itrs=2000, Progress: 42.65%[0m
[36m[2023-07-10 16:40:06,546][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 16:40:06,546][227910] FPS: 332543.30[0m
[36m[2023-07-10 16:40:11,220][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:40:11,221][227910] Reward + Measures: [[4852.85281829    0.18372992    0.42669147    0.19527961    0.18579781]][0m
[37m[1m[2023-07-10 16:40:11,221][227910] Max Reward on eval: 4852.852818292441[0m
[37m[1m[2023-07-10 16:40:11,221][227910] Min Reward on eval: 4852.852818292441[0m
[37m[1m[2023-07-10 16:40:11,221][227910] Mean Reward across all agents: 4852.852818292441[0m
[37m[1m[2023-07-10 16:40:11,222][227910] Average Trajectory Length: 988.6953333333333[0m
[36m[2023-07-10 16:40:16,618][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:40:16,619][227910] Reward + Measures: [[3136.80833       0.24538676    0.36249125    0.23662601    0.1614909 ]
 [4196.4433037     0.2172        0.36690003    0.19489999    0.19580001]
 [1858.17199739    0.33653995    0.37605259    0.19934919    0.13399613]
 ...
 [2910.30386366    0.1988        0.40400001    0.1935        0.17      ]
 [2951.41850756    0.2247        0.45809999    0.19810002    0.21280001]
 [3915.70759012    0.22400001    0.40330002    0.24690001    0.1937    ]][0m
[37m[1m[2023-07-10 16:40:16,619][227910] Max Reward on eval: 4777.636466919119[0m
[37m[1m[2023-07-10 16:40:16,619][227910] Min Reward on eval: 824.1354225760675[0m
[37m[1m[2023-07-10 16:40:16,619][227910] Mean Reward across all agents: 3242.157120507083[0m
[37m[1m[2023-07-10 16:40:16,620][227910] Average Trajectory Length: 935.9813333333333[0m
[36m[2023-07-10 16:40:16,622][227910] mean_value=-1117.9561013114755, max_value=2325.9312448153933[0m
[37m[1m[2023-07-10 16:40:16,624][227910] New mean coefficients: [[ 0.47007835 -0.31315032  0.27204287 -0.08144321  0.22586687]][0m
[37m[1m[2023-07-10 16:40:16,625][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:40:26,265][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 16:40:26,265][227910] FPS: 398411.08[0m
[36m[2023-07-10 16:40:26,268][227910] itr=854, itrs=2000, Progress: 42.70%[0m
[36m[2023-07-10 16:40:37,783][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 16:40:37,784][227910] FPS: 333963.39[0m
[36m[2023-07-10 16:40:42,489][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:40:42,489][227910] Reward + Measures: [[4881.44769019    0.18440087    0.42203704    0.19615039    0.18537536]][0m
[37m[1m[2023-07-10 16:40:42,489][227910] Max Reward on eval: 4881.447690193581[0m
[37m[1m[2023-07-10 16:40:42,490][227910] Min Reward on eval: 4881.447690193581[0m
[37m[1m[2023-07-10 16:40:42,490][227910] Mean Reward across all agents: 4881.447690193581[0m
[37m[1m[2023-07-10 16:40:42,490][227910] Average Trajectory Length: 983.6143333333333[0m
[36m[2023-07-10 16:40:48,064][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:40:48,065][227910] Reward + Measures: [[4516.55378672    0.1913        0.41249999    0.23360001    0.17980002]
 [4864.38649068    0.1926        0.41889998    0.211         0.17910001]
 [4738.47914184    0.18120001    0.42090002    0.1825        0.18319999]
 ...
 [4695.77421254    0.1869        0.42480001    0.19950001    0.17580001]
 [2701.6618466     0.19573839    0.36091837    0.16955839    0.17264023]
 [4207.55849503    0.18669425    0.42511961    0.18010557    0.18043625]][0m
[37m[1m[2023-07-10 16:40:48,065][227910] Max Reward on eval: 4933.277436503209[0m
[37m[1m[2023-07-10 16:40:48,065][227910] Min Reward on eval: 876.4003201403436[0m
[37m[1m[2023-07-10 16:40:48,065][227910] Mean Reward across all agents: 3781.5995006115713[0m
[37m[1m[2023-07-10 16:40:48,066][227910] Average Trajectory Length: 966.5686666666667[0m
[36m[2023-07-10 16:40:48,068][227910] mean_value=-550.2336056515559, max_value=1870.6345162997627[0m
[37m[1m[2023-07-10 16:40:48,071][227910] New mean coefficients: [[ 0.43839052 -0.34567598  0.3127732   0.26641452  0.15255795]][0m
[37m[1m[2023-07-10 16:40:48,072][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:40:57,767][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 16:40:57,767][227910] FPS: 396148.67[0m
[36m[2023-07-10 16:40:57,769][227910] itr=855, itrs=2000, Progress: 42.75%[0m
[36m[2023-07-10 16:41:09,328][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 16:41:09,329][227910] FPS: 332709.08[0m
[36m[2023-07-10 16:41:13,890][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:41:13,891][227910] Reward + Measures: [[4962.19300239    0.18798484    0.42978746    0.20162231    0.18163705]][0m
[37m[1m[2023-07-10 16:41:13,891][227910] Max Reward on eval: 4962.193002394877[0m
[37m[1m[2023-07-10 16:41:13,891][227910] Min Reward on eval: 4962.193002394877[0m
[37m[1m[2023-07-10 16:41:13,891][227910] Mean Reward across all agents: 4962.193002394877[0m
[37m[1m[2023-07-10 16:41:13,892][227910] Average Trajectory Length: 991.2453333333333[0m
[36m[2023-07-10 16:41:19,207][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:41:19,208][227910] Reward + Measures: [[4990.76492729    0.18769999    0.43620005    0.2036        0.18080001]
 [4912.57797834    0.1913        0.41949996    0.2158        0.18550001]
 [4882.63573757    0.1925        0.43490002    0.22320001    0.17819999]
 ...
 [4920.20461562    0.1894        0.43969998    0.2119        0.1778    ]
 [4886.82818938    0.1928        0.41459998    0.2148        0.18380001]
 [4587.91431051    0.1744        0.38439998    0.1948        0.1884    ]][0m
[37m[1m[2023-07-10 16:41:19,208][227910] Max Reward on eval: 5081.82021654984[0m
[37m[1m[2023-07-10 16:41:19,208][227910] Min Reward on eval: 4396.420222839341[0m
[37m[1m[2023-07-10 16:41:19,209][227910] Mean Reward across all agents: 4871.896810119262[0m
[37m[1m[2023-07-10 16:41:19,209][227910] Average Trajectory Length: 994.6793333333333[0m
[36m[2023-07-10 16:41:19,214][227910] mean_value=90.2471508743874, max_value=434.7448578700323[0m
[37m[1m[2023-07-10 16:41:19,217][227910] New mean coefficients: [[ 0.42004213 -0.15707897  0.33196855  0.16857654  0.1257416 ]][0m
[37m[1m[2023-07-10 16:41:19,218][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:41:28,838][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 16:41:28,838][227910] FPS: 399267.42[0m
[36m[2023-07-10 16:41:28,840][227910] itr=856, itrs=2000, Progress: 42.80%[0m
[36m[2023-07-10 16:41:40,406][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 16:41:40,406][227910] FPS: 332549.52[0m
[36m[2023-07-10 16:41:45,180][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:41:45,180][227910] Reward + Measures: [[5034.69452172    0.18556693    0.42704371    0.19963582    0.18650216]][0m
[37m[1m[2023-07-10 16:41:45,180][227910] Max Reward on eval: 5034.694521716765[0m
[37m[1m[2023-07-10 16:41:45,181][227910] Min Reward on eval: 5034.694521716765[0m
[37m[1m[2023-07-10 16:41:45,181][227910] Mean Reward across all agents: 5034.694521716765[0m
[37m[1m[2023-07-10 16:41:45,181][227910] Average Trajectory Length: 995.4356666666666[0m
[36m[2023-07-10 16:41:50,201][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:41:50,202][227910] Reward + Measures: [[3255.12401035    0.16610001    0.4526        0.25530002    0.18359999]
 [4277.13936092    0.17749958    0.43846962    0.2218093     0.18689831]
 [3731.10099541    0.17330001    0.44689998    0.27560002    0.1857    ]
 ...
 [3570.19873044    0.19752161    0.33207497    0.18460467    0.17739902]
 [4722.15937431    0.18621081    0.43165872    0.2253201     0.18068348]
 [2591.65753056    0.16080001    0.29850003    0.26079997    0.1904    ]][0m
[37m[1m[2023-07-10 16:41:50,202][227910] Max Reward on eval: 4953.310388509184[0m
[37m[1m[2023-07-10 16:41:50,202][227910] Min Reward on eval: 924.47845839652[0m
[37m[1m[2023-07-10 16:41:50,202][227910] Mean Reward across all agents: 3645.735428300842[0m
[37m[1m[2023-07-10 16:41:50,203][227910] Average Trajectory Length: 964.2646666666666[0m
[36m[2023-07-10 16:41:50,205][227910] mean_value=-790.0519754364046, max_value=1074.5556808535384[0m
[37m[1m[2023-07-10 16:41:50,207][227910] New mean coefficients: [[ 0.3789981  -0.00551222  0.22260256  0.06556658  0.17303222]][0m
[37m[1m[2023-07-10 16:41:50,208][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:41:59,927][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 16:41:59,927][227910] FPS: 395213.11[0m
[36m[2023-07-10 16:41:59,929][227910] itr=857, itrs=2000, Progress: 42.85%[0m
[36m[2023-07-10 16:42:11,576][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 16:42:11,576][227910] FPS: 330196.41[0m
[36m[2023-07-10 16:42:16,350][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:42:16,351][227910] Reward + Measures: [[5084.073813      0.18134332    0.42728344    0.19482817    0.18410103]][0m
[37m[1m[2023-07-10 16:42:16,351][227910] Max Reward on eval: 5084.0738129961155[0m
[37m[1m[2023-07-10 16:42:16,351][227910] Min Reward on eval: 5084.0738129961155[0m
[37m[1m[2023-07-10 16:42:16,351][227910] Mean Reward across all agents: 5084.0738129961155[0m
[37m[1m[2023-07-10 16:42:16,351][227910] Average Trajectory Length: 990.9966666666667[0m
[36m[2023-07-10 16:42:21,934][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:42:21,935][227910] Reward + Measures: [[1884.1546952     0.20953332    0.39367831    0.21259716    0.16909692]
 [3883.56826924    0.2185        0.46650001    0.27860001    0.1311    ]
 [3398.41035737    0.24730001    0.48320004    0.26040003    0.0964    ]
 ...
 [3164.27271969    0.24460001    0.3867        0.1955        0.19840001]
 [3313.78033937    0.19564581    0.4962883     0.21052869    0.15773839]
 [1434.99115206    0.12159999    0.32430002    0.17760001    0.11390001]][0m
[37m[1m[2023-07-10 16:42:21,935][227910] Max Reward on eval: 5085.460306735336[0m
[37m[1m[2023-07-10 16:42:21,935][227910] Min Reward on eval: 535.4644176475762[0m
[37m[1m[2023-07-10 16:42:21,936][227910] Mean Reward across all agents: 3298.187112532988[0m
[37m[1m[2023-07-10 16:42:21,936][227910] Average Trajectory Length: 925.3159999999999[0m
[36m[2023-07-10 16:42:21,939][227910] mean_value=-685.7234267034348, max_value=3340.6449501999596[0m
[37m[1m[2023-07-10 16:42:21,942][227910] New mean coefficients: [[ 0.35692835  0.09429534  0.54491127 -0.06314147  0.21371105]][0m
[37m[1m[2023-07-10 16:42:21,943][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:42:31,584][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 16:42:31,584][227910] FPS: 398392.51[0m
[36m[2023-07-10 16:42:31,586][227910] itr=858, itrs=2000, Progress: 42.90%[0m
[36m[2023-07-10 16:42:43,339][227910] train() took 11.73 seconds to complete[0m
[36m[2023-07-10 16:42:43,340][227910] FPS: 327283.65[0m
[36m[2023-07-10 16:42:48,179][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:42:48,179][227910] Reward + Measures: [[5144.69361182    0.18869767    0.43306786    0.19199963    0.18773302]][0m
[37m[1m[2023-07-10 16:42:48,180][227910] Max Reward on eval: 5144.693611821162[0m
[37m[1m[2023-07-10 16:42:48,180][227910] Min Reward on eval: 5144.693611821162[0m
[37m[1m[2023-07-10 16:42:48,180][227910] Mean Reward across all agents: 5144.693611821162[0m
[37m[1m[2023-07-10 16:42:48,180][227910] Average Trajectory Length: 993.569[0m
[36m[2023-07-10 16:42:53,563][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:42:53,564][227910] Reward + Measures: [[1636.70706036    0.1269        0.26289999    0.20510001    0.1737    ]
 [4544.23929768    0.20630002    0.45730001    0.22290002    0.18099999]
 [3764.58268605    0.23369999    0.45620003    0.22650002    0.1779    ]
 ...
 [1496.94879       0.1866        0.32300001    0.2088        0.1664    ]
 [2781.94662668    0.19839934    0.37366468    0.19085522    0.16131482]
 [2137.09305156    0.14100002    0.3409        0.2217        0.21679997]][0m
[37m[1m[2023-07-10 16:42:53,564][227910] Max Reward on eval: 4981.1802979802715[0m
[37m[1m[2023-07-10 16:42:53,565][227910] Min Reward on eval: 1018.6644282021647[0m
[37m[1m[2023-07-10 16:42:53,565][227910] Mean Reward across all agents: 3292.840531718485[0m
[37m[1m[2023-07-10 16:42:53,565][227910] Average Trajectory Length: 973.199[0m
[36m[2023-07-10 16:42:53,567][227910] mean_value=-1224.8678072216208, max_value=1799.2265120046748[0m
[37m[1m[2023-07-10 16:42:53,569][227910] New mean coefficients: [[ 0.4342051  -0.08054519  0.4583936   0.09363091  0.21517825]][0m
[37m[1m[2023-07-10 16:42:53,571][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:43:03,305][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 16:43:03,306][227910] FPS: 394527.48[0m
[36m[2023-07-10 16:43:03,308][227910] itr=859, itrs=2000, Progress: 42.95%[0m
[36m[2023-07-10 16:43:14,888][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 16:43:14,889][227910] FPS: 332082.77[0m
[36m[2023-07-10 16:43:19,624][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:43:19,624][227910] Reward + Measures: [[5171.792195      0.18718781    0.42945316    0.19190422    0.18667899]][0m
[37m[1m[2023-07-10 16:43:19,625][227910] Max Reward on eval: 5171.792194996173[0m
[37m[1m[2023-07-10 16:43:19,625][227910] Min Reward on eval: 5171.792194996173[0m
[37m[1m[2023-07-10 16:43:19,625][227910] Mean Reward across all agents: 5171.792194996173[0m
[37m[1m[2023-07-10 16:43:19,625][227910] Average Trajectory Length: 995.122[0m
[36m[2023-07-10 16:43:25,330][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:43:25,330][227910] Reward + Measures: [[1791.55898934    0.30898666    0.37292525    0.20579593    0.18240787]
 [2229.79198005    0.17267647    0.34363529    0.19411765    0.18325882]
 [2982.32616767    0.16765542    0.39914224    0.21384692    0.20492053]
 ...
 [2678.84131248    0.21900001    0.36129999    0.20079999    0.16710001]
 [2893.69632365    0.1969        0.37030002    0.17440002    0.17840001]
 [2706.54732382    0.18500002    0.47940001    0.2462        0.26700002]][0m
[37m[1m[2023-07-10 16:43:25,330][227910] Max Reward on eval: 5075.159563156218[0m
[37m[1m[2023-07-10 16:43:25,331][227910] Min Reward on eval: 659.2530323889398[0m
[37m[1m[2023-07-10 16:43:25,331][227910] Mean Reward across all agents: 3128.4630765234497[0m
[37m[1m[2023-07-10 16:43:25,331][227910] Average Trajectory Length: 952.6626666666666[0m
[36m[2023-07-10 16:43:25,333][227910] mean_value=-1380.0600538196784, max_value=1358.2697111017487[0m
[37m[1m[2023-07-10 16:43:25,335][227910] New mean coefficients: [[ 0.25190324  0.09596014  0.07392615 -0.15099034  0.19162859]][0m
[37m[1m[2023-07-10 16:43:25,336][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:43:35,252][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 16:43:35,253][227910] FPS: 387317.77[0m
[36m[2023-07-10 16:43:35,255][227910] itr=860, itrs=2000, Progress: 43.00%[0m
[37m[1m[2023-07-10 16:43:38,723][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000840[0m
[36m[2023-07-10 16:43:50,468][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 16:43:50,468][227910] FPS: 334521.94[0m
[36m[2023-07-10 16:43:55,180][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:43:55,180][227910] Reward + Measures: [[5198.18996085    0.18660441    0.42749682    0.19120613    0.18664703]][0m
[37m[1m[2023-07-10 16:43:55,181][227910] Max Reward on eval: 5198.189960845937[0m
[37m[1m[2023-07-10 16:43:55,181][227910] Min Reward on eval: 5198.189960845937[0m
[37m[1m[2023-07-10 16:43:55,181][227910] Mean Reward across all agents: 5198.189960845937[0m
[37m[1m[2023-07-10 16:43:55,181][227910] Average Trajectory Length: 993.7303333333333[0m
[36m[2023-07-10 16:44:00,667][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:44:00,667][227910] Reward + Measures: [[4202.3415996     0.2431        0.4571        0.23099999    0.15840001]
 [3635.90533448    0.20093334    0.34592566    0.2140564     0.19312565]
 [4191.18088367    0.19230001    0.42640001    0.31350002    0.16579999]
 ...
 [3112.78120468    0.1947        0.4686        0.2323        0.2146    ]
 [2230.58453821    0.16108356    0.32085416    0.20416956    0.18381432]
 [3986.65064027    0.18109897    0.38219073    0.2821773     0.21437629]][0m
[37m[1m[2023-07-10 16:44:00,667][227910] Max Reward on eval: 5087.5385687381495[0m
[37m[1m[2023-07-10 16:44:00,668][227910] Min Reward on eval: 879.8290444297279[0m
[37m[1m[2023-07-10 16:44:00,668][227910] Mean Reward across all agents: 3338.883194900233[0m
[37m[1m[2023-07-10 16:44:00,668][227910] Average Trajectory Length: 962.4986666666666[0m
[36m[2023-07-10 16:44:00,670][227910] mean_value=-1034.466610667606, max_value=1284.3598244338887[0m
[37m[1m[2023-07-10 16:44:00,672][227910] New mean coefficients: [[ 0.23261389  0.14434314  0.03269956 -0.20365426  0.23410779]][0m
[37m[1m[2023-07-10 16:44:00,673][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:44:10,411][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 16:44:10,411][227910] FPS: 394416.48[0m
[36m[2023-07-10 16:44:10,413][227910] itr=861, itrs=2000, Progress: 43.05%[0m
[36m[2023-07-10 16:44:22,075][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 16:44:22,076][227910] FPS: 329882.49[0m
[36m[2023-07-10 16:44:26,923][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:44:26,924][227910] Reward + Measures: [[5305.79527322    0.18639176    0.43329439    0.18380481    0.18808548]][0m
[37m[1m[2023-07-10 16:44:26,924][227910] Max Reward on eval: 5305.795273215571[0m
[37m[1m[2023-07-10 16:44:26,924][227910] Min Reward on eval: 5305.795273215571[0m
[37m[1m[2023-07-10 16:44:26,924][227910] Mean Reward across all agents: 5305.795273215571[0m
[37m[1m[2023-07-10 16:44:26,924][227910] Average Trajectory Length: 994.7316666666667[0m
[36m[2023-07-10 16:44:32,479][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:44:32,480][227910] Reward + Measures: [[4086.89500643    0.18321185    0.45555171    0.23524265    0.18372242]
 [3218.13969502    0.17394848    0.52797318    0.25184005    0.22381309]
 [3733.64588897    0.1734        0.46720001    0.26930001    0.19320001]
 ...
 [3318.80332851    0.17          0.47550002    0.26610002    0.1909    ]
 [5051.37177247    0.19390002    0.41549999    0.19159999    0.1904    ]
 [3336.12277445    0.20566101    0.45251188    0.20687795    0.19839494]][0m
[37m[1m[2023-07-10 16:44:32,480][227910] Max Reward on eval: 5096.971217864751[0m
[37m[1m[2023-07-10 16:44:32,480][227910] Min Reward on eval: 426.23585237308873[0m
[37m[1m[2023-07-10 16:44:32,481][227910] Mean Reward across all agents: 3319.5253138949033[0m
[37m[1m[2023-07-10 16:44:32,481][227910] Average Trajectory Length: 961.9599999999999[0m
[36m[2023-07-10 16:44:32,483][227910] mean_value=-933.9213844804087, max_value=1956.257955865186[0m
[37m[1m[2023-07-10 16:44:32,485][227910] New mean coefficients: [[ 0.02427813  0.08355372  0.1330191  -0.07668839  0.28533924]][0m
[37m[1m[2023-07-10 16:44:32,486][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:44:42,343][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 16:44:42,344][227910] FPS: 389623.57[0m
[36m[2023-07-10 16:44:42,346][227910] itr=862, itrs=2000, Progress: 43.10%[0m
[36m[2023-07-10 16:44:54,106][227910] train() took 11.74 seconds to complete[0m
[36m[2023-07-10 16:44:54,106][227910] FPS: 327031.06[0m
[36m[2023-07-10 16:44:58,967][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:44:58,967][227910] Reward + Measures: [[5276.71102666    0.18494309    0.42855269    0.18851487    0.18953025]][0m
[37m[1m[2023-07-10 16:44:58,967][227910] Max Reward on eval: 5276.711026655515[0m
[37m[1m[2023-07-10 16:44:58,968][227910] Min Reward on eval: 5276.711026655515[0m
[37m[1m[2023-07-10 16:44:58,968][227910] Mean Reward across all agents: 5276.711026655515[0m
[37m[1m[2023-07-10 16:44:58,968][227910] Average Trajectory Length: 995.06[0m
[36m[2023-07-10 16:45:04,456][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:45:04,457][227910] Reward + Measures: [[3990.52725464    0.2167        0.37359998    0.2527        0.2036    ]
 [3532.96739519    0.1829        0.46550003    0.23509999    0.24270001]
 [2300.20020287    0.17910001    0.50060004    0.2818        0.27950001]
 ...
 [3744.92824848    0.1786        0.37490001    0.21870001    0.21429999]
 [2675.41851427    0.16860001    0.53369999    0.233         0.25750002]
 [3132.73007739    0.15690002    0.48860002    0.23699999    0.24239998]][0m
[37m[1m[2023-07-10 16:45:04,457][227910] Max Reward on eval: 5272.024709938048[0m
[37m[1m[2023-07-10 16:45:04,457][227910] Min Reward on eval: 341.6386012270086[0m
[37m[1m[2023-07-10 16:45:04,457][227910] Mean Reward across all agents: 3308.6520706522647[0m
[37m[1m[2023-07-10 16:45:04,457][227910] Average Trajectory Length: 978.2783333333333[0m
[36m[2023-07-10 16:45:04,459][227910] mean_value=-843.5524197432908, max_value=1574.663859015472[0m
[37m[1m[2023-07-10 16:45:04,462][227910] New mean coefficients: [[0.06781475 0.02718082 0.1545326  0.13853829 0.19461498]][0m
[37m[1m[2023-07-10 16:45:04,463][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:45:14,359][227910] train() took 9.89 seconds to complete[0m
[36m[2023-07-10 16:45:14,359][227910] FPS: 388099.43[0m
[36m[2023-07-10 16:45:14,361][227910] itr=863, itrs=2000, Progress: 43.15%[0m
[36m[2023-07-10 16:45:26,110][227910] train() took 11.73 seconds to complete[0m
[36m[2023-07-10 16:45:26,111][227910] FPS: 327312.90[0m
[36m[2023-07-10 16:45:30,953][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:45:30,954][227910] Reward + Measures: [[5307.37047982    0.18473805    0.4337303     0.18557461    0.19093013]][0m
[37m[1m[2023-07-10 16:45:30,954][227910] Max Reward on eval: 5307.3704798244835[0m
[37m[1m[2023-07-10 16:45:30,954][227910] Min Reward on eval: 5307.3704798244835[0m
[37m[1m[2023-07-10 16:45:30,954][227910] Mean Reward across all agents: 5307.3704798244835[0m
[37m[1m[2023-07-10 16:45:30,955][227910] Average Trajectory Length: 996.177[0m
[36m[2023-07-10 16:45:36,412][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:45:36,413][227910] Reward + Measures: [[1941.84863526    0.25669125    0.41190609    0.19706525    0.19677585]
 [1999.35548862    0.22122434    0.34938493    0.21393183    0.22571734]
 [3784.32363142    0.2095294     0.4177978     0.1979721     0.17916492]
 ...
 [1286.07452273    0.26459351    0.28163052    0.20223692    0.17338014]
 [4581.90558007    0.1953        0.46240002    0.21530001    0.199     ]
 [2794.49118708    0.1847        0.53789997    0.2297        0.26110002]][0m
[37m[1m[2023-07-10 16:45:36,413][227910] Max Reward on eval: 5256.032777094841[0m
[37m[1m[2023-07-10 16:45:36,413][227910] Min Reward on eval: 308.37452184770956[0m
[37m[1m[2023-07-10 16:45:36,414][227910] Mean Reward across all agents: 3230.987629817784[0m
[37m[1m[2023-07-10 16:45:36,414][227910] Average Trajectory Length: 942.1903333333333[0m
[36m[2023-07-10 16:45:36,416][227910] mean_value=-1027.6138366655293, max_value=2462.8139596907117[0m
[37m[1m[2023-07-10 16:45:36,419][227910] New mean coefficients: [[-0.09941521 -0.10401346  0.10797118  0.2327533   0.21408008]][0m
[37m[1m[2023-07-10 16:45:36,420][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:45:46,191][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 16:45:46,192][227910] FPS: 393038.95[0m
[36m[2023-07-10 16:45:46,194][227910] itr=864, itrs=2000, Progress: 43.20%[0m
[36m[2023-07-10 16:45:57,843][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 16:45:57,843][227910] FPS: 330245.14[0m
[36m[2023-07-10 16:46:02,674][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:46:02,674][227910] Reward + Measures: [[2545.79495722    0.23484753    0.33912194    0.24705327    0.17694885]][0m
[37m[1m[2023-07-10 16:46:02,674][227910] Max Reward on eval: 2545.7949572227158[0m
[37m[1m[2023-07-10 16:46:02,675][227910] Min Reward on eval: 2545.7949572227158[0m
[37m[1m[2023-07-10 16:46:02,675][227910] Mean Reward across all agents: 2545.7949572227158[0m
[37m[1m[2023-07-10 16:46:02,675][227910] Average Trajectory Length: 999.587[0m
[36m[2023-07-10 16:46:08,175][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:46:08,176][227910] Reward + Measures: [[ 759.63641515    0.09909999    0.17569999    0.19660001    0.22360002]
 [1812.27538555    0.17659999    0.40600005    0.28060001    0.22239999]
 [1666.09307246    0.31126925    0.38829133    0.29864714    0.23032157]
 ...
 [2684.9262127     0.26120001    0.35910001    0.26830003    0.16650002]
 [2320.76069769    0.27110001    0.33250004    0.30130002    0.19059999]
 [2106.18510335    0.21780001    0.34040004    0.2771        0.17689998]][0m
[37m[1m[2023-07-10 16:46:08,176][227910] Max Reward on eval: 2894.636983113736[0m
[37m[1m[2023-07-10 16:46:08,176][227910] Min Reward on eval: 722.0653819467058[0m
[37m[1m[2023-07-10 16:46:08,176][227910] Mean Reward across all agents: 2218.4616116618927[0m
[37m[1m[2023-07-10 16:46:08,177][227910] Average Trajectory Length: 996.6713333333333[0m
[36m[2023-07-10 16:46:08,178][227910] mean_value=-1903.1600179544448, max_value=822.7689669505271[0m
[37m[1m[2023-07-10 16:46:08,180][227910] New mean coefficients: [[ 0.1277949  -0.01874869  0.07707331  0.3109308   0.09197187]][0m
[37m[1m[2023-07-10 16:46:08,181][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:46:17,941][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 16:46:17,941][227910] FPS: 393534.07[0m
[36m[2023-07-10 16:46:17,943][227910] itr=865, itrs=2000, Progress: 43.25%[0m
[36m[2023-07-10 16:46:29,505][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 16:46:29,506][227910] FPS: 332621.24[0m
[36m[2023-07-10 16:46:34,154][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:46:34,154][227910] Reward + Measures: [[2663.54920221    0.24828261    0.34836271    0.25687313    0.17735897]][0m
[37m[1m[2023-07-10 16:46:34,154][227910] Max Reward on eval: 2663.5492022082935[0m
[37m[1m[2023-07-10 16:46:34,154][227910] Min Reward on eval: 2663.5492022082935[0m
[37m[1m[2023-07-10 16:46:34,155][227910] Mean Reward across all agents: 2663.5492022082935[0m
[37m[1m[2023-07-10 16:46:34,155][227910] Average Trajectory Length: 999.454[0m
[36m[2023-07-10 16:46:39,476][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:46:39,481][227910] Reward + Measures: [[2422.18263132    0.24440001    0.31889999    0.31580001    0.17809999]
 [2043.95552998    0.20010002    0.28049999    0.2825        0.16980001]
 [1729.23862431    0.20910001    0.292         0.271         0.16700001]
 ...
 [1338.28245703    0.13609999    0.19630001    0.2333        0.1569    ]
 [ 822.38445521    0.1593        0.20170002    0.23899999    0.134     ]
 [2543.9812696     0.27329999    0.3998        0.287         0.18050002]][0m
[37m[1m[2023-07-10 16:46:39,482][227910] Max Reward on eval: 2807.637378702173[0m
[37m[1m[2023-07-10 16:46:39,482][227910] Min Reward on eval: 48.44876412323792[0m
[37m[1m[2023-07-10 16:46:39,482][227910] Mean Reward across all agents: 2065.1878239900993[0m
[37m[1m[2023-07-10 16:46:39,483][227910] Average Trajectory Length: 995.6816666666666[0m
[36m[2023-07-10 16:46:39,484][227910] mean_value=-1665.4017731150632, max_value=820.6732709531102[0m
[37m[1m[2023-07-10 16:46:39,487][227910] New mean coefficients: [[ 0.2573082  -0.04232072  0.35324246  0.43454108  0.05754742]][0m
[37m[1m[2023-07-10 16:46:39,487][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:46:49,194][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 16:46:49,194][227910] FPS: 395690.88[0m
[36m[2023-07-10 16:46:49,196][227910] itr=866, itrs=2000, Progress: 43.30%[0m
[36m[2023-07-10 16:47:00,846][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 16:47:00,846][227910] FPS: 330114.81[0m
[36m[2023-07-10 16:47:05,650][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:47:05,650][227910] Reward + Measures: [[2805.18883733    0.25134009    0.35650399    0.25814566    0.17803019]][0m
[37m[1m[2023-07-10 16:47:05,650][227910] Max Reward on eval: 2805.1888373297948[0m
[37m[1m[2023-07-10 16:47:05,650][227910] Min Reward on eval: 2805.1888373297948[0m
[37m[1m[2023-07-10 16:47:05,650][227910] Mean Reward across all agents: 2805.1888373297948[0m
[37m[1m[2023-07-10 16:47:05,651][227910] Average Trajectory Length: 999.726[0m
[36m[2023-07-10 16:47:11,330][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:47:11,331][227910] Reward + Measures: [[ 156.67178357    0.07970001    0.1128        0.25760004    0.1921    ]
 [2415.73483862    0.24609999    0.47250006    0.30790001    0.20019999]
 [2443.30426894    0.2053        0.33010003    0.2538        0.1908    ]
 ...
 [2455.15091449    0.24299999    0.30900002    0.30590001    0.1839    ]
 [2370.34921299    0.26799998    0.33870003    0.26165       0.16645001]
 [1720.54901408    0.20379999    0.3046        0.32529998    0.212     ]][0m
[37m[1m[2023-07-10 16:47:11,331][227910] Max Reward on eval: 3195.7995880488074[0m
[37m[1m[2023-07-10 16:47:11,331][227910] Min Reward on eval: -268.9473227703071[0m
[37m[1m[2023-07-10 16:47:11,331][227910] Mean Reward across all agents: 1888.071055523027[0m
[37m[1m[2023-07-10 16:47:11,332][227910] Average Trajectory Length: 996.0193333333333[0m
[36m[2023-07-10 16:47:11,333][227910] mean_value=-1320.9166350209625, max_value=1119.6717712247198[0m
[37m[1m[2023-07-10 16:47:11,336][227910] New mean coefficients: [[ 0.45093077 -0.09587623  0.567366    0.22229351  0.22587068]][0m
[37m[1m[2023-07-10 16:47:11,337][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:47:21,170][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 16:47:21,170][227910] FPS: 390597.58[0m
[36m[2023-07-10 16:47:21,172][227910] itr=867, itrs=2000, Progress: 43.35%[0m
[36m[2023-07-10 16:47:32,742][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 16:47:32,743][227910] FPS: 332412.48[0m
[36m[2023-07-10 16:47:37,603][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:47:37,604][227910] Reward + Measures: [[3002.96281828    0.25415471    0.37387908    0.25127614    0.17682865]][0m
[37m[1m[2023-07-10 16:47:37,604][227910] Max Reward on eval: 3002.962818276292[0m
[37m[1m[2023-07-10 16:47:37,604][227910] Min Reward on eval: 3002.962818276292[0m
[37m[1m[2023-07-10 16:47:37,604][227910] Mean Reward across all agents: 3002.962818276292[0m
[37m[1m[2023-07-10 16:47:37,605][227910] Average Trajectory Length: 999.6143333333333[0m
[36m[2023-07-10 16:47:43,016][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:47:43,016][227910] Reward + Measures: [[1233.46827995    0.27700001    0.29809999    0.25440001    0.1691    ]
 [1514.51023531    0.17736773    0.29579154    0.20096032    0.15027301]
 [3061.49836937    0.23270002    0.3788        0.24160002    0.16870001]
 ...
 [3122.66545585    0.24330001    0.38669997    0.23310001    0.1638    ]
 [ 724.28247568    0.19601177    0.23239915    0.20865043    0.1175647 ]
 [2040.95728221    0.27340001    0.34010002    0.25479999    0.16129999]][0m
[37m[1m[2023-07-10 16:47:43,016][227910] Max Reward on eval: 3163.478871537908[0m
[37m[1m[2023-07-10 16:47:43,017][227910] Min Reward on eval: -76.75344126435229[0m
[37m[1m[2023-07-10 16:47:43,017][227910] Mean Reward across all agents: 1897.8044464830239[0m
[37m[1m[2023-07-10 16:47:43,017][227910] Average Trajectory Length: 979.5166666666667[0m
[36m[2023-07-10 16:47:43,019][227910] mean_value=-1920.7789925487848, max_value=980.7790483393788[0m
[37m[1m[2023-07-10 16:47:43,021][227910] New mean coefficients: [[0.32887465 0.16154788 0.55692434 0.36007088 0.26889533]][0m
[37m[1m[2023-07-10 16:47:43,022][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:47:52,677][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 16:47:52,677][227910] FPS: 397796.83[0m
[36m[2023-07-10 16:47:52,680][227910] itr=868, itrs=2000, Progress: 43.40%[0m
[36m[2023-07-10 16:48:04,307][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 16:48:04,307][227910] FPS: 330877.93[0m
[36m[2023-07-10 16:48:09,118][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:48:09,118][227910] Reward + Measures: [[3180.93267526    0.25329304    0.39521056    0.24425292    0.17661096]][0m
[37m[1m[2023-07-10 16:48:09,118][227910] Max Reward on eval: 3180.9326752636903[0m
[37m[1m[2023-07-10 16:48:09,118][227910] Min Reward on eval: 3180.9326752636903[0m
[37m[1m[2023-07-10 16:48:09,119][227910] Mean Reward across all agents: 3180.9326752636903[0m
[37m[1m[2023-07-10 16:48:09,119][227910] Average Trajectory Length: 999.7336666666666[0m
[36m[2023-07-10 16:48:14,702][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:48:14,702][227910] Reward + Measures: [[3091.1737024     0.23959999    0.38299999    0.24700001    0.17520002]
 [2979.39984595    0.206         0.32549998    0.25569999    0.18410002]
 [2833.19984855    0.23189998    0.38429999    0.22919999    0.1838    ]
 ...
 [2953.92796698    0.2193806     0.31939703    0.26611495    0.1744597 ]
 [3135.37630949    0.24800001    0.37300003    0.24960001    0.16849999]
 [2806.32333004    0.2705        0.42999998    0.24870002    0.1742    ]][0m
[37m[1m[2023-07-10 16:48:14,702][227910] Max Reward on eval: 3357.0555637696757[0m
[37m[1m[2023-07-10 16:48:14,703][227910] Min Reward on eval: 1296.8156770918577[0m
[37m[1m[2023-07-10 16:48:14,703][227910] Mean Reward across all agents: 2775.7120776557317[0m
[37m[1m[2023-07-10 16:48:14,703][227910] Average Trajectory Length: 992.405[0m
[36m[2023-07-10 16:48:14,704][227910] mean_value=-1635.5754396511636, max_value=643.2488308815708[0m
[37m[1m[2023-07-10 16:48:14,707][227910] New mean coefficients: [[ 0.21659863 -0.01298636  0.47744137  0.5558375   0.29477853]][0m
[37m[1m[2023-07-10 16:48:14,708][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:48:24,423][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 16:48:24,423][227910] FPS: 395314.61[0m
[36m[2023-07-10 16:48:24,426][227910] itr=869, itrs=2000, Progress: 43.45%[0m
[36m[2023-07-10 16:48:35,957][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 16:48:35,957][227910] FPS: 333559.33[0m
[36m[2023-07-10 16:48:40,755][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:48:40,756][227910] Reward + Measures: [[3269.6342591     0.24427043    0.41635954    0.24622756    0.17817988]][0m
[37m[1m[2023-07-10 16:48:40,756][227910] Max Reward on eval: 3269.6342590956274[0m
[37m[1m[2023-07-10 16:48:40,756][227910] Min Reward on eval: 3269.6342590956274[0m
[37m[1m[2023-07-10 16:48:40,756][227910] Mean Reward across all agents: 3269.6342590956274[0m
[37m[1m[2023-07-10 16:48:40,756][227910] Average Trajectory Length: 999.9773333333333[0m
[36m[2023-07-10 16:48:46,228][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:48:46,229][227910] Reward + Measures: [[2486.8829358     0.21230002    0.33150002    0.23340002    0.16140001]
 [1078.06828475    0.25680003    0.24110003    0.27010003    0.1461    ]
 [3275.50691241    0.24879999    0.39740002    0.23980001    0.1811    ]
 ...
 [1320.30252657    0.20060001    0.29140002    0.25610003    0.15930001]
 [1093.67869293    0.20120001    0.29260001    0.23669998    0.1506    ]
 [1259.35677579    0.206         0.26229998    0.2256        0.169     ]][0m
[37m[1m[2023-07-10 16:48:46,229][227910] Max Reward on eval: 3384.664603849128[0m
[37m[1m[2023-07-10 16:48:46,230][227910] Min Reward on eval: -209.528285818937[0m
[37m[1m[2023-07-10 16:48:46,230][227910] Mean Reward across all agents: 2025.3892972156002[0m
[37m[1m[2023-07-10 16:48:46,230][227910] Average Trajectory Length: 996.4446666666666[0m
[36m[2023-07-10 16:48:46,231][227910] mean_value=-1958.3040909571102, max_value=-101.58808030639102[0m
[36m[2023-07-10 16:48:46,234][227910] XNES is restarting with a new solution whose measures are [0.368      0.0764     0.41409999 0.31909999] and objective is -258.6586931366008[0m
[36m[2023-07-10 16:48:46,235][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 16:48:46,237][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 16:48:46,238][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:48:55,932][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 16:48:55,932][227910] FPS: 396175.56[0m
[36m[2023-07-10 16:48:55,934][227910] itr=870, itrs=2000, Progress: 43.50%[0m
[37m[1m[2023-07-10 16:48:59,488][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000850[0m
[36m[2023-07-10 16:49:11,324][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 16:49:11,324][227910] FPS: 331880.73[0m
[36m[2023-07-10 16:49:16,016][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:49:16,016][227910] Reward + Measures: [[131.78200887   0.17552035   0.65344596   0.18997099   0.63867033]][0m
[37m[1m[2023-07-10 16:49:16,016][227910] Max Reward on eval: 131.7820088724164[0m
[37m[1m[2023-07-10 16:49:16,016][227910] Min Reward on eval: 131.7820088724164[0m
[37m[1m[2023-07-10 16:49:16,017][227910] Mean Reward across all agents: 131.7820088724164[0m
[37m[1m[2023-07-10 16:49:16,017][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 16:49:21,431][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:49:21,432][227910] Reward + Measures: [[ -494.50742319     0.55009997     0.63499999     0.3012
      0.49000001]
 [ -577.10365561     0.26730004     0.39410001     0.21040002
      0.40060002]
 [ -548.91222655     0.38478488     0.42927438     0.40467033
      0.373873  ]
 ...
 [ -187.09744669     0.35069999     0.28599998     0.33759999
      0.31640002]
 [ -533.25213428     0.23511629     0.30232263     0.22694801
      0.30206123]
 [-1209.43315391     0.59414864     0.59469819     0.14096667
      0.55273962]][0m
[37m[1m[2023-07-10 16:49:21,432][227910] Max Reward on eval: 566.1301321647595[0m
[37m[1m[2023-07-10 16:49:21,432][227910] Min Reward on eval: -1840.0342827705667[0m
[37m[1m[2023-07-10 16:49:21,432][227910] Mean Reward across all agents: -750.0200074831697[0m
[37m[1m[2023-07-10 16:49:21,433][227910] Average Trajectory Length: 952.8273333333333[0m
[36m[2023-07-10 16:49:21,434][227910] mean_value=-1907.1582092530728, max_value=378.6380543356223[0m
[37m[1m[2023-07-10 16:49:21,437][227910] New mean coefficients: [[ 0.13255596  0.23098868 -0.83769566 -1.395551   -1.8358836 ]][0m
[37m[1m[2023-07-10 16:49:21,438][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:49:31,097][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 16:49:31,098][227910] FPS: 397593.69[0m
[36m[2023-07-10 16:49:31,100][227910] itr=871, itrs=2000, Progress: 43.55%[0m
[36m[2023-07-10 16:49:42,595][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 16:49:42,595][227910] FPS: 334592.96[0m
[36m[2023-07-10 16:49:47,434][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:49:47,435][227910] Reward + Measures: [[132.94772713   0.165004     0.59745336   0.17790334   0.48487365]][0m
[37m[1m[2023-07-10 16:49:47,435][227910] Max Reward on eval: 132.94772712608767[0m
[37m[1m[2023-07-10 16:49:47,435][227910] Min Reward on eval: 132.94772712608767[0m
[37m[1m[2023-07-10 16:49:47,435][227910] Mean Reward across all agents: 132.94772712608767[0m
[37m[1m[2023-07-10 16:49:47,435][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 16:49:52,866][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:49:52,867][227910] Reward + Measures: [[-1372.88545436     0.12683186     0.15159021     0.16792966
      0.14956877]
 [ -544.43238748     0.4638879      0.29251516     0.52472126
      0.3045879 ]
 [ -911.15944389     0.17546569     0.22918165     0.20344725
      0.18046056]
 ...
 [ -719.6684057      0.24027024     0.29554176     0.30858493
      0.21496777]
 [-1636.32411364     0.4382         0.37429997     0.47159997
      0.44600001]
 [ -519.72866417     0.19070001     0.37020001     0.25690001
      0.41020003]][0m
[37m[1m[2023-07-10 16:49:52,867][227910] Max Reward on eval: 298.03960012791214[0m
[37m[1m[2023-07-10 16:49:52,868][227910] Min Reward on eval: -2238.816808593832[0m
[37m[1m[2023-07-10 16:49:52,868][227910] Mean Reward across all agents: -791.3436522379054[0m
[37m[1m[2023-07-10 16:49:52,868][227910] Average Trajectory Length: 873.6703333333334[0m
[36m[2023-07-10 16:49:52,870][227910] mean_value=-2022.272200574597, max_value=524.1487453543257[0m
[37m[1m[2023-07-10 16:49:52,872][227910] New mean coefficients: [[-0.1872198   1.4613137  -0.78683794 -1.3367047  -0.99346435]][0m
[37m[1m[2023-07-10 16:49:52,873][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:50:02,535][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 16:50:02,535][227910] FPS: 397504.95[0m
[36m[2023-07-10 16:50:02,538][227910] itr=872, itrs=2000, Progress: 43.60%[0m
[36m[2023-07-10 16:50:14,176][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 16:50:14,177][227910] FPS: 330544.81[0m
[36m[2023-07-10 16:50:19,008][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:50:19,009][227910] Reward + Measures: [[-21.91491597   0.21143633   0.63154262   0.194539     0.4914887 ]][0m
[37m[1m[2023-07-10 16:50:19,009][227910] Max Reward on eval: -21.914915973951356[0m
[37m[1m[2023-07-10 16:50:19,009][227910] Min Reward on eval: -21.914915973951356[0m
[37m[1m[2023-07-10 16:50:19,010][227910] Mean Reward across all agents: -21.914915973951356[0m
[37m[1m[2023-07-10 16:50:19,010][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 16:50:24,516][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:50:24,517][227910] Reward + Measures: [[-1305.89255298     0.39310002     0.49630004     0.45120001
      0.4447    ]
 [ -938.48939018     0.37409574     0.47948208     0.34247589
      0.35060158]
 [ -756.11819914     0.266          0.39949998     0.14140001
      0.49910003]
 ...
 [ -322.45949081     0.32160002     0.42459998     0.27719998
      0.33780003]
 [-1374.97961926     0.13510001     0.147          0.1252
      0.14120001]
 [ -410.50820041     0.25753564     0.47767088     0.16805135
      0.39864293]][0m
[37m[1m[2023-07-10 16:50:24,517][227910] Max Reward on eval: 282.19213536054593[0m
[37m[1m[2023-07-10 16:50:24,518][227910] Min Reward on eval: -2093.8438818775116[0m
[37m[1m[2023-07-10 16:50:24,518][227910] Mean Reward across all agents: -789.6583640587356[0m
[37m[1m[2023-07-10 16:50:24,518][227910] Average Trajectory Length: 944.088[0m
[36m[2023-07-10 16:50:24,520][227910] mean_value=-2592.5050634750532, max_value=477.03356532410254[0m
[37m[1m[2023-07-10 16:50:24,523][227910] New mean coefficients: [[-1.1681608  -0.05776668  0.10186875 -0.38102388 -1.4818137 ]][0m
[37m[1m[2023-07-10 16:50:24,523][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:50:34,390][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 16:50:34,390][227910] FPS: 389279.36[0m
[36m[2023-07-10 16:50:34,392][227910] itr=873, itrs=2000, Progress: 43.65%[0m
[36m[2023-07-10 16:50:46,018][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 16:50:46,018][227910] FPS: 330828.32[0m
[36m[2023-07-10 16:50:50,828][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:50:50,828][227910] Reward + Measures: [[-491.38545588    0.22837697    0.55920154    0.13044016    0.50692439]][0m
[37m[1m[2023-07-10 16:50:50,829][227910] Max Reward on eval: -491.3854558844537[0m
[37m[1m[2023-07-10 16:50:50,829][227910] Min Reward on eval: -491.3854558844537[0m
[37m[1m[2023-07-10 16:50:50,829][227910] Mean Reward across all agents: -491.3854558844537[0m
[37m[1m[2023-07-10 16:50:50,829][227910] Average Trajectory Length: 991.6323333333333[0m
[36m[2023-07-10 16:50:56,389][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:50:56,390][227910] Reward + Measures: [[ -895.17224991     0.15369999     0.4601         0.27159998
      0.39219999]
 [ -500.90703748     0.18740001     0.56310004     0.1455
      0.50520003]
 [ -985.41496943     0.35710001     0.38830003     0.24120001
      0.32210001]
 ...
 [-1148.48707974     0.16670001     0.59750003     0.56660002
      0.62      ]
 [ -462.61389482     0.09380001     0.73019999     0.2036
      0.73939997]
 [ -443.55807419     0.1398         0.68190002     0.3337
      0.57249999]][0m
[37m[1m[2023-07-10 16:50:56,390][227910] Max Reward on eval: 64.32857938840752[0m
[37m[1m[2023-07-10 16:50:56,390][227910] Min Reward on eval: -2184.049738778244[0m
[37m[1m[2023-07-10 16:50:56,390][227910] Mean Reward across all agents: -804.022584485372[0m
[37m[1m[2023-07-10 16:50:56,391][227910] Average Trajectory Length: 991.326[0m
[36m[2023-07-10 16:50:56,392][227910] mean_value=-1417.031754575557, max_value=132.38818038352582[0m
[37m[1m[2023-07-10 16:50:56,395][227910] New mean coefficients: [[-0.4163159   0.01416727 -0.12111267 -0.56977844 -1.778747  ]][0m
[37m[1m[2023-07-10 16:50:56,395][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:51:06,071][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 16:51:06,071][227910] FPS: 396965.40[0m
[36m[2023-07-10 16:51:06,073][227910] itr=874, itrs=2000, Progress: 43.70%[0m
[36m[2023-07-10 16:51:17,635][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 16:51:17,635][227910] FPS: 332682.13[0m
[36m[2023-07-10 16:51:22,411][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:51:22,412][227910] Reward + Measures: [[-552.29479485    0.20600069    0.61801529    0.12369462    0.56514877]][0m
[37m[1m[2023-07-10 16:51:22,412][227910] Max Reward on eval: -552.2947948533985[0m
[37m[1m[2023-07-10 16:51:22,412][227910] Min Reward on eval: -552.2947948533985[0m
[37m[1m[2023-07-10 16:51:22,412][227910] Mean Reward across all agents: -552.2947948533985[0m
[37m[1m[2023-07-10 16:51:22,412][227910] Average Trajectory Length: 995.7489999999999[0m
[36m[2023-07-10 16:51:28,020][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:51:28,021][227910] Reward + Measures: [[-471.14455323    0.29820001    0.55289996    0.25239998    0.45369998]
 [-877.83964015    0.58280003    0.32119998    0.56420004    0.32020003]
 [-536.18757894    0.22849999    0.4066        0.31239995    0.34180003]
 ...
 [-275.59014832    0.18920001    0.34110004    0.193         0.33870003]
 [-998.8672922     0.23738797    0.35515639    0.38263309    0.33577669]
 [-674.3857867     0.26767835    0.37056476    0.35811958    0.34187242]][0m
[37m[1m[2023-07-10 16:51:28,021][227910] Max Reward on eval: -106.81422607442364[0m
[37m[1m[2023-07-10 16:51:28,021][227910] Min Reward on eval: -1792.7704985182384[0m
[37m[1m[2023-07-10 16:51:28,021][227910] Mean Reward across all agents: -771.1006719626483[0m
[37m[1m[2023-07-10 16:51:28,022][227910] Average Trajectory Length: 980.7273333333333[0m
[36m[2023-07-10 16:51:28,023][227910] mean_value=-2057.1401033835477, max_value=-23.774779878801837[0m
[36m[2023-07-10 16:51:28,025][227910] XNES is restarting with a new solution whose measures are [0.10090001 0.93880004 0.54909998 0.89390004] and objective is 583.8813495560083[0m
[36m[2023-07-10 16:51:28,026][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 16:51:28,029][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 16:51:28,029][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:51:37,693][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 16:51:37,693][227910] FPS: 397439.53[0m
[36m[2023-07-10 16:51:37,695][227910] itr=875, itrs=2000, Progress: 43.75%[0m
[36m[2023-07-10 16:51:49,301][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 16:51:49,301][227910] FPS: 331364.55[0m
[36m[2023-07-10 16:51:54,036][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:51:54,036][227910] Reward + Measures: [[872.79794164   0.52786964   0.53108764   0.38637134   0.33343631]][0m
[37m[1m[2023-07-10 16:51:54,036][227910] Max Reward on eval: 872.7979416427867[0m
[37m[1m[2023-07-10 16:51:54,037][227910] Min Reward on eval: 872.7979416427867[0m
[37m[1m[2023-07-10 16:51:54,037][227910] Mean Reward across all agents: 872.7979416427867[0m
[37m[1m[2023-07-10 16:51:54,037][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 16:51:59,458][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:51:59,458][227910] Reward + Measures: [[  107.52794614     0.41620001     0.40319997     0.37170002
      0.3461    ]
 [-1130.66796647     0.19766377     0.18300547     0.1704371
      0.25927123]
 [ -910.43612798     0.18380001     0.44720003     0.22160001
      0.28900003]
 ...
 [ -999.69287513     0.23049031     0.28853759     0.31190547
      0.29937488]
 [-1090.93259393     0.21398602     0.22060429     0.26961026
      0.30083883]
 [-1116.14345084     0.31987238     0.32317394     0.30323011
      0.2120093 ]][0m
[37m[1m[2023-07-10 16:51:59,458][227910] Max Reward on eval: 1018.0321180190891[0m
[37m[1m[2023-07-10 16:51:59,459][227910] Min Reward on eval: -2196.455696217227[0m
[37m[1m[2023-07-10 16:51:59,459][227910] Mean Reward across all agents: -895.3206402267275[0m
[37m[1m[2023-07-10 16:51:59,459][227910] Average Trajectory Length: 911.7926666666666[0m
[36m[2023-07-10 16:51:59,461][227910] mean_value=-2319.052360520959, max_value=780.3780760337181[0m
[37m[1m[2023-07-10 16:51:59,463][227910] New mean coefficients: [[ 0.45106238 -0.3233537  -0.99046034 -0.10230553 -0.15476573]][0m
[37m[1m[2023-07-10 16:51:59,464][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:52:09,152][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 16:52:09,152][227910] FPS: 396446.70[0m
[36m[2023-07-10 16:52:09,154][227910] itr=876, itrs=2000, Progress: 43.80%[0m
[36m[2023-07-10 16:52:21,010][227910] train() took 11.84 seconds to complete[0m
[36m[2023-07-10 16:52:21,010][227910] FPS: 324382.42[0m
[36m[2023-07-10 16:52:25,698][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:52:25,698][227910] Reward + Measures: [[1001.85208366    0.44545674    0.4589794     0.29686937    0.29873025]][0m
[37m[1m[2023-07-10 16:52:25,698][227910] Max Reward on eval: 1001.8520836614933[0m
[37m[1m[2023-07-10 16:52:25,699][227910] Min Reward on eval: 1001.8520836614933[0m
[37m[1m[2023-07-10 16:52:25,699][227910] Mean Reward across all agents: 1001.8520836614933[0m
[37m[1m[2023-07-10 16:52:25,699][227910] Average Trajectory Length: 999.6876666666666[0m
[36m[2023-07-10 16:52:31,163][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:52:31,163][227910] Reward + Measures: [[ -368.56555526     0.53039998     0.15890001     0.55800003
      0.412     ]
 [-2209.82967613     0.1181         0.68870002     0.51019996
      0.61470002]
 [-1073.647259       0.72340006     0.31399998     0.76290005
      0.1024    ]
 ...
 [-1049.83251668     0.47109994     0.56520003     0.2902
      0.40440002]
 [ -697.25642136     0.32140315     0.30528364     0.35341153
      0.30301309]
 [  612.27861084     0.56809998     0.2309         0.53080004
      0.46259999]][0m
[37m[1m[2023-07-10 16:52:31,164][227910] Max Reward on eval: 1006.6396188448416[0m
[37m[1m[2023-07-10 16:52:31,164][227910] Min Reward on eval: -2371.0035503429594[0m
[37m[1m[2023-07-10 16:52:31,164][227910] Mean Reward across all agents: -701.5196780859168[0m
[37m[1m[2023-07-10 16:52:31,164][227910] Average Trajectory Length: 965.8873333333333[0m
[36m[2023-07-10 16:52:31,166][227910] mean_value=-1754.0413665523602, max_value=634.2338690602128[0m
[37m[1m[2023-07-10 16:52:31,169][227910] New mean coefficients: [[ 0.27306083 -0.43222144  0.52251714 -0.34474978  0.14278448]][0m
[37m[1m[2023-07-10 16:52:31,169][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:52:40,877][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 16:52:40,877][227910] FPS: 395637.89[0m
[36m[2023-07-10 16:52:40,879][227910] itr=877, itrs=2000, Progress: 43.85%[0m
[36m[2023-07-10 16:52:52,660][227910] train() took 11.76 seconds to complete[0m
[36m[2023-07-10 16:52:52,660][227910] FPS: 326440.66[0m
[36m[2023-07-10 16:52:57,424][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:52:57,424][227910] Reward + Measures: [[1025.54605665    0.32239237    0.36178905    0.3033402     0.29132822]][0m
[37m[1m[2023-07-10 16:52:57,425][227910] Max Reward on eval: 1025.54605664701[0m
[37m[1m[2023-07-10 16:52:57,425][227910] Min Reward on eval: 1025.54605664701[0m
[37m[1m[2023-07-10 16:52:57,425][227910] Mean Reward across all agents: 1025.54605664701[0m
[37m[1m[2023-07-10 16:52:57,425][227910] Average Trajectory Length: 992.3976666666666[0m
[36m[2023-07-10 16:53:02,959][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:53:02,959][227910] Reward + Measures: [[ -160.0982695      0.22483218     0.33384743     0.28443292
      0.23430108]
 [-1742.6981254      0.79520649     0.80881292     0.79658061
      0.8105613 ]
 [-1183.26624592     0.74920005     0.72480005     0.75439996
      0.67839998]
 ...
 [-1236.36211768     0.71690005     0.0774         0.55320001
      0.46739998]
 [  311.16363635     0.37190142     0.38669369     0.2677978
      0.29604083]
 [ -140.18946052     0.0763         0.85740006     0.55229998
      0.89419997]][0m
[37m[1m[2023-07-10 16:53:02,960][227910] Max Reward on eval: 853.0672084023012[0m
[37m[1m[2023-07-10 16:53:02,960][227910] Min Reward on eval: -2065.6127630600704[0m
[37m[1m[2023-07-10 16:53:02,960][227910] Mean Reward across all agents: -647.984524519662[0m
[37m[1m[2023-07-10 16:53:02,960][227910] Average Trajectory Length: 918.6496666666667[0m
[36m[2023-07-10 16:53:02,962][227910] mean_value=-1921.2189120349296, max_value=539.9660622960303[0m
[37m[1m[2023-07-10 16:53:02,965][227910] New mean coefficients: [[ 0.64467204 -0.44913217  0.95270675  0.61356866 -0.5479933 ]][0m
[37m[1m[2023-07-10 16:53:02,966][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:53:12,763][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 16:53:12,763][227910] FPS: 392016.90[0m
[36m[2023-07-10 16:53:12,765][227910] itr=878, itrs=2000, Progress: 43.90%[0m
[36m[2023-07-10 16:53:24,301][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 16:53:24,301][227910] FPS: 333480.48[0m
[36m[2023-07-10 16:53:29,143][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:53:29,144][227910] Reward + Measures: [[859.58870028   0.29904079   0.31809843   0.26772541   0.2534079 ]][0m
[37m[1m[2023-07-10 16:53:29,144][227910] Max Reward on eval: 859.5887002815087[0m
[37m[1m[2023-07-10 16:53:29,144][227910] Min Reward on eval: 859.5887002815087[0m
[37m[1m[2023-07-10 16:53:29,144][227910] Mean Reward across all agents: 859.5887002815087[0m
[37m[1m[2023-07-10 16:53:29,145][227910] Average Trajectory Length: 995.3406666666666[0m
[36m[2023-07-10 16:53:34,706][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:53:34,706][227910] Reward + Measures: [[-1196.59611729     0.08999223     0.20730744     0.12047184
      0.16534984]
 [ -137.33549564     0.16022225     0.28709391     0.17627613
      0.22654514]
 [-1115.34799853     0.19090001     0.34619999     0.16659999
      0.33900005]
 ...
 [ -635.6794546      0.1168         0.40840003     0.21440001
      0.46349999]
 [ -310.03063761     0.2327857      0.31054285     0.41044283
      0.29601428]
 [-1420.95150429     0.12672783     0.36643711     0.27851135
      0.32012579]][0m
[37m[1m[2023-07-10 16:53:34,707][227910] Max Reward on eval: 842.6762517439929[0m
[37m[1m[2023-07-10 16:53:34,707][227910] Min Reward on eval: -1898.228711224557[0m
[37m[1m[2023-07-10 16:53:34,707][227910] Mean Reward across all agents: -587.7050511159513[0m
[37m[1m[2023-07-10 16:53:34,707][227910] Average Trajectory Length: 941.0383333333333[0m
[36m[2023-07-10 16:53:34,709][227910] mean_value=-1833.412730431989, max_value=1008.8360432707938[0m
[37m[1m[2023-07-10 16:53:34,712][227910] New mean coefficients: [[ 0.18577039  0.725222    0.57241434  0.18355614 -1.6691684 ]][0m
[37m[1m[2023-07-10 16:53:34,713][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:53:44,617][227910] train() took 9.90 seconds to complete[0m
[36m[2023-07-10 16:53:44,618][227910] FPS: 387772.68[0m
[36m[2023-07-10 16:53:44,620][227910] itr=879, itrs=2000, Progress: 43.95%[0m
[36m[2023-07-10 16:53:56,334][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 16:53:56,335][227910] FPS: 328317.46[0m
[36m[2023-07-10 16:54:01,090][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:54:01,090][227910] Reward + Measures: [[592.92554488   0.40953535   0.36789435   0.30489171   0.29286763]][0m
[37m[1m[2023-07-10 16:54:01,091][227910] Max Reward on eval: 592.9255448755664[0m
[37m[1m[2023-07-10 16:54:01,091][227910] Min Reward on eval: 592.9255448755664[0m
[37m[1m[2023-07-10 16:54:01,091][227910] Mean Reward across all agents: 592.9255448755664[0m
[37m[1m[2023-07-10 16:54:01,091][227910] Average Trajectory Length: 962.9446666666666[0m
[36m[2023-07-10 16:54:06,539][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:54:06,539][227910] Reward + Measures: [[ -409.18368753     0.30047974     0.3671259      0.2281353
      0.30502325]
 [ -845.23093091     0.63268894     0.61314124     0.22991829
      0.5127641 ]
 [ -575.2943861      0.5546         0.73170006     0.2951
      0.70130002]
 ...
 [  -27.7495078      0.32571515     0.38346878     0.21200578
      0.30961519]
 [-1168.60970993     0.24519999     0.1908         0.26760003
      0.11360001]
 [-1053.52291038     0.40693846     0.39990002     0.39003846
      0.30737692]][0m
[37m[1m[2023-07-10 16:54:06,540][227910] Max Reward on eval: 566.4342574433831[0m
[37m[1m[2023-07-10 16:54:06,540][227910] Min Reward on eval: -1708.241821661964[0m
[37m[1m[2023-07-10 16:54:06,540][227910] Mean Reward across all agents: -542.540952925095[0m
[37m[1m[2023-07-10 16:54:06,540][227910] Average Trajectory Length: 925.9233333333333[0m
[36m[2023-07-10 16:54:06,542][227910] mean_value=-1848.9156216371684, max_value=370.4109678055707[0m
[37m[1m[2023-07-10 16:54:06,545][227910] New mean coefficients: [[ 0.7000649   1.1761936   0.8593255  -0.01117976 -0.9757145 ]][0m
[37m[1m[2023-07-10 16:54:06,546][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:54:16,308][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 16:54:16,308][227910] FPS: 393436.49[0m
[36m[2023-07-10 16:54:16,310][227910] itr=880, itrs=2000, Progress: 44.00%[0m
[37m[1m[2023-07-10 16:54:19,817][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000860[0m
[36m[2023-07-10 16:54:31,740][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 16:54:31,740][227910] FPS: 327880.45[0m
[36m[2023-07-10 16:54:36,465][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:54:36,465][227910] Reward + Measures: [[339.50973439   0.44769326   0.36250943   0.34818774   0.31538558]][0m
[37m[1m[2023-07-10 16:54:36,465][227910] Max Reward on eval: 339.5097343905324[0m
[37m[1m[2023-07-10 16:54:36,466][227910] Min Reward on eval: 339.5097343905324[0m
[37m[1m[2023-07-10 16:54:36,466][227910] Mean Reward across all agents: 339.5097343905324[0m
[37m[1m[2023-07-10 16:54:36,466][227910] Average Trajectory Length: 906.1659999999999[0m
[36m[2023-07-10 16:54:41,819][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:54:41,824][227910] Reward + Measures: [[ -381.35919617     0.54982966     0.27264938     0.44533584
      0.45351973]
 [  -42.51438074     0.3651         0.33090001     0.24639998
      0.21180001]
 [-1481.91773129     0.21244268     0.1654893      0.26027304
      0.14700951]
 ...
 [-1222.75999504     0.34120002     0.46040002     0.37090001
      0.35049999]
 [ -615.46567678     0.46689996     0.67339993     0.38129997
      0.65200001]
 [-1304.22584846     0.47779998     0.5266         0.41560003
      0.49750003]][0m
[37m[1m[2023-07-10 16:54:41,825][227910] Max Reward on eval: 441.50819709323116[0m
[37m[1m[2023-07-10 16:54:41,825][227910] Min Reward on eval: -1961.6280947664752[0m
[37m[1m[2023-07-10 16:54:41,825][227910] Mean Reward across all agents: -723.5611716357893[0m
[37m[1m[2023-07-10 16:54:41,825][227910] Average Trajectory Length: 911.2703333333333[0m
[36m[2023-07-10 16:54:41,827][227910] mean_value=-2131.0857928583214, max_value=561.739861697187[0m
[37m[1m[2023-07-10 16:54:41,829][227910] New mean coefficients: [[ 0.6866336   0.2645089   0.3945898  -0.49060214 -1.3403552 ]][0m
[37m[1m[2023-07-10 16:54:41,830][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:54:51,442][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 16:54:51,442][227910] FPS: 399600.37[0m
[36m[2023-07-10 16:54:51,444][227910] itr=881, itrs=2000, Progress: 44.05%[0m
[36m[2023-07-10 16:55:03,006][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 16:55:03,006][227910] FPS: 332622.34[0m
[36m[2023-07-10 16:55:07,718][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:55:07,719][227910] Reward + Measures: [[456.10799049   0.55210471   0.48095068   0.34003225   0.34207976]][0m
[37m[1m[2023-07-10 16:55:07,719][227910] Max Reward on eval: 456.10799049291876[0m
[37m[1m[2023-07-10 16:55:07,719][227910] Min Reward on eval: 456.10799049291876[0m
[37m[1m[2023-07-10 16:55:07,720][227910] Mean Reward across all agents: 456.10799049291876[0m
[37m[1m[2023-07-10 16:55:07,720][227910] Average Trajectory Length: 966.4733333333334[0m
[36m[2023-07-10 16:55:13,310][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:55:13,311][227910] Reward + Measures: [[-1326.52012639     0.80857414     0.71605414     0.68335468
      0.61879033]
 [ -829.76161097     0.39329997     0.26970002     0.35069999
      0.16760002]
 [ -704.3864169      0.30871749     0.27042338     0.24943128
      0.1946297 ]
 ...
 [ -531.92060565     0.4255704      0.33745357     0.26307112
      0.34067765]
 [ -897.2218291      0.2497025      0.24825795     0.22485466
      0.1961693 ]
 [ -920.61028179     0.69500005     0.43400002     0.6505
      0.26340002]][0m
[37m[1m[2023-07-10 16:55:13,311][227910] Max Reward on eval: 497.67529517028015[0m
[37m[1m[2023-07-10 16:55:13,311][227910] Min Reward on eval: -2136.6268327516036[0m
[37m[1m[2023-07-10 16:55:13,312][227910] Mean Reward across all agents: -821.9156495308387[0m
[37m[1m[2023-07-10 16:55:13,312][227910] Average Trajectory Length: 912.3036666666667[0m
[36m[2023-07-10 16:55:13,314][227910] mean_value=-2072.5742318390153, max_value=529.5675090895558[0m
[37m[1m[2023-07-10 16:55:13,316][227910] New mean coefficients: [[ 0.6220955   0.08074698  0.870069    0.02530915 -0.46798122]][0m
[37m[1m[2023-07-10 16:55:13,317][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:55:23,066][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 16:55:23,066][227910] FPS: 393950.02[0m
[36m[2023-07-10 16:55:23,069][227910] itr=882, itrs=2000, Progress: 44.10%[0m
[36m[2023-07-10 16:55:34,986][227910] train() took 11.90 seconds to complete[0m
[36m[2023-07-10 16:55:34,986][227910] FPS: 322804.61[0m
[36m[2023-07-10 16:55:39,147][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:55:39,148][227910] Reward + Measures: [[395.99863386   0.53304416   0.46530589   0.33024028   0.33565894]][0m
[37m[1m[2023-07-10 16:55:39,148][227910] Max Reward on eval: 395.99863386426244[0m
[37m[1m[2023-07-10 16:55:39,148][227910] Min Reward on eval: 395.99863386426244[0m
[37m[1m[2023-07-10 16:55:39,148][227910] Mean Reward across all agents: 395.99863386426244[0m
[37m[1m[2023-07-10 16:55:39,149][227910] Average Trajectory Length: 918.5953333333333[0m
[36m[2023-07-10 16:55:44,898][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:55:44,898][227910] Reward + Measures: [[ -736.19267667     0.43129998     0.29070002     0.41340002
      0.37090001]
 [ -622.13743348     0.26630002     0.29449999     0.2881
      0.24340001]
 [-1590.46152026     0.15250002     0.30990002     0.11970001
      0.2053    ]
 ...
 [ -485.38159053     0.61474127     0.22200875     0.48817167
      0.29500896]
 [-1542.10976652     0.14901347     0.20064282     0.13142262
      0.10430469]
 [ -803.92370346     0.84420007     0.0717         0.82699996
      0.83910006]][0m
[37m[1m[2023-07-10 16:55:44,899][227910] Max Reward on eval: 498.0854800833622[0m
[37m[1m[2023-07-10 16:55:44,899][227910] Min Reward on eval: -1894.7641696869046[0m
[37m[1m[2023-07-10 16:55:44,899][227910] Mean Reward across all agents: -705.7707919499213[0m
[37m[1m[2023-07-10 16:55:44,899][227910] Average Trajectory Length: 954.1826666666666[0m
[36m[2023-07-10 16:55:44,901][227910] mean_value=-2423.526262765579, max_value=248.36382680352222[0m
[37m[1m[2023-07-10 16:55:44,911][227910] New mean coefficients: [[0.3460203  0.16334194 1.5251989  0.12603933 0.35741365]][0m
[37m[1m[2023-07-10 16:55:44,912][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:55:54,640][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 16:55:54,640][227910] FPS: 394815.99[0m
[36m[2023-07-10 16:55:54,643][227910] itr=883, itrs=2000, Progress: 44.15%[0m
[36m[2023-07-10 16:56:06,352][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 16:56:06,352][227910] FPS: 328592.59[0m
[36m[2023-07-10 16:56:11,167][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:56:11,168][227910] Reward + Measures: [[502.46927506   0.51353753   0.46689945   0.32156229   0.33300182]][0m
[37m[1m[2023-07-10 16:56:11,168][227910] Max Reward on eval: 502.4692750570358[0m
[37m[1m[2023-07-10 16:56:11,168][227910] Min Reward on eval: 502.4692750570358[0m
[37m[1m[2023-07-10 16:56:11,168][227910] Mean Reward across all agents: 502.4692750570358[0m
[37m[1m[2023-07-10 16:56:11,169][227910] Average Trajectory Length: 943.6256666666667[0m
[36m[2023-07-10 16:56:16,651][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:56:16,652][227910] Reward + Measures: [[-1452.68383633     0.60910004     0.77089995     0.8064
      0.72100002]
 [   37.33955604     0.4684         0.35589999     0.36480001
      0.1866    ]
 [ -565.85627495     0.4025         0.44260001     0.42210004
      0.31099999]
 ...
 [ -313.86741431     0.33842811     0.35069686     0.31595939
      0.29064688]
 [-1032.29134356     0.43039998     0.32260001     0.41689998
      0.1098    ]
 [-1365.40391966     0.25310001     0.54010004     0.1445
      0.44499999]][0m
[37m[1m[2023-07-10 16:56:16,652][227910] Max Reward on eval: 436.95600957724963[0m
[37m[1m[2023-07-10 16:56:16,652][227910] Min Reward on eval: -1829.290681319509[0m
[37m[1m[2023-07-10 16:56:16,653][227910] Mean Reward across all agents: -726.7952241252679[0m
[37m[1m[2023-07-10 16:56:16,653][227910] Average Trajectory Length: 911.4983333333333[0m
[36m[2023-07-10 16:56:16,654][227910] mean_value=-2322.2332581806427, max_value=440.6133076130762[0m
[37m[1m[2023-07-10 16:56:16,657][227910] New mean coefficients: [[ 0.8599715  -0.17713362  1.2877066   0.06834839 -0.7238401 ]][0m
[37m[1m[2023-07-10 16:56:16,658][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:56:26,384][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 16:56:26,384][227910] FPS: 394881.19[0m
[36m[2023-07-10 16:56:26,386][227910] itr=884, itrs=2000, Progress: 44.20%[0m
[36m[2023-07-10 16:56:38,046][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 16:56:38,047][227910] FPS: 329829.35[0m
[36m[2023-07-10 16:56:42,799][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:56:42,805][227910] Reward + Measures: [[-49.42854004   0.71860433   0.7163592    0.62285089   0.6329996 ]][0m
[37m[1m[2023-07-10 16:56:42,805][227910] Max Reward on eval: -49.42854003994537[0m
[37m[1m[2023-07-10 16:56:42,806][227910] Min Reward on eval: -49.42854003994537[0m
[37m[1m[2023-07-10 16:56:42,806][227910] Mean Reward across all agents: -49.42854003994537[0m
[37m[1m[2023-07-10 16:56:42,806][227910] Average Trajectory Length: 981.423[0m
[36m[2023-07-10 16:56:48,259][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:56:48,259][227910] Reward + Measures: [[ -606.18952012     0.30761227     0.30915132     0.35822889
      0.27474582]
 [-1129.2717183      0.42539999     0.4883         0.30490002
      0.33070001]
 [-1664.14750787     0.17110001     0.28480002     0.2597
      0.13850001]
 ...
 [ -701.62056375     0.49060002     0.4698         0.42360002
      0.39990002]
 [-1090.3369223      0.2543422      0.19147055     0.23855343
      0.11954253]
 [-1637.49075254     0.19920138     0.23891006     0.27508387
      0.18725137]][0m
[37m[1m[2023-07-10 16:56:48,260][227910] Max Reward on eval: 203.3491380972555[0m
[37m[1m[2023-07-10 16:56:48,260][227910] Min Reward on eval: -2159.953594710212[0m
[37m[1m[2023-07-10 16:56:48,260][227910] Mean Reward across all agents: -808.0583831371349[0m
[37m[1m[2023-07-10 16:56:48,260][227910] Average Trajectory Length: 949.1473333333333[0m
[36m[2023-07-10 16:56:48,262][227910] mean_value=-2165.682741217859, max_value=494.0629257132241[0m
[37m[1m[2023-07-10 16:56:48,264][227910] New mean coefficients: [[ 1.2819766   0.4014644   0.96030784  0.8839365  -1.1124932 ]][0m
[37m[1m[2023-07-10 16:56:48,265][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:56:57,930][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 16:56:57,930][227910] FPS: 397406.32[0m
[36m[2023-07-10 16:56:57,932][227910] itr=885, itrs=2000, Progress: 44.25%[0m
[36m[2023-07-10 16:57:09,551][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 16:57:09,551][227910] FPS: 331083.09[0m
[36m[2023-07-10 16:57:14,352][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:57:14,352][227910] Reward + Measures: [[124.97727704   0.64967942   0.65360743   0.54924619   0.54876548]][0m
[37m[1m[2023-07-10 16:57:14,352][227910] Max Reward on eval: 124.97727703816214[0m
[37m[1m[2023-07-10 16:57:14,352][227910] Min Reward on eval: 124.97727703816214[0m
[37m[1m[2023-07-10 16:57:14,352][227910] Mean Reward across all agents: 124.97727703816214[0m
[37m[1m[2023-07-10 16:57:14,353][227910] Average Trajectory Length: 980.7676666666666[0m
[36m[2023-07-10 16:57:19,867][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:57:19,873][227910] Reward + Measures: [[ -765.49508518     0.57190001     0.57639998     0.55629998
      0.51840001]
 [-1291.26125661     0.0781         0.44039997     0.45950004
      0.55770004]
 [-1005.74742656     0.24430001     0.32339999     0.3285
      0.31230003]
 ...
 [-1424.49072965     0.10420001     0.55299997     0.42859998
      0.49899998]
 [-1215.29158896     0.3053         0.56120008     0.40579996
      0.48900005]
 [-1081.24862249     0.62160003     0.68250006     0.17830001
      0.65090007]][0m
[37m[1m[2023-07-10 16:57:19,873][227910] Max Reward on eval: 417.5783880678006[0m
[37m[1m[2023-07-10 16:57:19,874][227910] Min Reward on eval: -2229.130099118373[0m
[37m[1m[2023-07-10 16:57:19,874][227910] Mean Reward across all agents: -765.50574451352[0m
[37m[1m[2023-07-10 16:57:19,874][227910] Average Trajectory Length: 917.6916666666666[0m
[36m[2023-07-10 16:57:19,876][227910] mean_value=-2143.46253925238, max_value=73.69972684619881[0m
[37m[1m[2023-07-10 16:57:19,878][227910] New mean coefficients: [[ 0.9330958   0.5896923   1.2008586   0.33109587 -0.39782876]][0m
[37m[1m[2023-07-10 16:57:19,879][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:57:29,501][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 16:57:29,502][227910] FPS: 399153.51[0m
[36m[2023-07-10 16:57:29,504][227910] itr=886, itrs=2000, Progress: 44.30%[0m
[36m[2023-07-10 16:57:41,109][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 16:57:41,109][227910] FPS: 331383.23[0m
[36m[2023-07-10 16:57:45,901][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:57:45,902][227910] Reward + Measures: [[236.01986877   0.60576087   0.60400689   0.51476198   0.48429784]][0m
[37m[1m[2023-07-10 16:57:45,902][227910] Max Reward on eval: 236.01986876637986[0m
[37m[1m[2023-07-10 16:57:45,902][227910] Min Reward on eval: 236.01986876637986[0m
[37m[1m[2023-07-10 16:57:45,902][227910] Mean Reward across all agents: 236.01986876637986[0m
[37m[1m[2023-07-10 16:57:45,903][227910] Average Trajectory Length: 986.1056666666666[0m
[36m[2023-07-10 16:57:51,558][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:57:51,559][227910] Reward + Measures: [[ -163.97735152     0.31848592     0.42968628     0.36184779
      0.28174099]
 [ -882.82973577     0.7902         0.67539996     0.78779995
      0.52489996]
 [-1167.48702204     0.43880001     0.3039         0.41409999
      0.36840001]
 ...
 [ -698.94655966     0.33570439     0.4281716      0.41722843
      0.43780175]
 [ -572.99540133     0.37601209     0.31827459     0.35433117
      0.2775712 ]
 [ -183.95757194     0.46693251     0.5193736      0.42151412
      0.37737939]][0m
[37m[1m[2023-07-10 16:57:51,559][227910] Max Reward on eval: 396.837487331219[0m
[37m[1m[2023-07-10 16:57:51,559][227910] Min Reward on eval: -1527.9413983970182[0m
[37m[1m[2023-07-10 16:57:51,559][227910] Mean Reward across all agents: -599.7106186063421[0m
[37m[1m[2023-07-10 16:57:51,560][227910] Average Trajectory Length: 916.723[0m
[36m[2023-07-10 16:57:51,561][227910] mean_value=-1994.233806604292, max_value=467.4390580756762[0m
[37m[1m[2023-07-10 16:57:51,564][227910] New mean coefficients: [[ 0.6490799 -0.045017   1.0661937  0.818403  -1.1644719]][0m
[37m[1m[2023-07-10 16:57:51,564][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:58:01,417][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 16:58:01,417][227910] FPS: 389817.71[0m
[36m[2023-07-10 16:58:01,419][227910] itr=887, itrs=2000, Progress: 44.35%[0m
[36m[2023-07-10 16:58:13,036][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 16:58:13,036][227910] FPS: 331148.21[0m
[36m[2023-07-10 16:58:17,793][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:58:17,793][227910] Reward + Measures: [[306.04819884   0.57465154   0.57438499   0.50115943   0.44193116]][0m
[37m[1m[2023-07-10 16:58:17,793][227910] Max Reward on eval: 306.0481988443408[0m
[37m[1m[2023-07-10 16:58:17,794][227910] Min Reward on eval: 306.0481988443408[0m
[37m[1m[2023-07-10 16:58:17,794][227910] Mean Reward across all agents: 306.0481988443408[0m
[37m[1m[2023-07-10 16:58:17,794][227910] Average Trajectory Length: 985.491[0m
[36m[2023-07-10 16:58:23,301][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:58:23,302][227910] Reward + Measures: [[ -925.70705389     0.56719363     0.3282319      0.57948726
      0.50472134]
 [-1458.94876291     0.37976527     0.24349025     0.27308208
      0.18620217]
 [ -825.22700189     0.60970002     0.71390003     0.3549
      0.65830004]
 ...
 [ -238.80080556     0.49270001     0.2843         0.32449999
      0.21529999]
 [-1269.70440572     0.33800003     0.47580001     0.1948
      0.48740003]
 [ -636.081496       0.4375535      0.2579985      0.45039532
      0.24874234]][0m
[37m[1m[2023-07-10 16:58:23,302][227910] Max Reward on eval: 447.84097807923683[0m
[37m[1m[2023-07-10 16:58:23,302][227910] Min Reward on eval: -1935.3578113597118[0m
[37m[1m[2023-07-10 16:58:23,302][227910] Mean Reward across all agents: -893.9467682252421[0m
[37m[1m[2023-07-10 16:58:23,303][227910] Average Trajectory Length: 922.8073333333333[0m
[36m[2023-07-10 16:58:23,304][227910] mean_value=-2426.7946628438976, max_value=-146.7559231116457[0m
[36m[2023-07-10 16:58:23,311][227910] XNES is restarting with a new solution whose measures are [0.88780004 0.3863     0.86440003 0.85299999] and objective is 332.76613026815465[0m
[36m[2023-07-10 16:58:23,312][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 16:58:23,315][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 16:58:23,316][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:58:32,983][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 16:58:32,983][227910] FPS: 397271.16[0m
[36m[2023-07-10 16:58:32,986][227910] itr=888, itrs=2000, Progress: 44.40%[0m
[36m[2023-07-10 16:58:44,507][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 16:58:44,507][227910] FPS: 333867.59[0m
[36m[2023-07-10 16:58:49,290][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:58:49,291][227910] Reward + Measures: [[253.42319811   0.8962757    0.14851268   0.886132     0.89094865]][0m
[37m[1m[2023-07-10 16:58:49,291][227910] Max Reward on eval: 253.42319811300874[0m
[37m[1m[2023-07-10 16:58:49,291][227910] Min Reward on eval: 253.42319811300874[0m
[37m[1m[2023-07-10 16:58:49,291][227910] Mean Reward across all agents: 253.42319811300874[0m
[37m[1m[2023-07-10 16:58:49,292][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 16:58:54,515][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:58:54,521][227910] Reward + Measures: [[   58.94689061     0.50650001     0.26750001     0.45930001
      0.35640001]
 [-2410.16787281     0.85360003     0.85599995     0.85350001
      0.86289996]
 [ -747.22227567     0.87439996     0.78600001     0.86159992
      0.8896001 ]
 ...
 [ -511.40919947     0.87819999     0.92410004     0.87550002
      0.91100007]
 [-1781.66912085     0.79549998     0.8592         0.77540004
      0.87819999]
 [-1048.70710939     0.15489815     0.1678866      0.14609383
      0.18239541]][0m
[37m[1m[2023-07-10 16:58:54,521][227910] Max Reward on eval: 278.1756180354394[0m
[37m[1m[2023-07-10 16:58:54,521][227910] Min Reward on eval: -2588.747819632129[0m
[37m[1m[2023-07-10 16:58:54,522][227910] Mean Reward across all agents: -590.2319698264149[0m
[37m[1m[2023-07-10 16:58:54,522][227910] Average Trajectory Length: 959.3919999999999[0m
[36m[2023-07-10 16:58:54,524][227910] mean_value=-1434.754181859071, max_value=639.301853213183[0m
[37m[1m[2023-07-10 16:58:54,527][227910] New mean coefficients: [[ 0.15802273 -0.26533794 -0.6528667  -1.4638101  -0.98726976]][0m
[37m[1m[2023-07-10 16:58:54,528][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:59:04,188][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 16:59:04,188][227910] FPS: 397574.57[0m
[36m[2023-07-10 16:59:04,190][227910] itr=889, itrs=2000, Progress: 44.45%[0m
[36m[2023-07-10 16:59:15,906][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 16:59:15,906][227910] FPS: 328335.89[0m
[36m[2023-07-10 16:59:20,696][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:59:20,696][227910] Reward + Measures: [[285.15981616   0.86438799   0.12633076   0.85296226   0.85276413]][0m
[37m[1m[2023-07-10 16:59:20,696][227910] Max Reward on eval: 285.1598161645652[0m
[37m[1m[2023-07-10 16:59:20,696][227910] Min Reward on eval: 285.1598161645652[0m
[37m[1m[2023-07-10 16:59:20,697][227910] Mean Reward across all agents: 285.1598161645652[0m
[37m[1m[2023-07-10 16:59:20,697][227910] Average Trajectory Length: 999.725[0m
[36m[2023-07-10 16:59:26,196][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:59:26,202][227910] Reward + Measures: [[  51.39077433    0.73409998    0.1639        0.62199998    0.45340005]
 [-826.55142486    0.50999039    0.5076685     0.47148904    0.46779862]
 [-676.84712073    0.60176152    0.17796925    0.619102      0.50135434]
 ...
 [-908.03250664    0.22813034    0.46546337    0.25768661    0.48804465]
 [-425.10492882    0.50730002    0.29530001    0.56109995    0.33850002]
 [-190.63021032    0.51870006    0.24609999    0.55030006    0.3536    ]][0m
[37m[1m[2023-07-10 16:59:26,203][227910] Max Reward on eval: 459.1811012618826[0m
[37m[1m[2023-07-10 16:59:26,204][227910] Min Reward on eval: -1449.659024970088[0m
[37m[1m[2023-07-10 16:59:26,204][227910] Mean Reward across all agents: -352.5221942735887[0m
[37m[1m[2023-07-10 16:59:26,205][227910] Average Trajectory Length: 970.8033333333333[0m
[36m[2023-07-10 16:59:26,210][227910] mean_value=-920.8863015344025, max_value=679.141440566082[0m
[37m[1m[2023-07-10 16:59:26,215][227910] New mean coefficients: [[ 0.4249621   0.36903077  0.23024517 -1.0464087  -1.0426717 ]][0m
[37m[1m[2023-07-10 16:59:26,217][227910] Moving the mean solution point...[0m
[36m[2023-07-10 16:59:35,992][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 16:59:35,992][227910] FPS: 392901.79[0m
[36m[2023-07-10 16:59:35,995][227910] itr=890, itrs=2000, Progress: 44.50%[0m
[37m[1m[2023-07-10 16:59:39,681][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000870[0m
[36m[2023-07-10 16:59:51,398][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 16:59:51,398][227910] FPS: 329365.26[0m
[36m[2023-07-10 16:59:56,229][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 16:59:56,229][227910] Reward + Measures: [[439.15762277   0.79644036   0.17811999   0.77806634   0.77789503]][0m
[37m[1m[2023-07-10 16:59:56,230][227910] Max Reward on eval: 439.15762276857254[0m
[37m[1m[2023-07-10 16:59:56,230][227910] Min Reward on eval: 439.15762276857254[0m
[37m[1m[2023-07-10 16:59:56,230][227910] Mean Reward across all agents: 439.15762276857254[0m
[37m[1m[2023-07-10 16:59:56,230][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:00:01,683][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:00:01,744][227910] Reward + Measures: [[  -88.65360412     0.55830002     0.59639996     0.47390005
      0.64359999]
 [ -717.12492951     0.47960001     0.4113         0.49669996
      0.5219    ]
 [ -217.35367849     0.80779999     0.60039997     0.79140002
      0.66970003]
 ...
 [  -34.65081286     0.41540003     0.46359998     0.37079999
      0.289     ]
 [-1515.43560539     0.62120003     0.56599998     0.56700003
      0.0515    ]
 [ -133.94950978     0.63009995     0.30510002     0.62890005
      0.45169997]][0m
[37m[1m[2023-07-10 17:00:01,745][227910] Max Reward on eval: 1781.770910806689[0m
[37m[1m[2023-07-10 17:00:01,745][227910] Min Reward on eval: -1686.5126626260667[0m
[37m[1m[2023-07-10 17:00:01,745][227910] Mean Reward across all agents: 51.4664211662215[0m
[37m[1m[2023-07-10 17:00:01,745][227910] Average Trajectory Length: 991.881[0m
[36m[2023-07-10 17:00:01,748][227910] mean_value=-983.7435063820822, max_value=542.6987706320392[0m
[37m[1m[2023-07-10 17:00:01,750][227910] New mean coefficients: [[ 0.11219254  0.05018196  0.46724737 -0.47034806 -0.49091917]][0m
[37m[1m[2023-07-10 17:00:01,751][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:00:11,667][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 17:00:11,667][227910] FPS: 387345.91[0m
[36m[2023-07-10 17:00:11,669][227910] itr=891, itrs=2000, Progress: 44.55%[0m
[36m[2023-07-10 17:00:23,692][227910] train() took 12.00 seconds to complete[0m
[36m[2023-07-10 17:00:23,692][227910] FPS: 319900.77[0m
[36m[2023-07-10 17:00:28,501][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:00:28,502][227910] Reward + Measures: [[716.07784312   0.6871472    0.24626684   0.66189301   0.66054517]][0m
[37m[1m[2023-07-10 17:00:28,502][227910] Max Reward on eval: 716.0778431182733[0m
[37m[1m[2023-07-10 17:00:28,502][227910] Min Reward on eval: 716.0778431182733[0m
[37m[1m[2023-07-10 17:00:28,503][227910] Mean Reward across all agents: 716.0778431182733[0m
[37m[1m[2023-07-10 17:00:28,503][227910] Average Trajectory Length: 999.275[0m
[36m[2023-07-10 17:00:34,060][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:00:34,061][227910] Reward + Measures: [[ 234.46063884    0.52960002    0.43200001    0.55470008    0.46089998]
 [ 715.88367648    0.6512        0.55769998    0.6455        0.61020005]
 [-161.36437219    0.5395        0.52220005    0.3362        0.65850002]
 ...
 [ 787.0313511     0.6372        0.40949997    0.61919999    0.55080003]
 [ 288.71769585    0.43059999    0.57459998    0.41750002    0.46020004]
 [ 325.01708225    0.44330001    0.44390002    0.3488        0.3251    ]][0m
[37m[1m[2023-07-10 17:00:34,061][227910] Max Reward on eval: 1073.095451682224[0m
[37m[1m[2023-07-10 17:00:34,061][227910] Min Reward on eval: -1106.3091300365165[0m
[37m[1m[2023-07-10 17:00:34,061][227910] Mean Reward across all agents: 101.68167624961143[0m
[37m[1m[2023-07-10 17:00:34,062][227910] Average Trajectory Length: 986.0016666666667[0m
[36m[2023-07-10 17:00:34,064][227910] mean_value=-1036.3154111149722, max_value=553.048035029283[0m
[37m[1m[2023-07-10 17:00:34,067][227910] New mean coefficients: [[ 0.46819514  0.20071478  0.17752057 -0.4357944  -0.30839714]][0m
[37m[1m[2023-07-10 17:00:34,067][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:00:43,760][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 17:00:43,760][227910] FPS: 396253.44[0m
[36m[2023-07-10 17:00:43,762][227910] itr=892, itrs=2000, Progress: 44.60%[0m
[36m[2023-07-10 17:00:55,353][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 17:00:55,353][227910] FPS: 331903.53[0m
[36m[2023-07-10 17:01:00,120][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:01:00,121][227910] Reward + Measures: [[1077.17979558    0.57977694    0.30812907    0.54905421    0.5368486 ]][0m
[37m[1m[2023-07-10 17:01:00,121][227910] Max Reward on eval: 1077.1797955758445[0m
[37m[1m[2023-07-10 17:01:00,121][227910] Min Reward on eval: 1077.1797955758445[0m
[37m[1m[2023-07-10 17:01:00,121][227910] Mean Reward across all agents: 1077.1797955758445[0m
[37m[1m[2023-07-10 17:01:00,121][227910] Average Trajectory Length: 999.4233333333333[0m
[36m[2023-07-10 17:01:05,535][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:01:05,536][227910] Reward + Measures: [[1439.18574699    0.49180004    0.42269999    0.42109999    0.39919996]
 [ 342.50696122    0.71680003    0.53820002    0.68100005    0.70539999]
 [ 491.4187509     0.7762        0.4402        0.74010003    0.75230002]
 ...
 [1346.28136684    0.44930002    0.40780002    0.3917        0.38930002]
 [ 134.80940508    0.69330001    0.12990001    0.65690005    0.57770008]
 [ 223.72681527    0.84089994    0.098         0.84090006    0.78100002]][0m
[37m[1m[2023-07-10 17:01:05,536][227910] Max Reward on eval: 1871.9839441031684[0m
[37m[1m[2023-07-10 17:01:05,536][227910] Min Reward on eval: -542.3218667584937[0m
[37m[1m[2023-07-10 17:01:05,537][227910] Mean Reward across all agents: 561.7170180747137[0m
[37m[1m[2023-07-10 17:01:05,537][227910] Average Trajectory Length: 994.2246666666666[0m
[36m[2023-07-10 17:01:05,540][227910] mean_value=-407.72471761705094, max_value=619.8886612079458[0m
[37m[1m[2023-07-10 17:01:05,542][227910] New mean coefficients: [[ 0.22918124 -0.08413719  1.0197784  -0.3254125  -0.2287518 ]][0m
[37m[1m[2023-07-10 17:01:05,543][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:01:15,205][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 17:01:15,205][227910] FPS: 397497.97[0m
[36m[2023-07-10 17:01:15,208][227910] itr=893, itrs=2000, Progress: 44.65%[0m
[36m[2023-07-10 17:01:27,296][227910] train() took 12.07 seconds to complete[0m
[36m[2023-07-10 17:01:27,296][227910] FPS: 318128.30[0m
[36m[2023-07-10 17:01:31,676][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:01:31,676][227910] Reward + Measures: [[1387.89615233    0.4982779     0.37628353    0.46088704    0.44103768]][0m
[37m[1m[2023-07-10 17:01:31,677][227910] Max Reward on eval: 1387.896152330374[0m
[37m[1m[2023-07-10 17:01:31,677][227910] Min Reward on eval: 1387.896152330374[0m
[37m[1m[2023-07-10 17:01:31,677][227910] Mean Reward across all agents: 1387.896152330374[0m
[37m[1m[2023-07-10 17:01:31,677][227910] Average Trajectory Length: 999.9453333333333[0m
[36m[2023-07-10 17:01:37,175][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:01:37,176][227910] Reward + Measures: [[-228.91599329    0.16829273    0.15286446    0.15411992    0.16445696]
 [-181.43461323    0.25980863    0.19930679    0.250108      0.24430488]
 [-240.82693851    0.21335237    0.19763194    0.2173759     0.21855436]
 ...
 [ 125.12603179    0.6336        0.81610006    0.66119999    0.84710008]
 [-136.27733088    0.3527        0.58240002    0.32900003    0.47220001]
 [  65.83170448    0.6257        0.84840006    0.6663        0.86739999]][0m
[37m[1m[2023-07-10 17:01:37,176][227910] Max Reward on eval: 2022.2190394895617[0m
[37m[1m[2023-07-10 17:01:37,176][227910] Min Reward on eval: -1827.031069294631[0m
[37m[1m[2023-07-10 17:01:37,177][227910] Mean Reward across all agents: 80.61122030796993[0m
[37m[1m[2023-07-10 17:01:37,177][227910] Average Trajectory Length: 990.925[0m
[36m[2023-07-10 17:01:37,180][227910] mean_value=-946.5096054446075, max_value=549.1729523136487[0m
[37m[1m[2023-07-10 17:01:37,182][227910] New mean coefficients: [[-0.07269113  0.03436599  0.9734244   0.12629801  0.0446258 ]][0m
[37m[1m[2023-07-10 17:01:37,183][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:01:46,880][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 17:01:46,881][227910] FPS: 396047.84[0m
[36m[2023-07-10 17:01:46,883][227910] itr=894, itrs=2000, Progress: 44.70%[0m
[36m[2023-07-10 17:01:59,140][227910] train() took 12.24 seconds to complete[0m
[36m[2023-07-10 17:01:59,141][227910] FPS: 313755.97[0m
[36m[2023-07-10 17:02:03,588][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:02:03,588][227910] Reward + Measures: [[1265.32136558    0.42504382    0.50152236    0.39604455    0.41665584]][0m
[37m[1m[2023-07-10 17:02:03,588][227910] Max Reward on eval: 1265.3213655817997[0m
[37m[1m[2023-07-10 17:02:03,588][227910] Min Reward on eval: 1265.3213655817997[0m
[37m[1m[2023-07-10 17:02:03,589][227910] Mean Reward across all agents: 1265.3213655817997[0m
[37m[1m[2023-07-10 17:02:03,589][227910] Average Trajectory Length: 999.9576666666667[0m
[36m[2023-07-10 17:02:09,212][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:02:09,212][227910] Reward + Measures: [[-554.43585196    0.64729995    0.79942685    0.13015853    0.76759511]
 [ 488.69510477    0.41389999    0.458         0.31650001    0.39720002]
 [  39.92175706    0.38520691    0.32270804    0.34194025    0.42741153]
 ...
 [ 596.72642377    0.5262        0.41670004    0.47830001    0.55489999]
 [  53.15804068    0.38460001    0.33990002    0.33670002    0.39740002]
 [-370.45485506    0.52969998    0.70590001    0.47250006    0.69620007]][0m
[37m[1m[2023-07-10 17:02:09,213][227910] Max Reward on eval: 1622.5375080281112[0m
[37m[1m[2023-07-10 17:02:09,213][227910] Min Reward on eval: -1017.6595170094573[0m
[37m[1m[2023-07-10 17:02:09,213][227910] Mean Reward across all agents: 322.8256063867474[0m
[37m[1m[2023-07-10 17:02:09,213][227910] Average Trajectory Length: 993.188[0m
[36m[2023-07-10 17:02:09,216][227910] mean_value=-464.57891295358473, max_value=852.6910428138485[0m
[37m[1m[2023-07-10 17:02:09,219][227910] New mean coefficients: [[-0.1734856   0.29781884  0.6517271   0.34461796  0.27339107]][0m
[37m[1m[2023-07-10 17:02:09,220][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:02:19,035][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 17:02:19,035][227910] FPS: 391287.29[0m
[36m[2023-07-10 17:02:19,038][227910] itr=895, itrs=2000, Progress: 44.75%[0m
[36m[2023-07-10 17:02:30,663][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 17:02:30,663][227910] FPS: 331012.68[0m
[36m[2023-07-10 17:02:35,335][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:02:35,336][227910] Reward + Measures: [[508.08086684   0.44020435   0.66762036   0.44571033   0.57923096]][0m
[37m[1m[2023-07-10 17:02:35,336][227910] Max Reward on eval: 508.0808668381612[0m
[37m[1m[2023-07-10 17:02:35,336][227910] Min Reward on eval: 508.0808668381612[0m
[37m[1m[2023-07-10 17:02:35,336][227910] Mean Reward across all agents: 508.0808668381612[0m
[37m[1m[2023-07-10 17:02:35,337][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:02:40,841][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:02:40,846][227910] Reward + Measures: [[729.9928018    0.2933       0.55310005   0.38720003   0.39310002]
 [514.0039479    0.23280001   0.61350006   0.41640002   0.47280002]
 [363.41845216   0.33489999   0.49890003   0.34169999   0.38680002]
 ...
 [-98.50379403   0.64899999   0.79900002   0.43329999   0.86100006]
 [502.20960753   0.30669999   0.48810002   0.35349998   0.345     ]
 [117.45820667   0.50559998   0.79080003   0.58140004   0.72010005]][0m
[37m[1m[2023-07-10 17:02:40,847][227910] Max Reward on eval: 959.263439392412[0m
[37m[1m[2023-07-10 17:02:40,847][227910] Min Reward on eval: -689.8275823652396[0m
[37m[1m[2023-07-10 17:02:40,847][227910] Mean Reward across all agents: 165.42129535600432[0m
[37m[1m[2023-07-10 17:02:40,848][227910] Average Trajectory Length: 996.0823333333333[0m
[36m[2023-07-10 17:02:40,850][227910] mean_value=-428.26902348531587, max_value=487.8312791309156[0m
[37m[1m[2023-07-10 17:02:40,853][227910] New mean coefficients: [[-0.7451744  -0.00344568  1.2755041   0.88431185  0.7507688 ]][0m
[37m[1m[2023-07-10 17:02:40,854][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:02:50,609][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 17:02:50,609][227910] FPS: 393728.51[0m
[36m[2023-07-10 17:02:50,611][227910] itr=896, itrs=2000, Progress: 44.80%[0m
[36m[2023-07-10 17:03:02,088][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 17:03:02,088][227910] FPS: 335202.23[0m
[36m[2023-07-10 17:03:06,819][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:03:06,819][227910] Reward + Measures: [[10.18279469  0.60167933  0.84869695  0.62383938  0.77987301]][0m
[37m[1m[2023-07-10 17:03:06,820][227910] Max Reward on eval: 10.182794687828773[0m
[37m[1m[2023-07-10 17:03:06,820][227910] Min Reward on eval: 10.182794687828773[0m
[37m[1m[2023-07-10 17:03:06,820][227910] Mean Reward across all agents: 10.182794687828773[0m
[37m[1m[2023-07-10 17:03:06,820][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:03:12,290][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:03:12,291][227910] Reward + Measures: [[ 125.3419375     0.44870001    0.82369995    0.50550002    0.68610001]
 [-151.59598208    0.86759996    0.83630002    0.87250006    0.88339996]
 [-124.93784852    0.94370002    0.85329992    0.87740004    0.96390003]
 ...
 [-194.22018173    0.82919997    0.93979996    0.76179999    0.95410007]
 [-125.7855919     0.86729997    0.85150003    0.82930005    0.94070005]
 [-148.70427633    0.8858        0.79290003    0.85890001    0.9242    ]][0m
[37m[1m[2023-07-10 17:03:12,291][227910] Max Reward on eval: 1142.9768315293593[0m
[37m[1m[2023-07-10 17:03:12,291][227910] Min Reward on eval: -281.2056415161002[0m
[37m[1m[2023-07-10 17:03:12,291][227910] Mean Reward across all agents: 41.36451736394092[0m
[37m[1m[2023-07-10 17:03:12,291][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:03:12,294][227910] mean_value=-327.06300200308743, max_value=576.7873700307624[0m
[37m[1m[2023-07-10 17:03:12,297][227910] New mean coefficients: [[-0.2237004  -0.14768311  1.6383336   0.8896599   0.31465703]][0m
[37m[1m[2023-07-10 17:03:12,298][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:03:21,985][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 17:03:21,985][227910] FPS: 396483.59[0m
[36m[2023-07-10 17:03:21,987][227910] itr=897, itrs=2000, Progress: 44.85%[0m
[36m[2023-07-10 17:03:33,550][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 17:03:33,550][227910] FPS: 332730.59[0m
[36m[2023-07-10 17:03:38,350][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:03:38,350][227910] Reward + Measures: [[-776.6633906     0.97385597    0.95218003    0.95629793    0.95416826]][0m
[37m[1m[2023-07-10 17:03:38,350][227910] Max Reward on eval: -776.6633905965332[0m
[37m[1m[2023-07-10 17:03:38,351][227910] Min Reward on eval: -776.6633905965332[0m
[37m[1m[2023-07-10 17:03:38,351][227910] Mean Reward across all agents: -776.6633905965332[0m
[37m[1m[2023-07-10 17:03:38,351][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:03:44,033][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:03:44,034][227910] Reward + Measures: [[-875.91033893    0.91480011    0.92229998    0.77689999    0.90220004]
 [-256.08080291    0.5007        0.62799996    0.1452        0.51279998]
 [-352.23392303    0.61090004    0.66020006    0.25400001    0.59920001]
 ...
 [-617.23280811    0.97799999    0.92810005    0.95840007    0.96520007]
 [ -20.54763621    0.83120006    0.67320001    0.74599999    0.84490007]
 [-739.6897302     0.9698        0.93580002    0.95249999    0.93549997]][0m
[37m[1m[2023-07-10 17:03:44,034][227910] Max Reward on eval: 27.259591571625787[0m
[37m[1m[2023-07-10 17:03:44,034][227910] Min Reward on eval: -1083.1508524209028[0m
[37m[1m[2023-07-10 17:03:44,034][227910] Mean Reward across all agents: -676.9727242067895[0m
[37m[1m[2023-07-10 17:03:44,035][227910] Average Trajectory Length: 999.4206666666666[0m
[36m[2023-07-10 17:03:44,036][227910] mean_value=-1131.4418451264878, max_value=-101.42298215772973[0m
[36m[2023-07-10 17:03:44,038][227910] XNES is restarting with a new solution whose measures are [0.80899996 0.45000002 0.77689999 0.82690001] and objective is 270.7087979488075[0m
[36m[2023-07-10 17:03:44,039][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 17:03:44,042][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 17:03:44,043][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:03:53,783][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 17:03:53,784][227910] FPS: 394276.44[0m
[36m[2023-07-10 17:03:53,786][227910] itr=898, itrs=2000, Progress: 44.90%[0m
[36m[2023-07-10 17:04:05,249][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 17:04:05,249][227910] FPS: 335602.43[0m
[36m[2023-07-10 17:04:10,061][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:04:10,061][227910] Reward + Measures: [[250.40073222   0.85202408   0.27850732   0.82677162   0.83311915]][0m
[37m[1m[2023-07-10 17:04:10,062][227910] Max Reward on eval: 250.40073221582514[0m
[37m[1m[2023-07-10 17:04:10,062][227910] Min Reward on eval: 250.40073221582514[0m
[37m[1m[2023-07-10 17:04:10,062][227910] Mean Reward across all agents: 250.40073221582514[0m
[37m[1m[2023-07-10 17:04:10,062][227910] Average Trajectory Length: 999.8343333333333[0m
[36m[2023-07-10 17:04:15,457][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:04:15,457][227910] Reward + Measures: [[  91.86325819    0.92440003    0.83530009    0.88819999    0.91440004]
 [-575.42134437    0.49267527    0.18458821    0.47538564    0.41601178]
 [ 262.91298166    0.40219998    0.41790006    0.47709998    0.46630001]
 ...
 [-146.03475256    0.2669        0.56240004    0.67839998    0.67589998]
 [-586.14147806    0.40743971    0.21895759    0.44105798    0.35886627]
 [-653.28955104    0.36487529    0.42076617    0.33506927    0.37756428]][0m
[37m[1m[2023-07-10 17:04:15,458][227910] Max Reward on eval: 649.0094595355331[0m
[37m[1m[2023-07-10 17:04:15,458][227910] Min Reward on eval: -1268.9131825284567[0m
[37m[1m[2023-07-10 17:04:15,458][227910] Mean Reward across all agents: -35.51666768900612[0m
[37m[1m[2023-07-10 17:04:15,459][227910] Average Trajectory Length: 987.4636666666667[0m
[36m[2023-07-10 17:04:15,462][227910] mean_value=-626.0562707439248, max_value=855.6844636934777[0m
[37m[1m[2023-07-10 17:04:15,465][227910] New mean coefficients: [[ 1.2609799  -0.03423464 -0.7722377  -1.5684116  -2.12464   ]][0m
[37m[1m[2023-07-10 17:04:15,466][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:04:25,157][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 17:04:25,157][227910] FPS: 396306.93[0m
[36m[2023-07-10 17:04:25,159][227910] itr=899, itrs=2000, Progress: 44.95%[0m
[36m[2023-07-10 17:04:36,683][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 17:04:36,683][227910] FPS: 333735.04[0m
[36m[2023-07-10 17:04:41,479][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:04:41,480][227910] Reward + Measures: [[333.33532189   0.73051107   0.23215832   0.67692131   0.64463931]][0m
[37m[1m[2023-07-10 17:04:41,480][227910] Max Reward on eval: 333.3353218914267[0m
[37m[1m[2023-07-10 17:04:41,480][227910] Min Reward on eval: 333.3353218914267[0m
[37m[1m[2023-07-10 17:04:41,481][227910] Mean Reward across all agents: 333.3353218914267[0m
[37m[1m[2023-07-10 17:04:41,481][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:04:46,921][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:04:46,921][227910] Reward + Measures: [[  99.61375659    0.68900001    0.41689998    0.58610004    0.55550003]
 [ 125.37077182    0.86120003    0.1196        0.80120003    0.68979996]
 [ 377.66384452    0.5837        0.29260001    0.5126        0.43109998]
 ...
 [1025.16822046    0.29300001    0.38170001    0.30420002    0.26869997]
 [ 265.84623761    0.55360001    0.35390002    0.5126        0.50520009]
 [ 369.52213854    0.56220001    0.33320001    0.4736        0.4217    ]][0m
[37m[1m[2023-07-10 17:04:46,921][227910] Max Reward on eval: 1424.985833466379[0m
[37m[1m[2023-07-10 17:04:46,922][227910] Min Reward on eval: -76.11104993299232[0m
[37m[1m[2023-07-10 17:04:46,922][227910] Mean Reward across all agents: 372.6777879869467[0m
[37m[1m[2023-07-10 17:04:46,922][227910] Average Trajectory Length: 997.6816666666666[0m
[36m[2023-07-10 17:04:46,924][227910] mean_value=-681.7259328066674, max_value=559.9938875229103[0m
[37m[1m[2023-07-10 17:04:46,927][227910] New mean coefficients: [[ 0.32609642 -0.0304292  -1.3730602  -1.6590312  -0.81221294]][0m
[37m[1m[2023-07-10 17:04:46,928][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:04:56,570][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 17:04:56,570][227910] FPS: 398311.34[0m
[36m[2023-07-10 17:04:56,573][227910] itr=900, itrs=2000, Progress: 45.00%[0m
[37m[1m[2023-07-10 17:05:00,081][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000880[0m
[36m[2023-07-10 17:05:11,731][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 17:05:11,731][227910] FPS: 331296.44[0m
[36m[2023-07-10 17:05:16,354][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:05:16,355][227910] Reward + Measures: [[507.04326385   0.60410661   0.30169091   0.53254443   0.51251107]][0m
[37m[1m[2023-07-10 17:05:16,355][227910] Max Reward on eval: 507.04326384617076[0m
[37m[1m[2023-07-10 17:05:16,355][227910] Min Reward on eval: 507.04326384617076[0m
[37m[1m[2023-07-10 17:05:16,355][227910] Mean Reward across all agents: 507.04326384617076[0m
[37m[1m[2023-07-10 17:05:16,356][227910] Average Trajectory Length: 999.8439999999999[0m
[36m[2023-07-10 17:05:21,976][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:05:21,977][227910] Reward + Measures: [[ -18.84478827    0.70570004    0.19939999    0.69          0.65500003]
 [ 215.2107222     0.3355        0.62400001    0.50739998    0.5801    ]
 [  92.66520429    0.69469994    0.20020001    0.68149996    0.66569996]
 ...
 [-947.47088188    0.53509998    0.63589996    0.1357        0.64579999]
 [-972.08165056    0.40529999    0.38790002    0.25119999    0.46810004]
 [ 143.57778595    0.51439995    0.1934        0.52080005    0.46919999]][0m
[37m[1m[2023-07-10 17:05:21,977][227910] Max Reward on eval: 974.1748730299063[0m
[37m[1m[2023-07-10 17:05:21,977][227910] Min Reward on eval: -1155.0652548980784[0m
[37m[1m[2023-07-10 17:05:21,977][227910] Mean Reward across all agents: 67.54371802520798[0m
[37m[1m[2023-07-10 17:05:21,978][227910] Average Trajectory Length: 991.2006666666666[0m
[36m[2023-07-10 17:05:21,979][227910] mean_value=-877.7294060733158, max_value=758.7722581964626[0m
[37m[1m[2023-07-10 17:05:21,982][227910] New mean coefficients: [[-0.6060186   0.6926546  -1.374291   -1.9718541  -0.44987178]][0m
[37m[1m[2023-07-10 17:05:21,983][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:05:31,861][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 17:05:31,862][227910] FPS: 388785.72[0m
[36m[2023-07-10 17:05:31,864][227910] itr=901, itrs=2000, Progress: 45.05%[0m
[36m[2023-07-10 17:05:43,591][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 17:05:43,591][227910] FPS: 328034.28[0m
[36m[2023-07-10 17:05:48,411][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:05:48,412][227910] Reward + Measures: [[334.82513097   0.60322505   0.28727457   0.501625     0.49581331]][0m
[37m[1m[2023-07-10 17:05:48,412][227910] Max Reward on eval: 334.8251309737203[0m
[37m[1m[2023-07-10 17:05:48,412][227910] Min Reward on eval: 334.8251309737203[0m
[37m[1m[2023-07-10 17:05:48,413][227910] Mean Reward across all agents: 334.8251309737203[0m
[37m[1m[2023-07-10 17:05:48,413][227910] Average Trajectory Length: 999.6593333333333[0m
[36m[2023-07-10 17:05:53,967][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:05:53,968][227910] Reward + Measures: [[497.86475611   0.46900001   0.4312       0.41249999   0.41249999]
 [427.55717098   0.63959998   0.2246       0.53400004   0.49310002]
 [ 68.48656227   0.74340004   0.65950006   0.68079996   0.78630006]
 ...
 [413.35010806   0.48607278   0.27739057   0.39010733   0.32995448]
 [160.80213117   0.6013       0.18889999   0.5126       0.46820003]
 [534.57826873   0.54269999   0.4093       0.45660001   0.4707    ]][0m
[37m[1m[2023-07-10 17:05:53,968][227910] Max Reward on eval: 991.0776754163438[0m
[37m[1m[2023-07-10 17:05:53,968][227910] Min Reward on eval: -74.10717965011717[0m
[37m[1m[2023-07-10 17:05:53,969][227910] Mean Reward across all agents: 313.23696956722995[0m
[37m[1m[2023-07-10 17:05:53,969][227910] Average Trajectory Length: 988.04[0m
[36m[2023-07-10 17:05:53,971][227910] mean_value=-923.088568703859, max_value=700.3172875780231[0m
[37m[1m[2023-07-10 17:05:53,974][227910] New mean coefficients: [[-0.16145325  0.2469207  -1.5918297  -1.8944807   0.7799778 ]][0m
[37m[1m[2023-07-10 17:05:53,975][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:06:03,765][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 17:06:03,765][227910] FPS: 392295.47[0m
[36m[2023-07-10 17:06:03,768][227910] itr=902, itrs=2000, Progress: 45.10%[0m
[36m[2023-07-10 17:06:15,383][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 17:06:15,383][227910] FPS: 331103.03[0m
[36m[2023-07-10 17:06:20,240][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:06:20,241][227910] Reward + Measures: [[228.89242859   0.65306699   0.21720012   0.51621646   0.49966374]][0m
[37m[1m[2023-07-10 17:06:20,241][227910] Max Reward on eval: 228.89242858834115[0m
[37m[1m[2023-07-10 17:06:20,241][227910] Min Reward on eval: 228.89242858834115[0m
[37m[1m[2023-07-10 17:06:20,241][227910] Mean Reward across all agents: 228.89242858834115[0m
[37m[1m[2023-07-10 17:06:20,241][227910] Average Trajectory Length: 998.591[0m
[36m[2023-07-10 17:06:25,821][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:06:25,822][227910] Reward + Measures: [[ 228.10007752    0.70920002    0.1646        0.542         0.49019995]
 [-246.42442078    0.53150004    0.61900002    0.5309        0.57100004]
 [  41.54560657    0.67090207    0.42242128    0.54623616    0.49562764]
 ...
 [  61.7361669     0.26970002    0.51590002    0.31430003    0.50220001]
 [  40.70483519    0.50650001    0.3545        0.38260004    0.34810001]
 [-322.00825387    0.54879999    0.38639998    0.40400001    0.35660002]][0m
[37m[1m[2023-07-10 17:06:25,822][227910] Max Reward on eval: 456.8252350067138[0m
[37m[1m[2023-07-10 17:06:25,822][227910] Min Reward on eval: -1278.2769087957452[0m
[37m[1m[2023-07-10 17:06:25,822][227910] Mean Reward across all agents: -137.68574742800303[0m
[37m[1m[2023-07-10 17:06:25,823][227910] Average Trajectory Length: 993.65[0m
[36m[2023-07-10 17:06:25,825][227910] mean_value=-1079.5793670039893, max_value=624.09515422197[0m
[37m[1m[2023-07-10 17:06:25,827][227910] New mean coefficients: [[-0.02450532  0.80510783 -2.2001858  -1.2841253   1.3325438 ]][0m
[37m[1m[2023-07-10 17:06:25,828][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:06:35,827][227910] train() took 10.00 seconds to complete[0m
[36m[2023-07-10 17:06:35,827][227910] FPS: 384119.61[0m
[36m[2023-07-10 17:06:35,829][227910] itr=903, itrs=2000, Progress: 45.15%[0m
[36m[2023-07-10 17:06:47,575][227910] train() took 11.73 seconds to complete[0m
[36m[2023-07-10 17:06:47,575][227910] FPS: 327435.23[0m
[36m[2023-07-10 17:06:52,381][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:06:52,382][227910] Reward + Measures: [[202.05878269   0.7451399    0.1391938    0.59327692   0.55045289]][0m
[37m[1m[2023-07-10 17:06:52,382][227910] Max Reward on eval: 202.05878268760225[0m
[37m[1m[2023-07-10 17:06:52,382][227910] Min Reward on eval: 202.05878268760225[0m
[37m[1m[2023-07-10 17:06:52,382][227910] Mean Reward across all agents: 202.05878268760225[0m
[37m[1m[2023-07-10 17:06:52,383][227910] Average Trajectory Length: 997.5196666666666[0m
[36m[2023-07-10 17:06:57,927][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:06:57,928][227910] Reward + Measures: [[-67.95997864   0.64370006   0.2631       0.56940001   0.55610001]
 [153.32571657   0.42130002   0.50559998   0.37810001   0.47310001]
 [419.38647751   0.37439999   0.49079999   0.41510001   0.45059997]
 ...
 [742.70402376   0.48380002   0.44989997   0.40489998   0.4224    ]
 [239.57434112   0.37359998   0.48530003   0.26910004   0.46700001]
 [759.00119474   0.45410004   0.43210003   0.35619998   0.40019998]][0m
[37m[1m[2023-07-10 17:06:57,928][227910] Max Reward on eval: 1471.3050634156214[0m
[37m[1m[2023-07-10 17:06:57,929][227910] Min Reward on eval: -601.906511788111[0m
[37m[1m[2023-07-10 17:06:57,929][227910] Mean Reward across all agents: 259.46416179218284[0m
[37m[1m[2023-07-10 17:06:57,929][227910] Average Trajectory Length: 991.2316666666667[0m
[36m[2023-07-10 17:06:57,932][227910] mean_value=-980.2841444594007, max_value=689.4770361807663[0m
[37m[1m[2023-07-10 17:06:57,934][227910] New mean coefficients: [[ 0.4722995   0.59904826 -1.0448003  -0.8035331   1.7283814 ]][0m
[37m[1m[2023-07-10 17:06:57,935][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:07:07,834][227910] train() took 9.90 seconds to complete[0m
[36m[2023-07-10 17:07:07,835][227910] FPS: 387978.77[0m
[36m[2023-07-10 17:07:07,837][227910] itr=904, itrs=2000, Progress: 45.20%[0m
[36m[2023-07-10 17:07:19,597][227910] train() took 11.74 seconds to complete[0m
[36m[2023-07-10 17:07:19,597][227910] FPS: 327019.59[0m
[36m[2023-07-10 17:07:24,355][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:07:24,355][227910] Reward + Measures: [[216.42324718   0.83103216   0.09121037   0.69518179   0.6491394 ]][0m
[37m[1m[2023-07-10 17:07:24,355][227910] Max Reward on eval: 216.42324718226428[0m
[37m[1m[2023-07-10 17:07:24,356][227910] Min Reward on eval: 216.42324718226428[0m
[37m[1m[2023-07-10 17:07:24,356][227910] Mean Reward across all agents: 216.42324718226428[0m
[37m[1m[2023-07-10 17:07:24,356][227910] Average Trajectory Length: 999.1146666666666[0m
[36m[2023-07-10 17:07:29,989][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:07:29,990][227910] Reward + Measures: [[-437.69288604    0.68309999    0.24650002    0.63180006    0.70710003]
 [ 511.74742542    0.50701165    0.39007211    0.36546743    0.37803254]
 [   3.69498308    0.47729999    0.5219        0.40000001    0.51279998]
 ...
 [ -82.14222048    0.95219994    0.03400001    0.92480004    0.88900006]
 [-150.40874589    0.39130002    0.51850003    0.33939999    0.45620003]
 [ 240.83935709    0.82870007    0.1865        0.70239997    0.671     ]][0m
[37m[1m[2023-07-10 17:07:29,990][227910] Max Reward on eval: 1057.3969608361192[0m
[37m[1m[2023-07-10 17:07:29,990][227910] Min Reward on eval: -988.9690747915768[0m
[37m[1m[2023-07-10 17:07:29,991][227910] Mean Reward across all agents: 153.801304831755[0m
[37m[1m[2023-07-10 17:07:29,991][227910] Average Trajectory Length: 991.889[0m
[36m[2023-07-10 17:07:29,994][227910] mean_value=-674.1055803734856, max_value=707.4734333792701[0m
[37m[1m[2023-07-10 17:07:29,996][227910] New mean coefficients: [[ 0.20239493  0.9557177  -0.4944734  -0.43891522  2.379943  ]][0m
[37m[1m[2023-07-10 17:07:29,998][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:07:39,819][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 17:07:39,819][227910] FPS: 391042.53[0m
[36m[2023-07-10 17:07:39,822][227910] itr=905, itrs=2000, Progress: 45.25%[0m
[36m[2023-07-10 17:07:51,536][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 17:07:51,536][227910] FPS: 328306.99[0m
[36m[2023-07-10 17:07:56,422][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:07:56,423][227910] Reward + Measures: [[216.24888264   0.87478077   0.07345662   0.75482243   0.71966004]][0m
[37m[1m[2023-07-10 17:07:56,423][227910] Max Reward on eval: 216.24888264110004[0m
[37m[1m[2023-07-10 17:07:56,423][227910] Min Reward on eval: 216.24888264110004[0m
[37m[1m[2023-07-10 17:07:56,423][227910] Mean Reward across all agents: 216.24888264110004[0m
[37m[1m[2023-07-10 17:07:56,423][227910] Average Trajectory Length: 999.5326666666666[0m
[36m[2023-07-10 17:08:01,873][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:08:01,873][227910] Reward + Measures: [[325.74946336   0.81949997   0.16040002   0.72390002   0.73509997]
 [301.49055198   0.82590002   0.18249999   0.76190007   0.78879994]
 [168.46345665   0.84850007   0.0723       0.7026       0.63190001]
 ...
 [322.43382791   0.72900003   0.27900001   0.63510001   0.69709998]
 [127.21752352   0.71890002   0.14500001   0.54500002   0.51990002]
 [363.55673407   0.87239999   0.42020002   0.82600003   0.83339995]][0m
[37m[1m[2023-07-10 17:08:01,874][227910] Max Reward on eval: 588.4747900149086[0m
[37m[1m[2023-07-10 17:08:01,874][227910] Min Reward on eval: -99.0176831554505[0m
[37m[1m[2023-07-10 17:08:01,874][227910] Mean Reward across all agents: 242.11901635234966[0m
[37m[1m[2023-07-10 17:08:01,874][227910] Average Trajectory Length: 997.9496666666666[0m
[36m[2023-07-10 17:08:01,879][227910] mean_value=-43.028534237239874, max_value=911.6887985135616[0m
[37m[1m[2023-07-10 17:08:01,882][227910] New mean coefficients: [[-0.4301817   0.03492719 -0.41929004 -1.1206654   2.1514115 ]][0m
[37m[1m[2023-07-10 17:08:01,883][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:08:11,758][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 17:08:11,759][227910] FPS: 388901.19[0m
[36m[2023-07-10 17:08:11,761][227910] itr=906, itrs=2000, Progress: 45.30%[0m
[36m[2023-07-10 17:08:23,478][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 17:08:23,478][227910] FPS: 328248.63[0m
[36m[2023-07-10 17:08:28,274][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:08:28,274][227910] Reward + Measures: [[188.61968358   0.90651369   0.06147947   0.81617713   0.7964927 ]][0m
[37m[1m[2023-07-10 17:08:28,274][227910] Max Reward on eval: 188.61968357788655[0m
[37m[1m[2023-07-10 17:08:28,275][227910] Min Reward on eval: 188.61968357788655[0m
[37m[1m[2023-07-10 17:08:28,275][227910] Mean Reward across all agents: 188.61968357788655[0m
[37m[1m[2023-07-10 17:08:28,275][227910] Average Trajectory Length: 999.1239999999999[0m
[36m[2023-07-10 17:08:33,804][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:08:33,805][227910] Reward + Measures: [[305.8173748    0.82080001   0.1117       0.65460002   0.69200003]
 [-38.29191166   0.60530001   0.52030003   0.4413       0.64889997]
 [127.87562256   0.89190006   0.1224       0.83170003   0.85210001]
 ...
 [198.46548201   0.81070006   0.23379998   0.65000004   0.72010005]
 [ 84.25574498   0.81140006   0.1797       0.73040003   0.77869999]
 [147.76428553   0.61740005   0.30969998   0.47189999   0.55370009]][0m
[37m[1m[2023-07-10 17:08:33,805][227910] Max Reward on eval: 415.0732355911052[0m
[37m[1m[2023-07-10 17:08:33,805][227910] Min Reward on eval: -337.25061073903925[0m
[37m[1m[2023-07-10 17:08:33,806][227910] Mean Reward across all agents: 114.9418518699547[0m
[37m[1m[2023-07-10 17:08:33,806][227910] Average Trajectory Length: 999.5286666666666[0m
[36m[2023-07-10 17:08:33,809][227910] mean_value=-373.1781106667219, max_value=746.1062297934835[0m
[37m[1m[2023-07-10 17:08:33,812][227910] New mean coefficients: [[-0.98762095  1.176538    0.4386259  -1.1550056   2.0105946 ]][0m
[37m[1m[2023-07-10 17:08:33,813][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:08:43,526][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 17:08:43,527][227910] FPS: 395382.32[0m
[36m[2023-07-10 17:08:43,529][227910] itr=907, itrs=2000, Progress: 45.35%[0m
[36m[2023-07-10 17:08:55,187][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 17:08:55,187][227910] FPS: 329927.30[0m
[36m[2023-07-10 17:09:00,043][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:09:00,044][227910] Reward + Measures: [[160.34908573   0.91216934   0.065499     0.82674199   0.82989329]][0m
[37m[1m[2023-07-10 17:09:00,044][227910] Max Reward on eval: 160.34908573495804[0m
[37m[1m[2023-07-10 17:09:00,044][227910] Min Reward on eval: 160.34908573495804[0m
[37m[1m[2023-07-10 17:09:00,044][227910] Mean Reward across all agents: 160.34908573495804[0m
[37m[1m[2023-07-10 17:09:00,045][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:09:05,545][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:09:05,546][227910] Reward + Measures: [[115.85097982   0.9332       0.17709999   0.8448       0.84960002]
 [153.35981885   0.9113       0.1294       0.8434       0.84979993]
 [103.82483104   0.90760005   0.34190002   0.84190005   0.87159997]
 ...
 [132.20418897   0.86630005   0.373        0.76439995   0.7978    ]
 [ 82.57300052   0.8545       0.16500001   0.6717       0.68559998]
 [ 53.94463176   0.63079995   0.60159999   0.55680001   0.73439997]][0m
[37m[1m[2023-07-10 17:09:05,546][227910] Max Reward on eval: 246.02735353705938[0m
[37m[1m[2023-07-10 17:09:05,546][227910] Min Reward on eval: -42.693819167360196[0m
[37m[1m[2023-07-10 17:09:05,546][227910] Mean Reward across all agents: 137.68023190569767[0m
[37m[1m[2023-07-10 17:09:05,547][227910] Average Trajectory Length: 999.8296666666666[0m
[36m[2023-07-10 17:09:05,550][227910] mean_value=-86.28829809437399, max_value=676.1513777275802[0m
[37m[1m[2023-07-10 17:09:05,553][227910] New mean coefficients: [[-2.337913    0.685962    0.20749044 -1.738992    2.3217447 ]][0m
[37m[1m[2023-07-10 17:09:05,554][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:09:15,295][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 17:09:15,295][227910] FPS: 394273.79[0m
[36m[2023-07-10 17:09:15,298][227910] itr=908, itrs=2000, Progress: 45.40%[0m
[36m[2023-07-10 17:09:26,880][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 17:09:26,880][227910] FPS: 332118.83[0m
[36m[2023-07-10 17:09:31,569][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:09:31,570][227910] Reward + Measures: [[109.3912727    0.91884303   0.06011866   0.819242     0.83649796]][0m
[37m[1m[2023-07-10 17:09:31,570][227910] Max Reward on eval: 109.3912727042294[0m
[37m[1m[2023-07-10 17:09:31,570][227910] Min Reward on eval: 109.3912727042294[0m
[37m[1m[2023-07-10 17:09:31,570][227910] Mean Reward across all agents: 109.3912727042294[0m
[37m[1m[2023-07-10 17:09:31,571][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:09:37,006][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:09:37,012][227910] Reward + Measures: [[  70.23729559    0.4192        0.58379996    0.26500002    0.63820004]
 [ -91.22224344    0.80120003    0.4542        0.59320003    0.69180006]
 [ 259.38428297    0.4289        0.73209995    0.15980001    0.80119991]
 ...
 [-174.07464746    0.71810001    0.192         0.59910005    0.5632    ]
 [-272.2722738     0.6127001     0.3026        0.49789998    0.58059996]
 [-160.9394944     0.62844628    0.20342837    0.39726567    0.40231344]][0m
[37m[1m[2023-07-10 17:09:37,012][227910] Max Reward on eval: 351.3204377691145[0m
[37m[1m[2023-07-10 17:09:37,013][227910] Min Reward on eval: -816.3261631655507[0m
[37m[1m[2023-07-10 17:09:37,013][227910] Mean Reward across all agents: -79.91538233025383[0m
[37m[1m[2023-07-10 17:09:37,013][227910] Average Trajectory Length: 994.7126666666667[0m
[36m[2023-07-10 17:09:37,018][227910] mean_value=-328.468854706316, max_value=749.5519906405476[0m
[37m[1m[2023-07-10 17:09:37,020][227910] New mean coefficients: [[-1.7406104   0.80272293 -0.46840233 -2.5043347   2.4940248 ]][0m
[37m[1m[2023-07-10 17:09:37,021][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:09:46,720][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 17:09:46,721][227910] FPS: 395976.60[0m
[36m[2023-07-10 17:09:46,723][227910] itr=909, itrs=2000, Progress: 45.45%[0m
[36m[2023-07-10 17:09:58,237][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 17:09:58,237][227910] FPS: 334010.77[0m
[36m[2023-07-10 17:10:02,982][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:10:02,982][227910] Reward + Measures: [[56.6850475   0.91450733  0.06322159  0.78036237  0.82670176]][0m
[37m[1m[2023-07-10 17:10:02,982][227910] Max Reward on eval: 56.68504749976661[0m
[37m[1m[2023-07-10 17:10:02,983][227910] Min Reward on eval: 56.68504749976661[0m
[37m[1m[2023-07-10 17:10:02,983][227910] Mean Reward across all agents: 56.68504749976661[0m
[37m[1m[2023-07-10 17:10:02,983][227910] Average Trajectory Length: 999.0996666666666[0m
[36m[2023-07-10 17:10:08,728][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:10:08,729][227910] Reward + Measures: [[ 48.15501418   0.72530001   0.46359998   0.65259999   0.75769997]
 [ 80.92347168   0.91499996   0.0486       0.84170002   0.83960003]
 [-53.75756376   0.3637       0.53719997   0.3626       0.5072    ]
 ...
 [-91.00806063   0.40380001   0.57460004   0.42480001   0.58990002]
 [ 33.3650785    0.42859998   0.58220005   0.36899999   0.56500006]
 [ 94.45517091   0.87620002   0.13200001   0.75210005   0.81590003]][0m
[37m[1m[2023-07-10 17:10:08,729][227910] Max Reward on eval: 182.18237965431763[0m
[37m[1m[2023-07-10 17:10:08,729][227910] Min Reward on eval: -669.8095915900485[0m
[37m[1m[2023-07-10 17:10:08,730][227910] Mean Reward across all agents: -9.143240955081838[0m
[37m[1m[2023-07-10 17:10:08,730][227910] Average Trajectory Length: 996.144[0m
[36m[2023-07-10 17:10:08,733][227910] mean_value=-446.51306348335737, max_value=643.3901557939738[0m
[37m[1m[2023-07-10 17:10:08,735][227910] New mean coefficients: [[-1.0697403   0.3033853  -0.20235795 -1.9271086   1.9846973 ]][0m
[37m[1m[2023-07-10 17:10:08,736][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:10:18,468][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 17:10:18,468][227910] FPS: 394670.35[0m
[36m[2023-07-10 17:10:18,471][227910] itr=910, itrs=2000, Progress: 45.50%[0m
[37m[1m[2023-07-10 17:10:21,961][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000890[0m
[36m[2023-07-10 17:10:33,574][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 17:10:33,575][227910] FPS: 332908.79[0m
[36m[2023-07-10 17:10:38,265][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:10:38,265][227910] Reward + Measures: [[31.92244295  0.89048493  0.08008367  0.666228    0.77991462]][0m
[37m[1m[2023-07-10 17:10:38,265][227910] Max Reward on eval: 31.922442945674604[0m
[37m[1m[2023-07-10 17:10:38,265][227910] Min Reward on eval: 31.922442945674604[0m
[37m[1m[2023-07-10 17:10:38,266][227910] Mean Reward across all agents: 31.922442945674604[0m
[37m[1m[2023-07-10 17:10:38,266][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:10:43,713][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:10:43,714][227910] Reward + Measures: [[160.96444121   0.86610001   0.0936       0.67920005   0.73790002]
 [ 18.81394041   0.68430007   0.18179999   0.47390005   0.51800007]
 [ 70.43593591   0.89680004   0.14210002   0.72840005   0.81720001]
 ...
 [ 83.18200042   0.8768       0.0573       0.59179997   0.68279999]
 [134.67276107   0.77979994   0.32800001   0.6767       0.7525    ]
 [149.6186817    0.77810001   0.1247       0.57749999   0.62760001]][0m
[37m[1m[2023-07-10 17:10:43,714][227910] Max Reward on eval: 222.89723188010393[0m
[37m[1m[2023-07-10 17:10:43,714][227910] Min Reward on eval: -103.2988269273541[0m
[37m[1m[2023-07-10 17:10:43,714][227910] Mean Reward across all agents: 73.71725248850805[0m
[37m[1m[2023-07-10 17:10:43,715][227910] Average Trajectory Length: 998.7693333333333[0m
[36m[2023-07-10 17:10:43,719][227910] mean_value=110.50052072428075, max_value=596.8478327877586[0m
[37m[1m[2023-07-10 17:10:43,722][227910] New mean coefficients: [[-1.1372685  -0.29269764 -0.66367316 -1.9686158   1.8608272 ]][0m
[37m[1m[2023-07-10 17:10:43,723][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:10:53,488][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 17:10:53,489][227910] FPS: 393302.03[0m
[36m[2023-07-10 17:10:53,491][227910] itr=911, itrs=2000, Progress: 45.55%[0m
[36m[2023-07-10 17:11:04,923][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 17:11:04,923][227910] FPS: 336416.81[0m
[36m[2023-07-10 17:11:09,645][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:11:09,645][227910] Reward + Measures: [[-14.10095919   0.87042999   0.09848958   0.61752278   0.76976627]][0m
[37m[1m[2023-07-10 17:11:09,646][227910] Max Reward on eval: -14.100959186999347[0m
[37m[1m[2023-07-10 17:11:09,646][227910] Min Reward on eval: -14.100959186999347[0m
[37m[1m[2023-07-10 17:11:09,646][227910] Mean Reward across all agents: -14.100959186999347[0m
[37m[1m[2023-07-10 17:11:09,646][227910] Average Trajectory Length: 999.348[0m
[36m[2023-07-10 17:11:15,253][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:11:15,253][227910] Reward + Measures: [[-163.22018242    0.93699998    0.0423        0.86770004    0.8761    ]
 [-120.54478555    0.95600003    0.0345        0.9339        0.92830002]
 [  -0.35221814    0.89480001    0.0671        0.62589997    0.74210006]
 ...
 [  48.49693267    0.81529999    0.12800001    0.58829999    0.6376    ]
 [-163.41765531    0.92500001    0.06          0.884         0.89160007]
 [  11.52369795    0.86549997    0.09730001    0.61330003    0.75010008]][0m
[37m[1m[2023-07-10 17:11:15,253][227910] Max Reward on eval: 73.80253801021027[0m
[37m[1m[2023-07-10 17:11:15,254][227910] Min Reward on eval: -425.8682585165254[0m
[37m[1m[2023-07-10 17:11:15,254][227910] Mean Reward across all agents: -113.22893003996792[0m
[37m[1m[2023-07-10 17:11:15,254][227910] Average Trajectory Length: 998.2526666666666[0m
[36m[2023-07-10 17:11:15,258][227910] mean_value=-217.27929062285222, max_value=548.1191892633818[0m
[37m[1m[2023-07-10 17:11:15,261][227910] New mean coefficients: [[-1.8360181   0.11299884  0.92695534 -1.7982326   1.7151837 ]][0m
[37m[1m[2023-07-10 17:11:15,262][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:11:25,002][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 17:11:25,002][227910] FPS: 394329.76[0m
[36m[2023-07-10 17:11:25,004][227910] itr=912, itrs=2000, Progress: 45.60%[0m
[36m[2023-07-10 17:11:36,576][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 17:11:36,576][227910] FPS: 332370.39[0m
[36m[2023-07-10 17:11:41,434][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:11:41,435][227910] Reward + Measures: [[-90.90425139   0.83983511   0.13659985   0.5544098    0.74015397]][0m
[37m[1m[2023-07-10 17:11:41,435][227910] Max Reward on eval: -90.90425138923564[0m
[37m[1m[2023-07-10 17:11:41,435][227910] Min Reward on eval: -90.90425138923564[0m
[37m[1m[2023-07-10 17:11:41,435][227910] Mean Reward across all agents: -90.90425138923564[0m
[37m[1m[2023-07-10 17:11:41,435][227910] Average Trajectory Length: 999.746[0m
[36m[2023-07-10 17:11:46,872][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:11:46,877][227910] Reward + Measures: [[ -40.02823902    0.9278        0.0326        0.64430004    0.71250004]
 [-141.12384917    0.77700007    0.20560002    0.51929998    0.69910002]
 [  25.68836394    0.92369998    0.0516        0.63260001    0.78879994]
 ...
 [ -50.18777521    0.74610007    0.23000002    0.55089998    0.68219995]
 [-102.77904888    0.87110007    0.0787        0.57120001    0.63590002]
 [-127.98740674    0.83850002    0.13960001    0.59570003    0.65710002]][0m
[37m[1m[2023-07-10 17:11:46,878][227910] Max Reward on eval: 200.81040389126866[0m
[37m[1m[2023-07-10 17:11:46,878][227910] Min Reward on eval: -256.92311681312276[0m
[37m[1m[2023-07-10 17:11:46,878][227910] Mean Reward across all agents: -47.32134982229875[0m
[37m[1m[2023-07-10 17:11:46,878][227910] Average Trajectory Length: 999.756[0m
[36m[2023-07-10 17:11:46,882][227910] mean_value=-9.559248809849342, max_value=497.7882794200792[0m
[37m[1m[2023-07-10 17:11:46,885][227910] New mean coefficients: [[-1.3293889   0.45258737  1.1500003  -1.1332812   1.6302166 ]][0m
[37m[1m[2023-07-10 17:11:46,886][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:11:56,567][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 17:11:56,568][227910] FPS: 396687.90[0m
[36m[2023-07-10 17:11:56,570][227910] itr=913, itrs=2000, Progress: 45.65%[0m
[36m[2023-07-10 17:12:08,008][227910] train() took 11.42 seconds to complete[0m
[36m[2023-07-10 17:12:08,009][227910] FPS: 336351.65[0m
[36m[2023-07-10 17:12:12,741][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:12:12,742][227910] Reward + Measures: [[-130.9345537     0.80646092    0.17624687    0.53606761    0.73638541]][0m
[37m[1m[2023-07-10 17:12:12,742][227910] Max Reward on eval: -130.93455370284826[0m
[37m[1m[2023-07-10 17:12:12,742][227910] Min Reward on eval: -130.93455370284826[0m
[37m[1m[2023-07-10 17:12:12,742][227910] Mean Reward across all agents: -130.93455370284826[0m
[37m[1m[2023-07-10 17:12:12,743][227910] Average Trajectory Length: 999.6893333333333[0m
[36m[2023-07-10 17:12:18,167][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:12:18,168][227910] Reward + Measures: [[ -37.89842       0.78770012    0.15880001    0.61649996    0.69169998]
 [-170.04256018    0.67659998    0.30520001    0.39680001    0.62130004]
 [ -38.85077812    0.72970003    0.3066        0.54589999    0.72230005]
 ...
 [ -15.84232334    0.537         0.45510003    0.29969999    0.56399995]
 [-344.04435994    0.75340003    0.16970001    0.46510002    0.53319997]
 [ -68.32304579    0.75369996    0.2617        0.48750001    0.72900003]][0m
[37m[1m[2023-07-10 17:12:18,168][227910] Max Reward on eval: 418.65060504650467[0m
[37m[1m[2023-07-10 17:12:18,168][227910] Min Reward on eval: -1561.8600216544232[0m
[37m[1m[2023-07-10 17:12:18,168][227910] Mean Reward across all agents: -224.99086190524997[0m
[37m[1m[2023-07-10 17:12:18,169][227910] Average Trajectory Length: 991.165[0m
[36m[2023-07-10 17:12:18,173][227910] mean_value=-361.8640345523834, max_value=504.71232627017537[0m
[37m[1m[2023-07-10 17:12:18,176][227910] New mean coefficients: [[-0.6164337   0.64749396  1.2183374  -1.6845387   2.5393653 ]][0m
[37m[1m[2023-07-10 17:12:18,177][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:12:27,841][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 17:12:27,842][227910] FPS: 397411.95[0m
[36m[2023-07-10 17:12:27,844][227910] itr=914, itrs=2000, Progress: 45.70%[0m
[36m[2023-07-10 17:12:39,514][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 17:12:39,514][227910] FPS: 329638.17[0m
[36m[2023-07-10 17:12:44,230][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:12:44,230][227910] Reward + Measures: [[-129.29097883    0.7215153     0.28685001    0.46801034    0.72004962]][0m
[37m[1m[2023-07-10 17:12:44,230][227910] Max Reward on eval: -129.29097882593777[0m
[37m[1m[2023-07-10 17:12:44,231][227910] Min Reward on eval: -129.29097882593777[0m
[37m[1m[2023-07-10 17:12:44,231][227910] Mean Reward across all agents: -129.29097882593777[0m
[37m[1m[2023-07-10 17:12:44,231][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:12:49,734][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:12:49,739][227910] Reward + Measures: [[-205.36958766    0.83820003    0.09240001    0.62640011    0.65499997]
 [-224.64420032    0.87460005    0.09299999    0.77170002    0.778     ]
 [ -69.7134752     0.83050007    0.1313        0.59040004    0.73079997]
 ...
 [-407.05395597    0.85087961    0.11941297    0.77873147    0.8094759 ]
 [-414.37143744    0.83579999    0.0778        0.81240004    0.70609999]
 [-216.70513904    0.89990008    0.0891        0.8064        0.86280006]][0m
[37m[1m[2023-07-10 17:12:49,740][227910] Max Reward on eval: 69.93831370801199[0m
[37m[1m[2023-07-10 17:12:49,740][227910] Min Reward on eval: -596.7543722230591[0m
[37m[1m[2023-07-10 17:12:49,740][227910] Mean Reward across all agents: -258.8534479835556[0m
[37m[1m[2023-07-10 17:12:49,740][227910] Average Trajectory Length: 997.389[0m
[36m[2023-07-10 17:12:49,743][227910] mean_value=-533.0743799066499, max_value=383.98444032058995[0m
[37m[1m[2023-07-10 17:12:49,745][227910] New mean coefficients: [[ 0.5512505   1.0578063   0.43073863 -1.5456638   1.7349343 ]][0m
[37m[1m[2023-07-10 17:12:49,747][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:12:59,589][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 17:12:59,589][227910] FPS: 390211.20[0m
[36m[2023-07-10 17:12:59,591][227910] itr=915, itrs=2000, Progress: 45.75%[0m
[36m[2023-07-10 17:13:11,287][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 17:13:11,288][227910] FPS: 328812.55[0m
[36m[2023-07-10 17:13:16,127][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:13:16,127][227910] Reward + Measures: [[-71.57938915   0.6223436    0.43968499   0.32223031   0.67591465]][0m
[37m[1m[2023-07-10 17:13:16,127][227910] Max Reward on eval: -71.57938914552982[0m
[37m[1m[2023-07-10 17:13:16,127][227910] Min Reward on eval: -71.57938914552982[0m
[37m[1m[2023-07-10 17:13:16,128][227910] Mean Reward across all agents: -71.57938914552982[0m
[37m[1m[2023-07-10 17:13:16,128][227910] Average Trajectory Length: 999.8443333333333[0m
[36m[2023-07-10 17:13:21,636][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:13:21,641][227910] Reward + Measures: [[ -43.21314064    0.61550003    0.46750003    0.41249999    0.66890001]
 [ -64.4327335     0.60110003    0.55590004    0.3691        0.67769998]
 [ -23.6821799     0.74130005    0.64380002    0.57440001    0.80940002]
 ...
 [-148.92300671    0.69709998    0.27780002    0.36919999    0.65959996]
 [-111.30855006    0.83939999    0.58420002    0.72850001    0.89050001]
 [ -23.69597679    0.55600005    0.4851        0.3506        0.68219995]][0m
[37m[1m[2023-07-10 17:13:21,642][227910] Max Reward on eval: 42.61561472516041[0m
[37m[1m[2023-07-10 17:13:21,642][227910] Min Reward on eval: -212.99193838714854[0m
[37m[1m[2023-07-10 17:13:21,642][227910] Mean Reward across all agents: -48.49537108284319[0m
[37m[1m[2023-07-10 17:13:21,642][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:13:21,647][227910] mean_value=-89.26321731249821, max_value=501.50175531802233[0m
[37m[1m[2023-07-10 17:13:21,650][227910] New mean coefficients: [[-0.13282013  1.0525143   0.9708744  -0.99609303  2.0386841 ]][0m
[37m[1m[2023-07-10 17:13:21,651][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:13:31,442][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 17:13:31,443][227910] FPS: 392242.16[0m
[36m[2023-07-10 17:13:31,445][227910] itr=916, itrs=2000, Progress: 45.80%[0m
[36m[2023-07-10 17:13:42,951][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 17:13:42,951][227910] FPS: 334256.44[0m
[36m[2023-07-10 17:13:47,679][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:13:47,679][227910] Reward + Measures: [[-51.70851288   0.59528631   0.58142871   0.29008532   0.70500761]][0m
[37m[1m[2023-07-10 17:13:47,680][227910] Max Reward on eval: -51.70851288021142[0m
[37m[1m[2023-07-10 17:13:47,680][227910] Min Reward on eval: -51.70851288021142[0m
[37m[1m[2023-07-10 17:13:47,680][227910] Mean Reward across all agents: -51.70851288021142[0m
[37m[1m[2023-07-10 17:13:47,680][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:13:53,183][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:13:53,184][227910] Reward + Measures: [[-106.6075401     0.67300004    0.55439997    0.41550002    0.70819998]
 [ -75.77243459    0.6063        0.73270005    0.30060002    0.72539991]
 [ -66.59838373    0.59700006    0.69910002    0.36809999    0.74310005]
 ...
 [-105.26184742    0.63270009    0.52860004    0.36609998    0.72469997]
 [-219.27267138    0.82970011    0.1242        0.76620007    0.73430002]
 [ -82.62421168    0.52150005    0.52090001    0.31609997    0.64719999]][0m
[37m[1m[2023-07-10 17:13:53,184][227910] Max Reward on eval: 143.62037857321556[0m
[37m[1m[2023-07-10 17:13:53,184][227910] Min Reward on eval: -302.3313816600479[0m
[37m[1m[2023-07-10 17:13:53,184][227910] Mean Reward across all agents: -97.55158318344276[0m
[37m[1m[2023-07-10 17:13:53,185][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:13:53,189][227910] mean_value=-251.27686738308674, max_value=454.9343363152933[0m
[37m[1m[2023-07-10 17:13:53,191][227910] New mean coefficients: [[ 0.14752725  1.3778074   0.40688276 -0.9518519   1.9038976 ]][0m
[37m[1m[2023-07-10 17:13:53,193][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:14:02,836][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 17:14:02,836][227910] FPS: 398289.29[0m
[36m[2023-07-10 17:14:02,838][227910] itr=917, itrs=2000, Progress: 45.85%[0m
[36m[2023-07-10 17:14:14,315][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 17:14:14,316][227910] FPS: 335091.00[0m
[36m[2023-07-10 17:14:19,007][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:14:19,013][227910] Reward + Measures: [[-33.0963577    0.70944959   0.70576799   0.38261864   0.7787993 ]][0m
[37m[1m[2023-07-10 17:14:19,013][227910] Max Reward on eval: -33.09635770262638[0m
[37m[1m[2023-07-10 17:14:19,013][227910] Min Reward on eval: -33.09635770262638[0m
[37m[1m[2023-07-10 17:14:19,014][227910] Mean Reward across all agents: -33.09635770262638[0m
[37m[1m[2023-07-10 17:14:19,014][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:14:24,513][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:14:24,519][227910] Reward + Measures: [[ -90.0670201     0.69410002    0.5381        0.361         0.75500005]
 [ -24.96676484    0.7719        0.79269999    0.49130002    0.79030001]
 [  25.15503952    0.35630003    0.62239999    0.22140001    0.62130004]
 ...
 [ -54.82008716    0.91239995    0.85860008    0.74629998    0.88880008]
 [-148.28594517    0.368         0.51539999    0.35200003    0.67089999]
 [-112.65479915    0.70059997    0.44400001    0.50809997    0.82410002]][0m
[37m[1m[2023-07-10 17:14:24,519][227910] Max Reward on eval: 128.70192872232874[0m
[37m[1m[2023-07-10 17:14:24,520][227910] Min Reward on eval: -363.9660619193222[0m
[37m[1m[2023-07-10 17:14:24,520][227910] Mean Reward across all agents: -46.066592215529674[0m
[37m[1m[2023-07-10 17:14:24,520][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:14:24,525][227910] mean_value=-85.83283322060701, max_value=505.1197009635639[0m
[37m[1m[2023-07-10 17:14:24,527][227910] New mean coefficients: [[ 0.7737944  1.227782   0.4888414 -0.4622616  1.1692699]][0m
[37m[1m[2023-07-10 17:14:24,529][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:14:34,174][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 17:14:34,174][227910] FPS: 398189.83[0m
[36m[2023-07-10 17:14:34,177][227910] itr=918, itrs=2000, Progress: 45.90%[0m
[36m[2023-07-10 17:14:45,586][227910] train() took 11.39 seconds to complete[0m
[36m[2023-07-10 17:14:45,587][227910] FPS: 337069.24[0m
[36m[2023-07-10 17:14:50,366][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:14:50,366][227910] Reward + Measures: [[5.50007008 0.78277606 0.79459637 0.45400995 0.82155174]][0m
[37m[1m[2023-07-10 17:14:50,366][227910] Max Reward on eval: 5.500070081569567[0m
[37m[1m[2023-07-10 17:14:50,366][227910] Min Reward on eval: 5.500070081569567[0m
[37m[1m[2023-07-10 17:14:50,367][227910] Mean Reward across all agents: 5.500070081569567[0m
[37m[1m[2023-07-10 17:14:50,367][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:14:55,819][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:14:55,824][227910] Reward + Measures: [[-303.01334972    0.80030006    0.4165        0.68970007    0.75349998]
 [-106.36691987    0.67699999    0.26760003    0.49379998    0.6638    ]
 [-214.14476681    0.59549999    0.3168        0.4348        0.4745    ]
 ...
 [ -85.25234581    0.77930003    0.126         0.5851        0.60589999]
 [ -24.66397222    0.55970001    0.34030002    0.49270001    0.5535    ]
 [   7.42035879    0.39250001    0.48370001    0.30020002    0.54969996]][0m
[37m[1m[2023-07-10 17:14:55,825][227910] Max Reward on eval: 276.36483696390644[0m
[37m[1m[2023-07-10 17:14:55,825][227910] Min Reward on eval: -621.2318757383036[0m
[37m[1m[2023-07-10 17:14:55,825][227910] Mean Reward across all agents: -47.10606263630163[0m
[37m[1m[2023-07-10 17:14:55,825][227910] Average Trajectory Length: 997.1323333333333[0m
[36m[2023-07-10 17:14:55,829][227910] mean_value=-465.6770211518969, max_value=523.9574453149573[0m
[37m[1m[2023-07-10 17:14:55,832][227910] New mean coefficients: [[ 0.25563204  1.7762915   0.6330222  -1.0357128   1.4610888 ]][0m
[37m[1m[2023-07-10 17:14:55,833][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:15:05,394][227910] train() took 9.56 seconds to complete[0m
[36m[2023-07-10 17:15:05,394][227910] FPS: 401709.46[0m
[36m[2023-07-10 17:15:05,396][227910] itr=919, itrs=2000, Progress: 45.95%[0m
[36m[2023-07-10 17:15:16,940][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 17:15:16,940][227910] FPS: 333152.64[0m
[36m[2023-07-10 17:15:21,876][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:15:21,877][227910] Reward + Measures: [[30.7692143   0.82444304  0.83170199  0.47606435  0.84565932]][0m
[37m[1m[2023-07-10 17:15:21,877][227910] Max Reward on eval: 30.769214297500778[0m
[37m[1m[2023-07-10 17:15:21,877][227910] Min Reward on eval: 30.769214297500778[0m
[37m[1m[2023-07-10 17:15:21,878][227910] Mean Reward across all agents: 30.769214297500778[0m
[37m[1m[2023-07-10 17:15:21,878][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:15:27,258][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:15:27,259][227910] Reward + Measures: [[  4.51569744   0.45600006   0.65270001   0.18130001   0.7021001 ]
 [ -0.52684225   0.7579       0.81700003   0.42570001   0.82849997]
 [183.15832302   0.40773016   0.38439569   0.26650912   0.3308866 ]
 ...
 [-59.08965573   0.96820003   0.92039996   0.89470005   0.95290005]
 [ 41.88013658   0.43560001   0.47390005   0.23730002   0.4535    ]
 [-29.89365132   0.37809998   0.60970002   0.19670001   0.66959995]][0m
[37m[1m[2023-07-10 17:15:27,259][227910] Max Reward on eval: 268.1948439027066[0m
[37m[1m[2023-07-10 17:15:27,259][227910] Min Reward on eval: -735.7048934923951[0m
[37m[1m[2023-07-10 17:15:27,260][227910] Mean Reward across all agents: -43.33353779846667[0m
[37m[1m[2023-07-10 17:15:27,260][227910] Average Trajectory Length: 998.3626666666667[0m
[36m[2023-07-10 17:15:27,263][227910] mean_value=-640.8075685927031, max_value=513.4068794334191[0m
[37m[1m[2023-07-10 17:15:27,266][227910] New mean coefficients: [[-0.04086217  2.016405    1.1070786  -1.2980192   1.6500093 ]][0m
[37m[1m[2023-07-10 17:15:27,267][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:15:37,054][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 17:15:37,055][227910] FPS: 392397.67[0m
[36m[2023-07-10 17:15:37,057][227910] itr=920, itrs=2000, Progress: 46.00%[0m
[37m[1m[2023-07-10 17:15:40,732][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000900[0m
[36m[2023-07-10 17:15:52,376][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 17:15:52,376][227910] FPS: 333715.03[0m
[36m[2023-07-10 17:15:57,078][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:15:57,078][227910] Reward + Measures: [[41.47070367  0.88327897  0.88562167  0.54461664  0.88304001]][0m
[37m[1m[2023-07-10 17:15:57,078][227910] Max Reward on eval: 41.470703668097826[0m
[37m[1m[2023-07-10 17:15:57,079][227910] Min Reward on eval: 41.470703668097826[0m
[37m[1m[2023-07-10 17:15:57,079][227910] Mean Reward across all agents: 41.470703668097826[0m
[37m[1m[2023-07-10 17:15:57,079][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:16:02,574][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:16:02,574][227910] Reward + Measures: [[ 80.24946086   0.92889994   0.90819997   0.68989998   0.93350011]
 [ 27.45077985   0.94599992   0.9472       0.82849997   0.95710003]
 [ 26.8462086    0.81610006   0.87229997   0.42680001   0.83410007]
 ...
 [-61.46813286   0.95620006   0.91860002   0.72319996   0.94890004]
 [ 31.88458619   0.64219999   0.63510001   0.24959998   0.73360002]
 [  1.24434219   0.87299997   0.86309999   0.52010006   0.86199993]][0m
[37m[1m[2023-07-10 17:16:02,575][227910] Max Reward on eval: 150.80912924917646[0m
[37m[1m[2023-07-10 17:16:02,575][227910] Min Reward on eval: -166.47825674363412[0m
[37m[1m[2023-07-10 17:16:02,575][227910] Mean Reward across all agents: 16.21787100549165[0m
[37m[1m[2023-07-10 17:16:02,575][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:16:02,581][227910] mean_value=208.30567075858815, max_value=596.130255863606[0m
[37m[1m[2023-07-10 17:16:02,584][227910] New mean coefficients: [[ 0.15806474  2.3210845   0.9232694  -0.69735694  2.8048303 ]][0m
[37m[1m[2023-07-10 17:16:02,585][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:16:12,532][227910] train() took 9.95 seconds to complete[0m
[36m[2023-07-10 17:16:12,532][227910] FPS: 386098.08[0m
[36m[2023-07-10 17:16:12,535][227910] itr=921, itrs=2000, Progress: 46.05%[0m
[36m[2023-07-10 17:16:24,116][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 17:16:24,116][227910] FPS: 332164.98[0m
[36m[2023-07-10 17:16:28,918][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:16:28,918][227910] Reward + Measures: [[4.14256886 0.9501434  0.93159926 0.70785332 0.93920034]][0m
[37m[1m[2023-07-10 17:16:28,919][227910] Max Reward on eval: 4.142568864149713[0m
[37m[1m[2023-07-10 17:16:28,919][227910] Min Reward on eval: 4.142568864149713[0m
[37m[1m[2023-07-10 17:16:28,919][227910] Mean Reward across all agents: 4.142568864149713[0m
[37m[1m[2023-07-10 17:16:28,919][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:16:34,331][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:16:34,331][227910] Reward + Measures: [[67.43259087  0.87029999  0.90539998  0.49639997  0.86070007]
 [49.47883468  0.9472      0.92890006  0.7015      0.9479    ]
 [38.09994155  0.91000003  0.92830002  0.62220001  0.90070003]
 ...
 [82.15534432  0.80409998  0.84520006  0.35800001  0.84670001]
 [51.52260835  0.84709996  0.88749999  0.42860004  0.86920005]
 [-2.54256461  0.96550006  0.93349999  0.75830001  0.9684    ]][0m
[37m[1m[2023-07-10 17:16:34,331][227910] Max Reward on eval: 149.53693997164956[0m
[37m[1m[2023-07-10 17:16:34,332][227910] Min Reward on eval: -90.36092675058171[0m
[37m[1m[2023-07-10 17:16:34,332][227910] Mean Reward across all agents: 31.533661446034312[0m
[37m[1m[2023-07-10 17:16:34,332][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:16:34,338][227910] mean_value=87.4603412548546, max_value=577.7005146487139[0m
[37m[1m[2023-07-10 17:16:34,340][227910] New mean coefficients: [[ 0.48860294  2.363593    0.4434795  -1.3726704   2.6621804 ]][0m
[37m[1m[2023-07-10 17:16:34,341][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:16:44,065][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 17:16:44,065][227910] FPS: 394975.63[0m
[36m[2023-07-10 17:16:44,068][227910] itr=922, itrs=2000, Progress: 46.10%[0m
[36m[2023-07-10 17:16:55,761][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 17:16:55,761][227910] FPS: 328935.20[0m
[36m[2023-07-10 17:17:00,515][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:17:00,515][227910] Reward + Measures: [[-13.46678792   0.96532232   0.92974138   0.735434     0.96046031]][0m
[37m[1m[2023-07-10 17:17:00,516][227910] Max Reward on eval: -13.466787920958135[0m
[37m[1m[2023-07-10 17:17:00,516][227910] Min Reward on eval: -13.466787920958135[0m
[37m[1m[2023-07-10 17:17:00,516][227910] Mean Reward across all agents: -13.466787920958135[0m
[37m[1m[2023-07-10 17:17:00,516][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:17:05,863][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:17:05,864][227910] Reward + Measures: [[ 34.49735761   0.89930004   0.88949996   0.48810002   0.90710002]
 [ 59.28634013   0.94559997   0.90869999   0.58400005   0.91499996]
 [ 92.8921979    0.70520002   0.72349995   0.20130001   0.77489996]
 ...
 [ 58.10074002   0.93099993   0.92059994   0.60680002   0.93079996]
 [ 99.33246235   0.88140005   0.88569993   0.39579999   0.87089998]
 [160.18716483   0.68540001   0.74049997   0.1772       0.76389998]][0m
[37m[1m[2023-07-10 17:17:05,864][227910] Max Reward on eval: 250.99873738867464[0m
[37m[1m[2023-07-10 17:17:05,864][227910] Min Reward on eval: -95.0997672886704[0m
[37m[1m[2023-07-10 17:17:05,864][227910] Mean Reward across all agents: 71.45871111208128[0m
[37m[1m[2023-07-10 17:17:05,865][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:17:05,871][227910] mean_value=246.20082805706502, max_value=630.8329285626882[0m
[37m[1m[2023-07-10 17:17:05,873][227910] New mean coefficients: [[ 0.5227332   2.3578167   0.22418155 -1.4600791   2.9738703 ]][0m
[37m[1m[2023-07-10 17:17:05,874][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:17:15,667][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 17:17:15,667][227910] FPS: 392218.40[0m
[36m[2023-07-10 17:17:15,669][227910] itr=923, itrs=2000, Progress: 46.15%[0m
[36m[2023-07-10 17:17:27,264][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 17:17:27,265][227910] FPS: 331775.27[0m
[36m[2023-07-10 17:17:32,014][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:17:32,019][227910] Reward + Measures: [[25.79788904  0.95726097  0.90742368  0.65368295  0.95622802]][0m
[37m[1m[2023-07-10 17:17:32,020][227910] Max Reward on eval: 25.79788904475414[0m
[37m[1m[2023-07-10 17:17:32,020][227910] Min Reward on eval: 25.79788904475414[0m
[37m[1m[2023-07-10 17:17:32,020][227910] Mean Reward across all agents: 25.79788904475414[0m
[37m[1m[2023-07-10 17:17:32,020][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:17:37,641][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:17:37,642][227910] Reward + Measures: [[ 89.91369364   0.97250003   0.93230003   0.73489994   0.97180003]
 [ 55.24675319   0.63560003   0.66070002   0.17209999   0.6979    ]
 [ 76.47944865   0.9429       0.91050005   0.60210001   0.94729996]
 ...
 [ 86.58832321   0.89449996   0.8120001    0.49200001   0.90799999]
 [ 52.92654737   0.90889996   0.89609998   0.53859997   0.91450006]
 [113.81088015   0.81440002   0.79640001   0.35969999   0.82709998]][0m
[37m[1m[2023-07-10 17:17:37,642][227910] Max Reward on eval: 206.27568950512213[0m
[37m[1m[2023-07-10 17:17:37,643][227910] Min Reward on eval: -75.14153004991822[0m
[37m[1m[2023-07-10 17:17:37,643][227910] Mean Reward across all agents: 66.3595944528863[0m
[37m[1m[2023-07-10 17:17:37,643][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:17:37,649][227910] mean_value=150.58782548737548, max_value=647.315494702442[0m
[37m[1m[2023-07-10 17:17:37,652][227910] New mean coefficients: [[ 0.89794445  2.7589793   0.96479523 -1.4984837   2.8519068 ]][0m
[37m[1m[2023-07-10 17:17:37,653][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:17:47,494][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 17:17:47,494][227910] FPS: 390263.16[0m
[36m[2023-07-10 17:17:47,497][227910] itr=924, itrs=2000, Progress: 46.20%[0m
[36m[2023-07-10 17:17:59,120][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 17:17:59,121][227910] FPS: 330858.12[0m
[36m[2023-07-10 17:18:03,845][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:18:03,845][227910] Reward + Measures: [[87.75934156  0.9574253   0.91820508  0.63537997  0.95672262]][0m
[37m[1m[2023-07-10 17:18:03,846][227910] Max Reward on eval: 87.75934155773166[0m
[37m[1m[2023-07-10 17:18:03,846][227910] Min Reward on eval: 87.75934155773166[0m
[37m[1m[2023-07-10 17:18:03,846][227910] Mean Reward across all agents: 87.75934155773166[0m
[37m[1m[2023-07-10 17:18:03,846][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:18:09,387][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:18:09,388][227910] Reward + Measures: [[   7.53308966    0.88169998    0.91029996    0.41430002    0.86679995]
 [-241.95262553    0.97780001    0.95209998    0.85060006    0.98750001]
 [ 118.04791127    0.82840008    0.8531        0.31280002    0.86450005]
 ...
 [  84.31867065    0.77899998    0.83949995    0.24690001    0.82579994]
 [ 125.97706996    0.89790004    0.82450002    0.49570003    0.90630007]
 [   1.72180517    0.9655        0.94510001    0.76700002    0.96210003]][0m
[37m[1m[2023-07-10 17:18:09,388][227910] Max Reward on eval: 191.8822757005808[0m
[37m[1m[2023-07-10 17:18:09,388][227910] Min Reward on eval: -241.95262553234352[0m
[37m[1m[2023-07-10 17:18:09,388][227910] Mean Reward across all agents: 55.961962424607094[0m
[37m[1m[2023-07-10 17:18:09,389][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:18:09,393][227910] mean_value=40.51832291822767, max_value=586.983163530681[0m
[37m[1m[2023-07-10 17:18:09,396][227910] New mean coefficients: [[ 1.3610055  3.2831874  1.0383302 -2.1545858  1.1905258]][0m
[37m[1m[2023-07-10 17:18:09,397][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:18:19,191][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 17:18:19,191][227910] FPS: 392144.43[0m
[36m[2023-07-10 17:18:19,193][227910] itr=925, itrs=2000, Progress: 46.25%[0m
[36m[2023-07-10 17:18:30,877][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 17:18:30,877][227910] FPS: 329191.59[0m
[36m[2023-07-10 17:18:35,735][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:18:35,736][227910] Reward + Measures: [[151.40966584   0.89495134   0.9022361    0.42114869   0.92523664]][0m
[37m[1m[2023-07-10 17:18:35,736][227910] Max Reward on eval: 151.40966584311735[0m
[37m[1m[2023-07-10 17:18:35,736][227910] Min Reward on eval: 151.40966584311735[0m
[37m[1m[2023-07-10 17:18:35,736][227910] Mean Reward across all agents: 151.40966584311735[0m
[37m[1m[2023-07-10 17:18:35,736][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:18:41,248][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:18:41,249][227910] Reward + Measures: [[ -8.58000062   0.9325       0.93050003   0.64199996   0.94539994]
 [193.43512579   0.89340001   0.88959998   0.4348       0.88339996]
 [ 95.24280597   0.7597       0.84070009   0.25759998   0.8434    ]
 ...
 [-67.01626744   0.96800005   0.93860006   0.74140006   0.96439999]
 [ -7.76869901   0.90760005   0.92460006   0.59549999   0.90679997]
 [  5.68487236   0.93349999   0.93190002   0.58789998   0.93219995]][0m
[37m[1m[2023-07-10 17:18:41,249][227910] Max Reward on eval: 272.39182370871424[0m
[37m[1m[2023-07-10 17:18:41,249][227910] Min Reward on eval: -96.87602513193852[0m
[37m[1m[2023-07-10 17:18:41,250][227910] Mean Reward across all agents: 49.64833061434976[0m
[37m[1m[2023-07-10 17:18:41,250][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:18:41,253][227910] mean_value=-5.719945274598164, max_value=598.4876616326976[0m
[37m[1m[2023-07-10 17:18:41,256][227910] New mean coefficients: [[ 0.8702455  1.6061739  0.7212883 -2.74507    0.5673012]][0m
[37m[1m[2023-07-10 17:18:41,257][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:18:50,968][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 17:18:50,969][227910] FPS: 395482.82[0m
[36m[2023-07-10 17:18:50,971][227910] itr=926, itrs=2000, Progress: 46.30%[0m
[36m[2023-07-10 17:19:02,567][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 17:19:02,567][227910] FPS: 331673.62[0m
[36m[2023-07-10 17:19:07,354][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:19:07,355][227910] Reward + Measures: [[172.55967207   0.82449836   0.87429136   0.27800432   0.90867698]][0m
[37m[1m[2023-07-10 17:19:07,355][227910] Max Reward on eval: 172.5596720713518[0m
[37m[1m[2023-07-10 17:19:07,355][227910] Min Reward on eval: 172.5596720713518[0m
[37m[1m[2023-07-10 17:19:07,355][227910] Mean Reward across all agents: 172.5596720713518[0m
[37m[1m[2023-07-10 17:19:07,355][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:19:12,775][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:19:12,776][227910] Reward + Measures: [[109.97963896   0.67050004   0.78940004   0.1605       0.84380001]
 [148.18041155   0.83000004   0.8179       0.38059998   0.88850003]
 [104.51266465   0.62880003   0.72370005   0.19050001   0.78200001]
 ...
 [140.56873699   0.71509999   0.79930001   0.2211       0.84470004]
 [146.51465902   0.9052       0.87449998   0.5808       0.92199993]
 [154.47604501   0.72110003   0.8351       0.22839999   0.884     ]][0m
[37m[1m[2023-07-10 17:19:12,776][227910] Max Reward on eval: 218.15806857710703[0m
[37m[1m[2023-07-10 17:19:12,777][227910] Min Reward on eval: 27.36895697975415[0m
[37m[1m[2023-07-10 17:19:12,777][227910] Mean Reward across all agents: 139.67693400066773[0m
[37m[1m[2023-07-10 17:19:12,777][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:19:12,783][227910] mean_value=147.24152312143062, max_value=702.1269048793765[0m
[37m[1m[2023-07-10 17:19:12,786][227910] New mean coefficients: [[-0.2378264  1.4654524  0.7948183 -2.9214346  1.8637035]][0m
[37m[1m[2023-07-10 17:19:12,787][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:19:22,526][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 17:19:22,527][227910] FPS: 394342.12[0m
[36m[2023-07-10 17:19:22,529][227910] itr=927, itrs=2000, Progress: 46.35%[0m
[36m[2023-07-10 17:19:33,996][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 17:19:33,996][227910] FPS: 335387.78[0m
[36m[2023-07-10 17:19:38,693][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:19:38,693][227910] Reward + Measures: [[-76.73578956   0.44029835   0.54297984   0.37936011   0.7018733 ]][0m
[37m[1m[2023-07-10 17:19:38,694][227910] Max Reward on eval: -76.73578955560286[0m
[37m[1m[2023-07-10 17:19:38,694][227910] Min Reward on eval: -76.73578955560286[0m
[37m[1m[2023-07-10 17:19:38,694][227910] Mean Reward across all agents: -76.73578955560286[0m
[37m[1m[2023-07-10 17:19:38,694][227910] Average Trajectory Length: 999.7343333333333[0m
[36m[2023-07-10 17:19:44,268][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:19:44,268][227910] Reward + Measures: [[  22.89932954    0.47539997    0.58030003    0.3809        0.72040004]
 [  25.10188381    0.5323        0.46689996    0.34209999    0.69559997]
 [-102.70954476    0.55680001    0.4082        0.41440001    0.67590004]
 ...
 [  97.34419036    0.55669999    0.45970002    0.3766        0.73400003]
 [-137.25340843    0.64080006    0.34150001    0.4632        0.70590001]
 [  29.44783373    0.53590006    0.48049998    0.3644        0.70469993]][0m
[37m[1m[2023-07-10 17:19:44,268][227910] Max Reward on eval: 157.93517989327665[0m
[37m[1m[2023-07-10 17:19:44,269][227910] Min Reward on eval: -179.40065254269865[0m
[37m[1m[2023-07-10 17:19:44,269][227910] Mean Reward across all agents: -12.813758284217478[0m
[37m[1m[2023-07-10 17:19:44,269][227910] Average Trajectory Length: 999.569[0m
[36m[2023-07-10 17:19:44,273][227910] mean_value=-111.89429037354576, max_value=544.1359699420631[0m
[37m[1m[2023-07-10 17:19:44,275][227910] New mean coefficients: [[ 0.3707955  1.6169424  0.875915  -2.9740033  2.5206356]][0m
[37m[1m[2023-07-10 17:19:44,276][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:19:53,944][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 17:19:53,944][227910] FPS: 397264.59[0m
[36m[2023-07-10 17:19:53,947][227910] itr=928, itrs=2000, Progress: 46.40%[0m
[36m[2023-07-10 17:20:05,412][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 17:20:05,412][227910] FPS: 335457.30[0m
[36m[2023-07-10 17:20:10,228][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:20:10,228][227910] Reward + Measures: [[-37.29580819   0.40422541   0.60295826   0.27980104   0.71578312]][0m
[37m[1m[2023-07-10 17:20:10,229][227910] Max Reward on eval: -37.2958081893029[0m
[37m[1m[2023-07-10 17:20:10,229][227910] Min Reward on eval: -37.2958081893029[0m
[37m[1m[2023-07-10 17:20:10,229][227910] Mean Reward across all agents: -37.2958081893029[0m
[37m[1m[2023-07-10 17:20:10,229][227910] Average Trajectory Length: 999.706[0m
[36m[2023-07-10 17:20:15,877][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:20:15,882][227910] Reward + Measures: [[  7.21088143   0.62169999   0.58149999   0.46730003   0.77350003]
 [ -3.02312017   0.37910002   0.65930003   0.25690001   0.70500004]
 [-47.41097585   0.68559998   0.68450004   0.52899998   0.8071    ]
 ...
 [  1.02810559   0.63959998   0.38390002   0.37970003   0.67180008]
 [ 16.17656988   0.35460001   0.60710001   0.21039999   0.63759995]
 [-22.78774482   0.41680002   0.56629997   0.26009998   0.69460005]][0m
[37m[1m[2023-07-10 17:20:15,883][227910] Max Reward on eval: 118.21384912519716[0m
[37m[1m[2023-07-10 17:20:15,883][227910] Min Reward on eval: -141.48221850944682[0m
[37m[1m[2023-07-10 17:20:15,883][227910] Mean Reward across all agents: 1.4215992169500822[0m
[37m[1m[2023-07-10 17:20:15,883][227910] Average Trajectory Length: 999.5583333333333[0m
[36m[2023-07-10 17:20:15,886][227910] mean_value=-323.9441625341571, max_value=475.172630059324[0m
[37m[1m[2023-07-10 17:20:15,889][227910] New mean coefficients: [[ 0.66368204  0.37810135  0.20843369 -2.9918225   2.6527085 ]][0m
[37m[1m[2023-07-10 17:20:15,890][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:20:25,653][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 17:20:25,653][227910] FPS: 393381.71[0m
[36m[2023-07-10 17:20:25,656][227910] itr=929, itrs=2000, Progress: 46.45%[0m
[36m[2023-07-10 17:20:37,162][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 17:20:37,162][227910] FPS: 334334.36[0m
[36m[2023-07-10 17:20:41,925][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:20:41,925][227910] Reward + Measures: [[-7.48892441  0.36636099  0.66051137  0.18385233  0.72561896]][0m
[37m[1m[2023-07-10 17:20:41,925][227910] Max Reward on eval: -7.488924408315583[0m
[37m[1m[2023-07-10 17:20:41,926][227910] Min Reward on eval: -7.488924408315583[0m
[37m[1m[2023-07-10 17:20:41,926][227910] Mean Reward across all agents: -7.488924408315583[0m
[37m[1m[2023-07-10 17:20:41,926][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:20:47,432][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:20:47,433][227910] Reward + Measures: [[ 51.97605345   0.53689998   0.49149999   0.31280002   0.73990005]
 [ -9.92499797   0.40279999   0.6006       0.2263       0.72539997]
 [ 66.0039365    0.78249997   0.44969997   0.63070005   0.83220005]
 ...
 [ 73.40868267   0.73410004   0.2748       0.57040006   0.71100003]
 [-40.62093168   0.60980004   0.62760007   0.4535       0.79479998]
 [ 58.26689668   0.7112       0.49869999   0.50690001   0.81970006]][0m
[37m[1m[2023-07-10 17:20:47,433][227910] Max Reward on eval: 142.28372257690063[0m
[37m[1m[2023-07-10 17:20:47,433][227910] Min Reward on eval: -171.96081101200542[0m
[37m[1m[2023-07-10 17:20:47,433][227910] Mean Reward across all agents: 19.218680743250115[0m
[37m[1m[2023-07-10 17:20:47,434][227910] Average Trajectory Length: 999.2096666666666[0m
[36m[2023-07-10 17:20:47,437][227910] mean_value=-156.3705964990572, max_value=568.4050740744802[0m
[37m[1m[2023-07-10 17:20:47,440][227910] New mean coefficients: [[ 0.36856407  0.346047    1.0456533  -2.7767684   3.1525152 ]][0m
[37m[1m[2023-07-10 17:20:47,441][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:20:57,215][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 17:20:57,215][227910] FPS: 392946.43[0m
[36m[2023-07-10 17:20:57,217][227910] itr=930, itrs=2000, Progress: 46.50%[0m
[37m[1m[2023-07-10 17:21:00,645][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000910[0m
[36m[2023-07-10 17:21:12,479][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 17:21:12,479][227910] FPS: 332330.58[0m
[36m[2023-07-10 17:21:17,322][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:21:17,322][227910] Reward + Measures: [[19.27121153  0.34032398  0.7389456   0.12550934  0.78510034]][0m
[37m[1m[2023-07-10 17:21:17,322][227910] Max Reward on eval: 19.27121153214332[0m
[37m[1m[2023-07-10 17:21:17,322][227910] Min Reward on eval: 19.27121153214332[0m
[37m[1m[2023-07-10 17:21:17,323][227910] Mean Reward across all agents: 19.27121153214332[0m
[37m[1m[2023-07-10 17:21:17,323][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:21:23,046][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:21:23,047][227910] Reward + Measures: [[132.14885136   0.50190002   0.56400007   0.26760003   0.8021    ]
 [ 14.11385333   0.40650001   0.66250002   0.1243       0.80409998]
 [ 54.98022504   0.22150002   0.83649999   0.14950001   0.86770004]
 ...
 [-24.25931096   0.3186       0.82739991   0.1094       0.79339999]
 [ 88.99655231   0.51380002   0.6505       0.28029999   0.79479998]
 [246.62357995   0.40110001   0.68170005   0.2191       0.65110004]][0m
[37m[1m[2023-07-10 17:21:23,047][227910] Max Reward on eval: 278.86298965645835[0m
[37m[1m[2023-07-10 17:21:23,047][227910] Min Reward on eval: -124.4121028974303[0m
[37m[1m[2023-07-10 17:21:23,048][227910] Mean Reward across all agents: 57.13665311102469[0m
[37m[1m[2023-07-10 17:21:23,048][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:21:23,051][227910] mean_value=-152.45499257141745, max_value=636.3925878610928[0m
[37m[1m[2023-07-10 17:21:23,054][227910] New mean coefficients: [[-0.34185106  1.3200654   1.3421514  -1.8973081   4.549176  ]][0m
[37m[1m[2023-07-10 17:21:23,055][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:21:32,879][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 17:21:32,879][227910] FPS: 390936.52[0m
[36m[2023-07-10 17:21:32,881][227910] itr=931, itrs=2000, Progress: 46.55%[0m
[36m[2023-07-10 17:21:44,466][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 17:21:44,466][227910] FPS: 331985.16[0m
[36m[2023-07-10 17:21:49,336][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:21:49,337][227910] Reward + Measures: [[48.47398421  0.38429165  0.75954801  0.080687    0.82974565]][0m
[37m[1m[2023-07-10 17:21:49,337][227910] Max Reward on eval: 48.47398420544454[0m
[37m[1m[2023-07-10 17:21:49,337][227910] Min Reward on eval: 48.47398420544454[0m
[37m[1m[2023-07-10 17:21:49,337][227910] Mean Reward across all agents: 48.47398420544454[0m
[37m[1m[2023-07-10 17:21:49,338][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:21:54,816][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:21:54,817][227910] Reward + Measures: [[36.1751653   0.45179996  0.73789996  0.08199999  0.84740001]
 [89.06848705  0.34110001  0.81750005  0.0837      0.84630007]
 [59.35725232  0.47460005  0.79049999  0.0155      0.84320003]
 ...
 [25.90027693  0.54269999  0.72360009  0.0061      0.83710003]
 [89.18686771  0.51779997  0.79430002  0.0128      0.86540002]
 [90.03089993  0.414       0.81480008  0.0262      0.86429995]][0m
[37m[1m[2023-07-10 17:21:54,817][227910] Max Reward on eval: 168.40682167403867[0m
[37m[1m[2023-07-10 17:21:54,817][227910] Min Reward on eval: -126.37010693955236[0m
[37m[1m[2023-07-10 17:21:54,817][227910] Mean Reward across all agents: 78.15081376658274[0m
[37m[1m[2023-07-10 17:21:54,818][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:21:54,824][227910] mean_value=363.6812092368966, max_value=656.7027433376436[0m
[37m[1m[2023-07-10 17:21:54,827][227910] New mean coefficients: [[ 0.86230713  2.6080313   1.038783   -1.2039764   4.4425116 ]][0m
[37m[1m[2023-07-10 17:21:54,828][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:22:04,665][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 17:22:04,666][227910] FPS: 390417.55[0m
[36m[2023-07-10 17:22:04,668][227910] itr=932, itrs=2000, Progress: 46.60%[0m
[36m[2023-07-10 17:22:16,280][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 17:22:16,280][227910] FPS: 331233.27[0m
[36m[2023-07-10 17:22:21,068][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:22:21,069][227910] Reward + Measures: [[94.42050692  0.47630265  0.78137606  0.05086467  0.88319659]][0m
[37m[1m[2023-07-10 17:22:21,069][227910] Max Reward on eval: 94.42050691610152[0m
[37m[1m[2023-07-10 17:22:21,069][227910] Min Reward on eval: 94.42050691610152[0m
[37m[1m[2023-07-10 17:22:21,069][227910] Mean Reward across all agents: 94.42050691610152[0m
[37m[1m[2023-07-10 17:22:21,070][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:22:26,569][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:22:26,570][227910] Reward + Measures: [[ 93.33678786   0.43050003   0.83339995   0.0308       0.89390004]
 [126.01007166   0.42180005   0.76220006   0.0279       0.81440002]
 [ 32.72763745   0.33150002   0.76030004   0.20089999   0.80390006]
 ...
 [191.60190826   0.48059997   0.69910002   0.14330001   0.86219996]
 [-48.30279947   0.39189997   0.82340002   0.11540001   0.89580005]
 [ 88.78073642   0.36199999   0.79699999   0.1103       0.86680001]][0m
[37m[1m[2023-07-10 17:22:26,570][227910] Max Reward on eval: 206.20755120310932[0m
[37m[1m[2023-07-10 17:22:26,570][227910] Min Reward on eval: -116.50631444802566[0m
[37m[1m[2023-07-10 17:22:26,570][227910] Mean Reward across all agents: 72.6843566559222[0m
[37m[1m[2023-07-10 17:22:26,571][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:22:26,574][227910] mean_value=-90.41585433925705, max_value=535.8873352099015[0m
[37m[1m[2023-07-10 17:22:26,577][227910] New mean coefficients: [[ 1.828646    2.9162269   0.42607445 -1.3644096   3.0571704 ]][0m
[37m[1m[2023-07-10 17:22:26,578][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:22:36,351][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 17:22:36,352][227910] FPS: 392952.46[0m
[36m[2023-07-10 17:22:36,354][227910] itr=933, itrs=2000, Progress: 46.65%[0m
[36m[2023-07-10 17:22:47,860][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 17:22:47,861][227910] FPS: 334272.92[0m
[36m[2023-07-10 17:22:52,503][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:22:52,504][227910] Reward + Measures: [[197.95464166   0.56425935   0.80099535   0.04749734   0.91633034]][0m
[37m[1m[2023-07-10 17:22:52,504][227910] Max Reward on eval: 197.9546416556574[0m
[37m[1m[2023-07-10 17:22:52,504][227910] Min Reward on eval: 197.9546416556574[0m
[37m[1m[2023-07-10 17:22:52,505][227910] Mean Reward across all agents: 197.9546416556574[0m
[37m[1m[2023-07-10 17:22:52,505][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:22:57,978][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:22:57,979][227910] Reward + Measures: [[104.5031203    0.54100007   0.84779996   0.0048       0.91359997]
 [218.94687766   0.55450004   0.76300001   0.08810001   0.89880002]
 [ 86.2330614    0.54910004   0.82190001   0.0052       0.92539996]
 ...
 [266.87708819   0.43860003   0.8398       0.0887       0.89289999]
 [260.05844678   0.51239997   0.80030006   0.0597       0.89580005]
 [160.80709691   0.41339999   0.87530005   0.0264       0.91060001]][0m
[37m[1m[2023-07-10 17:22:57,979][227910] Max Reward on eval: 389.4302876915317[0m
[37m[1m[2023-07-10 17:22:57,979][227910] Min Reward on eval: 43.16999177347752[0m
[37m[1m[2023-07-10 17:22:57,980][227910] Mean Reward across all agents: 183.36462397538102[0m
[37m[1m[2023-07-10 17:22:57,980][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:22:57,986][227910] mean_value=191.06287814739937, max_value=764.6716399716679[0m
[37m[1m[2023-07-10 17:22:57,989][227910] New mean coefficients: [[ 2.3155565   3.195807   -0.28768837 -1.6135896   1.9542303 ]][0m
[37m[1m[2023-07-10 17:22:57,990][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:23:07,713][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 17:23:07,714][227910] FPS: 394971.99[0m
[36m[2023-07-10 17:23:07,716][227910] itr=934, itrs=2000, Progress: 46.70%[0m
[36m[2023-07-10 17:23:19,325][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 17:23:19,326][227910] FPS: 331271.06[0m
[36m[2023-07-10 17:23:24,078][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:23:24,078][227910] Reward + Measures: [[316.35395947   0.66328233   0.81978732   0.034996     0.94222832]][0m
[37m[1m[2023-07-10 17:23:24,079][227910] Max Reward on eval: 316.3539594675017[0m
[37m[1m[2023-07-10 17:23:24,079][227910] Min Reward on eval: 316.3539594675017[0m
[37m[1m[2023-07-10 17:23:24,079][227910] Mean Reward across all agents: 316.3539594675017[0m
[37m[1m[2023-07-10 17:23:24,079][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:23:29,417][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:23:29,417][227910] Reward + Measures: [[ 270.19605197    0.41739997    0.6803        0.25619999    0.69610006]
 [ 768.11175589    0.47490001    0.5485        0.41580001    0.43400002]
 [-191.55650143    0.58540004    0.53960001    0.3759        0.60530007]
 ...
 [ 143.37183523    0.44970003    0.55579996    0.30600002    0.54249996]
 [ 534.57000993    0.5068        0.74669999    0.29390001    0.63380003]
 [-633.18703322    0.90100002    0.7561        0.8028        0.92810005]][0m
[37m[1m[2023-07-10 17:23:29,417][227910] Max Reward on eval: 1322.3833830156364[0m
[37m[1m[2023-07-10 17:23:29,418][227910] Min Reward on eval: -1651.7567275138222[0m
[37m[1m[2023-07-10 17:23:29,418][227910] Mean Reward across all agents: -142.22935260599203[0m
[37m[1m[2023-07-10 17:23:29,418][227910] Average Trajectory Length: 914.957[0m
[36m[2023-07-10 17:23:29,421][227910] mean_value=-1751.2122236817777, max_value=921.8121174513701[0m
[37m[1m[2023-07-10 17:23:29,424][227910] New mean coefficients: [[ 0.47855175  2.9761646   0.1388734  -0.729362    2.718648  ]][0m
[37m[1m[2023-07-10 17:23:29,425][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:23:39,417][227910] train() took 9.99 seconds to complete[0m
[36m[2023-07-10 17:23:39,418][227910] FPS: 384362.68[0m
[36m[2023-07-10 17:23:39,420][227910] itr=935, itrs=2000, Progress: 46.75%[0m
[36m[2023-07-10 17:23:51,033][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 17:23:51,033][227910] FPS: 331299.01[0m
[36m[2023-07-10 17:23:55,913][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:23:55,914][227910] Reward + Measures: [[390.03897375   0.75236928   0.86828059   0.01280133   0.96965396]][0m
[37m[1m[2023-07-10 17:23:55,914][227910] Max Reward on eval: 390.0389737502437[0m
[37m[1m[2023-07-10 17:23:55,914][227910] Min Reward on eval: 390.0389737502437[0m
[37m[1m[2023-07-10 17:23:55,914][227910] Mean Reward across all agents: 390.0389737502437[0m
[37m[1m[2023-07-10 17:23:55,915][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:24:01,601][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:24:01,602][227910] Reward + Measures: [[ 40.34669691   0.48330003   0.44459996   0.4901       0.49569997]
 [308.17100054   0.88290006   0.23169999   0.70139998   0.83459997]
 [119.32623522   0.36210001   0.47999999   0.20380001   0.40910003]
 ...
 [330.67473362   0.74649996   0.87770003   0.004        0.98199999]
 [-44.78333061   0.5043       0.43449998   0.368        0.50019997]
 [398.39405292   0.74680001   0.83890003   0.06960001   0.96880001]][0m
[37m[1m[2023-07-10 17:24:01,602][227910] Max Reward on eval: 535.5021059242775[0m
[37m[1m[2023-07-10 17:24:01,602][227910] Min Reward on eval: -791.8361272527138[0m
[37m[1m[2023-07-10 17:24:01,602][227910] Mean Reward across all agents: 106.8365877713443[0m
[37m[1m[2023-07-10 17:24:01,603][227910] Average Trajectory Length: 999.2743333333333[0m
[36m[2023-07-10 17:24:01,609][227910] mean_value=-69.89246590192691, max_value=996.4522941884818[0m
[37m[1m[2023-07-10 17:24:01,612][227910] New mean coefficients: [[-0.21761388  2.6875548   0.11670085 -0.42696986  3.4485853 ]][0m
[37m[1m[2023-07-10 17:24:01,613][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:24:11,310][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 17:24:11,311][227910] FPS: 396046.09[0m
[36m[2023-07-10 17:24:11,313][227910] itr=936, itrs=2000, Progress: 46.80%[0m
[36m[2023-07-10 17:24:22,794][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 17:24:22,794][227910] FPS: 335022.16[0m
[36m[2023-07-10 17:24:27,559][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:24:27,559][227910] Reward + Measures: [[415.42012598   0.82702333   0.9030267    0.00424833   0.98369628]][0m
[37m[1m[2023-07-10 17:24:27,560][227910] Max Reward on eval: 415.42012597508403[0m
[37m[1m[2023-07-10 17:24:27,560][227910] Min Reward on eval: 415.42012597508403[0m
[37m[1m[2023-07-10 17:24:27,560][227910] Mean Reward across all agents: 415.42012597508403[0m
[37m[1m[2023-07-10 17:24:27,560][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:24:32,926][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:24:32,927][227910] Reward + Measures: [[ -93.54919287    0.36100003    0.65310001    0.11670001    0.74730003]
 [-306.65988872    0.63099998    0.31979999    0.57969999    0.76940006]
 [ 213.18019087    0.6886        0.86809999    0.0058        0.96390003]
 ...
 [ 384.28986887    0.824         0.78150004    0.57269996    0.9285    ]
 [ -40.66729688    0.53890002    0.71760005    0.15910001    0.85860008]
 [-511.62782357    0.50570005    0.41070005    0.53579998    0.81529999]][0m
[37m[1m[2023-07-10 17:24:32,927][227910] Max Reward on eval: 451.55551352654584[0m
[37m[1m[2023-07-10 17:24:32,927][227910] Min Reward on eval: -781.497328070621[0m
[37m[1m[2023-07-10 17:24:32,927][227910] Mean Reward across all agents: 187.38282387511018[0m
[37m[1m[2023-07-10 17:24:32,928][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:24:32,934][227910] mean_value=221.12435487736298, max_value=919.624931361876[0m
[37m[1m[2023-07-10 17:24:32,937][227910] New mean coefficients: [[ 0.6966262   2.504052    0.02230661 -0.7949512   3.136676  ]][0m
[37m[1m[2023-07-10 17:24:32,938][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:24:42,540][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 17:24:42,541][227910] FPS: 399972.84[0m
[36m[2023-07-10 17:24:42,543][227910] itr=937, itrs=2000, Progress: 46.85%[0m
[36m[2023-07-10 17:24:54,074][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 17:24:54,075][227910] FPS: 333521.15[0m
[36m[2023-07-10 17:24:58,760][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:24:58,760][227910] Reward + Measures: [[61.25761737  0.92956197  0.91217172  0.00003167  0.98512363]][0m
[37m[1m[2023-07-10 17:24:58,760][227910] Max Reward on eval: 61.25761736527415[0m
[37m[1m[2023-07-10 17:24:58,760][227910] Min Reward on eval: 61.25761736527415[0m
[37m[1m[2023-07-10 17:24:58,761][227910] Mean Reward across all agents: 61.25761736527415[0m
[37m[1m[2023-07-10 17:24:58,761][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:25:04,107][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:25:04,108][227910] Reward + Measures: [[224.18581824   0.77460003   0.76719999   0.003        0.93570006]
 [170.75192558   0.6455       0.69259995   0.0727       0.92790002]
 [226.90134775   0.89250004   0.90560001   0.0014       0.96340001]
 ...
 [200.15008363   0.90010005   0.88249999   0.           0.97399998]
 [174.7873648    0.77209997   0.78030002   0.0036       0.95380002]
 [183.54716394   0.8818       0.86700004   0.           0.9698    ]][0m
[37m[1m[2023-07-10 17:25:04,108][227910] Max Reward on eval: 327.6603965838091[0m
[37m[1m[2023-07-10 17:25:04,108][227910] Min Reward on eval: -68.63266442362219[0m
[37m[1m[2023-07-10 17:25:04,108][227910] Mean Reward across all agents: 165.30742480411828[0m
[37m[1m[2023-07-10 17:25:04,109][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:25:04,113][227910] mean_value=279.0134258369764, max_value=827.6603965838091[0m
[37m[1m[2023-07-10 17:25:04,116][227910] New mean coefficients: [[ 0.60523605  2.3791933  -0.08047009 -1.1003822   3.9245274 ]][0m
[37m[1m[2023-07-10 17:25:04,117][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:25:13,786][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 17:25:13,786][227910] FPS: 397206.85[0m
[36m[2023-07-10 17:25:13,789][227910] itr=938, itrs=2000, Progress: 46.90%[0m
[36m[2023-07-10 17:25:25,261][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 17:25:25,261][227910] FPS: 335327.29[0m
[36m[2023-07-10 17:25:29,985][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:25:29,985][227910] Reward + Measures: [[-445.90844459    0.60815436    0.80742401    0.031225      0.94374037]][0m
[37m[1m[2023-07-10 17:25:29,985][227910] Max Reward on eval: -445.90844458794487[0m
[37m[1m[2023-07-10 17:25:29,986][227910] Min Reward on eval: -445.90844458794487[0m
[37m[1m[2023-07-10 17:25:29,986][227910] Mean Reward across all agents: -445.90844458794487[0m
[37m[1m[2023-07-10 17:25:29,986][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:25:35,397][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:25:35,398][227910] Reward + Measures: [[-1036.63626173     0.90250009     0.92979997     0.
      0.99669999]
 [  -20.03889532     0.31900001     0.66020006     0.2474
      0.87540007]
 [ -261.68190346     0.28690001     0.83379996     0.07050001
      0.91919994]
 ...
 [ -404.37537731     0.57639998     0.84840006     0.0028
      0.97139996]
 [ -136.75705861     0.36410001     0.68250006     0.21159999
      0.70959997]
 [ -107.14811158     0.2791         0.83240002     0.1163
      0.93889999]][0m
[37m[1m[2023-07-10 17:25:35,398][227910] Max Reward on eval: 146.728623580368[0m
[37m[1m[2023-07-10 17:25:35,398][227910] Min Reward on eval: -1223.2022482157686[0m
[37m[1m[2023-07-10 17:25:35,398][227910] Mean Reward across all agents: -366.7693858519934[0m
[37m[1m[2023-07-10 17:25:35,399][227910] Average Trajectory Length: 999.286[0m
[36m[2023-07-10 17:25:35,402][227910] mean_value=-416.70376669844177, max_value=590.4110811258433[0m
[37m[1m[2023-07-10 17:25:35,405][227910] New mean coefficients: [[ 0.16800481  2.1487179  -0.35724548 -1.1467698   3.9668374 ]][0m
[37m[1m[2023-07-10 17:25:35,406][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:25:45,052][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 17:25:45,052][227910] FPS: 398174.51[0m
[36m[2023-07-10 17:25:45,055][227910] itr=939, itrs=2000, Progress: 46.95%[0m
[36m[2023-07-10 17:25:56,724][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 17:25:56,724][227910] FPS: 329598.63[0m
[36m[2023-07-10 17:26:01,428][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:26:01,428][227910] Reward + Measures: [[-708.96066595    0.9206        0.96769971    0.            0.99281329]][0m
[37m[1m[2023-07-10 17:26:01,428][227910] Max Reward on eval: -708.9606659453061[0m
[37m[1m[2023-07-10 17:26:01,428][227910] Min Reward on eval: -708.9606659453061[0m
[37m[1m[2023-07-10 17:26:01,429][227910] Mean Reward across all agents: -708.9606659453061[0m
[37m[1m[2023-07-10 17:26:01,429][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:26:06,908][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:26:06,908][227910] Reward + Measures: [[-499.41497631    0.93219995    0.93940002    0.            0.98470002]
 [-543.46252439    0.90630001    0.89820004    0.            0.98740005]
 [-924.07045889    0.86140007    0.95719999    0.            0.99080002]
 ...
 [-498.22108099    0.8387        0.86049998    0.            0.97510004]
 [-416.41546927    0.74779999    0.90889996    0.29480001    0.95480007]
 [-496.03749484    0.71929997    0.44529995    0.62130004    0.79510003]][0m
[37m[1m[2023-07-10 17:26:06,908][227910] Max Reward on eval: -117.06851262973505[0m
[37m[1m[2023-07-10 17:26:06,909][227910] Min Reward on eval: -1169.8066677240072[0m
[37m[1m[2023-07-10 17:26:06,909][227910] Mean Reward across all agents: -555.7754948358546[0m
[37m[1m[2023-07-10 17:26:06,909][227910] Average Trajectory Length: 999.7533333333333[0m
[36m[2023-07-10 17:26:06,911][227910] mean_value=-661.2956117287339, max_value=203.92208051857654[0m
[37m[1m[2023-07-10 17:26:06,914][227910] New mean coefficients: [[ 1.201345   2.2815952 -0.8279022 -1.573159   4.0150747]][0m
[37m[1m[2023-07-10 17:26:06,915][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:26:16,706][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 17:26:16,706][227910] FPS: 392262.45[0m
[36m[2023-07-10 17:26:16,708][227910] itr=940, itrs=2000, Progress: 47.00%[0m
[37m[1m[2023-07-10 17:26:20,409][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000920[0m
[36m[2023-07-10 17:26:32,298][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 17:26:32,298][227910] FPS: 330878.10[0m
[36m[2023-07-10 17:26:36,998][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:26:36,999][227910] Reward + Measures: [[-675.3471768     0.95847768    0.98201096    0.            0.9963733 ]][0m
[37m[1m[2023-07-10 17:26:36,999][227910] Max Reward on eval: -675.34717679501[0m
[37m[1m[2023-07-10 17:26:36,999][227910] Min Reward on eval: -675.34717679501[0m
[37m[1m[2023-07-10 17:26:36,999][227910] Mean Reward across all agents: -675.34717679501[0m
[37m[1m[2023-07-10 17:26:37,000][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:26:42,455][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:26:42,456][227910] Reward + Measures: [[ -835.37603421     0.88730001     0.9939         0.0008
      0.9946    ]
 [ -320.6355162      0.93040007     0.93440002     0.
      0.98480004]
 [ -566.63064479     0.91919994     0.97620004     0.
      0.94859999]
 ...
 [-1466.00128372     0.68740004     0.9016         0.0975
      0.5607    ]
 [ -208.21943676     0.89670002     0.94910002     0.
      0.93989992]
 [ -460.43438893     0.71999997     0.66530007     0.53109998
      0.85990012]][0m
[37m[1m[2023-07-10 17:26:42,456][227910] Max Reward on eval: -21.274426170368677[0m
[37m[1m[2023-07-10 17:26:42,456][227910] Min Reward on eval: -2120.6810439098163[0m
[37m[1m[2023-07-10 17:26:42,456][227910] Mean Reward across all agents: -700.506307797456[0m
[37m[1m[2023-07-10 17:26:42,456][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:26:42,459][227910] mean_value=-685.3291786134353, max_value=432.01269212209155[0m
[37m[1m[2023-07-10 17:26:42,461][227910] New mean coefficients: [[ 0.85976934  2.1387959  -0.89450604 -1.7455065   4.352609  ]][0m
[37m[1m[2023-07-10 17:26:42,462][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:26:52,188][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 17:26:52,189][227910] FPS: 394887.06[0m
[36m[2023-07-10 17:26:52,191][227910] itr=941, itrs=2000, Progress: 47.05%[0m
[36m[2023-07-10 17:27:03,694][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 17:27:03,694][227910] FPS: 334346.97[0m
[36m[2023-07-10 17:27:08,501][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:27:08,502][227910] Reward + Measures: [[73.02848812  0.91651559  0.69277602  0.45725167  0.955356  ]][0m
[37m[1m[2023-07-10 17:27:08,502][227910] Max Reward on eval: 73.02848811878893[0m
[37m[1m[2023-07-10 17:27:08,502][227910] Min Reward on eval: 73.02848811878893[0m
[37m[1m[2023-07-10 17:27:08,503][227910] Mean Reward across all agents: 73.02848811878893[0m
[37m[1m[2023-07-10 17:27:08,503][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:27:14,022][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:27:14,022][227910] Reward + Measures: [[-693.64689955    0.56050009    0.79840004    0.2775        0.88880008]
 [-392.5522583     0.23239999    0.63740003    0.18310001    0.46140003]
 [-235.79142719    0.49950001    0.69670004    0.0944        0.83789998]
 ...
 [-379.38765536    0.25960001    0.84579992    0.14219999    0.829     ]
 [-317.30829511    0.29480001    0.65979999    0.13079999    0.72830003]
 [-699.52251944    0.58890003    0.84380001    0.0362        0.87110007]][0m
[37m[1m[2023-07-10 17:27:14,023][227910] Max Reward on eval: 92.9416480010841[0m
[37m[1m[2023-07-10 17:27:14,023][227910] Min Reward on eval: -1103.4040947972971[0m
[37m[1m[2023-07-10 17:27:14,023][227910] Mean Reward across all agents: -407.2390900306731[0m
[37m[1m[2023-07-10 17:27:14,023][227910] Average Trajectory Length: 995.935[0m
[36m[2023-07-10 17:27:14,027][227910] mean_value=-434.2419541138304, max_value=592.9416480010841[0m
[37m[1m[2023-07-10 17:27:14,030][227910] New mean coefficients: [[-0.02440393  2.02738    -0.8789964  -0.90275484  4.671993  ]][0m
[37m[1m[2023-07-10 17:27:14,031][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:27:23,906][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 17:27:23,906][227910] FPS: 388933.10[0m
[36m[2023-07-10 17:27:23,908][227910] itr=942, itrs=2000, Progress: 47.10%[0m
[36m[2023-07-10 17:27:35,469][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 17:27:35,469][227910] FPS: 332670.44[0m
[36m[2023-07-10 17:27:40,205][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:27:40,205][227910] Reward + Measures: [[-457.23858826    0.70306069    0.66082466    0.00227067    0.98297727]][0m
[37m[1m[2023-07-10 17:27:40,205][227910] Max Reward on eval: -457.2385882590779[0m
[37m[1m[2023-07-10 17:27:40,206][227910] Min Reward on eval: -457.2385882590779[0m
[37m[1m[2023-07-10 17:27:40,206][227910] Mean Reward across all agents: -457.2385882590779[0m
[37m[1m[2023-07-10 17:27:40,206][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:27:45,852][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:27:45,853][227910] Reward + Measures: [[ -63.61052823    0.87130004    0.89420003    0.55659997    0.96280003]
 [-337.30085899    0.58459997    0.73410004    0.16610001    0.85170001]
 [-164.92038045    0.67090005    0.73180002    0.12800001    0.84170002]
 ...
 [-558.86178929    0.78610003    0.74810004    0.0987        0.95559996]
 [-105.28672133    0.55319995    0.6936        0.0242        0.80140001]
 [ -21.92197782    0.63870001    0.7798        0.1868        0.88920003]][0m
[37m[1m[2023-07-10 17:27:45,853][227910] Max Reward on eval: 340.4440440106555[0m
[37m[1m[2023-07-10 17:27:45,854][227910] Min Reward on eval: -1194.6839545786847[0m
[37m[1m[2023-07-10 17:27:45,854][227910] Mean Reward across all agents: -340.59739944934347[0m
[37m[1m[2023-07-10 17:27:45,854][227910] Average Trajectory Length: 999.7946666666667[0m
[36m[2023-07-10 17:27:45,859][227910] mean_value=-302.35615867846263, max_value=686.6881956940406[0m
[37m[1m[2023-07-10 17:27:45,862][227910] New mean coefficients: [[ 0.3557901   2.225537   -1.1585747  -0.64416516  4.719674  ]][0m
[37m[1m[2023-07-10 17:27:45,863][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:27:55,590][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 17:27:55,591][227910] FPS: 394810.45[0m
[36m[2023-07-10 17:27:55,593][227910] itr=943, itrs=2000, Progress: 47.15%[0m
[36m[2023-07-10 17:28:07,176][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 17:28:07,176][227910] FPS: 332015.60[0m
[36m[2023-07-10 17:28:11,850][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:28:11,850][227910] Reward + Measures: [[-364.90121639    0.7406854     0.67540932    0.00544733    0.98664194]][0m
[37m[1m[2023-07-10 17:28:11,851][227910] Max Reward on eval: -364.90121638958243[0m
[37m[1m[2023-07-10 17:28:11,851][227910] Min Reward on eval: -364.90121638958243[0m
[37m[1m[2023-07-10 17:28:11,851][227910] Mean Reward across all agents: -364.90121638958243[0m
[37m[1m[2023-07-10 17:28:11,851][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:28:17,213][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:28:17,214][227910] Reward + Measures: [[ -20.73863364    0.69400007    0.78470004    0.003         0.93349999]
 [ -95.87454204    0.88679999    0.1211        0.80940002    0.875     ]
 [-494.5257273     0.84359998    0.5183        0.41289997    0.88660002]
 ...
 [  97.27289784    0.71640003    0.75279999    0.0383        0.90620005]
 [ 113.97332165    0.84329998    0.44299999    0.64230007    0.88850003]
 [-344.93105613    0.71460003    0.35120001    0.51029998    0.76730001]][0m
[37m[1m[2023-07-10 17:28:17,214][227910] Max Reward on eval: 424.0447719301912[0m
[37m[1m[2023-07-10 17:28:17,214][227910] Min Reward on eval: -1786.8162820530124[0m
[37m[1m[2023-07-10 17:28:17,215][227910] Mean Reward across all agents: -80.0812038227305[0m
[37m[1m[2023-07-10 17:28:17,215][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:28:17,221][227910] mean_value=4.177540400004412, max_value=803.6911279147956[0m
[37m[1m[2023-07-10 17:28:17,224][227910] New mean coefficients: [[ 0.14845973  2.137058   -1.1418849   0.14658284  4.857358  ]][0m
[37m[1m[2023-07-10 17:28:17,225][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:28:26,877][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 17:28:26,877][227910] FPS: 397928.60[0m
[36m[2023-07-10 17:28:26,879][227910] itr=944, itrs=2000, Progress: 47.20%[0m
[36m[2023-07-10 17:28:38,486][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 17:28:38,486][227910] FPS: 331375.88[0m
[36m[2023-07-10 17:28:43,218][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:28:43,218][227910] Reward + Measures: [[-277.25736036    0.77128631    0.69266927    0.00936467    0.98852229]][0m
[37m[1m[2023-07-10 17:28:43,219][227910] Max Reward on eval: -277.2573603567024[0m
[37m[1m[2023-07-10 17:28:43,219][227910] Min Reward on eval: -277.2573603567024[0m
[37m[1m[2023-07-10 17:28:43,219][227910] Mean Reward across all agents: -277.2573603567024[0m
[37m[1m[2023-07-10 17:28:43,219][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:28:48,676][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:28:48,677][227910] Reward + Measures: [[  61.01555266    0.85560006    0.26730001    0.53330004    0.88090003]
 [-523.97438495    0.74199998    0.80290002    0.0001        0.96189994]
 [-549.72749178    0.80389994    0.75940007    0.            0.99060005]
 ...
 [-320.61144529    0.81580001    0.47929999    0.303         0.94959992]
 [  97.49058899    0.76269996    0.66080004    0.25460002    0.81949997]
 [  97.92107153    0.83880007    0.3619        0.44300005    0.93310004]][0m
[37m[1m[2023-07-10 17:28:48,677][227910] Max Reward on eval: 444.32781258423347[0m
[37m[1m[2023-07-10 17:28:48,677][227910] Min Reward on eval: -775.2232992446516[0m
[37m[1m[2023-07-10 17:28:48,678][227910] Mean Reward across all agents: -104.90810433319677[0m
[37m[1m[2023-07-10 17:28:48,678][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:28:48,684][227910] mean_value=-19.192792217556125, max_value=896.165010879701[0m
[37m[1m[2023-07-10 17:28:48,687][227910] New mean coefficients: [[ 0.9613242   2.2597766  -0.9589712   0.23985618  4.5147815 ]][0m
[37m[1m[2023-07-10 17:28:48,688][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:28:58,394][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 17:28:58,394][227910] FPS: 395706.57[0m
[36m[2023-07-10 17:28:58,396][227910] itr=945, itrs=2000, Progress: 47.25%[0m
[36m[2023-07-10 17:29:10,052][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 17:29:10,053][227910] FPS: 329992.80[0m
[36m[2023-07-10 17:29:14,803][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:29:14,808][227910] Reward + Measures: [[-223.23471685    0.80628735    0.71994495    0.01212367    0.99153906]][0m
[37m[1m[2023-07-10 17:29:14,809][227910] Max Reward on eval: -223.2347168488569[0m
[37m[1m[2023-07-10 17:29:14,809][227910] Min Reward on eval: -223.2347168488569[0m
[37m[1m[2023-07-10 17:29:14,809][227910] Mean Reward across all agents: -223.2347168488569[0m
[37m[1m[2023-07-10 17:29:14,810][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:29:20,333][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:29:20,334][227910] Reward + Measures: [[-195.75302094    0.79840004    0.73269999    0.0008        0.99230003]
 [ 323.13644052    0.94150001    0.0683        0.78170007    0.89429998]
 [ -20.56041438    0.74959999    0.59759998    0.083         0.96900004]
 ...
 [ 261.879784      0.84279996    0.1549        0.63029999    0.77820003]
 [-159.31568453    0.7511        0.69200003    0.0035        0.98890001]
 [ -40.51230272    0.81290001    0.52730006    0.23199999    0.95440006]][0m
[37m[1m[2023-07-10 17:29:20,334][227910] Max Reward on eval: 410.0481121867313[0m
[37m[1m[2023-07-10 17:29:20,334][227910] Min Reward on eval: -486.09659196137216[0m
[37m[1m[2023-07-10 17:29:20,335][227910] Mean Reward across all agents: 99.39347859439081[0m
[37m[1m[2023-07-10 17:29:20,335][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:29:20,342][227910] mean_value=325.6235184168383, max_value=861.2967470334604[0m
[37m[1m[2023-07-10 17:29:20,345][227910] New mean coefficients: [[ 1.6539533  1.876626  -0.5571165 -0.4319467  4.877086 ]][0m
[37m[1m[2023-07-10 17:29:20,346][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:29:30,078][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 17:29:30,078][227910] FPS: 394654.17[0m
[36m[2023-07-10 17:29:30,080][227910] itr=946, itrs=2000, Progress: 47.30%[0m
[36m[2023-07-10 17:29:41,605][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 17:29:41,605][227910] FPS: 333734.71[0m
[36m[2023-07-10 17:29:46,306][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:29:46,306][227910] Reward + Measures: [[249.58155998   0.84370863   0.47754997   0.65333265   0.88029766]][0m
[37m[1m[2023-07-10 17:29:46,307][227910] Max Reward on eval: 249.58155998162118[0m
[37m[1m[2023-07-10 17:29:46,307][227910] Min Reward on eval: 249.58155998162118[0m
[37m[1m[2023-07-10 17:29:46,307][227910] Mean Reward across all agents: 249.58155998162118[0m
[37m[1m[2023-07-10 17:29:46,307][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:29:51,906][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:29:51,913][227910] Reward + Measures: [[146.37992861   0.83750004   0.47929999   0.40050003   0.91170007]
 [214.15950523   0.87099999   0.34109998   0.55010003   0.91810006]
 [185.86713247   0.83960003   0.41910002   0.465        0.92329997]
 ...
 [367.06749929   0.9228       0.0969       0.81780005   0.89449996]
 [328.85471482   0.92589998   0.0857       0.80450004   0.86920005]
 [ -3.92430907   0.80120003   0.65609998   0.23930001   0.93380004]][0m
[37m[1m[2023-07-10 17:29:51,914][227910] Max Reward on eval: 387.43410863166207[0m
[37m[1m[2023-07-10 17:29:51,914][227910] Min Reward on eval: -34.18831222045701[0m
[37m[1m[2023-07-10 17:29:51,914][227910] Mean Reward across all agents: 228.09296966291387[0m
[37m[1m[2023-07-10 17:29:51,914][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:29:51,920][227910] mean_value=219.3763970155786, max_value=884.4966289666714[0m
[37m[1m[2023-07-10 17:29:51,923][227910] New mean coefficients: [[ 1.8673701   2.2162476  -0.14014009 -0.5312707   5.300738  ]][0m
[37m[1m[2023-07-10 17:29:51,924][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:30:01,566][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 17:30:01,566][227910] FPS: 398360.14[0m
[36m[2023-07-10 17:30:01,568][227910] itr=947, itrs=2000, Progress: 47.35%[0m
[36m[2023-07-10 17:30:13,106][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 17:30:13,107][227910] FPS: 333336.20[0m
[36m[2023-07-10 17:30:17,911][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:30:17,911][227910] Reward + Measures: [[299.65701431   0.88127232   0.58761996   0.7449736    0.9194876 ]][0m
[37m[1m[2023-07-10 17:30:17,912][227910] Max Reward on eval: 299.65701430686795[0m
[37m[1m[2023-07-10 17:30:17,912][227910] Min Reward on eval: 299.65701430686795[0m
[37m[1m[2023-07-10 17:30:17,912][227910] Mean Reward across all agents: 299.65701430686795[0m
[37m[1m[2023-07-10 17:30:17,913][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:30:23,425][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:30:23,430][227910] Reward + Measures: [[-615.79951145    0.84810001    0.60190004    0.82870001    0.59190005]
 [-312.09858064    0.2633        0.33400002    0.029         0.3184    ]
 [-277.05636603    0.24720001    0.30719998    0.0803        0.398     ]
 ...
 [-190.62970061    0.37654287    0.30472857    0.10634287    0.56857145]
 [ 115.70197856    0.90439999    0.0615        0.89940006    0.84540004]
 [-184.12310879    0.36720002    0.43530002    0.0246        0.54080003]][0m
[37m[1m[2023-07-10 17:30:23,431][227910] Max Reward on eval: 341.71302650668656[0m
[37m[1m[2023-07-10 17:30:23,431][227910] Min Reward on eval: -935.0215521863895[0m
[37m[1m[2023-07-10 17:30:23,431][227910] Mean Reward across all agents: -141.07614318009598[0m
[37m[1m[2023-07-10 17:30:23,431][227910] Average Trajectory Length: 998.813[0m
[36m[2023-07-10 17:30:23,439][227910] mean_value=20.88543286932101, max_value=777.2500912888675[0m
[37m[1m[2023-07-10 17:30:23,442][227910] New mean coefficients: [[ 1.1526657   2.5370896  -0.62075484 -0.42937523  5.5602937 ]][0m
[37m[1m[2023-07-10 17:30:23,443][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:30:33,146][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 17:30:33,146][227910] FPS: 395830.37[0m
[36m[2023-07-10 17:30:33,148][227910] itr=948, itrs=2000, Progress: 47.40%[0m
[36m[2023-07-10 17:30:44,815][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 17:30:44,815][227910] FPS: 329667.31[0m
[36m[2023-07-10 17:30:49,557][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:30:49,558][227910] Reward + Measures: [[265.01452232   0.88608664   0.655029     0.66315633   0.95545638]][0m
[37m[1m[2023-07-10 17:30:49,558][227910] Max Reward on eval: 265.014522320319[0m
[37m[1m[2023-07-10 17:30:49,558][227910] Min Reward on eval: 265.014522320319[0m
[37m[1m[2023-07-10 17:30:49,558][227910] Mean Reward across all agents: 265.014522320319[0m
[37m[1m[2023-07-10 17:30:49,558][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:30:55,031][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:30:55,031][227910] Reward + Measures: [[ 281.50548024    0.63420004    0.49560004    0.25780001    0.9011001 ]
 [-263.62312677    0.48660001    0.25100002    0.36740002    0.54610002]
 [ 173.92847909    0.62169999    0.3899        0.41770002    0.81830007]
 ...
 [ 249.08305386    0.70410007    0.40809998    0.35859999    0.8853001 ]
 [  82.48616998    0.86669999    0.38920003    0.54039997    0.86689997]
 [  23.2571251     0.77689999    0.37810004    0.48200002    0.6573    ]][0m
[37m[1m[2023-07-10 17:30:55,032][227910] Max Reward on eval: 354.68568075375515[0m
[37m[1m[2023-07-10 17:30:55,032][227910] Min Reward on eval: -995.8920272007351[0m
[37m[1m[2023-07-10 17:30:55,032][227910] Mean Reward across all agents: 64.63219882474009[0m
[37m[1m[2023-07-10 17:30:55,032][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:30:55,040][227910] mean_value=85.56464394285189, max_value=807.2718415442272[0m
[37m[1m[2023-07-10 17:30:55,043][227910] New mean coefficients: [[ 1.1245211   2.872512   -0.5670654   0.50618446  5.841578  ]][0m
[37m[1m[2023-07-10 17:30:55,044][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:31:04,922][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 17:31:04,923][227910] FPS: 388800.29[0m
[36m[2023-07-10 17:31:04,925][227910] itr=949, itrs=2000, Progress: 47.45%[0m
[36m[2023-07-10 17:31:16,525][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 17:31:16,526][227910] FPS: 331548.37[0m
[36m[2023-07-10 17:31:21,413][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:31:21,413][227910] Reward + Measures: [[285.41307788   0.90854436   0.58207631   0.73242569   0.96063429]][0m
[37m[1m[2023-07-10 17:31:21,414][227910] Max Reward on eval: 285.41307788043474[0m
[37m[1m[2023-07-10 17:31:21,414][227910] Min Reward on eval: 285.41307788043474[0m
[37m[1m[2023-07-10 17:31:21,414][227910] Mean Reward across all agents: 285.41307788043474[0m
[37m[1m[2023-07-10 17:31:21,415][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:31:26,907][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:31:26,907][227910] Reward + Measures: [[ 322.25148064    0.90920001    0.38390002    0.84799999    0.88080007]
 [ 151.86999087    0.87630004    0.93990004    0.0003        0.97850001]
 [-361.46239945    0.72080004    0.1382        0.67330003    0.83109993]
 ...
 [  26.44850311    0.82460004    0.1347        0.75550002    0.91040003]
 [ 243.58641338    0.93029994    0.37079999    0.89450008    0.93050003]
 [-599.07069758    0.54860002    0.55129999    0.62689996    0.6067    ]][0m
[37m[1m[2023-07-10 17:31:26,907][227910] Max Reward on eval: 568.3532063472783[0m
[37m[1m[2023-07-10 17:31:26,908][227910] Min Reward on eval: -849.2874812306051[0m
[37m[1m[2023-07-10 17:31:26,908][227910] Mean Reward across all agents: 16.19829288571834[0m
[37m[1m[2023-07-10 17:31:26,908][227910] Average Trajectory Length: 997.2856666666667[0m
[36m[2023-07-10 17:31:26,914][227910] mean_value=-32.6859518989155, max_value=932.7877279122814[0m
[37m[1m[2023-07-10 17:31:26,917][227910] New mean coefficients: [[ 0.47168154  2.7635412  -0.70650876  0.02839085  6.112488  ]][0m
[37m[1m[2023-07-10 17:31:26,918][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:31:36,631][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 17:31:36,632][227910] FPS: 395380.57[0m
[36m[2023-07-10 17:31:36,634][227910] itr=950, itrs=2000, Progress: 47.50%[0m
[37m[1m[2023-07-10 17:31:40,435][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000930[0m
[36m[2023-07-10 17:31:52,244][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 17:31:52,245][227910] FPS: 332422.42[0m
[36m[2023-07-10 17:31:56,996][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:31:57,001][227910] Reward + Measures: [[337.53871199   0.92179769   0.62660134   0.75822735   0.97215903]][0m
[37m[1m[2023-07-10 17:31:57,001][227910] Max Reward on eval: 337.5387119850457[0m
[37m[1m[2023-07-10 17:31:57,002][227910] Min Reward on eval: 337.5387119850457[0m
[37m[1m[2023-07-10 17:31:57,002][227910] Mean Reward across all agents: 337.5387119850457[0m
[37m[1m[2023-07-10 17:31:57,002][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:32:02,470][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:32:02,470][227910] Reward + Measures: [[390.94641511   0.69839996   0.28120002   0.65140003   0.39970002]
 [300.37658034   0.89460003   0.62800002   0.30340001   0.93980008]
 [-31.03317125   0.5492       0.55550003   0.2987       0.71090001]
 ...
 [222.9576124    0.89039993   0.70030004   0.39649999   0.9587    ]
 [196.60183408   0.57020003   0.54799998   0.43169999   0.81640005]
 [380.82044403   0.85750002   0.42640001   0.75         0.90560001]][0m
[37m[1m[2023-07-10 17:32:02,470][227910] Max Reward on eval: 606.4673834974062[0m
[37m[1m[2023-07-10 17:32:02,471][227910] Min Reward on eval: -1309.5640513105318[0m
[37m[1m[2023-07-10 17:32:02,471][227910] Mean Reward across all agents: 182.4637381445091[0m
[37m[1m[2023-07-10 17:32:02,471][227910] Average Trajectory Length: 998.3086666666667[0m
[36m[2023-07-10 17:32:02,480][227910] mean_value=157.2496783403637, max_value=959.9181767860194[0m
[37m[1m[2023-07-10 17:32:02,482][227910] New mean coefficients: [[ 0.6105197   2.6847975  -0.5286586  -0.06557856  5.567446  ]][0m
[37m[1m[2023-07-10 17:32:02,483][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:32:12,325][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 17:32:12,325][227910] FPS: 390243.64[0m
[36m[2023-07-10 17:32:12,327][227910] itr=951, itrs=2000, Progress: 47.55%[0m
[36m[2023-07-10 17:32:23,932][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 17:32:23,932][227910] FPS: 331407.55[0m
[36m[2023-07-10 17:32:28,704][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:32:28,709][227910] Reward + Measures: [[345.80816088   0.93278301   0.51369464   0.74371994   0.98028886]][0m
[37m[1m[2023-07-10 17:32:28,709][227910] Max Reward on eval: 345.8081608756925[0m
[37m[1m[2023-07-10 17:32:28,710][227910] Min Reward on eval: 345.8081608756925[0m
[37m[1m[2023-07-10 17:32:28,710][227910] Mean Reward across all agents: 345.8081608756925[0m
[37m[1m[2023-07-10 17:32:28,710][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:32:34,237][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:32:34,238][227910] Reward + Measures: [[ 465.90960294    0.9054001     0.1901        0.84689999    0.86000007]
 [ 376.18513056    0.57380003    0.87250006    0.0943        0.97170001]
 [  65.89124325    0.64449996    0.76779997    0.14030001    0.92740005]
 ...
 [-192.94305566    0.8035        0.16679999    0.76109999    0.79970002]
 [ 282.56221053    0.89580005    0.09370001    0.82719994    0.9479    ]
 [ 174.43275404    0.74690002    0.3021        0.68009996    0.85500002]][0m
[37m[1m[2023-07-10 17:32:34,238][227910] Max Reward on eval: 572.9576888846699[0m
[37m[1m[2023-07-10 17:32:34,238][227910] Min Reward on eval: -1041.7703082219464[0m
[37m[1m[2023-07-10 17:32:34,239][227910] Mean Reward across all agents: 170.03962865809487[0m
[37m[1m[2023-07-10 17:32:34,239][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:32:34,248][227910] mean_value=86.18555903780742, max_value=947.0479478573275[0m
[37m[1m[2023-07-10 17:32:34,251][227910] New mean coefficients: [[-0.0569461   2.6412392  -0.2305476   0.12828392  6.70989   ]][0m
[37m[1m[2023-07-10 17:32:34,252][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:32:43,966][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 17:32:43,966][227910] FPS: 395373.53[0m
[36m[2023-07-10 17:32:43,968][227910] itr=952, itrs=2000, Progress: 47.60%[0m
[36m[2023-07-10 17:32:55,605][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 17:32:55,606][227910] FPS: 330618.02[0m
[36m[2023-07-10 17:33:00,393][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:33:00,398][227910] Reward + Measures: [[312.49515526   0.91732901   0.62505299   0.565117     0.98567396]][0m
[37m[1m[2023-07-10 17:33:00,398][227910] Max Reward on eval: 312.4951552627797[0m
[37m[1m[2023-07-10 17:33:00,399][227910] Min Reward on eval: 312.4951552627797[0m
[37m[1m[2023-07-10 17:33:00,399][227910] Mean Reward across all agents: 312.4951552627797[0m
[37m[1m[2023-07-10 17:33:00,399][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:33:05,751][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:33:05,756][227910] Reward + Measures: [[ 559.70488036    0.54290003    0.83809996    0.24109998    0.97310013]
 [-501.24598564    0.92309999    0.0904        0.89820004    0.96829998]
 [ 148.33173716    0.74349993    0.34060001    0.6214        0.7723    ]
 ...
 [  76.58883288    0.60740006    0.8136        0.22229998    0.97110003]
 [ 116.32645985    0.91420001    0.26279998    0.85640001    0.92670006]
 [  64.75173766    0.55330002    0.81259996    0.003         0.8976    ]][0m
[37m[1m[2023-07-10 17:33:05,757][227910] Max Reward on eval: 704.236731885327[0m
[37m[1m[2023-07-10 17:33:05,757][227910] Min Reward on eval: -1248.2908810947556[0m
[37m[1m[2023-07-10 17:33:05,757][227910] Mean Reward across all agents: -10.655376682025647[0m
[37m[1m[2023-07-10 17:33:05,757][227910] Average Trajectory Length: 999.6776666666666[0m
[36m[2023-07-10 17:33:05,765][227910] mean_value=-16.414016597182247, max_value=1186.6675261139985[0m
[37m[1m[2023-07-10 17:33:05,768][227910] New mean coefficients: [[0.67514986 2.767283   0.9305696  0.34557563 6.6677003 ]][0m
[37m[1m[2023-07-10 17:33:05,769][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:33:15,479][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 17:33:15,479][227910] FPS: 395534.36[0m
[36m[2023-07-10 17:33:15,482][227910] itr=953, itrs=2000, Progress: 47.65%[0m
[36m[2023-07-10 17:33:26,915][227910] train() took 11.42 seconds to complete[0m
[36m[2023-07-10 17:33:26,915][227910] FPS: 336384.95[0m
[36m[2023-07-10 17:33:31,620][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:33:31,621][227910] Reward + Measures: [[329.46266535   0.92274934   0.69893003   0.48529032   0.98759335]][0m
[37m[1m[2023-07-10 17:33:31,621][227910] Max Reward on eval: 329.4626653509302[0m
[37m[1m[2023-07-10 17:33:31,621][227910] Min Reward on eval: 329.4626653509302[0m
[37m[1m[2023-07-10 17:33:31,621][227910] Mean Reward across all agents: 329.4626653509302[0m
[37m[1m[2023-07-10 17:33:31,621][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:33:37,256][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:33:37,257][227910] Reward + Measures: [[  45.63238016    0.82369995    0.87699997    0.0237        0.9874    ]
 [ 139.43617562    0.89050001    0.1165        0.81399995    0.92690003]
 [ 181.28775185    0.93810004    0.0569        0.88700002    0.93829995]
 ...
 [  53.58663094    0.89829999    0.28029999    0.69480002    0.95890009]
 [ 222.2411626     0.86900008    0.33089998    0.65780002    0.94580001]
 [-764.89415997    0.75790006    0.85799998    0.0002        0.97819996]][0m
[37m[1m[2023-07-10 17:33:37,257][227910] Max Reward on eval: 619.8204767829739[0m
[37m[1m[2023-07-10 17:33:37,257][227910] Min Reward on eval: -896.4642304888694[0m
[37m[1m[2023-07-10 17:33:37,257][227910] Mean Reward across all agents: 141.76344676402212[0m
[37m[1m[2023-07-10 17:33:37,258][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:33:37,264][227910] mean_value=68.16754595916822, max_value=879.5304327789686[0m
[37m[1m[2023-07-10 17:33:37,267][227910] New mean coefficients: [[0.7476883  2.8454416  0.31447595 0.59437156 6.1380033 ]][0m
[37m[1m[2023-07-10 17:33:37,268][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:33:47,104][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 17:33:47,104][227910] FPS: 390455.52[0m
[36m[2023-07-10 17:33:47,106][227910] itr=954, itrs=2000, Progress: 47.70%[0m
[36m[2023-07-10 17:33:58,587][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 17:33:58,587][227910] FPS: 335033.85[0m
[36m[2023-07-10 17:34:03,352][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:34:03,352][227910] Reward + Measures: [[347.09982205   0.9360593    0.71960098   0.44828734   0.98959035]][0m
[37m[1m[2023-07-10 17:34:03,352][227910] Max Reward on eval: 347.09982205339185[0m
[37m[1m[2023-07-10 17:34:03,352][227910] Min Reward on eval: 347.09982205339185[0m
[37m[1m[2023-07-10 17:34:03,353][227910] Mean Reward across all agents: 347.09982205339185[0m
[37m[1m[2023-07-10 17:34:03,353][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:34:08,822][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:34:08,827][227910] Reward + Measures: [[311.83204296   0.74290001   0.73629999   0.0859       0.93110001]
 [430.54883847   0.75890005   0.77290004   0.36980003   0.94069999]
 [369.72825705   0.52039999   0.63160002   0.0732       0.93640006]
 ...
 [404.58123109   0.94499999   0.84630007   0.77430004   0.96350002]
 [387.60279192   0.88129997   0.63120002   0.71600002   0.60860008]
 [157.75031968   0.75639999   0.40830001   0.69340003   0.86139995]][0m
[37m[1m[2023-07-10 17:34:08,828][227910] Max Reward on eval: 560.5794331897516[0m
[37m[1m[2023-07-10 17:34:08,828][227910] Min Reward on eval: -1529.914119863743[0m
[37m[1m[2023-07-10 17:34:08,828][227910] Mean Reward across all agents: 21.58917800820212[0m
[37m[1m[2023-07-10 17:34:08,829][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:34:08,838][227910] mean_value=110.95032114982362, max_value=1060.5794331897516[0m
[37m[1m[2023-07-10 17:34:08,840][227910] New mean coefficients: [[ 0.56951106  2.554375    0.51163197 -0.21446681  6.34556   ]][0m
[37m[1m[2023-07-10 17:34:08,841][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:34:18,537][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 17:34:18,538][227910] FPS: 396110.48[0m
[36m[2023-07-10 17:34:18,540][227910] itr=955, itrs=2000, Progress: 47.75%[0m
[36m[2023-07-10 17:34:29,988][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 17:34:29,989][227910] FPS: 335989.30[0m
[36m[2023-07-10 17:34:34,948][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:34:34,949][227910] Reward + Measures: [[97.94201831  0.79207337  0.98136574  0.00014933  0.9698143 ]][0m
[37m[1m[2023-07-10 17:34:34,949][227910] Max Reward on eval: 97.94201831431454[0m
[37m[1m[2023-07-10 17:34:34,949][227910] Min Reward on eval: 97.94201831431454[0m
[37m[1m[2023-07-10 17:34:34,949][227910] Mean Reward across all agents: 97.94201831431454[0m
[37m[1m[2023-07-10 17:34:34,949][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:34:40,383][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:34:40,383][227910] Reward + Measures: [[ 219.61498107    0.61490005    0.91020006    0.0206        0.87700003]
 [ 234.7345976     0.47889996    0.79459995    0.0242        0.78390002]
 [ 139.09100975    0.6983        0.94150001    0.0021        0.91219997]
 ...
 [ 426.50846532    0.45390001    0.83660001    0.0455        0.8811    ]
 [ 238.3231532     0.50820005    0.77720004    0.0085        0.83030003]
 [-138.64211846    0.67329997    0.85839999    0.0875        0.72619998]][0m
[37m[1m[2023-07-10 17:34:40,383][227910] Max Reward on eval: 590.0995161944535[0m
[37m[1m[2023-07-10 17:34:40,384][227910] Min Reward on eval: -544.5763026324566[0m
[37m[1m[2023-07-10 17:34:40,384][227910] Mean Reward across all agents: 76.6524608858399[0m
[37m[1m[2023-07-10 17:34:40,384][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:34:40,390][227910] mean_value=101.51423142711978, max_value=1078.9810672216931[0m
[37m[1m[2023-07-10 17:34:40,393][227910] New mean coefficients: [[ 0.8361383   2.3729763  -0.27900004 -0.2786623   5.938529  ]][0m
[37m[1m[2023-07-10 17:34:40,394][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:34:50,112][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 17:34:50,112][227910] FPS: 395203.75[0m
[36m[2023-07-10 17:34:50,114][227910] itr=956, itrs=2000, Progress: 47.80%[0m
[36m[2023-07-10 17:35:01,696][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 17:35:01,696][227910] FPS: 332064.65[0m
[36m[2023-07-10 17:35:06,514][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:35:06,514][227910] Reward + Measures: [[-358.3022923     0.95557129    0.99438834    0.00035467    0.99432695]][0m
[37m[1m[2023-07-10 17:35:06,514][227910] Max Reward on eval: -358.3022922975605[0m
[37m[1m[2023-07-10 17:35:06,514][227910] Min Reward on eval: -358.3022922975605[0m
[37m[1m[2023-07-10 17:35:06,514][227910] Mean Reward across all agents: -358.3022922975605[0m
[37m[1m[2023-07-10 17:35:06,515][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:35:11,990][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:35:11,991][227910] Reward + Measures: [[-32.96196182   0.29099998   0.8955       0.0631       0.82120001]
 [264.66992688   0.5323       0.93190002   0.0302       0.92459995]
 [ 11.88702799   0.52360004   0.87930006   0.1173       0.88519996]
 ...
 [-78.97335304   0.34639999   0.89840001   0.0882       0.83850002]
 [ 35.01030476   0.87050003   0.90010005   0.0031       0.9684    ]
 [ 75.26935562   0.73620003   0.96709996   0.0005       0.98710006]][0m
[37m[1m[2023-07-10 17:35:11,991][227910] Max Reward on eval: 356.3330620884255[0m
[37m[1m[2023-07-10 17:35:11,991][227910] Min Reward on eval: -1267.5658401102294[0m
[37m[1m[2023-07-10 17:35:11,991][227910] Mean Reward across all agents: -90.4212472299297[0m
[37m[1m[2023-07-10 17:35:11,991][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:35:11,995][227910] mean_value=-170.01498112368836, max_value=672.8939573989205[0m
[37m[1m[2023-07-10 17:35:11,998][227910] New mean coefficients: [[ 1.0426581   1.98886    -0.06334633 -0.4642927   5.524366  ]][0m
[37m[1m[2023-07-10 17:35:11,999][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:35:21,855][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 17:35:21,856][227910] FPS: 389639.13[0m
[36m[2023-07-10 17:35:21,858][227910] itr=957, itrs=2000, Progress: 47.85%[0m
[36m[2023-07-10 17:35:33,412][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 17:35:33,412][227910] FPS: 332966.75[0m
[36m[2023-07-10 17:35:38,237][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:35:38,237][227910] Reward + Measures: [[312.06891383   0.95960671   0.95602036   0.00043      0.99056566]][0m
[37m[1m[2023-07-10 17:35:38,237][227910] Max Reward on eval: 312.06891383420276[0m
[37m[1m[2023-07-10 17:35:38,237][227910] Min Reward on eval: 312.06891383420276[0m
[37m[1m[2023-07-10 17:35:38,238][227910] Mean Reward across all agents: 312.06891383420276[0m
[37m[1m[2023-07-10 17:35:38,238][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:35:43,696][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:35:43,701][227910] Reward + Measures: [[-569.46351326    0.41949996    0.85319996    0.12840001    0.87550002]
 [-650.07476571    0.53920001    0.97109997    0.0743        0.98590004]
 [-386.73989491    0.49639997    0.81000006    0.0966        0.88570005]
 ...
 [ -40.84387673    0.65000004    0.9012        0.0304        0.96929997]
 [-393.85792926    0.83579999    0.88659996    0.0126        0.88620007]
 [ 173.63888882    0.96509993    0.97439998    0.0007        0.99420005]][0m
[37m[1m[2023-07-10 17:35:43,702][227910] Max Reward on eval: 415.4652732120536[0m
[37m[1m[2023-07-10 17:35:43,702][227910] Min Reward on eval: -1102.4648016357562[0m
[37m[1m[2023-07-10 17:35:43,702][227910] Mean Reward across all agents: -163.43048000721356[0m
[37m[1m[2023-07-10 17:35:43,702][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:35:43,705][227910] mean_value=-320.667811018964, max_value=582.2373133922994[0m
[37m[1m[2023-07-10 17:35:43,707][227910] New mean coefficients: [[ 1.0901438   2.2811637  -0.6887067  -0.09578651  4.43908   ]][0m
[37m[1m[2023-07-10 17:35:43,708][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:35:53,388][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 17:35:53,388][227910] FPS: 396782.92[0m
[36m[2023-07-10 17:35:53,391][227910] itr=958, itrs=2000, Progress: 47.90%[0m
[36m[2023-07-10 17:36:05,015][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 17:36:05,015][227910] FPS: 330952.97[0m
[36m[2023-07-10 17:36:09,816][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:36:09,816][227910] Reward + Measures: [[241.64233132   0.98315805   0.07385067   0.89015836   0.96713459]][0m
[37m[1m[2023-07-10 17:36:09,817][227910] Max Reward on eval: 241.64233132334388[0m
[37m[1m[2023-07-10 17:36:09,817][227910] Min Reward on eval: 241.64233132334388[0m
[37m[1m[2023-07-10 17:36:09,817][227910] Mean Reward across all agents: 241.64233132334388[0m
[37m[1m[2023-07-10 17:36:09,817][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:36:15,421][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:36:15,422][227910] Reward + Measures: [[ -72.39660521    0.9637        0.86650002    0.47480002    0.95269996]
 [ 119.5647116     0.93619996    0.32070002    0.95720005    0.79180002]
 [   4.5810047     0.88370007    0.36630002    0.90020007    0.69280005]
 ...
 [-152.59149564    0.98320001    0.86009997    0.92379999    0.98089999]
 [ -65.45484774    0.96680003    0.10730001    0.87060004    0.90710002]
 [-737.64588748    0.92970002    0.0233        0.84680003    0.86730003]][0m
[37m[1m[2023-07-10 17:36:15,422][227910] Max Reward on eval: 410.49176347840114[0m
[37m[1m[2023-07-10 17:36:15,422][227910] Min Reward on eval: -864.4051573687[0m
[37m[1m[2023-07-10 17:36:15,422][227910] Mean Reward across all agents: -19.388031584593975[0m
[37m[1m[2023-07-10 17:36:15,422][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:36:15,427][227910] mean_value=1.0573970244941544, max_value=690.1462191547355[0m
[37m[1m[2023-07-10 17:36:15,430][227910] New mean coefficients: [[ 1.5037076   2.231741   -0.5994551  -0.41665465  3.5356092 ]][0m
[37m[1m[2023-07-10 17:36:15,431][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:36:25,081][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 17:36:25,081][227910] FPS: 397996.25[0m
[36m[2023-07-10 17:36:25,083][227910] itr=959, itrs=2000, Progress: 47.95%[0m
[36m[2023-07-10 17:36:36,512][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 17:36:36,512][227910] FPS: 336622.30[0m
[36m[2023-07-10 17:36:41,212][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:36:41,212][227910] Reward + Measures: [[268.17220525   0.97786671   0.05541333   0.90015364   0.97696435]][0m
[37m[1m[2023-07-10 17:36:41,213][227910] Max Reward on eval: 268.17220524873244[0m
[37m[1m[2023-07-10 17:36:41,213][227910] Min Reward on eval: 268.17220524873244[0m
[37m[1m[2023-07-10 17:36:41,213][227910] Mean Reward across all agents: 268.17220524873244[0m
[37m[1m[2023-07-10 17:36:41,213][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:36:46,642][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:36:46,647][227910] Reward + Measures: [[ 382.93860253    0.74000007    0.48100001    0.73250002    0.6401    ]
 [-218.23189645    0.97920001    0.91180003    0.91580003    0.98509997]
 [ 416.13303591    0.82590008    0.51709998    0.84400004    0.69440001]
 ...
 [-398.75492141    0.97299999    0.76540005    0.91079998    0.91479999]
 [ 354.71211436    0.96219999    0.1661        0.92700005    0.78439999]
 [ 438.557961      0.91970009    0.34830001    0.67659998    0.96719998]][0m
[37m[1m[2023-07-10 17:36:46,648][227910] Max Reward on eval: 572.1123421231459[0m
[37m[1m[2023-07-10 17:36:46,648][227910] Min Reward on eval: -634.6671688618371[0m
[37m[1m[2023-07-10 17:36:46,648][227910] Mean Reward across all agents: 134.35625357300566[0m
[37m[1m[2023-07-10 17:36:46,649][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:36:46,655][227910] mean_value=11.412703515634712, max_value=903.9070920415525[0m
[37m[1m[2023-07-10 17:36:46,658][227910] New mean coefficients: [[ 1.4134364   2.4327855  -0.14022118 -0.46559042  4.225068  ]][0m
[37m[1m[2023-07-10 17:36:46,659][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:36:56,310][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 17:36:56,310][227910] FPS: 397940.56[0m
[36m[2023-07-10 17:36:56,312][227910] itr=960, itrs=2000, Progress: 48.00%[0m
[37m[1m[2023-07-10 17:36:59,929][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000940[0m
[36m[2023-07-10 17:37:11,703][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 17:37:11,703][227910] FPS: 333881.39[0m
[36m[2023-07-10 17:37:16,508][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:37:16,508][227910] Reward + Measures: [[-397.74712124    0.98471695    0.00353767    0.97619772    0.985506  ]][0m
[37m[1m[2023-07-10 17:37:16,508][227910] Max Reward on eval: -397.74712123911377[0m
[37m[1m[2023-07-10 17:37:16,508][227910] Min Reward on eval: -397.74712123911377[0m
[37m[1m[2023-07-10 17:37:16,508][227910] Mean Reward across all agents: -397.74712123911377[0m
[37m[1m[2023-07-10 17:37:16,509][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:37:22,017][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:37:22,017][227910] Reward + Measures: [[ -88.37996617    0.88230002    0.45909998    0.93230003    0.83719999]
 [-426.59583248    0.76440001    0.0121        0.76370001    0.77180004]
 [-222.27840843    0.90650004    0.0425        0.89470005    0.92889994]
 ...
 [  92.68604713    0.74229997    0.2863        0.71870005    0.51570004]
 [-158.71633388    0.87560004    0.78099996    0.72600001    0.89520007]
 [-473.01980559    0.90130007    0.0387        0.89060003    0.92889994]][0m
[37m[1m[2023-07-10 17:37:22,018][227910] Max Reward on eval: 393.7685916183982[0m
[37m[1m[2023-07-10 17:37:22,018][227910] Min Reward on eval: -1011.5067985209404[0m
[37m[1m[2023-07-10 17:37:22,018][227910] Mean Reward across all agents: -115.82759028538945[0m
[37m[1m[2023-07-10 17:37:22,018][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:37:22,021][227910] mean_value=-361.9850854402954, max_value=677.7843600739259[0m
[37m[1m[2023-07-10 17:37:22,024][227910] New mean coefficients: [[ 1.8636367   2.7066722   0.65866405 -0.6130091   5.0774736 ]][0m
[37m[1m[2023-07-10 17:37:22,025][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:37:31,740][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 17:37:31,740][227910] FPS: 395347.67[0m
[36m[2023-07-10 17:37:31,742][227910] itr=961, itrs=2000, Progress: 48.05%[0m
[36m[2023-07-10 17:37:43,214][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 17:37:43,214][227910] FPS: 335267.39[0m
[36m[2023-07-10 17:37:48,033][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:37:48,039][227910] Reward + Measures: [[504.80765887   0.88783234   0.03026867   0.879722     0.85003203]][0m
[37m[1m[2023-07-10 17:37:48,039][227910] Max Reward on eval: 504.8076588698278[0m
[37m[1m[2023-07-10 17:37:48,039][227910] Min Reward on eval: 504.8076588698278[0m
[37m[1m[2023-07-10 17:37:48,039][227910] Mean Reward across all agents: 504.8076588698278[0m
[37m[1m[2023-07-10 17:37:48,040][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:37:53,750][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:37:53,751][227910] Reward + Measures: [[653.38459729   0.8818       0.039        0.86129999   0.58109999]
 [657.07190868   0.64520001   0.13589999   0.69590008   0.58330005]
 [567.77399728   0.95200008   0.033        0.89920008   0.94239998]
 ...
 [627.60819095   0.91470003   0.0292       0.82450002   0.72780001]
 [606.2562424    0.87540001   0.029        0.85080004   0.72710001]
 [567.18800435   0.94980001   0.0285       0.8858       0.91180003]][0m
[37m[1m[2023-07-10 17:37:53,751][227910] Max Reward on eval: 735.6331869960529[0m
[37m[1m[2023-07-10 17:37:53,751][227910] Min Reward on eval: 83.65039916768437[0m
[37m[1m[2023-07-10 17:37:53,751][227910] Mean Reward across all agents: 559.0223312856025[0m
[37m[1m[2023-07-10 17:37:53,752][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:37:53,759][227910] mean_value=244.31265632135066, max_value=1066.7583048891975[0m
[37m[1m[2023-07-10 17:37:53,762][227910] New mean coefficients: [[ 1.6891909   2.5925264   0.30999678 -1.6348597   5.2096767 ]][0m
[37m[1m[2023-07-10 17:37:53,763][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:38:03,503][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 17:38:03,503][227910] FPS: 394315.25[0m
[36m[2023-07-10 17:38:03,505][227910] itr=962, itrs=2000, Progress: 48.10%[0m
[36m[2023-07-10 17:38:15,017][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 17:38:15,018][227910] FPS: 334124.97[0m
[36m[2023-07-10 17:38:19,781][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:38:19,781][227910] Reward + Measures: [[462.6171181    0.91543669   0.03258433   0.89220732   0.90069026]][0m
[37m[1m[2023-07-10 17:38:19,782][227910] Max Reward on eval: 462.61711810311937[0m
[37m[1m[2023-07-10 17:38:19,782][227910] Min Reward on eval: 462.61711810311937[0m
[37m[1m[2023-07-10 17:38:19,782][227910] Mean Reward across all agents: 462.61711810311937[0m
[37m[1m[2023-07-10 17:38:19,782][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:38:25,273][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:38:25,273][227910] Reward + Measures: [[738.48276227   0.86949998   0.039        0.87509996   0.71390003]
 [701.67762654   0.89930004   0.0307       0.90609998   0.91289997]
 [656.78830062   0.85960007   0.0672       0.83690006   0.8707    ]
 ...
 [623.65291306   0.90240002   0.0147       0.76499999   0.80130005]
 [390.39720009   0.85659999   0.0209       0.741        0.75260001]
 [622.25366296   0.90560001   0.0253       0.77629995   0.8373    ]][0m
[37m[1m[2023-07-10 17:38:25,273][227910] Max Reward on eval: 791.1599320570473[0m
[37m[1m[2023-07-10 17:38:25,274][227910] Min Reward on eval: 34.24778170409845[0m
[37m[1m[2023-07-10 17:38:25,274][227910] Mean Reward across all agents: 591.0120024434895[0m
[37m[1m[2023-07-10 17:38:25,274][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:38:25,281][227910] mean_value=246.87273204640505, max_value=1261.429671891313[0m
[37m[1m[2023-07-10 17:38:25,284][227910] New mean coefficients: [[ 1.946678   3.1074028  0.7394786 -1.6678356  6.409557 ]][0m
[37m[1m[2023-07-10 17:38:25,284][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:38:35,080][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 17:38:35,080][227910] FPS: 392080.15[0m
[36m[2023-07-10 17:38:35,082][227910] itr=963, itrs=2000, Progress: 48.15%[0m
[36m[2023-07-10 17:38:46,703][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 17:38:46,703][227910] FPS: 330978.74[0m
[36m[2023-07-10 17:38:51,533][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:38:51,533][227910] Reward + Measures: [[232.98311074   0.94189793   0.042155     0.90015429   0.94471204]][0m
[37m[1m[2023-07-10 17:38:51,534][227910] Max Reward on eval: 232.98311073911918[0m
[37m[1m[2023-07-10 17:38:51,534][227910] Min Reward on eval: 232.98311073911918[0m
[37m[1m[2023-07-10 17:38:51,534][227910] Mean Reward across all agents: 232.98311073911918[0m
[37m[1m[2023-07-10 17:38:51,534][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:38:57,100][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:38:57,101][227910] Reward + Measures: [[593.04728995   0.90879995   0.0194       0.90220004   0.82550001]
 [618.39901991   0.91650003   0.0227       0.93850005   0.85780001]
 [463.28477597   0.93829995   0.0327       0.92679995   0.9325    ]
 ...
 [452.16608646   0.95710003   0.0225       0.94480002   0.9533    ]
 [385.5039235    0.96289998   0.0122       0.95240003   0.95500004]
 [616.82052654   0.87869996   0.0326       0.90430003   0.745     ]][0m
[37m[1m[2023-07-10 17:38:57,101][227910] Max Reward on eval: 665.5771216813242[0m
[37m[1m[2023-07-10 17:38:57,101][227910] Min Reward on eval: 207.93448467242996[0m
[37m[1m[2023-07-10 17:38:57,101][227910] Mean Reward across all agents: 537.497417003228[0m
[37m[1m[2023-07-10 17:38:57,101][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:38:57,104][227910] mean_value=-16.7413226409134, max_value=1063.4246946662665[0m
[37m[1m[2023-07-10 17:38:57,106][227910] New mean coefficients: [[ 2.2328956  3.4141288  0.7379501 -1.6401482  6.505959 ]][0m
[37m[1m[2023-07-10 17:38:57,107][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:39:06,830][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 17:39:06,831][227910] FPS: 395007.62[0m
[36m[2023-07-10 17:39:06,833][227910] itr=964, itrs=2000, Progress: 48.20%[0m
[36m[2023-07-10 17:39:18,535][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 17:39:18,535][227910] FPS: 328755.49[0m
[36m[2023-07-10 17:39:23,404][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:39:23,404][227910] Reward + Measures: [[226.75871774   0.93612158   0.049962     0.88473207   0.94162303]][0m
[37m[1m[2023-07-10 17:39:23,404][227910] Max Reward on eval: 226.75871774483122[0m
[37m[1m[2023-07-10 17:39:23,404][227910] Min Reward on eval: 226.75871774483122[0m
[37m[1m[2023-07-10 17:39:23,405][227910] Mean Reward across all agents: 226.75871774483122[0m
[37m[1m[2023-07-10 17:39:23,405][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:39:28,921][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:39:28,926][227910] Reward + Measures: [[470.77838514   0.84569997   0.0758       0.75390005   0.84679997]
 [-20.1133475    0.42820001   0.3091       0.36300001   0.4113    ]
 [ 75.90673736   0.71440005   0.49460003   0.67750001   0.7331    ]
 ...
 [-53.82625169   0.42089996   0.35069999   0.34720001   0.39900002]
 [ -1.99117309   0.41140005   0.35139999   0.345        0.41790006]
 [-34.79845366   0.51770002   0.3547       0.45170003   0.52279997]][0m
[37m[1m[2023-07-10 17:39:28,927][227910] Max Reward on eval: 592.5638476247899[0m
[37m[1m[2023-07-10 17:39:28,927][227910] Min Reward on eval: -655.3154419790692[0m
[37m[1m[2023-07-10 17:39:28,927][227910] Mean Reward across all agents: 124.44063306522047[0m
[37m[1m[2023-07-10 17:39:28,927][227910] Average Trajectory Length: 999.1426666666666[0m
[36m[2023-07-10 17:39:28,932][227910] mean_value=-520.6426455306091, max_value=845.3476212858347[0m
[37m[1m[2023-07-10 17:39:28,935][227910] New mean coefficients: [[ 1.6752006   2.829894    0.75082266 -2.160964    6.3313203 ]][0m
[37m[1m[2023-07-10 17:39:28,936][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:39:38,829][227910] train() took 9.89 seconds to complete[0m
[36m[2023-07-10 17:39:38,830][227910] FPS: 388195.04[0m
[36m[2023-07-10 17:39:38,832][227910] itr=965, itrs=2000, Progress: 48.25%[0m
[36m[2023-07-10 17:39:50,430][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 17:39:50,430][227910] FPS: 331639.91[0m
[36m[2023-07-10 17:39:55,248][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:39:55,248][227910] Reward + Measures: [[304.97657715   0.88260496   0.24211366   0.69907171   0.9735173 ]][0m
[37m[1m[2023-07-10 17:39:55,249][227910] Max Reward on eval: 304.97657714909707[0m
[37m[1m[2023-07-10 17:39:55,249][227910] Min Reward on eval: 304.97657714909707[0m
[37m[1m[2023-07-10 17:39:55,249][227910] Mean Reward across all agents: 304.97657714909707[0m
[37m[1m[2023-07-10 17:39:55,249][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:40:00,843][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:40:00,844][227910] Reward + Measures: [[215.6336131    0.96429998   0.0089       0.96630001   0.97030002]
 [286.86238941   0.91409999   0.0211       0.92740005   0.92830002]
 [ 64.90359228   0.93669999   0.0233       0.92189997   0.93809998]
 ...
 [-99.54091349   0.7274       0.44209996   0.38069999   0.85830003]
 [225.8719769    0.96259993   0.0086       0.96450007   0.96940005]
 [185.87448434   0.95590001   0.008        0.95810002   0.95660001]][0m
[37m[1m[2023-07-10 17:40:00,844][227910] Max Reward on eval: 622.1256109715439[0m
[37m[1m[2023-07-10 17:40:00,844][227910] Min Reward on eval: -819.6630119155859[0m
[37m[1m[2023-07-10 17:40:00,845][227910] Mean Reward across all agents: 174.13037411478354[0m
[37m[1m[2023-07-10 17:40:00,845][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:40:00,847][227910] mean_value=-254.81735147884464, max_value=644.7638819550962[0m
[37m[1m[2023-07-10 17:40:00,850][227910] New mean coefficients: [[ 1.0329286   2.0113385  -0.04212952 -2.930807    6.603472  ]][0m
[37m[1m[2023-07-10 17:40:00,851][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:40:10,554][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 17:40:10,555][227910] FPS: 395790.31[0m
[36m[2023-07-10 17:40:10,557][227910] itr=966, itrs=2000, Progress: 48.30%[0m
[36m[2023-07-10 17:40:21,985][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 17:40:21,986][227910] FPS: 336583.13[0m
[36m[2023-07-10 17:40:26,722][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:40:26,728][227910] Reward + Measures: [[374.72054223   0.67106467   0.67151231   0.16973865   0.8635813 ]][0m
[37m[1m[2023-07-10 17:40:26,728][227910] Max Reward on eval: 374.7205422320903[0m
[37m[1m[2023-07-10 17:40:26,728][227910] Min Reward on eval: 374.7205422320903[0m
[37m[1m[2023-07-10 17:40:26,728][227910] Mean Reward across all agents: 374.7205422320903[0m
[37m[1m[2023-07-10 17:40:26,729][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:40:32,263][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:40:32,264][227910] Reward + Measures: [[261.46919124   0.7317       0.71200001   0.31549999   0.88140005]
 [ 40.62502923   0.97619992   0.96969998   0.0001       0.98049992]
 [479.67647574   0.49400002   0.60070002   0.44650003   0.79560006]
 ...
 [442.64851503   0.54640001   0.74470001   0.11870001   0.89069998]
 [331.11326978   0.59540004   0.75620002   0.27260002   0.8847    ]
 [351.80515494   0.71820003   0.6692       0.1019       0.85960007]][0m
[37m[1m[2023-07-10 17:40:32,264][227910] Max Reward on eval: 592.8349279203452[0m
[37m[1m[2023-07-10 17:40:32,264][227910] Min Reward on eval: -421.0660990997218[0m
[37m[1m[2023-07-10 17:40:32,265][227910] Mean Reward across all agents: 327.4207593488828[0m
[37m[1m[2023-07-10 17:40:32,265][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:40:32,272][227910] mean_value=376.9196344226699, max_value=911.7372050084654[0m
[37m[1m[2023-07-10 17:40:32,275][227910] New mean coefficients: [[ 1.0476711   1.6225839  -0.30826914 -2.885452    6.479229  ]][0m
[37m[1m[2023-07-10 17:40:32,276][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:40:41,986][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 17:40:41,987][227910] FPS: 395511.89[0m
[36m[2023-07-10 17:40:41,989][227910] itr=967, itrs=2000, Progress: 48.35%[0m
[36m[2023-07-10 17:40:53,467][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 17:40:53,467][227910] FPS: 335173.13[0m
[36m[2023-07-10 17:40:58,248][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:40:58,248][227910] Reward + Measures: [[420.39526346   0.76460099   0.88268441   0.01215867   0.9563387 ]][0m
[37m[1m[2023-07-10 17:40:58,248][227910] Max Reward on eval: 420.39526346495444[0m
[37m[1m[2023-07-10 17:40:58,249][227910] Min Reward on eval: 420.39526346495444[0m
[37m[1m[2023-07-10 17:40:58,249][227910] Mean Reward across all agents: 420.39526346495444[0m
[37m[1m[2023-07-10 17:40:58,249][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:41:03,786][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:41:03,791][227910] Reward + Measures: [[369.53132558   0.8531       0.86379999   0.0007       0.95419997]
 [453.24392591   0.56529999   0.63709998   0.15800001   0.83389997]
 [508.06753883   0.56419998   0.56580001   0.2131       0.79879999]
 ...
 [567.59769105   0.64740002   0.72890007   0.09240001   0.87639999]
 [569.27087549   0.76230007   0.86580002   0.0009       0.94999999]
 [584.46625648   0.71420002   0.87559998   0.0036       0.917     ]][0m
[37m[1m[2023-07-10 17:41:03,792][227910] Max Reward on eval: 705.2184469261672[0m
[37m[1m[2023-07-10 17:41:03,792][227910] Min Reward on eval: 223.84669190834975[0m
[37m[1m[2023-07-10 17:41:03,792][227910] Mean Reward across all agents: 489.4054225987442[0m
[37m[1m[2023-07-10 17:41:03,793][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:41:03,800][227910] mean_value=252.6549262810047, max_value=860.0874545480501[0m
[37m[1m[2023-07-10 17:41:03,803][227910] New mean coefficients: [[ 0.8119062   3.3146904  -0.12578906 -2.2518191   7.6163044 ]][0m
[37m[1m[2023-07-10 17:41:03,804][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:41:13,460][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 17:41:13,460][227910] FPS: 397737.08[0m
[36m[2023-07-10 17:41:13,463][227910] itr=968, itrs=2000, Progress: 48.40%[0m
[36m[2023-07-10 17:41:24,925][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 17:41:24,925][227910] FPS: 335644.85[0m
[36m[2023-07-10 17:41:29,706][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:41:29,712][227910] Reward + Measures: [[-466.14185162    0.82569402    0.79330373    0.00083167    0.95317066]][0m
[37m[1m[2023-07-10 17:41:29,712][227910] Max Reward on eval: -466.1418516215357[0m
[37m[1m[2023-07-10 17:41:29,712][227910] Min Reward on eval: -466.1418516215357[0m
[37m[1m[2023-07-10 17:41:29,713][227910] Mean Reward across all agents: -466.1418516215357[0m
[37m[1m[2023-07-10 17:41:29,713][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:41:35,185][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:41:35,186][227910] Reward + Measures: [[ -392.75320761     0.9016         0.94390005     0.
      0.96370012]
 [ -386.13809602     0.62910002     0.93510002     0.0045
      0.97300005]
 [  215.65347813     0.87670004     0.33940002     0.6056
      0.69560003]
 ...
 [-1379.89732152     0.65749997     0.72100002     0.0503
      0.70130002]
 [ -812.18204858     0.86790001     0.91919994     0.
      0.95900005]
 [ -265.8234237      0.75299996     0.97840005     0.0031
      0.98509997]][0m
[37m[1m[2023-07-10 17:41:35,186][227910] Max Reward on eval: 509.9952077852562[0m
[37m[1m[2023-07-10 17:41:35,186][227910] Min Reward on eval: -1590.2746842859779[0m
[37m[1m[2023-07-10 17:41:35,187][227910] Mean Reward across all agents: -263.7005749544827[0m
[37m[1m[2023-07-10 17:41:35,187][227910] Average Trajectory Length: 999.709[0m
[36m[2023-07-10 17:41:35,190][227910] mean_value=-455.5554874196157, max_value=746.4496139840126[0m
[37m[1m[2023-07-10 17:41:35,192][227910] New mean coefficients: [[ 0.05378097  3.2131128   0.1915196  -2.3411222   7.9764442 ]][0m
[37m[1m[2023-07-10 17:41:35,193][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:41:44,967][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 17:41:44,967][227910] FPS: 392980.32[0m
[36m[2023-07-10 17:41:44,969][227910] itr=969, itrs=2000, Progress: 48.45%[0m
[36m[2023-07-10 17:41:56,493][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 17:41:56,493][227910] FPS: 333730.61[0m
[36m[2023-07-10 17:42:01,333][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:42:01,334][227910] Reward + Measures: [[187.31115659   0.43004534   0.78420001   0.052126     0.91417331]][0m
[37m[1m[2023-07-10 17:42:01,334][227910] Max Reward on eval: 187.31115658883456[0m
[37m[1m[2023-07-10 17:42:01,334][227910] Min Reward on eval: 187.31115658883456[0m
[37m[1m[2023-07-10 17:42:01,334][227910] Mean Reward across all agents: 187.31115658883456[0m
[37m[1m[2023-07-10 17:42:01,335][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:42:06,822][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:42:06,823][227910] Reward + Measures: [[-519.55009569    0.4499        0.24270001    0.5007        0.38270003]
 [-507.04527482    0.52140003    0.16680001    0.5891        0.60120004]
 [  21.93332458    0.71150005    0.13900001    0.67919993    0.74439996]
 ...
 [-108.16272035    0.34759998    0.50050002    0.38350001    0.60579997]
 [-476.84732275    0.47389999    0.12119999    0.52340001    0.47849998]
 [-312.50322906    0.65399998    0.18529999    0.68440002    0.46090004]][0m
[37m[1m[2023-07-10 17:42:06,823][227910] Max Reward on eval: 482.44419410380067[0m
[37m[1m[2023-07-10 17:42:06,823][227910] Min Reward on eval: -1321.9519353746903[0m
[37m[1m[2023-07-10 17:42:06,824][227910] Mean Reward across all agents: -166.64617508786216[0m
[37m[1m[2023-07-10 17:42:06,824][227910] Average Trajectory Length: 998.0396666666667[0m
[36m[2023-07-10 17:42:06,827][227910] mean_value=-564.7017505516941, max_value=821.9986799076548[0m
[37m[1m[2023-07-10 17:42:06,830][227910] New mean coefficients: [[ 0.47215608  3.941887    0.92561024 -1.2234597   8.180856  ]][0m
[37m[1m[2023-07-10 17:42:06,832][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:42:16,479][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 17:42:16,479][227910] FPS: 398114.79[0m
[36m[2023-07-10 17:42:16,481][227910] itr=970, itrs=2000, Progress: 48.50%[0m
[37m[1m[2023-07-10 17:42:20,276][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000950[0m
[36m[2023-07-10 17:42:32,022][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 17:42:32,027][227910] FPS: 334955.54[0m
[36m[2023-07-10 17:42:36,844][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:42:36,845][227910] Reward + Measures: [[187.83121985   0.61149001   0.92692107   0.02520733   0.96871734]][0m
[37m[1m[2023-07-10 17:42:36,845][227910] Max Reward on eval: 187.83121985314483[0m
[37m[1m[2023-07-10 17:42:36,845][227910] Min Reward on eval: 187.83121985314483[0m
[37m[1m[2023-07-10 17:42:36,845][227910] Mean Reward across all agents: 187.83121985314483[0m
[37m[1m[2023-07-10 17:42:36,846][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:42:42,325][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:42:42,325][227910] Reward + Measures: [[ -786.15389478     0.51750004     0.92199993     0.0049
      0.96250004]
 [ -681.84495567     0.63749999     0.95160002     0.0041
      0.96600002]
 [  107.75943744     0.7669         0.57870001     0.74729997
      0.74970001]
 ...
 [-1237.32424488     0.54160005     0.78570002     0.0494
      0.76389998]
 [-1226.62702927     0.77979994     0.94520009     0.0118
      0.95409995]
 [  626.66692058     0.67910004     0.2139         0.67559999
      0.55250007]][0m
[37m[1m[2023-07-10 17:42:42,326][227910] Max Reward on eval: 649.299682727782[0m
[37m[1m[2023-07-10 17:42:42,326][227910] Min Reward on eval: -1617.57187451208[0m
[37m[1m[2023-07-10 17:42:42,326][227910] Mean Reward across all agents: -366.05190428150684[0m
[37m[1m[2023-07-10 17:42:42,326][227910] Average Trajectory Length: 999.6719999999999[0m
[36m[2023-07-10 17:42:42,329][227910] mean_value=-567.969699414852, max_value=804.5251218751393[0m
[37m[1m[2023-07-10 17:42:42,332][227910] New mean coefficients: [[ 1.2439643  3.7994015 -0.2761641 -1.094274   6.9479427]][0m
[37m[1m[2023-07-10 17:42:42,333][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:42:52,161][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 17:42:52,161][227910] FPS: 390785.24[0m
[36m[2023-07-10 17:42:52,163][227910] itr=971, itrs=2000, Progress: 48.55%[0m
[36m[2023-07-10 17:43:03,715][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 17:43:03,715][227910] FPS: 332947.61[0m
[36m[2023-07-10 17:43:08,499][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:43:08,499][227910] Reward + Measures: [[699.91310652   0.82007158   0.13422      0.76626939   0.80752361]][0m
[37m[1m[2023-07-10 17:43:08,499][227910] Max Reward on eval: 699.9131065215589[0m
[37m[1m[2023-07-10 17:43:08,499][227910] Min Reward on eval: 699.9131065215589[0m
[37m[1m[2023-07-10 17:43:08,500][227910] Mean Reward across all agents: 699.9131065215589[0m
[37m[1m[2023-07-10 17:43:08,500][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:43:14,053][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:43:14,054][227910] Reward + Measures: [[585.8023051    0.69209999   0.27980003   0.58960003   0.77980006]
 [689.53917821   0.76820004   0.36989999   0.49900004   0.86189997]
 [540.75157191   0.75729996   0.72250003   0.51960003   0.8818    ]
 ...
 [668.29735777   0.66000003   0.83680004   0.09640001   0.94029999]
 [673.99226945   0.62270004   0.72430009   0.17880002   0.89810002]
 [701.79808564   0.86580002   0.0704       0.82019997   0.84930003]][0m
[37m[1m[2023-07-10 17:43:14,054][227910] Max Reward on eval: 782.5532478170935[0m
[37m[1m[2023-07-10 17:43:14,054][227910] Min Reward on eval: -306.926657999598[0m
[37m[1m[2023-07-10 17:43:14,055][227910] Mean Reward across all agents: 596.4952953782555[0m
[37m[1m[2023-07-10 17:43:14,055][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:43:14,065][227910] mean_value=442.3707792018621, max_value=1141.543783513935[0m
[37m[1m[2023-07-10 17:43:14,068][227910] New mean coefficients: [[ 1.0879636   3.3929355   0.11336681 -1.1303326   6.556928  ]][0m
[37m[1m[2023-07-10 17:43:14,069][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:43:23,716][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 17:43:23,716][227910] FPS: 398124.73[0m
[36m[2023-07-10 17:43:23,718][227910] itr=972, itrs=2000, Progress: 48.60%[0m
[36m[2023-07-10 17:43:35,293][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 17:43:35,293][227910] FPS: 332276.19[0m
[36m[2023-07-10 17:43:40,022][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:43:40,022][227910] Reward + Measures: [[678.36629759   0.76044333   0.292631     0.6698153    0.90232891]][0m
[37m[1m[2023-07-10 17:43:40,023][227910] Max Reward on eval: 678.3662975945513[0m
[37m[1m[2023-07-10 17:43:40,023][227910] Min Reward on eval: 678.3662975945513[0m
[37m[1m[2023-07-10 17:43:40,023][227910] Mean Reward across all agents: 678.3662975945513[0m
[37m[1m[2023-07-10 17:43:40,023][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:43:45,604][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:43:45,604][227910] Reward + Measures: [[475.26904803   0.57199997   0.8624       0.1181       0.93110007]
 [431.89139266   0.56389999   0.3488       0.65799999   0.68380004]
 [612.61254395   0.8707       0.0472       0.89410001   0.8459    ]
 ...
 [762.07387347   0.48800001   0.71410006   0.21859999   0.88749999]
 [117.60099306   0.8391       0.0303       0.88370001   0.79170007]
 [502.33115873   0.69390005   0.56779999   0.71730006   0.82130003]][0m
[37m[1m[2023-07-10 17:43:45,605][227910] Max Reward on eval: 804.8838647768832[0m
[37m[1m[2023-07-10 17:43:45,605][227910] Min Reward on eval: 49.35537358204601[0m
[37m[1m[2023-07-10 17:43:45,605][227910] Mean Reward across all agents: 524.799056122529[0m
[37m[1m[2023-07-10 17:43:45,605][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:43:45,612][227910] mean_value=291.14451473252973, max_value=1021.2285800710684[0m
[37m[1m[2023-07-10 17:43:45,615][227910] New mean coefficients: [[ 1.1857208  2.64404   -0.6306871 -1.2565396  5.680802 ]][0m
[37m[1m[2023-07-10 17:43:45,616][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:43:55,271][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 17:43:55,272][227910] FPS: 397782.63[0m
[36m[2023-07-10 17:43:55,274][227910] itr=973, itrs=2000, Progress: 48.65%[0m
[36m[2023-07-10 17:44:06,787][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 17:44:06,788][227910] FPS: 334039.40[0m
[36m[2023-07-10 17:44:11,703][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:44:11,704][227910] Reward + Measures: [[373.438434     0.93181491   0.09745833   0.89912093   0.981489  ]][0m
[37m[1m[2023-07-10 17:44:11,704][227910] Max Reward on eval: 373.43843399565185[0m
[37m[1m[2023-07-10 17:44:11,704][227910] Min Reward on eval: 373.43843399565185[0m
[37m[1m[2023-07-10 17:44:11,704][227910] Mean Reward across all agents: 373.43843399565185[0m
[37m[1m[2023-07-10 17:44:11,705][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:44:17,156][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:44:17,157][227910] Reward + Measures: [[ 255.86053164    0.98659992    0.0114        0.98220009    0.99480003]
 [ 471.6331254     0.78929996    0.28799999    0.65210003    0.91569996]
 [  94.87452603    0.96120006    0.47989997    0.50800002    0.99169999]
 ...
 [-965.80845984    0.85120004    0.96919996    0.0006        0.98240006]
 [-827.54559114    0.91860002    0.89239997    0.0844        0.98730004]
 [-936.84531379    0.88659996    0.98579997    0.            0.99570006]][0m
[37m[1m[2023-07-10 17:44:17,157][227910] Max Reward on eval: 813.5089764828444[0m
[37m[1m[2023-07-10 17:44:17,157][227910] Min Reward on eval: -1450.4381288336358[0m
[37m[1m[2023-07-10 17:44:17,157][227910] Mean Reward across all agents: -16.97137661515046[0m
[37m[1m[2023-07-10 17:44:17,157][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:44:17,162][227910] mean_value=-58.016234944232004, max_value=959.4698687049432[0m
[37m[1m[2023-07-10 17:44:17,165][227910] New mean coefficients: [[ 0.47683126  2.3251128  -0.21958378 -1.4518478   5.650202  ]][0m
[37m[1m[2023-07-10 17:44:17,165][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:44:27,010][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 17:44:27,010][227910] FPS: 390134.56[0m
[36m[2023-07-10 17:44:27,012][227910] itr=974, itrs=2000, Progress: 48.70%[0m
[36m[2023-07-10 17:44:38,606][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 17:44:38,606][227910] FPS: 331769.85[0m
[36m[2023-07-10 17:44:43,401][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:44:43,402][227910] Reward + Measures: [[370.06483122   0.96741527   0.05002733   0.94557601   0.99316698]][0m
[37m[1m[2023-07-10 17:44:43,402][227910] Max Reward on eval: 370.06483121634795[0m
[37m[1m[2023-07-10 17:44:43,402][227910] Min Reward on eval: 370.06483121634795[0m
[37m[1m[2023-07-10 17:44:43,402][227910] Mean Reward across all agents: 370.06483121634795[0m
[37m[1m[2023-07-10 17:44:43,402][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:44:48,915][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:44:48,921][227910] Reward + Measures: [[ 79.20200759   0.98649997   0.0225       0.97589999   0.99610007]
 [646.87612344   0.6117       0.72039998   0.18010001   0.92480004]
 [304.58294359   0.66950005   0.78909999   0.0252       0.98290008]
 ...
 [308.14359751   0.7143001    0.82419997   0.           0.995     ]
 [516.25634375   0.41209999   0.59460002   0.3856       0.81440002]
 [537.15416982   0.39789999   0.64770001   0.34820002   0.85410005]][0m
[37m[1m[2023-07-10 17:44:48,921][227910] Max Reward on eval: 739.8548985924339[0m
[37m[1m[2023-07-10 17:44:48,921][227910] Min Reward on eval: -1047.011626957194[0m
[37m[1m[2023-07-10 17:44:48,921][227910] Mean Reward across all agents: 275.81653366533027[0m
[37m[1m[2023-07-10 17:44:48,922][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:44:48,928][227910] mean_value=93.49384185096164, max_value=967.3352393653208[0m
[37m[1m[2023-07-10 17:44:48,931][227910] New mean coefficients: [[ 0.10735387  1.5043457  -0.21972364 -1.7898428   5.709349  ]][0m
[37m[1m[2023-07-10 17:44:48,932][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:44:58,614][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 17:44:58,614][227910] FPS: 396689.12[0m
[36m[2023-07-10 17:44:58,616][227910] itr=975, itrs=2000, Progress: 48.75%[0m
[36m[2023-07-10 17:45:10,135][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 17:45:10,135][227910] FPS: 333906.11[0m
[36m[2023-07-10 17:45:14,934][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:45:14,940][227910] Reward + Measures: [[318.01204465   0.85127831   0.33796301   0.59192669   0.98466796]][0m
[37m[1m[2023-07-10 17:45:14,940][227910] Max Reward on eval: 318.01204464755[0m
[37m[1m[2023-07-10 17:45:14,940][227910] Min Reward on eval: 318.01204464755[0m
[37m[1m[2023-07-10 17:45:14,941][227910] Mean Reward across all agents: 318.01204464755[0m
[37m[1m[2023-07-10 17:45:14,941][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:45:20,365][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:45:20,371][227910] Reward + Measures: [[324.8711925    0.79810005   0.45620003   0.45730001   0.97670001]
 [236.18786139   0.76230001   0.48680001   0.37459999   0.9781    ]
 [218.39227829   0.71289998   0.80140001   0.0101       0.98540002]
 ...
 [155.75478768   0.73350006   0.77940005   0.0581       0.9896    ]
 [201.33961625   0.69840002   0.72839999   0.0701       0.98250008]
 [237.26048037   0.71849996   0.76270002   0.14140001   0.97200006]][0m
[37m[1m[2023-07-10 17:45:20,371][227910] Max Reward on eval: 573.3229435676592[0m
[37m[1m[2023-07-10 17:45:20,371][227910] Min Reward on eval: 131.3124599741539[0m
[37m[1m[2023-07-10 17:45:20,372][227910] Mean Reward across all agents: 283.60596392855587[0m
[37m[1m[2023-07-10 17:45:20,372][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:45:20,376][227910] mean_value=-14.570241476162616, max_value=993.6575652880921[0m
[37m[1m[2023-07-10 17:45:20,378][227910] New mean coefficients: [[ 0.21865883 -0.5477648   0.35789335 -2.312127    4.798268  ]][0m
[37m[1m[2023-07-10 17:45:20,379][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:45:30,105][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 17:45:30,105][227910] FPS: 394909.42[0m
[36m[2023-07-10 17:45:30,107][227910] itr=976, itrs=2000, Progress: 48.80%[0m
[36m[2023-07-10 17:45:41,676][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 17:45:41,676][227910] FPS: 332461.31[0m
[36m[2023-07-10 17:45:46,533][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:45:46,534][227910] Reward + Measures: [[146.28632955   0.72239965   0.80149037   0.01329133   0.9873473 ]][0m
[37m[1m[2023-07-10 17:45:46,534][227910] Max Reward on eval: 146.28632954677343[0m
[37m[1m[2023-07-10 17:45:46,534][227910] Min Reward on eval: 146.28632954677343[0m
[37m[1m[2023-07-10 17:45:46,534][227910] Mean Reward across all agents: 146.28632954677343[0m
[37m[1m[2023-07-10 17:45:46,534][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:45:52,096][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:45:52,097][227910] Reward + Measures: [[-448.81506781    0.95270008    0.65869999    0.31889999    0.91830009]
 [-517.22872128    0.91380006    0.95749998    0.            0.99539995]
 [-768.94311069    0.87180007    0.39789999    0.5406        0.84860003]
 ...
 [-420.73267197    0.91710007    0.85890001    0.1117        0.98730004]
 [ 328.57937947    0.9278        0.2024        0.75660002    0.98649997]
 [-718.66256802    0.97110003    0.82770008    0.15510002    0.95469999]][0m
[37m[1m[2023-07-10 17:45:52,097][227910] Max Reward on eval: 730.9582730152761[0m
[37m[1m[2023-07-10 17:45:52,097][227910] Min Reward on eval: -1585.6699710584478[0m
[37m[1m[2023-07-10 17:45:52,097][227910] Mean Reward across all agents: -43.68815694319187[0m
[37m[1m[2023-07-10 17:45:52,098][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:45:52,104][227910] mean_value=-118.6184652047874, max_value=991.1740490460501[0m
[37m[1m[2023-07-10 17:45:52,107][227910] New mean coefficients: [[-0.12503394 -0.53042734  0.8118552  -2.5077684   6.2391143 ]][0m
[37m[1m[2023-07-10 17:45:52,108][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:46:01,772][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 17:46:01,773][227910] FPS: 397386.91[0m
[36m[2023-07-10 17:46:01,775][227910] itr=977, itrs=2000, Progress: 48.85%[0m
[36m[2023-07-10 17:46:13,333][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 17:46:13,333][227910] FPS: 332747.81[0m
[36m[2023-07-10 17:46:18,024][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:46:18,029][227910] Reward + Measures: [[-152.53353278    0.54104197    0.91168666    0.000126      0.99162799]][0m
[37m[1m[2023-07-10 17:46:18,030][227910] Max Reward on eval: -152.53353277671084[0m
[37m[1m[2023-07-10 17:46:18,030][227910] Min Reward on eval: -152.53353277671084[0m
[37m[1m[2023-07-10 17:46:18,030][227910] Mean Reward across all agents: -152.53353277671084[0m
[37m[1m[2023-07-10 17:46:18,030][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:46:23,480][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:46:23,486][227910] Reward + Measures: [[ -43.43216036    0.51459998    0.93660003    0.0007        0.98380005]
 [-102.56076152    0.48919997    0.91799992    0.            0.98000002]
 [ -85.05560484    0.4901        0.90679997    0.            0.9806    ]
 ...
 [ -62.93278533    0.53789997    0.95220006    0.            0.98619998]
 [-190.29510486    0.50480002    0.92040008    0.            0.98820001]
 [-138.04147521    0.4822        0.89589995    0.0002        0.98929995]][0m
[37m[1m[2023-07-10 17:46:23,486][227910] Max Reward on eval: 39.17682268181816[0m
[37m[1m[2023-07-10 17:46:23,487][227910] Min Reward on eval: -256.11154261605697[0m
[37m[1m[2023-07-10 17:46:23,487][227910] Mean Reward across all agents: -123.9845981868857[0m
[37m[1m[2023-07-10 17:46:23,487][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:46:23,488][227910] mean_value=-315.36481844033415, max_value=-135.03838827628633[0m
[36m[2023-07-10 17:46:23,491][227910] XNES is restarting with a new solution whose measures are [0.91839999 0.63880002 0.90829992 0.92020005] and objective is 493.885251699714[0m
[36m[2023-07-10 17:46:23,492][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 17:46:23,494][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 17:46:23,495][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:46:33,083][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 17:46:33,083][227910] FPS: 400573.60[0m
[36m[2023-07-10 17:46:33,085][227910] itr=978, itrs=2000, Progress: 48.90%[0m
[36m[2023-07-10 17:46:44,673][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 17:46:44,673][227910] FPS: 331894.77[0m
[36m[2023-07-10 17:46:49,479][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:46:49,479][227910] Reward + Measures: [[515.91492936   0.88909227   0.09209234   0.88843763   0.90287369]][0m
[37m[1m[2023-07-10 17:46:49,480][227910] Max Reward on eval: 515.9149293635703[0m
[37m[1m[2023-07-10 17:46:49,480][227910] Min Reward on eval: 515.9149293635703[0m
[37m[1m[2023-07-10 17:46:49,480][227910] Mean Reward across all agents: 515.9149293635703[0m
[37m[1m[2023-07-10 17:46:49,480][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:46:54,848][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:46:54,854][227910] Reward + Measures: [[  149.74784015     0.86849993     0.1538         0.88029999
      0.87540007]
 [-1065.1378797      0.4936702      0.30901334     0.43659836
      0.33221424]
 [-1182.83508367     0.37635937     0.3363249      0.27285394
      0.32880786]
 ...
 [  -93.18958197     0.75389999     0.71780002     0.65030003
      0.85430002]
 [-1200.2199186      0.30899698     0.33313581     0.2143755
      0.37938935]
 [ -973.4527327      0.59780002     0.67140001     0.24630001
      0.6541    ]][0m
[37m[1m[2023-07-10 17:46:54,854][227910] Max Reward on eval: 599.7093115897617[0m
[37m[1m[2023-07-10 17:46:54,855][227910] Min Reward on eval: -1920.0849397521465[0m
[37m[1m[2023-07-10 17:46:54,855][227910] Mean Reward across all agents: -893.7337014091779[0m
[37m[1m[2023-07-10 17:46:54,855][227910] Average Trajectory Length: 956.111[0m
[36m[2023-07-10 17:46:54,857][227910] mean_value=-1454.1253147172927, max_value=963.9913153591698[0m
[37m[1m[2023-07-10 17:46:54,860][227910] New mean coefficients: [[ 0.46657902 -0.08494484  0.2840891  -0.37494695 -1.6996878 ]][0m
[37m[1m[2023-07-10 17:46:54,861][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:47:04,470][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 17:47:04,470][227910] FPS: 399679.59[0m
[36m[2023-07-10 17:47:04,473][227910] itr=979, itrs=2000, Progress: 48.95%[0m
[36m[2023-07-10 17:47:15,929][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 17:47:15,930][227910] FPS: 335747.90[0m
[36m[2023-07-10 17:47:20,669][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:47:20,675][227910] Reward + Measures: [[577.89376297   0.91203207   0.41885233   0.91877931   0.87026471]][0m
[37m[1m[2023-07-10 17:47:20,675][227910] Max Reward on eval: 577.8937629652212[0m
[37m[1m[2023-07-10 17:47:20,675][227910] Min Reward on eval: 577.8937629652212[0m
[37m[1m[2023-07-10 17:47:20,675][227910] Mean Reward across all agents: 577.8937629652212[0m
[37m[1m[2023-07-10 17:47:20,676][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:47:26,091][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:47:26,091][227910] Reward + Measures: [[ -400.68724265     0.62780005     0.71610004     0.0759
      0.70100003]
 [ -646.8486167      0.66479999     0.81970006     0.2289
      0.81890005]
 [ -553.22525978     0.33912179     0.20839553     0.31279826
      0.43249336]
 ...
 [ -703.28860761     0.19927026     0.3229973      0.2478864
      0.29362723]
 [-1094.8585128      0.08587898     0.14422846     0.14555405
      0.13499218]
 [  173.47500593     0.36740002     0.33720002     0.45550004
      0.4515    ]][0m
[37m[1m[2023-07-10 17:47:26,092][227910] Max Reward on eval: 769.3555276603554[0m
[37m[1m[2023-07-10 17:47:26,092][227910] Min Reward on eval: -1551.9436891585124[0m
[37m[1m[2023-07-10 17:47:26,092][227910] Mean Reward across all agents: -165.6460680302141[0m
[37m[1m[2023-07-10 17:47:26,092][227910] Average Trajectory Length: 945.6986666666667[0m
[36m[2023-07-10 17:47:26,097][227910] mean_value=-472.44958411900006, max_value=1269.3555276603554[0m
[37m[1m[2023-07-10 17:47:26,100][227910] New mean coefficients: [[ 0.78174376  0.0230876   1.0798783  -0.16039935 -1.2994195 ]][0m
[37m[1m[2023-07-10 17:47:26,101][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:47:35,688][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 17:47:35,689][227910] FPS: 400599.79[0m
[36m[2023-07-10 17:47:35,691][227910] itr=980, itrs=2000, Progress: 49.00%[0m
[37m[1m[2023-07-10 17:47:39,547][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000960[0m
[36m[2023-07-10 17:47:51,337][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 17:47:51,338][227910] FPS: 333092.96[0m
[36m[2023-07-10 17:47:56,124][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:47:56,130][227910] Reward + Measures: [[523.9416439    0.54630768   0.46111202   0.50994599   0.53281736]][0m
[37m[1m[2023-07-10 17:47:56,131][227910] Max Reward on eval: 523.941643897224[0m
[37m[1m[2023-07-10 17:47:56,132][227910] Min Reward on eval: 523.941643897224[0m
[37m[1m[2023-07-10 17:47:56,132][227910] Mean Reward across all agents: 523.941643897224[0m
[37m[1m[2023-07-10 17:47:56,133][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:48:01,633][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:48:01,639][227910] Reward + Measures: [[  -47.78605804     0.66730005     0.79530001     0.0475
      0.81169999]
 [ -275.3946072      0.29920003     0.57030004     0.36859998
      0.72299999]
 [   83.12357296     0.58579999     0.48950005     0.39340001
      0.75370008]
 ...
 [-1232.14460018     0.19822435     0.17462702     0.17192972
      0.15992793]
 [  488.51911007     0.33580002     0.4104         0.2791
      0.49770004]
 [  362.91839922     0.58780003     0.3705         0.52159995
      0.685     ]][0m
[37m[1m[2023-07-10 17:48:01,639][227910] Max Reward on eval: 609.9241821086849[0m
[37m[1m[2023-07-10 17:48:01,639][227910] Min Reward on eval: -1554.4803039551364[0m
[37m[1m[2023-07-10 17:48:01,640][227910] Mean Reward across all agents: -69.02000040677859[0m
[37m[1m[2023-07-10 17:48:01,640][227910] Average Trajectory Length: 972.5703333333333[0m
[36m[2023-07-10 17:48:01,645][227910] mean_value=-439.18305540134725, max_value=991.4091042977573[0m
[37m[1m[2023-07-10 17:48:01,648][227910] New mean coefficients: [[ 1.0552554   0.5243305   0.66326845 -0.12499921 -1.1861256 ]][0m
[37m[1m[2023-07-10 17:48:01,649][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:48:11,412][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 17:48:11,412][227910] FPS: 393396.65[0m
[36m[2023-07-10 17:48:11,414][227910] itr=981, itrs=2000, Progress: 49.05%[0m
[36m[2023-07-10 17:48:22,883][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 17:48:22,884][227910] FPS: 335387.55[0m
[36m[2023-07-10 17:48:27,774][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:48:27,774][227910] Reward + Measures: [[586.52549919   0.55527169   0.48453364   0.53163165   0.53275234]][0m
[37m[1m[2023-07-10 17:48:27,775][227910] Max Reward on eval: 586.5254991914686[0m
[37m[1m[2023-07-10 17:48:27,775][227910] Min Reward on eval: 586.5254991914686[0m
[37m[1m[2023-07-10 17:48:27,775][227910] Mean Reward across all agents: 586.5254991914686[0m
[37m[1m[2023-07-10 17:48:27,775][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 17:48:33,221][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:48:33,221][227910] Reward + Measures: [[-346.69544594    0.64219999    0.57240003    0.22119999    0.78200001]
 [  78.14601955    0.59390002    0.64099997    0.16919999    0.7335    ]
 [-258.84901179    0.46309996    0.57480001    0.51889998    0.82950002]
 ...
 [ 382.67439294    0.37310001    0.66340005    0.3132        0.76380002]
 [-384.39327808    0.43649998    0.55760002    0.0949        0.58759999]
 [-463.39966161    0.4490985     0.33426315    0.29156747    0.34914464]][0m
[37m[1m[2023-07-10 17:48:33,222][227910] Max Reward on eval: 732.9834004373639[0m
[37m[1m[2023-07-10 17:48:33,222][227910] Min Reward on eval: -1433.327403288713[0m
[37m[1m[2023-07-10 17:48:33,222][227910] Mean Reward across all agents: -65.90781970536962[0m
[37m[1m[2023-07-10 17:48:33,222][227910] Average Trajectory Length: 971.6436666666666[0m
[36m[2023-07-10 17:48:33,227][227910] mean_value=-649.0913548260464, max_value=1232.9834004373638[0m
[37m[1m[2023-07-10 17:48:33,230][227910] New mean coefficients: [[ 1.5261669   0.2758789   0.781774   -0.18075049 -1.2163347 ]][0m
[37m[1m[2023-07-10 17:48:33,231][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:48:42,923][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 17:48:42,923][227910] FPS: 396267.50[0m
[36m[2023-07-10 17:48:42,925][227910] itr=982, itrs=2000, Progress: 49.10%[0m
[36m[2023-07-10 17:48:54,553][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 17:48:54,553][227910] FPS: 330822.93[0m
[36m[2023-07-10 17:48:59,398][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:48:59,398][227910] Reward + Measures: [[655.89883089   0.51822966   0.48669344   0.51206088   0.51115251]][0m
[37m[1m[2023-07-10 17:48:59,398][227910] Max Reward on eval: 655.8988308946894[0m
[37m[1m[2023-07-10 17:48:59,399][227910] Min Reward on eval: 655.8988308946894[0m
[37m[1m[2023-07-10 17:48:59,399][227910] Mean Reward across all agents: 655.8988308946894[0m
[37m[1m[2023-07-10 17:48:59,399][227910] Average Trajectory Length: 999.961[0m
[36m[2023-07-10 17:49:04,862][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:49:04,862][227910] Reward + Measures: [[ 196.1607963     0.3576        0.69010001    0.25170001    0.67359996]
 [-725.6750478     0.5214985     0.38627756    0.50464088    0.37288278]
 [-314.06096547    0.62940001    0.71780008    0.1506        0.71530002]
 ...
 [ 534.02740021    0.35880002    0.55669999    0.29710004    0.58359998]
 [-993.45049006    0.59380001    0.78949994    0.1867        0.74290001]
 [ -32.89465349    0.63169998    0.71719998    0.50209999    0.8071    ]][0m
[37m[1m[2023-07-10 17:49:04,863][227910] Max Reward on eval: 627.5858902106178[0m
[37m[1m[2023-07-10 17:49:04,863][227910] Min Reward on eval: -1757.1272448084317[0m
[37m[1m[2023-07-10 17:49:04,863][227910] Mean Reward across all agents: -269.12773069668117[0m
[37m[1m[2023-07-10 17:49:04,863][227910] Average Trajectory Length: 966.882[0m
[36m[2023-07-10 17:49:04,867][227910] mean_value=-766.122532692466, max_value=960.3653099856447[0m
[37m[1m[2023-07-10 17:49:04,870][227910] New mean coefficients: [[ 1.7784815  -0.4294414   0.40848437 -0.12331811 -1.1727229 ]][0m
[37m[1m[2023-07-10 17:49:04,870][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:49:14,702][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 17:49:14,702][227910] FPS: 390665.06[0m
[36m[2023-07-10 17:49:14,704][227910] itr=983, itrs=2000, Progress: 49.15%[0m
[36m[2023-07-10 17:49:26,134][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 17:49:26,135][227910] FPS: 336576.98[0m
[36m[2023-07-10 17:49:30,944][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:49:30,945][227910] Reward + Measures: [[762.6148559    0.37423709   0.40094942   0.38070476   0.45289746]][0m
[37m[1m[2023-07-10 17:49:30,945][227910] Max Reward on eval: 762.6148559012596[0m
[37m[1m[2023-07-10 17:49:30,945][227910] Min Reward on eval: 762.6148559012596[0m
[37m[1m[2023-07-10 17:49:30,946][227910] Mean Reward across all agents: 762.6148559012596[0m
[37m[1m[2023-07-10 17:49:30,946][227910] Average Trajectory Length: 999.7036666666667[0m
[36m[2023-07-10 17:49:36,599][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:49:36,605][227910] Reward + Measures: [[ -167.28475966     0.44330001     0.41749999     0.0909
      0.58100003]
 [  325.58482987     0.68550009     0.67119998     0.32890001
      0.76990002]
 [-1046.20271999     0.39914408     0.49068305     0.04916441
      0.71518648]
 ...
 [  133.92011764     0.49680001     0.58260006     0.71670002
      0.53479999]
 [ -588.49207612     0.31916994     0.21488981     0.21068135
      0.3946023 ]
 [   92.06743818     0.39739999     0.54910004     0.5011
      0.55080003]][0m
[37m[1m[2023-07-10 17:49:36,605][227910] Max Reward on eval: 917.9751975279127[0m
[37m[1m[2023-07-10 17:49:36,605][227910] Min Reward on eval: -1466.7851480652462[0m
[37m[1m[2023-07-10 17:49:36,606][227910] Mean Reward across all agents: 39.58411692964969[0m
[37m[1m[2023-07-10 17:49:36,606][227910] Average Trajectory Length: 983.2326666666667[0m
[36m[2023-07-10 17:49:36,612][227910] mean_value=-30.69943113535575, max_value=1059.5163091742995[0m
[37m[1m[2023-07-10 17:49:36,615][227910] New mean coefficients: [[ 1.7370217  -0.39614078  0.15675941  0.4560338  -1.087532  ]][0m
[37m[1m[2023-07-10 17:49:36,616][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:49:46,582][227910] train() took 9.96 seconds to complete[0m
[36m[2023-07-10 17:49:46,583][227910] FPS: 385359.47[0m
[36m[2023-07-10 17:49:46,585][227910] itr=984, itrs=2000, Progress: 49.20%[0m
[36m[2023-07-10 17:49:58,208][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 17:49:58,208][227910] FPS: 330999.59[0m
[36m[2023-07-10 17:50:02,963][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:50:02,963][227910] Reward + Measures: [[403.07560318   0.3473044    0.45533928   0.4082391    0.55120242]][0m
[37m[1m[2023-07-10 17:50:02,964][227910] Max Reward on eval: 403.0756031757894[0m
[37m[1m[2023-07-10 17:50:02,964][227910] Min Reward on eval: 403.0756031757894[0m
[37m[1m[2023-07-10 17:50:02,964][227910] Mean Reward across all agents: 403.0756031757894[0m
[37m[1m[2023-07-10 17:50:02,964][227910] Average Trajectory Length: 999.9263333333333[0m
[36m[2023-07-10 17:50:08,476][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:50:08,477][227910] Reward + Measures: [[ -70.35506403    0.32679999    0.1718        0.45250002    0.35330003]
 [ 115.86499932    0.29729998    0.5449        0.24650002    0.71780002]
 [  63.1358669     0.39309999    0.43350002    0.25649998    0.69420004]
 ...
 [ 189.14691134    0.22930001    0.37460002    0.27970001    0.53890002]
 [-480.32578516    0.34331495    0.39221415    0.23934725    0.60926771]
 [  -7.64012704    0.41050002    0.53540003    0.42279997    0.65470004]][0m
[37m[1m[2023-07-10 17:50:08,477][227910] Max Reward on eval: 687.9632937737751[0m
[37m[1m[2023-07-10 17:50:08,477][227910] Min Reward on eval: -1346.578401003196[0m
[37m[1m[2023-07-10 17:50:08,478][227910] Mean Reward across all agents: 23.379365379349334[0m
[37m[1m[2023-07-10 17:50:08,478][227910] Average Trajectory Length: 996.476[0m
[36m[2023-07-10 17:50:08,483][227910] mean_value=-218.6075349550342, max_value=778.4303971651605[0m
[37m[1m[2023-07-10 17:50:08,485][227910] New mean coefficients: [[ 2.2001967   0.2394462   0.22342095 -0.62921655 -0.8628956 ]][0m
[37m[1m[2023-07-10 17:50:08,486][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:50:18,232][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 17:50:18,233][227910] FPS: 394077.90[0m
[36m[2023-07-10 17:50:18,235][227910] itr=985, itrs=2000, Progress: 49.25%[0m
[36m[2023-07-10 17:50:29,791][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 17:50:29,791][227910] FPS: 332812.97[0m
[36m[2023-07-10 17:50:34,718][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:50:34,719][227910] Reward + Measures: [[532.60966907   0.33068904   0.37662703   0.3593975    0.46397299]][0m
[37m[1m[2023-07-10 17:50:34,719][227910] Max Reward on eval: 532.6096690718409[0m
[37m[1m[2023-07-10 17:50:34,719][227910] Min Reward on eval: 532.6096690718409[0m
[37m[1m[2023-07-10 17:50:34,719][227910] Mean Reward across all agents: 532.6096690718409[0m
[37m[1m[2023-07-10 17:50:34,719][227910] Average Trajectory Length: 999.7473333333332[0m
[36m[2023-07-10 17:50:40,178][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:50:40,178][227910] Reward + Measures: [[ 286.52346263    0.36050001    0.44870004    0.1736        0.40620002]
 [ 135.38662738    0.30220002    0.45910001    0.30239999    0.57060003]
 [-209.92597558    0.35285714    0.30027142    0.24466734    0.29107141]
 ...
 [ 227.03833741    0.39000002    0.37250003    0.31500003    0.50299996]
 [-639.19306318    0.22633334    0.27573332    0.16963334    0.25873333]
 [  48.41510789    0.28930002    0.5025        0.2445        0.50310004]][0m
[37m[1m[2023-07-10 17:50:40,178][227910] Max Reward on eval: 642.928563167178[0m
[37m[1m[2023-07-10 17:50:40,179][227910] Min Reward on eval: -944.9264192225994[0m
[37m[1m[2023-07-10 17:50:40,179][227910] Mean Reward across all agents: -38.246195920295925[0m
[37m[1m[2023-07-10 17:50:40,179][227910] Average Trajectory Length: 970.3156666666666[0m
[36m[2023-07-10 17:50:40,183][227910] mean_value=-460.9137025333488, max_value=665.4373425521947[0m
[37m[1m[2023-07-10 17:50:40,185][227910] New mean coefficients: [[ 1.9650218   0.19418731 -0.08690742 -0.92649305 -0.05055791]][0m
[37m[1m[2023-07-10 17:50:40,186][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:50:49,696][227910] train() took 9.51 seconds to complete[0m
[36m[2023-07-10 17:50:49,696][227910] FPS: 403857.14[0m
[36m[2023-07-10 17:50:49,699][227910] itr=986, itrs=2000, Progress: 49.30%[0m
[36m[2023-07-10 17:51:01,151][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 17:51:01,152][227910] FPS: 335912.32[0m
[36m[2023-07-10 17:51:05,831][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:51:05,832][227910] Reward + Measures: [[625.6296392    0.31127149   0.33462149   0.32350057   0.40743405]][0m
[37m[1m[2023-07-10 17:51:05,832][227910] Max Reward on eval: 625.6296392027897[0m
[37m[1m[2023-07-10 17:51:05,832][227910] Min Reward on eval: 625.6296392027897[0m
[37m[1m[2023-07-10 17:51:05,832][227910] Mean Reward across all agents: 625.6296392027897[0m
[37m[1m[2023-07-10 17:51:05,833][227910] Average Trajectory Length: 999.5803333333333[0m
[36m[2023-07-10 17:51:11,246][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:51:11,246][227910] Reward + Measures: [[ 476.9219088     0.33790001    0.26620004    0.46520001    0.4729    ]
 [-153.02672942    0.28870001    0.42389998    0.17709999    0.6728    ]
 [ 555.84872553    0.33320004    0.44499999    0.32220003    0.49989995]
 ...
 [  93.73866467    0.35490003    0.31570002    0.1567        0.48690006]
 [ 194.25529834    0.22500001    0.6196        0.2053        0.77400005]
 [ -52.94004306    0.2289        0.39430001    0.17950001    0.56439996]][0m
[37m[1m[2023-07-10 17:51:11,246][227910] Max Reward on eval: 604.9063402681379[0m
[37m[1m[2023-07-10 17:51:11,247][227910] Min Reward on eval: -845.1645723197842[0m
[37m[1m[2023-07-10 17:51:11,247][227910] Mean Reward across all agents: 53.23260705723055[0m
[37m[1m[2023-07-10 17:51:11,247][227910] Average Trajectory Length: 992.8316666666666[0m
[36m[2023-07-10 17:51:11,253][227910] mean_value=-52.059749692071755, max_value=925.8160550288792[0m
[37m[1m[2023-07-10 17:51:11,256][227910] New mean coefficients: [[ 2.0831993   0.17665863  0.29017004 -1.8419071  -0.581675  ]][0m
[37m[1m[2023-07-10 17:51:11,257][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:51:20,934][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 17:51:20,934][227910] FPS: 396878.22[0m
[36m[2023-07-10 17:51:20,937][227910] itr=987, itrs=2000, Progress: 49.35%[0m
[36m[2023-07-10 17:51:32,529][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 17:51:32,530][227910] FPS: 331849.10[0m
[36m[2023-07-10 17:51:37,370][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:51:37,371][227910] Reward + Measures: [[726.46777426   0.29869044   0.34245533   0.28842825   0.38869929]][0m
[37m[1m[2023-07-10 17:51:37,371][227910] Max Reward on eval: 726.4677742582298[0m
[37m[1m[2023-07-10 17:51:37,371][227910] Min Reward on eval: 726.4677742582298[0m
[37m[1m[2023-07-10 17:51:37,372][227910] Mean Reward across all agents: 726.4677742582298[0m
[37m[1m[2023-07-10 17:51:37,372][227910] Average Trajectory Length: 999.1366666666667[0m
[36m[2023-07-10 17:51:42,870][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:51:42,876][227910] Reward + Measures: [[-290.38216747    0.36322218    0.30973032    0.18599544    0.49549124]
 [-183.14753968    0.38030002    0.39300001    0.19949999    0.54039997]
 [   9.50822685    0.29377237    0.29349148    0.19801275    0.36332768]
 ...
 [-187.21646405    0.49090004    0.66580003    0.25890002    0.83039999]
 [-905.47989413    0.3122246     0.40835276    0.30348137    0.57040352]
 [-711.68056209    0.83070004    0.12530001    0.77950007    0.65560001]][0m
[37m[1m[2023-07-10 17:51:42,876][227910] Max Reward on eval: 775.4396393114235[0m
[37m[1m[2023-07-10 17:51:42,877][227910] Min Reward on eval: -1317.4408308255836[0m
[37m[1m[2023-07-10 17:51:42,877][227910] Mean Reward across all agents: -65.28566692865293[0m
[37m[1m[2023-07-10 17:51:42,877][227910] Average Trajectory Length: 929.4983333333333[0m
[36m[2023-07-10 17:51:42,880][227910] mean_value=-602.3143038120884, max_value=685.9059744580416[0m
[37m[1m[2023-07-10 17:51:42,883][227910] New mean coefficients: [[ 2.017561   -0.12754425  0.72342354 -0.739177    0.37114233]][0m
[37m[1m[2023-07-10 17:51:42,884][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:51:52,725][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 17:51:52,726][227910] FPS: 390237.41[0m
[36m[2023-07-10 17:51:52,728][227910] itr=988, itrs=2000, Progress: 49.40%[0m
[36m[2023-07-10 17:52:04,282][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 17:52:04,283][227910] FPS: 332964.07[0m
[36m[2023-07-10 17:52:09,058][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:52:09,058][227910] Reward + Measures: [[812.03767268   0.29380497   0.34022614   0.28685591   0.3755855 ]][0m
[37m[1m[2023-07-10 17:52:09,058][227910] Max Reward on eval: 812.0376726834284[0m
[37m[1m[2023-07-10 17:52:09,059][227910] Min Reward on eval: 812.0376726834284[0m
[37m[1m[2023-07-10 17:52:09,059][227910] Mean Reward across all agents: 812.0376726834284[0m
[37m[1m[2023-07-10 17:52:09,059][227910] Average Trajectory Length: 998.9956666666666[0m
[36m[2023-07-10 17:52:14,696][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:52:14,697][227910] Reward + Measures: [[ 155.8744518     0.41802183    0.35544074    0.26270762    0.57388145]
 [ -33.51682769    0.32570001    0.47890002    0.26009998    0.58199996]
 [ 210.91619978    0.40370002    0.47639999    0.17129999    0.5697    ]
 ...
 [-619.2050305     0.29948553    0.29773059    0.17194177    0.42464557]
 [-259.67023112    0.42449999    0.41160002    0.38589999    0.59920001]
 [ 199.06502924    0.23000002    0.37540004    0.37200004    0.35900003]][0m
[37m[1m[2023-07-10 17:52:14,697][227910] Max Reward on eval: 782.9996238987427[0m
[37m[1m[2023-07-10 17:52:14,697][227910] Min Reward on eval: -1050.8474652311065[0m
[37m[1m[2023-07-10 17:52:14,698][227910] Mean Reward across all agents: -14.273160592965175[0m
[37m[1m[2023-07-10 17:52:14,698][227910] Average Trajectory Length: 958.9456666666666[0m
[36m[2023-07-10 17:52:14,702][227910] mean_value=-544.3415177789374, max_value=566.0221472882816[0m
[37m[1m[2023-07-10 17:52:14,704][227910] New mean coefficients: [[ 2.08865    -0.40147126 -0.10191697 -0.71235055  0.66006315]][0m
[37m[1m[2023-07-10 17:52:14,705][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:52:24,552][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 17:52:24,553][227910] FPS: 390027.98[0m
[36m[2023-07-10 17:52:24,555][227910] itr=989, itrs=2000, Progress: 49.45%[0m
[36m[2023-07-10 17:52:36,010][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 17:52:36,010][227910] FPS: 335789.87[0m
[36m[2023-07-10 17:52:40,741][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:52:40,742][227910] Reward + Measures: [[914.81735551   0.28591472   0.34178418   0.28064907   0.36143637]][0m
[37m[1m[2023-07-10 17:52:40,742][227910] Max Reward on eval: 914.817355513594[0m
[37m[1m[2023-07-10 17:52:40,742][227910] Min Reward on eval: 914.817355513594[0m
[37m[1m[2023-07-10 17:52:40,742][227910] Mean Reward across all agents: 914.817355513594[0m
[37m[1m[2023-07-10 17:52:40,743][227910] Average Trajectory Length: 997.9486666666667[0m
[36m[2023-07-10 17:52:46,255][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:52:46,255][227910] Reward + Measures: [[661.13437759   0.27340001   0.49379998   0.34150001   0.50620002]
 [434.37336257   0.28740001   0.62840003   0.2264       0.73019999]
 [136.24797893   0.22630003   0.72609997   0.1767       0.78210002]
 ...
 [332.0956988    0.33699998   0.29539999   0.34319997   0.46309996]
 [333.61642017   0.20489998   0.54329997   0.27919999   0.52570003]
 [867.40343309   0.27130002   0.4226       0.29700002   0.44549999]][0m
[37m[1m[2023-07-10 17:52:46,256][227910] Max Reward on eval: 934.0381966908229[0m
[37m[1m[2023-07-10 17:52:46,256][227910] Min Reward on eval: -483.82436906654857[0m
[37m[1m[2023-07-10 17:52:46,256][227910] Mean Reward across all agents: 433.6657567760804[0m
[37m[1m[2023-07-10 17:52:46,256][227910] Average Trajectory Length: 998.8843333333333[0m
[36m[2023-07-10 17:52:46,262][227910] mean_value=20.32470436409778, max_value=937.4146272940619[0m
[37m[1m[2023-07-10 17:52:46,265][227910] New mean coefficients: [[ 2.8636765   0.8867123  -0.19691676 -0.39592955  0.55850595]][0m
[37m[1m[2023-07-10 17:52:46,266][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:52:56,051][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 17:52:56,051][227910] FPS: 392485.07[0m
[36m[2023-07-10 17:52:56,054][227910] itr=990, itrs=2000, Progress: 49.50%[0m
[37m[1m[2023-07-10 17:52:59,723][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000970[0m
[36m[2023-07-10 17:53:11,455][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 17:53:11,455][227910] FPS: 335146.68[0m
[36m[2023-07-10 17:53:16,326][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:53:16,326][227910] Reward + Measures: [[1023.32946451    0.28230563    0.33573601    0.29056263    0.35049197]][0m
[37m[1m[2023-07-10 17:53:16,326][227910] Max Reward on eval: 1023.3294645118583[0m
[37m[1m[2023-07-10 17:53:16,327][227910] Min Reward on eval: 1023.3294645118583[0m
[37m[1m[2023-07-10 17:53:16,327][227910] Mean Reward across all agents: 1023.3294645118583[0m
[37m[1m[2023-07-10 17:53:16,327][227910] Average Trajectory Length: 996.1973333333333[0m
[36m[2023-07-10 17:53:21,845][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:53:21,851][227910] Reward + Measures: [[ 155.8394822     0.38920003    0.40619999    0.5266        0.42770001]
 [-111.7147794     0.31826273    0.36684659    0.21718045    0.4949306 ]
 [1089.12242342    0.2383        0.33330002    0.36940002    0.3283    ]
 ...
 [-111.06014381    0.19680001    0.57960004    0.21329999    0.67570007]
 [-322.4711021     0.45890003    0.78179997    0.13690001    0.79570001]
 [ 171.54780832    0.28067514    0.40694186    0.34571168    0.41131359]][0m
[37m[1m[2023-07-10 17:53:21,852][227910] Max Reward on eval: 1089.1224234181223[0m
[37m[1m[2023-07-10 17:53:21,853][227910] Min Reward on eval: -1235.0300681419903[0m
[37m[1m[2023-07-10 17:53:21,854][227910] Mean Reward across all agents: 57.260458480312856[0m
[37m[1m[2023-07-10 17:53:21,855][227910] Average Trajectory Length: 991.3696666666666[0m
[36m[2023-07-10 17:53:21,863][227910] mean_value=-431.2399553107847, max_value=1282.831729403406[0m
[37m[1m[2023-07-10 17:53:21,867][227910] New mean coefficients: [[ 2.468076    0.31310678 -0.90098673 -0.5310176   0.7558718 ]][0m
[37m[1m[2023-07-10 17:53:21,869][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:53:31,754][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 17:53:31,754][227910] FPS: 388559.09[0m
[36m[2023-07-10 17:53:31,756][227910] itr=991, itrs=2000, Progress: 49.55%[0m
[36m[2023-07-10 17:53:43,454][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 17:53:43,454][227910] FPS: 328790.48[0m
[36m[2023-07-10 17:53:48,342][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:53:48,342][227910] Reward + Measures: [[1132.64879057    0.27505761    0.32793748    0.29456949    0.33789924]][0m
[37m[1m[2023-07-10 17:53:48,342][227910] Max Reward on eval: 1132.6487905709962[0m
[37m[1m[2023-07-10 17:53:48,342][227910] Min Reward on eval: 1132.6487905709962[0m
[37m[1m[2023-07-10 17:53:48,343][227910] Mean Reward across all agents: 1132.6487905709962[0m
[37m[1m[2023-07-10 17:53:48,343][227910] Average Trajectory Length: 992.703[0m
[36m[2023-07-10 17:53:53,920][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:53:53,921][227910] Reward + Measures: [[   6.38046395    0.63980001    0.6304        0.2868        0.73640007]
 [-374.70760831    0.2748        0.7044        0.17230001    0.73879999]
 [ 612.08952734    0.38519999    0.71209997    0.18849999    0.83249998]
 ...
 [ 298.00517436    0.28480002    0.3608        0.29550001    0.5729    ]
 [  15.57192537    0.29350001    0.55269998    0.26300001    0.54100007]
 [ 614.13863919    0.32980001    0.38470003    0.2757        0.50920004]][0m
[37m[1m[2023-07-10 17:53:53,921][227910] Max Reward on eval: 1023.6563048681128[0m
[37m[1m[2023-07-10 17:53:53,922][227910] Min Reward on eval: -1209.430631893559[0m
[37m[1m[2023-07-10 17:53:53,922][227910] Mean Reward across all agents: 108.6161182893177[0m
[37m[1m[2023-07-10 17:53:53,922][227910] Average Trajectory Length: 993.8639999999999[0m
[36m[2023-07-10 17:53:53,926][227910] mean_value=-400.2776795384782, max_value=947.102327830462[0m
[37m[1m[2023-07-10 17:53:53,929][227910] New mean coefficients: [[ 2.4788086   0.55905455 -0.13960421 -0.43737423  0.9608096 ]][0m
[37m[1m[2023-07-10 17:53:53,930][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:54:03,714][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 17:54:03,714][227910] FPS: 392553.14[0m
[36m[2023-07-10 17:54:03,716][227910] itr=992, itrs=2000, Progress: 49.60%[0m
[36m[2023-07-10 17:54:15,236][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 17:54:15,237][227910] FPS: 333949.71[0m
[36m[2023-07-10 17:54:20,044][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:54:20,045][227910] Reward + Measures: [[1225.82647791    0.27187544    0.33036506    0.28784359    0.32970789]][0m
[37m[1m[2023-07-10 17:54:20,045][227910] Max Reward on eval: 1225.8264779077986[0m
[37m[1m[2023-07-10 17:54:20,045][227910] Min Reward on eval: 1225.8264779077986[0m
[37m[1m[2023-07-10 17:54:20,046][227910] Mean Reward across all agents: 1225.8264779077986[0m
[37m[1m[2023-07-10 17:54:20,046][227910] Average Trajectory Length: 992.1763333333333[0m
[36m[2023-07-10 17:54:25,605][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:54:25,606][227910] Reward + Measures: [[ 396.91503899    0.25430003    0.43669996    0.26710001    0.5474    ]
 [ 248.445266      0.26890001    0.30240002    0.32269999    0.3899    ]
 [ 899.8513005     0.26029083    0.34693217    0.32198966    0.35258505]
 ...
 [ -22.92793249    0.47980005    0.57010001    0.2086        0.72340006]
 [-346.98097185    0.31329998    0.66050005    0.23169999    0.86269999]
 [-640.13268877    0.24389999    0.43150002    0.4395        0.58660001]][0m
[37m[1m[2023-07-10 17:54:25,606][227910] Max Reward on eval: 1217.5073268184672[0m
[37m[1m[2023-07-10 17:54:25,606][227910] Min Reward on eval: -719.531525662064[0m
[37m[1m[2023-07-10 17:54:25,606][227910] Mean Reward across all agents: 258.07418020704597[0m
[37m[1m[2023-07-10 17:54:25,607][227910] Average Trajectory Length: 984.1253333333333[0m
[36m[2023-07-10 17:54:25,611][227910] mean_value=-164.12919062633793, max_value=646.6478596821028[0m
[37m[1m[2023-07-10 17:54:25,614][227910] New mean coefficients: [[2.6011755  0.42338297 0.0696529  0.17005765 1.4158573 ]][0m
[37m[1m[2023-07-10 17:54:25,615][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:54:35,310][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 17:54:35,310][227910] FPS: 396151.82[0m
[36m[2023-07-10 17:54:35,312][227910] itr=993, itrs=2000, Progress: 49.65%[0m
[36m[2023-07-10 17:54:47,007][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 17:54:47,007][227910] FPS: 328875.01[0m
[36m[2023-07-10 17:54:51,764][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:54:51,765][227910] Reward + Measures: [[1317.79830608    0.26769343    0.3345232     0.29286391    0.31950492]][0m
[37m[1m[2023-07-10 17:54:51,765][227910] Max Reward on eval: 1317.7983060836393[0m
[37m[1m[2023-07-10 17:54:51,765][227910] Min Reward on eval: 1317.7983060836393[0m
[37m[1m[2023-07-10 17:54:51,765][227910] Mean Reward across all agents: 1317.7983060836393[0m
[37m[1m[2023-07-10 17:54:51,766][227910] Average Trajectory Length: 985.5776666666667[0m
[36m[2023-07-10 17:54:57,252][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:54:57,253][227910] Reward + Measures: [[ -188.93784872     0.11739486     0.25993848     0.28701794
      0.41105387]
 [  268.32038601     0.60350001     0.44169998     0.59050006
      0.70960009]
 [  -76.80927458     0.85080004     0.53350008     0.38280001
      0.89309996]
 ...
 [-1107.97313018     0.2307         0.76370001     0.69850004
      0.87160009]
 [ -870.06942577     0.58050001     0.55739993     0.74400008
      0.84649992]
 [  535.78877459     0.3238         0.3389         0.51200002
      0.29800001]][0m
[37m[1m[2023-07-10 17:54:57,253][227910] Max Reward on eval: 1117.1599864152377[0m
[37m[1m[2023-07-10 17:54:57,253][227910] Min Reward on eval: -1603.5357324582524[0m
[37m[1m[2023-07-10 17:54:57,253][227910] Mean Reward across all agents: 51.35413567621576[0m
[37m[1m[2023-07-10 17:54:57,254][227910] Average Trajectory Length: 983.323[0m
[36m[2023-07-10 17:54:57,258][227910] mean_value=-353.6292281851156, max_value=1036.8468447211617[0m
[37m[1m[2023-07-10 17:54:57,261][227910] New mean coefficients: [[ 2.5162427   0.25209624 -0.5487117   0.25426745  2.6356027 ]][0m
[37m[1m[2023-07-10 17:54:57,262][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:55:06,813][227910] train() took 9.55 seconds to complete[0m
[36m[2023-07-10 17:55:06,813][227910] FPS: 402155.20[0m
[36m[2023-07-10 17:55:06,815][227910] itr=994, itrs=2000, Progress: 49.70%[0m
[36m[2023-07-10 17:55:18,390][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 17:55:18,390][227910] FPS: 332308.91[0m
[36m[2023-07-10 17:55:23,245][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:55:23,246][227910] Reward + Measures: [[1411.07481285    0.26220366    0.33573768    0.29814231    0.30940896]][0m
[37m[1m[2023-07-10 17:55:23,246][227910] Max Reward on eval: 1411.074812852635[0m
[37m[1m[2023-07-10 17:55:23,246][227910] Min Reward on eval: 1411.074812852635[0m
[37m[1m[2023-07-10 17:55:23,247][227910] Mean Reward across all agents: 1411.074812852635[0m
[37m[1m[2023-07-10 17:55:23,247][227910] Average Trajectory Length: 988.6783333333333[0m
[36m[2023-07-10 17:55:28,778][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:55:28,779][227910] Reward + Measures: [[-207.2506422     0.292137      0.25120685    0.19674794    0.3638356 ]
 [ -77.43807027    0.47389999    0.43829998    0.19930001    0.5535    ]
 [  43.59121526    0.34909999    0.39339998    0.34510002    0.48850003]
 ...
 [1229.32920921    0.30740002    0.40019998    0.32789999    0.38549998]
 [-155.42850336    0.77590001    0.23280001    0.72130007    0.68750012]
 [ 151.26847282    0.46720001    0.4786        0.45480004    0.574     ]][0m
[37m[1m[2023-07-10 17:55:28,779][227910] Max Reward on eval: 1399.3375319360523[0m
[37m[1m[2023-07-10 17:55:28,779][227910] Min Reward on eval: -1461.3201997002936[0m
[37m[1m[2023-07-10 17:55:28,780][227910] Mean Reward across all agents: 257.3374592514202[0m
[37m[1m[2023-07-10 17:55:28,780][227910] Average Trajectory Length: 986.6366666666667[0m
[36m[2023-07-10 17:55:28,785][227910] mean_value=-233.8137919286477, max_value=1686.200348547881[0m
[37m[1m[2023-07-10 17:55:28,788][227910] New mean coefficients: [[ 3.4208572  -0.47426218 -0.86320615 -0.460397    2.6268456 ]][0m
[37m[1m[2023-07-10 17:55:28,789][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:55:38,519][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 17:55:38,519][227910] FPS: 394707.17[0m
[36m[2023-07-10 17:55:38,522][227910] itr=995, itrs=2000, Progress: 49.75%[0m
[36m[2023-07-10 17:55:49,972][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 17:55:49,972][227910] FPS: 335902.03[0m
[36m[2023-07-10 17:55:54,837][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:55:54,837][227910] Reward + Measures: [[1489.61303681    0.25998709    0.3365342     0.28656098    0.30210656]][0m
[37m[1m[2023-07-10 17:55:54,837][227910] Max Reward on eval: 1489.6130368113238[0m
[37m[1m[2023-07-10 17:55:54,837][227910] Min Reward on eval: 1489.6130368113238[0m
[37m[1m[2023-07-10 17:55:54,837][227910] Mean Reward across all agents: 1489.6130368113238[0m
[37m[1m[2023-07-10 17:55:54,838][227910] Average Trajectory Length: 983.1909999999999[0m
[36m[2023-07-10 17:56:00,485][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:56:00,485][227910] Reward + Measures: [[  76.59616165    0.29840001    0.31810001    0.3283        0.4138    ]
 [ 339.67974388    0.36949998    0.37990004    0.2845        0.53680003]
 [ 412.086261      0.4876        0.36410001    0.54710001    0.3985    ]
 ...
 [ 546.26814281    0.22930001    0.5248        0.36380002    0.51330006]
 [-628.11903013    0.70209998    0.4323        0.48670003    0.29060003]
 [ 182.11852575    0.48610002    0.54030001    0.48920003    0.51410007]][0m
[37m[1m[2023-07-10 17:56:00,485][227910] Max Reward on eval: 1211.1329491954064[0m
[37m[1m[2023-07-10 17:56:00,486][227910] Min Reward on eval: -792.0691063278588[0m
[37m[1m[2023-07-10 17:56:00,486][227910] Mean Reward across all agents: 243.3738340698129[0m
[37m[1m[2023-07-10 17:56:00,486][227910] Average Trajectory Length: 996.2976666666666[0m
[36m[2023-07-10 17:56:00,490][227910] mean_value=-464.47506890693785, max_value=804.8639953636633[0m
[37m[1m[2023-07-10 17:56:00,493][227910] New mean coefficients: [[ 3.4424381  -0.25329864 -1.0550364   0.09601378  1.874767  ]][0m
[37m[1m[2023-07-10 17:56:00,494][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:56:10,244][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 17:56:10,244][227910] FPS: 393917.13[0m
[36m[2023-07-10 17:56:10,246][227910] itr=996, itrs=2000, Progress: 49.80%[0m
[36m[2023-07-10 17:56:21,965][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 17:56:21,966][227910] FPS: 328231.81[0m
[36m[2023-07-10 17:56:26,870][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:56:26,870][227910] Reward + Measures: [[1556.34889211    0.25792637    0.34220234    0.28875783    0.29283431]][0m
[37m[1m[2023-07-10 17:56:26,870][227910] Max Reward on eval: 1556.3488921099827[0m
[37m[1m[2023-07-10 17:56:26,871][227910] Min Reward on eval: 1556.3488921099827[0m
[37m[1m[2023-07-10 17:56:26,871][227910] Mean Reward across all agents: 1556.3488921099827[0m
[37m[1m[2023-07-10 17:56:26,871][227910] Average Trajectory Length: 984.4143333333333[0m
[36m[2023-07-10 17:56:32,402][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:56:32,403][227910] Reward + Measures: [[590.16432882   0.60530001   0.59400004   0.60010004   0.41459998]
 [429.79938489   0.23889998   0.1957       0.44619998   0.33399999]
 [ 66.14502582   0.42519999   0.34990001   0.37240002   0.52600002]
 ...
 [787.77544386   0.26280001   0.45520002   0.27019998   0.44600001]
 [162.92443948   0.15350001   0.75520003   0.26190001   0.80240005]
 [397.24171688   0.37080002   0.43310004   0.54290003   0.54469997]][0m
[37m[1m[2023-07-10 17:56:32,403][227910] Max Reward on eval: 1122.8606107846367[0m
[37m[1m[2023-07-10 17:56:32,404][227910] Min Reward on eval: -1136.0967832300696[0m
[37m[1m[2023-07-10 17:56:32,404][227910] Mean Reward across all agents: 116.88678448087667[0m
[37m[1m[2023-07-10 17:56:32,404][227910] Average Trajectory Length: 994.1263333333333[0m
[36m[2023-07-10 17:56:32,408][227910] mean_value=-405.73340712511066, max_value=869.9225924625175[0m
[37m[1m[2023-07-10 17:56:32,411][227910] New mean coefficients: [[ 3.8616447   0.25084728 -1.1987766   0.2339416   1.3800993 ]][0m
[37m[1m[2023-07-10 17:56:32,412][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:56:42,295][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 17:56:42,295][227910] FPS: 388641.15[0m
[36m[2023-07-10 17:56:42,297][227910] itr=997, itrs=2000, Progress: 49.85%[0m
[36m[2023-07-10 17:56:53,833][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 17:56:53,833][227910] FPS: 333458.99[0m
[36m[2023-07-10 17:56:58,601][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:56:58,601][227910] Reward + Measures: [[1642.73518033    0.25180852    0.33909184    0.2858811     0.2829555 ]][0m
[37m[1m[2023-07-10 17:56:58,601][227910] Max Reward on eval: 1642.735180328345[0m
[37m[1m[2023-07-10 17:56:58,601][227910] Min Reward on eval: 1642.735180328345[0m
[37m[1m[2023-07-10 17:56:58,602][227910] Mean Reward across all agents: 1642.735180328345[0m
[37m[1m[2023-07-10 17:56:58,602][227910] Average Trajectory Length: 981.0029999999999[0m
[36m[2023-07-10 17:57:03,980][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:57:03,981][227910] Reward + Measures: [[ 197.00160302    0.2105        0.29780003    0.30210003    0.2879    ]
 [ 350.89481739    0.28120002    0.2836        0.30200002    0.35339999]
 [ 170.24750969    0.40260002    0.62639999    0.189         0.76559991]
 ...
 [-240.7429152     0.31908008    0.33224544    0.25756982    0.29999819]
 [ 469.67383832    0.49710003    0.51710004    0.38869998    0.58240002]
 [-242.71331343    0.13509999    0.18780001    0.2624        0.38949999]][0m
[37m[1m[2023-07-10 17:57:03,981][227910] Max Reward on eval: 1343.2337361927493[0m
[37m[1m[2023-07-10 17:57:03,982][227910] Min Reward on eval: -729.4056040206808[0m
[37m[1m[2023-07-10 17:57:03,982][227910] Mean Reward across all agents: 361.42742175688727[0m
[37m[1m[2023-07-10 17:57:03,982][227910] Average Trajectory Length: 958.8603333333333[0m
[36m[2023-07-10 17:57:03,987][227910] mean_value=-396.58704333671324, max_value=704.051508912278[0m
[37m[1m[2023-07-10 17:57:03,989][227910] New mean coefficients: [[ 3.638938   -0.43085134 -0.6036436   0.19139186  2.543799  ]][0m
[37m[1m[2023-07-10 17:57:03,990][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:57:13,834][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 17:57:13,834][227910] FPS: 390156.94[0m
[36m[2023-07-10 17:57:13,837][227910] itr=998, itrs=2000, Progress: 49.90%[0m
[36m[2023-07-10 17:57:25,467][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 17:57:25,468][227910] FPS: 330680.82[0m
[36m[2023-07-10 17:57:30,346][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:57:30,346][227910] Reward + Measures: [[1715.79230949    0.25249705    0.33706558    0.29124081    0.27979636]][0m
[37m[1m[2023-07-10 17:57:30,346][227910] Max Reward on eval: 1715.7923094919877[0m
[37m[1m[2023-07-10 17:57:30,347][227910] Min Reward on eval: 1715.7923094919877[0m
[37m[1m[2023-07-10 17:57:30,347][227910] Mean Reward across all agents: 1715.7923094919877[0m
[37m[1m[2023-07-10 17:57:30,347][227910] Average Trajectory Length: 980.3193333333332[0m
[36m[2023-07-10 17:57:35,880][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:57:35,880][227910] Reward + Measures: [[ 166.76921921    0.25840002    0.60770005    0.32759997    0.54090005]
 [-305.21822296    0.32519999    0.68799996    0.42659998    0.69220001]
 [-621.40561968    0.52290004    0.48800001    0.61370003    0.18600002]
 ...
 [ 425.23792053    0.36759999    0.49349999    0.26290002    0.56549996]
 [ -23.4860792     0.29980001    0.5079        0.38030002    0.60249996]
 [ -26.7789732     0.37620002    0.36250001    0.36530003    0.41980001]][0m
[37m[1m[2023-07-10 17:57:35,880][227910] Max Reward on eval: 1744.194101308775[0m
[37m[1m[2023-07-10 17:57:35,881][227910] Min Reward on eval: -1274.864715614915[0m
[37m[1m[2023-07-10 17:57:35,881][227910] Mean Reward across all agents: 293.8266061630846[0m
[37m[1m[2023-07-10 17:57:35,881][227910] Average Trajectory Length: 986.4309999999999[0m
[36m[2023-07-10 17:57:35,885][227910] mean_value=-376.7288006544788, max_value=927.7248047971401[0m
[37m[1m[2023-07-10 17:57:35,888][227910] New mean coefficients: [[ 4.19168    -0.825275   -0.17895272  0.65687394  1.968782  ]][0m
[37m[1m[2023-07-10 17:57:35,888][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:57:45,639][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 17:57:45,639][227910] FPS: 393894.03[0m
[36m[2023-07-10 17:57:45,641][227910] itr=999, itrs=2000, Progress: 49.95%[0m
[36m[2023-07-10 17:57:57,303][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 17:57:57,303][227910] FPS: 329805.97[0m
[36m[2023-07-10 17:58:02,083][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:58:02,084][227910] Reward + Measures: [[1790.30315106    0.25268546    0.3436363     0.29011673    0.27396029]][0m
[37m[1m[2023-07-10 17:58:02,084][227910] Max Reward on eval: 1790.3031510637122[0m
[37m[1m[2023-07-10 17:58:02,084][227910] Min Reward on eval: 1790.3031510637122[0m
[37m[1m[2023-07-10 17:58:02,085][227910] Mean Reward across all agents: 1790.3031510637122[0m
[37m[1m[2023-07-10 17:58:02,085][227910] Average Trajectory Length: 983.8616666666667[0m
[36m[2023-07-10 17:58:07,518][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:58:07,519][227910] Reward + Measures: [[-340.2106542     0.61260003    0.45220003    0.43000004    0.72620004]
 [ 675.65167671    0.35110003    0.44350001    0.22490001    0.5018    ]
 [ 200.62875392    0.2306        0.67210001    0.22230001    0.68510002]
 ...
 [ 485.03292561    0.39649999    0.54100001    0.63900006    0.78170007]
 [ 638.38951977    0.38139996    0.46709999    0.2326        0.47820002]
 [ 104.79050305    0.18800001    0.736         0.3549        0.76169997]][0m
[37m[1m[2023-07-10 17:58:07,519][227910] Max Reward on eval: 1321.4453710360801[0m
[37m[1m[2023-07-10 17:58:07,519][227910] Min Reward on eval: -1116.732260195166[0m
[37m[1m[2023-07-10 17:58:07,519][227910] Mean Reward across all agents: 227.1680758341822[0m
[37m[1m[2023-07-10 17:58:07,520][227910] Average Trajectory Length: 999.1406666666667[0m
[36m[2023-07-10 17:58:07,525][227910] mean_value=-212.15124681463146, max_value=924.410079471244[0m
[37m[1m[2023-07-10 17:58:07,528][227910] New mean coefficients: [[ 4.6739745  -0.6559094  -0.88524795  0.13455492  0.725626  ]][0m
[37m[1m[2023-07-10 17:58:07,529][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:58:17,151][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 17:58:17,151][227910] FPS: 399142.03[0m
[36m[2023-07-10 17:58:17,154][227910] itr=1000, itrs=2000, Progress: 50.00%[0m
[37m[1m[2023-07-10 17:58:21,003][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000980[0m
[36m[2023-07-10 17:58:32,854][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 17:58:32,854][227910] FPS: 332099.63[0m
[36m[2023-07-10 17:58:37,633][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:58:37,633][227910] Reward + Measures: [[1890.14808078    0.25228569    0.34490949    0.2875503     0.26988196]][0m
[37m[1m[2023-07-10 17:58:37,634][227910] Max Reward on eval: 1890.1480807792432[0m
[37m[1m[2023-07-10 17:58:37,634][227910] Min Reward on eval: 1890.1480807792432[0m
[37m[1m[2023-07-10 17:58:37,634][227910] Mean Reward across all agents: 1890.1480807792432[0m
[37m[1m[2023-07-10 17:58:37,634][227910] Average Trajectory Length: 985.1823333333333[0m
[36m[2023-07-10 17:58:43,178][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:58:43,179][227910] Reward + Measures: [[  103.19687292     0.19880001     0.2149         0.29819998
      0.3057    ]
 [  795.32852984     0.2142         0.27509999     0.32469997
      0.40340003]
 [  121.42278062     0.77410001     0.25340003     0.75240004
      0.46269998]
 ...
 [  -20.45396922     0.6361683      0.59404629     0.5446732
      0.45209026]
 [ -335.12127665     0.44189999     0.23220001     0.55980003
      0.31479999]
 [-1094.11069319     0.52514362     0.34691182     0.55125225
      0.1427097 ]][0m
[37m[1m[2023-07-10 17:58:43,179][227910] Max Reward on eval: 1527.1518558858545[0m
[37m[1m[2023-07-10 17:58:43,179][227910] Min Reward on eval: -1833.0553967319197[0m
[37m[1m[2023-07-10 17:58:43,179][227910] Mean Reward across all agents: 49.67952619233068[0m
[37m[1m[2023-07-10 17:58:43,180][227910] Average Trajectory Length: 978.3979999999999[0m
[36m[2023-07-10 17:58:43,184][227910] mean_value=-503.03352974779534, max_value=1143.2427626177273[0m
[37m[1m[2023-07-10 17:58:43,187][227910] New mean coefficients: [[ 4.4380164  -0.70980793 -0.5616402   0.05275396  0.18709314]][0m
[37m[1m[2023-07-10 17:58:43,188][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:58:52,996][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 17:58:52,996][227910] FPS: 391596.09[0m
[36m[2023-07-10 17:58:52,998][227910] itr=1001, itrs=2000, Progress: 50.05%[0m
[36m[2023-07-10 17:59:04,616][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 17:59:04,617][227910] FPS: 331137.61[0m
[36m[2023-07-10 17:59:09,414][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:59:09,415][227910] Reward + Measures: [[2010.24675087    0.25138539    0.34560052    0.2931568     0.26247868]][0m
[37m[1m[2023-07-10 17:59:09,415][227910] Max Reward on eval: 2010.2467508675823[0m
[37m[1m[2023-07-10 17:59:09,415][227910] Min Reward on eval: 2010.2467508675823[0m
[37m[1m[2023-07-10 17:59:09,415][227910] Mean Reward across all agents: 2010.2467508675823[0m
[37m[1m[2023-07-10 17:59:09,415][227910] Average Trajectory Length: 992.025[0m
[36m[2023-07-10 17:59:14,979][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:59:14,980][227910] Reward + Measures: [[ 891.76996229    0.34840003    0.3479        0.27160001    0.3721    ]
 [ 812.76133293    0.22580002    0.23179999    0.30140001    0.26680002]
 [1192.76682799    0.26540002    0.28420001    0.34580001    0.2913    ]
 ...
 [ 151.78626579    0.31870002    0.5539        0.1718        0.63569999]
 [1514.4395319     0.26719999    0.3141        0.30070001    0.31460002]
 [ 591.53343347    0.2299        0.2129        0.38680002    0.31990001]][0m
[37m[1m[2023-07-10 17:59:14,980][227910] Max Reward on eval: 1876.8138087807224[0m
[37m[1m[2023-07-10 17:59:14,981][227910] Min Reward on eval: -562.7835723278695[0m
[37m[1m[2023-07-10 17:59:14,981][227910] Mean Reward across all agents: 803.2521499008817[0m
[37m[1m[2023-07-10 17:59:14,981][227910] Average Trajectory Length: 985.6026666666667[0m
[36m[2023-07-10 17:59:14,984][227910] mean_value=-444.8222383992338, max_value=1401.5316713547527[0m
[37m[1m[2023-07-10 17:59:14,987][227910] New mean coefficients: [[ 4.2589784  -0.33022642 -0.35815945  0.45661685  0.24584442]][0m
[37m[1m[2023-07-10 17:59:14,988][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:59:24,763][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 17:59:24,763][227910] FPS: 392899.04[0m
[36m[2023-07-10 17:59:24,765][227910] itr=1002, itrs=2000, Progress: 50.10%[0m
[36m[2023-07-10 17:59:36,456][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 17:59:36,456][227910] FPS: 328979.61[0m
[36m[2023-07-10 17:59:41,213][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:59:41,218][227910] Reward + Measures: [[2089.34373896    0.2477078     0.35231707    0.29504687    0.25652552]][0m
[37m[1m[2023-07-10 17:59:41,219][227910] Max Reward on eval: 2089.3437389594087[0m
[37m[1m[2023-07-10 17:59:41,219][227910] Min Reward on eval: 2089.3437389594087[0m
[37m[1m[2023-07-10 17:59:41,219][227910] Mean Reward across all agents: 2089.3437389594087[0m
[37m[1m[2023-07-10 17:59:41,219][227910] Average Trajectory Length: 991.5726666666666[0m
[36m[2023-07-10 17:59:46,612][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 17:59:46,617][227910] Reward + Measures: [[ 681.48710632    0.43020001    0.39159998    0.42580006    0.43730003]
 [ 241.60655121    0.1319        0.81440002    0.5205        0.80530006]
 [ 316.43681124    0.59180003    0.10399999    0.70370001    0.63029999]
 ...
 [ 576.80904066    0.8193        0.0398        0.88260001    0.74980003]
 [1324.40155655    0.26070002    0.35560003    0.25600001    0.33399999]
 [ 493.12460251    0.4526        0.29990003    0.3831        0.5575    ]][0m
[37m[1m[2023-07-10 17:59:46,617][227910] Max Reward on eval: 1563.9625582935405[0m
[37m[1m[2023-07-10 17:59:46,618][227910] Min Reward on eval: -1288.0820209686876[0m
[37m[1m[2023-07-10 17:59:46,618][227910] Mean Reward across all agents: 291.43886573698825[0m
[37m[1m[2023-07-10 17:59:46,618][227910] Average Trajectory Length: 987.312[0m
[36m[2023-07-10 17:59:46,623][227910] mean_value=-328.10145217054696, max_value=970.1332101685578[0m
[37m[1m[2023-07-10 17:59:46,626][227910] New mean coefficients: [[ 4.3118668  -0.89943254 -0.46699136  0.33317465 -0.22245818]][0m
[37m[1m[2023-07-10 17:59:46,627][227910] Moving the mean solution point...[0m
[36m[2023-07-10 17:59:56,330][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 17:59:56,330][227910] FPS: 395841.20[0m
[36m[2023-07-10 17:59:56,332][227910] itr=1003, itrs=2000, Progress: 50.15%[0m
[36m[2023-07-10 18:00:07,806][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 18:00:07,806][227910] FPS: 335205.85[0m
[36m[2023-07-10 18:00:12,528][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:00:12,529][227910] Reward + Measures: [[2204.79374813    0.24064913    0.35891995    0.29183012    0.24926768]][0m
[37m[1m[2023-07-10 18:00:12,529][227910] Max Reward on eval: 2204.7937481317454[0m
[37m[1m[2023-07-10 18:00:12,529][227910] Min Reward on eval: 2204.7937481317454[0m
[37m[1m[2023-07-10 18:00:12,529][227910] Mean Reward across all agents: 2204.7937481317454[0m
[37m[1m[2023-07-10 18:00:12,529][227910] Average Trajectory Length: 985.3729999999999[0m
[36m[2023-07-10 18:00:17,918][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:00:17,918][227910] Reward + Measures: [[-200.88493946    0.57419997    0.57950002    0.077         0.63749999]
 [ 288.5180072     0.22150002    0.49850002    0.59359998    0.5618    ]
 [ 219.37177694    0.30684763    0.39319524    0.31860954    0.24180952]
 ...
 [ 406.64252636    0.32370001    0.37080002    0.3272        0.26400003]
 [ 533.28772231    0.4395        0.40910003    0.28490001    0.70960003]
 [-595.94247942    0.55480003    0.18340001    0.48240003    0.42290002]][0m
[37m[1m[2023-07-10 18:00:17,919][227910] Max Reward on eval: 2099.494128667959[0m
[37m[1m[2023-07-10 18:00:17,919][227910] Min Reward on eval: -1181.9934538623434[0m
[37m[1m[2023-07-10 18:00:17,919][227910] Mean Reward across all agents: 240.55123446066776[0m
[37m[1m[2023-07-10 18:00:17,919][227910] Average Trajectory Length: 954.8806666666667[0m
[36m[2023-07-10 18:00:17,923][227910] mean_value=-1121.4504247712393, max_value=1085.519146285078[0m
[37m[1m[2023-07-10 18:00:17,926][227910] New mean coefficients: [[ 4.2409167  -0.12998593 -0.5022581  -0.20603901  1.3426394 ]][0m
[37m[1m[2023-07-10 18:00:17,927][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:00:27,482][227910] train() took 9.55 seconds to complete[0m
[36m[2023-07-10 18:00:27,482][227910] FPS: 401947.81[0m
[36m[2023-07-10 18:00:27,484][227910] itr=1004, itrs=2000, Progress: 50.20%[0m
[36m[2023-07-10 18:00:38,988][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 18:00:38,989][227910] FPS: 334358.50[0m
[36m[2023-07-10 18:00:43,748][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:00:43,748][227910] Reward + Measures: [[2321.72416884    0.2372334     0.35846654    0.28408188    0.2438487 ]][0m
[37m[1m[2023-07-10 18:00:43,749][227910] Max Reward on eval: 2321.7241688411773[0m
[37m[1m[2023-07-10 18:00:43,749][227910] Min Reward on eval: 2321.7241688411773[0m
[37m[1m[2023-07-10 18:00:43,749][227910] Mean Reward across all agents: 2321.7241688411773[0m
[37m[1m[2023-07-10 18:00:43,750][227910] Average Trajectory Length: 982.4903333333333[0m
[36m[2023-07-10 18:00:49,276][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:00:49,277][227910] Reward + Measures: [[ -362.3024253      0.67650002     0.26550001     0.5952
      0.53820002]
 [-1177.640363       0.7762         0.13710001     0.82969999
      0.64160007]
 [  392.89290344     0.58849996     0.57080001     0.25930002
      0.75469995]
 ...
 [  557.81227138     0.81760007     0.73140001     0.40489998
      0.75769997]
 [  818.45145226     0.23220001     0.30520001     0.366
      0.49489999]
 [  131.03974695     0.92360002     0.0892         0.91970009
      0.82419997]][0m
[37m[1m[2023-07-10 18:00:49,277][227910] Max Reward on eval: 1743.7920462890295[0m
[37m[1m[2023-07-10 18:00:49,278][227910] Min Reward on eval: -1177.6403630036862[0m
[37m[1m[2023-07-10 18:00:49,278][227910] Mean Reward across all agents: 263.1916856281239[0m
[37m[1m[2023-07-10 18:00:49,278][227910] Average Trajectory Length: 990.6126666666667[0m
[36m[2023-07-10 18:00:49,284][227910] mean_value=-191.01084400682623, max_value=1479.3997509129579[0m
[37m[1m[2023-07-10 18:00:49,287][227910] New mean coefficients: [[ 3.9934006 -0.6311685 -0.6428124 -0.2559047  0.5011408]][0m
[37m[1m[2023-07-10 18:00:49,288][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:00:59,003][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 18:00:59,003][227910] FPS: 395330.73[0m
[36m[2023-07-10 18:00:59,005][227910] itr=1005, itrs=2000, Progress: 50.25%[0m
[36m[2023-07-10 18:01:10,575][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 18:01:10,575][227910] FPS: 332495.86[0m
[36m[2023-07-10 18:01:15,369][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:01:15,370][227910] Reward + Measures: [[2415.17614887    0.2336496     0.35339502    0.27435124    0.23946112]][0m
[37m[1m[2023-07-10 18:01:15,370][227910] Max Reward on eval: 2415.1761488699985[0m
[37m[1m[2023-07-10 18:01:15,370][227910] Min Reward on eval: 2415.1761488699985[0m
[37m[1m[2023-07-10 18:01:15,370][227910] Mean Reward across all agents: 2415.1761488699985[0m
[37m[1m[2023-07-10 18:01:15,371][227910] Average Trajectory Length: 977.7043333333334[0m
[36m[2023-07-10 18:01:20,814][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:01:20,815][227910] Reward + Measures: [[1123.62161521    0.278         0.36970001    0.32620001    0.34670001]
 [ -65.2354611     0.57350004    0.4921        0.5905        0.25409999]
 [ 528.98110856    0.39480001    0.32370001    0.2411        0.45819998]
 ...
 [ 895.1889956     0.34841987    0.34785423    0.39278576    0.27378377]
 [  98.70894371    0.27127013    0.39612728    0.33770651    0.45651039]
 [1016.07706509    0.2749531     0.27873203    0.35801682    0.30441758]][0m
[37m[1m[2023-07-10 18:01:20,815][227910] Max Reward on eval: 1767.989701600431[0m
[37m[1m[2023-07-10 18:01:20,815][227910] Min Reward on eval: -988.472985085682[0m
[37m[1m[2023-07-10 18:01:20,815][227910] Mean Reward across all agents: 498.0334839837319[0m
[37m[1m[2023-07-10 18:01:20,816][227910] Average Trajectory Length: 984.3003333333334[0m
[36m[2023-07-10 18:01:20,820][227910] mean_value=-427.21943911171314, max_value=855.2835869142541[0m
[37m[1m[2023-07-10 18:01:20,823][227910] New mean coefficients: [[ 4.4430637  -1.0527321  -0.7260359   0.02204701 -0.20302701]][0m
[37m[1m[2023-07-10 18:01:20,823][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:01:30,566][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 18:01:30,567][227910] FPS: 394196.16[0m
[36m[2023-07-10 18:01:30,569][227910] itr=1006, itrs=2000, Progress: 50.30%[0m
[36m[2023-07-10 18:01:42,041][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 18:01:42,041][227910] FPS: 335271.55[0m
[36m[2023-07-10 18:01:46,684][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:01:46,685][227910] Reward + Measures: [[2533.01795178    0.22681335    0.35826093    0.27952346    0.23540282]][0m
[37m[1m[2023-07-10 18:01:46,685][227910] Max Reward on eval: 2533.0179517805523[0m
[37m[1m[2023-07-10 18:01:46,686][227910] Min Reward on eval: 2533.0179517805523[0m
[37m[1m[2023-07-10 18:01:46,686][227910] Mean Reward across all agents: 2533.0179517805523[0m
[37m[1m[2023-07-10 18:01:46,686][227910] Average Trajectory Length: 974.5516666666666[0m
[36m[2023-07-10 18:01:52,199][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:01:52,200][227910] Reward + Measures: [[ 940.5297779     0.27300003    0.34400001    0.3066        0.37340003]
 [1528.5681714     0.26100001    0.3673        0.31620002    0.30809999]
 [1145.72688418    0.30059361    0.40881276    0.24873054    0.28996012]
 ...
 [ 718.01928177    0.31399998    0.35875154    0.27373031    0.30263939]
 [ 737.45955836    0.26390001    0.39200002    0.3497        0.37980002]
 [ 352.79639053    0.34960002    0.30019999    0.40630004    0.39789999]][0m
[37m[1m[2023-07-10 18:01:52,200][227910] Max Reward on eval: 2023.0958616592457[0m
[37m[1m[2023-07-10 18:01:52,200][227910] Min Reward on eval: -813.2229640144157[0m
[37m[1m[2023-07-10 18:01:52,200][227910] Mean Reward across all agents: 685.0961169769431[0m
[37m[1m[2023-07-10 18:01:52,201][227910] Average Trajectory Length: 991.6006666666666[0m
[36m[2023-07-10 18:01:52,204][227910] mean_value=-558.2961967172832, max_value=846.0370285393466[0m
[37m[1m[2023-07-10 18:01:52,207][227910] New mean coefficients: [[ 4.464216   -0.72416234 -0.795857   -0.408584    1.1782048 ]][0m
[37m[1m[2023-07-10 18:01:52,208][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:02:01,801][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 18:02:01,801][227910] FPS: 400385.83[0m
[36m[2023-07-10 18:02:01,803][227910] itr=1007, itrs=2000, Progress: 50.35%[0m
[36m[2023-07-10 18:02:13,344][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 18:02:13,344][227910] FPS: 333280.36[0m
[36m[2023-07-10 18:02:18,189][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:02:18,189][227910] Reward + Measures: [[1003.33289757    0.26452273    0.34631306    0.33080044    0.26682508]][0m
[37m[1m[2023-07-10 18:02:18,189][227910] Max Reward on eval: 1003.3328975701702[0m
[37m[1m[2023-07-10 18:02:18,190][227910] Min Reward on eval: 1003.3328975701702[0m
[37m[1m[2023-07-10 18:02:18,190][227910] Mean Reward across all agents: 1003.3328975701702[0m
[37m[1m[2023-07-10 18:02:18,190][227910] Average Trajectory Length: 994.7213333333333[0m
[36m[2023-07-10 18:02:23,699][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:02:23,700][227910] Reward + Measures: [[ 852.35917783    0.25510001    0.29210001    0.25490004    0.35119998]
 [ 376.58721868    0.47740003    0.62339997    0.2132        0.58149999]
 [ 926.86680549    0.25760004    0.27790001    0.33680001    0.31619999]
 ...
 [ 156.70966994    0.56110001    0.72410005    0.149         0.6498    ]
 [ 764.43082732    0.30110002    0.31030002    0.35420004    0.33720002]
 [1160.93659747    0.27016744    0.34703895    0.30271551    0.29613942]][0m
[37m[1m[2023-07-10 18:02:23,700][227910] Max Reward on eval: 1252.680951511819[0m
[37m[1m[2023-07-10 18:02:23,700][227910] Min Reward on eval: -267.4486416311905[0m
[37m[1m[2023-07-10 18:02:23,701][227910] Mean Reward across all agents: 626.417073317824[0m
[37m[1m[2023-07-10 18:02:23,701][227910] Average Trajectory Length: 989.0939999999999[0m
[36m[2023-07-10 18:02:23,703][227910] mean_value=-1061.4805204608836, max_value=510.5937378638988[0m
[37m[1m[2023-07-10 18:02:23,705][227910] New mean coefficients: [[ 5.1135917  -1.1111495  -0.6444788  -0.84700525  1.4500744 ]][0m
[37m[1m[2023-07-10 18:02:23,706][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:02:33,459][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 18:02:33,459][227910] FPS: 393817.07[0m
[36m[2023-07-10 18:02:33,461][227910] itr=1008, itrs=2000, Progress: 50.40%[0m
[36m[2023-07-10 18:02:45,138][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 18:02:45,139][227910] FPS: 329360.83[0m
[36m[2023-07-10 18:02:49,920][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:02:49,920][227910] Reward + Measures: [[1137.47706817    0.25667003    0.3486402     0.33968341    0.25187594]][0m
[37m[1m[2023-07-10 18:02:49,920][227910] Max Reward on eval: 1137.4770681663856[0m
[37m[1m[2023-07-10 18:02:49,921][227910] Min Reward on eval: 1137.4770681663856[0m
[37m[1m[2023-07-10 18:02:49,921][227910] Mean Reward across all agents: 1137.4770681663856[0m
[37m[1m[2023-07-10 18:02:49,921][227910] Average Trajectory Length: 994.7206666666666[0m
[36m[2023-07-10 18:02:55,309][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:02:55,310][227910] Reward + Measures: [[890.5581849    0.33730003   0.32710001   0.4858       0.29410002]
 [828.66085742   0.33970001   0.30990002   0.49190003   0.27250001]
 [620.91623231   0.3317       0.3572       0.56300002   0.46799999]
 ...
 [ 14.06202981   0.5115       0.40649995   0.31550002   0.40580001]
 [225.28918089   0.72149998   0.45249996   0.55059999   0.48210001]
 [524.58071143   0.40459999   0.37620002   0.38150001   0.35610002]][0m
[37m[1m[2023-07-10 18:02:55,310][227910] Max Reward on eval: 1295.9003879305558[0m
[37m[1m[2023-07-10 18:02:55,311][227910] Min Reward on eval: -1058.2148715322487[0m
[37m[1m[2023-07-10 18:02:55,311][227910] Mean Reward across all agents: 542.3423521408142[0m
[37m[1m[2023-07-10 18:02:55,311][227910] Average Trajectory Length: 995.6366666666667[0m
[36m[2023-07-10 18:02:55,314][227910] mean_value=-811.291887383785, max_value=787.9138047489082[0m
[37m[1m[2023-07-10 18:02:55,317][227910] New mean coefficients: [[ 4.9528255  -1.3935438  -0.89381146 -1.4458961   1.6288178 ]][0m
[37m[1m[2023-07-10 18:02:55,318][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:03:05,010][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 18:03:05,010][227910] FPS: 396276.03[0m
[36m[2023-07-10 18:03:05,012][227910] itr=1009, itrs=2000, Progress: 50.45%[0m
[36m[2023-07-10 18:03:16,576][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 18:03:16,576][227910] FPS: 332622.08[0m
[36m[2023-07-10 18:03:21,445][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:03:21,445][227910] Reward + Measures: [[1283.48421804    0.25399283    0.35088766    0.35109553    0.24389835]][0m
[37m[1m[2023-07-10 18:03:21,446][227910] Max Reward on eval: 1283.4842180403896[0m
[37m[1m[2023-07-10 18:03:21,446][227910] Min Reward on eval: 1283.4842180403896[0m
[37m[1m[2023-07-10 18:03:21,446][227910] Mean Reward across all agents: 1283.4842180403896[0m
[37m[1m[2023-07-10 18:03:21,446][227910] Average Trajectory Length: 993.6676666666666[0m
[36m[2023-07-10 18:03:26,931][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:03:26,932][227910] Reward + Measures: [[ -89.31444137    0.44161749    0.49729809    0.10824662    0.65392685]
 [-488.18041868    0.37790003    0.43700001    0.14830001    0.52780002]
 [-392.18205162    0.3344        0.40270001    0.1367        0.45269999]
 ...
 [-602.88441748    0.25869998    0.31819999    0.21210001    0.40030003]
 [-666.26786299    0.40349999    0.28260002    0.38779998    0.37129998]
 [-384.33468992    0.5244        0.61009997    0.1287        0.72610003]][0m
[37m[1m[2023-07-10 18:03:26,932][227910] Max Reward on eval: 1267.9258425048786[0m
[37m[1m[2023-07-10 18:03:26,932][227910] Min Reward on eval: -1109.042744052643[0m
[37m[1m[2023-07-10 18:03:26,933][227910] Mean Reward across all agents: -43.978176089402204[0m
[37m[1m[2023-07-10 18:03:26,933][227910] Average Trajectory Length: 950.3673333333332[0m
[36m[2023-07-10 18:03:26,936][227910] mean_value=-660.9225332505328, max_value=673.0075344474753[0m
[37m[1m[2023-07-10 18:03:26,938][227910] New mean coefficients: [[ 4.6656494  -2.1253617  -0.10391146 -1.3192239   1.9458914 ]][0m
[37m[1m[2023-07-10 18:03:26,939][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:03:36,816][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 18:03:36,816][227910] FPS: 388883.24[0m
[36m[2023-07-10 18:03:36,818][227910] itr=1010, itrs=2000, Progress: 50.50%[0m
[37m[1m[2023-07-10 18:03:40,772][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00000990[0m
[36m[2023-07-10 18:03:52,670][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 18:03:52,671][227910] FPS: 330010.71[0m
[36m[2023-07-10 18:03:57,544][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:03:57,545][227910] Reward + Measures: [[556.96671905   0.23133896   0.47168607   0.20757495   0.2721383 ]][0m
[37m[1m[2023-07-10 18:03:57,545][227910] Max Reward on eval: 556.9667190522633[0m
[37m[1m[2023-07-10 18:03:57,545][227910] Min Reward on eval: 556.9667190522633[0m
[37m[1m[2023-07-10 18:03:57,546][227910] Mean Reward across all agents: 556.9667190522633[0m
[37m[1m[2023-07-10 18:03:57,546][227910] Average Trajectory Length: 920.8486666666666[0m
[36m[2023-07-10 18:04:03,034][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:04:03,039][227910] Reward + Measures: [[-102.95038546    0.55230004    0.36480001    0.48400003    0.3874    ]
 [-425.97594676    0.5219        0.26069999    0.47680002    0.31670001]
 [-975.69859962    0.55860007    0.44420001    0.23280001    0.47399998]
 ...
 [-828.02183395    0.58921462    0.65970731    0.11940243    0.69705123]
 [-960.02435278    0.28520003    0.36330003    0.21430002    0.39309999]
 [-277.93491462    0.70669997    0.74509996    0.0597        0.81650001]][0m
[37m[1m[2023-07-10 18:04:03,040][227910] Max Reward on eval: 565.0287386686192[0m
[37m[1m[2023-07-10 18:04:03,040][227910] Min Reward on eval: -1648.9584733531751[0m
[37m[1m[2023-07-10 18:04:03,040][227910] Mean Reward across all agents: -721.7259756700095[0m
[37m[1m[2023-07-10 18:04:03,041][227910] Average Trajectory Length: 980.1[0m
[36m[2023-07-10 18:04:03,042][227910] mean_value=-1375.7409033184138, max_value=338.9953893876667[0m
[37m[1m[2023-07-10 18:04:03,045][227910] New mean coefficients: [[ 4.3887177  -1.4303384   0.0300435  -0.86084294  1.7890433 ]][0m
[37m[1m[2023-07-10 18:04:03,046][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:04:12,793][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 18:04:12,793][227910] FPS: 394038.55[0m
[36m[2023-07-10 18:04:12,796][227910] itr=1011, itrs=2000, Progress: 50.55%[0m
[36m[2023-07-10 18:04:24,238][227910] train() took 11.42 seconds to complete[0m
[36m[2023-07-10 18:04:24,238][227910] FPS: 336156.69[0m
[36m[2023-07-10 18:04:28,913][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:04:28,913][227910] Reward + Measures: [[631.4588159    0.21842907   0.33166382   0.27927148   0.19955428]][0m
[37m[1m[2023-07-10 18:04:28,914][227910] Max Reward on eval: 631.4588158970613[0m
[37m[1m[2023-07-10 18:04:28,914][227910] Min Reward on eval: 631.4588158970613[0m
[37m[1m[2023-07-10 18:04:28,914][227910] Mean Reward across all agents: 631.4588158970613[0m
[37m[1m[2023-07-10 18:04:28,914][227910] Average Trajectory Length: 936.86[0m
[36m[2023-07-10 18:04:34,403][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:04:34,403][227910] Reward + Measures: [[-247.09235424    0.51730007    0.5776        0.14780001    0.65430003]
 [-424.22573271    0.54089999    0.50890005    0.37          0.56510001]
 [-181.26462689    0.32720003    0.51060003    0.1972        0.4224    ]
 ...
 [-523.7900997     0.41622224    0.46550235    0.33066368    0.59400707]
 [-404.76806884    0.22861211    0.21412289    0.21836534    0.30190641]
 [ -71.78990498    0.41389999    0.49869999    0.25330001    0.54100001]][0m
[37m[1m[2023-07-10 18:04:34,403][227910] Max Reward on eval: 667.1436762035243[0m
[37m[1m[2023-07-10 18:04:34,404][227910] Min Reward on eval: -1104.5650774672365[0m
[37m[1m[2023-07-10 18:04:34,404][227910] Mean Reward across all agents: -188.6011893777251[0m
[37m[1m[2023-07-10 18:04:34,404][227910] Average Trajectory Length: 912.5949999999999[0m
[36m[2023-07-10 18:04:34,406][227910] mean_value=-1465.8538408359314, max_value=606.5136341013192[0m
[37m[1m[2023-07-10 18:04:34,408][227910] New mean coefficients: [[ 4.282804   -0.37474883 -0.1256735  -0.9768711   2.8973308 ]][0m
[37m[1m[2023-07-10 18:04:34,409][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:04:44,119][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 18:04:44,120][227910] FPS: 395533.37[0m
[36m[2023-07-10 18:04:44,122][227910] itr=1012, itrs=2000, Progress: 50.60%[0m
[36m[2023-07-10 18:04:55,802][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 18:04:55,802][227910] FPS: 329329.31[0m
[36m[2023-07-10 18:05:00,570][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:05:00,570][227910] Reward + Measures: [[524.7365504    0.17556867   0.40785962   0.23495702   0.22854936]][0m
[37m[1m[2023-07-10 18:05:00,570][227910] Max Reward on eval: 524.7365503965481[0m
[37m[1m[2023-07-10 18:05:00,571][227910] Min Reward on eval: 524.7365503965481[0m
[37m[1m[2023-07-10 18:05:00,571][227910] Mean Reward across all agents: 524.7365503965481[0m
[37m[1m[2023-07-10 18:05:00,571][227910] Average Trajectory Length: 988.9233333333333[0m
[36m[2023-07-10 18:05:06,018][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:05:06,018][227910] Reward + Measures: [[ 187.47960382    0.36210001    0.41659999    0.1786        0.42560002]
 [-487.97278117    0.24957661    0.31753799    0.16663544    0.3090356 ]
 [-612.92596339    0.1838        0.1939        0.18380001    0.1842    ]
 ...
 [ 567.77904311    0.2198        0.3238        0.24520002    0.29580003]
 [  47.06669949    0.2007        0.35180002    0.26690003    0.37009999]
 [ 456.01223431    0.15790001    0.51029998    0.30160001    0.34530002]][0m
[37m[1m[2023-07-10 18:05:06,018][227910] Max Reward on eval: 763.7108391584363[0m
[37m[1m[2023-07-10 18:05:06,019][227910] Min Reward on eval: -680.5961575779947[0m
[37m[1m[2023-07-10 18:05:06,019][227910] Mean Reward across all agents: 113.93190523612444[0m
[37m[1m[2023-07-10 18:05:06,019][227910] Average Trajectory Length: 958.5509999999999[0m
[36m[2023-07-10 18:05:06,021][227910] mean_value=-1868.2391423471172, max_value=507.27981785918723[0m
[37m[1m[2023-07-10 18:05:06,023][227910] New mean coefficients: [[ 4.139052   -0.88282806 -0.10229148 -1.1396476   2.621798  ]][0m
[37m[1m[2023-07-10 18:05:06,024][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:05:15,812][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 18:05:15,812][227910] FPS: 392398.09[0m
[36m[2023-07-10 18:05:15,814][227910] itr=1013, itrs=2000, Progress: 50.65%[0m
[36m[2023-07-10 18:05:27,376][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 18:05:27,376][227910] FPS: 332666.82[0m
[36m[2023-07-10 18:05:32,219][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:05:32,219][227910] Reward + Measures: [[917.21351126   0.18747635   0.38364264   0.32002079   0.25475198]][0m
[37m[1m[2023-07-10 18:05:32,220][227910] Max Reward on eval: 917.2135112642534[0m
[37m[1m[2023-07-10 18:05:32,220][227910] Min Reward on eval: 917.2135112642534[0m
[37m[1m[2023-07-10 18:05:32,220][227910] Mean Reward across all agents: 917.2135112642534[0m
[37m[1m[2023-07-10 18:05:32,220][227910] Average Trajectory Length: 992.0923333333333[0m
[36m[2023-07-10 18:05:37,763][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:05:37,764][227910] Reward + Measures: [[-521.21887141    0.13770001    0.64359999    0.4501        0.6845001 ]
 [ 909.49360367    0.2263        0.36190003    0.2445        0.2527    ]
 [-365.40660368    0.0831        0.55720001    0.31440002    0.57860005]
 ...
 [ -13.75767124    0.11680001    0.63459998    0.49079999    0.6372    ]
 [ 999.63089807    0.20030001    0.3955        0.29229999    0.27129999]
 [ 180.73821444    0.1832        0.39330003    0.27490002    0.33060002]][0m
[37m[1m[2023-07-10 18:05:37,764][227910] Max Reward on eval: 1219.3119586665066[0m
[37m[1m[2023-07-10 18:05:37,764][227910] Min Reward on eval: -1263.5008581250672[0m
[37m[1m[2023-07-10 18:05:37,764][227910] Mean Reward across all agents: 499.34355656756173[0m
[37m[1m[2023-07-10 18:05:37,765][227910] Average Trajectory Length: 995.377[0m
[36m[2023-07-10 18:05:37,766][227910] mean_value=-1888.1650106921768, max_value=706.9858340937051[0m
[37m[1m[2023-07-10 18:05:37,769][227910] New mean coefficients: [[ 5.151202  -1.4854063  0.380135  -1.308151   2.128104 ]][0m
[37m[1m[2023-07-10 18:05:37,770][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:05:47,392][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 18:05:47,393][227910] FPS: 399125.11[0m
[36m[2023-07-10 18:05:47,395][227910] itr=1014, itrs=2000, Progress: 50.70%[0m
[36m[2023-07-10 18:05:59,032][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 18:05:59,032][227910] FPS: 330589.91[0m
[36m[2023-07-10 18:06:03,773][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:06:03,773][227910] Reward + Measures: [[780.12987903   0.22665779   0.45950276   0.34007093   0.25145122]][0m
[37m[1m[2023-07-10 18:06:03,774][227910] Max Reward on eval: 780.1298790337369[0m
[37m[1m[2023-07-10 18:06:03,774][227910] Min Reward on eval: 780.1298790337369[0m
[37m[1m[2023-07-10 18:06:03,774][227910] Mean Reward across all agents: 780.1298790337369[0m
[37m[1m[2023-07-10 18:06:03,774][227910] Average Trajectory Length: 999.7959999999999[0m
[36m[2023-07-10 18:06:09,397][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:06:09,398][227910] Reward + Measures: [[  13.12862726    0.19059999    0.41610003    0.23540001    0.47300002]
 [ 550.77565303    0.22320001    0.47340003    0.28979999    0.3558    ]
 [-276.06540828    0.1434        0.42150003    0.24770001    0.51770002]
 ...
 [  52.64404079    0.15809999    0.47890002    0.25119999    0.50800008]
 [ -36.22309225    0.18693392    0.28470412    0.20209701    0.31476149]
 [-125.04094491    0.17580001    0.42180005    0.2405        0.49240002]][0m
[37m[1m[2023-07-10 18:06:09,398][227910] Max Reward on eval: 771.8781923006521[0m
[37m[1m[2023-07-10 18:06:09,398][227910] Min Reward on eval: -807.7475985089026[0m
[37m[1m[2023-07-10 18:06:09,398][227910] Mean Reward across all agents: 7.230751413019537[0m
[37m[1m[2023-07-10 18:06:09,398][227910] Average Trajectory Length: 975.4943333333333[0m
[36m[2023-07-10 18:06:09,400][227910] mean_value=-688.1215141219327, max_value=153.11126851122737[0m
[37m[1m[2023-07-10 18:06:09,402][227910] New mean coefficients: [[ 4.842447  -1.4826691  0.3110102 -1.6626929  2.4751174]][0m
[37m[1m[2023-07-10 18:06:09,403][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:06:19,123][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 18:06:19,123][227910] FPS: 395162.90[0m
[36m[2023-07-10 18:06:19,125][227910] itr=1015, itrs=2000, Progress: 50.75%[0m
[36m[2023-07-10 18:06:30,763][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 18:06:30,764][227910] FPS: 330462.79[0m
[36m[2023-07-10 18:06:35,577][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:06:35,578][227910] Reward + Measures: [[786.37533181   0.20559904   0.45429757   0.3251093    0.23701456]][0m
[37m[1m[2023-07-10 18:06:35,578][227910] Max Reward on eval: 786.3753318149634[0m
[37m[1m[2023-07-10 18:06:35,578][227910] Min Reward on eval: 786.3753318149634[0m
[37m[1m[2023-07-10 18:06:35,579][227910] Mean Reward across all agents: 786.3753318149634[0m
[37m[1m[2023-07-10 18:06:35,579][227910] Average Trajectory Length: 988.0[0m
[36m[2023-07-10 18:06:41,121][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:06:41,121][227910] Reward + Measures: [[ 223.71689224    0.20019828    0.41929564    0.36411664    0.29147732]
 [ 723.02915986    0.1756604     0.53631139    0.31313828    0.24081074]
 [-726.77595597    0.18296666    0.50413334    0.45366669    0.57893336]
 ...
 [ 487.57386824    0.14850001    0.35139999    0.2282        0.26120001]
 [-347.34500077    0.12751527    0.30304131    0.19857977    0.30120286]
 [  14.61548818    0.11555715    0.46901426    0.30019999    0.42118573]][0m
[37m[1m[2023-07-10 18:06:41,122][227910] Max Reward on eval: 905.3254655012745[0m
[37m[1m[2023-07-10 18:06:41,122][227910] Min Reward on eval: -949.9900435487623[0m
[37m[1m[2023-07-10 18:06:41,122][227910] Mean Reward across all agents: 101.7511188130526[0m
[37m[1m[2023-07-10 18:06:41,122][227910] Average Trajectory Length: 927.2603333333333[0m
[36m[2023-07-10 18:06:41,124][227910] mean_value=-1446.2653663107244, max_value=287.6573864874257[0m
[37m[1m[2023-07-10 18:06:41,127][227910] New mean coefficients: [[ 5.2061195 -1.8192693  0.6032846 -1.574537   2.7251258]][0m
[37m[1m[2023-07-10 18:06:41,128][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:06:50,946][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 18:06:50,946][227910] FPS: 391175.07[0m
[36m[2023-07-10 18:06:50,948][227910] itr=1016, itrs=2000, Progress: 50.80%[0m
[36m[2023-07-10 18:07:02,580][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 18:07:02,580][227910] FPS: 330680.26[0m
[36m[2023-07-10 18:07:07,368][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:07:07,368][227910] Reward + Measures: [[832.46947878   0.19913289   0.4580625    0.3125369    0.22641332]][0m
[37m[1m[2023-07-10 18:07:07,368][227910] Max Reward on eval: 832.4694787813372[0m
[37m[1m[2023-07-10 18:07:07,369][227910] Min Reward on eval: 832.4694787813372[0m
[37m[1m[2023-07-10 18:07:07,369][227910] Mean Reward across all agents: 832.4694787813372[0m
[37m[1m[2023-07-10 18:07:07,369][227910] Average Trajectory Length: 982.3366666666666[0m
[36m[2023-07-10 18:07:12,911][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:07:12,912][227910] Reward + Measures: [[652.79589023   0.1795       0.56510001   0.30940002   0.40480003]
 [776.21362178   0.19889998   0.47950003   0.32749999   0.26710001]
 [897.82924447   0.21020003   0.49330005   0.33950001   0.2606    ]
 ...
 [880.69805571   0.2185       0.51419997   0.33860001   0.33330002]
 [752.80682022   0.20809999   0.52360004   0.33059999   0.32940003]
 [589.08861909   0.18090001   0.52140003   0.31040001   0.38620001]][0m
[37m[1m[2023-07-10 18:07:12,912][227910] Max Reward on eval: 982.7912421998801[0m
[37m[1m[2023-07-10 18:07:12,912][227910] Min Reward on eval: 434.3505501950276[0m
[37m[1m[2023-07-10 18:07:12,913][227910] Mean Reward across all agents: 784.3616839750534[0m
[37m[1m[2023-07-10 18:07:12,913][227910] Average Trajectory Length: 998.9593333333333[0m
[36m[2023-07-10 18:07:12,914][227910] mean_value=-1651.8878976849544, max_value=34.778740084589685[0m
[37m[1m[2023-07-10 18:07:12,917][227910] New mean coefficients: [[ 5.795497  -2.2743778  1.1121948 -1.6367171  4.5869665]][0m
[37m[1m[2023-07-10 18:07:12,918][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:07:22,582][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 18:07:22,582][227910] FPS: 397424.19[0m
[36m[2023-07-10 18:07:22,584][227910] itr=1017, itrs=2000, Progress: 50.85%[0m
[36m[2023-07-10 18:07:34,189][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 18:07:34,189][227910] FPS: 331515.54[0m
[36m[2023-07-10 18:07:38,936][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:07:38,936][227910] Reward + Measures: [[669.3090701    0.17528333   0.31110713   0.34011889   0.21949676]][0m
[37m[1m[2023-07-10 18:07:38,936][227910] Max Reward on eval: 669.3090701023125[0m
[37m[1m[2023-07-10 18:07:38,937][227910] Min Reward on eval: 669.3090701023125[0m
[37m[1m[2023-07-10 18:07:38,937][227910] Mean Reward across all agents: 669.3090701023125[0m
[37m[1m[2023-07-10 18:07:38,937][227910] Average Trajectory Length: 825.5383333333333[0m
[36m[2023-07-10 18:07:44,356][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:07:44,356][227910] Reward + Measures: [[-326.70245381    0.23433931    0.20638998    0.26751348    0.28883252]
 [-967.41494046    0.41011363    0.1200068     0.39280817    0.29149386]
 [-160.4889097     0.26859999    0.32149997    0.35520002    0.41590005]
 ...
 [-453.27662365    0.21365464    0.17196459    0.23063365    0.24798162]
 [-970.5989        0.80004513    0.04655484    0.7788806     0.57767099]
 [-334.44050763    0.20971814    0.15604606    0.28349674    0.24766178]][0m
[37m[1m[2023-07-10 18:07:44,357][227910] Max Reward on eval: 617.501312231872[0m
[37m[1m[2023-07-10 18:07:44,357][227910] Min Reward on eval: -1169.3975246297427[0m
[37m[1m[2023-07-10 18:07:44,357][227910] Mean Reward across all agents: -290.11324616738915[0m
[37m[1m[2023-07-10 18:07:44,358][227910] Average Trajectory Length: 795.1526666666666[0m
[36m[2023-07-10 18:07:44,359][227910] mean_value=-1170.030318886988, max_value=192.14966469084646[0m
[37m[1m[2023-07-10 18:07:44,361][227910] New mean coefficients: [[ 6.5009594 -2.4274116  1.0156369 -1.7016973  4.7397723]][0m
[37m[1m[2023-07-10 18:07:44,362][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:07:54,079][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 18:07:54,079][227910] FPS: 395272.20[0m
[36m[2023-07-10 18:07:54,082][227910] itr=1018, itrs=2000, Progress: 50.90%[0m
[36m[2023-07-10 18:08:05,688][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 18:08:05,688][227910] FPS: 331381.49[0m
[36m[2023-07-10 18:08:10,522][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:08:10,523][227910] Reward + Measures: [[831.09334602   0.17879732   0.33423609   0.34379989   0.21899308]][0m
[37m[1m[2023-07-10 18:08:10,523][227910] Max Reward on eval: 831.0933460178945[0m
[37m[1m[2023-07-10 18:08:10,523][227910] Min Reward on eval: 831.0933460178945[0m
[37m[1m[2023-07-10 18:08:10,523][227910] Mean Reward across all agents: 831.0933460178945[0m
[37m[1m[2023-07-10 18:08:10,523][227910] Average Trajectory Length: 849.0803333333333[0m
[36m[2023-07-10 18:08:16,155][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:08:16,156][227910] Reward + Measures: [[-313.61258675    0.1605        0.145         0.1426        0.2057    ]
 [ -34.5657578     0.20390001    0.6785        0.40470001    0.7568    ]
 [ 133.09900862    0.1374        0.68000001    0.36050001    0.76060003]
 ...
 [-510.23819337    0.56239998    0.60050005    0.3734        0.62250006]
 [-965.0639198     0.55339998    0.56199998    0.52980006    0.55470002]
 [  82.83561112    0.1199        0.65740007    0.28920001    0.66390002]][0m
[37m[1m[2023-07-10 18:08:16,156][227910] Max Reward on eval: 723.3788804343465[0m
[37m[1m[2023-07-10 18:08:16,156][227910] Min Reward on eval: -1139.821462363319[0m
[37m[1m[2023-07-10 18:08:16,157][227910] Mean Reward across all agents: -78.11385153142736[0m
[37m[1m[2023-07-10 18:08:16,157][227910] Average Trajectory Length: 937.2256666666666[0m
[36m[2023-07-10 18:08:16,159][227910] mean_value=-767.0590987050152, max_value=295.43169063558133[0m
[37m[1m[2023-07-10 18:08:16,161][227910] New mean coefficients: [[ 5.495839  -1.8017957  1.0299982 -1.9170834  4.8971024]][0m
[37m[1m[2023-07-10 18:08:16,163][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:08:25,778][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 18:08:25,778][227910] FPS: 399431.17[0m
[36m[2023-07-10 18:08:25,780][227910] itr=1019, itrs=2000, Progress: 50.95%[0m
[36m[2023-07-10 18:08:37,548][227910] train() took 11.75 seconds to complete[0m
[36m[2023-07-10 18:08:37,548][227910] FPS: 326829.27[0m
[36m[2023-07-10 18:08:42,435][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:08:42,435][227910] Reward + Measures: [[963.48524385   0.18237513   0.34614706   0.34325215   0.21737158]][0m
[37m[1m[2023-07-10 18:08:42,435][227910] Max Reward on eval: 963.4852438534564[0m
[37m[1m[2023-07-10 18:08:42,435][227910] Min Reward on eval: 963.4852438534564[0m
[37m[1m[2023-07-10 18:08:42,436][227910] Mean Reward across all agents: 963.4852438534564[0m
[37m[1m[2023-07-10 18:08:42,436][227910] Average Trajectory Length: 895.9209999999999[0m
[36m[2023-07-10 18:08:47,886][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:08:47,892][227910] Reward + Measures: [[-270.98549242    0.17580001    0.27849999    0.24249998    0.31819999]
 [ 417.97030945    0.18450001    0.32440001    0.2007        0.27630001]
 [ 332.10865552    0.17950001    0.35479999    0.20799999    0.28280002]
 ...
 [ 437.4430485     0.20805769    0.35252309    0.20595577    0.30301538]
 [ 312.9405044     0.17120002    0.26570001    0.1823        0.24750002]
 [ -20.46724378    0.2471        0.36939999    0.35420001    0.43920001]][0m
[37m[1m[2023-07-10 18:08:47,893][227910] Max Reward on eval: 946.542596935539[0m
[37m[1m[2023-07-10 18:08:47,893][227910] Min Reward on eval: -1191.073301557079[0m
[37m[1m[2023-07-10 18:08:47,894][227910] Mean Reward across all agents: 160.2422647149328[0m
[37m[1m[2023-07-10 18:08:47,894][227910] Average Trajectory Length: 969.5649999999999[0m
[36m[2023-07-10 18:08:47,898][227910] mean_value=-980.4288120157056, max_value=162.57263971224887[0m
[37m[1m[2023-07-10 18:08:47,903][227910] New mean coefficients: [[ 5.4733257  -1.691714    0.67240757 -2.1059809   5.1413345 ]][0m
[37m[1m[2023-07-10 18:08:47,904][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:08:57,693][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 18:08:57,693][227910] FPS: 392374.53[0m
[36m[2023-07-10 18:08:57,696][227910] itr=1020, itrs=2000, Progress: 51.00%[0m
[37m[1m[2023-07-10 18:09:01,402][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001000[0m
[36m[2023-07-10 18:09:13,186][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 18:09:13,187][227910] FPS: 333592.99[0m
[36m[2023-07-10 18:09:17,905][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:09:17,905][227910] Reward + Measures: [[1069.75133887    0.18253022    0.36533996    0.34692031    0.21548286]][0m
[37m[1m[2023-07-10 18:09:17,906][227910] Max Reward on eval: 1069.7513388700247[0m
[37m[1m[2023-07-10 18:09:17,906][227910] Min Reward on eval: 1069.7513388700247[0m
[37m[1m[2023-07-10 18:09:17,906][227910] Mean Reward across all agents: 1069.7513388700247[0m
[37m[1m[2023-07-10 18:09:17,906][227910] Average Trajectory Length: 882.262[0m
[36m[2023-07-10 18:09:23,402][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:09:23,402][227910] Reward + Measures: [[-326.23800582    0.61780006    0.7374        0.83759993    0.30840001]
 [ 611.12492498    0.1922698     0.28894806    0.26805216    0.25170007]
 [ 956.06684149    0.17460001    0.35600001    0.34689999    0.21039999]
 ...
 [1008.96727919    0.18972437    0.42530799    0.4153266     0.25434262]
 [ 326.386782      0.58060002    0.34399998    0.74260002    0.2066    ]
 [ 888.96589668    0.2155        0.3321        0.30960003    0.30830002]][0m
[37m[1m[2023-07-10 18:09:23,402][227910] Max Reward on eval: 1088.7366865277465[0m
[37m[1m[2023-07-10 18:09:23,403][227910] Min Reward on eval: -476.32233607537347[0m
[37m[1m[2023-07-10 18:09:23,403][227910] Mean Reward across all agents: 503.0054414923198[0m
[37m[1m[2023-07-10 18:09:23,403][227910] Average Trajectory Length: 967.289[0m
[36m[2023-07-10 18:09:23,407][227910] mean_value=-1037.1070641682381, max_value=853.0548227188177[0m
[37m[1m[2023-07-10 18:09:23,409][227910] New mean coefficients: [[ 5.1601367 -1.7481252  1.7509129 -2.0106957  5.796159 ]][0m
[37m[1m[2023-07-10 18:09:23,410][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:09:33,136][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 18:09:33,136][227910] FPS: 394912.34[0m
[36m[2023-07-10 18:09:33,138][227910] itr=1021, itrs=2000, Progress: 51.05%[0m
[36m[2023-07-10 18:09:44,706][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 18:09:44,706][227910] FPS: 332465.07[0m
[36m[2023-07-10 18:09:49,375][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:09:49,376][227910] Reward + Measures: [[332.64190038   0.20431446   0.26802045   0.32833043   0.24673907]][0m
[37m[1m[2023-07-10 18:09:49,376][227910] Max Reward on eval: 332.6419003807177[0m
[37m[1m[2023-07-10 18:09:49,376][227910] Min Reward on eval: 332.6419003807177[0m
[37m[1m[2023-07-10 18:09:49,376][227910] Mean Reward across all agents: 332.6419003807177[0m
[37m[1m[2023-07-10 18:09:49,377][227910] Average Trajectory Length: 964.5319999999999[0m
[36m[2023-07-10 18:09:54,839][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:09:54,839][227910] Reward + Measures: [[-1333.82363356     0.156          0.257          0.26640001
      0.33390003]
 [ -741.55716522     0.15668252     0.33566505     0.24227062
      0.35845733]
 [ -759.11155482     0.14128461     0.33769998     0.25102305
      0.33413845]
 ...
 [-1531.27012466     0.17380419     0.23044871     0.26474294
      0.33364767]
 [-1058.85491548     0.15123677     0.26466396     0.23235536
      0.32790598]
 [  -23.0717045      0.184          0.31880003     0.2809
      0.22730003]][0m
[37m[1m[2023-07-10 18:09:54,839][227910] Max Reward on eval: 496.0655192632112[0m
[37m[1m[2023-07-10 18:09:54,840][227910] Min Reward on eval: -1596.4389591286658[0m
[37m[1m[2023-07-10 18:09:54,840][227910] Mean Reward across all agents: -456.7045093969287[0m
[37m[1m[2023-07-10 18:09:54,840][227910] Average Trajectory Length: 931.3853333333333[0m
[36m[2023-07-10 18:09:54,841][227910] mean_value=-1795.3787418789507, max_value=-141.83060255406693[0m
[36m[2023-07-10 18:09:54,844][227910] XNES is restarting with a new solution whose measures are [0.32860002 0.333      0.09610001 0.61499995] and objective is -208.14703352893702[0m
[36m[2023-07-10 18:09:54,845][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 18:09:54,847][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 18:09:54,848][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:10:04,523][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 18:10:04,524][227910] FPS: 396954.62[0m
[36m[2023-07-10 18:10:04,526][227910] itr=1022, itrs=2000, Progress: 51.10%[0m
[36m[2023-07-10 18:10:16,220][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 18:10:16,221][227910] FPS: 328976.60[0m
[36m[2023-07-10 18:10:20,980][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:10:20,981][227910] Reward + Measures: [[-444.30992318    0.3034077     0.31104502    0.17282601    0.52601904]][0m
[37m[1m[2023-07-10 18:10:20,981][227910] Max Reward on eval: -444.3099231769868[0m
[37m[1m[2023-07-10 18:10:20,981][227910] Min Reward on eval: -444.3099231769868[0m
[37m[1m[2023-07-10 18:10:20,982][227910] Mean Reward across all agents: -444.3099231769868[0m
[37m[1m[2023-07-10 18:10:20,982][227910] Average Trajectory Length: 971.5716666666666[0m
[36m[2023-07-10 18:10:26,477][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:10:26,478][227910] Reward + Measures: [[ -515.94021777     0.4061         0.35410002     0.30700001
      0.5546    ]
 [-1640.8898571      0.0594891      0.17819235     0.10215801
      0.13318749]
 [-1498.82743238     0.1672         0.12340001     0.21829998
      0.17380001]
 ...
 [ -691.38988026     0.23699999     0.4948         0.24080002
      0.72539997]
 [-1947.58681229     0.15349296     0.15683761     0.19259793
      0.21137312]
 [-1381.84941293     0.19642936     0.48454323     0.24356793
      0.61489058]][0m
[37m[1m[2023-07-10 18:10:26,478][227910] Max Reward on eval: -244.4940139567596[0m
[37m[1m[2023-07-10 18:10:26,478][227910] Min Reward on eval: -1947.5868122921324[0m
[37m[1m[2023-07-10 18:10:26,479][227910] Mean Reward across all agents: -982.9493931251785[0m
[37m[1m[2023-07-10 18:10:26,479][227910] Average Trajectory Length: 954.0556666666666[0m
[36m[2023-07-10 18:10:26,480][227910] mean_value=-1531.4900892955968, max_value=-46.865621030870784[0m
[36m[2023-07-10 18:10:26,482][227910] XNES is restarting with a new solution whose measures are [0.92110008 0.75080001 0.55360001 0.60469997] and objective is -78.37471129246987[0m
[36m[2023-07-10 18:10:26,483][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 18:10:26,486][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 18:10:26,486][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:10:36,184][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 18:10:36,184][227910] FPS: 396042.46[0m
[36m[2023-07-10 18:10:36,186][227910] itr=1023, itrs=2000, Progress: 51.15%[0m
[36m[2023-07-10 18:10:47,637][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 18:10:47,638][227910] FPS: 335875.99[0m
[36m[2023-07-10 18:10:52,351][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:10:52,351][227910] Reward + Measures: [[216.2682719    0.95001239   0.7799536    0.92143732   0.12783767]][0m
[37m[1m[2023-07-10 18:10:52,352][227910] Max Reward on eval: 216.26827190209977[0m
[37m[1m[2023-07-10 18:10:52,352][227910] Min Reward on eval: 216.26827190209977[0m
[37m[1m[2023-07-10 18:10:52,352][227910] Mean Reward across all agents: 216.26827190209977[0m
[37m[1m[2023-07-10 18:10:52,352][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 18:10:57,738][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:10:57,744][227910] Reward + Measures: [[ -153.11782793     0.90149993     0.62349999     0.8075
      0.1637    ]
 [-1940.27725753     0.65425968     0.21131492     0.61128509
      0.72531646]
 [-2088.13509686     0.78170002     0.66619998     0.16610001
      0.7834    ]
 ...
 [-1314.44296419     0.87290001     0.71690005     0.21950002
      0.92320007]
 [-2009.09819772     0.63460004     0.1815         0.63169998
      0.68860006]
 [-1665.47118743     0.21126722     0.15689646     0.21886651
      0.22012153]][0m
[37m[1m[2023-07-10 18:10:57,744][227910] Max Reward on eval: 299.43025840166956[0m
[37m[1m[2023-07-10 18:10:57,744][227910] Min Reward on eval: -2604.3157806869363[0m
[37m[1m[2023-07-10 18:10:57,744][227910] Mean Reward across all agents: -1149.9471922773018[0m
[37m[1m[2023-07-10 18:10:57,745][227910] Average Trajectory Length: 974.7933333333333[0m
[36m[2023-07-10 18:10:57,747][227910] mean_value=-1450.2605368590735, max_value=694.5631099302066[0m
[37m[1m[2023-07-10 18:10:57,750][227910] New mean coefficients: [[ 0.15003079 -0.24302739 -0.5313435  -0.67636013 -1.6269722 ]][0m
[37m[1m[2023-07-10 18:10:57,751][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:11:07,335][227910] train() took 9.58 seconds to complete[0m
[36m[2023-07-10 18:11:07,335][227910] FPS: 400738.04[0m
[36m[2023-07-10 18:11:07,337][227910] itr=1024, itrs=2000, Progress: 51.20%[0m
[36m[2023-07-10 18:11:19,006][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 18:11:19,006][227910] FPS: 329603.37[0m
[36m[2023-07-10 18:11:23,837][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:11:23,837][227910] Reward + Measures: [[109.79715606   0.63518971   0.73726201   0.30620897   0.61096328]][0m
[37m[1m[2023-07-10 18:11:23,837][227910] Max Reward on eval: 109.79715606479071[0m
[37m[1m[2023-07-10 18:11:23,837][227910] Min Reward on eval: 109.79715606479071[0m
[37m[1m[2023-07-10 18:11:23,838][227910] Mean Reward across all agents: 109.79715606479071[0m
[37m[1m[2023-07-10 18:11:23,838][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 18:11:29,313][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:11:29,313][227910] Reward + Measures: [[-1310.29229191     0.20449646     0.68578118     0.5786888
      0.65815407]
 [ -406.67473255     0.59199995     0.56400001     0.56850004
      0.23480001]
 [ -580.17582514     0.88139993     0.48069999     0.83390009
      0.38349998]
 ...
 [-1144.74761428     0.36460003     0.2527         0.22970001
      0.21390001]
 [ -588.77897066     0.71340001     0.27770001     0.64590001
      0.35530001]
 [-1163.7421618      0.92140007     0.92859995     0.92820007
      0.94499999]][0m
[37m[1m[2023-07-10 18:11:29,313][227910] Max Reward on eval: 193.17222799810696[0m
[37m[1m[2023-07-10 18:11:29,314][227910] Min Reward on eval: -1866.3666888490784[0m
[37m[1m[2023-07-10 18:11:29,314][227910] Mean Reward across all agents: -695.8176074595378[0m
[37m[1m[2023-07-10 18:11:29,314][227910] Average Trajectory Length: 981.5473333333333[0m
[36m[2023-07-10 18:11:29,317][227910] mean_value=-1044.0680309317931, max_value=605.9008650599443[0m
[37m[1m[2023-07-10 18:11:29,320][227910] New mean coefficients: [[-0.8761628  -0.10952471 -0.5263656  -0.01741403 -2.1847856 ]][0m
[37m[1m[2023-07-10 18:11:29,321][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:11:39,142][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 18:11:39,143][227910] FPS: 391043.04[0m
[36m[2023-07-10 18:11:39,145][227910] itr=1025, itrs=2000, Progress: 51.25%[0m
[36m[2023-07-10 18:11:50,807][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 18:11:50,807][227910] FPS: 329862.28[0m
[36m[2023-07-10 18:11:55,612][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:11:55,613][227910] Reward + Measures: [[134.05269125   0.65136695   0.70979369   0.44388434   0.43334532]][0m
[37m[1m[2023-07-10 18:11:55,613][227910] Max Reward on eval: 134.0526912518783[0m
[37m[1m[2023-07-10 18:11:55,613][227910] Min Reward on eval: 134.0526912518783[0m
[37m[1m[2023-07-10 18:11:55,614][227910] Mean Reward across all agents: 134.0526912518783[0m
[37m[1m[2023-07-10 18:11:55,614][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 18:12:01,151][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:12:01,152][227910] Reward + Measures: [[ -476.78681381     0.92989999     0.80690002     0.88430005
      0.0782    ]
 [-1464.75600485     0.44899997     0.62939996     0.22570001
      0.62079996]
 [ -960.83965296     0.59530002     0.7755         0.57190001
      0.76109999]
 ...
 [ -840.76131953     0.54280007     0.611          0.1858
      0.58070004]
 [-1211.37397553     0.33969998     0.579          0.2617
      0.47480002]
 [ -435.12742553     0.63190001     0.65880001     0.60670006
      0.52700001]][0m
[37m[1m[2023-07-10 18:12:01,152][227910] Max Reward on eval: 144.26599995392607[0m
[37m[1m[2023-07-10 18:12:01,152][227910] Min Reward on eval: -1825.3579391629669[0m
[37m[1m[2023-07-10 18:12:01,153][227910] Mean Reward across all agents: -512.4348175516229[0m
[37m[1m[2023-07-10 18:12:01,153][227910] Average Trajectory Length: 991.3256666666666[0m
[36m[2023-07-10 18:12:01,156][227910] mean_value=-841.3826700134395, max_value=473.3153087796438[0m
[37m[1m[2023-07-10 18:12:01,159][227910] New mean coefficients: [[-0.8816849   0.36248308 -0.07487619 -0.38510153 -2.313072  ]][0m
[37m[1m[2023-07-10 18:12:01,160][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:12:10,822][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 18:12:10,822][227910] FPS: 397498.82[0m
[36m[2023-07-10 18:12:10,825][227910] itr=1026, itrs=2000, Progress: 51.30%[0m
[36m[2023-07-10 18:12:22,473][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 18:12:22,473][227910] FPS: 330211.76[0m
[36m[2023-07-10 18:12:27,265][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:12:27,266][227910] Reward + Measures: [[-736.14472232    0.78137839    0.64764297    0.58636862    0.14479932]][0m
[37m[1m[2023-07-10 18:12:27,266][227910] Max Reward on eval: -736.1447223167158[0m
[37m[1m[2023-07-10 18:12:27,266][227910] Min Reward on eval: -736.1447223167158[0m
[37m[1m[2023-07-10 18:12:27,267][227910] Mean Reward across all agents: -736.1447223167158[0m
[37m[1m[2023-07-10 18:12:27,267][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 18:12:32,723][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:12:32,724][227910] Reward + Measures: [[ -989.23884279     0.7044         0.65790004     0.56279999
      0.24890001]
 [-1030.68399305     0.72530001     0.67079997     0.57510006
      0.55220002]
 [-1179.75377945     0.60817826     0.29753914     0.54210001
      0.27932611]
 ...
 [ -527.49976006     0.75459999     0.5086         0.69929999
      0.0949    ]
 [ -699.16833725     0.79539996     0.61309999     0.58849996
      0.12199999]
 [ -558.68450589     0.8071         0.51850003     0.69940001
      0.1419    ]][0m
[37m[1m[2023-07-10 18:12:32,724][227910] Max Reward on eval: -468.0391887089587[0m
[37m[1m[2023-07-10 18:12:32,724][227910] Min Reward on eval: -1651.6844646477489[0m
[37m[1m[2023-07-10 18:12:32,725][227910] Mean Reward across all agents: -896.2087398062274[0m
[37m[1m[2023-07-10 18:12:32,725][227910] Average Trajectory Length: 997.4833333333333[0m
[36m[2023-07-10 18:12:32,726][227910] mean_value=-677.0516720446268, max_value=-53.771245612492066[0m
[36m[2023-07-10 18:12:32,729][227910] XNES is restarting with a new solution whose measures are [0.48850003 0.57100004 0.22089998 0.22790001] and objective is 2209.149008382438[0m
[36m[2023-07-10 18:12:32,730][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 18:12:32,732][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 18:12:32,733][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:12:42,400][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 18:12:42,400][227910] FPS: 397274.80[0m
[36m[2023-07-10 18:12:42,403][227910] itr=1027, itrs=2000, Progress: 51.35%[0m
[36m[2023-07-10 18:12:54,175][227910] train() took 11.75 seconds to complete[0m
[36m[2023-07-10 18:12:54,175][227910] FPS: 326732.41[0m
[36m[2023-07-10 18:12:58,997][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:12:58,998][227910] Reward + Measures: [[2326.11710003    0.49184626    0.54109943    0.21551676    0.2209447 ]][0m
[37m[1m[2023-07-10 18:12:58,998][227910] Max Reward on eval: 2326.1171000290915[0m
[37m[1m[2023-07-10 18:12:58,998][227910] Min Reward on eval: 2326.1171000290915[0m
[37m[1m[2023-07-10 18:12:58,999][227910] Mean Reward across all agents: 2326.1171000290915[0m
[37m[1m[2023-07-10 18:12:58,999][227910] Average Trajectory Length: 999.7803333333333[0m
[36m[2023-07-10 18:13:04,454][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:13:04,459][227910] Reward + Measures: [[-184.02415449    0.4316        0.6358        0.48460004    0.39430001]
 [ -24.26366194    0.39140001    0.54580003    0.4542        0.44770002]
 [-858.17800823    0.59399658    0.41943446    0.54228622    0.16756207]
 ...
 [-350.57262013    0.40200001    0.4219        0.32860002    0.2282    ]
 [-944.91382775    0.39031538    0.45582351    0.47229686    0.42616183]
 [-951.17946102    0.22000001    0.40949997    0.2832        0.40170002]][0m
[37m[1m[2023-07-10 18:13:04,460][227910] Max Reward on eval: 1312.3513008212205[0m
[37m[1m[2023-07-10 18:13:04,460][227910] Min Reward on eval: -1759.9340083320626[0m
[37m[1m[2023-07-10 18:13:04,460][227910] Mean Reward across all agents: -506.58458161606006[0m
[37m[1m[2023-07-10 18:13:04,461][227910] Average Trajectory Length: 931.2966666666666[0m
[36m[2023-07-10 18:13:04,462][227910] mean_value=-2123.91769187699, max_value=992.5375138237047[0m
[37m[1m[2023-07-10 18:13:04,465][227910] New mean coefficients: [[ 0.278724   -0.35512808 -0.60573316 -2.073658   -0.82989675]][0m
[37m[1m[2023-07-10 18:13:04,466][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:13:14,215][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 18:13:14,215][227910] FPS: 393950.42[0m
[36m[2023-07-10 18:13:14,217][227910] itr=1028, itrs=2000, Progress: 51.40%[0m
[36m[2023-07-10 18:13:25,763][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 18:13:25,763][227910] FPS: 333124.92[0m
[36m[2023-07-10 18:13:30,527][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:13:30,533][227910] Reward + Measures: [[950.7199091    0.34704301   0.39691466   0.30581567   0.19222933]][0m
[37m[1m[2023-07-10 18:13:30,534][227910] Max Reward on eval: 950.719909103334[0m
[37m[1m[2023-07-10 18:13:30,535][227910] Min Reward on eval: 950.719909103334[0m
[37m[1m[2023-07-10 18:13:30,535][227910] Mean Reward across all agents: 950.719909103334[0m
[37m[1m[2023-07-10 18:13:30,536][227910] Average Trajectory Length: 951.361[0m
[36m[2023-07-10 18:13:35,980][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:13:35,986][227910] Reward + Measures: [[-1234.83891422     0.49489623     0.30925438     0.46028095
      0.43024966]
 [ -404.59194934     0.25705716     0.13837144     0.2544286
      0.23144285]
 [  -10.3752695      0.47102657     0.31342801     0.37041566
      0.46267319]
 ...
 [ -130.73754142     0.49970004     0.58250004     0.50089997
      0.5165    ]
 [-1137.92630979     0.32444927     0.2523213      0.32777184
      0.26697847]
 [  -73.34574275     0.51125783     0.52090549     0.50930548
      0.45694038]][0m
[37m[1m[2023-07-10 18:13:35,986][227910] Max Reward on eval: 1533.4385211186716[0m
[37m[1m[2023-07-10 18:13:35,986][227910] Min Reward on eval: -1729.9242853811943[0m
[37m[1m[2023-07-10 18:13:35,986][227910] Mean Reward across all agents: -174.6300569877518[0m
[37m[1m[2023-07-10 18:13:35,987][227910] Average Trajectory Length: 875.2046666666666[0m
[36m[2023-07-10 18:13:35,989][227910] mean_value=-2192.2921694801817, max_value=785.9920154570689[0m
[37m[1m[2023-07-10 18:13:35,991][227910] New mean coefficients: [[ 0.06353799  0.5488354  -0.93294644 -0.9933572  -0.8381792 ]][0m
[37m[1m[2023-07-10 18:13:35,992][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:13:45,673][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 18:13:45,674][227910] FPS: 396716.35[0m
[36m[2023-07-10 18:13:45,676][227910] itr=1029, itrs=2000, Progress: 51.45%[0m
[36m[2023-07-10 18:13:57,186][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 18:13:57,186][227910] FPS: 334144.66[0m
[36m[2023-07-10 18:14:01,903][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:14:01,908][227910] Reward + Measures: [[853.86223756   0.32074752   0.35973129   0.27511844   0.17130955]][0m
[37m[1m[2023-07-10 18:14:01,909][227910] Max Reward on eval: 853.8622375643935[0m
[37m[1m[2023-07-10 18:14:01,909][227910] Min Reward on eval: 853.8622375643935[0m
[37m[1m[2023-07-10 18:14:01,909][227910] Mean Reward across all agents: 853.8622375643935[0m
[37m[1m[2023-07-10 18:14:01,910][227910] Average Trajectory Length: 905.808[0m
[36m[2023-07-10 18:14:07,371][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:14:07,371][227910] Reward + Measures: [[-638.89407376    0.67460006    0.64000005    0.70719999    0.0868    ]
 [-145.58109517    0.24300852    0.28846171    0.2825745     0.24802728]
 [ 246.35486827    0.26908478    0.32455426    0.28687808    0.21131335]
 ...
 [1048.30939488    0.3691425     0.45739278    0.27092943    0.20751503]
 [ -34.35024494    0.1507        0.71309996    0.56050003    0.4632    ]
 [ 651.77540918    0.43169999    0.39410001    0.33680001    0.13250001]][0m
[37m[1m[2023-07-10 18:14:07,372][227910] Max Reward on eval: 1167.6477300715692[0m
[37m[1m[2023-07-10 18:14:07,372][227910] Min Reward on eval: -1736.806448940793[0m
[37m[1m[2023-07-10 18:14:07,372][227910] Mean Reward across all agents: 103.86954221201357[0m
[37m[1m[2023-07-10 18:14:07,372][227910] Average Trajectory Length: 959.1483333333333[0m
[36m[2023-07-10 18:14:07,374][227910] mean_value=-1704.4618952338587, max_value=700.949052694998[0m
[37m[1m[2023-07-10 18:14:07,377][227910] New mean coefficients: [[ 0.84050703  0.41333163 -0.9950258  -1.7225628  -0.53583455]][0m
[37m[1m[2023-07-10 18:14:07,378][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:14:17,138][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 18:14:17,138][227910] FPS: 393483.77[0m
[36m[2023-07-10 18:14:17,141][227910] itr=1030, itrs=2000, Progress: 51.50%[0m
[37m[1m[2023-07-10 18:14:21,115][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001010[0m
[36m[2023-07-10 18:14:33,191][227910] train() took 11.79 seconds to complete[0m
[36m[2023-07-10 18:14:33,191][227910] FPS: 325751.07[0m
[36m[2023-07-10 18:14:37,904][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:14:37,910][227910] Reward + Measures: [[810.14446571   0.29858929   0.32961243   0.25690266   0.15037584]][0m
[37m[1m[2023-07-10 18:14:37,910][227910] Max Reward on eval: 810.1444657128925[0m
[37m[1m[2023-07-10 18:14:37,910][227910] Min Reward on eval: 810.1444657128925[0m
[37m[1m[2023-07-10 18:14:37,911][227910] Mean Reward across all agents: 810.1444657128925[0m
[37m[1m[2023-07-10 18:14:37,911][227910] Average Trajectory Length: 878.7909999999999[0m
[36m[2023-07-10 18:14:43,365][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:14:43,366][227910] Reward + Measures: [[-932.55277261    0.75209999    0.11900001    0.78389996    0.23819999]
 [1262.563665      0.2678        0.35390002    0.39250001    0.21799998]
 [-485.64506976    0.50870699    0.20353146    0.60589093    0.13978462]
 ...
 [-272.23305437    0.2687        0.17019999    0.1939        0.20460001]
 [ 685.76091669    0.51350003    0.45559999    0.27320001    0.2441    ]
 [ 454.51178678    0.2933        0.55360001    0.35209998    0.41220003]][0m
[37m[1m[2023-07-10 18:14:43,366][227910] Max Reward on eval: 1297.3489027479663[0m
[37m[1m[2023-07-10 18:14:43,366][227910] Min Reward on eval: -1921.6606937812874[0m
[37m[1m[2023-07-10 18:14:43,367][227910] Mean Reward across all agents: -239.11548264061193[0m
[37m[1m[2023-07-10 18:14:43,367][227910] Average Trajectory Length: 943.3046666666667[0m
[36m[2023-07-10 18:14:43,369][227910] mean_value=-2187.2408069591665, max_value=842.6860701275547[0m
[37m[1m[2023-07-10 18:14:43,371][227910] New mean coefficients: [[ 1.356976   -0.5273215  -0.47138596  0.02068627 -0.07445309]][0m
[37m[1m[2023-07-10 18:14:43,372][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:14:53,006][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 18:14:53,006][227910] FPS: 398655.05[0m
[36m[2023-07-10 18:14:53,008][227910] itr=1031, itrs=2000, Progress: 51.55%[0m
[36m[2023-07-10 18:15:04,544][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 18:15:04,544][227910] FPS: 333446.76[0m
[36m[2023-07-10 18:15:09,317][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:15:09,317][227910] Reward + Measures: [[934.13414306   0.30276459   0.33295059   0.27178779   0.14815032]][0m
[37m[1m[2023-07-10 18:15:09,317][227910] Max Reward on eval: 934.1341430593378[0m
[37m[1m[2023-07-10 18:15:09,317][227910] Min Reward on eval: 934.1341430593378[0m
[37m[1m[2023-07-10 18:15:09,318][227910] Mean Reward across all agents: 934.1341430593378[0m
[37m[1m[2023-07-10 18:15:09,318][227910] Average Trajectory Length: 911.1526666666666[0m
[36m[2023-07-10 18:15:14,860][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:15:14,861][227910] Reward + Measures: [[  343.99337986     0.22860323     0.4181129      0.26304516
      0.30955806]
 [ -378.06255564     0.80760002     0.91399997     0.84180003
      0.91340011]
 [ -352.02959218     0.54370004     0.81100005     0.53859997
      0.6002    ]
 ...
 [  554.94465196     0.22335282     0.3695474      0.28656808
      0.22519827]
 [-1242.55395993     0.16452105     0.18360877     0.16544035
      0.10545263]
 [-1202.294461       0.64700001     0.73220003     0.6796
      0.73089999]][0m
[37m[1m[2023-07-10 18:15:14,861][227910] Max Reward on eval: 1198.5739829549682[0m
[37m[1m[2023-07-10 18:15:14,861][227910] Min Reward on eval: -1670.819050693605[0m
[37m[1m[2023-07-10 18:15:14,862][227910] Mean Reward across all agents: -207.19338344472422[0m
[37m[1m[2023-07-10 18:15:14,862][227910] Average Trajectory Length: 955.3793333333333[0m
[36m[2023-07-10 18:15:14,864][227910] mean_value=-1909.4088916236244, max_value=878.017011496929[0m
[37m[1m[2023-07-10 18:15:14,866][227910] New mean coefficients: [[ 1.3569273   0.22432464 -0.40267867  0.4417111  -1.1890122 ]][0m
[37m[1m[2023-07-10 18:15:14,867][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:15:24,748][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 18:15:24,749][227910] FPS: 388687.37[0m
[36m[2023-07-10 18:15:24,751][227910] itr=1032, itrs=2000, Progress: 51.60%[0m
[36m[2023-07-10 18:15:36,517][227910] train() took 11.75 seconds to complete[0m
[36m[2023-07-10 18:15:36,517][227910] FPS: 326880.07[0m
[36m[2023-07-10 18:15:41,328][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:15:41,329][227910] Reward + Measures: [[1014.46312866    0.31224632    0.33565286    0.28336564    0.14185908]][0m
[37m[1m[2023-07-10 18:15:41,329][227910] Max Reward on eval: 1014.4631286564215[0m
[37m[1m[2023-07-10 18:15:41,329][227910] Min Reward on eval: 1014.4631286564215[0m
[37m[1m[2023-07-10 18:15:41,329][227910] Mean Reward across all agents: 1014.4631286564215[0m
[37m[1m[2023-07-10 18:15:41,330][227910] Average Trajectory Length: 917.8253333333333[0m
[36m[2023-07-10 18:15:46,997][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:15:46,998][227910] Reward + Measures: [[ -540.2237371      0.57380003     0.48869997     0.2969
      0.37330005]
 [  523.87296117     0.36450002     0.30920002     0.4086
      0.16129999]
 [ -697.4178519      0.18699537     0.26264793     0.27358744
      0.12362838]
 ...
 [ -257.7357028      0.34502399     0.33698484     0.26940835
      0.3104341 ]
 [-1326.60689478     0.51470006     0.4571         0.3628
      0.38820001]
 [ -155.44922231     0.25365943     0.2424279      0.19189358
      0.23174457]][0m
[37m[1m[2023-07-10 18:15:46,998][227910] Max Reward on eval: 1645.8464434383554[0m
[37m[1m[2023-07-10 18:15:46,998][227910] Min Reward on eval: -1815.581282722042[0m
[37m[1m[2023-07-10 18:15:46,999][227910] Mean Reward across all agents: -194.0527249822845[0m
[37m[1m[2023-07-10 18:15:46,999][227910] Average Trajectory Length: 924.2513333333333[0m
[36m[2023-07-10 18:15:47,001][227910] mean_value=-2086.1519895150122, max_value=525.985055040521[0m
[37m[1m[2023-07-10 18:15:47,003][227910] New mean coefficients: [[ 1.2650386   0.33634755 -0.04673421  0.10929668 -2.2341623 ]][0m
[37m[1m[2023-07-10 18:15:47,004][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:15:56,881][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 18:15:56,882][227910] FPS: 388836.36[0m
[36m[2023-07-10 18:15:56,884][227910] itr=1033, itrs=2000, Progress: 51.65%[0m
[36m[2023-07-10 18:16:08,523][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 18:16:08,523][227910] FPS: 330507.66[0m
[36m[2023-07-10 18:16:13,353][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:16:13,354][227910] Reward + Measures: [[1100.08960476    0.31025699    0.3361336     0.28916374    0.13514352]][0m
[37m[1m[2023-07-10 18:16:13,354][227910] Max Reward on eval: 1100.089604761055[0m
[37m[1m[2023-07-10 18:16:13,354][227910] Min Reward on eval: 1100.089604761055[0m
[37m[1m[2023-07-10 18:16:13,354][227910] Mean Reward across all agents: 1100.089604761055[0m
[37m[1m[2023-07-10 18:16:13,355][227910] Average Trajectory Length: 926.496[0m
[36m[2023-07-10 18:16:18,783][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:16:18,843][227910] Reward + Measures: [[ -865.2456541      0.36310002     0.3046         0.46829996
      0.27359998]
 [ -488.10769953     0.35856923     0.26097474     0.46595827
      0.25786814]
 [  370.5198459      0.32102498     0.37154749     0.28816113
      0.10906676]
 ...
 [ 1002.99189865     0.3795         0.35050002     0.32520002
      0.20739999]
 [ -613.69184617     0.23509446     0.56202286     0.36345518
      0.34426656]
 [-1207.44122161     0.70090002     0.54000008     0.68049997
      0.0757    ]][0m
[37m[1m[2023-07-10 18:16:18,844][227910] Max Reward on eval: 1585.0178615773098[0m
[37m[1m[2023-07-10 18:16:18,844][227910] Min Reward on eval: -1770.9921311987564[0m
[37m[1m[2023-07-10 18:16:18,844][227910] Mean Reward across all agents: -90.37506181770406[0m
[37m[1m[2023-07-10 18:16:18,844][227910] Average Trajectory Length: 960.0373333333333[0m
[36m[2023-07-10 18:16:18,846][227910] mean_value=-1749.384848613274, max_value=539.4432392905744[0m
[37m[1m[2023-07-10 18:16:18,849][227910] New mean coefficients: [[ 1.9632862  -1.2467688   0.2761339   0.37255704 -1.2948117 ]][0m
[37m[1m[2023-07-10 18:16:18,850][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:16:28,568][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 18:16:28,568][227910] FPS: 395205.21[0m
[36m[2023-07-10 18:16:28,570][227910] itr=1034, itrs=2000, Progress: 51.70%[0m
[36m[2023-07-10 18:16:40,170][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 18:16:40,170][227910] FPS: 331567.88[0m
[36m[2023-07-10 18:16:45,081][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:16:45,082][227910] Reward + Measures: [[1239.88524331    0.31631237    0.34327888    0.2981194     0.12926003]][0m
[37m[1m[2023-07-10 18:16:45,082][227910] Max Reward on eval: 1239.8852433126262[0m
[37m[1m[2023-07-10 18:16:45,082][227910] Min Reward on eval: 1239.8852433126262[0m
[37m[1m[2023-07-10 18:16:45,082][227910] Mean Reward across all agents: 1239.8852433126262[0m
[37m[1m[2023-07-10 18:16:45,083][227910] Average Trajectory Length: 937.2136666666667[0m
[36m[2023-07-10 18:16:50,650][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:16:50,651][227910] Reward + Measures: [[ 919.06061061    0.25077161    0.30782774    0.2599017     0.12898956]
 [-332.25004993    0.53619999    0.39480001    0.27980003    0.33219999]
 [-329.6613608     0.31700376    0.23102455    0.29700568    0.14480378]
 ...
 [-136.56114483    0.52579999    0.41560003    0.53920001    0.34299999]
 [-748.25632703    0.21181443    0.20377187    0.18645082    0.12232792]
 [1225.1296824     0.34180516    0.40236148    0.28672066    0.1692446 ]][0m
[37m[1m[2023-07-10 18:16:50,651][227910] Max Reward on eval: 1662.3946287014521[0m
[37m[1m[2023-07-10 18:16:50,652][227910] Min Reward on eval: -2024.8377379257931[0m
[37m[1m[2023-07-10 18:16:50,652][227910] Mean Reward across all agents: 124.74573816561596[0m
[37m[1m[2023-07-10 18:16:50,652][227910] Average Trajectory Length: 957.3983333333333[0m
[36m[2023-07-10 18:16:50,654][227910] mean_value=-2211.3660264835225, max_value=1014.2573259330286[0m
[37m[1m[2023-07-10 18:16:50,657][227910] New mean coefficients: [[ 1.9493656  -0.63881314  0.520095    1.2698734  -0.6367224 ]][0m
[37m[1m[2023-07-10 18:16:50,658][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:17:00,414][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 18:17:00,414][227910] FPS: 393656.27[0m
[36m[2023-07-10 18:17:00,416][227910] itr=1035, itrs=2000, Progress: 51.75%[0m
[36m[2023-07-10 18:17:12,102][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 18:17:12,102][227910] FPS: 329133.84[0m
[36m[2023-07-10 18:17:16,895][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:17:16,896][227910] Reward + Measures: [[1602.85443017    0.30527306    0.26757967    0.3286078     0.12459612]][0m
[37m[1m[2023-07-10 18:17:16,896][227910] Max Reward on eval: 1602.854430165741[0m
[37m[1m[2023-07-10 18:17:16,896][227910] Min Reward on eval: 1602.854430165741[0m
[37m[1m[2023-07-10 18:17:16,896][227910] Mean Reward across all agents: 1602.854430165741[0m
[37m[1m[2023-07-10 18:17:16,897][227910] Average Trajectory Length: 966.6513333333334[0m
[36m[2023-07-10 18:17:22,352][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:17:22,353][227910] Reward + Measures: [[-384.39502053    0.34279999    0.30060002    0.42770001    0.2951    ]
 [1274.91402759    0.33699998    0.33160001    0.27590001    0.2307    ]
 [-158.30944018    0.69240004    0.1097        0.50050002    0.26070002]
 ...
 [-506.63002777    0.60810006    0.13590001    0.5575        0.2122    ]
 [-167.53194386    0.53175914    0.27502355    0.48180613    0.50756341]
 [-660.33280104    0.2349145     0.2739301     0.16438659    0.2233814 ]][0m
[37m[1m[2023-07-10 18:17:22,353][227910] Max Reward on eval: 1614.4993896290428[0m
[37m[1m[2023-07-10 18:17:22,353][227910] Min Reward on eval: -1357.8316961276462[0m
[37m[1m[2023-07-10 18:17:22,353][227910] Mean Reward across all agents: -74.4876237035299[0m
[37m[1m[2023-07-10 18:17:22,353][227910] Average Trajectory Length: 963.3373333333333[0m
[36m[2023-07-10 18:17:22,357][227910] mean_value=-1412.0725476923533, max_value=919.6781136020436[0m
[37m[1m[2023-07-10 18:17:22,359][227910] New mean coefficients: [[ 2.3197067  -0.8589764   0.07877693  2.0728273   0.69238335]][0m
[37m[1m[2023-07-10 18:17:22,360][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:17:32,035][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 18:17:32,035][227910] FPS: 396974.51[0m
[36m[2023-07-10 18:17:32,038][227910] itr=1036, itrs=2000, Progress: 51.80%[0m
[36m[2023-07-10 18:17:43,660][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 18:17:43,660][227910] FPS: 330946.80[0m
[36m[2023-07-10 18:17:48,482][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:17:48,483][227910] Reward + Measures: [[1774.37655706    0.29614773    0.25970781    0.31393346    0.12235662]][0m
[37m[1m[2023-07-10 18:17:48,483][227910] Max Reward on eval: 1774.3765570594246[0m
[37m[1m[2023-07-10 18:17:48,483][227910] Min Reward on eval: 1774.3765570594246[0m
[37m[1m[2023-07-10 18:17:48,483][227910] Mean Reward across all agents: 1774.3765570594246[0m
[37m[1m[2023-07-10 18:17:48,484][227910] Average Trajectory Length: 966.648[0m
[36m[2023-07-10 18:17:54,205][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:17:54,206][227910] Reward + Measures: [[ 1551.83722691     0.28850874     0.27337295     0.33930704
      0.12172078]
 [  577.22197832     0.32282898     0.59328264     0.2371348
      0.47329274]
 [ 1001.12142025     0.35179999     0.31650001     0.45249996
      0.1214    ]
 ...
 [ 1251.84165417     0.30050001     0.2577         0.3536
      0.10450001]
 [  384.47293731     0.23825835     0.65216666     0.30720001
      0.57812494]
 [-1258.79609511     0.20385264     0.22580805     0.21850191
      0.15056643]][0m
[37m[1m[2023-07-10 18:17:54,206][227910] Max Reward on eval: 1899.1497650274425[0m
[37m[1m[2023-07-10 18:17:54,206][227910] Min Reward on eval: -1531.4907824631664[0m
[37m[1m[2023-07-10 18:17:54,206][227910] Mean Reward across all agents: 451.4190279664625[0m
[37m[1m[2023-07-10 18:17:54,207][227910] Average Trajectory Length: 977.3876666666666[0m
[36m[2023-07-10 18:17:54,208][227910] mean_value=-2142.5382801838773, max_value=1109.5601743252482[0m
[37m[1m[2023-07-10 18:17:54,211][227910] New mean coefficients: [[ 2.4951525  -0.5399946  -0.43187717 -0.08898687  0.5825916 ]][0m
[37m[1m[2023-07-10 18:17:54,212][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:18:03,986][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 18:18:03,986][227910] FPS: 392935.20[0m
[36m[2023-07-10 18:18:03,988][227910] itr=1037, itrs=2000, Progress: 51.85%[0m
[36m[2023-07-10 18:18:15,708][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 18:18:15,708][227910] FPS: 328180.88[0m
[36m[2023-07-10 18:18:20,558][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:18:20,559][227910] Reward + Measures: [[1915.72456921    0.28743657    0.25719261    0.30596226    0.12404272]][0m
[37m[1m[2023-07-10 18:18:20,559][227910] Max Reward on eval: 1915.724569212534[0m
[37m[1m[2023-07-10 18:18:20,559][227910] Min Reward on eval: 1915.724569212534[0m
[37m[1m[2023-07-10 18:18:20,559][227910] Mean Reward across all agents: 1915.724569212534[0m
[37m[1m[2023-07-10 18:18:20,560][227910] Average Trajectory Length: 959.972[0m
[36m[2023-07-10 18:18:26,077][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:18:26,078][227910] Reward + Measures: [[2060.39677471    0.30420002    0.27599999    0.34770003    0.1295    ]
 [1695.67987739    0.3272        0.229         0.32830003    0.108     ]
 [1503.72963145    0.33340001    0.3434        0.29800001    0.20410001]
 ...
 [1056.39695002    0.23043971    0.40199214    0.37446615    0.21933715]
 [1725.08452871    0.2379        0.26440001    0.30680001    0.1245    ]
 [1546.40431063    0.36570001    0.22670002    0.32820001    0.1243    ]][0m
[37m[1m[2023-07-10 18:18:26,078][227910] Max Reward on eval: 2122.155171120749[0m
[37m[1m[2023-07-10 18:18:26,079][227910] Min Reward on eval: 708.9563115207129[0m
[37m[1m[2023-07-10 18:18:26,079][227910] Mean Reward across all agents: 1667.3754400390812[0m
[37m[1m[2023-07-10 18:18:26,079][227910] Average Trajectory Length: 951.461[0m
[36m[2023-07-10 18:18:26,080][227910] mean_value=-2303.480100552956, max_value=321.06070494095104[0m
[37m[1m[2023-07-10 18:18:26,083][227910] New mean coefficients: [[ 2.6617026  -0.09999076 -0.42273903  1.0811701   1.0673773 ]][0m
[37m[1m[2023-07-10 18:18:26,084][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:18:36,076][227910] train() took 9.99 seconds to complete[0m
[36m[2023-07-10 18:18:36,076][227910] FPS: 384355.40[0m
[36m[2023-07-10 18:18:36,079][227910] itr=1038, itrs=2000, Progress: 51.90%[0m
[36m[2023-07-10 18:18:47,694][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 18:18:47,694][227910] FPS: 331132.39[0m
[36m[2023-07-10 18:18:52,580][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:18:52,581][227910] Reward + Measures: [[2056.76897019    0.28448263    0.25117752    0.30711982    0.12221713]][0m
[37m[1m[2023-07-10 18:18:52,581][227910] Max Reward on eval: 2056.768970186632[0m
[37m[1m[2023-07-10 18:18:52,581][227910] Min Reward on eval: 2056.768970186632[0m
[37m[1m[2023-07-10 18:18:52,581][227910] Mean Reward across all agents: 2056.768970186632[0m
[37m[1m[2023-07-10 18:18:52,582][227910] Average Trajectory Length: 971.9533333333333[0m
[36m[2023-07-10 18:18:58,098][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:18:58,099][227910] Reward + Measures: [[1952.33813111    0.3190507     0.25683966    0.31296808    0.12154807]
 [2078.02880029    0.31619999    0.29699999    0.33179998    0.1355    ]
 [1732.7357211     0.27035555    0.24806666    0.3123        0.11596667]
 ...
 [2148.06646222    0.29990003    0.28700003    0.32690001    0.1382    ]
 [1775.48379077    0.2967        0.29550001    0.3238        0.1471    ]
 [2134.25672048    0.29539999    0.27610001    0.32339999    0.13000001]][0m
[37m[1m[2023-07-10 18:18:58,099][227910] Max Reward on eval: 2273.1425372770755[0m
[37m[1m[2023-07-10 18:18:58,099][227910] Min Reward on eval: 798.2601544636069[0m
[37m[1m[2023-07-10 18:18:58,100][227910] Mean Reward across all agents: 1858.4450005990907[0m
[37m[1m[2023-07-10 18:18:58,100][227910] Average Trajectory Length: 978.9586666666667[0m
[36m[2023-07-10 18:18:58,101][227910] mean_value=-1966.910743786986, max_value=570.0796827350061[0m
[37m[1m[2023-07-10 18:18:58,103][227910] New mean coefficients: [[ 3.562719    0.34447688 -0.96234953  1.5633487   1.4550922 ]][0m
[37m[1m[2023-07-10 18:18:58,104][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:19:07,877][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 18:19:07,878][227910] FPS: 392989.07[0m
[36m[2023-07-10 18:19:07,880][227910] itr=1039, itrs=2000, Progress: 51.95%[0m
[36m[2023-07-10 18:19:19,546][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 18:19:19,546][227910] FPS: 329681.67[0m
[36m[2023-07-10 18:19:24,320][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:19:24,320][227910] Reward + Measures: [[2204.07356904    0.27782667    0.24859469    0.30241415    0.12329399]][0m
[37m[1m[2023-07-10 18:19:24,321][227910] Max Reward on eval: 2204.0735690401034[0m
[37m[1m[2023-07-10 18:19:24,321][227910] Min Reward on eval: 2204.0735690401034[0m
[37m[1m[2023-07-10 18:19:24,321][227910] Mean Reward across all agents: 2204.0735690401034[0m
[37m[1m[2023-07-10 18:19:24,321][227910] Average Trajectory Length: 970.0373333333333[0m
[36m[2023-07-10 18:19:29,792][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:19:29,798][227910] Reward + Measures: [[-1090.98020612     0.66350001     0.5575         0.73799998
      0.1671    ]
 [ -461.80330486     0.63160002     0.78330004     0.25850001
      0.77240002]
 [  532.51145137     0.38890001     0.45070001     0.48890001
      0.21630001]
 ...
 [ -345.35450917     0.40299997     0.51019996     0.35450003
      0.56640005]
 [  118.99833396     0.25229999     0.38940004     0.3369
      0.19719999]
 [-1344.93753925     0.40030003     0.29240003     0.44520003
      0.2493    ]][0m
[37m[1m[2023-07-10 18:19:29,798][227910] Max Reward on eval: 2348.583028169419[0m
[37m[1m[2023-07-10 18:19:29,798][227910] Min Reward on eval: -1700.0345566462668[0m
[37m[1m[2023-07-10 18:19:29,799][227910] Mean Reward across all agents: -77.53411573753365[0m
[37m[1m[2023-07-10 18:19:29,799][227910] Average Trajectory Length: 980.308[0m
[36m[2023-07-10 18:19:29,801][227910] mean_value=-1544.3540994716802, max_value=964.8820691259616[0m
[37m[1m[2023-07-10 18:19:29,804][227910] New mean coefficients: [[ 3.3242815   0.31122202 -1.9636054   1.4174597   0.49344093]][0m
[37m[1m[2023-07-10 18:19:29,805][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:19:39,590][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 18:19:39,590][227910] FPS: 392510.55[0m
[36m[2023-07-10 18:19:39,593][227910] itr=1040, itrs=2000, Progress: 52.00%[0m
[37m[1m[2023-07-10 18:19:43,483][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001020[0m
[36m[2023-07-10 18:19:55,292][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 18:19:55,293][227910] FPS: 332550.99[0m
[36m[2023-07-10 18:20:00,168][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:20:00,168][227910] Reward + Measures: [[2331.9018766     0.27942556    0.2426495     0.29885525    0.12123334]][0m
[37m[1m[2023-07-10 18:20:00,169][227910] Max Reward on eval: 2331.9018765960427[0m
[37m[1m[2023-07-10 18:20:00,169][227910] Min Reward on eval: 2331.9018765960427[0m
[37m[1m[2023-07-10 18:20:00,169][227910] Mean Reward across all agents: 2331.9018765960427[0m
[37m[1m[2023-07-10 18:20:00,169][227910] Average Trajectory Length: 967.8573333333333[0m
[36m[2023-07-10 18:20:05,747][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:20:05,748][227910] Reward + Measures: [[2142.54691038    0.32300004    0.25          0.30849999    0.13590001]
 [1597.42592999    0.3127        0.20810001    0.32100001    0.1331    ]
 [1802.83002371    0.29639292    0.23005758    0.32801315    0.1406313 ]
 ...
 [2176.46466357    0.2784        0.21929999    0.33700004    0.15280001]
 [1849.18597862    0.3642        0.2386        0.34150001    0.13950001]
 [2062.51669094    0.29239997    0.233         0.31440002    0.12890001]][0m
[37m[1m[2023-07-10 18:20:05,748][227910] Max Reward on eval: 2469.057419436588[0m
[37m[1m[2023-07-10 18:20:05,748][227910] Min Reward on eval: 902.0778977071692[0m
[37m[1m[2023-07-10 18:20:05,748][227910] Mean Reward across all agents: 1904.0839735598618[0m
[37m[1m[2023-07-10 18:20:05,749][227910] Average Trajectory Length: 985.0153333333333[0m
[36m[2023-07-10 18:20:05,750][227910] mean_value=-1594.17725644842, max_value=611.7213769590583[0m
[37m[1m[2023-07-10 18:20:05,753][227910] New mean coefficients: [[ 4.217062   -0.20228183 -1.7759418   2.4910269   1.0282801 ]][0m
[37m[1m[2023-07-10 18:20:05,754][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:20:15,530][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 18:20:15,530][227910] FPS: 392850.49[0m
[36m[2023-07-10 18:20:15,533][227910] itr=1041, itrs=2000, Progress: 52.05%[0m
[36m[2023-07-10 18:20:27,137][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 18:20:27,138][227910] FPS: 331438.40[0m
[36m[2023-07-10 18:20:31,976][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:20:31,976][227910] Reward + Measures: [[1966.51159974    0.38386184    0.37451562    0.31903431    0.16143507]][0m
[37m[1m[2023-07-10 18:20:31,977][227910] Max Reward on eval: 1966.511599738627[0m
[37m[1m[2023-07-10 18:20:31,977][227910] Min Reward on eval: 1966.511599738627[0m
[37m[1m[2023-07-10 18:20:31,977][227910] Mean Reward across all agents: 1966.511599738627[0m
[37m[1m[2023-07-10 18:20:31,977][227910] Average Trajectory Length: 997.567[0m
[36m[2023-07-10 18:20:37,416][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:20:37,416][227910] Reward + Measures: [[ 115.03699708    0.50200003    0.49340001    0.375         0.41610003]
 [  34.08622338    0.75749999    0.54350001    0.69709998    0.48559999]
 [ -16.02303118    0.42449999    0.49040005    0.31170002    0.40770003]
 ...
 [-121.78839676    0.36939999    0.27519998    0.41430002    0.32220003]
 [-386.35976252    0.45699999    0.48520002    0.33559999    0.50740004]
 [ 869.17498525    0.47469997    0.44970003    0.41009998    0.2208    ]][0m
[37m[1m[2023-07-10 18:20:37,416][227910] Max Reward on eval: 2152.9430142777505[0m
[37m[1m[2023-07-10 18:20:37,417][227910] Min Reward on eval: -1558.553115677461[0m
[37m[1m[2023-07-10 18:20:37,417][227910] Mean Reward across all agents: 23.283460288838718[0m
[37m[1m[2023-07-10 18:20:37,417][227910] Average Trajectory Length: 991.9463333333333[0m
[36m[2023-07-10 18:20:37,419][227910] mean_value=-1751.0366307088384, max_value=795.4934079165978[0m
[37m[1m[2023-07-10 18:20:37,422][227910] New mean coefficients: [[ 4.271125    0.5897116  -1.154819    0.7892016   0.53706646]][0m
[37m[1m[2023-07-10 18:20:37,423][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:20:47,098][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 18:20:47,099][227910] FPS: 396939.40[0m
[36m[2023-07-10 18:20:47,101][227910] itr=1042, itrs=2000, Progress: 52.10%[0m
[36m[2023-07-10 18:20:58,615][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 18:20:58,615][227910] FPS: 334033.07[0m
[36m[2023-07-10 18:21:03,367][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:21:03,367][227910] Reward + Measures: [[2133.07013447    0.38055566    0.37094793    0.2993336     0.15133791]][0m
[37m[1m[2023-07-10 18:21:03,367][227910] Max Reward on eval: 2133.070134472509[0m
[37m[1m[2023-07-10 18:21:03,368][227910] Min Reward on eval: 2133.070134472509[0m
[37m[1m[2023-07-10 18:21:03,368][227910] Mean Reward across all agents: 2133.070134472509[0m
[37m[1m[2023-07-10 18:21:03,368][227910] Average Trajectory Length: 996.8663333333333[0m
[36m[2023-07-10 18:21:08,870][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:21:08,871][227910] Reward + Measures: [[1407.16119183    0.41170001    0.26729998    0.42120001    0.24239998]
 [2083.48583295    0.3888        0.36719999    0.26209998    0.15030001]
 [2007.26249214    0.42442083    0.37891293    0.27557698    0.16254675]
 ...
 [1964.61223157    0.37940001    0.48850003    0.29000002    0.1609    ]
 [2017.32240318    0.36020002    0.40920001    0.31990001    0.15449999]
 [2050.65302737    0.37550002    0.40840003    0.3285        0.16790001]][0m
[37m[1m[2023-07-10 18:21:08,871][227910] Max Reward on eval: 2291.6718008559196[0m
[37m[1m[2023-07-10 18:21:08,871][227910] Min Reward on eval: 1068.536867248651[0m
[37m[1m[2023-07-10 18:21:08,872][227910] Mean Reward across all agents: 1906.438999538495[0m
[37m[1m[2023-07-10 18:21:08,872][227910] Average Trajectory Length: 992.5029999999999[0m
[36m[2023-07-10 18:21:08,874][227910] mean_value=-1264.6033685585212, max_value=801.9242427063393[0m
[37m[1m[2023-07-10 18:21:08,876][227910] New mean coefficients: [[4.492421   1.4475482  0.82038283 0.94780904 1.864339  ]][0m
[37m[1m[2023-07-10 18:21:08,877][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:21:18,586][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 18:21:18,586][227910] FPS: 395578.76[0m
[36m[2023-07-10 18:21:18,589][227910] itr=1043, itrs=2000, Progress: 52.15%[0m
[36m[2023-07-10 18:21:30,229][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 18:21:30,229][227910] FPS: 330442.97[0m
[36m[2023-07-10 18:21:34,944][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:21:34,944][227910] Reward + Measures: [[2325.27131886    0.37778115    0.38043156    0.29389223    0.1511876 ]][0m
[37m[1m[2023-07-10 18:21:34,945][227910] Max Reward on eval: 2325.2713188608413[0m
[37m[1m[2023-07-10 18:21:34,945][227910] Min Reward on eval: 2325.2713188608413[0m
[37m[1m[2023-07-10 18:21:34,945][227910] Mean Reward across all agents: 2325.2713188608413[0m
[37m[1m[2023-07-10 18:21:34,945][227910] Average Trajectory Length: 997.9803333333333[0m
[36m[2023-07-10 18:21:40,659][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:21:40,660][227910] Reward + Measures: [[-346.97163419    0.2227416     0.38052666    0.29529849    0.32257444]
 [ 209.73953589    0.23898557    0.27181584    0.23305248    0.17895949]
 [2271.55175014    0.39930001    0.29300004    0.29809999    0.15899999]
 ...
 [2153.53294027    0.38170001    0.34220001    0.27329999    0.16149999]
 [  78.55653126    0.49349999    0.34719998    0.57419997    0.44799995]
 [ 452.39674507    0.29710004    0.37000003    0.38880002    0.26830003]][0m
[37m[1m[2023-07-10 18:21:40,660][227910] Max Reward on eval: 2349.918342470564[0m
[37m[1m[2023-07-10 18:21:40,660][227910] Min Reward on eval: -765.6743938584375[0m
[37m[1m[2023-07-10 18:21:40,660][227910] Mean Reward across all agents: 1126.039546656665[0m
[37m[1m[2023-07-10 18:21:40,661][227910] Average Trajectory Length: 950.3373333333333[0m
[36m[2023-07-10 18:21:40,662][227910] mean_value=-2107.2348975700465, max_value=480.04964505299813[0m
[37m[1m[2023-07-10 18:21:40,665][227910] New mean coefficients: [[ 4.371159   -0.4164256   0.87091756  1.2045254   1.3051497 ]][0m
[37m[1m[2023-07-10 18:21:40,665][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:21:50,554][227910] train() took 9.89 seconds to complete[0m
[36m[2023-07-10 18:21:50,555][227910] FPS: 388377.46[0m
[36m[2023-07-10 18:21:50,557][227910] itr=1044, itrs=2000, Progress: 52.20%[0m
[36m[2023-07-10 18:22:02,172][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 18:22:02,172][227910] FPS: 331235.25[0m
[36m[2023-07-10 18:22:07,042][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:22:07,043][227910] Reward + Measures: [[2459.150712      0.36460385    0.38656583    0.28557989    0.14678682]][0m
[37m[1m[2023-07-10 18:22:07,043][227910] Max Reward on eval: 2459.1507119972725[0m
[37m[1m[2023-07-10 18:22:07,043][227910] Min Reward on eval: 2459.1507119972725[0m
[37m[1m[2023-07-10 18:22:07,043][227910] Mean Reward across all agents: 2459.1507119972725[0m
[37m[1m[2023-07-10 18:22:07,044][227910] Average Trajectory Length: 996.6803333333334[0m
[36m[2023-07-10 18:22:12,522][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:22:12,523][227910] Reward + Measures: [[1030.42581458    0.3184        0.42570001    0.2264        0.22000001]
 [1572.36559217    0.36700001    0.4664        0.23410001    0.22309999]
 [1844.37723538    0.29310003    0.3125        0.22150002    0.14290002]
 ...
 [1351.91599179    0.41819999    0.37670001    0.2588        0.2225    ]
 [-776.61298157    0.1929        0.28299999    0.12800001    0.2642    ]
 [1900.30422839    0.31940001    0.39630002    0.2638        0.17199999]][0m
[37m[1m[2023-07-10 18:22:12,523][227910] Max Reward on eval: 2577.687475405843[0m
[37m[1m[2023-07-10 18:22:12,523][227910] Min Reward on eval: -1314.5228141242405[0m
[37m[1m[2023-07-10 18:22:12,523][227910] Mean Reward across all agents: 929.9744184231045[0m
[37m[1m[2023-07-10 18:22:12,524][227910] Average Trajectory Length: 991.8753333333333[0m
[36m[2023-07-10 18:22:12,526][227910] mean_value=-1346.1350957032653, max_value=1050.4884732387386[0m
[37m[1m[2023-07-10 18:22:12,529][227910] New mean coefficients: [[ 4.062204   -0.74951595 -1.0535322   1.5305989   1.3360125 ]][0m
[37m[1m[2023-07-10 18:22:12,530][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:22:22,232][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 18:22:22,233][227910] FPS: 395826.32[0m
[36m[2023-07-10 18:22:22,235][227910] itr=1045, itrs=2000, Progress: 52.25%[0m
[36m[2023-07-10 18:22:33,775][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 18:22:33,776][227910] FPS: 333257.36[0m
[36m[2023-07-10 18:22:38,575][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:22:38,575][227910] Reward + Measures: [[2598.3353922     0.36432531    0.39520344    0.28080052    0.14849885]][0m
[37m[1m[2023-07-10 18:22:38,575][227910] Max Reward on eval: 2598.335392197757[0m
[37m[1m[2023-07-10 18:22:38,575][227910] Min Reward on eval: 2598.335392197757[0m
[37m[1m[2023-07-10 18:22:38,576][227910] Mean Reward across all agents: 2598.335392197757[0m
[37m[1m[2023-07-10 18:22:38,576][227910] Average Trajectory Length: 998.6143333333333[0m
[36m[2023-07-10 18:22:44,127][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:22:44,128][227910] Reward + Measures: [[2277.93600216    0.38609999    0.43059999    0.30950004    0.1592    ]
 [2456.93735443    0.35940003    0.44220001    0.30270001    0.18260001]
 [2165.84170184    0.36680004    0.4594        0.3048        0.19710003]
 ...
 [2425.67236632    0.34370002    0.4007        0.2739        0.1522    ]
 [2594.37274374    0.37779999    0.38710004    0.26430002    0.13880001]
 [2110.8435289     0.31299999    0.34300002    0.26620001    0.18380001]][0m
[37m[1m[2023-07-10 18:22:44,128][227910] Max Reward on eval: 2650.6001072593035[0m
[37m[1m[2023-07-10 18:22:44,128][227910] Min Reward on eval: 1159.7089093425893[0m
[37m[1m[2023-07-10 18:22:44,128][227910] Mean Reward across all agents: 2147.8126697859275[0m
[37m[1m[2023-07-10 18:22:44,129][227910] Average Trajectory Length: 999.0206666666667[0m
[36m[2023-07-10 18:22:44,131][227910] mean_value=-533.4494092049953, max_value=359.4472496168155[0m
[37m[1m[2023-07-10 18:22:44,133][227910] New mean coefficients: [[ 3.645036    0.38520247 -0.9182473   0.81535876  2.495245  ]][0m
[37m[1m[2023-07-10 18:22:44,134][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:22:53,897][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 18:22:53,897][227910] FPS: 393401.24[0m
[36m[2023-07-10 18:22:53,899][227910] itr=1046, itrs=2000, Progress: 52.30%[0m
[36m[2023-07-10 18:23:05,421][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 18:23:05,421][227910] FPS: 333851.21[0m
[36m[2023-07-10 18:23:10,247][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:23:10,247][227910] Reward + Measures: [[2710.56526087    0.36361063    0.38674885    0.28303626    0.14613931]][0m
[37m[1m[2023-07-10 18:23:10,247][227910] Max Reward on eval: 2710.5652608704445[0m
[37m[1m[2023-07-10 18:23:10,247][227910] Min Reward on eval: 2710.5652608704445[0m
[37m[1m[2023-07-10 18:23:10,248][227910] Mean Reward across all agents: 2710.5652608704445[0m
[37m[1m[2023-07-10 18:23:10,248][227910] Average Trajectory Length: 999.1826666666666[0m
[36m[2023-07-10 18:23:15,702][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:23:15,702][227910] Reward + Measures: [[2311.36804085    0.40889999    0.317         0.34290001    0.1522    ]
 [2155.17956598    0.45010003    0.45169997    0.27650005    0.18700001]
 [2156.91387061    0.42290002    0.47690001    0.24070001    0.18070002]
 ...
 [2049.54895444    0.45210001    0.29210001    0.35800001    0.17330001]
 [2081.82511609    0.433         0.48409995    0.21440001    0.20300002]
 [2081.20828917    0.40040001    0.37260002    0.37560001    0.16779999]][0m
[37m[1m[2023-07-10 18:23:15,702][227910] Max Reward on eval: 2728.7214562301524[0m
[37m[1m[2023-07-10 18:23:15,703][227910] Min Reward on eval: 1446.4219951291307[0m
[37m[1m[2023-07-10 18:23:15,703][227910] Mean Reward across all agents: 2253.9474604029115[0m
[37m[1m[2023-07-10 18:23:15,703][227910] Average Trajectory Length: 998.5889999999999[0m
[36m[2023-07-10 18:23:15,706][227910] mean_value=-450.46001423321235, max_value=813.1672668706183[0m
[37m[1m[2023-07-10 18:23:15,709][227910] New mean coefficients: [[ 3.6752112  -0.0928221  -0.01577461 -0.02191931  2.9077768 ]][0m
[37m[1m[2023-07-10 18:23:15,710][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:23:25,431][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 18:23:25,431][227910] FPS: 395072.15[0m
[36m[2023-07-10 18:23:25,434][227910] itr=1047, itrs=2000, Progress: 52.35%[0m
[36m[2023-07-10 18:23:37,183][227910] train() took 11.73 seconds to complete[0m
[36m[2023-07-10 18:23:37,184][227910] FPS: 327416.43[0m
[36m[2023-07-10 18:23:41,913][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:23:41,914][227910] Reward + Measures: [[2860.39635302    0.36428869    0.40249112    0.27469423    0.15225598]][0m
[37m[1m[2023-07-10 18:23:41,914][227910] Max Reward on eval: 2860.39635301933[0m
[37m[1m[2023-07-10 18:23:41,914][227910] Min Reward on eval: 2860.39635301933[0m
[37m[1m[2023-07-10 18:23:41,914][227910] Mean Reward across all agents: 2860.39635301933[0m
[37m[1m[2023-07-10 18:23:41,914][227910] Average Trajectory Length: 999.1573333333333[0m
[36m[2023-07-10 18:23:47,396][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:23:47,396][227910] Reward + Measures: [[1451.81421354    0.4499        0.27060002    0.36429998    0.24550001]
 [ 874.92193461    0.42790005    0.24860001    0.48800001    0.30560002]
 [ 978.79826081    0.38450003    0.32500002    0.38110003    0.25719997]
 ...
 [1318.45805937    0.23881033    0.29495054    0.22700205    0.15711239]
 [ 748.83340044    0.31150001    0.22579999    0.44819999    0.27410004]
 [1611.83149511    0.40290004    0.31350002    0.38530001    0.226     ]][0m
[37m[1m[2023-07-10 18:23:47,396][227910] Max Reward on eval: 2852.4661718145944[0m
[37m[1m[2023-07-10 18:23:47,397][227910] Min Reward on eval: -392.6243440666236[0m
[37m[1m[2023-07-10 18:23:47,397][227910] Mean Reward across all agents: 1546.4369910397527[0m
[37m[1m[2023-07-10 18:23:47,397][227910] Average Trajectory Length: 997.0989999999999[0m
[36m[2023-07-10 18:23:47,399][227910] mean_value=-1103.9385413089417, max_value=498.5757542473052[0m
[37m[1m[2023-07-10 18:23:47,401][227910] New mean coefficients: [[4.6886168  0.10120524 0.05372631 0.5344594  4.1921024 ]][0m
[37m[1m[2023-07-10 18:23:47,402][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:23:57,093][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 18:23:57,093][227910] FPS: 396322.77[0m
[36m[2023-07-10 18:23:57,096][227910] itr=1048, itrs=2000, Progress: 52.40%[0m
[36m[2023-07-10 18:24:08,838][227910] train() took 11.72 seconds to complete[0m
[36m[2023-07-10 18:24:08,838][227910] FPS: 327570.93[0m
[36m[2023-07-10 18:24:13,651][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:24:13,652][227910] Reward + Measures: [[3002.54978839    0.35327715    0.41111165    0.27285799    0.15039547]][0m
[37m[1m[2023-07-10 18:24:13,652][227910] Max Reward on eval: 3002.549788386178[0m
[37m[1m[2023-07-10 18:24:13,652][227910] Min Reward on eval: 3002.549788386178[0m
[37m[1m[2023-07-10 18:24:13,652][227910] Mean Reward across all agents: 3002.549788386178[0m
[37m[1m[2023-07-10 18:24:13,653][227910] Average Trajectory Length: 999.6466666666666[0m
[36m[2023-07-10 18:24:19,191][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:24:19,192][227910] Reward + Measures: [[-416.8167701     0.43979999    0.42050001    0.36560002    0.3319    ]
 [ 806.8990317     0.39151415    0.49302045    0.20771496    0.35708061]
 [ -37.56248659    0.4183        0.47760001    0.164         0.4289    ]
 ...
 [1258.59312488    0.28889999    0.21519999    0.37149999    0.26500002]
 [1423.39339631    0.44780001    0.44700003    0.28140002    0.2297    ]
 [-504.15955009    0.4542        0.61919999    0.31029996    0.55860001]][0m
[37m[1m[2023-07-10 18:24:19,192][227910] Max Reward on eval: 2611.7400048181416[0m
[37m[1m[2023-07-10 18:24:19,192][227910] Min Reward on eval: -705.7993629868026[0m
[37m[1m[2023-07-10 18:24:19,193][227910] Mean Reward across all agents: 618.2582171044653[0m
[37m[1m[2023-07-10 18:24:19,193][227910] Average Trajectory Length: 982.8639999999999[0m
[36m[2023-07-10 18:24:19,195][227910] mean_value=-1424.8373934952708, max_value=2523.2812644421588[0m
[37m[1m[2023-07-10 18:24:19,197][227910] New mean coefficients: [[ 4.93926     0.15117303 -0.28661978 -0.24482423  3.174476  ]][0m
[37m[1m[2023-07-10 18:24:19,198][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:24:28,953][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 18:24:28,954][227910] FPS: 393696.34[0m
[36m[2023-07-10 18:24:28,956][227910] itr=1049, itrs=2000, Progress: 52.45%[0m
[36m[2023-07-10 18:24:40,603][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 18:24:40,604][227910] FPS: 330204.00[0m
[36m[2023-07-10 18:24:45,334][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:24:45,334][227910] Reward + Measures: [[3139.58768945    0.34636295    0.42777556    0.26714808    0.14801408]][0m
[37m[1m[2023-07-10 18:24:45,334][227910] Max Reward on eval: 3139.5876894460803[0m
[37m[1m[2023-07-10 18:24:45,334][227910] Min Reward on eval: 3139.5876894460803[0m
[37m[1m[2023-07-10 18:24:45,335][227910] Mean Reward across all agents: 3139.5876894460803[0m
[37m[1m[2023-07-10 18:24:45,335][227910] Average Trajectory Length: 998.9593333333333[0m
[36m[2023-07-10 18:24:50,764][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:24:50,765][227910] Reward + Measures: [[-187.54892371    0.24700001    0.4887        0.3994        0.3608    ]
 [  56.22327216    0.1786        0.4323        0.32090002    0.33399999]
 [2978.47387081    0.36160001    0.4804        0.24679999    0.14330001]
 ...
 [-637.64821831    0.25998628    0.36571375    0.34518036    0.28311375]
 [-653.31641623    0.8502        0.0418        0.87999994    0.78570002]
 [-543.50704831    0.42278418    0.49341652    0.44197735    0.24973324]][0m
[37m[1m[2023-07-10 18:24:50,765][227910] Max Reward on eval: 2978.4738708132877[0m
[37m[1m[2023-07-10 18:24:50,765][227910] Min Reward on eval: -1463.229481482954[0m
[37m[1m[2023-07-10 18:24:50,766][227910] Mean Reward across all agents: 647.9231332863243[0m
[37m[1m[2023-07-10 18:24:50,766][227910] Average Trajectory Length: 987.328[0m
[36m[2023-07-10 18:24:50,770][227910] mean_value=-753.9418107462117, max_value=1765.0515432691352[0m
[37m[1m[2023-07-10 18:24:50,773][227910] New mean coefficients: [[ 5.4605284   0.16898116  0.12648481 -0.68463254  3.4360251 ]][0m
[37m[1m[2023-07-10 18:24:50,774][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:25:00,433][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 18:25:00,434][227910] FPS: 397602.68[0m
[36m[2023-07-10 18:25:00,436][227910] itr=1050, itrs=2000, Progress: 52.50%[0m
[37m[1m[2023-07-10 18:25:04,152][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001030[0m
[36m[2023-07-10 18:25:15,922][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 18:25:15,922][227910] FPS: 334005.27[0m
[36m[2023-07-10 18:25:20,619][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:25:20,620][227910] Reward + Measures: [[3283.05393558    0.34260839    0.4275263     0.26472747    0.14423835]][0m
[37m[1m[2023-07-10 18:25:20,620][227910] Max Reward on eval: 3283.053935575917[0m
[37m[1m[2023-07-10 18:25:20,620][227910] Min Reward on eval: 3283.053935575917[0m
[37m[1m[2023-07-10 18:25:20,620][227910] Mean Reward across all agents: 3283.053935575917[0m
[37m[1m[2023-07-10 18:25:20,621][227910] Average Trajectory Length: 999.1806666666666[0m
[36m[2023-07-10 18:25:26,262][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:25:26,262][227910] Reward + Measures: [[ 729.24782934    0.40110001    0.36650002    0.27880001    0.317     ]
 [ 521.00481744    0.4237        0.43350002    0.3865        0.41159996]
 [1255.72703235    0.40532133    0.36935386    0.33740029    0.32117841]
 ...
 [ 641.45974759    0.37490001    0.32880002    0.31920001    0.29250002]
 [2648.93972971    0.42680001    0.26929998    0.24679999    0.1411    ]
 [ 371.06119239    0.35909998    0.38319999    0.48730001    0.47529998]][0m
[37m[1m[2023-07-10 18:25:26,263][227910] Max Reward on eval: 3144.328145976365[0m
[37m[1m[2023-07-10 18:25:26,263][227910] Min Reward on eval: -1377.5893183276755[0m
[37m[1m[2023-07-10 18:25:26,263][227910] Mean Reward across all agents: 763.1692797148349[0m
[37m[1m[2023-07-10 18:25:26,263][227910] Average Trajectory Length: 966.904[0m
[36m[2023-07-10 18:25:26,266][227910] mean_value=-1673.9998661783573, max_value=1851.395224734955[0m
[37m[1m[2023-07-10 18:25:26,268][227910] New mean coefficients: [[5.7942758  0.05081553 0.18309116 0.7688267  3.4932117 ]][0m
[37m[1m[2023-07-10 18:25:26,269][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:25:35,924][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 18:25:35,924][227910] FPS: 397816.34[0m
[36m[2023-07-10 18:25:35,926][227910] itr=1051, itrs=2000, Progress: 52.55%[0m
[36m[2023-07-10 18:25:47,420][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 18:25:47,420][227910] FPS: 334622.91[0m
[36m[2023-07-10 18:25:52,168][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:25:52,168][227910] Reward + Measures: [[3426.96510543    0.34024942    0.44125333    0.25846979    0.14286743]][0m
[37m[1m[2023-07-10 18:25:52,168][227910] Max Reward on eval: 3426.9651054299447[0m
[37m[1m[2023-07-10 18:25:52,169][227910] Min Reward on eval: 3426.9651054299447[0m
[37m[1m[2023-07-10 18:25:52,169][227910] Mean Reward across all agents: 3426.9651054299447[0m
[37m[1m[2023-07-10 18:25:52,169][227910] Average Trajectory Length: 999.7103333333333[0m
[36m[2023-07-10 18:25:57,646][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:25:57,647][227910] Reward + Measures: [[-268.48858793    0.46399999    0.72270006    0.18969999    0.67970002]
 [ 548.68236268    0.3536        0.5438        0.52060002    0.29889998]
 [2311.26090795    0.31229997    0.50030005    0.2244        0.149     ]
 ...
 [ 205.17895088    0.68260002    0.76209992    0.10469999    0.60319996]
 [2366.38225452    0.32573652    0.32003495    0.22330201    0.12945943]
 [ 841.08695488    0.37649998    0.49950004    0.36739999    0.3818    ]][0m
[37m[1m[2023-07-10 18:25:57,647][227910] Max Reward on eval: 3304.9676753427834[0m
[37m[1m[2023-07-10 18:25:57,647][227910] Min Reward on eval: -1588.992126842495[0m
[37m[1m[2023-07-10 18:25:57,647][227910] Mean Reward across all agents: 835.1441529418979[0m
[37m[1m[2023-07-10 18:25:57,648][227910] Average Trajectory Length: 993.3303333333333[0m
[36m[2023-07-10 18:25:57,650][227910] mean_value=-1129.595799818929, max_value=1518.834767577001[0m
[37m[1m[2023-07-10 18:25:57,653][227910] New mean coefficients: [[ 5.9115267  -0.3111764   0.52589357  0.49054417  3.260834  ]][0m
[37m[1m[2023-07-10 18:25:57,653][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:26:07,262][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 18:26:07,262][227910] FPS: 399725.85[0m
[36m[2023-07-10 18:26:07,264][227910] itr=1052, itrs=2000, Progress: 52.60%[0m
[36m[2023-07-10 18:26:18,911][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 18:26:18,911][227910] FPS: 330330.51[0m
[36m[2023-07-10 18:26:23,729][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:26:23,730][227910] Reward + Measures: [[3568.99265601    0.3354201     0.44122887    0.2550382     0.14015865]][0m
[37m[1m[2023-07-10 18:26:23,730][227910] Max Reward on eval: 3568.99265601312[0m
[37m[1m[2023-07-10 18:26:23,730][227910] Min Reward on eval: 3568.99265601312[0m
[37m[1m[2023-07-10 18:26:23,730][227910] Mean Reward across all agents: 3568.99265601312[0m
[37m[1m[2023-07-10 18:26:23,730][227910] Average Trajectory Length: 999.5483333333333[0m
[36m[2023-07-10 18:26:29,202][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:26:29,202][227910] Reward + Measures: [[2794.55132715    0.34240001    0.39350003    0.26570001    0.1549    ]
 [2372.42780416    0.38485163    0.42224756    0.19916169    0.17785814]
 [2807.93857054    0.33160001    0.35759997    0.2361        0.1788    ]
 ...
 [3162.5296216     0.3845        0.42480001    0.23210001    0.1768    ]
 [3113.62803146    0.38339999    0.47139999    0.25939998    0.1622    ]
 [2136.31970597    0.3583        0.34300002    0.26149997    0.2086    ]][0m
[37m[1m[2023-07-10 18:26:29,202][227910] Max Reward on eval: 3547.078754070215[0m
[37m[1m[2023-07-10 18:26:29,203][227910] Min Reward on eval: 1640.1869868608192[0m
[37m[1m[2023-07-10 18:26:29,203][227910] Mean Reward across all agents: 2964.2398352561004[0m
[37m[1m[2023-07-10 18:26:29,203][227910] Average Trajectory Length: 994.3776666666666[0m
[36m[2023-07-10 18:26:29,206][227910] mean_value=-214.5636758976338, max_value=935.6084594814793[0m
[37m[1m[2023-07-10 18:26:29,208][227910] New mean coefficients: [[ 5.393739    0.41476038  0.24484918 -0.12136272  2.6499636 ]][0m
[37m[1m[2023-07-10 18:26:29,209][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:26:38,960][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 18:26:38,960][227910] FPS: 393908.21[0m
[36m[2023-07-10 18:26:38,962][227910] itr=1053, itrs=2000, Progress: 52.65%[0m
[36m[2023-07-10 18:26:50,745][227910] train() took 11.76 seconds to complete[0m
[36m[2023-07-10 18:26:50,745][227910] FPS: 326497.44[0m
[36m[2023-07-10 18:26:55,548][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:26:55,548][227910] Reward + Measures: [[3687.78198955    0.32820925    0.42976773    0.25691462    0.13807157]][0m
[37m[1m[2023-07-10 18:26:55,548][227910] Max Reward on eval: 3687.781989547818[0m
[37m[1m[2023-07-10 18:26:55,549][227910] Min Reward on eval: 3687.781989547818[0m
[37m[1m[2023-07-10 18:26:55,549][227910] Mean Reward across all agents: 3687.781989547818[0m
[37m[1m[2023-07-10 18:26:55,549][227910] Average Trajectory Length: 999.5803333333333[0m
[36m[2023-07-10 18:27:01,084][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:27:01,085][227910] Reward + Measures: [[-1115.60639567     0.44580004     0.18239999     0.55680001
      0.51420003]
 [-1220.30219646     0.14610001     0.16980001     0.156
      0.184     ]
 [  908.46478491     0.27990001     0.35900003     0.21069999
      0.21529999]
 ...
 [ -427.49251874     0.45040002     0.08020001     0.4341
      0.3141    ]
 [ 1548.517594       0.31277797     0.45107237     0.23184152
      0.22991256]
 [ -427.14581835     0.61739999     0.1517         0.6202001
      0.32929999]][0m
[37m[1m[2023-07-10 18:27:01,085][227910] Max Reward on eval: 2990.7589706474914[0m
[37m[1m[2023-07-10 18:27:01,086][227910] Min Reward on eval: -1531.1905055189272[0m
[37m[1m[2023-07-10 18:27:01,086][227910] Mean Reward across all agents: 165.2554137600149[0m
[37m[1m[2023-07-10 18:27:01,086][227910] Average Trajectory Length: 971.0866666666666[0m
[36m[2023-07-10 18:27:01,088][227910] mean_value=-1585.9726381764406, max_value=1219.2515845800363[0m
[37m[1m[2023-07-10 18:27:01,090][227910] New mean coefficients: [[5.104121   0.34348196 0.01789889 0.4734237  2.4007614 ]][0m
[37m[1m[2023-07-10 18:27:01,091][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:27:10,744][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 18:27:10,744][227910] FPS: 397886.40[0m
[36m[2023-07-10 18:27:10,746][227910] itr=1054, itrs=2000, Progress: 52.70%[0m
[36m[2023-07-10 18:27:22,333][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 18:27:22,334][227910] FPS: 332023.40[0m
[36m[2023-07-10 18:27:27,191][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:27:27,191][227910] Reward + Measures: [[3760.77064468    0.32476681    0.43803418    0.25289187    0.13612659]][0m
[37m[1m[2023-07-10 18:27:27,192][227910] Max Reward on eval: 3760.770644675892[0m
[37m[1m[2023-07-10 18:27:27,192][227910] Min Reward on eval: 3760.770644675892[0m
[37m[1m[2023-07-10 18:27:27,192][227910] Mean Reward across all agents: 3760.770644675892[0m
[37m[1m[2023-07-10 18:27:27,192][227910] Average Trajectory Length: 999.1583333333333[0m
[36m[2023-07-10 18:27:32,912][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:27:32,917][227910] Reward + Measures: [[  -9.82623765    0.82579994    0.50950003    0.81609994    0.0573    ]
 [ 115.08187859    0.23874044    0.25629398    0.33428526    0.30185848]
 [1323.62950867    0.44170004    0.28959998    0.35849997    0.14320001]
 ...
 [1970.80498711    0.47740003    0.3118        0.32480001    0.27680001]
 [1418.82546653    0.60639995    0.23720001    0.458         0.1454    ]
 [3195.97203867    0.4161        0.40560004    0.28530002    0.1649    ]][0m
[37m[1m[2023-07-10 18:27:32,918][227910] Max Reward on eval: 3377.0324824504555[0m
[37m[1m[2023-07-10 18:27:32,918][227910] Min Reward on eval: -1335.9906147587346[0m
[37m[1m[2023-07-10 18:27:32,918][227910] Mean Reward across all agents: 903.7035223709706[0m
[37m[1m[2023-07-10 18:27:32,919][227910] Average Trajectory Length: 990.7273333333333[0m
[36m[2023-07-10 18:27:32,922][227910] mean_value=-818.9969261559728, max_value=2584.107004572201[0m
[37m[1m[2023-07-10 18:27:32,925][227910] New mean coefficients: [[ 5.043025   1.1407251  0.7487174 -0.3197885  2.9030995]][0m
[37m[1m[2023-07-10 18:27:32,926][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:27:42,611][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 18:27:42,611][227910] FPS: 396560.39[0m
[36m[2023-07-10 18:27:42,613][227910] itr=1055, itrs=2000, Progress: 52.75%[0m
[36m[2023-07-10 18:27:54,185][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 18:27:54,185][227910] FPS: 332457.87[0m
[36m[2023-07-10 18:27:58,916][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:27:58,917][227910] Reward + Measures: [[3867.07603747    0.32395014    0.43870381    0.25329715    0.13852727]][0m
[37m[1m[2023-07-10 18:27:58,917][227910] Max Reward on eval: 3867.0760374698066[0m
[37m[1m[2023-07-10 18:27:58,917][227910] Min Reward on eval: 3867.0760374698066[0m
[37m[1m[2023-07-10 18:27:58,918][227910] Mean Reward across all agents: 3867.0760374698066[0m
[37m[1m[2023-07-10 18:27:58,918][227910] Average Trajectory Length: 998.7163333333333[0m
[36m[2023-07-10 18:28:04,423][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:28:04,423][227910] Reward + Measures: [[ 410.40948131    0.34689999    0.34899998    0.24380003    0.15989999]
 [ 186.65915786    0.20226507    0.23251824    0.12313079    0.12031174]
 [ 163.18860725    0.59287089    0.55638117    0.45999265    0.42758605]
 ...
 [1245.89020418    0.4461        0.3328        0.31370002    0.19919999]
 [ 121.20261743    0.35679489    0.52000552    0.23826484    0.43246451]
 [-107.35088237    0.2516        0.32550001    0.22189999    0.2392    ]][0m
[37m[1m[2023-07-10 18:28:04,423][227910] Max Reward on eval: 3436.454615418427[0m
[37m[1m[2023-07-10 18:28:04,424][227910] Min Reward on eval: -1280.6126426097703[0m
[37m[1m[2023-07-10 18:28:04,424][227910] Mean Reward across all agents: 622.6996983704674[0m
[37m[1m[2023-07-10 18:28:04,424][227910] Average Trajectory Length: 934.227[0m
[36m[2023-07-10 18:28:04,426][227910] mean_value=-2055.4895000134884, max_value=1157.4065704014922[0m
[37m[1m[2023-07-10 18:28:04,428][227910] New mean coefficients: [[4.6530566  0.21228456 0.42000768 0.33442816 2.5840788 ]][0m
[37m[1m[2023-07-10 18:28:04,429][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:28:14,262][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 18:28:14,262][227910] FPS: 390566.38[0m
[36m[2023-07-10 18:28:14,265][227910] itr=1056, itrs=2000, Progress: 52.80%[0m
[36m[2023-07-10 18:28:25,924][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 18:28:25,924][227910] FPS: 329910.03[0m
[36m[2023-07-10 18:28:30,705][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:28:30,711][227910] Reward + Measures: [[2702.80491213    0.29546729    0.36855102    0.29886556    0.15761758]][0m
[37m[1m[2023-07-10 18:28:30,711][227910] Max Reward on eval: 2702.8049121296112[0m
[37m[1m[2023-07-10 18:28:30,711][227910] Min Reward on eval: 2702.8049121296112[0m
[37m[1m[2023-07-10 18:28:30,711][227910] Mean Reward across all agents: 2702.8049121296112[0m
[37m[1m[2023-07-10 18:28:30,712][227910] Average Trajectory Length: 996.3276666666667[0m
[36m[2023-07-10 18:28:36,221][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:28:36,221][227910] Reward + Measures: [[  183.303906       0.31385449     0.22687779     0.20897417
      0.24597101]
 [  916.1442665      0.31979999     0.5169         0.15750001
      0.21329999]
 [ 1203.20051665     0.35400003     0.30720001     0.17209999
      0.18540001]
 ...
 [  599.20229221     0.28679982     0.37015232     0.18516037
      0.17004037]
 [-1324.72398109     0.53109998     0.58470005     0.19820002
      0.40159997]
 [  289.70485981     0.58485919     0.53022498     0.53051579
      0.50306857]][0m
[37m[1m[2023-07-10 18:28:36,221][227910] Max Reward on eval: 2692.3847540102433[0m
[37m[1m[2023-07-10 18:28:36,222][227910] Min Reward on eval: -1324.7239810863161[0m
[37m[1m[2023-07-10 18:28:36,222][227910] Mean Reward across all agents: 147.4122924147178[0m
[37m[1m[2023-07-10 18:28:36,222][227910] Average Trajectory Length: 828.4336666666667[0m
[36m[2023-07-10 18:28:36,224][227910] mean_value=-1949.9676533753354, max_value=1193.8067998196266[0m
[37m[1m[2023-07-10 18:28:36,226][227910] New mean coefficients: [[4.624014   0.5810125  0.33831632 0.23683408 1.9031872 ]][0m
[37m[1m[2023-07-10 18:28:36,227][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:28:45,987][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 18:28:45,988][227910] FPS: 393484.96[0m
[36m[2023-07-10 18:28:45,990][227910] itr=1057, itrs=2000, Progress: 52.85%[0m
[36m[2023-07-10 18:28:57,667][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 18:28:57,667][227910] FPS: 329360.84[0m
[36m[2023-07-10 18:29:02,491][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:29:02,491][227910] Reward + Measures: [[2879.26416818    0.28954077    0.37362862    0.30239376    0.15203625]][0m
[37m[1m[2023-07-10 18:29:02,492][227910] Max Reward on eval: 2879.264168179807[0m
[37m[1m[2023-07-10 18:29:02,492][227910] Min Reward on eval: 2879.264168179807[0m
[37m[1m[2023-07-10 18:29:02,492][227910] Mean Reward across all agents: 2879.264168179807[0m
[37m[1m[2023-07-10 18:29:02,492][227910] Average Trajectory Length: 994.9746666666666[0m
[36m[2023-07-10 18:29:08,024][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:29:08,025][227910] Reward + Measures: [[ 197.68340039    0.2264        0.44099998    0.26500002    0.3233    ]
 [ 923.01091577    0.28830001    0.37979999    0.33249998    0.23770002]
 [  69.51005671    0.30640003    0.35610002    0.3479        0.2613    ]
 ...
 [ 142.96027003    0.22750001    0.4219        0.27760002    0.48130003]
 [ 714.81824965    0.28040001    0.34490001    0.24600001    0.37669998]
 [2202.94119478    0.36289999    0.2667        0.24080001    0.15980001]][0m
[37m[1m[2023-07-10 18:29:08,025][227910] Max Reward on eval: 2612.5831531066506[0m
[37m[1m[2023-07-10 18:29:08,026][227910] Min Reward on eval: -1067.8265185624593[0m
[37m[1m[2023-07-10 18:29:08,026][227910] Mean Reward across all agents: 544.6196422368605[0m
[37m[1m[2023-07-10 18:29:08,026][227910] Average Trajectory Length: 946.5849999999999[0m
[36m[2023-07-10 18:29:08,028][227910] mean_value=-1548.4955465720404, max_value=1401.2235591381366[0m
[37m[1m[2023-07-10 18:29:08,030][227910] New mean coefficients: [[4.7366996  0.14905879 0.11789384 0.01044708 1.0302886 ]][0m
[37m[1m[2023-07-10 18:29:08,031][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:29:17,877][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 18:29:17,877][227910] FPS: 390107.33[0m
[36m[2023-07-10 18:29:17,879][227910] itr=1058, itrs=2000, Progress: 52.90%[0m
[36m[2023-07-10 18:29:29,500][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 18:29:29,500][227910] FPS: 331065.18[0m
[36m[2023-07-10 18:29:34,244][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:29:34,245][227910] Reward + Measures: [[2255.99775244    0.30756283    0.39559701    0.30629045    0.22173014]][0m
[37m[1m[2023-07-10 18:29:34,245][227910] Max Reward on eval: 2255.997752440666[0m
[37m[1m[2023-07-10 18:29:34,245][227910] Min Reward on eval: 2255.997752440666[0m
[37m[1m[2023-07-10 18:29:34,245][227910] Mean Reward across all agents: 2255.997752440666[0m
[37m[1m[2023-07-10 18:29:34,245][227910] Average Trajectory Length: 995.39[0m
[36m[2023-07-10 18:29:39,756][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:29:39,757][227910] Reward + Measures: [[ 440.05029908    0.1358        0.53509998    0.37330002    0.49520001]
 [ 149.44246963    0.2431        0.35700002    0.1723        0.43760005]
 [  24.33178181    0.0186        0.81649989    0.68080002    0.82730001]
 ...
 [ 382.90835123    0.48499998    0.56129998    0.1832        0.49869996]
 [-516.78379387    0.4474        0.27900001    0.38750002    0.39320001]
 [-229.56876158    0.0249        0.76920003    0.72789997    0.78080004]][0m
[37m[1m[2023-07-10 18:29:39,757][227910] Max Reward on eval: 2349.5775250267006[0m
[37m[1m[2023-07-10 18:29:39,757][227910] Min Reward on eval: -1158.169037435681[0m
[37m[1m[2023-07-10 18:29:39,757][227910] Mean Reward across all agents: 438.3273668451757[0m
[37m[1m[2023-07-10 18:29:39,757][227910] Average Trajectory Length: 989.7883333333333[0m
[36m[2023-07-10 18:29:39,762][227910] mean_value=-576.2827236274514, max_value=700.4323509510575[0m
[37m[1m[2023-07-10 18:29:39,764][227910] New mean coefficients: [[4.7632704  0.19313467 1.2265131  0.3486517  1.2085855 ]][0m
[37m[1m[2023-07-10 18:29:39,765][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:29:49,630][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 18:29:49,631][227910] FPS: 389318.77[0m
[36m[2023-07-10 18:29:49,633][227910] itr=1059, itrs=2000, Progress: 52.95%[0m
[36m[2023-07-10 18:30:01,350][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 18:30:01,351][227910] FPS: 328272.89[0m
[36m[2023-07-10 18:30:06,145][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:30:06,146][227910] Reward + Measures: [[2472.90392782    0.31077224    0.40690812    0.30514112    0.21223602]][0m
[37m[1m[2023-07-10 18:30:06,146][227910] Max Reward on eval: 2472.9039278238674[0m
[37m[1m[2023-07-10 18:30:06,146][227910] Min Reward on eval: 2472.9039278238674[0m
[37m[1m[2023-07-10 18:30:06,146][227910] Mean Reward across all agents: 2472.9039278238674[0m
[37m[1m[2023-07-10 18:30:06,146][227910] Average Trajectory Length: 997.8673333333332[0m
[36m[2023-07-10 18:30:11,747][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:30:11,747][227910] Reward + Measures: [[-1022.22852848     0.83150005     0.63880008     0.75769997
      0.49069998]
 [ -641.34395581     0.84020007     0.54339999     0.80740005
      0.37689999]
 [ 1058.14568962     0.48419997     0.37200001     0.52600002
      0.37470001]
 ...
 [ 1839.30564292     0.26760003     0.40349999     0.3204
      0.24819998]
 [  165.39005731     0.1626         0.34160003     0.22189999
      0.24780002]
 [ 2068.17944684     0.32745001     0.42913333     0.26248333
      0.20574999]][0m
[37m[1m[2023-07-10 18:30:11,747][227910] Max Reward on eval: 2580.315364886471[0m
[37m[1m[2023-07-10 18:30:11,748][227910] Min Reward on eval: -1348.3342677334324[0m
[37m[1m[2023-07-10 18:30:11,748][227910] Mean Reward across all agents: 1424.9452054380529[0m
[37m[1m[2023-07-10 18:30:11,748][227910] Average Trajectory Length: 996.704[0m
[36m[2023-07-10 18:30:11,750][227910] mean_value=-1081.2292216512885, max_value=549.8670231779381[0m
[37m[1m[2023-07-10 18:30:11,752][227910] New mean coefficients: [[4.3795476  0.41197288 1.0256137  1.7812142  1.6514761 ]][0m
[37m[1m[2023-07-10 18:30:11,753][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:30:21,511][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 18:30:21,512][227910] FPS: 393571.52[0m
[36m[2023-07-10 18:30:21,514][227910] itr=1060, itrs=2000, Progress: 53.00%[0m
[37m[1m[2023-07-10 18:30:25,353][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001040[0m
[36m[2023-07-10 18:30:37,353][227910] train() took 11.72 seconds to complete[0m
[36m[2023-07-10 18:30:37,353][227910] FPS: 327636.41[0m
[36m[2023-07-10 18:30:42,186][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:30:42,187][227910] Reward + Measures: [[2634.05855668    0.31003419    0.41224062    0.3062056     0.20967539]][0m
[37m[1m[2023-07-10 18:30:42,187][227910] Max Reward on eval: 2634.058556678718[0m
[37m[1m[2023-07-10 18:30:42,187][227910] Min Reward on eval: 2634.058556678718[0m
[37m[1m[2023-07-10 18:30:42,188][227910] Mean Reward across all agents: 2634.058556678718[0m
[37m[1m[2023-07-10 18:30:42,188][227910] Average Trajectory Length: 998.8683333333333[0m
[36m[2023-07-10 18:30:47,656][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:30:47,657][227910] Reward + Measures: [[ 451.39524356    0.39489999    0.23660003    0.3705        0.199     ]
 [-125.22367687    0.42070004    0.2129        0.38970003    0.22490001]
 [-290.93138808    0.39780003    0.24510002    0.39120001    0.26470003]
 ...
 [-470.414371      0.37809998    0.26199999    0.34440002    0.2658    ]
 [-654.88569722    0.28972307    0.43146244    0.3908807     0.40770021]
 [ 451.70127853    0.33206788    0.24820586    0.37000728    0.17357811]][0m
[37m[1m[2023-07-10 18:30:47,657][227910] Max Reward on eval: 2774.884115268104[0m
[37m[1m[2023-07-10 18:30:47,657][227910] Min Reward on eval: -955.0950714818085[0m
[37m[1m[2023-07-10 18:30:47,657][227910] Mean Reward across all agents: 1225.423791256289[0m
[37m[1m[2023-07-10 18:30:47,658][227910] Average Trajectory Length: 992.3399999999999[0m
[36m[2023-07-10 18:30:47,659][227910] mean_value=-1400.454466282146, max_value=186.40492528682137[0m
[37m[1m[2023-07-10 18:30:47,661][227910] New mean coefficients: [[4.810145  0.5130928 0.4788676 1.4620805 1.7274549]][0m
[37m[1m[2023-07-10 18:30:47,662][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:30:57,398][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 18:30:57,398][227910] FPS: 394500.00[0m
[36m[2023-07-10 18:30:57,400][227910] itr=1061, itrs=2000, Progress: 53.05%[0m
[36m[2023-07-10 18:31:08,887][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 18:31:08,888][227910] FPS: 334797.42[0m
[36m[2023-07-10 18:31:13,591][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:31:13,591][227910] Reward + Measures: [[2710.55522258    0.30738282    0.4129757     0.30825195    0.21034533]][0m
[37m[1m[2023-07-10 18:31:13,592][227910] Max Reward on eval: 2710.5552225836204[0m
[37m[1m[2023-07-10 18:31:13,592][227910] Min Reward on eval: 2710.5552225836204[0m
[37m[1m[2023-07-10 18:31:13,592][227910] Mean Reward across all agents: 2710.5552225836204[0m
[37m[1m[2023-07-10 18:31:13,592][227910] Average Trajectory Length: 999.3163333333333[0m
[36m[2023-07-10 18:31:19,144][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:31:19,145][227910] Reward + Measures: [[ 371.68500093    0.70459998    0.28580004    0.61689997    0.19030002]
 [-433.00082449    0.29047725    0.34230164    0.29663533    0.25264922]
 [-658.73377941    0.25441441    0.47901493    0.24204496    0.40019724]
 ...
 [ 652.72730116    0.48520002    0.4021        0.44070002    0.3642    ]
 [ -18.91081857    0.26713085    0.33179781    0.24043086    0.24831598]
 [  47.16843713    0.30500001    0.37789997    0.18540001    0.26369998]][0m
[37m[1m[2023-07-10 18:31:19,145][227910] Max Reward on eval: 2629.2631510553183[0m
[37m[1m[2023-07-10 18:31:19,146][227910] Min Reward on eval: -1413.4548305499134[0m
[37m[1m[2023-07-10 18:31:19,146][227910] Mean Reward across all agents: 556.0609536059429[0m
[37m[1m[2023-07-10 18:31:19,146][227910] Average Trajectory Length: 971.084[0m
[36m[2023-07-10 18:31:19,147][227910] mean_value=-1820.4709947887295, max_value=254.61030956901487[0m
[37m[1m[2023-07-10 18:31:19,149][227910] New mean coefficients: [[4.579503   0.75369036 1.1758449  1.0743346  1.6686281 ]][0m
[37m[1m[2023-07-10 18:31:19,150][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:31:28,797][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 18:31:28,797][227910] FPS: 398123.16[0m
[36m[2023-07-10 18:31:28,800][227910] itr=1062, itrs=2000, Progress: 53.10%[0m
[36m[2023-07-10 18:31:40,512][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 18:31:40,512][227910] FPS: 328463.94[0m
[36m[2023-07-10 18:31:45,300][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:31:45,301][227910] Reward + Measures: [[2838.70544054    0.3154529     0.41449755    0.31271338    0.20642492]][0m
[37m[1m[2023-07-10 18:31:45,301][227910] Max Reward on eval: 2838.705440544196[0m
[37m[1m[2023-07-10 18:31:45,301][227910] Min Reward on eval: 2838.705440544196[0m
[37m[1m[2023-07-10 18:31:45,301][227910] Mean Reward across all agents: 2838.705440544196[0m
[37m[1m[2023-07-10 18:31:45,302][227910] Average Trajectory Length: 998.8249999999999[0m
[36m[2023-07-10 18:31:50,734][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:31:50,735][227910] Reward + Measures: [[-577.13026324    0.2069        0.54570001    0.2931        0.50049996]
 [ 271.06393963    0.47300002    0.37290001    0.29480001    0.25460002]
 [-106.51876168    0.15090001    0.80410004    0.56970006    0.76970005]
 ...
 [-405.39790194    0.22910002    0.479         0.31080002    0.4612    ]
 [ 889.08214203    0.45380002    0.38910002    0.17639999    0.22979999]
 [-627.15083725    0.14650001    0.64950001    0.33080003    0.5959    ]][0m
[37m[1m[2023-07-10 18:31:50,735][227910] Max Reward on eval: 2725.165866228845[0m
[37m[1m[2023-07-10 18:31:50,735][227910] Min Reward on eval: -1411.487050321931[0m
[37m[1m[2023-07-10 18:31:50,735][227910] Mean Reward across all agents: 26.67252127665276[0m
[37m[1m[2023-07-10 18:31:50,735][227910] Average Trajectory Length: 969.99[0m
[36m[2023-07-10 18:31:50,737][227910] mean_value=-1685.5428012122159, max_value=833.9268456552393[0m
[37m[1m[2023-07-10 18:31:50,739][227910] New mean coefficients: [[4.3650646 0.600237  1.256287  0.5459656 0.8781522]][0m
[37m[1m[2023-07-10 18:31:50,740][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:32:00,371][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 18:32:00,371][227910] FPS: 398785.73[0m
[36m[2023-07-10 18:32:00,373][227910] itr=1063, itrs=2000, Progress: 53.15%[0m
[36m[2023-07-10 18:32:11,931][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 18:32:11,931][227910] FPS: 332824.14[0m
[36m[2023-07-10 18:32:16,714][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:32:16,714][227910] Reward + Measures: [[2914.56200324    0.31327       0.42435196    0.30941641    0.2077385 ]][0m
[37m[1m[2023-07-10 18:32:16,714][227910] Max Reward on eval: 2914.5620032427855[0m
[37m[1m[2023-07-10 18:32:16,715][227910] Min Reward on eval: 2914.5620032427855[0m
[37m[1m[2023-07-10 18:32:16,715][227910] Mean Reward across all agents: 2914.5620032427855[0m
[37m[1m[2023-07-10 18:32:16,715][227910] Average Trajectory Length: 999.0823333333333[0m
[36m[2023-07-10 18:32:22,241][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:32:22,242][227910] Reward + Measures: [[2231.25732052    0.35229999    0.54619998    0.40229997    0.19520001]
 [ 517.27697453    0.46850005    0.20490001    0.43290001    0.13150001]
 [1300.0727449     0.1954        0.30940002    0.25770003    0.21519999]
 ...
 [2599.71308675    0.29860002    0.41389999    0.26770002    0.21000002]
 [1818.35058986    0.40079999    0.55690002    0.34380001    0.2411    ]
 [-170.98527853    0.56639999    0.58100003    0.69200003    0.32610002]][0m
[37m[1m[2023-07-10 18:32:22,242][227910] Max Reward on eval: 2864.9299007082823[0m
[37m[1m[2023-07-10 18:32:22,242][227910] Min Reward on eval: -895.2679999448941[0m
[37m[1m[2023-07-10 18:32:22,242][227910] Mean Reward across all agents: 1009.3909277276633[0m
[37m[1m[2023-07-10 18:32:22,242][227910] Average Trajectory Length: 982.129[0m
[36m[2023-07-10 18:32:22,246][227910] mean_value=-895.7486905676792, max_value=1657.8855043568915[0m
[37m[1m[2023-07-10 18:32:22,249][227910] New mean coefficients: [[4.210726   0.5574718  1.2178277  0.93336296 1.3434398 ]][0m
[37m[1m[2023-07-10 18:32:22,250][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:32:32,201][227910] train() took 9.95 seconds to complete[0m
[36m[2023-07-10 18:32:32,201][227910] FPS: 385946.41[0m
[36m[2023-07-10 18:32:32,203][227910] itr=1064, itrs=2000, Progress: 53.20%[0m
[36m[2023-07-10 18:32:43,860][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 18:32:43,860][227910] FPS: 330054.05[0m
[36m[2023-07-10 18:32:48,613][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:32:48,613][227910] Reward + Measures: [[2992.37874544    0.31998637    0.43055013    0.31252748    0.20367289]][0m
[37m[1m[2023-07-10 18:32:48,614][227910] Max Reward on eval: 2992.3787454391245[0m
[37m[1m[2023-07-10 18:32:48,614][227910] Min Reward on eval: 2992.3787454391245[0m
[37m[1m[2023-07-10 18:32:48,614][227910] Mean Reward across all agents: 2992.3787454391245[0m
[37m[1m[2023-07-10 18:32:48,614][227910] Average Trajectory Length: 999.286[0m
[36m[2023-07-10 18:32:54,000][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:32:54,000][227910] Reward + Measures: [[-433.8118134     0.2976        0.48459998    0.16600001    0.4797    ]
 [ -17.14629131    0.23196819    0.28787813    0.22383416    0.2596758 ]
 [-373.08770536    0.25480002    0.49540001    0.27190003    0.40769997]
 ...
 [2531.26778767    0.31370002    0.52220005    0.35010001    0.1884    ]
 [-275.56846405    0.19100001    0.49940005    0.19270001    0.51130003]
 [ 308.41488747    0.33419999    0.47869998    0.60409999    0.55400002]][0m
[37m[1m[2023-07-10 18:32:54,001][227910] Max Reward on eval: 2933.2331205117516[0m
[37m[1m[2023-07-10 18:32:54,001][227910] Min Reward on eval: -1262.106538013136[0m
[37m[1m[2023-07-10 18:32:54,001][227910] Mean Reward across all agents: 672.929675391355[0m
[37m[1m[2023-07-10 18:32:54,001][227910] Average Trajectory Length: 981.0833333333333[0m
[36m[2023-07-10 18:32:54,003][227910] mean_value=-1127.7889163853847, max_value=642.397459097329[0m
[37m[1m[2023-07-10 18:32:54,005][227910] New mean coefficients: [[4.0365105  0.01600957 0.53058773 1.1187475  1.5306985 ]][0m
[37m[1m[2023-07-10 18:32:54,006][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:33:03,761][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 18:33:03,761][227910] FPS: 393729.68[0m
[36m[2023-07-10 18:33:03,763][227910] itr=1065, itrs=2000, Progress: 53.25%[0m
[36m[2023-07-10 18:33:15,327][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 18:33:15,328][227910] FPS: 332589.68[0m
[36m[2023-07-10 18:33:20,121][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:33:20,121][227910] Reward + Measures: [[3077.72756269    0.32448375    0.42382488    0.30737513    0.19959061]][0m
[37m[1m[2023-07-10 18:33:20,122][227910] Max Reward on eval: 3077.727562685281[0m
[37m[1m[2023-07-10 18:33:20,122][227910] Min Reward on eval: 3077.727562685281[0m
[37m[1m[2023-07-10 18:33:20,122][227910] Mean Reward across all agents: 3077.727562685281[0m
[37m[1m[2023-07-10 18:33:20,122][227910] Average Trajectory Length: 999.6293333333333[0m
[36m[2023-07-10 18:33:25,574][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:33:25,575][227910] Reward + Measures: [[-365.70717492    0.64161873    0.75459731    0.77425188    0.23802505]
 [1585.39467221    0.31099999    0.39680001    0.36520001    0.23150001]
 [ -30.39875907    0.13133053    0.53124237    0.50275803    0.55614609]
 ...
 [ 496.66224564    0.32699999    0.70429999    0.50240004    0.49850002]
 [ 185.1588128     0.38930002    0.37599999    0.36620003    0.28600001]
 [-129.13177702    0.34277579    0.31176567    0.48592123    0.38627577]][0m
[37m[1m[2023-07-10 18:33:25,575][227910] Max Reward on eval: 3086.693787945807[0m
[37m[1m[2023-07-10 18:33:25,575][227910] Min Reward on eval: -1646.0831746670417[0m
[37m[1m[2023-07-10 18:33:25,575][227910] Mean Reward across all agents: 500.8898388708352[0m
[37m[1m[2023-07-10 18:33:25,575][227910] Average Trajectory Length: 978.8426666666667[0m
[36m[2023-07-10 18:33:25,579][227910] mean_value=-901.7344133858028, max_value=1265.0296773848938[0m
[37m[1m[2023-07-10 18:33:25,581][227910] New mean coefficients: [[ 4.2671137  -0.38371035  0.7518339   1.1119142   2.1127083 ]][0m
[37m[1m[2023-07-10 18:33:25,583][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:33:35,365][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 18:33:35,365][227910] FPS: 392616.94[0m
[36m[2023-07-10 18:33:35,367][227910] itr=1066, itrs=2000, Progress: 53.30%[0m
[36m[2023-07-10 18:33:46,954][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 18:33:46,954][227910] FPS: 332037.53[0m
[36m[2023-07-10 18:33:51,806][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:33:51,806][227910] Reward + Measures: [[3124.1916214     0.32526237    0.43172818    0.3085576     0.1995813 ]][0m
[37m[1m[2023-07-10 18:33:51,807][227910] Max Reward on eval: 3124.191621398303[0m
[37m[1m[2023-07-10 18:33:51,807][227910] Min Reward on eval: 3124.191621398303[0m
[37m[1m[2023-07-10 18:33:51,807][227910] Mean Reward across all agents: 3124.191621398303[0m
[37m[1m[2023-07-10 18:33:51,807][227910] Average Trajectory Length: 999.105[0m
[36m[2023-07-10 18:33:57,460][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:33:57,461][227910] Reward + Measures: [[2074.49458726    0.26649377    0.30455002    0.28300625    0.1830125 ]
 [ 169.41099299    0.26440984    0.35354683    0.27182475    0.23461144]
 [2691.86258264    0.27860001    0.45970002    0.29450002    0.20739999]
 ...
 [2331.51006748    0.28910002    0.38820001    0.29820001    0.22920001]
 [   1.13705691    0.1142        0.80590004    0.70910001    0.75550002]
 [2212.71628406    0.35329998    0.44210002    0.25009999    0.20290001]][0m
[37m[1m[2023-07-10 18:33:57,461][227910] Max Reward on eval: 3128.940645815339[0m
[37m[1m[2023-07-10 18:33:57,461][227910] Min Reward on eval: -464.8231455524685[0m
[37m[1m[2023-07-10 18:33:57,461][227910] Mean Reward across all agents: 1683.741110286649[0m
[37m[1m[2023-07-10 18:33:57,461][227910] Average Trajectory Length: 956.2506666666667[0m
[36m[2023-07-10 18:33:57,464][227910] mean_value=-1500.3111020079014, max_value=522.3700892184689[0m
[37m[1m[2023-07-10 18:33:57,466][227910] New mean coefficients: [[3.9296536  0.4379103  0.9259177  0.15271169 1.9271976 ]][0m
[37m[1m[2023-07-10 18:33:57,467][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:34:07,123][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 18:34:07,123][227910] FPS: 397753.71[0m
[36m[2023-07-10 18:34:07,125][227910] itr=1067, itrs=2000, Progress: 53.35%[0m
[36m[2023-07-10 18:34:18,607][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 18:34:18,607][227910] FPS: 334978.18[0m
[36m[2023-07-10 18:34:23,273][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:34:23,279][227910] Reward + Measures: [[3210.95965121    0.32626384    0.42837572    0.30551776    0.19363302]][0m
[37m[1m[2023-07-10 18:34:23,279][227910] Max Reward on eval: 3210.959651208895[0m
[37m[1m[2023-07-10 18:34:23,279][227910] Min Reward on eval: 3210.959651208895[0m
[37m[1m[2023-07-10 18:34:23,280][227910] Mean Reward across all agents: 3210.959651208895[0m
[37m[1m[2023-07-10 18:34:23,280][227910] Average Trajectory Length: 998.7246666666666[0m
[36m[2023-07-10 18:34:28,763][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:34:28,764][227910] Reward + Measures: [[   92.60696162     0.43790004     0.5729         0.49960002
      0.4973    ]
 [ 1150.36080682     0.23439999     0.46900001     0.28190002
      0.36789998]
 [   16.81091908     0.22979999     0.64490002     0.33800003
      0.53470004]
 ...
 [ -195.13772248     0.26400003     0.69519997     0.53490001
      0.55199999]
 [-1018.04288914     0.12520002     0.47370002     0.29950002
      0.42279997]
 [ -492.40423261     0.2383         0.49960002     0.31819999
      0.42449999]][0m
[37m[1m[2023-07-10 18:34:28,764][227910] Max Reward on eval: 3060.494207679294[0m
[37m[1m[2023-07-10 18:34:28,764][227910] Min Reward on eval: -1866.7352157841901[0m
[37m[1m[2023-07-10 18:34:28,765][227910] Mean Reward across all agents: 334.25935885684146[0m
[37m[1m[2023-07-10 18:34:28,765][227910] Average Trajectory Length: 983.217[0m
[36m[2023-07-10 18:34:28,767][227910] mean_value=-1154.8199925414247, max_value=805.4913521265523[0m
[37m[1m[2023-07-10 18:34:28,770][227910] New mean coefficients: [[3.4426847  0.5952655  0.9443129  0.91464674 1.6048265 ]][0m
[37m[1m[2023-07-10 18:34:28,771][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:34:38,554][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 18:34:38,554][227910] FPS: 392586.83[0m
[36m[2023-07-10 18:34:38,556][227910] itr=1068, itrs=2000, Progress: 53.40%[0m
[36m[2023-07-10 18:34:50,054][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 18:34:50,055][227910] FPS: 334494.47[0m
[36m[2023-07-10 18:34:54,861][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:34:54,862][227910] Reward + Measures: [[3298.37750579    0.32493833    0.43248686    0.30393791    0.19495721]][0m
[37m[1m[2023-07-10 18:34:54,862][227910] Max Reward on eval: 3298.3775057901403[0m
[37m[1m[2023-07-10 18:34:54,862][227910] Min Reward on eval: 3298.3775057901403[0m
[37m[1m[2023-07-10 18:34:54,862][227910] Mean Reward across all agents: 3298.3775057901403[0m
[37m[1m[2023-07-10 18:34:54,862][227910] Average Trajectory Length: 999.516[0m
[36m[2023-07-10 18:35:00,418][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:35:00,419][227910] Reward + Measures: [[ 543.54609691    0.33769998    0.65300006    0.1375        0.53089994]
 [  58.57867477    0.22760001    0.31879997    0.2841        0.24679999]
 [ 590.3721515     0.2393        0.31030002    0.37030002    0.2748    ]
 ...
 [ 431.0863388     0.29730001    0.30970001    0.18410002    0.20390001]
 [-628.87516987    0.2096        0.28830001    0.25530002    0.25830001]
 [-212.27066202    0.33740002    0.44730002    0.35709998    0.41810003]][0m
[37m[1m[2023-07-10 18:35:00,419][227910] Max Reward on eval: 2966.592355134385[0m
[37m[1m[2023-07-10 18:35:00,419][227910] Min Reward on eval: -1229.7860642755434[0m
[37m[1m[2023-07-10 18:35:00,420][227910] Mean Reward across all agents: 270.43781484322295[0m
[37m[1m[2023-07-10 18:35:00,420][227910] Average Trajectory Length: 967.0293333333333[0m
[36m[2023-07-10 18:35:00,422][227910] mean_value=-1745.8445559795146, max_value=1494.7507466343814[0m
[37m[1m[2023-07-10 18:35:00,424][227910] New mean coefficients: [[ 3.6314993  -0.21512753  0.8660901   0.52135396  1.4330559 ]][0m
[37m[1m[2023-07-10 18:35:00,425][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:35:10,218][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 18:35:10,218][227910] FPS: 392201.28[0m
[36m[2023-07-10 18:35:10,221][227910] itr=1069, itrs=2000, Progress: 53.45%[0m
[36m[2023-07-10 18:35:21,923][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 18:35:21,923][227910] FPS: 328656.03[0m
[36m[2023-07-10 18:35:26,746][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:35:26,746][227910] Reward + Measures: [[3360.89937966    0.32059565    0.43508258    0.29944837    0.19065091]][0m
[37m[1m[2023-07-10 18:35:26,747][227910] Max Reward on eval: 3360.899379656265[0m
[37m[1m[2023-07-10 18:35:26,747][227910] Min Reward on eval: 3360.899379656265[0m
[37m[1m[2023-07-10 18:35:26,747][227910] Mean Reward across all agents: 3360.899379656265[0m
[37m[1m[2023-07-10 18:35:26,747][227910] Average Trajectory Length: 999.2463333333333[0m
[36m[2023-07-10 18:35:32,259][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:35:32,259][227910] Reward + Measures: [[-895.34654125    0.28730002    0.78090006    0.52400005    0.67680001]
 [ -29.73916757    0.3206        0.61589998    0.33790001    0.47839999]
 [1150.58158836    0.2346406     0.2935586     0.2224278     0.18743324]
 ...
 [1545.64515662    0.18090001    0.25009999    0.25460002    0.16019998]
 [-347.39538756    0.2067        0.3423        0.16760002    0.1947    ]
 [1775.43046052    0.33129999    0.45150003    0.28700003    0.22060001]][0m
[37m[1m[2023-07-10 18:35:32,260][227910] Max Reward on eval: 3139.881687676441[0m
[37m[1m[2023-07-10 18:35:32,260][227910] Min Reward on eval: -1476.1780603464751[0m
[37m[1m[2023-07-10 18:35:32,260][227910] Mean Reward across all agents: 261.1595481188151[0m
[37m[1m[2023-07-10 18:35:32,260][227910] Average Trajectory Length: 961.2676666666666[0m
[36m[2023-07-10 18:35:32,262][227910] mean_value=-2434.3174406409394, max_value=608.8466243979069[0m
[37m[1m[2023-07-10 18:35:32,265][227910] New mean coefficients: [[3.1837401  0.20138875 0.33327538 0.01773477 0.5128245 ]][0m
[37m[1m[2023-07-10 18:35:32,266][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:35:42,025][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 18:35:42,026][227910] FPS: 393528.60[0m
[36m[2023-07-10 18:35:42,028][227910] itr=1070, itrs=2000, Progress: 53.50%[0m
[37m[1m[2023-07-10 18:35:46,028][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001050[0m
[36m[2023-07-10 18:35:57,779][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 18:35:57,780][227910] FPS: 334325.44[0m
[36m[2023-07-10 18:36:02,520][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:36:02,521][227910] Reward + Measures: [[3406.43226395    0.3213625     0.43359041    0.29918465    0.18997271]][0m
[37m[1m[2023-07-10 18:36:02,521][227910] Max Reward on eval: 3406.4322639548855[0m
[37m[1m[2023-07-10 18:36:02,521][227910] Min Reward on eval: 3406.4322639548855[0m
[37m[1m[2023-07-10 18:36:02,521][227910] Mean Reward across all agents: 3406.4322639548855[0m
[37m[1m[2023-07-10 18:36:02,521][227910] Average Trajectory Length: 998.794[0m
[36m[2023-07-10 18:36:07,970][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:36:07,971][227910] Reward + Measures: [[2176.80282196    0.3416        0.4756        0.27499998    0.2203    ]
 [1453.79404449    0.23975956    0.49609575    0.38851705    0.20814894]
 [ 730.94865988    0.25569999    0.48880005    0.24270001    0.38569999]
 ...
 [1923.35303232    0.38530001    0.26710001    0.40430003    0.20610002]
 [1645.43221263    0.27179998    0.377         0.41079998    0.24779999]
 [1255.19446029    0.19939999    0.24419999    0.3594        0.19939999]][0m
[37m[1m[2023-07-10 18:36:07,971][227910] Max Reward on eval: 3491.151336225169[0m
[37m[1m[2023-07-10 18:36:07,971][227910] Min Reward on eval: -1169.9542777096387[0m
[37m[1m[2023-07-10 18:36:07,971][227910] Mean Reward across all agents: 909.6191864694418[0m
[37m[1m[2023-07-10 18:36:07,971][227910] Average Trajectory Length: 962.2556666666667[0m
[36m[2023-07-10 18:36:07,974][227910] mean_value=-1174.1945756164152, max_value=1406.3361492361546[0m
[37m[1m[2023-07-10 18:36:07,977][227910] New mean coefficients: [[ 3.0195196   0.6458285   0.11287875 -0.7459115   0.6938301 ]][0m
[37m[1m[2023-07-10 18:36:07,978][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:36:17,527][227910] train() took 9.55 seconds to complete[0m
[36m[2023-07-10 18:36:17,527][227910] FPS: 402188.23[0m
[36m[2023-07-10 18:36:17,530][227910] itr=1071, itrs=2000, Progress: 53.55%[0m
[36m[2023-07-10 18:36:29,225][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 18:36:29,226][227910] FPS: 328889.94[0m
[36m[2023-07-10 18:36:33,946][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:36:33,946][227910] Reward + Measures: [[3476.97892733    0.32562444    0.42805469    0.29557058    0.19057821]][0m
[37m[1m[2023-07-10 18:36:33,946][227910] Max Reward on eval: 3476.9789273330393[0m
[37m[1m[2023-07-10 18:36:33,947][227910] Min Reward on eval: 3476.9789273330393[0m
[37m[1m[2023-07-10 18:36:33,947][227910] Mean Reward across all agents: 3476.9789273330393[0m
[37m[1m[2023-07-10 18:36:33,947][227910] Average Trajectory Length: 999.0103333333333[0m
[36m[2023-07-10 18:36:39,499][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:36:39,499][227910] Reward + Measures: [[-459.17197691    0.21900001    0.13869999    0.3511        0.1732    ]
 [ 171.35827029    0.4779        0.41260001    0.36470005    0.25909999]
 [  88.40951774    0.35395285    0.28886461    0.24494195    0.26339981]
 ...
 [1253.78499479    0.1617        0.49130002    0.3274        0.26120004]
 [-332.28140144    0.59450001    0.3935        0.62849998    0.0895    ]
 [ 337.195664      0.25600001    0.139         0.37330002    0.1644    ]][0m
[37m[1m[2023-07-10 18:36:39,500][227910] Max Reward on eval: 3053.5686732525005[0m
[37m[1m[2023-07-10 18:36:39,500][227910] Min Reward on eval: -1303.6754964499153[0m
[37m[1m[2023-07-10 18:36:39,500][227910] Mean Reward across all agents: 445.300127768313[0m
[37m[1m[2023-07-10 18:36:39,500][227910] Average Trajectory Length: 922.3053333333334[0m
[36m[2023-07-10 18:36:39,502][227910] mean_value=-1762.5059746464917, max_value=1136.7077687087724[0m
[37m[1m[2023-07-10 18:36:39,505][227910] New mean coefficients: [[ 3.1196725   0.8971242   0.47821724 -0.7967831   1.353883  ]][0m
[37m[1m[2023-07-10 18:36:39,506][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:36:49,400][227910] train() took 9.89 seconds to complete[0m
[36m[2023-07-10 18:36:49,401][227910] FPS: 388143.77[0m
[36m[2023-07-10 18:36:49,403][227910] itr=1072, itrs=2000, Progress: 53.60%[0m
[36m[2023-07-10 18:37:01,122][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 18:37:01,122][227910] FPS: 328279.60[0m
[36m[2023-07-10 18:37:05,908][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:37:05,908][227910] Reward + Measures: [[3540.2415312     0.33088878    0.42794931    0.28893498    0.18780652]][0m
[37m[1m[2023-07-10 18:37:05,908][227910] Max Reward on eval: 3540.241531200787[0m
[37m[1m[2023-07-10 18:37:05,909][227910] Min Reward on eval: 3540.241531200787[0m
[37m[1m[2023-07-10 18:37:05,909][227910] Mean Reward across all agents: 3540.241531200787[0m
[37m[1m[2023-07-10 18:37:05,909][227910] Average Trajectory Length: 998.3126666666666[0m
[36m[2023-07-10 18:37:11,529][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:37:11,529][227910] Reward + Measures: [[ 691.20868478    0.29554197    0.34366971    0.29193917    0.14284956]
 [ 247.49666627    0.13320452    0.16413932    0.25250605    0.17097591]
 [ 198.04301761    0.27783903    0.42182198    0.24845122    0.33353901]
 ...
 [ 878.71028822    0.25599912    0.34841129    0.34572512    0.16913645]
 [2273.98374506    0.34219998    0.42109999    0.2516        0.20349999]
 [-177.25232375    0.22590001    0.2145        0.22040001    0.22379999]][0m
[37m[1m[2023-07-10 18:37:11,529][227910] Max Reward on eval: 3138.0652499762364[0m
[37m[1m[2023-07-10 18:37:11,530][227910] Min Reward on eval: -1260.0777292211774[0m
[37m[1m[2023-07-10 18:37:11,530][227910] Mean Reward across all agents: 597.5912808805198[0m
[37m[1m[2023-07-10 18:37:11,530][227910] Average Trajectory Length: 915.3876666666666[0m
[36m[2023-07-10 18:37:11,532][227910] mean_value=-1950.9588496596712, max_value=838.4895762664571[0m
[37m[1m[2023-07-10 18:37:11,534][227910] New mean coefficients: [[ 2.5089614   0.11519116  0.05942696 -1.0277883   0.8623209 ]][0m
[37m[1m[2023-07-10 18:37:11,535][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:37:21,194][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 18:37:21,194][227910] FPS: 397626.06[0m
[36m[2023-07-10 18:37:21,197][227910] itr=1073, itrs=2000, Progress: 53.65%[0m
[36m[2023-07-10 18:37:32,752][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 18:37:32,752][227910] FPS: 332874.18[0m
[36m[2023-07-10 18:37:37,561][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:37:37,562][227910] Reward + Measures: [[3610.61795154    0.33473092    0.42012191    0.28118744    0.18680026]][0m
[37m[1m[2023-07-10 18:37:37,562][227910] Max Reward on eval: 3610.6179515413205[0m
[37m[1m[2023-07-10 18:37:37,562][227910] Min Reward on eval: 3610.6179515413205[0m
[37m[1m[2023-07-10 18:37:37,562][227910] Mean Reward across all agents: 3610.6179515413205[0m
[37m[1m[2023-07-10 18:37:37,563][227910] Average Trajectory Length: 999.6826666666666[0m
[36m[2023-07-10 18:37:43,112][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:37:43,112][227910] Reward + Measures: [[ 586.29549993    0.60353339    0.29785559    0.45587778    0.21966667]
 [ -84.88082321    0.31720003    0.32250002    0.28290001    0.29020002]
 [ -21.75688133    0.38299999    0.53010005    0.32839999    0.62920004]
 ...
 [-271.61832094    0.55746418    0.3376604     0.51097268    0.3568283 ]
 [-932.88591576    0.49501458    0.18430917    0.43463871    0.20615816]
 [-451.50177725    0.0563        0.0832        0.11619999    0.0848    ]][0m
[37m[1m[2023-07-10 18:37:43,113][227910] Max Reward on eval: 3260.1556428147946[0m
[37m[1m[2023-07-10 18:37:43,113][227910] Min Reward on eval: -1420.3327242000182[0m
[37m[1m[2023-07-10 18:37:43,113][227910] Mean Reward across all agents: 338.77070000792173[0m
[37m[1m[2023-07-10 18:37:43,113][227910] Average Trajectory Length: 927.2389999999999[0m
[36m[2023-07-10 18:37:43,116][227910] mean_value=-1495.755867356765, max_value=2829.28810991075[0m
[37m[1m[2023-07-10 18:37:43,119][227910] New mean coefficients: [[ 2.5537195   0.13067386 -0.23762554 -0.90490174  0.72686815]][0m
[37m[1m[2023-07-10 18:37:43,119][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:37:53,052][227910] train() took 9.93 seconds to complete[0m
[36m[2023-07-10 18:37:53,052][227910] FPS: 386691.72[0m
[36m[2023-07-10 18:37:53,054][227910] itr=1074, itrs=2000, Progress: 53.70%[0m
[36m[2023-07-10 18:38:04,731][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 18:38:04,732][227910] FPS: 329359.59[0m
[36m[2023-07-10 18:38:09,509][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:38:09,509][227910] Reward + Measures: [[3698.85308886    0.34637344    0.41912913    0.27198583    0.18018135]][0m
[37m[1m[2023-07-10 18:38:09,510][227910] Max Reward on eval: 3698.853088855023[0m
[37m[1m[2023-07-10 18:38:09,510][227910] Min Reward on eval: 3698.853088855023[0m
[37m[1m[2023-07-10 18:38:09,510][227910] Mean Reward across all agents: 3698.853088855023[0m
[37m[1m[2023-07-10 18:38:09,510][227910] Average Trajectory Length: 998.558[0m
[36m[2023-07-10 18:38:14,997][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:38:14,998][227910] Reward + Measures: [[ 1642.01815429     0.27725556     0.33751112     0.21032222
      0.21906666]
 [-1096.98272597     0.2509         0.26299998     0.26519999
      0.1794    ]
 [ 3395.97686681     0.36579999     0.42150003     0.3258
      0.19410001]
 ...
 [  432.44645892     0.25177583     0.42152172     0.48801595
      0.27672192]
 [ 1483.11559158     0.17690001     0.21069999     0.20319998
      0.13030002]
 [  136.38872904     0.16799998     0.17989999     0.12640001
      0.10150001]][0m
[37m[1m[2023-07-10 18:38:14,998][227910] Max Reward on eval: 3395.97686680709[0m
[37m[1m[2023-07-10 18:38:14,999][227910] Min Reward on eval: -1211.5326273313956[0m
[37m[1m[2023-07-10 18:38:14,999][227910] Mean Reward across all agents: 544.386049409791[0m
[37m[1m[2023-07-10 18:38:14,999][227910] Average Trajectory Length: 917.2343333333333[0m
[36m[2023-07-10 18:38:15,003][227910] mean_value=-1255.7612602112833, max_value=1733.6157696524847[0m
[37m[1m[2023-07-10 18:38:15,006][227910] New mean coefficients: [[ 2.3593965   0.09165891 -0.20757513 -0.75890666  1.123835  ]][0m
[37m[1m[2023-07-10 18:38:15,007][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:38:24,866][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 18:38:24,867][227910] FPS: 389546.72[0m
[36m[2023-07-10 18:38:24,869][227910] itr=1075, itrs=2000, Progress: 53.75%[0m
[36m[2023-07-10 18:38:36,544][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 18:38:36,544][227910] FPS: 329431.45[0m
[36m[2023-07-10 18:38:41,293][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:38:41,293][227910] Reward + Measures: [[3703.79374551    0.34709272    0.39856896    0.26716384    0.17775272]][0m
[37m[1m[2023-07-10 18:38:41,294][227910] Max Reward on eval: 3703.793745513202[0m
[37m[1m[2023-07-10 18:38:41,294][227910] Min Reward on eval: 3703.793745513202[0m
[37m[1m[2023-07-10 18:38:41,294][227910] Mean Reward across all agents: 3703.793745513202[0m
[37m[1m[2023-07-10 18:38:41,294][227910] Average Trajectory Length: 997.4876666666667[0m
[36m[2023-07-10 18:38:46,635][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:38:46,636][227910] Reward + Measures: [[1272.16976922    0.23758994    0.45340872    0.27464566    0.26327452]
 [ 377.4009744     0.1857        0.2545        0.1714        0.19200002]
 [-419.39232739    0.14860001    0.21589999    0.1453        0.15979999]
 ...
 [ -49.14587149    0.19351444    0.269766      0.17645217    0.20170103]
 [ -24.03324424    0.1876        0.26760003    0.18010001    0.2005    ]
 [-501.70781339    0.2572        0.46790001    0.4337        0.47419998]][0m
[37m[1m[2023-07-10 18:38:46,636][227910] Max Reward on eval: 3041.9745615724705[0m
[37m[1m[2023-07-10 18:38:46,636][227910] Min Reward on eval: -1568.0242685290752[0m
[37m[1m[2023-07-10 18:38:46,636][227910] Mean Reward across all agents: 214.49700624570366[0m
[37m[1m[2023-07-10 18:38:46,637][227910] Average Trajectory Length: 941.9586666666667[0m
[36m[2023-07-10 18:38:46,639][227910] mean_value=-1721.4686849190955, max_value=797.0086349586024[0m
[37m[1m[2023-07-10 18:38:46,641][227910] New mean coefficients: [[ 1.8319468   0.14628565 -0.44001174 -0.5376109   0.22051102]][0m
[37m[1m[2023-07-10 18:38:46,642][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:38:56,330][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 18:38:56,330][227910] FPS: 396446.76[0m
[36m[2023-07-10 18:38:56,332][227910] itr=1076, itrs=2000, Progress: 53.80%[0m
[36m[2023-07-10 18:39:07,869][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 18:39:07,869][227910] FPS: 333488.67[0m
[36m[2023-07-10 18:39:12,592][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:39:12,592][227910] Reward + Measures: [[3752.5998938     0.35110271    0.39068425    0.25495249    0.17543522]][0m
[37m[1m[2023-07-10 18:39:12,592][227910] Max Reward on eval: 3752.599893797548[0m
[37m[1m[2023-07-10 18:39:12,593][227910] Min Reward on eval: 3752.599893797548[0m
[37m[1m[2023-07-10 18:39:12,593][227910] Mean Reward across all agents: 3752.599893797548[0m
[37m[1m[2023-07-10 18:39:12,593][227910] Average Trajectory Length: 998.7969999999999[0m
[36m[2023-07-10 18:39:18,002][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:39:18,003][227910] Reward + Measures: [[-809.78356464    0.16490002    0.18970001    0.3229        0.1864    ]
 [ 529.67743593    0.38410002    0.34699997    0.22649999    0.29130003]
 [-344.22671529    0.32322225    0.44522223    0.1916889     0.38218889]
 ...
 [-197.19808497    0.27342689    0.29543462    0.20122717    0.28395024]
 [1939.82956394    0.44772291    0.33096454    0.19253829    0.17557161]
 [ 489.90987781    0.16          0.22680001    0.25490001    0.1944    ]][0m
[37m[1m[2023-07-10 18:39:18,003][227910] Max Reward on eval: 3712.2301501850598[0m
[37m[1m[2023-07-10 18:39:18,004][227910] Min Reward on eval: -931.1811664048349[0m
[37m[1m[2023-07-10 18:39:18,004][227910] Mean Reward across all agents: 739.6437922499327[0m
[37m[1m[2023-07-10 18:39:18,004][227910] Average Trajectory Length: 935.211[0m
[36m[2023-07-10 18:39:18,006][227910] mean_value=-1818.3579720116802, max_value=1899.9102338160785[0m
[37m[1m[2023-07-10 18:39:18,008][227910] New mean coefficients: [[ 2.031559    0.22506908  0.12548822 -0.43641934  0.7043735 ]][0m
[37m[1m[2023-07-10 18:39:18,009][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:39:27,546][227910] train() took 9.53 seconds to complete[0m
[36m[2023-07-10 18:39:27,546][227910] FPS: 402730.51[0m
[36m[2023-07-10 18:39:27,548][227910] itr=1077, itrs=2000, Progress: 53.85%[0m
[36m[2023-07-10 18:39:39,048][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 18:39:39,048][227910] FPS: 334512.66[0m
[36m[2023-07-10 18:39:43,872][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:39:43,872][227910] Reward + Measures: [[3789.42386359    0.34707633    0.39404517    0.26009288    0.17470168]][0m
[37m[1m[2023-07-10 18:39:43,873][227910] Max Reward on eval: 3789.4238635892957[0m
[37m[1m[2023-07-10 18:39:43,873][227910] Min Reward on eval: 3789.4238635892957[0m
[37m[1m[2023-07-10 18:39:43,873][227910] Mean Reward across all agents: 3789.4238635892957[0m
[37m[1m[2023-07-10 18:39:43,873][227910] Average Trajectory Length: 997.5456666666666[0m
[36m[2023-07-10 18:39:49,515][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:39:49,516][227910] Reward + Measures: [[1536.79183992    0.31999999    0.42210004    0.2617        0.20570002]
 [-403.08705104    0.40279999    0.186         0.53280002    0.15510002]
 [ 721.18973887    0.32468548    0.25308654    0.26057479    0.14332013]
 ...
 [ 678.2929667     0.31276166    0.16996951    0.38012692    0.17985807]
 [1134.23357397    0.2237381     0.30170855    0.25379673    0.23860042]
 [-244.38705667    0.13434742    0.17021401    0.15984514    0.17933314]][0m
[37m[1m[2023-07-10 18:39:49,516][227910] Max Reward on eval: 3589.925049583241[0m
[37m[1m[2023-07-10 18:39:49,516][227910] Min Reward on eval: -1160.1710646000224[0m
[37m[1m[2023-07-10 18:39:49,516][227910] Mean Reward across all agents: 952.3095694790047[0m
[37m[1m[2023-07-10 18:39:49,516][227910] Average Trajectory Length: 927.2679999999999[0m
[36m[2023-07-10 18:39:49,518][227910] mean_value=-1881.1630713934214, max_value=1316.211764420671[0m
[37m[1m[2023-07-10 18:39:49,521][227910] New mean coefficients: [[ 2.070107    0.90126824 -0.01683202  0.04233912  0.7662643 ]][0m
[37m[1m[2023-07-10 18:39:49,522][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:39:59,226][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 18:39:59,226][227910] FPS: 395770.51[0m
[36m[2023-07-10 18:39:59,228][227910] itr=1078, itrs=2000, Progress: 53.90%[0m
[36m[2023-07-10 18:40:10,789][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 18:40:10,789][227910] FPS: 332799.49[0m
[36m[2023-07-10 18:40:15,629][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:40:15,630][227910] Reward + Measures: [[3857.01969192    0.34565556    0.38421011    0.26114056    0.17445439]][0m
[37m[1m[2023-07-10 18:40:15,630][227910] Max Reward on eval: 3857.0196919218847[0m
[37m[1m[2023-07-10 18:40:15,630][227910] Min Reward on eval: 3857.0196919218847[0m
[37m[1m[2023-07-10 18:40:15,630][227910] Mean Reward across all agents: 3857.0196919218847[0m
[37m[1m[2023-07-10 18:40:15,631][227910] Average Trajectory Length: 998.8303333333333[0m
[36m[2023-07-10 18:40:21,173][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:40:21,174][227910] Reward + Measures: [[ 832.22836914    0.5061        0.21950002    0.26690003    0.19420002]
 [-207.01040032    0.15460001    0.4594        0.31979999    0.48859999]
 [ -53.3814499     0.2748        0.32699999    0.42309999    0.25680003]
 ...
 [1263.74798815    0.27840003    0.4052        0.48049998    0.2289    ]
 [1454.24627325    0.26300001    0.27959999    0.28049999    0.18170001]
 [ 113.65845562    0.24834839    0.40390155    0.35361278    0.32113898]][0m
[37m[1m[2023-07-10 18:40:21,174][227910] Max Reward on eval: 3528.415958302468[0m
[37m[1m[2023-07-10 18:40:21,174][227910] Min Reward on eval: -1239.9028821739253[0m
[37m[1m[2023-07-10 18:40:21,175][227910] Mean Reward across all agents: 695.4957063997314[0m
[37m[1m[2023-07-10 18:40:21,175][227910] Average Trajectory Length: 962.4699999999999[0m
[36m[2023-07-10 18:40:21,177][227910] mean_value=-1354.4082284605959, max_value=1031.087413765072[0m
[37m[1m[2023-07-10 18:40:21,180][227910] New mean coefficients: [[ 1.9525552   0.78347087 -0.13485305  0.6386223   0.5227425 ]][0m
[37m[1m[2023-07-10 18:40:21,181][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:40:30,990][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 18:40:30,991][227910] FPS: 391509.95[0m
[36m[2023-07-10 18:40:30,993][227910] itr=1079, itrs=2000, Progress: 53.95%[0m
[36m[2023-07-10 18:40:42,610][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 18:40:42,610][227910] FPS: 331183.82[0m
[36m[2023-07-10 18:40:47,469][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:40:47,469][227910] Reward + Measures: [[3880.97327073    0.34515592    0.39921382    0.26602131    0.17926119]][0m
[37m[1m[2023-07-10 18:40:47,470][227910] Max Reward on eval: 3880.973270728997[0m
[37m[1m[2023-07-10 18:40:47,470][227910] Min Reward on eval: 3880.973270728997[0m
[37m[1m[2023-07-10 18:40:47,470][227910] Mean Reward across all agents: 3880.973270728997[0m
[37m[1m[2023-07-10 18:40:47,470][227910] Average Trajectory Length: 998.525[0m
[36m[2023-07-10 18:40:52,847][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:40:52,847][227910] Reward + Measures: [[1748.20151163    0.3594        0.3522        0.31440002    0.17550002]
 [1640.3275104     0.29349321    0.34333149    0.31040975    0.23532975]
 [ 683.18322619    0.28310001    0.4693        0.37990001    0.3263    ]
 ...
 [ 737.00850791    0.20162295    0.25995171    0.27922612    0.1616542 ]
 [  41.96483197    0.17475238    0.55623335    0.41073331    0.35119048]
 [-215.84134867    0.36060771    0.52785391    0.44246921    0.47223845]][0m
[37m[1m[2023-07-10 18:40:52,848][227910] Max Reward on eval: 3679.4565113329795[0m
[37m[1m[2023-07-10 18:40:52,848][227910] Min Reward on eval: -1275.6139411752345[0m
[37m[1m[2023-07-10 18:40:52,848][227910] Mean Reward across all agents: 892.012021085606[0m
[37m[1m[2023-07-10 18:40:52,849][227910] Average Trajectory Length: 923.2046666666666[0m
[36m[2023-07-10 18:40:52,851][227910] mean_value=-1722.8194127307745, max_value=761.6495357999354[0m
[37m[1m[2023-07-10 18:40:52,853][227910] New mean coefficients: [[2.2855973  0.17604935 0.38421547 0.67591995 0.836995  ]][0m
[37m[1m[2023-07-10 18:40:52,854][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:41:02,650][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 18:41:02,651][227910] FPS: 392052.94[0m
[36m[2023-07-10 18:41:02,653][227910] itr=1080, itrs=2000, Progress: 54.00%[0m
[37m[1m[2023-07-10 18:41:06,611][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001060[0m
[36m[2023-07-10 18:41:18,569][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 18:41:18,570][227910] FPS: 328648.96[0m
[36m[2023-07-10 18:41:23,299][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:41:23,299][227910] Reward + Measures: [[3930.25784813    0.34431455    0.39291659    0.26694772    0.17799747]][0m
[37m[1m[2023-07-10 18:41:23,300][227910] Max Reward on eval: 3930.2578481298287[0m
[37m[1m[2023-07-10 18:41:23,300][227910] Min Reward on eval: 3930.2578481298287[0m
[37m[1m[2023-07-10 18:41:23,300][227910] Mean Reward across all agents: 3930.2578481298287[0m
[37m[1m[2023-07-10 18:41:23,300][227910] Average Trajectory Length: 999.084[0m
[36m[2023-07-10 18:41:28,736][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:41:28,736][227910] Reward + Measures: [[ 318.27445003    0.61449999    0.67510003    0.64120001    0.70209998]
 [ 789.76004686    0.39219999    0.28649998    0.5535        0.21329999]
 [ 229.35020149    0.70380002    0.74739999    0.66100001    0.70260006]
 ...
 [  78.52063221    0.78390008    0.40440002    0.68519992    0.19670001]
 [1206.89046748    0.25610447    0.3669672     0.32645521    0.20784178]
 [ 263.69930852    0.58260006    0.43600002    0.23650001    0.3299    ]][0m
[37m[1m[2023-07-10 18:41:28,736][227910] Max Reward on eval: 3769.4698861155657[0m
[37m[1m[2023-07-10 18:41:28,737][227910] Min Reward on eval: -1181.0116907694726[0m
[37m[1m[2023-07-10 18:41:28,737][227910] Mean Reward across all agents: 817.7646370695243[0m
[37m[1m[2023-07-10 18:41:28,737][227910] Average Trajectory Length: 975.5783333333333[0m
[36m[2023-07-10 18:41:28,742][227910] mean_value=-965.8586442936429, max_value=2326.0778519996306[0m
[37m[1m[2023-07-10 18:41:28,744][227910] New mean coefficients: [[ 2.187704    0.76425856 -0.13979232  0.46053317  0.6091044 ]][0m
[37m[1m[2023-07-10 18:41:28,745][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:41:38,402][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 18:41:38,402][227910] FPS: 397711.48[0m
[36m[2023-07-10 18:41:38,405][227910] itr=1081, itrs=2000, Progress: 54.05%[0m
[36m[2023-07-10 18:41:49,934][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 18:41:49,935][227910] FPS: 333578.82[0m
[36m[2023-07-10 18:41:54,737][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:41:54,738][227910] Reward + Measures: [[4002.65268332    0.34746468    0.37775648    0.26652122    0.17494158]][0m
[37m[1m[2023-07-10 18:41:54,738][227910] Max Reward on eval: 4002.652683324644[0m
[37m[1m[2023-07-10 18:41:54,738][227910] Min Reward on eval: 4002.652683324644[0m
[37m[1m[2023-07-10 18:41:54,738][227910] Mean Reward across all agents: 4002.652683324644[0m
[37m[1m[2023-07-10 18:41:54,738][227910] Average Trajectory Length: 998.324[0m
[36m[2023-07-10 18:42:00,216][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:42:00,217][227910] Reward + Measures: [[2197.58653196    0.31529999    0.47589999    0.40150005    0.2067    ]
 [ 162.76961649    0.29850003    0.41940004    0.37909999    0.22490001]
 [1859.77507153    0.38700002    0.34530002    0.27270001    0.24700001]
 ...
 [3551.86520255    0.34230003    0.37860003    0.249         0.1945    ]
 [ 434.24307854    0.1663        0.32149997    0.2428        0.2066    ]
 [ 230.86430811    0.50580001    0.46529999    0.43479997    0.30669999]][0m
[37m[1m[2023-07-10 18:42:00,217][227910] Max Reward on eval: 3903.5321025732906[0m
[37m[1m[2023-07-10 18:42:00,217][227910] Min Reward on eval: -969.3156129138777[0m
[37m[1m[2023-07-10 18:42:00,218][227910] Mean Reward across all agents: 1072.2002477122383[0m
[37m[1m[2023-07-10 18:42:00,218][227910] Average Trajectory Length: 973.8933333333333[0m
[36m[2023-07-10 18:42:00,219][227910] mean_value=-1759.6319538283192, max_value=863.5109365708313[0m
[37m[1m[2023-07-10 18:42:00,222][227910] New mean coefficients: [[1.9279578  1.2098227  0.16286552 0.11777136 0.4585169 ]][0m
[37m[1m[2023-07-10 18:42:00,223][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:42:10,003][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 18:42:10,003][227910] FPS: 392698.61[0m
[36m[2023-07-10 18:42:10,006][227910] itr=1082, itrs=2000, Progress: 54.10%[0m
[36m[2023-07-10 18:42:21,716][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 18:42:21,716][227910] FPS: 328450.63[0m
[36m[2023-07-10 18:42:26,573][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:42:26,573][227910] Reward + Measures: [[4067.06306703    0.35209832    0.36713713    0.2600303     0.17438123]][0m
[37m[1m[2023-07-10 18:42:26,574][227910] Max Reward on eval: 4067.0630670264463[0m
[37m[1m[2023-07-10 18:42:26,574][227910] Min Reward on eval: 4067.0630670264463[0m
[37m[1m[2023-07-10 18:42:26,574][227910] Mean Reward across all agents: 4067.0630670264463[0m
[37m[1m[2023-07-10 18:42:26,575][227910] Average Trajectory Length: 998.6266666666667[0m
[36m[2023-07-10 18:42:31,973][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:42:31,974][227910] Reward + Measures: [[ 224.15659834    0.28891414    0.21624966    0.29737249    0.06107045]
 [1132.63159657    0.2902002     0.35804555    0.27809468    0.32766613]
 [1977.09923874    0.32611027    0.29840717    0.32125443    0.20531487]
 ...
 [1547.47036817    0.26139998    0.32300001    0.33010003    0.22730003]
 [1473.99017847    0.21418412    0.28024235    0.3387439     0.16797748]
 [ -39.34484493    0.25822398    0.52648616    0.37483147    0.39677563]][0m
[37m[1m[2023-07-10 18:42:31,974][227910] Max Reward on eval: 3733.5387396421283[0m
[37m[1m[2023-07-10 18:42:31,974][227910] Min Reward on eval: -389.41696135721287[0m
[37m[1m[2023-07-10 18:42:31,974][227910] Mean Reward across all agents: 1452.0667615647983[0m
[37m[1m[2023-07-10 18:42:31,975][227910] Average Trajectory Length: 939.2413333333333[0m
[36m[2023-07-10 18:42:31,977][227910] mean_value=-1498.821774865982, max_value=781.1673194415634[0m
[37m[1m[2023-07-10 18:42:31,980][227910] New mean coefficients: [[2.353815   0.3898192  0.34611103 0.5098975  0.9572903 ]][0m
[37m[1m[2023-07-10 18:42:31,981][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:42:41,693][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 18:42:41,694][227910] FPS: 395430.68[0m
[36m[2023-07-10 18:42:41,696][227910] itr=1083, itrs=2000, Progress: 54.15%[0m
[36m[2023-07-10 18:42:53,169][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 18:42:53,170][227910] FPS: 335307.83[0m
[36m[2023-07-10 18:42:57,894][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:42:57,894][227910] Reward + Measures: [[4140.16485836    0.35481244    0.35432389    0.25424197    0.17298779]][0m
[37m[1m[2023-07-10 18:42:57,895][227910] Max Reward on eval: 4140.164858362047[0m
[37m[1m[2023-07-10 18:42:57,895][227910] Min Reward on eval: 4140.164858362047[0m
[37m[1m[2023-07-10 18:42:57,895][227910] Mean Reward across all agents: 4140.164858362047[0m
[37m[1m[2023-07-10 18:42:57,895][227910] Average Trajectory Length: 998.6129999999999[0m
[36m[2023-07-10 18:43:03,331][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:43:03,332][227910] Reward + Measures: [[1137.37149708    0.57120001    0.287         0.37170002    0.1556    ]
 [2095.05941643    0.39440003    0.27830002    0.2744        0.13600002]
 [1577.08623893    0.36880001    0.35640001    0.32670003    0.17160001]
 ...
 [3107.15640735    0.36380002    0.31820002    0.29949999    0.1499    ]
 [-323.84966747    0.32958177    0.2240392     0.29924804    0.11126897]
 [-330.37126623    0.59361374    0.32685766    0.48549461    0.19708498]][0m
[37m[1m[2023-07-10 18:43:03,332][227910] Max Reward on eval: 3743.2362172726544[0m
[37m[1m[2023-07-10 18:43:03,332][227910] Min Reward on eval: -1298.5645433639409[0m
[37m[1m[2023-07-10 18:43:03,333][227910] Mean Reward across all agents: 872.1043749293855[0m
[37m[1m[2023-07-10 18:43:03,333][227910] Average Trajectory Length: 957.4466666666666[0m
[36m[2023-07-10 18:43:03,335][227910] mean_value=-1833.6250144224393, max_value=983.6788497589031[0m
[37m[1m[2023-07-10 18:43:03,337][227910] New mean coefficients: [[ 2.4725885  -0.19646668 -0.07968566 -0.02874058  0.5722376 ]][0m
[37m[1m[2023-07-10 18:43:03,338][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:43:13,111][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 18:43:13,111][227910] FPS: 393002.75[0m
[36m[2023-07-10 18:43:13,113][227910] itr=1084, itrs=2000, Progress: 54.20%[0m
[36m[2023-07-10 18:43:24,694][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 18:43:24,695][227910] FPS: 332187.67[0m
[36m[2023-07-10 18:43:29,597][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:43:29,597][227910] Reward + Measures: [[4188.61054891    0.35792434    0.35577717    0.25416499    0.1738449 ]][0m
[37m[1m[2023-07-10 18:43:29,597][227910] Max Reward on eval: 4188.610548911935[0m
[37m[1m[2023-07-10 18:43:29,598][227910] Min Reward on eval: 4188.610548911935[0m
[37m[1m[2023-07-10 18:43:29,598][227910] Mean Reward across all agents: 4188.610548911935[0m
[37m[1m[2023-07-10 18:43:29,598][227910] Average Trajectory Length: 998.4153333333333[0m
[36m[2023-07-10 18:43:35,171][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:43:35,172][227910] Reward + Measures: [[ 719.67447903    0.34289575    0.30190012    0.29854092    0.22358404]
 [ -48.14174301    0.72869998    0.12049999    0.77590001    0.50800002]
 [2180.42418606    0.2605198     0.36621317    0.35329339    0.21208131]
 ...
 [1118.25614155    0.32771024    0.26007608    0.23521881    0.1930393 ]
 [2266.46375005    0.32449999    0.22410002    0.22690001    0.1767    ]
 [ -52.06412604    0.24250002    0.27760002    0.2825        0.22459999]][0m
[37m[1m[2023-07-10 18:43:35,172][227910] Max Reward on eval: 4054.997100344114[0m
[37m[1m[2023-07-10 18:43:35,172][227910] Min Reward on eval: -627.6013697884744[0m
[37m[1m[2023-07-10 18:43:35,173][227910] Mean Reward across all agents: 1434.3047393975216[0m
[37m[1m[2023-07-10 18:43:35,173][227910] Average Trajectory Length: 969.6743333333333[0m
[36m[2023-07-10 18:43:35,175][227910] mean_value=-1760.5765723365337, max_value=1777.7648931880535[0m
[37m[1m[2023-07-10 18:43:35,177][227910] New mean coefficients: [[ 2.34258    -0.24916595 -0.2444227   0.34727076  0.7581295 ]][0m
[37m[1m[2023-07-10 18:43:35,178][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:43:44,910][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 18:43:44,911][227910] FPS: 394631.45[0m
[36m[2023-07-10 18:43:44,913][227910] itr=1085, itrs=2000, Progress: 54.25%[0m
[36m[2023-07-10 18:43:56,548][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 18:43:56,548][227910] FPS: 330626.36[0m
[36m[2023-07-10 18:44:01,342][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:44:01,342][227910] Reward + Measures: [[4253.39204646    0.35456139    0.34611297    0.25385857    0.17199892]][0m
[37m[1m[2023-07-10 18:44:01,342][227910] Max Reward on eval: 4253.3920464591965[0m
[37m[1m[2023-07-10 18:44:01,342][227910] Min Reward on eval: 4253.3920464591965[0m
[37m[1m[2023-07-10 18:44:01,343][227910] Mean Reward across all agents: 4253.3920464591965[0m
[37m[1m[2023-07-10 18:44:01,343][227910] Average Trajectory Length: 997.7143333333333[0m
[36m[2023-07-10 18:44:06,833][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:44:06,833][227910] Reward + Measures: [[ 675.45297062    0.28459999    0.45180002    0.29679999    0.21729998]
 [1093.36509862    0.22775403    0.38326707    0.25824431    0.30584383]
 [1832.10166927    0.25219998    0.50030005    0.2554        0.28079998]
 ...
 [ 766.60500605    0.24010001    0.48140001    0.2836        0.23450001]
 [ -59.02040035    0.34470001    0.52179998    0.35479999    0.36230001]
 [ 162.0561433     0.25364739    0.32028949    0.31123158    0.22385788]][0m
[37m[1m[2023-07-10 18:44:06,834][227910] Max Reward on eval: 4071.384806293482[0m
[37m[1m[2023-07-10 18:44:06,834][227910] Min Reward on eval: -984.7076648435323[0m
[37m[1m[2023-07-10 18:44:06,834][227910] Mean Reward across all agents: 890.6439313246302[0m
[37m[1m[2023-07-10 18:44:06,834][227910] Average Trajectory Length: 973.882[0m
[36m[2023-07-10 18:44:06,836][227910] mean_value=-1903.177984284683, max_value=261.043613391799[0m
[37m[1m[2023-07-10 18:44:06,838][227910] New mean coefficients: [[ 2.5299683   0.15601647  0.0261482  -0.22649434  0.65291494]][0m
[37m[1m[2023-07-10 18:44:06,839][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:44:16,514][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 18:44:16,514][227910] FPS: 396985.22[0m
[36m[2023-07-10 18:44:16,516][227910] itr=1086, itrs=2000, Progress: 54.30%[0m
[36m[2023-07-10 18:44:28,040][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 18:44:28,041][227910] FPS: 333723.06[0m
[36m[2023-07-10 18:44:32,760][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:44:32,761][227910] Reward + Measures: [[4318.31717003    0.34987658    0.3592416     0.26006895    0.16960898]][0m
[37m[1m[2023-07-10 18:44:32,761][227910] Max Reward on eval: 4318.317170031273[0m
[37m[1m[2023-07-10 18:44:32,761][227910] Min Reward on eval: 4318.317170031273[0m
[37m[1m[2023-07-10 18:44:32,761][227910] Mean Reward across all agents: 4318.317170031273[0m
[37m[1m[2023-07-10 18:44:32,762][227910] Average Trajectory Length: 998.001[0m
[36m[2023-07-10 18:44:38,211][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:44:38,212][227910] Reward + Measures: [[  12.83572609    0.2719        0.55330002    0.24609999    0.40990001]
 [ 587.72349781    0.2807005     0.30806887    0.3661423     0.2499119 ]
 [ 120.02268061    0.27334777    0.57140744    0.24661462    0.33027393]
 ...
 [1520.4660899     0.34969997    0.38009998    0.329         0.3497    ]
 [-923.30314435    0.45802966    0.57325184    0.51772588    0.17950001]
 [2720.77698732    0.31075558    0.3964037     0.30100125    0.13311976]][0m
[37m[1m[2023-07-10 18:44:38,212][227910] Max Reward on eval: 4070.193408113718[0m
[37m[1m[2023-07-10 18:44:38,213][227910] Min Reward on eval: -1124.5109804267297[0m
[37m[1m[2023-07-10 18:44:38,213][227910] Mean Reward across all agents: 999.5372037461175[0m
[37m[1m[2023-07-10 18:44:38,213][227910] Average Trajectory Length: 942.4283333333333[0m
[36m[2023-07-10 18:44:38,215][227910] mean_value=-1543.2871764587815, max_value=797.7095731370782[0m
[37m[1m[2023-07-10 18:44:38,218][227910] New mean coefficients: [[ 2.5721154   0.12653069 -0.22833392 -0.0598084   0.45377082]][0m
[37m[1m[2023-07-10 18:44:38,219][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:44:48,066][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 18:44:48,066][227910] FPS: 390030.14[0m
[36m[2023-07-10 18:44:48,068][227910] itr=1087, itrs=2000, Progress: 54.35%[0m
[36m[2023-07-10 18:44:59,634][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 18:44:59,634][227910] FPS: 332640.35[0m
[36m[2023-07-10 18:45:04,480][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:45:04,481][227910] Reward + Measures: [[4395.41591841    0.35854873    0.33773065    0.25067729    0.17092563]][0m
[37m[1m[2023-07-10 18:45:04,481][227910] Max Reward on eval: 4395.415918409084[0m
[37m[1m[2023-07-10 18:45:04,481][227910] Min Reward on eval: 4395.415918409084[0m
[37m[1m[2023-07-10 18:45:04,482][227910] Mean Reward across all agents: 4395.415918409084[0m
[37m[1m[2023-07-10 18:45:04,482][227910] Average Trajectory Length: 997.928[0m
[36m[2023-07-10 18:45:09,915][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:45:09,920][227910] Reward + Measures: [[2381.14276255    0.39860001    0.34980002    0.30969998    0.24559999]
 [ 509.27815152    0.43780002    0.41499996    0.30490002    0.36219999]
 [ 615.4410145     0.27940002    0.30380002    0.32410002    0.2474    ]
 ...
 [1076.30215527    0.26331148    0.28716797    0.37696308    0.23043852]
 [1094.6997288     0.19359569    0.39742681    0.30042824    0.22610958]
 [-215.02443524    0.0459        0.72480005    0.59210002    0.69590002]][0m
[37m[1m[2023-07-10 18:45:09,921][227910] Max Reward on eval: 3948.295166291087[0m
[37m[1m[2023-07-10 18:45:09,921][227910] Min Reward on eval: -1022.4584425348846[0m
[37m[1m[2023-07-10 18:45:09,921][227910] Mean Reward across all agents: 943.980782167667[0m
[37m[1m[2023-07-10 18:45:09,921][227910] Average Trajectory Length: 938.0363333333333[0m
[36m[2023-07-10 18:45:09,923][227910] mean_value=-1828.7817697956204, max_value=876.6912055450907[0m
[37m[1m[2023-07-10 18:45:09,926][227910] New mean coefficients: [[ 2.4730816   0.12385941 -0.18321009  0.14009807  0.85766613]][0m
[37m[1m[2023-07-10 18:45:09,927][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:45:19,665][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 18:45:19,665][227910] FPS: 394403.55[0m
[36m[2023-07-10 18:45:19,667][227910] itr=1088, itrs=2000, Progress: 54.40%[0m
[36m[2023-07-10 18:45:31,229][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 18:45:31,230][227910] FPS: 332637.68[0m
[36m[2023-07-10 18:45:36,037][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:45:36,037][227910] Reward + Measures: [[4475.07522154    0.35386968    0.32408205    0.24303466    0.17090084]][0m
[37m[1m[2023-07-10 18:45:36,038][227910] Max Reward on eval: 4475.075221538887[0m
[37m[1m[2023-07-10 18:45:36,038][227910] Min Reward on eval: 4475.075221538887[0m
[37m[1m[2023-07-10 18:45:36,038][227910] Mean Reward across all agents: 4475.075221538887[0m
[37m[1m[2023-07-10 18:45:36,038][227910] Average Trajectory Length: 996.877[0m
[36m[2023-07-10 18:45:41,595][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:45:41,596][227910] Reward + Measures: [[2791.22179712    0.32009998    0.48499998    0.39229998    0.18239999]
 [2054.01359666    0.24904306    0.23694526    0.17575768    0.14452045]
 [1009.77602401    0.19619302    0.42739305    0.2998651     0.25498605]
 ...
 [ -24.13507864    0.18614721    0.17374425    0.22889493    0.16572067]
 [1004.2545829     0.17309999    0.25304693    0.27827436    0.16278408]
 [ 314.15306577    0.31878024    0.31860355    0.34171888    0.1560421 ]][0m
[37m[1m[2023-07-10 18:45:41,596][227910] Max Reward on eval: 4408.230703207851[0m
[37m[1m[2023-07-10 18:45:41,597][227910] Min Reward on eval: -769.6979356940195[0m
[37m[1m[2023-07-10 18:45:41,597][227910] Mean Reward across all agents: 1366.767765264855[0m
[37m[1m[2023-07-10 18:45:41,597][227910] Average Trajectory Length: 910.8323333333333[0m
[36m[2023-07-10 18:45:41,599][227910] mean_value=-1842.1033535396227, max_value=1033.0898900223171[0m
[37m[1m[2023-07-10 18:45:41,602][227910] New mean coefficients: [[ 2.2861826   0.3088265   0.23658581 -0.07367538  0.26279426]][0m
[37m[1m[2023-07-10 18:45:41,603][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:45:51,431][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 18:45:51,432][227910] FPS: 390766.89[0m
[36m[2023-07-10 18:45:51,434][227910] itr=1089, itrs=2000, Progress: 54.45%[0m
[36m[2023-07-10 18:46:03,060][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 18:46:03,060][227910] FPS: 330918.34[0m
[36m[2023-07-10 18:46:07,783][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:46:07,789][227910] Reward + Measures: [[4133.45944798    0.32094112    0.31106904    0.24329011    0.18378544]][0m
[37m[1m[2023-07-10 18:46:07,789][227910] Max Reward on eval: 4133.459447982186[0m
[37m[1m[2023-07-10 18:46:07,790][227910] Min Reward on eval: 4133.459447982186[0m
[37m[1m[2023-07-10 18:46:07,790][227910] Mean Reward across all agents: 4133.459447982186[0m
[37m[1m[2023-07-10 18:46:07,790][227910] Average Trajectory Length: 992.2413333333333[0m
[36m[2023-07-10 18:46:13,389][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:46:13,390][227910] Reward + Measures: [[ 879.8790572     0.2590875     0.29045078    0.22388735    0.24865718]
 [ 507.21323209    0.15150002    0.43619999    0.38980001    0.2667    ]
 [3662.14278069    0.40910003    0.35250002    0.2789        0.18709999]
 ...
 [ 102.09852867    0.1111        0.3716        0.33870003    0.20689999]
 [ 977.2597588     0.29999998    0.40540001    0.36070001    0.29720002]
 [1449.97733401    0.35732627    0.27803016    0.35569176    0.16108438]][0m
[37m[1m[2023-07-10 18:46:13,390][227910] Max Reward on eval: 3987.3047237399965[0m
[37m[1m[2023-07-10 18:46:13,390][227910] Min Reward on eval: -1467.2670462306705[0m
[37m[1m[2023-07-10 18:46:13,391][227910] Mean Reward across all agents: 957.7151095154679[0m
[37m[1m[2023-07-10 18:46:13,391][227910] Average Trajectory Length: 980.9783333333334[0m
[36m[2023-07-10 18:46:13,393][227910] mean_value=-1667.8843109278764, max_value=1055.8825511047357[0m
[37m[1m[2023-07-10 18:46:13,396][227910] New mean coefficients: [[ 2.1230483   0.49138302 -0.01758321 -0.38953775  0.05120152]][0m
[37m[1m[2023-07-10 18:46:13,396][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:46:23,185][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 18:46:23,186][227910] FPS: 392343.52[0m
[36m[2023-07-10 18:46:23,188][227910] itr=1090, itrs=2000, Progress: 54.50%[0m
[37m[1m[2023-07-10 18:46:26,963][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001070[0m
[36m[2023-07-10 18:46:38,896][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 18:46:38,896][227910] FPS: 329546.23[0m
[36m[2023-07-10 18:46:43,796][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:46:43,796][227910] Reward + Measures: [[4351.04892287    0.3409622     0.29688343    0.23884016    0.1816123 ]][0m
[37m[1m[2023-07-10 18:46:43,796][227910] Max Reward on eval: 4351.048922872605[0m
[37m[1m[2023-07-10 18:46:43,797][227910] Min Reward on eval: 4351.048922872605[0m
[37m[1m[2023-07-10 18:46:43,797][227910] Mean Reward across all agents: 4351.048922872605[0m
[37m[1m[2023-07-10 18:46:43,797][227910] Average Trajectory Length: 994.021[0m
[36m[2023-07-10 18:46:49,535][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:46:49,536][227910] Reward + Measures: [[ 993.96529571    0.28706479    0.29533663    0.36881971    0.23988357]
 [ 450.62788239    0.28807935    0.31058055    0.25193059    0.12911046]
 [ 656.83947787    0.22880001    0.39820001    0.27410001    0.27419999]
 ...
 [ 210.03824988    0.4092724     0.19670345    0.4114463     0.2178084 ]
 [2886.05280286    0.39199999    0.34180003    0.34629998    0.2033    ]
 [ 904.16801942    0.21890001    0.30089998    0.2852        0.25619999]][0m
[37m[1m[2023-07-10 18:46:49,536][227910] Max Reward on eval: 3894.0694995223544[0m
[37m[1m[2023-07-10 18:46:49,536][227910] Min Reward on eval: -956.2517361917795[0m
[37m[1m[2023-07-10 18:46:49,536][227910] Mean Reward across all agents: 1125.430196575019[0m
[37m[1m[2023-07-10 18:46:49,537][227910] Average Trajectory Length: 956.6356666666667[0m
[36m[2023-07-10 18:46:49,539][227910] mean_value=-1412.1659999749554, max_value=1136.0810728403023[0m
[37m[1m[2023-07-10 18:46:49,542][227910] New mean coefficients: [[ 2.0639374   1.0207034  -0.10399862 -0.529958    0.38162583]][0m
[37m[1m[2023-07-10 18:46:49,543][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:46:59,284][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 18:46:59,284][227910] FPS: 394266.62[0m
[36m[2023-07-10 18:46:59,287][227910] itr=1091, itrs=2000, Progress: 54.55%[0m
[36m[2023-07-10 18:47:10,979][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 18:47:10,980][227910] FPS: 328925.76[0m
[36m[2023-07-10 18:47:15,773][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:47:15,773][227910] Reward + Measures: [[4494.58024456    0.3701058     0.2852897     0.23232174    0.18161401]][0m
[37m[1m[2023-07-10 18:47:15,774][227910] Max Reward on eval: 4494.580244564848[0m
[37m[1m[2023-07-10 18:47:15,774][227910] Min Reward on eval: 4494.580244564848[0m
[37m[1m[2023-07-10 18:47:15,774][227910] Mean Reward across all agents: 4494.580244564848[0m
[37m[1m[2023-07-10 18:47:15,775][227910] Average Trajectory Length: 995.7723333333333[0m
[36m[2023-07-10 18:47:21,214][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:47:21,215][227910] Reward + Measures: [[ 272.91172108    0.55430001    0.71329999    0.0991        0.70269996]
 [1681.48222799    0.26300001    0.2359        0.27480003    0.19599999]
 [ 581.28870823    0.24840002    0.34330001    0.32870001    0.32009998]
 ...
 [ 509.28378518    0.62160003    0.16340001    0.44370005    0.29850003]
 [ 680.5580954     0.27507004    0.25458544    0.31104684    0.18590625]
 [ -85.89401247    0.69390005    0.18699999    0.57120001    0.28660002]][0m
[37m[1m[2023-07-10 18:47:21,215][227910] Max Reward on eval: 3999.4059607412664[0m
[37m[1m[2023-07-10 18:47:21,216][227910] Min Reward on eval: -1193.4625829542056[0m
[37m[1m[2023-07-10 18:47:21,216][227910] Mean Reward across all agents: 840.4724283673262[0m
[37m[1m[2023-07-10 18:47:21,216][227910] Average Trajectory Length: 984.3173333333333[0m
[36m[2023-07-10 18:47:21,219][227910] mean_value=-1159.5832962429026, max_value=2750.989514701613[0m
[37m[1m[2023-07-10 18:47:21,222][227910] New mean coefficients: [[ 1.9046109   0.81981367  0.06602246 -0.6691463  -0.19439828]][0m
[37m[1m[2023-07-10 18:47:21,223][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:47:31,062][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 18:47:31,062][227910] FPS: 390344.27[0m
[36m[2023-07-10 18:47:31,064][227910] itr=1092, itrs=2000, Progress: 54.60%[0m
[36m[2023-07-10 18:47:42,532][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 18:47:42,532][227910] FPS: 335397.03[0m
[36m[2023-07-10 18:47:47,363][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:47:47,364][227910] Reward + Measures: [[4611.87731259    0.3707388     0.28397962    0.2296297     0.18395595]][0m
[37m[1m[2023-07-10 18:47:47,364][227910] Max Reward on eval: 4611.877312588841[0m
[37m[1m[2023-07-10 18:47:47,364][227910] Min Reward on eval: 4611.877312588841[0m
[37m[1m[2023-07-10 18:47:47,365][227910] Mean Reward across all agents: 4611.877312588841[0m
[37m[1m[2023-07-10 18:47:47,365][227910] Average Trajectory Length: 997.4506666666666[0m
[36m[2023-07-10 18:47:52,797][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:47:52,802][227910] Reward + Measures: [[1126.64638204    0.37090001    0.59240001    0.2185        0.2455    ]
 [1040.2393528     0.22562571    0.33477852    0.18204038    0.17041318]
 [1994.41182749    0.37652373    0.28229624    0.20743719    0.131496  ]
 ...
 [2901.58515838    0.2694        0.34290001    0.29130003    0.24440001]
 [ 450.96223908    0.25209999    0.51329994    0.23870002    0.421     ]
 [1127.97368829    0.26571807    0.33283612    0.24411216    0.20117965]][0m
[37m[1m[2023-07-10 18:47:52,803][227910] Max Reward on eval: 4390.308907016634[0m
[37m[1m[2023-07-10 18:47:52,803][227910] Min Reward on eval: -797.9721525613364[0m
[37m[1m[2023-07-10 18:47:52,803][227910] Mean Reward across all agents: 1408.0457604145972[0m
[37m[1m[2023-07-10 18:47:52,803][227910] Average Trajectory Length: 959.3146666666667[0m
[36m[2023-07-10 18:47:52,806][227910] mean_value=-1589.1572341635772, max_value=2746.978266778958[0m
[37m[1m[2023-07-10 18:47:52,808][227910] New mean coefficients: [[ 1.5905802   0.3455855  -0.3320525  -0.23643392 -0.40064597]][0m
[37m[1m[2023-07-10 18:47:52,809][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:48:02,423][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 18:48:02,423][227910] FPS: 399523.94[0m
[36m[2023-07-10 18:48:02,425][227910] itr=1093, itrs=2000, Progress: 54.65%[0m
[36m[2023-07-10 18:48:13,931][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 18:48:13,932][227910] FPS: 334258.19[0m
[36m[2023-07-10 18:48:18,627][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:48:18,627][227910] Reward + Measures: [[4704.4773814     0.38391438    0.27841675    0.2265667     0.18688513]][0m
[37m[1m[2023-07-10 18:48:18,627][227910] Max Reward on eval: 4704.477381403318[0m
[37m[1m[2023-07-10 18:48:18,628][227910] Min Reward on eval: 4704.477381403318[0m
[37m[1m[2023-07-10 18:48:18,628][227910] Mean Reward across all agents: 4704.477381403318[0m
[37m[1m[2023-07-10 18:48:18,628][227910] Average Trajectory Length: 998.016[0m
[36m[2023-07-10 18:48:23,971][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:48:23,972][227910] Reward + Measures: [[ 563.64411516    0.2210803     0.29589534    0.24312465    0.14315133]
 [ 894.34991468    0.30200002    0.33159998    0.23360001    0.3457    ]
 [ 562.06409683    0.30163333    0.30868706    0.35863674    0.2135516 ]
 ...
 [ 974.14116813    0.26813751    0.23724376    0.32450005    0.18305624]
 [  42.25353092    0.35556513    0.32922634    0.38754186    0.21472405]
 [2350.56413198    0.2811        0.31570002    0.23370002    0.2527    ]][0m
[37m[1m[2023-07-10 18:48:23,972][227910] Max Reward on eval: 4399.485912463162[0m
[37m[1m[2023-07-10 18:48:23,972][227910] Min Reward on eval: -1029.940912427625[0m
[37m[1m[2023-07-10 18:48:23,973][227910] Mean Reward across all agents: 939.1846797998875[0m
[37m[1m[2023-07-10 18:48:23,973][227910] Average Trajectory Length: 960.1956666666666[0m
[36m[2023-07-10 18:48:23,976][227910] mean_value=-1411.1601420123834, max_value=1236.5747050169243[0m
[37m[1m[2023-07-10 18:48:23,978][227910] New mean coefficients: [[ 1.8059876   0.44580746 -0.13925703 -0.05035411  0.06116349]][0m
[37m[1m[2023-07-10 18:48:23,979][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:48:33,722][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 18:48:33,722][227910] FPS: 394194.65[0m
[36m[2023-07-10 18:48:33,724][227910] itr=1094, itrs=2000, Progress: 54.70%[0m
[36m[2023-07-10 18:48:45,254][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 18:48:45,254][227910] FPS: 333581.82[0m
[36m[2023-07-10 18:48:50,147][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:48:50,147][227910] Reward + Measures: [[4785.95215       0.39549848    0.26633024    0.22033833    0.18414469]][0m
[37m[1m[2023-07-10 18:48:50,148][227910] Max Reward on eval: 4785.952149995615[0m
[37m[1m[2023-07-10 18:48:50,148][227910] Min Reward on eval: 4785.952149995615[0m
[37m[1m[2023-07-10 18:48:50,148][227910] Mean Reward across all agents: 4785.952149995615[0m
[37m[1m[2023-07-10 18:48:50,148][227910] Average Trajectory Length: 999.0753333333333[0m
[36m[2023-07-10 18:48:55,718][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:48:55,724][227910] Reward + Measures: [[1802.15784236    0.2989094     0.2528412     0.26228502    0.11878456]
 [1108.47844547    0.42080003    0.24129999    0.32340002    0.2096    ]
 [1429.48516282    0.25278404    0.23874989    0.18985371    0.17058733]
 ...
 [-191.35328605    0.5334174     0.39490435    0.41202608    0.46142173]
 [3082.18849513    0.30986881    0.289038      0.27879623    0.1208365 ]
 [ 443.20900319    0.3752        0.25619999    0.4014        0.25429997]][0m
[37m[1m[2023-07-10 18:48:55,724][227910] Max Reward on eval: 4318.367083972878[0m
[37m[1m[2023-07-10 18:48:55,724][227910] Min Reward on eval: -1020.5042047441996[0m
[37m[1m[2023-07-10 18:48:55,724][227910] Mean Reward across all agents: 1189.19187045801[0m
[37m[1m[2023-07-10 18:48:55,725][227910] Average Trajectory Length: 960.1669999999999[0m
[36m[2023-07-10 18:48:55,728][227910] mean_value=-1443.7013114676881, max_value=1195.6664815528309[0m
[37m[1m[2023-07-10 18:48:55,731][227910] New mean coefficients: [[ 1.9380363  -0.03105369 -0.5188673  -0.01142517  0.24653998]][0m
[37m[1m[2023-07-10 18:48:55,732][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:49:05,582][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 18:49:05,582][227910] FPS: 389914.61[0m
[36m[2023-07-10 18:49:05,584][227910] itr=1095, itrs=2000, Progress: 54.75%[0m
[36m[2023-07-10 18:49:17,212][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 18:49:17,212][227910] FPS: 330767.62[0m
[36m[2023-07-10 18:49:22,026][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:49:22,032][227910] Reward + Measures: [[4811.92544853    0.40211833    0.25063542    0.2124631     0.17984582]][0m
[37m[1m[2023-07-10 18:49:22,032][227910] Max Reward on eval: 4811.925448527426[0m
[37m[1m[2023-07-10 18:49:22,033][227910] Min Reward on eval: 4811.925448527426[0m
[37m[1m[2023-07-10 18:49:22,033][227910] Mean Reward across all agents: 4811.925448527426[0m
[37m[1m[2023-07-10 18:49:22,033][227910] Average Trajectory Length: 997.1123333333333[0m
[36m[2023-07-10 18:49:27,746][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:49:27,752][227910] Reward + Measures: [[2152.12373248    0.2606        0.52940005    0.31930003    0.22149999]
 [-102.82493219    0.21098652    0.17900115    0.20474277    0.14053975]
 [ 368.59239796    0.33205652    0.32536668    0.26934493    0.18408117]
 ...
 [1861.51059332    0.2382936     0.26280627    0.18310903    0.14551799]
 [1077.84694968    0.38931558    0.21426983    0.34436288    0.22330999]
 [2052.70256881    0.35454878    0.24334228    0.23039675    0.1682992 ]][0m
[37m[1m[2023-07-10 18:49:27,753][227910] Max Reward on eval: 4737.24234866947[0m
[37m[1m[2023-07-10 18:49:27,753][227910] Min Reward on eval: -811.6450020034914[0m
[37m[1m[2023-07-10 18:49:27,753][227910] Mean Reward across all agents: 1812.541334378277[0m
[37m[1m[2023-07-10 18:49:27,753][227910] Average Trajectory Length: 963.637[0m
[36m[2023-07-10 18:49:27,756][227910] mean_value=-1233.0548855628774, max_value=3072.7241876049898[0m
[37m[1m[2023-07-10 18:49:27,759][227910] New mean coefficients: [[ 1.4260757  -0.39567882 -0.5614764   0.32682854  0.20470503]][0m
[37m[1m[2023-07-10 18:49:27,760][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:49:37,438][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 18:49:37,438][227910] FPS: 396836.54[0m
[36m[2023-07-10 18:49:37,440][227910] itr=1096, itrs=2000, Progress: 54.80%[0m
[36m[2023-07-10 18:49:49,047][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 18:49:49,047][227910] FPS: 331365.20[0m
[36m[2023-07-10 18:49:53,787][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:49:53,787][227910] Reward + Measures: [[4861.07186006    0.39981106    0.23523341    0.20526347    0.17562649]][0m
[37m[1m[2023-07-10 18:49:53,787][227910] Max Reward on eval: 4861.07186005922[0m
[37m[1m[2023-07-10 18:49:53,788][227910] Min Reward on eval: 4861.07186005922[0m
[37m[1m[2023-07-10 18:49:53,788][227910] Mean Reward across all agents: 4861.07186005922[0m
[37m[1m[2023-07-10 18:49:53,788][227910] Average Trajectory Length: 994.9606666666666[0m
[36m[2023-07-10 18:49:59,333][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:49:59,334][227910] Reward + Measures: [[  86.19925094    0.4025        0.51499999    0.43060002    0.49580002]
 [ 206.68380104    0.61980003    0.17370002    0.58179998    0.29439998]
 [ 996.68128555    0.31049693    0.32882065    0.33569691    0.27111855]
 ...
 [2189.00384769    0.23369999    0.23720002    0.23360001    0.18700001]
 [1997.30646391    0.3206        0.35070002    0.2703        0.21569999]
 [2702.74676276    0.3705        0.30780002    0.3001        0.2651    ]][0m
[37m[1m[2023-07-10 18:49:59,334][227910] Max Reward on eval: 4587.725440009986[0m
[37m[1m[2023-07-10 18:49:59,334][227910] Min Reward on eval: -590.5943638579338[0m
[37m[1m[2023-07-10 18:49:59,335][227910] Mean Reward across all agents: 1458.663387638744[0m
[37m[1m[2023-07-10 18:49:59,335][227910] Average Trajectory Length: 973.333[0m
[36m[2023-07-10 18:49:59,338][227910] mean_value=-1289.0889445852329, max_value=3114.2138360744343[0m
[37m[1m[2023-07-10 18:49:59,341][227910] New mean coefficients: [[ 1.1821773  -0.5975218  -0.20389009  0.09220095  0.03093463]][0m
[37m[1m[2023-07-10 18:49:59,342][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:50:09,296][227910] train() took 9.95 seconds to complete[0m
[36m[2023-07-10 18:50:09,297][227910] FPS: 385810.29[0m
[36m[2023-07-10 18:50:09,299][227910] itr=1097, itrs=2000, Progress: 54.85%[0m
[36m[2023-07-10 18:50:20,971][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 18:50:20,971][227910] FPS: 329510.17[0m
[36m[2023-07-10 18:50:25,768][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:50:25,768][227910] Reward + Measures: [[4952.40000384    0.40322512    0.23505944    0.20378503    0.17583963]][0m
[37m[1m[2023-07-10 18:50:25,769][227910] Max Reward on eval: 4952.400003838242[0m
[37m[1m[2023-07-10 18:50:25,769][227910] Min Reward on eval: 4952.400003838242[0m
[37m[1m[2023-07-10 18:50:25,769][227910] Mean Reward across all agents: 4952.400003838242[0m
[37m[1m[2023-07-10 18:50:25,769][227910] Average Trajectory Length: 994.9133333333333[0m
[36m[2023-07-10 18:50:31,266][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:50:31,267][227910] Reward + Measures: [[1128.42697853    0.45469999    0.30130002    0.29200003    0.21660002]
 [ 938.80272269    0.18185829    0.19592366    0.36745709    0.1991487 ]
 [1146.74273599    0.2359        0.48519999    0.3969        0.24520002]
 ...
 [ -65.89953436    0.16144778    0.12389895    0.19423604    0.11186554]
 [1148.42485011    0.20990001    0.37139997    0.31399998    0.31020001]
 [1105.40763832    0.25490218    0.34450412    0.30899689    0.22136234]][0m
[37m[1m[2023-07-10 18:50:31,267][227910] Max Reward on eval: 4608.3903232637795[0m
[37m[1m[2023-07-10 18:50:31,268][227910] Min Reward on eval: -731.4425527841784[0m
[37m[1m[2023-07-10 18:50:31,268][227910] Mean Reward across all agents: 1645.9286744702301[0m
[37m[1m[2023-07-10 18:50:31,268][227910] Average Trajectory Length: 956.7943333333333[0m
[36m[2023-07-10 18:50:31,270][227910] mean_value=-1426.065081567622, max_value=1540.638407924383[0m
[37m[1m[2023-07-10 18:50:31,273][227910] New mean coefficients: [[ 1.1342859  -0.4906612   0.23630244 -0.10525818 -0.01445121]][0m
[37m[1m[2023-07-10 18:50:31,274][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:50:41,060][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 18:50:41,060][227910] FPS: 392474.52[0m
[36m[2023-07-10 18:50:41,062][227910] itr=1098, itrs=2000, Progress: 54.90%[0m
[36m[2023-07-10 18:50:52,658][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 18:50:52,658][227910] FPS: 331708.28[0m
[36m[2023-07-10 18:50:57,393][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:50:57,394][227910] Reward + Measures: [[5015.85030478    0.39045775    0.24090375    0.20189106    0.17593217]][0m
[37m[1m[2023-07-10 18:50:57,394][227910] Max Reward on eval: 5015.85030477799[0m
[37m[1m[2023-07-10 18:50:57,394][227910] Min Reward on eval: 5015.85030477799[0m
[37m[1m[2023-07-10 18:50:57,394][227910] Mean Reward across all agents: 5015.85030477799[0m
[37m[1m[2023-07-10 18:50:57,395][227910] Average Trajectory Length: 994.458[0m
[36m[2023-07-10 18:51:02,889][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:51:02,894][227910] Reward + Measures: [[ 268.80886106    0.19242835    0.18811344    0.15458061    0.14571045]
 [ 265.24189015    0.1328        0.14070001    0.11210001    0.12229999]
 [4009.09100843    0.34996852    0.29176411    0.20114329    0.16963127]
 ...
 [ 381.91179015    0.2465        0.2369        0.18230002    0.1928    ]
 [1138.80567656    0.26761565    0.18629999    0.20670068    0.1420068 ]
 [1297.628039      0.38839999    0.55360001    0.1578        0.24730001]][0m
[37m[1m[2023-07-10 18:51:02,895][227910] Max Reward on eval: 4730.872202474927[0m
[37m[1m[2023-07-10 18:51:02,895][227910] Min Reward on eval: -896.0285653672588[0m
[37m[1m[2023-07-10 18:51:02,895][227910] Mean Reward across all agents: 1326.884164341443[0m
[37m[1m[2023-07-10 18:51:02,896][227910] Average Trajectory Length: 965.5936666666666[0m
[36m[2023-07-10 18:51:02,899][227910] mean_value=-1066.5699383706803, max_value=2528.5317199222563[0m
[37m[1m[2023-07-10 18:51:02,902][227910] New mean coefficients: [[ 1.0479398  -0.27492505  0.30035415 -0.0589828   0.34335014]][0m
[37m[1m[2023-07-10 18:51:02,903][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:51:12,570][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 18:51:12,571][227910] FPS: 397273.17[0m
[36m[2023-07-10 18:51:12,573][227910] itr=1099, itrs=2000, Progress: 54.95%[0m
[36m[2023-07-10 18:51:24,132][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 18:51:24,132][227910] FPS: 332782.23[0m
[36m[2023-07-10 18:51:28,834][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:51:28,834][227910] Reward + Measures: [[5119.12074365    0.38781351    0.24357605    0.20158467    0.17884372]][0m
[37m[1m[2023-07-10 18:51:28,834][227910] Max Reward on eval: 5119.120743650892[0m
[37m[1m[2023-07-10 18:51:28,835][227910] Min Reward on eval: 5119.120743650892[0m
[37m[1m[2023-07-10 18:51:28,835][227910] Mean Reward across all agents: 5119.120743650892[0m
[37m[1m[2023-07-10 18:51:28,835][227910] Average Trajectory Length: 996.5029999999999[0m
[36m[2023-07-10 18:51:34,276][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:51:34,277][227910] Reward + Measures: [[4504.19053364    0.32160002    0.36290002    0.2155        0.15030001]
 [1460.07006412    0.28623334    0.51390004    0.17091213    0.2072515 ]
 [2733.84752344    0.38079998    0.32600003    0.33729997    0.23279999]
 ...
 [1665.20277154    0.3522        0.3734        0.3098        0.1943    ]
 [2120.4049177     0.3206        0.39180002    0.31529999    0.229     ]
 [1323.02904321    0.20289998    0.2296        0.27079999    0.16370001]][0m
[37m[1m[2023-07-10 18:51:34,277][227910] Max Reward on eval: 5077.122004754841[0m
[37m[1m[2023-07-10 18:51:34,277][227910] Min Reward on eval: -1111.7907429805025[0m
[37m[1m[2023-07-10 18:51:34,278][227910] Mean Reward across all agents: 1693.869787013267[0m
[37m[1m[2023-07-10 18:51:34,278][227910] Average Trajectory Length: 980.034[0m
[36m[2023-07-10 18:51:34,281][227910] mean_value=-1297.9906327360977, max_value=1635.6434364589281[0m
[37m[1m[2023-07-10 18:51:34,284][227910] New mean coefficients: [[0.9014275  0.3419289  0.3056332  0.13572754 0.1501286 ]][0m
[37m[1m[2023-07-10 18:51:34,285][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:51:43,978][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 18:51:43,978][227910] FPS: 396243.55[0m
[36m[2023-07-10 18:51:43,980][227910] itr=1100, itrs=2000, Progress: 55.00%[0m
[37m[1m[2023-07-10 18:51:47,894][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001080[0m
[36m[2023-07-10 18:51:59,786][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 18:51:59,786][227910] FPS: 330429.13[0m
[36m[2023-07-10 18:52:04,523][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:52:04,523][227910] Reward + Measures: [[5178.39746598    0.40051425    0.23738198    0.19881411    0.17782417]][0m
[37m[1m[2023-07-10 18:52:04,524][227910] Max Reward on eval: 5178.397465976114[0m
[37m[1m[2023-07-10 18:52:04,524][227910] Min Reward on eval: 5178.397465976114[0m
[37m[1m[2023-07-10 18:52:04,524][227910] Mean Reward across all agents: 5178.397465976114[0m
[37m[1m[2023-07-10 18:52:04,524][227910] Average Trajectory Length: 994.4563333333333[0m
[36m[2023-07-10 18:52:10,032][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:52:10,032][227910] Reward + Measures: [[2469.02744724    0.3285293     0.42987376    0.23686969    0.1737899 ]
 [1630.45153359    0.22292399    0.27381179    0.30302435    0.18072405]
 [2807.79382833    0.25849998    0.20750001    0.24819998    0.15450001]
 ...
 [2170.28620827    0.2402        0.33460003    0.29159999    0.176     ]
 [1976.68123949    0.26050001    0.31159997    0.27340004    0.1751    ]
 [1579.95205832    0.24089999    0.16059999    0.27160001    0.18719999]][0m
[37m[1m[2023-07-10 18:52:10,033][227910] Max Reward on eval: 5022.066174188629[0m
[37m[1m[2023-07-10 18:52:10,033][227910] Min Reward on eval: -887.7151089700404[0m
[37m[1m[2023-07-10 18:52:10,033][227910] Mean Reward across all agents: 1709.1708124879674[0m
[37m[1m[2023-07-10 18:52:10,033][227910] Average Trajectory Length: 970.6913333333333[0m
[36m[2023-07-10 18:52:10,036][227910] mean_value=-1317.3329481659887, max_value=1843.3535322705227[0m
[37m[1m[2023-07-10 18:52:10,039][227910] New mean coefficients: [[ 0.82450294  0.5549258  -0.19598025 -0.02145745 -0.20304734]][0m
[37m[1m[2023-07-10 18:52:10,040][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:52:19,841][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 18:52:19,841][227910] FPS: 391861.79[0m
[36m[2023-07-10 18:52:19,844][227910] itr=1101, itrs=2000, Progress: 55.05%[0m
[36m[2023-07-10 18:52:31,344][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 18:52:31,344][227910] FPS: 334498.20[0m
[36m[2023-07-10 18:52:36,127][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:52:36,128][227910] Reward + Measures: [[5218.52030475    0.40793613    0.23518479    0.19869222    0.17601582]][0m
[37m[1m[2023-07-10 18:52:36,128][227910] Max Reward on eval: 5218.520304751961[0m
[37m[1m[2023-07-10 18:52:36,128][227910] Min Reward on eval: 5218.520304751961[0m
[37m[1m[2023-07-10 18:52:36,128][227910] Mean Reward across all agents: 5218.520304751961[0m
[37m[1m[2023-07-10 18:52:36,128][227910] Average Trajectory Length: 994.9123333333333[0m
[36m[2023-07-10 18:52:41,681][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:52:41,682][227910] Reward + Measures: [[4336.56153722    0.38500002    0.29319999    0.1989        0.1787    ]
 [ 769.75751527    0.27910003    0.41479999    0.3558        0.29679999]
 [ 355.1811332     0.29617339    0.19232821    0.14554484    0.14587365]
 ...
 [1244.13645487    0.26851565    0.24307655    0.18031868    0.1628329 ]
 [3695.59729371    0.32769999    0.33660001    0.21570002    0.189     ]
 [1604.78447936    0.34310001    0.50879997    0.33430001    0.31830001]][0m
[37m[1m[2023-07-10 18:52:41,682][227910] Max Reward on eval: 5004.628633468598[0m
[37m[1m[2023-07-10 18:52:41,682][227910] Min Reward on eval: -549.2676560307445[0m
[37m[1m[2023-07-10 18:52:41,682][227910] Mean Reward across all agents: 1846.6790872530694[0m
[37m[1m[2023-07-10 18:52:41,683][227910] Average Trajectory Length: 900.4243333333333[0m
[36m[2023-07-10 18:52:41,685][227910] mean_value=-1228.0881851092313, max_value=1362.271673176158[0m
[37m[1m[2023-07-10 18:52:41,688][227910] New mean coefficients: [[1.0093403  0.5580258  0.03523898 0.01729927 0.21517006]][0m
[37m[1m[2023-07-10 18:52:41,689][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:52:51,300][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 18:52:51,301][227910] FPS: 399577.61[0m
[36m[2023-07-10 18:52:51,303][227910] itr=1102, itrs=2000, Progress: 55.10%[0m
[36m[2023-07-10 18:53:02,793][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 18:53:02,793][227910] FPS: 334720.86[0m
[36m[2023-07-10 18:53:07,585][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:53:07,586][227910] Reward + Measures: [[5298.62333406    0.42420557    0.23172905    0.19578452    0.17540202]][0m
[37m[1m[2023-07-10 18:53:07,586][227910] Max Reward on eval: 5298.623334062688[0m
[37m[1m[2023-07-10 18:53:07,586][227910] Min Reward on eval: 5298.623334062688[0m
[37m[1m[2023-07-10 18:53:07,586][227910] Mean Reward across all agents: 5298.623334062688[0m
[37m[1m[2023-07-10 18:53:07,586][227910] Average Trajectory Length: 998.2266666666667[0m
[36m[2023-07-10 18:53:13,001][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:53:13,002][227910] Reward + Measures: [[ 910.12781029    0.22184706    0.17408587    0.28027591    0.17845123]
 [3545.46493018    0.41444954    0.32883459    0.25834695    0.16897871]
 [2855.4574519     0.3053        0.222         0.2313        0.15380001]
 ...
 [1010.87944734    0.21325476    0.18141921    0.221763      0.13525283]
 [1264.70737522    0.35670003    0.34590003    0.17819999    0.20390001]
 [-117.67294961    0.26789999    0.35589999    0.26810002    0.24870001]][0m
[37m[1m[2023-07-10 18:53:13,002][227910] Max Reward on eval: 4774.652607547492[0m
[37m[1m[2023-07-10 18:53:13,003][227910] Min Reward on eval: -1049.601702156861[0m
[37m[1m[2023-07-10 18:53:13,003][227910] Mean Reward across all agents: 1583.8053682532898[0m
[37m[1m[2023-07-10 18:53:13,003][227910] Average Trajectory Length: 965.8996666666667[0m
[36m[2023-07-10 18:53:13,006][227910] mean_value=-1072.4101766531198, max_value=1458.1709025368318[0m
[37m[1m[2023-07-10 18:53:13,009][227910] New mean coefficients: [[1.3133845  0.68713015 0.446431   0.03379482 0.35751325]][0m
[37m[1m[2023-07-10 18:53:13,010][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:53:22,627][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 18:53:22,627][227910] FPS: 399359.93[0m
[36m[2023-07-10 18:53:22,629][227910] itr=1103, itrs=2000, Progress: 55.15%[0m
[36m[2023-07-10 18:53:34,070][227910] train() took 11.42 seconds to complete[0m
[36m[2023-07-10 18:53:34,070][227910] FPS: 336187.94[0m
[36m[2023-07-10 18:53:38,844][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:53:38,844][227910] Reward + Measures: [[5309.47558675    0.43194246    0.22772656    0.19690682    0.17522156]][0m
[37m[1m[2023-07-10 18:53:38,844][227910] Max Reward on eval: 5309.475586753445[0m
[37m[1m[2023-07-10 18:53:38,844][227910] Min Reward on eval: 5309.475586753445[0m
[37m[1m[2023-07-10 18:53:38,845][227910] Mean Reward across all agents: 5309.475586753445[0m
[37m[1m[2023-07-10 18:53:38,845][227910] Average Trajectory Length: 996.6656666666667[0m
[36m[2023-07-10 18:53:44,283][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:53:44,284][227910] Reward + Measures: [[1488.7760452     0.36340001    0.38480002    0.48700005    0.18050002]
 [4653.21489166    0.41389999    0.2552        0.21160002    0.16760001]
 [3751.36094868    0.35999998    0.33080003    0.30630001    0.17089999]
 ...
 [2166.44760703    0.28330001    0.30070001    0.38260004    0.17550001]
 [ 799.50138786    0.22059999    0.17560001    0.35090002    0.20060001]
 [4129.0491397     0.3881        0.30930001    0.2633        0.176     ]][0m
[37m[1m[2023-07-10 18:53:44,284][227910] Max Reward on eval: 4958.913965543313[0m
[37m[1m[2023-07-10 18:53:44,284][227910] Min Reward on eval: -700.0463804084008[0m
[37m[1m[2023-07-10 18:53:44,285][227910] Mean Reward across all agents: 1547.2182102853321[0m
[37m[1m[2023-07-10 18:53:44,285][227910] Average Trajectory Length: 983.48[0m
[36m[2023-07-10 18:53:44,289][227910] mean_value=-782.4979094033093, max_value=2474.0295357126947[0m
[37m[1m[2023-07-10 18:53:44,291][227910] New mean coefficients: [[1.3101673  0.78864694 0.24963094 0.21456617 0.61673415]][0m
[37m[1m[2023-07-10 18:53:44,292][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:53:54,047][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 18:53:54,048][227910] FPS: 393709.22[0m
[36m[2023-07-10 18:53:54,050][227910] itr=1104, itrs=2000, Progress: 55.20%[0m
[36m[2023-07-10 18:54:05,543][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 18:54:05,543][227910] FPS: 334694.92[0m
[36m[2023-07-10 18:54:10,241][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:54:10,242][227910] Reward + Measures: [[5342.25450033    0.44478378    0.22293627    0.19402543    0.17565522]][0m
[37m[1m[2023-07-10 18:54:10,242][227910] Max Reward on eval: 5342.254500327807[0m
[37m[1m[2023-07-10 18:54:10,242][227910] Min Reward on eval: 5342.254500327807[0m
[37m[1m[2023-07-10 18:54:10,242][227910] Mean Reward across all agents: 5342.254500327807[0m
[37m[1m[2023-07-10 18:54:10,243][227910] Average Trajectory Length: 996.5503333333334[0m
[36m[2023-07-10 18:54:15,680][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:54:15,686][227910] Reward + Measures: [[2438.17938438    0.50690001    0.4883        0.26930004    0.1858    ]
 [3153.74840076    0.33220002    0.28200004    0.24960001    0.17889999]
 [2603.57826255    0.29550001    0.38420001    0.32089999    0.24130002]
 ...
 [2387.68961879    0.36525175    0.48981959    0.33312413    0.22982894]
 [1524.60026369    0.17629999    0.21470001    0.2454        0.17569999]
 [2633.7987106     0.37979999    0.40170002    0.30670002    0.19170001]][0m
[37m[1m[2023-07-10 18:54:15,686][227910] Max Reward on eval: 5156.411534852814[0m
[37m[1m[2023-07-10 18:54:15,686][227910] Min Reward on eval: -987.6821795085212[0m
[37m[1m[2023-07-10 18:54:15,687][227910] Mean Reward across all agents: 1486.3978354508122[0m
[37m[1m[2023-07-10 18:54:15,687][227910] Average Trajectory Length: 975.2636666666666[0m
[36m[2023-07-10 18:54:15,690][227910] mean_value=-1074.3238676245187, max_value=3574.7487819084154[0m
[37m[1m[2023-07-10 18:54:15,692][227910] New mean coefficients: [[ 1.1316079   0.80182636  0.13385415 -0.00795491  0.13913596]][0m
[37m[1m[2023-07-10 18:54:15,693][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:54:25,477][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 18:54:25,477][227910] FPS: 392570.29[0m
[36m[2023-07-10 18:54:25,479][227910] itr=1105, itrs=2000, Progress: 55.25%[0m
[36m[2023-07-10 18:54:37,052][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 18:54:37,053][227910] FPS: 332420.56[0m
[36m[2023-07-10 18:54:41,831][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:54:41,831][227910] Reward + Measures: [[5397.57126729    0.43240556    0.21733604    0.19201577    0.17569961]][0m
[37m[1m[2023-07-10 18:54:41,832][227910] Max Reward on eval: 5397.571267288662[0m
[37m[1m[2023-07-10 18:54:41,832][227910] Min Reward on eval: 5397.571267288662[0m
[37m[1m[2023-07-10 18:54:41,832][227910] Mean Reward across all agents: 5397.571267288662[0m
[37m[1m[2023-07-10 18:54:41,832][227910] Average Trajectory Length: 997.0073333333333[0m
[36m[2023-07-10 18:54:47,377][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:54:47,378][227910] Reward + Measures: [[1971.64170358    0.23873487    0.27290529    0.26290527    0.20234346]
 [2939.86168484    0.28570002    0.30960003    0.2516        0.1655    ]
 [1708.59641833    0.39610001    0.44040003    0.20870002    0.3127    ]
 ...
 [3324.86595597    0.32710001    0.289         0.204         0.19939999]
 [3675.58864692    0.32061911    0.33961609    0.2202636     0.18572788]
 [2799.54438023    0.2773        0.25889999    0.24490002    0.1833    ]][0m
[37m[1m[2023-07-10 18:54:47,378][227910] Max Reward on eval: 4930.6779254172[0m
[37m[1m[2023-07-10 18:54:47,379][227910] Min Reward on eval: -462.747861633671[0m
[37m[1m[2023-07-10 18:54:47,379][227910] Mean Reward across all agents: 1996.7330866693658[0m
[37m[1m[2023-07-10 18:54:47,379][227910] Average Trajectory Length: 979.687[0m
[36m[2023-07-10 18:54:47,382][227910] mean_value=-1504.9697491300499, max_value=1589.6175506431289[0m
[37m[1m[2023-07-10 18:54:47,384][227910] New mean coefficients: [[ 0.6627091   0.5547043   0.19710064  0.0081147  -0.8321357 ]][0m
[37m[1m[2023-07-10 18:54:47,385][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:54:57,118][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 18:54:57,118][227910] FPS: 394619.37[0m
[36m[2023-07-10 18:54:57,120][227910] itr=1106, itrs=2000, Progress: 55.30%[0m
[36m[2023-07-10 18:55:08,751][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 18:55:08,752][227910] FPS: 330664.80[0m
[36m[2023-07-10 18:55:13,499][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:55:13,500][227910] Reward + Measures: [[5419.60561958    0.44345385    0.21890096    0.19346529    0.17573321]][0m
[37m[1m[2023-07-10 18:55:13,500][227910] Max Reward on eval: 5419.605619580443[0m
[37m[1m[2023-07-10 18:55:13,500][227910] Min Reward on eval: 5419.605619580443[0m
[37m[1m[2023-07-10 18:55:13,500][227910] Mean Reward across all agents: 5419.605619580443[0m
[37m[1m[2023-07-10 18:55:13,501][227910] Average Trajectory Length: 997.717[0m
[36m[2023-07-10 18:55:19,069][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:55:19,069][227910] Reward + Measures: [[2592.88478653    0.34085608    0.336027      0.18744127    0.17708306]
 [3530.2171695     0.36804089    0.3421168     0.21590292    0.17128833]
 [3716.64749907    0.36559999    0.33720002    0.2062        0.1794    ]
 ...
 [2251.85638011    0.32643151    0.3419219     0.18910639    0.19532147]
 [3118.18235189    0.29609999    0.36239997    0.20700002    0.1684    ]
 [ 898.55120768    0.27069998    0.2474        0.1925        0.24250002]][0m
[37m[1m[2023-07-10 18:55:19,069][227910] Max Reward on eval: 5289.592030848842[0m
[37m[1m[2023-07-10 18:55:19,070][227910] Min Reward on eval: -604.5205827930592[0m
[37m[1m[2023-07-10 18:55:19,070][227910] Mean Reward across all agents: 2099.529057402083[0m
[37m[1m[2023-07-10 18:55:19,070][227910] Average Trajectory Length: 965.6326666666666[0m
[36m[2023-07-10 18:55:19,072][227910] mean_value=-1536.825321634986, max_value=2576.354512930056[0m
[37m[1m[2023-07-10 18:55:19,075][227910] New mean coefficients: [[ 0.90264803  0.49365547  0.18265603  0.05039339 -0.329629  ]][0m
[37m[1m[2023-07-10 18:55:19,076][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:55:28,795][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 18:55:28,795][227910] FPS: 395168.95[0m
[36m[2023-07-10 18:55:28,797][227910] itr=1107, itrs=2000, Progress: 55.35%[0m
[36m[2023-07-10 18:55:40,496][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 18:55:40,497][227910] FPS: 328807.74[0m
[36m[2023-07-10 18:55:45,329][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:55:45,330][227910] Reward + Measures: [[5397.37671979    0.45876527    0.2188348     0.19425584    0.17547669]][0m
[37m[1m[2023-07-10 18:55:45,330][227910] Max Reward on eval: 5397.376719793217[0m
[37m[1m[2023-07-10 18:55:45,330][227910] Min Reward on eval: 5397.376719793217[0m
[37m[1m[2023-07-10 18:55:45,330][227910] Mean Reward across all agents: 5397.376719793217[0m
[37m[1m[2023-07-10 18:55:45,330][227910] Average Trajectory Length: 996.9096666666667[0m
[36m[2023-07-10 18:55:50,821][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:55:50,822][227910] Reward + Measures: [[ 212.11815496    0.5632        0.169         0.61379999    0.4233    ]
 [3771.15891156    0.42103773    0.32789931    0.1914553     0.17505644]
 [3459.650884      0.36704212    0.26717719    0.24672456    0.20085965]
 ...
 [2771.09104548    0.40790001    0.35709998    0.23369999    0.20750001]
 [3725.84164328    0.30907232    0.32506236    0.23387769    0.19330859]
 [ 270.56435668    0.21510001    0.61120003    0.2888        0.6038    ]][0m
[37m[1m[2023-07-10 18:55:50,822][227910] Max Reward on eval: 5245.90606575628[0m
[37m[1m[2023-07-10 18:55:50,822][227910] Min Reward on eval: -536.6909462801356[0m
[37m[1m[2023-07-10 18:55:50,823][227910] Mean Reward across all agents: 1768.5047115872362[0m
[37m[1m[2023-07-10 18:55:50,823][227910] Average Trajectory Length: 960.7916666666666[0m
[36m[2023-07-10 18:55:50,825][227910] mean_value=-1353.1139211955074, max_value=1363.5626238343757[0m
[37m[1m[2023-07-10 18:55:50,828][227910] New mean coefficients: [[ 0.7832057   0.36016864  0.25910252  0.13192648 -0.5718794 ]][0m
[37m[1m[2023-07-10 18:55:50,829][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:56:00,851][227910] train() took 10.02 seconds to complete[0m
[36m[2023-07-10 18:56:00,852][227910] FPS: 383196.91[0m
[36m[2023-07-10 18:56:00,854][227910] itr=1108, itrs=2000, Progress: 55.40%[0m
[36m[2023-07-10 18:56:12,685][227910] train() took 11.81 seconds to complete[0m
[36m[2023-07-10 18:56:12,686][227910] FPS: 325056.80[0m
[36m[2023-07-10 18:56:17,578][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:56:17,579][227910] Reward + Measures: [[5443.45065479    0.45609894    0.22342999    0.18944657    0.17280099]][0m
[37m[1m[2023-07-10 18:56:17,579][227910] Max Reward on eval: 5443.450654789418[0m
[37m[1m[2023-07-10 18:56:17,579][227910] Min Reward on eval: 5443.450654789418[0m
[37m[1m[2023-07-10 18:56:17,580][227910] Mean Reward across all agents: 5443.450654789418[0m
[37m[1m[2023-07-10 18:56:17,580][227910] Average Trajectory Length: 996.1999999999999[0m
[36m[2023-07-10 18:56:23,065][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:56:23,066][227910] Reward + Measures: [[2682.86527635    0.28950438    0.32753035    0.24373873    0.17223041]
 [ 785.68659254    0.56269997    0.41160002    0.22679999    0.37400001]
 [ 328.63234738    0.11570001    0.07750001    0.07309999    0.0988    ]
 ...
 [1683.47831735    0.38404804    0.22911575    0.41673541    0.25487167]
 [1319.34907981    0.48860002    0.59530002    0.32390001    0.49170002]
 [ 561.81795475    0.6038        0.16590001    0.51750004    0.47930002]][0m
[37m[1m[2023-07-10 18:56:23,066][227910] Max Reward on eval: 5140.029252695758[0m
[37m[1m[2023-07-10 18:56:23,066][227910] Min Reward on eval: -1011.6190629147983[0m
[37m[1m[2023-07-10 18:56:23,067][227910] Mean Reward across all agents: 1141.2369783287015[0m
[37m[1m[2023-07-10 18:56:23,067][227910] Average Trajectory Length: 950.475[0m
[36m[2023-07-10 18:56:23,071][227910] mean_value=-757.9491472981855, max_value=2568.015536298624[0m
[37m[1m[2023-07-10 18:56:23,074][227910] New mean coefficients: [[ 0.56417906  0.4738919   0.46952784  0.11279336 -0.5526231 ]][0m
[37m[1m[2023-07-10 18:56:23,075][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:56:32,814][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 18:56:32,814][227910] FPS: 394334.80[0m
[36m[2023-07-10 18:56:32,817][227910] itr=1109, itrs=2000, Progress: 55.45%[0m
[36m[2023-07-10 18:56:44,375][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 18:56:44,375][227910] FPS: 332863.62[0m
[36m[2023-07-10 18:56:49,087][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:56:49,087][227910] Reward + Measures: [[5507.79657574    0.46255493    0.22551432    0.18574703    0.17219327]][0m
[37m[1m[2023-07-10 18:56:49,087][227910] Max Reward on eval: 5507.796575739993[0m
[37m[1m[2023-07-10 18:56:49,088][227910] Min Reward on eval: 5507.796575739993[0m
[37m[1m[2023-07-10 18:56:49,088][227910] Mean Reward across all agents: 5507.796575739993[0m
[37m[1m[2023-07-10 18:56:49,088][227910] Average Trajectory Length: 997.4889999999999[0m
[36m[2023-07-10 18:56:54,654][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:56:54,655][227910] Reward + Measures: [[ 324.00946124    0.28130001    0.74310005    0.14050001    0.58639997]
 [1519.52331477    0.28720003    0.42019996    0.27110001    0.31640002]
 [2971.92740554    0.32769999    0.42770001    0.20710002    0.17740001]
 ...
 [ 268.47534634    0.48639998    0.80919999    0.0878        0.70450002]
 [2409.2775625     0.2770578     0.32802188    0.3339797     0.17365469]
 [1565.26571498    0.28737435    0.24559684    0.20375763    0.13707753]][0m
[37m[1m[2023-07-10 18:56:54,655][227910] Max Reward on eval: 5155.45625764858[0m
[37m[1m[2023-07-10 18:56:54,655][227910] Min Reward on eval: -1134.1939590187744[0m
[37m[1m[2023-07-10 18:56:54,655][227910] Mean Reward across all agents: 1440.3230815884503[0m
[37m[1m[2023-07-10 18:56:54,656][227910] Average Trajectory Length: 960.1866666666666[0m
[36m[2023-07-10 18:56:54,659][227910] mean_value=-1287.2238720760645, max_value=939.3608110955138[0m
[37m[1m[2023-07-10 18:56:54,662][227910] New mean coefficients: [[ 0.4943019   0.8108692   0.16059461 -0.14641929 -0.21422568]][0m
[37m[1m[2023-07-10 18:56:54,663][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:57:04,389][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 18:57:04,390][227910] FPS: 394851.02[0m
[36m[2023-07-10 18:57:04,392][227910] itr=1110, itrs=2000, Progress: 55.50%[0m
[37m[1m[2023-07-10 18:57:08,328][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001090[0m
[36m[2023-07-10 18:57:20,131][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 18:57:20,131][227910] FPS: 333114.29[0m
[36m[2023-07-10 18:57:24,893][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:57:24,893][227910] Reward + Measures: [[5517.25643548    0.47830895    0.21888511    0.17702767    0.1699405 ]][0m
[37m[1m[2023-07-10 18:57:24,893][227910] Max Reward on eval: 5517.256435475737[0m
[37m[1m[2023-07-10 18:57:24,893][227910] Min Reward on eval: 5517.256435475737[0m
[37m[1m[2023-07-10 18:57:24,894][227910] Mean Reward across all agents: 5517.256435475737[0m
[37m[1m[2023-07-10 18:57:24,894][227910] Average Trajectory Length: 996.7836666666666[0m
[36m[2023-07-10 18:57:30,321][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:57:30,322][227910] Reward + Measures: [[1962.58546902    0.2635017     0.26956305    0.20808068    0.15788825]
 [ 546.89899325    0.38916159    0.32222685    0.22464371    0.20461316]
 [ 444.38149926    0.28280002    0.35159999    0.49499997    0.22149999]
 ...
 [ 534.49607638    0.36860001    0.25229999    0.22720002    0.19509999]
 [-154.60396884    0.21345936    0.17529754    0.1848601     0.23409259]
 [ 937.36483906    0.40450001    0.24730001    0.60939997    0.20670001]][0m
[37m[1m[2023-07-10 18:57:30,322][227910] Max Reward on eval: 4786.24700176306[0m
[37m[1m[2023-07-10 18:57:30,322][227910] Min Reward on eval: -470.4686618732987[0m
[37m[1m[2023-07-10 18:57:30,323][227910] Mean Reward across all agents: 1498.0166406897392[0m
[37m[1m[2023-07-10 18:57:30,323][227910] Average Trajectory Length: 933.98[0m
[36m[2023-07-10 18:57:30,326][227910] mean_value=-1867.111783473144, max_value=2064.241294833847[0m
[37m[1m[2023-07-10 18:57:30,328][227910] New mean coefficients: [[ 0.9551742   0.35957253  0.4708163  -0.0120576   0.21004963]][0m
[37m[1m[2023-07-10 18:57:30,329][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:57:39,902][227910] train() took 9.57 seconds to complete[0m
[36m[2023-07-10 18:57:39,902][227910] FPS: 401202.62[0m
[36m[2023-07-10 18:57:39,904][227910] itr=1111, itrs=2000, Progress: 55.55%[0m
[36m[2023-07-10 18:57:51,359][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 18:57:51,359][227910] FPS: 335769.78[0m
[36m[2023-07-10 18:57:56,093][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:57:56,093][227910] Reward + Measures: [[5311.22979921    0.4878273     0.26916772    0.19550581    0.18181124]][0m
[37m[1m[2023-07-10 18:57:56,093][227910] Max Reward on eval: 5311.229799210223[0m
[37m[1m[2023-07-10 18:57:56,094][227910] Min Reward on eval: 5311.229799210223[0m
[37m[1m[2023-07-10 18:57:56,094][227910] Mean Reward across all agents: 5311.229799210223[0m
[37m[1m[2023-07-10 18:57:56,094][227910] Average Trajectory Length: 999.2546666666666[0m
[36m[2023-07-10 18:58:01,513][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:58:01,513][227910] Reward + Measures: [[3228.61427942    0.38031301    0.33607346    0.25112882    0.21854559]
 [ 989.17055054    0.2247        0.48909998    0.31000003    0.35390002]
 [1434.44599145    0.26429999    0.1724        0.27340001    0.1266    ]
 ...
 [1020.45935047    0.28099999    0.16680001    0.34469998    0.16489999]
 [1496.15918054    0.33810002    0.47499999    0.18090001    0.3646    ]
 [ 447.95611212    0.34670001    0.48470002    0.1147        0.56700003]][0m
[37m[1m[2023-07-10 18:58:01,513][227910] Max Reward on eval: 5084.839176599309[0m
[37m[1m[2023-07-10 18:58:01,514][227910] Min Reward on eval: -505.6359647080768[0m
[37m[1m[2023-07-10 18:58:01,514][227910] Mean Reward across all agents: 1602.7854256251176[0m
[37m[1m[2023-07-10 18:58:01,514][227910] Average Trajectory Length: 978.3696666666666[0m
[36m[2023-07-10 18:58:01,520][227910] mean_value=-461.5965756166117, max_value=4086.106105559649[0m
[37m[1m[2023-07-10 18:58:01,523][227910] New mean coefficients: [[ 0.9728097   0.18020879  0.45068073 -0.03486077  0.50415486]][0m
[37m[1m[2023-07-10 18:58:01,524][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:58:11,235][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 18:58:11,236][227910] FPS: 395465.69[0m
[36m[2023-07-10 18:58:11,238][227910] itr=1112, itrs=2000, Progress: 55.60%[0m
[36m[2023-07-10 18:58:22,697][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 18:58:22,697][227910] FPS: 335673.51[0m
[36m[2023-07-10 18:58:27,579][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:58:27,580][227910] Reward + Measures: [[5507.93597823    0.49638093    0.25739744    0.19173157    0.17898893]][0m
[37m[1m[2023-07-10 18:58:27,580][227910] Max Reward on eval: 5507.9359782320425[0m
[37m[1m[2023-07-10 18:58:27,580][227910] Min Reward on eval: 5507.9359782320425[0m
[37m[1m[2023-07-10 18:58:27,580][227910] Mean Reward across all agents: 5507.9359782320425[0m
[37m[1m[2023-07-10 18:58:27,581][227910] Average Trajectory Length: 999.247[0m
[36m[2023-07-10 18:58:33,126][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:58:33,126][227910] Reward + Measures: [[1490.95245529    0.2518        0.36359999    0.28240001    0.18349999]
 [ 851.67083661    0.17197235    0.16350976    0.17962684    0.13535123]
 [2342.35321454    0.50719994    0.37490001    0.26370001    0.20109999]
 ...
 [3078.31046207    0.32440001    0.31280002    0.20299999    0.15000001]
 [3253.52269636    0.37861463    0.33377692    0.24242115    0.16782507]
 [2022.14412407    0.26570001    0.53360003    0.3777        0.2192    ]][0m
[37m[1m[2023-07-10 18:58:33,127][227910] Max Reward on eval: 5201.547156690433[0m
[37m[1m[2023-07-10 18:58:33,127][227910] Min Reward on eval: 78.79430372499628[0m
[37m[1m[2023-07-10 18:58:33,127][227910] Mean Reward across all agents: 2480.5856451168847[0m
[37m[1m[2023-07-10 18:58:33,127][227910] Average Trajectory Length: 975.73[0m
[36m[2023-07-10 18:58:33,131][227910] mean_value=-1008.0179353516772, max_value=2378.478589189607[0m
[37m[1m[2023-07-10 18:58:33,133][227910] New mean coefficients: [[ 0.69224083  0.3931867   0.14533904 -0.11790515 -0.12549502]][0m
[37m[1m[2023-07-10 18:58:33,134][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:58:42,858][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 18:58:42,858][227910] FPS: 394993.57[0m
[36m[2023-07-10 18:58:42,860][227910] itr=1113, itrs=2000, Progress: 55.65%[0m
[36m[2023-07-10 18:58:54,406][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 18:58:54,406][227910] FPS: 333122.46[0m
[36m[2023-07-10 18:58:59,270][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:58:59,271][227910] Reward + Measures: [[5645.9279597     0.50903606    0.2538971     0.19061059    0.17929167]][0m
[37m[1m[2023-07-10 18:58:59,271][227910] Max Reward on eval: 5645.927959703047[0m
[37m[1m[2023-07-10 18:58:59,271][227910] Min Reward on eval: 5645.927959703047[0m
[37m[1m[2023-07-10 18:58:59,272][227910] Mean Reward across all agents: 5645.927959703047[0m
[37m[1m[2023-07-10 18:58:59,272][227910] Average Trajectory Length: 999.6759999999999[0m
[36m[2023-07-10 18:59:04,919][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:59:04,920][227910] Reward + Measures: [[1139.74485032    0.2983        0.3831        0.30900002    0.3382    ]
 [2453.95133633    0.3159        0.38290003    0.25460002    0.21350001]
 [3690.65671219    0.36870003    0.40459999    0.26060003    0.22860001]
 ...
 [2982.75755842    0.43899998    0.4707        0.25050002    0.1645    ]
 [2709.70680405    0.34652144    0.35552144    0.26612857    0.19316429]
 [1652.92825645    0.35849997    0.3475        0.2333        0.2156    ]][0m
[37m[1m[2023-07-10 18:59:04,920][227910] Max Reward on eval: 5401.677099962998[0m
[37m[1m[2023-07-10 18:59:04,920][227910] Min Reward on eval: -633.2764172288705[0m
[37m[1m[2023-07-10 18:59:04,921][227910] Mean Reward across all agents: 2126.6229598347772[0m
[37m[1m[2023-07-10 18:59:04,921][227910] Average Trajectory Length: 968.1646666666667[0m
[36m[2023-07-10 18:59:04,925][227910] mean_value=-788.9818798338199, max_value=3318.572834398719[0m
[37m[1m[2023-07-10 18:59:04,928][227910] New mean coefficients: [[ 0.8371278   0.43646505  0.1100565  -0.14636183  0.22480774]][0m
[37m[1m[2023-07-10 18:59:04,929][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:59:14,549][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 18:59:14,549][227910] FPS: 399241.41[0m
[36m[2023-07-10 18:59:14,552][227910] itr=1114, itrs=2000, Progress: 55.70%[0m
[36m[2023-07-10 18:59:26,114][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 18:59:26,114][227910] FPS: 332668.03[0m
[36m[2023-07-10 18:59:30,881][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:59:30,882][227910] Reward + Measures: [[5730.48616833    0.52223301    0.2557213     0.18944329    0.17872332]][0m
[37m[1m[2023-07-10 18:59:30,882][227910] Max Reward on eval: 5730.486168331236[0m
[37m[1m[2023-07-10 18:59:30,882][227910] Min Reward on eval: 5730.486168331236[0m
[37m[1m[2023-07-10 18:59:30,882][227910] Mean Reward across all agents: 5730.486168331236[0m
[37m[1m[2023-07-10 18:59:30,883][227910] Average Trajectory Length: 999.9413333333333[0m
[36m[2023-07-10 18:59:36,341][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 18:59:36,342][227910] Reward + Measures: [[  97.71166147    0.86020005    0.0915        0.79870003    0.73299998]
 [5019.72069306    0.41930005    0.3066        0.1859        0.182     ]
 [3949.7250389     0.46450004    0.35429999    0.23369999    0.18980001]
 ...
 [-112.6672276     0.87230009    0.0346        0.86059999    0.74590003]
 [1845.77658447    0.49309999    0.4883        0.32980004    0.25479999]
 [2207.55661797    0.39344999    0.43152499    0.23999999    0.171625  ]][0m
[37m[1m[2023-07-10 18:59:36,342][227910] Max Reward on eval: 5141.415210109204[0m
[37m[1m[2023-07-10 18:59:36,342][227910] Min Reward on eval: -406.9137064887909[0m
[37m[1m[2023-07-10 18:59:36,342][227910] Mean Reward across all agents: 2471.101462155654[0m
[37m[1m[2023-07-10 18:59:36,342][227910] Average Trajectory Length: 987.3473333333333[0m
[36m[2023-07-10 18:59:36,346][227910] mean_value=-695.9231569327342, max_value=4572.408187481915[0m
[37m[1m[2023-07-10 18:59:36,349][227910] New mean coefficients: [[ 0.20168865  0.5720537  -0.25361043 -0.22463822 -0.08084583]][0m
[37m[1m[2023-07-10 18:59:36,350][227910] Moving the mean solution point...[0m
[36m[2023-07-10 18:59:45,939][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 18:59:45,940][227910] FPS: 400525.95[0m
[36m[2023-07-10 18:59:45,942][227910] itr=1115, itrs=2000, Progress: 55.75%[0m
[36m[2023-07-10 18:59:57,436][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 18:59:57,437][227910] FPS: 334603.94[0m
[36m[2023-07-10 19:00:02,217][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:00:02,218][227910] Reward + Measures: [[5001.62208628    0.50796157    0.23575288    0.17173351    0.16631781]][0m
[37m[1m[2023-07-10 19:00:02,218][227910] Max Reward on eval: 5001.622086280902[0m
[37m[1m[2023-07-10 19:00:02,218][227910] Min Reward on eval: 5001.622086280902[0m
[37m[1m[2023-07-10 19:00:02,218][227910] Mean Reward across all agents: 5001.622086280902[0m
[37m[1m[2023-07-10 19:00:02,218][227910] Average Trajectory Length: 995.7856666666667[0m
[36m[2023-07-10 19:00:07,753][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:00:07,754][227910] Reward + Measures: [[2245.49324464    0.54229999    0.22290002    0.25500003    0.23650001]
 [2329.23538864    0.3972778     0.44438154    0.17946199    0.1539433 ]
 [3027.00487436    0.5025        0.2633        0.24560001    0.23480001]
 ...
 [1425.06224946    0.33810988    0.28680325    0.2584565     0.1412303 ]
 [1598.47987875    0.51700002    0.1681        0.33200002    0.26279998]
 [1980.81018226    0.51109999    0.20180002    0.2647        0.24900003]][0m
[37m[1m[2023-07-10 19:00:07,754][227910] Max Reward on eval: 5105.7752896273505[0m
[37m[1m[2023-07-10 19:00:07,755][227910] Min Reward on eval: -545.4952629736275[0m
[37m[1m[2023-07-10 19:00:07,755][227910] Mean Reward across all agents: 2029.793718416541[0m
[37m[1m[2023-07-10 19:00:07,755][227910] Average Trajectory Length: 975.6413333333333[0m
[36m[2023-07-10 19:00:07,760][227910] mean_value=-285.5174249323564, max_value=4300.299410686995[0m
[37m[1m[2023-07-10 19:00:07,762][227910] New mean coefficients: [[ 0.78686714  0.49756682  0.04862946 -0.03542903  0.22451136]][0m
[37m[1m[2023-07-10 19:00:07,764][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:00:17,503][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 19:00:17,503][227910] FPS: 394331.29[0m
[36m[2023-07-10 19:00:17,506][227910] itr=1116, itrs=2000, Progress: 55.80%[0m
[36m[2023-07-10 19:00:29,037][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 19:00:29,037][227910] FPS: 333543.78[0m
[36m[2023-07-10 19:00:33,818][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:00:33,818][227910] Reward + Measures: [[5218.76371737    0.51141715    0.23632935    0.17098294    0.16622785]][0m
[37m[1m[2023-07-10 19:00:33,818][227910] Max Reward on eval: 5218.7637173672765[0m
[37m[1m[2023-07-10 19:00:33,819][227910] Min Reward on eval: 5218.7637173672765[0m
[37m[1m[2023-07-10 19:00:33,819][227910] Mean Reward across all agents: 5218.7637173672765[0m
[37m[1m[2023-07-10 19:00:33,819][227910] Average Trajectory Length: 997.1803333333334[0m
[36m[2023-07-10 19:00:39,340][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:00:39,340][227910] Reward + Measures: [[1641.41167789    0.50230002    0.42570001    0.3008        0.25729999]
 [2666.92288214    0.32918683    0.27639505    0.21052222    0.18279918]
 [1177.98015972    0.39669999    0.60989994    0.2234        0.4966    ]
 ...
 [4610.36242177    0.48769999    0.2877        0.18770002    0.18719999]
 [2865.87600616    0.45749998    0.57489997    0.264         0.2455    ]
 [4593.69314866    0.48999998    0.27379999    0.1716        0.15989999]][0m
[37m[1m[2023-07-10 19:00:39,340][227910] Max Reward on eval: 5165.415256866347[0m
[37m[1m[2023-07-10 19:00:39,341][227910] Min Reward on eval: 37.893282461282794[0m
[37m[1m[2023-07-10 19:00:39,341][227910] Mean Reward across all agents: 2197.6758898534526[0m
[37m[1m[2023-07-10 19:00:39,341][227910] Average Trajectory Length: 993.1356666666667[0m
[36m[2023-07-10 19:00:39,346][227910] mean_value=-36.59483354169158, max_value=3749.424237123537[0m
[37m[1m[2023-07-10 19:00:39,349][227910] New mean coefficients: [[ 0.73547655  0.614655   -0.02898839 -0.30210626 -0.23376119]][0m
[37m[1m[2023-07-10 19:00:39,351][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:00:49,054][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 19:00:49,054][227910] FPS: 395803.08[0m
[36m[2023-07-10 19:00:49,057][227910] itr=1117, itrs=2000, Progress: 55.85%[0m
[36m[2023-07-10 19:01:00,597][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 19:01:00,597][227910] FPS: 333274.91[0m
[36m[2023-07-10 19:01:05,249][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:01:05,250][227910] Reward + Measures: [[5438.49669853    0.53560138    0.23230529    0.17132604    0.16847894]][0m
[37m[1m[2023-07-10 19:01:05,250][227910] Max Reward on eval: 5438.496698533934[0m
[37m[1m[2023-07-10 19:01:05,250][227910] Min Reward on eval: 5438.496698533934[0m
[37m[1m[2023-07-10 19:01:05,250][227910] Mean Reward across all agents: 5438.496698533934[0m
[37m[1m[2023-07-10 19:01:05,251][227910] Average Trajectory Length: 997.9833333333333[0m
[36m[2023-07-10 19:01:10,815][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:01:10,816][227910] Reward + Measures: [[2500.21850275    0.51070005    0.44499999    0.18260001    0.1635    ]
 [2411.82016271    0.61400002    0.25289997    0.35029998    0.25219998]
 [1571.68988677    0.41807064    0.47401792    0.25252649    0.13049072]
 ...
 [1749.33998715    0.33710003    0.3788        0.22500001    0.28130001]
 [2386.37409354    0.45949998    0.39760002    0.2059        0.15179999]
 [1758.30753163    0.49740002    0.45480004    0.27040002    0.31440002]][0m
[37m[1m[2023-07-10 19:01:10,816][227910] Max Reward on eval: 5104.8896935403345[0m
[37m[1m[2023-07-10 19:01:10,816][227910] Min Reward on eval: -340.13554588725094[0m
[37m[1m[2023-07-10 19:01:10,816][227910] Mean Reward across all agents: 2306.2854735145006[0m
[37m[1m[2023-07-10 19:01:10,817][227910] Average Trajectory Length: 978.3589999999999[0m
[36m[2023-07-10 19:01:10,820][227910] mean_value=-652.780831143972, max_value=2879.274506140312[0m
[37m[1m[2023-07-10 19:01:10,823][227910] New mean coefficients: [[ 0.3235626   0.43074948 -0.04671102 -0.14358445 -0.32778418]][0m
[37m[1m[2023-07-10 19:01:10,824][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:01:20,446][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 19:01:20,447][227910] FPS: 399155.36[0m
[36m[2023-07-10 19:01:20,449][227910] itr=1118, itrs=2000, Progress: 55.90%[0m
[36m[2023-07-10 19:01:31,979][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 19:01:31,979][227910] FPS: 333667.21[0m
[36m[2023-07-10 19:01:36,798][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:01:36,798][227910] Reward + Measures: [[5653.36501016    0.54717571    0.2341191     0.17651826    0.17349756]][0m
[37m[1m[2023-07-10 19:01:36,798][227910] Max Reward on eval: 5653.365010158109[0m
[37m[1m[2023-07-10 19:01:36,798][227910] Min Reward on eval: 5653.365010158109[0m
[37m[1m[2023-07-10 19:01:36,799][227910] Mean Reward across all agents: 5653.365010158109[0m
[37m[1m[2023-07-10 19:01:36,799][227910] Average Trajectory Length: 998.54[0m
[36m[2023-07-10 19:01:42,276][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:01:42,276][227910] Reward + Measures: [[2309.30203626    0.51080006    0.61969995    0.23000002    0.24370001]
 [3075.5207734     0.46290001    0.41          0.23369999    0.2023    ]
 [3845.41753605    0.52469999    0.2493        0.21159999    0.2472    ]
 ...
 [2395.20263705    0.58380002    0.5201        0.21600001    0.19719999]
 [2571.80155147    0.62480003    0.43030006    0.22309999    0.18380001]
 [2132.7336514     0.39739567    0.31783709    0.26955041    0.20326407]][0m
[37m[1m[2023-07-10 19:01:42,277][227910] Max Reward on eval: 5403.97558669881[0m
[37m[1m[2023-07-10 19:01:42,277][227910] Min Reward on eval: 103.22806100513553[0m
[37m[1m[2023-07-10 19:01:42,277][227910] Mean Reward across all agents: 2662.192913919529[0m
[37m[1m[2023-07-10 19:01:42,277][227910] Average Trajectory Length: 988.4639999999999[0m
[36m[2023-07-10 19:01:42,283][227910] mean_value=-12.745668522256699, max_value=4298.963605780108[0m
[37m[1m[2023-07-10 19:01:42,285][227910] New mean coefficients: [[ 0.64957154  0.43726552 -0.26112404 -0.25938857 -0.5766308 ]][0m
[37m[1m[2023-07-10 19:01:42,287][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:01:51,867][227910] train() took 9.58 seconds to complete[0m
[36m[2023-07-10 19:01:51,867][227910] FPS: 400882.93[0m
[36m[2023-07-10 19:01:51,870][227910] itr=1119, itrs=2000, Progress: 55.95%[0m
[36m[2023-07-10 19:02:03,342][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 19:02:03,343][227910] FPS: 335234.70[0m
[36m[2023-07-10 19:02:08,093][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:02:08,094][227910] Reward + Measures: [[5693.8480712     0.55656207    0.233395      0.17637667    0.17095931]][0m
[37m[1m[2023-07-10 19:02:08,094][227910] Max Reward on eval: 5693.848071202768[0m
[37m[1m[2023-07-10 19:02:08,094][227910] Min Reward on eval: 5693.848071202768[0m
[37m[1m[2023-07-10 19:02:08,094][227910] Mean Reward across all agents: 5693.848071202768[0m
[37m[1m[2023-07-10 19:02:08,095][227910] Average Trajectory Length: 999.0333333333333[0m
[36m[2023-07-10 19:02:13,561][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:02:13,561][227910] Reward + Measures: [[3105.99319688    0.41582665    0.33132872    0.16662157    0.14638697]
 [1758.86551552    0.4224        0.41280004    0.34840003    0.2228    ]
 [4623.567585      0.44780001    0.37919998    0.21470001    0.18670002]
 ...
 [3528.01313568    0.47370002    0.32519999    0.20299999    0.1788    ]
 [3011.21024211    0.48260003    0.3698        0.21900001    0.22160001]
 [ 391.26667214    0.45970002    0.31960002    0.35639998    0.22690001]][0m
[37m[1m[2023-07-10 19:02:13,562][227910] Max Reward on eval: 5456.308995380998[0m
[37m[1m[2023-07-10 19:02:13,562][227910] Min Reward on eval: -685.636639572261[0m
[37m[1m[2023-07-10 19:02:13,562][227910] Mean Reward across all agents: 2023.7399598154066[0m
[37m[1m[2023-07-10 19:02:13,562][227910] Average Trajectory Length: 989.117[0m
[36m[2023-07-10 19:02:13,567][227910] mean_value=-458.6700322952562, max_value=3119.8625631723135[0m
[37m[1m[2023-07-10 19:02:13,570][227910] New mean coefficients: [[ 0.9169414   0.11272532 -0.02416204 -0.2092149  -0.49655604]][0m
[37m[1m[2023-07-10 19:02:13,571][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:02:23,257][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 19:02:23,257][227910] FPS: 396532.10[0m
[36m[2023-07-10 19:02:23,259][227910] itr=1120, itrs=2000, Progress: 56.00%[0m
[37m[1m[2023-07-10 19:02:27,093][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001100[0m
[36m[2023-07-10 19:02:38,832][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 19:02:38,832][227910] FPS: 335186.61[0m
[36m[2023-07-10 19:02:43,682][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:02:43,682][227910] Reward + Measures: [[5758.78328982    0.55153322    0.23099913    0.17939599    0.17305733]][0m
[37m[1m[2023-07-10 19:02:43,682][227910] Max Reward on eval: 5758.783289822312[0m
[37m[1m[2023-07-10 19:02:43,683][227910] Min Reward on eval: 5758.783289822312[0m
[37m[1m[2023-07-10 19:02:43,683][227910] Mean Reward across all agents: 5758.783289822312[0m
[37m[1m[2023-07-10 19:02:43,683][227910] Average Trajectory Length: 999.5229999999999[0m
[36m[2023-07-10 19:02:49,354][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:02:49,354][227910] Reward + Measures: [[ 409.93540249    0.49439999    0.60600001    0.07970001    0.55810004]
 [2183.70946713    0.42309999    0.4578        0.163         0.2832    ]
 [ 871.12870589    0.40548405    0.38738748    0.12675072    0.37910452]
 ...
 [ 770.43031626    0.23556435    0.22977929    0.29380223    0.23766378]
 [2773.88419259    0.55965209    0.26783213    0.27407941    0.17035544]
 [1843.58549589    0.28830001    0.46960002    0.1998        0.30360004]][0m
[37m[1m[2023-07-10 19:02:49,354][227910] Max Reward on eval: 5753.298117113952[0m
[37m[1m[2023-07-10 19:02:49,355][227910] Min Reward on eval: -217.24324951965826[0m
[37m[1m[2023-07-10 19:02:49,355][227910] Mean Reward across all agents: 2012.1235858997966[0m
[37m[1m[2023-07-10 19:02:49,355][227910] Average Trajectory Length: 955.8879999999999[0m
[36m[2023-07-10 19:02:49,360][227910] mean_value=-560.8666485433803, max_value=2608.5264282929043[0m
[37m[1m[2023-07-10 19:02:49,363][227910] New mean coefficients: [[ 1.1584375   0.19723397  0.20317067 -0.15688549 -0.42558596]][0m
[37m[1m[2023-07-10 19:02:49,364][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:02:59,055][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 19:02:59,056][227910] FPS: 396279.99[0m
[36m[2023-07-10 19:02:59,058][227910] itr=1121, itrs=2000, Progress: 56.05%[0m
[36m[2023-07-10 19:03:10,504][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 19:03:10,504][227910] FPS: 336032.82[0m
[36m[2023-07-10 19:03:15,366][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:03:15,366][227910] Reward + Measures: [[5832.70648854    0.54519457    0.22979683    0.17581807    0.17154516]][0m
[37m[1m[2023-07-10 19:03:15,367][227910] Max Reward on eval: 5832.706488537955[0m
[37m[1m[2023-07-10 19:03:15,367][227910] Min Reward on eval: 5832.706488537955[0m
[37m[1m[2023-07-10 19:03:15,367][227910] Mean Reward across all agents: 5832.706488537955[0m
[37m[1m[2023-07-10 19:03:15,367][227910] Average Trajectory Length: 999.259[0m
[36m[2023-07-10 19:03:20,911][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:03:20,911][227910] Reward + Measures: [[2488.81578151    0.50709999    0.38680002    0.30440003    0.2033    ]
 [ 252.07587724    0.63480002    0.45720005    0.50800002    0.2922    ]
 [ 629.27122554    0.35770002    0.33670002    0.35130003    0.39970002]
 ...
 [2430.51847469    0.40880004    0.38430002    0.1919        0.25050002]
 [2413.79416126    0.48429999    0.30029997    0.2098        0.24029998]
 [4786.00012445    0.42919999    0.37890002    0.2059        0.18380001]][0m
[37m[1m[2023-07-10 19:03:20,911][227910] Max Reward on eval: 5632.033818348777[0m
[37m[1m[2023-07-10 19:03:20,912][227910] Min Reward on eval: -128.02120865859325[0m
[37m[1m[2023-07-10 19:03:20,912][227910] Mean Reward across all agents: 2495.8050173261713[0m
[37m[1m[2023-07-10 19:03:20,912][227910] Average Trajectory Length: 974.2833333333333[0m
[36m[2023-07-10 19:03:20,916][227910] mean_value=-723.464363811812, max_value=5405.362171867489[0m
[37m[1m[2023-07-10 19:03:20,918][227910] New mean coefficients: [[ 1.1095339   0.16331074  0.30482548  0.05717193 -0.05449659]][0m
[37m[1m[2023-07-10 19:03:20,919][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:03:30,700][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 19:03:30,700][227910] FPS: 392687.26[0m
[36m[2023-07-10 19:03:30,703][227910] itr=1122, itrs=2000, Progress: 56.10%[0m
[36m[2023-07-10 19:03:42,216][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 19:03:42,216][227910] FPS: 334078.38[0m
[36m[2023-07-10 19:03:47,076][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:03:47,076][227910] Reward + Measures: [[5901.20342547    0.54132438    0.236938      0.17741549    0.17621534]][0m
[37m[1m[2023-07-10 19:03:47,077][227910] Max Reward on eval: 5901.203425470648[0m
[37m[1m[2023-07-10 19:03:47,077][227910] Min Reward on eval: 5901.203425470648[0m
[37m[1m[2023-07-10 19:03:47,077][227910] Mean Reward across all agents: 5901.203425470648[0m
[37m[1m[2023-07-10 19:03:47,078][227910] Average Trajectory Length: 999.0793333333334[0m
[36m[2023-07-10 19:03:52,426][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:03:52,427][227910] Reward + Measures: [[1736.90201967    0.2069        0.13940001    0.20389998    0.14549999]
 [3060.46401166    0.40129995    0.3145        0.21560001    0.21279998]
 [3237.41153837    0.54250002    0.52380002    0.21669999    0.2189    ]
 ...
 [5293.01457379    0.43070003    0.3026        0.20390001    0.20109999]
 [2081.50008239    0.46715936    0.57002497    0.20749374    0.29690626]
 [2731.4887075     0.34709999    0.3504        0.2647        0.16990001]][0m
[37m[1m[2023-07-10 19:03:52,427][227910] Max Reward on eval: 5772.515048226342[0m
[37m[1m[2023-07-10 19:03:52,427][227910] Min Reward on eval: -206.42855279280337[0m
[37m[1m[2023-07-10 19:03:52,427][227910] Mean Reward across all agents: 2823.7083493331393[0m
[37m[1m[2023-07-10 19:03:52,428][227910] Average Trajectory Length: 983.9313333333333[0m
[36m[2023-07-10 19:03:52,433][227910] mean_value=-496.4863372196925, max_value=5647.500568411883[0m
[37m[1m[2023-07-10 19:03:52,435][227910] New mean coefficients: [[ 1.3165717  -0.05372551  0.41018388  0.09234516  0.29316568]][0m
[37m[1m[2023-07-10 19:03:52,436][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:04:02,138][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 19:04:02,138][227910] FPS: 395872.56[0m
[36m[2023-07-10 19:04:02,141][227910] itr=1123, itrs=2000, Progress: 56.15%[0m
[36m[2023-07-10 19:04:13,649][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 19:04:13,650][227910] FPS: 334216.76[0m
[36m[2023-07-10 19:04:18,413][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:04:18,413][227910] Reward + Measures: [[5954.06161743    0.53653377    0.24095762    0.17364888    0.17369869]][0m
[37m[1m[2023-07-10 19:04:18,413][227910] Max Reward on eval: 5954.061617431216[0m
[37m[1m[2023-07-10 19:04:18,413][227910] Min Reward on eval: 5954.061617431216[0m
[37m[1m[2023-07-10 19:04:18,413][227910] Mean Reward across all agents: 5954.061617431216[0m
[37m[1m[2023-07-10 19:04:18,414][227910] Average Trajectory Length: 999.3389999999999[0m
[36m[2023-07-10 19:04:23,908][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:04:23,908][227910] Reward + Measures: [[3508.03339234    0.48979998    0.43280002    0.223         0.17920001]
 [2936.22605721    0.44549999    0.4948        0.28120002    0.2052    ]
 [2669.4108895     0.54880005    0.37100002    0.20910001    0.1839    ]
 ...
 [2597.34673674    0.25570002    0.3705        0.25189999    0.23380001]
 [4094.10481658    0.47750002    0.3836        0.2225        0.2103    ]
 [ 741.83868104    0.19700001    0.19930001    0.2244        0.25009999]][0m
[37m[1m[2023-07-10 19:04:23,909][227910] Max Reward on eval: 5777.189062987454[0m
[37m[1m[2023-07-10 19:04:23,909][227910] Min Reward on eval: 176.20820641281898[0m
[37m[1m[2023-07-10 19:04:23,909][227910] Mean Reward across all agents: 2631.3216371498797[0m
[37m[1m[2023-07-10 19:04:23,909][227910] Average Trajectory Length: 976.385[0m
[36m[2023-07-10 19:04:23,912][227910] mean_value=-906.1454425559056, max_value=1590.0755688135114[0m
[37m[1m[2023-07-10 19:04:23,914][227910] New mean coefficients: [[ 1.4921815  -0.00605623  0.23920561 -0.11965813  0.27002162]][0m
[37m[1m[2023-07-10 19:04:23,915][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:04:33,648][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 19:04:33,648][227910] FPS: 394618.41[0m
[36m[2023-07-10 19:04:33,651][227910] itr=1124, itrs=2000, Progress: 56.20%[0m
[36m[2023-07-10 19:04:45,251][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 19:04:45,251][227910] FPS: 331642.91[0m
[36m[2023-07-10 19:04:50,009][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:04:50,014][227910] Reward + Measures: [[6029.25008592    0.53718024    0.2374609     0.17333764    0.17104268]][0m
[37m[1m[2023-07-10 19:04:50,015][227910] Max Reward on eval: 6029.250085915798[0m
[37m[1m[2023-07-10 19:04:50,016][227910] Min Reward on eval: 6029.250085915798[0m
[37m[1m[2023-07-10 19:04:50,017][227910] Mean Reward across all agents: 6029.250085915798[0m
[37m[1m[2023-07-10 19:04:50,017][227910] Average Trajectory Length: 999.786[0m
[36m[2023-07-10 19:04:55,640][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:04:55,646][227910] Reward + Measures: [[4320.54807536    0.51020503    0.21344085    0.18670629    0.17750044]
 [3788.76230413    0.46943274    0.36329558    0.25314602    0.19045399]
 [5973.4824548     0.52770001    0.24100001    0.17470001    0.17400001]
 ...
 [1994.49971646    0.40162554    0.20543192    0.19696736    0.22765036]
 [3475.88334489    0.48336133    0.23783047    0.19460949    0.18695137]
 [3047.58037652    0.46332738    0.22726838    0.20105486    0.18485908]][0m
[37m[1m[2023-07-10 19:04:55,647][227910] Max Reward on eval: 5973.482454804704[0m
[37m[1m[2023-07-10 19:04:55,648][227910] Min Reward on eval: 822.7885891157755[0m
[37m[1m[2023-07-10 19:04:55,648][227910] Mean Reward across all agents: 3419.9728837596545[0m
[37m[1m[2023-07-10 19:04:55,649][227910] Average Trajectory Length: 971.0129999999999[0m
[36m[2023-07-10 19:04:55,655][227910] mean_value=-743.3262537442406, max_value=5600.990381799396[0m
[37m[1m[2023-07-10 19:04:55,660][227910] New mean coefficients: [[ 1.5978222   0.034608    0.17085218 -0.06417014 -0.04259446]][0m
[37m[1m[2023-07-10 19:04:55,662][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:05:05,357][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 19:05:05,358][227910] FPS: 396141.66[0m
[36m[2023-07-10 19:05:05,360][227910] itr=1125, itrs=2000, Progress: 56.25%[0m
[36m[2023-07-10 19:05:16,994][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 19:05:16,994][227910] FPS: 330600.09[0m
[36m[2023-07-10 19:05:21,732][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:05:21,738][227910] Reward + Measures: [[6088.17273741    0.53192276    0.24310091    0.17516841    0.17164689]][0m
[37m[1m[2023-07-10 19:05:21,738][227910] Max Reward on eval: 6088.172737410628[0m
[37m[1m[2023-07-10 19:05:21,738][227910] Min Reward on eval: 6088.172737410628[0m
[37m[1m[2023-07-10 19:05:21,738][227910] Mean Reward across all agents: 6088.172737410628[0m
[37m[1m[2023-07-10 19:05:21,739][227910] Average Trajectory Length: 998.5353333333333[0m
[36m[2023-07-10 19:05:27,251][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:05:27,252][227910] Reward + Measures: [[4403.05338229    0.36489999    0.41890001    0.1978        0.18700002]
 [3072.41909206    0.42910001    0.35880002    0.32550001    0.22819999]
 [2275.07512617    0.32531494    0.39218542    0.25748599    0.24069361]
 ...
 [2623.8221189     0.34839997    0.3899        0.30379999    0.2218    ]
 [3968.25998137    0.58770001    0.28239998    0.26430002    0.21110001]
 [3908.96611165    0.34170002    0.47220001    0.26180002    0.23099999]][0m
[37m[1m[2023-07-10 19:05:27,252][227910] Max Reward on eval: 5696.020217915206[0m
[37m[1m[2023-07-10 19:05:27,252][227910] Min Reward on eval: 165.63611045887228[0m
[37m[1m[2023-07-10 19:05:27,252][227910] Mean Reward across all agents: 2651.549659532142[0m
[37m[1m[2023-07-10 19:05:27,253][227910] Average Trajectory Length: 975.725[0m
[36m[2023-07-10 19:05:27,256][227910] mean_value=-801.7741543933915, max_value=2236.0161541748653[0m
[37m[1m[2023-07-10 19:05:27,259][227910] New mean coefficients: [[ 1.8551962  -0.21981734  0.3393479   0.06641947  0.14549176]][0m
[37m[1m[2023-07-10 19:05:27,260][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:05:37,082][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 19:05:37,088][227910] FPS: 391026.25[0m
[36m[2023-07-10 19:05:37,090][227910] itr=1126, itrs=2000, Progress: 56.30%[0m
[36m[2023-07-10 19:05:48,580][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 19:05:48,580][227910] FPS: 334830.24[0m
[36m[2023-07-10 19:05:53,282][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:05:53,282][227910] Reward + Measures: [[6108.68647731    0.53822422    0.24470219    0.17297232    0.17180377]][0m
[37m[1m[2023-07-10 19:05:53,282][227910] Max Reward on eval: 6108.6864773075295[0m
[37m[1m[2023-07-10 19:05:53,283][227910] Min Reward on eval: 6108.6864773075295[0m
[37m[1m[2023-07-10 19:05:53,283][227910] Mean Reward across all agents: 6108.6864773075295[0m
[37m[1m[2023-07-10 19:05:53,283][227910] Average Trajectory Length: 999.0939999999999[0m
[36m[2023-07-10 19:05:58,726][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:05:58,727][227910] Reward + Measures: [[3622.98346609    0.35970002    0.36010003    0.18859999    0.18530001]
 [2435.35274838    0.43310004    0.35929999    0.24460001    0.1724    ]
 [1894.28235061    0.43816549    0.28497383    0.23971057    0.31170404]
 ...
 [ 858.21567533    0.39780003    0.74130005    0.10609999    0.71499997]
 [1439.22389474    0.36130032    0.29371005    0.29692596    0.22171973]
 [4675.77193345    0.50050002    0.2811        0.2067        0.18869999]][0m
[37m[1m[2023-07-10 19:05:58,727][227910] Max Reward on eval: 5867.862426208053[0m
[37m[1m[2023-07-10 19:05:58,727][227910] Min Reward on eval: -3.479466776107438[0m
[37m[1m[2023-07-10 19:05:58,728][227910] Mean Reward across all agents: 2954.629268815585[0m
[37m[1m[2023-07-10 19:05:58,728][227910] Average Trajectory Length: 973.901[0m
[36m[2023-07-10 19:05:58,733][227910] mean_value=-583.7456416265445, max_value=4940.643636353406[0m
[37m[1m[2023-07-10 19:05:58,735][227910] New mean coefficients: [[ 2.050212   -0.28040114  0.5462805   0.10858699  0.07444944]][0m
[37m[1m[2023-07-10 19:05:58,736][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:06:08,428][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 19:06:08,428][227910] FPS: 396297.59[0m
[36m[2023-07-10 19:06:08,431][227910] itr=1127, itrs=2000, Progress: 56.35%[0m
[36m[2023-07-10 19:06:19,879][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 19:06:19,880][227910] FPS: 335971.44[0m
[36m[2023-07-10 19:06:24,617][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:06:24,617][227910] Reward + Measures: [[6148.58532247    0.53438359    0.23985225    0.17060807    0.16902107]][0m
[37m[1m[2023-07-10 19:06:24,617][227910] Max Reward on eval: 6148.585322471681[0m
[37m[1m[2023-07-10 19:06:24,617][227910] Min Reward on eval: 6148.585322471681[0m
[37m[1m[2023-07-10 19:06:24,617][227910] Mean Reward across all agents: 6148.585322471681[0m
[37m[1m[2023-07-10 19:06:24,618][227910] Average Trajectory Length: 998.274[0m
[36m[2023-07-10 19:06:30,058][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:06:30,058][227910] Reward + Measures: [[3598.57274796    0.53909999    0.22880001    0.23080002    0.25130004]
 [2540.11592278    0.2613        0.36320001    0.21339999    0.19139999]
 [1358.89677473    0.34584951    0.24752541    0.2244921     0.17112844]
 ...
 [2678.77143478    0.49440002    0.25940004    0.2397        0.23070002]
 [ 858.82903105    0.41750002    0.33289999    0.30089998    0.31999999]
 [1344.07030215    0.28230003    0.36360002    0.24419999    0.2502    ]][0m
[37m[1m[2023-07-10 19:06:30,058][227910] Max Reward on eval: 6004.8156635152645[0m
[37m[1m[2023-07-10 19:06:30,059][227910] Min Reward on eval: 124.49792913416168[0m
[37m[1m[2023-07-10 19:06:30,059][227910] Mean Reward across all agents: 2558.2690144616954[0m
[37m[1m[2023-07-10 19:06:30,059][227910] Average Trajectory Length: 958.045[0m
[36m[2023-07-10 19:06:30,062][227910] mean_value=-1342.2835118506766, max_value=3092.6168535665893[0m
[37m[1m[2023-07-10 19:06:30,064][227910] New mean coefficients: [[ 1.8206104  -0.11311303  0.41337502 -0.00813404  0.16032356]][0m
[37m[1m[2023-07-10 19:06:30,065][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:06:39,630][227910] train() took 9.56 seconds to complete[0m
[36m[2023-07-10 19:06:39,630][227910] FPS: 401555.07[0m
[36m[2023-07-10 19:06:39,632][227910] itr=1128, itrs=2000, Progress: 56.40%[0m
[36m[2023-07-10 19:06:51,132][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 19:06:51,132][227910] FPS: 334554.85[0m
[36m[2023-07-10 19:06:55,846][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:06:55,851][227910] Reward + Measures: [[6257.93426609    0.51350945    0.23505944    0.16514081    0.16141045]][0m
[37m[1m[2023-07-10 19:06:55,852][227910] Max Reward on eval: 6257.934266092666[0m
[37m[1m[2023-07-10 19:06:55,852][227910] Min Reward on eval: 6257.934266092666[0m
[37m[1m[2023-07-10 19:06:55,852][227910] Mean Reward across all agents: 6257.934266092666[0m
[37m[1m[2023-07-10 19:06:55,853][227910] Average Trajectory Length: 998.4649999999999[0m
[36m[2023-07-10 19:07:01,310][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:07:01,311][227910] Reward + Measures: [[6014.79816634    0.45140001    0.25139999    0.1725        0.1639    ]
 [4092.54643773    0.3418        0.46619996    0.18440001    0.18080001]
 [1432.40033948    0.3353        0.39410001    0.30560002    0.28350002]
 ...
 [1936.4499774     0.2217        0.2237        0.2007        0.1831    ]
 [2043.15762282    0.25869998    0.20170002    0.23989999    0.1983    ]
 [ 788.64919634    0.19169603    0.15536986    0.18383248    0.11347707]][0m
[37m[1m[2023-07-10 19:07:01,311][227910] Max Reward on eval: 6153.27202009228[0m
[37m[1m[2023-07-10 19:07:01,311][227910] Min Reward on eval: 158.9437862380699[0m
[37m[1m[2023-07-10 19:07:01,311][227910] Mean Reward across all agents: 2886.5311844191087[0m
[37m[1m[2023-07-10 19:07:01,312][227910] Average Trajectory Length: 974.062[0m
[36m[2023-07-10 19:07:01,315][227910] mean_value=-1041.7025251491086, max_value=2160.754513932938[0m
[37m[1m[2023-07-10 19:07:01,317][227910] New mean coefficients: [[1.8662821  0.06412476 0.39725882 0.10705526 0.19027174]][0m
[37m[1m[2023-07-10 19:07:01,318][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:07:10,951][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 19:07:10,951][227910] FPS: 398718.35[0m
[36m[2023-07-10 19:07:10,953][227910] itr=1129, itrs=2000, Progress: 56.45%[0m
[36m[2023-07-10 19:07:22,383][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 19:07:22,384][227910] FPS: 336585.88[0m
[36m[2023-07-10 19:07:27,067][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:07:27,068][227910] Reward + Measures: [[6329.35515069    0.49440446    0.24297534    0.16366945    0.16199709]][0m
[37m[1m[2023-07-10 19:07:27,068][227910] Max Reward on eval: 6329.355150691714[0m
[37m[1m[2023-07-10 19:07:27,068][227910] Min Reward on eval: 6329.355150691714[0m
[37m[1m[2023-07-10 19:07:27,069][227910] Mean Reward across all agents: 6329.355150691714[0m
[37m[1m[2023-07-10 19:07:27,069][227910] Average Trajectory Length: 996.3929999999999[0m
[36m[2023-07-10 19:07:32,691][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:07:32,691][227910] Reward + Measures: [[5084.30657716    0.40509996    0.347         0.18480001    0.18090001]
 [3341.60061053    0.42580006    0.56540006    0.17040001    0.18340002]
 [1652.71540017    0.27720425    0.53641814    0.22463587    0.20538354]
 ...
 [3895.75136833    0.41799998    0.41710001    0.18550001    0.1851    ]
 [4580.97883758    0.48935667    0.21030858    0.1814684     0.16799913]
 [2124.56144966    0.4601        0.52870005    0.15720001    0.2244    ]][0m
[37m[1m[2023-07-10 19:07:32,691][227910] Max Reward on eval: 6228.242602923046[0m
[37m[1m[2023-07-10 19:07:32,692][227910] Min Reward on eval: -177.70051194512635[0m
[37m[1m[2023-07-10 19:07:32,692][227910] Mean Reward across all agents: 2831.8606201304347[0m
[37m[1m[2023-07-10 19:07:32,692][227910] Average Trajectory Length: 968.8376666666667[0m
[36m[2023-07-10 19:07:32,696][227910] mean_value=-245.77831627154742, max_value=4588.317519133738[0m
[37m[1m[2023-07-10 19:07:32,699][227910] New mean coefficients: [[1.8350518  0.11922066 0.5582608  0.17588621 0.2320618 ]][0m
[37m[1m[2023-07-10 19:07:32,700][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:07:42,362][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 19:07:42,362][227910] FPS: 397513.51[0m
[36m[2023-07-10 19:07:42,364][227910] itr=1130, itrs=2000, Progress: 56.50%[0m
[37m[1m[2023-07-10 19:07:46,229][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001110[0m
[36m[2023-07-10 19:07:58,022][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 19:07:58,023][227910] FPS: 333282.70[0m
[36m[2023-07-10 19:08:02,766][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:08:02,771][227910] Reward + Measures: [[5732.0569164     0.46384814    0.27311897    0.17495139    0.17016512]][0m
[37m[1m[2023-07-10 19:08:02,772][227910] Max Reward on eval: 5732.056916402425[0m
[37m[1m[2023-07-10 19:08:02,772][227910] Min Reward on eval: 5732.056916402425[0m
[37m[1m[2023-07-10 19:08:02,772][227910] Mean Reward across all agents: 5732.056916402425[0m
[37m[1m[2023-07-10 19:08:02,772][227910] Average Trajectory Length: 985.5113333333333[0m
[36m[2023-07-10 19:08:08,242][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:08:08,243][227910] Reward + Measures: [[3390.72719937    0.4686        0.37090001    0.1978        0.18889999]
 [3771.0003122     0.4131        0.4059        0.20480001    0.17209999]
 [2842.68759257    0.53389996    0.33570001    0.26630002    0.21300001]
 ...
 [2856.57638122    0.47180006    0.36519998    0.27990001    0.17260002]
 [2794.12859047    0.4729        0.31739998    0.2757        0.15620002]
 [4021.50064995    0.49935007    0.29876146    0.2149014     0.21295391]][0m
[37m[1m[2023-07-10 19:08:08,243][227910] Max Reward on eval: 5760.691326391883[0m
[37m[1m[2023-07-10 19:08:08,244][227910] Min Reward on eval: 641.3593750696979[0m
[37m[1m[2023-07-10 19:08:08,244][227910] Mean Reward across all agents: 3066.2069609740383[0m
[37m[1m[2023-07-10 19:08:08,244][227910] Average Trajectory Length: 990.27[0m
[36m[2023-07-10 19:08:08,246][227910] mean_value=-1120.1041890352487, max_value=3217.6847425191163[0m
[37m[1m[2023-07-10 19:08:08,249][227910] New mean coefficients: [[1.6283883  0.20367365 0.43550208 0.2414189  0.45529914]][0m
[37m[1m[2023-07-10 19:08:08,250][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:08:17,923][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 19:08:17,924][227910] FPS: 397013.57[0m
[36m[2023-07-10 19:08:17,926][227910] itr=1131, itrs=2000, Progress: 56.55%[0m
[36m[2023-07-10 19:08:29,417][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 19:08:29,417][227910] FPS: 334752.21[0m
[36m[2023-07-10 19:08:34,158][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:08:34,159][227910] Reward + Measures: [[2889.58383332    0.34672081    0.30429158    0.25118053    0.19179484]][0m
[37m[1m[2023-07-10 19:08:34,159][227910] Max Reward on eval: 2889.583833322801[0m
[37m[1m[2023-07-10 19:08:34,159][227910] Min Reward on eval: 2889.583833322801[0m
[37m[1m[2023-07-10 19:08:34,159][227910] Mean Reward across all agents: 2889.583833322801[0m
[37m[1m[2023-07-10 19:08:34,160][227910] Average Trajectory Length: 978.6656666666667[0m
[36m[2023-07-10 19:08:39,778][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:08:39,778][227910] Reward + Measures: [[1383.8582957     0.19992313    0.21662329    0.2687951     0.23838185]
 [1193.5381778     0.24800001    0.25909999    0.33410001    0.28850001]
 [1675.42865832    0.2318        0.27320001    0.26540002    0.22130001]
 ...
 [1813.5553309     0.28700003    0.27000001    0.35840002    0.2304    ]
 [ 799.52366471    0.176         0.22810002    0.2545        0.2534    ]
 [ 954.9728994     0.20588879    0.21544866    0.32800061    0.27200624]][0m
[37m[1m[2023-07-10 19:08:39,778][227910] Max Reward on eval: 3316.4377607554898[0m
[37m[1m[2023-07-10 19:08:39,779][227910] Min Reward on eval: 537.31217972046[0m
[37m[1m[2023-07-10 19:08:39,779][227910] Mean Reward across all agents: 1656.753832032304[0m
[37m[1m[2023-07-10 19:08:39,779][227910] Average Trajectory Length: 964.5816666666666[0m
[36m[2023-07-10 19:08:39,781][227910] mean_value=-1263.2680021465737, max_value=756.384690698351[0m
[37m[1m[2023-07-10 19:08:39,783][227910] New mean coefficients: [[1.4250782  0.34620774 0.5361545  0.46349347 0.7898407 ]][0m
[37m[1m[2023-07-10 19:08:39,784][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:08:49,449][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 19:08:49,449][227910] FPS: 397378.30[0m
[36m[2023-07-10 19:08:49,451][227910] itr=1132, itrs=2000, Progress: 56.60%[0m
[36m[2023-07-10 19:09:01,144][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 19:09:01,144][227910] FPS: 328918.97[0m
[36m[2023-07-10 19:09:05,840][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:09:05,841][227910] Reward + Measures: [[2139.97459154    0.31176135    0.23199855    0.18559693    0.17012054]][0m
[37m[1m[2023-07-10 19:09:05,841][227910] Max Reward on eval: 2139.974591542606[0m
[37m[1m[2023-07-10 19:09:05,841][227910] Min Reward on eval: 2139.974591542606[0m
[37m[1m[2023-07-10 19:09:05,842][227910] Mean Reward across all agents: 2139.974591542606[0m
[37m[1m[2023-07-10 19:09:05,842][227910] Average Trajectory Length: 859.934[0m
[36m[2023-07-10 19:09:11,298][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:09:11,303][227910] Reward + Measures: [[ 476.46069947    0.34513563    0.37751231    0.32317808    0.38400683]
 [ 673.86511571    0.42319998    0.66890001    0.43260002    0.60100001]
 [ -39.46589395    0.30122763    0.66908967    0.4895862     0.71315861]
 ...
 [-245.56572987    0.51340002    0.7863        0.6437        0.82989997]
 [ 690.22685635    0.27038318    0.20773838    0.23703597    0.25110593]
 [-226.90295273    0.6124        0.78300005    0.67130005    0.76990002]][0m
[37m[1m[2023-07-10 19:09:11,304][227910] Max Reward on eval: 2772.5234343590537[0m
[37m[1m[2023-07-10 19:09:11,304][227910] Min Reward on eval: -754.9769886518595[0m
[37m[1m[2023-07-10 19:09:11,304][227910] Mean Reward across all agents: 543.0666682765744[0m
[37m[1m[2023-07-10 19:09:11,304][227910] Average Trajectory Length: 934.1809999999999[0m
[36m[2023-07-10 19:09:11,307][227910] mean_value=-988.0980494842207, max_value=632.599423581923[0m
[37m[1m[2023-07-10 19:09:11,310][227910] New mean coefficients: [[1.5579609  0.17016841 0.47415084 0.4764508  1.0923362 ]][0m
[37m[1m[2023-07-10 19:09:11,311][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:09:21,038][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 19:09:21,038][227910] FPS: 394845.60[0m
[36m[2023-07-10 19:09:21,040][227910] itr=1133, itrs=2000, Progress: 56.65%[0m
[36m[2023-07-10 19:09:32,459][227910] train() took 11.40 seconds to complete[0m
[36m[2023-07-10 19:09:32,460][227910] FPS: 336804.81[0m
[36m[2023-07-10 19:09:37,269][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:09:37,270][227910] Reward + Measures: [[2628.39375872    0.3436248     0.24693532    0.18892847    0.17098458]][0m
[37m[1m[2023-07-10 19:09:37,270][227910] Max Reward on eval: 2628.393758723283[0m
[37m[1m[2023-07-10 19:09:37,270][227910] Min Reward on eval: 2628.393758723283[0m
[37m[1m[2023-07-10 19:09:37,270][227910] Mean Reward across all agents: 2628.393758723283[0m
[37m[1m[2023-07-10 19:09:37,270][227910] Average Trajectory Length: 873.5733333333333[0m
[36m[2023-07-10 19:09:42,569][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:09:42,570][227910] Reward + Measures: [[1564.35259573    0.37790003    0.18770002    0.28180003    0.18120001]
 [1327.89455339    0.53260005    0.23380001    0.3777        0.22839999]
 [3075.62790943    0.41179535    0.24597208    0.21667674    0.16382559]
 ...
 [1697.69132677    0.48994967    0.18981062    0.26062933    0.18520264]
 [1326.78147347    0.37321377    0.20735569    0.26794848    0.17405929]
 [2040.39655559    0.45640001    0.2158        0.2832        0.17900001]][0m
[37m[1m[2023-07-10 19:09:42,570][227910] Max Reward on eval: 3365.626366731897[0m
[37m[1m[2023-07-10 19:09:42,570][227910] Min Reward on eval: 115.81805209833547[0m
[37m[1m[2023-07-10 19:09:42,570][227910] Mean Reward across all agents: 1937.7088042754062[0m
[37m[1m[2023-07-10 19:09:42,571][227910] Average Trajectory Length: 942.3426666666667[0m
[36m[2023-07-10 19:09:42,572][227910] mean_value=-2353.360488498607, max_value=2109.985591133883[0m
[37m[1m[2023-07-10 19:09:42,574][227910] New mean coefficients: [[1.7288811  0.12521732 0.4032409  0.54919755 0.9119659 ]][0m
[37m[1m[2023-07-10 19:09:42,575][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:09:52,291][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 19:09:52,292][227910] FPS: 395281.28[0m
[36m[2023-07-10 19:09:52,294][227910] itr=1134, itrs=2000, Progress: 56.70%[0m
[36m[2023-07-10 19:10:03,810][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 19:10:03,810][227910] FPS: 333978.46[0m
[36m[2023-07-10 19:10:08,528][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:10:08,529][227910] Reward + Measures: [[3264.590517      0.37777343    0.26250798    0.1874827     0.17017904]][0m
[37m[1m[2023-07-10 19:10:08,529][227910] Max Reward on eval: 3264.590516999386[0m
[37m[1m[2023-07-10 19:10:08,529][227910] Min Reward on eval: 3264.590516999386[0m
[37m[1m[2023-07-10 19:10:08,529][227910] Mean Reward across all agents: 3264.590516999386[0m
[37m[1m[2023-07-10 19:10:08,530][227910] Average Trajectory Length: 898.9633333333333[0m
[36m[2023-07-10 19:10:14,003][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:10:14,003][227910] Reward + Measures: [[2034.36454591    0.34200001    0.36249998    0.24800001    0.2703    ]
 [1953.29398586    0.4772        0.29460001    0.26620001    0.28290001]
 [2225.67190045    0.41050002    0.26970002    0.17740001    0.21210001]
 ...
 [2296.46510421    0.35049319    0.34850666    0.24254619    0.28589448]
 [2243.68159091    0.39340001    0.30719998    0.17950001    0.21640001]
 [1917.27691611    0.33930001    0.34830001    0.30000001    0.30280003]][0m
[37m[1m[2023-07-10 19:10:14,004][227910] Max Reward on eval: 3732.064657001151[0m
[37m[1m[2023-07-10 19:10:14,004][227910] Min Reward on eval: -770.0122666739277[0m
[37m[1m[2023-07-10 19:10:14,004][227910] Mean Reward across all agents: 1772.7645135396285[0m
[37m[1m[2023-07-10 19:10:14,004][227910] Average Trajectory Length: 944.6959999999999[0m
[36m[2023-07-10 19:10:14,007][227910] mean_value=-1671.3944091977094, max_value=528.9466198717688[0m
[37m[1m[2023-07-10 19:10:14,009][227910] New mean coefficients: [[1.2637548  0.11096118 0.45247665 0.82767856 1.3812954 ]][0m
[37m[1m[2023-07-10 19:10:14,010][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:10:23,729][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 19:10:23,729][227910] FPS: 395179.34[0m
[36m[2023-07-10 19:10:23,732][227910] itr=1135, itrs=2000, Progress: 56.75%[0m
[36m[2023-07-10 19:10:35,287][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 19:10:35,287][227910] FPS: 332831.76[0m
[36m[2023-07-10 19:10:40,039][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:10:40,040][227910] Reward + Measures: [[1086.45621733    0.33652577    0.18421549    0.16793792    0.17709184]][0m
[37m[1m[2023-07-10 19:10:40,040][227910] Max Reward on eval: 1086.4562173346499[0m
[37m[1m[2023-07-10 19:10:40,040][227910] Min Reward on eval: 1086.4562173346499[0m
[37m[1m[2023-07-10 19:10:40,041][227910] Mean Reward across all agents: 1086.4562173346499[0m
[37m[1m[2023-07-10 19:10:40,041][227910] Average Trajectory Length: 919.4893333333333[0m
[36m[2023-07-10 19:10:45,529][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:10:45,535][227910] Reward + Measures: [[1017.35313474    0.359         0.1772        0.14920001    0.20079999]
 [1331.29913916    0.39900002    0.2093        0.1797        0.2115    ]
 [1170.92064571    0.32680002    0.16589999    0.1549        0.1675    ]
 ...
 [ 868.44418635    0.3580471     0.19207297    0.12771621    0.22225869]
 [1289.34534105    0.36933622    0.17709489    0.1767735     0.19644405]
 [ 651.40531187    0.26659998    0.23980001    0.176         0.2474    ]][0m
[37m[1m[2023-07-10 19:10:45,535][227910] Max Reward on eval: 1407.7070380194345[0m
[37m[1m[2023-07-10 19:10:45,536][227910] Min Reward on eval: 110.79766716990852[0m
[37m[1m[2023-07-10 19:10:45,536][227910] Mean Reward across all agents: 802.0420460374038[0m
[37m[1m[2023-07-10 19:10:45,536][227910] Average Trajectory Length: 914.6153333333333[0m
[36m[2023-07-10 19:10:45,539][227910] mean_value=-1649.5499814819093, max_value=1648.112310690491[0m
[37m[1m[2023-07-10 19:10:45,542][227910] New mean coefficients: [[1.5554988  0.09204667 0.6092133  0.84480804 1.4126016 ]][0m
[37m[1m[2023-07-10 19:10:45,543][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:10:55,229][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 19:10:55,230][227910] FPS: 396480.16[0m
[36m[2023-07-10 19:10:55,232][227910] itr=1136, itrs=2000, Progress: 56.80%[0m
[36m[2023-07-10 19:11:06,764][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 19:11:06,764][227910] FPS: 333645.52[0m
[36m[2023-07-10 19:11:11,633][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:11:11,634][227910] Reward + Measures: [[1208.21993102    0.33679768    0.19094642    0.17053898    0.17385876]][0m
[37m[1m[2023-07-10 19:11:11,634][227910] Max Reward on eval: 1208.2199310165163[0m
[37m[1m[2023-07-10 19:11:11,634][227910] Min Reward on eval: 1208.2199310165163[0m
[37m[1m[2023-07-10 19:11:11,634][227910] Mean Reward across all agents: 1208.2199310165163[0m
[37m[1m[2023-07-10 19:11:11,635][227910] Average Trajectory Length: 922.606[0m
[36m[2023-07-10 19:11:17,403][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:11:17,403][227910] Reward + Measures: [[1494.10429112    0.45000002    0.22580002    0.2041        0.2026    ]
 [ 735.14899233    0.27309999    0.1868        0.1547        0.1517    ]
 [1458.92363096    0.37280002    0.2086        0.16610001    0.17480001]
 ...
 [ 365.08110856    0.27159998    0.308         0.22479999    0.29179999]
 [-103.87365807    0.27700001    0.46010002    0.43249997    0.42659998]
 [ 975.50791537    0.31799999    0.17330001    0.18090001    0.1745    ]][0m
[37m[1m[2023-07-10 19:11:17,404][227910] Max Reward on eval: 1793.3328229899403[0m
[37m[1m[2023-07-10 19:11:17,404][227910] Min Reward on eval: -280.54088483855594[0m
[37m[1m[2023-07-10 19:11:17,404][227910] Mean Reward across all agents: 838.7760084163749[0m
[37m[1m[2023-07-10 19:11:17,404][227910] Average Trajectory Length: 983.924[0m
[36m[2023-07-10 19:11:17,406][227910] mean_value=-2254.817329570557, max_value=662.7167115106869[0m
[37m[1m[2023-07-10 19:11:17,408][227910] New mean coefficients: [[1.286191   0.21576768 0.7197977  0.8045941  1.6969042 ]][0m
[37m[1m[2023-07-10 19:11:17,409][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:11:27,304][227910] train() took 9.89 seconds to complete[0m
[36m[2023-07-10 19:11:27,304][227910] FPS: 388161.32[0m
[36m[2023-07-10 19:11:27,306][227910] itr=1137, itrs=2000, Progress: 56.85%[0m
[36m[2023-07-10 19:11:38,834][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 19:11:38,835][227910] FPS: 333660.45[0m
[36m[2023-07-10 19:11:43,566][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:11:43,566][227910] Reward + Measures: [[1419.10808905    0.35472679    0.20241626    0.17579544    0.17636153]][0m
[37m[1m[2023-07-10 19:11:43,567][227910] Max Reward on eval: 1419.1080890534154[0m
[37m[1m[2023-07-10 19:11:43,567][227910] Min Reward on eval: 1419.1080890534154[0m
[37m[1m[2023-07-10 19:11:43,567][227910] Mean Reward across all agents: 1419.1080890534154[0m
[37m[1m[2023-07-10 19:11:43,567][227910] Average Trajectory Length: 941.9816666666667[0m
[36m[2023-07-10 19:11:49,016][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:11:49,017][227910] Reward + Measures: [[ 404.0706941     0.32868785    0.24080332    0.20327297    0.1999127 ]
 [ 229.05537376    0.34220001    0.26520002    0.2146        0.2208    ]
 [ 247.50743339    0.28929999    0.2366        0.1901        0.1918    ]
 ...
 [-442.22328252    0.50230002    0.43780002    0.43730003    0.28150001]
 [-127.80196946    0.47839999    0.38229999    0.3459        0.28530002]
 [-265.71034512    0.50560004    0.43919998    0.40550002    0.30420002]][0m
[37m[1m[2023-07-10 19:11:49,017][227910] Max Reward on eval: 1559.1268976063438[0m
[37m[1m[2023-07-10 19:11:49,017][227910] Min Reward on eval: -521.4996116566981[0m
[37m[1m[2023-07-10 19:11:49,017][227910] Mean Reward across all agents: 309.648690485155[0m
[37m[1m[2023-07-10 19:11:49,018][227910] Average Trajectory Length: 930.9173333333333[0m
[36m[2023-07-10 19:11:49,019][227910] mean_value=-2940.477554428891, max_value=-110.66352212940672[0m
[36m[2023-07-10 19:11:49,021][227910] XNES is restarting with a new solution whose measures are [0.1882     0.41619998 0.27180001 0.4375    ] and objective is 479.13232848149494[0m
[36m[2023-07-10 19:11:49,022][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 19:11:49,024][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 19:11:49,025][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:11:58,722][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 19:11:58,723][227910] FPS: 396055.17[0m
[36m[2023-07-10 19:11:58,725][227910] itr=1138, itrs=2000, Progress: 56.90%[0m
[36m[2023-07-10 19:12:10,269][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 19:12:10,269][227910] FPS: 333261.98[0m
[36m[2023-07-10 19:12:15,098][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:12:15,098][227910] Reward + Measures: [[449.90352911   0.1964768    0.47483194   0.30047861   0.48968515]][0m
[37m[1m[2023-07-10 19:12:15,098][227910] Max Reward on eval: 449.9035291087454[0m
[37m[1m[2023-07-10 19:12:15,099][227910] Min Reward on eval: 449.9035291087454[0m
[37m[1m[2023-07-10 19:12:15,099][227910] Mean Reward across all agents: 449.9035291087454[0m
[37m[1m[2023-07-10 19:12:15,099][227910] Average Trajectory Length: 990.3616666666667[0m
[36m[2023-07-10 19:12:20,601][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:12:20,602][227910] Reward + Measures: [[  -92.73513105     0.40229997     0.49170002     0.27060002
      0.38100004]
 [  -78.37803491     0.26741925     0.29305989     0.20726719
      0.31913424]
 [-1158.7062917      0.08654794     0.51007336     0.39701504
      0.48791161]
 ...
 [  961.54716307     0.29899874     0.3697103      0.23570426
      0.26007572]
 [ -545.30835732     0.3184         0.2339         0.23249999
      0.2705    ]
 [ -682.17656284     0.57500517     0.63213331     0.54400516
      0.62006921]][0m
[37m[1m[2023-07-10 19:12:20,602][227910] Max Reward on eval: 961.5471630670829[0m
[37m[1m[2023-07-10 19:12:20,602][227910] Min Reward on eval: -2031.9861096369336[0m
[37m[1m[2023-07-10 19:12:20,602][227910] Mean Reward across all agents: -545.840170166828[0m
[37m[1m[2023-07-10 19:12:20,602][227910] Average Trajectory Length: 949.8686666666666[0m
[36m[2023-07-10 19:12:20,604][227910] mean_value=-1716.4060924924308, max_value=1034.106562135695[0m
[37m[1m[2023-07-10 19:12:20,607][227910] New mean coefficients: [[-0.04304725 -1.2384756  -0.08090341 -1.6173708  -1.1192999 ]][0m
[37m[1m[2023-07-10 19:12:20,608][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:12:30,459][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 19:12:30,459][227910] FPS: 389874.53[0m
[36m[2023-07-10 19:12:30,461][227910] itr=1139, itrs=2000, Progress: 56.95%[0m
[36m[2023-07-10 19:12:42,100][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 19:12:42,100][227910] FPS: 330564.38[0m
[36m[2023-07-10 19:12:46,902][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:12:46,907][227910] Reward + Measures: [[644.55187211   0.19107325   0.37018523   0.24770829   0.37953743]][0m
[37m[1m[2023-07-10 19:12:46,908][227910] Max Reward on eval: 644.5518721078976[0m
[37m[1m[2023-07-10 19:12:46,908][227910] Min Reward on eval: 644.5518721078976[0m
[37m[1m[2023-07-10 19:12:46,908][227910] Mean Reward across all agents: 644.5518721078976[0m
[37m[1m[2023-07-10 19:12:46,908][227910] Average Trajectory Length: 987.905[0m
[36m[2023-07-10 19:12:52,433][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:12:52,434][227910] Reward + Measures: [[-1441.08304984     0.15719999     0.10290001     0.1832
      0.1646    ]
 [ -381.31979442     0.22817855     0.30416736     0.24193755
      0.25739178]
 [ -323.5963581      0.59992784     0.15647765     0.51285487
      0.32531649]
 ...
 [ -508.98757543     0.32710001     0.28800002     0.37870002
      0.3653    ]
 [-1455.8795394      0.43068418     0.20174734     0.40648946
      0.34282634]
 [ -491.7426823      0.37895238     0.17672858     0.46369115
      0.28872994]][0m
[37m[1m[2023-07-10 19:12:52,434][227910] Max Reward on eval: 838.487008478152[0m
[37m[1m[2023-07-10 19:12:52,434][227910] Min Reward on eval: -1815.436526415433[0m
[37m[1m[2023-07-10 19:12:52,434][227910] Mean Reward across all agents: -537.2888157893611[0m
[37m[1m[2023-07-10 19:12:52,434][227910] Average Trajectory Length: 921.1116666666667[0m
[36m[2023-07-10 19:12:52,436][227910] mean_value=-2030.0634714599257, max_value=499.69800320740904[0m
[37m[1m[2023-07-10 19:12:52,439][227910] New mean coefficients: [[ 0.33380353 -1.0855069   0.41663754 -0.32228208 -0.7545948 ]][0m
[37m[1m[2023-07-10 19:12:52,439][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:13:02,322][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 19:13:02,323][227910] FPS: 388608.93[0m
[36m[2023-07-10 19:13:02,325][227910] itr=1140, itrs=2000, Progress: 57.00%[0m
[37m[1m[2023-07-10 19:13:06,394][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001120[0m
[36m[2023-07-10 19:13:18,281][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 19:13:18,281][227910] FPS: 330790.05[0m
[36m[2023-07-10 19:13:22,952][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:13:22,952][227910] Reward + Measures: [[798.26969176   0.2005229    0.31232348   0.22379331   0.316576  ]][0m
[37m[1m[2023-07-10 19:13:22,952][227910] Max Reward on eval: 798.2696917572187[0m
[37m[1m[2023-07-10 19:13:22,953][227910] Min Reward on eval: 798.2696917572187[0m
[37m[1m[2023-07-10 19:13:22,953][227910] Mean Reward across all agents: 798.2696917572187[0m
[37m[1m[2023-07-10 19:13:22,953][227910] Average Trajectory Length: 979.6553333333333[0m
[36m[2023-07-10 19:13:28,440][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:13:28,441][227910] Reward + Measures: [[ -333.49609953     0.3021         0.0913         0.28350002
      0.29650003]
 [-1023.33331205     0.41740003     0.39289999     0.37100002
      0.45199999]
 [ -274.33199881     0.29640001     0.60589999     0.30830002
      0.55980003]
 ...
 [  506.62222802     0.20350002     0.53180003     0.3321
      0.44860002]
 [ -244.18105611     0.14850001     0.1184         0.20780002
      0.16059999]
 [ 1206.85209473     0.26210001     0.29969999     0.33109999
      0.25289997]][0m
[37m[1m[2023-07-10 19:13:28,441][227910] Max Reward on eval: 1206.852094725659[0m
[37m[1m[2023-07-10 19:13:28,441][227910] Min Reward on eval: -1970.394348948123[0m
[37m[1m[2023-07-10 19:13:28,441][227910] Mean Reward across all agents: -278.1466607549108[0m
[37m[1m[2023-07-10 19:13:28,442][227910] Average Trajectory Length: 977.3779999999999[0m
[36m[2023-07-10 19:13:28,443][227910] mean_value=-1297.9793638098065, max_value=485.38088102702966[0m
[37m[1m[2023-07-10 19:13:28,446][227910] New mean coefficients: [[ 0.26276997  0.3828174   0.37645435 -0.64599687 -0.38191664]][0m
[37m[1m[2023-07-10 19:13:28,447][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:13:38,281][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 19:13:38,282][227910] FPS: 390517.92[0m
[36m[2023-07-10 19:13:38,284][227910] itr=1141, itrs=2000, Progress: 57.05%[0m
[36m[2023-07-10 19:13:49,770][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 19:13:49,770][227910] FPS: 334865.88[0m
[36m[2023-07-10 19:13:54,571][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:13:54,571][227910] Reward + Measures: [[901.19562328   0.2032806    0.26459897   0.20319852   0.26589534]][0m
[37m[1m[2023-07-10 19:13:54,572][227910] Max Reward on eval: 901.1956232818867[0m
[37m[1m[2023-07-10 19:13:54,572][227910] Min Reward on eval: 901.1956232818867[0m
[37m[1m[2023-07-10 19:13:54,572][227910] Mean Reward across all agents: 901.1956232818867[0m
[37m[1m[2023-07-10 19:13:54,572][227910] Average Trajectory Length: 975.8233333333333[0m
[36m[2023-07-10 19:14:00,003][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:14:00,004][227910] Reward + Measures: [[-656.74761127    0.14844857    0.17512126    0.13384913    0.12631741]
 [ 193.55881562    0.43113351    0.45473656    0.28560916    0.40430579]
 [-247.68120065    0.33797908    0.32418838    0.26794288    0.249897  ]
 ...
 [  92.93469988    0.10960001    0.63230002    0.43889999    0.68269998]
 [-613.17413356    0.10570001    0.78950006    0.60100001    0.86119998]
 [-933.24801606    0.44890004    0.46850005    0.45160004    0.44210002]][0m
[37m[1m[2023-07-10 19:14:00,004][227910] Max Reward on eval: 1081.9041921026771[0m
[37m[1m[2023-07-10 19:14:00,004][227910] Min Reward on eval: -1642.6574116010568[0m
[37m[1m[2023-07-10 19:14:00,005][227910] Mean Reward across all agents: -151.16494007078393[0m
[37m[1m[2023-07-10 19:14:00,005][227910] Average Trajectory Length: 913.9046666666667[0m
[36m[2023-07-10 19:14:00,007][227910] mean_value=-1993.2619814482098, max_value=460.07550941846046[0m
[37m[1m[2023-07-10 19:14:00,009][227910] New mean coefficients: [[ 0.5490371   0.11503994 -1.5459212  -0.3135636  -0.15586695]][0m
[37m[1m[2023-07-10 19:14:00,010][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:14:09,655][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 19:14:09,655][227910] FPS: 398211.58[0m
[36m[2023-07-10 19:14:09,657][227910] itr=1142, itrs=2000, Progress: 57.10%[0m
[36m[2023-07-10 19:14:21,325][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 19:14:21,325][227910] FPS: 329624.93[0m
[36m[2023-07-10 19:14:26,183][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:14:26,184][227910] Reward + Measures: [[1086.880853      0.21710126    0.21398644    0.18961285    0.21736082]][0m
[37m[1m[2023-07-10 19:14:26,184][227910] Max Reward on eval: 1086.8808530012814[0m
[37m[1m[2023-07-10 19:14:26,184][227910] Min Reward on eval: 1086.8808530012814[0m
[37m[1m[2023-07-10 19:14:26,184][227910] Mean Reward across all agents: 1086.8808530012814[0m
[37m[1m[2023-07-10 19:14:26,184][227910] Average Trajectory Length: 969.198[0m
[36m[2023-07-10 19:14:31,836][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:14:31,837][227910] Reward + Measures: [[  216.71566694     0.24330001     0.2168         0.3062
      0.26809999]
 [ -295.77176717     0.26300001     0.21440001     0.27130002
      0.27659997]
 [  898.15153287     0.4508         0.42300001     0.2218
      0.36100003]
 ...
 [  492.68244806     0.2199         0.31510001     0.27589998
      0.3741    ]
 [-1161.41127756     0.2184         0.5309         0.3321
      0.5557    ]
 [ -754.08488889     0.33090001     0.60690004     0.21280001
      0.61920005]][0m
[37m[1m[2023-07-10 19:14:31,837][227910] Max Reward on eval: 1428.3491580306727[0m
[37m[1m[2023-07-10 19:14:31,837][227910] Min Reward on eval: -1813.9931061139564[0m
[37m[1m[2023-07-10 19:14:31,838][227910] Mean Reward across all agents: -114.41586581664693[0m
[37m[1m[2023-07-10 19:14:31,838][227910] Average Trajectory Length: 970.3203333333333[0m
[36m[2023-07-10 19:14:31,839][227910] mean_value=-1533.842228799006, max_value=727.0286757849598[0m
[37m[1m[2023-07-10 19:14:31,842][227910] New mean coefficients: [[ 0.2666025  -1.032369   -1.5845976  -0.7689042  -0.06613304]][0m
[37m[1m[2023-07-10 19:14:31,843][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:14:41,657][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 19:14:41,657][227910] FPS: 391344.21[0m
[36m[2023-07-10 19:14:41,659][227910] itr=1143, itrs=2000, Progress: 57.15%[0m
[36m[2023-07-10 19:14:53,304][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 19:14:53,304][227910] FPS: 330318.00[0m
[36m[2023-07-10 19:14:58,047][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:14:58,047][227910] Reward + Measures: [[1119.91931118    0.21288089    0.20365979    0.18531576    0.20662627]][0m
[37m[1m[2023-07-10 19:14:58,047][227910] Max Reward on eval: 1119.919311184933[0m
[37m[1m[2023-07-10 19:14:58,048][227910] Min Reward on eval: 1119.919311184933[0m
[37m[1m[2023-07-10 19:14:58,048][227910] Mean Reward across all agents: 1119.919311184933[0m
[37m[1m[2023-07-10 19:14:58,048][227910] Average Trajectory Length: 964.2393333333333[0m
[36m[2023-07-10 19:15:03,492][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:15:03,493][227910] Reward + Measures: [[ -590.66139567     0.7245         0.10110001     0.67820007
      0.61360002]
 [ -421.22886062     0.38183045     0.38406524     0.18551305
      0.36520872]
 [ -899.09888824     0.373005       0.23701198     0.4300262
      0.23486213]
 ...
 [ 1031.53464649     0.29636636     0.27303353     0.31472623
      0.19749105]
 [-1473.09086481     0.31744686     0.171259       0.29300404
      0.3405419 ]
 [  335.6418556      0.25459522     0.26126876     0.35666773
      0.24914455]][0m
[37m[1m[2023-07-10 19:15:03,493][227910] Max Reward on eval: 1652.4462934918934[0m
[37m[1m[2023-07-10 19:15:03,493][227910] Min Reward on eval: -1909.0111382738455[0m
[37m[1m[2023-07-10 19:15:03,494][227910] Mean Reward across all agents: -267.3495861251218[0m
[37m[1m[2023-07-10 19:15:03,494][227910] Average Trajectory Length: 940.848[0m
[36m[2023-07-10 19:15:03,496][227910] mean_value=-2166.863374398541, max_value=338.8659289178638[0m
[37m[1m[2023-07-10 19:15:03,498][227910] New mean coefficients: [[ 0.51099414 -1.7653583  -1.4829922  -0.08404744 -0.45946002]][0m
[37m[1m[2023-07-10 19:15:03,499][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:15:13,350][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 19:15:13,350][227910] FPS: 389878.35[0m
[36m[2023-07-10 19:15:13,352][227910] itr=1144, itrs=2000, Progress: 57.20%[0m
[36m[2023-07-10 19:15:25,026][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 19:15:25,027][227910] FPS: 329453.95[0m
[36m[2023-07-10 19:15:29,846][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:15:29,847][227910] Reward + Measures: [[1207.08906126    0.21193257    0.19372082    0.18087791    0.19109768]][0m
[37m[1m[2023-07-10 19:15:29,847][227910] Max Reward on eval: 1207.0890612646754[0m
[37m[1m[2023-07-10 19:15:29,847][227910] Min Reward on eval: 1207.0890612646754[0m
[37m[1m[2023-07-10 19:15:29,847][227910] Mean Reward across all agents: 1207.0890612646754[0m
[37m[1m[2023-07-10 19:15:29,848][227910] Average Trajectory Length: 955.905[0m
[36m[2023-07-10 19:15:35,389][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:15:35,390][227910] Reward + Measures: [[ 263.90060393    0.38139999    0.35830003    0.4278        0.21700001]
 [-163.79409791    0.57010001    0.43399999    0.59470004    0.204     ]
 [ 459.15856345    0.42659998    0.36649999    0.42420003    0.3371    ]
 ...
 [ 560.91974009    0.20560001    0.54439998    0.3633        0.53200001]
 [-818.72401447    0.7675001     0.56470007    0.84209996    0.13399999]
 [ 525.2106835     0.13030002    0.64530003    0.30180001    0.60799998]][0m
[37m[1m[2023-07-10 19:15:35,390][227910] Max Reward on eval: 1823.1029746744985[0m
[37m[1m[2023-07-10 19:15:35,390][227910] Min Reward on eval: -1818.074034233729[0m
[37m[1m[2023-07-10 19:15:35,390][227910] Mean Reward across all agents: 82.72982821280704[0m
[37m[1m[2023-07-10 19:15:35,391][227910] Average Trajectory Length: 971.3123333333333[0m
[36m[2023-07-10 19:15:35,392][227910] mean_value=-1286.4400529823188, max_value=343.43877628296525[0m
[37m[1m[2023-07-10 19:15:35,395][227910] New mean coefficients: [[ 0.43335307 -1.8746369  -1.3840398   0.43775493  0.43962228]][0m
[37m[1m[2023-07-10 19:15:35,396][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:15:45,201][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 19:15:45,202][227910] FPS: 391672.52[0m
[36m[2023-07-10 19:15:45,204][227910] itr=1145, itrs=2000, Progress: 57.25%[0m
[36m[2023-07-10 19:15:56,805][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 19:15:56,805][227910] FPS: 331540.65[0m
[36m[2023-07-10 19:16:01,633][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:16:01,633][227910] Reward + Measures: [[1291.68694226    0.21551198    0.19280164    0.18074659    0.18562441]][0m
[37m[1m[2023-07-10 19:16:01,633][227910] Max Reward on eval: 1291.6869422623[0m
[37m[1m[2023-07-10 19:16:01,634][227910] Min Reward on eval: 1291.6869422623[0m
[37m[1m[2023-07-10 19:16:01,634][227910] Mean Reward across all agents: 1291.6869422623[0m
[37m[1m[2023-07-10 19:16:01,634][227910] Average Trajectory Length: 942.7856666666667[0m
[36m[2023-07-10 19:16:07,107][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:16:07,108][227910] Reward + Measures: [[ -862.1155264      0.4303         0.33930001     0.1322
      0.53330004]
 [ -963.64126649     0.38119999     0.6663         0.1081
      0.66399997]
 [-1070.07567316     0.51280004     0.37150002     0.20320001
      0.49449998]
 ...
 [-1599.45242484     0.2809         0.40439996     0.0784
      0.47659999]
 [ -178.93197507     0.40477416     0.32404017     0.35063154
      0.21433249]
 [ -700.2768537      0.38340002     0.47840005     0.1175
      0.55600005]][0m
[37m[1m[2023-07-10 19:16:07,108][227910] Max Reward on eval: 1431.4000875801605[0m
[37m[1m[2023-07-10 19:16:07,108][227910] Min Reward on eval: -2034.8527871449012[0m
[37m[1m[2023-07-10 19:16:07,108][227910] Mean Reward across all agents: -474.666678001039[0m
[37m[1m[2023-07-10 19:16:07,109][227910] Average Trajectory Length: 982.8249999999999[0m
[36m[2023-07-10 19:16:07,111][227910] mean_value=-1262.2701967239282, max_value=715.7421715553901[0m
[37m[1m[2023-07-10 19:16:07,113][227910] New mean coefficients: [[ 0.43823934 -1.858521   -1.218601   -0.20379698  0.8927252 ]][0m
[37m[1m[2023-07-10 19:16:07,114][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:16:16,972][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 19:16:16,972][227910] FPS: 389620.35[0m
[36m[2023-07-10 19:16:16,974][227910] itr=1146, itrs=2000, Progress: 57.30%[0m
[36m[2023-07-10 19:16:28,487][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 19:16:28,487][227910] FPS: 334088.89[0m
[36m[2023-07-10 19:16:33,184][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:16:33,185][227910] Reward + Measures: [[1375.95309912    0.22158675    0.19721094    0.18021493    0.18544404]][0m
[37m[1m[2023-07-10 19:16:33,185][227910] Max Reward on eval: 1375.9530991198385[0m
[37m[1m[2023-07-10 19:16:33,185][227910] Min Reward on eval: 1375.9530991198385[0m
[37m[1m[2023-07-10 19:16:33,185][227910] Mean Reward across all agents: 1375.9530991198385[0m
[37m[1m[2023-07-10 19:16:33,186][227910] Average Trajectory Length: 932.6986666666667[0m
[36m[2023-07-10 19:16:38,747][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:16:38,747][227910] Reward + Measures: [[-475.33781233    0.37012976    0.29125786    0.2481405     0.36048844]
 [ -14.89369579    0.46270105    0.32424012    0.36265364    0.38154054]
 [-199.25678016    0.40529999    0.23          0.4474        0.27110001]
 ...
 [1381.30863111    0.29969999    0.24590002    0.2462        0.24199998]
 [-914.65917587    0.54519999    0.32390001    0.37909999    0.38610002]
 [  10.53463504    0.52570003    0.31530002    0.51590008    0.30969998]][0m
[37m[1m[2023-07-10 19:16:38,747][227910] Max Reward on eval: 1896.260333363025[0m
[37m[1m[2023-07-10 19:16:38,748][227910] Min Reward on eval: -2423.679901690665[0m
[37m[1m[2023-07-10 19:16:38,748][227910] Mean Reward across all agents: -281.69713346230014[0m
[37m[1m[2023-07-10 19:16:38,748][227910] Average Trajectory Length: 957.0833333333333[0m
[36m[2023-07-10 19:16:38,750][227910] mean_value=-1778.6791691241476, max_value=84.64320391295144[0m
[37m[1m[2023-07-10 19:16:38,752][227910] New mean coefficients: [[-0.3180495  -1.1801741  -1.2250932   0.07525507  0.6223655 ]][0m
[37m[1m[2023-07-10 19:16:38,753][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:16:48,445][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 19:16:48,445][227910] FPS: 396296.17[0m
[36m[2023-07-10 19:16:48,447][227910] itr=1147, itrs=2000, Progress: 57.35%[0m
[36m[2023-07-10 19:17:00,015][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 19:17:00,016][227910] FPS: 332465.15[0m
[36m[2023-07-10 19:17:04,767][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:17:04,768][227910] Reward + Measures: [[1347.70547116    0.21243876    0.19064197    0.18004596    0.18505599]][0m
[37m[1m[2023-07-10 19:17:04,768][227910] Max Reward on eval: 1347.7054711626383[0m
[37m[1m[2023-07-10 19:17:04,768][227910] Min Reward on eval: 1347.7054711626383[0m
[37m[1m[2023-07-10 19:17:04,768][227910] Mean Reward across all agents: 1347.7054711626383[0m
[37m[1m[2023-07-10 19:17:04,769][227910] Average Trajectory Length: 949.8356666666666[0m
[36m[2023-07-10 19:17:10,454][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:17:10,455][227910] Reward + Measures: [[ 1282.5096169      0.33549997     0.25840002     0.22289999
      0.1674    ]
 [  901.94170394     0.39151335     0.3108615      0.21135774
      0.21525775]
 [  894.35173198     0.46754929     0.235779       0.22440863
      0.23590373]
 ...
 [   56.99558983     0.2588         0.43430001     0.3312
      0.42360002]
 [-1358.98471526     0.1875         0.1865         0.1566
      0.0871    ]
 [-1130.4438938      0.37033674     0.27182221     0.36353585
      0.3460319 ]][0m
[37m[1m[2023-07-10 19:17:10,455][227910] Max Reward on eval: 1991.104010682949[0m
[37m[1m[2023-07-10 19:17:10,455][227910] Min Reward on eval: -1380.502316634031[0m
[37m[1m[2023-07-10 19:17:10,455][227910] Mean Reward across all agents: 175.85200188677686[0m
[37m[1m[2023-07-10 19:17:10,456][227910] Average Trajectory Length: 952.8853333333333[0m
[36m[2023-07-10 19:17:10,457][227910] mean_value=-2395.7307207840804, max_value=528.8682829007212[0m
[37m[1m[2023-07-10 19:17:10,460][227910] New mean coefficients: [[-0.8732573  -1.4525702  -0.21758938  0.55539024  1.1236963 ]][0m
[37m[1m[2023-07-10 19:17:10,461][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:17:20,404][227910] train() took 9.94 seconds to complete[0m
[36m[2023-07-10 19:17:20,404][227910] FPS: 386270.92[0m
[36m[2023-07-10 19:17:20,406][227910] itr=1148, itrs=2000, Progress: 57.40%[0m
[36m[2023-07-10 19:17:32,060][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 19:17:32,061][227910] FPS: 330013.30[0m
[36m[2023-07-10 19:17:36,768][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:17:36,768][227910] Reward + Measures: [[1181.86618724    0.19750243    0.17932947    0.17749271    0.18313785]][0m
[37m[1m[2023-07-10 19:17:36,768][227910] Max Reward on eval: 1181.8661872374319[0m
[37m[1m[2023-07-10 19:17:36,769][227910] Min Reward on eval: 1181.8661872374319[0m
[37m[1m[2023-07-10 19:17:36,769][227910] Mean Reward across all agents: 1181.8661872374319[0m
[37m[1m[2023-07-10 19:17:36,769][227910] Average Trajectory Length: 954.62[0m
[36m[2023-07-10 19:17:42,159][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:17:42,159][227910] Reward + Measures: [[1307.11982207    0.26469132    0.26781315    0.22522433    0.28048706]
 [ 940.43505508    0.18869869    0.19211578    0.18036316    0.199975  ]
 [ 972.06271658    0.20706022    0.14805484    0.1723312     0.1875086 ]
 ...
 [1589.06675709    0.24440001    0.29530001    0.22809999    0.25440001]
 [1428.24430244    0.2045        0.23570001    0.20279999    0.20999999]
 [1635.09083075    0.31210002    0.25789997    0.21520002    0.2431    ]][0m
[37m[1m[2023-07-10 19:17:42,160][227910] Max Reward on eval: 1879.359071815238[0m
[37m[1m[2023-07-10 19:17:42,160][227910] Min Reward on eval: 414.39679556716584[0m
[37m[1m[2023-07-10 19:17:42,160][227910] Mean Reward across all agents: 1151.828046886753[0m
[37m[1m[2023-07-10 19:17:42,160][227910] Average Trajectory Length: 972.7353333333333[0m
[36m[2023-07-10 19:17:42,162][227910] mean_value=-1105.767593867337, max_value=1032.634617683441[0m
[37m[1m[2023-07-10 19:17:42,165][227910] New mean coefficients: [[ 0.08309931 -1.2878183   0.54867715  1.3866429   2.713615  ]][0m
[37m[1m[2023-07-10 19:17:42,166][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:17:51,767][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 19:17:51,768][227910] FPS: 400022.82[0m
[36m[2023-07-10 19:17:51,770][227910] itr=1149, itrs=2000, Progress: 57.45%[0m
[36m[2023-07-10 19:18:03,417][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 19:18:03,417][227910] FPS: 330315.36[0m
[36m[2023-07-10 19:18:08,160][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:18:08,160][227910] Reward + Measures: [[1061.67693023    0.18027686    0.1682817     0.17402643    0.17908385]][0m
[37m[1m[2023-07-10 19:18:08,161][227910] Max Reward on eval: 1061.6769302299488[0m
[37m[1m[2023-07-10 19:18:08,161][227910] Min Reward on eval: 1061.6769302299488[0m
[37m[1m[2023-07-10 19:18:08,161][227910] Mean Reward across all agents: 1061.6769302299488[0m
[37m[1m[2023-07-10 19:18:08,161][227910] Average Trajectory Length: 959.1806666666666[0m
[36m[2023-07-10 19:18:13,550][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:18:13,556][227910] Reward + Measures: [[ 841.95087262    0.22707815    0.19017366    0.18082784    0.20009618]
 [-736.83777706    0.07540001    0.74270004    0.73250002    0.80720007]
 [-699.04005137    0.61440003    0.4025        0.58630002    0.57700002]
 ...
 [ -97.45254045    0.40840003    0.1715        0.49589998    0.17549999]
 [ 109.24667891    0.38909999    0.54500002    0.31470001    0.66029996]
 [ 454.09453525    0.27919999    0.47080001    0.41710001    0.49219999]][0m
[37m[1m[2023-07-10 19:18:13,556][227910] Max Reward on eval: 1266.5686024490744[0m
[37m[1m[2023-07-10 19:18:13,557][227910] Min Reward on eval: -1939.1967863644472[0m
[37m[1m[2023-07-10 19:18:13,557][227910] Mean Reward across all agents: -45.791312221062505[0m
[37m[1m[2023-07-10 19:18:13,557][227910] Average Trajectory Length: 982.6263333333333[0m
[36m[2023-07-10 19:18:13,559][227910] mean_value=-916.2033487123831, max_value=487.39558622095797[0m
[37m[1m[2023-07-10 19:18:13,562][227910] New mean coefficients: [[ 0.09110664 -1.3260112   1.0361774   1.7543952   4.605566  ]][0m
[37m[1m[2023-07-10 19:18:13,563][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:18:23,367][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 19:18:23,367][227910] FPS: 391741.78[0m
[36m[2023-07-10 19:18:23,370][227910] itr=1150, itrs=2000, Progress: 57.50%[0m
[37m[1m[2023-07-10 19:18:27,285][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001130[0m
[36m[2023-07-10 19:18:39,188][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 19:18:39,188][227910] FPS: 330189.18[0m
[36m[2023-07-10 19:18:43,980][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:18:43,980][227910] Reward + Measures: [[1069.46214755    0.18098478    0.17050187    0.17840327    0.19324392]][0m
[37m[1m[2023-07-10 19:18:43,981][227910] Max Reward on eval: 1069.462147550722[0m
[37m[1m[2023-07-10 19:18:43,981][227910] Min Reward on eval: 1069.462147550722[0m
[37m[1m[2023-07-10 19:18:43,981][227910] Mean Reward across all agents: 1069.462147550722[0m
[37m[1m[2023-07-10 19:18:43,981][227910] Average Trajectory Length: 970.9593333333333[0m
[36m[2023-07-10 19:18:49,403][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:18:49,404][227910] Reward + Measures: [[-668.36757535    0.3935        0.44520003    0.34279999    0.46720001]
 [ 727.51382959    0.38980001    0.3337        0.40450001    0.35620004]
 [-519.40634733    0.3441        0.36590001    0.3001        0.35619998]
 ...
 [  16.55660439    0.29720002    0.38330004    0.35950002    0.41230002]
 [-271.93128186    0.4172        0.38859996    0.39089999    0.3572    ]
 [-322.77466205    0.46581134    0.30175978    0.42029896    0.31053504]][0m
[37m[1m[2023-07-10 19:18:49,404][227910] Max Reward on eval: 1506.3361505063717[0m
[37m[1m[2023-07-10 19:18:49,404][227910] Min Reward on eval: -1393.4958875158568[0m
[37m[1m[2023-07-10 19:18:49,404][227910] Mean Reward across all agents: -97.86124927671125[0m
[37m[1m[2023-07-10 19:18:49,405][227910] Average Trajectory Length: 987.131[0m
[36m[2023-07-10 19:18:49,406][227910] mean_value=-1996.1522995255282, max_value=60.58314593106826[0m
[37m[1m[2023-07-10 19:18:49,408][227910] New mean coefficients: [[ 0.38150775 -0.35421222  1.1086882   1.6662934   2.5271573 ]][0m
[37m[1m[2023-07-10 19:18:49,409][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:18:59,099][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 19:18:59,099][227910] FPS: 396356.72[0m
[36m[2023-07-10 19:18:59,101][227910] itr=1151, itrs=2000, Progress: 57.55%[0m
[36m[2023-07-10 19:19:10,710][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 19:19:10,710][227910] FPS: 331309.60[0m
[36m[2023-07-10 19:19:15,544][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:19:15,544][227910] Reward + Measures: [[1155.15943611    0.19510777    0.19050021    0.19044989    0.21712039]][0m
[37m[1m[2023-07-10 19:19:15,544][227910] Max Reward on eval: 1155.1594361090483[0m
[37m[1m[2023-07-10 19:19:15,544][227910] Min Reward on eval: 1155.1594361090483[0m
[37m[1m[2023-07-10 19:19:15,545][227910] Mean Reward across all agents: 1155.1594361090483[0m
[37m[1m[2023-07-10 19:19:15,545][227910] Average Trajectory Length: 970.0463333333333[0m
[36m[2023-07-10 19:19:21,058][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:19:21,058][227910] Reward + Measures: [[311.93408756   0.12594311   0.54638588   0.45748875   0.58793187]
 [134.39461608   0.11398061   0.60323179   0.53458142   0.66398299]
 [-65.44300242   0.1148       0.5302       0.50550002   0.5535    ]
 ...
 [187.32666443   0.16604078   0.30855352   0.24336243   0.33312103]
 [826.79444404   0.19915624   0.28716519   0.26227066   0.26395506]
 [357.04794281   0.1239       0.42150003   0.4172       0.44039997]][0m
[37m[1m[2023-07-10 19:19:21,059][227910] Max Reward on eval: 1859.8771301276254[0m
[37m[1m[2023-07-10 19:19:21,059][227910] Min Reward on eval: -969.0904354747618[0m
[37m[1m[2023-07-10 19:19:21,059][227910] Mean Reward across all agents: 428.3025965342083[0m
[37m[1m[2023-07-10 19:19:21,059][227910] Average Trajectory Length: 933.7883333333333[0m
[36m[2023-07-10 19:19:21,063][227910] mean_value=-1115.234310581511, max_value=1283.5970951908766[0m
[37m[1m[2023-07-10 19:19:21,066][227910] New mean coefficients: [[ 0.0799042 -1.2969307  1.3626113  1.1186363  2.6273255]][0m
[37m[1m[2023-07-10 19:19:21,067][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:19:30,807][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 19:19:30,807][227910] FPS: 394317.12[0m
[36m[2023-07-10 19:19:30,809][227910] itr=1152, itrs=2000, Progress: 57.60%[0m
[36m[2023-07-10 19:19:42,304][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 19:19:42,305][227910] FPS: 334569.58[0m
[36m[2023-07-10 19:19:47,080][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:19:47,081][227910] Reward + Measures: [[1203.89759279    0.20666978    0.21994635    0.20446271    0.24916206]][0m
[37m[1m[2023-07-10 19:19:47,081][227910] Max Reward on eval: 1203.8975927900851[0m
[37m[1m[2023-07-10 19:19:47,081][227910] Min Reward on eval: 1203.8975927900851[0m
[37m[1m[2023-07-10 19:19:47,081][227910] Mean Reward across all agents: 1203.8975927900851[0m
[37m[1m[2023-07-10 19:19:47,082][227910] Average Trajectory Length: 976.0226666666666[0m
[36m[2023-07-10 19:19:52,550][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:19:52,551][227910] Reward + Measures: [[ 705.70210423    0.32360002    0.52320004    0.24080001    0.34480003]
 [ 494.20559009    0.1855        0.1513        0.18620001    0.1646    ]
 [ 752.92360119    0.32140002    0.33629999    0.3132        0.2296    ]
 ...
 [ 788.15907486    0.34810001    0.28029999    0.31619999    0.2146    ]
 [ 181.02643407    0.39089999    0.2095        0.3637        0.33269998]
 [-240.56110036    0.30690002    0.52990001    0.3082        0.53730005]][0m
[37m[1m[2023-07-10 19:19:52,551][227910] Max Reward on eval: 1598.7005805603171[0m
[37m[1m[2023-07-10 19:19:52,551][227910] Min Reward on eval: -963.443971686007[0m
[37m[1m[2023-07-10 19:19:52,551][227910] Mean Reward across all agents: 350.63380944451757[0m
[37m[1m[2023-07-10 19:19:52,551][227910] Average Trajectory Length: 990.4309999999999[0m
[36m[2023-07-10 19:19:52,553][227910] mean_value=-2168.4358273026764, max_value=750.4685991222266[0m
[37m[1m[2023-07-10 19:19:52,556][227910] New mean coefficients: [[ 0.64303505 -0.33117646  1.6538281   0.39859104  3.2638073 ]][0m
[37m[1m[2023-07-10 19:19:52,557][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:20:02,366][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 19:20:02,366][227910] FPS: 391518.33[0m
[36m[2023-07-10 19:20:02,369][227910] itr=1153, itrs=2000, Progress: 57.65%[0m
[36m[2023-07-10 19:20:14,097][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 19:20:14,097][227910] FPS: 328031.16[0m
[36m[2023-07-10 19:20:18,929][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:20:18,929][227910] Reward + Measures: [[1055.7441324     0.20255487    0.3235656     0.23919925    0.34738442]][0m
[37m[1m[2023-07-10 19:20:18,929][227910] Max Reward on eval: 1055.7441324008082[0m
[37m[1m[2023-07-10 19:20:18,929][227910] Min Reward on eval: 1055.7441324008082[0m
[37m[1m[2023-07-10 19:20:18,930][227910] Mean Reward across all agents: 1055.7441324008082[0m
[37m[1m[2023-07-10 19:20:18,930][227910] Average Trajectory Length: 975.447[0m
[36m[2023-07-10 19:20:24,421][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:20:24,421][227910] Reward + Measures: [[ 315.53510494    0.35505635    0.37945378    0.2030106     0.43830997]
 [ 225.03431252    0.20990001    0.66719997    0.31920001    0.5693    ]
 [-563.4347613     0.65669996    0.82129997    0.186         0.7044    ]
 ...
 [ 910.14200442    0.3707        0.37219998    0.27710003    0.39659998]
 [ 956.9291927     0.29000002    0.27490002    0.25830004    0.3136    ]
 [ 518.1686205     0.45671177    0.44761562    0.20721507    0.54096425]][0m
[37m[1m[2023-07-10 19:20:24,422][227910] Max Reward on eval: 1638.8223127444594[0m
[37m[1m[2023-07-10 19:20:24,422][227910] Min Reward on eval: -1127.1176901256665[0m
[37m[1m[2023-07-10 19:20:24,422][227910] Mean Reward across all agents: 313.77299616058616[0m
[37m[1m[2023-07-10 19:20:24,422][227910] Average Trajectory Length: 989.5236666666666[0m
[36m[2023-07-10 19:20:24,425][227910] mean_value=-440.93323903724047, max_value=1098.1723947188386[0m
[37m[1m[2023-07-10 19:20:24,428][227910] New mean coefficients: [[ 0.83797103 -0.78553474  1.6021051   0.2870164   2.9265394 ]][0m
[37m[1m[2023-07-10 19:20:24,429][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:20:34,149][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 19:20:34,149][227910] FPS: 395116.42[0m
[36m[2023-07-10 19:20:34,152][227910] itr=1154, itrs=2000, Progress: 57.70%[0m
[36m[2023-07-10 19:20:45,638][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 19:20:45,638][227910] FPS: 334841.90[0m
[36m[2023-07-10 19:20:50,494][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:20:50,494][227910] Reward + Measures: [[722.26675096   0.17250386   0.49611768   0.29702216   0.50817233]][0m
[37m[1m[2023-07-10 19:20:50,494][227910] Max Reward on eval: 722.2667509560055[0m
[37m[1m[2023-07-10 19:20:50,494][227910] Min Reward on eval: 722.2667509560055[0m
[37m[1m[2023-07-10 19:20:50,495][227910] Mean Reward across all agents: 722.2667509560055[0m
[37m[1m[2023-07-10 19:20:50,495][227910] Average Trajectory Length: 987.3013333333333[0m
[36m[2023-07-10 19:20:55,913][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:20:55,913][227910] Reward + Measures: [[  -70.36442197     0.30319998     0.51140004     0.29709998
      0.49940005]
 [   94.56572581     0.27670002     0.32180002     0.30690002
      0.31090003]
 [   91.04541532     0.1099         0.58350003     0.289
      0.45500001]
 ...
 [ -502.39938271     0.0382         0.79070002     0.5402
      0.89740002]
 [-1300.51976967     0.0271         0.74540007     0.48009998
      0.86110002]
 [-1201.31214024     0.0376         0.70180005     0.45089999
      0.80660003]][0m
[37m[1m[2023-07-10 19:20:55,914][227910] Max Reward on eval: 1081.4472082067746[0m
[37m[1m[2023-07-10 19:20:55,914][227910] Min Reward on eval: -1611.3885962140746[0m
[37m[1m[2023-07-10 19:20:55,914][227910] Mean Reward across all agents: -304.43220084116143[0m
[37m[1m[2023-07-10 19:20:55,914][227910] Average Trajectory Length: 988.519[0m
[36m[2023-07-10 19:20:55,917][227910] mean_value=-865.4460618049267, max_value=578.7761858762428[0m
[37m[1m[2023-07-10 19:20:55,919][227910] New mean coefficients: [[ 1.0029212  -0.13600433  2.380277    0.06932873  4.0910945 ]][0m
[37m[1m[2023-07-10 19:20:55,920][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:21:05,610][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 19:21:05,610][227910] FPS: 396368.65[0m
[36m[2023-07-10 19:21:05,612][227910] itr=1155, itrs=2000, Progress: 57.75%[0m
[36m[2023-07-10 19:21:17,106][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 19:21:17,106][227910] FPS: 334615.94[0m
[36m[2023-07-10 19:21:21,850][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:21:21,851][227910] Reward + Measures: [[590.0098409    0.16525328   0.57934242   0.34581298   0.59286702]][0m
[37m[1m[2023-07-10 19:21:21,851][227910] Max Reward on eval: 590.0098409005121[0m
[37m[1m[2023-07-10 19:21:21,851][227910] Min Reward on eval: 590.0098409005121[0m
[37m[1m[2023-07-10 19:21:21,852][227910] Mean Reward across all agents: 590.0098409005121[0m
[37m[1m[2023-07-10 19:21:21,852][227910] Average Trajectory Length: 993.0153333333333[0m
[36m[2023-07-10 19:21:27,299][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:21:27,299][227910] Reward + Measures: [[ 703.15520402    0.24980001    0.4382        0.30560002    0.421     ]
 [-160.23408404    0.39309999    0.62849998    0.32950002    0.66680002]
 [  46.36896838    0.42790005    0.61949998    0.3637        0.64219999]
 ...
 [-134.81377322    0.30680001    0.087         0.2811        0.2617    ]
 [ -93.70245486    0.2904        0.72940004    0.273         0.73140001]
 [ 218.28324361    0.21209998    0.64480001    0.53829998    0.71619999]][0m
[37m[1m[2023-07-10 19:21:27,300][227910] Max Reward on eval: 1694.77541982696[0m
[37m[1m[2023-07-10 19:21:27,300][227910] Min Reward on eval: -1430.7459829679167[0m
[37m[1m[2023-07-10 19:21:27,300][227910] Mean Reward across all agents: 230.4996432669514[0m
[37m[1m[2023-07-10 19:21:27,300][227910] Average Trajectory Length: 995.1949999999999[0m
[36m[2023-07-10 19:21:27,304][227910] mean_value=-488.00841766471956, max_value=722.5732058914227[0m
[37m[1m[2023-07-10 19:21:27,306][227910] New mean coefficients: [[ 1.819711   -0.49045342  2.091727    0.32193035  3.5610375 ]][0m
[37m[1m[2023-07-10 19:21:27,307][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:21:36,996][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 19:21:36,996][227910] FPS: 396407.07[0m
[36m[2023-07-10 19:21:36,998][227910] itr=1156, itrs=2000, Progress: 57.80%[0m
[36m[2023-07-10 19:21:48,501][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 19:21:48,501][227910] FPS: 334387.88[0m
[36m[2023-07-10 19:21:53,318][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:21:53,318][227910] Reward + Measures: [[644.87737411   0.15751626   0.62504649   0.38683367   0.65083778]][0m
[37m[1m[2023-07-10 19:21:53,318][227910] Max Reward on eval: 644.8773741129611[0m
[37m[1m[2023-07-10 19:21:53,318][227910] Min Reward on eval: 644.8773741129611[0m
[37m[1m[2023-07-10 19:21:53,319][227910] Mean Reward across all agents: 644.8773741129611[0m
[37m[1m[2023-07-10 19:21:53,319][227910] Average Trajectory Length: 991.05[0m
[36m[2023-07-10 19:21:58,824][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:21:58,825][227910] Reward + Measures: [[ 353.84902597    0.25830004    0.72329998    0.37529999    0.71240002]
 [ 181.46929304    0.19950001    0.69639999    0.3942        0.71240008]
 [ 253.36508572    0.2076        0.62849998    0.37200001    0.65430003]
 ...
 [1259.99747419    0.21430002    0.22679999    0.2185        0.23860002]
 [1234.22572281    0.25999999    0.27430001    0.2581        0.24299999]
 [1455.88333304    0.2388        0.21500002    0.24969999    0.21870001]][0m
[37m[1m[2023-07-10 19:21:58,825][227910] Max Reward on eval: 1656.029422288609[0m
[37m[1m[2023-07-10 19:21:58,825][227910] Min Reward on eval: -350.1355063173687[0m
[37m[1m[2023-07-10 19:21:58,826][227910] Mean Reward across all agents: 587.4621557859999[0m
[37m[1m[2023-07-10 19:21:58,826][227910] Average Trajectory Length: 978.4713333333333[0m
[36m[2023-07-10 19:21:58,828][227910] mean_value=-905.831868261069, max_value=448.1033340806965[0m
[37m[1m[2023-07-10 19:21:58,831][227910] New mean coefficients: [[ 2.17241    -0.13440868  2.3330052   1.0899899   3.2979786 ]][0m
[37m[1m[2023-07-10 19:21:58,832][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:22:08,678][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 19:22:08,679][227910] FPS: 390040.94[0m
[36m[2023-07-10 19:22:08,681][227910] itr=1157, itrs=2000, Progress: 57.85%[0m
[36m[2023-07-10 19:22:20,126][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 19:22:20,126][227910] FPS: 336085.40[0m
[36m[2023-07-10 19:22:24,845][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:22:24,845][227910] Reward + Measures: [[807.22679562   0.12423521   0.66167909   0.43491146   0.69552845]][0m
[37m[1m[2023-07-10 19:22:24,845][227910] Max Reward on eval: 807.2267956158831[0m
[37m[1m[2023-07-10 19:22:24,845][227910] Min Reward on eval: 807.2267956158831[0m
[37m[1m[2023-07-10 19:22:24,846][227910] Mean Reward across all agents: 807.2267956158831[0m
[37m[1m[2023-07-10 19:22:24,846][227910] Average Trajectory Length: 992.9499999999999[0m
[36m[2023-07-10 19:22:30,253][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:22:30,254][227910] Reward + Measures: [[1191.66668961    0.2018919     0.52899462    0.4062514     0.55435407]
 [ 588.70798684    0.09020001    0.65650004    0.509         0.72189999]
 [ 505.62620683    0.25770003    0.58070004    0.36120003    0.64449996]
 ...
 [ 951.97827183    0.1565        0.48979998    0.39129996    0.52380002]
 [ 583.92806487    0.34920001    0.51000005    0.42679998    0.60149997]
 [ 743.72149841    0.18090002    0.67680001    0.55839998    0.7137    ]][0m
[37m[1m[2023-07-10 19:22:30,254][227910] Max Reward on eval: 1510.0950615745473[0m
[37m[1m[2023-07-10 19:22:30,254][227910] Min Reward on eval: 271.9491836712863[0m
[37m[1m[2023-07-10 19:22:30,255][227910] Mean Reward across all agents: 696.1001094411857[0m
[37m[1m[2023-07-10 19:22:30,255][227910] Average Trajectory Length: 991.5459999999999[0m
[36m[2023-07-10 19:22:30,263][227910] mean_value=393.4639886979929, max_value=1128.319591475994[0m
[37m[1m[2023-07-10 19:22:30,266][227910] New mean coefficients: [[ 2.921605   -0.22343701  1.7323232   1.4760456   3.515888  ]][0m
[37m[1m[2023-07-10 19:22:30,267][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:22:39,869][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 19:22:39,870][227910] FPS: 399974.99[0m
[36m[2023-07-10 19:22:39,872][227910] itr=1158, itrs=2000, Progress: 57.90%[0m
[36m[2023-07-10 19:22:51,354][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 19:22:51,354][227910] FPS: 334970.33[0m
[36m[2023-07-10 19:22:56,095][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:22:56,095][227910] Reward + Measures: [[1055.76779907    0.13289899    0.628977      0.42962483    0.64966106]][0m
[37m[1m[2023-07-10 19:22:56,096][227910] Max Reward on eval: 1055.7677990735779[0m
[37m[1m[2023-07-10 19:22:56,096][227910] Min Reward on eval: 1055.7677990735779[0m
[37m[1m[2023-07-10 19:22:56,096][227910] Mean Reward across all agents: 1055.7677990735779[0m
[37m[1m[2023-07-10 19:22:56,096][227910] Average Trajectory Length: 987.266[0m
[36m[2023-07-10 19:23:01,612][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:23:01,612][227910] Reward + Measures: [[1092.89744994    0.37490001    0.17550001    0.31759998    0.29309997]
 [ 626.57435442    0.30650002    0.14969999    0.28099999    0.25500003]
 [ 378.82148861    0.28999999    0.3502        0.30620003    0.36670002]
 ...
 [1307.76175776    0.3490631     0.24500895    0.29439846    0.32818604]
 [ 383.12627565    0.34259996    0.61559999    0.2481        0.64679998]
 [ 267.59317193    0.1787        0.2642        0.2052        0.22880001]][0m
[37m[1m[2023-07-10 19:23:01,612][227910] Max Reward on eval: 1981.4991404182451[0m
[37m[1m[2023-07-10 19:23:01,613][227910] Min Reward on eval: -1249.5850702221505[0m
[37m[1m[2023-07-10 19:23:01,613][227910] Mean Reward across all agents: 670.1996665817084[0m
[37m[1m[2023-07-10 19:23:01,613][227910] Average Trajectory Length: 979.9583333333333[0m
[36m[2023-07-10 19:23:01,617][227910] mean_value=-778.5527548736549, max_value=1127.7123872883017[0m
[37m[1m[2023-07-10 19:23:01,620][227910] New mean coefficients: [[ 3.0676289  -0.07411419  1.2343143   1.5978786   3.1953766 ]][0m
[37m[1m[2023-07-10 19:23:01,620][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:23:11,248][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 19:23:11,249][227910] FPS: 398903.86[0m
[36m[2023-07-10 19:23:11,251][227910] itr=1159, itrs=2000, Progress: 57.95%[0m
[36m[2023-07-10 19:23:22,739][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 19:23:22,739][227910] FPS: 334800.60[0m
[36m[2023-07-10 19:23:27,534][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:23:27,534][227910] Reward + Measures: [[1472.77385411    0.18450952    0.52765721    0.3818768     0.5309217 ]][0m
[37m[1m[2023-07-10 19:23:27,535][227910] Max Reward on eval: 1472.7738541082008[0m
[37m[1m[2023-07-10 19:23:27,535][227910] Min Reward on eval: 1472.7738541082008[0m
[37m[1m[2023-07-10 19:23:27,535][227910] Mean Reward across all agents: 1472.7738541082008[0m
[37m[1m[2023-07-10 19:23:27,535][227910] Average Trajectory Length: 973.9666666666666[0m
[36m[2023-07-10 19:23:32,982][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:23:32,983][227910] Reward + Measures: [[ 207.38257129    0.44580004    0.56449997    0.47909999    0.58970004]
 [1498.24158764    0.29030225    0.31753299    0.26370502    0.32241565]
 [ 482.01449502    0.271         0.37920001    0.37810001    0.3547    ]
 ...
 [ 908.26627223    0.21730001    0.22409999    0.2726        0.21140002]
 [1840.41989478    0.28910002    0.38369998    0.3021        0.3179    ]
 [ 753.1630561     0.2969        0.42210004    0.41740003    0.47559997]][0m
[37m[1m[2023-07-10 19:23:32,983][227910] Max Reward on eval: 2479.0566096693046[0m
[37m[1m[2023-07-10 19:23:32,984][227910] Min Reward on eval: -1640.130521528609[0m
[37m[1m[2023-07-10 19:23:32,984][227910] Mean Reward across all agents: 363.9611918105123[0m
[37m[1m[2023-07-10 19:23:32,984][227910] Average Trajectory Length: 988.2933333333333[0m
[36m[2023-07-10 19:23:32,987][227910] mean_value=-762.3020902900314, max_value=929.2636539140765[0m
[37m[1m[2023-07-10 19:23:32,990][227910] New mean coefficients: [[3.2710352  0.13707735 1.4350448  0.28085315 3.529015  ]][0m
[37m[1m[2023-07-10 19:23:32,991][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:23:42,685][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 19:23:42,686][227910] FPS: 396157.25[0m
[36m[2023-07-10 19:23:42,688][227910] itr=1160, itrs=2000, Progress: 58.00%[0m
[37m[1m[2023-07-10 19:23:46,442][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001140[0m
[36m[2023-07-10 19:23:58,359][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 19:23:58,359][227910] FPS: 329696.32[0m
[36m[2023-07-10 19:24:03,243][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:24:03,244][227910] Reward + Measures: [[2034.50078394    0.23620202    0.43954501    0.31104428    0.41799992]][0m
[37m[1m[2023-07-10 19:24:03,244][227910] Max Reward on eval: 2034.500783936584[0m
[37m[1m[2023-07-10 19:24:03,244][227910] Min Reward on eval: 2034.500783936584[0m
[37m[1m[2023-07-10 19:24:03,244][227910] Mean Reward across all agents: 2034.500783936584[0m
[37m[1m[2023-07-10 19:24:03,244][227910] Average Trajectory Length: 974.7983333333333[0m
[36m[2023-07-10 19:24:08,841][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:24:08,842][227910] Reward + Measures: [[ 947.47717615    0.48850003    0.47950003    0.3786        0.61120003]
 [ 824.10134116    0.26827815    0.6657607     0.46648523    0.73771256]
 [1672.30945668    0.1955        0.52410001    0.37990001    0.52780002]
 ...
 [1596.28350515    0.25439167    0.20971484    0.22016647    0.23544621]
 [1629.55693569    0.30668887    0.43118334    0.30202779    0.46377221]
 [2231.55347993    0.39222103    0.27633628    0.27833632    0.27755603]][0m
[37m[1m[2023-07-10 19:24:08,842][227910] Max Reward on eval: 2519.69615159838[0m
[37m[1m[2023-07-10 19:24:08,842][227910] Min Reward on eval: 525.7408194209623[0m
[37m[1m[2023-07-10 19:24:08,843][227910] Mean Reward across all agents: 1394.559267812409[0m
[37m[1m[2023-07-10 19:24:08,843][227910] Average Trajectory Length: 979.1146666666666[0m
[36m[2023-07-10 19:24:08,850][227910] mean_value=217.4470010220584, max_value=1349.1901325720705[0m
[37m[1m[2023-07-10 19:24:08,853][227910] New mean coefficients: [[ 3.7490437  -0.33019686  1.88131     0.48450485  3.451508  ]][0m
[37m[1m[2023-07-10 19:24:08,854][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:24:18,570][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 19:24:18,571][227910] FPS: 395270.10[0m
[36m[2023-07-10 19:24:18,573][227910] itr=1161, itrs=2000, Progress: 58.05%[0m
[36m[2023-07-10 19:24:30,063][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 19:24:30,063][227910] FPS: 334721.45[0m
[36m[2023-07-10 19:24:34,749][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:24:34,749][227910] Reward + Measures: [[2502.99592211    0.27805328    0.38781387    0.27043161    0.34810314]][0m
[37m[1m[2023-07-10 19:24:34,750][227910] Max Reward on eval: 2502.9959221058252[0m
[37m[1m[2023-07-10 19:24:34,750][227910] Min Reward on eval: 2502.9959221058252[0m
[37m[1m[2023-07-10 19:24:34,750][227910] Mean Reward across all agents: 2502.9959221058252[0m
[37m[1m[2023-07-10 19:24:34,750][227910] Average Trajectory Length: 979.6396666666666[0m
[36m[2023-07-10 19:24:40,333][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:24:40,339][227910] Reward + Measures: [[1626.0298499     0.51770002    0.49499997    0.15660001    0.4824    ]
 [2192.78092924    0.39417407    0.24215598    0.25102016    0.27144709]
 [1570.93500178    0.40110001    0.3319        0.176         0.32429999]
 ...
 [2189.74325594    0.36279997    0.2122        0.2361        0.25640002]
 [1892.17684219    0.38322422    0.34025154    0.32174507    0.33960924]
 [1899.09839931    0.3395896     0.26117274    0.27910131    0.27739742]][0m
[37m[1m[2023-07-10 19:24:40,339][227910] Max Reward on eval: 2860.228018111363[0m
[37m[1m[2023-07-10 19:24:40,339][227910] Min Reward on eval: -601.4989639315172[0m
[37m[1m[2023-07-10 19:24:40,340][227910] Mean Reward across all agents: 1699.0937480807836[0m
[37m[1m[2023-07-10 19:24:40,340][227910] Average Trajectory Length: 975.5469999999999[0m
[36m[2023-07-10 19:24:40,344][227910] mean_value=-924.8755062321934, max_value=2184.688264838983[0m
[37m[1m[2023-07-10 19:24:40,346][227910] New mean coefficients: [[ 3.5400736  -0.85931385  2.7149177   0.47308356  3.762804  ]][0m
[37m[1m[2023-07-10 19:24:40,347][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:24:50,031][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 19:24:50,032][227910] FPS: 396600.31[0m
[36m[2023-07-10 19:24:50,034][227910] itr=1162, itrs=2000, Progress: 58.10%[0m
[36m[2023-07-10 19:25:01,529][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 19:25:01,529][227910] FPS: 334586.84[0m
[36m[2023-07-10 19:25:06,353][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:25:06,354][227910] Reward + Measures: [[3022.84680906    0.31878632    0.32157272    0.21991821    0.26114711]][0m
[37m[1m[2023-07-10 19:25:06,354][227910] Max Reward on eval: 3022.8468090644715[0m
[37m[1m[2023-07-10 19:25:06,354][227910] Min Reward on eval: 3022.8468090644715[0m
[37m[1m[2023-07-10 19:25:06,354][227910] Mean Reward across all agents: 3022.8468090644715[0m
[37m[1m[2023-07-10 19:25:06,355][227910] Average Trajectory Length: 975.7426666666667[0m
[36m[2023-07-10 19:25:11,880][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:25:11,881][227910] Reward + Measures: [[1203.41580331    0.36069998    0.35789999    0.37970003    0.2184    ]
 [  24.70818508    0.55909997    0.23940001    0.6196        0.2036    ]
 [  73.10685621    0.47626525    0.36852941    0.25993988    0.35312366]
 ...
 [1103.7526184     0.29010001    0.3391        0.36420003    0.28420001]
 [ 877.54050806    0.28369999    0.19670001    0.30919999    0.16370001]
 [ 790.49103266    0.48308778    0.19285099    0.46891099    0.16768382]][0m
[37m[1m[2023-07-10 19:25:11,881][227910] Max Reward on eval: 2408.615481809806[0m
[37m[1m[2023-07-10 19:25:11,881][227910] Min Reward on eval: -1336.0705528886058[0m
[37m[1m[2023-07-10 19:25:11,881][227910] Mean Reward across all agents: 573.2475500228079[0m
[37m[1m[2023-07-10 19:25:11,882][227910] Average Trajectory Length: 961.3933333333333[0m
[36m[2023-07-10 19:25:11,884][227910] mean_value=-1293.1630678439958, max_value=922.621721737669[0m
[37m[1m[2023-07-10 19:25:11,887][227910] New mean coefficients: [[ 3.2199636  -0.6352928   2.948107    0.14530322  4.6980076 ]][0m
[37m[1m[2023-07-10 19:25:11,888][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:25:21,703][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 19:25:21,703][227910] FPS: 391284.59[0m
[36m[2023-07-10 19:25:21,706][227910] itr=1163, itrs=2000, Progress: 58.15%[0m
[36m[2023-07-10 19:25:33,246][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 19:25:33,246][227910] FPS: 333320.94[0m
[36m[2023-07-10 19:25:37,992][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:25:37,993][227910] Reward + Measures: [[3268.17165144    0.32722664    0.31483838    0.2041678     0.24313679]][0m
[37m[1m[2023-07-10 19:25:37,993][227910] Max Reward on eval: 3268.171651435905[0m
[37m[1m[2023-07-10 19:25:37,993][227910] Min Reward on eval: 3268.171651435905[0m
[37m[1m[2023-07-10 19:25:37,994][227910] Mean Reward across all agents: 3268.171651435905[0m
[37m[1m[2023-07-10 19:25:37,994][227910] Average Trajectory Length: 983.3546666666666[0m
[36m[2023-07-10 19:25:43,499][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:25:43,499][227910] Reward + Measures: [[2141.08037908    0.32359457    0.21183661    0.2468607     0.19893253]
 [-421.45357276    0.18915997    0.3887507     0.42471251    0.42067441]
 [1821.55445565    0.28443456    0.20422177    0.21722202    0.1638208 ]
 ...
 [1500.79094774    0.1829        0.48779997    0.3249        0.48199996]
 [ 245.66060471    0.1107        0.7802        0.58059996    0.8768    ]
 [ 317.19415661    0.2096        0.2414        0.2782        0.20780002]][0m
[37m[1m[2023-07-10 19:25:43,500][227910] Max Reward on eval: 3369.8761744944845[0m
[37m[1m[2023-07-10 19:25:43,500][227910] Min Reward on eval: -575.845392869541[0m
[37m[1m[2023-07-10 19:25:43,500][227910] Mean Reward across all agents: 1109.1684102853915[0m
[37m[1m[2023-07-10 19:25:43,500][227910] Average Trajectory Length: 969.9879999999999[0m
[36m[2023-07-10 19:25:43,504][227910] mean_value=-1110.5962247065315, max_value=1077.1190237668015[0m
[37m[1m[2023-07-10 19:25:43,507][227910] New mean coefficients: [[3.22148    0.04913265 3.5106099  0.08854711 4.516376  ]][0m
[37m[1m[2023-07-10 19:25:43,508][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:25:53,413][227910] train() took 9.90 seconds to complete[0m
[36m[2023-07-10 19:25:53,413][227910] FPS: 387740.98[0m
[36m[2023-07-10 19:25:53,415][227910] itr=1164, itrs=2000, Progress: 58.20%[0m
[36m[2023-07-10 19:26:05,052][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 19:26:05,052][227910] FPS: 330605.23[0m
[36m[2023-07-10 19:26:09,870][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:26:09,871][227910] Reward + Measures: [[3485.04229765    0.33491278    0.30802733    0.20038605    0.2278395 ]][0m
[37m[1m[2023-07-10 19:26:09,871][227910] Max Reward on eval: 3485.04229765124[0m
[37m[1m[2023-07-10 19:26:09,871][227910] Min Reward on eval: 3485.04229765124[0m
[37m[1m[2023-07-10 19:26:09,871][227910] Mean Reward across all agents: 3485.04229765124[0m
[37m[1m[2023-07-10 19:26:09,872][227910] Average Trajectory Length: 981.0163333333333[0m
[36m[2023-07-10 19:26:15,362][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:26:15,363][227910] Reward + Measures: [[ 133.84408715    0.39770004    0.31819996    0.27079999    0.44729996]
 [  83.45787275    0.28099999    0.47200003    0.41499996    0.48930001]
 [1744.61356803    0.27720001    0.25689998    0.20019999    0.22000001]
 ...
 [ 447.77184921    0.41920003    0.4219        0.26820001    0.50369996]
 [ 435.31946982    0.103         0.81549996    0.55299997    0.88239998]
 [ 187.89294401    0.25187966    0.19628626    0.43936911    0.2017871 ]][0m
[37m[1m[2023-07-10 19:26:15,363][227910] Max Reward on eval: 2995.391253827166[0m
[37m[1m[2023-07-10 19:26:15,364][227910] Min Reward on eval: -537.8645930171245[0m
[37m[1m[2023-07-10 19:26:15,364][227910] Mean Reward across all agents: 745.3638892607146[0m
[37m[1m[2023-07-10 19:26:15,364][227910] Average Trajectory Length: 969.3873333333333[0m
[36m[2023-07-10 19:26:15,368][227910] mean_value=-741.9055983432119, max_value=1031.3246989449488[0m
[37m[1m[2023-07-10 19:26:15,370][227910] New mean coefficients: [[ 3.3164532  -0.38592568  3.056134    0.4359812   3.9386594 ]][0m
[37m[1m[2023-07-10 19:26:15,371][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:26:25,145][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 19:26:25,145][227910] FPS: 392977.30[0m
[36m[2023-07-10 19:26:25,147][227910] itr=1165, itrs=2000, Progress: 58.25%[0m
[36m[2023-07-10 19:26:36,722][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 19:26:36,722][227910] FPS: 332343.96[0m
[36m[2023-07-10 19:26:41,566][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:26:41,566][227910] Reward + Measures: [[3685.87386083    0.34836161    0.30621237    0.19546776    0.22116457]][0m
[37m[1m[2023-07-10 19:26:41,566][227910] Max Reward on eval: 3685.8738608283115[0m
[37m[1m[2023-07-10 19:26:41,567][227910] Min Reward on eval: 3685.8738608283115[0m
[37m[1m[2023-07-10 19:26:41,567][227910] Mean Reward across all agents: 3685.8738608283115[0m
[37m[1m[2023-07-10 19:26:41,567][227910] Average Trajectory Length: 984.1406666666667[0m
[36m[2023-07-10 19:26:47,240][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:26:47,241][227910] Reward + Measures: [[1680.23071503    0.25466728    0.24113093    0.18148847    0.15848793]
 [1692.16129357    0.34279999    0.39140001    0.28690001    0.27090001]
 [1116.94965894    0.58700007    0.46569997    0.54049999    0.5413    ]
 ...
 [2327.85508543    0.29879999    0.30609998    0.22319999    0.2273    ]
 [1132.5465161     0.40319997    0.42360002    0.32240003    0.41089997]
 [ 980.67795051    0.29100001    0.69620001    0.55150002    0.73270005]][0m
[37m[1m[2023-07-10 19:26:47,241][227910] Max Reward on eval: 3031.7600479959974[0m
[37m[1m[2023-07-10 19:26:47,241][227910] Min Reward on eval: -599.2001237778459[0m
[37m[1m[2023-07-10 19:26:47,241][227910] Mean Reward across all agents: 1290.0887374283552[0m
[37m[1m[2023-07-10 19:26:47,242][227910] Average Trajectory Length: 984.9203333333332[0m
[36m[2023-07-10 19:26:47,247][227910] mean_value=-710.5359641328396, max_value=1199.6999152409535[0m
[37m[1m[2023-07-10 19:26:47,250][227910] New mean coefficients: [[ 2.990325   -0.5764529   2.6144052   0.89173293  3.8471246 ]][0m
[37m[1m[2023-07-10 19:26:47,251][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:26:57,073][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 19:26:57,073][227910] FPS: 391049.52[0m
[36m[2023-07-10 19:26:57,075][227910] itr=1166, itrs=2000, Progress: 58.30%[0m
[36m[2023-07-10 19:27:08,612][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 19:27:08,612][227910] FPS: 333473.61[0m
[36m[2023-07-10 19:27:13,355][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:27:13,355][227910] Reward + Measures: [[3953.61780154    0.35597542    0.30991003    0.19056128    0.21280992]][0m
[37m[1m[2023-07-10 19:27:13,356][227910] Max Reward on eval: 3953.617801537603[0m
[37m[1m[2023-07-10 19:27:13,356][227910] Min Reward on eval: 3953.617801537603[0m
[37m[1m[2023-07-10 19:27:13,356][227910] Mean Reward across all agents: 3953.617801537603[0m
[37m[1m[2023-07-10 19:27:13,356][227910] Average Trajectory Length: 986.4983333333333[0m
[36m[2023-07-10 19:27:18,830][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:27:18,831][227910] Reward + Measures: [[ 718.8603756     0.3576        0.40549999    0.29449999    0.38910004]
 [ 563.17611273    0.22143184    0.31497431    0.23825088    0.27762961]
 [1959.62007321    0.33730003    0.57559997    0.2471        0.38590002]
 ...
 [ 440.62982648    0.625         0.28980002    0.58339995    0.52570003]
 [ -12.64732485    0.21297804    0.34514666    0.21097806    0.27672774]
 [ 328.99214452    0.47569999    0.51959997    0.59930003    0.52700007]][0m
[37m[1m[2023-07-10 19:27:18,831][227910] Max Reward on eval: 2353.3596157264546[0m
[37m[1m[2023-07-10 19:27:18,831][227910] Min Reward on eval: -873.4705477199634[0m
[37m[1m[2023-07-10 19:27:18,831][227910] Mean Reward across all agents: 522.6020471099498[0m
[37m[1m[2023-07-10 19:27:18,832][227910] Average Trajectory Length: 981.3883333333333[0m
[36m[2023-07-10 19:27:18,833][227910] mean_value=-1999.7091832307442, max_value=590.3133995986364[0m
[37m[1m[2023-07-10 19:27:18,836][227910] New mean coefficients: [[ 2.512001   -0.9068787   3.1297493   0.57253474  2.2705073 ]][0m
[37m[1m[2023-07-10 19:27:18,836][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:27:28,655][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 19:27:28,656][227910] FPS: 391146.64[0m
[36m[2023-07-10 19:27:28,658][227910] itr=1167, itrs=2000, Progress: 58.35%[0m
[36m[2023-07-10 19:27:40,177][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 19:27:40,177][227910] FPS: 333890.42[0m
[36m[2023-07-10 19:27:44,963][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:27:44,963][227910] Reward + Measures: [[4210.79958824    0.36384764    0.3202002     0.1896006     0.21202329]][0m
[37m[1m[2023-07-10 19:27:44,964][227910] Max Reward on eval: 4210.799588236903[0m
[37m[1m[2023-07-10 19:27:44,964][227910] Min Reward on eval: 4210.799588236903[0m
[37m[1m[2023-07-10 19:27:44,964][227910] Mean Reward across all agents: 4210.799588236903[0m
[37m[1m[2023-07-10 19:27:44,964][227910] Average Trajectory Length: 989.375[0m
[36m[2023-07-10 19:27:50,469][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:27:50,470][227910] Reward + Measures: [[2421.91252633    0.48100001    0.2244        0.23570001    0.25789997]
 [2441.76150863    0.3019        0.45559999    0.20410001    0.2422    ]
 [3888.1693899     0.40450001    0.32030001    0.20109999    0.2418    ]
 ...
 [1469.22660068    0.32780001    0.48969999    0.27090001    0.27519998]
 [2405.05596173    0.37709999    0.28490001    0.26010001    0.24720001]
 [3304.34875577    0.38480002    0.33149999    0.22839999    0.2563    ]][0m
[37m[1m[2023-07-10 19:27:50,470][227910] Max Reward on eval: 4064.3849785894154[0m
[37m[1m[2023-07-10 19:27:50,470][227910] Min Reward on eval: 360.09121727112796[0m
[37m[1m[2023-07-10 19:27:50,471][227910] Mean Reward across all agents: 2322.128877281171[0m
[37m[1m[2023-07-10 19:27:50,471][227910] Average Trajectory Length: 995.3596666666666[0m
[36m[2023-07-10 19:27:50,475][227910] mean_value=-712.4710885399676, max_value=1821.5671459718212[0m
[37m[1m[2023-07-10 19:27:50,477][227910] New mean coefficients: [[ 2.1706305  -0.62974036  4.354492    1.1014627   1.127123  ]][0m
[37m[1m[2023-07-10 19:27:50,478][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:28:00,279][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 19:28:00,279][227910] FPS: 391877.56[0m
[36m[2023-07-10 19:28:00,281][227910] itr=1168, itrs=2000, Progress: 58.40%[0m
[36m[2023-07-10 19:28:11,860][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 19:28:11,861][227910] FPS: 332163.59[0m
[36m[2023-07-10 19:28:16,674][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:28:16,674][227910] Reward + Measures: [[4390.72904737    0.37788114    0.32421523    0.18958691    0.21088965]][0m
[37m[1m[2023-07-10 19:28:16,674][227910] Max Reward on eval: 4390.729047366891[0m
[37m[1m[2023-07-10 19:28:16,675][227910] Min Reward on eval: 4390.729047366891[0m
[37m[1m[2023-07-10 19:28:16,675][227910] Mean Reward across all agents: 4390.729047366891[0m
[37m[1m[2023-07-10 19:28:16,675][227910] Average Trajectory Length: 991.7066666666666[0m
[36m[2023-07-10 19:28:22,184][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:28:22,184][227910] Reward + Measures: [[1224.80066939    0.23103809    0.32401904    0.25384283    0.21739049]
 [ 596.99929251    0.187839      0.22402911    0.19622581    0.16605131]
 [1683.24517908    0.35929999    0.55070001    0.23360001    0.28389999]
 ...
 [ 505.23459733    0.38399997    0.42870003    0.37810001    0.37750003]
 [1973.77350719    0.25738153    0.19368547    0.24946959    0.18268412]
 [ 379.55504881    0.22662108    0.2435274     0.27166581    0.19285169]][0m
[37m[1m[2023-07-10 19:28:22,184][227910] Max Reward on eval: 3558.0004748168635[0m
[37m[1m[2023-07-10 19:28:22,185][227910] Min Reward on eval: -1230.7586508385489[0m
[37m[1m[2023-07-10 19:28:22,185][227910] Mean Reward across all agents: 955.1777628203668[0m
[37m[1m[2023-07-10 19:28:22,185][227910] Average Trajectory Length: 882.5426666666666[0m
[36m[2023-07-10 19:28:22,187][227910] mean_value=-1467.0627079872634, max_value=1675.4602355725424[0m
[37m[1m[2023-07-10 19:28:22,190][227910] New mean coefficients: [[ 1.5747192  -0.75400895  4.085542   -0.17414963  1.9885827 ]][0m
[37m[1m[2023-07-10 19:28:22,190][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:28:31,855][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 19:28:31,855][227910] FPS: 397390.44[0m
[36m[2023-07-10 19:28:31,858][227910] itr=1169, itrs=2000, Progress: 58.45%[0m
[36m[2023-07-10 19:28:43,421][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 19:28:43,421][227910] FPS: 332700.54[0m
[36m[2023-07-10 19:28:48,244][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:28:48,244][227910] Reward + Measures: [[4507.11108312    0.37482917    0.33218741    0.18653463    0.20309082]][0m
[37m[1m[2023-07-10 19:28:48,244][227910] Max Reward on eval: 4507.111083115314[0m
[37m[1m[2023-07-10 19:28:48,245][227910] Min Reward on eval: 4507.111083115314[0m
[37m[1m[2023-07-10 19:28:48,245][227910] Mean Reward across all agents: 4507.111083115314[0m
[37m[1m[2023-07-10 19:28:48,245][227910] Average Trajectory Length: 990.9006666666667[0m
[36m[2023-07-10 19:28:53,695][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:28:53,695][227910] Reward + Measures: [[1136.56346491    0.23920003    0.23410001    0.29560003    0.2868    ]
 [1704.86675716    0.28480002    0.1726        0.26449999    0.2122    ]
 [ 850.74264787    0.24800001    0.35770002    0.44109997    0.2368    ]
 ...
 [ 456.61069982    0.38750002    0.41359997    0.52820003    0.3671    ]
 [ 700.99298809    0.32710001    0.45559999    0.3874        0.3515    ]
 [1117.17189488    0.23370002    0.41570002    0.45290002    0.23240001]][0m
[37m[1m[2023-07-10 19:28:53,695][227910] Max Reward on eval: 3176.1752663644206[0m
[37m[1m[2023-07-10 19:28:53,696][227910] Min Reward on eval: -296.0919814060646[0m
[37m[1m[2023-07-10 19:28:53,696][227910] Mean Reward across all agents: 1085.6335694917261[0m
[37m[1m[2023-07-10 19:28:53,696][227910] Average Trajectory Length: 991.7666666666667[0m
[36m[2023-07-10 19:28:53,698][227910] mean_value=-1355.1770276914274, max_value=909.1938129508592[0m
[37m[1m[2023-07-10 19:28:53,701][227910] New mean coefficients: [[ 2.4535117  -0.8880961   3.5437355   0.25795922  3.0058594 ]][0m
[37m[1m[2023-07-10 19:28:53,702][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:29:03,326][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 19:29:03,326][227910] FPS: 399064.97[0m
[36m[2023-07-10 19:29:03,328][227910] itr=1170, itrs=2000, Progress: 58.50%[0m
[37m[1m[2023-07-10 19:29:07,241][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001150[0m
[36m[2023-07-10 19:29:19,013][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 19:29:19,013][227910] FPS: 333821.50[0m
[36m[2023-07-10 19:29:23,773][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:29:23,774][227910] Reward + Measures: [[4620.31206395    0.3811875     0.32924938    0.1860518     0.19735506]][0m
[37m[1m[2023-07-10 19:29:23,774][227910] Max Reward on eval: 4620.312063954595[0m
[37m[1m[2023-07-10 19:29:23,774][227910] Min Reward on eval: 4620.312063954595[0m
[37m[1m[2023-07-10 19:29:23,774][227910] Mean Reward across all agents: 4620.312063954595[0m
[37m[1m[2023-07-10 19:29:23,775][227910] Average Trajectory Length: 986.7339999999999[0m
[36m[2023-07-10 19:29:29,296][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:29:29,306][227910] Reward + Measures: [[1987.79732617    0.30900002    0.54159999    0.22219999    0.30700001]
 [1951.62833241    0.32349998    0.54399997    0.2221        0.32079998]
 [1399.77713994    0.27560002    0.59400004    0.2762        0.4068    ]
 ...
 [3011.51360582    0.41210005    0.41910002    0.22680001    0.23020001]
 [3162.25968197    0.34709999    0.4842        0.20990001    0.24070001]
 [2183.09579006    0.31589648    0.44572279    0.23312984    0.21262105]][0m
[37m[1m[2023-07-10 19:29:29,306][227910] Max Reward on eval: 4312.593689791788[0m
[37m[1m[2023-07-10 19:29:29,306][227910] Min Reward on eval: 211.95250475674985[0m
[37m[1m[2023-07-10 19:29:29,307][227910] Mean Reward across all agents: 2124.0640861992474[0m
[37m[1m[2023-07-10 19:29:29,307][227910] Average Trajectory Length: 989.5993333333333[0m
[36m[2023-07-10 19:29:29,310][227910] mean_value=-538.7652280410512, max_value=1805.4232276085036[0m
[37m[1m[2023-07-10 19:29:29,313][227910] New mean coefficients: [[ 2.485013  -1.1528889  4.3566294  0.2518512  3.53177  ]][0m
[37m[1m[2023-07-10 19:29:29,314][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:29:39,182][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 19:29:39,182][227910] FPS: 389184.13[0m
[36m[2023-07-10 19:29:39,185][227910] itr=1171, itrs=2000, Progress: 58.55%[0m
[36m[2023-07-10 19:29:50,891][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 19:29:50,892][227910] FPS: 328540.67[0m
[36m[2023-07-10 19:29:55,635][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:29:55,636][227910] Reward + Measures: [[4752.67802413    0.37726268    0.33508673    0.183965      0.19405098]][0m
[37m[1m[2023-07-10 19:29:55,636][227910] Max Reward on eval: 4752.678024127357[0m
[37m[1m[2023-07-10 19:29:55,636][227910] Min Reward on eval: 4752.678024127357[0m
[37m[1m[2023-07-10 19:29:55,636][227910] Mean Reward across all agents: 4752.678024127357[0m
[37m[1m[2023-07-10 19:29:55,636][227910] Average Trajectory Length: 986.841[0m
[36m[2023-07-10 19:30:01,124][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:30:01,125][227910] Reward + Measures: [[  807.10143143     0.49330002     0.40219998     0.40079999
      0.43340001]
 [ -472.15010243     0.1717         0.5848         0.37409997
      0.67889994]
 [  -18.96614211     0.20479999     0.80140001     0.30110002
      0.79860002]
 ...
 [-1080.48008556     0.21254782     0.41943914     0.41643482
      0.66060436]
 [  360.87405785     0.19000001     0.7191         0.273
      0.50910002]
 [ -484.88957261     0.0958         0.66169995     0.33899999
      0.74049997]][0m
[37m[1m[2023-07-10 19:30:01,125][227910] Max Reward on eval: 3759.771469552349[0m
[37m[1m[2023-07-10 19:30:01,125][227910] Min Reward on eval: -1386.7576498194946[0m
[37m[1m[2023-07-10 19:30:01,126][227910] Mean Reward across all agents: -67.02664109463745[0m
[37m[1m[2023-07-10 19:30:01,126][227910] Average Trajectory Length: 986.072[0m
[36m[2023-07-10 19:30:01,128][227910] mean_value=-980.355462138529, max_value=1052.2391349707846[0m
[37m[1m[2023-07-10 19:30:01,131][227910] New mean coefficients: [[ 2.2370703  -0.8805692   4.539376   -0.25581843  3.0542645 ]][0m
[37m[1m[2023-07-10 19:30:01,132][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:30:10,935][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 19:30:10,935][227910] FPS: 391775.11[0m
[36m[2023-07-10 19:30:10,938][227910] itr=1172, itrs=2000, Progress: 58.60%[0m
[36m[2023-07-10 19:30:22,513][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 19:30:22,513][227910] FPS: 332306.32[0m
[36m[2023-07-10 19:30:27,372][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:30:27,373][227910] Reward + Measures: [[4919.37993163    0.38015595    0.34873405    0.18199418    0.19674908]][0m
[37m[1m[2023-07-10 19:30:27,373][227910] Max Reward on eval: 4919.379931633116[0m
[37m[1m[2023-07-10 19:30:27,373][227910] Min Reward on eval: 4919.379931633116[0m
[37m[1m[2023-07-10 19:30:27,373][227910] Mean Reward across all agents: 4919.379931633116[0m
[37m[1m[2023-07-10 19:30:27,374][227910] Average Trajectory Length: 991.7819999999999[0m
[36m[2023-07-10 19:30:33,018][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:30:33,050][227910] Reward + Measures: [[ 535.93116552    0.0974        0.76419997    0.44169998    0.70779997]
 [3099.13577834    0.29635736    0.50094628    0.22899468    0.2151691 ]
 [-304.13463332    0.14420001    0.71420002    0.39499998    0.4427    ]
 ...
 [2371.11455675    0.21324909    0.25325018    0.18700539    0.14642115]
 [-447.99334546    0.0438        0.78730005    0.42800003    0.53009999]
 [ 437.577746      0.1754        0.73379999    0.3448        0.61610001]][0m
[37m[1m[2023-07-10 19:30:33,050][227910] Max Reward on eval: 3715.935607268149[0m
[37m[1m[2023-07-10 19:30:33,050][227910] Min Reward on eval: -2075.1108474149833[0m
[37m[1m[2023-07-10 19:30:33,050][227910] Mean Reward across all agents: 287.54723211927774[0m
[37m[1m[2023-07-10 19:30:33,051][227910] Average Trajectory Length: 988.5563333333333[0m
[36m[2023-07-10 19:30:33,053][227910] mean_value=-577.2631889576027, max_value=1280.9587665680003[0m
[37m[1m[2023-07-10 19:30:33,055][227910] New mean coefficients: [[ 1.4834214  -0.8056685   4.651444   -0.61942947  1.7799982 ]][0m
[37m[1m[2023-07-10 19:30:33,056][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:30:42,765][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 19:30:42,766][227910] FPS: 395570.67[0m
[36m[2023-07-10 19:30:42,768][227910] itr=1173, itrs=2000, Progress: 58.65%[0m
[36m[2023-07-10 19:30:54,287][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 19:30:54,287][227910] FPS: 333889.83[0m
[36m[2023-07-10 19:30:58,918][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:30:58,918][227910] Reward + Measures: [[5006.61103976    0.37709779    0.36733687    0.18030381    0.19411825]][0m
[37m[1m[2023-07-10 19:30:58,919][227910] Max Reward on eval: 5006.611039764852[0m
[37m[1m[2023-07-10 19:30:58,919][227910] Min Reward on eval: 5006.611039764852[0m
[37m[1m[2023-07-10 19:30:58,919][227910] Mean Reward across all agents: 5006.611039764852[0m
[37m[1m[2023-07-10 19:30:58,919][227910] Average Trajectory Length: 991.632[0m
[36m[2023-07-10 19:31:04,342][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:31:04,343][227910] Reward + Measures: [[-135.7429778     0.39830002    0.58240002    0.2789        0.55470008]
 [1257.04982269    0.31689999    0.41430002    0.45760003    0.4199    ]
 [-523.05982722    0.58280003    0.5104        0.59429997    0.44249997]
 ...
 [ 348.19324964    0.38579997    0.39999998    0.3364        0.32160002]
 [2026.00628911    0.49339995    0.37220001    0.39549997    0.2811    ]
 [-177.5238409     0.60270005    0.28470001    0.55759996    0.3565    ]][0m
[37m[1m[2023-07-10 19:31:04,343][227910] Max Reward on eval: 3457.7877272455953[0m
[37m[1m[2023-07-10 19:31:04,344][227910] Min Reward on eval: -1687.1612081669737[0m
[37m[1m[2023-07-10 19:31:04,344][227910] Mean Reward across all agents: 106.52071843079975[0m
[37m[1m[2023-07-10 19:31:04,344][227910] Average Trajectory Length: 988.812[0m
[36m[2023-07-10 19:31:04,347][227910] mean_value=-871.5874728624545, max_value=957.0689215994562[0m
[37m[1m[2023-07-10 19:31:04,350][227910] New mean coefficients: [[ 1.5365994  -0.71651095  4.045379   -0.563559    1.8191706 ]][0m
[37m[1m[2023-07-10 19:31:04,351][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:31:13,925][227910] train() took 9.57 seconds to complete[0m
[36m[2023-07-10 19:31:13,926][227910] FPS: 401128.55[0m
[36m[2023-07-10 19:31:13,928][227910] itr=1174, itrs=2000, Progress: 58.70%[0m
[36m[2023-07-10 19:31:25,473][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 19:31:25,473][227910] FPS: 333196.48[0m
[36m[2023-07-10 19:31:30,369][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:31:30,375][227910] Reward + Measures: [[5155.06411003    0.38011044    0.37360635    0.17792146    0.1944484 ]][0m
[37m[1m[2023-07-10 19:31:30,375][227910] Max Reward on eval: 5155.0641100321545[0m
[37m[1m[2023-07-10 19:31:30,375][227910] Min Reward on eval: 5155.0641100321545[0m
[37m[1m[2023-07-10 19:31:30,376][227910] Mean Reward across all agents: 5155.0641100321545[0m
[37m[1m[2023-07-10 19:31:30,376][227910] Average Trajectory Length: 996.2996666666667[0m
[36m[2023-07-10 19:31:35,908][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:31:35,914][227910] Reward + Measures: [[ 398.08808305    0.4858        0.48409995    0.41440001    0.47200003]
 [4644.11365932    0.42915091    0.34967476    0.18178736    0.18733087]
 [  34.52558127    0.54360002    0.44300005    0.49970004    0.47599998]
 ...
 [ 428.99759572    0.396         0.42400002    0.37059999    0.42480001]
 [ 498.95804817    0.36789998    0.45679998    0.35530001    0.3554    ]
 [-671.2393578     0.2872        0.46510002    0.37909999    0.49840003]][0m
[37m[1m[2023-07-10 19:31:35,914][227910] Max Reward on eval: 4644.113659316581[0m
[37m[1m[2023-07-10 19:31:35,914][227910] Min Reward on eval: -823.6314041253761[0m
[37m[1m[2023-07-10 19:31:35,914][227910] Mean Reward across all agents: 776.7884420863479[0m
[37m[1m[2023-07-10 19:31:35,915][227910] Average Trajectory Length: 988.5673333333333[0m
[36m[2023-07-10 19:31:35,916][227910] mean_value=-1202.1490887066136, max_value=493.4685563594572[0m
[37m[1m[2023-07-10 19:31:35,919][227910] New mean coefficients: [[ 2.092085   -0.7077037   2.993168   -0.98497796  2.0724714 ]][0m
[37m[1m[2023-07-10 19:31:35,920][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:31:45,639][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 19:31:45,639][227910] FPS: 395173.05[0m
[36m[2023-07-10 19:31:45,641][227910] itr=1175, itrs=2000, Progress: 58.75%[0m
[36m[2023-07-10 19:31:57,282][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 19:31:57,282][227910] FPS: 330391.75[0m
[36m[2023-07-10 19:32:02,111][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:32:02,117][227910] Reward + Measures: [[5272.07306431    0.3861599     0.37439319    0.17897268    0.19206299]][0m
[37m[1m[2023-07-10 19:32:02,118][227910] Max Reward on eval: 5272.0730643126435[0m
[37m[1m[2023-07-10 19:32:02,118][227910] Min Reward on eval: 5272.0730643126435[0m
[37m[1m[2023-07-10 19:32:02,119][227910] Mean Reward across all agents: 5272.0730643126435[0m
[37m[1m[2023-07-10 19:32:02,120][227910] Average Trajectory Length: 995.3813333333333[0m
[36m[2023-07-10 19:32:07,711][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:32:07,712][227910] Reward + Measures: [[-823.07323573    0.1753        0.18700002    0.1416        0.108     ]
 [-684.24106951    0.18413134    0.18009792    0.10534308    0.10196555]
 [-232.41245386    0.19803296    0.20929988    0.299867      0.14457697]
 ...
 [2662.14089542    0.36386609    0.23037751    0.22620597    0.20071883]
 [-440.70608599    0.14530002    0.29981428    0.29068571    0.18528572]
 [ 949.63634068    0.28461727    0.23816299    0.30868641    0.19457531]][0m
[37m[1m[2023-07-10 19:32:07,712][227910] Max Reward on eval: 3739.3697704020888[0m
[37m[1m[2023-07-10 19:32:07,712][227910] Min Reward on eval: -1157.6249969047378[0m
[37m[1m[2023-07-10 19:32:07,712][227910] Mean Reward across all agents: 619.0670676931258[0m
[37m[1m[2023-07-10 19:32:07,713][227910] Average Trajectory Length: 921.0[0m
[36m[2023-07-10 19:32:07,714][227910] mean_value=-2602.3349866381755, max_value=318.74165074939833[0m
[37m[1m[2023-07-10 19:32:07,717][227910] New mean coefficients: [[ 2.4820576  -0.7654096   1.1796763  -0.64755803  2.0718694 ]][0m
[37m[1m[2023-07-10 19:32:07,717][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:32:17,589][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 19:32:17,589][227910] FPS: 389071.92[0m
[36m[2023-07-10 19:32:17,591][227910] itr=1176, itrs=2000, Progress: 58.80%[0m
[36m[2023-07-10 19:32:29,247][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 19:32:29,247][227910] FPS: 330107.73[0m
[36m[2023-07-10 19:32:34,008][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:32:34,008][227910] Reward + Measures: [[5366.39137161    0.38611352    0.36616048    0.17871538    0.18633077]][0m
[37m[1m[2023-07-10 19:32:34,008][227910] Max Reward on eval: 5366.391371609366[0m
[37m[1m[2023-07-10 19:32:34,009][227910] Min Reward on eval: 5366.391371609366[0m
[37m[1m[2023-07-10 19:32:34,009][227910] Mean Reward across all agents: 5366.391371609366[0m
[37m[1m[2023-07-10 19:32:34,009][227910] Average Trajectory Length: 994.597[0m
[36m[2023-07-10 19:32:39,785][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:32:39,785][227910] Reward + Measures: [[-783.16458313    0.46479902    0.25950158    0.36595392    0.30008391]
 [1099.89550408    0.2590642     0.17780553    0.21104975    0.17478633]
 [ 980.24202891    0.38777038    0.35443586    0.30017602    0.3804577 ]
 ...
 [ 744.10452116    0.4718        0.2789        0.2457        0.25270003]
 [ 139.18775174    0.42211667    0.33537027    0.2447855     0.1888653 ]
 [-819.79805087    0.54510003    0.46830001    0.3732        0.37130001]][0m
[37m[1m[2023-07-10 19:32:39,786][227910] Max Reward on eval: 3518.1480477652512[0m
[37m[1m[2023-07-10 19:32:39,786][227910] Min Reward on eval: -1878.3695372465663[0m
[37m[1m[2023-07-10 19:32:39,786][227910] Mean Reward across all agents: 404.20867332547647[0m
[37m[1m[2023-07-10 19:32:39,786][227910] Average Trajectory Length: 911.775[0m
[36m[2023-07-10 19:32:39,788][227910] mean_value=-2653.6607090304565, max_value=1389.2677625994897[0m
[37m[1m[2023-07-10 19:32:39,791][227910] New mean coefficients: [[ 2.6835318  -0.41027853  0.7217959   0.2881909   1.4062054 ]][0m
[37m[1m[2023-07-10 19:32:39,792][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:32:49,604][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 19:32:49,605][227910] FPS: 391399.14[0m
[36m[2023-07-10 19:32:49,607][227910] itr=1177, itrs=2000, Progress: 58.85%[0m
[36m[2023-07-10 19:33:01,247][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 19:33:01,248][227910] FPS: 330411.36[0m
[36m[2023-07-10 19:33:06,162][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:33:06,162][227910] Reward + Measures: [[5446.52150222    0.39537108    0.35666734    0.17805493    0.18115383]][0m
[37m[1m[2023-07-10 19:33:06,162][227910] Max Reward on eval: 5446.52150222209[0m
[37m[1m[2023-07-10 19:33:06,162][227910] Min Reward on eval: 5446.52150222209[0m
[37m[1m[2023-07-10 19:33:06,163][227910] Mean Reward across all agents: 5446.52150222209[0m
[37m[1m[2023-07-10 19:33:06,163][227910] Average Trajectory Length: 994.1709999999999[0m
[36m[2023-07-10 19:33:11,659][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:33:11,665][227910] Reward + Measures: [[-606.11117945    0.25150001    0.26659998    0.31030002    0.30370003]
 [-771.20728514    0.40799999    0.44440004    0.36900002    0.4197    ]
 [ 279.3921519     0.27498007    0.25219771    0.35425815    0.32568493]
 ...
 [1220.67087159    0.32409999    0.37070003    0.29169998    0.23280001]
 [-191.17823922    0.0769        0.69819993    0.57510006    0.63889998]
 [2035.73384079    0.27090001    0.33180004    0.289         0.23459999]][0m
[37m[1m[2023-07-10 19:33:11,665][227910] Max Reward on eval: 4446.480910677416[0m
[37m[1m[2023-07-10 19:33:11,666][227910] Min Reward on eval: -1401.0759981104522[0m
[37m[1m[2023-07-10 19:33:11,666][227910] Mean Reward across all agents: 1044.8717802226834[0m
[37m[1m[2023-07-10 19:33:11,666][227910] Average Trajectory Length: 980.6453333333333[0m
[36m[2023-07-10 19:33:11,668][227910] mean_value=-1958.5432122824204, max_value=134.93248327117885[0m
[37m[1m[2023-07-10 19:33:11,670][227910] New mean coefficients: [[ 2.435633   -0.6263707   0.6343196  -0.31861663  0.8270202 ]][0m
[37m[1m[2023-07-10 19:33:11,671][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:33:21,352][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 19:33:21,352][227910] FPS: 396746.75[0m
[36m[2023-07-10 19:33:21,354][227910] itr=1178, itrs=2000, Progress: 58.90%[0m
[36m[2023-07-10 19:33:32,915][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 19:33:32,915][227910] FPS: 332779.53[0m
[36m[2023-07-10 19:33:37,574][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:33:37,574][227910] Reward + Measures: [[5546.91355911    0.41988701    0.34429809    0.17805497    0.17970264]][0m
[37m[1m[2023-07-10 19:33:37,574][227910] Max Reward on eval: 5546.91355910699[0m
[37m[1m[2023-07-10 19:33:37,575][227910] Min Reward on eval: 5546.91355910699[0m
[37m[1m[2023-07-10 19:33:37,575][227910] Mean Reward across all agents: 5546.91355910699[0m
[37m[1m[2023-07-10 19:33:37,575][227910] Average Trajectory Length: 993.0716666666666[0m
[36m[2023-07-10 19:33:43,039][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:33:43,040][227910] Reward + Measures: [[ -55.59720908    0.26957843    0.23127837    0.30363244    0.28074324]
 [2219.31265037    0.29270002    0.23439999    0.24650002    0.2455    ]
 [1149.80871951    0.26199999    0.29510003    0.2703        0.22690001]
 ...
 [1437.69883064    0.3308        0.32389998    0.38610002    0.37909999]
 [-393.15516528    0.56890005    0.72099996    0.23240001    0.6728    ]
 [-392.13508274    0.27306375    0.1783037     0.22102633    0.21860695]][0m
[37m[1m[2023-07-10 19:33:43,040][227910] Max Reward on eval: 4960.220312956441[0m
[37m[1m[2023-07-10 19:33:43,040][227910] Min Reward on eval: -1388.2288054953794[0m
[37m[1m[2023-07-10 19:33:43,040][227910] Mean Reward across all agents: 965.3286058170428[0m
[37m[1m[2023-07-10 19:33:43,041][227910] Average Trajectory Length: 941.225[0m
[36m[2023-07-10 19:33:43,043][227910] mean_value=-1517.5977477619554, max_value=1410.1218636852213[0m
[37m[1m[2023-07-10 19:33:43,046][227910] New mean coefficients: [[ 2.4824915  -0.69686776  0.08216196 -0.49823993  0.4266739 ]][0m
[37m[1m[2023-07-10 19:33:43,047][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:33:52,775][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 19:33:52,776][227910] FPS: 394779.64[0m
[36m[2023-07-10 19:33:52,778][227910] itr=1179, itrs=2000, Progress: 58.95%[0m
[36m[2023-07-10 19:34:04,269][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 19:34:04,269][227910] FPS: 334714.22[0m
[36m[2023-07-10 19:34:08,959][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:34:08,964][227910] Reward + Measures: [[5664.17784164    0.39909542    0.34914404    0.17530143    0.1788623 ]][0m
[37m[1m[2023-07-10 19:34:08,964][227910] Max Reward on eval: 5664.177841636083[0m
[37m[1m[2023-07-10 19:34:08,965][227910] Min Reward on eval: 5664.177841636083[0m
[37m[1m[2023-07-10 19:34:08,965][227910] Mean Reward across all agents: 5664.177841636083[0m
[37m[1m[2023-07-10 19:34:08,965][227910] Average Trajectory Length: 995.4259999999999[0m
[36m[2023-07-10 19:34:14,404][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:34:14,410][227910] Reward + Measures: [[ 306.88946214    0.34719881    0.45823032    0.31724453    0.41199651]
 [1773.56905945    0.36049998    0.28920001    0.37280002    0.24349999]
 [4367.56000756    0.3996        0.35110003    0.21159999    0.226     ]
 ...
 [ 456.26590774    0.11236413    0.14656208    0.20917793    0.18639937]
 [  87.89623378    0.62260002    0.55120003    0.38550001    0.49019995]
 [-209.78807629    0.45030004    0.72349995    0.17110001    0.69190001]][0m
[37m[1m[2023-07-10 19:34:14,410][227910] Max Reward on eval: 4816.178533656802[0m
[37m[1m[2023-07-10 19:34:14,411][227910] Min Reward on eval: -1824.4211955758742[0m
[37m[1m[2023-07-10 19:34:14,411][227910] Mean Reward across all agents: 840.5610192828776[0m
[37m[1m[2023-07-10 19:34:14,411][227910] Average Trajectory Length: 907.1949999999999[0m
[36m[2023-07-10 19:34:14,413][227910] mean_value=-1355.8483591165304, max_value=880.7299858214919[0m
[37m[1m[2023-07-10 19:34:14,416][227910] New mean coefficients: [[ 2.3876822  -0.44383916 -0.29158118 -0.17297548  0.26343906]][0m
[37m[1m[2023-07-10 19:34:14,417][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:34:24,164][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 19:34:24,165][227910] FPS: 394014.22[0m
[36m[2023-07-10 19:34:24,167][227910] itr=1180, itrs=2000, Progress: 59.00%[0m
[37m[1m[2023-07-10 19:34:28,101][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001160[0m
[36m[2023-07-10 19:34:39,919][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 19:34:39,919][227910] FPS: 332676.50[0m
[36m[2023-07-10 19:34:44,731][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:34:44,731][227910] Reward + Measures: [[2600.65002524    0.40473357    0.34495923    0.24487394    0.28481942]][0m
[37m[1m[2023-07-10 19:34:44,732][227910] Max Reward on eval: 2600.650025244357[0m
[37m[1m[2023-07-10 19:34:44,732][227910] Min Reward on eval: 2600.650025244357[0m
[37m[1m[2023-07-10 19:34:44,732][227910] Mean Reward across all agents: 2600.650025244357[0m
[37m[1m[2023-07-10 19:34:44,732][227910] Average Trajectory Length: 964.0426666666666[0m
[36m[2023-07-10 19:34:50,224][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:34:50,225][227910] Reward + Measures: [[-446.9262048     0.39045063    0.57038486    0.25234053    0.54071903]
 [-227.08288262    0.27494588    0.31379542    0.20352827    0.32382911]
 [ 533.26162913    0.34279671    0.39887404    0.36693153    0.25592431]
 ...
 [ 295.54367337    0.18657936    0.2609776     0.2332758     0.23059464]
 [ 379.85046244    0.30519998    0.38860002    0.33539999    0.45100003]
 [1234.8045489     0.38159999    0.28760001    0.22669998    0.25549999]][0m
[37m[1m[2023-07-10 19:34:50,225][227910] Max Reward on eval: 2900.2048576394795[0m
[37m[1m[2023-07-10 19:34:50,225][227910] Min Reward on eval: -1032.764656051778[0m
[37m[1m[2023-07-10 19:34:50,225][227910] Mean Reward across all agents: 287.70543532657376[0m
[37m[1m[2023-07-10 19:34:50,226][227910] Average Trajectory Length: 949.26[0m
[36m[2023-07-10 19:34:50,228][227910] mean_value=-1548.5126988766258, max_value=1230.5442711370476[0m
[37m[1m[2023-07-10 19:34:50,230][227910] New mean coefficients: [[ 2.3905923  -0.06014481  0.6125976   0.6399029   0.91447693]][0m
[37m[1m[2023-07-10 19:34:50,231][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:34:59,986][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 19:34:59,987][227910] FPS: 393698.86[0m
[36m[2023-07-10 19:34:59,989][227910] itr=1181, itrs=2000, Progress: 59.05%[0m
[36m[2023-07-10 19:35:11,547][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 19:35:11,548][227910] FPS: 332757.32[0m
[36m[2023-07-10 19:35:16,337][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:35:16,337][227910] Reward + Measures: [[2059.91127906    0.37480068    0.29659095    0.30847621    0.27382582]][0m
[37m[1m[2023-07-10 19:35:16,338][227910] Max Reward on eval: 2059.9112790636887[0m
[37m[1m[2023-07-10 19:35:16,338][227910] Min Reward on eval: 2059.9112790636887[0m
[37m[1m[2023-07-10 19:35:16,338][227910] Mean Reward across all agents: 2059.9112790636887[0m
[37m[1m[2023-07-10 19:35:16,338][227910] Average Trajectory Length: 997.4426666666666[0m
[36m[2023-07-10 19:35:21,782][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:35:21,783][227910] Reward + Measures: [[ 386.46143103    0.1681        0.13630001    0.2168        0.16      ]
 [ 493.42383461    0.5697        0.37880003    0.39769998    0.48480001]
 [ 941.50140778    0.38499999    0.27850002    0.35499999    0.2309    ]
 ...
 [1827.73559989    0.2098        0.32230002    0.34270003    0.21720003]
 [ 622.99575982    0.73010004    0.19430001    0.62720001    0.39739999]
 [ 141.00661265    0.75580001    0.0936        0.71079999    0.50529999]][0m
[37m[1m[2023-07-10 19:35:21,783][227910] Max Reward on eval: 2044.4334619325587[0m
[37m[1m[2023-07-10 19:35:21,783][227910] Min Reward on eval: -954.6707246083067[0m
[37m[1m[2023-07-10 19:35:21,784][227910] Mean Reward across all agents: 373.8689257978338[0m
[37m[1m[2023-07-10 19:35:21,784][227910] Average Trajectory Length: 983.04[0m
[36m[2023-07-10 19:35:21,786][227910] mean_value=-1238.5380547683806, max_value=826.1963716246909[0m
[37m[1m[2023-07-10 19:35:21,789][227910] New mean coefficients: [[ 2.5547137  -0.29823968 -0.03527147  0.17494121  1.3113978 ]][0m
[37m[1m[2023-07-10 19:35:21,790][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:35:31,595][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 19:35:31,595][227910] FPS: 391687.30[0m
[36m[2023-07-10 19:35:31,598][227910] itr=1182, itrs=2000, Progress: 59.10%[0m
[36m[2023-07-10 19:35:43,108][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 19:35:43,109][227910] FPS: 334160.76[0m
[36m[2023-07-10 19:35:47,972][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:35:47,973][227910] Reward + Measures: [[2346.33453884    0.36889726    0.29001337    0.2975128     0.25673017]][0m
[37m[1m[2023-07-10 19:35:47,973][227910] Max Reward on eval: 2346.334538843831[0m
[37m[1m[2023-07-10 19:35:47,973][227910] Min Reward on eval: 2346.334538843831[0m
[37m[1m[2023-07-10 19:35:47,974][227910] Mean Reward across all agents: 2346.334538843831[0m
[37m[1m[2023-07-10 19:35:47,974][227910] Average Trajectory Length: 996.3326666666667[0m
[36m[2023-07-10 19:35:53,437][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:35:53,438][227910] Reward + Measures: [[ 325.19850249    0.36940169    0.17308463    0.34375489    0.21743345]
 [ 806.75070649    0.29560134    0.37859869    0.29133546    0.27434897]
 [  62.96600347    0.44499999    0.4594        0.42010003    0.48389998]
 ...
 [-477.18502755    0.37946805    0.6056124     0.52789658    0.6185329 ]
 [ 157.79817835    0.36722824    0.22057417    0.411856      0.25631723]
 [ 555.11804911    0.37391111    0.1881105     0.29587278    0.19785278]][0m
[37m[1m[2023-07-10 19:35:53,438][227910] Max Reward on eval: 1972.6616453308147[0m
[37m[1m[2023-07-10 19:35:53,438][227910] Min Reward on eval: -1000.6218224867713[0m
[37m[1m[2023-07-10 19:35:53,439][227910] Mean Reward across all agents: 324.5123332621664[0m
[37m[1m[2023-07-10 19:35:53,439][227910] Average Trajectory Length: 939.4459999999999[0m
[36m[2023-07-10 19:35:53,440][227910] mean_value=-1999.0318216553194, max_value=454.5297784029765[0m
[37m[1m[2023-07-10 19:35:53,443][227910] New mean coefficients: [[ 3.010542   -0.29750234 -0.7168682   0.8364105   0.840795  ]][0m
[37m[1m[2023-07-10 19:35:53,444][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:36:03,265][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 19:36:03,266][227910] FPS: 391048.70[0m
[36m[2023-07-10 19:36:03,268][227910] itr=1183, itrs=2000, Progress: 59.15%[0m
[36m[2023-07-10 19:36:14,960][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 19:36:14,961][227910] FPS: 328928.26[0m
[36m[2023-07-10 19:36:19,754][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:36:19,754][227910] Reward + Measures: [[2208.97106076    0.29791832    0.23688924    0.23758948    0.20011669]][0m
[37m[1m[2023-07-10 19:36:19,755][227910] Max Reward on eval: 2208.9710607577867[0m
[37m[1m[2023-07-10 19:36:19,755][227910] Min Reward on eval: 2208.9710607577867[0m
[37m[1m[2023-07-10 19:36:19,755][227910] Mean Reward across all agents: 2208.9710607577867[0m
[37m[1m[2023-07-10 19:36:19,755][227910] Average Trajectory Length: 962.7453333333333[0m
[36m[2023-07-10 19:36:25,351][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:36:25,357][227910] Reward + Measures: [[ 639.73887193    0.29191566    0.14342169    0.29187712    0.25298914]
 [1376.77243647    0.433         0.1276        0.31609997    0.17400001]
 [ -49.33537084    0.30364853    0.33391771    0.19883947    0.23594163]
 ...
 [ 120.01755149    0.22544883    0.16827063    0.26916143    0.2186244 ]
 [ 959.6059545     0.22693121    0.23229185    0.28925276    0.26056582]
 [ 990.97285873    0.34073025    0.29446086    0.31571242    0.22841112]][0m
[37m[1m[2023-07-10 19:36:25,357][227910] Max Reward on eval: 2571.488469968771[0m
[37m[1m[2023-07-10 19:36:25,358][227910] Min Reward on eval: -1775.7353259142023[0m
[37m[1m[2023-07-10 19:36:25,358][227910] Mean Reward across all agents: -2.939104445583028[0m
[37m[1m[2023-07-10 19:36:25,358][227910] Average Trajectory Length: 895.0636666666667[0m
[36m[2023-07-10 19:36:25,360][227910] mean_value=-1588.1055987846385, max_value=1823.2646941982289[0m
[37m[1m[2023-07-10 19:36:25,363][227910] New mean coefficients: [[ 3.1341658  -0.28170183 -0.6403518   0.98101544  0.66262925]][0m
[37m[1m[2023-07-10 19:36:25,364][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:36:35,049][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 19:36:35,049][227910] FPS: 396552.79[0m
[36m[2023-07-10 19:36:35,051][227910] itr=1184, itrs=2000, Progress: 59.20%[0m
[36m[2023-07-10 19:36:46,588][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 19:36:46,588][227910] FPS: 333375.10[0m
[36m[2023-07-10 19:36:51,317][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:36:51,323][227910] Reward + Measures: [[2387.77128027    0.29666093    0.24005343    0.23017289    0.19737764]][0m
[37m[1m[2023-07-10 19:36:51,323][227910] Max Reward on eval: 2387.7712802664723[0m
[37m[1m[2023-07-10 19:36:51,323][227910] Min Reward on eval: 2387.7712802664723[0m
[37m[1m[2023-07-10 19:36:51,323][227910] Mean Reward across all agents: 2387.7712802664723[0m
[37m[1m[2023-07-10 19:36:51,324][227910] Average Trajectory Length: 941.2613333333333[0m
[36m[2023-07-10 19:36:56,705][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:36:56,711][227910] Reward + Measures: [[ 838.48528137    0.19766636    0.24764693    0.2408057     0.22227488]
 [-779.90894684    0.73399997    0.66049999    0.36149999    0.6135    ]
 [1781.02516686    0.27723584    0.20030534    0.23689957    0.1841691 ]
 ...
 [-338.28996957    0.35190001    0.49650002    0.46370003    0.48540002]
 [-736.76840633    0.17647497    0.34498414    0.13619167    0.3352904 ]
 [ -96.46567026    0.51539999    0.44900003    0.29250002    0.37529999]][0m
[37m[1m[2023-07-10 19:36:56,711][227910] Max Reward on eval: 2806.532133718231[0m
[37m[1m[2023-07-10 19:36:56,712][227910] Min Reward on eval: -1821.4610610928619[0m
[37m[1m[2023-07-10 19:36:56,712][227910] Mean Reward across all agents: 124.34298013425592[0m
[37m[1m[2023-07-10 19:36:56,712][227910] Average Trajectory Length: 955.3666666666667[0m
[36m[2023-07-10 19:36:56,714][227910] mean_value=-1803.1111042221569, max_value=468.58173679814564[0m
[37m[1m[2023-07-10 19:36:56,716][227910] New mean coefficients: [[ 2.6365597  -0.15109593 -0.29542854  0.7076878   0.79040825]][0m
[37m[1m[2023-07-10 19:36:56,717][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:37:06,331][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 19:37:06,331][227910] FPS: 399508.15[0m
[36m[2023-07-10 19:37:06,333][227910] itr=1185, itrs=2000, Progress: 59.25%[0m
[36m[2023-07-10 19:37:17,863][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 19:37:17,864][227910] FPS: 333564.18[0m
[36m[2023-07-10 19:37:22,541][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:37:22,546][227910] Reward + Measures: [[2362.1051969     0.29237852    0.24455196    0.22140315    0.19451511]][0m
[37m[1m[2023-07-10 19:37:22,547][227910] Max Reward on eval: 2362.105196895944[0m
[37m[1m[2023-07-10 19:37:22,547][227910] Min Reward on eval: 2362.105196895944[0m
[37m[1m[2023-07-10 19:37:22,547][227910] Mean Reward across all agents: 2362.105196895944[0m
[37m[1m[2023-07-10 19:37:22,547][227910] Average Trajectory Length: 860.1963333333333[0m
[36m[2023-07-10 19:37:28,009][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:37:28,010][227910] Reward + Measures: [[1302.30591307    0.21813761    0.14146386    0.18677261    0.18120109]
 [ 783.72218659    0.40289998    0.22070001    0.4048        0.31549999]
 [ 457.6721155     0.20949383    0.09211004    0.25417551    0.15982322]
 ...
 [ -96.98061929    0.29765877    0.25040543    0.28554925    0.22255091]
 [1428.69963903    0.28122643    0.15747486    0.21921842    0.19781712]
 [3003.57383873    0.33800003    0.25010002    0.2422        0.2017    ]][0m
[37m[1m[2023-07-10 19:37:28,010][227910] Max Reward on eval: 3003.5738387307965[0m
[37m[1m[2023-07-10 19:37:28,011][227910] Min Reward on eval: -1182.391925123596[0m
[37m[1m[2023-07-10 19:37:28,011][227910] Mean Reward across all agents: 757.4556727645952[0m
[37m[1m[2023-07-10 19:37:28,011][227910] Average Trajectory Length: 951.5883333333333[0m
[36m[2023-07-10 19:37:28,012][227910] mean_value=-1739.3945162784496, max_value=559.1526452231673[0m
[37m[1m[2023-07-10 19:37:28,015][227910] New mean coefficients: [[ 3.0973172  -0.40045974  0.7504964   0.61689913  0.9296901 ]][0m
[37m[1m[2023-07-10 19:37:28,016][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:37:37,652][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 19:37:37,652][227910] FPS: 398563.09[0m
[36m[2023-07-10 19:37:37,654][227910] itr=1186, itrs=2000, Progress: 59.30%[0m
[36m[2023-07-10 19:37:49,320][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 19:37:49,320][227910] FPS: 329694.50[0m
[36m[2023-07-10 19:37:54,139][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:37:54,139][227910] Reward + Measures: [[2594.72512666    0.29206511    0.24849536    0.21441968    0.19240914]][0m
[37m[1m[2023-07-10 19:37:54,139][227910] Max Reward on eval: 2594.7251266568733[0m
[37m[1m[2023-07-10 19:37:54,139][227910] Min Reward on eval: 2594.7251266568733[0m
[37m[1m[2023-07-10 19:37:54,140][227910] Mean Reward across all agents: 2594.7251266568733[0m
[37m[1m[2023-07-10 19:37:54,140][227910] Average Trajectory Length: 855.433[0m
[36m[2023-07-10 19:37:59,548][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:37:59,549][227910] Reward + Measures: [[ -66.36621411    0.1736        0.2714        0.22000001    0.19950001]
 [ 780.19328292    0.21457878    0.1925879     0.20734484    0.13256674]
 [1517.73131009    0.27199998    0.21859999    0.2793        0.19589999]
 ...
 [1468.8825996     0.35912856    0.21896665    0.28850952    0.16238098]
 [1311.60252574    0.29025552    0.2267928     0.29296121    0.17893232]
 [1548.30467312    0.32069191    0.20191276    0.23830903    0.18255974]][0m
[37m[1m[2023-07-10 19:37:59,549][227910] Max Reward on eval: 3150.9542464755477[0m
[37m[1m[2023-07-10 19:37:59,550][227910] Min Reward on eval: -1781.2031006285456[0m
[37m[1m[2023-07-10 19:37:59,550][227910] Mean Reward across all agents: 620.442967397037[0m
[37m[1m[2023-07-10 19:37:59,550][227910] Average Trajectory Length: 935.629[0m
[36m[2023-07-10 19:37:59,552][227910] mean_value=-1966.9455876486907, max_value=1758.589381671662[0m
[37m[1m[2023-07-10 19:37:59,555][227910] New mean coefficients: [[ 2.9301584  -0.2299863  -0.40065515  0.9792092   0.629338  ]][0m
[37m[1m[2023-07-10 19:37:59,556][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:38:09,467][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 19:38:09,467][227910] FPS: 387517.82[0m
[36m[2023-07-10 19:38:09,469][227910] itr=1187, itrs=2000, Progress: 59.35%[0m
[36m[2023-07-10 19:38:21,177][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 19:38:21,177][227910] FPS: 328499.98[0m
[36m[2023-07-10 19:38:26,103][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:38:26,103][227910] Reward + Measures: [[1420.41528555    0.2823016     0.20521101    0.19777757    0.16619252]][0m
[37m[1m[2023-07-10 19:38:26,104][227910] Max Reward on eval: 1420.4152855479244[0m
[37m[1m[2023-07-10 19:38:26,104][227910] Min Reward on eval: 1420.4152855479244[0m
[37m[1m[2023-07-10 19:38:26,104][227910] Mean Reward across all agents: 1420.4152855479244[0m
[37m[1m[2023-07-10 19:38:26,104][227910] Average Trajectory Length: 892.6[0m
[36m[2023-07-10 19:38:31,622][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:38:31,623][227910] Reward + Measures: [[-212.72592537    0.16461019    0.37745532    0.25975543    0.2022575 ]
 [ 939.42156673    0.35993919    0.25186822    0.20659672    0.16331406]
 [-549.00001152    0.25018889    0.28582594    0.19959632    0.25275186]
 ...
 [ 340.71081165    0.36796698    0.26751965    0.24111937    0.17368792]
 [  68.70944549    0.29077074    0.28173268    0.14514549    0.18188956]
 [   9.27298155    0.2350111     0.53649449    0.3483898     0.38017282]][0m
[37m[1m[2023-07-10 19:38:31,623][227910] Max Reward on eval: 2298.9522906804923[0m
[37m[1m[2023-07-10 19:38:31,623][227910] Min Reward on eval: -932.3610559530207[0m
[37m[1m[2023-07-10 19:38:31,623][227910] Mean Reward across all agents: 242.92749250245532[0m
[37m[1m[2023-07-10 19:38:31,623][227910] Average Trajectory Length: 824.254[0m
[36m[2023-07-10 19:38:31,625][227910] mean_value=-2750.339118757707, max_value=339.91776790281324[0m
[37m[1m[2023-07-10 19:38:31,628][227910] New mean coefficients: [[2.7590454  0.08699621 0.84779644 1.160099   0.58253556]][0m
[37m[1m[2023-07-10 19:38:31,629][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:38:41,440][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 19:38:41,440][227910] FPS: 391442.35[0m
[36m[2023-07-10 19:38:41,443][227910] itr=1188, itrs=2000, Progress: 59.40%[0m
[36m[2023-07-10 19:38:53,060][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 19:38:53,061][227910] FPS: 331163.79[0m
[36m[2023-07-10 19:38:57,925][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:38:57,926][227910] Reward + Measures: [[1931.74455438    0.31398779    0.22925512    0.20901412    0.16927733]][0m
[37m[1m[2023-07-10 19:38:57,926][227910] Max Reward on eval: 1931.7445543771203[0m
[37m[1m[2023-07-10 19:38:57,926][227910] Min Reward on eval: 1931.7445543771203[0m
[37m[1m[2023-07-10 19:38:57,926][227910] Mean Reward across all agents: 1931.7445543771203[0m
[37m[1m[2023-07-10 19:38:57,927][227910] Average Trajectory Length: 898.036[0m
[36m[2023-07-10 19:39:03,607][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:39:03,608][227910] Reward + Measures: [[ 717.70210471    0.26789999    0.3064        0.24510001    0.20420001]
 [-261.1547684     0.29390001    0.5758        0.41359997    0.52560002]
 [ 173.01699767    0.18765076    0.31071359    0.18691808    0.20495327]
 ...
 [ 888.482974      0.28849491    0.42914125    0.22547391    0.26486462]
 [ 958.46943252    0.29969999    0.2694        0.20240001    0.23769999]
 [2465.10486151    0.46273547    0.25480646    0.26841292    0.21863733]][0m
[37m[1m[2023-07-10 19:39:03,613][227910] Max Reward on eval: 2964.767889737617[0m
[37m[1m[2023-07-10 19:39:03,613][227910] Min Reward on eval: -548.7997035464621[0m
[37m[1m[2023-07-10 19:39:03,613][227910] Mean Reward across all agents: 881.7411180341151[0m
[37m[1m[2023-07-10 19:39:03,614][227910] Average Trajectory Length: 888.0433333333333[0m
[36m[2023-07-10 19:39:03,615][227910] mean_value=-2896.072618848209, max_value=1359.3484728322821[0m
[37m[1m[2023-07-10 19:39:03,618][227910] New mean coefficients: [[ 3.1200273  -0.17001124  0.24981952  0.88181996  0.7764463 ]][0m
[37m[1m[2023-07-10 19:39:03,619][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:39:13,358][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 19:39:13,358][227910] FPS: 394382.47[0m
[36m[2023-07-10 19:39:13,360][227910] itr=1189, itrs=2000, Progress: 59.45%[0m
[36m[2023-07-10 19:39:24,841][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 19:39:24,842][227910] FPS: 335008.06[0m
[36m[2023-07-10 19:39:29,606][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:39:29,607][227910] Reward + Measures: [[2536.15184351    0.33195141    0.26331091    0.21024668    0.17176606]][0m
[37m[1m[2023-07-10 19:39:29,607][227910] Max Reward on eval: 2536.151843506546[0m
[37m[1m[2023-07-10 19:39:29,607][227910] Min Reward on eval: 2536.151843506546[0m
[37m[1m[2023-07-10 19:39:29,608][227910] Mean Reward across all agents: 2536.151843506546[0m
[37m[1m[2023-07-10 19:39:29,608][227910] Average Trajectory Length: 898.006[0m
[36m[2023-07-10 19:39:35,036][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:39:35,037][227910] Reward + Measures: [[1767.48886893    0.29946792    0.25398546    0.20754281    0.16953547]
 [1186.07412389    0.42729998    0.2251        0.35120001    0.1856    ]
 [1347.50207351    0.46778527    0.25016472    0.31919557    0.16697942]
 ...
 [1533.55460596    0.40110001    0.23899999    0.2581        0.1675    ]
 [1055.11626424    0.45796689    0.24323988    0.33339205    0.16291903]
 [ 179.00879979    0.50050002    0.24860001    0.4639        0.26320001]][0m
[37m[1m[2023-07-10 19:39:35,037][227910] Max Reward on eval: 2759.9314662630595[0m
[37m[1m[2023-07-10 19:39:35,037][227910] Min Reward on eval: -795.570018080069[0m
[37m[1m[2023-07-10 19:39:35,038][227910] Mean Reward across all agents: 1241.2091875315748[0m
[37m[1m[2023-07-10 19:39:35,038][227910] Average Trajectory Length: 902.6433333333333[0m
[36m[2023-07-10 19:39:35,039][227910] mean_value=-2383.3605374321974, max_value=727.3280153218018[0m
[37m[1m[2023-07-10 19:39:35,042][227910] New mean coefficients: [[2.7608542  0.43229008 0.7038648  1.2443986  0.59988534]][0m
[37m[1m[2023-07-10 19:39:35,043][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:39:44,708][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 19:39:44,708][227910] FPS: 397378.10[0m
[36m[2023-07-10 19:39:44,710][227910] itr=1190, itrs=2000, Progress: 59.50%[0m
[37m[1m[2023-07-10 19:39:48,506][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001170[0m
[36m[2023-07-10 19:40:00,470][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 19:40:00,470][227910] FPS: 328522.76[0m
[36m[2023-07-10 19:40:05,207][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:40:05,208][227910] Reward + Measures: [[3010.75598185    0.34090373    0.27735773    0.20328629    0.17010926]][0m
[37m[1m[2023-07-10 19:40:05,208][227910] Max Reward on eval: 3010.755981849131[0m
[37m[1m[2023-07-10 19:40:05,208][227910] Min Reward on eval: 3010.755981849131[0m
[37m[1m[2023-07-10 19:40:05,208][227910] Mean Reward across all agents: 3010.755981849131[0m
[37m[1m[2023-07-10 19:40:05,208][227910] Average Trajectory Length: 901.6926666666666[0m
[36m[2023-07-10 19:40:10,848][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:40:10,849][227910] Reward + Measures: [[2075.215672      0.37746438    0.22001128    0.20944758    0.18579833]
 [2286.89230193    0.52920246    0.21420376    0.3174502     0.25848392]
 [1834.65205997    0.33090001    0.1858        0.1946        0.14690001]
 ...
 [1128.64484009    0.61230004    0.21810003    0.40030003    0.37290001]
 [-260.19071128    0.62840003    0.59170002    0.64490002    0.77259994]
 [-318.42309329    0.67170137    0.55322307    0.65789217    0.65968299]][0m
[37m[1m[2023-07-10 19:40:10,849][227910] Max Reward on eval: 3461.985870648711[0m
[37m[1m[2023-07-10 19:40:10,849][227910] Min Reward on eval: -517.5730908062658[0m
[37m[1m[2023-07-10 19:40:10,849][227910] Mean Reward across all agents: 1504.2167019376225[0m
[37m[1m[2023-07-10 19:40:10,850][227910] Average Trajectory Length: 926.163[0m
[36m[2023-07-10 19:40:10,852][227910] mean_value=-1635.1434695738437, max_value=950.0462233887049[0m
[37m[1m[2023-07-10 19:40:10,854][227910] New mean coefficients: [[3.1540139  0.49756134 0.9741498  1.263773   2.0140204 ]][0m
[37m[1m[2023-07-10 19:40:10,855][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:40:20,625][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 19:40:20,626][227910] FPS: 393106.34[0m
[36m[2023-07-10 19:40:20,628][227910] itr=1191, itrs=2000, Progress: 59.55%[0m
[36m[2023-07-10 19:40:32,253][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 19:40:32,253][227910] FPS: 330838.24[0m
[36m[2023-07-10 19:40:37,000][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:40:37,005][227910] Reward + Measures: [[3395.14361926    0.34444955    0.28800204    0.19857527    0.17124462]][0m
[37m[1m[2023-07-10 19:40:37,005][227910] Max Reward on eval: 3395.1436192636775[0m
[37m[1m[2023-07-10 19:40:37,006][227910] Min Reward on eval: 3395.1436192636775[0m
[37m[1m[2023-07-10 19:40:37,006][227910] Mean Reward across all agents: 3395.1436192636775[0m
[37m[1m[2023-07-10 19:40:37,006][227910] Average Trajectory Length: 922.4209999999999[0m
[36m[2023-07-10 19:40:42,421][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:40:42,428][227910] Reward + Measures: [[-583.25965664    0.2314        0.48869997    0.31189999    0.2352    ]
 [-224.02624592    0.19490002    0.49529997    0.31950003    0.26019999]
 [1062.01027173    0.28740001    0.31128749    0.24068752    0.18357499]
 ...
 [-602.42449585    0.21870001    0.2067        0.13180001    0.1278    ]
 [-410.17782068    0.11610001    0.51150006    0.28850004    0.29250002]
 [1649.01297963    0.3371        0.30110002    0.2784        0.21570002]][0m
[37m[1m[2023-07-10 19:40:42,429][227910] Max Reward on eval: 3016.976905304799[0m
[37m[1m[2023-07-10 19:40:42,429][227910] Min Reward on eval: -1421.720477174688[0m
[37m[1m[2023-07-10 19:40:42,430][227910] Mean Reward across all agents: 371.2662902551831[0m
[37m[1m[2023-07-10 19:40:42,430][227910] Average Trajectory Length: 973.7199999999999[0m
[36m[2023-07-10 19:40:42,434][227910] mean_value=-2678.85351409648, max_value=874.0347511645494[0m
[37m[1m[2023-07-10 19:40:42,439][227910] New mean coefficients: [[3.0980048  0.2496829  1.3082986  0.12037325 1.6691428 ]][0m
[37m[1m[2023-07-10 19:40:42,441][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:40:52,190][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 19:40:52,190][227910] FPS: 393962.54[0m
[36m[2023-07-10 19:40:52,193][227910] itr=1192, itrs=2000, Progress: 59.60%[0m
[36m[2023-07-10 19:41:03,754][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 19:41:03,754][227910] FPS: 332764.81[0m
[36m[2023-07-10 19:41:08,556][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:41:08,561][227910] Reward + Measures: [[3741.65107057    0.34374669    0.30552176    0.18973079    0.16947837]][0m
[37m[1m[2023-07-10 19:41:08,561][227910] Max Reward on eval: 3741.651070566731[0m
[37m[1m[2023-07-10 19:41:08,562][227910] Min Reward on eval: 3741.651070566731[0m
[37m[1m[2023-07-10 19:41:08,562][227910] Mean Reward across all agents: 3741.651070566731[0m
[37m[1m[2023-07-10 19:41:08,562][227910] Average Trajectory Length: 932.8566666666667[0m
[36m[2023-07-10 19:41:13,959][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:41:13,965][227910] Reward + Measures: [[ 550.76619879    0.46003771    0.35436782    0.32927832    0.17349301]
 [ 945.92897021    0.24243765    0.21737318    0.20649374    0.16571288]
 [-553.31605466    0.11009999    0.50629997    0.33540004    0.46710005]
 ...
 [2673.2061168     0.36849642    0.26912239    0.22510321    0.18467532]
 [1043.69916014    0.2610352     0.27697712    0.30134103    0.2055283 ]
 [-702.67774173    0.0988        0.2149        0.14569999    0.19580002]][0m
[37m[1m[2023-07-10 19:41:13,966][227910] Max Reward on eval: 3421.4193030112424[0m
[37m[1m[2023-07-10 19:41:13,966][227910] Min Reward on eval: -1676.0413195160916[0m
[37m[1m[2023-07-10 19:41:13,966][227910] Mean Reward across all agents: 537.6593138109525[0m
[37m[1m[2023-07-10 19:41:13,966][227910] Average Trajectory Length: 899.2163333333333[0m
[36m[2023-07-10 19:41:13,968][227910] mean_value=-2314.924050612048, max_value=557.6472393281007[0m
[37m[1m[2023-07-10 19:41:13,971][227910] New mean coefficients: [[ 3.0315523   0.72970676  1.6958071  -0.03103629  1.6533973 ]][0m
[37m[1m[2023-07-10 19:41:13,972][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:41:23,816][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 19:41:23,817][227910] FPS: 390124.47[0m
[36m[2023-07-10 19:41:23,819][227910] itr=1193, itrs=2000, Progress: 59.65%[0m
[36m[2023-07-10 19:41:35,603][227910] train() took 11.76 seconds to complete[0m
[36m[2023-07-10 19:41:35,603][227910] FPS: 326469.76[0m
[36m[2023-07-10 19:41:40,388][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:41:40,389][227910] Reward + Measures: [[4061.30208331    0.34581071    0.30797565    0.18591498    0.16855204]][0m
[37m[1m[2023-07-10 19:41:40,389][227910] Max Reward on eval: 4061.3020833056626[0m
[37m[1m[2023-07-10 19:41:40,389][227910] Min Reward on eval: 4061.3020833056626[0m
[37m[1m[2023-07-10 19:41:40,390][227910] Mean Reward across all agents: 4061.3020833056626[0m
[37m[1m[2023-07-10 19:41:40,390][227910] Average Trajectory Length: 939.9093333333333[0m
[36m[2023-07-10 19:41:45,932][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:41:45,932][227910] Reward + Measures: [[1051.66726056    0.48179999    0.24870001    0.3193        0.23510002]
 [-402.79148942    0.24609999    0.1494        0.2122        0.13610001]
 [-405.50021784    0.4021        0.23080002    0.345         0.21850002]
 ...
 [-374.41376404    0.38635808    0.22260645    0.35565591    0.160557  ]
 [ 110.03524437    0.35880002    0.12899999    0.33980003    0.19270001]
 [-419.0778716     0.43259802    0.29530746    0.29443344    0.28491446]][0m
[37m[1m[2023-07-10 19:41:45,932][227910] Max Reward on eval: 4130.674130263203[0m
[37m[1m[2023-07-10 19:41:45,933][227910] Min Reward on eval: -1663.3771707429319[0m
[37m[1m[2023-07-10 19:41:45,933][227910] Mean Reward across all agents: 759.1915631097744[0m
[37m[1m[2023-07-10 19:41:45,933][227910] Average Trajectory Length: 969.689[0m
[36m[2023-07-10 19:41:45,935][227910] mean_value=-2620.5598740383416, max_value=1130.6947586238884[0m
[37m[1m[2023-07-10 19:41:45,938][227910] New mean coefficients: [[3.5350742  0.7379612  0.9399971  0.23057076 1.7118351 ]][0m
[37m[1m[2023-07-10 19:41:45,939][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:41:55,878][227910] train() took 9.94 seconds to complete[0m
[36m[2023-07-10 19:41:55,878][227910] FPS: 386398.07[0m
[36m[2023-07-10 19:41:55,881][227910] itr=1194, itrs=2000, Progress: 59.70%[0m
[36m[2023-07-10 19:42:07,604][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 19:42:07,604][227910] FPS: 328072.92[0m
[36m[2023-07-10 19:42:12,541][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:42:12,541][227910] Reward + Measures: [[4358.06716344    0.35116854    0.31366846    0.18340819    0.17020962]][0m
[37m[1m[2023-07-10 19:42:12,541][227910] Max Reward on eval: 4358.06716344312[0m
[37m[1m[2023-07-10 19:42:12,541][227910] Min Reward on eval: 4358.06716344312[0m
[37m[1m[2023-07-10 19:42:12,542][227910] Mean Reward across all agents: 4358.06716344312[0m
[37m[1m[2023-07-10 19:42:12,542][227910] Average Trajectory Length: 955.5709999999999[0m
[36m[2023-07-10 19:42:18,086][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:42:18,086][227910] Reward + Measures: [[3015.44644154    0.45392519    0.28850994    0.19988386    0.18495271]
 [3474.15248003    0.37554178    0.258331      0.18497813    0.16257182]
 [3614.11736331    0.45740005    0.25489998    0.22219999    0.18800001]
 ...
 [4054.84064056    0.39636207    0.30787802    0.18946978    0.1738555 ]
 [3750.83664567    0.37580001    0.27430001    0.19860001    0.17770001]
 [3606.95406712    0.38318464    0.26621282    0.20391284    0.17294104]][0m
[37m[1m[2023-07-10 19:42:18,086][227910] Max Reward on eval: 4633.890891610086[0m
[37m[1m[2023-07-10 19:42:18,087][227910] Min Reward on eval: 2140.0678000191865[0m
[37m[1m[2023-07-10 19:42:18,087][227910] Mean Reward across all agents: 3640.356692255602[0m
[37m[1m[2023-07-10 19:42:18,087][227910] Average Trajectory Length: 941.5409999999999[0m
[36m[2023-07-10 19:42:18,089][227910] mean_value=-1764.4089140426477, max_value=681.8862022351809[0m
[37m[1m[2023-07-10 19:42:18,091][227910] New mean coefficients: [[2.3026156  0.66648453 0.84685385 0.56196094 1.1176639 ]][0m
[37m[1m[2023-07-10 19:42:18,092][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:42:27,734][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 19:42:27,734][227910] FPS: 398325.68[0m
[36m[2023-07-10 19:42:27,736][227910] itr=1195, itrs=2000, Progress: 59.75%[0m
[36m[2023-07-10 19:42:39,219][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 19:42:39,219][227910] FPS: 335049.54[0m
[36m[2023-07-10 19:42:44,061][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:42:44,062][227910] Reward + Measures: [[4643.43077103    0.34696758    0.31452125    0.18090644    0.16980174]][0m
[37m[1m[2023-07-10 19:42:44,062][227910] Max Reward on eval: 4643.430771034341[0m
[37m[1m[2023-07-10 19:42:44,062][227910] Min Reward on eval: 4643.430771034341[0m
[37m[1m[2023-07-10 19:42:44,062][227910] Mean Reward across all agents: 4643.430771034341[0m
[37m[1m[2023-07-10 19:42:44,063][227910] Average Trajectory Length: 971.0743333333334[0m
[36m[2023-07-10 19:42:49,637][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:42:49,637][227910] Reward + Measures: [[1186.66881061    0.30670002    0.3037        0.2321        0.25209999]
 [ 975.55283964    0.26446623    0.14716779    0.20868981    0.128701  ]
 [2573.15029496    0.33142301    0.26387951    0.19600694    0.16770735]
 ...
 [3370.32616965    0.38411364    0.23732121    0.1875606     0.18089698]
 [3658.68560403    0.36322448    0.30623612    0.17897764    0.15810849]
 [-198.31850804    0.22379588    0.26751652    0.23254299    0.24282563]][0m
[37m[1m[2023-07-10 19:42:49,638][227910] Max Reward on eval: 4691.269351915643[0m
[37m[1m[2023-07-10 19:42:49,638][227910] Min Reward on eval: -1169.7757902777637[0m
[37m[1m[2023-07-10 19:42:49,638][227910] Mean Reward across all agents: 1710.871642024838[0m
[37m[1m[2023-07-10 19:42:49,638][227910] Average Trajectory Length: 900.7133333333333[0m
[36m[2023-07-10 19:42:49,640][227910] mean_value=-1705.3078419055262, max_value=500.10653016220294[0m
[37m[1m[2023-07-10 19:42:49,642][227910] New mean coefficients: [[ 2.1969361   0.6504409  -0.06129831  0.75682914  1.3151491 ]][0m
[37m[1m[2023-07-10 19:42:49,643][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:42:59,329][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 19:42:59,329][227910] FPS: 396535.46[0m
[36m[2023-07-10 19:42:59,331][227910] itr=1196, itrs=2000, Progress: 59.80%[0m
[36m[2023-07-10 19:43:10,806][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 19:43:10,807][227910] FPS: 335267.19[0m
[36m[2023-07-10 19:43:15,536][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:43:15,536][227910] Reward + Measures: [[4867.91204839    0.35140315    0.31317723    0.18054037    0.16987801]][0m
[37m[1m[2023-07-10 19:43:15,537][227910] Max Reward on eval: 4867.912048388532[0m
[37m[1m[2023-07-10 19:43:15,537][227910] Min Reward on eval: 4867.912048388532[0m
[37m[1m[2023-07-10 19:43:15,537][227910] Mean Reward across all agents: 4867.912048388532[0m
[37m[1m[2023-07-10 19:43:15,537][227910] Average Trajectory Length: 979.889[0m
[36m[2023-07-10 19:43:20,957][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:43:20,958][227910] Reward + Measures: [[-229.98777793    0.41350004    0.34780002    0.4395        0.58030003]
 [  44.28242536    0.2167        0.1381        0.2299        0.1833    ]
 [-221.97929853    0.30360001    0.28043333    0.27796668    0.29066667]
 ...
 [-410.23773672    0.239499      0.17529704    0.26231685    0.23274456]
 [-609.04992458    0.19145194    0.14861147    0.24934851    0.1789958 ]
 [-162.67212957    0.23782663    0.19508049    0.41007677    0.36122879]][0m
[37m[1m[2023-07-10 19:43:20,958][227910] Max Reward on eval: 4274.478597181686[0m
[37m[1m[2023-07-10 19:43:20,958][227910] Min Reward on eval: -1467.8097386040492[0m
[37m[1m[2023-07-10 19:43:20,958][227910] Mean Reward across all agents: 228.46823296427547[0m
[37m[1m[2023-07-10 19:43:20,959][227910] Average Trajectory Length: 912.4726666666667[0m
[36m[2023-07-10 19:43:20,961][227910] mean_value=-1509.0408892353996, max_value=2751.247742556455[0m
[37m[1m[2023-07-10 19:43:20,964][227910] New mean coefficients: [[1.9509    0.7965711 0.5516283 0.8544767 0.7273552]][0m
[37m[1m[2023-07-10 19:43:20,964][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:43:30,597][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 19:43:30,597][227910] FPS: 398734.23[0m
[36m[2023-07-10 19:43:30,599][227910] itr=1197, itrs=2000, Progress: 59.85%[0m
[36m[2023-07-10 19:43:42,225][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 19:43:42,225][227910] FPS: 330810.34[0m
[36m[2023-07-10 19:43:46,974][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:43:46,974][227910] Reward + Measures: [[5042.97679935    0.36875707    0.29801753    0.17922089    0.16845655]][0m
[37m[1m[2023-07-10 19:43:46,974][227910] Max Reward on eval: 5042.976799351698[0m
[37m[1m[2023-07-10 19:43:46,975][227910] Min Reward on eval: 5042.976799351698[0m
[37m[1m[2023-07-10 19:43:46,975][227910] Mean Reward across all agents: 5042.976799351698[0m
[37m[1m[2023-07-10 19:43:46,975][227910] Average Trajectory Length: 974.0413333333333[0m
[36m[2023-07-10 19:43:52,642][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:43:52,643][227910] Reward + Measures: [[ 712.39836448    0.42821065    0.3232522     0.27027455    0.23146768]
 [4086.30963276    0.37490001    0.29229999    0.20300002    0.18889999]
 [2757.95250733    0.40454611    0.3049444     0.21187399    0.20032875]
 ...
 [1852.25834766    0.39500001    0.25119999    0.2974        0.22409999]
 [-135.37047348    0.33571494    0.29447094    0.24922042    0.19426742]
 [4295.49311358    0.37860003    0.26160002    0.1859        0.1663    ]][0m
[37m[1m[2023-07-10 19:43:52,643][227910] Max Reward on eval: 5251.8543303372335[0m
[37m[1m[2023-07-10 19:43:52,644][227910] Min Reward on eval: -1397.694673905801[0m
[37m[1m[2023-07-10 19:43:52,644][227910] Mean Reward across all agents: 2818.214152262318[0m
[37m[1m[2023-07-10 19:43:52,644][227910] Average Trajectory Length: 918.1743333333333[0m
[36m[2023-07-10 19:43:52,646][227910] mean_value=-1967.048639159716, max_value=1055.8713443041952[0m
[37m[1m[2023-07-10 19:43:52,648][227910] New mean coefficients: [[2.019386   0.5990829  0.06557754 0.7810732  1.2504395 ]][0m
[37m[1m[2023-07-10 19:43:52,649][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:44:02,587][227910] train() took 9.94 seconds to complete[0m
[36m[2023-07-10 19:44:02,587][227910] FPS: 386462.30[0m
[36m[2023-07-10 19:44:02,589][227910] itr=1198, itrs=2000, Progress: 59.90%[0m
[36m[2023-07-10 19:44:14,238][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 19:44:14,238][227910] FPS: 330212.08[0m
[36m[2023-07-10 19:44:19,157][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:44:19,157][227910] Reward + Measures: [[5208.41279058    0.3667115     0.29869139    0.17800732    0.16838641]][0m
[37m[1m[2023-07-10 19:44:19,158][227910] Max Reward on eval: 5208.41279057686[0m
[37m[1m[2023-07-10 19:44:19,158][227910] Min Reward on eval: 5208.41279057686[0m
[37m[1m[2023-07-10 19:44:19,158][227910] Mean Reward across all agents: 5208.41279057686[0m
[37m[1m[2023-07-10 19:44:19,158][227910] Average Trajectory Length: 980.0649999999999[0m
[36m[2023-07-10 19:44:24,695][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:44:24,696][227910] Reward + Measures: [[2718.03847745    0.46967059    0.16575883    0.21107648    0.17462353]
 [5189.42336688    0.38837337    0.31680733    0.18219267    0.17304313]
 [2879.94240305    0.39582762    0.27926621    0.18488297    0.1763996 ]
 ...
 [1059.97992773    0.49180648    0.17813003    0.34601822    0.25709507]
 [1218.56595333    0.45635       0.13094999    0.31815001    0.23150001]
 [-229.51164194    0.33481666    0.20532222    0.35406667    0.30090556]][0m
[37m[1m[2023-07-10 19:44:24,696][227910] Max Reward on eval: 5284.088352510147[0m
[37m[1m[2023-07-10 19:44:24,696][227910] Min Reward on eval: -526.2412687281903[0m
[37m[1m[2023-07-10 19:44:24,696][227910] Mean Reward across all agents: 2739.5814582816774[0m
[37m[1m[2023-07-10 19:44:24,697][227910] Average Trajectory Length: 936.9639999999999[0m
[36m[2023-07-10 19:44:24,699][227910] mean_value=-1265.8467702150285, max_value=3537.38573051861[0m
[37m[1m[2023-07-10 19:44:24,701][227910] New mean coefficients: [[ 1.8788123  0.6333655 -0.1371201  0.6682574  1.5881491]][0m
[37m[1m[2023-07-10 19:44:24,702][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:44:34,510][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 19:44:34,511][227910] FPS: 391581.88[0m
[36m[2023-07-10 19:44:34,513][227910] itr=1199, itrs=2000, Progress: 59.95%[0m
[36m[2023-07-10 19:44:46,043][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 19:44:46,043][227910] FPS: 333568.14[0m
[36m[2023-07-10 19:44:50,716][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:44:50,716][227910] Reward + Measures: [[5340.49779578    0.35993403    0.29728171    0.17503503    0.165433  ]][0m
[37m[1m[2023-07-10 19:44:50,717][227910] Max Reward on eval: 5340.497795783757[0m
[37m[1m[2023-07-10 19:44:50,717][227910] Min Reward on eval: 5340.497795783757[0m
[37m[1m[2023-07-10 19:44:50,717][227910] Mean Reward across all agents: 5340.497795783757[0m
[37m[1m[2023-07-10 19:44:50,717][227910] Average Trajectory Length: 977.943[0m
[36m[2023-07-10 19:44:56,111][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:44:56,111][227910] Reward + Measures: [[2613.37113766    0.45210001    0.2059        0.25289997    0.1954    ]
 [3133.8608172     0.35805106    0.32456914    0.1860009     0.19092736]
 [1992.11762467    0.44029999    0.22479999    0.33699998    0.20560001]
 ...
 [ 679.40688645    0.42289996    0.39429998    0.3026        0.37330002]
 [ 655.41989716    0.46220002    0.18270001    0.46549997    0.2201    ]
 [ -51.65631263    0.37200001    0.25960001    0.45660001    0.30730003]][0m
[37m[1m[2023-07-10 19:44:56,111][227910] Max Reward on eval: 5239.158757512574[0m
[37m[1m[2023-07-10 19:44:56,112][227910] Min Reward on eval: -934.3617150012869[0m
[37m[1m[2023-07-10 19:44:56,112][227910] Mean Reward across all agents: 1560.6825101176219[0m
[37m[1m[2023-07-10 19:44:56,112][227910] Average Trajectory Length: 975.1463333333332[0m
[36m[2023-07-10 19:44:56,114][227910] mean_value=-1052.8146029231496, max_value=904.7338642119385[0m
[37m[1m[2023-07-10 19:44:56,117][227910] New mean coefficients: [[ 1.9170784   0.55044436 -0.5458838   0.6226275   1.9186382 ]][0m
[37m[1m[2023-07-10 19:44:56,118][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:45:05,858][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 19:45:05,858][227910] FPS: 394317.33[0m
[36m[2023-07-10 19:45:05,860][227910] itr=1200, itrs=2000, Progress: 60.00%[0m
[37m[1m[2023-07-10 19:45:09,830][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001180[0m
[36m[2023-07-10 19:45:21,658][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 19:45:21,659][227910] FPS: 332233.93[0m
[36m[2023-07-10 19:45:26,450][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:45:26,455][227910] Reward + Measures: [[5451.42504329    0.35900187    0.29670268    0.17193961    0.16204944]][0m
[37m[1m[2023-07-10 19:45:26,456][227910] Max Reward on eval: 5451.4250432874105[0m
[37m[1m[2023-07-10 19:45:26,456][227910] Min Reward on eval: 5451.4250432874105[0m
[37m[1m[2023-07-10 19:45:26,456][227910] Mean Reward across all agents: 5451.4250432874105[0m
[37m[1m[2023-07-10 19:45:26,456][227910] Average Trajectory Length: 973.3666666666667[0m
[36m[2023-07-10 19:45:32,046][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:45:32,052][227910] Reward + Measures: [[4587.65349892    0.44176814    0.23436315    0.17927121    0.16398406]
 [5325.59789567    0.36687207    0.33476326    0.1817935     0.17519651]
 [5493.96330788    0.40228105    0.29314798    0.17387132    0.16349956]
 ...
 [5163.17181405    0.33950001    0.352         0.184         0.17740001]
 [4599.14752328    0.48839998    0.24580002    0.1946        0.17410001]
 [5621.38506251    0.35880002    0.3071        0.1743        0.16610001]][0m
[37m[1m[2023-07-10 19:45:32,052][227910] Max Reward on eval: 5711.4865939604115[0m
[37m[1m[2023-07-10 19:45:32,052][227910] Min Reward on eval: 3222.167823089537[0m
[37m[1m[2023-07-10 19:45:32,053][227910] Mean Reward across all agents: 5106.044934175117[0m
[37m[1m[2023-07-10 19:45:32,053][227910] Average Trajectory Length: 981.286[0m
[36m[2023-07-10 19:45:32,055][227910] mean_value=-408.1596332510319, max_value=1304.036334542161[0m
[37m[1m[2023-07-10 19:45:32,058][227910] New mean coefficients: [[ 1.1017158   0.4448527  -0.16920394  0.7200065   1.3845749 ]][0m
[37m[1m[2023-07-10 19:45:32,059][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:45:42,004][227910] train() took 9.94 seconds to complete[0m
[36m[2023-07-10 19:45:42,004][227910] FPS: 386185.73[0m
[36m[2023-07-10 19:45:42,006][227910] itr=1201, itrs=2000, Progress: 60.05%[0m
[36m[2023-07-10 19:45:53,762][227910] train() took 11.74 seconds to complete[0m
[36m[2023-07-10 19:45:53,762][227910] FPS: 327179.08[0m
[36m[2023-07-10 19:45:58,580][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:45:58,581][227910] Reward + Measures: [[5541.00415684    0.36156863    0.28836185    0.1708647     0.16118382]][0m
[37m[1m[2023-07-10 19:45:58,581][227910] Max Reward on eval: 5541.004156844073[0m
[37m[1m[2023-07-10 19:45:58,581][227910] Min Reward on eval: 5541.004156844073[0m
[37m[1m[2023-07-10 19:45:58,581][227910] Mean Reward across all agents: 5541.004156844073[0m
[37m[1m[2023-07-10 19:45:58,582][227910] Average Trajectory Length: 977.4253333333334[0m
[36m[2023-07-10 19:46:04,264][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:46:04,265][227910] Reward + Measures: [[3933.42916069    0.35975003    0.30628961    0.17074758    0.16187446]
 [1673.52450784    0.35927236    0.27683085    0.17513655    0.18883578]
 [5250.02544692    0.37216085    0.29017136    0.17918181    0.16288321]
 ...
 [4200.81347438    0.34785745    0.31796232    0.16916417    0.16285676]
 [5369.60674122    0.35479999    0.3303        0.18370001    0.17910001]
 [3503.79471716    0.48435083    0.25125858    0.22519052    0.1770997 ]][0m
[37m[1m[2023-07-10 19:46:04,265][227910] Max Reward on eval: 5687.490246167965[0m
[37m[1m[2023-07-10 19:46:04,265][227910] Min Reward on eval: 216.0180195389781[0m
[37m[1m[2023-07-10 19:46:04,266][227910] Mean Reward across all agents: 3476.50340892468[0m
[37m[1m[2023-07-10 19:46:04,266][227910] Average Trajectory Length: 911.3833333333333[0m
[36m[2023-07-10 19:46:04,269][227910] mean_value=-884.8656238144165, max_value=1096.2184235996947[0m
[37m[1m[2023-07-10 19:46:04,271][227910] New mean coefficients: [[ 1.6150188   0.22941053 -1.1060451   0.9276053   1.5876467 ]][0m
[37m[1m[2023-07-10 19:46:04,272][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:46:14,174][227910] train() took 9.90 seconds to complete[0m
[36m[2023-07-10 19:46:14,174][227910] FPS: 387881.02[0m
[36m[2023-07-10 19:46:14,176][227910] itr=1202, itrs=2000, Progress: 60.10%[0m
[36m[2023-07-10 19:46:25,965][227910] train() took 11.77 seconds to complete[0m
[36m[2023-07-10 19:46:25,965][227910] FPS: 326266.26[0m
[36m[2023-07-10 19:46:30,798][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:46:30,798][227910] Reward + Measures: [[5625.08226597    0.36054593    0.28559816    0.17325196    0.16115808]][0m
[37m[1m[2023-07-10 19:46:30,798][227910] Max Reward on eval: 5625.08226597175[0m
[37m[1m[2023-07-10 19:46:30,798][227910] Min Reward on eval: 5625.08226597175[0m
[37m[1m[2023-07-10 19:46:30,799][227910] Mean Reward across all agents: 5625.08226597175[0m
[37m[1m[2023-07-10 19:46:30,799][227910] Average Trajectory Length: 981.4663333333333[0m
[36m[2023-07-10 19:46:36,305][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:46:36,306][227910] Reward + Measures: [[ 254.18632654    0.53740001    0.1692        0.4068        0.28870001]
 [1507.35043329    0.4206        0.1153        0.2647        0.20150001]
 [5005.37011642    0.37760001    0.29169998    0.19260001    0.17730001]
 ...
 [5190.11064425    0.31599998    0.3211        0.17330001    0.1646    ]
 [-832.41234724    0.32064912    0.16590333    0.34288552    0.22223234]
 [ 225.82714255    0.35250002    0.14289999    0.27760002    0.2254    ]][0m
[37m[1m[2023-07-10 19:46:36,306][227910] Max Reward on eval: 5814.644222961599[0m
[37m[1m[2023-07-10 19:46:36,306][227910] Min Reward on eval: -1104.942042573332[0m
[37m[1m[2023-07-10 19:46:36,307][227910] Mean Reward across all agents: 1412.9007903988459[0m
[37m[1m[2023-07-10 19:46:36,307][227910] Average Trajectory Length: 967.7326666666667[0m
[36m[2023-07-10 19:46:36,309][227910] mean_value=-1747.3553256641042, max_value=1715.1570549238065[0m
[37m[1m[2023-07-10 19:46:36,311][227910] New mean coefficients: [[ 1.3421292   0.27702242 -0.54619294  0.9572087   1.4046507 ]][0m
[37m[1m[2023-07-10 19:46:36,312][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:46:46,051][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 19:46:46,052][227910] FPS: 394363.04[0m
[36m[2023-07-10 19:46:46,054][227910] itr=1203, itrs=2000, Progress: 60.15%[0m
[36m[2023-07-10 19:46:57,729][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 19:46:57,729][227910] FPS: 329518.42[0m
[36m[2023-07-10 19:47:02,480][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:47:02,480][227910] Reward + Measures: [[5756.59218314    0.35548043    0.27961808    0.17365789    0.16294451]][0m
[37m[1m[2023-07-10 19:47:02,480][227910] Max Reward on eval: 5756.592183142849[0m
[37m[1m[2023-07-10 19:47:02,481][227910] Min Reward on eval: 5756.592183142849[0m
[37m[1m[2023-07-10 19:47:02,481][227910] Mean Reward across all agents: 5756.592183142849[0m
[37m[1m[2023-07-10 19:47:02,481][227910] Average Trajectory Length: 986.25[0m
[36m[2023-07-10 19:47:07,952][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:47:07,958][227910] Reward + Measures: [[ 962.45076154    0.55839998    0.2307        0.45809999    0.2395    ]
 [5373.75544915    0.45930001    0.22250001    0.1823        0.16620003]
 [4500.62928691    0.42129999    0.20280002    0.1876        0.1681    ]
 ...
 [4080.74565773    0.43575427    0.22709182    0.2039461     0.17982973]
 [2988.13155801    0.48853698    0.22229667    0.25094455    0.1939659 ]
 [4803.56813256    0.42840001    0.24120001    0.1997        0.17569999]][0m
[37m[1m[2023-07-10 19:47:07,958][227910] Max Reward on eval: 5849.162251737854[0m
[37m[1m[2023-07-10 19:47:07,959][227910] Min Reward on eval: 139.6992717138841[0m
[37m[1m[2023-07-10 19:47:07,959][227910] Mean Reward across all agents: 3688.066583384867[0m
[37m[1m[2023-07-10 19:47:07,959][227910] Average Trajectory Length: 970.1236666666666[0m
[36m[2023-07-10 19:47:07,962][227910] mean_value=-859.967794122964, max_value=1563.2369097637861[0m
[37m[1m[2023-07-10 19:47:07,964][227910] New mean coefficients: [[ 1.570734    0.132375   -0.51460963  0.36284357  1.0144928 ]][0m
[37m[1m[2023-07-10 19:47:07,965][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:47:17,705][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 19:47:17,705][227910] FPS: 394321.76[0m
[36m[2023-07-10 19:47:17,707][227910] itr=1204, itrs=2000, Progress: 60.20%[0m
[36m[2023-07-10 19:47:29,523][227910] train() took 11.79 seconds to complete[0m
[36m[2023-07-10 19:47:29,523][227910] FPS: 325601.00[0m
[36m[2023-07-10 19:47:34,424][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:47:34,429][227910] Reward + Measures: [[5822.82354796    0.36552542    0.2767522     0.17266369    0.16017929]][0m
[37m[1m[2023-07-10 19:47:34,430][227910] Max Reward on eval: 5822.823547958272[0m
[37m[1m[2023-07-10 19:47:34,430][227910] Min Reward on eval: 5822.823547958272[0m
[37m[1m[2023-07-10 19:47:34,430][227910] Mean Reward across all agents: 5822.823547958272[0m
[37m[1m[2023-07-10 19:47:34,430][227910] Average Trajectory Length: 988.9413333333333[0m
[36m[2023-07-10 19:47:39,885][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:47:39,891][227910] Reward + Measures: [[2322.23553612    0.38021109    0.24940422    0.23253563    0.17917328]
 [1024.30350025    0.18509999    0.39500004    0.31560001    0.308     ]
 [2346.29053117    0.36430001    0.30899999    0.25120002    0.23510002]
 ...
 [2566.06652316    0.28099999    0.3448        0.24820001    0.24070001]
 [2141.06706377    0.3362        0.38140002    0.27019998    0.25750002]
 [ 931.62852497    0.18669999    0.37259999    0.2983        0.31119999]][0m
[37m[1m[2023-07-10 19:47:39,892][227910] Max Reward on eval: 5711.51778935279[0m
[37m[1m[2023-07-10 19:47:39,893][227910] Min Reward on eval: -1188.829155822983[0m
[37m[1m[2023-07-10 19:47:39,894][227910] Mean Reward across all agents: 1414.7341312630142[0m
[37m[1m[2023-07-10 19:47:39,894][227910] Average Trajectory Length: 959.4516666666666[0m
[36m[2023-07-10 19:47:39,898][227910] mean_value=-1655.9075504037635, max_value=1625.7526250357387[0m
[37m[1m[2023-07-10 19:47:39,903][227910] New mean coefficients: [[ 1.2360168   0.09314312 -0.42449355  0.37074828  0.44515347]][0m
[37m[1m[2023-07-10 19:47:39,905][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:47:49,622][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 19:47:49,622][227910] FPS: 395283.92[0m
[36m[2023-07-10 19:47:49,624][227910] itr=1205, itrs=2000, Progress: 60.25%[0m
[36m[2023-07-10 19:48:01,241][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 19:48:01,241][227910] FPS: 331065.00[0m
[36m[2023-07-10 19:48:06,053][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:48:06,053][227910] Reward + Measures: [[5909.53697904    0.36370295    0.27017695    0.17096122    0.15877591]][0m
[37m[1m[2023-07-10 19:48:06,054][227910] Max Reward on eval: 5909.536979035847[0m
[37m[1m[2023-07-10 19:48:06,054][227910] Min Reward on eval: 5909.536979035847[0m
[37m[1m[2023-07-10 19:48:06,054][227910] Mean Reward across all agents: 5909.536979035847[0m
[37m[1m[2023-07-10 19:48:06,054][227910] Average Trajectory Length: 984.2186666666666[0m
[36m[2023-07-10 19:48:11,591][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:48:11,592][227910] Reward + Measures: [[3473.70740961    0.4316        0.35609999    0.23480001    0.2412    ]
 [1241.89204008    0.52509999    0.15710001    0.32969996    0.1567    ]
 [1061.3860133     0.31890002    0.36600003    0.34299999    0.3213    ]
 ...
 [-631.75438555    0.16340001    0.36910003    0.28639999    0.38249999]
 [4802.61145762    0.35956153    0.31136197    0.17544936    0.16705583]
 [1380.33530418    0.35555947    0.17095061    0.20238034    0.14241603]][0m
[37m[1m[2023-07-10 19:48:11,592][227910] Max Reward on eval: 6040.948727120226[0m
[37m[1m[2023-07-10 19:48:11,592][227910] Min Reward on eval: -799.9704685796926[0m
[37m[1m[2023-07-10 19:48:11,593][227910] Mean Reward across all agents: 1755.3382637343657[0m
[37m[1m[2023-07-10 19:48:11,593][227910] Average Trajectory Length: 910.0509999999999[0m
[36m[2023-07-10 19:48:11,595][227910] mean_value=-1653.8156317452251, max_value=1230.969603767343[0m
[37m[1m[2023-07-10 19:48:11,597][227910] New mean coefficients: [[ 1.0755767   0.09045817 -0.41576543 -0.02629986  0.20008641]][0m
[37m[1m[2023-07-10 19:48:11,598][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:48:21,310][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 19:48:21,310][227910] FPS: 395473.96[0m
[36m[2023-07-10 19:48:21,312][227910] itr=1206, itrs=2000, Progress: 60.30%[0m
[36m[2023-07-10 19:48:32,835][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 19:48:32,835][227910] FPS: 333788.56[0m
[36m[2023-07-10 19:48:37,656][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:48:37,656][227910] Reward + Measures: [[6075.38933942    0.35892317    0.27491158    0.17046817    0.15837325]][0m
[37m[1m[2023-07-10 19:48:37,657][227910] Max Reward on eval: 6075.389339424149[0m
[37m[1m[2023-07-10 19:48:37,657][227910] Min Reward on eval: 6075.389339424149[0m
[37m[1m[2023-07-10 19:48:37,657][227910] Mean Reward across all agents: 6075.389339424149[0m
[37m[1m[2023-07-10 19:48:37,657][227910] Average Trajectory Length: 990.6096666666666[0m
[36m[2023-07-10 19:48:43,282][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:48:43,282][227910] Reward + Measures: [[ 145.34508258    0.66361749    0.21462487    0.54799664    0.31254128]
 [ 201.07503873    0.11720001    0.57549995    0.4725        0.45749998]
 [-559.54092918    0.61080003    0.36489999    0.48249999    0.39720002]
 ...
 [3053.24733149    0.2868        0.33570001    0.21170001    0.20190001]
 [ 225.26932047    0.26396391    0.33108935    0.16784699    0.24225836]
 [3412.33351983    0.26463851    0.28175229    0.23681553    0.19940479]][0m
[37m[1m[2023-07-10 19:48:43,283][227910] Max Reward on eval: 6200.39034138408[0m
[37m[1m[2023-07-10 19:48:43,283][227910] Min Reward on eval: -1511.6122613197542[0m
[37m[1m[2023-07-10 19:48:43,283][227910] Mean Reward across all agents: 1402.4034836451438[0m
[37m[1m[2023-07-10 19:48:43,283][227910] Average Trajectory Length: 913.1856666666666[0m
[36m[2023-07-10 19:48:43,286][227910] mean_value=-1598.0543178557343, max_value=2020.0409370221576[0m
[37m[1m[2023-07-10 19:48:43,289][227910] New mean coefficients: [[ 1.1490252   0.10157731 -0.39766058  0.40222782  0.31504068]][0m
[37m[1m[2023-07-10 19:48:43,290][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:48:52,975][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 19:48:52,975][227910] FPS: 396553.05[0m
[36m[2023-07-10 19:48:52,978][227910] itr=1207, itrs=2000, Progress: 60.35%[0m
[36m[2023-07-10 19:49:04,487][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 19:49:04,488][227910] FPS: 334265.60[0m
[36m[2023-07-10 19:49:09,233][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:49:09,234][227910] Reward + Measures: [[6123.13530712    0.35482943    0.26692119    0.16745648    0.15613262]][0m
[37m[1m[2023-07-10 19:49:09,234][227910] Max Reward on eval: 6123.1353071195335[0m
[37m[1m[2023-07-10 19:49:09,234][227910] Min Reward on eval: 6123.1353071195335[0m
[37m[1m[2023-07-10 19:49:09,234][227910] Mean Reward across all agents: 6123.1353071195335[0m
[37m[1m[2023-07-10 19:49:09,235][227910] Average Trajectory Length: 989.1553333333333[0m
[36m[2023-07-10 19:49:14,651][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:49:14,657][227910] Reward + Measures: [[-120.73313457    0.1024        0.48050004    0.34310001    0.52150005]
 [ 355.03791122    0.39660001    0.3087        0.36450002    0.42910001]
 [ 602.28891452    0.21573579    0.50762033    0.34818131    0.5751    ]
 ...
 [5547.72129614    0.40884051    0.24807291    0.18242261    0.16348229]
 [ 423.04316878    0.4028824     0.34528825    0.35681769    0.51280588]
 [2692.89394267    0.29094684    0.21073022    0.18994346    0.15100074]][0m
[37m[1m[2023-07-10 19:49:14,657][227910] Max Reward on eval: 6039.0619788909335[0m
[37m[1m[2023-07-10 19:49:14,658][227910] Min Reward on eval: -858.6451412257854[0m
[37m[1m[2023-07-10 19:49:14,658][227910] Mean Reward across all agents: 2116.3967500127105[0m
[37m[1m[2023-07-10 19:49:14,658][227910] Average Trajectory Length: 961.9743333333333[0m
[36m[2023-07-10 19:49:14,662][227910] mean_value=-632.8055068916648, max_value=2851.7741066042363[0m
[37m[1m[2023-07-10 19:49:14,665][227910] New mean coefficients: [[ 1.026062    0.17937988 -0.18375877  0.6249675   0.39355016]][0m
[37m[1m[2023-07-10 19:49:14,666][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:49:24,377][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 19:49:24,377][227910] FPS: 395476.32[0m
[36m[2023-07-10 19:49:24,380][227910] itr=1208, itrs=2000, Progress: 60.40%[0m
[36m[2023-07-10 19:49:35,827][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 19:49:35,827][227910] FPS: 336015.59[0m
[36m[2023-07-10 19:49:40,541][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:49:40,542][227910] Reward + Measures: [[6175.47441057    0.33782968    0.27220556    0.16494407    0.15509976]][0m
[37m[1m[2023-07-10 19:49:40,542][227910] Max Reward on eval: 6175.474410571708[0m
[37m[1m[2023-07-10 19:49:40,542][227910] Min Reward on eval: 6175.474410571708[0m
[37m[1m[2023-07-10 19:49:40,542][227910] Mean Reward across all agents: 6175.474410571708[0m
[37m[1m[2023-07-10 19:49:40,542][227910] Average Trajectory Length: 989.1933333333333[0m
[36m[2023-07-10 19:49:46,032][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:49:46,038][227910] Reward + Measures: [[2884.48204682    0.32485148    0.24487031    0.2052933     0.21010776]
 [2489.7896915     0.31140003    0.23190001    0.19860001    0.20279999]
 [-636.29269455    0.70600003    0.70170003    0.0983        0.74270004]
 ...
 [1702.86246246    0.19635358    0.19014065    0.23557369    0.16946813]
 [3480.77582048    0.31498292    0.24031126    0.17073567    0.15015128]
 [4144.26479838    0.27868479    0.24896224    0.17908333    0.17183535]][0m
[37m[1m[2023-07-10 19:49:46,038][227910] Max Reward on eval: 6186.432498228923[0m
[37m[1m[2023-07-10 19:49:46,039][227910] Min Reward on eval: -1049.0063589351018[0m
[37m[1m[2023-07-10 19:49:46,039][227910] Mean Reward across all agents: 2797.876427423566[0m
[37m[1m[2023-07-10 19:49:46,039][227910] Average Trajectory Length: 908.4616666666666[0m
[36m[2023-07-10 19:49:46,042][227910] mean_value=-912.3216477183344, max_value=1396.147092625697[0m
[37m[1m[2023-07-10 19:49:46,045][227910] New mean coefficients: [[ 0.88285327  0.20476162 -0.34640697  0.40395752  0.22346483]][0m
[37m[1m[2023-07-10 19:49:46,046][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:49:55,727][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 19:49:55,727][227910] FPS: 396704.60[0m
[36m[2023-07-10 19:49:55,729][227910] itr=1209, itrs=2000, Progress: 60.45%[0m
[36m[2023-07-10 19:50:07,180][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 19:50:07,180][227910] FPS: 335880.51[0m
[36m[2023-07-10 19:50:11,909][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:50:11,909][227910] Reward + Measures: [[6125.1260465     0.34576219    0.26038304    0.16430575    0.15440601]][0m
[37m[1m[2023-07-10 19:50:11,909][227910] Max Reward on eval: 6125.126046502343[0m
[37m[1m[2023-07-10 19:50:11,909][227910] Min Reward on eval: 6125.126046502343[0m
[37m[1m[2023-07-10 19:50:11,909][227910] Mean Reward across all agents: 6125.126046502343[0m
[37m[1m[2023-07-10 19:50:11,910][227910] Average Trajectory Length: 983.328[0m
[36m[2023-07-10 19:50:17,414][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:50:17,415][227910] Reward + Measures: [[5718.66298161    0.29959998    0.33649999    0.1772        0.178     ]
 [1985.59212631    0.34439999    0.29820001    0.31430003    0.31710002]
 [2669.11212093    0.34209999    0.29659998    0.26869997    0.27760002]
 ...
 [2538.17945492    0.43433294    0.15170979    0.19614242    0.14753087]
 [4247.09742977    0.30590001    0.33759999    0.23460002    0.24129999]
 [2736.52786345    0.34459218    0.29371214    0.25904247    0.26561719]][0m
[37m[1m[2023-07-10 19:50:17,415][227910] Max Reward on eval: 6349.747722679609[0m
[37m[1m[2023-07-10 19:50:17,415][227910] Min Reward on eval: -952.9391115615726[0m
[37m[1m[2023-07-10 19:50:17,415][227910] Mean Reward across all agents: 3491.496436006197[0m
[37m[1m[2023-07-10 19:50:17,415][227910] Average Trajectory Length: 975.5939999999999[0m
[36m[2023-07-10 19:50:17,419][227910] mean_value=-625.8697520627829, max_value=2682.964162410583[0m
[37m[1m[2023-07-10 19:50:17,421][227910] New mean coefficients: [[0.33152956 0.35895902 0.13641927 0.2721936  0.09507279]][0m
[37m[1m[2023-07-10 19:50:17,422][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:50:27,112][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 19:50:27,112][227910] FPS: 396345.70[0m
[36m[2023-07-10 19:50:27,115][227910] itr=1210, itrs=2000, Progress: 60.50%[0m
[37m[1m[2023-07-10 19:50:31,079][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001190[0m
[36m[2023-07-10 19:50:42,838][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 19:50:42,838][227910] FPS: 334323.39[0m
[36m[2023-07-10 19:50:47,719][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:50:47,719][227910] Reward + Measures: [[4553.70178627    0.47291541    0.17695975    0.20390838    0.1835863 ]][0m
[37m[1m[2023-07-10 19:50:47,720][227910] Max Reward on eval: 4553.7017862703615[0m
[37m[1m[2023-07-10 19:50:47,720][227910] Min Reward on eval: 4553.7017862703615[0m
[37m[1m[2023-07-10 19:50:47,720][227910] Mean Reward across all agents: 4553.7017862703615[0m
[37m[1m[2023-07-10 19:50:47,720][227910] Average Trajectory Length: 985.4639999999999[0m
[36m[2023-07-10 19:50:53,184][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:50:53,185][227910] Reward + Measures: [[3136.37686467    0.56184733    0.14118294    0.24013412    0.18485053]
 [2908.83645251    0.4887        0.1717        0.24990001    0.18699999]
 [2434.84835887    0.26242989    0.30632931    0.2511254     0.19200276]
 ...
 [ 897.52694304    0.39570001    0.11180001    0.34940001    0.1813    ]
 [ 712.05399507    0.33212632    0.11019474    0.28840002    0.16532105]
 [5316.83389431    0.42339998    0.21210001    0.19530001    0.18359999]][0m
[37m[1m[2023-07-10 19:50:53,185][227910] Max Reward on eval: 5509.471089443565[0m
[37m[1m[2023-07-10 19:50:53,186][227910] Min Reward on eval: -302.79940217823605[0m
[37m[1m[2023-07-10 19:50:53,186][227910] Mean Reward across all agents: 2362.0290671822436[0m
[37m[1m[2023-07-10 19:50:53,186][227910] Average Trajectory Length: 960.9663333333333[0m
[36m[2023-07-10 19:50:53,189][227910] mean_value=-1184.8682884482585, max_value=1857.422463271395[0m
[37m[1m[2023-07-10 19:50:53,191][227910] New mean coefficients: [[-0.04507852  0.3366913   0.44791654 -0.29455042  0.20053944]][0m
[37m[1m[2023-07-10 19:50:53,192][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:51:02,854][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 19:51:02,854][227910] FPS: 397533.08[0m
[36m[2023-07-10 19:51:02,856][227910] itr=1211, itrs=2000, Progress: 60.55%[0m
[36m[2023-07-10 19:51:14,394][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 19:51:14,394][227910] FPS: 333344.86[0m
[36m[2023-07-10 19:51:19,123][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:51:19,124][227910] Reward + Measures: [[4739.12680226    0.49560067    0.17990552    0.20333266    0.18368168]][0m
[37m[1m[2023-07-10 19:51:19,124][227910] Max Reward on eval: 4739.126802256993[0m
[37m[1m[2023-07-10 19:51:19,124][227910] Min Reward on eval: 4739.126802256993[0m
[37m[1m[2023-07-10 19:51:19,124][227910] Mean Reward across all agents: 4739.126802256993[0m
[37m[1m[2023-07-10 19:51:19,125][227910] Average Trajectory Length: 988.459[0m
[36m[2023-07-10 19:51:24,551][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:51:24,552][227910] Reward + Measures: [[4282.87162621    0.49675187    0.17446952    0.2113587     0.17964357]
 [4363.81393378    0.4745        0.1689        0.2027        0.18380001]
 [4867.43202438    0.45140001    0.1779        0.18360001    0.16759999]
 ...
 [4740.31733381    0.47890002    0.1776        0.2057        0.18050002]
 [4660.95195184    0.50369996    0.18200003    0.21560001    0.19250001]
 [4807.01724826    0.52969998    0.17050001    0.21280003    0.1902    ]][0m
[37m[1m[2023-07-10 19:51:24,552][227910] Max Reward on eval: 5693.257140575163[0m
[37m[1m[2023-07-10 19:51:24,552][227910] Min Reward on eval: 1924.3054839338176[0m
[37m[1m[2023-07-10 19:51:24,553][227910] Mean Reward across all agents: 4468.36508444601[0m
[37m[1m[2023-07-10 19:51:24,553][227910] Average Trajectory Length: 977.534[0m
[36m[2023-07-10 19:51:24,558][227910] mean_value=406.6842476965943, max_value=2462.745796461252[0m
[37m[1m[2023-07-10 19:51:24,560][227910] New mean coefficients: [[-0.4246113   0.60897565  0.19370931 -0.0471133   0.12230594]][0m
[37m[1m[2023-07-10 19:51:24,561][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:51:34,274][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 19:51:34,274][227910] FPS: 395436.22[0m
[36m[2023-07-10 19:51:34,276][227910] itr=1212, itrs=2000, Progress: 60.60%[0m
[36m[2023-07-10 19:51:45,907][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 19:51:45,907][227910] FPS: 330685.54[0m
[36m[2023-07-10 19:51:50,618][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:51:50,618][227910] Reward + Measures: [[72.13320782  0.32792619  0.2932663   0.371712    0.41023767]][0m
[37m[1m[2023-07-10 19:51:50,618][227910] Max Reward on eval: 72.1332078153043[0m
[37m[1m[2023-07-10 19:51:50,619][227910] Min Reward on eval: 72.1332078153043[0m
[37m[1m[2023-07-10 19:51:50,619][227910] Mean Reward across all agents: 72.1332078153043[0m
[37m[1m[2023-07-10 19:51:50,619][227910] Average Trajectory Length: 992.6703333333332[0m
[36m[2023-07-10 19:51:56,092][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:51:56,093][227910] Reward + Measures: [[ -52.16615304    0.28452906    0.12159373    0.32161784    0.23089986]
 [1724.54334926    0.43379998    0.23769999    0.27310002    0.24650002]
 [ -56.97602311    0.237298      0.16379897    0.29433438    0.26467475]
 ...
 [-185.9293999     0.19949999    0.86070007    0.61730003    0.84740001]
 [ 368.06594073    0.35020003    0.3409        0.50629997    0.36450002]
 [-295.02160024    0.2600958     0.0900045     0.35758662    0.23781109]][0m
[37m[1m[2023-07-10 19:51:56,093][227910] Max Reward on eval: 2049.181481596769[0m
[37m[1m[2023-07-10 19:51:56,093][227910] Min Reward on eval: -904.6742468027165[0m
[37m[1m[2023-07-10 19:51:56,094][227910] Mean Reward across all agents: 75.54853978336591[0m
[37m[1m[2023-07-10 19:51:56,094][227910] Average Trajectory Length: 965.5356666666667[0m
[36m[2023-07-10 19:51:56,096][227910] mean_value=-871.6454133430541, max_value=1129.3571493792936[0m
[37m[1m[2023-07-10 19:51:56,099][227910] New mean coefficients: [[0.10187265 0.68242866 0.28948164 0.05241612 0.36297894]][0m
[37m[1m[2023-07-10 19:51:56,100][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:52:05,840][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 19:52:05,840][227910] FPS: 394293.45[0m
[36m[2023-07-10 19:52:05,843][227910] itr=1213, itrs=2000, Progress: 60.65%[0m
[36m[2023-07-10 19:52:17,469][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 19:52:17,469][227910] FPS: 330812.45[0m
[36m[2023-07-10 19:52:22,193][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:52:22,194][227910] Reward + Measures: [[-11.5749969    0.32230008   0.40506572   0.39111823   0.52101976]][0m
[37m[1m[2023-07-10 19:52:22,194][227910] Max Reward on eval: -11.574996904397134[0m
[37m[1m[2023-07-10 19:52:22,194][227910] Min Reward on eval: -11.574996904397134[0m
[37m[1m[2023-07-10 19:52:22,194][227910] Mean Reward across all agents: -11.574996904397134[0m
[37m[1m[2023-07-10 19:52:22,195][227910] Average Trajectory Length: 995.2576666666666[0m
[36m[2023-07-10 19:52:27,812][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:52:27,813][227910] Reward + Measures: [[ 223.20451464    0.39829999    0.40229997    0.454         0.51700002]
 [ -15.08557413    0.36849999    0.42550001    0.38939998    0.54280001]
 [ -22.35584558    0.26100001    0.43449998    0.36549997    0.51899999]
 ...
 [ -45.69265827    0.31010002    0.51300001    0.44819999    0.62080002]
 [-166.4892336     0.18349999    0.46900007    0.375         0.5223    ]
 [   5.466499      0.26820001    0.47040001    0.41600004    0.50030005]][0m
[37m[1m[2023-07-10 19:52:27,813][227910] Max Reward on eval: 395.9837125154969[0m
[37m[1m[2023-07-10 19:52:27,813][227910] Min Reward on eval: -213.18526885753963[0m
[37m[1m[2023-07-10 19:52:27,814][227910] Mean Reward across all agents: 13.021533792552157[0m
[37m[1m[2023-07-10 19:52:27,814][227910] Average Trajectory Length: 996.1726666666666[0m
[36m[2023-07-10 19:52:27,815][227910] mean_value=-809.077009639961, max_value=30.939901654584844[0m
[37m[1m[2023-07-10 19:52:27,818][227910] New mean coefficients: [[-0.36607262  0.70948035  0.63615566  0.15102917 -0.05337751]][0m
[37m[1m[2023-07-10 19:52:27,819][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:52:37,625][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 19:52:37,625][227910] FPS: 391649.35[0m
[36m[2023-07-10 19:52:37,627][227910] itr=1214, itrs=2000, Progress: 60.70%[0m
[36m[2023-07-10 19:52:49,172][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 19:52:49,172][227910] FPS: 333197.15[0m
[36m[2023-07-10 19:52:53,887][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:52:53,888][227910] Reward + Measures: [[-112.84699174    0.28573608    0.53911972    0.4554987     0.61426497]][0m
[37m[1m[2023-07-10 19:52:53,888][227910] Max Reward on eval: -112.84699174216883[0m
[37m[1m[2023-07-10 19:52:53,888][227910] Min Reward on eval: -112.84699174216883[0m
[37m[1m[2023-07-10 19:52:53,888][227910] Mean Reward across all agents: -112.84699174216883[0m
[37m[1m[2023-07-10 19:52:53,889][227910] Average Trajectory Length: 995.8439999999999[0m
[36m[2023-07-10 19:52:59,369][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:52:59,370][227910] Reward + Measures: [[1289.22261443    0.35309997    0.19930001    0.31200001    0.1893    ]
 [ 255.48379307    0.55970001    0.127         0.51210004    0.30270001]
 [1569.34950911    0.40489998    0.22379999    0.34500003    0.25640002]
 ...
 [1229.01402689    0.42010003    0.1781        0.3348        0.24680002]
 [-416.48899422    0.66950005    0.7346999     0.0862        0.75960004]
 [ 292.7450044     0.42930004    0.37420002    0.2323        0.41820002]][0m
[37m[1m[2023-07-10 19:52:59,370][227910] Max Reward on eval: 2103.239958242979[0m
[37m[1m[2023-07-10 19:52:59,370][227910] Min Reward on eval: -781.0545149613405[0m
[37m[1m[2023-07-10 19:52:59,370][227910] Mean Reward across all agents: 469.1481140560494[0m
[37m[1m[2023-07-10 19:52:59,371][227910] Average Trajectory Length: 978.9206666666666[0m
[36m[2023-07-10 19:52:59,373][227910] mean_value=-1876.6917339910567, max_value=550.8601228621403[0m
[37m[1m[2023-07-10 19:52:59,375][227910] New mean coefficients: [[-0.41688594  0.595682   -0.03099906  0.20508859  0.32417053]][0m
[37m[1m[2023-07-10 19:52:59,376][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:53:09,072][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 19:53:09,072][227910] FPS: 396116.40[0m
[36m[2023-07-10 19:53:09,075][227910] itr=1215, itrs=2000, Progress: 60.75%[0m
[36m[2023-07-10 19:53:20,592][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 19:53:20,592][227910] FPS: 334018.20[0m
[36m[2023-07-10 19:53:25,364][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:53:25,365][227910] Reward + Measures: [[-193.89630529    0.26865715    0.61645651    0.51234525    0.68076116]][0m
[37m[1m[2023-07-10 19:53:25,365][227910] Max Reward on eval: -193.89630529357467[0m
[37m[1m[2023-07-10 19:53:25,365][227910] Min Reward on eval: -193.89630529357467[0m
[37m[1m[2023-07-10 19:53:25,365][227910] Mean Reward across all agents: -193.89630529357467[0m
[37m[1m[2023-07-10 19:53:25,366][227910] Average Trajectory Length: 997.4576666666667[0m
[36m[2023-07-10 19:53:30,792][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:53:30,798][227910] Reward + Measures: [[ 392.3594118     0.32999998    0.30899999    0.46680003    0.3813    ]
 [-147.32834586    0.32099998    0.64180005    0.58270001    0.69929999]
 [1161.56720287    0.4034        0.26640001    0.39499998    0.27330002]
 ...
 [  43.64318694    0.57870007    0.3037        0.51449996    0.41840002]
 [ -81.1341343     0.39830002    0.5183        0.57679999    0.60300004]
 [ -17.38462757    0.3337        0.37660003    0.37210003    0.46919999]][0m
[37m[1m[2023-07-10 19:53:30,798][227910] Max Reward on eval: 1260.6607401433807[0m
[37m[1m[2023-07-10 19:53:30,798][227910] Min Reward on eval: -476.1768434493686[0m
[37m[1m[2023-07-10 19:53:30,799][227910] Mean Reward across all agents: 85.18180946773577[0m
[37m[1m[2023-07-10 19:53:30,799][227910] Average Trajectory Length: 997.8083333333333[0m
[36m[2023-07-10 19:53:30,801][227910] mean_value=-752.6211312907506, max_value=655.0967455503878[0m
[37m[1m[2023-07-10 19:53:30,803][227910] New mean coefficients: [[-0.10756505  0.65917987 -0.01275808  0.29516435  0.49811774]][0m
[37m[1m[2023-07-10 19:53:30,804][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:53:40,538][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 19:53:40,538][227910] FPS: 394561.46[0m
[36m[2023-07-10 19:53:40,541][227910] itr=1216, itrs=2000, Progress: 60.80%[0m
[36m[2023-07-10 19:53:52,169][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 19:53:52,169][227910] FPS: 330846.28[0m
[36m[2023-07-10 19:53:56,986][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:53:56,986][227910] Reward + Measures: [[-235.9600119     0.35748884    0.66794854    0.54866457    0.72231543]][0m
[37m[1m[2023-07-10 19:53:56,987][227910] Max Reward on eval: -235.96001189698597[0m
[37m[1m[2023-07-10 19:53:56,987][227910] Min Reward on eval: -235.96001189698597[0m
[37m[1m[2023-07-10 19:53:56,987][227910] Mean Reward across all agents: -235.96001189698597[0m
[37m[1m[2023-07-10 19:53:56,987][227910] Average Trajectory Length: 998.7239999999999[0m
[36m[2023-07-10 19:54:02,450][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:54:02,451][227910] Reward + Measures: [[-102.97990326    0.29799998    0.16870001    0.32270002    0.22310002]
 [-335.17130075    0.43390003    0.74229997    0.65790004    0.75730008]
 [-189.18932095    0.31870002    0.1294        0.38340002    0.16500001]
 ...
 [-109.6824039     0.52180004    0.20020001    0.51789999    0.25279999]
 [-177.64319929    0.60600007    0.46010002    0.57880002    0.51739997]
 [-114.90364865    0.57910007    0.2264        0.58430004    0.31350002]][0m
[37m[1m[2023-07-10 19:54:02,451][227910] Max Reward on eval: 1261.3067608632089[0m
[37m[1m[2023-07-10 19:54:02,451][227910] Min Reward on eval: -495.10124863170785[0m
[37m[1m[2023-07-10 19:54:02,451][227910] Mean Reward across all agents: 47.314533416038884[0m
[37m[1m[2023-07-10 19:54:02,452][227910] Average Trajectory Length: 999.3756666666667[0m
[36m[2023-07-10 19:54:02,454][227910] mean_value=-741.9565739569889, max_value=672.5770801705943[0m
[37m[1m[2023-07-10 19:54:02,456][227910] New mean coefficients: [[-1.4422349   0.6337294  -0.17859937  0.35717982  0.6971521 ]][0m
[37m[1m[2023-07-10 19:54:02,457][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:54:12,097][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 19:54:12,098][227910] FPS: 398407.61[0m
[36m[2023-07-10 19:54:12,100][227910] itr=1217, itrs=2000, Progress: 60.85%[0m
[36m[2023-07-10 19:54:23,619][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 19:54:23,619][227910] FPS: 333898.33[0m
[36m[2023-07-10 19:54:28,429][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:54:28,430][227910] Reward + Measures: [[-313.64391034    0.46181265    0.71332318    0.61442387    0.76684976]][0m
[37m[1m[2023-07-10 19:54:28,430][227910] Max Reward on eval: -313.6439103366445[0m
[37m[1m[2023-07-10 19:54:28,430][227910] Min Reward on eval: -313.6439103366445[0m
[37m[1m[2023-07-10 19:54:28,430][227910] Mean Reward across all agents: -313.6439103366445[0m
[37m[1m[2023-07-10 19:54:28,430][227910] Average Trajectory Length: 999.0646666666667[0m
[36m[2023-07-10 19:54:33,985][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:54:33,991][227910] Reward + Measures: [[-122.52439715    0.58390003    0.4815        0.54100007    0.56070006]
 [-199.85571179    0.4966        0.64239997    0.59429997    0.69020003]
 [-291.567878      0.51739997    0.69999999    0.60870004    0.73990005]
 ...
 [-215.52695816    0.63169998    0.64630002    0.6516        0.72290003]
 [-148.53179574    0.65810007    0.4797        0.6092        0.63270003]
 [-144.47094905    0.55089998    0.46949998    0.53220004    0.61199999]][0m
[37m[1m[2023-07-10 19:54:33,991][227910] Max Reward on eval: -13.212670422438531[0m
[37m[1m[2023-07-10 19:54:33,991][227910] Min Reward on eval: -403.4060758856358[0m
[37m[1m[2023-07-10 19:54:33,992][227910] Mean Reward across all agents: -219.05271758382776[0m
[37m[1m[2023-07-10 19:54:33,992][227910] Average Trajectory Length: 997.754[0m
[36m[2023-07-10 19:54:33,994][227910] mean_value=-752.3201699792463, max_value=118.6822459276845[0m
[37m[1m[2023-07-10 19:54:33,996][227910] New mean coefficients: [[-0.7125413   0.31575006 -0.31051636 -0.02168992  0.33137685]][0m
[37m[1m[2023-07-10 19:54:33,997][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:54:43,576][227910] train() took 9.58 seconds to complete[0m
[36m[2023-07-10 19:54:43,576][227910] FPS: 400947.46[0m
[36m[2023-07-10 19:54:43,578][227910] itr=1218, itrs=2000, Progress: 60.90%[0m
[36m[2023-07-10 19:54:55,128][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 19:54:55,129][227910] FPS: 332992.57[0m
[36m[2023-07-10 19:54:59,878][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:54:59,878][227910] Reward + Measures: [[-325.8136596     0.53349787    0.73168838    0.64524847    0.78066301]][0m
[37m[1m[2023-07-10 19:54:59,878][227910] Max Reward on eval: -325.8136595955785[0m
[37m[1m[2023-07-10 19:54:59,879][227910] Min Reward on eval: -325.8136595955785[0m
[37m[1m[2023-07-10 19:54:59,879][227910] Mean Reward across all agents: -325.8136595955785[0m
[37m[1m[2023-07-10 19:54:59,879][227910] Average Trajectory Length: 998.0946666666666[0m
[36m[2023-07-10 19:55:05,278][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:55:05,278][227910] Reward + Measures: [[-597.11643793    0.48769999    0.1442        0.5927        0.2138    ]
 [-718.05665377    0.48790002    0.2431        0.57689995    0.29770002]
 [-199.5853318     0.48890001    0.18969999    0.5302        0.29089999]
 ...
 [-344.43761406    0.37293279    0.12829295    0.46776018    0.21648923]
 [-147.41102047    0.51859999    0.18960001    0.55120003    0.30509999]
 [-383.99899207    0.67750001    0.24430001    0.7403        0.31780002]][0m
[37m[1m[2023-07-10 19:55:05,278][227910] Max Reward on eval: -106.44506111309165[0m
[37m[1m[2023-07-10 19:55:05,279][227910] Min Reward on eval: -1022.6296096161939[0m
[37m[1m[2023-07-10 19:55:05,279][227910] Mean Reward across all agents: -380.54535353583225[0m
[37m[1m[2023-07-10 19:55:05,279][227910] Average Trajectory Length: 990.1519999999999[0m
[36m[2023-07-10 19:55:05,281][227910] mean_value=-771.7870871012228, max_value=252.80938054399195[0m
[37m[1m[2023-07-10 19:55:05,283][227910] New mean coefficients: [[-1.0073947   0.24041387 -0.37807548 -0.12598068  0.05595309]][0m
[37m[1m[2023-07-10 19:55:05,284][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:55:15,026][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 19:55:15,027][227910] FPS: 394222.64[0m
[36m[2023-07-10 19:55:15,029][227910] itr=1219, itrs=2000, Progress: 60.95%[0m
[36m[2023-07-10 19:55:26,797][227910] train() took 11.75 seconds to complete[0m
[36m[2023-07-10 19:55:26,798][227910] FPS: 326896.79[0m
[36m[2023-07-10 19:55:31,483][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:55:31,484][227910] Reward + Measures: [[-367.87617896    0.55168515    0.76005971    0.66783798    0.79958862]][0m
[37m[1m[2023-07-10 19:55:31,484][227910] Max Reward on eval: -367.876178956417[0m
[37m[1m[2023-07-10 19:55:31,484][227910] Min Reward on eval: -367.876178956417[0m
[37m[1m[2023-07-10 19:55:31,484][227910] Mean Reward across all agents: -367.876178956417[0m
[37m[1m[2023-07-10 19:55:31,484][227910] Average Trajectory Length: 998.0933333333332[0m
[36m[2023-07-10 19:55:36,928][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:55:36,980][227910] Reward + Measures: [[-243.15072459    0.54520005    0.09119999    0.55689996    0.36149999]
 [-436.96987625    0.63160002    0.0681        0.70900005    0.37180001]
 [-324.64549222    0.6426        0.36500001    0.61660004    0.52960008]
 ...
 [-304.31491903    0.4271        0.1356        0.47119999    0.27379999]
 [-284.67107514    0.4869        0.37170002    0.49970004    0.49400002]
 [-532.65150001    0.7198        0.1419        0.76890004    0.38720003]][0m
[37m[1m[2023-07-10 19:55:36,980][227910] Max Reward on eval: 1079.942661930973[0m
[37m[1m[2023-07-10 19:55:36,981][227910] Min Reward on eval: -1006.7789244541898[0m
[37m[1m[2023-07-10 19:55:36,981][227910] Mean Reward across all agents: -327.53487618368683[0m
[37m[1m[2023-07-10 19:55:36,981][227910] Average Trajectory Length: 999.0613333333333[0m
[36m[2023-07-10 19:55:36,983][227910] mean_value=-710.8071280323213, max_value=762.2175375224972[0m
[37m[1m[2023-07-10 19:55:36,985][227910] New mean coefficients: [[-0.33947676  0.1922892  -0.48959908 -0.1540699  -0.09367758]][0m
[37m[1m[2023-07-10 19:55:36,986][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:55:46,741][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 19:55:46,741][227910] FPS: 393706.39[0m
[36m[2023-07-10 19:55:46,744][227910] itr=1220, itrs=2000, Progress: 61.00%[0m
[37m[1m[2023-07-10 19:55:50,701][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001200[0m
[36m[2023-07-10 19:56:02,732][227910] train() took 11.76 seconds to complete[0m
[36m[2023-07-10 19:56:02,733][227910] FPS: 326588.81[0m
[36m[2023-07-10 19:56:07,455][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:56:07,455][227910] Reward + Measures: [[-415.56762316    0.57186425    0.76566112    0.66055328    0.81229883]][0m
[37m[1m[2023-07-10 19:56:07,455][227910] Max Reward on eval: -415.56762316262837[0m
[37m[1m[2023-07-10 19:56:07,456][227910] Min Reward on eval: -415.56762316262837[0m
[37m[1m[2023-07-10 19:56:07,456][227910] Mean Reward across all agents: -415.56762316262837[0m
[37m[1m[2023-07-10 19:56:07,456][227910] Average Trajectory Length: 999.3723333333332[0m
[36m[2023-07-10 19:56:12,918][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:56:12,970][227910] Reward + Measures: [[-422.3715764     0.37910002    0.7471        0.57710004    0.7931    ]
 [-313.53068089    0.4901        0.61589998    0.51800007    0.67920005]
 [-414.88180523    0.5686        0.79150003    0.68530005    0.86180001]
 ...
 [-407.48111117    0.34099999    0.80200005    0.54280001    0.87379998]
 [-150.68628324    0.39820001    0.36340001    0.43269998    0.4224    ]
 [-267.58463715    0.5187        0.57600003    0.54430002    0.62630004]][0m
[37m[1m[2023-07-10 19:56:12,970][227910] Max Reward on eval: -55.5186586344149[0m
[37m[1m[2023-07-10 19:56:12,970][227910] Min Reward on eval: -565.6683704268478[0m
[37m[1m[2023-07-10 19:56:12,970][227910] Mean Reward across all agents: -355.4419423443611[0m
[37m[1m[2023-07-10 19:56:12,971][227910] Average Trajectory Length: 999.0336666666666[0m
[36m[2023-07-10 19:56:12,972][227910] mean_value=-959.6137935774777, max_value=66.28500905161962[0m
[37m[1m[2023-07-10 19:56:12,975][227910] New mean coefficients: [[ 0.74872166  0.42399353  0.23525694 -0.25375682  0.3177787 ]][0m
[37m[1m[2023-07-10 19:56:12,976][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:56:22,730][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 19:56:22,730][227910] FPS: 393728.79[0m
[36m[2023-07-10 19:56:22,732][227910] itr=1221, itrs=2000, Progress: 61.05%[0m
[36m[2023-07-10 19:56:34,347][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 19:56:34,348][227910] FPS: 331225.10[0m
[36m[2023-07-10 19:56:39,237][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:56:39,237][227910] Reward + Measures: [[-431.91931497    0.68502784    0.78029537    0.71808648    0.83006263]][0m
[37m[1m[2023-07-10 19:56:39,238][227910] Max Reward on eval: -431.91931497385406[0m
[37m[1m[2023-07-10 19:56:39,238][227910] Min Reward on eval: -431.91931497385406[0m
[37m[1m[2023-07-10 19:56:39,238][227910] Mean Reward across all agents: -431.91931497385406[0m
[37m[1m[2023-07-10 19:56:39,238][227910] Average Trajectory Length: 999.3676666666667[0m
[36m[2023-07-10 19:56:44,642][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:56:44,642][227910] Reward + Measures: [[ -148.42856709     0.58999997     0.1416         0.66379994
      0.30509999]
 [  -86.77859433     0.49390003     0.1195         0.5309
      0.27779999]
 [ -965.81128743     0.0533         0.82179993     0.64710009
      0.8847    ]
 ...
 [ -706.40620077     0.142          0.82100004     0.55650002
      0.87039995]
 [ -623.30007988     0.4413         0.15670002     0.5636
      0.2343    ]
 [-1073.8792417      0.0395         0.84170002     0.6552
      0.8919    ]][0m
[37m[1m[2023-07-10 19:56:44,642][227910] Max Reward on eval: -24.891201581500354[0m
[37m[1m[2023-07-10 19:56:44,643][227910] Min Reward on eval: -1536.2853184843668[0m
[37m[1m[2023-07-10 19:56:44,643][227910] Mean Reward across all agents: -510.09422824702847[0m
[37m[1m[2023-07-10 19:56:44,643][227910] Average Trajectory Length: 994.962[0m
[36m[2023-07-10 19:56:44,645][227910] mean_value=-937.1605739044822, max_value=151.278821385836[0m
[37m[1m[2023-07-10 19:56:44,647][227910] New mean coefficients: [[ 0.87176883  0.3794073   0.38119575 -0.49508262  0.21048677]][0m
[37m[1m[2023-07-10 19:56:44,648][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:56:54,460][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 19:56:54,460][227910] FPS: 391405.42[0m
[36m[2023-07-10 19:56:54,463][227910] itr=1222, itrs=2000, Progress: 61.10%[0m
[36m[2023-07-10 19:57:06,149][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 19:57:06,149][227910] FPS: 329210.11[0m
[36m[2023-07-10 19:57:10,944][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:57:10,944][227910] Reward + Measures: [[-335.60123576    0.69452977    0.70001239    0.65422165    0.7599349 ]][0m
[37m[1m[2023-07-10 19:57:10,945][227910] Max Reward on eval: -335.60123575848144[0m
[37m[1m[2023-07-10 19:57:10,945][227910] Min Reward on eval: -335.60123575848144[0m
[37m[1m[2023-07-10 19:57:10,945][227910] Mean Reward across all agents: -335.60123575848144[0m
[37m[1m[2023-07-10 19:57:10,945][227910] Average Trajectory Length: 999.6863333333333[0m
[36m[2023-07-10 19:57:16,414][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:57:16,420][227910] Reward + Measures: [[-180.41066242    0.59900004    0.43070003    0.54570001    0.4808    ]
 [-212.07178149    0.64230001    0.55249995    0.61849999    0.61440003]
 [-381.68275032    0.74510002    0.74579996    0.69510001    0.79420006]
 ...
 [-337.60033201    0.7123        0.67519999    0.64110005    0.74360007]
 [-333.82422318    0.75139993    0.75830001    0.71790004    0.81720001]
 [-273.8556275     0.71670002    0.61770004    0.66390002    0.67380005]][0m
[37m[1m[2023-07-10 19:57:16,420][227910] Max Reward on eval: -126.1127811577986[0m
[37m[1m[2023-07-10 19:57:16,421][227910] Min Reward on eval: -448.5411498313071[0m
[37m[1m[2023-07-10 19:57:16,421][227910] Mean Reward across all agents: -299.17937463862654[0m
[37m[1m[2023-07-10 19:57:16,421][227910] Average Trajectory Length: 999.6793333333333[0m
[36m[2023-07-10 19:57:16,422][227910] mean_value=-1080.9634931848834, max_value=-168.08120184991753[0m
[36m[2023-07-10 19:57:16,425][227910] XNES is restarting with a new solution whose measures are [0.23400001 0.83420002 0.1173     0.77599996] and objective is 247.74495926448145[0m
[36m[2023-07-10 19:57:16,426][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 19:57:16,428][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 19:57:16,429][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:57:26,287][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 19:57:26,287][227910] FPS: 389602.97[0m
[36m[2023-07-10 19:57:26,290][227910] itr=1223, itrs=2000, Progress: 61.15%[0m
[36m[2023-07-10 19:57:37,930][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 19:57:37,931][227910] FPS: 330493.37[0m
[36m[2023-07-10 19:57:42,739][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:57:42,739][227910] Reward + Measures: [[934.60911467   0.26816171   0.3102639    0.18951127   0.21378584]][0m
[37m[1m[2023-07-10 19:57:42,739][227910] Max Reward on eval: 934.6091146716803[0m
[37m[1m[2023-07-10 19:57:42,739][227910] Min Reward on eval: 934.6091146716803[0m
[37m[1m[2023-07-10 19:57:42,740][227910] Mean Reward across all agents: 934.6091146716803[0m
[37m[1m[2023-07-10 19:57:42,740][227910] Average Trajectory Length: 761.6816666666666[0m
[36m[2023-07-10 19:57:48,161][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:57:48,161][227910] Reward + Measures: [[ -132.0606506      0.44080001     0.34850001     0.301
      0.40599999]
 [-1161.67366226     0.79860002     0.60900003     0.83389997
      0.77060002]
 [-1147.24986178     0.15290001     0.16229999     0.1506
      0.18020001]
 ...
 [-1106.2052031      0.59820002     0.3089         0.62480003
      0.3186    ]
 [-1183.84583432     0.18826105     0.16738442     0.15578052
      0.20105715]
 [-1311.12854836     0.31290004     0.51429999     0.29830003
      0.31220004]][0m
[37m[1m[2023-07-10 19:57:48,162][227910] Max Reward on eval: 1721.333341079508[0m
[37m[1m[2023-07-10 19:57:48,162][227910] Min Reward on eval: -2124.6940552169226[0m
[37m[1m[2023-07-10 19:57:48,162][227910] Mean Reward across all agents: -843.1643888626704[0m
[37m[1m[2023-07-10 19:57:48,162][227910] Average Trajectory Length: 923.3036666666667[0m
[36m[2023-07-10 19:57:48,164][227910] mean_value=-2763.9888891071523, max_value=-29.57424103598646[0m
[36m[2023-07-10 19:57:48,166][227910] XNES is restarting with a new solution whose measures are [0.78109998 0.67340004 0.30469999 0.92719996] and objective is 382.3588331714389[0m
[36m[2023-07-10 19:57:48,167][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 19:57:48,169][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 19:57:48,170][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:57:57,818][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 19:57:57,818][227910] FPS: 398078.54[0m
[36m[2023-07-10 19:57:57,820][227910] itr=1224, itrs=2000, Progress: 61.20%[0m
[36m[2023-07-10 19:58:09,415][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 19:58:09,415][227910] FPS: 331703.00[0m
[36m[2023-07-10 19:58:14,205][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:58:14,206][227910] Reward + Measures: [[-25.3875068    0.85710692   0.31255201   0.63145697   0.96926594]][0m
[37m[1m[2023-07-10 19:58:14,206][227910] Max Reward on eval: -25.387506802079898[0m
[37m[1m[2023-07-10 19:58:14,206][227910] Min Reward on eval: -25.387506802079898[0m
[37m[1m[2023-07-10 19:58:14,206][227910] Mean Reward across all agents: -25.387506802079898[0m
[37m[1m[2023-07-10 19:58:14,206][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 19:58:19,659][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:58:19,664][227910] Reward + Measures: [[  -13.83827733     0.18730001     0.79659998     0.44700003
      0.78979999]
 [-1660.53732431     0.41529998     0.40150005     0.52950001
      0.40079999]
 [ -982.05821341     0.36660001     0.345          0.2545
      0.46630001]
 ...
 [  -97.05084118     0.70249999     0.76610005     0.22870003
      0.73029995]
 [-1482.16067081     0.1992         0.4131         0.23599999
      0.40820003]
 [-1585.03474102     0.68580002     0.61949998     0.67340004
      0.61549997]][0m
[37m[1m[2023-07-10 19:58:19,665][227910] Max Reward on eval: 579.392332969245[0m
[37m[1m[2023-07-10 19:58:19,665][227910] Min Reward on eval: -2127.7884067875334[0m
[37m[1m[2023-07-10 19:58:19,665][227910] Mean Reward across all agents: -894.1075789449498[0m
[37m[1m[2023-07-10 19:58:19,666][227910] Average Trajectory Length: 959.7043333333334[0m
[36m[2023-07-10 19:58:19,668][227910] mean_value=-1602.7095842176645, max_value=813.7647218285559[0m
[37m[1m[2023-07-10 19:58:19,670][227910] New mean coefficients: [[ 0.7233486  -0.14079022 -0.15161943 -0.31996644 -1.5249443 ]][0m
[37m[1m[2023-07-10 19:58:19,671][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:58:29,349][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 19:58:29,350][227910] FPS: 396843.48[0m
[36m[2023-07-10 19:58:29,352][227910] itr=1225, itrs=2000, Progress: 61.25%[0m
[36m[2023-07-10 19:58:40,991][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 19:58:40,991][227910] FPS: 330438.44[0m
[36m[2023-07-10 19:58:45,810][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:58:45,811][227910] Reward + Measures: [[370.5601886    0.71309441   0.8544153    0.01640233   0.90975469]][0m
[37m[1m[2023-07-10 19:58:45,811][227910] Max Reward on eval: 370.5601886028667[0m
[37m[1m[2023-07-10 19:58:45,811][227910] Min Reward on eval: 370.5601886028667[0m
[37m[1m[2023-07-10 19:58:45,811][227910] Mean Reward across all agents: 370.5601886028667[0m
[37m[1m[2023-07-10 19:58:45,811][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 19:58:51,438][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:58:51,439][227910] Reward + Measures: [[ -888.5096036      0.3883         0.54640001     0.1798
      0.4815    ]
 [-1670.48384133     0.67640001     0.4725         0.27990001
      0.66259998]
 [-1193.11292134     0.45829996     0.54530001     0.45769998
      0.52070004]
 ...
 [-1351.83314139     0.26269841     0.26410645     0.28751904
      0.37727076]
 [-1322.04543305     0.40977731     0.39276174     0.2545777
      0.27501735]
 [ -423.26666038     0.30130002     0.67000002     0.30850002
      0.50690001]][0m
[37m[1m[2023-07-10 19:58:51,439][227910] Max Reward on eval: 263.6210883790103[0m
[37m[1m[2023-07-10 19:58:51,440][227910] Min Reward on eval: -2500.6655931020273[0m
[37m[1m[2023-07-10 19:58:51,440][227910] Mean Reward across all agents: -928.2958364206435[0m
[37m[1m[2023-07-10 19:58:51,440][227910] Average Trajectory Length: 967.5076666666666[0m
[36m[2023-07-10 19:58:51,442][227910] mean_value=-1676.3718214315518, max_value=225.06012302585123[0m
[37m[1m[2023-07-10 19:58:51,444][227910] New mean coefficients: [[ 0.7018714  -0.0214764  -0.01543009 -0.36773553 -1.297813  ]][0m
[37m[1m[2023-07-10 19:58:51,445][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:59:01,112][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 19:59:01,112][227910] FPS: 397301.62[0m
[36m[2023-07-10 19:59:01,115][227910] itr=1226, itrs=2000, Progress: 61.30%[0m
[36m[2023-07-10 19:59:12,762][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 19:59:12,762][227910] FPS: 330244.70[0m
[36m[2023-07-10 19:59:17,454][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:59:17,454][227910] Reward + Measures: [[323.43635988   0.42954865   0.38829768   0.35670963   0.46322399]][0m
[37m[1m[2023-07-10 19:59:17,454][227910] Max Reward on eval: 323.43635988208933[0m
[37m[1m[2023-07-10 19:59:17,455][227910] Min Reward on eval: 323.43635988208933[0m
[37m[1m[2023-07-10 19:59:17,455][227910] Mean Reward across all agents: 323.43635988208933[0m
[37m[1m[2023-07-10 19:59:17,455][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 19:59:22,887][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:59:22,888][227910] Reward + Measures: [[-1216.81396492     0.80559999     0.9156         0.0697
      0.9073    ]
 [-1005.55396417     0.30879971     0.19080848     0.43593875
      0.38083455]
 [-1157.72396005     0.54430002     0.0938         0.59580004
      0.52569997]
 ...
 [-1793.95436029     0.37720001     0.75600004     0.13000001
      0.80960006]
 [ -817.94202226     0.17621711     0.63164276     0.4681749
      0.67612225]
 [ -483.2526058      0.6261         0.40130001     0.52210003
      0.41470003]][0m
[37m[1m[2023-07-10 19:59:22,888][227910] Max Reward on eval: 523.7796194576891[0m
[37m[1m[2023-07-10 19:59:22,889][227910] Min Reward on eval: -2440.3547006016133[0m
[37m[1m[2023-07-10 19:59:22,889][227910] Mean Reward across all agents: -616.5325217722448[0m
[37m[1m[2023-07-10 19:59:22,889][227910] Average Trajectory Length: 930.3556666666666[0m
[36m[2023-07-10 19:59:22,891][227910] mean_value=-1886.1249806374512, max_value=335.1439616324613[0m
[37m[1m[2023-07-10 19:59:22,893][227910] New mean coefficients: [[ 0.62387615  0.37672895  0.11312899 -0.51683104 -0.26091468]][0m
[37m[1m[2023-07-10 19:59:22,894][227910] Moving the mean solution point...[0m
[36m[2023-07-10 19:59:32,559][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 19:59:32,559][227910] FPS: 397388.47[0m
[36m[2023-07-10 19:59:32,562][227910] itr=1227, itrs=2000, Progress: 61.35%[0m
[36m[2023-07-10 19:59:44,382][227910] train() took 11.80 seconds to complete[0m
[36m[2023-07-10 19:59:44,383][227910] FPS: 325358.74[0m
[36m[2023-07-10 19:59:49,071][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:59:49,072][227910] Reward + Measures: [[385.71848356   0.53631997   0.38843602   0.49267468   0.36649331]][0m
[37m[1m[2023-07-10 19:59:49,072][227910] Max Reward on eval: 385.7184835605393[0m
[37m[1m[2023-07-10 19:59:49,072][227910] Min Reward on eval: 385.7184835605393[0m
[37m[1m[2023-07-10 19:59:49,073][227910] Mean Reward across all agents: 385.7184835605393[0m
[37m[1m[2023-07-10 19:59:49,073][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 19:59:54,463][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 19:59:54,463][227910] Reward + Measures: [[-1399.65710631     0.72665954     0.36922768     0.75142336
      0.64316386]
 [   25.98417111     0.3407         0.45170003     0.45039997
      0.37250003]
 [-1220.37235072     0.39955354     0.27991849     0.3632493
      0.31397489]
 ...
 [-1082.42841007     0.17629999     0.31500003     0.2023
      0.3475    ]
 [ -535.7325637      0.67490005     0.13560002     0.61869997
      0.4251    ]
 [-1273.30492723     0.38530001     0.64289999     0.2658
      0.70039999]][0m
[37m[1m[2023-07-10 19:59:54,463][227910] Max Reward on eval: 277.5048553284141[0m
[37m[1m[2023-07-10 19:59:54,464][227910] Min Reward on eval: -1894.5359195718077[0m
[37m[1m[2023-07-10 19:59:54,464][227910] Mean Reward across all agents: -736.0538474884562[0m
[37m[1m[2023-07-10 19:59:54,464][227910] Average Trajectory Length: 970.405[0m
[36m[2023-07-10 19:59:54,466][227910] mean_value=-1954.3823753970373, max_value=431.8053490532264[0m
[37m[1m[2023-07-10 19:59:54,468][227910] New mean coefficients: [[-0.18388844  0.60362303  0.59635556 -0.52031535  0.11909056]][0m
[37m[1m[2023-07-10 19:59:54,469][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:00:04,177][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 20:00:04,177][227910] FPS: 395628.39[0m
[36m[2023-07-10 20:00:04,180][227910] itr=1228, itrs=2000, Progress: 61.40%[0m
[36m[2023-07-10 20:00:15,798][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 20:00:15,798][227910] FPS: 331038.29[0m
[36m[2023-07-10 20:00:20,595][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:00:20,595][227910] Reward + Measures: [[271.03999929   0.60036689   0.58618921   0.20165531   0.58940524]][0m
[37m[1m[2023-07-10 20:00:20,596][227910] Max Reward on eval: 271.03999928886924[0m
[37m[1m[2023-07-10 20:00:20,596][227910] Min Reward on eval: 271.03999928886924[0m
[37m[1m[2023-07-10 20:00:20,596][227910] Mean Reward across all agents: 271.03999928886924[0m
[37m[1m[2023-07-10 20:00:20,596][227910] Average Trajectory Length: 999.9853333333333[0m
[36m[2023-07-10 20:00:26,033][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:00:26,033][227910] Reward + Measures: [[-554.95218681    0.77170002    0.7022        0.32740003    0.62229997]
 [-872.48381172    0.23889999    0.39089999    0.16960001    0.56490004]
 [-131.40735422    0.34430003    0.5025        0.0763        0.62200004]
 ...
 [-160.84008086    0.5406        0.76099998    0.09780001    0.6742    ]
 [-908.45498217    0.2064964     0.38349444    0.23821817    0.40680239]
 [-930.87937696    0.22836709    0.14417273    0.17931882    0.19129185]][0m
[37m[1m[2023-07-10 20:00:26,034][227910] Max Reward on eval: 516.3941335772048[0m
[37m[1m[2023-07-10 20:00:26,034][227910] Min Reward on eval: -2301.7761816889047[0m
[37m[1m[2023-07-10 20:00:26,034][227910] Mean Reward across all agents: -700.5556885441093[0m
[37m[1m[2023-07-10 20:00:26,034][227910] Average Trajectory Length: 975.4243333333333[0m
[36m[2023-07-10 20:00:26,037][227910] mean_value=-1550.8623926399655, max_value=554.020290350347[0m
[37m[1m[2023-07-10 20:00:26,039][227910] New mean coefficients: [[-0.22600457  0.6398184   1.1149206  -0.8888521   0.05642793]][0m
[37m[1m[2023-07-10 20:00:26,040][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:00:35,894][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 20:00:35,895][227910] FPS: 389751.77[0m
[36m[2023-07-10 20:00:35,897][227910] itr=1229, itrs=2000, Progress: 61.45%[0m
[36m[2023-07-10 20:00:47,486][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 20:00:47,487][227910] FPS: 331905.52[0m
[36m[2023-07-10 20:00:52,242][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:00:52,243][227910] Reward + Measures: [[351.27449764   0.27785099   0.6268093    0.10344366   0.77478606]][0m
[37m[1m[2023-07-10 20:00:52,243][227910] Max Reward on eval: 351.27449763890456[0m
[37m[1m[2023-07-10 20:00:52,243][227910] Min Reward on eval: 351.27449763890456[0m
[37m[1m[2023-07-10 20:00:52,243][227910] Mean Reward across all agents: 351.27449763890456[0m
[37m[1m[2023-07-10 20:00:52,244][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 20:00:57,750][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:00:57,751][227910] Reward + Measures: [[-1068.82744738     0.63050002     0.0627         0.58160001
      0.52940005]
 [ -438.67833969     0.36559999     0.37939999     0.4368
      0.53340006]
 [ -823.17729713     0.42835093     0.27217153     0.33350903
      0.30255738]
 ...
 [ -485.92152892     0.45129997     0.44420001     0.34730002
      0.4851    ]
 [ -603.02306444     0.32804105     0.24416676     0.35610363
      0.38210234]
 [ -445.48657153     0.44770002     0.32050002     0.36830002
      0.39450002]][0m
[37m[1m[2023-07-10 20:00:57,751][227910] Max Reward on eval: 500.72065576591996[0m
[37m[1m[2023-07-10 20:00:57,751][227910] Min Reward on eval: -1935.5880154061829[0m
[37m[1m[2023-07-10 20:00:57,751][227910] Mean Reward across all agents: -507.4867280692909[0m
[37m[1m[2023-07-10 20:00:57,752][227910] Average Trajectory Length: 987.7736666666666[0m
[36m[2023-07-10 20:00:57,754][227910] mean_value=-1216.181680019629, max_value=791.7610790376139[0m
[37m[1m[2023-07-10 20:00:57,756][227910] New mean coefficients: [[-0.10606914 -0.2547195   0.3376304  -0.6291889   0.11651124]][0m
[37m[1m[2023-07-10 20:00:57,757][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:01:07,554][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 20:01:07,555][227910] FPS: 392009.63[0m
[36m[2023-07-10 20:01:07,557][227910] itr=1230, itrs=2000, Progress: 61.50%[0m
[37m[1m[2023-07-10 20:01:11,646][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001210[0m
[36m[2023-07-10 20:01:23,577][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 20:01:23,577][227910] FPS: 329360.49[0m
[36m[2023-07-10 20:01:28,431][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:01:28,431][227910] Reward + Measures: [[119.12935384   0.32167634   0.74462873   0.15852267   0.80455369]][0m
[37m[1m[2023-07-10 20:01:28,431][227910] Max Reward on eval: 119.12935383838163[0m
[37m[1m[2023-07-10 20:01:28,431][227910] Min Reward on eval: 119.12935383838163[0m
[37m[1m[2023-07-10 20:01:28,432][227910] Mean Reward across all agents: 119.12935383838163[0m
[37m[1m[2023-07-10 20:01:28,432][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 20:01:33,942][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:01:33,943][227910] Reward + Measures: [[-341.9380754     0.1865        0.51379997    0.26800001    0.68059999]
 [ -43.87634438    0.50559998    0.47410002    0.44060001    0.49830005]
 [-380.01328132    0.38210002    0.36540002    0.37050003    0.40419999]
 ...
 [-576.58572091    0.4481        0.62159997    0.55470002    0.55900002]
 [-604.42416399    0.28663468    0.24469212    0.29226944    0.30600455]
 [-592.86498577    0.79950005    0.53360003    0.80870003    0.81029999]][0m
[37m[1m[2023-07-10 20:01:33,943][227910] Max Reward on eval: 444.53812097809276[0m
[37m[1m[2023-07-10 20:01:33,944][227910] Min Reward on eval: -2723.5447275699116[0m
[37m[1m[2023-07-10 20:01:33,944][227910] Mean Reward across all agents: -734.1316622221107[0m
[37m[1m[2023-07-10 20:01:33,944][227910] Average Trajectory Length: 987.3693333333333[0m
[36m[2023-07-10 20:01:33,946][227910] mean_value=-1251.2436046338385, max_value=436.5607417257084[0m
[37m[1m[2023-07-10 20:01:33,948][227910] New mean coefficients: [[-0.21759325 -0.49150938  0.61294806 -0.28561455  0.3158408 ]][0m
[37m[1m[2023-07-10 20:01:33,949][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:01:43,837][227910] train() took 9.89 seconds to complete[0m
[36m[2023-07-10 20:01:43,837][227910] FPS: 388435.53[0m
[36m[2023-07-10 20:01:43,840][227910] itr=1231, itrs=2000, Progress: 61.55%[0m
[36m[2023-07-10 20:01:55,452][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 20:01:55,453][227910] FPS: 331224.97[0m
[36m[2023-07-10 20:02:00,172][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:02:00,172][227910] Reward + Measures: [[-379.25648131    0.21979934    0.73229563    0.23002401    0.80280924]][0m
[37m[1m[2023-07-10 20:02:00,173][227910] Max Reward on eval: -379.25648131428875[0m
[37m[1m[2023-07-10 20:02:00,173][227910] Min Reward on eval: -379.25648131428875[0m
[37m[1m[2023-07-10 20:02:00,173][227910] Mean Reward across all agents: -379.25648131428875[0m
[37m[1m[2023-07-10 20:02:00,173][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 20:02:05,909][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:02:05,909][227910] Reward + Measures: [[ -847.79839138     0.91149998     0.98520005     0.0012
      0.98390001]
 [ -197.95192521     0.20190001     0.77279997     0.39719999
      0.7719    ]
 [-1261.96723127     0.87840003     0.99270004     0.002
      0.97959995]
 ...
 [ -860.12302418     0.59079999     0.72490007     0.0051
      0.83039999]
 [ -794.14545138     0.0755         0.61020005     0.3012
      0.77419996]
 [ -565.44413249     0.15090001     0.52210003     0.23
      0.59060001]][0m
[37m[1m[2023-07-10 20:02:05,909][227910] Max Reward on eval: 166.22213110581507[0m
[37m[1m[2023-07-10 20:02:05,910][227910] Min Reward on eval: -1911.6035936409376[0m
[37m[1m[2023-07-10 20:02:05,910][227910] Mean Reward across all agents: -619.7050366373178[0m
[37m[1m[2023-07-10 20:02:05,910][227910] Average Trajectory Length: 999.015[0m
[36m[2023-07-10 20:02:05,912][227910] mean_value=-1053.6983133700885, max_value=18.427985524483177[0m
[37m[1m[2023-07-10 20:02:05,914][227910] New mean coefficients: [[-0.42029673 -0.1042566   0.7015201   0.07625982  0.8534968 ]][0m
[37m[1m[2023-07-10 20:02:05,915][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:02:15,672][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 20:02:15,672][227910] FPS: 393602.35[0m
[36m[2023-07-10 20:02:15,675][227910] itr=1232, itrs=2000, Progress: 61.60%[0m
[36m[2023-07-10 20:02:27,167][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 20:02:27,167][227910] FPS: 334771.51[0m
[36m[2023-07-10 20:02:31,970][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:02:31,971][227910] Reward + Measures: [[-565.52716403    0.22354633    0.72160304    0.22576733    0.81493592]][0m
[37m[1m[2023-07-10 20:02:31,971][227910] Max Reward on eval: -565.5271640276804[0m
[37m[1m[2023-07-10 20:02:31,971][227910] Min Reward on eval: -565.5271640276804[0m
[37m[1m[2023-07-10 20:02:31,971][227910] Mean Reward across all agents: -565.5271640276804[0m
[37m[1m[2023-07-10 20:02:31,972][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 20:02:37,468][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:02:37,469][227910] Reward + Measures: [[ -992.69423093     0.26680002     0.69150001     0.29260001
      0.71160001]
 [-1275.32735463     0.87           0.86499995     0.0815
      0.84650004]
 [ -460.86202484     0.14399999     0.78330004     0.57270002
      0.81240004]
 ...
 [-1347.2475705      0.2271         0.736          0.37760001
      0.75139999]
 [ -850.55905178     0.1891         0.70450002     0.25400001
      0.76980001]
 [-1291.34940086     0.67690003     0.52520001     0.27830002
      0.65220004]][0m
[37m[1m[2023-07-10 20:02:37,469][227910] Max Reward on eval: 62.136437998787734[0m
[37m[1m[2023-07-10 20:02:37,469][227910] Min Reward on eval: -2379.88705570912[0m
[37m[1m[2023-07-10 20:02:37,470][227910] Mean Reward across all agents: -955.8754894881546[0m
[37m[1m[2023-07-10 20:02:37,470][227910] Average Trajectory Length: 994.7123333333333[0m
[36m[2023-07-10 20:02:37,471][227910] mean_value=-1309.6663057581036, max_value=-75.97002235863476[0m
[36m[2023-07-10 20:02:37,474][227910] XNES is restarting with a new solution whose measures are [0.30850002 0.67699999 0.43330002 0.68540001] and objective is 882.3611512538744[0m
[36m[2023-07-10 20:02:37,475][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 20:02:37,477][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 20:02:37,478][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:02:47,308][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 20:02:47,308][227910] FPS: 390697.85[0m
[36m[2023-07-10 20:02:47,310][227910] itr=1233, itrs=2000, Progress: 61.65%[0m
[36m[2023-07-10 20:02:59,000][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 20:02:59,001][227910] FPS: 328997.04[0m
[36m[2023-07-10 20:03:03,835][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:03:03,836][227910] Reward + Measures: [[1477.31950798    0.44579467    0.50328797    0.34790665    0.49200398]][0m
[37m[1m[2023-07-10 20:03:03,836][227910] Max Reward on eval: 1477.3195079781897[0m
[37m[1m[2023-07-10 20:03:03,836][227910] Min Reward on eval: 1477.3195079781897[0m
[37m[1m[2023-07-10 20:03:03,836][227910] Mean Reward across all agents: 1477.3195079781897[0m
[37m[1m[2023-07-10 20:03:03,836][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 20:03:09,384][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:03:09,385][227910] Reward + Measures: [[-1325.00165147     0.84820002     0.91939992     0.82460004
      0.91469997]
 [-1269.7574954      0.59939998     0.60350001     0.4989
      0.6182    ]
 [-1465.17827057     0.0319         0.88500005     0.48190004
      0.89679998]
 ...
 [ -117.13880879     0.58540004     0.2448         0.55190003
      0.42300001]
 [-1526.6077085      0.78319997     0.5018         0.71109998
      0.5442    ]
 [-1140.54472888     0.67860001     0.70780003     0.2106
      0.71239996]][0m
[37m[1m[2023-07-10 20:03:09,385][227910] Max Reward on eval: 2193.6741699975682[0m
[37m[1m[2023-07-10 20:03:09,385][227910] Min Reward on eval: -2170.534943468729[0m
[37m[1m[2023-07-10 20:03:09,385][227910] Mean Reward across all agents: -684.2339287268207[0m
[37m[1m[2023-07-10 20:03:09,386][227910] Average Trajectory Length: 992.002[0m
[36m[2023-07-10 20:03:09,387][227910] mean_value=-1679.2544482471005, max_value=329.74495127466184[0m
[37m[1m[2023-07-10 20:03:09,389][227910] New mean coefficients: [[ 0.28533834 -0.0461477  -0.9069353  -0.57730556 -1.2932527 ]][0m
[37m[1m[2023-07-10 20:03:09,390][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:03:19,113][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 20:03:19,114][227910] FPS: 395005.93[0m
[36m[2023-07-10 20:03:19,116][227910] itr=1234, itrs=2000, Progress: 61.70%[0m
[36m[2023-07-10 20:03:30,815][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 20:03:30,815][227910] FPS: 328755.49[0m
[36m[2023-07-10 20:03:35,694][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:03:35,694][227910] Reward + Measures: [[1437.39416709    0.39434341    0.18092938    0.34929582    0.22974589]][0m
[37m[1m[2023-07-10 20:03:35,694][227910] Max Reward on eval: 1437.394167094267[0m
[37m[1m[2023-07-10 20:03:35,694][227910] Min Reward on eval: 1437.394167094267[0m
[37m[1m[2023-07-10 20:03:35,695][227910] Mean Reward across all agents: 1437.394167094267[0m
[37m[1m[2023-07-10 20:03:35,695][227910] Average Trajectory Length: 999.1066666666667[0m
[36m[2023-07-10 20:03:41,169][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:03:41,170][227910] Reward + Measures: [[ -738.52255337     0.2053         0.45109996     0.21730001
      0.44889998]
 [-1248.08848998     0.33239999     0.17565002     0.34665
      0.2225    ]
 [-1659.51483433     0.34010002     0.3145         0.23799999
      0.29890001]
 ...
 [-1241.05335579     0.15901177     0.15267646     0.20638824
      0.20122941]
 [-1755.68213968     0.09900001     0.12660001     0.14690001
      0.13060001]
 [ -171.83011784     0.13540001     0.1437         0.17649999
      0.11490001]][0m
[37m[1m[2023-07-10 20:03:41,170][227910] Max Reward on eval: 1316.305213978677[0m
[37m[1m[2023-07-10 20:03:41,170][227910] Min Reward on eval: -1792.2125433840556[0m
[37m[1m[2023-07-10 20:03:41,171][227910] Mean Reward across all agents: -657.9496740314257[0m
[37m[1m[2023-07-10 20:03:41,171][227910] Average Trajectory Length: 948.995[0m
[36m[2023-07-10 20:03:41,172][227910] mean_value=-1799.2811822047138, max_value=1492.7834108132245[0m
[37m[1m[2023-07-10 20:03:41,175][227910] New mean coefficients: [[ 0.34717888 -1.1817775  -1.2395587  -1.0472171  -1.594209  ]][0m
[37m[1m[2023-07-10 20:03:41,176][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:03:51,069][227910] train() took 9.89 seconds to complete[0m
[36m[2023-07-10 20:03:51,069][227910] FPS: 388203.37[0m
[36m[2023-07-10 20:03:51,072][227910] itr=1235, itrs=2000, Progress: 61.75%[0m
[36m[2023-07-10 20:04:02,744][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 20:04:02,744][227910] FPS: 329537.79[0m
[36m[2023-07-10 20:04:07,521][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:04:07,521][227910] Reward + Measures: [[823.86838705   0.17899585   0.14413896   0.21550719   0.17763358]][0m
[37m[1m[2023-07-10 20:04:07,522][227910] Max Reward on eval: 823.868387054231[0m
[37m[1m[2023-07-10 20:04:07,522][227910] Min Reward on eval: 823.868387054231[0m
[37m[1m[2023-07-10 20:04:07,522][227910] Mean Reward across all agents: 823.868387054231[0m
[37m[1m[2023-07-10 20:04:07,522][227910] Average Trajectory Length: 927.8936666666666[0m
[36m[2023-07-10 20:04:13,148][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:04:13,154][227910] Reward + Measures: [[  -26.56910457     0.39399999     0.33559999     0.32690001
      0.3432    ]
 [ -769.08820218     0.32901847     0.48835382     0.16418104
      0.37781742]
 [ -838.90895645     0.0837         0.12279999     0.1798
      0.10219999]
 ...
 [-1091.01145603     0.3684032      0.51414281     0.22850986
      0.54547936]
 [   10.88242855     0.3664         0.31119999     0.28889999
      0.24989998]
 [ -227.86092608     0.38276249     0.32378057     0.23685586
      0.27445602]][0m
[37m[1m[2023-07-10 20:04:13,155][227910] Max Reward on eval: 631.0256299629225[0m
[37m[1m[2023-07-10 20:04:13,155][227910] Min Reward on eval: -1948.680178301828[0m
[37m[1m[2023-07-10 20:04:13,155][227910] Mean Reward across all agents: -749.6699855699501[0m
[37m[1m[2023-07-10 20:04:13,155][227910] Average Trajectory Length: 913.5506666666666[0m
[36m[2023-07-10 20:04:13,157][227910] mean_value=-2588.2141888316537, max_value=111.4297719184118[0m
[37m[1m[2023-07-10 20:04:13,159][227910] New mean coefficients: [[ 0.35583767 -1.1495445  -0.74470377 -0.7425437  -0.8532719 ]][0m
[37m[1m[2023-07-10 20:04:13,160][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:04:23,088][227910] train() took 9.93 seconds to complete[0m
[36m[2023-07-10 20:04:23,088][227910] FPS: 386864.92[0m
[36m[2023-07-10 20:04:23,091][227910] itr=1236, itrs=2000, Progress: 61.80%[0m
[36m[2023-07-10 20:04:34,710][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 20:04:34,710][227910] FPS: 331050.99[0m
[36m[2023-07-10 20:04:39,573][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:04:39,574][227910] Reward + Measures: [[1289.62914877    0.25190961    0.24156785    0.29569611    0.19879374]][0m
[37m[1m[2023-07-10 20:04:39,574][227910] Max Reward on eval: 1289.6291487722428[0m
[37m[1m[2023-07-10 20:04:39,574][227910] Min Reward on eval: 1289.6291487722428[0m
[37m[1m[2023-07-10 20:04:39,575][227910] Mean Reward across all agents: 1289.6291487722428[0m
[37m[1m[2023-07-10 20:04:39,575][227910] Average Trajectory Length: 946.6343333333333[0m
[36m[2023-07-10 20:04:45,253][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:04:45,254][227910] Reward + Measures: [[   96.0475468      0.29060003     0.29320002     0.3206
      0.2502    ]
 [  380.93016746     0.20254388     0.2028662      0.3151072
      0.17746618]
 [-1099.69473814     0.47793373     0.21611451     0.31827557
      0.24290095]
 ...
 [-1215.11039278     0.33040002     0.221          0.27540001
      0.23900001]
 [ -876.27896756     0.26122648     0.36204663     0.28594664
      0.38085896]
 [ -714.31394113     0.79010004     0.3651         0.82719994
      0.17850001]][0m
[37m[1m[2023-07-10 20:04:45,254][227910] Max Reward on eval: 1301.2638272232841[0m
[37m[1m[2023-07-10 20:04:45,254][227910] Min Reward on eval: -1935.0713180576683[0m
[37m[1m[2023-07-10 20:04:45,254][227910] Mean Reward across all agents: -506.0157014615209[0m
[37m[1m[2023-07-10 20:04:45,255][227910] Average Trajectory Length: 959.198[0m
[36m[2023-07-10 20:04:45,256][227910] mean_value=-2607.7903574472675, max_value=797.592352950049[0m
[37m[1m[2023-07-10 20:04:45,259][227910] New mean coefficients: [[ 0.8056811   0.41791844  0.39642465 -0.769576   -1.0095502 ]][0m
[37m[1m[2023-07-10 20:04:45,260][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:04:55,015][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 20:04:55,015][227910] FPS: 393701.13[0m
[36m[2023-07-10 20:04:55,017][227910] itr=1237, itrs=2000, Progress: 61.85%[0m
[36m[2023-07-10 20:05:06,628][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 20:05:06,629][227910] FPS: 331342.44[0m
[36m[2023-07-10 20:05:11,395][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:05:11,400][227910] Reward + Measures: [[1906.24529769    0.37393045    0.30786419    0.28823212    0.24016899]][0m
[37m[1m[2023-07-10 20:05:11,401][227910] Max Reward on eval: 1906.2452976873726[0m
[37m[1m[2023-07-10 20:05:11,401][227910] Min Reward on eval: 1906.2452976873726[0m
[37m[1m[2023-07-10 20:05:11,401][227910] Mean Reward across all agents: 1906.2452976873726[0m
[37m[1m[2023-07-10 20:05:11,402][227910] Average Trajectory Length: 974.832[0m
[36m[2023-07-10 20:05:16,891][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:05:16,896][227910] Reward + Measures: [[ 1405.26318612     0.34898326     0.40825817     0.29079792
      0.19137196]
 [ -660.76794351     0.480414       0.44047195     0.27117291
      0.2745533 ]
 [   73.23223649     0.35759997     0.43380004     0.4357
      0.31860003]
 ...
 [ 2398.0428653      0.41008577     0.33035028     0.30413851
      0.1862319 ]
 [-1670.00188291     0.23550001     0.43290001     0.15210001
      0.3951    ]
 [-1464.57973655     0.21562682     0.33783418     0.28990489
      0.27719268]][0m
[37m[1m[2023-07-10 20:05:16,897][227910] Max Reward on eval: 2398.0428653014824[0m
[37m[1m[2023-07-10 20:05:16,897][227910] Min Reward on eval: -1838.5672516682534[0m
[37m[1m[2023-07-10 20:05:16,897][227910] Mean Reward across all agents: -365.70580624650603[0m
[37m[1m[2023-07-10 20:05:16,898][227910] Average Trajectory Length: 970.247[0m
[36m[2023-07-10 20:05:16,899][227910] mean_value=-2401.2909820734553, max_value=313.0173818538418[0m
[37m[1m[2023-07-10 20:05:16,902][227910] New mean coefficients: [[1.4326687  0.41754767 0.6439954  0.29684597 0.9566519 ]][0m
[37m[1m[2023-07-10 20:05:16,902][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:05:26,621][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 20:05:26,621][227910] FPS: 395179.92[0m
[36m[2023-07-10 20:05:26,623][227910] itr=1238, itrs=2000, Progress: 61.90%[0m
[36m[2023-07-10 20:05:38,189][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 20:05:38,190][227910] FPS: 332537.89[0m
[36m[2023-07-10 20:05:43,059][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:05:43,060][227910] Reward + Measures: [[2037.25445847    0.33153844    0.19499952    0.32652724    0.19614217]][0m
[37m[1m[2023-07-10 20:05:43,060][227910] Max Reward on eval: 2037.2544584675593[0m
[37m[1m[2023-07-10 20:05:43,060][227910] Min Reward on eval: 2037.2544584675593[0m
[37m[1m[2023-07-10 20:05:43,061][227910] Mean Reward across all agents: 2037.2544584675593[0m
[37m[1m[2023-07-10 20:05:43,061][227910] Average Trajectory Length: 997.0369999999999[0m
[36m[2023-07-10 20:05:48,585][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:05:48,586][227910] Reward + Measures: [[   58.82929208     0.2696         0.43520004     0.23480001
      0.37620002]
 [   88.68575303     0.28379685     0.1993504      0.22164789
      0.20290302]
 [-1573.795375       0.53509998     0.43740001     0.55050004
      0.40050003]
 ...
 [ -378.03489607     0.56726784     0.24089681     0.36922723
      0.42626563]
 [ -642.70464073     0.2897895      0.42950439     0.26072893
      0.3562395 ]
 [ -430.11975619     0.4488         0.3364         0.34700003
      0.47440004]][0m
[37m[1m[2023-07-10 20:05:48,586][227910] Max Reward on eval: 1859.197774181515[0m
[37m[1m[2023-07-10 20:05:48,586][227910] Min Reward on eval: -1573.795375000895[0m
[37m[1m[2023-07-10 20:05:48,586][227910] Mean Reward across all agents: -224.26522397775625[0m
[37m[1m[2023-07-10 20:05:48,587][227910] Average Trajectory Length: 926.8399999999999[0m
[36m[2023-07-10 20:05:48,588][227910] mean_value=-2279.3062799592744, max_value=565.60783439704[0m
[37m[1m[2023-07-10 20:05:48,590][227910] New mean coefficients: [[ 1.6995113  -0.11382034  0.37939447  0.34960717  0.6153995 ]][0m
[37m[1m[2023-07-10 20:05:48,591][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:05:58,646][227910] train() took 10.05 seconds to complete[0m
[36m[2023-07-10 20:05:58,646][227910] FPS: 381962.10[0m
[36m[2023-07-10 20:05:58,649][227910] itr=1239, itrs=2000, Progress: 61.95%[0m
[36m[2023-07-10 20:06:10,196][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 20:06:10,196][227910] FPS: 333088.63[0m
[36m[2023-07-10 20:06:14,905][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:06:14,905][227910] Reward + Measures: [[1557.61628955    0.37342647    0.17810902    0.26014525    0.23116398]][0m
[37m[1m[2023-07-10 20:06:14,906][227910] Max Reward on eval: 1557.6162895507807[0m
[37m[1m[2023-07-10 20:06:14,906][227910] Min Reward on eval: 1557.6162895507807[0m
[37m[1m[2023-07-10 20:06:14,906][227910] Mean Reward across all agents: 1557.6162895507807[0m
[37m[1m[2023-07-10 20:06:14,906][227910] Average Trajectory Length: 995.1226666666666[0m
[36m[2023-07-10 20:06:20,337][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:06:20,338][227910] Reward + Measures: [[-1027.23504516     0.21673398     0.15837704     0.16238375
      0.14283574]
 [ -256.61605588     0.32870817     0.15263768     0.21091127
      0.19436346]
 [  -96.29971821     0.36560002     0.43179998     0.3926
      0.3928    ]
 ...
 [  244.29484906     0.26574865     0.29481623     0.31715402
      0.30909729]
 [ -820.2405861      0.41580001     0.50840002     0.29809999
      0.48859999]
 [ -697.87890196     0.47250006     0.43239999     0.54189998
      0.35980004]][0m
[37m[1m[2023-07-10 20:06:20,338][227910] Max Reward on eval: 1765.6415572636179[0m
[37m[1m[2023-07-10 20:06:20,338][227910] Min Reward on eval: -1361.1362993474236[0m
[37m[1m[2023-07-10 20:06:20,339][227910] Mean Reward across all agents: -33.935826748000736[0m
[37m[1m[2023-07-10 20:06:20,339][227910] Average Trajectory Length: 890.6866666666666[0m
[36m[2023-07-10 20:06:20,341][227910] mean_value=-2091.5528569250414, max_value=204.04974764688404[0m
[37m[1m[2023-07-10 20:06:20,343][227910] New mean coefficients: [[1.7287221  1.3984851  1.0823009  0.83090234 1.0066484 ]][0m
[37m[1m[2023-07-10 20:06:20,344][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:06:30,095][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 20:06:30,095][227910] FPS: 393882.87[0m
[36m[2023-07-10 20:06:30,097][227910] itr=1240, itrs=2000, Progress: 62.00%[0m
[37m[1m[2023-07-10 20:06:34,047][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001220[0m
[36m[2023-07-10 20:06:45,949][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 20:06:45,955][227910] FPS: 330235.39[0m
[36m[2023-07-10 20:06:50,687][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:06:50,687][227910] Reward + Measures: [[1789.80459718    0.37095079    0.18018758    0.27262822    0.22397837]][0m
[37m[1m[2023-07-10 20:06:50,687][227910] Max Reward on eval: 1789.8045971764811[0m
[37m[1m[2023-07-10 20:06:50,687][227910] Min Reward on eval: 1789.8045971764811[0m
[37m[1m[2023-07-10 20:06:50,688][227910] Mean Reward across all agents: 1789.8045971764811[0m
[37m[1m[2023-07-10 20:06:50,688][227910] Average Trajectory Length: 993.629[0m
[36m[2023-07-10 20:06:56,197][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:06:56,198][227910] Reward + Measures: [[ 376.83937799    0.3434        0.3141        0.36590001    0.228     ]
 [-628.2788754     0.38448167    0.32823002    0.10603569    0.2347436 ]
 [-283.36362193    0.17065707    0.23865362    0.34483451    0.28022322]
 ...
 [-421.17535076    0.45030004    0.30879998    0.46070004    0.20770001]
 [ 117.54156714    0.32980004    0.38369998    0.3513        0.40790007]
 [-994.04581677    0.38369998    0.44049999    0.11370001    0.27160001]][0m
[37m[1m[2023-07-10 20:06:56,198][227910] Max Reward on eval: 1540.053907531989[0m
[37m[1m[2023-07-10 20:06:56,198][227910] Min Reward on eval: -2004.2053764844313[0m
[37m[1m[2023-07-10 20:06:56,199][227910] Mean Reward across all agents: -330.04257083548123[0m
[37m[1m[2023-07-10 20:06:56,199][227910] Average Trajectory Length: 912.1646666666667[0m
[36m[2023-07-10 20:06:56,201][227910] mean_value=-2521.725791484908, max_value=428.5847364470904[0m
[37m[1m[2023-07-10 20:06:56,204][227910] New mean coefficients: [[ 1.3004199   1.5085746  -0.20649326  0.40154222  1.0026938 ]][0m
[37m[1m[2023-07-10 20:06:56,205][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:07:05,924][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 20:07:05,924][227910] FPS: 395176.49[0m
[36m[2023-07-10 20:07:05,927][227910] itr=1241, itrs=2000, Progress: 62.05%[0m
[36m[2023-07-10 20:07:17,425][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 20:07:17,425][227910] FPS: 334533.08[0m
[36m[2023-07-10 20:07:22,222][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:07:22,223][227910] Reward + Measures: [[2098.85730111    0.3469851     0.20631438    0.25717121    0.20136227]][0m
[37m[1m[2023-07-10 20:07:22,223][227910] Max Reward on eval: 2098.8573011090284[0m
[37m[1m[2023-07-10 20:07:22,223][227910] Min Reward on eval: 2098.8573011090284[0m
[37m[1m[2023-07-10 20:07:22,223][227910] Mean Reward across all agents: 2098.8573011090284[0m
[37m[1m[2023-07-10 20:07:22,224][227910] Average Trajectory Length: 991.5593333333333[0m
[36m[2023-07-10 20:07:27,640][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:07:27,646][227910] Reward + Measures: [[ 343.69932348    0.60430002    0.33080003    0.40200001    0.43190002]
 [ 898.21577538    0.3795        0.23650001    0.47989997    0.26429996]
 [-974.9014539     0.28350002    0.48300001    0.43020001    0.29260001]
 ...
 [-641.07252048    0.41730005    0.35409999    0.33430001    0.2904    ]
 [-216.66617792    0.17283277    0.50956869    0.21685636    0.3318193 ]
 [-916.85962583    0.31296727    0.49626094    0.13472462    0.48790503]][0m
[37m[1m[2023-07-10 20:07:27,646][227910] Max Reward on eval: 2073.623935737973[0m
[37m[1m[2023-07-10 20:07:27,646][227910] Min Reward on eval: -2021.4666034643772[0m
[37m[1m[2023-07-10 20:07:27,647][227910] Mean Reward across all agents: -189.56888912418339[0m
[37m[1m[2023-07-10 20:07:27,647][227910] Average Trajectory Length: 960.6926666666666[0m
[36m[2023-07-10 20:07:27,648][227910] mean_value=-2480.0729369389646, max_value=342.85836846980993[0m
[37m[1m[2023-07-10 20:07:27,651][227910] New mean coefficients: [[ 1.2466881   0.7828391   0.23145598 -1.3745029   0.782172  ]][0m
[37m[1m[2023-07-10 20:07:27,652][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:07:37,410][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 20:07:37,410][227910] FPS: 393585.01[0m
[36m[2023-07-10 20:07:37,413][227910] itr=1242, itrs=2000, Progress: 62.10%[0m
[36m[2023-07-10 20:07:48,952][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 20:07:48,953][227910] FPS: 333323.88[0m
[36m[2023-07-10 20:07:53,836][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:07:53,842][227910] Reward + Measures: [[2242.02321076    0.34778923    0.20093833    0.24656506    0.19978182]][0m
[37m[1m[2023-07-10 20:07:53,842][227910] Max Reward on eval: 2242.0232107558604[0m
[37m[1m[2023-07-10 20:07:53,843][227910] Min Reward on eval: 2242.0232107558604[0m
[37m[1m[2023-07-10 20:07:53,843][227910] Mean Reward across all agents: 2242.0232107558604[0m
[37m[1m[2023-07-10 20:07:53,843][227910] Average Trajectory Length: 988.0476666666666[0m
[36m[2023-07-10 20:07:59,409][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:07:59,410][227910] Reward + Measures: [[ -547.51431506     0.23616245     0.2632167      0.30626214
      0.25821671]
 [ -327.07960789     0.23857661     0.33742976     0.3314766
      0.35255957]
 [ -628.34798697     0.27118537     0.18359025     0.27436829
      0.19905123]
 ...
 [ -309.15543663     0.37040001     0.35549998     0.57080001
      0.26680002]
 [ -872.31301976     0.27320001     0.30939999     0.26929998
      0.1679    ]
 [-1038.09159908     0.4482151      0.48857728     0.34035251
      0.51539868]][0m
[37m[1m[2023-07-10 20:07:59,410][227910] Max Reward on eval: 2354.3487520120107[0m
[37m[1m[2023-07-10 20:07:59,411][227910] Min Reward on eval: -1368.2313291835133[0m
[37m[1m[2023-07-10 20:07:59,411][227910] Mean Reward across all agents: 5.679515027779007[0m
[37m[1m[2023-07-10 20:07:59,411][227910] Average Trajectory Length: 937.454[0m
[36m[2023-07-10 20:07:59,413][227910] mean_value=-2373.548900916418, max_value=689.9787428722103[0m
[37m[1m[2023-07-10 20:07:59,415][227910] New mean coefficients: [[ 1.4377755   0.44899768  0.7391487  -0.34907413  1.0798757 ]][0m
[37m[1m[2023-07-10 20:07:59,416][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:08:09,173][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 20:08:09,173][227910] FPS: 393625.60[0m
[36m[2023-07-10 20:08:09,175][227910] itr=1243, itrs=2000, Progress: 62.15%[0m
[36m[2023-07-10 20:08:20,704][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 20:08:20,704][227910] FPS: 333638.71[0m
[36m[2023-07-10 20:08:25,337][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:08:25,337][227910] Reward + Measures: [[1693.81840869    0.3548522     0.15871993    0.36910778    0.19515637]][0m
[37m[1m[2023-07-10 20:08:25,337][227910] Max Reward on eval: 1693.8184086941897[0m
[37m[1m[2023-07-10 20:08:25,337][227910] Min Reward on eval: 1693.8184086941897[0m
[37m[1m[2023-07-10 20:08:25,338][227910] Mean Reward across all agents: 1693.8184086941897[0m
[37m[1m[2023-07-10 20:08:25,338][227910] Average Trajectory Length: 999.185[0m
[36m[2023-07-10 20:08:30,799][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:08:30,805][227910] Reward + Measures: [[ 342.44662992    0.26644072    0.12908797    0.19104861    0.14817126]
 [-333.94370854    0.66112322    0.05608151    0.79309338    0.57642889]
 [ 273.0093868     0.4172        0.1873        0.5363        0.28509998]
 ...
 [-110.30743118    0.27585411    0.14244334    0.19736384    0.13822579]
 [ 329.01329308    0.34121567    0.20313907    0.38608888    0.1885647 ]
 [ 293.41588054    0.39590001    0.36290002    0.37180004    0.29969999]][0m
[37m[1m[2023-07-10 20:08:30,805][227910] Max Reward on eval: 2016.008019632986[0m
[37m[1m[2023-07-10 20:08:30,806][227910] Min Reward on eval: -1333.255434223311[0m
[37m[1m[2023-07-10 20:08:30,806][227910] Mean Reward across all agents: 397.2957069360654[0m
[37m[1m[2023-07-10 20:08:30,806][227910] Average Trajectory Length: 908.8796666666666[0m
[36m[2023-07-10 20:08:30,808][227910] mean_value=-2025.4358056625779, max_value=726.1834968236008[0m
[37m[1m[2023-07-10 20:08:30,811][227910] New mean coefficients: [[ 1.3809415   1.3201263  -0.6954544   0.79905486  0.7762277 ]][0m
[37m[1m[2023-07-10 20:08:30,812][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:08:40,403][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 20:08:40,404][227910] FPS: 400407.98[0m
[36m[2023-07-10 20:08:40,406][227910] itr=1244, itrs=2000, Progress: 62.20%[0m
[36m[2023-07-10 20:08:52,003][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 20:08:52,004][227910] FPS: 331663.09[0m
[36m[2023-07-10 20:08:56,772][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:08:56,778][227910] Reward + Measures: [[836.60555422   0.37652111   0.16784993   0.37136912   0.2139314 ]][0m
[37m[1m[2023-07-10 20:08:56,778][227910] Max Reward on eval: 836.6055542159113[0m
[37m[1m[2023-07-10 20:08:56,779][227910] Min Reward on eval: 836.6055542159113[0m
[37m[1m[2023-07-10 20:08:56,779][227910] Mean Reward across all agents: 836.6055542159113[0m
[37m[1m[2023-07-10 20:08:56,779][227910] Average Trajectory Length: 997.5213333333332[0m
[36m[2023-07-10 20:09:02,280][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:09:02,281][227910] Reward + Measures: [[ 330.55292354    0.31870005    0.27481252    0.31456253    0.24321249]
 [-171.57701232    0.28072664    0.27609488    0.3042697     0.28872469]
 [ 342.42313249    0.4887        0.14250001    0.44530001    0.19860001]
 ...
 [-345.92631891    0.2414131     0.24872215    0.24939559    0.29303125]
 [-834.35199096    0.18607275    0.17369783    0.23487051    0.23858099]
 [ 144.8769278     0.27508727    0.21789344    0.28125271    0.24987698]][0m
[37m[1m[2023-07-10 20:09:02,281][227910] Max Reward on eval: 1105.853856846504[0m
[37m[1m[2023-07-10 20:09:02,281][227910] Min Reward on eval: -1305.8893796807156[0m
[37m[1m[2023-07-10 20:09:02,282][227910] Mean Reward across all agents: -124.56541294008554[0m
[37m[1m[2023-07-10 20:09:02,282][227910] Average Trajectory Length: 920.0976666666667[0m
[36m[2023-07-10 20:09:02,284][227910] mean_value=-1835.2726795549, max_value=1278.0552502414444[0m
[37m[1m[2023-07-10 20:09:02,287][227910] New mean coefficients: [[ 1.3926127   1.7737725   0.04335344  0.927712   -1.1848677 ]][0m
[37m[1m[2023-07-10 20:09:02,288][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:09:12,083][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 20:09:12,084][227910] FPS: 392076.65[0m
[36m[2023-07-10 20:09:12,086][227910] itr=1245, itrs=2000, Progress: 62.25%[0m
[36m[2023-07-10 20:09:23,747][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 20:09:23,747][227910] FPS: 329813.38[0m
[36m[2023-07-10 20:09:28,552][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:09:28,552][227910] Reward + Measures: [[1000.39572974    0.37722424    0.16881023    0.38038704    0.21069366]][0m
[37m[1m[2023-07-10 20:09:28,553][227910] Max Reward on eval: 1000.3957297364298[0m
[37m[1m[2023-07-10 20:09:28,553][227910] Min Reward on eval: 1000.3957297364298[0m
[37m[1m[2023-07-10 20:09:28,553][227910] Mean Reward across all agents: 1000.3957297364298[0m
[37m[1m[2023-07-10 20:09:28,553][227910] Average Trajectory Length: 996.0976666666667[0m
[36m[2023-07-10 20:09:33,948][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:09:33,949][227910] Reward + Measures: [[ 592.52143829    0.37709999    0.106         0.3012        0.1911    ]
 [ 814.3437003     0.4765        0.17570001    0.40370002    0.2067    ]
 [1164.76076102    0.36490002    0.1649        0.3671        0.1938    ]
 ...
 [1208.40517569    0.41170001    0.20310001    0.40990001    0.21690002]
 [1041.22697692    0.33669996    0.2447        0.4578        0.23049998]
 [ 752.8808277     0.51519996    0.12730001    0.4456        0.18990001]][0m
[37m[1m[2023-07-10 20:09:33,949][227910] Max Reward on eval: 1680.237624453567[0m
[37m[1m[2023-07-10 20:09:33,949][227910] Min Reward on eval: 245.31844162236666[0m
[37m[1m[2023-07-10 20:09:33,950][227910] Mean Reward across all agents: 1090.4927743045746[0m
[37m[1m[2023-07-10 20:09:33,950][227910] Average Trajectory Length: 996.347[0m
[36m[2023-07-10 20:09:33,951][227910] mean_value=-1381.3765590126866, max_value=475.11501815273925[0m
[37m[1m[2023-07-10 20:09:33,954][227910] New mean coefficients: [[ 2.1026502  1.6065239 -0.963918   1.10852   -1.7791398]][0m
[37m[1m[2023-07-10 20:09:33,955][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:09:43,622][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 20:09:43,622][227910] FPS: 397271.92[0m
[36m[2023-07-10 20:09:43,625][227910] itr=1246, itrs=2000, Progress: 62.30%[0m
[36m[2023-07-10 20:09:55,188][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 20:09:55,189][227910] FPS: 332702.43[0m
[36m[2023-07-10 20:09:59,928][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:09:59,928][227910] Reward + Measures: [[1171.32459347    0.37834078    0.17273065    0.38758287    0.20791598]][0m
[37m[1m[2023-07-10 20:09:59,929][227910] Max Reward on eval: 1171.3245934661838[0m
[37m[1m[2023-07-10 20:09:59,929][227910] Min Reward on eval: 1171.3245934661838[0m
[37m[1m[2023-07-10 20:09:59,929][227910] Mean Reward across all agents: 1171.3245934661838[0m
[37m[1m[2023-07-10 20:09:59,930][227910] Average Trajectory Length: 998.6876666666666[0m
[36m[2023-07-10 20:10:05,353][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:10:05,358][227910] Reward + Measures: [[1009.51173279    0.3398        0.15000001    0.32230002    0.2009    ]
 [1144.72851006    0.43029997    0.17739999    0.45299998    0.2168    ]
 [1283.93619028    0.41510001    0.1882        0.41800004    0.20109999]
 ...
 [ 982.69111226    0.53220004    0.17760001    0.44779998    0.2053    ]
 [1095.9892839     0.43789998    0.1869        0.43240005    0.21640001]
 [1113.54135678    0.39270002    0.16739999    0.40869999    0.20850001]][0m
[37m[1m[2023-07-10 20:10:05,359][227910] Max Reward on eval: 1499.5334261141252[0m
[37m[1m[2023-07-10 20:10:05,359][227910] Min Reward on eval: 302.2064312550589[0m
[37m[1m[2023-07-10 20:10:05,359][227910] Mean Reward across all agents: 937.834191581427[0m
[37m[1m[2023-07-10 20:10:05,359][227910] Average Trajectory Length: 996.7673333333333[0m
[36m[2023-07-10 20:10:05,361][227910] mean_value=-1120.9320369757588, max_value=541.1583911033704[0m
[37m[1m[2023-07-10 20:10:05,364][227910] New mean coefficients: [[ 2.9355714   2.7393007  -0.61542755  2.075212   -1.6343634 ]][0m
[37m[1m[2023-07-10 20:10:05,365][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:10:15,020][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 20:10:15,020][227910] FPS: 397785.78[0m
[36m[2023-07-10 20:10:15,022][227910] itr=1247, itrs=2000, Progress: 62.35%[0m
[36m[2023-07-10 20:10:26,661][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 20:10:26,661][227910] FPS: 330455.57[0m
[36m[2023-07-10 20:10:31,435][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:10:31,435][227910] Reward + Measures: [[1332.85685524    0.37136433    0.17767057    0.39103344    0.20253487]][0m
[37m[1m[2023-07-10 20:10:31,435][227910] Max Reward on eval: 1332.8568552432969[0m
[37m[1m[2023-07-10 20:10:31,435][227910] Min Reward on eval: 1332.8568552432969[0m
[37m[1m[2023-07-10 20:10:31,436][227910] Mean Reward across all agents: 1332.8568552432969[0m
[37m[1m[2023-07-10 20:10:31,436][227910] Average Trajectory Length: 997.444[0m
[36m[2023-07-10 20:10:37,081][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:10:37,082][227910] Reward + Measures: [[1127.21306761    0.44750005    0.17          0.3892        0.2066    ]
 [1090.96164544    0.45170003    0.19910002    0.412         0.2093    ]
 [1220.78722259    0.3955        0.1816        0.41690001    0.2172    ]
 ...
 [1023.61774217    0.36149999    0.20060001    0.42179999    0.2177    ]
 [ 554.31847214    0.55360001    0.1305        0.4736        0.20970002]
 [ 711.49856047    0.53189999    0.17389999    0.4278        0.21599999]][0m
[37m[1m[2023-07-10 20:10:37,082][227910] Max Reward on eval: 1634.0809887921089[0m
[37m[1m[2023-07-10 20:10:37,082][227910] Min Reward on eval: 333.89498443336925[0m
[37m[1m[2023-07-10 20:10:37,082][227910] Mean Reward across all agents: 1036.0017819621164[0m
[37m[1m[2023-07-10 20:10:37,083][227910] Average Trajectory Length: 996.5513333333333[0m
[36m[2023-07-10 20:10:37,084][227910] mean_value=-867.9617436234371, max_value=476.05076067887035[0m
[37m[1m[2023-07-10 20:10:37,087][227910] New mean coefficients: [[ 2.2131567   2.4154155   0.32674724  2.2355494  -1.8361886 ]][0m
[37m[1m[2023-07-10 20:10:37,088][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:10:46,667][227910] train() took 9.58 seconds to complete[0m
[36m[2023-07-10 20:10:46,667][227910] FPS: 400933.37[0m
[36m[2023-07-10 20:10:46,669][227910] itr=1248, itrs=2000, Progress: 62.40%[0m
[36m[2023-07-10 20:10:58,289][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 20:10:58,289][227910] FPS: 331037.49[0m
[36m[2023-07-10 20:11:03,034][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:11:03,035][227910] Reward + Measures: [[1441.53194389    0.36779207    0.1769636     0.38766959    0.19670767]][0m
[37m[1m[2023-07-10 20:11:03,035][227910] Max Reward on eval: 1441.5319438894862[0m
[37m[1m[2023-07-10 20:11:03,035][227910] Min Reward on eval: 1441.5319438894862[0m
[37m[1m[2023-07-10 20:11:03,035][227910] Mean Reward across all agents: 1441.5319438894862[0m
[37m[1m[2023-07-10 20:11:03,036][227910] Average Trajectory Length: 998.3396666666666[0m
[36m[2023-07-10 20:11:08,389][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:11:08,390][227910] Reward + Measures: [[ 281.05654305    0.35994181    0.23851314    0.35043338    0.26138264]
 [1001.1259305     0.49360004    0.1426        0.48900005    0.22150002]
 [ 449.10441202    0.56590003    0.0891        0.53540003    0.24980001]
 ...
 [ 183.1678544     0.34          0.30740002    0.25980002    0.28579998]
 [ 342.58890221    0.34801292    0.31651762    0.29302779    0.25509384]
 [1415.56323811    0.38350001    0.2237        0.4488        0.21440001]][0m
[37m[1m[2023-07-10 20:11:08,390][227910] Max Reward on eval: 1715.169672333356[0m
[37m[1m[2023-07-10 20:11:08,390][227910] Min Reward on eval: -165.93311533721862[0m
[37m[1m[2023-07-10 20:11:08,390][227910] Mean Reward across all agents: 956.4016244255162[0m
[37m[1m[2023-07-10 20:11:08,391][227910] Average Trajectory Length: 996.8566666666667[0m
[36m[2023-07-10 20:11:08,394][227910] mean_value=-591.5507182383778, max_value=1474.8711822237356[0m
[37m[1m[2023-07-10 20:11:08,396][227910] New mean coefficients: [[ 2.3268435  3.336434  -0.9916473  2.5481389 -2.421936 ]][0m
[37m[1m[2023-07-10 20:11:08,397][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:11:18,028][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 20:11:18,029][227910] FPS: 398783.32[0m
[36m[2023-07-10 20:11:18,031][227910] itr=1249, itrs=2000, Progress: 62.45%[0m
[36m[2023-07-10 20:11:29,641][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 20:11:29,642][227910] FPS: 331259.04[0m
[36m[2023-07-10 20:11:34,391][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:11:34,391][227910] Reward + Measures: [[1569.82636335    0.36805046    0.1789373     0.39518541    0.19518816]][0m
[37m[1m[2023-07-10 20:11:34,391][227910] Max Reward on eval: 1569.826363349686[0m
[37m[1m[2023-07-10 20:11:34,392][227910] Min Reward on eval: 1569.826363349686[0m
[37m[1m[2023-07-10 20:11:34,392][227910] Mean Reward across all agents: 1569.826363349686[0m
[37m[1m[2023-07-10 20:11:34,392][227910] Average Trajectory Length: 996.4723333333333[0m
[36m[2023-07-10 20:11:39,789][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:11:39,795][227910] Reward + Measures: [[-296.7396303     0.43280002    0.34649998    0.3425        0.39000002]
 [ 140.98861935    0.44910002    0.22480002    0.34370002    0.31110001]
 [ 841.23696839    0.4454        0.15220001    0.44409999    0.20280002]
 ...
 [1065.54751179    0.4804        0.17279999    0.43350002    0.22159998]
 [1161.06058044    0.34150001    0.25840002    0.47229996    0.22679999]
 [1128.54606752    0.30380005    0.14980002    0.39289999    0.1605    ]][0m
[37m[1m[2023-07-10 20:11:39,795][227910] Max Reward on eval: 1851.3330198161711[0m
[37m[1m[2023-07-10 20:11:39,795][227910] Min Reward on eval: -720.2907921598293[0m
[37m[1m[2023-07-10 20:11:39,796][227910] Mean Reward across all agents: 759.2999480633936[0m
[37m[1m[2023-07-10 20:11:39,796][227910] Average Trajectory Length: 995.7946666666667[0m
[36m[2023-07-10 20:11:39,798][227910] mean_value=-1237.424967937345, max_value=1280.3843574767156[0m
[37m[1m[2023-07-10 20:11:39,800][227910] New mean coefficients: [[ 1.8846518   1.7525617  -0.63359386  3.1884513  -2.9425735 ]][0m
[37m[1m[2023-07-10 20:11:39,801][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:11:49,546][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 20:11:49,546][227910] FPS: 394128.02[0m
[36m[2023-07-10 20:11:49,549][227910] itr=1250, itrs=2000, Progress: 62.50%[0m
[37m[1m[2023-07-10 20:11:53,680][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001230[0m
[36m[2023-07-10 20:12:05,674][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 20:12:05,675][227910] FPS: 327830.90[0m
[36m[2023-07-10 20:12:10,378][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:12:10,379][227910] Reward + Measures: [[1735.95023272    0.35680667    0.18416686    0.39664647    0.19212492]][0m
[37m[1m[2023-07-10 20:12:10,379][227910] Max Reward on eval: 1735.9502327160214[0m
[37m[1m[2023-07-10 20:12:10,379][227910] Min Reward on eval: 1735.9502327160214[0m
[37m[1m[2023-07-10 20:12:10,380][227910] Mean Reward across all agents: 1735.9502327160214[0m
[37m[1m[2023-07-10 20:12:10,380][227910] Average Trajectory Length: 998.115[0m
[36m[2023-07-10 20:12:15,803][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:12:15,803][227910] Reward + Measures: [[1476.82853298    0.44759998    0.17739999    0.43870002    0.2052    ]
 [ 948.80550316    0.5553        0.14310001    0.50279999    0.20729999]
 [1184.30734927    0.52389997    0.12539999    0.49489999    0.19940001]
 ...
 [1063.41826027    0.43140003    0.13770001    0.47499999    0.16940001]
 [1154.08934114    0.48530003    0.17120002    0.51780003    0.22979999]
 [1543.03821037    0.36939999    0.15030001    0.38509998    0.17940001]][0m
[37m[1m[2023-07-10 20:12:15,803][227910] Max Reward on eval: 1915.668179593631[0m
[37m[1m[2023-07-10 20:12:15,804][227910] Min Reward on eval: 342.41190944369885[0m
[37m[1m[2023-07-10 20:12:15,804][227910] Mean Reward across all agents: 1200.0629905499038[0m
[37m[1m[2023-07-10 20:12:15,804][227910] Average Trajectory Length: 998.7666666666667[0m
[36m[2023-07-10 20:12:15,809][227910] mean_value=196.8082567726976, max_value=1588.814937413247[0m
[37m[1m[2023-07-10 20:12:15,811][227910] New mean coefficients: [[ 2.3376489  2.0536342 -0.6867185  4.81653   -3.2816894]][0m
[37m[1m[2023-07-10 20:12:15,812][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:12:25,545][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 20:12:25,545][227910] FPS: 394633.68[0m
[36m[2023-07-10 20:12:25,547][227910] itr=1251, itrs=2000, Progress: 62.55%[0m
[36m[2023-07-10 20:12:37,169][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 20:12:37,169][227910] FPS: 330958.29[0m
[36m[2023-07-10 20:12:41,928][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:12:41,934][227910] Reward + Measures: [[1869.27971116    0.34953675    0.1881147     0.39956275    0.19137806]][0m
[37m[1m[2023-07-10 20:12:41,935][227910] Max Reward on eval: 1869.2797111637508[0m
[37m[1m[2023-07-10 20:12:41,935][227910] Min Reward on eval: 1869.2797111637508[0m
[37m[1m[2023-07-10 20:12:41,936][227910] Mean Reward across all agents: 1869.2797111637508[0m
[37m[1m[2023-07-10 20:12:41,936][227910] Average Trajectory Length: 997.736[0m
[36m[2023-07-10 20:12:47,358][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:12:47,359][227910] Reward + Measures: [[ 889.08273682    0.2586        0.29370001    0.38820001    0.20560001]
 [ 804.8374065     0.40710002    0.18730001    0.38840005    0.2057    ]
 [ 544.24706378    0.54519999    0.2306        0.47499999    0.23290001]
 ...
 [1518.62432291    0.32220003    0.19560002    0.41760001    0.21019998]
 [1804.55023955    0.3211        0.21619999    0.46510002    0.20750001]
 [ 692.45346716    0.50270003    0.19550002    0.42249998    0.20799999]][0m
[37m[1m[2023-07-10 20:12:47,359][227910] Max Reward on eval: 2041.2291109943064[0m
[37m[1m[2023-07-10 20:12:47,359][227910] Min Reward on eval: -244.7547277199832[0m
[37m[1m[2023-07-10 20:12:47,360][227910] Mean Reward across all agents: 915.0403726243104[0m
[37m[1m[2023-07-10 20:12:47,360][227910] Average Trajectory Length: 984.6313333333333[0m
[36m[2023-07-10 20:12:47,362][227910] mean_value=-1496.1268396505266, max_value=1087.4408907672698[0m
[37m[1m[2023-07-10 20:12:47,364][227910] New mean coefficients: [[ 1.8636954  1.0859852 -1.8051858  3.9685783 -3.274166 ]][0m
[37m[1m[2023-07-10 20:12:47,365][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:12:57,093][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 20:12:57,093][227910] FPS: 394770.33[0m
[36m[2023-07-10 20:12:57,096][227910] itr=1252, itrs=2000, Progress: 62.60%[0m
[36m[2023-07-10 20:13:08,588][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 20:13:08,588][227910] FPS: 334770.94[0m
[36m[2023-07-10 20:13:13,396][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:13:13,397][227910] Reward + Measures: [[1982.92836333    0.348297      0.18982291    0.40735185    0.18720673]][0m
[37m[1m[2023-07-10 20:13:13,397][227910] Max Reward on eval: 1982.9283633317866[0m
[37m[1m[2023-07-10 20:13:13,397][227910] Min Reward on eval: 1982.9283633317866[0m
[37m[1m[2023-07-10 20:13:13,397][227910] Mean Reward across all agents: 1982.9283633317866[0m
[37m[1m[2023-07-10 20:13:13,397][227910] Average Trajectory Length: 997.435[0m
[36m[2023-07-10 20:13:18,706][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:13:18,711][227910] Reward + Measures: [[1382.4832664     0.2967        0.17290001    0.4395        0.18280001]
 [ 918.53313085    0.29029998    0.1194        0.27690002    0.16510001]
 [1699.04349933    0.37797257    0.18098946    0.36350593    0.17004712]
 ...
 [1273.99774004    0.3382        0.16370001    0.4059        0.18579999]
 [1951.68371747    0.36670002    0.1939        0.41350004    0.1851    ]
 [1255.11171481    0.30360001    0.16500001    0.45860001    0.19319999]][0m
[37m[1m[2023-07-10 20:13:18,711][227910] Max Reward on eval: 2114.391690507857[0m
[37m[1m[2023-07-10 20:13:18,711][227910] Min Reward on eval: 808.9283947265649[0m
[37m[1m[2023-07-10 20:13:18,712][227910] Mean Reward across all agents: 1540.0581576175944[0m
[37m[1m[2023-07-10 20:13:18,712][227910] Average Trajectory Length: 994.8043333333333[0m
[36m[2023-07-10 20:13:18,714][227910] mean_value=-249.20747209762573, max_value=1007.8484013210193[0m
[37m[1m[2023-07-10 20:13:18,717][227910] New mean coefficients: [[ 1.3648055  2.3901129 -0.5901363  4.424045  -2.4491868]][0m
[37m[1m[2023-07-10 20:13:18,718][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:13:28,415][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 20:13:28,416][227910] FPS: 396038.90[0m
[36m[2023-07-10 20:13:28,418][227910] itr=1253, itrs=2000, Progress: 62.65%[0m
[36m[2023-07-10 20:13:39,886][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 20:13:39,886][227910] FPS: 335388.27[0m
[36m[2023-07-10 20:13:44,671][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:13:44,671][227910] Reward + Measures: [[2120.73393802    0.35262313    0.19320731    0.42060378    0.18559872]][0m
[37m[1m[2023-07-10 20:13:44,671][227910] Max Reward on eval: 2120.733938023459[0m
[37m[1m[2023-07-10 20:13:44,672][227910] Min Reward on eval: 2120.733938023459[0m
[37m[1m[2023-07-10 20:13:44,672][227910] Mean Reward across all agents: 2120.733938023459[0m
[37m[1m[2023-07-10 20:13:44,672][227910] Average Trajectory Length: 998.7536666666666[0m
[36m[2023-07-10 20:13:50,032][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:13:50,033][227910] Reward + Measures: [[-1786.63443831     0.90030003     0.64650005     0.87279999
      0.0649    ]
 [-1841.60552397     0.87309998     0.73229998     0.83350003
      0.0872    ]
 [ 1763.38836687     0.4165         0.18969999     0.42069998
      0.1876    ]
 ...
 [-1529.28371939     0.82791775     0.3453036      0.80117458
      0.20502113]
 [ 1085.1633735      0.391          0.1596         0.4817
      0.2138    ]
 [ 1558.65624013     0.3554         0.16429999     0.4657
      0.1728    ]][0m
[37m[1m[2023-07-10 20:13:50,033][227910] Max Reward on eval: 2095.9001655915986[0m
[37m[1m[2023-07-10 20:13:50,033][227910] Min Reward on eval: -2307.5931867562817[0m
[37m[1m[2023-07-10 20:13:50,034][227910] Mean Reward across all agents: -74.43204948313891[0m
[37m[1m[2023-07-10 20:13:50,034][227910] Average Trajectory Length: 984.9006666666667[0m
[36m[2023-07-10 20:13:50,036][227910] mean_value=-914.9303147675976, max_value=941.9232039676906[0m
[37m[1m[2023-07-10 20:13:50,038][227910] New mean coefficients: [[ 1.7654161   1.2963889  -0.66316164  4.2085195  -1.8565831 ]][0m
[37m[1m[2023-07-10 20:13:50,039][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:13:59,737][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 20:13:59,738][227910] FPS: 396009.74[0m
[36m[2023-07-10 20:13:59,740][227910] itr=1254, itrs=2000, Progress: 62.70%[0m
[36m[2023-07-10 20:14:11,210][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 20:14:11,210][227910] FPS: 335333.47[0m
[36m[2023-07-10 20:14:15,876][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:14:15,882][227910] Reward + Measures: [[2268.93248167    0.34017369    0.1928997     0.43406537    0.18207556]][0m
[37m[1m[2023-07-10 20:14:15,882][227910] Max Reward on eval: 2268.932481666355[0m
[37m[1m[2023-07-10 20:14:15,883][227910] Min Reward on eval: 2268.932481666355[0m
[37m[1m[2023-07-10 20:14:15,883][227910] Mean Reward across all agents: 2268.932481666355[0m
[37m[1m[2023-07-10 20:14:15,883][227910] Average Trajectory Length: 998.4176666666666[0m
[36m[2023-07-10 20:14:21,467][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:14:21,473][227910] Reward + Measures: [[1938.21966943    0.41389999    0.2339        0.47040001    0.20190001]
 [2121.66440091    0.35970002    0.1928        0.46739998    0.1754    ]
 [1540.59006532    0.47200003    0.2145        0.52700001    0.19690001]
 ...
 [1213.42707544    0.5147        0.15800001    0.56389999    0.18020001]
 [2090.17638689    0.41339999    0.21409999    0.48359999    0.17829999]
 [1812.81183814    0.45690003    0.19660001    0.47709998    0.19560002]][0m
[37m[1m[2023-07-10 20:14:21,473][227910] Max Reward on eval: 2360.221422900283[0m
[37m[1m[2023-07-10 20:14:21,474][227910] Min Reward on eval: 508.07731596318774[0m
[37m[1m[2023-07-10 20:14:21,474][227910] Mean Reward across all agents: 1626.345287349032[0m
[37m[1m[2023-07-10 20:14:21,474][227910] Average Trajectory Length: 998.7186666666666[0m
[36m[2023-07-10 20:14:21,478][227910] mean_value=110.74968081632856, max_value=743.7975113052305[0m
[37m[1m[2023-07-10 20:14:21,480][227910] New mean coefficients: [[ 2.5254273   1.4092684  -0.33382428  4.28813    -1.8408928 ]][0m
[37m[1m[2023-07-10 20:14:21,481][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:14:31,102][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 20:14:31,102][227910] FPS: 399226.21[0m
[36m[2023-07-10 20:14:31,104][227910] itr=1255, itrs=2000, Progress: 62.75%[0m
[36m[2023-07-10 20:14:42,766][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 20:14:42,766][227910] FPS: 329848.38[0m
[36m[2023-07-10 20:14:47,581][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:14:47,581][227910] Reward + Measures: [[2388.72608577    0.32886308    0.19160271    0.43423653    0.17720845]][0m
[37m[1m[2023-07-10 20:14:47,581][227910] Max Reward on eval: 2388.726085765717[0m
[37m[1m[2023-07-10 20:14:47,582][227910] Min Reward on eval: 2388.726085765717[0m
[37m[1m[2023-07-10 20:14:47,582][227910] Mean Reward across all agents: 2388.726085765717[0m
[37m[1m[2023-07-10 20:14:47,582][227910] Average Trajectory Length: 999.6146666666666[0m
[36m[2023-07-10 20:14:53,010][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:14:53,016][227910] Reward + Measures: [[1169.603194      0.34240001    0.20740001    0.55540001    0.22289999]
 [ 780.27250122    0.28790003    0.32280001    0.51710004    0.29169998]
 [1906.47663475    0.30090001    0.2052        0.46630001    0.1837    ]
 ...
 [1303.27321648    0.4594        0.1787        0.48740003    0.19899999]
 [1813.56827082    0.3213        0.20580001    0.4059        0.193     ]
 [1297.02757451    0.32570001    0.21110001    0.44829997    0.20519999]][0m
[37m[1m[2023-07-10 20:14:53,016][227910] Max Reward on eval: 2428.88949481491[0m
[37m[1m[2023-07-10 20:14:53,016][227910] Min Reward on eval: 663.1733398189477[0m
[37m[1m[2023-07-10 20:14:53,017][227910] Mean Reward across all agents: 1754.9457708862722[0m
[37m[1m[2023-07-10 20:14:53,017][227910] Average Trajectory Length: 997.5886666666667[0m
[36m[2023-07-10 20:14:53,019][227910] mean_value=-273.8161047060204, max_value=536.3570232846769[0m
[37m[1m[2023-07-10 20:14:53,022][227910] New mean coefficients: [[ 2.0235717   1.4432571  -0.29154018  3.144267   -1.7654669 ]][0m
[37m[1m[2023-07-10 20:14:53,023][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:15:02,790][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 20:15:02,791][227910] FPS: 393202.48[0m
[36m[2023-07-10 20:15:02,793][227910] itr=1256, itrs=2000, Progress: 62.80%[0m
[36m[2023-07-10 20:15:14,369][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 20:15:14,369][227910] FPS: 332287.14[0m
[36m[2023-07-10 20:15:19,126][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:15:19,126][227910] Reward + Measures: [[2475.62470371    0.33160666    0.18906203    0.4348636     0.17628394]][0m
[37m[1m[2023-07-10 20:15:19,126][227910] Max Reward on eval: 2475.6247037052703[0m
[37m[1m[2023-07-10 20:15:19,126][227910] Min Reward on eval: 2475.6247037052703[0m
[37m[1m[2023-07-10 20:15:19,126][227910] Mean Reward across all agents: 2475.6247037052703[0m
[37m[1m[2023-07-10 20:15:19,127][227910] Average Trajectory Length: 999.3116666666666[0m
[36m[2023-07-10 20:15:24,534][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:15:24,587][227910] Reward + Measures: [[2307.3801732     0.32860002    0.1865        0.4901        0.1734    ]
 [2148.25777834    0.34230003    0.22049999    0.51610005    0.178     ]
 [1827.40136386    0.29420003    0.16510001    0.42880002    0.17299999]
 ...
 [2253.50317295    0.31190002    0.20130001    0.5072        0.17560001]
 [1897.18134724    0.3378        0.18520001    0.53549999    0.189     ]
 [1620.06768772    0.32050005    0.22090001    0.57560003    0.18920001]][0m
[37m[1m[2023-07-10 20:15:24,587][227910] Max Reward on eval: 2592.1997941980603[0m
[37m[1m[2023-07-10 20:15:24,587][227910] Min Reward on eval: 70.06029255092145[0m
[37m[1m[2023-07-10 20:15:24,587][227910] Mean Reward across all agents: 1748.6175224383164[0m
[37m[1m[2023-07-10 20:15:24,587][227910] Average Trajectory Length: 998.2503333333333[0m
[36m[2023-07-10 20:15:24,590][227910] mean_value=-261.09092631798865, max_value=2274.504792579173[0m
[37m[1m[2023-07-10 20:15:24,593][227910] New mean coefficients: [[ 1.7289525   0.5281656  -0.13847107  2.9812016  -1.2595308 ]][0m
[37m[1m[2023-07-10 20:15:24,594][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:15:34,283][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 20:15:34,283][227910] FPS: 396390.76[0m
[36m[2023-07-10 20:15:34,285][227910] itr=1257, itrs=2000, Progress: 62.85%[0m
[36m[2023-07-10 20:15:45,980][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 20:15:45,980][227910] FPS: 328904.31[0m
[36m[2023-07-10 20:15:50,771][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:15:50,772][227910] Reward + Measures: [[2605.57992126    0.32646608    0.18986088    0.43853125    0.17356274]][0m
[37m[1m[2023-07-10 20:15:50,772][227910] Max Reward on eval: 2605.579921262719[0m
[37m[1m[2023-07-10 20:15:50,772][227910] Min Reward on eval: 2605.579921262719[0m
[37m[1m[2023-07-10 20:15:50,772][227910] Mean Reward across all agents: 2605.579921262719[0m
[37m[1m[2023-07-10 20:15:50,773][227910] Average Trajectory Length: 998.3446666666666[0m
[36m[2023-07-10 20:15:56,301][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:15:56,301][227910] Reward + Measures: [[1959.91144739    0.33810002    0.1778        0.52580005    0.17220001]
 [2255.75564442    0.32010004    0.20770001    0.47150001    0.189     ]
 [2639.48888984    0.3251        0.1997        0.45749998    0.1805    ]
 ...
 [2347.1144746     0.3184        0.18980001    0.4686        0.175     ]
 [1892.73090588    0.3161        0.1547        0.41850001    0.17470001]
 [2466.34648247    0.35110003    0.1753        0.4698        0.16330002]][0m
[37m[1m[2023-07-10 20:15:56,301][227910] Max Reward on eval: 2711.763263041619[0m
[37m[1m[2023-07-10 20:15:56,302][227910] Min Reward on eval: 806.2946296410985[0m
[37m[1m[2023-07-10 20:15:56,302][227910] Mean Reward across all agents: 2103.487765841054[0m
[37m[1m[2023-07-10 20:15:56,302][227910] Average Trajectory Length: 998.9183333333333[0m
[36m[2023-07-10 20:15:56,305][227910] mean_value=77.15753735017584, max_value=2321.0796604989637[0m
[37m[1m[2023-07-10 20:15:56,308][227910] New mean coefficients: [[ 1.6653578  -0.4018038  -1.0883425   2.310484   -0.90012145]][0m
[37m[1m[2023-07-10 20:15:56,309][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:16:06,038][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 20:16:06,038][227910] FPS: 394746.38[0m
[36m[2023-07-10 20:16:06,041][227910] itr=1258, itrs=2000, Progress: 62.90%[0m
[36m[2023-07-10 20:16:17,572][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 20:16:17,573][227910] FPS: 333640.80[0m
[36m[2023-07-10 20:16:22,344][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:16:22,345][227910] Reward + Measures: [[2676.38991731    0.32070822    0.18689023    0.43612364    0.17190297]][0m
[37m[1m[2023-07-10 20:16:22,345][227910] Max Reward on eval: 2676.389917308349[0m
[37m[1m[2023-07-10 20:16:22,345][227910] Min Reward on eval: 2676.389917308349[0m
[37m[1m[2023-07-10 20:16:22,345][227910] Mean Reward across all agents: 2676.389917308349[0m
[37m[1m[2023-07-10 20:16:22,346][227910] Average Trajectory Length: 998.824[0m
[36m[2023-07-10 20:16:27,861][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:16:27,867][227910] Reward + Measures: [[1857.51902003    0.37059999    0.1577        0.48839998    0.2043    ]
 [1941.31012708    0.35893595    0.17457603    0.33832428    0.17183474]
 [2338.92728997    0.35520002    0.16320001    0.4831        0.18180001]
 ...
 [2570.31508764    0.36160001    0.1741        0.4621        0.1724    ]
 [2328.58441547    0.3488        0.16500001    0.45580003    0.18010001]
 [1764.45261308    0.3177        0.13960001    0.4183        0.1847    ]][0m
[37m[1m[2023-07-10 20:16:27,867][227910] Max Reward on eval: 2833.0616360818967[0m
[37m[1m[2023-07-10 20:16:27,867][227910] Min Reward on eval: 1075.3463749240618[0m
[37m[1m[2023-07-10 20:16:27,868][227910] Mean Reward across all agents: 2167.831260786505[0m
[37m[1m[2023-07-10 20:16:27,868][227910] Average Trajectory Length: 998.9373333333333[0m
[36m[2023-07-10 20:16:27,871][227910] mean_value=-128.718021913736, max_value=654.669645120965[0m
[37m[1m[2023-07-10 20:16:27,874][227910] New mean coefficients: [[ 1.4844154  -0.8362168  -0.72691125  2.1071014  -0.52797186]][0m
[37m[1m[2023-07-10 20:16:27,874][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:16:37,618][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 20:16:37,619][227910] FPS: 394160.50[0m
[36m[2023-07-10 20:16:37,621][227910] itr=1259, itrs=2000, Progress: 62.95%[0m
[36m[2023-07-10 20:16:49,322][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 20:16:49,322][227910] FPS: 328790.08[0m
[36m[2023-07-10 20:16:54,005][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:16:54,005][227910] Reward + Measures: [[2805.18963621    0.31873485    0.19342671    0.44428781    0.17123161]][0m
[37m[1m[2023-07-10 20:16:54,006][227910] Max Reward on eval: 2805.189636206293[0m
[37m[1m[2023-07-10 20:16:54,006][227910] Min Reward on eval: 2805.189636206293[0m
[37m[1m[2023-07-10 20:16:54,006][227910] Mean Reward across all agents: 2805.189636206293[0m
[37m[1m[2023-07-10 20:16:54,006][227910] Average Trajectory Length: 999.18[0m
[36m[2023-07-10 20:16:59,606][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:16:59,607][227910] Reward + Measures: [[2221.51138339    0.37259999    0.2253        0.47440001    0.19690001]
 [ 318.14322748    0.22800003    0.30410001    0.30410001    0.25680003]
 [-216.74962768    0.42575046    0.15787546    0.36990973    0.20352082]
 ...
 [ 654.02827543    0.52719998    0.142         0.39429998    0.1736    ]
 [1165.76050802    0.22765933    0.20871525    0.31756443    0.19245085]
 [1501.82542843    0.23110001    0.26120001    0.40859994    0.20850001]][0m
[37m[1m[2023-07-10 20:16:59,607][227910] Max Reward on eval: 2793.5504273497036[0m
[37m[1m[2023-07-10 20:16:59,607][227910] Min Reward on eval: -1059.7007141504787[0m
[37m[1m[2023-07-10 20:16:59,608][227910] Mean Reward across all agents: 997.6383254870582[0m
[37m[1m[2023-07-10 20:16:59,608][227910] Average Trajectory Length: 975.3436666666666[0m
[36m[2023-07-10 20:16:59,610][227910] mean_value=-1516.104340808359, max_value=880.9143570735064[0m
[37m[1m[2023-07-10 20:16:59,612][227910] New mean coefficients: [[ 1.321147   -0.13629413 -0.5486778   2.1232517  -0.41841015]][0m
[37m[1m[2023-07-10 20:16:59,613][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:17:09,329][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 20:17:09,329][227910] FPS: 395292.07[0m
[36m[2023-07-10 20:17:09,332][227910] itr=1260, itrs=2000, Progress: 63.00%[0m
[37m[1m[2023-07-10 20:17:13,212][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001240[0m
[36m[2023-07-10 20:17:25,164][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 20:17:25,164][227910] FPS: 328834.37[0m
[36m[2023-07-10 20:17:29,958][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:17:29,963][227910] Reward + Measures: [[2899.23225986    0.31521246    0.19422388    0.4507142     0.16870865]][0m
[37m[1m[2023-07-10 20:17:29,964][227910] Max Reward on eval: 2899.2322598563132[0m
[37m[1m[2023-07-10 20:17:29,964][227910] Min Reward on eval: 2899.2322598563132[0m
[37m[1m[2023-07-10 20:17:29,964][227910] Mean Reward across all agents: 2899.2322598563132[0m
[37m[1m[2023-07-10 20:17:29,964][227910] Average Trajectory Length: 999.151[0m
[36m[2023-07-10 20:17:35,628][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:17:35,634][227910] Reward + Measures: [[1869.92653258    0.24915372    0.23299678    0.29745406    0.16220506]
 [ 996.95572828    0.37979999    0.15930001    0.43240005    0.1691    ]
 [1563.27366714    0.32875791    0.16934209    0.38998947    0.17128421]
 ...
 [2559.89862364    0.2947        0.27720001    0.3673        0.1716    ]
 [2718.61022956    0.34439999    0.19849999    0.41880003    0.17100002]
 [2843.61181215    0.32860002    0.20180002    0.4501        0.17040001]][0m
[37m[1m[2023-07-10 20:17:35,634][227910] Max Reward on eval: 2988.223277347349[0m
[37m[1m[2023-07-10 20:17:35,635][227910] Min Reward on eval: -924.339142959658[0m
[37m[1m[2023-07-10 20:17:35,635][227910] Mean Reward across all agents: 1825.2025634647832[0m
[37m[1m[2023-07-10 20:17:35,635][227910] Average Trajectory Length: 991.0596666666667[0m
[36m[2023-07-10 20:17:35,637][227910] mean_value=-974.0505541836379, max_value=716.9373943997102[0m
[37m[1m[2023-07-10 20:17:35,640][227910] New mean coefficients: [[ 1.6086339  -0.07579042 -0.750456    1.5192163  -0.24554677]][0m
[37m[1m[2023-07-10 20:17:35,641][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:17:45,434][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 20:17:45,435][227910] FPS: 392153.06[0m
[36m[2023-07-10 20:17:45,437][227910] itr=1261, itrs=2000, Progress: 63.05%[0m
[36m[2023-07-10 20:17:57,055][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 20:17:57,055][227910] FPS: 331073.14[0m
[36m[2023-07-10 20:18:01,781][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:18:01,781][227910] Reward + Measures: [[3003.04311786    0.31325898    0.19443156    0.45866439    0.16777815]][0m
[37m[1m[2023-07-10 20:18:01,782][227910] Max Reward on eval: 3003.0431178617814[0m
[37m[1m[2023-07-10 20:18:01,782][227910] Min Reward on eval: 3003.0431178617814[0m
[37m[1m[2023-07-10 20:18:01,782][227910] Mean Reward across all agents: 3003.0431178617814[0m
[37m[1m[2023-07-10 20:18:01,782][227910] Average Trajectory Length: 999.1933333333333[0m
[36m[2023-07-10 20:18:07,187][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:18:07,188][227910] Reward + Measures: [[ 503.30220242    0.26879999    0.24519999    0.37960002    0.21830001]
 [2236.73599157    0.2658        0.23920003    0.43260002    0.19140002]
 [2259.02103162    0.26790002    0.2189        0.39930004    0.19510001]
 ...
 [ 609.63472043    0.26347741    0.23785138    0.40118048    0.21760745]
 [ 419.4283493     0.46350002    0.14060001    0.45620003    0.16610001]
 [ 712.54157334    0.27340001    0.23989999    0.4402        0.2156    ]][0m
[37m[1m[2023-07-10 20:18:07,188][227910] Max Reward on eval: 3043.8513079214376[0m
[37m[1m[2023-07-10 20:18:07,188][227910] Min Reward on eval: -303.90856875977477[0m
[37m[1m[2023-07-10 20:18:07,189][227910] Mean Reward across all agents: 1599.4864349444567[0m
[37m[1m[2023-07-10 20:18:07,189][227910] Average Trajectory Length: 989.689[0m
[36m[2023-07-10 20:18:07,191][227910] mean_value=-957.7655653700915, max_value=1824.1599176435207[0m
[37m[1m[2023-07-10 20:18:07,194][227910] New mean coefficients: [[ 1.2830248   0.67786443 -1.2578177   1.6740687  -0.76362365]][0m
[37m[1m[2023-07-10 20:18:07,195][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:18:16,935][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 20:18:16,935][227910] FPS: 394311.90[0m
[36m[2023-07-10 20:18:16,937][227910] itr=1262, itrs=2000, Progress: 63.10%[0m
[36m[2023-07-10 20:18:28,445][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 20:18:28,446][227910] FPS: 334204.86[0m
[36m[2023-07-10 20:18:33,190][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:18:33,190][227910] Reward + Measures: [[3135.72328637    0.31053907    0.19403671    0.45267168    0.16296941]][0m
[37m[1m[2023-07-10 20:18:33,190][227910] Max Reward on eval: 3135.723286370941[0m
[37m[1m[2023-07-10 20:18:33,190][227910] Min Reward on eval: 3135.723286370941[0m
[37m[1m[2023-07-10 20:18:33,191][227910] Mean Reward across all agents: 3135.723286370941[0m
[37m[1m[2023-07-10 20:18:33,191][227910] Average Trajectory Length: 999.6516666666666[0m
[36m[2023-07-10 20:18:38,631][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:18:38,631][227910] Reward + Measures: [[2834.6069659     0.29380003    0.23370002    0.45000002    0.17650001]
 [3135.82413115    0.29309997    0.21960001    0.47550002    0.1714    ]
 [2880.03956323    0.29542741    0.20698965    0.41871625    0.16213286]
 ...
 [3050.28974847    0.29800001    0.20910001    0.49530002    0.17279999]
 [2375.50477997    0.30920002    0.18840002    0.39970002    0.17569999]
 [3139.17564815    0.30560002    0.2096        0.45809999    0.16260001]][0m
[37m[1m[2023-07-10 20:18:38,632][227910] Max Reward on eval: 3274.0971442390232[0m
[37m[1m[2023-07-10 20:18:38,632][227910] Min Reward on eval: 1777.48126741639[0m
[37m[1m[2023-07-10 20:18:38,632][227910] Mean Reward across all agents: 2890.655334428969[0m
[37m[1m[2023-07-10 20:18:38,632][227910] Average Trajectory Length: 996.7413333333333[0m
[36m[2023-07-10 20:18:38,636][227910] mean_value=41.99649863849181, max_value=736.1248238081853[0m
[37m[1m[2023-07-10 20:18:38,639][227910] New mean coefficients: [[ 2.2537808  -0.01712579 -1.0874839   1.2484035  -1.1511517 ]][0m
[37m[1m[2023-07-10 20:18:38,640][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:18:48,265][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 20:18:48,266][227910] FPS: 399004.35[0m
[36m[2023-07-10 20:18:48,268][227910] itr=1263, itrs=2000, Progress: 63.15%[0m
[36m[2023-07-10 20:18:59,794][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 20:18:59,794][227910] FPS: 333786.97[0m
[36m[2023-07-10 20:19:04,608][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:19:04,609][227910] Reward + Measures: [[3240.83631712    0.30958796    0.1947649     0.44980761    0.15981385]][0m
[37m[1m[2023-07-10 20:19:04,609][227910] Max Reward on eval: 3240.8363171153565[0m
[37m[1m[2023-07-10 20:19:04,609][227910] Min Reward on eval: 3240.8363171153565[0m
[37m[1m[2023-07-10 20:19:04,609][227910] Mean Reward across all agents: 3240.8363171153565[0m
[37m[1m[2023-07-10 20:19:04,610][227910] Average Trajectory Length: 999.7719999999999[0m
[36m[2023-07-10 20:19:10,035][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:19:10,036][227910] Reward + Measures: [[3180.73176638    0.30579999    0.22550002    0.44370005    0.16339999]
 [3086.72219668    0.30560002    0.2299        0.43400002    0.1644    ]
 [3148.75157408    0.31690001    0.19510001    0.4436        0.1601    ]
 ...
 [2553.2089134     0.30329999    0.2132        0.46599999    0.17190002]
 [2641.96942399    0.29800001    0.2149        0.40570003    0.16599999]
 [2733.26458483    0.3116        0.23239999    0.42349997    0.16650002]][0m
[37m[1m[2023-07-10 20:19:10,036][227910] Max Reward on eval: 3316.2275262601906[0m
[37m[1m[2023-07-10 20:19:10,036][227910] Min Reward on eval: 1824.206840577419[0m
[37m[1m[2023-07-10 20:19:10,036][227910] Mean Reward across all agents: 2845.0738424936253[0m
[37m[1m[2023-07-10 20:19:10,037][227910] Average Trajectory Length: 998.9176666666666[0m
[36m[2023-07-10 20:19:10,039][227910] mean_value=-408.4589752235154, max_value=756.1085727891659[0m
[37m[1m[2023-07-10 20:19:10,041][227910] New mean coefficients: [[ 1.5788803  -0.11492299 -0.14159524  0.31616664 -0.39645725]][0m
[37m[1m[2023-07-10 20:19:10,042][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:19:19,901][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 20:19:19,901][227910] FPS: 389559.71[0m
[36m[2023-07-10 20:19:19,904][227910] itr=1264, itrs=2000, Progress: 63.20%[0m
[36m[2023-07-10 20:19:31,669][227910] train() took 11.74 seconds to complete[0m
[36m[2023-07-10 20:19:31,670][227910] FPS: 326978.16[0m
[36m[2023-07-10 20:19:36,386][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:19:36,387][227910] Reward + Measures: [[3330.36787488    0.30843392    0.20476721    0.44708264    0.16035517]][0m
[37m[1m[2023-07-10 20:19:36,387][227910] Max Reward on eval: 3330.367874875225[0m
[37m[1m[2023-07-10 20:19:36,387][227910] Min Reward on eval: 3330.367874875225[0m
[37m[1m[2023-07-10 20:19:36,388][227910] Mean Reward across all agents: 3330.367874875225[0m
[37m[1m[2023-07-10 20:19:36,388][227910] Average Trajectory Length: 999.0793333333334[0m
[36m[2023-07-10 20:19:41,805][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:19:41,806][227910] Reward + Measures: [[2782.87726279    0.29380003    0.20170002    0.45760003    0.1952    ]
 [2774.47425721    0.3012        0.20990001    0.43939996    0.1849    ]
 [1895.6845807     0.39369997    0.17920001    0.3601        0.16239999]
 ...
 [1182.40470824    0.46287307    0.22074585    0.34231555    0.16439916]
 [2892.95565288    0.32189998    0.2059        0.43650004    0.17570001]
 [2772.9243264     0.37410003    0.204         0.44619998    0.16800001]][0m
[37m[1m[2023-07-10 20:19:41,806][227910] Max Reward on eval: 3365.002979619801[0m
[37m[1m[2023-07-10 20:19:41,806][227910] Min Reward on eval: -581.1677497299504[0m
[37m[1m[2023-07-10 20:19:41,807][227910] Mean Reward across all agents: 2117.176159266013[0m
[37m[1m[2023-07-10 20:19:41,807][227910] Average Trajectory Length: 967.6953333333333[0m
[36m[2023-07-10 20:19:41,809][227910] mean_value=-801.8697344829524, max_value=654.4123221905938[0m
[37m[1m[2023-07-10 20:19:41,811][227910] New mean coefficients: [[ 0.86386055  0.03850435  0.47686017  0.45453703 -0.23469354]][0m
[37m[1m[2023-07-10 20:19:41,812][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:19:51,403][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 20:19:51,404][227910] FPS: 400439.10[0m
[36m[2023-07-10 20:19:51,406][227910] itr=1265, itrs=2000, Progress: 63.25%[0m
[36m[2023-07-10 20:20:02,918][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 20:20:02,918][227910] FPS: 334097.48[0m
[36m[2023-07-10 20:20:07,669][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:20:07,670][227910] Reward + Measures: [[1938.1794678     0.20861368    0.29110056    0.47407699    0.20402852]][0m
[37m[1m[2023-07-10 20:20:07,670][227910] Max Reward on eval: 1938.1794678049143[0m
[37m[1m[2023-07-10 20:20:07,670][227910] Min Reward on eval: 1938.1794678049143[0m
[37m[1m[2023-07-10 20:20:07,671][227910] Mean Reward across all agents: 1938.1794678049143[0m
[37m[1m[2023-07-10 20:20:07,671][227910] Average Trajectory Length: 986.8356666666666[0m
[36m[2023-07-10 20:20:13,312][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:20:13,312][227910] Reward + Measures: [[1683.15767123    0.21070002    0.30990002    0.54470003    0.2105    ]
 [1711.41948502    0.20549999    0.29920003    0.55890006    0.2066    ]
 [1832.84345749    0.18900001    0.3026        0.4928        0.20920001]
 ...
 [1992.56771184    0.23559999    0.2595        0.45240003    0.207     ]
 [1994.15454968    0.20944385    0.27823979    0.41538998    0.19911562]
 [2017.1840462     0.2211        0.32370001    0.37760001    0.18429999]][0m
[37m[1m[2023-07-10 20:20:13,313][227910] Max Reward on eval: 2319.8248138875233[0m
[37m[1m[2023-07-10 20:20:13,313][227910] Min Reward on eval: -155.15783428655703[0m
[37m[1m[2023-07-10 20:20:13,313][227910] Mean Reward across all agents: 1626.9817776521684[0m
[37m[1m[2023-07-10 20:20:13,313][227910] Average Trajectory Length: 991.958[0m
[36m[2023-07-10 20:20:13,315][227910] mean_value=-2193.5676989310855, max_value=381.96386988073095[0m
[37m[1m[2023-07-10 20:20:13,317][227910] New mean coefficients: [[ 2.164985   -0.3414344  -0.67499876  0.07907045 -0.2139638 ]][0m
[37m[1m[2023-07-10 20:20:13,318][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:20:23,115][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 20:20:23,116][227910] FPS: 392010.06[0m
[36m[2023-07-10 20:20:23,118][227910] itr=1266, itrs=2000, Progress: 63.30%[0m
[36m[2023-07-10 20:20:34,829][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 20:20:34,829][227910] FPS: 328457.39[0m
[36m[2023-07-10 20:20:39,614][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:20:39,614][227910] Reward + Measures: [[2150.8182141     0.21142393    0.28531647    0.48034504    0.19873938]][0m
[37m[1m[2023-07-10 20:20:39,614][227910] Max Reward on eval: 2150.818214099965[0m
[37m[1m[2023-07-10 20:20:39,615][227910] Min Reward on eval: 2150.818214099965[0m
[37m[1m[2023-07-10 20:20:39,615][227910] Mean Reward across all agents: 2150.818214099965[0m
[37m[1m[2023-07-10 20:20:39,615][227910] Average Trajectory Length: 988.2763333333334[0m
[36m[2023-07-10 20:20:45,115][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:20:45,115][227910] Reward + Measures: [[1783.50593565    0.2053        0.22879998    0.36699998    0.17820001]
 [2073.36654086    0.22290002    0.27779999    0.43259999    0.2077    ]
 [2214.76885076    0.21233959    0.29480726    0.45732781    0.19623111]
 ...
 [1972.01627814    0.2059        0.271         0.42880002    0.20460001]
 [2066.79195237    0.18409136    0.27763113    0.42913038    0.1846606 ]
 [1971.74193978    0.20349999    0.26930001    0.45390001    0.20510001]][0m
[37m[1m[2023-07-10 20:20:45,115][227910] Max Reward on eval: 2396.9623779900953[0m
[37m[1m[2023-07-10 20:20:45,116][227910] Min Reward on eval: 1390.6276058500284[0m
[37m[1m[2023-07-10 20:20:45,116][227910] Mean Reward across all agents: 2032.5719471125274[0m
[37m[1m[2023-07-10 20:20:45,116][227910] Average Trajectory Length: 987.5943333333333[0m
[36m[2023-07-10 20:20:45,118][227910] mean_value=-2140.700449306405, max_value=1840.8418425435543[0m
[37m[1m[2023-07-10 20:20:45,120][227910] New mean coefficients: [[ 2.7195926   0.19710955 -0.03778988 -0.56855273 -0.67999804]][0m
[37m[1m[2023-07-10 20:20:45,121][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:20:54,798][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 20:20:54,798][227910] FPS: 396905.12[0m
[36m[2023-07-10 20:20:54,800][227910] itr=1267, itrs=2000, Progress: 63.35%[0m
[36m[2023-07-10 20:21:06,283][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 20:21:06,284][227910] FPS: 335033.17[0m
[36m[2023-07-10 20:21:11,071][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:21:11,072][227910] Reward + Measures: [[2338.53358121    0.21275887    0.28535172    0.48801214    0.1947604 ]][0m
[37m[1m[2023-07-10 20:21:11,072][227910] Max Reward on eval: 2338.533581206156[0m
[37m[1m[2023-07-10 20:21:11,072][227910] Min Reward on eval: 2338.533581206156[0m
[37m[1m[2023-07-10 20:21:11,072][227910] Mean Reward across all agents: 2338.533581206156[0m
[37m[1m[2023-07-10 20:21:11,073][227910] Average Trajectory Length: 990.6343333333333[0m
[36m[2023-07-10 20:21:16,498][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:21:16,504][227910] Reward + Measures: [[2225.35641831    0.2079        0.34299999    0.46160004    0.20639999]
 [ 572.08567066    0.29340002    0.52440006    0.27640003    0.42139998]
 [ 486.90256783    0.2622        0.46880004    0.46790004    0.40270001]
 ...
 [2240.90198261    0.21469998    0.32339999    0.50749999    0.20750001]
 [1341.71223782    0.2421        0.2375        0.24960001    0.18520001]
 [1845.44377629    0.22320001    0.30930001    0.54790002    0.21010001]][0m
[37m[1m[2023-07-10 20:21:16,504][227910] Max Reward on eval: 2568.604477152415[0m
[37m[1m[2023-07-10 20:21:16,505][227910] Min Reward on eval: -412.1777647815703[0m
[37m[1m[2023-07-10 20:21:16,505][227910] Mean Reward across all agents: 1750.6982563177244[0m
[37m[1m[2023-07-10 20:21:16,505][227910] Average Trajectory Length: 990.0926666666667[0m
[36m[2023-07-10 20:21:16,507][227910] mean_value=-1778.103298650609, max_value=173.4946143156833[0m
[37m[1m[2023-07-10 20:21:16,509][227910] New mean coefficients: [[ 3.2222652   0.96891475 -0.21453953 -0.76275945 -0.5394279 ]][0m
[37m[1m[2023-07-10 20:21:16,510][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:21:26,189][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 20:21:26,189][227910] FPS: 396805.65[0m
[36m[2023-07-10 20:21:26,191][227910] itr=1268, itrs=2000, Progress: 63.40%[0m
[36m[2023-07-10 20:21:37,707][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 20:21:37,708][227910] FPS: 333984.89[0m
[36m[2023-07-10 20:21:42,554][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:21:42,555][227910] Reward + Measures: [[2524.75964884    0.2674948     0.23087625    0.44393358    0.18805739]][0m
[37m[1m[2023-07-10 20:21:42,555][227910] Max Reward on eval: 2524.7596488356535[0m
[37m[1m[2023-07-10 20:21:42,555][227910] Min Reward on eval: 2524.7596488356535[0m
[37m[1m[2023-07-10 20:21:42,555][227910] Mean Reward across all agents: 2524.7596488356535[0m
[37m[1m[2023-07-10 20:21:42,556][227910] Average Trajectory Length: 993.582[0m
[36m[2023-07-10 20:21:47,966][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:21:47,967][227910] Reward + Measures: [[1929.2485214     0.2852        0.22850001    0.47910005    0.18789999]
 [1104.67107631    0.24070001    0.25030002    0.32139999    0.21790002]
 [ 579.37637754    0.24473868    0.27620104    0.36161026    0.2246574 ]
 ...
 [1986.36928069    0.354         0.28690001    0.54490006    0.1997    ]
 [1555.23780796    0.3328        0.29599997    0.43989998    0.27290002]
 [1590.9551638     0.4039        0.2714        0.60080004    0.19669999]][0m
[37m[1m[2023-07-10 20:21:47,967][227910] Max Reward on eval: 2794.8163802589056[0m
[37m[1m[2023-07-10 20:21:47,968][227910] Min Reward on eval: -1021.6955930660712[0m
[37m[1m[2023-07-10 20:21:47,968][227910] Mean Reward across all agents: 1128.3632110702379[0m
[37m[1m[2023-07-10 20:21:47,968][227910] Average Trajectory Length: 954.1393333333333[0m
[36m[2023-07-10 20:21:47,970][227910] mean_value=-1300.2445948518302, max_value=632.4304188162279[0m
[37m[1m[2023-07-10 20:21:47,972][227910] New mean coefficients: [[ 3.1394143   0.6783057  -0.27782977  0.31764948 -0.3046775 ]][0m
[37m[1m[2023-07-10 20:21:47,973][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:21:57,708][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 20:21:57,708][227910] FPS: 394536.44[0m
[36m[2023-07-10 20:21:57,710][227910] itr=1269, itrs=2000, Progress: 63.45%[0m
[36m[2023-07-10 20:22:09,416][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 20:22:09,416][227910] FPS: 328567.03[0m
[36m[2023-07-10 20:22:14,193][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:22:14,193][227910] Reward + Measures: [[2694.43402234    0.27118686    0.24068294    0.40732902    0.18507101]][0m
[37m[1m[2023-07-10 20:22:14,194][227910] Max Reward on eval: 2694.434022336884[0m
[37m[1m[2023-07-10 20:22:14,194][227910] Min Reward on eval: 2694.434022336884[0m
[37m[1m[2023-07-10 20:22:14,194][227910] Mean Reward across all agents: 2694.434022336884[0m
[37m[1m[2023-07-10 20:22:14,194][227910] Average Trajectory Length: 988.9076666666666[0m
[36m[2023-07-10 20:22:19,853][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:22:19,858][227910] Reward + Measures: [[2180.63945829    0.25350001    0.22050002    0.37759998    0.16770001]
 [2301.63748282    0.2737        0.23290001    0.4646        0.18780001]
 [2502.33919599    0.25530002    0.22760001    0.38780001    0.1882    ]
 ...
 [2332.24036787    0.23985164    0.22744504    0.35161304    0.18009104]
 [2341.96816348    0.2615        0.21610001    0.44080001    0.1797    ]
 [2042.44315382    0.24208097    0.23476683    0.28984833    0.17795773]][0m
[37m[1m[2023-07-10 20:22:19,859][227910] Max Reward on eval: 2786.7615287296007[0m
[37m[1m[2023-07-10 20:22:19,859][227910] Min Reward on eval: 776.7611782041728[0m
[37m[1m[2023-07-10 20:22:19,859][227910] Mean Reward across all agents: 2190.0437003106176[0m
[37m[1m[2023-07-10 20:22:19,860][227910] Average Trajectory Length: 955.971[0m
[36m[2023-07-10 20:22:19,861][227910] mean_value=-733.8847737397608, max_value=1679.099018971032[0m
[37m[1m[2023-07-10 20:22:19,864][227910] New mean coefficients: [[ 3.3222811   0.45471212 -0.26331055 -0.09347156 -0.79614955]][0m
[37m[1m[2023-07-10 20:22:19,865][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:22:29,517][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 20:22:29,517][227910] FPS: 397926.04[0m
[36m[2023-07-10 20:22:29,519][227910] itr=1270, itrs=2000, Progress: 63.50%[0m
[37m[1m[2023-07-10 20:22:33,352][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001250[0m
[36m[2023-07-10 20:22:45,225][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 20:22:45,225][227910] FPS: 331239.86[0m
[36m[2023-07-10 20:22:49,972][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:22:49,972][227910] Reward + Measures: [[2892.3754244     0.2869322     0.25141767    0.39703262    0.18557929]][0m
[37m[1m[2023-07-10 20:22:49,972][227910] Max Reward on eval: 2892.375424399081[0m
[37m[1m[2023-07-10 20:22:49,972][227910] Min Reward on eval: 2892.375424399081[0m
[37m[1m[2023-07-10 20:22:49,973][227910] Mean Reward across all agents: 2892.375424399081[0m
[37m[1m[2023-07-10 20:22:49,973][227910] Average Trajectory Length: 990.887[0m
[36m[2023-07-10 20:22:55,304][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:22:55,304][227910] Reward + Measures: [[ 979.6356256     0.19960189    0.20757416    0.2174152     0.16001494]
 [-159.69518689    0.15525565    0.17158598    0.183275      0.12592487]
 [1454.17704252    0.19830233    0.23659895    0.25329787    0.16337906]
 ...
 [ 877.24352727    0.20284639    0.21340282    0.26844007    0.16992389]
 [2405.7073332     0.35429999    0.32099998    0.49330002    0.18870001]
 [2251.80993236    0.36929998    0.2852        0.52899998    0.1846    ]][0m
[37m[1m[2023-07-10 20:22:55,305][227910] Max Reward on eval: 2968.155098248017[0m
[37m[1m[2023-07-10 20:22:55,305][227910] Min Reward on eval: -445.3813959879364[0m
[37m[1m[2023-07-10 20:22:55,305][227910] Mean Reward across all agents: 1357.1667620185374[0m
[37m[1m[2023-07-10 20:22:55,305][227910] Average Trajectory Length: 767.3083333333333[0m
[36m[2023-07-10 20:22:55,308][227910] mean_value=-1417.4862125067957, max_value=630.7254769334052[0m
[37m[1m[2023-07-10 20:22:55,310][227910] New mean coefficients: [[ 3.215899    0.26375222 -0.07864563  0.05974711  0.42597646]][0m
[37m[1m[2023-07-10 20:22:55,311][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:23:05,208][227910] train() took 9.90 seconds to complete[0m
[36m[2023-07-10 20:23:05,208][227910] FPS: 388066.92[0m
[36m[2023-07-10 20:23:05,211][227910] itr=1271, itrs=2000, Progress: 63.55%[0m
[36m[2023-07-10 20:23:16,727][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 20:23:16,727][227910] FPS: 334125.93[0m
[36m[2023-07-10 20:23:21,449][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:23:21,449][227910] Reward + Measures: [[3078.46547008    0.29393008    0.26010001    0.38156453    0.18625033]][0m
[37m[1m[2023-07-10 20:23:21,449][227910] Max Reward on eval: 3078.465470077577[0m
[37m[1m[2023-07-10 20:23:21,450][227910] Min Reward on eval: 3078.465470077577[0m
[37m[1m[2023-07-10 20:23:21,450][227910] Mean Reward across all agents: 3078.465470077577[0m
[37m[1m[2023-07-10 20:23:21,450][227910] Average Trajectory Length: 995.327[0m
[36m[2023-07-10 20:23:27,156][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:23:27,157][227910] Reward + Measures: [[-631.10437216    0.15441455    0.10729211    0.18572113    0.16070627]
 [2298.60734686    0.40770003    0.2429        0.47080001    0.16890001]
 [ 826.25630347    0.3204        0.17479999    0.46650001    0.18870001]
 ...
 [2123.78417495    0.2368805     0.21570244    0.32537317    0.18328536]
 [ 933.34685035    0.1899        0.2325        0.27449998    0.1849    ]
 [ 319.7341798     0.14643133    0.21875067    0.31904349    0.17067853]][0m
[37m[1m[2023-07-10 20:23:27,157][227910] Max Reward on eval: 2854.1803402061573[0m
[37m[1m[2023-07-10 20:23:27,157][227910] Min Reward on eval: -1054.669727660832[0m
[37m[1m[2023-07-10 20:23:27,158][227910] Mean Reward across all agents: 954.8776097845504[0m
[37m[1m[2023-07-10 20:23:27,158][227910] Average Trajectory Length: 931.847[0m
[36m[2023-07-10 20:23:27,160][227910] mean_value=-1335.0019461599666, max_value=1473.8501185197265[0m
[37m[1m[2023-07-10 20:23:27,162][227910] New mean coefficients: [[ 2.6490455   0.31955707  0.01430383  0.20670168 -0.2735439 ]][0m
[37m[1m[2023-07-10 20:23:27,163][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:23:37,063][227910] train() took 9.90 seconds to complete[0m
[36m[2023-07-10 20:23:37,064][227910] FPS: 387937.69[0m
[36m[2023-07-10 20:23:37,066][227910] itr=1272, itrs=2000, Progress: 63.60%[0m
[36m[2023-07-10 20:23:48,828][227910] train() took 11.74 seconds to complete[0m
[36m[2023-07-10 20:23:48,828][227910] FPS: 327008.15[0m
[36m[2023-07-10 20:23:53,740][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:23:53,740][227910] Reward + Measures: [[3220.51755275    0.30316511    0.26549724    0.35837975    0.18616296]][0m
[37m[1m[2023-07-10 20:23:53,740][227910] Max Reward on eval: 3220.5175527539204[0m
[37m[1m[2023-07-10 20:23:53,741][227910] Min Reward on eval: 3220.5175527539204[0m
[37m[1m[2023-07-10 20:23:53,741][227910] Mean Reward across all agents: 3220.5175527539204[0m
[37m[1m[2023-07-10 20:23:53,741][227910] Average Trajectory Length: 990.0116666666667[0m
[36m[2023-07-10 20:23:59,245][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:23:59,245][227910] Reward + Measures: [[2309.17068329    0.34446043    0.23845835    0.40105835    0.17684792]
 [  -5.17941435    0.66390008    0.0367        0.79909998    0.6573    ]
 [2724.31238799    0.359         0.23899999    0.4111        0.17290001]
 ...
 [2988.42776093    0.34039998    0.27669999    0.38520002    0.19099998]
 [ 706.74986399    0.29090002    0.42389998    0.48260003    0.42360002]
 [1894.83246038    0.26659998    0.2261        0.3565        0.1631    ]][0m
[37m[1m[2023-07-10 20:23:59,246][227910] Max Reward on eval: 3349.8268010511993[0m
[37m[1m[2023-07-10 20:23:59,246][227910] Min Reward on eval: -340.00185059409125[0m
[37m[1m[2023-07-10 20:23:59,246][227910] Mean Reward across all agents: 1698.0284555512508[0m
[37m[1m[2023-07-10 20:23:59,246][227910] Average Trajectory Length: 986.4693333333333[0m
[36m[2023-07-10 20:23:59,249][227910] mean_value=-1078.0170375529456, max_value=1092.6928068100358[0m
[37m[1m[2023-07-10 20:23:59,252][227910] New mean coefficients: [[ 3.1407862  -0.3437711  -0.06832903  0.06819829  0.34708983]][0m
[37m[1m[2023-07-10 20:23:59,253][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:24:09,063][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 20:24:09,063][227910] FPS: 391487.31[0m
[36m[2023-07-10 20:24:09,066][227910] itr=1273, itrs=2000, Progress: 63.65%[0m
[36m[2023-07-10 20:24:20,695][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 20:24:20,695][227910] FPS: 330759.01[0m
[36m[2023-07-10 20:24:25,409][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:24:25,409][227910] Reward + Measures: [[3375.15451579    0.30097982    0.2634227     0.34160089    0.18767141]][0m
[37m[1m[2023-07-10 20:24:25,409][227910] Max Reward on eval: 3375.154515786502[0m
[37m[1m[2023-07-10 20:24:25,409][227910] Min Reward on eval: 3375.154515786502[0m
[37m[1m[2023-07-10 20:24:25,410][227910] Mean Reward across all agents: 3375.154515786502[0m
[37m[1m[2023-07-10 20:24:25,410][227910] Average Trajectory Length: 989.831[0m
[36m[2023-07-10 20:24:30,841][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:24:30,842][227910] Reward + Measures: [[ 263.2464653     0.11843266    0.11687207    0.15497208    0.11024888]
 [3138.01843146    0.27629918    0.27087274    0.30227852    0.18345344]
 [2717.51339244    0.27361515    0.25534242    0.33022124    0.1780788 ]
 ...
 [2580.20894285    0.2668483     0.27007937    0.28380984    0.18105459]
 [1788.46197308    0.25603351    0.21614337    0.48315722    0.19862139]
 [3157.59973326    0.28080001    0.27479997    0.35930002    0.1973    ]][0m
[37m[1m[2023-07-10 20:24:30,842][227910] Max Reward on eval: 3474.5798520608805[0m
[37m[1m[2023-07-10 20:24:30,842][227910] Min Reward on eval: -273.7745360418776[0m
[37m[1m[2023-07-10 20:24:30,842][227910] Mean Reward across all agents: 2219.6092483892085[0m
[37m[1m[2023-07-10 20:24:30,843][227910] Average Trajectory Length: 957.2433333333333[0m
[36m[2023-07-10 20:24:30,845][227910] mean_value=-890.7633557146095, max_value=1191.0920708615[0m
[37m[1m[2023-07-10 20:24:30,847][227910] New mean coefficients: [[ 2.6142607  -0.268143   -0.00062849 -0.3008138   0.11055163]][0m
[37m[1m[2023-07-10 20:24:30,848][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:24:40,575][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 20:24:40,576][227910] FPS: 394842.37[0m
[36m[2023-07-10 20:24:40,578][227910] itr=1274, itrs=2000, Progress: 63.70%[0m
[36m[2023-07-10 20:24:52,152][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 20:24:52,152][227910] FPS: 332408.67[0m
[36m[2023-07-10 20:24:56,980][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:24:56,980][227910] Reward + Measures: [[3572.45576262    0.31378314    0.27062455    0.31947604    0.189022  ]][0m
[37m[1m[2023-07-10 20:24:56,980][227910] Max Reward on eval: 3572.455762617046[0m
[37m[1m[2023-07-10 20:24:56,981][227910] Min Reward on eval: 3572.455762617046[0m
[37m[1m[2023-07-10 20:24:56,981][227910] Mean Reward across all agents: 3572.455762617046[0m
[37m[1m[2023-07-10 20:24:56,981][227910] Average Trajectory Length: 995.274[0m
[36m[2023-07-10 20:25:02,329][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:25:02,329][227910] Reward + Measures: [[2246.19738346    0.34590003    0.28370002    0.51290005    0.2194    ]
 [3008.19640298    0.3585        0.27210003    0.33970001    0.18430001]
 [2308.55515272    0.18359999    0.22019999    0.23889999    0.14750002]
 ...
 [3067.65903138    0.31259999    0.2622        0.36680001    0.1718    ]
 [1885.57389902    0.32780001    0.2273        0.38750002    0.18900001]
 [2135.42541228    0.32570001    0.21110001    0.45020005    0.1894    ]][0m
[37m[1m[2023-07-10 20:25:02,329][227910] Max Reward on eval: 3677.3808947885873[0m
[37m[1m[2023-07-10 20:25:02,330][227910] Min Reward on eval: -209.97246039715827[0m
[37m[1m[2023-07-10 20:25:02,330][227910] Mean Reward across all agents: 1656.8593451370996[0m
[37m[1m[2023-07-10 20:25:02,330][227910] Average Trajectory Length: 896.3093333333333[0m
[36m[2023-07-10 20:25:02,332][227910] mean_value=-1545.8744133089092, max_value=1210.9437575679285[0m
[37m[1m[2023-07-10 20:25:02,335][227910] New mean coefficients: [[ 2.4798603  -0.7635499   0.20953774 -0.28375205  0.81663823]][0m
[37m[1m[2023-07-10 20:25:02,336][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:25:12,094][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 20:25:12,094][227910] FPS: 393589.47[0m
[36m[2023-07-10 20:25:12,096][227910] itr=1275, itrs=2000, Progress: 63.75%[0m
[36m[2023-07-10 20:25:23,717][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 20:25:23,718][227910] FPS: 330944.37[0m
[36m[2023-07-10 20:25:28,504][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:25:28,505][227910] Reward + Measures: [[3714.06871084    0.31985205    0.26836577    0.30154639    0.18917799]][0m
[37m[1m[2023-07-10 20:25:28,505][227910] Max Reward on eval: 3714.068710839022[0m
[37m[1m[2023-07-10 20:25:28,505][227910] Min Reward on eval: 3714.068710839022[0m
[37m[1m[2023-07-10 20:25:28,505][227910] Mean Reward across all agents: 3714.068710839022[0m
[37m[1m[2023-07-10 20:25:28,505][227910] Average Trajectory Length: 992.7186666666666[0m
[36m[2023-07-10 20:25:34,059][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:25:34,060][227910] Reward + Measures: [[3186.89387419    0.31189999    0.28170002    0.37380001    0.1908    ]
 [3583.65461513    0.29732138    0.2742846     0.28832564    0.18253504]
 [3399.31663404    0.28352985    0.26440737    0.3012366     0.18782964]
 ...
 [3111.4893552     0.31836733    0.28271291    0.30846635    0.19002376]
 [3039.08348266    0.29860002    0.2511        0.35170004    0.1858    ]
 [3101.83858466    0.35780001    0.2712        0.34489998    0.18700001]][0m
[37m[1m[2023-07-10 20:25:34,060][227910] Max Reward on eval: 3804.716926303646[0m
[37m[1m[2023-07-10 20:25:34,060][227910] Min Reward on eval: 569.9770319621196[0m
[37m[1m[2023-07-10 20:25:34,061][227910] Mean Reward across all agents: 2945.620548389002[0m
[37m[1m[2023-07-10 20:25:34,061][227910] Average Trajectory Length: 983.6326666666666[0m
[36m[2023-07-10 20:25:34,063][227910] mean_value=-822.8485455497563, max_value=310.62968509410985[0m
[37m[1m[2023-07-10 20:25:34,065][227910] New mean coefficients: [[ 1.4583981  -0.5726578   0.23512486 -0.00228527  0.4370969 ]][0m
[37m[1m[2023-07-10 20:25:34,066][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:25:43,691][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 20:25:43,692][227910] FPS: 399014.08[0m
[36m[2023-07-10 20:25:43,694][227910] itr=1276, itrs=2000, Progress: 63.80%[0m
[36m[2023-07-10 20:25:55,191][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 20:25:55,192][227910] FPS: 334502.57[0m
[36m[2023-07-10 20:25:59,949][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:25:59,949][227910] Reward + Measures: [[3842.20206359    0.32435647    0.25969967    0.28897858    0.18865205]][0m
[37m[1m[2023-07-10 20:25:59,949][227910] Max Reward on eval: 3842.2020635936024[0m
[37m[1m[2023-07-10 20:25:59,950][227910] Min Reward on eval: 3842.2020635936024[0m
[37m[1m[2023-07-10 20:25:59,950][227910] Mean Reward across all agents: 3842.2020635936024[0m
[37m[1m[2023-07-10 20:25:59,950][227910] Average Trajectory Length: 993.978[0m
[36m[2023-07-10 20:26:05,414][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:26:05,415][227910] Reward + Measures: [[2482.96074063    0.3513        0.21970001    0.32769999    0.1925    ]
 [1745.75314002    0.26100001    0.27849999    0.43000004    0.20830002]
 [1963.41507928    0.24370001    0.33750001    0.40149999    0.1978    ]
 ...
 [2303.83495037    0.24890001    0.2237        0.3407        0.1743    ]
 [2714.6994008     0.29450002    0.21429999    0.34479997    0.1816    ]
 [2274.67602245    0.31300002    0.2052        0.32479998    0.17660001]][0m
[37m[1m[2023-07-10 20:26:05,415][227910] Max Reward on eval: 3712.5766646022444[0m
[37m[1m[2023-07-10 20:26:05,415][227910] Min Reward on eval: -411.8339902146137[0m
[37m[1m[2023-07-10 20:26:05,416][227910] Mean Reward across all agents: 2113.083472398103[0m
[37m[1m[2023-07-10 20:26:05,416][227910] Average Trajectory Length: 981.6653333333333[0m
[36m[2023-07-10 20:26:05,418][227910] mean_value=-1355.9144785115395, max_value=743.7268382501759[0m
[37m[1m[2023-07-10 20:26:05,420][227910] New mean coefficients: [[ 1.0870107  -0.71728987  0.09085962  0.3144863   0.8468406 ]][0m
[37m[1m[2023-07-10 20:26:05,421][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:26:15,046][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 20:26:15,046][227910] FPS: 399031.61[0m
[36m[2023-07-10 20:26:15,048][227910] itr=1277, itrs=2000, Progress: 63.85%[0m
[36m[2023-07-10 20:26:26,629][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 20:26:26,630][227910] FPS: 332127.72[0m
[36m[2023-07-10 20:26:31,373][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:26:31,373][227910] Reward + Measures: [[3969.57451153    0.33476323    0.25801608    0.28687665    0.1903431 ]][0m
[37m[1m[2023-07-10 20:26:31,374][227910] Max Reward on eval: 3969.574511526448[0m
[37m[1m[2023-07-10 20:26:31,374][227910] Min Reward on eval: 3969.574511526448[0m
[37m[1m[2023-07-10 20:26:31,374][227910] Mean Reward across all agents: 3969.574511526448[0m
[37m[1m[2023-07-10 20:26:31,374][227910] Average Trajectory Length: 994.807[0m
[36m[2023-07-10 20:26:36,890][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:26:36,890][227910] Reward + Measures: [[3039.46490607    0.3389        0.3017        0.354         0.20640002]
 [1894.622856      0.3303        0.25770003    0.29719999    0.22950001]
 [ 459.75632239    0.19081156    0.14054489    0.21161699    0.12034013]
 ...
 [ 815.61964818    0.48100001    0.30750003    0.44749999    0.32300001]
 [3148.34950085    0.28209192    0.2423555     0.30082604    0.17706302]
 [1184.92524499    0.34220001    0.26550001    0.322         0.2404    ]][0m
[37m[1m[2023-07-10 20:26:36,890][227910] Max Reward on eval: 3881.758245069161[0m
[37m[1m[2023-07-10 20:26:36,891][227910] Min Reward on eval: -335.65958386979764[0m
[37m[1m[2023-07-10 20:26:36,891][227910] Mean Reward across all agents: 1237.8398741173974[0m
[37m[1m[2023-07-10 20:26:36,891][227910] Average Trajectory Length: 947.281[0m
[36m[2023-07-10 20:26:36,893][227910] mean_value=-1363.2269861371508, max_value=750.7788372022096[0m
[37m[1m[2023-07-10 20:26:36,895][227910] New mean coefficients: [[ 0.8230634  -0.28259572 -0.51610744  0.682804    0.8055392 ]][0m
[37m[1m[2023-07-10 20:26:36,896][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:26:46,686][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 20:26:46,686][227910] FPS: 392312.83[0m
[36m[2023-07-10 20:26:46,688][227910] itr=1278, itrs=2000, Progress: 63.90%[0m
[36m[2023-07-10 20:26:58,316][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 20:26:58,316][227910] FPS: 330848.38[0m
[36m[2023-07-10 20:27:03,149][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:27:03,149][227910] Reward + Measures: [[4104.28197688    0.33572114    0.25389478    0.26947153    0.18869893]][0m
[37m[1m[2023-07-10 20:27:03,150][227910] Max Reward on eval: 4104.281976883411[0m
[37m[1m[2023-07-10 20:27:03,150][227910] Min Reward on eval: 4104.281976883411[0m
[37m[1m[2023-07-10 20:27:03,150][227910] Mean Reward across all agents: 4104.281976883411[0m
[37m[1m[2023-07-10 20:27:03,150][227910] Average Trajectory Length: 988.872[0m
[36m[2023-07-10 20:27:08,618][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:27:08,619][227910] Reward + Measures: [[2501.72847684    0.2142375     0.28124711    0.30359617    0.17777117]
 [1428.85840044    0.29959682    0.15108065    0.35954517    0.17168386]
 [2477.00088342    0.3951        0.23290001    0.34290001    0.21340001]
 ...
 [2116.48893798    0.32540479    0.21226449    0.35009947    0.16240707]
 [2921.12301955    0.23512535    0.29127747    0.32526761    0.17874649]
 [1623.1233178     0.28153682    0.15256315    0.31517372    0.16489211]][0m
[37m[1m[2023-07-10 20:27:08,619][227910] Max Reward on eval: 4114.621729145851[0m
[37m[1m[2023-07-10 20:27:08,619][227910] Min Reward on eval: -459.58759606515525[0m
[37m[1m[2023-07-10 20:27:08,619][227910] Mean Reward across all agents: 2075.7914268521436[0m
[37m[1m[2023-07-10 20:27:08,620][227910] Average Trajectory Length: 938.207[0m
[36m[2023-07-10 20:27:08,621][227910] mean_value=-1532.6213537076753, max_value=799.5736514697868[0m
[37m[1m[2023-07-10 20:27:08,623][227910] New mean coefficients: [[ 0.9957865  -0.28861585 -0.47964367  0.16337621  0.77989906]][0m
[37m[1m[2023-07-10 20:27:08,624][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:27:18,301][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 20:27:18,301][227910] FPS: 396917.16[0m
[36m[2023-07-10 20:27:18,303][227910] itr=1279, itrs=2000, Progress: 63.95%[0m
[36m[2023-07-10 20:27:29,974][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 20:27:29,975][227910] FPS: 329564.97[0m
[36m[2023-07-10 20:27:34,770][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:27:34,771][227910] Reward + Measures: [[4228.01166055    0.34194824    0.24966455    0.24663928    0.18539584]][0m
[37m[1m[2023-07-10 20:27:34,771][227910] Max Reward on eval: 4228.011660549057[0m
[37m[1m[2023-07-10 20:27:34,771][227910] Min Reward on eval: 4228.011660549057[0m
[37m[1m[2023-07-10 20:27:34,771][227910] Mean Reward across all agents: 4228.011660549057[0m
[37m[1m[2023-07-10 20:27:34,772][227910] Average Trajectory Length: 985.413[0m
[36m[2023-07-10 20:27:40,252][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:27:40,253][227910] Reward + Measures: [[1763.2128837     0.18520282    0.17733365    0.23264113    0.13725701]
 [1282.23492812    0.38383481    0.24049225    0.27115294    0.15966322]
 [3857.85185888    0.35089999    0.25530002    0.28040001    0.1875    ]
 ...
 [2825.49292753    0.24417448    0.24939103    0.37171254    0.18237956]
 [1677.19266908    0.21610001    0.2027        0.38870001    0.18889999]
 [2631.6155704     0.43150002    0.25419998    0.33430001    0.192     ]][0m
[37m[1m[2023-07-10 20:27:40,253][227910] Max Reward on eval: 4300.058723930642[0m
[37m[1m[2023-07-10 20:27:40,253][227910] Min Reward on eval: 31.98517279882217[0m
[37m[1m[2023-07-10 20:27:40,253][227910] Mean Reward across all agents: 2714.5556083315[0m
[37m[1m[2023-07-10 20:27:40,254][227910] Average Trajectory Length: 961.4073333333333[0m
[36m[2023-07-10 20:27:40,255][227910] mean_value=-950.7179345285454, max_value=633.2369337352002[0m
[37m[1m[2023-07-10 20:27:40,257][227910] New mean coefficients: [[ 0.7525902  -0.23398462 -0.773353    0.42541566  0.896586  ]][0m
[37m[1m[2023-07-10 20:27:40,258][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:27:49,923][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 20:27:49,923][227910] FPS: 397383.88[0m
[36m[2023-07-10 20:27:49,926][227910] itr=1280, itrs=2000, Progress: 64.00%[0m
[37m[1m[2023-07-10 20:27:53,882][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001260[0m
[36m[2023-07-10 20:28:05,705][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 20:28:05,705][227910] FPS: 332652.22[0m
[36m[2023-07-10 20:28:10,479][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:28:10,480][227910] Reward + Measures: [[4351.30703315    0.34941459    0.24982965    0.24133557    0.18505315]][0m
[37m[1m[2023-07-10 20:28:10,480][227910] Max Reward on eval: 4351.3070331496865[0m
[37m[1m[2023-07-10 20:28:10,480][227910] Min Reward on eval: 4351.3070331496865[0m
[37m[1m[2023-07-10 20:28:10,481][227910] Mean Reward across all agents: 4351.3070331496865[0m
[37m[1m[2023-07-10 20:28:10,481][227910] Average Trajectory Length: 990.0506666666666[0m
[36m[2023-07-10 20:28:15,877][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:28:15,878][227910] Reward + Measures: [[2347.93460335    0.23360534    0.23452321    0.2186356     0.17069645]
 [  93.547407      0.26970002    0.40690002    0.1793        0.43439999]
 [2262.53495713    0.29280001    0.2888        0.3179        0.1874    ]
 ...
 [ 958.15830737    0.40000001    0.36500001    0.52490002    0.30050001]
 [2809.16484627    0.24899995    0.27916273    0.27605271    0.18362053]
 [3591.7312983     0.30649367    0.24123783    0.30844626    0.1922559 ]][0m
[37m[1m[2023-07-10 20:28:15,878][227910] Max Reward on eval: 4211.197462396883[0m
[37m[1m[2023-07-10 20:28:15,878][227910] Min Reward on eval: -1056.676839541737[0m
[37m[1m[2023-07-10 20:28:15,878][227910] Mean Reward across all agents: 1999.0005783949928[0m
[37m[1m[2023-07-10 20:28:15,879][227910] Average Trajectory Length: 959.6669999999999[0m
[36m[2023-07-10 20:28:15,880][227910] mean_value=-1371.6180071356937, max_value=652.2580044144851[0m
[37m[1m[2023-07-10 20:28:15,883][227910] New mean coefficients: [[ 1.2344705   0.1258407  -1.0490289   0.19113879  0.27663863]][0m
[37m[1m[2023-07-10 20:28:15,883][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:28:25,570][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 20:28:25,570][227910] FPS: 396494.10[0m
[36m[2023-07-10 20:28:25,572][227910] itr=1281, itrs=2000, Progress: 64.05%[0m
[36m[2023-07-10 20:28:37,133][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 20:28:37,134][227910] FPS: 332687.44[0m
[36m[2023-07-10 20:28:41,855][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:28:41,856][227910] Reward + Measures: [[4453.77529783    0.35504729    0.24771312    0.23476616    0.18506116]][0m
[37m[1m[2023-07-10 20:28:41,856][227910] Max Reward on eval: 4453.775297828315[0m
[37m[1m[2023-07-10 20:28:41,856][227910] Min Reward on eval: 4453.775297828315[0m
[37m[1m[2023-07-10 20:28:41,856][227910] Mean Reward across all agents: 4453.775297828315[0m
[37m[1m[2023-07-10 20:28:41,857][227910] Average Trajectory Length: 991.7393333333333[0m
[36m[2023-07-10 20:28:47,366][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:28:47,366][227910] Reward + Measures: [[4233.11327306    0.35120001    0.29609999    0.29790002    0.18980001]
 [2723.3697878     0.41300002    0.31760001    0.36790001    0.24790001]
 [2749.12844919    0.37909999    0.30040002    0.37869999    0.22650002]
 ...
 [2326.01141413    0.37579998    0.26589999    0.39030001    0.23629999]
 [1682.95493118    0.35320002    0.2422        0.3362        0.21750002]
 [ 411.41052852    0.27452013    0.30789372    0.31497797    0.2398283 ]][0m
[37m[1m[2023-07-10 20:28:47,367][227910] Max Reward on eval: 4234.73095976034[0m
[37m[1m[2023-07-10 20:28:47,367][227910] Min Reward on eval: -348.64362835799693[0m
[37m[1m[2023-07-10 20:28:47,367][227910] Mean Reward across all agents: 1712.4406559535994[0m
[37m[1m[2023-07-10 20:28:47,367][227910] Average Trajectory Length: 905.4786666666666[0m
[36m[2023-07-10 20:28:47,369][227910] mean_value=-1776.8249979018562, max_value=198.2505082675534[0m
[37m[1m[2023-07-10 20:28:47,371][227910] New mean coefficients: [[ 0.73626196 -0.28259438 -0.45733315  0.12120747  0.46750128]][0m
[37m[1m[2023-07-10 20:28:47,372][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:28:56,968][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 20:28:56,968][227910] FPS: 400247.54[0m
[36m[2023-07-10 20:28:56,970][227910] itr=1282, itrs=2000, Progress: 64.10%[0m
[36m[2023-07-10 20:29:08,730][227910] train() took 11.74 seconds to complete[0m
[36m[2023-07-10 20:29:08,731][227910] FPS: 327133.92[0m
[36m[2023-07-10 20:29:13,499][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:29:13,500][227910] Reward + Measures: [[4584.11795332    0.35625187    0.24331726    0.22411466    0.18364517]][0m
[37m[1m[2023-07-10 20:29:13,500][227910] Max Reward on eval: 4584.11795332151[0m
[37m[1m[2023-07-10 20:29:13,500][227910] Min Reward on eval: 4584.11795332151[0m
[37m[1m[2023-07-10 20:29:13,500][227910] Mean Reward across all agents: 4584.11795332151[0m
[37m[1m[2023-07-10 20:29:13,500][227910] Average Trajectory Length: 989.631[0m
[36m[2023-07-10 20:29:19,144][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:29:19,144][227910] Reward + Measures: [[-1069.92663299     0.70170003     0.66070002     0.42010003
      0.56239998]
 [ 2750.23938139     0.44461456     0.29337788     0.34532309
      0.1839221 ]
 [  564.11366828     0.25650001     0.37539998     0.37960002
      0.33379999]
 ...
 [ 2693.86844812     0.35169998     0.29520002     0.45099998
      0.21510001]
 [ 1400.71055756     0.1938         0.2958         0.31130001
      0.18810001]
 [  943.03523492     0.35254583     0.1961211      0.43601432
      0.16812223]][0m
[37m[1m[2023-07-10 20:29:19,145][227910] Max Reward on eval: 4553.26911616223[0m
[37m[1m[2023-07-10 20:29:19,145][227910] Min Reward on eval: -1069.9266329918114[0m
[37m[1m[2023-07-10 20:29:19,145][227910] Mean Reward across all agents: 1708.8483592465382[0m
[37m[1m[2023-07-10 20:29:19,145][227910] Average Trajectory Length: 942.9356666666666[0m
[36m[2023-07-10 20:29:19,147][227910] mean_value=-1497.0966689603176, max_value=902.6988695151983[0m
[37m[1m[2023-07-10 20:29:19,149][227910] New mean coefficients: [[ 0.3604924  -0.07422285 -0.59487987 -0.25495657  0.40450415]][0m
[37m[1m[2023-07-10 20:29:19,150][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:29:29,001][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 20:29:29,001][227910] FPS: 389883.04[0m
[36m[2023-07-10 20:29:29,004][227910] itr=1283, itrs=2000, Progress: 64.15%[0m
[36m[2023-07-10 20:29:40,568][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 20:29:40,568][227910] FPS: 332675.08[0m
[36m[2023-07-10 20:29:45,352][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:29:45,352][227910] Reward + Measures: [[4626.4069815     0.36339906    0.24024214    0.21880336    0.18368566]][0m
[37m[1m[2023-07-10 20:29:45,353][227910] Max Reward on eval: 4626.406981502395[0m
[37m[1m[2023-07-10 20:29:45,353][227910] Min Reward on eval: 4626.406981502395[0m
[37m[1m[2023-07-10 20:29:45,353][227910] Mean Reward across all agents: 4626.406981502395[0m
[37m[1m[2023-07-10 20:29:45,353][227910] Average Trajectory Length: 988.8046666666667[0m
[36m[2023-07-10 20:29:50,811][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:29:50,812][227910] Reward + Measures: [[1556.53976202    0.25253996    0.23039158    0.2848663     0.18100302]
 [ 774.43795014    0.26890001    0.1964        0.3915        0.25279999]
 [3850.65097446    0.29480001    0.25930002    0.29679999    0.1937    ]
 ...
 [2198.64899756    0.25030002    0.21440001    0.30230004    0.16770001]
 [3082.66246604    0.41079998    0.27149999    0.32090002    0.17990001]
 [1743.15654581    0.28530002    0.2498        0.3132        0.15699999]][0m
[37m[1m[2023-07-10 20:29:50,812][227910] Max Reward on eval: 4533.375973098632[0m
[37m[1m[2023-07-10 20:29:50,813][227910] Min Reward on eval: -75.8931936346984[0m
[37m[1m[2023-07-10 20:29:50,813][227910] Mean Reward across all agents: 2303.1787363318294[0m
[37m[1m[2023-07-10 20:29:50,813][227910] Average Trajectory Length: 942.9[0m
[36m[2023-07-10 20:29:50,815][227910] mean_value=-1558.019002689653, max_value=948.2764440234724[0m
[37m[1m[2023-07-10 20:29:50,817][227910] New mean coefficients: [[ 0.79371643  0.22882344 -0.15293276 -0.0523881   0.4383626 ]][0m
[37m[1m[2023-07-10 20:29:50,818][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:30:00,644][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 20:30:00,644][227910] FPS: 390860.44[0m
[36m[2023-07-10 20:30:00,646][227910] itr=1284, itrs=2000, Progress: 64.20%[0m
[36m[2023-07-10 20:30:12,237][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 20:30:12,238][227910] FPS: 331816.64[0m
[36m[2023-07-10 20:30:16,969][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:30:16,969][227910] Reward + Measures: [[4706.48403569    0.35806501    0.2404955     0.20794906    0.17982034]][0m
[37m[1m[2023-07-10 20:30:16,970][227910] Max Reward on eval: 4706.484035691045[0m
[37m[1m[2023-07-10 20:30:16,970][227910] Min Reward on eval: 4706.484035691045[0m
[37m[1m[2023-07-10 20:30:16,970][227910] Mean Reward across all agents: 4706.484035691045[0m
[37m[1m[2023-07-10 20:30:16,970][227910] Average Trajectory Length: 987.1603333333333[0m
[36m[2023-07-10 20:30:22,461][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:30:22,462][227910] Reward + Measures: [[3682.65548361    0.36810002    0.2299        0.23840001    0.18280001]
 [1828.42005703    0.42282635    0.22822165    0.25610471    0.17796753]
 [2732.01105912    0.20489998    0.25799999    0.33780003    0.1848    ]
 ...
 [3026.13145465    0.21696065    0.27670833    0.30705538    0.18544406]
 [2408.8148269     0.23059709    0.25306401    0.29216161    0.19399236]
 [2783.96816554    0.31459999    0.1974        0.3888        0.2023    ]][0m
[37m[1m[2023-07-10 20:30:22,462][227910] Max Reward on eval: 4831.614043971803[0m
[37m[1m[2023-07-10 20:30:22,462][227910] Min Reward on eval: 680.6639644774725[0m
[37m[1m[2023-07-10 20:30:22,462][227910] Mean Reward across all agents: 3053.0101777791[0m
[37m[1m[2023-07-10 20:30:22,463][227910] Average Trajectory Length: 966.8176666666666[0m
[36m[2023-07-10 20:30:22,464][227910] mean_value=-1403.8039369039716, max_value=895.7934951168704[0m
[37m[1m[2023-07-10 20:30:22,467][227910] New mean coefficients: [[ 1.4644499  -0.54924345 -0.2430746  -0.2054415   0.53802866]][0m
[37m[1m[2023-07-10 20:30:22,468][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:30:32,252][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 20:30:32,252][227910] FPS: 392557.49[0m
[36m[2023-07-10 20:30:32,254][227910] itr=1285, itrs=2000, Progress: 64.25%[0m
[36m[2023-07-10 20:30:43,950][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 20:30:43,950][227910] FPS: 328945.31[0m
[36m[2023-07-10 20:30:48,781][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:30:48,782][227910] Reward + Measures: [[4711.09730224    0.35000098    0.23945357    0.2058221     0.1780341 ]][0m
[37m[1m[2023-07-10 20:30:48,782][227910] Max Reward on eval: 4711.097302239074[0m
[37m[1m[2023-07-10 20:30:48,782][227910] Min Reward on eval: 4711.097302239074[0m
[37m[1m[2023-07-10 20:30:48,782][227910] Mean Reward across all agents: 4711.097302239074[0m
[37m[1m[2023-07-10 20:30:48,782][227910] Average Trajectory Length: 987.2339999999999[0m
[36m[2023-07-10 20:30:54,274][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:30:54,280][227910] Reward + Measures: [[3395.33897634    0.36782956    0.21510945    0.29069814    0.17851321]
 [ 974.62209445    0.26381391    0.28870216    0.2329586     0.1823068 ]
 [2178.97518463    0.216951      0.31263313    0.2745131     0.16486637]
 ...
 [2348.68763157    0.32391       0.25173059    0.21172269    0.17384283]
 [1029.51832741    0.27975127    0.26288682    0.23327254    0.21959928]
 [ 790.15243584    0.42836341    0.18308677    0.35444602    0.15406981]][0m
[37m[1m[2023-07-10 20:30:54,280][227910] Max Reward on eval: 4729.382805771753[0m
[37m[1m[2023-07-10 20:30:54,280][227910] Min Reward on eval: -558.8161146578495[0m
[37m[1m[2023-07-10 20:30:54,281][227910] Mean Reward across all agents: 2007.9544407404815[0m
[37m[1m[2023-07-10 20:30:54,281][227910] Average Trajectory Length: 827.0533333333333[0m
[36m[2023-07-10 20:30:54,283][227910] mean_value=-2137.3266308141956, max_value=1582.079645982623[0m
[37m[1m[2023-07-10 20:30:54,285][227910] New mean coefficients: [[ 1.4149508   0.5045457  -0.2994605  -0.0145857   0.35765618]][0m
[37m[1m[2023-07-10 20:30:54,286][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:31:04,120][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 20:31:04,120][227910] FPS: 390557.01[0m
[36m[2023-07-10 20:31:04,123][227910] itr=1286, itrs=2000, Progress: 64.30%[0m
[36m[2023-07-10 20:31:15,829][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 20:31:15,829][227910] FPS: 328550.14[0m
[36m[2023-07-10 20:31:20,587][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:31:20,587][227910] Reward + Measures: [[3636.88899307    0.34505281    0.33403212    0.34258854    0.19294287]][0m
[37m[1m[2023-07-10 20:31:20,587][227910] Max Reward on eval: 3636.888993065706[0m
[37m[1m[2023-07-10 20:31:20,588][227910] Min Reward on eval: 3636.888993065706[0m
[37m[1m[2023-07-10 20:31:20,588][227910] Mean Reward across all agents: 3636.888993065706[0m
[37m[1m[2023-07-10 20:31:20,588][227910] Average Trajectory Length: 999.788[0m
[36m[2023-07-10 20:31:26,216][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:31:26,216][227910] Reward + Measures: [[1753.85762432    0.34829998    0.31150004    0.31259999    0.228     ]
 [2338.79338035    0.33969998    0.3335        0.289         0.21110001]
 [1018.45092189    0.18130001    0.37020001    0.38100001    0.21440001]
 ...
 [-107.0106243     0.19160959    0.27131855    0.2754533     0.40968028]
 [1671.8388516     0.43260002    0.53969997    0.31570002    0.47740003]
 [ 145.09933854    0.24889998    0.26140001    0.26130003    0.38600001]][0m
[37m[1m[2023-07-10 20:31:26,216][227910] Max Reward on eval: 3685.2318672818133[0m
[37m[1m[2023-07-10 20:31:26,217][227910] Min Reward on eval: -324.19872950298594[0m
[37m[1m[2023-07-10 20:31:26,217][227910] Mean Reward across all agents: 1641.7343335788562[0m
[37m[1m[2023-07-10 20:31:26,217][227910] Average Trajectory Length: 996.7733333333333[0m
[36m[2023-07-10 20:31:26,219][227910] mean_value=-1260.8239878865047, max_value=739.0819161925792[0m
[37m[1m[2023-07-10 20:31:26,222][227910] New mean coefficients: [[ 1.6172847   0.38687396  0.20484999 -0.23375542  0.66241264]][0m
[37m[1m[2023-07-10 20:31:26,223][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:31:35,977][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 20:31:35,978][227910] FPS: 393732.50[0m
[36m[2023-07-10 20:31:35,980][227910] itr=1287, itrs=2000, Progress: 64.35%[0m
[36m[2023-07-10 20:31:47,492][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 20:31:47,492][227910] FPS: 334209.36[0m
[36m[2023-07-10 20:31:52,215][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:31:52,216][227910] Reward + Measures: [[4078.76719792    0.3778756     0.30678511    0.28779513    0.1967089 ]][0m
[37m[1m[2023-07-10 20:31:52,216][227910] Max Reward on eval: 4078.7671979225606[0m
[37m[1m[2023-07-10 20:31:52,216][227910] Min Reward on eval: 4078.7671979225606[0m
[37m[1m[2023-07-10 20:31:52,217][227910] Mean Reward across all agents: 4078.7671979225606[0m
[37m[1m[2023-07-10 20:31:52,217][227910] Average Trajectory Length: 999.7336666666666[0m
[36m[2023-07-10 20:31:57,673][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:31:57,674][227910] Reward + Measures: [[1850.96471729    0.43924615    0.24293466    0.29797587    0.16310628]
 [3840.80049569    0.3795        0.31560001    0.2854        0.20050001]
 [1324.30802947    0.41289997    0.29249999    0.34279999    0.2714    ]
 ...
 [3220.94475495    0.4244        0.26550004    0.30000001    0.19410001]
 [3601.38014006    0.34940001    0.33140001    0.33060002    0.19750001]
 [ 679.03242811    0.42028746    0.30587983    0.26618955    0.29326788]][0m
[37m[1m[2023-07-10 20:31:57,674][227910] Max Reward on eval: 4211.257285288302[0m
[37m[1m[2023-07-10 20:31:57,674][227910] Min Reward on eval: -187.66167459686986[0m
[37m[1m[2023-07-10 20:31:57,675][227910] Mean Reward across all agents: 2565.9830352347676[0m
[37m[1m[2023-07-10 20:31:57,675][227910] Average Trajectory Length: 969.9456666666666[0m
[36m[2023-07-10 20:31:57,677][227910] mean_value=-1464.684572657843, max_value=1393.4464635880875[0m
[37m[1m[2023-07-10 20:31:57,679][227910] New mean coefficients: [[1.7651975  0.2641697  0.01618426 0.30877608 0.1689071 ]][0m
[37m[1m[2023-07-10 20:31:57,680][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:32:07,370][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 20:32:07,370][227910] FPS: 396374.86[0m
[36m[2023-07-10 20:32:07,372][227910] itr=1288, itrs=2000, Progress: 64.40%[0m
[36m[2023-07-10 20:32:18,890][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 20:32:18,890][227910] FPS: 333937.14[0m
[36m[2023-07-10 20:32:23,757][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:32:23,757][227910] Reward + Measures: [[4377.17891364    0.38578531    0.28017223    0.25490001    0.1939998 ]][0m
[37m[1m[2023-07-10 20:32:23,757][227910] Max Reward on eval: 4377.178913636165[0m
[37m[1m[2023-07-10 20:32:23,757][227910] Min Reward on eval: 4377.178913636165[0m
[37m[1m[2023-07-10 20:32:23,758][227910] Mean Reward across all agents: 4377.178913636165[0m
[37m[1m[2023-07-10 20:32:23,758][227910] Average Trajectory Length: 999.4943333333333[0m
[36m[2023-07-10 20:32:29,180][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:32:29,180][227910] Reward + Measures: [[ 713.6527643     0.44519997    0.38110003    0.30969998    0.34670001]
 [1028.36407371    0.2474        0.27520001    0.38030002    0.23319998]
 [ 277.82791974    0.35021678    0.37909308    0.26580462    0.28465611]
 ...
 [2402.40035453    0.2906        0.28000003    0.39069998    0.19149999]
 [ 728.50765697    0.317         0.40459999    0.37490001    0.38860002]
 [ 964.43619099    0.26416802    0.31944284    0.31500161    0.25179461]][0m
[37m[1m[2023-07-10 20:32:29,180][227910] Max Reward on eval: 4039.217057885765[0m
[37m[1m[2023-07-10 20:32:29,181][227910] Min Reward on eval: -714.977629264351[0m
[37m[1m[2023-07-10 20:32:29,181][227910] Mean Reward across all agents: 1525.4634776806508[0m
[37m[1m[2023-07-10 20:32:29,181][227910] Average Trajectory Length: 973.7656666666667[0m
[36m[2023-07-10 20:32:29,183][227910] mean_value=-1580.7323832186833, max_value=1301.4006842735118[0m
[37m[1m[2023-07-10 20:32:29,185][227910] New mean coefficients: [[ 1.5950676   0.54269224 -0.4427368   0.21408749 -0.01007316]][0m
[37m[1m[2023-07-10 20:32:29,186][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:32:38,898][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 20:32:38,898][227910] FPS: 395474.16[0m
[36m[2023-07-10 20:32:38,900][227910] itr=1289, itrs=2000, Progress: 64.45%[0m
[36m[2023-07-10 20:32:50,874][227910] train() took 11.95 seconds to complete[0m
[36m[2023-07-10 20:32:50,875][227910] FPS: 321222.60[0m
[36m[2023-07-10 20:32:55,583][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:32:55,584][227910] Reward + Measures: [[4685.96028443    0.37657949    0.26772228    0.2284185     0.18530391]][0m
[37m[1m[2023-07-10 20:32:55,584][227910] Max Reward on eval: 4685.96028443443[0m
[37m[1m[2023-07-10 20:32:55,584][227910] Min Reward on eval: 4685.96028443443[0m
[37m[1m[2023-07-10 20:32:55,584][227910] Mean Reward across all agents: 4685.96028443443[0m
[37m[1m[2023-07-10 20:32:55,585][227910] Average Trajectory Length: 998.0123333333333[0m
[36m[2023-07-10 20:33:01,164][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:33:01,169][227910] Reward + Measures: [[1723.4932012     0.36980006    0.3096        0.3457        0.2007    ]
 [1605.12472102    0.3572        0.2746        0.33579999    0.18920001]
 [ 745.53108502    0.24389999    0.2141        0.32830003    0.26789999]
 ...
 [4347.75571143    0.35910001    0.31389999    0.28410003    0.18620001]
 [1973.55915487    0.26285294    0.39554825    0.21133736    0.1531954 ]
 [2174.87177306    0.30871645    0.41804346    0.24244638    0.17852388]][0m
[37m[1m[2023-07-10 20:33:01,170][227910] Max Reward on eval: 4596.3817610599335[0m
[37m[1m[2023-07-10 20:33:01,170][227910] Min Reward on eval: -311.8290893870988[0m
[37m[1m[2023-07-10 20:33:01,170][227910] Mean Reward across all agents: 1769.2200336603782[0m
[37m[1m[2023-07-10 20:33:01,171][227910] Average Trajectory Length: 972.1183333333333[0m
[36m[2023-07-10 20:33:01,172][227910] mean_value=-1657.5745832416492, max_value=610.1951757951656[0m
[37m[1m[2023-07-10 20:33:01,175][227910] New mean coefficients: [[ 1.7608672  -0.27459508 -0.8355261   0.20808594 -0.14068753]][0m
[37m[1m[2023-07-10 20:33:01,176][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:33:10,952][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 20:33:10,952][227910] FPS: 392862.53[0m
[36m[2023-07-10 20:33:10,954][227910] itr=1290, itrs=2000, Progress: 64.50%[0m
[37m[1m[2023-07-10 20:33:15,031][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001270[0m
[36m[2023-07-10 20:33:27,002][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 20:33:27,002][227910] FPS: 328357.69[0m
[36m[2023-07-10 20:33:31,760][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:33:31,761][227910] Reward + Measures: [[4779.81748162    0.37046906    0.25145325    0.21447399    0.18008792]][0m
[37m[1m[2023-07-10 20:33:31,761][227910] Max Reward on eval: 4779.817481618881[0m
[37m[1m[2023-07-10 20:33:31,761][227910] Min Reward on eval: 4779.817481618881[0m
[37m[1m[2023-07-10 20:33:31,762][227910] Mean Reward across all agents: 4779.817481618881[0m
[37m[1m[2023-07-10 20:33:31,762][227910] Average Trajectory Length: 992.8629999999999[0m
[36m[2023-07-10 20:33:37,105][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:33:37,106][227910] Reward + Measures: [[3687.71040708    0.3987        0.27340001    0.25030002    0.1952    ]
 [1382.89593591    0.37050003    0.2339        0.45100003    0.26409999]
 [2321.98070534    0.32624459    0.17114095    0.32810727    0.19942771]
 ...
 [1019.61107868    0.27969438    0.22781689    0.2045282     0.23409013]
 [2705.0702793     0.4068        0.27809998    0.31360003    0.2369    ]
 [ 763.70326792    0.47950003    0.52780002    0.46690002    0.30299997]][0m
[37m[1m[2023-07-10 20:33:37,106][227910] Max Reward on eval: 4720.837605995219[0m
[37m[1m[2023-07-10 20:33:37,106][227910] Min Reward on eval: -1047.3325184943678[0m
[37m[1m[2023-07-10 20:33:37,107][227910] Mean Reward across all agents: 1540.9493439401779[0m
[37m[1m[2023-07-10 20:33:37,107][227910] Average Trajectory Length: 964.8173333333333[0m
[36m[2023-07-10 20:33:37,109][227910] mean_value=-1222.1689191847556, max_value=921.9977180505946[0m
[37m[1m[2023-07-10 20:33:37,111][227910] New mean coefficients: [[ 1.8018906   0.211456   -0.6363619   0.478374   -0.17030054]][0m
[37m[1m[2023-07-10 20:33:37,112][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:33:46,629][227910] train() took 9.52 seconds to complete[0m
[36m[2023-07-10 20:33:46,629][227910] FPS: 403537.61[0m
[36m[2023-07-10 20:33:46,631][227910] itr=1291, itrs=2000, Progress: 64.55%[0m
[36m[2023-07-10 20:33:58,150][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 20:33:58,151][227910] FPS: 333940.47[0m
[36m[2023-07-10 20:34:02,862][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:34:02,863][227910] Reward + Measures: [[4931.47347297    0.37820712    0.25316879    0.2145575     0.18244518]][0m
[37m[1m[2023-07-10 20:34:02,863][227910] Max Reward on eval: 4931.473472966843[0m
[37m[1m[2023-07-10 20:34:02,863][227910] Min Reward on eval: 4931.473472966843[0m
[37m[1m[2023-07-10 20:34:02,864][227910] Mean Reward across all agents: 4931.473472966843[0m
[37m[1m[2023-07-10 20:34:02,864][227910] Average Trajectory Length: 998.4626666666667[0m
[36m[2023-07-10 20:34:08,226][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:34:08,226][227910] Reward + Measures: [[2215.54489502    0.51410002    0.2782        0.33989999    0.22070001]
 [ 426.13515694    0.43870002    0.50889999    0.39790002    0.477     ]
 [1843.93296119    0.46070001    0.2956        0.40840003    0.2419    ]
 ...
 [1744.93864008    0.50450003    0.27550003    0.35929999    0.24259999]
 [ 658.59632419    0.47690001    0.2994        0.37020001    0.29370001]
 [1924.30890716    0.35100004    0.32160002    0.3175        0.205     ]][0m
[37m[1m[2023-07-10 20:34:08,226][227910] Max Reward on eval: 4851.098486750945[0m
[37m[1m[2023-07-10 20:34:08,227][227910] Min Reward on eval: -600.462955683947[0m
[37m[1m[2023-07-10 20:34:08,227][227910] Mean Reward across all agents: 1142.1309924922316[0m
[37m[1m[2023-07-10 20:34:08,227][227910] Average Trajectory Length: 930.9523333333333[0m
[36m[2023-07-10 20:34:08,229][227910] mean_value=-1659.9028075488388, max_value=1478.704470963683[0m
[37m[1m[2023-07-10 20:34:08,231][227910] New mean coefficients: [[ 1.6032374   0.84359646 -0.22539645  0.54421556 -0.7045998 ]][0m
[37m[1m[2023-07-10 20:34:08,232][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:34:17,914][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 20:34:17,914][227910] FPS: 396682.30[0m
[36m[2023-07-10 20:34:17,917][227910] itr=1292, itrs=2000, Progress: 64.60%[0m
[36m[2023-07-10 20:34:29,388][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 20:34:29,388][227910] FPS: 335287.81[0m
[36m[2023-07-10 20:34:34,076][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:34:34,077][227910] Reward + Measures: [[5083.32654081    0.37347984    0.2448281     0.20942949    0.18119121]][0m
[37m[1m[2023-07-10 20:34:34,077][227910] Max Reward on eval: 5083.326540812171[0m
[37m[1m[2023-07-10 20:34:34,077][227910] Min Reward on eval: 5083.326540812171[0m
[37m[1m[2023-07-10 20:34:34,077][227910] Mean Reward across all agents: 5083.326540812171[0m
[37m[1m[2023-07-10 20:34:34,077][227910] Average Trajectory Length: 996.8976666666666[0m
[36m[2023-07-10 20:34:39,407][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:34:39,408][227910] Reward + Measures: [[1891.66582614    0.48340002    0.31710002    0.31990001    0.20900002]
 [3062.05528769    0.24796978    0.33475271    0.27916357    0.19914263]
 [2154.68824268    0.32179999    0.33140001    0.44580004    0.178     ]
 ...
 [ 826.04305151    0.33386281    0.30389258    0.52122992    0.32380185]
 [-316.4977599     0.388993      0.22346087    0.30137548    0.1388734 ]
 [2808.14535276    0.32080004    0.26930004    0.33810002    0.20420001]][0m
[37m[1m[2023-07-10 20:34:39,408][227910] Max Reward on eval: 5214.079543714784[0m
[37m[1m[2023-07-10 20:34:39,408][227910] Min Reward on eval: -552.9529374942067[0m
[37m[1m[2023-07-10 20:34:39,408][227910] Mean Reward across all agents: 1719.6424273026225[0m
[37m[1m[2023-07-10 20:34:39,409][227910] Average Trajectory Length: 965.7149999999999[0m
[36m[2023-07-10 20:34:39,411][227910] mean_value=-1055.9722855546113, max_value=1075.711085907723[0m
[37m[1m[2023-07-10 20:34:39,413][227910] New mean coefficients: [[ 1.383053    1.0710598  -0.30334222  0.45114648 -0.72973925]][0m
[37m[1m[2023-07-10 20:34:39,414][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:34:49,054][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 20:34:49,054][227910] FPS: 398408.55[0m
[36m[2023-07-10 20:34:49,056][227910] itr=1293, itrs=2000, Progress: 64.65%[0m
[36m[2023-07-10 20:35:00,566][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 20:35:00,567][227910] FPS: 334252.16[0m
[36m[2023-07-10 20:35:05,378][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:35:05,378][227910] Reward + Measures: [[5214.4410551     0.36486813    0.24307512    0.20386206    0.17854655]][0m
[37m[1m[2023-07-10 20:35:05,379][227910] Max Reward on eval: 5214.441055103368[0m
[37m[1m[2023-07-10 20:35:05,379][227910] Min Reward on eval: 5214.441055103368[0m
[37m[1m[2023-07-10 20:35:05,379][227910] Mean Reward across all agents: 5214.441055103368[0m
[37m[1m[2023-07-10 20:35:05,379][227910] Average Trajectory Length: 996.9463333333333[0m
[36m[2023-07-10 20:35:11,069][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:35:11,070][227910] Reward + Measures: [[2671.80731293    0.57609999    0.2509        0.329         0.19780001]
 [-315.40993558    0.17320001    0.63240004    0.52030003    0.63819999]
 [ 555.27966925    0.35980001    0.44880006    0.46479997    0.43290001]
 ...
 [4793.92361263    0.30739999    0.26930001    0.24260001    0.176     ]
 [3024.97526415    0.25130001    0.28459999    0.40669999    0.19059999]
 [ 906.70088051    0.24200001    0.18359999    0.26500002    0.15810001]][0m
[37m[1m[2023-07-10 20:35:11,070][227910] Max Reward on eval: 5220.239829198084[0m
[37m[1m[2023-07-10 20:35:11,070][227910] Min Reward on eval: -854.0533983956324[0m
[37m[1m[2023-07-10 20:35:11,070][227910] Mean Reward across all agents: 1985.8233631893572[0m
[37m[1m[2023-07-10 20:35:11,071][227910] Average Trajectory Length: 988.7996666666667[0m
[36m[2023-07-10 20:35:11,073][227910] mean_value=-1125.0114859948612, max_value=1890.657214703906[0m
[37m[1m[2023-07-10 20:35:11,075][227910] New mean coefficients: [[ 1.808344    1.2276057  -0.40185285  0.62381864 -0.40220812]][0m
[37m[1m[2023-07-10 20:35:11,076][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:35:20,843][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 20:35:20,843][227910] FPS: 393240.26[0m
[36m[2023-07-10 20:35:20,845][227910] itr=1294, itrs=2000, Progress: 64.70%[0m
[36m[2023-07-10 20:35:32,509][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 20:35:32,509][227910] FPS: 329797.00[0m
[36m[2023-07-10 20:35:37,236][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:35:37,236][227910] Reward + Measures: [[5287.29020168    0.36782825    0.23876847    0.1988461     0.1751689 ]][0m
[37m[1m[2023-07-10 20:35:37,236][227910] Max Reward on eval: 5287.290201684956[0m
[37m[1m[2023-07-10 20:35:37,236][227910] Min Reward on eval: 5287.290201684956[0m
[37m[1m[2023-07-10 20:35:37,236][227910] Mean Reward across all agents: 5287.290201684956[0m
[37m[1m[2023-07-10 20:35:37,237][227910] Average Trajectory Length: 994.5213333333332[0m
[36m[2023-07-10 20:35:42,729][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:35:42,730][227910] Reward + Measures: [[3868.20707083    0.38550001    0.3114        0.3057        0.19200002]
 [1116.53308545    0.27669999    0.30579999    0.49950001    0.1794    ]
 [1095.35111196    0.21908148    0.28376243    0.35103631    0.17694943]
 ...
 [ 926.00061857    0.26443285    0.34712639    0.30774203    0.17828138]
 [1244.84950001    0.33909997    0.34489998    0.39070001    0.19669999]
 [2352.55702727    0.38979998    0.2748        0.28590003    0.2066    ]][0m
[37m[1m[2023-07-10 20:35:42,730][227910] Max Reward on eval: 4986.922811194881[0m
[37m[1m[2023-07-10 20:35:42,730][227910] Min Reward on eval: -857.9127038938692[0m
[37m[1m[2023-07-10 20:35:42,731][227910] Mean Reward across all agents: 1743.1385344364621[0m
[37m[1m[2023-07-10 20:35:42,731][227910] Average Trajectory Length: 961.8626666666667[0m
[36m[2023-07-10 20:35:42,732][227910] mean_value=-1898.78693644072, max_value=694.4007343181413[0m
[37m[1m[2023-07-10 20:35:42,735][227910] New mean coefficients: [[ 1.7668265   1.0716561   0.23272973  0.25042838 -0.38909623]][0m
[37m[1m[2023-07-10 20:35:42,736][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:35:52,455][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 20:35:52,455][227910] FPS: 395168.18[0m
[36m[2023-07-10 20:35:52,457][227910] itr=1295, itrs=2000, Progress: 64.75%[0m
[36m[2023-07-10 20:36:04,351][227910] train() took 11.88 seconds to complete[0m
[36m[2023-07-10 20:36:04,351][227910] FPS: 323365.79[0m
[36m[2023-07-10 20:36:09,169][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:36:09,169][227910] Reward + Measures: [[5329.63621674    0.36443716    0.23830704    0.19567037    0.17335454]][0m
[37m[1m[2023-07-10 20:36:09,169][227910] Max Reward on eval: 5329.636216737417[0m
[37m[1m[2023-07-10 20:36:09,170][227910] Min Reward on eval: 5329.636216737417[0m
[37m[1m[2023-07-10 20:36:09,170][227910] Mean Reward across all agents: 5329.636216737417[0m
[37m[1m[2023-07-10 20:36:09,170][227910] Average Trajectory Length: 989.803[0m
[36m[2023-07-10 20:36:14,576][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:36:14,581][227910] Reward + Measures: [[ 575.50331853    0.38550001    0.48210001    0.45320001    0.58879995]
 [ 789.55023341    0.39164764    0.17953061    0.30965731    0.1524134 ]
 [4615.85832065    0.33330002    0.24490002    0.20320001    0.18090001]
 ...
 [ -23.88368826    0.31653178    0.15436736    0.29656008    0.12342653]
 [1595.34898591    0.20039999    0.2712        0.32979998    0.23769999]
 [1928.78318377    0.21964028    0.25491849    0.3474147     0.24329667]][0m
[37m[1m[2023-07-10 20:36:14,582][227910] Max Reward on eval: 5146.415379054844[0m
[37m[1m[2023-07-10 20:36:14,582][227910] Min Reward on eval: -904.250564082386[0m
[37m[1m[2023-07-10 20:36:14,582][227910] Mean Reward across all agents: 1610.5656898155653[0m
[37m[1m[2023-07-10 20:36:14,583][227910] Average Trajectory Length: 963.3276666666667[0m
[36m[2023-07-10 20:36:14,584][227910] mean_value=-1576.6311153671352, max_value=765.7190723470844[0m
[37m[1m[2023-07-10 20:36:14,587][227910] New mean coefficients: [[ 1.7943587   0.8345432  -0.54964477  0.04738909 -0.5047639 ]][0m
[37m[1m[2023-07-10 20:36:14,588][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:36:24,197][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 20:36:24,197][227910] FPS: 399682.75[0m
[36m[2023-07-10 20:36:24,199][227910] itr=1296, itrs=2000, Progress: 64.80%[0m
[36m[2023-07-10 20:36:35,686][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 20:36:35,687][227910] FPS: 334921.21[0m
[36m[2023-07-10 20:36:40,482][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:36:40,482][227910] Reward + Measures: [[5438.37355211    0.36941916    0.23163301    0.19322951    0.17337839]][0m
[37m[1m[2023-07-10 20:36:40,483][227910] Max Reward on eval: 5438.373552107067[0m
[37m[1m[2023-07-10 20:36:40,483][227910] Min Reward on eval: 5438.373552107067[0m
[37m[1m[2023-07-10 20:36:40,483][227910] Mean Reward across all agents: 5438.373552107067[0m
[37m[1m[2023-07-10 20:36:40,483][227910] Average Trajectory Length: 993.0416666666666[0m
[36m[2023-07-10 20:36:45,755][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:36:45,756][227910] Reward + Measures: [[1749.76159819    0.50959998    0.3159        0.38229999    0.2059    ]
 [2389.31370767    0.41279998    0.36090001    0.34440002    0.2026    ]
 [2521.21421947    0.41739997    0.317         0.32230002    0.17950001]
 ...
 [1591.8158606     0.52500004    0.32220003    0.34260002    0.20200001]
 [3998.93483113    0.4269        0.29519999    0.25689998    0.1734    ]
 [2622.34938986    0.41980001    0.30159998    0.3466        0.18280001]][0m
[37m[1m[2023-07-10 20:36:45,756][227910] Max Reward on eval: 5433.878171855211[0m
[37m[1m[2023-07-10 20:36:45,756][227910] Min Reward on eval: 279.5191621597449[0m
[37m[1m[2023-07-10 20:36:45,756][227910] Mean Reward across all agents: 2965.993117932206[0m
[37m[1m[2023-07-10 20:36:45,757][227910] Average Trajectory Length: 982.9116666666666[0m
[36m[2023-07-10 20:36:45,758][227910] mean_value=-1090.2721800057698, max_value=684.7656074190031[0m
[37m[1m[2023-07-10 20:36:45,761][227910] New mean coefficients: [[ 1.998795    0.73048204 -0.8462767  -0.21561784 -0.79878235]][0m
[37m[1m[2023-07-10 20:36:45,761][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:36:55,430][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 20:36:55,431][227910] FPS: 397207.08[0m
[36m[2023-07-10 20:36:55,433][227910] itr=1297, itrs=2000, Progress: 64.85%[0m
[36m[2023-07-10 20:37:07,151][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 20:37:07,152][227910] FPS: 328199.36[0m
[36m[2023-07-10 20:37:11,908][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:37:11,909][227910] Reward + Measures: [[5515.22571362    0.36688715    0.22922504    0.18974471    0.17142153]][0m
[37m[1m[2023-07-10 20:37:11,909][227910] Max Reward on eval: 5515.225713615224[0m
[37m[1m[2023-07-10 20:37:11,909][227910] Min Reward on eval: 5515.225713615224[0m
[37m[1m[2023-07-10 20:37:11,910][227910] Mean Reward across all agents: 5515.225713615224[0m
[37m[1m[2023-07-10 20:37:11,910][227910] Average Trajectory Length: 990.091[0m
[36m[2023-07-10 20:37:17,456][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:37:17,456][227910] Reward + Measures: [[3764.61735291    0.28040001    0.21999998    0.20949998    0.16070001]
 [3286.46740527    0.27014995    0.31176636    0.29418394    0.19123261]
 [2150.50915288    0.26931119    0.34276298    0.32270086    0.24371207]
 ...
 [1640.72287786    0.2280906     0.17519487    0.29750171    0.16870342]
 [1683.59631216    0.26991656    0.28259668    0.32002807    0.20746355]
 [1314.09395758    0.3046        0.36040002    0.28730002    0.27630004]][0m
[37m[1m[2023-07-10 20:37:17,457][227910] Max Reward on eval: 5398.036973212904[0m
[37m[1m[2023-07-10 20:37:17,457][227910] Min Reward on eval: 43.66080384339439[0m
[37m[1m[2023-07-10 20:37:17,457][227910] Mean Reward across all agents: 2766.306119364921[0m
[37m[1m[2023-07-10 20:37:17,457][227910] Average Trajectory Length: 964.8076666666666[0m
[36m[2023-07-10 20:37:17,459][227910] mean_value=-831.6628735651594, max_value=928.3263981940088[0m
[37m[1m[2023-07-10 20:37:17,462][227910] New mean coefficients: [[ 2.0095804   0.6845454  -0.65963936 -0.72233576 -0.83291024]][0m
[37m[1m[2023-07-10 20:37:17,463][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:37:27,158][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 20:37:27,158][227910] FPS: 396139.39[0m
[36m[2023-07-10 20:37:27,160][227910] itr=1298, itrs=2000, Progress: 64.90%[0m
[36m[2023-07-10 20:37:38,662][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 20:37:38,663][227910] FPS: 334438.94[0m
[36m[2023-07-10 20:37:43,345][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:37:43,345][227910] Reward + Measures: [[5663.12389537    0.36927843    0.23715812    0.1866347     0.16945525]][0m
[37m[1m[2023-07-10 20:37:43,346][227910] Max Reward on eval: 5663.123895371531[0m
[37m[1m[2023-07-10 20:37:43,346][227910] Min Reward on eval: 5663.123895371531[0m
[37m[1m[2023-07-10 20:37:43,346][227910] Mean Reward across all agents: 5663.123895371531[0m
[37m[1m[2023-07-10 20:37:43,346][227910] Average Trajectory Length: 994.2563333333333[0m
[36m[2023-07-10 20:37:48,943][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:37:48,944][227910] Reward + Measures: [[1673.15960368    0.18710001    0.32619998    0.35170001    0.24879999]
 [2798.11843365    0.32755491    0.22786975    0.31641263    0.16696723]
 [3800.64529973    0.41439995    0.27950001    0.31740004    0.176     ]
 ...
 [3398.40842475    0.31130001    0.35639998    0.37419999    0.2041    ]
 [ 717.94029308    0.21900001    0.28239998    0.36399999    0.25580001]
 [1987.08329268    0.2827476     0.20237087    0.37933752    0.17178488]][0m
[37m[1m[2023-07-10 20:37:48,944][227910] Max Reward on eval: 5593.521954061091[0m
[37m[1m[2023-07-10 20:37:48,945][227910] Min Reward on eval: -229.53575070467778[0m
[37m[1m[2023-07-10 20:37:48,945][227910] Mean Reward across all agents: 2497.9045632612297[0m
[37m[1m[2023-07-10 20:37:48,945][227910] Average Trajectory Length: 969.6899999999999[0m
[36m[2023-07-10 20:37:48,946][227910] mean_value=-1147.4857783709786, max_value=586.3515525290308[0m
[37m[1m[2023-07-10 20:37:48,949][227910] New mean coefficients: [[ 1.9097786   0.5670619  -0.59224975 -0.5615987  -0.6275526 ]][0m
[37m[1m[2023-07-10 20:37:48,950][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:37:58,647][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 20:37:58,647][227910] FPS: 396040.33[0m
[36m[2023-07-10 20:37:58,650][227910] itr=1299, itrs=2000, Progress: 64.95%[0m
[36m[2023-07-10 20:38:10,300][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 20:38:10,301][227910] FPS: 330110.87[0m
[36m[2023-07-10 20:38:15,066][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:38:15,066][227910] Reward + Measures: [[5648.4178744     0.37692332    0.22946106    0.18023369    0.16452712]][0m
[37m[1m[2023-07-10 20:38:15,066][227910] Max Reward on eval: 5648.417874402659[0m
[37m[1m[2023-07-10 20:38:15,066][227910] Min Reward on eval: 5648.417874402659[0m
[37m[1m[2023-07-10 20:38:15,067][227910] Mean Reward across all agents: 5648.417874402659[0m
[37m[1m[2023-07-10 20:38:15,067][227910] Average Trajectory Length: 986.9359999999999[0m
[36m[2023-07-10 20:38:20,498][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:38:20,498][227910] Reward + Measures: [[1215.92517741    0.24970002    0.23849998    0.35089999    0.20850001]
 [2998.06750612    0.39859998    0.29540002    0.4797        0.21689999]
 [2646.56161027    0.28659999    0.28870001    0.32099998    0.1938    ]
 ...
 [2499.60056005    0.24374774    0.24947241    0.2955589     0.17679754]
 [1944.14877393    0.1957        0.30640003    0.39289999    0.20920001]
 [4446.22438529    0.36580005    0.28380004    0.31610003    0.1903    ]][0m
[37m[1m[2023-07-10 20:38:20,498][227910] Max Reward on eval: 4986.887238875637[0m
[37m[1m[2023-07-10 20:38:20,499][227910] Min Reward on eval: 2.6246706947218628[0m
[37m[1m[2023-07-10 20:38:20,499][227910] Mean Reward across all agents: 1950.5536161016323[0m
[37m[1m[2023-07-10 20:38:20,499][227910] Average Trajectory Length: 944.7806666666667[0m
[36m[2023-07-10 20:38:20,501][227910] mean_value=-1683.7362351220952, max_value=1115.9466663117908[0m
[37m[1m[2023-07-10 20:38:20,503][227910] New mean coefficients: [[ 2.123505    0.75330484 -0.5095829  -0.12083295 -0.55590063]][0m
[37m[1m[2023-07-10 20:38:20,505][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:38:30,199][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 20:38:30,205][227910] FPS: 396156.97[0m
[36m[2023-07-10 20:38:30,208][227910] itr=1300, itrs=2000, Progress: 65.00%[0m
[37m[1m[2023-07-10 20:38:34,234][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001280[0m
[36m[2023-07-10 20:38:46,094][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 20:38:46,094][227910] FPS: 331590.44[0m
[36m[2023-07-10 20:38:50,882][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:38:50,883][227910] Reward + Measures: [[5734.19674081    0.38249454    0.22674194    0.18258944    0.16656557]][0m
[37m[1m[2023-07-10 20:38:50,883][227910] Max Reward on eval: 5734.196740813764[0m
[37m[1m[2023-07-10 20:38:50,883][227910] Min Reward on eval: 5734.196740813764[0m
[37m[1m[2023-07-10 20:38:50,883][227910] Mean Reward across all agents: 5734.196740813764[0m
[37m[1m[2023-07-10 20:38:50,883][227910] Average Trajectory Length: 990.1083333333333[0m
[36m[2023-07-10 20:38:56,376][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:38:56,377][227910] Reward + Measures: [[3681.35358421    0.38960001    0.3125        0.37270001    0.18789999]
 [1658.88456285    0.43290001    0.25440001    0.3836        0.22250001]
 [3542.89191327    0.34209803    0.30910441    0.22461723    0.17448542]
 ...
 [ 514.70719836    0.45919999    0.46970001    0.48830006    0.47579995]
 [3590.78302652    0.42290002    0.29359999    0.3048        0.1692    ]
 [1662.21373117    0.47709998    0.32210001    0.50209999    0.1929    ]][0m
[37m[1m[2023-07-10 20:38:56,377][227910] Max Reward on eval: 5233.987089177407[0m
[37m[1m[2023-07-10 20:38:56,377][227910] Min Reward on eval: -951.1637633548817[0m
[37m[1m[2023-07-10 20:38:56,377][227910] Mean Reward across all agents: 2223.0832328399697[0m
[37m[1m[2023-07-10 20:38:56,377][227910] Average Trajectory Length: 936.852[0m
[36m[2023-07-10 20:38:56,379][227910] mean_value=-1425.006942341079, max_value=677.8589448915545[0m
[37m[1m[2023-07-10 20:38:56,382][227910] New mean coefficients: [[ 1.7583029   0.7868127  -0.13286561 -0.23379874 -0.5593955 ]][0m
[37m[1m[2023-07-10 20:38:56,382][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:39:06,134][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 20:39:06,135][227910] FPS: 393840.94[0m
[36m[2023-07-10 20:39:06,137][227910] itr=1301, itrs=2000, Progress: 65.05%[0m
[36m[2023-07-10 20:39:17,872][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 20:39:17,872][227910] FPS: 327846.18[0m
[36m[2023-07-10 20:39:22,686][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:39:22,686][227910] Reward + Measures: [[5848.57301009    0.37997144    0.22867702    0.17915842    0.16491473]][0m
[37m[1m[2023-07-10 20:39:22,687][227910] Max Reward on eval: 5848.573010085699[0m
[37m[1m[2023-07-10 20:39:22,687][227910] Min Reward on eval: 5848.573010085699[0m
[37m[1m[2023-07-10 20:39:22,687][227910] Mean Reward across all agents: 5848.573010085699[0m
[37m[1m[2023-07-10 20:39:22,687][227910] Average Trajectory Length: 990.8426666666667[0m
[36m[2023-07-10 20:39:28,180][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:39:28,181][227910] Reward + Measures: [[2072.48612955    0.24054785    0.25954658    0.23835352    0.19692409]
 [1164.29950942    0.3822448     0.32394239    0.25486279    0.22005807]
 [2876.40862686    0.3263        0.28760001    0.30180001    0.20299998]
 ...
 [1963.50051354    0.3475        0.3619        0.27469999    0.1973    ]
 [ 801.48164368    0.49250004    0.442         0.52809995    0.49379998]
 [2916.84896208    0.42840001    0.2705        0.33150002    0.18969999]][0m
[37m[1m[2023-07-10 20:39:28,181][227910] Max Reward on eval: 5864.887159953452[0m
[37m[1m[2023-07-10 20:39:28,181][227910] Min Reward on eval: -35.37429030782077[0m
[37m[1m[2023-07-10 20:39:28,181][227910] Mean Reward across all agents: 2651.52630815775[0m
[37m[1m[2023-07-10 20:39:28,182][227910] Average Trajectory Length: 909.415[0m
[36m[2023-07-10 20:39:28,183][227910] mean_value=-1610.1113947702145, max_value=408.57371450818096[0m
[37m[1m[2023-07-10 20:39:28,186][227910] New mean coefficients: [[ 1.7342457   0.15292364 -0.17187744 -0.5222721  -0.49597755]][0m
[37m[1m[2023-07-10 20:39:28,187][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:39:38,152][227910] train() took 9.96 seconds to complete[0m
[36m[2023-07-10 20:39:38,153][227910] FPS: 385385.41[0m
[36m[2023-07-10 20:39:38,155][227910] itr=1302, itrs=2000, Progress: 65.10%[0m
[36m[2023-07-10 20:39:49,826][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 20:39:49,826][227910] FPS: 329638.77[0m
[36m[2023-07-10 20:39:54,500][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:39:54,500][227910] Reward + Measures: [[5989.81144175    0.38687366    0.21795578    0.17950261    0.16611674]][0m
[37m[1m[2023-07-10 20:39:54,500][227910] Max Reward on eval: 5989.811441745724[0m
[37m[1m[2023-07-10 20:39:54,501][227910] Min Reward on eval: 5989.811441745724[0m
[37m[1m[2023-07-10 20:39:54,501][227910] Mean Reward across all agents: 5989.811441745724[0m
[37m[1m[2023-07-10 20:39:54,501][227910] Average Trajectory Length: 993.5596666666667[0m
[36m[2023-07-10 20:39:59,932][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:39:59,933][227910] Reward + Measures: [[1233.29375663    0.30970001    0.29300001    0.32710001    0.28400001]
 [1674.82204055    0.38489184    0.3567237     0.30987927    0.17895573]
 [2516.43009449    0.2354753     0.25931939    0.28334031    0.20059279]
 ...
 [5550.21934718    0.3479        0.2454        0.1885        0.1736    ]
 [4049.75953101    0.39040002    0.26120001    0.2185        0.19170001]
 [4222.94123521    0.35369968    0.263174      0.18396892    0.17029159]][0m
[37m[1m[2023-07-10 20:39:59,933][227910] Max Reward on eval: 5960.097529929131[0m
[37m[1m[2023-07-10 20:39:59,934][227910] Min Reward on eval: 231.84787185955793[0m
[37m[1m[2023-07-10 20:39:59,934][227910] Mean Reward across all agents: 2748.669525039389[0m
[37m[1m[2023-07-10 20:39:59,934][227910] Average Trajectory Length: 804.7003333333333[0m
[36m[2023-07-10 20:39:59,936][227910] mean_value=-2159.400528448929, max_value=2550.669944781944[0m
[37m[1m[2023-07-10 20:39:59,938][227910] New mean coefficients: [[ 1.422977    0.17945755  0.33568573  0.00439668 -0.24562085]][0m
[37m[1m[2023-07-10 20:39:59,939][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:40:09,616][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 20:40:09,616][227910] FPS: 396912.58[0m
[36m[2023-07-10 20:40:09,618][227910] itr=1303, itrs=2000, Progress: 65.15%[0m
[36m[2023-07-10 20:40:21,149][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 20:40:21,149][227910] FPS: 333552.46[0m
[36m[2023-07-10 20:40:25,932][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:40:25,937][227910] Reward + Measures: [[6053.08346428    0.38190585    0.21601741    0.17772697    0.16455221]][0m
[37m[1m[2023-07-10 20:40:25,937][227910] Max Reward on eval: 6053.083464280523[0m
[37m[1m[2023-07-10 20:40:25,937][227910] Min Reward on eval: 6053.083464280523[0m
[37m[1m[2023-07-10 20:40:25,938][227910] Mean Reward across all agents: 6053.083464280523[0m
[37m[1m[2023-07-10 20:40:25,938][227910] Average Trajectory Length: 992.1229999999999[0m
[36m[2023-07-10 20:40:31,429][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:40:31,430][227910] Reward + Measures: [[ 632.52469961    0.22523943    0.17324737    0.32980242    0.19201176]
 [1885.83620005    0.21163921    0.26630196    0.29680786    0.19755882]
 [1338.32224114    0.22719999    0.43979999    0.38580003    0.28420001]
 ...
 [ 946.2769755     0.2658        0.1609        0.3619        0.20760003]
 [4094.03942976    0.36170003    0.3242        0.24120001    0.1859    ]
 [3889.97059642    0.3118        0.2622        0.28550002    0.1895    ]][0m
[37m[1m[2023-07-10 20:40:31,430][227910] Max Reward on eval: 5909.397136894613[0m
[37m[1m[2023-07-10 20:40:31,430][227910] Min Reward on eval: -574.2351855991001[0m
[37m[1m[2023-07-10 20:40:31,431][227910] Mean Reward across all agents: 2390.989342983134[0m
[37m[1m[2023-07-10 20:40:31,431][227910] Average Trajectory Length: 931.798[0m
[36m[2023-07-10 20:40:31,433][227910] mean_value=-1293.8083679922743, max_value=809.4651552195235[0m
[37m[1m[2023-07-10 20:40:31,435][227910] New mean coefficients: [[ 1.5410854   0.3817268   0.17991328  0.42383665 -0.07817493]][0m
[37m[1m[2023-07-10 20:40:31,436][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:40:41,122][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 20:40:41,122][227910] FPS: 396530.08[0m
[36m[2023-07-10 20:40:41,125][227910] itr=1304, itrs=2000, Progress: 65.20%[0m
[36m[2023-07-10 20:40:52,703][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 20:40:52,703][227910] FPS: 332191.44[0m
[36m[2023-07-10 20:40:57,503][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:40:57,503][227910] Reward + Measures: [[6148.30405626    0.38591948    0.22175266    0.17782965    0.16431911]][0m
[37m[1m[2023-07-10 20:40:57,503][227910] Max Reward on eval: 6148.304056257958[0m
[37m[1m[2023-07-10 20:40:57,504][227910] Min Reward on eval: 6148.304056257958[0m
[37m[1m[2023-07-10 20:40:57,504][227910] Mean Reward across all agents: 6148.304056257958[0m
[37m[1m[2023-07-10 20:40:57,504][227910] Average Trajectory Length: 994.1759999999999[0m
[36m[2023-07-10 20:41:02,902][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:41:02,902][227910] Reward + Measures: [[2123.03503592    0.2894564     0.28435516    0.26021463    0.17871751]
 [1154.43489456    0.32248232    0.1994856     0.28066406    0.15389806]
 [3087.98460037    0.45019999    0.33610001    0.30050001    0.24560001]
 ...
 [ 929.26366391    0.2251824     0.497931      0.33080405    0.41481099]
 [1412.00208621    0.40862533    0.23194189    0.42738041    0.17711465]
 [2402.06316501    0.17200001    0.2069        0.198         0.1577    ]][0m
[37m[1m[2023-07-10 20:41:02,903][227910] Max Reward on eval: 5932.6803069739835[0m
[37m[1m[2023-07-10 20:41:02,903][227910] Min Reward on eval: 15.045223185955546[0m
[37m[1m[2023-07-10 20:41:02,903][227910] Mean Reward across all agents: 2294.2438184916537[0m
[37m[1m[2023-07-10 20:41:02,903][227910] Average Trajectory Length: 928.4386666666667[0m
[36m[2023-07-10 20:41:02,905][227910] mean_value=-1726.4499301368821, max_value=680.3058518369144[0m
[37m[1m[2023-07-10 20:41:02,908][227910] New mean coefficients: [[ 1.6164274   0.28114253  0.07193358  0.05010575 -0.17145944]][0m
[37m[1m[2023-07-10 20:41:02,909][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:41:12,523][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 20:41:12,524][227910] FPS: 399447.01[0m
[36m[2023-07-10 20:41:12,526][227910] itr=1305, itrs=2000, Progress: 65.25%[0m
[36m[2023-07-10 20:41:24,078][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 20:41:24,078][227910] FPS: 332968.35[0m
[36m[2023-07-10 20:41:28,911][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:41:28,911][227910] Reward + Measures: [[6222.60912871    0.38161382    0.21790221    0.17925507    0.16586913]][0m
[37m[1m[2023-07-10 20:41:28,911][227910] Max Reward on eval: 6222.60912871119[0m
[37m[1m[2023-07-10 20:41:28,912][227910] Min Reward on eval: 6222.60912871119[0m
[37m[1m[2023-07-10 20:41:28,912][227910] Mean Reward across all agents: 6222.60912871119[0m
[37m[1m[2023-07-10 20:41:28,912][227910] Average Trajectory Length: 993.9203333333332[0m
[36m[2023-07-10 20:41:34,545][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:41:34,545][227910] Reward + Measures: [[5367.00787822    0.39519       0.25102562    0.18410532    0.17087011]
 [4417.69622214    0.39289999    0.35670003    0.34440002    0.19720002]
 [3603.46043718    0.4244        0.2595        0.2552        0.2033    ]
 ...
 [2888.13509139    0.23712949    0.26470819    0.24134374    0.1680399 ]
 [2918.48446218    0.30723929    0.3080208     0.24408729    0.18330728]
 [3014.28467667    0.26120001    0.27320001    0.26910001    0.1972    ]][0m
[37m[1m[2023-07-10 20:41:34,546][227910] Max Reward on eval: 6019.483055590652[0m
[37m[1m[2023-07-10 20:41:34,546][227910] Min Reward on eval: 7.0693482170492645[0m
[37m[1m[2023-07-10 20:41:34,546][227910] Mean Reward across all agents: 2850.2725746827837[0m
[37m[1m[2023-07-10 20:41:34,546][227910] Average Trajectory Length: 918.9326666666666[0m
[36m[2023-07-10 20:41:34,548][227910] mean_value=-1629.7661682750004, max_value=1036.9228945557625[0m
[37m[1m[2023-07-10 20:41:34,551][227910] New mean coefficients: [[1.3959124  0.3386529  0.19949044 0.24628077 0.06860645]][0m
[37m[1m[2023-07-10 20:41:34,552][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:41:44,326][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 20:41:44,326][227910] FPS: 392930.29[0m
[36m[2023-07-10 20:41:44,329][227910] itr=1306, itrs=2000, Progress: 65.30%[0m
[36m[2023-07-10 20:41:56,016][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 20:41:56,016][227910] FPS: 329181.77[0m
[36m[2023-07-10 20:42:00,821][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:42:00,821][227910] Reward + Measures: [[6261.24432126    0.38338915    0.21969533    0.17495394    0.16453707]][0m
[37m[1m[2023-07-10 20:42:00,821][227910] Max Reward on eval: 6261.244321262943[0m
[37m[1m[2023-07-10 20:42:00,821][227910] Min Reward on eval: 6261.244321262943[0m
[37m[1m[2023-07-10 20:42:00,822][227910] Mean Reward across all agents: 6261.244321262943[0m
[37m[1m[2023-07-10 20:42:00,822][227910] Average Trajectory Length: 990.5213333333332[0m
[36m[2023-07-10 20:42:06,312][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:42:06,312][227910] Reward + Measures: [[3683.9505969     0.31389999    0.33590001    0.27850002    0.18520001]
 [2786.6571505     0.41149998    0.31529999    0.39429998    0.2093    ]
 [3665.02870675    0.48190004    0.25340003    0.28300002    0.19390002]
 ...
 [2918.65807179    0.22827359    0.27302009    0.25958118    0.17552124]
 [2693.97533871    0.22922428    0.28439057    0.2345715     0.1646055 ]
 [3993.29464121    0.42700005    0.2462        0.2218        0.17309999]][0m
[37m[1m[2023-07-10 20:42:06,313][227910] Max Reward on eval: 5934.689935897477[0m
[37m[1m[2023-07-10 20:42:06,313][227910] Min Reward on eval: -211.9002679318015[0m
[37m[1m[2023-07-10 20:42:06,313][227910] Mean Reward across all agents: 2938.86783398627[0m
[37m[1m[2023-07-10 20:42:06,313][227910] Average Trajectory Length: 954.7203333333333[0m
[36m[2023-07-10 20:42:06,315][227910] mean_value=-1335.8679573899847, max_value=916.864689744616[0m
[37m[1m[2023-07-10 20:42:06,317][227910] New mean coefficients: [[0.9511871  0.35546055 0.3986345  0.37763375 0.04973004]][0m
[37m[1m[2023-07-10 20:42:06,318][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:42:16,157][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 20:42:16,157][227910] FPS: 390368.03[0m
[36m[2023-07-10 20:42:16,159][227910] itr=1307, itrs=2000, Progress: 65.35%[0m
[36m[2023-07-10 20:42:27,769][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 20:42:27,770][227910] FPS: 331286.10[0m
[36m[2023-07-10 20:42:32,616][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:42:32,617][227910] Reward + Measures: [[6061.63451142    0.38677433    0.22282588    0.18116671    0.16475388]][0m
[37m[1m[2023-07-10 20:42:32,617][227910] Max Reward on eval: 6061.634511419562[0m
[37m[1m[2023-07-10 20:42:32,617][227910] Min Reward on eval: 6061.634511419562[0m
[37m[1m[2023-07-10 20:42:32,617][227910] Mean Reward across all agents: 6061.634511419562[0m
[37m[1m[2023-07-10 20:42:32,618][227910] Average Trajectory Length: 990.6553333333333[0m
[36m[2023-07-10 20:42:38,076][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:42:38,077][227910] Reward + Measures: [[2591.6859991     0.32155415    0.23879991    0.25509053    0.17945987]
 [ 917.18957575    0.42340001    0.41169998    0.34830001    0.3644    ]
 [1983.1504126     0.2007        0.2877        0.26989999    0.1698    ]
 ...
 [3037.75255596    0.28279999    0.40240002    0.37110001    0.23510002]
 [3509.44474611    0.28490001    0.37510005    0.27309999    0.17      ]
 [ 890.88587166    0.31825617    0.24610367    0.22450571    0.12623399]][0m
[37m[1m[2023-07-10 20:42:38,077][227910] Max Reward on eval: 5233.190028095851[0m
[37m[1m[2023-07-10 20:42:38,077][227910] Min Reward on eval: 49.20450532314135[0m
[37m[1m[2023-07-10 20:42:38,078][227910] Mean Reward across all agents: 2426.0197206872285[0m
[37m[1m[2023-07-10 20:42:38,078][227910] Average Trajectory Length: 947.2526666666666[0m
[36m[2023-07-10 20:42:38,080][227910] mean_value=-1488.6220450923263, max_value=874.1761578723209[0m
[37m[1m[2023-07-10 20:42:38,083][227910] New mean coefficients: [[ 0.55791456  0.6622917   0.43773237  0.1930713  -0.6302076 ]][0m
[37m[1m[2023-07-10 20:42:38,084][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:42:47,927][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 20:42:47,928][227910] FPS: 390159.83[0m
[36m[2023-07-10 20:42:47,930][227910] itr=1308, itrs=2000, Progress: 65.40%[0m
[36m[2023-07-10 20:42:59,584][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 20:42:59,584][227910] FPS: 330135.17[0m
[36m[2023-07-10 20:43:04,445][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:43:04,445][227910] Reward + Measures: [[6184.89924405    0.38257819    0.22349988    0.1776499     0.16410598]][0m
[37m[1m[2023-07-10 20:43:04,446][227910] Max Reward on eval: 6184.8992440453485[0m
[37m[1m[2023-07-10 20:43:04,446][227910] Min Reward on eval: 6184.8992440453485[0m
[37m[1m[2023-07-10 20:43:04,446][227910] Mean Reward across all agents: 6184.8992440453485[0m
[37m[1m[2023-07-10 20:43:04,446][227910] Average Trajectory Length: 991.2976666666666[0m
[36m[2023-07-10 20:43:09,829][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:43:09,830][227910] Reward + Measures: [[2349.36107576    0.36359608    0.46234271    0.30258152    0.15950936]
 [1992.83486686    0.24440001    0.42640001    0.41240001    0.19329999]
 [2337.1518215     0.36389524    0.43664989    0.32503513    0.15483844]
 ...
 [1125.5110585     0.33630019    0.3301613     0.30492124    0.14944628]
 [ 913.04593955    0.30065703    0.25742331    0.32729077    0.25428161]
 [2227.58957553    0.45070001    0.2357        0.33260003    0.19149999]][0m
[37m[1m[2023-07-10 20:43:09,830][227910] Max Reward on eval: 5577.65804323703[0m
[37m[1m[2023-07-10 20:43:09,830][227910] Min Reward on eval: -655.1443918572622[0m
[37m[1m[2023-07-10 20:43:09,831][227910] Mean Reward across all agents: 2036.1310494261963[0m
[37m[1m[2023-07-10 20:43:09,831][227910] Average Trajectory Length: 945.7076666666667[0m
[36m[2023-07-10 20:43:09,832][227910] mean_value=-1949.2703981671505, max_value=377.3520194326129[0m
[37m[1m[2023-07-10 20:43:09,835][227910] New mean coefficients: [[ 0.6128434   0.7190263   0.39108402  0.28114277 -0.3437813 ]][0m
[37m[1m[2023-07-10 20:43:09,836][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:43:19,590][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 20:43:19,590][227910] FPS: 393739.11[0m
[36m[2023-07-10 20:43:19,592][227910] itr=1309, itrs=2000, Progress: 65.45%[0m
[36m[2023-07-10 20:43:31,180][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 20:43:31,180][227910] FPS: 331914.32[0m
[36m[2023-07-10 20:43:35,968][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:43:35,969][227910] Reward + Measures: [[6289.66497081    0.40039575    0.2221842     0.17800234    0.1630836 ]][0m
[37m[1m[2023-07-10 20:43:35,969][227910] Max Reward on eval: 6289.664970813052[0m
[37m[1m[2023-07-10 20:43:35,969][227910] Min Reward on eval: 6289.664970813052[0m
[37m[1m[2023-07-10 20:43:35,969][227910] Mean Reward across all agents: 6289.664970813052[0m
[37m[1m[2023-07-10 20:43:35,970][227910] Average Trajectory Length: 994.005[0m
[36m[2023-07-10 20:43:41,570][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:43:41,570][227910] Reward + Measures: [[ 448.06118481    0.46790001    0.24330001    0.34910002    0.22260001]
 [2923.27891969    0.28792056    0.279672      0.19143583    0.16114949]
 [1587.21284894    0.3163        0.1816        0.2282        0.16379999]
 ...
 [1739.75597744    0.41589999    0.26499999    0.28819999    0.22409999]
 [1953.4048005     0.36820003    0.26019999    0.2471        0.20750001]
 [5026.80579577    0.41750002    0.21180001    0.23980001    0.18180001]][0m
[37m[1m[2023-07-10 20:43:41,571][227910] Max Reward on eval: 6303.461750719696[0m
[37m[1m[2023-07-10 20:43:41,571][227910] Min Reward on eval: -201.24231336171505[0m
[37m[1m[2023-07-10 20:43:41,571][227910] Mean Reward across all agents: 2022.3087103115822[0m
[37m[1m[2023-07-10 20:43:41,571][227910] Average Trajectory Length: 965.1763333333333[0m
[36m[2023-07-10 20:43:41,573][227910] mean_value=-2413.3907077298545, max_value=537.6188981222331[0m
[37m[1m[2023-07-10 20:43:41,576][227910] New mean coefficients: [[0.4541539  0.09982467 0.16919051 0.4271742  0.08935368]][0m
[37m[1m[2023-07-10 20:43:41,577][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:43:51,360][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 20:43:51,361][227910] FPS: 392555.09[0m
[36m[2023-07-10 20:43:51,363][227910] itr=1310, itrs=2000, Progress: 65.50%[0m
[37m[1m[2023-07-10 20:43:55,186][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001290[0m
[36m[2023-07-10 20:44:07,151][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 20:44:07,151][227910] FPS: 328382.85[0m
[36m[2023-07-10 20:44:12,015][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:44:12,016][227910] Reward + Measures: [[6348.51062378    0.40242353    0.22083685    0.17969792    0.16202648]][0m
[37m[1m[2023-07-10 20:44:12,016][227910] Max Reward on eval: 6348.510623782338[0m
[37m[1m[2023-07-10 20:44:12,016][227910] Min Reward on eval: 6348.510623782338[0m
[37m[1m[2023-07-10 20:44:12,016][227910] Mean Reward across all agents: 6348.510623782338[0m
[37m[1m[2023-07-10 20:44:12,017][227910] Average Trajectory Length: 995.438[0m
[36m[2023-07-10 20:44:17,563][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:44:17,564][227910] Reward + Measures: [[2735.40466766    0.354         0.3204        0.3132        0.17110001]
 [2617.60864506    0.41829997    0.2976        0.28330001    0.2375    ]
 [4635.07670668    0.38480002    0.28739998    0.2538        0.17279999]
 ...
 [3043.62211004    0.33157298    0.27564594    0.29402974    0.15859459]
 [4145.78210707    0.32590002    0.33740002    0.27550003    0.18980001]
 [2952.92264576    0.44119999    0.3263        0.2852        0.18910001]][0m
[37m[1m[2023-07-10 20:44:17,564][227910] Max Reward on eval: 6070.724418867566[0m
[37m[1m[2023-07-10 20:44:17,564][227910] Min Reward on eval: -370.6654961060587[0m
[37m[1m[2023-07-10 20:44:17,564][227910] Mean Reward across all agents: 2822.0857844223533[0m
[37m[1m[2023-07-10 20:44:17,565][227910] Average Trajectory Length: 962.483[0m
[36m[2023-07-10 20:44:17,566][227910] mean_value=-1337.247043162053, max_value=508.1576959120602[0m
[37m[1m[2023-07-10 20:44:17,569][227910] New mean coefficients: [[-0.0909977   0.09873826  0.26572067  0.582201    0.02743594]][0m
[37m[1m[2023-07-10 20:44:17,570][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:44:27,333][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 20:44:27,333][227910] FPS: 393380.32[0m
[36m[2023-07-10 20:44:27,335][227910] itr=1311, itrs=2000, Progress: 65.55%[0m
[36m[2023-07-10 20:44:39,006][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 20:44:39,007][227910] FPS: 329536.97[0m
[36m[2023-07-10 20:44:43,706][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:44:43,707][227910] Reward + Measures: [[5388.39166137    0.378272      0.24621522    0.20653223    0.17853297]][0m
[37m[1m[2023-07-10 20:44:43,707][227910] Max Reward on eval: 5388.391661365176[0m
[37m[1m[2023-07-10 20:44:43,707][227910] Min Reward on eval: 5388.391661365176[0m
[37m[1m[2023-07-10 20:44:43,708][227910] Mean Reward across all agents: 5388.391661365176[0m
[37m[1m[2023-07-10 20:44:43,708][227910] Average Trajectory Length: 989.3896666666666[0m
[36m[2023-07-10 20:44:49,430][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:44:49,431][227910] Reward + Measures: [[3391.49971952    0.30360001    0.27020001    0.36380002    0.20410001]
 [ 248.80575199    0.58630008    0.36380002    0.56380004    0.2879    ]
 [1437.20405108    0.53350002    0.3055        0.54730004    0.22830001]
 ...
 [2982.82576381    0.25443661    0.23225906    0.27055064    0.16297042]
 [3115.5422926     0.36970001    0.28099999    0.37729999    0.1829    ]
 [1346.02410767    0.57210004    0.28930002    0.50710005    0.2024    ]][0m
[37m[1m[2023-07-10 20:44:49,431][227910] Max Reward on eval: 5576.098682578653[0m
[37m[1m[2023-07-10 20:44:49,431][227910] Min Reward on eval: -122.46431308409664[0m
[37m[1m[2023-07-10 20:44:49,432][227910] Mean Reward across all agents: 2072.737635048448[0m
[37m[1m[2023-07-10 20:44:49,432][227910] Average Trajectory Length: 967.8666666666667[0m
[36m[2023-07-10 20:44:49,435][227910] mean_value=-960.8977413046529, max_value=857.3593361760425[0m
[37m[1m[2023-07-10 20:44:49,437][227910] New mean coefficients: [[-0.24752238  0.39286935  0.09419847  1.1089334   0.31433696]][0m
[37m[1m[2023-07-10 20:44:49,438][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:44:59,299][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 20:44:59,299][227910] FPS: 389492.19[0m
[36m[2023-07-10 20:44:59,302][227910] itr=1312, itrs=2000, Progress: 65.60%[0m
[36m[2023-07-10 20:45:10,931][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 20:45:10,931][227910] FPS: 330795.18[0m
[36m[2023-07-10 20:45:15,734][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:45:15,735][227910] Reward + Measures: [[3468.81464716    0.30469829    0.22938292    0.23599666    0.17859718]][0m
[37m[1m[2023-07-10 20:45:15,735][227910] Max Reward on eval: 3468.8146471640084[0m
[37m[1m[2023-07-10 20:45:15,735][227910] Min Reward on eval: 3468.8146471640084[0m
[37m[1m[2023-07-10 20:45:15,736][227910] Mean Reward across all agents: 3468.8146471640084[0m
[37m[1m[2023-07-10 20:45:15,736][227910] Average Trajectory Length: 944.1793333333333[0m
[36m[2023-07-10 20:45:21,272][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:45:21,273][227910] Reward + Measures: [[ 837.35574451    0.37620002    0.24330001    0.34580001    0.18810001]
 [1969.25022772    0.35520002    0.29730001    0.31890002    0.2023    ]
 [1608.38693343    0.29560003    0.2502        0.30650002    0.19239998]
 ...
 [ 768.04219749    0.40439233    0.23888461    0.36484617    0.20377694]
 [1564.57131154    0.31100002    0.26350001    0.37560001    0.2246    ]
 [3178.84107106    0.27340001    0.28920001    0.38620001    0.21100001]][0m
[37m[1m[2023-07-10 20:45:21,273][227910] Max Reward on eval: 4252.224800286465[0m
[37m[1m[2023-07-10 20:45:21,273][227910] Min Reward on eval: -228.60323891187363[0m
[37m[1m[2023-07-10 20:45:21,274][227910] Mean Reward across all agents: 1984.6931706076105[0m
[37m[1m[2023-07-10 20:45:21,274][227910] Average Trajectory Length: 964.074[0m
[36m[2023-07-10 20:45:21,275][227910] mean_value=-1992.0544691851844, max_value=39.3202652547202[0m
[37m[1m[2023-07-10 20:45:21,278][227910] New mean coefficients: [[-0.11334106  0.6584796  -0.08087748  0.73207146 -0.15046114]][0m
[37m[1m[2023-07-10 20:45:21,278][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:45:31,040][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 20:45:31,040][227910] FPS: 393467.48[0m
[36m[2023-07-10 20:45:31,042][227910] itr=1313, itrs=2000, Progress: 65.65%[0m
[36m[2023-07-10 20:45:42,651][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 20:45:42,651][227910] FPS: 331447.08[0m
[36m[2023-07-10 20:45:47,400][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:45:47,401][227910] Reward + Measures: [[2180.44788261    0.24752235    0.20335911    0.23711646    0.16778573]][0m
[37m[1m[2023-07-10 20:45:47,401][227910] Max Reward on eval: 2180.4478826051536[0m
[37m[1m[2023-07-10 20:45:47,401][227910] Min Reward on eval: 2180.4478826051536[0m
[37m[1m[2023-07-10 20:45:47,401][227910] Mean Reward across all agents: 2180.4478826051536[0m
[37m[1m[2023-07-10 20:45:47,402][227910] Average Trajectory Length: 921.9259999999999[0m
[36m[2023-07-10 20:45:52,814][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:45:52,815][227910] Reward + Measures: [[1775.90785848    0.26190001    0.31570002    0.50260001    0.18769999]
 [ 797.00762347    0.1323        0.1079        0.1452        0.12630001]
 [1100.02634247    0.18313555    0.15389165    0.28174344    0.17389834]
 ...
 [ 367.2929559     0.1053        0.0842        0.127         0.1061    ]
 [2723.96270038    0.25478777    0.21218081    0.27749547    0.17649738]
 [ 753.35807537    0.20913482    0.23044538    0.26410335    0.18709807]][0m
[37m[1m[2023-07-10 20:45:52,815][227910] Max Reward on eval: 4206.443276975217[0m
[37m[1m[2023-07-10 20:45:52,816][227910] Min Reward on eval: -240.30138634367614[0m
[37m[1m[2023-07-10 20:45:52,816][227910] Mean Reward across all agents: 1309.8544233548462[0m
[37m[1m[2023-07-10 20:45:52,816][227910] Average Trajectory Length: 918.5373333333333[0m
[36m[2023-07-10 20:45:52,817][227910] mean_value=-1723.0784770271166, max_value=370.67270277281546[0m
[37m[1m[2023-07-10 20:45:52,820][227910] New mean coefficients: [[ 0.01622663  0.35794276 -0.11085358  0.9162507   0.01790556]][0m
[37m[1m[2023-07-10 20:45:52,821][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:46:02,610][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 20:46:02,611][227910] FPS: 392317.45[0m
[36m[2023-07-10 20:46:02,613][227910] itr=1314, itrs=2000, Progress: 65.70%[0m
[36m[2023-07-10 20:46:14,228][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 20:46:14,229][227910] FPS: 331215.63[0m
[36m[2023-07-10 20:46:19,031][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:46:19,031][227910] Reward + Measures: [[2312.10370486    0.26158592    0.21932422    0.26239324    0.17617179]][0m
[37m[1m[2023-07-10 20:46:19,032][227910] Max Reward on eval: 2312.1037048624457[0m
[37m[1m[2023-07-10 20:46:19,032][227910] Min Reward on eval: 2312.1037048624457[0m
[37m[1m[2023-07-10 20:46:19,032][227910] Mean Reward across all agents: 2312.1037048624457[0m
[37m[1m[2023-07-10 20:46:19,032][227910] Average Trajectory Length: 925.4166666666666[0m
[36m[2023-07-10 20:46:24,557][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:46:24,558][227910] Reward + Measures: [[1901.72536213    0.24190001    0.22220002    0.31420001    0.16939999]
 [1836.23037922    0.22439499    0.2025048     0.22147606    0.15699065]
 [1349.65423038    0.18154557    0.18407004    0.24807005    0.15091932]
 ...
 [1729.95415186    0.19732867    0.21369766    0.2493187     0.16720138]
 [1040.07513494    0.1549        0.15549999    0.18979999    0.1196    ]
 [2183.43832084    0.19490001    0.2167        0.23669998    0.16260001]][0m
[37m[1m[2023-07-10 20:46:24,558][227910] Max Reward on eval: 4012.766712055355[0m
[37m[1m[2023-07-10 20:46:24,558][227910] Min Reward on eval: 632.9383098872728[0m
[37m[1m[2023-07-10 20:46:24,559][227910] Mean Reward across all agents: 2267.0716972741106[0m
[37m[1m[2023-07-10 20:46:24,559][227910] Average Trajectory Length: 943.7363333333333[0m
[36m[2023-07-10 20:46:24,561][227910] mean_value=-1464.8407858324636, max_value=364.94091644975515[0m
[37m[1m[2023-07-10 20:46:24,563][227910] New mean coefficients: [[0.324147   0.15605012 0.00118563 1.0238369  0.02048655]][0m
[37m[1m[2023-07-10 20:46:24,564][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:46:34,387][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 20:46:34,387][227910] FPS: 390985.89[0m
[36m[2023-07-10 20:46:34,389][227910] itr=1315, itrs=2000, Progress: 65.75%[0m
[36m[2023-07-10 20:46:46,040][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 20:46:46,041][227910] FPS: 330208.64[0m
[36m[2023-07-10 20:46:50,854][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:46:50,854][227910] Reward + Measures: [[3048.20895412    0.29679283    0.2366664     0.26593062    0.18236789]][0m
[37m[1m[2023-07-10 20:46:50,854][227910] Max Reward on eval: 3048.208954123665[0m
[37m[1m[2023-07-10 20:46:50,855][227910] Min Reward on eval: 3048.208954123665[0m
[37m[1m[2023-07-10 20:46:50,855][227910] Mean Reward across all agents: 3048.208954123665[0m
[37m[1m[2023-07-10 20:46:50,855][227910] Average Trajectory Length: 940.9373333333333[0m
[36m[2023-07-10 20:46:56,265][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:46:56,266][227910] Reward + Measures: [[2728.25311516    0.30969998    0.3008        0.26280001    0.20219998]
 [1527.71273881    0.59799999    0.333         0.46830001    0.20869999]
 [2245.27831409    0.29179999    0.26560003    0.2362        0.19829999]
 ...
 [1686.8204789     0.4955        0.35120001    0.43860003    0.1895    ]
 [1174.02280837    0.51599997    0.39899999    0.40079999    0.19850002]
 [1891.52784593    0.31999999    0.32659999    0.33849999    0.21519999]][0m
[37m[1m[2023-07-10 20:46:56,266][227910] Max Reward on eval: 4168.187623644847[0m
[37m[1m[2023-07-10 20:46:56,266][227910] Min Reward on eval: -467.05977302702956[0m
[37m[1m[2023-07-10 20:46:56,267][227910] Mean Reward across all agents: 1569.2625881320514[0m
[37m[1m[2023-07-10 20:46:56,267][227910] Average Trajectory Length: 955.319[0m
[36m[2023-07-10 20:46:56,269][227910] mean_value=-1850.7865524673668, max_value=1350.1013200431803[0m
[37m[1m[2023-07-10 20:46:56,271][227910] New mean coefficients: [[ 0.5570214   0.09534591 -0.318831    0.54233146  0.11635561]][0m
[37m[1m[2023-07-10 20:46:56,272][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:47:06,065][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 20:47:06,066][227910] FPS: 392168.82[0m
[36m[2023-07-10 20:47:06,068][227910] itr=1316, itrs=2000, Progress: 65.80%[0m
[36m[2023-07-10 20:47:17,573][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 20:47:17,574][227910] FPS: 334381.87[0m
[36m[2023-07-10 20:47:22,393][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:47:22,394][227910] Reward + Measures: [[4148.09872334    0.34111372    0.24684456    0.25454655    0.18464424]][0m
[37m[1m[2023-07-10 20:47:22,394][227910] Max Reward on eval: 4148.098723341219[0m
[37m[1m[2023-07-10 20:47:22,394][227910] Min Reward on eval: 4148.098723341219[0m
[37m[1m[2023-07-10 20:47:22,394][227910] Mean Reward across all agents: 4148.098723341219[0m
[37m[1m[2023-07-10 20:47:22,395][227910] Average Trajectory Length: 963.0749999999999[0m
[36m[2023-07-10 20:47:28,154][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:47:28,160][227910] Reward + Measures: [[2082.6463432     0.44019994    0.35260001    0.35349998    0.20649998]
 [1158.73282581    0.19231205    0.18374205    0.21195059    0.1327749 ]
 [-682.80394163    0.38930002    0.45439997    0.70090002    0.64880002]
 ...
 [2561.20610994    0.36548001    0.27172002    0.39780003    0.17298   ]
 [ 674.60308882    0.32499999    0.35769999    0.3827        0.359     ]
 [1066.60557604    0.20507495    0.41743517    0.24013424    0.19802181]][0m
[37m[1m[2023-07-10 20:47:28,161][227910] Max Reward on eval: 4801.439641297236[0m
[37m[1m[2023-07-10 20:47:28,161][227910] Min Reward on eval: -682.8039416261483[0m
[37m[1m[2023-07-10 20:47:28,162][227910] Mean Reward across all agents: 1817.8756223097707[0m
[37m[1m[2023-07-10 20:47:28,163][227910] Average Trajectory Length: 874.3816666666667[0m
[36m[2023-07-10 20:47:28,166][227910] mean_value=-1981.2077030186088, max_value=744.6847680191242[0m
[37m[1m[2023-07-10 20:47:28,171][227910] New mean coefficients: [[ 0.50374866  0.00069167 -0.5620356   0.25123158 -0.22928223]][0m
[37m[1m[2023-07-10 20:47:28,173][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:47:37,949][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 20:47:37,950][227910] FPS: 392877.23[0m
[36m[2023-07-10 20:47:37,952][227910] itr=1317, itrs=2000, Progress: 65.85%[0m
[36m[2023-07-10 20:47:49,717][227910] train() took 11.75 seconds to complete[0m
[36m[2023-07-10 20:47:49,718][227910] FPS: 326903.20[0m
[36m[2023-07-10 20:47:54,442][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:47:54,442][227910] Reward + Measures: [[5358.27661335    0.37514484    0.23474444    0.19896448    0.17935795]][0m
[37m[1m[2023-07-10 20:47:54,443][227910] Max Reward on eval: 5358.276613345023[0m
[37m[1m[2023-07-10 20:47:54,443][227910] Min Reward on eval: 5358.276613345023[0m
[37m[1m[2023-07-10 20:47:54,443][227910] Mean Reward across all agents: 5358.276613345023[0m
[37m[1m[2023-07-10 20:47:54,443][227910] Average Trajectory Length: 990.5793333333334[0m
[36m[2023-07-10 20:47:59,937][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:47:59,937][227910] Reward + Measures: [[2918.43643565    0.3884514     0.22961299    0.27891326    0.17805439]
 [2084.07377498    0.4454        0.19930001    0.25420001    0.1675    ]
 [ 957.87071124    0.37373388    0.24299236    0.25149798    0.16616623]
 ...
 [ 642.20457264    0.3272768     0.25361368    0.24868932    0.16064869]
 [4845.94832288    0.36840001    0.25680003    0.2342        0.19190001]
 [3732.4678737     0.31650001    0.29260001    0.35670003    0.20680001]][0m
[37m[1m[2023-07-10 20:47:59,938][227910] Max Reward on eval: 5286.3487889992075[0m
[37m[1m[2023-07-10 20:47:59,938][227910] Min Reward on eval: -175.17791252571624[0m
[37m[1m[2023-07-10 20:47:59,938][227910] Mean Reward across all agents: 2470.8472753933784[0m
[37m[1m[2023-07-10 20:47:59,938][227910] Average Trajectory Length: 843.5823333333333[0m
[36m[2023-07-10 20:47:59,940][227910] mean_value=-2157.127384974554, max_value=604.8224929427815[0m
[37m[1m[2023-07-10 20:47:59,942][227910] New mean coefficients: [[ 0.47704047  0.20207477  0.17312974  0.8502537  -0.00143096]][0m
[37m[1m[2023-07-10 20:47:59,943][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:48:09,760][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 20:48:09,760][227910] FPS: 391230.49[0m
[36m[2023-07-10 20:48:09,763][227910] itr=1318, itrs=2000, Progress: 65.90%[0m
[36m[2023-07-10 20:48:21,391][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 20:48:21,391][227910] FPS: 330758.68[0m
[36m[2023-07-10 20:48:26,139][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:48:26,140][227910] Reward + Measures: [[5557.22289131    0.37733096    0.22782376    0.19879191    0.17887282]][0m
[37m[1m[2023-07-10 20:48:26,140][227910] Max Reward on eval: 5557.222891305211[0m
[37m[1m[2023-07-10 20:48:26,140][227910] Min Reward on eval: 5557.222891305211[0m
[37m[1m[2023-07-10 20:48:26,140][227910] Mean Reward across all agents: 5557.222891305211[0m
[37m[1m[2023-07-10 20:48:26,141][227910] Average Trajectory Length: 995.8216666666666[0m
[36m[2023-07-10 20:48:31,627][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:48:31,627][227910] Reward + Measures: [[2134.24482262    0.30859479    0.31907555    0.34750944    0.24066953]
 [ 342.01685902    0.28272483    0.36728114    0.24129806    0.20040455]
 [2127.78530892    0.23973684    0.25787899    0.18941186    0.15985918]
 ...
 [5095.2777113     0.34589997    0.27309999    0.21350001    0.18740001]
 [4735.76335241    0.33619997    0.28150001    0.21730001    0.19330001]
 [ 162.01340779    0.22360311    0.31062093    0.35555431    0.30487368]][0m
[37m[1m[2023-07-10 20:48:31,627][227910] Max Reward on eval: 5135.075320820138[0m
[37m[1m[2023-07-10 20:48:31,628][227910] Min Reward on eval: -560.6549187379366[0m
[37m[1m[2023-07-10 20:48:31,628][227910] Mean Reward across all agents: 1603.4250752679138[0m
[37m[1m[2023-07-10 20:48:31,628][227910] Average Trajectory Length: 915.02[0m
[36m[2023-07-10 20:48:31,630][227910] mean_value=-1826.8620341100047, max_value=851.5651262765642[0m
[37m[1m[2023-07-10 20:48:31,632][227910] New mean coefficients: [[0.22520909 0.3161201  0.11367615 0.9759944  0.151076  ]][0m
[37m[1m[2023-07-10 20:48:31,633][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:48:41,352][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 20:48:41,352][227910] FPS: 395190.54[0m
[36m[2023-07-10 20:48:41,354][227910] itr=1319, itrs=2000, Progress: 65.95%[0m
[36m[2023-07-10 20:48:52,838][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 20:48:52,838][227910] FPS: 334928.30[0m
[36m[2023-07-10 20:48:57,656][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:48:57,656][227910] Reward + Measures: [[5660.97276776    0.38196144    0.23259778    0.20026687    0.17918542]][0m
[37m[1m[2023-07-10 20:48:57,656][227910] Max Reward on eval: 5660.972767755702[0m
[37m[1m[2023-07-10 20:48:57,656][227910] Min Reward on eval: 5660.972767755702[0m
[37m[1m[2023-07-10 20:48:57,657][227910] Mean Reward across all agents: 5660.972767755702[0m
[37m[1m[2023-07-10 20:48:57,657][227910] Average Trajectory Length: 995.024[0m
[36m[2023-07-10 20:49:03,045][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:49:03,046][227910] Reward + Measures: [[2464.98320587    0.28756595    0.24095416    0.32931265    0.18090391]
 [5704.64014176    0.37490001    0.24609999    0.19620001    0.17650001]
 [4941.04951754    0.34254584    0.24472998    0.21127032    0.17711841]
 ...
 [2441.13442003    0.24704461    0.44018784    0.24497567    0.18399461]
 [2772.68899482    0.30689999    0.30879998    0.28430003    0.16159999]
 [2553.15819673    0.28710005    0.29800001    0.31240001    0.16500001]][0m
[37m[1m[2023-07-10 20:49:03,046][227910] Max Reward on eval: 5704.640141757392[0m
[37m[1m[2023-07-10 20:49:03,046][227910] Min Reward on eval: 190.70567950560945[0m
[37m[1m[2023-07-10 20:49:03,046][227910] Mean Reward across all agents: 2729.642621484894[0m
[37m[1m[2023-07-10 20:49:03,047][227910] Average Trajectory Length: 917.9976666666666[0m
[36m[2023-07-10 20:49:03,048][227910] mean_value=-1868.7369081938034, max_value=426.3604642806886[0m
[37m[1m[2023-07-10 20:49:03,050][227910] New mean coefficients: [[ 0.3067972   0.0311918  -0.32992873  0.64680386  0.26641342]][0m
[37m[1m[2023-07-10 20:49:03,051][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:49:12,674][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 20:49:12,674][227910] FPS: 399129.75[0m
[36m[2023-07-10 20:49:12,676][227910] itr=1320, itrs=2000, Progress: 66.00%[0m
[37m[1m[2023-07-10 20:49:16,576][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001300[0m
[36m[2023-07-10 20:49:28,360][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 20:49:28,360][227910] FPS: 333617.86[0m
[36m[2023-07-10 20:49:33,141][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:49:33,142][227910] Reward + Measures: [[5728.27461077    0.38082412    0.22396877    0.20162879    0.18101653]][0m
[37m[1m[2023-07-10 20:49:33,142][227910] Max Reward on eval: 5728.274610765043[0m
[37m[1m[2023-07-10 20:49:33,142][227910] Min Reward on eval: 5728.274610765043[0m
[37m[1m[2023-07-10 20:49:33,142][227910] Mean Reward across all agents: 5728.274610765043[0m
[37m[1m[2023-07-10 20:49:33,143][227910] Average Trajectory Length: 995.1643333333333[0m
[36m[2023-07-10 20:49:38,601][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:49:38,601][227910] Reward + Measures: [[2982.72172136    0.31176105    0.41621858    0.30540746    0.18580282]
 [3790.10550316    0.30770001    0.21500002    0.21180001    0.17220001]
 [3982.07224574    0.36637357    0.24693166    0.20323606    0.1621733 ]
 ...
 [2618.49279618    0.30088869    0.30040434    0.23016016    0.18563001]
 [1339.14878362    0.29579183    0.28404623    0.21946283    0.14606774]
 [4134.54430689    0.30942184    0.25042725    0.19712389    0.16643812]][0m
[37m[1m[2023-07-10 20:49:38,601][227910] Max Reward on eval: 5637.118524002377[0m
[37m[1m[2023-07-10 20:49:38,602][227910] Min Reward on eval: 420.76877541287104[0m
[37m[1m[2023-07-10 20:49:38,602][227910] Mean Reward across all agents: 2990.107849020731[0m
[37m[1m[2023-07-10 20:49:38,602][227910] Average Trajectory Length: 853.5656666666666[0m
[36m[2023-07-10 20:49:38,604][227910] mean_value=-1904.6705017822776, max_value=1332.7029480821375[0m
[37m[1m[2023-07-10 20:49:38,606][227910] New mean coefficients: [[ 0.29047233 -0.11514716  0.08021769  0.44798478  0.09116493]][0m
[37m[1m[2023-07-10 20:49:38,607][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:49:48,349][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 20:49:48,349][227910] FPS: 394245.89[0m
[36m[2023-07-10 20:49:48,352][227910] itr=1321, itrs=2000, Progress: 66.05%[0m
[36m[2023-07-10 20:49:59,896][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 20:49:59,896][227910] FPS: 333188.10[0m
[36m[2023-07-10 20:50:04,586][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:50:04,587][227910] Reward + Measures: [[5843.61944906    0.37592891    0.22047888    0.19500025    0.17550834]][0m
[37m[1m[2023-07-10 20:50:04,587][227910] Max Reward on eval: 5843.6194490630005[0m
[37m[1m[2023-07-10 20:50:04,587][227910] Min Reward on eval: 5843.6194490630005[0m
[37m[1m[2023-07-10 20:50:04,587][227910] Mean Reward across all agents: 5843.6194490630005[0m
[37m[1m[2023-07-10 20:50:04,588][227910] Average Trajectory Length: 992.2273333333333[0m
[36m[2023-07-10 20:50:10,184][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:50:10,184][227910] Reward + Measures: [[4798.65655876    0.32510963    0.23700106    0.24766417    0.19215134]
 [4587.82955079    0.2581        0.31920001    0.21710001    0.1771    ]
 [4901.03972191    0.393181      0.21091811    0.20569146    0.17394581]
 ...
 [2158.17345189    0.26091108    0.28175321    0.23771825    0.17056368]
 [3982.22190468    0.28380367    0.26027942    0.24988686    0.18429627]
 [3360.04814582    0.2412        0.2859        0.35699999    0.22669999]][0m
[37m[1m[2023-07-10 20:50:10,184][227910] Max Reward on eval: 5897.965077819676[0m
[37m[1m[2023-07-10 20:50:10,185][227910] Min Reward on eval: 512.8823298154632[0m
[37m[1m[2023-07-10 20:50:10,185][227910] Mean Reward across all agents: 3000.7366679145907[0m
[37m[1m[2023-07-10 20:50:10,185][227910] Average Trajectory Length: 942.0916666666666[0m
[36m[2023-07-10 20:50:10,187][227910] mean_value=-1390.7936075553, max_value=1689.4337313684573[0m
[37m[1m[2023-07-10 20:50:10,190][227910] New mean coefficients: [[ 0.35367465 -0.0301493   0.11593363  0.67549515  0.3126613 ]][0m
[37m[1m[2023-07-10 20:50:10,191][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:50:20,021][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 20:50:20,022][227910] FPS: 390704.51[0m
[36m[2023-07-10 20:50:20,024][227910] itr=1322, itrs=2000, Progress: 66.10%[0m
[36m[2023-07-10 20:50:31,546][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 20:50:31,547][227910] FPS: 333785.56[0m
[36m[2023-07-10 20:50:36,232][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:50:36,233][227910] Reward + Measures: [[5983.3435446     0.37063548    0.21713793    0.18705872    0.16985042]][0m
[37m[1m[2023-07-10 20:50:36,233][227910] Max Reward on eval: 5983.343544595042[0m
[37m[1m[2023-07-10 20:50:36,233][227910] Min Reward on eval: 5983.343544595042[0m
[37m[1m[2023-07-10 20:50:36,233][227910] Mean Reward across all agents: 5983.343544595042[0m
[37m[1m[2023-07-10 20:50:36,234][227910] Average Trajectory Length: 988.6223333333332[0m
[36m[2023-07-10 20:50:41,875][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:50:41,876][227910] Reward + Measures: [[2629.21107283    0.28389999    0.33540002    0.2811        0.2208    ]
 [ 403.23803183    0.1573        0.13060001    0.2379        0.17809999]
 [1406.0233589     0.31250483    0.20808835    0.26018319    0.21067479]
 ...
 [2902.70084186    0.28380004    0.2746        0.28849998    0.23550001]
 [1980.51013575    0.28670001    0.29930001    0.3003        0.27290002]
 [3270.76750208    0.28316274    0.28546271    0.41984248    0.20794021]][0m
[37m[1m[2023-07-10 20:50:41,876][227910] Max Reward on eval: 6041.832915971708[0m
[37m[1m[2023-07-10 20:50:41,876][227910] Min Reward on eval: -453.5992600889498[0m
[37m[1m[2023-07-10 20:50:41,876][227910] Mean Reward across all agents: 2274.6298920812815[0m
[37m[1m[2023-07-10 20:50:41,877][227910] Average Trajectory Length: 978.7529999999999[0m
[36m[2023-07-10 20:50:41,879][227910] mean_value=-1475.1730240630063, max_value=878.2169299491184[0m
[37m[1m[2023-07-10 20:50:41,882][227910] New mean coefficients: [[ 0.6046184  -0.20271505  0.0081359   0.57464385 -0.00423133]][0m
[37m[1m[2023-07-10 20:50:41,883][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:50:51,633][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 20:50:51,633][227910] FPS: 393891.59[0m
[36m[2023-07-10 20:50:51,635][227910] itr=1323, itrs=2000, Progress: 66.15%[0m
[36m[2023-07-10 20:51:03,188][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 20:51:03,189][227910] FPS: 332944.11[0m
[36m[2023-07-10 20:51:07,938][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:51:07,944][227910] Reward + Measures: [[5970.69332731    0.3728911     0.21715674    0.18390605    0.16756919]][0m
[37m[1m[2023-07-10 20:51:07,945][227910] Max Reward on eval: 5970.693327311359[0m
[37m[1m[2023-07-10 20:51:07,946][227910] Min Reward on eval: 5970.693327311359[0m
[37m[1m[2023-07-10 20:51:07,946][227910] Mean Reward across all agents: 5970.693327311359[0m
[37m[1m[2023-07-10 20:51:07,947][227910] Average Trajectory Length: 983.3643333333333[0m
[36m[2023-07-10 20:51:13,489][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:51:13,494][227910] Reward + Measures: [[ 689.68317227    0.24945708    0.24535453    0.21963274    0.17042442]
 [ 543.91921065    0.23662268    0.32820472    0.23031814    0.18901865]
 [ 626.78759963    0.2146776     0.25036952    0.2444319     0.16754524]
 ...
 [5407.38159074    0.34048888    0.19826938    0.19203682    0.16889969]
 [1042.67197165    0.23539174    0.27692738    0.24321826    0.17707543]
 [1209.38292659    0.24281214    0.25410804    0.23803759    0.17829515]][0m
[37m[1m[2023-07-10 20:51:13,495][227910] Max Reward on eval: 5753.901763522905[0m
[37m[1m[2023-07-10 20:51:13,495][227910] Min Reward on eval: 154.48998540150933[0m
[37m[1m[2023-07-10 20:51:13,495][227910] Mean Reward across all agents: 2221.3148063136637[0m
[37m[1m[2023-07-10 20:51:13,495][227910] Average Trajectory Length: 681.18[0m
[36m[2023-07-10 20:51:13,497][227910] mean_value=-2440.932492387569, max_value=1049.0087876736561[0m
[37m[1m[2023-07-10 20:51:13,500][227910] New mean coefficients: [[ 0.4097244  -0.1655225  -0.14554332  0.07086152 -0.09599851]][0m
[37m[1m[2023-07-10 20:51:13,501][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:51:23,369][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 20:51:23,369][227910] FPS: 389211.15[0m
[36m[2023-07-10 20:51:23,371][227910] itr=1324, itrs=2000, Progress: 66.20%[0m
[36m[2023-07-10 20:51:35,006][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 20:51:35,006][227910] FPS: 330588.41[0m
[36m[2023-07-10 20:51:39,757][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:51:39,757][227910] Reward + Measures: [[6099.41520936    0.37357658    0.22240068    0.1841415     0.1679031 ]][0m
[37m[1m[2023-07-10 20:51:39,757][227910] Max Reward on eval: 6099.415209360201[0m
[37m[1m[2023-07-10 20:51:39,757][227910] Min Reward on eval: 6099.415209360201[0m
[37m[1m[2023-07-10 20:51:39,758][227910] Mean Reward across all agents: 6099.415209360201[0m
[37m[1m[2023-07-10 20:51:39,758][227910] Average Trajectory Length: 990.453[0m
[36m[2023-07-10 20:51:45,168][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:51:45,169][227910] Reward + Measures: [[2125.35459105    0.24115126    0.25879246    0.288093      0.16106693]
 [ 749.81536901    0.34100002    0.31309998    0.44500002    0.2978    ]
 [4260.56721641    0.36680004    0.3055        0.22319999    0.18670002]
 ...
 [ 535.59486623    0.30025846    0.2587119     0.40613642    0.21361478]
 [2146.59631316    0.26350257    0.29246205    0.29517174    0.16422032]
 [1260.32209206    0.27760002    0.27900001    0.3836        0.25579998]][0m
[37m[1m[2023-07-10 20:51:45,169][227910] Max Reward on eval: 6009.0213511399925[0m
[37m[1m[2023-07-10 20:51:45,169][227910] Min Reward on eval: 435.0574202812568[0m
[37m[1m[2023-07-10 20:51:45,170][227910] Mean Reward across all agents: 2523.22880923197[0m
[37m[1m[2023-07-10 20:51:45,170][227910] Average Trajectory Length: 900.7816666666666[0m
[36m[2023-07-10 20:51:45,171][227910] mean_value=-1875.442295505278, max_value=772.839978513598[0m
[37m[1m[2023-07-10 20:51:45,174][227910] New mean coefficients: [[ 0.42740053 -0.24925762  0.10354334  0.05291221 -0.17721808]][0m
[37m[1m[2023-07-10 20:51:45,175][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:51:54,898][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 20:51:54,899][227910] FPS: 394984.22[0m
[36m[2023-07-10 20:51:54,901][227910] itr=1325, itrs=2000, Progress: 66.25%[0m
[36m[2023-07-10 20:52:06,514][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 20:52:06,514][227910] FPS: 331281.40[0m
[36m[2023-07-10 20:52:11,289][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:52:11,289][227910] Reward + Measures: [[6158.32414421    0.36672905    0.2149733     0.17942868    0.16456328]][0m
[37m[1m[2023-07-10 20:52:11,290][227910] Max Reward on eval: 6158.324144207085[0m
[37m[1m[2023-07-10 20:52:11,290][227910] Min Reward on eval: 6158.324144207085[0m
[37m[1m[2023-07-10 20:52:11,290][227910] Mean Reward across all agents: 6158.324144207085[0m
[37m[1m[2023-07-10 20:52:11,291][227910] Average Trajectory Length: 984.1906666666666[0m
[36m[2023-07-10 20:52:16,672][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:52:16,678][227910] Reward + Measures: [[1266.39277266    0.16068952    0.17932284    0.19415607    0.15989213]
 [1818.74168835    0.36469999    0.32560003    0.41159996    0.23339999]
 [2318.50989006    0.23909998    0.34800002    0.42129999    0.211     ]
 ...
 [5382.9650821     0.33540577    0.20388702    0.19118473    0.17013742]
 [5919.29298456    0.40499997    0.22390001    0.21070002    0.17459999]
 [2089.37358332    0.29035091    0.32299578    0.21743834    0.19118805]][0m
[37m[1m[2023-07-10 20:52:16,678][227910] Max Reward on eval: 6169.763973804936[0m
[37m[1m[2023-07-10 20:52:16,678][227910] Min Reward on eval: 217.95782672164495[0m
[37m[1m[2023-07-10 20:52:16,679][227910] Mean Reward across all agents: 2215.854252798125[0m
[37m[1m[2023-07-10 20:52:16,679][227910] Average Trajectory Length: 787.9929999999999[0m
[36m[2023-07-10 20:52:16,681][227910] mean_value=-1995.8254674202944, max_value=586.7313252615768[0m
[37m[1m[2023-07-10 20:52:16,684][227910] New mean coefficients: [[ 0.61189824 -0.25229332 -0.3486061  -0.01516708  0.1948233 ]][0m
[37m[1m[2023-07-10 20:52:16,685][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:52:26,342][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 20:52:26,342][227910] FPS: 397718.15[0m
[36m[2023-07-10 20:52:26,344][227910] itr=1326, itrs=2000, Progress: 66.30%[0m
[36m[2023-07-10 20:52:38,005][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 20:52:38,005][227910] FPS: 329847.56[0m
[36m[2023-07-10 20:52:42,766][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:52:42,767][227910] Reward + Measures: [[6244.72621434    0.363469      0.21434924    0.17438383    0.16337791]][0m
[37m[1m[2023-07-10 20:52:42,767][227910] Max Reward on eval: 6244.72621433778[0m
[37m[1m[2023-07-10 20:52:42,767][227910] Min Reward on eval: 6244.72621433778[0m
[37m[1m[2023-07-10 20:52:42,767][227910] Mean Reward across all agents: 6244.72621433778[0m
[37m[1m[2023-07-10 20:52:42,768][227910] Average Trajectory Length: 984.2106666666666[0m
[36m[2023-07-10 20:52:48,322][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:52:48,323][227910] Reward + Measures: [[5727.81571501    0.40000001    0.2493        0.2088        0.1811    ]
 [5582.78083696    0.39390001    0.24349999    0.20420001    0.18539999]
 [3419.07604108    0.23550001    0.2447        0.27420002    0.1725    ]
 ...
 [2201.65142042    0.39070001    0.20809999    0.39930001    0.19530001]
 [4854.41954628    0.35349214    0.21207495    0.1743969     0.16434924]
 [3123.06104112    0.3752887     0.2935079     0.35022327    0.22372484]][0m
[37m[1m[2023-07-10 20:52:48,323][227910] Max Reward on eval: 6288.14587544077[0m
[37m[1m[2023-07-10 20:52:48,323][227910] Min Reward on eval: 1553.9338224344306[0m
[37m[1m[2023-07-10 20:52:48,324][227910] Mean Reward across all agents: 4564.4828565516555[0m
[37m[1m[2023-07-10 20:52:48,324][227910] Average Trajectory Length: 953.1283333333333[0m
[36m[2023-07-10 20:52:48,326][227910] mean_value=-848.0889434188921, max_value=571.4589413693875[0m
[37m[1m[2023-07-10 20:52:48,328][227910] New mean coefficients: [[ 0.32653672 -0.23642236  0.10736847  0.10941155  0.16403131]][0m
[37m[1m[2023-07-10 20:52:48,329][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:52:58,048][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 20:52:58,048][227910] FPS: 395201.83[0m
[36m[2023-07-10 20:52:58,050][227910] itr=1327, itrs=2000, Progress: 66.35%[0m
[36m[2023-07-10 20:53:09,805][227910] train() took 11.73 seconds to complete[0m
[36m[2023-07-10 20:53:09,805][227910] FPS: 327313.31[0m
[36m[2023-07-10 20:53:14,818][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:53:14,819][227910] Reward + Measures: [[6339.7667597     0.36431152    0.21250317    0.17509098    0.16594589]][0m
[37m[1m[2023-07-10 20:53:14,819][227910] Max Reward on eval: 6339.766759704806[0m
[37m[1m[2023-07-10 20:53:14,819][227910] Min Reward on eval: 6339.766759704806[0m
[37m[1m[2023-07-10 20:53:14,820][227910] Mean Reward across all agents: 6339.766759704806[0m
[37m[1m[2023-07-10 20:53:14,820][227910] Average Trajectory Length: 988.2733333333333[0m
[36m[2023-07-10 20:53:20,325][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:53:20,326][227910] Reward + Measures: [[1362.90112001    0.23980001    0.36339998    0.32010001    0.31220001]
 [1415.31036974    0.2016        0.35949999    0.29630002    0.26660001]
 [1893.87302174    0.38352472    0.32717833    0.25873899    0.15224783]
 ...
 [2870.74601435    0.2933        0.35159999    0.31870002    0.21320002]
 [2521.67359368    0.19531301    0.30228293    0.26377088    0.17991832]
 [2454.2889346     0.34530002    0.32940003    0.31939998    0.2102    ]][0m
[37m[1m[2023-07-10 20:53:20,326][227910] Max Reward on eval: 6137.031252215058[0m
[37m[1m[2023-07-10 20:53:20,326][227910] Min Reward on eval: -678.4800607062759[0m
[37m[1m[2023-07-10 20:53:20,326][227910] Mean Reward across all agents: 2800.7616437276447[0m
[37m[1m[2023-07-10 20:53:20,327][227910] Average Trajectory Length: 904.033[0m
[36m[2023-07-10 20:53:20,328][227910] mean_value=-1847.4706248658483, max_value=512.0908215888239[0m
[37m[1m[2023-07-10 20:53:20,330][227910] New mean coefficients: [[ 0.44252166 -0.35623735  0.11591543 -0.05980014  0.34904474]][0m
[37m[1m[2023-07-10 20:53:20,332][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:53:30,136][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 20:53:30,137][227910] FPS: 391713.39[0m
[36m[2023-07-10 20:53:30,139][227910] itr=1328, itrs=2000, Progress: 66.40%[0m
[36m[2023-07-10 20:53:41,799][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 20:53:41,799][227910] FPS: 329877.13[0m
[36m[2023-07-10 20:53:46,467][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:53:46,467][227910] Reward + Measures: [[6430.14808982    0.37557909    0.20599537    0.17412435    0.16527718]][0m
[37m[1m[2023-07-10 20:53:46,467][227910] Max Reward on eval: 6430.148089823901[0m
[37m[1m[2023-07-10 20:53:46,468][227910] Min Reward on eval: 6430.148089823901[0m
[37m[1m[2023-07-10 20:53:46,468][227910] Mean Reward across all agents: 6430.148089823901[0m
[37m[1m[2023-07-10 20:53:46,468][227910] Average Trajectory Length: 990.7203333333333[0m
[36m[2023-07-10 20:53:51,824][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:53:51,825][227910] Reward + Measures: [[3624.01201605    0.2333        0.25530002    0.22239999    0.17309999]
 [3659.05429074    0.31780002    0.20309091    0.17964546    0.15555456]
 [4435.84769001    0.35339922    0.24159883    0.24957037    0.17480554]
 ...
 [3508.72183937    0.34779605    0.21470514    0.17845246    0.16280715]
 [3489.02995479    0.24590002    0.27780002    0.2554        0.18859999]
 [1657.614957      0.1945        0.3161        0.42589998    0.24660002]][0m
[37m[1m[2023-07-10 20:53:51,825][227910] Max Reward on eval: 6449.879786149133[0m
[37m[1m[2023-07-10 20:53:51,825][227910] Min Reward on eval: 170.83268188633957[0m
[37m[1m[2023-07-10 20:53:51,826][227910] Mean Reward across all agents: 2946.417278692881[0m
[37m[1m[2023-07-10 20:53:51,826][227910] Average Trajectory Length: 885.3606666666666[0m
[36m[2023-07-10 20:53:51,828][227910] mean_value=-1543.7516612386391, max_value=1284.6989744246866[0m
[37m[1m[2023-07-10 20:53:51,831][227910] New mean coefficients: [[ 0.30063903 -0.40374246 -0.27437705 -0.22947645  0.371056  ]][0m
[37m[1m[2023-07-10 20:53:51,832][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:54:01,413][227910] train() took 9.58 seconds to complete[0m
[36m[2023-07-10 20:54:01,414][227910] FPS: 400827.05[0m
[36m[2023-07-10 20:54:01,416][227910] itr=1329, itrs=2000, Progress: 66.45%[0m
[36m[2023-07-10 20:54:12,974][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 20:54:12,975][227910] FPS: 332850.05[0m
[36m[2023-07-10 20:54:17,761][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:54:17,761][227910] Reward + Measures: [[6501.89560003    0.37369207    0.21234606    0.17392997    0.16406944]][0m
[37m[1m[2023-07-10 20:54:17,761][227910] Max Reward on eval: 6501.895600028978[0m
[37m[1m[2023-07-10 20:54:17,761][227910] Min Reward on eval: 6501.895600028978[0m
[37m[1m[2023-07-10 20:54:17,762][227910] Mean Reward across all agents: 6501.895600028978[0m
[37m[1m[2023-07-10 20:54:17,762][227910] Average Trajectory Length: 987.0746666666666[0m
[36m[2023-07-10 20:54:23,269][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:54:23,270][227910] Reward + Measures: [[4088.03831467    0.34940001    0.25320002    0.2339        0.17750001]
 [1361.62594859    0.49119997    0.23480001    0.53039998    0.22789998]
 [3116.68678672    0.33407179    0.1850739     0.24799402    0.15290886]
 ...
 [2853.57751924    0.28990003    0.3204        0.23629999    0.21300001]
 [5468.01825559    0.34370002    0.2854        0.23169999    0.18740001]
 [1759.64179906    0.14170001    0.3008        0.4066        0.2392    ]][0m
[37m[1m[2023-07-10 20:54:23,270][227910] Max Reward on eval: 6625.377962765842[0m
[37m[1m[2023-07-10 20:54:23,270][227910] Min Reward on eval: 406.1950658677612[0m
[37m[1m[2023-07-10 20:54:23,271][227910] Mean Reward across all agents: 3020.239939511363[0m
[37m[1m[2023-07-10 20:54:23,271][227910] Average Trajectory Length: 934.428[0m
[36m[2023-07-10 20:54:23,273][227910] mean_value=-1472.9634941442682, max_value=719.8282222226417[0m
[37m[1m[2023-07-10 20:54:23,275][227910] New mean coefficients: [[ 0.42066112 -0.28559282 -0.2705643  -0.4025663   0.29352713]][0m
[37m[1m[2023-07-10 20:54:23,276][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:54:33,126][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 20:54:33,126][227910] FPS: 389947.54[0m
[36m[2023-07-10 20:54:33,128][227910] itr=1330, itrs=2000, Progress: 66.50%[0m
[37m[1m[2023-07-10 20:54:37,129][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001310[0m
[36m[2023-07-10 20:54:49,142][227910] train() took 11.74 seconds to complete[0m
[36m[2023-07-10 20:54:49,143][227910] FPS: 327098.75[0m
[36m[2023-07-10 20:54:53,959][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:54:53,959][227910] Reward + Measures: [[6560.04039682    0.36579153    0.2095004     0.17105171    0.16338022]][0m
[37m[1m[2023-07-10 20:54:53,959][227910] Max Reward on eval: 6560.040396824571[0m
[37m[1m[2023-07-10 20:54:53,960][227910] Min Reward on eval: 6560.040396824571[0m
[37m[1m[2023-07-10 20:54:53,960][227910] Mean Reward across all agents: 6560.040396824571[0m
[37m[1m[2023-07-10 20:54:53,960][227910] Average Trajectory Length: 990.456[0m
[36m[2023-07-10 20:54:59,440][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:54:59,441][227910] Reward + Measures: [[3943.13627598    0.36760002    0.24010001    0.1876        0.15830001]
 [2324.19160334    0.23546186    0.28611794    0.20951045    0.16839412]
 [5934.66826876    0.34999999    0.2438        0.18130001    0.1697    ]
 ...
 [2660.72117384    0.48650002    0.35159999    0.3206        0.17180002]
 [1618.81744162    0.36195561    0.21005534    0.20780261    0.13846849]
 [2938.28625144    0.41352293    0.31335664    0.24602771    0.16078071]][0m
[37m[1m[2023-07-10 20:54:59,441][227910] Max Reward on eval: 6578.214490864915[0m
[37m[1m[2023-07-10 20:54:59,441][227910] Min Reward on eval: -213.0839282324072[0m
[37m[1m[2023-07-10 20:54:59,442][227910] Mean Reward across all agents: 2668.909178698444[0m
[37m[1m[2023-07-10 20:54:59,442][227910] Average Trajectory Length: 877.8643333333333[0m
[36m[2023-07-10 20:54:59,443][227910] mean_value=-2357.2222328733333, max_value=253.83859553915318[0m
[37m[1m[2023-07-10 20:54:59,446][227910] New mean coefficients: [[ 0.4898915  -0.26526242 -0.39225927 -0.27709973 -0.13501686]][0m
[37m[1m[2023-07-10 20:54:59,447][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:55:09,118][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 20:55:09,119][227910] FPS: 397105.91[0m
[36m[2023-07-10 20:55:09,121][227910] itr=1331, itrs=2000, Progress: 66.55%[0m
[36m[2023-07-10 20:55:20,622][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 20:55:20,622][227910] FPS: 334418.03[0m
[36m[2023-07-10 20:55:25,375][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:55:25,376][227910] Reward + Measures: [[6630.617227      0.36053824    0.21179903    0.17217541    0.16370441]][0m
[37m[1m[2023-07-10 20:55:25,376][227910] Max Reward on eval: 6630.61722699601[0m
[37m[1m[2023-07-10 20:55:25,376][227910] Min Reward on eval: 6630.61722699601[0m
[37m[1m[2023-07-10 20:55:25,376][227910] Mean Reward across all agents: 6630.61722699601[0m
[37m[1m[2023-07-10 20:55:25,377][227910] Average Trajectory Length: 990.9446666666666[0m
[36m[2023-07-10 20:55:30,916][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:55:30,917][227910] Reward + Measures: [[2461.25278592    0.21913171    0.25419709    0.21538006    0.16907509]
 [ 682.6072535     0.20773847    0.27761737    0.33683643    0.21259871]
 [1849.30026024    0.19245116    0.28224882    0.21650933    0.17163023]
 ...
 [4360.21250704    0.2845149     0.23060761    0.20159209    0.16151612]
 [1258.66452737    0.17853744    0.23747896    0.2983813     0.16868947]
 [5973.80352868    0.35852438    0.22709413    0.21072352    0.17776723]][0m
[37m[1m[2023-07-10 20:55:30,917][227910] Max Reward on eval: 6312.5510572500525[0m
[37m[1m[2023-07-10 20:55:30,917][227910] Min Reward on eval: -28.070794859534363[0m
[37m[1m[2023-07-10 20:55:30,917][227910] Mean Reward across all agents: 2669.6241195368916[0m
[37m[1m[2023-07-10 20:55:30,918][227910] Average Trajectory Length: 880.2036666666667[0m
[36m[2023-07-10 20:55:30,920][227910] mean_value=-1660.3449970406111, max_value=1798.9876629121386[0m
[37m[1m[2023-07-10 20:55:30,922][227910] New mean coefficients: [[ 0.25325418 -0.06384732 -0.16452727 -0.10081619  0.2721815 ]][0m
[37m[1m[2023-07-10 20:55:30,923][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:55:40,605][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 20:55:40,605][227910] FPS: 396694.33[0m
[36m[2023-07-10 20:55:40,607][227910] itr=1332, itrs=2000, Progress: 66.60%[0m
[36m[2023-07-10 20:55:52,136][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 20:55:52,136][227910] FPS: 333714.92[0m
[36m[2023-07-10 20:55:56,912][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:55:56,912][227910] Reward + Measures: [[6669.17770992    0.36710876    0.20499292    0.16593498    0.16015795]][0m
[37m[1m[2023-07-10 20:55:56,913][227910] Max Reward on eval: 6669.1777099236015[0m
[37m[1m[2023-07-10 20:55:56,913][227910] Min Reward on eval: 6669.1777099236015[0m
[37m[1m[2023-07-10 20:55:56,913][227910] Mean Reward across all agents: 6669.1777099236015[0m
[37m[1m[2023-07-10 20:55:56,913][227910] Average Trajectory Length: 986.918[0m
[36m[2023-07-10 20:56:02,387][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:56:02,388][227910] Reward + Measures: [[2843.98306568    0.27422565    0.281481      0.18613276    0.14683422]
 [1473.03350794    0.32620001    0.31230003    0.51430005    0.2313    ]
 [3377.34622889    0.33926186    0.2431583     0.24232244    0.16414216]
 ...
 [5103.99712134    0.35435411    0.26305094    0.19229518    0.16796884]
 [3266.5377508     0.29990003    0.32870004    0.3448        0.20410001]
 [2990.45399468    0.25710002    0.22330001    0.36690003    0.1954    ]][0m
[37m[1m[2023-07-10 20:56:02,388][227910] Max Reward on eval: 6729.9069355365355[0m
[37m[1m[2023-07-10 20:56:02,388][227910] Min Reward on eval: 449.10931291928284[0m
[37m[1m[2023-07-10 20:56:02,389][227910] Mean Reward across all agents: 3526.295687535292[0m
[37m[1m[2023-07-10 20:56:02,389][227910] Average Trajectory Length: 859.8226666666667[0m
[36m[2023-07-10 20:56:02,391][227910] mean_value=-1587.591779273707, max_value=1293.3747447978994[0m
[37m[1m[2023-07-10 20:56:02,393][227910] New mean coefficients: [[ 0.4357065  -0.03745383 -0.36553562 -0.11197572  0.09221619]][0m
[37m[1m[2023-07-10 20:56:02,394][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:56:12,112][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 20:56:12,112][227910] FPS: 395213.94[0m
[36m[2023-07-10 20:56:12,115][227910] itr=1333, itrs=2000, Progress: 66.65%[0m
[36m[2023-07-10 20:56:23,756][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 20:56:23,756][227910] FPS: 330415.83[0m
[36m[2023-07-10 20:56:28,528][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:56:28,528][227910] Reward + Measures: [[6731.16260213    0.36259487    0.20056298    0.16912815    0.16065064]][0m
[37m[1m[2023-07-10 20:56:28,528][227910] Max Reward on eval: 6731.16260213443[0m
[37m[1m[2023-07-10 20:56:28,529][227910] Min Reward on eval: 6731.16260213443[0m
[37m[1m[2023-07-10 20:56:28,529][227910] Mean Reward across all agents: 6731.16260213443[0m
[37m[1m[2023-07-10 20:56:28,529][227910] Average Trajectory Length: 989.7446666666666[0m
[36m[2023-07-10 20:56:34,213][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:56:34,214][227910] Reward + Measures: [[3251.67173082    0.27568007    0.34055439    0.23181152    0.18436538]
 [2044.15832232    0.18700001    0.38589999    0.29850003    0.18359999]
 [2748.45250334    0.266         0.3479        0.31259999    0.18059999]
 ...
 [4867.35247557    0.30700001    0.35109997    0.23080002    0.1816    ]
 [2763.50939378    0.25903326    0.24968953    0.22440682    0.17941041]
 [4318.71406286    0.31220433    0.26670107    0.20194006    0.17422061]][0m
[37m[1m[2023-07-10 20:56:34,214][227910] Max Reward on eval: 6774.8737963383555[0m
[37m[1m[2023-07-10 20:56:34,214][227910] Min Reward on eval: 437.660405466854[0m
[37m[1m[2023-07-10 20:56:34,215][227910] Mean Reward across all agents: 3178.448534347536[0m
[37m[1m[2023-07-10 20:56:34,215][227910] Average Trajectory Length: 874.374[0m
[36m[2023-07-10 20:56:34,216][227910] mean_value=-1729.2218778259812, max_value=859.7336179859585[0m
[37m[1m[2023-07-10 20:56:34,219][227910] New mean coefficients: [[ 0.31931618 -0.02219942 -0.46515572  0.0516003   0.16767272]][0m
[37m[1m[2023-07-10 20:56:34,220][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:56:43,920][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 20:56:43,920][227910] FPS: 395952.50[0m
[36m[2023-07-10 20:56:43,922][227910] itr=1334, itrs=2000, Progress: 66.70%[0m
[36m[2023-07-10 20:56:55,480][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 20:56:55,481][227910] FPS: 332856.52[0m
[36m[2023-07-10 20:57:00,333][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:57:00,334][227910] Reward + Measures: [[6833.02458717    0.36831501    0.19625917    0.16674738    0.16032092]][0m
[37m[1m[2023-07-10 20:57:00,334][227910] Max Reward on eval: 6833.024587173308[0m
[37m[1m[2023-07-10 20:57:00,334][227910] Min Reward on eval: 6833.024587173308[0m
[37m[1m[2023-07-10 20:57:00,335][227910] Mean Reward across all agents: 6833.024587173308[0m
[37m[1m[2023-07-10 20:57:00,335][227910] Average Trajectory Length: 991.5686666666667[0m
[36m[2023-07-10 20:57:05,778][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:57:05,778][227910] Reward + Measures: [[2677.77661615    0.35905123    0.18828867    0.17207941    0.15886866]
 [5674.70313471    0.38398474    0.18442558    0.18181096    0.16151576]
 [1168.61556458    0.27277598    0.28674743    0.1785183     0.16018969]
 ...
 [5950.41101233    0.33770001    0.2405        0.20320001    0.17459999]
 [4161.27930441    0.30330831    0.25261459    0.2159359     0.17761946]
 [1779.03579082    0.40045452    0.26432326    0.24693494    0.15005225]][0m
[37m[1m[2023-07-10 20:57:05,779][227910] Max Reward on eval: 6862.013663658314[0m
[37m[1m[2023-07-10 20:57:05,779][227910] Min Reward on eval: 105.34217319451272[0m
[37m[1m[2023-07-10 20:57:05,779][227910] Mean Reward across all agents: 2997.1574768815126[0m
[37m[1m[2023-07-10 20:57:05,779][227910] Average Trajectory Length: 696.0696666666666[0m
[36m[2023-07-10 20:57:05,781][227910] mean_value=-2388.902027798481, max_value=2520.405731668176[0m
[37m[1m[2023-07-10 20:57:05,783][227910] New mean coefficients: [[ 0.16076657  0.05101795 -0.05723035  0.18563625  0.2581284 ]][0m
[37m[1m[2023-07-10 20:57:05,784][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:57:15,392][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 20:57:15,392][227910] FPS: 399753.17[0m
[36m[2023-07-10 20:57:15,394][227910] itr=1335, itrs=2000, Progress: 66.75%[0m
[36m[2023-07-10 20:57:26,974][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 20:57:26,974][227910] FPS: 332144.72[0m
[36m[2023-07-10 20:57:31,737][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:57:31,738][227910] Reward + Measures: [[4692.59547898    0.30887339    0.26393086    0.19238482    0.16628779]][0m
[37m[1m[2023-07-10 20:57:31,738][227910] Max Reward on eval: 4692.59547897591[0m
[37m[1m[2023-07-10 20:57:31,738][227910] Min Reward on eval: 4692.59547897591[0m
[37m[1m[2023-07-10 20:57:31,739][227910] Mean Reward across all agents: 4692.59547897591[0m
[37m[1m[2023-07-10 20:57:31,739][227910] Average Trajectory Length: 955.2153333333333[0m
[36m[2023-07-10 20:57:37,220][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:57:37,221][227910] Reward + Measures: [[3906.67632735    0.33241075    0.23455968    0.19378991    0.16598943]
 [2020.95017374    0.29890001    0.34810001    0.33129999    0.22690001]
 [2615.14618577    0.27855295    0.23039578    0.2454046     0.16731809]
 ...
 [3958.11987303    0.29245138    0.27021089    0.18542364    0.16633801]
 [3045.18330858    0.3729752     0.24684083    0.21422689    0.17440484]
 [1548.52605806    0.34559998    0.36629999    0.30990002    0.24000001]][0m
[37m[1m[2023-07-10 20:57:37,221][227910] Max Reward on eval: 4986.816528822482[0m
[37m[1m[2023-07-10 20:57:37,221][227910] Min Reward on eval: 624.5952373553562[0m
[37m[1m[2023-07-10 20:57:37,222][227910] Mean Reward across all agents: 3261.6624469349354[0m
[37m[1m[2023-07-10 20:57:37,222][227910] Average Trajectory Length: 931.1553333333333[0m
[36m[2023-07-10 20:57:37,223][227910] mean_value=-2024.8213200241466, max_value=-6.734731525495135[0m
[36m[2023-07-10 20:57:37,225][227910] XNES is restarting with a new solution whose measures are [0.20390001 0.84960002 0.35499999 0.52810001] and objective is 662.9634826066671[0m
[36m[2023-07-10 20:57:37,226][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 20:57:37,229][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 20:57:37,229][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:57:47,111][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 20:57:47,111][227910] FPS: 388672.64[0m
[36m[2023-07-10 20:57:47,113][227910] itr=1336, itrs=2000, Progress: 66.80%[0m
[36m[2023-07-10 20:57:58,634][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 20:57:58,634][227910] FPS: 333997.51[0m
[36m[2023-07-10 20:58:03,511][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:58:03,512][227910] Reward + Measures: [[712.97713478   0.25636968   0.73579234   0.27934498   0.52122402]][0m
[37m[1m[2023-07-10 20:58:03,512][227910] Max Reward on eval: 712.9771347838852[0m
[37m[1m[2023-07-10 20:58:03,512][227910] Min Reward on eval: 712.9771347838852[0m
[37m[1m[2023-07-10 20:58:03,512][227910] Mean Reward across all agents: 712.9771347838852[0m
[37m[1m[2023-07-10 20:58:03,513][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 20:58:09,026][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:58:09,027][227910] Reward + Measures: [[ -341.4711596      0.26030001     0.56080002     0.26659998
      0.49439999]
 [-1954.63192608     0.75880003     0.8351         0.1858
      0.87269992]
 [-1155.48038945     0.59709996     0.50260001     0.6196
      0.43670002]
 ...
 [  404.03612381     0.41679999     0.68450004     0.1778
      0.50690001]
 [-1727.88999576     0.42930004     0.70340002     0.30410001
      0.77100003]
 [-1106.86396282     0.44510004     0.63239998     0.64899999
      0.52230006]][0m
[37m[1m[2023-07-10 20:58:09,027][227910] Max Reward on eval: 773.946281687892[0m
[37m[1m[2023-07-10 20:58:09,027][227910] Min Reward on eval: -2128.461516977381[0m
[37m[1m[2023-07-10 20:58:09,028][227910] Mean Reward across all agents: -709.6831924642009[0m
[37m[1m[2023-07-10 20:58:09,028][227910] Average Trajectory Length: 993.54[0m
[36m[2023-07-10 20:58:09,030][227910] mean_value=-1210.4109128996147, max_value=609.12909962045[0m
[37m[1m[2023-07-10 20:58:09,032][227910] New mean coefficients: [[ 0.70749813  0.16728407 -0.7072936   0.23742235 -1.6015458 ]][0m
[37m[1m[2023-07-10 20:58:09,033][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:58:18,711][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 20:58:18,712][227910] FPS: 396828.76[0m
[36m[2023-07-10 20:58:18,714][227910] itr=1337, itrs=2000, Progress: 66.85%[0m
[36m[2023-07-10 20:58:30,234][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 20:58:30,235][227910] FPS: 333842.38[0m
[36m[2023-07-10 20:58:34,961][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:58:34,962][227910] Reward + Measures: [[855.35343627   0.19839099   0.62200034   0.35895267   0.45139766]][0m
[37m[1m[2023-07-10 20:58:34,962][227910] Max Reward on eval: 855.3534362696661[0m
[37m[1m[2023-07-10 20:58:34,962][227910] Min Reward on eval: 855.3534362696661[0m
[37m[1m[2023-07-10 20:58:34,962][227910] Mean Reward across all agents: 855.3534362696661[0m
[37m[1m[2023-07-10 20:58:34,962][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 20:58:40,508][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:58:40,509][227910] Reward + Measures: [[ -480.58000429     0.25920686     0.372372       0.31273991
      0.28819051]
 [-1690.90175752     0.88660002     0.84650004     0.90529996
      0.84320003]
 [ -665.15792868     0.32890001     0.35770002     0.2877
      0.44790003]
 ...
 [-1079.65572173     0.47980005     0.25570002     0.50819999
      0.5018    ]
 [-1613.44912768     0.76167989     0.7955305      0.81012863
      0.78505796]
 [  291.28627915     0.33419999     0.45290002     0.1832
      0.3202    ]][0m
[37m[1m[2023-07-10 20:58:40,509][227910] Max Reward on eval: 1046.1719242352294[0m
[37m[1m[2023-07-10 20:58:40,509][227910] Min Reward on eval: -2049.647634372278[0m
[37m[1m[2023-07-10 20:58:40,509][227910] Mean Reward across all agents: -701.2513161462322[0m
[37m[1m[2023-07-10 20:58:40,509][227910] Average Trajectory Length: 950.1293333333333[0m
[36m[2023-07-10 20:58:40,511][227910] mean_value=-1871.5657449407593, max_value=334.2606191151299[0m
[37m[1m[2023-07-10 20:58:40,514][227910] New mean coefficients: [[ 0.79377115  0.29812783 -1.1179892   0.2437546  -0.29444814]][0m
[37m[1m[2023-07-10 20:58:40,514][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:58:50,198][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 20:58:50,198][227910] FPS: 396623.09[0m
[36m[2023-07-10 20:58:50,200][227910] itr=1338, itrs=2000, Progress: 66.90%[0m
[36m[2023-07-10 20:59:01,731][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 20:59:01,731][227910] FPS: 333554.55[0m
[36m[2023-07-10 20:59:06,470][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:59:06,470][227910] Reward + Measures: [[913.64823389   0.24897699   0.58773798   0.35476699   0.39824364]][0m
[37m[1m[2023-07-10 20:59:06,471][227910] Max Reward on eval: 913.6482338926851[0m
[37m[1m[2023-07-10 20:59:06,471][227910] Min Reward on eval: 913.6482338926851[0m
[37m[1m[2023-07-10 20:59:06,471][227910] Mean Reward across all agents: 913.6482338926851[0m
[37m[1m[2023-07-10 20:59:06,471][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 20:59:12,097][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:59:12,098][227910] Reward + Measures: [[  -53.33386639     0.14136784     0.34354463     0.20722881
      0.2606355 ]
 [ -913.52388936     0.23940001     0.2429         0.25749999
      0.31640002]
 [-1394.19071152     0.35759997     0.80450004     0.2156
      0.82709998]
 ...
 [   77.86562834     0.5808         0.74399996     0.49790001
      0.62190002]
 [  -40.14140711     0.33000001     0.62099999     0.27200001
      0.50649995]
 [  -63.82666659     0.51730001     0.83820003     0.1411
      0.80410004]][0m
[37m[1m[2023-07-10 20:59:12,098][227910] Max Reward on eval: 911.3733176113514[0m
[37m[1m[2023-07-10 20:59:12,098][227910] Min Reward on eval: -1952.078317462816[0m
[37m[1m[2023-07-10 20:59:12,099][227910] Mean Reward across all agents: -199.72368525774863[0m
[37m[1m[2023-07-10 20:59:12,099][227910] Average Trajectory Length: 983.247[0m
[36m[2023-07-10 20:59:12,100][227910] mean_value=-1196.9424881100729, max_value=585.0223583061289[0m
[37m[1m[2023-07-10 20:59:12,103][227910] New mean coefficients: [[ 1.3428665  -0.4937821  -0.29486006  0.36376935 -0.93605053]][0m
[37m[1m[2023-07-10 20:59:12,104][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:59:21,933][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 20:59:21,933][227910] FPS: 390723.95[0m
[36m[2023-07-10 20:59:21,936][227910] itr=1339, itrs=2000, Progress: 66.95%[0m
[36m[2023-07-10 20:59:33,564][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 20:59:33,564][227910] FPS: 330748.44[0m
[36m[2023-07-10 20:59:38,287][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:59:38,287][227910] Reward + Measures: [[970.73317031   0.24134833   0.58029932   0.3454833    0.37984064]][0m
[37m[1m[2023-07-10 20:59:38,287][227910] Max Reward on eval: 970.7331703083636[0m
[37m[1m[2023-07-10 20:59:38,287][227910] Min Reward on eval: 970.7331703083636[0m
[37m[1m[2023-07-10 20:59:38,287][227910] Mean Reward across all agents: 970.7331703083636[0m
[37m[1m[2023-07-10 20:59:38,288][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 20:59:43,836][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 20:59:43,837][227910] Reward + Measures: [[ -782.69445599     0.2608         0.46669999     0.29350001
      0.37729999]
 [ -108.45687038     0.31619999     0.57160002     0.31909999
      0.42589998]
 [ -762.61342972     0.18691833     0.29910982     0.16774897
      0.34520507]
 ...
 [-1528.07858979     0.30599472     0.61754209     0.76673692
      0.72766316]
 [  727.00989426     0.15510002     0.51820004     0.40289998
      0.45070001]
 [ -690.78514126     0.2141         0.56750005     0.35800001
      0.46950004]][0m
[37m[1m[2023-07-10 20:59:43,837][227910] Max Reward on eval: 913.2625876931822[0m
[37m[1m[2023-07-10 20:59:43,837][227910] Min Reward on eval: -1853.570950073586[0m
[37m[1m[2023-07-10 20:59:43,838][227910] Mean Reward across all agents: -387.322043063222[0m
[37m[1m[2023-07-10 20:59:43,838][227910] Average Trajectory Length: 980.4366666666666[0m
[36m[2023-07-10 20:59:43,840][227910] mean_value=-1413.0572442407088, max_value=477.00654120780337[0m
[37m[1m[2023-07-10 20:59:43,842][227910] New mean coefficients: [[ 1.549713   -0.37722903  0.10748452 -0.06845602 -0.4756262 ]][0m
[37m[1m[2023-07-10 20:59:43,843][227910] Moving the mean solution point...[0m
[36m[2023-07-10 20:59:53,681][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 20:59:53,681][227910] FPS: 390390.14[0m
[36m[2023-07-10 20:59:53,684][227910] itr=1340, itrs=2000, Progress: 67.00%[0m
[37m[1m[2023-07-10 20:59:57,518][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001320[0m
[36m[2023-07-10 21:00:09,314][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 21:00:09,314][227910] FPS: 333181.66[0m
[36m[2023-07-10 21:00:14,063][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:00:14,064][227910] Reward + Measures: [[1036.47363428    0.213085      0.58759165    0.34125733    0.374295  ]][0m
[37m[1m[2023-07-10 21:00:14,064][227910] Max Reward on eval: 1036.4736342767396[0m
[37m[1m[2023-07-10 21:00:14,064][227910] Min Reward on eval: 1036.4736342767396[0m
[37m[1m[2023-07-10 21:00:14,064][227910] Mean Reward across all agents: 1036.4736342767396[0m
[37m[1m[2023-07-10 21:00:14,065][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:00:19,646][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:00:19,646][227910] Reward + Measures: [[ -318.99540208     0.85020012     0.903          0.04940001
      0.86989993]
 [ -265.3778068      0.32431385     0.48949751     0.348353
      0.39490449]
 [ -618.99455214     0.63300002     0.54800004     0.61510003
      0.2362    ]
 ...
 [  891.01015379     0.39180002     0.63849998     0.3651
      0.40980002]
 [-1013.97361817     0.48400003     0.2124         0.59330004
      0.46860003]
 [-1205.27407808     0.53133625     0.38323188     0.48943624
      0.31761158]][0m
[37m[1m[2023-07-10 21:00:19,646][227910] Max Reward on eval: 900.0842745584872[0m
[37m[1m[2023-07-10 21:00:19,647][227910] Min Reward on eval: -1595.558186301682[0m
[37m[1m[2023-07-10 21:00:19,647][227910] Mean Reward across all agents: -405.6786058344359[0m
[37m[1m[2023-07-10 21:00:19,647][227910] Average Trajectory Length: 979.608[0m
[36m[2023-07-10 21:00:19,649][227910] mean_value=-1599.468934649274, max_value=902.3069738589771[0m
[37m[1m[2023-07-10 21:00:19,652][227910] New mean coefficients: [[ 1.8169069  -0.7561392   0.00141115  0.9036614  -0.8603003 ]][0m
[37m[1m[2023-07-10 21:00:19,653][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:00:29,462][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 21:00:29,462][227910] FPS: 391530.94[0m
[36m[2023-07-10 21:00:29,465][227910] itr=1341, itrs=2000, Progress: 67.05%[0m
[36m[2023-07-10 21:00:40,969][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 21:00:40,969][227910] FPS: 334450.59[0m
[36m[2023-07-10 21:00:45,783][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:00:45,784][227910] Reward + Measures: [[1087.72959194    0.192212      0.59305102    0.33922035    0.3767873 ]][0m
[37m[1m[2023-07-10 21:00:45,784][227910] Max Reward on eval: 1087.7295919380078[0m
[37m[1m[2023-07-10 21:00:45,784][227910] Min Reward on eval: 1087.7295919380078[0m
[37m[1m[2023-07-10 21:00:45,784][227910] Mean Reward across all agents: 1087.7295919380078[0m
[37m[1m[2023-07-10 21:00:45,785][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:00:51,197][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:00:51,198][227910] Reward + Measures: [[  50.5600938     0.16346933    0.36208138    0.20477276    0.23097177]
 [1034.21427162    0.22070001    0.55099994    0.2572        0.3876    ]
 [-663.54660852    0.16474631    0.2535691     0.29618841    0.18706049]
 ...
 [-450.70922095    0.435         0.60109997    0.60220003    0.47150001]
 [-853.22890198    0.15792875    0.20727395    0.15473373    0.17557012]
 [-736.03314355    0.41149998    0.52520001    0.47790003    0.34460005]][0m
[37m[1m[2023-07-10 21:00:51,198][227910] Max Reward on eval: 1185.627399205236[0m
[37m[1m[2023-07-10 21:00:51,199][227910] Min Reward on eval: -1670.2269432802218[0m
[37m[1m[2023-07-10 21:00:51,199][227910] Mean Reward across all agents: -112.75735822688738[0m
[37m[1m[2023-07-10 21:00:51,199][227910] Average Trajectory Length: 977.831[0m
[36m[2023-07-10 21:00:51,201][227910] mean_value=-1828.2886961976124, max_value=791.3714859451004[0m
[37m[1m[2023-07-10 21:00:51,204][227910] New mean coefficients: [[1.8762634  0.02913272 0.25024778 0.39539427 0.50279546]][0m
[37m[1m[2023-07-10 21:00:51,204][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:01:00,862][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 21:01:00,862][227910] FPS: 397705.95[0m
[36m[2023-07-10 21:01:00,864][227910] itr=1342, itrs=2000, Progress: 67.10%[0m
[36m[2023-07-10 21:01:12,388][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 21:01:12,389][227910] FPS: 333765.34[0m
[36m[2023-07-10 21:01:17,068][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:01:17,068][227910] Reward + Measures: [[1140.42481949    0.18255065    0.60049903    0.33290833    0.36597899]][0m
[37m[1m[2023-07-10 21:01:17,068][227910] Max Reward on eval: 1140.4248194914346[0m
[37m[1m[2023-07-10 21:01:17,069][227910] Min Reward on eval: 1140.4248194914346[0m
[37m[1m[2023-07-10 21:01:17,069][227910] Mean Reward across all agents: 1140.4248194914346[0m
[37m[1m[2023-07-10 21:01:17,069][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:01:22,384][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:01:22,385][227910] Reward + Measures: [[-396.93102715    0.64600003    0.78360003    0.74180001    0.74629992]
 [-225.6732297     0.39570001    0.67870009    0.42070004    0.63220006]
 [ -80.92002582    0.3475        0.47060004    0.40789995    0.4355    ]
 ...
 [  70.04873873    0.2692        0.66830009    0.45880005    0.55350006]
 [-593.54401532    0.31130001    0.48149997    0.32540002    0.51850003]
 [-658.94625426    0.20502026    0.21235143    0.30970237    0.19246139]][0m
[37m[1m[2023-07-10 21:01:22,385][227910] Max Reward on eval: 1123.3625350040616[0m
[37m[1m[2023-07-10 21:01:22,385][227910] Min Reward on eval: -1594.4177119749133[0m
[37m[1m[2023-07-10 21:01:22,386][227910] Mean Reward across all agents: -105.78736637498805[0m
[37m[1m[2023-07-10 21:01:22,386][227910] Average Trajectory Length: 968.1683333333333[0m
[36m[2023-07-10 21:01:22,389][227910] mean_value=-1287.5226347423204, max_value=1187.0558265149466[0m
[37m[1m[2023-07-10 21:01:22,391][227910] New mean coefficients: [[1.6116251  0.16341954 0.64873856 0.50513446 0.6059167 ]][0m
[37m[1m[2023-07-10 21:01:22,392][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:01:32,090][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 21:01:32,090][227910] FPS: 396055.68[0m
[36m[2023-07-10 21:01:32,092][227910] itr=1343, itrs=2000, Progress: 67.15%[0m
[36m[2023-07-10 21:01:43,578][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 21:01:43,578][227910] FPS: 334850.53[0m
[36m[2023-07-10 21:01:48,275][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:01:48,275][227910] Reward + Measures: [[1205.17342238    0.17165181    0.62179512    0.33315983    0.36543205]][0m
[37m[1m[2023-07-10 21:01:48,275][227910] Max Reward on eval: 1205.1734223766186[0m
[37m[1m[2023-07-10 21:01:48,276][227910] Min Reward on eval: 1205.1734223766186[0m
[37m[1m[2023-07-10 21:01:48,276][227910] Mean Reward across all agents: 1205.1734223766186[0m
[37m[1m[2023-07-10 21:01:48,276][227910] Average Trajectory Length: 999.704[0m
[36m[2023-07-10 21:01:53,661][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:01:53,661][227910] Reward + Measures: [[ 549.28624402    0.77240002    0.33419999    0.82359999    0.19989999]
 [ 424.44909301    0.53189999    0.5977        0.65070003    0.23550001]
 [  17.53848049    0.53299999    0.33199999    0.67449999    0.16950001]
 ...
 [ -70.14908499    0.95040005    0.0457        0.92390007    0.73699999]
 [-837.67817077    0.1481647     0.33595094    0.23341227    0.30693522]
 [ 768.25311121    0.35430002    0.5546        0.41640002    0.37139997]][0m
[37m[1m[2023-07-10 21:01:53,661][227910] Max Reward on eval: 1234.8116804913268[0m
[37m[1m[2023-07-10 21:01:53,662][227910] Min Reward on eval: -1392.4644160057767[0m
[37m[1m[2023-07-10 21:01:53,662][227910] Mean Reward across all agents: 55.79646080793392[0m
[37m[1m[2023-07-10 21:01:53,662][227910] Average Trajectory Length: 974.4069999999999[0m
[36m[2023-07-10 21:01:53,665][227910] mean_value=-915.3493509560682, max_value=761.991458193962[0m
[37m[1m[2023-07-10 21:01:53,668][227910] New mean coefficients: [[ 1.9657122  -0.69801533  0.8210819   1.0951858   0.5768095 ]][0m
[37m[1m[2023-07-10 21:01:53,669][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:02:03,444][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 21:02:03,444][227910] FPS: 392912.37[0m
[36m[2023-07-10 21:02:03,446][227910] itr=1344, itrs=2000, Progress: 67.20%[0m
[36m[2023-07-10 21:02:14,960][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 21:02:14,960][227910] FPS: 334091.22[0m
[36m[2023-07-10 21:02:19,783][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:02:19,783][227910] Reward + Measures: [[1241.1047723     0.15580113    0.63478547    0.334757      0.37264127]][0m
[37m[1m[2023-07-10 21:02:19,783][227910] Max Reward on eval: 1241.1047723040867[0m
[37m[1m[2023-07-10 21:02:19,783][227910] Min Reward on eval: 1241.1047723040867[0m
[37m[1m[2023-07-10 21:02:19,784][227910] Mean Reward across all agents: 1241.1047723040867[0m
[37m[1m[2023-07-10 21:02:19,784][227910] Average Trajectory Length: 999.4736666666666[0m
[36m[2023-07-10 21:02:25,202][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:02:25,202][227910] Reward + Measures: [[-472.80395992    0.38          0.73979998    0.1925        0.7148    ]
 [1170.6172158     0.1789        0.50950003    0.31959999    0.33540002]
 [ 291.61830539    0.16704236    0.52974004    0.38850039    0.46156093]
 ...
 [-146.79998042    0.92970002    0.31060001    0.92500001    0.0811    ]
 [-148.78805079    0.20520453    0.38301155    0.31308636    0.31504771]
 [-221.4388464     0.3284803     0.44346055    0.45494363    0.32003883]][0m
[37m[1m[2023-07-10 21:02:25,203][227910] Max Reward on eval: 1245.4377604874317[0m
[37m[1m[2023-07-10 21:02:25,203][227910] Min Reward on eval: -1599.5508831941522[0m
[37m[1m[2023-07-10 21:02:25,203][227910] Mean Reward across all agents: -119.08015651043567[0m
[37m[1m[2023-07-10 21:02:25,203][227910] Average Trajectory Length: 977.6243333333333[0m
[36m[2023-07-10 21:02:25,206][227910] mean_value=-1098.551608015154, max_value=1193.6554821321463[0m
[37m[1m[2023-07-10 21:02:25,208][227910] New mean coefficients: [[ 1.9326649   0.02684885  1.0434582   0.27698416 -0.01767427]][0m
[37m[1m[2023-07-10 21:02:25,209][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:02:34,936][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 21:02:34,937][227910] FPS: 394829.10[0m
[36m[2023-07-10 21:02:34,939][227910] itr=1345, itrs=2000, Progress: 67.25%[0m
[36m[2023-07-10 21:02:46,408][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 21:02:46,408][227910] FPS: 335451.29[0m
[36m[2023-07-10 21:02:51,175][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:02:51,175][227910] Reward + Measures: [[1293.23773083    0.15083766    0.64854401    0.32937101    0.37183765]][0m
[37m[1m[2023-07-10 21:02:51,176][227910] Max Reward on eval: 1293.2377308268976[0m
[37m[1m[2023-07-10 21:02:51,176][227910] Min Reward on eval: 1293.2377308268976[0m
[37m[1m[2023-07-10 21:02:51,176][227910] Mean Reward across all agents: 1293.2377308268976[0m
[37m[1m[2023-07-10 21:02:51,176][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:02:56,792][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:02:56,793][227910] Reward + Measures: [[  -18.20418911     0.37289998     0.50590003     0.2357
      0.49190003]
 [ -311.29784069     0.40560004     0.54770005     0.38389999
      0.53520006]
 [ -669.69735475     0.52455896     0.42542559     0.41345745
      0.46148434]
 ...
 [    9.74258123     0.37086096     0.50369662     0.38739839
      0.49043265]
 [  355.79878625     0.30710003     0.47170001     0.33530003
      0.2667    ]
 [-1140.93207239     0.27070001     0.47259998     0.3184
      0.38190001]][0m
[37m[1m[2023-07-10 21:02:56,793][227910] Max Reward on eval: 1307.9990334014874[0m
[37m[1m[2023-07-10 21:02:56,794][227910] Min Reward on eval: -1151.8572738589312[0m
[37m[1m[2023-07-10 21:02:56,794][227910] Mean Reward across all agents: 155.55439541725636[0m
[37m[1m[2023-07-10 21:02:56,794][227910] Average Trajectory Length: 991.884[0m
[36m[2023-07-10 21:02:56,797][227910] mean_value=-955.6074708207079, max_value=1109.7612225640335[0m
[37m[1m[2023-07-10 21:02:56,799][227910] New mean coefficients: [[ 2.0096438   0.19193089  1.4566149   0.8320661  -0.42725185]][0m
[37m[1m[2023-07-10 21:02:56,800][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:03:06,484][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 21:03:06,485][227910] FPS: 396593.89[0m
[36m[2023-07-10 21:03:06,487][227910] itr=1346, itrs=2000, Progress: 67.30%[0m
[36m[2023-07-10 21:03:17,968][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 21:03:17,968][227910] FPS: 335003.74[0m
[36m[2023-07-10 21:03:22,684][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:03:22,685][227910] Reward + Measures: [[1355.89939263    0.14273967    0.65969634    0.32303867    0.36714271]][0m
[37m[1m[2023-07-10 21:03:22,685][227910] Max Reward on eval: 1355.8993926280366[0m
[37m[1m[2023-07-10 21:03:22,685][227910] Min Reward on eval: 1355.8993926280366[0m
[37m[1m[2023-07-10 21:03:22,685][227910] Mean Reward across all agents: 1355.8993926280366[0m
[37m[1m[2023-07-10 21:03:22,686][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:03:28,080][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:03:28,080][227910] Reward + Measures: [[ 138.91860922    0.64300001    0.76920003    0.69749999    0.72390002]
 [-472.8286842     0.68183148    0.67200005    0.66948777    0.5804525 ]
 [ -36.753454      0.69319999    0.69329995    0.66190004    0.63879997]
 ...
 [  61.8260019     0.296         0.45539999    0.31459999    0.37170002]
 [-109.73108484    0.61409998    0.58829993    0.55940002    0.60229999]
 [-894.68694399    0.59919995    0.68059999    0.46919996    0.4258    ]][0m
[37m[1m[2023-07-10 21:03:28,081][227910] Max Reward on eval: 1303.698875204788[0m
[37m[1m[2023-07-10 21:03:28,081][227910] Min Reward on eval: -2289.5823760745116[0m
[37m[1m[2023-07-10 21:03:28,081][227910] Mean Reward across all agents: -20.841158202848845[0m
[37m[1m[2023-07-10 21:03:28,081][227910] Average Trajectory Length: 977.776[0m
[36m[2023-07-10 21:03:28,084][227910] mean_value=-1166.55463072082, max_value=602.9776625506073[0m
[37m[1m[2023-07-10 21:03:28,086][227910] New mean coefficients: [[ 2.2622967   0.0901385   0.84192735  1.0170308  -0.51994824]][0m
[37m[1m[2023-07-10 21:03:28,087][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:03:37,830][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 21:03:37,830][227910] FPS: 394220.59[0m
[36m[2023-07-10 21:03:37,832][227910] itr=1347, itrs=2000, Progress: 67.35%[0m
[36m[2023-07-10 21:03:49,309][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 21:03:49,309][227910] FPS: 335117.44[0m
[36m[2023-07-10 21:03:54,083][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:03:54,083][227910] Reward + Measures: [[1423.70770636    0.14079033    0.66622198    0.32741666    0.35573098]][0m
[37m[1m[2023-07-10 21:03:54,083][227910] Max Reward on eval: 1423.7077063621807[0m
[37m[1m[2023-07-10 21:03:54,084][227910] Min Reward on eval: 1423.7077063621807[0m
[37m[1m[2023-07-10 21:03:54,084][227910] Mean Reward across all agents: 1423.7077063621807[0m
[37m[1m[2023-07-10 21:03:54,084][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:03:59,597][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:03:59,597][227910] Reward + Measures: [[  555.14814537     0.29230002     0.60140002     0.34349999
      0.41120005]
 [-1280.21700398     0.97010005     0.98100007     0.88959998
      0.96239996]
 [ -265.66754662     0.30109999     0.62770003     0.37549999
      0.5298    ]
 ...
 [ -199.45447644     0.74059999     0.2237         0.7899
      0.62910002]
 [   70.26738925     0.40869999     0.70940006     0.41939998
      0.57950002]
 [  986.39235239     0.22980002     0.59170002     0.32290003
      0.32619998]][0m
[37m[1m[2023-07-10 21:03:59,598][227910] Max Reward on eval: 1377.4210699853488[0m
[37m[1m[2023-07-10 21:03:59,598][227910] Min Reward on eval: -2006.7841914734104[0m
[37m[1m[2023-07-10 21:03:59,598][227910] Mean Reward across all agents: 159.24822968567324[0m
[37m[1m[2023-07-10 21:03:59,598][227910] Average Trajectory Length: 991.2303333333333[0m
[36m[2023-07-10 21:03:59,602][227910] mean_value=-916.2065076546747, max_value=1620.5541114610503[0m
[37m[1m[2023-07-10 21:03:59,605][227910] New mean coefficients: [[2.4205904  0.18904752 0.71144503 0.6996548  0.7807381 ]][0m
[37m[1m[2023-07-10 21:03:59,606][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:04:09,245][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 21:04:09,245][227910] FPS: 398446.88[0m
[36m[2023-07-10 21:04:09,248][227910] itr=1348, itrs=2000, Progress: 67.40%[0m
[36m[2023-07-10 21:04:20,864][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 21:04:20,864][227910] FPS: 331085.07[0m
[36m[2023-07-10 21:04:25,696][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:04:25,696][227910] Reward + Measures: [[1488.47928284    0.13175634    0.67477399    0.33002368    0.35652766]][0m
[37m[1m[2023-07-10 21:04:25,696][227910] Max Reward on eval: 1488.4792828404693[0m
[37m[1m[2023-07-10 21:04:25,697][227910] Min Reward on eval: 1488.4792828404693[0m
[37m[1m[2023-07-10 21:04:25,697][227910] Mean Reward across all agents: 1488.4792828404693[0m
[37m[1m[2023-07-10 21:04:25,697][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:04:31,120][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:04:31,120][227910] Reward + Measures: [[ 1303.88978259     0.16470002     0.52740002     0.38990003
      0.33400002]
 [   86.31843754     0.32810003     0.50940001     0.21490002
      0.48359999]
 [  588.63259782     0.0963         0.72189999     0.51390004
      0.60649997]
 ...
 [-1088.35234249     0.42570001     0.68880004     0.13880001
      0.65859997]
 [  110.62192768     0.23120001     0.7353         0.52559996
      0.69880003]
 [-1130.59959893     0.31812903     0.71405268     0.10536989
      0.6869688 ]][0m
[37m[1m[2023-07-10 21:04:31,120][227910] Max Reward on eval: 1598.1144871653291[0m
[37m[1m[2023-07-10 21:04:31,121][227910] Min Reward on eval: -2270.6154703020466[0m
[37m[1m[2023-07-10 21:04:31,121][227910] Mean Reward across all agents: 74.75670880089832[0m
[37m[1m[2023-07-10 21:04:31,121][227910] Average Trajectory Length: 994.8146666666667[0m
[36m[2023-07-10 21:04:31,125][227910] mean_value=-709.2765229744639, max_value=1263.3349141970166[0m
[37m[1m[2023-07-10 21:04:31,128][227910] New mean coefficients: [[2.3335588  0.29731548 0.8798465  1.0934523  0.23046696]][0m
[37m[1m[2023-07-10 21:04:31,129][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:04:40,938][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 21:04:40,938][227910] FPS: 391552.82[0m
[36m[2023-07-10 21:04:40,940][227910] itr=1349, itrs=2000, Progress: 67.45%[0m
[36m[2023-07-10 21:04:52,569][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 21:04:52,569][227910] FPS: 330758.06[0m
[36m[2023-07-10 21:04:57,322][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:04:57,328][227910] Reward + Measures: [[1543.5690392     0.13276233    0.682962      0.32079199    0.35158667]][0m
[37m[1m[2023-07-10 21:04:57,328][227910] Max Reward on eval: 1543.5690392026172[0m
[37m[1m[2023-07-10 21:04:57,329][227910] Min Reward on eval: 1543.5690392026172[0m
[37m[1m[2023-07-10 21:04:57,329][227910] Mean Reward across all agents: 1543.5690392026172[0m
[37m[1m[2023-07-10 21:04:57,329][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:05:02,842][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:05:02,843][227910] Reward + Measures: [[1241.43712696    0.13340001    0.64310002    0.2843        0.32010004]
 [-185.35613594    0.0981        0.84389991    0.5557        0.80550003]
 [-243.59602325    0.0239        0.87159997    0.46949998    0.8448    ]
 ...
 [-438.88646878    0.0884        0.82960004    0.53750002    0.80970001]
 [ 677.1253204     0.23120001    0.61110002    0.35119998    0.51310009]
 [ 470.91120031    0.0732        0.71310002    0.35069999    0.6358    ]][0m
[37m[1m[2023-07-10 21:05:02,843][227910] Max Reward on eval: 1469.0000108673237[0m
[37m[1m[2023-07-10 21:05:02,843][227910] Min Reward on eval: -892.7545064021367[0m
[37m[1m[2023-07-10 21:05:02,843][227910] Mean Reward across all agents: 457.01581605588706[0m
[37m[1m[2023-07-10 21:05:02,843][227910] Average Trajectory Length: 996.1646666666667[0m
[36m[2023-07-10 21:05:02,847][227910] mean_value=-469.11929323647735, max_value=993.4033088120865[0m
[37m[1m[2023-07-10 21:05:02,850][227910] New mean coefficients: [[ 2.3159037   1.2216306   1.140943    0.6874575  -0.23104402]][0m
[37m[1m[2023-07-10 21:05:02,851][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:05:12,554][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 21:05:12,554][227910] FPS: 395801.24[0m
[36m[2023-07-10 21:05:12,556][227910] itr=1350, itrs=2000, Progress: 67.50%[0m
[37m[1m[2023-07-10 21:05:16,487][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001330[0m
[36m[2023-07-10 21:05:28,305][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 21:05:28,305][227910] FPS: 332760.32[0m
[36m[2023-07-10 21:05:33,044][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:05:33,044][227910] Reward + Measures: [[1591.86671413    0.13135134    0.69109929    0.31256631    0.3450717 ]][0m
[37m[1m[2023-07-10 21:05:33,044][227910] Max Reward on eval: 1591.8667141326207[0m
[37m[1m[2023-07-10 21:05:33,045][227910] Min Reward on eval: 1591.8667141326207[0m
[37m[1m[2023-07-10 21:05:33,045][227910] Mean Reward across all agents: 1591.8667141326207[0m
[37m[1m[2023-07-10 21:05:33,045][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:05:38,582][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:05:38,583][227910] Reward + Measures: [[1263.69444831    0.16180001    0.63840002    0.3752        0.39340001]
 [ 888.31260427    0.45510003    0.60860008    0.2685        0.38609999]
 [1198.47086211    0.1895        0.55739999    0.3928        0.37330002]
 ...
 [ 986.10755105    0.55110002    0.68970001    0.54350001    0.31960002]
 [ -99.88069444    0.33240002    0.32740003    0.22519998    0.26859999]
 [ -38.29710102    0.456         0.4858        0.0511        0.45660001]][0m
[37m[1m[2023-07-10 21:05:38,583][227910] Max Reward on eval: 1686.619915193296[0m
[37m[1m[2023-07-10 21:05:38,584][227910] Min Reward on eval: -870.7118259444484[0m
[37m[1m[2023-07-10 21:05:38,584][227910] Mean Reward across all agents: 516.5003309386907[0m
[37m[1m[2023-07-10 21:05:38,584][227910] Average Trajectory Length: 996.4183333333333[0m
[36m[2023-07-10 21:05:38,591][227910] mean_value=-536.3221093218075, max_value=2106.206659995165[0m
[37m[1m[2023-07-10 21:05:38,594][227910] New mean coefficients: [[ 1.7611873   0.55122924  0.61722314  1.1555555  -0.60877144]][0m
[37m[1m[2023-07-10 21:05:38,595][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:05:48,570][227910] train() took 9.97 seconds to complete[0m
[36m[2023-07-10 21:05:48,571][227910] FPS: 385013.88[0m
[36m[2023-07-10 21:05:48,573][227910] itr=1351, itrs=2000, Progress: 67.55%[0m
[36m[2023-07-10 21:06:00,075][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 21:06:00,081][227910] FPS: 334484.78[0m
[36m[2023-07-10 21:06:04,870][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:06:04,871][227910] Reward + Measures: [[1639.95281296    0.129915      0.68863535    0.31821164    0.33555034]][0m
[37m[1m[2023-07-10 21:06:04,871][227910] Max Reward on eval: 1639.9528129587459[0m
[37m[1m[2023-07-10 21:06:04,871][227910] Min Reward on eval: 1639.9528129587459[0m
[37m[1m[2023-07-10 21:06:04,871][227910] Mean Reward across all agents: 1639.9528129587459[0m
[37m[1m[2023-07-10 21:06:04,872][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:06:10,626][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:06:10,626][227910] Reward + Measures: [[-481.21621262    0.72419995    0.79620004    0.58360004    0.69940007]
 [1392.33505142    0.26100001    0.64940006    0.41440001    0.33870003]
 [ -37.861612      0.67650002    0.69770002    0.6893        0.1244    ]
 ...
 [ 691.29178131    0.2077        0.61969995    0.3524        0.491     ]
 [ 837.42007194    0.1882        0.5783        0.3152        0.29710001]
 [1390.15222731    0.1178        0.71890002    0.59050006    0.33249998]][0m
[37m[1m[2023-07-10 21:06:10,626][227910] Max Reward on eval: 1706.805124867754[0m
[37m[1m[2023-07-10 21:06:10,627][227910] Min Reward on eval: -887.7732513480557[0m
[37m[1m[2023-07-10 21:06:10,627][227910] Mean Reward across all agents: 407.1305953942752[0m
[37m[1m[2023-07-10 21:06:10,627][227910] Average Trajectory Length: 994.923[0m
[36m[2023-07-10 21:06:10,632][227910] mean_value=-680.6920944607813, max_value=1890.152227312047[0m
[37m[1m[2023-07-10 21:06:10,635][227910] New mean coefficients: [[ 1.7480081  0.3801875  1.0106275  1.350005  -0.2002146]][0m
[37m[1m[2023-07-10 21:06:10,636][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:06:20,459][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 21:06:20,460][227910] FPS: 390981.96[0m
[36m[2023-07-10 21:06:20,462][227910] itr=1352, itrs=2000, Progress: 67.60%[0m
[36m[2023-07-10 21:06:31,991][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 21:06:31,992][227910] FPS: 333588.21[0m
[36m[2023-07-10 21:06:36,803][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:06:36,804][227910] Reward + Measures: [[1692.62637703    0.13116103    0.68553776    0.31760082    0.32935718]][0m
[37m[1m[2023-07-10 21:06:36,804][227910] Max Reward on eval: 1692.6263770261749[0m
[37m[1m[2023-07-10 21:06:36,804][227910] Min Reward on eval: 1692.6263770261749[0m
[37m[1m[2023-07-10 21:06:36,804][227910] Mean Reward across all agents: 1692.6263770261749[0m
[37m[1m[2023-07-10 21:06:36,804][227910] Average Trajectory Length: 999.8013333333333[0m
[36m[2023-07-10 21:06:42,330][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:06:42,330][227910] Reward + Measures: [[ 122.01845643    0.42739996    0.39470002    0.57370001    0.29879999]
 [ 714.44234511    0.38480002    0.4585        0.4224        0.41030002]
 [ 270.9765856     0.25869998    0.52030003    0.35180002    0.271     ]
 ...
 [1153.57192726    0.228         0.60550004    0.47879997    0.44369999]
 [-250.51733874    0.18310001    0.58199996    0.3743        0.62379998]
 [  45.74838422    0.19688524    0.30493248    0.16961625    0.27398929]][0m
[37m[1m[2023-07-10 21:06:42,330][227910] Max Reward on eval: 1618.2636579258135[0m
[37m[1m[2023-07-10 21:06:42,331][227910] Min Reward on eval: -2046.7279399382417[0m
[37m[1m[2023-07-10 21:06:42,331][227910] Mean Reward across all agents: 257.5525864956233[0m
[37m[1m[2023-07-10 21:06:42,331][227910] Average Trajectory Length: 995.9503333333333[0m
[36m[2023-07-10 21:06:42,334][227910] mean_value=-1132.3151279431413, max_value=1309.945019624372[0m
[37m[1m[2023-07-10 21:06:42,337][227910] New mean coefficients: [[ 1.6351615   0.4687972   1.5796297   0.78591967 -0.16299851]][0m
[37m[1m[2023-07-10 21:06:42,337][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:06:51,988][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 21:06:51,988][227910] FPS: 397963.14[0m
[36m[2023-07-10 21:06:51,991][227910] itr=1353, itrs=2000, Progress: 67.65%[0m
[36m[2023-07-10 21:07:03,400][227910] train() took 11.39 seconds to complete[0m
[36m[2023-07-10 21:07:03,400][227910] FPS: 337214.04[0m
[36m[2023-07-10 21:07:08,159][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:07:08,159][227910] Reward + Measures: [[1769.30365521    0.13410701    0.68923622    0.31285602    0.31784734]][0m
[37m[1m[2023-07-10 21:07:08,159][227910] Max Reward on eval: 1769.3036552145534[0m
[37m[1m[2023-07-10 21:07:08,160][227910] Min Reward on eval: 1769.3036552145534[0m
[37m[1m[2023-07-10 21:07:08,160][227910] Mean Reward across all agents: 1769.3036552145534[0m
[37m[1m[2023-07-10 21:07:08,160][227910] Average Trajectory Length: 999.7959999999999[0m
[36m[2023-07-10 21:07:13,604][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:07:13,605][227910] Reward + Measures: [[ 274.43219817    0.0237        0.90340006    0.85220003    0.83719999]
 [-472.18671547    0.90249997    0.65579998    0.91759998    0.0444    ]
 [ 288.59783665    0.27770001    0.50619996    0.38280001    0.28130001]
 ...
 [-106.18680368    0.39390001    0.5363        0.36320001    0.47300002]
 [-479.35636316    0.32879996    0.4434        0.38400003    0.233     ]
 [1493.55634289    0.2244        0.67129999    0.47030002    0.28350002]][0m
[37m[1m[2023-07-10 21:07:13,605][227910] Max Reward on eval: 1712.4855909705161[0m
[37m[1m[2023-07-10 21:07:13,605][227910] Min Reward on eval: -1395.668383516278[0m
[37m[1m[2023-07-10 21:07:13,605][227910] Mean Reward across all agents: 358.86759540778826[0m
[37m[1m[2023-07-10 21:07:13,606][227910] Average Trajectory Length: 991.0263333333334[0m
[36m[2023-07-10 21:07:13,609][227910] mean_value=-985.4390068303687, max_value=1870.938691515783[0m
[37m[1m[2023-07-10 21:07:13,611][227910] New mean coefficients: [[ 1.3316514   0.66659534  1.7160976  -0.1067248   0.21847932]][0m
[37m[1m[2023-07-10 21:07:13,612][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:07:23,394][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 21:07:23,394][227910] FPS: 392635.87[0m
[36m[2023-07-10 21:07:23,397][227910] itr=1354, itrs=2000, Progress: 67.70%[0m
[36m[2023-07-10 21:07:34,937][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 21:07:34,938][227910] FPS: 333262.07[0m
[36m[2023-07-10 21:07:39,688][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:07:39,688][227910] Reward + Measures: [[1840.97218766    0.13386492    0.70214868    0.30934918    0.3166045 ]][0m
[37m[1m[2023-07-10 21:07:39,688][227910] Max Reward on eval: 1840.9721876573333[0m
[37m[1m[2023-07-10 21:07:39,689][227910] Min Reward on eval: 1840.9721876573333[0m
[37m[1m[2023-07-10 21:07:39,689][227910] Mean Reward across all agents: 1840.9721876573333[0m
[37m[1m[2023-07-10 21:07:39,689][227910] Average Trajectory Length: 999.6626666666666[0m
[36m[2023-07-10 21:07:45,160][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:07:45,166][227910] Reward + Measures: [[1108.09799293    0.29720002    0.64140004    0.31990001    0.24890001]
 [ 756.56732367    0.30510002    0.51440001    0.38699999    0.36420003]
 [ 997.33066347    0.33470002    0.49880001    0.32960001    0.29440001]
 ...
 [1396.77948523    0.14030001    0.65189999    0.31210002    0.36629999]
 [ 672.20488151    0.23309998    0.56590003    0.4348        0.38250002]
 [ 746.19591541    0.11390001    0.56769997    0.3152        0.3362    ]][0m
[37m[1m[2023-07-10 21:07:45,166][227910] Max Reward on eval: 1804.3383449702524[0m
[37m[1m[2023-07-10 21:07:45,167][227910] Min Reward on eval: -918.3037976703956[0m
[37m[1m[2023-07-10 21:07:45,167][227910] Mean Reward across all agents: 752.9728197760145[0m
[37m[1m[2023-07-10 21:07:45,168][227910] Average Trajectory Length: 996.8186666666667[0m
[36m[2023-07-10 21:07:45,177][227910] mean_value=-351.83863594995876, max_value=2054.8906155671225[0m
[37m[1m[2023-07-10 21:07:45,181][227910] New mean coefficients: [[1.2192532  0.41353887 2.4000597  0.1296571  0.36497182]][0m
[37m[1m[2023-07-10 21:07:45,183][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:07:54,880][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 21:07:54,880][227910] FPS: 396073.67[0m
[36m[2023-07-10 21:07:54,883][227910] itr=1355, itrs=2000, Progress: 67.75%[0m
[36m[2023-07-10 21:08:06,519][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 21:08:06,519][227910] FPS: 330639.63[0m
[36m[2023-07-10 21:08:11,418][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:08:11,418][227910] Reward + Measures: [[1876.51410961    0.13506559    0.72059971    0.31056145    0.31430945]][0m
[37m[1m[2023-07-10 21:08:11,418][227910] Max Reward on eval: 1876.514109605539[0m
[37m[1m[2023-07-10 21:08:11,419][227910] Min Reward on eval: 1876.514109605539[0m
[37m[1m[2023-07-10 21:08:11,419][227910] Mean Reward across all agents: 1876.514109605539[0m
[37m[1m[2023-07-10 21:08:11,419][227910] Average Trajectory Length: 999.7669999999999[0m
[36m[2023-07-10 21:08:16,856][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:08:16,857][227910] Reward + Measures: [[1231.45700756    0.20630001    0.58530003    0.17520002    0.259     ]
 [1286.86751098    0.3883        0.62910002    0.3152        0.3256    ]
 [ 231.57024178    0.26589999    0.68850011    0.70600003    0.44490001]
 ...
 [ 106.1888372     0.2861        0.46059999    0.38340002    0.1398    ]
 [-569.79836804    0.89270002    0.74710006    0.90280002    0.0465    ]
 [1196.81731344    0.20390001    0.59030002    0.3389        0.27880001]][0m
[37m[1m[2023-07-10 21:08:16,857][227910] Max Reward on eval: 1865.1009077401832[0m
[37m[1m[2023-07-10 21:08:16,858][227910] Min Reward on eval: -880.4961508332228[0m
[37m[1m[2023-07-10 21:08:16,858][227910] Mean Reward across all agents: 778.9323110473531[0m
[37m[1m[2023-07-10 21:08:16,858][227910] Average Trajectory Length: 992.621[0m
[36m[2023-07-10 21:08:16,863][227910] mean_value=-569.4960107936646, max_value=1940.68271724449[0m
[37m[1m[2023-07-10 21:08:16,866][227910] New mean coefficients: [[ 1.3820337   0.20233826  2.778767   -0.22266088  0.89039046]][0m
[37m[1m[2023-07-10 21:08:16,867][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:08:26,567][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 21:08:26,567][227910] FPS: 395964.34[0m
[36m[2023-07-10 21:08:26,569][227910] itr=1356, itrs=2000, Progress: 67.80%[0m
[36m[2023-07-10 21:08:38,189][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 21:08:38,189][227910] FPS: 331000.54[0m
[36m[2023-07-10 21:08:42,942][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:08:42,942][227910] Reward + Measures: [[1918.56420741    0.126214      0.73550892    0.30406633    0.316825  ]][0m
[37m[1m[2023-07-10 21:08:42,943][227910] Max Reward on eval: 1918.5642074096131[0m
[37m[1m[2023-07-10 21:08:42,943][227910] Min Reward on eval: 1918.5642074096131[0m
[37m[1m[2023-07-10 21:08:42,943][227910] Mean Reward across all agents: 1918.5642074096131[0m
[37m[1m[2023-07-10 21:08:42,943][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:08:48,576][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:08:48,577][227910] Reward + Measures: [[1244.59312145    0.34040001    0.47010002    0.3529        0.27530003]
 [ 186.6822242     0.57319999    0.58919996    0.6311        0.2545    ]
 [  28.63545647    0.50400001    0.42120001    0.30539998    0.30669999]
 ...
 [ 103.63450737    0.25250003    0.79259998    0.078         0.81829995]
 [ 262.40895587    0.8071        0.70199996    0.90480006    0.1688    ]
 [ 609.54062256    0.27079999    0.48270002    0.26949999    0.33590004]][0m
[37m[1m[2023-07-10 21:08:48,577][227910] Max Reward on eval: 1835.9134626504033[0m
[37m[1m[2023-07-10 21:08:48,577][227910] Min Reward on eval: -800.247612783534[0m
[37m[1m[2023-07-10 21:08:48,577][227910] Mean Reward across all agents: 539.7128548921542[0m
[37m[1m[2023-07-10 21:08:48,578][227910] Average Trajectory Length: 995.4993333333333[0m
[36m[2023-07-10 21:08:48,582][227910] mean_value=-510.72198624087935, max_value=1474.6591595100458[0m
[37m[1m[2023-07-10 21:08:48,585][227910] New mean coefficients: [[2.064591   0.14846006 2.040259   0.2504217  1.0524521 ]][0m
[37m[1m[2023-07-10 21:08:48,586][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:08:58,391][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 21:08:58,391][227910] FPS: 391710.71[0m
[36m[2023-07-10 21:08:58,394][227910] itr=1357, itrs=2000, Progress: 67.85%[0m
[36m[2023-07-10 21:09:09,976][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 21:09:09,976][227910] FPS: 332065.17[0m
[36m[2023-07-10 21:09:14,718][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:09:14,718][227910] Reward + Measures: [[1987.50384862    0.118704      0.7486847     0.29993433    0.31550467]][0m
[37m[1m[2023-07-10 21:09:14,719][227910] Max Reward on eval: 1987.5038486181647[0m
[37m[1m[2023-07-10 21:09:14,719][227910] Min Reward on eval: 1987.5038486181647[0m
[37m[1m[2023-07-10 21:09:14,719][227910] Mean Reward across all agents: 1987.5038486181647[0m
[37m[1m[2023-07-10 21:09:14,719][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:09:20,154][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:09:20,160][227910] Reward + Measures: [[-273.09743138    0.43670002    0.71190006    0.26980001    0.59240001]
 [1620.45137158    0.14660001    0.66070002    0.36410001    0.35329998]
 [ 534.77379878    0.13816963    0.52699602    0.32542211    0.356316  ]
 ...
 [ 786.75918904    0.2467        0.44150001    0.26460001    0.3098    ]
 [1838.90779696    0.1416        0.75889999    0.35200003    0.33200002]
 [-269.01238328    0.60759997    0.70460004    0.221         0.61309999]][0m
[37m[1m[2023-07-10 21:09:20,160][227910] Max Reward on eval: 1907.5174790164922[0m
[37m[1m[2023-07-10 21:09:20,160][227910] Min Reward on eval: -928.7628201379121[0m
[37m[1m[2023-07-10 21:09:20,160][227910] Mean Reward across all agents: 786.3681443427082[0m
[37m[1m[2023-07-10 21:09:20,161][227910] Average Trajectory Length: 985.5166666666667[0m
[36m[2023-07-10 21:09:20,164][227910] mean_value=-563.6265844537867, max_value=2391.0124083183705[0m
[37m[1m[2023-07-10 21:09:20,166][227910] New mean coefficients: [[ 2.211724   -0.06140856  2.1900258  -0.06351972  0.62245   ]][0m
[37m[1m[2023-07-10 21:09:20,167][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:09:29,857][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 21:09:29,858][227910] FPS: 396348.11[0m
[36m[2023-07-10 21:09:29,860][227910] itr=1358, itrs=2000, Progress: 67.90%[0m
[36m[2023-07-10 21:09:41,429][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 21:09:41,429][227910] FPS: 332558.84[0m
[36m[2023-07-10 21:09:46,302][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:09:46,302][227910] Reward + Measures: [[2038.18982306    0.11321102    0.75303727    0.3024109     0.31233189]][0m
[37m[1m[2023-07-10 21:09:46,302][227910] Max Reward on eval: 2038.189823055356[0m
[37m[1m[2023-07-10 21:09:46,302][227910] Min Reward on eval: 2038.189823055356[0m
[37m[1m[2023-07-10 21:09:46,303][227910] Mean Reward across all agents: 2038.189823055356[0m
[37m[1m[2023-07-10 21:09:46,303][227910] Average Trajectory Length: 999.9593333333333[0m
[36m[2023-07-10 21:09:51,888][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:09:51,889][227910] Reward + Measures: [[ 790.72346054    0.29970002    0.4745        0.2454        0.26989999]
 [ 393.58474759    0.41619998    0.41300002    0.3292        0.322     ]
 [1542.44236397    0.08890001    0.67919999    0.32750002    0.36570001]
 ...
 [ 157.57324075    0.87470001    0.1752        0.8707        0.15140001]
 [1350.2319047     0.10209999    0.64849997    0.3364        0.34349999]
 [ 459.18327246    0.70139998    0.433         0.67510003    0.234     ]][0m
[37m[1m[2023-07-10 21:09:51,889][227910] Max Reward on eval: 2127.872597071808[0m
[37m[1m[2023-07-10 21:09:51,890][227910] Min Reward on eval: -875.684085900744[0m
[37m[1m[2023-07-10 21:09:51,890][227910] Mean Reward across all agents: 1136.4919937367304[0m
[37m[1m[2023-07-10 21:09:51,890][227910] Average Trajectory Length: 999.8103333333333[0m
[36m[2023-07-10 21:09:51,895][227910] mean_value=-230.62068689361118, max_value=2200.0064113304297[0m
[37m[1m[2023-07-10 21:09:51,898][227910] New mean coefficients: [[ 2.345439   -0.27998513  2.861778   -0.10899861  0.47453767]][0m
[37m[1m[2023-07-10 21:09:51,899][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:10:01,829][227910] train() took 9.93 seconds to complete[0m
[36m[2023-07-10 21:10:01,829][227910] FPS: 386789.39[0m
[36m[2023-07-10 21:10:01,831][227910] itr=1359, itrs=2000, Progress: 67.95%[0m
[36m[2023-07-10 21:10:13,513][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 21:10:13,514][227910] FPS: 329236.50[0m
[36m[2023-07-10 21:10:18,230][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:10:18,230][227910] Reward + Measures: [[2097.16724921    0.10926133    0.76231933    0.30212834    0.30778301]][0m
[37m[1m[2023-07-10 21:10:18,230][227910] Max Reward on eval: 2097.167249214235[0m
[37m[1m[2023-07-10 21:10:18,231][227910] Min Reward on eval: 2097.167249214235[0m
[37m[1m[2023-07-10 21:10:18,231][227910] Mean Reward across all agents: 2097.167249214235[0m
[37m[1m[2023-07-10 21:10:18,231][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:10:23,795][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:10:23,796][227910] Reward + Measures: [[1417.54254909    0.13250001    0.5977        0.42700005    0.34369999]
 [1586.71070263    0.1304        0.61140007    0.43800002    0.29070002]
 [1663.05666359    0.1217        0.65779996    0.43940002    0.32300001]
 ...
 [1697.15692018    0.1153        0.62799996    0.37680003    0.31549999]
 [1820.71847783    0.11189999    0.68960005    0.28800002    0.35699996]
 [ 491.63881868    0.57910001    0.74599999    0.48359999    0.49980003]][0m
[37m[1m[2023-07-10 21:10:23,796][227910] Max Reward on eval: 2186.190019080602[0m
[37m[1m[2023-07-10 21:10:23,796][227910] Min Reward on eval: -520.6499740778934[0m
[37m[1m[2023-07-10 21:10:23,796][227910] Mean Reward across all agents: 1286.6275596865926[0m
[37m[1m[2023-07-10 21:10:23,797][227910] Average Trajectory Length: 998.288[0m
[36m[2023-07-10 21:10:23,803][227910] mean_value=199.49298189996503, max_value=2286.0237551908476[0m
[37m[1m[2023-07-10 21:10:23,806][227910] New mean coefficients: [[ 2.1348302  -0.48640865  2.4093828   0.23798127 -0.01040661]][0m
[37m[1m[2023-07-10 21:10:23,807][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:10:33,628][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 21:10:33,628][227910] FPS: 391041.86[0m
[36m[2023-07-10 21:10:33,631][227910] itr=1360, itrs=2000, Progress: 68.00%[0m
[37m[1m[2023-07-10 21:10:37,714][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001340[0m
[36m[2023-07-10 21:10:49,568][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 21:10:49,568][227910] FPS: 331670.25[0m
[36m[2023-07-10 21:10:54,263][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:10:54,264][227910] Reward + Measures: [[2157.74259452    0.10677       0.77414727    0.29326499    0.29912767]][0m
[37m[1m[2023-07-10 21:10:54,264][227910] Max Reward on eval: 2157.7425945195346[0m
[37m[1m[2023-07-10 21:10:54,264][227910] Min Reward on eval: 2157.7425945195346[0m
[37m[1m[2023-07-10 21:10:54,264][227910] Mean Reward across all agents: 2157.7425945195346[0m
[37m[1m[2023-07-10 21:10:54,265][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:10:59,645][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:10:59,646][227910] Reward + Measures: [[ 952.87715035    0.18360001    0.65460002    0.44099998    0.57690001]
 [ 599.95118381    0.116         0.57240003    0.3687        0.55940002]
 [1296.27845164    0.0692        0.58230001    0.33740002    0.34770003]
 ...
 [ 280.59734159    0.58090001    0.74030006    0.69400001    0.76010001]
 [1821.21143804    0.1033        0.74340004    0.33049998    0.39719999]
 [1549.09846935    0.07759999    0.61400002    0.30329999    0.34740001]][0m
[37m[1m[2023-07-10 21:10:59,646][227910] Max Reward on eval: 2140.595428002067[0m
[37m[1m[2023-07-10 21:10:59,646][227910] Min Reward on eval: -671.794061913481[0m
[37m[1m[2023-07-10 21:10:59,647][227910] Mean Reward across all agents: 1041.3764201151384[0m
[37m[1m[2023-07-10 21:10:59,647][227910] Average Trajectory Length: 998.1203333333333[0m
[36m[2023-07-10 21:10:59,653][227910] mean_value=186.00101321877125, max_value=1975.456128082797[0m
[37m[1m[2023-07-10 21:10:59,656][227910] New mean coefficients: [[ 2.043304   -0.36267152  2.184229    0.07012391  0.27610913]][0m
[37m[1m[2023-07-10 21:10:59,657][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:11:09,282][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 21:11:09,282][227910] FPS: 399029.07[0m
[36m[2023-07-10 21:11:09,284][227910] itr=1361, itrs=2000, Progress: 68.05%[0m
[36m[2023-07-10 21:11:20,730][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 21:11:20,730][227910] FPS: 336038.44[0m
[36m[2023-07-10 21:11:25,511][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:11:25,511][227910] Reward + Measures: [[2217.54287207    0.10256081    0.77828354    0.2899369     0.29713321]][0m
[37m[1m[2023-07-10 21:11:25,511][227910] Max Reward on eval: 2217.542872074961[0m
[37m[1m[2023-07-10 21:11:25,511][227910] Min Reward on eval: 2217.542872074961[0m
[37m[1m[2023-07-10 21:11:25,511][227910] Mean Reward across all agents: 2217.542872074961[0m
[37m[1m[2023-07-10 21:11:25,512][227910] Average Trajectory Length: 999.8979999999999[0m
[36m[2023-07-10 21:11:31,004][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:11:31,005][227910] Reward + Measures: [[1114.52226717    0.111         0.7198        0.37990001    0.45699999]
 [ -61.4493106     0.28190002    0.5898        0.11000001    0.55109996]
 [1873.2702431     0.1499        0.69810003    0.37409997    0.31510001]
 ...
 [1955.71177825    0.1419        0.72909999    0.33129999    0.3098    ]
 [1050.12833394    0.3125        0.58700001    0.30690002    0.46199998]
 [ 922.4393976     0.3184        0.64130002    0.1372        0.4436    ]][0m
[37m[1m[2023-07-10 21:11:31,005][227910] Max Reward on eval: 2195.1218940582125[0m
[37m[1m[2023-07-10 21:11:31,005][227910] Min Reward on eval: -246.06640374745476[0m
[37m[1m[2023-07-10 21:11:31,005][227910] Mean Reward across all agents: 1249.2615616306891[0m
[37m[1m[2023-07-10 21:11:31,006][227910] Average Trajectory Length: 999.5726666666666[0m
[36m[2023-07-10 21:11:31,011][227910] mean_value=105.68255135786637, max_value=2608.964956474537[0m
[37m[1m[2023-07-10 21:11:31,014][227910] New mean coefficients: [[ 1.6924825  -0.24335423  2.8401568   0.4611923  -0.07384276]][0m
[37m[1m[2023-07-10 21:11:31,015][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:11:40,703][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 21:11:40,703][227910] FPS: 396435.24[0m
[36m[2023-07-10 21:11:40,705][227910] itr=1362, itrs=2000, Progress: 68.10%[0m
[36m[2023-07-10 21:11:52,254][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 21:11:52,255][227910] FPS: 333053.79[0m
[36m[2023-07-10 21:11:57,002][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:11:57,002][227910] Reward + Measures: [[2266.43610139    0.10663267    0.78942764    0.293446      0.28962466]][0m
[37m[1m[2023-07-10 21:11:57,002][227910] Max Reward on eval: 2266.4361013854045[0m
[37m[1m[2023-07-10 21:11:57,003][227910] Min Reward on eval: 2266.4361013854045[0m
[37m[1m[2023-07-10 21:11:57,003][227910] Mean Reward across all agents: 2266.4361013854045[0m
[37m[1m[2023-07-10 21:11:57,003][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:12:02,449][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:12:02,450][227910] Reward + Measures: [[  439.95654843     0.71570003     0.71680003     0.71240008
      0.1779    ]
 [-1233.65035375     0.88389999     0.9386         0.0473
      0.88500005]
 [ 1460.35661557     0.108          0.70300001     0.33400002
      0.3987    ]
 ...
 [ 2129.77244512     0.126          0.76059997     0.24629998
      0.29959998]
 [  986.27012304     0.09370001     0.78530002     0.4619
      0.47040001]
 [ -311.39927653     0.63099998     0.86800003     0.20089999
      0.75289994]][0m
[37m[1m[2023-07-10 21:12:02,450][227910] Max Reward on eval: 2237.0361184334383[0m
[37m[1m[2023-07-10 21:12:02,450][227910] Min Reward on eval: -1746.0212530004094[0m
[37m[1m[2023-07-10 21:12:02,451][227910] Mean Reward across all agents: 1010.2372221027813[0m
[37m[1m[2023-07-10 21:12:02,451][227910] Average Trajectory Length: 999.3386666666667[0m
[36m[2023-07-10 21:12:02,457][227910] mean_value=153.70624443718611, max_value=2509.247023616219[0m
[37m[1m[2023-07-10 21:12:02,460][227910] New mean coefficients: [[ 2.0236974  -0.32558486  3.466301    0.24923903 -0.05000948]][0m
[37m[1m[2023-07-10 21:12:02,461][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:12:12,338][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 21:12:12,338][227910] FPS: 388849.24[0m
[36m[2023-07-10 21:12:12,340][227910] itr=1363, itrs=2000, Progress: 68.15%[0m
[36m[2023-07-10 21:12:24,024][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 21:12:24,025][227910] FPS: 329223.83[0m
[36m[2023-07-10 21:12:28,787][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:12:28,788][227910] Reward + Measures: [[2332.97174649    0.10865033    0.79348063    0.28975999    0.28118065]][0m
[37m[1m[2023-07-10 21:12:28,788][227910] Max Reward on eval: 2332.971746493297[0m
[37m[1m[2023-07-10 21:12:28,788][227910] Min Reward on eval: 2332.971746493297[0m
[37m[1m[2023-07-10 21:12:28,788][227910] Mean Reward across all agents: 2332.971746493297[0m
[37m[1m[2023-07-10 21:12:28,788][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:12:34,415][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:12:34,416][227910] Reward + Measures: [[1918.30749246    0.14649999    0.71340001    0.30109999    0.32139999]
 [ 180.77443196    0.22710001    0.67670006    0.49460003    0.56420004]
 [1607.86523184    0.0992        0.7439        0.28029999    0.31219998]
 ...
 [1675.94746437    0.26160002    0.69349998    0.33780003    0.27449998]
 [ 201.38589531    0.45239997    0.74300003    0.21930002    0.5643    ]
 [1918.99160235    0.22129999    0.67140001    0.33080003    0.27610001]][0m
[37m[1m[2023-07-10 21:12:34,416][227910] Max Reward on eval: 2254.4725067753343[0m
[37m[1m[2023-07-10 21:12:34,416][227910] Min Reward on eval: -735.3674723036005[0m
[37m[1m[2023-07-10 21:12:34,416][227910] Mean Reward across all agents: 1248.171785630127[0m
[37m[1m[2023-07-10 21:12:34,417][227910] Average Trajectory Length: 998.711[0m
[36m[2023-07-10 21:12:34,420][227910] mean_value=-216.9156523166146, max_value=2241.768405087313[0m
[37m[1m[2023-07-10 21:12:34,423][227910] New mean coefficients: [[ 1.8454195 -0.1401032  3.6275394  0.2698935 -0.0063702]][0m
[37m[1m[2023-07-10 21:12:34,424][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:12:44,006][227910] train() took 9.58 seconds to complete[0m
[36m[2023-07-10 21:12:44,006][227910] FPS: 400808.07[0m
[36m[2023-07-10 21:12:44,008][227910] itr=1364, itrs=2000, Progress: 68.20%[0m
[36m[2023-07-10 21:12:55,617][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 21:12:55,617][227910] FPS: 331327.75[0m
[36m[2023-07-10 21:13:00,424][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:13:00,425][227910] Reward + Measures: [[2389.71552463    0.108979      0.80214435    0.28873599    0.27644065]][0m
[37m[1m[2023-07-10 21:13:00,425][227910] Max Reward on eval: 2389.7155246321913[0m
[37m[1m[2023-07-10 21:13:00,425][227910] Min Reward on eval: 2389.7155246321913[0m
[37m[1m[2023-07-10 21:13:00,425][227910] Mean Reward across all agents: 2389.7155246321913[0m
[37m[1m[2023-07-10 21:13:00,425][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:13:05,824][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:13:05,829][227910] Reward + Measures: [[1510.72907905    0.0408        0.71820003    0.61269999    0.37130001]
 [1927.70429306    0.0512        0.74760002    0.42480001    0.33540002]
 [1443.77385642    0.2854        0.67020005    0.48949996    0.37189999]
 ...
 [1217.31027992    0.19829999    0.6516        0.49689999    0.39330003]
 [1485.34859457    0.2696        0.56919998    0.30539998    0.18570001]
 [ 440.6777411     0.2124        0.58070004    0.4382        0.3488    ]][0m
[37m[1m[2023-07-10 21:13:05,829][227910] Max Reward on eval: 2379.208095508511[0m
[37m[1m[2023-07-10 21:13:05,830][227910] Min Reward on eval: -766.6936193991802[0m
[37m[1m[2023-07-10 21:13:05,830][227910] Mean Reward across all agents: 1463.6771110496297[0m
[37m[1m[2023-07-10 21:13:05,830][227910] Average Trajectory Length: 999.101[0m
[36m[2023-07-10 21:13:05,836][227910] mean_value=98.27052487429067, max_value=2675.399135306873[0m
[37m[1m[2023-07-10 21:13:05,839][227910] New mean coefficients: [[ 2.0321035  -0.38738665  3.7557986  -0.0100621  -0.05198578]][0m
[37m[1m[2023-07-10 21:13:05,840][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:13:15,518][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 21:13:15,518][227910] FPS: 396853.82[0m
[36m[2023-07-10 21:13:15,520][227910] itr=1365, itrs=2000, Progress: 68.25%[0m
[36m[2023-07-10 21:13:27,138][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 21:13:27,138][227910] FPS: 331152.90[0m
[36m[2023-07-10 21:13:32,003][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:13:32,004][227910] Reward + Measures: [[2435.988051      0.11054233    0.81247836    0.28452966    0.27165136]][0m
[37m[1m[2023-07-10 21:13:32,004][227910] Max Reward on eval: 2435.9880510039416[0m
[37m[1m[2023-07-10 21:13:32,004][227910] Min Reward on eval: 2435.9880510039416[0m
[37m[1m[2023-07-10 21:13:32,004][227910] Mean Reward across all agents: 2435.9880510039416[0m
[37m[1m[2023-07-10 21:13:32,005][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:13:37,493][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:13:37,499][227910] Reward + Measures: [[1903.20446761    0.1505        0.80830002    0.33489999    0.34300002]
 [1618.69807576    0.24000001    0.59630007    0.33159998    0.29490003]
 [1791.3152505     0.0856        0.8373        0.39489999    0.30000001]
 ...
 [1659.88155293    0.2181        0.62799996    0.34620002    0.34010002]
 [1508.25847691    0.25140002    0.61409998    0.39090005    0.19950001]
 [1789.64446375    0.20050001    0.71939999    0.34550002    0.34180003]][0m
[37m[1m[2023-07-10 21:13:37,499][227910] Max Reward on eval: 2406.8736618736757[0m
[37m[1m[2023-07-10 21:13:37,499][227910] Min Reward on eval: 332.3325799094164[0m
[37m[1m[2023-07-10 21:13:37,500][227910] Mean Reward across all agents: 1598.9771648432327[0m
[37m[1m[2023-07-10 21:13:37,500][227910] Average Trajectory Length: 998.3736666666666[0m
[36m[2023-07-10 21:13:37,504][227910] mean_value=94.54854144084695, max_value=2673.8813260008583[0m
[37m[1m[2023-07-10 21:13:37,507][227910] New mean coefficients: [[ 1.8855999  -0.3019065   3.2855983   0.38766834 -0.2421759 ]][0m
[37m[1m[2023-07-10 21:13:37,508][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:13:47,457][227910] train() took 9.95 seconds to complete[0m
[36m[2023-07-10 21:13:47,457][227910] FPS: 386042.37[0m
[36m[2023-07-10 21:13:47,459][227910] itr=1366, itrs=2000, Progress: 68.30%[0m
[36m[2023-07-10 21:13:59,166][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 21:13:59,167][227910] FPS: 328522.43[0m
[36m[2023-07-10 21:14:03,995][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:14:03,995][227910] Reward + Measures: [[2554.66941395    0.11086967    0.79856598    0.27787733    0.26133603]][0m
[37m[1m[2023-07-10 21:14:03,996][227910] Max Reward on eval: 2554.6694139520077[0m
[37m[1m[2023-07-10 21:14:03,996][227910] Min Reward on eval: 2554.6694139520077[0m
[37m[1m[2023-07-10 21:14:03,996][227910] Mean Reward across all agents: 2554.6694139520077[0m
[37m[1m[2023-07-10 21:14:03,996][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:14:09,435][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:14:09,436][227910] Reward + Measures: [[1901.05595607    0.20469999    0.77089995    0.37890002    0.32249999]
 [2125.00838744    0.13250001    0.7719        0.40180001    0.33249998]
 [1921.96970765    0.12279999    0.80089998    0.34959999    0.31870002]
 ...
 [ 649.69496442    0.58059996    0.46810004    0.61430001    0.44280002]
 [1959.17719082    0.1121        0.70079994    0.41170001    0.36170003]
 [1213.50783753    0.19840001    0.56940001    0.47589999    0.37190002]][0m
[37m[1m[2023-07-10 21:14:09,436][227910] Max Reward on eval: 2399.497373671271[0m
[37m[1m[2023-07-10 21:14:09,436][227910] Min Reward on eval: -146.70648489891317[0m
[37m[1m[2023-07-10 21:14:09,436][227910] Mean Reward across all agents: 1412.3259559800003[0m
[37m[1m[2023-07-10 21:14:09,436][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:14:09,444][227910] mean_value=287.7875171010783, max_value=2011.9782989455266[0m
[37m[1m[2023-07-10 21:14:09,447][227910] New mean coefficients: [[ 2.532831   -0.5131054   3.5696335   0.16437937 -0.4378777 ]][0m
[37m[1m[2023-07-10 21:14:09,447][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:14:19,214][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 21:14:19,214][227910] FPS: 393241.22[0m
[36m[2023-07-10 21:14:19,216][227910] itr=1367, itrs=2000, Progress: 68.35%[0m
[36m[2023-07-10 21:14:30,890][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 21:14:30,890][227910] FPS: 329558.06[0m
[36m[2023-07-10 21:14:35,724][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:14:35,724][227910] Reward + Measures: [[2602.63615657    0.11076655    0.79333496    0.26680028    0.25514948]][0m
[37m[1m[2023-07-10 21:14:35,725][227910] Max Reward on eval: 2602.636156574848[0m
[37m[1m[2023-07-10 21:14:35,725][227910] Min Reward on eval: 2602.636156574848[0m
[37m[1m[2023-07-10 21:14:35,725][227910] Mean Reward across all agents: 2602.636156574848[0m
[37m[1m[2023-07-10 21:14:35,725][227910] Average Trajectory Length: 999.8209999999999[0m
[36m[2023-07-10 21:14:41,326][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:14:41,327][227910] Reward + Measures: [[1879.03226346    0.13950001    0.78299999    0.44059998    0.2895    ]
 [1906.13991407    0.1689        0.72100002    0.39510003    0.29819998]
 [1713.04965694    0.17749999    0.76370001    0.27070001    0.30340001]
 ...
 [-268.27240901    0.98640007    0.85600007    0.95039999    0.061     ]
 [ 891.72575696    0.41669998    0.55010003    0.57459998    0.38789999]
 [1730.62874068    0.1665        0.72659999    0.3087        0.3107    ]][0m
[37m[1m[2023-07-10 21:14:41,327][227910] Max Reward on eval: 2576.390019696299[0m
[37m[1m[2023-07-10 21:14:41,327][227910] Min Reward on eval: -724.4737181521836[0m
[37m[1m[2023-07-10 21:14:41,327][227910] Mean Reward across all agents: 1262.424760444231[0m
[37m[1m[2023-07-10 21:14:41,328][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:14:41,336][227910] mean_value=392.2725650824529, max_value=2488.2485153047833[0m
[37m[1m[2023-07-10 21:14:41,339][227910] New mean coefficients: [[ 2.5651047  -0.67281747  3.2129202  -0.01814662 -0.6455189 ]][0m
[37m[1m[2023-07-10 21:14:41,340][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:14:51,238][227910] train() took 9.90 seconds to complete[0m
[36m[2023-07-10 21:14:51,238][227910] FPS: 387994.71[0m
[36m[2023-07-10 21:14:51,241][227910] itr=1368, itrs=2000, Progress: 68.40%[0m
[36m[2023-07-10 21:15:02,883][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 21:15:02,884][227910] FPS: 330389.46[0m
[36m[2023-07-10 21:15:07,691][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:15:07,692][227910] Reward + Measures: [[2033.35610393    0.23649842    0.61473548    0.31228614    0.20900194]][0m
[37m[1m[2023-07-10 21:15:07,692][227910] Max Reward on eval: 2033.356103933456[0m
[37m[1m[2023-07-10 21:15:07,692][227910] Min Reward on eval: 2033.356103933456[0m
[37m[1m[2023-07-10 21:15:07,692][227910] Mean Reward across all agents: 2033.356103933456[0m
[37m[1m[2023-07-10 21:15:07,693][227910] Average Trajectory Length: 999.9953333333333[0m
[36m[2023-07-10 21:15:13,180][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:15:13,181][227910] Reward + Measures: [[-726.33321125    0.61059999    0.95480007    0.62560004    0.9641    ]
 [-357.69558886    0.32140002    0.95679998    0.32360002    0.96619999]
 [ 134.96827324    0.27950001    0.88490003    0.5399        0.88219994]
 ...
 [  51.77380027    0.18139999    0.5108        0.20750001    0.38140002]
 [ 155.71712878    0.23449998    0.61580002    0.3475        0.51520002]
 [-897.14258173    0.06674264    0.78868431    0.54090559    0.7432493 ]][0m
[37m[1m[2023-07-10 21:15:13,181][227910] Max Reward on eval: 2060.407333441125[0m
[37m[1m[2023-07-10 21:15:13,181][227910] Min Reward on eval: -1515.1761585033848[0m
[37m[1m[2023-07-10 21:15:13,182][227910] Mean Reward across all agents: -71.89962207922225[0m
[37m[1m[2023-07-10 21:15:13,182][227910] Average Trajectory Length: 976.0656666666666[0m
[36m[2023-07-10 21:15:13,184][227910] mean_value=-903.6727886291771, max_value=647.8648869549479[0m
[37m[1m[2023-07-10 21:15:13,187][227910] New mean coefficients: [[ 2.198052   -0.5644889   2.385553    0.27316266 -0.55151993]][0m
[37m[1m[2023-07-10 21:15:13,187][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:15:22,926][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 21:15:22,926][227910] FPS: 394395.21[0m
[36m[2023-07-10 21:15:22,928][227910] itr=1369, itrs=2000, Progress: 68.45%[0m
[36m[2023-07-10 21:15:34,459][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 21:15:34,459][227910] FPS: 333564.68[0m
[36m[2023-07-10 21:15:39,000][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:15:39,000][227910] Reward + Measures: [[1886.31125099    0.18595365    0.5549407     0.34245214    0.23170155]][0m
[37m[1m[2023-07-10 21:15:39,000][227910] Max Reward on eval: 1886.3112509908467[0m
[37m[1m[2023-07-10 21:15:39,001][227910] Min Reward on eval: 1886.3112509908467[0m
[37m[1m[2023-07-10 21:15:39,001][227910] Mean Reward across all agents: 1886.3112509908467[0m
[37m[1m[2023-07-10 21:15:39,001][227910] Average Trajectory Length: 998.0006666666667[0m
[36m[2023-07-10 21:15:44,487][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:15:44,539][227910] Reward + Measures: [[1225.4563538     0.25960001    0.56280005    0.36470002    0.2538    ]
 [1110.53279497    0.21830001    0.54549998    0.36000001    0.23309998]
 [ 135.77477528    0.25260001    0.60129994    0.37579998    0.54000002]
 ...
 [-195.11737124    0.38921377    0.51800919    0.38657615    0.39495778]
 [-173.16667528    0.23126756    0.45505887    0.23887034    0.42970282]
 [ 922.09086177    0.37409997    0.53430003    0.48599997    0.2177    ]][0m
[37m[1m[2023-07-10 21:15:44,539][227910] Max Reward on eval: 1955.9071733075193[0m
[37m[1m[2023-07-10 21:15:44,539][227910] Min Reward on eval: -707.0850098982337[0m
[37m[1m[2023-07-10 21:15:44,540][227910] Mean Reward across all agents: 830.3323077882851[0m
[37m[1m[2023-07-10 21:15:44,540][227910] Average Trajectory Length: 992.7786666666666[0m
[36m[2023-07-10 21:15:44,542][227910] mean_value=-1027.1836551738395, max_value=937.114959493216[0m
[37m[1m[2023-07-10 21:15:44,544][227910] New mean coefficients: [[ 2.2680469  -0.41287625  1.8180255   0.38787627 -0.8923476 ]][0m
[37m[1m[2023-07-10 21:15:44,545][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:15:54,321][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 21:15:54,322][227910] FPS: 392851.32[0m
[36m[2023-07-10 21:15:54,324][227910] itr=1370, itrs=2000, Progress: 68.50%[0m
[37m[1m[2023-07-10 21:15:58,363][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001350[0m
[36m[2023-07-10 21:16:10,108][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 21:16:10,108][227910] FPS: 334645.36[0m
[36m[2023-07-10 21:16:14,893][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:16:14,894][227910] Reward + Measures: [[1994.56476271    0.18207222    0.57748133    0.34204915    0.22834775]][0m
[37m[1m[2023-07-10 21:16:14,894][227910] Max Reward on eval: 1994.564762710501[0m
[37m[1m[2023-07-10 21:16:14,894][227910] Min Reward on eval: 1994.564762710501[0m
[37m[1m[2023-07-10 21:16:14,894][227910] Mean Reward across all agents: 1994.564762710501[0m
[37m[1m[2023-07-10 21:16:14,895][227910] Average Trajectory Length: 999.4413333333333[0m
[36m[2023-07-10 21:16:20,418][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:16:20,419][227910] Reward + Measures: [[ 500.72244512    0.4598        0.29879999    0.5068        0.2493    ]
 [-168.93370225    0.39190003    0.30840001    0.35310003    0.21780001]
 [1241.30731224    0.1868        0.50080007    0.24880002    0.252     ]
 ...
 [ 481.11411002    0.28210533    0.49206337    0.34211415    0.32356939]
 [ 525.36987509    0.20739999    0.62440002    0.3865        0.4804    ]
 [ 919.35903849    0.19130002    0.48499998    0.25759998    0.2014    ]][0m
[37m[1m[2023-07-10 21:16:20,419][227910] Max Reward on eval: 1968.650363987917[0m
[37m[1m[2023-07-10 21:16:20,419][227910] Min Reward on eval: -296.00496734234036[0m
[37m[1m[2023-07-10 21:16:20,419][227910] Mean Reward across all agents: 924.4445434323524[0m
[37m[1m[2023-07-10 21:16:20,420][227910] Average Trajectory Length: 982.3056666666666[0m
[36m[2023-07-10 21:16:20,422][227910] mean_value=-1863.2001349539078, max_value=1049.2620981119699[0m
[37m[1m[2023-07-10 21:16:20,424][227910] New mean coefficients: [[ 2.5522723  -0.73519385  2.2596714   0.00025818 -0.38132983]][0m
[37m[1m[2023-07-10 21:16:20,425][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:16:30,200][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 21:16:30,201][227910] FPS: 392894.58[0m
[36m[2023-07-10 21:16:30,203][227910] itr=1371, itrs=2000, Progress: 68.55%[0m
[36m[2023-07-10 21:16:41,757][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 21:16:41,757][227910] FPS: 332891.34[0m
[36m[2023-07-10 21:16:46,541][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:16:46,542][227910] Reward + Measures: [[2088.25421852    0.1771276     0.59401023    0.34736532    0.22568056]][0m
[37m[1m[2023-07-10 21:16:46,542][227910] Max Reward on eval: 2088.254218520681[0m
[37m[1m[2023-07-10 21:16:46,542][227910] Min Reward on eval: 2088.254218520681[0m
[37m[1m[2023-07-10 21:16:46,543][227910] Mean Reward across all agents: 2088.254218520681[0m
[37m[1m[2023-07-10 21:16:46,543][227910] Average Trajectory Length: 999.2936666666666[0m
[36m[2023-07-10 21:16:52,036][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:16:52,036][227910] Reward + Measures: [[-207.22764454    0.57300001    0.40079999    0.62230003    0.55260003]
 [1406.26838172    0.14966404    0.46765557    0.28278235    0.20972483]
 [ 130.97534297    0.41744429    0.52174681    0.4874962     0.43835315]
 ...
 [-245.98857503    0.3766        0.15010001    0.4025        0.22880001]
 [ 362.84064343    0.13827793    0.29285583    0.23576753    0.13068703]
 [1621.39332415    0.1921        0.56120002    0.33450004    0.2402    ]][0m
[37m[1m[2023-07-10 21:16:52,037][227910] Max Reward on eval: 2072.476976094302[0m
[37m[1m[2023-07-10 21:16:52,037][227910] Min Reward on eval: -1131.7582135286532[0m
[37m[1m[2023-07-10 21:16:52,037][227910] Mean Reward across all agents: 1224.4941436530385[0m
[37m[1m[2023-07-10 21:16:52,037][227910] Average Trajectory Length: 990.39[0m
[36m[2023-07-10 21:16:52,039][227910] mean_value=-1259.7184730798765, max_value=290.96140675472554[0m
[37m[1m[2023-07-10 21:16:52,042][227910] New mean coefficients: [[ 2.2241063  -0.64717835  2.4115157  -0.11962543  0.3146823 ]][0m
[37m[1m[2023-07-10 21:16:52,042][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:17:01,846][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 21:17:01,846][227910] FPS: 391765.25[0m
[36m[2023-07-10 21:17:01,848][227910] itr=1372, itrs=2000, Progress: 68.60%[0m
[36m[2023-07-10 21:17:13,543][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 21:17:13,544][227910] FPS: 328879.13[0m
[36m[2023-07-10 21:17:18,286][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:17:18,287][227910] Reward + Measures: [[2179.00677959    0.17117256    0.61530983    0.33696276    0.21845955]][0m
[37m[1m[2023-07-10 21:17:18,287][227910] Max Reward on eval: 2179.0067795941204[0m
[37m[1m[2023-07-10 21:17:18,287][227910] Min Reward on eval: 2179.0067795941204[0m
[37m[1m[2023-07-10 21:17:18,288][227910] Mean Reward across all agents: 2179.0067795941204[0m
[37m[1m[2023-07-10 21:17:18,288][227910] Average Trajectory Length: 999.087[0m
[36m[2023-07-10 21:17:23,774][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:17:23,775][227910] Reward + Measures: [[1438.68644943    0.1442        0.61070001    0.33319998    0.2624    ]
 [1803.44515579    0.15949999    0.62110007    0.35730001    0.25690001]
 [1435.17363281    0.2009        0.54100007    0.1919        0.17840001]
 ...
 [1706.54019991    0.20669451    0.54134291    0.23384264    0.18689953]
 [1280.71114463    0.17759693    0.52881283    0.27421165    0.2127708 ]
 [1809.8362249     0.1913        0.52149999    0.32460001    0.2131    ]][0m
[37m[1m[2023-07-10 21:17:23,775][227910] Max Reward on eval: 2240.603984227916[0m
[37m[1m[2023-07-10 21:17:23,775][227910] Min Reward on eval: 226.9965310976375[0m
[37m[1m[2023-07-10 21:17:23,775][227910] Mean Reward across all agents: 1472.9963411541503[0m
[37m[1m[2023-07-10 21:17:23,776][227910] Average Trajectory Length: 969.7786666666666[0m
[36m[2023-07-10 21:17:23,777][227910] mean_value=-1840.6285748103094, max_value=2212.215497266954[0m
[37m[1m[2023-07-10 21:17:23,779][227910] New mean coefficients: [[ 1.4704294  -0.21366838  2.6020527  -0.15036649  0.56268054]][0m
[37m[1m[2023-07-10 21:17:23,780][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:17:33,665][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 21:17:33,666][227910] FPS: 388529.29[0m
[36m[2023-07-10 21:17:33,668][227910] itr=1373, itrs=2000, Progress: 68.65%[0m
[36m[2023-07-10 21:17:45,447][227910] train() took 11.76 seconds to complete[0m
[36m[2023-07-10 21:17:45,447][227910] FPS: 326599.57[0m
[36m[2023-07-10 21:17:50,105][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:17:50,106][227910] Reward + Measures: [[2242.40670148    0.168028      0.63700271    0.34065735    0.21997933]][0m
[37m[1m[2023-07-10 21:17:50,106][227910] Max Reward on eval: 2242.4067014787443[0m
[37m[1m[2023-07-10 21:17:50,106][227910] Min Reward on eval: 2242.4067014787443[0m
[37m[1m[2023-07-10 21:17:50,107][227910] Mean Reward across all agents: 2242.4067014787443[0m
[37m[1m[2023-07-10 21:17:50,107][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:17:55,534][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:17:55,534][227910] Reward + Measures: [[1040.09720769    0.12160001    0.75209999    0.47270003    0.45860004]
 [1734.33593514    0.20200001    0.57010001    0.35159999    0.2457    ]
 [ -97.09661556    0.22760001    0.79580003    0.18050002    0.72149998]
 ...
 [2083.15191944    0.15989999    0.63889998    0.36680001    0.21970001]
 [   5.34649343    0.12119999    0.76809996    0.44509998    0.60360003]
 [ 474.27324122    0.0536        0.83240002    0.50660008    0.60659999]][0m
[37m[1m[2023-07-10 21:17:55,535][227910] Max Reward on eval: 2269.173834261298[0m
[37m[1m[2023-07-10 21:17:55,535][227910] Min Reward on eval: -258.41731852000373[0m
[37m[1m[2023-07-10 21:17:55,535][227910] Mean Reward across all agents: 1344.4151187582634[0m
[37m[1m[2023-07-10 21:17:55,535][227910] Average Trajectory Length: 999.3056666666666[0m
[36m[2023-07-10 21:17:55,539][227910] mean_value=-228.5823301119581, max_value=2009.3585425013098[0m
[37m[1m[2023-07-10 21:17:55,542][227910] New mean coefficients: [[0.8724319  0.07226816 2.362399   0.02128957 0.7792115 ]][0m
[37m[1m[2023-07-10 21:17:55,543][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:18:05,294][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 21:18:05,294][227910] FPS: 393857.30[0m
[36m[2023-07-10 21:18:05,297][227910] itr=1374, itrs=2000, Progress: 68.70%[0m
[36m[2023-07-10 21:18:16,792][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 21:18:16,792][227910] FPS: 334621.49[0m
[36m[2023-07-10 21:18:21,599][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:18:21,600][227910] Reward + Measures: [[2279.38631689    0.1627674     0.66557109    0.3404032     0.22329071]][0m
[37m[1m[2023-07-10 21:18:21,600][227910] Max Reward on eval: 2279.386316885184[0m
[37m[1m[2023-07-10 21:18:21,600][227910] Min Reward on eval: 2279.386316885184[0m
[37m[1m[2023-07-10 21:18:21,600][227910] Mean Reward across all agents: 2279.386316885184[0m
[37m[1m[2023-07-10 21:18:21,601][227910] Average Trajectory Length: 999.7513333333333[0m
[36m[2023-07-10 21:18:27,135][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:18:27,135][227910] Reward + Measures: [[1447.67967545    0.26190001    0.48070002    0.4219        0.1895    ]
 [2032.81989094    0.1699        0.62730002    0.35900003    0.2538    ]
 [1632.28902156    0.18779999    0.56370002    0.3159        0.22690001]
 ...
 [1952.4127866     0.20550001    0.5984        0.3845        0.19230001]
 [1481.69003054    0.20390001    0.46810004    0.3423        0.2251    ]
 [1742.31311947    0.18030001    0.49119997    0.3378        0.1911    ]][0m
[37m[1m[2023-07-10 21:18:27,135][227910] Max Reward on eval: 2283.150987376715[0m
[37m[1m[2023-07-10 21:18:27,136][227910] Min Reward on eval: -205.76760469883448[0m
[37m[1m[2023-07-10 21:18:27,136][227910] Mean Reward across all agents: 1614.6091186940866[0m
[37m[1m[2023-07-10 21:18:27,136][227910] Average Trajectory Length: 994.2563333333333[0m
[36m[2023-07-10 21:18:27,138][227910] mean_value=-728.5618363614556, max_value=2674.5859809193294[0m
[37m[1m[2023-07-10 21:18:27,141][227910] New mean coefficients: [[ 1.2591934  -0.06901725  2.1523457  -0.10123862  0.48394504]][0m
[37m[1m[2023-07-10 21:18:27,142][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:18:36,846][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 21:18:36,846][227910] FPS: 395786.64[0m
[36m[2023-07-10 21:18:36,848][227910] itr=1375, itrs=2000, Progress: 68.75%[0m
[36m[2023-07-10 21:18:48,407][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 21:18:48,407][227910] FPS: 332758.07[0m
[36m[2023-07-10 21:18:53,093][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:18:53,093][227910] Reward + Measures: [[2345.45470245    0.15689339    0.68314397    0.33155501    0.21916293]][0m
[37m[1m[2023-07-10 21:18:53,094][227910] Max Reward on eval: 2345.4547024534054[0m
[37m[1m[2023-07-10 21:18:53,094][227910] Min Reward on eval: 2345.4547024534054[0m
[37m[1m[2023-07-10 21:18:53,094][227910] Mean Reward across all agents: 2345.4547024534054[0m
[37m[1m[2023-07-10 21:18:53,094][227910] Average Trajectory Length: 999.5653333333333[0m
[36m[2023-07-10 21:18:58,786][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:18:58,787][227910] Reward + Measures: [[1099.74919662    0.17830001    0.72210002    0.40939999    0.42120001]
 [-774.33635956    0.91289997    0.58109999    0.86860001    0.07380001]
 [1587.65359331    0.1257        0.69849998    0.37380004    0.31830001]
 ...
 [1913.71473873    0.15799999    0.71810001    0.3251        0.2323    ]
 [  32.82081502    0.06820001    0.91009998    0.55179995    0.87190002]
 [1628.73245142    0.15697342    0.55335313    0.25491649    0.27424812]][0m
[37m[1m[2023-07-10 21:18:58,787][227910] Max Reward on eval: 2368.717950120289[0m
[37m[1m[2023-07-10 21:18:58,787][227910] Min Reward on eval: -774.3363595592789[0m
[37m[1m[2023-07-10 21:18:58,787][227910] Mean Reward across all agents: 1473.1540051652753[0m
[37m[1m[2023-07-10 21:18:58,788][227910] Average Trajectory Length: 994.7886666666666[0m
[36m[2023-07-10 21:18:58,791][227910] mean_value=-317.63248806132947, max_value=1732.1775045376012[0m
[37m[1m[2023-07-10 21:18:58,794][227910] New mean coefficients: [[1.1315902  0.02539371 1.7602364  0.22633255 0.49036244]][0m
[37m[1m[2023-07-10 21:18:58,795][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:19:08,573][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 21:19:08,573][227910] FPS: 392780.56[0m
[36m[2023-07-10 21:19:08,576][227910] itr=1376, itrs=2000, Progress: 68.80%[0m
[36m[2023-07-10 21:19:20,260][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 21:19:20,260][227910] FPS: 329274.45[0m
[36m[2023-07-10 21:19:25,084][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:19:25,085][227910] Reward + Measures: [[2061.15904448    0.16345356    0.67845875    0.2900444     0.19661078]][0m
[37m[1m[2023-07-10 21:19:25,085][227910] Max Reward on eval: 2061.1590444753574[0m
[37m[1m[2023-07-10 21:19:25,085][227910] Min Reward on eval: 2061.1590444753574[0m
[37m[1m[2023-07-10 21:19:25,085][227910] Mean Reward across all agents: 2061.1590444753574[0m
[37m[1m[2023-07-10 21:19:25,085][227910] Average Trajectory Length: 995.7536666666666[0m
[36m[2023-07-10 21:19:30,604][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:19:30,604][227910] Reward + Measures: [[ 874.69133274    0.22167639    0.56349814    0.28731304    0.36898762]
 [2003.18345647    0.15760002    0.68670005    0.36499998    0.2397    ]
 [1995.47124445    0.18740001    0.6602        0.34209999    0.26279998]
 ...
 [1651.22030908    0.16510001    0.6117        0.32820001    0.22549999]
 [ 813.40913185    0.27869999    0.67620003    0.1869        0.34329998]
 [1384.49899731    0.22909999    0.53039998    0.36999997    0.35410002]][0m
[37m[1m[2023-07-10 21:19:30,605][227910] Max Reward on eval: 2196.5853686473565[0m
[37m[1m[2023-07-10 21:19:30,605][227910] Min Reward on eval: -16.805471517011757[0m
[37m[1m[2023-07-10 21:19:30,605][227910] Mean Reward across all agents: 1638.8615155639704[0m
[37m[1m[2023-07-10 21:19:30,605][227910] Average Trajectory Length: 992.2603333333333[0m
[36m[2023-07-10 21:19:30,608][227910] mean_value=-440.2552061265595, max_value=1157.9739545694076[0m
[37m[1m[2023-07-10 21:19:30,610][227910] New mean coefficients: [[0.42740458 0.17850448 1.9062866  0.16610393 0.29549682]][0m
[37m[1m[2023-07-10 21:19:30,611][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:19:40,435][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 21:19:40,435][227910] FPS: 390967.04[0m
[36m[2023-07-10 21:19:40,437][227910] itr=1377, itrs=2000, Progress: 68.85%[0m
[36m[2023-07-10 21:19:51,966][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 21:19:51,967][227910] FPS: 333695.84[0m
[36m[2023-07-10 21:19:56,798][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:19:56,798][227910] Reward + Measures: [[1188.30076362    0.1961118     0.50771552    0.22808306    0.18381193]][0m
[37m[1m[2023-07-10 21:19:56,798][227910] Max Reward on eval: 1188.3007636193968[0m
[37m[1m[2023-07-10 21:19:56,799][227910] Min Reward on eval: 1188.3007636193968[0m
[37m[1m[2023-07-10 21:19:56,799][227910] Mean Reward across all agents: 1188.3007636193968[0m
[37m[1m[2023-07-10 21:19:56,799][227910] Average Trajectory Length: 987.4936666666666[0m
[36m[2023-07-10 21:20:02,313][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:20:02,314][227910] Reward + Measures: [[ 400.09725695    0.52150005    0.58629996    0.52570003    0.1751    ]
 [ 542.35919495    0.55509996    0.62600005    0.57570004    0.26480001]
 [1279.92214187    0.20100001    0.47150001    0.31380001    0.1908    ]
 ...
 [ 145.83579707    0.54040003    0.70810002    0.49670002    0.5535    ]
 [  60.99230575    0.61000001    0.76159996    0.33829999    0.60050005]
 [1128.30872852    0.23540001    0.45570001    0.26470003    0.1892    ]][0m
[37m[1m[2023-07-10 21:20:02,314][227910] Max Reward on eval: 1401.2947766056052[0m
[37m[1m[2023-07-10 21:20:02,314][227910] Min Reward on eval: -594.2815155183314[0m
[37m[1m[2023-07-10 21:20:02,315][227910] Mean Reward across all agents: 704.9325260455242[0m
[37m[1m[2023-07-10 21:20:02,315][227910] Average Trajectory Length: 973.7316666666667[0m
[36m[2023-07-10 21:20:02,318][227910] mean_value=-1983.3537111994156, max_value=796.8434096851638[0m
[37m[1m[2023-07-10 21:20:02,320][227910] New mean coefficients: [[ 0.7704686   0.306599    2.1099012  -0.08776802  0.9087199 ]][0m
[37m[1m[2023-07-10 21:20:02,321][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:20:11,912][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 21:20:11,912][227910] FPS: 400476.65[0m
[36m[2023-07-10 21:20:11,914][227910] itr=1378, itrs=2000, Progress: 68.90%[0m
[36m[2023-07-10 21:20:23,399][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 21:20:23,399][227910] FPS: 335004.17[0m
[36m[2023-07-10 21:20:28,054][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:20:28,054][227910] Reward + Measures: [[176.7107003    0.72860211   0.6498524    0.79561704   0.74195433]][0m
[37m[1m[2023-07-10 21:20:28,055][227910] Max Reward on eval: 176.71070030387241[0m
[37m[1m[2023-07-10 21:20:28,055][227910] Min Reward on eval: 176.71070030387241[0m
[37m[1m[2023-07-10 21:20:28,055][227910] Mean Reward across all agents: 176.71070030387241[0m
[37m[1m[2023-07-10 21:20:28,056][227910] Average Trajectory Length: 996.1156666666666[0m
[36m[2023-07-10 21:20:33,472][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:20:33,473][227910] Reward + Measures: [[ 134.97713365    0.87280005    0.14740001    0.81610006    0.71279997]
 [ 183.09763129    0.89140004    0.0966        0.86059999    0.78890002]
 [ 203.21864028    0.59240001    0.4034        0.69879997    0.52250004]
 ...
 [-451.37897657    0.266         0.30520001    0.1762        0.0821    ]
 [ -40.55911765    0.47730002    0.9684        0.52959996    0.94960004]
 [  93.71990124    0.94530004    0.0357        0.9429        0.93330002]][0m
[37m[1m[2023-07-10 21:20:33,473][227910] Max Reward on eval: 396.8347208663996[0m
[37m[1m[2023-07-10 21:20:33,473][227910] Min Reward on eval: -664.5736689009238[0m
[37m[1m[2023-07-10 21:20:33,474][227910] Mean Reward across all agents: -5.431882791255849[0m
[37m[1m[2023-07-10 21:20:33,474][227910] Average Trajectory Length: 960.3336666666667[0m
[36m[2023-07-10 21:20:33,478][227910] mean_value=-894.6521325833822, max_value=708.6899885631049[0m
[37m[1m[2023-07-10 21:20:33,480][227910] New mean coefficients: [[ 1.3027747   0.3189373   1.9539196  -0.22364914  1.0060449 ]][0m
[37m[1m[2023-07-10 21:20:33,481][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:20:43,162][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 21:20:43,168][227910] FPS: 396729.51[0m
[36m[2023-07-10 21:20:43,170][227910] itr=1379, itrs=2000, Progress: 68.95%[0m
[36m[2023-07-10 21:20:54,654][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 21:20:54,654][227910] FPS: 334919.70[0m
[36m[2023-07-10 21:20:59,470][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:20:59,471][227910] Reward + Measures: [[121.87884792   0.76456606   0.86520374   0.84314233   0.88214141]][0m
[37m[1m[2023-07-10 21:20:59,471][227910] Max Reward on eval: 121.87884792329872[0m
[37m[1m[2023-07-10 21:20:59,471][227910] Min Reward on eval: 121.87884792329872[0m
[37m[1m[2023-07-10 21:20:59,471][227910] Mean Reward across all agents: 121.87884792329872[0m
[37m[1m[2023-07-10 21:20:59,472][227910] Average Trajectory Length: 996.1036666666666[0m
[36m[2023-07-10 21:21:05,061][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:21:05,062][227910] Reward + Measures: [[707.76901956   0.32470003   0.6045       0.31979999   0.308     ]
 [535.45466407   0.44330001   0.67530006   0.44779998   0.51499999]
 [551.6094587    0.25419998   0.68940002   0.42430001   0.5327    ]
 ...
 [242.29410792   0.35820004   0.49850002   0.2667       0.26010001]
 [650.42331813   0.31730002   0.60400003   0.2572       0.35960001]
 [245.91368896   0.59170002   0.74430001   0.44240004   0.64950007]][0m
[37m[1m[2023-07-10 21:21:05,062][227910] Max Reward on eval: 947.4131106086425[0m
[37m[1m[2023-07-10 21:21:05,062][227910] Min Reward on eval: -672.1629427943378[0m
[37m[1m[2023-07-10 21:21:05,062][227910] Mean Reward across all agents: 234.87233632319732[0m
[37m[1m[2023-07-10 21:21:05,063][227910] Average Trajectory Length: 998.9683333333332[0m
[36m[2023-07-10 21:21:05,065][227910] mean_value=-897.41808214036, max_value=589.664292632998[0m
[37m[1m[2023-07-10 21:21:05,067][227910] New mean coefficients: [[ 0.7841374   0.23301071  1.5903099  -0.19617596  0.7662786 ]][0m
[37m[1m[2023-07-10 21:21:05,068][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:21:14,712][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 21:21:14,712][227910] FPS: 398253.78[0m
[36m[2023-07-10 21:21:14,714][227910] itr=1380, itrs=2000, Progress: 69.00%[0m
[37m[1m[2023-07-10 21:21:18,701][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001360[0m
[36m[2023-07-10 21:21:30,502][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 21:21:30,503][227910] FPS: 333113.57[0m
[36m[2023-07-10 21:21:35,352][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:21:35,353][227910] Reward + Measures: [[98.41838638  0.58801186  0.92199314  0.74944043  0.90177202]][0m
[37m[1m[2023-07-10 21:21:35,353][227910] Max Reward on eval: 98.41838637858304[0m
[37m[1m[2023-07-10 21:21:35,353][227910] Min Reward on eval: 98.41838637858304[0m
[37m[1m[2023-07-10 21:21:35,353][227910] Mean Reward across all agents: 98.41838637858304[0m
[37m[1m[2023-07-10 21:21:35,353][227910] Average Trajectory Length: 997.6863333333333[0m
[36m[2023-07-10 21:21:40,792][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:21:40,793][227910] Reward + Measures: [[-614.27717283    0.67628783    0.27453673    0.72401017    0.67410821]
 [-210.79778769    0.95710003    0.4501        0.95769995    0.94670004]
 [-221.5481566     0.93069994    0.75170004    0.93689996    0.92430001]
 ...
 [  36.40898886    0.95350009    0.77710003    0.95219994    0.11199999]
 [-522.18703333    0.55430001    0.29379997    0.63560003    0.24390002]
 [-473.43986681    0.50550002    0.68610001    0.82019997    0.87810004]][0m
[37m[1m[2023-07-10 21:21:40,793][227910] Max Reward on eval: 714.4144386185566[0m
[37m[1m[2023-07-10 21:21:40,794][227910] Min Reward on eval: -1236.5402073131875[0m
[37m[1m[2023-07-10 21:21:40,794][227910] Mean Reward across all agents: -280.696194659116[0m
[37m[1m[2023-07-10 21:21:40,794][227910] Average Trajectory Length: 953.9436666666667[0m
[36m[2023-07-10 21:21:40,798][227910] mean_value=-495.79144265861726, max_value=625.4882857602765[0m
[37m[1m[2023-07-10 21:21:40,800][227910] New mean coefficients: [[ 0.09731293  0.20845824  1.6778305  -0.03918414  0.12105423]][0m
[37m[1m[2023-07-10 21:21:40,801][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:21:50,491][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 21:21:50,491][227910] FPS: 396364.30[0m
[36m[2023-07-10 21:21:50,494][227910] itr=1381, itrs=2000, Progress: 69.05%[0m
[36m[2023-07-10 21:22:01,991][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 21:22:01,991][227910] FPS: 334521.17[0m
[36m[2023-07-10 21:22:06,706][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:22:06,706][227910] Reward + Measures: [[101.07184717   0.88789946   0.92632532   0.90773284   0.90802503]][0m
[37m[1m[2023-07-10 21:22:06,707][227910] Max Reward on eval: 101.07184717289708[0m
[37m[1m[2023-07-10 21:22:06,707][227910] Min Reward on eval: 101.07184717289708[0m
[37m[1m[2023-07-10 21:22:06,707][227910] Mean Reward across all agents: 101.07184717289708[0m
[37m[1m[2023-07-10 21:22:06,707][227910] Average Trajectory Length: 997.895[0m
[36m[2023-07-10 21:22:12,311][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:22:12,312][227910] Reward + Measures: [[-525.91081978    0.28959998    0.94510001    0.66590005    0.92570001]
 [  12.05917829    0.76069993    0.67519999    0.74669999    0.1305    ]
 [-216.845577      0.46680003    0.93419999    0.53660005    0.9259001 ]
 ...
 [ -81.33486291    0.62279999    0.5431        0.68230003    0.1948    ]
 [ -12.16225935    0.8524        0.78369999    0.85190004    0.87880003]
 [  37.84813046    0.75309992    0.45560002    0.75240004    0.2313    ]][0m
[37m[1m[2023-07-10 21:22:12,312][227910] Max Reward on eval: 569.9480268440791[0m
[37m[1m[2023-07-10 21:22:12,312][227910] Min Reward on eval: -739.8692323022173[0m
[37m[1m[2023-07-10 21:22:12,312][227910] Mean Reward across all agents: 46.015144141911044[0m
[37m[1m[2023-07-10 21:22:12,313][227910] Average Trajectory Length: 989.608[0m
[36m[2023-07-10 21:22:12,319][227910] mean_value=-279.6298452336668, max_value=821.0559708986199[0m
[37m[1m[2023-07-10 21:22:12,322][227910] New mean coefficients: [[ 0.62002593  0.16385466  1.3171868   0.06022497 -0.19803119]][0m
[37m[1m[2023-07-10 21:22:12,323][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:22:21,989][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 21:22:21,989][227910] FPS: 397323.10[0m
[36m[2023-07-10 21:22:21,992][227910] itr=1382, itrs=2000, Progress: 69.10%[0m
[36m[2023-07-10 21:22:33,472][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 21:22:33,473][227910] FPS: 335126.28[0m
[36m[2023-07-10 21:22:38,148][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:22:38,148][227910] Reward + Measures: [[513.28356189   0.1222478    0.73307651   0.60847515   0.61740679]][0m
[37m[1m[2023-07-10 21:22:38,149][227910] Max Reward on eval: 513.2835618923897[0m
[37m[1m[2023-07-10 21:22:38,149][227910] Min Reward on eval: 513.2835618923897[0m
[37m[1m[2023-07-10 21:22:38,149][227910] Mean Reward across all agents: 513.2835618923897[0m
[37m[1m[2023-07-10 21:22:38,150][227910] Average Trajectory Length: 980.9573333333333[0m
[36m[2023-07-10 21:22:43,645][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:22:43,646][227910] Reward + Measures: [[942.63749868   0.23469999   0.51209998   0.29870003   0.24969999]
 [179.88811703   0.21596272   0.58307618   0.45345089   0.36602274]
 [305.51730542   0.23377509   0.61124116   0.50506318   0.37010938]
 ...
 [447.06327174   0.47919998   0.87719995   0.32440001   0.81999999]
 [844.61062958   0.29940003   0.6511001    0.26920003   0.45720002]
 [ 89.07417462   0.28400055   0.58914816   0.44501051   0.28203616]][0m
[37m[1m[2023-07-10 21:22:43,646][227910] Max Reward on eval: 1149.901887215185[0m
[37m[1m[2023-07-10 21:22:43,646][227910] Min Reward on eval: -498.9681434932398[0m
[37m[1m[2023-07-10 21:22:43,647][227910] Mean Reward across all agents: 433.9258216055553[0m
[37m[1m[2023-07-10 21:22:43,647][227910] Average Trajectory Length: 841.5196666666667[0m
[36m[2023-07-10 21:22:43,650][227910] mean_value=-1680.1920474246108, max_value=1057.915329183635[0m
[37m[1m[2023-07-10 21:22:43,653][227910] New mean coefficients: [[ 0.7560332   0.06114925  1.5134032  -0.03640966  0.37909794]][0m
[37m[1m[2023-07-10 21:22:43,654][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:22:53,550][227910] train() took 9.89 seconds to complete[0m
[36m[2023-07-10 21:22:53,551][227910] FPS: 388099.30[0m
[36m[2023-07-10 21:22:53,553][227910] itr=1383, itrs=2000, Progress: 69.15%[0m
[36m[2023-07-10 21:23:05,208][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 21:23:05,208][227910] FPS: 330054.12[0m
[36m[2023-07-10 21:23:10,041][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:23:10,042][227910] Reward + Measures: [[636.44279513   0.12698032   0.7088176    0.56674844   0.56672376]][0m
[37m[1m[2023-07-10 21:23:10,042][227910] Max Reward on eval: 636.4427951318901[0m
[37m[1m[2023-07-10 21:23:10,042][227910] Min Reward on eval: 636.4427951318901[0m
[37m[1m[2023-07-10 21:23:10,042][227910] Mean Reward across all agents: 636.4427951318901[0m
[37m[1m[2023-07-10 21:23:10,042][227910] Average Trajectory Length: 983.8156666666666[0m
[36m[2023-07-10 21:23:15,509][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:23:15,510][227910] Reward + Measures: [[ 649.38534142    0.1364        0.63900006    0.4684        0.46709999]
 [ 225.98675093    0.0376        0.94260007    0.91870004    0.9285    ]
 [ 967.95476359    0.18800589    0.4831059     0.22611175    0.20741177]
 ...
 [1285.87849951    0.18632115    0.45011958    0.21360445    0.21607101]
 [ 897.8331365     0.17389999    0.46070004    0.23719998    0.23410001]
 [ 498.98732804    0.1019        0.77590001    0.66890001    0.67430001]][0m
[37m[1m[2023-07-10 21:23:15,510][227910] Max Reward on eval: 1285.878499508067[0m
[37m[1m[2023-07-10 21:23:15,510][227910] Min Reward on eval: 41.90874887178652[0m
[37m[1m[2023-07-10 21:23:15,510][227910] Mean Reward across all agents: 642.4052279214385[0m
[37m[1m[2023-07-10 21:23:15,511][227910] Average Trajectory Length: 970.4356666666666[0m
[36m[2023-07-10 21:23:15,514][227910] mean_value=-949.2227890268125, max_value=774.1727366442997[0m
[37m[1m[2023-07-10 21:23:15,517][227910] New mean coefficients: [[ 0.5429597   0.06243572  0.9126378  -0.38349515  0.2748255 ]][0m
[37m[1m[2023-07-10 21:23:15,518][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:23:25,258][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 21:23:25,258][227910] FPS: 394330.86[0m
[36m[2023-07-10 21:23:25,260][227910] itr=1384, itrs=2000, Progress: 69.20%[0m
[36m[2023-07-10 21:23:36,688][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 21:23:36,689][227910] FPS: 336665.12[0m
[36m[2023-07-10 21:23:41,492][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:23:41,492][227910] Reward + Measures: [[722.22485136   0.12022994   0.7220065    0.55524349   0.57217991]][0m
[37m[1m[2023-07-10 21:23:41,493][227910] Max Reward on eval: 722.2248513605784[0m
[37m[1m[2023-07-10 21:23:41,493][227910] Min Reward on eval: 722.2248513605784[0m
[37m[1m[2023-07-10 21:23:41,493][227910] Mean Reward across all agents: 722.2248513605784[0m
[37m[1m[2023-07-10 21:23:41,493][227910] Average Trajectory Length: 982.866[0m
[36m[2023-07-10 21:23:47,043][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:23:47,044][227910] Reward + Measures: [[ 611.47316007    0.1026        0.80599993    0.60149997    0.6972    ]
 [ 247.61832211    0.0665        0.88630003    0.71579999    0.83960003]
 [ 298.842254      0.0816        0.80219996    0.60249996    0.73079997]
 ...
 [ 175.53697913    0.1015        0.89459991    0.74909997    0.88549995]
 [ -59.45337838    0.32030001    0.86930001    0.76669997    0.91530001]
 [1025.47718702    0.2278        0.42150003    0.35010001    0.24319999]][0m
[37m[1m[2023-07-10 21:23:47,044][227910] Max Reward on eval: 1442.5051856393227[0m
[37m[1m[2023-07-10 21:23:47,044][227910] Min Reward on eval: -226.5699049429386[0m
[37m[1m[2023-07-10 21:23:47,045][227910] Mean Reward across all agents: 520.9482006274388[0m
[37m[1m[2023-07-10 21:23:47,045][227910] Average Trajectory Length: 975.226[0m
[36m[2023-07-10 21:23:47,050][227910] mean_value=-588.8599840494072, max_value=759.5179992392909[0m
[37m[1m[2023-07-10 21:23:47,053][227910] New mean coefficients: [[ 0.43555635 -0.34400415  0.58484447 -0.35992002  0.22750014]][0m
[37m[1m[2023-07-10 21:23:47,054][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:23:56,773][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 21:23:56,774][227910] FPS: 395137.50[0m
[36m[2023-07-10 21:23:56,776][227910] itr=1385, itrs=2000, Progress: 69.25%[0m
[36m[2023-07-10 21:24:08,353][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 21:24:08,353][227910] FPS: 332248.33[0m
[36m[2023-07-10 21:24:12,930][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:24:12,930][227910] Reward + Measures: [[782.36459518   0.16418603   0.53326827   0.32752392   0.37498355]][0m
[37m[1m[2023-07-10 21:24:12,931][227910] Max Reward on eval: 782.3645951812344[0m
[37m[1m[2023-07-10 21:24:12,931][227910] Min Reward on eval: 782.3645951812344[0m
[37m[1m[2023-07-10 21:24:12,931][227910] Mean Reward across all agents: 782.3645951812344[0m
[37m[1m[2023-07-10 21:24:12,931][227910] Average Trajectory Length: 989.6896666666667[0m
[36m[2023-07-10 21:24:18,448][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:24:18,448][227910] Reward + Measures: [[336.47516836   0.0864       0.75560004   0.60890001   0.72070003]
 [986.30086427   0.1767       0.50030005   0.29790002   0.31990001]
 [709.0238162    0.2066       0.55089998   0.257        0.34029999]
 ...
 [673.74938357   0.1816       0.47989997   0.2608       0.3197    ]
 [697.00704977   0.15229101   0.58547658   0.40814063   0.4379057 ]
 [375.25669841   0.1206       0.62860006   0.34880003   0.5941    ]][0m
[37m[1m[2023-07-10 21:24:18,448][227910] Max Reward on eval: 1095.3294150787988[0m
[37m[1m[2023-07-10 21:24:18,449][227910] Min Reward on eval: 209.55878330898705[0m
[37m[1m[2023-07-10 21:24:18,449][227910] Mean Reward across all agents: 708.0160866675567[0m
[37m[1m[2023-07-10 21:24:18,449][227910] Average Trajectory Length: 992.9756666666666[0m
[36m[2023-07-10 21:24:18,451][227910] mean_value=-909.7902466127942, max_value=267.011312283302[0m
[37m[1m[2023-07-10 21:24:18,453][227910] New mean coefficients: [[ 0.156923   -0.27828652  1.438524   -0.32636914 -0.31746584]][0m
[37m[1m[2023-07-10 21:24:18,454][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:24:28,238][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 21:24:28,239][227910] FPS: 392528.47[0m
[36m[2023-07-10 21:24:28,241][227910] itr=1386, itrs=2000, Progress: 69.30%[0m
[36m[2023-07-10 21:24:39,910][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 21:24:39,911][227910] FPS: 329634.81[0m
[36m[2023-07-10 21:24:44,713][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:24:44,714][227910] Reward + Measures: [[862.84839182   0.16286448   0.54379046   0.30443615   0.35343474]][0m
[37m[1m[2023-07-10 21:24:44,714][227910] Max Reward on eval: 862.8483918243858[0m
[37m[1m[2023-07-10 21:24:44,714][227910] Min Reward on eval: 862.8483918243858[0m
[37m[1m[2023-07-10 21:24:44,714][227910] Mean Reward across all agents: 862.8483918243858[0m
[37m[1m[2023-07-10 21:24:44,714][227910] Average Trajectory Length: 992.168[0m
[36m[2023-07-10 21:24:50,220][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:24:50,220][227910] Reward + Measures: [[742.24534594   0.1337       0.634        0.4219       0.4901    ]
 [945.91650716   0.17760001   0.52360004   0.29010001   0.30160001]
 [540.33072674   0.12330001   0.68189996   0.42200002   0.55419999]
 ...
 [739.05453089   0.15332036   0.58470511   0.32771528   0.39877963]
 [683.83859153   0.1596       0.56090003   0.30130002   0.3849    ]
 [628.41948122   0.15770002   0.57209998   0.3251       0.41159996]][0m
[37m[1m[2023-07-10 21:24:50,221][227910] Max Reward on eval: 1082.6416622624733[0m
[37m[1m[2023-07-10 21:24:50,221][227910] Min Reward on eval: 214.25946818155936[0m
[37m[1m[2023-07-10 21:24:50,221][227910] Mean Reward across all agents: 679.67252358506[0m
[37m[1m[2023-07-10 21:24:50,221][227910] Average Trajectory Length: 992.9226666666666[0m
[36m[2023-07-10 21:24:50,223][227910] mean_value=-680.3880887741173, max_value=487.59553775894744[0m
[37m[1m[2023-07-10 21:24:50,226][227910] New mean coefficients: [[-0.47609538 -0.30161244  2.6920972  -0.40479797 -0.34798545]][0m
[37m[1m[2023-07-10 21:24:50,227][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:24:59,946][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 21:24:59,946][227910] FPS: 395163.49[0m
[36m[2023-07-10 21:24:59,948][227910] itr=1387, itrs=2000, Progress: 69.35%[0m
[36m[2023-07-10 21:25:11,467][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 21:25:11,467][227910] FPS: 334013.38[0m
[36m[2023-07-10 21:25:16,334][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:25:16,334][227910] Reward + Measures: [[747.92026179   0.14691505   0.59839028   0.34038755   0.41930121]][0m
[37m[1m[2023-07-10 21:25:16,334][227910] Max Reward on eval: 747.9202617902858[0m
[37m[1m[2023-07-10 21:25:16,335][227910] Min Reward on eval: 747.9202617902858[0m
[37m[1m[2023-07-10 21:25:16,335][227910] Mean Reward across all agents: 747.9202617902858[0m
[37m[1m[2023-07-10 21:25:16,335][227910] Average Trajectory Length: 993.7679999999999[0m
[36m[2023-07-10 21:25:21,971][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:25:21,971][227910] Reward + Measures: [[308.77023755   0.0874       0.72200006   0.5205       0.65930003]
 [553.02502861   0.10039999   0.73570001   0.52240002   0.65799999]
 [485.64161947   0.14910001   0.58990002   0.34460002   0.45210001]
 ...
 [244.7160146    0.1337       0.59750003   0.39019999   0.48160002]
 [518.74671366   0.15530001   0.56490004   0.29100004   0.40279999]
 [875.80623104   0.19660001   0.5104       0.23529999   0.29679999]][0m
[37m[1m[2023-07-10 21:25:21,971][227910] Max Reward on eval: 905.3574724810256[0m
[37m[1m[2023-07-10 21:25:21,972][227910] Min Reward on eval: -359.31963928848273[0m
[37m[1m[2023-07-10 21:25:21,972][227910] Mean Reward across all agents: 392.8251430951841[0m
[37m[1m[2023-07-10 21:25:21,972][227910] Average Trajectory Length: 992.5603333333333[0m
[36m[2023-07-10 21:25:21,974][227910] mean_value=-396.66248465407176, max_value=340.197396344599[0m
[37m[1m[2023-07-10 21:25:21,977][227910] New mean coefficients: [[-0.98307127  0.08339986  2.7046578  -0.4032276  -0.539707  ]][0m
[37m[1m[2023-07-10 21:25:21,978][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:25:31,924][227910] train() took 9.94 seconds to complete[0m
[36m[2023-07-10 21:25:31,924][227910] FPS: 386145.21[0m
[36m[2023-07-10 21:25:31,926][227910] itr=1388, itrs=2000, Progress: 69.40%[0m
[36m[2023-07-10 21:25:43,417][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 21:25:43,418][227910] FPS: 334818.29[0m
[36m[2023-07-10 21:25:48,258][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:25:48,259][227910] Reward + Measures: [[367.40004576   0.14547887   0.5844363    0.19678158   0.34190351]][0m
[37m[1m[2023-07-10 21:25:48,259][227910] Max Reward on eval: 367.4000457565855[0m
[37m[1m[2023-07-10 21:25:48,259][227910] Min Reward on eval: 367.4000457565855[0m
[37m[1m[2023-07-10 21:25:48,259][227910] Mean Reward across all agents: 367.4000457565855[0m
[37m[1m[2023-07-10 21:25:48,260][227910] Average Trajectory Length: 996.3009999999999[0m
[36m[2023-07-10 21:25:53,730][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:25:53,731][227910] Reward + Measures: [[   9.54666837    0.3515        0.59450001    0.37200001    0.2474    ]
 [ 236.07504694    0.12539999    0.61840004    0.1683        0.41189995]
 [  22.59450774    0.1332        0.68230003    0.2339        0.5341    ]
 ...
 [ 196.97203475    0.14170001    0.61390001    0.1779        0.37420002]
 [ 157.80538622    0.1278        0.63169998    0.20630001    0.43249997]
 [-102.26052283    0.63880002    0.70319998    0.66300005    0.16779999]][0m
[37m[1m[2023-07-10 21:25:53,731][227910] Max Reward on eval: 484.07595560047196[0m
[37m[1m[2023-07-10 21:25:53,731][227910] Min Reward on eval: -922.0656762365601[0m
[37m[1m[2023-07-10 21:25:53,731][227910] Mean Reward across all agents: 68.03053075271387[0m
[37m[1m[2023-07-10 21:25:53,732][227910] Average Trajectory Length: 997.7816666666666[0m
[36m[2023-07-10 21:25:53,734][227910] mean_value=-671.2839169599256, max_value=821.3212715023371[0m
[37m[1m[2023-07-10 21:25:53,737][227910] New mean coefficients: [[-0.57314795 -0.2122117   2.1104488  -0.3703412  -0.8311516 ]][0m
[37m[1m[2023-07-10 21:25:53,738][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:26:03,449][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 21:26:03,450][227910] FPS: 395464.15[0m
[36m[2023-07-10 21:26:03,452][227910] itr=1389, itrs=2000, Progress: 69.45%[0m
[36m[2023-07-10 21:26:15,038][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 21:26:15,038][227910] FPS: 332069.76[0m
[36m[2023-07-10 21:26:19,766][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:26:19,766][227910] Reward + Measures: [[208.533508     0.1401305    0.62529927   0.20222636   0.38676074]][0m
[37m[1m[2023-07-10 21:26:19,766][227910] Max Reward on eval: 208.53350800304077[0m
[37m[1m[2023-07-10 21:26:19,767][227910] Min Reward on eval: 208.53350800304077[0m
[37m[1m[2023-07-10 21:26:19,767][227910] Mean Reward across all agents: 208.53350800304077[0m
[37m[1m[2023-07-10 21:26:19,767][227910] Average Trajectory Length: 998.2793333333333[0m
[36m[2023-07-10 21:26:25,162][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:26:25,163][227910] Reward + Measures: [[176.57674422   0.1356       0.65450001   0.23480001   0.42570001]
 [-64.40077513   0.15989999   0.60170001   0.23099999   0.43310004]
 [148.71304512   0.156        0.58820003   0.2079       0.33610001]
 ...
 [312.5680266    0.1437       0.59799999   0.17950001   0.36129999]
 [150.33582364   0.1437       0.6336       0.16000001   0.37310001]
 [252.32156809   0.1397       0.60570002   0.1737       0.3466    ]][0m
[37m[1m[2023-07-10 21:26:25,163][227910] Max Reward on eval: 343.2201772987959[0m
[37m[1m[2023-07-10 21:26:25,163][227910] Min Reward on eval: -64.4007751293946[0m
[37m[1m[2023-07-10 21:26:25,163][227910] Mean Reward across all agents: 181.08897013238152[0m
[37m[1m[2023-07-10 21:26:25,164][227910] Average Trajectory Length: 998.6256666666667[0m
[36m[2023-07-10 21:26:25,165][227910] mean_value=-882.5548174127281, max_value=712.3625886897935[0m
[37m[1m[2023-07-10 21:26:25,168][227910] New mean coefficients: [[-0.42966717 -0.31012115  2.1266422  -0.39974725 -0.72187555]][0m
[37m[1m[2023-07-10 21:26:25,169][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:26:34,788][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 21:26:34,789][227910] FPS: 399254.92[0m
[36m[2023-07-10 21:26:34,791][227910] itr=1390, itrs=2000, Progress: 69.50%[0m
[37m[1m[2023-07-10 21:26:38,811][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001370[0m
[36m[2023-07-10 21:26:50,635][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 21:26:50,636][227910] FPS: 332534.83[0m
[36m[2023-07-10 21:26:55,415][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:26:55,415][227910] Reward + Measures: [[99.73863087  0.13024032  0.66048819  0.19169198  0.42619348]][0m
[37m[1m[2023-07-10 21:26:55,415][227910] Max Reward on eval: 99.73863086762361[0m
[37m[1m[2023-07-10 21:26:55,416][227910] Min Reward on eval: 99.73863086762361[0m
[37m[1m[2023-07-10 21:26:55,416][227910] Mean Reward across all agents: 99.73863086762361[0m
[37m[1m[2023-07-10 21:26:55,416][227910] Average Trajectory Length: 998.6223333333332[0m
[36m[2023-07-10 21:27:00,881][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:27:00,882][227910] Reward + Measures: [[163.85897029   0.12900001   0.64529997   0.19160001   0.40419999]
 [ 11.74915524   0.12890001   0.66540003   0.1921       0.43529996]
 [-68.75930363   0.12639999   0.75600004   0.30700001   0.63519996]
 ...
 [-12.18309039   0.1151       0.64420003   0.15560001   0.45649996]
 [456.86620433   0.1432       0.60229999   0.18959999   0.30089998]
 [118.47751295   0.1657       0.58600003   0.16399999   0.33000001]][0m
[37m[1m[2023-07-10 21:27:00,882][227910] Max Reward on eval: 626.9244743481395[0m
[37m[1m[2023-07-10 21:27:00,883][227910] Min Reward on eval: -246.85023023040267[0m
[37m[1m[2023-07-10 21:27:00,883][227910] Mean Reward across all agents: 87.29372253463455[0m
[37m[1m[2023-07-10 21:27:00,883][227910] Average Trajectory Length: 998.3206666666666[0m
[36m[2023-07-10 21:27:00,885][227910] mean_value=-482.1182754041828, max_value=553.7832459199824[0m
[37m[1m[2023-07-10 21:27:00,888][227910] New mean coefficients: [[-0.38626426 -0.28684902  2.1677532  -0.26944983 -0.80178744]][0m
[37m[1m[2023-07-10 21:27:00,889][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:27:10,621][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 21:27:10,621][227910] FPS: 394632.10[0m
[36m[2023-07-10 21:27:10,623][227910] itr=1391, itrs=2000, Progress: 69.55%[0m
[36m[2023-07-10 21:27:22,133][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 21:27:22,133][227910] FPS: 334208.09[0m
[36m[2023-07-10 21:27:26,859][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:27:26,859][227910] Reward + Measures: [[487.99336723   0.30218431   0.62672967   0.37193295   0.42240497]][0m
[37m[1m[2023-07-10 21:27:26,859][227910] Max Reward on eval: 487.99336723430423[0m
[37m[1m[2023-07-10 21:27:26,860][227910] Min Reward on eval: 487.99336723430423[0m
[37m[1m[2023-07-10 21:27:26,860][227910] Mean Reward across all agents: 487.99336723430423[0m
[37m[1m[2023-07-10 21:27:26,860][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:27:32,293][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:27:32,293][227910] Reward + Measures: [[591.82572061   0.25149998   0.63160002   0.29530001   0.40170002]
 [379.96012543   0.3673       0.69639999   0.43589997   0.48590001]
 [349.73398088   0.21040002   0.60780001   0.28059998   0.39560002]
 ...
 [462.88423958   0.28299999   0.56570005   0.3976       0.4041    ]
 [395.16384603   0.33860001   0.58289999   0.45889997   0.44119999]
 [408.31941552   0.23550001   0.61199999   0.30690002   0.43590003]][0m
[37m[1m[2023-07-10 21:27:32,294][227910] Max Reward on eval: 591.8257206123205[0m
[37m[1m[2023-07-10 21:27:32,294][227910] Min Reward on eval: -302.9050300788134[0m
[37m[1m[2023-07-10 21:27:32,294][227910] Mean Reward across all agents: 412.8014859712978[0m
[37m[1m[2023-07-10 21:27:32,294][227910] Average Trajectory Length: 999.0849999999999[0m
[36m[2023-07-10 21:27:32,296][227910] mean_value=-889.5373319283513, max_value=195.10079751918173[0m
[37m[1m[2023-07-10 21:27:32,298][227910] New mean coefficients: [[ 1.112113   -0.05734168  2.272964   -0.31741562 -0.5322577 ]][0m
[37m[1m[2023-07-10 21:27:32,299][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:27:42,013][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 21:27:42,013][227910] FPS: 395397.68[0m
[36m[2023-07-10 21:27:42,016][227910] itr=1392, itrs=2000, Progress: 69.60%[0m
[36m[2023-07-10 21:27:53,574][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 21:27:53,574][227910] FPS: 332771.76[0m
[36m[2023-07-10 21:27:58,384][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:27:58,384][227910] Reward + Measures: [[550.60615141   0.29324985   0.63846052   0.3529909    0.42629954]][0m
[37m[1m[2023-07-10 21:27:58,384][227910] Max Reward on eval: 550.606151414962[0m
[37m[1m[2023-07-10 21:27:58,385][227910] Min Reward on eval: 550.606151414962[0m
[37m[1m[2023-07-10 21:27:58,385][227910] Mean Reward across all agents: 550.606151414962[0m
[37m[1m[2023-07-10 21:27:58,385][227910] Average Trajectory Length: 999.81[0m
[36m[2023-07-10 21:28:04,122][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:28:04,123][227910] Reward + Measures: [[197.62735644   0.28760001   0.61869997   0.31150001   0.41170001]
 [450.27768644   0.28740001   0.63319999   0.30250001   0.38589999]
 [242.29357358   0.24800001   0.63500005   0.28130004   0.38339999]
 ...
 [504.90650186   0.38639998   0.65939999   0.39809999   0.41820002]
 [451.42671348   0.3822       0.68689996   0.329        0.4217    ]
 [448.40344829   0.207        0.58170003   0.21760002   0.33920002]][0m
[37m[1m[2023-07-10 21:28:04,123][227910] Max Reward on eval: 654.0558313496992[0m
[37m[1m[2023-07-10 21:28:04,123][227910] Min Reward on eval: -578.7522691224934[0m
[37m[1m[2023-07-10 21:28:04,124][227910] Mean Reward across all agents: 337.6346576555741[0m
[37m[1m[2023-07-10 21:28:04,124][227910] Average Trajectory Length: 997.7386666666666[0m
[36m[2023-07-10 21:28:04,125][227910] mean_value=-928.5068728847323, max_value=683.0124293894751[0m
[37m[1m[2023-07-10 21:28:04,128][227910] New mean coefficients: [[ 0.5085979  -0.5156579   2.4771612  -0.26891863 -0.6660924 ]][0m
[37m[1m[2023-07-10 21:28:04,129][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:28:13,822][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 21:28:13,822][227910] FPS: 396244.66[0m
[36m[2023-07-10 21:28:13,824][227910] itr=1393, itrs=2000, Progress: 69.65%[0m
[36m[2023-07-10 21:28:25,323][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 21:28:25,323][227910] FPS: 334516.23[0m
[36m[2023-07-10 21:28:30,012][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:28:30,013][227910] Reward + Measures: [[574.06738906   0.2827509    0.6610412    0.32762077   0.43505195]][0m
[37m[1m[2023-07-10 21:28:30,013][227910] Max Reward on eval: 574.0673890640895[0m
[37m[1m[2023-07-10 21:28:30,013][227910] Min Reward on eval: 574.0673890640895[0m
[37m[1m[2023-07-10 21:28:30,014][227910] Mean Reward across all agents: 574.0673890640895[0m
[37m[1m[2023-07-10 21:28:30,014][227910] Average Trajectory Length: 999.9583333333333[0m
[36m[2023-07-10 21:28:35,433][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:28:35,434][227910] Reward + Measures: [[495.7081744    0.27860001   0.6462       0.34039998   0.46930003]
 [301.4691343    0.35080001   0.69760001   0.31690001   0.53009999]
 [572.05175991   0.2843       0.58249998   0.33329996   0.3768    ]
 ...
 [121.061887     0.31870002   0.53579998   0.29179999   0.39770001]
 [372.22285896   0.3775       0.68370003   0.4014       0.51310003]
 [247.0396858    0.40349999   0.6512       0.42350003   0.53380007]][0m
[37m[1m[2023-07-10 21:28:35,434][227910] Max Reward on eval: 948.6284951393143[0m
[37m[1m[2023-07-10 21:28:35,434][227910] Min Reward on eval: -575.3200475067977[0m
[37m[1m[2023-07-10 21:28:35,435][227910] Mean Reward across all agents: 345.44347079066756[0m
[37m[1m[2023-07-10 21:28:35,435][227910] Average Trajectory Length: 988.376[0m
[36m[2023-07-10 21:28:35,436][227910] mean_value=-913.2278909394221, max_value=365.54704667575993[0m
[37m[1m[2023-07-10 21:28:35,439][227910] New mean coefficients: [[ 0.77241755 -0.516925    2.3273203  -0.01786041 -0.64840114]][0m
[37m[1m[2023-07-10 21:28:35,440][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:28:45,127][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 21:28:45,127][227910] FPS: 396474.01[0m
[36m[2023-07-10 21:28:45,130][227910] itr=1394, itrs=2000, Progress: 69.70%[0m
[36m[2023-07-10 21:28:56,749][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 21:28:56,750][227910] FPS: 331027.66[0m
[36m[2023-07-10 21:29:01,591][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:29:01,597][227910] Reward + Measures: [[629.33858518   0.27335173   0.67316169   0.30029419   0.41957515]][0m
[37m[1m[2023-07-10 21:29:01,597][227910] Max Reward on eval: 629.338585183516[0m
[37m[1m[2023-07-10 21:29:01,597][227910] Min Reward on eval: 629.338585183516[0m
[37m[1m[2023-07-10 21:29:01,597][227910] Mean Reward across all agents: 629.338585183516[0m
[37m[1m[2023-07-10 21:29:01,598][227910] Average Trajectory Length: 999.8183333333333[0m
[36m[2023-07-10 21:29:07,135][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:29:07,135][227910] Reward + Measures: [[848.28074106   0.28380004   0.61019999   0.257        0.29120001]
 [656.22914086   0.2861       0.61400002   0.27590001   0.32300001]
 [941.35842241   0.26569998   0.63320011   0.24920002   0.2983    ]
 ...
 [762.46678311   0.2687       0.57080001   0.26419997   0.28379998]
 [423.03847386   0.32870001   0.68810004   0.41530004   0.50340003]
 [611.97262818   0.31760001   0.67110008   0.28639999   0.40120003]][0m
[37m[1m[2023-07-10 21:29:07,135][227910] Max Reward on eval: 1108.6108545154334[0m
[37m[1m[2023-07-10 21:29:07,136][227910] Min Reward on eval: -314.158245984104[0m
[37m[1m[2023-07-10 21:29:07,136][227910] Mean Reward across all agents: 622.8749993899135[0m
[37m[1m[2023-07-10 21:29:07,136][227910] Average Trajectory Length: 983.2506666666667[0m
[36m[2023-07-10 21:29:07,138][227910] mean_value=-1241.6224058495477, max_value=955.7487077610871[0m
[37m[1m[2023-07-10 21:29:07,140][227910] New mean coefficients: [[ 0.28278282 -0.45490333  2.665992   -0.07369443 -0.47114286]][0m
[37m[1m[2023-07-10 21:29:07,142][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:29:17,048][227910] train() took 9.90 seconds to complete[0m
[36m[2023-07-10 21:29:17,048][227910] FPS: 387705.09[0m
[36m[2023-07-10 21:29:17,050][227910] itr=1395, itrs=2000, Progress: 69.75%[0m
[36m[2023-07-10 21:29:28,732][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 21:29:28,733][227910] FPS: 329229.39[0m
[36m[2023-07-10 21:29:33,516][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:29:33,516][227910] Reward + Measures: [[646.10422864   0.26443163   0.69074512   0.280653     0.42243049]][0m
[37m[1m[2023-07-10 21:29:33,516][227910] Max Reward on eval: 646.1042286389243[0m
[37m[1m[2023-07-10 21:29:33,517][227910] Min Reward on eval: 646.1042286389243[0m
[37m[1m[2023-07-10 21:29:33,517][227910] Mean Reward across all agents: 646.1042286389243[0m
[37m[1m[2023-07-10 21:29:33,517][227910] Average Trajectory Length: 999.911[0m
[36m[2023-07-10 21:29:38,990][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:29:38,991][227910] Reward + Measures: [[ 225.63949624    0.31580001    0.81300002    0.49610001    0.71579999]
 [ 499.6718038     0.23269999    0.70860004    0.23150001    0.49209997]
 [-109.23819481    0.48769999    0.87810004    0.78900003    0.89770001]
 ...
 [ 692.8976795     0.26470003    0.7033        0.29460001    0.4515    ]
 [ 515.1127123     0.23109999    0.68220001    0.27059999    0.46160004]
 [ 731.61004458    0.26589999    0.71240008    0.25209999    0.41950002]][0m
[37m[1m[2023-07-10 21:29:38,991][227910] Max Reward on eval: 841.1488065637299[0m
[37m[1m[2023-07-10 21:29:38,991][227910] Min Reward on eval: -202.11791649863008[0m
[37m[1m[2023-07-10 21:29:38,992][227910] Mean Reward across all agents: 532.6486089989793[0m
[37m[1m[2023-07-10 21:29:38,992][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:29:38,994][227910] mean_value=-370.25915248818416, max_value=432.1401911121758[0m
[37m[1m[2023-07-10 21:29:38,996][227910] New mean coefficients: [[ 0.5305322  -0.8753988   3.2549348  -0.11456586 -0.19566563]][0m
[37m[1m[2023-07-10 21:29:38,997][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:29:48,852][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 21:29:48,853][227910] FPS: 389722.51[0m
[36m[2023-07-10 21:29:48,855][227910] itr=1396, itrs=2000, Progress: 69.80%[0m
[36m[2023-07-10 21:30:00,615][227910] train() took 11.74 seconds to complete[0m
[36m[2023-07-10 21:30:00,616][227910] FPS: 327040.47[0m
[36m[2023-07-10 21:30:05,381][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:30:05,382][227910] Reward + Measures: [[675.50177639   0.26144299   0.70336163   0.27416566   0.42045569]][0m
[37m[1m[2023-07-10 21:30:05,382][227910] Max Reward on eval: 675.5017763867181[0m
[37m[1m[2023-07-10 21:30:05,382][227910] Min Reward on eval: 675.5017763867181[0m
[37m[1m[2023-07-10 21:30:05,382][227910] Mean Reward across all agents: 675.5017763867181[0m
[37m[1m[2023-07-10 21:30:05,382][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:30:11,009][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:30:11,010][227910] Reward + Measures: [[657.36686044   0.29350001   0.73170006   0.2987       0.44409999]
 [799.72883202   0.23799999   0.6875       0.2705       0.34770003]
 [803.70959088   0.23360001   0.66250002   0.28050002   0.33240002]
 ...
 [628.33710882   0.29520002   0.7245       0.30419999   0.4443    ]
 [655.83948014   0.27239999   0.70209998   0.27200004   0.40259996]
 [734.23421545   0.25639999   0.68300003   0.31040001   0.3901    ]][0m
[37m[1m[2023-07-10 21:30:11,010][227910] Max Reward on eval: 894.7763947879488[0m
[37m[1m[2023-07-10 21:30:11,010][227910] Min Reward on eval: 384.17792372007096[0m
[37m[1m[2023-07-10 21:30:11,010][227910] Mean Reward across all agents: 714.2237763495698[0m
[37m[1m[2023-07-10 21:30:11,011][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:30:11,012][227910] mean_value=-448.08395291255727, max_value=-1.2638500186131978[0m
[36m[2023-07-10 21:30:11,014][227910] XNES is restarting with a new solution whose measures are [0.56129998 0.56910002 0.51330006 0.16080001] and objective is 79.85667728529079[0m
[36m[2023-07-10 21:30:11,015][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 21:30:11,018][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 21:30:11,019][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:30:20,656][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 21:30:20,657][227910] FPS: 398503.54[0m
[36m[2023-07-10 21:30:20,659][227910] itr=1397, itrs=2000, Progress: 69.85%[0m
[36m[2023-07-10 21:30:32,142][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 21:30:32,142][227910] FPS: 335043.14[0m
[36m[2023-07-10 21:30:37,016][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:30:37,022][227910] Reward + Measures: [[128.29596158   0.44738412   0.55848813   0.4099367    0.20515569]][0m
[37m[1m[2023-07-10 21:30:37,022][227910] Max Reward on eval: 128.29596158043333[0m
[37m[1m[2023-07-10 21:30:37,022][227910] Min Reward on eval: 128.29596158043333[0m
[37m[1m[2023-07-10 21:30:37,022][227910] Mean Reward across all agents: 128.29596158043333[0m
[37m[1m[2023-07-10 21:30:37,023][227910] Average Trajectory Length: 999.9896666666666[0m
[36m[2023-07-10 21:30:42,640][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:30:42,646][227910] Reward + Measures: [[   58.00304631     0.80060005     0.58490002     0.71730006
      0.39829999]
 [ -902.03788602     0.24730001     0.60170001     0.57179999
      0.4533    ]
 [ -342.6945039      0.81419992     0.75019997     0.77489996
      0.0804    ]
 ...
 [ -206.85171998     0.70609999     0.67740005     0.63150001
      0.1181    ]
 [-1097.11287623     0.37470001     0.68889999     0.72150004
      0.48629999]
 [ -675.59430032     0.0731         0.7482         0.1885
      0.6803    ]][0m
[37m[1m[2023-07-10 21:30:42,646][227910] Max Reward on eval: 463.3994321240636[0m
[37m[1m[2023-07-10 21:30:42,646][227910] Min Reward on eval: -1643.8048971332028[0m
[37m[1m[2023-07-10 21:30:42,646][227910] Mean Reward across all agents: -142.2758808254729[0m
[37m[1m[2023-07-10 21:30:42,647][227910] Average Trajectory Length: 995.0899999999999[0m
[36m[2023-07-10 21:30:42,650][227910] mean_value=-444.1011780884561, max_value=741.2901937156197[0m
[37m[1m[2023-07-10 21:30:42,653][227910] New mean coefficients: [[-0.06999329  0.24321449 -0.3241455  -1.6668131  -1.6633586 ]][0m
[37m[1m[2023-07-10 21:30:42,654][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:30:52,480][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 21:30:52,480][227910] FPS: 390878.22[0m
[36m[2023-07-10 21:30:52,483][227910] itr=1398, itrs=2000, Progress: 69.90%[0m
[36m[2023-07-10 21:31:04,210][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 21:31:04,210][227910] FPS: 328065.73[0m
[36m[2023-07-10 21:31:09,051][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:31:09,052][227910] Reward + Measures: [[123.68870513   0.444278     0.54756629   0.39028868   0.20214833]][0m
[37m[1m[2023-07-10 21:31:09,052][227910] Max Reward on eval: 123.68870512730423[0m
[37m[1m[2023-07-10 21:31:09,052][227910] Min Reward on eval: 123.68870512730423[0m
[37m[1m[2023-07-10 21:31:09,053][227910] Mean Reward across all agents: 123.68870512730423[0m
[37m[1m[2023-07-10 21:31:09,053][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:31:14,577][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:31:14,578][227910] Reward + Measures: [[ -172.34872512     0.6771         0.55369997     0.4587
      0.30990002]
 [-1924.28412612     0.94999999     0.97769994     0.005
      0.9702999 ]
 [ -409.06476611     0.85799992     0.0813         0.74260002
      0.61840004]
 ...
 [-1570.81369595     0.43963334     0.25458968     0.41938391
      0.17649196]
 [   87.31909871     0.91009998     0.09240001     0.88500005
      0.71899998]
 [ -951.33857661     0.87740004     0.071          0.82590008
      0.68060005]][0m
[37m[1m[2023-07-10 21:31:14,578][227910] Max Reward on eval: 209.59672674993053[0m
[37m[1m[2023-07-10 21:31:14,579][227910] Min Reward on eval: -2341.519207087031[0m
[37m[1m[2023-07-10 21:31:14,579][227910] Mean Reward across all agents: -810.9880332826925[0m
[37m[1m[2023-07-10 21:31:14,579][227910] Average Trajectory Length: 996.853[0m
[36m[2023-07-10 21:31:14,581][227910] mean_value=-1181.8811410839724, max_value=589.7817261300299[0m
[37m[1m[2023-07-10 21:31:14,584][227910] New mean coefficients: [[ 0.3641189   1.4723762   0.04867399 -1.6409597  -1.522037  ]][0m
[37m[1m[2023-07-10 21:31:14,585][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:31:24,374][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 21:31:24,374][227910] FPS: 392375.10[0m
[36m[2023-07-10 21:31:24,376][227910] itr=1399, itrs=2000, Progress: 69.95%[0m
[36m[2023-07-10 21:31:35,990][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 21:31:35,990][227910] FPS: 331172.48[0m
[36m[2023-07-10 21:31:40,809][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:31:40,809][227910] Reward + Measures: [[82.00594308  0.49829268  0.52969664  0.39899671  0.17833598]][0m
[37m[1m[2023-07-10 21:31:40,810][227910] Max Reward on eval: 82.00594307910069[0m
[37m[1m[2023-07-10 21:31:40,810][227910] Min Reward on eval: 82.00594307910069[0m
[37m[1m[2023-07-10 21:31:40,810][227910] Mean Reward across all agents: 82.00594307910069[0m
[37m[1m[2023-07-10 21:31:40,810][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:31:46,259][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:31:46,260][227910] Reward + Measures: [[ -633.63711587     0.81000006     0.70230001     0.76010001
      0.0221    ]
 [ -222.01416278     0.63890004     0.50020003     0.4409
      0.61610001]
 [ -727.7246463      0.62819999     0.228          0.56049997
      0.1158    ]
 ...
 [ -648.90731372     0.60790008     0.29090002     0.5693
      0.29590002]
 [ -853.46188549     0.64230001     0.1866         0.66839993
      0.29180002]
 [-1102.76425214     0.57429999     0.5837         0.5636
      0.1213    ]][0m
[37m[1m[2023-07-10 21:31:46,260][227910] Max Reward on eval: 491.1542109026632[0m
[37m[1m[2023-07-10 21:31:46,261][227910] Min Reward on eval: -1626.803121676785[0m
[37m[1m[2023-07-10 21:31:46,261][227910] Mean Reward across all agents: -459.16908827410543[0m
[37m[1m[2023-07-10 21:31:46,261][227910] Average Trajectory Length: 997.7826666666666[0m
[36m[2023-07-10 21:31:46,263][227910] mean_value=-1297.7341384930087, max_value=505.571051022943[0m
[37m[1m[2023-07-10 21:31:46,266][227910] New mean coefficients: [[-0.14537361  1.7431886   1.0422754  -1.5246716  -0.10518682]][0m
[37m[1m[2023-07-10 21:31:46,267][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:31:55,940][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 21:31:55,941][227910] FPS: 397038.20[0m
[36m[2023-07-10 21:31:55,943][227910] itr=1400, itrs=2000, Progress: 70.00%[0m
[37m[1m[2023-07-10 21:32:00,040][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001380[0m
[36m[2023-07-10 21:32:11,992][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 21:32:11,992][227910] FPS: 328823.30[0m
[36m[2023-07-10 21:32:16,741][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:32:16,741][227910] Reward + Measures: [[164.71765731   0.86287206   0.52243447   0.83098918   0.03569846]][0m
[37m[1m[2023-07-10 21:32:16,742][227910] Max Reward on eval: 164.71765730839064[0m
[37m[1m[2023-07-10 21:32:16,742][227910] Min Reward on eval: 164.71765730839064[0m
[37m[1m[2023-07-10 21:32:16,742][227910] Mean Reward across all agents: 164.71765730839064[0m
[37m[1m[2023-07-10 21:32:16,743][227910] Average Trajectory Length: 999.7189999999999[0m
[36m[2023-07-10 21:32:22,327][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:32:22,328][227910] Reward + Measures: [[150.37857054   0.68379998   0.60090005   0.68790001   0.0298    ]
 [286.59240859   0.7622       0.45469999   0.73870003   0.0612    ]
 [-29.10960642   0.5478       0.52060002   0.47         0.35419998]
 ...
 [ 61.27254253   0.80900002   0.60830003   0.84259999   0.0171    ]
 [297.67427982   0.88760006   0.52080005   0.86049998   0.0502    ]
 [118.66829236   0.78000003   0.6128       0.74680001   0.07129999]][0m
[37m[1m[2023-07-10 21:32:22,328][227910] Max Reward on eval: 425.71871451484037[0m
[37m[1m[2023-07-10 21:32:22,328][227910] Min Reward on eval: -944.628965167192[0m
[37m[1m[2023-07-10 21:32:22,329][227910] Mean Reward across all agents: 19.552321366873155[0m
[37m[1m[2023-07-10 21:32:22,329][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:32:22,334][227910] mean_value=-158.2415764208107, max_value=633.9593898400203[0m
[37m[1m[2023-07-10 21:32:22,337][227910] New mean coefficients: [[-0.8161322  3.0041566  1.0457363 -1.8969505 -0.4304759]][0m
[37m[1m[2023-07-10 21:32:22,338][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:32:32,118][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 21:32:32,118][227910] FPS: 392707.98[0m
[36m[2023-07-10 21:32:32,120][227910] itr=1401, itrs=2000, Progress: 70.05%[0m
[36m[2023-07-10 21:32:43,692][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 21:32:43,692][227910] FPS: 332461.45[0m
[36m[2023-07-10 21:32:48,457][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:32:48,457][227910] Reward + Measures: [[156.03734488   0.89307797   0.54328001   0.84736574   0.02979633]][0m
[37m[1m[2023-07-10 21:32:48,457][227910] Max Reward on eval: 156.03734487926982[0m
[37m[1m[2023-07-10 21:32:48,457][227910] Min Reward on eval: 156.03734487926982[0m
[37m[1m[2023-07-10 21:32:48,458][227910] Mean Reward across all agents: 156.03734487926982[0m
[37m[1m[2023-07-10 21:32:48,458][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:32:54,017][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:32:54,018][227910] Reward + Measures: [[ 256.24854428    0.76229995    0.66480005    0.68670005    0.08400001]
 [-687.2412739     0.43330002    0.76309997    0.61639994    0.4285    ]
 [-396.91804886    0.61359996    0.82569999    0.7123        0.20920002]
 ...
 [ 115.16194541    0.69340003    0.56120002    0.5733        0.28650001]
 [  37.13806018    0.80870003    0.54339999    0.7633        0.0632    ]
 [-186.83849072    0.73100001    0.69740003    0.64680004    0.10830001]][0m
[37m[1m[2023-07-10 21:32:54,018][227910] Max Reward on eval: 388.9873186833109[0m
[37m[1m[2023-07-10 21:32:54,018][227910] Min Reward on eval: -1611.466790103633[0m
[37m[1m[2023-07-10 21:32:54,019][227910] Mean Reward across all agents: -282.3282111800332[0m
[37m[1m[2023-07-10 21:32:54,019][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:32:54,025][227910] mean_value=-221.2414092251152, max_value=738.0633977937512[0m
[37m[1m[2023-07-10 21:32:54,028][227910] New mean coefficients: [[-0.85004497  3.4987395  -0.27378392 -1.6519911  -0.7565489 ]][0m
[37m[1m[2023-07-10 21:32:54,029][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:33:03,838][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 21:33:03,838][227910] FPS: 391535.04[0m
[36m[2023-07-10 21:33:03,840][227910] itr=1402, itrs=2000, Progress: 70.10%[0m
[36m[2023-07-10 21:33:15,370][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 21:33:15,371][227910] FPS: 333584.16[0m
[36m[2023-07-10 21:33:20,197][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:33:20,197][227910] Reward + Measures: [[110.25029491   0.91183335   0.54864836   0.85382199   0.02408767]][0m
[37m[1m[2023-07-10 21:33:20,197][227910] Max Reward on eval: 110.25029490762647[0m
[37m[1m[2023-07-10 21:33:20,198][227910] Min Reward on eval: 110.25029490762647[0m
[37m[1m[2023-07-10 21:33:20,198][227910] Mean Reward across all agents: 110.25029490762647[0m
[37m[1m[2023-07-10 21:33:20,198][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:33:25,640][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:33:25,640][227910] Reward + Measures: [[-622.6939478     0.49130002    0.51160002    0.58560002    0.42870003]
 [-648.78667585    0.38731369    0.38722283    0.44633123    0.28510189]
 [-684.24654049    0.28819999    0.42449999    0.41119996    0.22040001]
 ...
 [-180.64381475    0.67200005    0.49820003    0.61219996    0.131     ]
 [ 148.27021545    0.74230003    0.57849997    0.71900004    0.086     ]
 [ -46.84014154    0.68810004    0.45960003    0.5851        0.16059999]][0m
[37m[1m[2023-07-10 21:33:25,640][227910] Max Reward on eval: 344.013009768194[0m
[37m[1m[2023-07-10 21:33:25,641][227910] Min Reward on eval: -1051.274732348195[0m
[37m[1m[2023-07-10 21:33:25,641][227910] Mean Reward across all agents: -126.5227272175146[0m
[37m[1m[2023-07-10 21:33:25,641][227910] Average Trajectory Length: 996.944[0m
[36m[2023-07-10 21:33:25,645][227910] mean_value=-432.60828948575727, max_value=762.1774989280733[0m
[37m[1m[2023-07-10 21:33:25,647][227910] New mean coefficients: [[ 1.0760448   2.8272219  -0.24229401 -0.06234634 -0.19834912]][0m
[37m[1m[2023-07-10 21:33:25,648][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:33:35,423][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 21:33:35,423][227910] FPS: 392931.08[0m
[36m[2023-07-10 21:33:35,425][227910] itr=1403, itrs=2000, Progress: 70.15%[0m
[36m[2023-07-10 21:33:46,967][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 21:33:46,967][227910] FPS: 333333.34[0m
[36m[2023-07-10 21:33:51,778][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:33:51,779][227910] Reward + Measures: [[145.03921446   0.9377107    0.56179327   0.88997865   0.02014333]][0m
[37m[1m[2023-07-10 21:33:51,779][227910] Max Reward on eval: 145.03921446179598[0m
[37m[1m[2023-07-10 21:33:51,779][227910] Min Reward on eval: 145.03921446179598[0m
[37m[1m[2023-07-10 21:33:51,779][227910] Mean Reward across all agents: 145.03921446179598[0m
[37m[1m[2023-07-10 21:33:51,779][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:33:57,447][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:33:57,447][227910] Reward + Measures: [[222.58096324   0.89149994   0.49779996   0.84909993   0.0322    ]
 [192.86321703   0.93199998   0.52099997   0.89169997   0.0206    ]
 [ 96.95888692   0.78490007   0.43180004   0.69040006   0.10769999]
 ...
 [194.71287762   0.92329997   0.6577       0.84320003   0.0194    ]
 [226.35580913   0.83190006   0.3127       0.67650002   0.3001    ]
 [102.25817728   0.93480009   0.42670003   0.91280001   0.041     ]][0m
[37m[1m[2023-07-10 21:33:57,447][227910] Max Reward on eval: 259.94549202711204[0m
[37m[1m[2023-07-10 21:33:57,448][227910] Min Reward on eval: -200.10705330956262[0m
[37m[1m[2023-07-10 21:33:57,448][227910] Mean Reward across all agents: 149.01765007423677[0m
[37m[1m[2023-07-10 21:33:57,448][227910] Average Trajectory Length: 999.3876666666666[0m
[36m[2023-07-10 21:33:57,452][227910] mean_value=23.46544039559656, max_value=579.2977366402687[0m
[37m[1m[2023-07-10 21:33:57,455][227910] New mean coefficients: [[-0.05989981  3.6213875  -0.99431    -1.5488832  -0.47226524]][0m
[37m[1m[2023-07-10 21:33:57,456][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:34:07,258][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 21:34:07,258][227910] FPS: 391812.98[0m
[36m[2023-07-10 21:34:07,260][227910] itr=1404, itrs=2000, Progress: 70.20%[0m
[36m[2023-07-10 21:34:18,744][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 21:34:18,744][227910] FPS: 335045.08[0m
[36m[2023-07-10 21:34:23,424][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:34:23,424][227910] Reward + Measures: [[163.21712039   0.92831475   0.46573275   0.85189718   0.02943966]][0m
[37m[1m[2023-07-10 21:34:23,424][227910] Max Reward on eval: 163.21712039341136[0m
[37m[1m[2023-07-10 21:34:23,425][227910] Min Reward on eval: 163.21712039341136[0m
[37m[1m[2023-07-10 21:34:23,425][227910] Mean Reward across all agents: 163.21712039341136[0m
[37m[1m[2023-07-10 21:34:23,425][227910] Average Trajectory Length: 999.9033333333333[0m
[36m[2023-07-10 21:34:28,790][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:34:28,790][227910] Reward + Measures: [[165.77144675   0.90799999   0.4585       0.84200001   0.0378    ]
 [136.48042549   0.95320004   0.56590003   0.91959995   0.014     ]
 [176.35616383   0.90640002   0.44040003   0.84899998   0.0415    ]
 ...
 [187.71240127   0.89180005   0.42270002   0.80470002   0.089     ]
 [ 22.17464837   0.88889998   0.50419998   0.79350001   0.026     ]
 [181.6984038    0.86569995   0.46020004   0.7827       0.0826    ]][0m
[37m[1m[2023-07-10 21:34:28,791][227910] Max Reward on eval: 286.38829030948693[0m
[37m[1m[2023-07-10 21:34:28,791][227910] Min Reward on eval: -143.47085657776333[0m
[37m[1m[2023-07-10 21:34:28,791][227910] Mean Reward across all agents: 120.97947038246646[0m
[37m[1m[2023-07-10 21:34:28,791][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:34:28,794][227910] mean_value=-39.29114949662044, max_value=727.4344568258733[0m
[37m[1m[2023-07-10 21:34:28,797][227910] New mean coefficients: [[ 0.7708614   1.7085276  -1.4030231  -0.67550397  1.7497375 ]][0m
[37m[1m[2023-07-10 21:34:28,798][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:34:38,544][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 21:34:38,544][227910] FPS: 394067.92[0m
[36m[2023-07-10 21:34:38,547][227910] itr=1405, itrs=2000, Progress: 70.25%[0m
[36m[2023-07-10 21:34:50,123][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 21:34:50,123][227910] FPS: 332352.21[0m
[36m[2023-07-10 21:34:54,987][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:34:54,988][227910] Reward + Measures: [[181.33471018   0.90634668   0.40050846   0.79808259   0.0561619 ]][0m
[37m[1m[2023-07-10 21:34:54,988][227910] Max Reward on eval: 181.3347101795302[0m
[37m[1m[2023-07-10 21:34:54,988][227910] Min Reward on eval: 181.3347101795302[0m
[37m[1m[2023-07-10 21:34:54,988][227910] Mean Reward across all agents: 181.3347101795302[0m
[37m[1m[2023-07-10 21:34:54,989][227910] Average Trajectory Length: 999.4463333333333[0m
[36m[2023-07-10 21:35:00,395][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:35:00,396][227910] Reward + Measures: [[188.87592645   0.91799992   0.40219998   0.81310004   0.0512    ]
 [296.30720584   0.7924       0.49139997   0.67070001   0.0772    ]
 [278.85746697   0.87010002   0.43360001   0.7608       0.0766    ]
 ...
 [153.89398361   0.91040003   0.33559999   0.83690006   0.1401    ]
 [323.80649896   0.79949999   0.4048       0.66409999   0.18699999]
 [263.84691377   0.86040002   0.1407       0.76839995   0.54630005]][0m
[37m[1m[2023-07-10 21:35:00,396][227910] Max Reward on eval: 344.96481840033664[0m
[37m[1m[2023-07-10 21:35:00,397][227910] Min Reward on eval: -316.4949153545778[0m
[37m[1m[2023-07-10 21:35:00,397][227910] Mean Reward across all agents: 172.5036693207528[0m
[37m[1m[2023-07-10 21:35:00,397][227910] Average Trajectory Length: 999.3449999999999[0m
[36m[2023-07-10 21:35:00,402][227910] mean_value=81.51377716745628, max_value=844.9648184003366[0m
[37m[1m[2023-07-10 21:35:00,405][227910] New mean coefficients: [[ 1.123879    0.25580227 -1.3498439  -0.4480018   3.948655  ]][0m
[37m[1m[2023-07-10 21:35:00,406][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:35:10,219][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 21:35:10,219][227910] FPS: 391376.80[0m
[36m[2023-07-10 21:35:10,221][227910] itr=1406, itrs=2000, Progress: 70.30%[0m
[36m[2023-07-10 21:35:21,683][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 21:35:21,683][227910] FPS: 335593.36[0m
[36m[2023-07-10 21:35:26,467][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:35:26,467][227910] Reward + Measures: [[253.05628763   0.82370669   0.3717095    0.65981585   0.12985982]][0m
[37m[1m[2023-07-10 21:35:26,468][227910] Max Reward on eval: 253.0562876260563[0m
[37m[1m[2023-07-10 21:35:26,468][227910] Min Reward on eval: 253.0562876260563[0m
[37m[1m[2023-07-10 21:35:26,468][227910] Mean Reward across all agents: 253.0562876260563[0m
[37m[1m[2023-07-10 21:35:26,468][227910] Average Trajectory Length: 999.6513333333334[0m
[36m[2023-07-10 21:35:31,994][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:35:32,000][227910] Reward + Measures: [[ 66.31462591   0.62109995   0.26930001   0.55050004   0.43780002]
 [ 77.87510221   0.53960001   0.35120001   0.44870004   0.1621    ]
 [ -9.84305643   0.62779999   0.3448       0.57349998   0.33039999]
 ...
 [ 66.29509172   0.70830005   0.49920008   0.63770002   0.10640001]
 [ 94.98609536   0.64780003   0.17959999   0.50519997   0.47339997]
 [162.89722493   0.87830001   0.38589999   0.83890003   0.1293    ]][0m
[37m[1m[2023-07-10 21:35:32,000][227910] Max Reward on eval: 351.10898649392766[0m
[37m[1m[2023-07-10 21:35:32,000][227910] Min Reward on eval: -222.5276408984093[0m
[37m[1m[2023-07-10 21:35:32,001][227910] Mean Reward across all agents: 127.80069131516036[0m
[37m[1m[2023-07-10 21:35:32,001][227910] Average Trajectory Length: 999.6023333333333[0m
[36m[2023-07-10 21:35:32,005][227910] mean_value=-96.20032203925908, max_value=851.1089864939277[0m
[37m[1m[2023-07-10 21:35:32,008][227910] New mean coefficients: [[ 0.8379435  1.100808  -1.7153511 -0.8187444  0.9337516]][0m
[37m[1m[2023-07-10 21:35:32,008][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:35:41,629][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 21:35:41,630][227910] FPS: 399200.52[0m
[36m[2023-07-10 21:35:41,632][227910] itr=1407, itrs=2000, Progress: 70.35%[0m
[36m[2023-07-10 21:35:53,276][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 21:35:53,276][227910] FPS: 330363.95[0m
[36m[2023-07-10 21:35:58,092][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:35:58,092][227910] Reward + Measures: [[319.13140227   0.74290806   0.34669238   0.53510028   0.20866767]][0m
[37m[1m[2023-07-10 21:35:58,092][227910] Max Reward on eval: 319.1314022657273[0m
[37m[1m[2023-07-10 21:35:58,093][227910] Min Reward on eval: 319.1314022657273[0m
[37m[1m[2023-07-10 21:35:58,093][227910] Mean Reward across all agents: 319.1314022657273[0m
[37m[1m[2023-07-10 21:35:58,093][227910] Average Trajectory Length: 999.0526666666666[0m
[36m[2023-07-10 21:36:03,787][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:36:03,792][227910] Reward + Measures: [[324.58391608   0.68440002   0.39660001   0.49230003   0.3567    ]
 [186.84936159   0.87129992   0.3953       0.72469991   0.1165    ]
 [208.26601277   0.88760006   0.28760001   0.81339997   0.16860001]
 ...
 [294.10851666   0.64740002   0.4632       0.45179996   0.40419999]
 [373.08833949   0.74649996   0.37360001   0.54540002   0.18250002]
 [-86.69100457   0.86620009   0.56730002   0.78689998   0.0447    ]][0m
[37m[1m[2023-07-10 21:36:03,793][227910] Max Reward on eval: 532.8997920633759[0m
[37m[1m[2023-07-10 21:36:03,793][227910] Min Reward on eval: -682.0798921970069[0m
[37m[1m[2023-07-10 21:36:03,793][227910] Mean Reward across all agents: 220.80874106813397[0m
[37m[1m[2023-07-10 21:36:03,794][227910] Average Trajectory Length: 999.3873333333333[0m
[36m[2023-07-10 21:36:03,799][227910] mean_value=143.86563842352808, max_value=907.4177276467875[0m
[37m[1m[2023-07-10 21:36:03,802][227910] New mean coefficients: [[ 2.0692053   0.5537688  -0.51355517  0.34484816  1.8671877 ]][0m
[37m[1m[2023-07-10 21:36:03,803][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:36:13,504][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 21:36:13,504][227910] FPS: 395913.51[0m
[36m[2023-07-10 21:36:13,507][227910] itr=1408, itrs=2000, Progress: 70.40%[0m
[36m[2023-07-10 21:36:25,068][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 21:36:25,068][227910] FPS: 332776.67[0m
[36m[2023-07-10 21:36:29,913][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:36:29,913][227910] Reward + Measures: [[403.99713773   0.75422239   0.27075839   0.55471891   0.32997712]][0m
[37m[1m[2023-07-10 21:36:29,913][227910] Max Reward on eval: 403.99713772766376[0m
[37m[1m[2023-07-10 21:36:29,914][227910] Min Reward on eval: 403.99713772766376[0m
[37m[1m[2023-07-10 21:36:29,914][227910] Mean Reward across all agents: 403.99713772766376[0m
[37m[1m[2023-07-10 21:36:29,914][227910] Average Trajectory Length: 999.0303333333333[0m
[36m[2023-07-10 21:36:35,403][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:36:35,404][227910] Reward + Measures: [[ -124.01925895     0.84810001     0.17990001     0.7683
      0.58030003]
 [-1100.13551823     0.72460002     0.19230001     0.70190001
      0.68149996]
 [  283.95053294     0.76050007     0.39610001     0.52310002
      0.2261    ]
 ...
 [  368.07422981     0.76429999     0.2251         0.58970004
      0.31710002]
 [ -420.41401833     0.86750001     0.13759999     0.71139997
      0.64029998]
 [ -942.5676255      0.829          0.09770001     0.68120003
      0.6832    ]][0m
[37m[1m[2023-07-10 21:36:35,404][227910] Max Reward on eval: 434.6466813252511[0m
[37m[1m[2023-07-10 21:36:35,405][227910] Min Reward on eval: -1631.8261806264286[0m
[37m[1m[2023-07-10 21:36:35,405][227910] Mean Reward across all agents: -633.1988271811493[0m
[37m[1m[2023-07-10 21:36:35,405][227910] Average Trajectory Length: 999.5996666666666[0m
[36m[2023-07-10 21:36:35,407][227910] mean_value=-1308.1187561546446, max_value=711.3090682657436[0m
[37m[1m[2023-07-10 21:36:35,410][227910] New mean coefficients: [[-0.3745978   1.2463996  -0.80665857 -0.8800006   0.8346559 ]][0m
[37m[1m[2023-07-10 21:36:35,411][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:36:45,150][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 21:36:45,150][227910] FPS: 394344.47[0m
[36m[2023-07-10 21:36:45,153][227910] itr=1409, itrs=2000, Progress: 70.45%[0m
[36m[2023-07-10 21:36:56,691][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 21:36:56,692][227910] FPS: 333392.86[0m
[36m[2023-07-10 21:37:01,490][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:37:01,490][227910] Reward + Measures: [[398.34688933   0.8553229    0.14788853   0.71410906   0.54760718]][0m
[37m[1m[2023-07-10 21:37:01,491][227910] Max Reward on eval: 398.3468893334358[0m
[37m[1m[2023-07-10 21:37:01,491][227910] Min Reward on eval: 398.3468893334358[0m
[37m[1m[2023-07-10 21:37:01,491][227910] Mean Reward across all agents: 398.3468893334358[0m
[37m[1m[2023-07-10 21:37:01,491][227910] Average Trajectory Length: 999.1766666666666[0m
[36m[2023-07-10 21:37:07,002][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:37:07,003][227910] Reward + Measures: [[226.08562578   0.6918       0.16700001   0.55260003   0.53530002]
 [ 55.47333167   0.54290003   0.54500002   0.52020001   0.36220002]
 [329.79468268   0.866        0.1398       0.76300001   0.65639997]
 ...
 [485.61915758   0.77230006   0.33310002   0.50839996   0.3425    ]
 [185.72668784   0.866        0.377        0.73540002   0.2299    ]
 [430.35220166   0.67140001   0.29720002   0.41869998   0.46540004]][0m
[37m[1m[2023-07-10 21:37:07,003][227910] Max Reward on eval: 504.6509683793876[0m
[37m[1m[2023-07-10 21:37:07,003][227910] Min Reward on eval: 8.184007092926185[0m
[37m[1m[2023-07-10 21:37:07,004][227910] Mean Reward across all agents: 343.871091333448[0m
[37m[1m[2023-07-10 21:37:07,004][227910] Average Trajectory Length: 999.2806666666667[0m
[36m[2023-07-10 21:37:07,010][227910] mean_value=134.18646580875134, max_value=990.6464855123311[0m
[37m[1m[2023-07-10 21:37:07,013][227910] New mean coefficients: [[ 0.01528776  2.605946   -0.5672966  -0.2844314   1.3290348 ]][0m
[37m[1m[2023-07-10 21:37:07,014][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:37:16,754][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 21:37:16,754][227910] FPS: 394327.79[0m
[36m[2023-07-10 21:37:16,757][227910] itr=1410, itrs=2000, Progress: 70.50%[0m
[37m[1m[2023-07-10 21:37:20,940][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001390[0m
[36m[2023-07-10 21:37:32,836][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 21:37:32,836][227910] FPS: 330432.34[0m
[36m[2023-07-10 21:37:37,627][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:37:37,628][227910] Reward + Measures: [[425.73560772   0.92952454   0.06790126   0.84824395   0.73348027]][0m
[37m[1m[2023-07-10 21:37:37,628][227910] Max Reward on eval: 425.7356077249327[0m
[37m[1m[2023-07-10 21:37:37,628][227910] Min Reward on eval: 425.7356077249327[0m
[37m[1m[2023-07-10 21:37:37,628][227910] Mean Reward across all agents: 425.7356077249327[0m
[37m[1m[2023-07-10 21:37:37,629][227910] Average Trajectory Length: 998.7429999999999[0m
[36m[2023-07-10 21:37:43,240][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:37:43,241][227910] Reward + Measures: [[479.06691454   0.77710003   0.26750001   0.55290002   0.41819999]
 [341.41407339   0.699        0.48590001   0.46339998   0.43880001]
 [177.26776184   0.86499995   0.33649999   0.75579995   0.0868    ]
 ...
 [ 54.66404137   0.76710004   0.76850003   0.8071       0.55280006]
 [417.26891276   0.59390002   0.66890001   0.41430002   0.60540003]
 [178.25141566   0.77650005   0.35619998   0.58779997   0.26630002]][0m
[37m[1m[2023-07-10 21:37:43,241][227910] Max Reward on eval: 593.6473546134541[0m
[37m[1m[2023-07-10 21:37:43,241][227910] Min Reward on eval: -1464.7877375068842[0m
[37m[1m[2023-07-10 21:37:43,241][227910] Mean Reward across all agents: 24.233745353019902[0m
[37m[1m[2023-07-10 21:37:43,242][227910] Average Trajectory Length: 999.2429999999999[0m
[36m[2023-07-10 21:37:43,247][227910] mean_value=-287.4150505551629, max_value=1051.673206625413[0m
[37m[1m[2023-07-10 21:37:43,250][227910] New mean coefficients: [[ 0.93027604  2.1218095  -0.21999794  0.3666243   1.9509766 ]][0m
[37m[1m[2023-07-10 21:37:43,251][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:37:53,101][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 21:37:53,101][227910] FPS: 389910.57[0m
[36m[2023-07-10 21:37:53,103][227910] itr=1411, itrs=2000, Progress: 70.55%[0m
[36m[2023-07-10 21:38:04,829][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 21:38:04,829][227910] FPS: 328051.54[0m
[36m[2023-07-10 21:38:09,497][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:38:09,497][227910] Reward + Measures: [[457.2272953    0.95179814   0.0441088    0.90026528   0.82284188]][0m
[37m[1m[2023-07-10 21:38:09,497][227910] Max Reward on eval: 457.2272953023262[0m
[37m[1m[2023-07-10 21:38:09,498][227910] Min Reward on eval: 457.2272953023262[0m
[37m[1m[2023-07-10 21:38:09,498][227910] Mean Reward across all agents: 457.2272953023262[0m
[37m[1m[2023-07-10 21:38:09,498][227910] Average Trajectory Length: 999.3846666666666[0m
[36m[2023-07-10 21:38:15,039][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:38:15,045][227910] Reward + Measures: [[283.63745606   0.84919995   0.12379999   0.78329998   0.66440004]
 [480.95137061   0.74470001   0.1648       0.61840004   0.42910001]
 [438.18706775   0.96400005   0.0674       0.93440002   0.81820005]
 ...
 [358.38760357   0.89180005   0.3768       0.7518       0.094     ]
 [327.80186405   0.89729995   0.40490004   0.82309991   0.0742    ]
 [280.37823755   0.87099999   0.40960002   0.83900005   0.0947    ]][0m
[37m[1m[2023-07-10 21:38:15,045][227910] Max Reward on eval: 629.4428967415355[0m
[37m[1m[2023-07-10 21:38:15,046][227910] Min Reward on eval: 30.924743941938505[0m
[37m[1m[2023-07-10 21:38:15,046][227910] Mean Reward across all agents: 427.7428496396263[0m
[37m[1m[2023-07-10 21:38:15,046][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:38:15,051][227910] mean_value=74.72498220464325, max_value=883.2783797679679[0m
[37m[1m[2023-07-10 21:38:15,054][227910] New mean coefficients: [[ 1.0552828   3.5717082  -0.01017395  0.3306422   1.0523587 ]][0m
[37m[1m[2023-07-10 21:38:15,055][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:38:24,970][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 21:38:24,971][227910] FPS: 387335.23[0m
[36m[2023-07-10 21:38:24,973][227910] itr=1412, itrs=2000, Progress: 70.60%[0m
[36m[2023-07-10 21:38:36,535][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 21:38:36,535][227910] FPS: 332667.78[0m
[36m[2023-07-10 21:38:41,440][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:38:41,446][227910] Reward + Measures: [[503.30070972   0.96521533   0.03720666   0.92159569   0.83082861]][0m
[37m[1m[2023-07-10 21:38:41,446][227910] Max Reward on eval: 503.3007097238729[0m
[37m[1m[2023-07-10 21:38:41,446][227910] Min Reward on eval: 503.3007097238729[0m
[37m[1m[2023-07-10 21:38:41,446][227910] Mean Reward across all agents: 503.3007097238729[0m
[37m[1m[2023-07-10 21:38:41,447][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:38:46,859][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:38:46,860][227910] Reward + Measures: [[-190.56179436    0.86990005    0.31740001    0.89539999    0.1068    ]
 [ 511.96702964    0.94670004    0.13950001    0.88640004    0.70339996]
 [ 348.08868196    0.92600006    0.33790001    0.87279999    0.22330001]
 ...
 [-517.434138      0.90710002    0.74660003    0.86870003    0.77450001]
 [ 197.66770178    0.84750003    0.0657        0.74290001    0.68080002]
 [-185.22624105    0.87740004    0.22220002    0.88040012    0.149     ]][0m
[37m[1m[2023-07-10 21:38:46,860][227910] Max Reward on eval: 595.340471603279[0m
[37m[1m[2023-07-10 21:38:46,861][227910] Min Reward on eval: -1597.5346986758057[0m
[37m[1m[2023-07-10 21:38:46,861][227910] Mean Reward across all agents: 49.187875635045806[0m
[37m[1m[2023-07-10 21:38:46,861][227910] Average Trajectory Length: 999.6999999999999[0m
[36m[2023-07-10 21:38:46,867][227910] mean_value=-84.67579408759933, max_value=983.1804031645262[0m
[37m[1m[2023-07-10 21:38:46,870][227910] New mean coefficients: [[ 0.08014071  3.8749926  -0.58848727 -0.118386    1.5428977 ]][0m
[37m[1m[2023-07-10 21:38:46,871][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:38:56,695][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 21:38:56,696][227910] FPS: 390926.71[0m
[36m[2023-07-10 21:38:56,698][227910] itr=1413, itrs=2000, Progress: 70.65%[0m
[36m[2023-07-10 21:39:08,503][227910] train() took 11.78 seconds to complete[0m
[36m[2023-07-10 21:39:08,503][227910] FPS: 325838.36[0m
[36m[2023-07-10 21:39:13,334][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:39:13,335][227910] Reward + Measures: [[514.76102499   0.97580433   0.026201     0.95152134   0.89933968]][0m
[37m[1m[2023-07-10 21:39:13,335][227910] Max Reward on eval: 514.761024993349[0m
[37m[1m[2023-07-10 21:39:13,335][227910] Min Reward on eval: 514.761024993349[0m
[37m[1m[2023-07-10 21:39:13,335][227910] Mean Reward across all agents: 514.761024993349[0m
[37m[1m[2023-07-10 21:39:13,336][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:39:18,856][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:39:18,862][227910] Reward + Measures: [[ 24.3115913    0.90620005   0.52070004   0.88129997   0.43959999]
 [460.20813637   0.87769997   0.19810002   0.75220007   0.3897    ]
 [428.82838779   0.95609999   0.12180001   0.92500001   0.56290001]
 ...
 [577.87451094   0.97199994   0.0329       0.91709995   0.77530003]
 [527.17387706   0.81579989   0.37919998   0.56059998   0.58090001]
 [581.0985183    0.97130007   0.0334       0.94859999   0.8082    ]][0m
[37m[1m[2023-07-10 21:39:18,862][227910] Max Reward on eval: 640.3346036883304[0m
[37m[1m[2023-07-10 21:39:18,862][227910] Min Reward on eval: -1268.568137251446[0m
[37m[1m[2023-07-10 21:39:18,863][227910] Mean Reward across all agents: 173.98831912740454[0m
[37m[1m[2023-07-10 21:39:18,863][227910] Average Trajectory Length: 999.6863333333333[0m
[36m[2023-07-10 21:39:18,871][227910] mean_value=156.8677976675662, max_value=1042.529870221104[0m
[37m[1m[2023-07-10 21:39:18,874][227910] New mean coefficients: [[0.9378032  3.342514   0.21002322 0.73536897 2.750642  ]][0m
[37m[1m[2023-07-10 21:39:18,875][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:39:28,665][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 21:39:28,665][227910] FPS: 392295.53[0m
[36m[2023-07-10 21:39:28,667][227910] itr=1414, itrs=2000, Progress: 70.70%[0m
[36m[2023-07-10 21:39:40,251][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 21:39:40,251][227910] FPS: 332152.74[0m
[36m[2023-07-10 21:39:44,947][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:39:44,948][227910] Reward + Measures: [[520.97351508   0.97898293   0.022012     0.96554369   0.93800926]][0m
[37m[1m[2023-07-10 21:39:44,948][227910] Max Reward on eval: 520.9735150763967[0m
[37m[1m[2023-07-10 21:39:44,948][227910] Min Reward on eval: 520.9735150763967[0m
[37m[1m[2023-07-10 21:39:44,948][227910] Mean Reward across all agents: 520.9735150763967[0m
[37m[1m[2023-07-10 21:39:44,948][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:39:50,508][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:39:50,509][227910] Reward + Measures: [[431.05312062   0.829        0.26100001   0.63459998   0.6275    ]
 [354.3107428    0.9794001    0.0207       0.95270008   0.87869996]
 [406.41818959   0.8344       0.289        0.67050004   0.61390001]
 ...
 [522.88385242   0.97670001   0.025        0.9605999    0.90840006]
 [-48.91912692   0.42630002   0.90829992   0.25009999   0.92119998]
 [433.72807588   0.81940001   0.3529       0.59899998   0.49079999]][0m
[37m[1m[2023-07-10 21:39:50,509][227910] Max Reward on eval: 585.3161946561188[0m
[37m[1m[2023-07-10 21:39:50,510][227910] Min Reward on eval: -48.91912691789912[0m
[37m[1m[2023-07-10 21:39:50,510][227910] Mean Reward across all agents: 393.4965687967426[0m
[37m[1m[2023-07-10 21:39:50,510][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:39:50,516][227910] mean_value=111.38835238492219, max_value=906.6467901839991[0m
[37m[1m[2023-07-10 21:39:50,518][227910] New mean coefficients: [[1.3191066 2.3949552 1.2244902 1.3759224 4.5923486]][0m
[37m[1m[2023-07-10 21:39:50,519][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:40:00,256][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 21:40:00,257][227910] FPS: 394434.47[0m
[36m[2023-07-10 21:40:00,259][227910] itr=1415, itrs=2000, Progress: 70.75%[0m
[36m[2023-07-10 21:40:11,872][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 21:40:11,872][227910] FPS: 331207.74[0m
[36m[2023-07-10 21:40:16,690][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:40:16,691][227910] Reward + Measures: [[505.11125247   0.97982502   0.02143167   0.96872193   0.94797468]][0m
[37m[1m[2023-07-10 21:40:16,691][227910] Max Reward on eval: 505.111252469745[0m
[37m[1m[2023-07-10 21:40:16,691][227910] Min Reward on eval: 505.111252469745[0m
[37m[1m[2023-07-10 21:40:16,691][227910] Mean Reward across all agents: 505.111252469745[0m
[37m[1m[2023-07-10 21:40:16,692][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:40:22,071][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:40:22,072][227910] Reward + Measures: [[337.5077393    0.96899998   0.0275       0.95069999   0.93259996]
 [438.53048185   0.9842       0.0148       0.97360003   0.95009995]
 [385.25316685   0.97640002   0.0252       0.95590001   0.91570008]
 ...
 [392.58638086   0.98170006   0.0209       0.96640009   0.94620001]
 [360.85324849   0.98360008   0.0191       0.96919996   0.94630003]
 [445.88365414   0.98390007   0.0156       0.97480005   0.9551    ]][0m
[37m[1m[2023-07-10 21:40:22,072][227910] Max Reward on eval: 616.5062186213211[0m
[37m[1m[2023-07-10 21:40:22,073][227910] Min Reward on eval: 75.46491147552152[0m
[37m[1m[2023-07-10 21:40:22,073][227910] Mean Reward across all agents: 358.4107260144762[0m
[37m[1m[2023-07-10 21:40:22,073][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:40:22,074][227910] mean_value=-288.74257060592817, max_value=69.02500799678285[0m
[37m[1m[2023-07-10 21:40:22,077][227910] New mean coefficients: [[1.2027678 1.6363486 1.8959956 2.3684607 1.7327478]][0m
[37m[1m[2023-07-10 21:40:22,078][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:40:31,847][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 21:40:31,847][227910] FPS: 393143.04[0m
[36m[2023-07-10 21:40:31,849][227910] itr=1416, itrs=2000, Progress: 70.80%[0m
[36m[2023-07-10 21:40:43,503][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 21:40:43,503][227910] FPS: 330035.53[0m
[36m[2023-07-10 21:40:48,234][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:40:48,235][227910] Reward + Measures: [[506.95176959   0.98012823   0.02085433   0.97001934   0.95135796]][0m
[37m[1m[2023-07-10 21:40:48,235][227910] Max Reward on eval: 506.9517695920136[0m
[37m[1m[2023-07-10 21:40:48,235][227910] Min Reward on eval: 506.9517695920136[0m
[37m[1m[2023-07-10 21:40:48,235][227910] Mean Reward across all agents: 506.9517695920136[0m
[37m[1m[2023-07-10 21:40:48,235][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:40:53,602][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:40:53,603][227910] Reward + Measures: [[  330.17987224     0.92629999     0.19130002     0.87050009
      0.19580001]
 [  578.9092819      0.95020002     0.0213         0.95190001
      0.81809998]
 [-1128.34784342     0.60790002     0.85200006     0.50369996
      0.43670002]
 ...
 [ -186.01811875     0.92819995     0.4923         0.81010002
      0.0349    ]
 [  -81.07694236     0.23379998     0.90399998     0.17120001
      0.88630003]
 [  360.99423938     0.96540004     0.0301         0.94209999
      0.84670013]][0m
[37m[1m[2023-07-10 21:40:53,603][227910] Max Reward on eval: 665.816881017969[0m
[37m[1m[2023-07-10 21:40:53,603][227910] Min Reward on eval: -1506.285988300806[0m
[37m[1m[2023-07-10 21:40:53,604][227910] Mean Reward across all agents: -167.95646884568907[0m
[37m[1m[2023-07-10 21:40:53,604][227910] Average Trajectory Length: 999.7819999999999[0m
[36m[2023-07-10 21:40:53,607][227910] mean_value=-368.8290004851998, max_value=941.4328029649687[0m
[37m[1m[2023-07-10 21:40:53,610][227910] New mean coefficients: [[1.2135507  2.4578607  0.00698721 1.9553496  2.080401  ]][0m
[37m[1m[2023-07-10 21:40:53,611][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:41:03,412][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 21:41:03,412][227910] FPS: 391854.63[0m
[36m[2023-07-10 21:41:03,415][227910] itr=1417, itrs=2000, Progress: 70.85%[0m
[36m[2023-07-10 21:41:14,957][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 21:41:14,957][227910] FPS: 333345.46[0m
[36m[2023-07-10 21:41:19,682][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:41:19,682][227910] Reward + Measures: [[533.13926478   0.98127133   0.021253     0.97151095   0.95653528]][0m
[37m[1m[2023-07-10 21:41:19,683][227910] Max Reward on eval: 533.1392647830603[0m
[37m[1m[2023-07-10 21:41:19,683][227910] Min Reward on eval: 533.1392647830603[0m
[37m[1m[2023-07-10 21:41:19,683][227910] Mean Reward across all agents: 533.1392647830603[0m
[37m[1m[2023-07-10 21:41:19,683][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:41:25,223][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:41:25,224][227910] Reward + Measures: [[-232.12828217    0.96520007    0.26930001    0.89930004    0.89930004]
 [-723.25363824    0.93239993    0.70640004    0.8872        0.89960003]
 [ 544.07445311    0.8775        0.37670001    0.72509998    0.59880006]
 ...
 [ 415.4772704     0.94440001    0.5521        0.90080005    0.0116    ]
 [-759.67759703    0.97109997    0.76560003    0.92449999    0.95040005]
 [-222.14971743    0.91940004    0.1434        0.75949997    0.82609999]][0m
[37m[1m[2023-07-10 21:41:25,224][227910] Max Reward on eval: 649.2347975051962[0m
[37m[1m[2023-07-10 21:41:25,224][227910] Min Reward on eval: -1195.6486393029568[0m
[37m[1m[2023-07-10 21:41:25,225][227910] Mean Reward across all agents: 56.77443980526264[0m
[37m[1m[2023-07-10 21:41:25,225][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:41:25,229][227910] mean_value=-221.34520976256454, max_value=1044.0744531082921[0m
[37m[1m[2023-07-10 21:41:25,232][227910] New mean coefficients: [[1.7101538 2.3782344 1.9928287 1.6362247 2.7632766]][0m
[37m[1m[2023-07-10 21:41:25,233][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:41:35,094][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 21:41:35,094][227910] FPS: 389489.48[0m
[36m[2023-07-10 21:41:35,096][227910] itr=1418, itrs=2000, Progress: 70.90%[0m
[36m[2023-07-10 21:41:46,568][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 21:41:46,568][227910] FPS: 335289.56[0m
[36m[2023-07-10 21:41:51,318][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:41:51,319][227910] Reward + Measures: [[570.36626123   0.98078001   0.02165467   0.97121727   0.960513  ]][0m
[37m[1m[2023-07-10 21:41:51,319][227910] Max Reward on eval: 570.366261225882[0m
[37m[1m[2023-07-10 21:41:51,319][227910] Min Reward on eval: 570.366261225882[0m
[37m[1m[2023-07-10 21:41:51,319][227910] Mean Reward across all agents: 570.366261225882[0m
[37m[1m[2023-07-10 21:41:51,320][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:41:56,754][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:41:56,759][227910] Reward + Measures: [[507.08516784   0.82800007   0.26089999   0.7603001    0.62410003]
 [618.95350126   0.97670001   0.0289       0.95999998   0.94569999]
 [505.51229472   0.94280005   0.0753       0.91400003   0.87369996]
 ...
 [333.64151007   0.98500007   0.0172       0.97180003   0.92189997]
 [564.36029706   0.97649997   0.0268       0.96579999   0.9479    ]
 [576.53148548   0.94859999   0.0562       0.93040001   0.91350001]][0m
[37m[1m[2023-07-10 21:41:56,759][227910] Max Reward on eval: 678.361830387765[0m
[37m[1m[2023-07-10 21:41:56,760][227910] Min Reward on eval: -87.95940256037284[0m
[37m[1m[2023-07-10 21:41:56,760][227910] Mean Reward across all agents: 500.94681307612376[0m
[37m[1m[2023-07-10 21:41:56,760][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:41:56,763][227910] mean_value=-72.35268483508592, max_value=822.2130641441072[0m
[37m[1m[2023-07-10 21:41:56,766][227910] New mean coefficients: [[ 1.9396826  -0.1797843   1.2936702   0.94794124  5.165946  ]][0m
[37m[1m[2023-07-10 21:41:56,767][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:42:06,448][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 21:42:06,448][227910] FPS: 396727.61[0m
[36m[2023-07-10 21:42:06,450][227910] itr=1419, itrs=2000, Progress: 70.95%[0m
[36m[2023-07-10 21:42:17,980][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 21:42:17,980][227910] FPS: 333637.48[0m
[36m[2023-07-10 21:42:22,793][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:42:22,793][227910] Reward + Measures: [[670.59500905   0.98344231   0.019488     0.97518098   0.96607095]][0m
[37m[1m[2023-07-10 21:42:22,793][227910] Max Reward on eval: 670.595009046637[0m
[37m[1m[2023-07-10 21:42:22,794][227910] Min Reward on eval: 670.595009046637[0m
[37m[1m[2023-07-10 21:42:22,794][227910] Mean Reward across all agents: 670.595009046637[0m
[37m[1m[2023-07-10 21:42:22,794][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:42:28,500][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:42:28,501][227910] Reward + Measures: [[317.23138378   0.51629996   0.83050007   0.10770001   0.78640002]
 [309.42729016   0.94410002   0.45080003   0.93559998   0.0807    ]
 [622.99908448   0.98610002   0.0164       0.97710001   0.97130007]
 ...
 [556.80047595   0.94630003   0.2007       0.89419997   0.31370002]
 [597.83082266   0.93339998   0.1042       0.86160004   0.66009998]
 [604.17889469   0.9375       0.1688       0.87179995   0.36199999]][0m
[37m[1m[2023-07-10 21:42:28,501][227910] Max Reward on eval: 756.2538458608091[0m
[37m[1m[2023-07-10 21:42:28,501][227910] Min Reward on eval: 190.14701634049416[0m
[37m[1m[2023-07-10 21:42:28,501][227910] Mean Reward across all agents: 499.0813784747204[0m
[37m[1m[2023-07-10 21:42:28,502][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:42:28,507][227910] mean_value=110.43279968381947, max_value=818.5260137193984[0m
[37m[1m[2023-07-10 21:42:28,510][227910] New mean coefficients: [[2.8158293  0.47194433 0.12394762 1.7451901  6.1726246 ]][0m
[37m[1m[2023-07-10 21:42:28,511][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:42:38,262][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 21:42:38,262][227910] FPS: 393884.12[0m
[36m[2023-07-10 21:42:38,264][227910] itr=1420, itrs=2000, Progress: 71.00%[0m
[37m[1m[2023-07-10 21:42:42,307][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001400[0m
[36m[2023-07-10 21:42:54,067][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 21:42:54,067][227910] FPS: 334400.16[0m
[36m[2023-07-10 21:42:58,793][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:42:58,793][227910] Reward + Measures: [[691.02468911   0.98285133   0.020286     0.97448266   0.96589297]][0m
[37m[1m[2023-07-10 21:42:58,793][227910] Max Reward on eval: 691.0246891063894[0m
[37m[1m[2023-07-10 21:42:58,794][227910] Min Reward on eval: 691.0246891063894[0m
[37m[1m[2023-07-10 21:42:58,794][227910] Mean Reward across all agents: 691.0246891063894[0m
[37m[1m[2023-07-10 21:42:58,794][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:43:04,333][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:43:04,334][227910] Reward + Measures: [[ 121.06559793    0.7026        0.49899998    0.61730003    0.59470004]
 [ 436.75693666    0.86739999    0.0866        0.79229993    0.51060003]
 [ -26.83281923    0.84469998    0.37910002    0.86790001    0.1605    ]
 ...
 [-518.06601254    0.29070002    0.6275        0.33360001    0.72770005]
 [ 477.94655699    0.95590001    0.0886        0.8818        0.84549999]
 [ 541.98507677    0.95679998    0.07960001    0.90809995    0.81099999]][0m
[37m[1m[2023-07-10 21:43:04,334][227910] Max Reward on eval: 605.0896144220372[0m
[37m[1m[2023-07-10 21:43:04,334][227910] Min Reward on eval: -1695.4966603493435[0m
[37m[1m[2023-07-10 21:43:04,335][227910] Mean Reward across all agents: -122.08689029523181[0m
[37m[1m[2023-07-10 21:43:04,335][227910] Average Trajectory Length: 994.2286666666666[0m
[36m[2023-07-10 21:43:04,338][227910] mean_value=-547.8042278954214, max_value=676.2719711788407[0m
[37m[1m[2023-07-10 21:43:04,340][227910] New mean coefficients: [[2.6509361  1.0255456  0.01908511 1.2665961  5.9612136 ]][0m
[37m[1m[2023-07-10 21:43:04,341][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:43:14,151][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 21:43:14,151][227910] FPS: 391513.90[0m
[36m[2023-07-10 21:43:14,154][227910] itr=1421, itrs=2000, Progress: 71.05%[0m
[36m[2023-07-10 21:43:25,694][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 21:43:25,695][227910] FPS: 333276.08[0m
[36m[2023-07-10 21:43:30,357][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:43:30,363][227910] Reward + Measures: [[704.46859167   0.98539799   0.018521     0.97762436   0.97132599]][0m
[37m[1m[2023-07-10 21:43:30,363][227910] Max Reward on eval: 704.4685916667931[0m
[37m[1m[2023-07-10 21:43:30,363][227910] Min Reward on eval: 704.4685916667931[0m
[37m[1m[2023-07-10 21:43:30,363][227910] Mean Reward across all agents: 704.4685916667931[0m
[37m[1m[2023-07-10 21:43:30,364][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:43:35,775][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:43:35,776][227910] Reward + Measures: [[ -62.6656629     0.72279996    0.0304        0.8089        0.63880002]
 [-714.82548148    0.8427        0.80079997    0.2983        0.65069997]
 [ 333.82241586    0.87260002    0.83520001    0.0493        0.9224    ]
 ...
 [-328.40175748    0.94220001    0.55190003    0.83409995    0.58770001]
 [ 572.4562396     0.82919997    0.29390001    0.83429998    0.29580003]
 [-250.88389497    0.45339999    0.21900001    0.45640001    0.2983    ]][0m
[37m[1m[2023-07-10 21:43:35,776][227910] Max Reward on eval: 675.0562516685925[0m
[37m[1m[2023-07-10 21:43:35,776][227910] Min Reward on eval: -1767.7559047311545[0m
[37m[1m[2023-07-10 21:43:35,776][227910] Mean Reward across all agents: -54.561570582413395[0m
[37m[1m[2023-07-10 21:43:35,777][227910] Average Trajectory Length: 999.5226666666666[0m
[36m[2023-07-10 21:43:35,783][227910] mean_value=-429.6301445340127, max_value=1089.022614503058[0m
[37m[1m[2023-07-10 21:43:35,785][227910] New mean coefficients: [[2.3870273  1.1484723  0.97602993 1.5187775  4.927532  ]][0m
[37m[1m[2023-07-10 21:43:35,786][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:43:45,578][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 21:43:45,578][227910] FPS: 392250.29[0m
[36m[2023-07-10 21:43:45,580][227910] itr=1422, itrs=2000, Progress: 71.10%[0m
[36m[2023-07-10 21:43:57,070][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 21:43:57,070][227910] FPS: 334752.31[0m
[36m[2023-07-10 21:44:01,880][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:44:01,881][227910] Reward + Measures: [[742.39627462   0.98690838   0.01897533   0.97817034   0.97289902]][0m
[37m[1m[2023-07-10 21:44:01,881][227910] Max Reward on eval: 742.3962746218067[0m
[37m[1m[2023-07-10 21:44:01,881][227910] Min Reward on eval: 742.3962746218067[0m
[37m[1m[2023-07-10 21:44:01,881][227910] Mean Reward across all agents: 742.3962746218067[0m
[37m[1m[2023-07-10 21:44:01,882][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:44:07,384][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:44:07,384][227910] Reward + Measures: [[570.57895598   0.85339993   0.69580001   0.25419998   0.81169999]
 [487.3522839    0.77130002   0.4831       0.52850002   0.58850002]
 [750.85298975   0.98979998   0.0197       0.97609997   0.97410005]
 ...
 [759.09999802   0.98660004   0.0196       0.97890007   0.96740001]
 [569.70389839   0.81930012   0.77759999   0.1426       0.88220006]
 [616.65786506   0.8695001    0.31040001   0.62920004   0.7026    ]][0m
[37m[1m[2023-07-10 21:44:07,384][227910] Max Reward on eval: 778.6865762867034[0m
[37m[1m[2023-07-10 21:44:07,385][227910] Min Reward on eval: 292.45901372956575[0m
[37m[1m[2023-07-10 21:44:07,385][227910] Mean Reward across all agents: 640.584902520571[0m
[37m[1m[2023-07-10 21:44:07,385][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:44:07,393][227910] mean_value=425.8998787709746, max_value=1160.3068727170141[0m
[37m[1m[2023-07-10 21:44:07,396][227910] New mean coefficients: [[2.482253  1.4582632 1.0040399 0.6094499 5.6885395]][0m
[37m[1m[2023-07-10 21:44:07,397][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:44:17,261][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 21:44:17,261][227910] FPS: 389347.30[0m
[36m[2023-07-10 21:44:17,264][227910] itr=1423, itrs=2000, Progress: 71.15%[0m
[36m[2023-07-10 21:44:28,922][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 21:44:28,922][227910] FPS: 329909.97[0m
[36m[2023-07-10 21:44:33,780][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:44:33,780][227910] Reward + Measures: [[744.68414802   0.98813963   0.01685333   0.98101431   0.97641069]][0m
[37m[1m[2023-07-10 21:44:33,780][227910] Max Reward on eval: 744.684148024132[0m
[37m[1m[2023-07-10 21:44:33,781][227910] Min Reward on eval: 744.684148024132[0m
[37m[1m[2023-07-10 21:44:33,781][227910] Mean Reward across all agents: 744.684148024132[0m
[37m[1m[2023-07-10 21:44:33,781][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:44:39,293][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:44:39,294][227910] Reward + Measures: [[ 268.8613664     0.9471001     0.67119998    0.90480006    0.0413    ]
 [-593.11242661    0.76969999    0.58610004    0.73979998    0.38649997]
 [-286.74595357    0.33189997    0.7572        0.53689998    0.76720005]
 ...
 [ 170.89963164    0.93510002    0.55680001    0.85079998    0.76520002]
 [ 249.88488123    0.95900005    0.63079995    0.9011001     0.14189999]
 [-637.60000321    0.80251962    0.68874639    0.74573898    0.27022928]][0m
[37m[1m[2023-07-10 21:44:39,294][227910] Max Reward on eval: 688.9672655216534[0m
[37m[1m[2023-07-10 21:44:39,294][227910] Min Reward on eval: -1419.0097060543253[0m
[37m[1m[2023-07-10 21:44:39,294][227910] Mean Reward across all agents: 109.40945154048158[0m
[37m[1m[2023-07-10 21:44:39,295][227910] Average Trajectory Length: 999.6803333333334[0m
[36m[2023-07-10 21:44:39,300][227910] mean_value=-20.808722854420896, max_value=1104.7796997208848[0m
[37m[1m[2023-07-10 21:44:39,303][227910] New mean coefficients: [[2.5369117  0.62864935 1.2254356  0.9293932  5.0359077 ]][0m
[37m[1m[2023-07-10 21:44:39,304][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:44:49,088][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 21:44:49,089][227910] FPS: 392510.81[0m
[36m[2023-07-10 21:44:49,091][227910] itr=1424, itrs=2000, Progress: 71.20%[0m
[36m[2023-07-10 21:45:00,672][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 21:45:00,673][227910] FPS: 332101.86[0m
[36m[2023-07-10 21:45:05,507][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:45:05,507][227910] Reward + Measures: [[785.8200442    0.9873457    0.018162     0.97954261   0.97642905]][0m
[37m[1m[2023-07-10 21:45:05,507][227910] Max Reward on eval: 785.8200442048867[0m
[37m[1m[2023-07-10 21:45:05,508][227910] Min Reward on eval: 785.8200442048867[0m
[37m[1m[2023-07-10 21:45:05,508][227910] Mean Reward across all agents: 785.8200442048867[0m
[37m[1m[2023-07-10 21:45:05,508][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:45:10,928][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:45:10,929][227910] Reward + Measures: [[ -546.62298727     0.82730001     0.83330005     0.0496
      0.81140006]
 [ -107.0784817      0.95270008     0.0557         0.95500004
      0.30630001]
 [  -69.60008589     0.84400004     0.86079997     0.0287
      0.85359997]
 ...
 [-2092.96266111     0.741          0.83029997     0.0219
      0.81409997]
 [-2247.85968668     0.63859999     0.68979996     0.0505
      0.64029998]
 [-2145.27883478     0.82910007     0.89050001     0.0106
      0.85970002]][0m
[37m[1m[2023-07-10 21:45:10,929][227910] Max Reward on eval: 719.4522351078224[0m
[37m[1m[2023-07-10 21:45:10,930][227910] Min Reward on eval: -2572.2951236169783[0m
[37m[1m[2023-07-10 21:45:10,930][227910] Mean Reward across all agents: -1372.0019203769023[0m
[37m[1m[2023-07-10 21:45:10,930][227910] Average Trajectory Length: 997.903[0m
[36m[2023-07-10 21:45:10,932][227910] mean_value=-1411.7341769151506, max_value=787.1253336073489[0m
[37m[1m[2023-07-10 21:45:10,935][227910] New mean coefficients: [[2.580925   0.81404424 1.0027542  0.66500854 1.7552338 ]][0m
[37m[1m[2023-07-10 21:45:10,936][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:45:20,708][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 21:45:20,708][227910] FPS: 393020.22[0m
[36m[2023-07-10 21:45:20,711][227910] itr=1425, itrs=2000, Progress: 71.25%[0m
[36m[2023-07-10 21:45:32,315][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 21:45:32,315][227910] FPS: 331488.26[0m
[36m[2023-07-10 21:45:36,983][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:45:36,983][227910] Reward + Measures: [[790.18940224   0.98776335   0.018689     0.97921437   0.977121  ]][0m
[37m[1m[2023-07-10 21:45:36,983][227910] Max Reward on eval: 790.189402242396[0m
[37m[1m[2023-07-10 21:45:36,984][227910] Min Reward on eval: 790.189402242396[0m
[37m[1m[2023-07-10 21:45:36,984][227910] Mean Reward across all agents: 790.189402242396[0m
[37m[1m[2023-07-10 21:45:36,984][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:45:42,783][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:45:42,784][227910] Reward + Measures: [[733.48222995   0.91969997   0.29100001   0.78400004   0.86790001]
 [740.85116449   0.92880005   0.37759998   0.759        0.88120002]
 [687.43682505   0.98649997   0.0197       0.97770005   0.95349997]
 ...
 [765.65450328   0.98909998   0.0157       0.98169994   0.97279996]
 [639.68278913   0.85900003   0.79869998   0.0399       0.9278    ]
 [733.3126314    0.98099995   0.0365       0.97380012   0.8757    ]][0m
[37m[1m[2023-07-10 21:45:42,784][227910] Max Reward on eval: 809.0697008810937[0m
[37m[1m[2023-07-10 21:45:42,784][227910] Min Reward on eval: 358.47746692346266[0m
[37m[1m[2023-07-10 21:45:42,784][227910] Mean Reward across all agents: 703.5709032055678[0m
[37m[1m[2023-07-10 21:45:42,785][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:45:42,793][227910] mean_value=260.1408435348184, max_value=1101.5262872202834[0m
[37m[1m[2023-07-10 21:45:42,795][227910] New mean coefficients: [[3.03618   1.5494981 2.1570916 1.0530488 2.916089 ]][0m
[37m[1m[2023-07-10 21:45:42,796][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:45:52,706][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 21:45:52,706][227910] FPS: 387571.53[0m
[36m[2023-07-10 21:45:52,708][227910] itr=1426, itrs=2000, Progress: 71.30%[0m
[36m[2023-07-10 21:46:04,461][227910] train() took 11.73 seconds to complete[0m
[36m[2023-07-10 21:46:04,462][227910] FPS: 327252.70[0m
[36m[2023-07-10 21:46:09,369][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:46:09,369][227910] Reward + Measures: [[476.43284461   0.67024732   0.84637469   0.28837699   0.73168701]][0m
[37m[1m[2023-07-10 21:46:09,370][227910] Max Reward on eval: 476.4328446095341[0m
[37m[1m[2023-07-10 21:46:09,370][227910] Min Reward on eval: 476.4328446095341[0m
[37m[1m[2023-07-10 21:46:09,370][227910] Mean Reward across all agents: 476.4328446095341[0m
[37m[1m[2023-07-10 21:46:09,371][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:46:14,930][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:46:14,931][227910] Reward + Measures: [[158.19054524   0.8743       0.56730002   0.84300005   0.45830002]
 [570.94588732   0.82539999   0.82600003   0.30039999   0.68620008]
 [522.7823072    0.83450001   0.45099998   0.76800001   0.36090001]
 ...
 [571.55261997   0.81410009   0.91720009   0.08629999   0.81160003]
 [536.40575927   0.8624       0.40820003   0.76620001   0.32690001]
 [109.7589829    0.69180006   0.77829999   0.60519999   0.78030002]][0m
[37m[1m[2023-07-10 21:46:14,931][227910] Max Reward on eval: 642.106789143174[0m
[37m[1m[2023-07-10 21:46:14,932][227910] Min Reward on eval: -294.17257894291544[0m
[37m[1m[2023-07-10 21:46:14,932][227910] Mean Reward across all agents: 375.8507155871244[0m
[37m[1m[2023-07-10 21:46:14,932][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:46:14,939][227910] mean_value=242.53022311104743, max_value=1090.8523608935764[0m
[37m[1m[2023-07-10 21:46:14,942][227910] New mean coefficients: [[ 3.0365348   1.9446476   2.0619051  -0.38152242  1.9211031 ]][0m
[37m[1m[2023-07-10 21:46:14,943][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:46:24,669][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 21:46:24,669][227910] FPS: 394889.94[0m
[36m[2023-07-10 21:46:24,672][227910] itr=1427, itrs=2000, Progress: 71.35%[0m
[36m[2023-07-10 21:46:36,139][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 21:46:36,139][227910] FPS: 335429.00[0m
[36m[2023-07-10 21:46:40,985][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:46:40,985][227910] Reward + Measures: [[541.99079582   0.75909799   0.91141903   0.33674932   0.8161577 ]][0m
[37m[1m[2023-07-10 21:46:40,986][227910] Max Reward on eval: 541.9907958184982[0m
[37m[1m[2023-07-10 21:46:40,986][227910] Min Reward on eval: 541.9907958184982[0m
[37m[1m[2023-07-10 21:46:40,986][227910] Mean Reward across all agents: 541.9907958184982[0m
[37m[1m[2023-07-10 21:46:40,986][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:46:46,473][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:46:46,473][227910] Reward + Measures: [[630.7199442    0.72280002   0.41379997   0.60510004   0.47629997]
 [524.49926175   0.64880002   0.35769999   0.56400001   0.38699999]
 [555.75513272   0.72430003   0.50459999   0.67940003   0.52580005]
 ...
 [ 99.93369294   0.35499999   0.84510005   0.0212       0.77259994]
 [580.38099967   0.74150002   0.47249994   0.6688       0.49920002]
 [701.87888872   0.78049999   0.50010002   0.78089994   0.41350004]][0m
[37m[1m[2023-07-10 21:46:46,474][227910] Max Reward on eval: 779.3006167852203[0m
[37m[1m[2023-07-10 21:46:46,474][227910] Min Reward on eval: -1227.1677772741416[0m
[37m[1m[2023-07-10 21:46:46,474][227910] Mean Reward across all agents: 348.841398837171[0m
[37m[1m[2023-07-10 21:46:46,474][227910] Average Trajectory Length: 999.5103333333333[0m
[36m[2023-07-10 21:46:46,482][227910] mean_value=-341.5838268326882, max_value=1179.3002381332353[0m
[37m[1m[2023-07-10 21:46:46,485][227910] New mean coefficients: [[3.2987351  0.6971849  1.8488193  0.21134341 2.0970325 ]][0m
[37m[1m[2023-07-10 21:46:46,486][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:46:56,304][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 21:46:56,304][227910] FPS: 391177.65[0m
[36m[2023-07-10 21:46:56,306][227910] itr=1428, itrs=2000, Progress: 71.40%[0m
[36m[2023-07-10 21:47:07,890][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 21:47:07,891][227910] FPS: 332055.53[0m
[36m[2023-07-10 21:47:12,682][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:47:12,682][227910] Reward + Measures: [[596.04556224   0.78840232   0.94463867   0.36368898   0.87454164]][0m
[37m[1m[2023-07-10 21:47:12,683][227910] Max Reward on eval: 596.0455622419943[0m
[37m[1m[2023-07-10 21:47:12,683][227910] Min Reward on eval: 596.0455622419943[0m
[37m[1m[2023-07-10 21:47:12,683][227910] Mean Reward across all agents: 596.0455622419943[0m
[37m[1m[2023-07-10 21:47:12,683][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:47:18,208][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:47:18,209][227910] Reward + Measures: [[ 684.21630869    0.84180003    0.88580006    0.70979995    0.87229997]
 [ 584.23742889    0.81549996    0.96149999    0.1018        0.89309996]
 [ 490.31002553    0.61260003    0.87120003    0.89589995    0.91720003]
 ...
 [-142.76701742    0.0952        0.58680004    0.65619999    0.67610008]
 [-223.74119421    0.22140001    0.61500001    0.75999999    0.80760002]
 [ 519.05683809    0.80380005    0.91539997    0.93549997    0.94300002]][0m
[37m[1m[2023-07-10 21:47:18,209][227910] Max Reward on eval: 697.0983023645589[0m
[37m[1m[2023-07-10 21:47:18,209][227910] Min Reward on eval: -631.6288825156283[0m
[37m[1m[2023-07-10 21:47:18,209][227910] Mean Reward across all agents: 384.36801645391154[0m
[37m[1m[2023-07-10 21:47:18,210][227910] Average Trajectory Length: 999.8163333333333[0m
[36m[2023-07-10 21:47:18,218][227910] mean_value=309.52264796146204, max_value=1147.4567676392523[0m
[37m[1m[2023-07-10 21:47:18,221][227910] New mean coefficients: [[ 3.003878   1.1400161  1.5326699 -1.3891015  1.58593  ]][0m
[37m[1m[2023-07-10 21:47:18,222][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:47:28,077][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 21:47:28,077][227910] FPS: 389720.39[0m
[36m[2023-07-10 21:47:28,079][227910] itr=1429, itrs=2000, Progress: 71.45%[0m
[36m[2023-07-10 21:47:39,627][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 21:47:39,627][227910] FPS: 333188.12[0m
[36m[2023-07-10 21:47:44,366][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:47:44,366][227910] Reward + Measures: [[654.38099389   0.82977229   0.9771266    0.012373     0.87554669]][0m
[37m[1m[2023-07-10 21:47:44,366][227910] Max Reward on eval: 654.3809938857117[0m
[37m[1m[2023-07-10 21:47:44,367][227910] Min Reward on eval: 654.3809938857117[0m
[37m[1m[2023-07-10 21:47:44,367][227910] Mean Reward across all agents: 654.3809938857117[0m
[37m[1m[2023-07-10 21:47:44,367][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:47:50,043][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:47:50,044][227910] Reward + Measures: [[-774.20393757    0.45970002    0.37069997    0.4409        0.31650001]
 [ -46.43219712    0.528         0.62830001    0.3858        0.62860006]
 [ 499.47729093    0.92970002    0.89709997    0.1437        0.92989999]
 ...
 [ 111.97126534    0.21052471    0.32607055    0.27503824    0.23139834]
 [-449.13329413    0.17491516    0.2488569     0.22715309    0.17705834]
 [ 268.92887257    0.3215        0.45650002    0.2           0.35589999]][0m
[37m[1m[2023-07-10 21:47:50,044][227910] Max Reward on eval: 846.5202249946072[0m
[37m[1m[2023-07-10 21:47:50,044][227910] Min Reward on eval: -1139.7878984250826[0m
[37m[1m[2023-07-10 21:47:50,044][227910] Mean Reward across all agents: 169.7636052543015[0m
[37m[1m[2023-07-10 21:47:50,045][227910] Average Trajectory Length: 937.2216666666666[0m
[36m[2023-07-10 21:47:50,049][227910] mean_value=-1174.0088104250285, max_value=1175.4347360162496[0m
[37m[1m[2023-07-10 21:47:50,052][227910] New mean coefficients: [[3.2850933 0.6127095 1.941294  0.3669268 1.9370395]][0m
[37m[1m[2023-07-10 21:47:50,053][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:47:59,839][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 21:47:59,839][227910] FPS: 392477.98[0m
[36m[2023-07-10 21:47:59,841][227910] itr=1430, itrs=2000, Progress: 71.50%[0m
[37m[1m[2023-07-10 21:48:03,831][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001410[0m
[36m[2023-07-10 21:48:15,700][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 21:48:15,701][227910] FPS: 331241.62[0m
[36m[2023-07-10 21:48:20,538][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:48:20,538][227910] Reward + Measures: [[651.14681741   0.90388966   0.98028594   0.73970068   0.90328228]][0m
[37m[1m[2023-07-10 21:48:20,539][227910] Max Reward on eval: 651.146817410071[0m
[37m[1m[2023-07-10 21:48:20,539][227910] Min Reward on eval: 651.146817410071[0m
[37m[1m[2023-07-10 21:48:20,539][227910] Mean Reward across all agents: 651.146817410071[0m
[37m[1m[2023-07-10 21:48:20,539][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:48:26,052][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:48:26,053][227910] Reward + Measures: [[  -99.56291891     0.88459998     0.89630002     0.0153
      0.94559997]
 [ -374.86164743     0.65549999     0.62400001     0.65460002
      0.50470001]
 [  720.35286657     0.50340003     0.95300001     0.1015
      0.82819998]
 ...
 [-1788.56640119     0.74620003     0.77789998     0.49380001
      0.71040004]
 [   58.93570397     0.73830003     0.44729996     0.76350003
      0.0642    ]
 [ -473.58827014     0.77380002     0.90310001     0.55840009
      0.81739998]][0m
[37m[1m[2023-07-10 21:48:26,053][227910] Max Reward on eval: 825.7487730424851[0m
[37m[1m[2023-07-10 21:48:26,053][227910] Min Reward on eval: -2876.632841297984[0m
[37m[1m[2023-07-10 21:48:26,054][227910] Mean Reward across all agents: -385.81674409474294[0m
[37m[1m[2023-07-10 21:48:26,054][227910] Average Trajectory Length: 999.7213333333333[0m
[36m[2023-07-10 21:48:26,059][227910] mean_value=-561.2962358741288, max_value=1298.7481875537894[0m
[37m[1m[2023-07-10 21:48:26,062][227910] New mean coefficients: [[ 3.2437577   1.2197022   2.1348116  -0.08614701  0.6801764 ]][0m
[37m[1m[2023-07-10 21:48:26,063][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:48:35,811][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 21:48:35,811][227910] FPS: 394003.69[0m
[36m[2023-07-10 21:48:35,814][227910] itr=1431, itrs=2000, Progress: 71.55%[0m
[36m[2023-07-10 21:48:47,363][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 21:48:47,364][227910] FPS: 333043.34[0m
[36m[2023-07-10 21:48:52,222][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:48:52,223][227910] Reward + Measures: [[778.9500021    0.79578894   0.55253834   0.66894734   0.6755147 ]][0m
[37m[1m[2023-07-10 21:48:52,223][227910] Max Reward on eval: 778.9500021034038[0m
[37m[1m[2023-07-10 21:48:52,223][227910] Min Reward on eval: 778.9500021034038[0m
[37m[1m[2023-07-10 21:48:52,223][227910] Mean Reward across all agents: 778.9500021034038[0m
[37m[1m[2023-07-10 21:48:52,224][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:48:57,818][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:48:57,818][227910] Reward + Measures: [[ 637.56761527    0.6225        0.68079996    0.39550003    0.63340002]
 [ -98.18284358    0.27040002    0.56510007    0.34400001    0.4975    ]
 [-581.96098737    0.4014        0.54040003    0.49580002    0.42290002]
 ...
 [ 814.43258304    0.79700005    0.207         0.74679995    0.65920001]
 [ 736.15359461    0.95889997    0.44180003    0.89610004    0.78530002]
 [ 621.67767624    0.64700001    0.72589999    0.28080001    0.7313    ]][0m
[37m[1m[2023-07-10 21:48:57,818][227910] Max Reward on eval: 839.6676777319983[0m
[37m[1m[2023-07-10 21:48:57,819][227910] Min Reward on eval: -602.8748150171247[0m
[37m[1m[2023-07-10 21:48:57,819][227910] Mean Reward across all agents: 573.1907603627217[0m
[37m[1m[2023-07-10 21:48:57,819][227910] Average Trajectory Length: 997.8816666666667[0m
[36m[2023-07-10 21:48:57,827][227910] mean_value=47.12130409598331, max_value=1176.6684291836907[0m
[37m[1m[2023-07-10 21:48:57,830][227910] New mean coefficients: [[ 3.571187  -0.2851951  2.3480113  1.0654097  1.0880516]][0m
[37m[1m[2023-07-10 21:48:57,831][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:49:07,648][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 21:49:07,648][227910] FPS: 391238.27[0m
[36m[2023-07-10 21:49:07,651][227910] itr=1432, itrs=2000, Progress: 71.60%[0m
[36m[2023-07-10 21:49:19,231][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 21:49:19,232][227910] FPS: 332217.84[0m
[36m[2023-07-10 21:49:24,020][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:49:24,020][227910] Reward + Measures: [[857.41380681   0.77471399   0.56639534   0.64392263   0.66017795]][0m
[37m[1m[2023-07-10 21:49:24,020][227910] Max Reward on eval: 857.4138068124231[0m
[37m[1m[2023-07-10 21:49:24,020][227910] Min Reward on eval: 857.4138068124231[0m
[37m[1m[2023-07-10 21:49:24,021][227910] Mean Reward across all agents: 857.4138068124231[0m
[37m[1m[2023-07-10 21:49:24,021][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:49:29,453][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:49:29,453][227910] Reward + Measures: [[-569.45676935    0.2332        0.6807        0.31150004    0.70430005]
 [ 648.24718254    0.51770002    0.77439994    0.37690002    0.71050006]
 [-440.51299306    0.11137887    0.72097319    0.51201969    0.69055647]
 ...
 [  -0.15904038    0.34299999    0.70609999    0.1362        0.71609998]
 [ -45.41867464    0.75760001    0.94469994    0.182         0.92490005]
 [-180.45223646    0.3985        0.83339995    0.07759999    0.81389999]][0m
[37m[1m[2023-07-10 21:49:29,454][227910] Max Reward on eval: 892.5000690262765[0m
[37m[1m[2023-07-10 21:49:29,454][227910] Min Reward on eval: -1453.9001054410824[0m
[37m[1m[2023-07-10 21:49:29,454][227910] Mean Reward across all agents: 66.12573561444212[0m
[37m[1m[2023-07-10 21:49:29,454][227910] Average Trajectory Length: 987.2993333333333[0m
[36m[2023-07-10 21:49:29,461][227910] mean_value=-281.33192130612247, max_value=1249.277579249353[0m
[37m[1m[2023-07-10 21:49:29,464][227910] New mean coefficients: [[3.4343624  0.11770615 2.287926   0.6178696  1.3992587 ]][0m
[37m[1m[2023-07-10 21:49:29,465][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:49:39,122][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 21:49:39,123][227910] FPS: 397676.82[0m
[36m[2023-07-10 21:49:39,125][227910] itr=1433, itrs=2000, Progress: 71.65%[0m
[36m[2023-07-10 21:49:50,751][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 21:49:50,751][227910] FPS: 330828.46[0m
[36m[2023-07-10 21:49:55,456][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:49:55,456][227910] Reward + Measures: [[966.94910333   0.73095167   0.57253635   0.62813735   0.63639534]][0m
[37m[1m[2023-07-10 21:49:55,457][227910] Max Reward on eval: 966.9491033317099[0m
[37m[1m[2023-07-10 21:49:55,457][227910] Min Reward on eval: 966.9491033317099[0m
[37m[1m[2023-07-10 21:49:55,457][227910] Mean Reward across all agents: 966.9491033317099[0m
[37m[1m[2023-07-10 21:49:55,457][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:50:00,910][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:50:00,911][227910] Reward + Measures: [[ -43.64572147    0.88120002    0.60179996    0.91610003    0.16049999]
 [ 270.13703805    0.93380004    0.29210001    0.88940001    0.76140004]
 [ 532.95522556    0.9479        0.60830003    0.90410006    0.62109995]
 ...
 [ 814.01721658    0.37740001    0.71680003    0.39559999    0.6397    ]
 [ 778.16243921    0.94          0.76630002    0.97059995    0.85960001]
 [-326.97934054    0.94480002    0.89239997    0.95850003    0.94580001]][0m
[37m[1m[2023-07-10 21:50:00,911][227910] Max Reward on eval: 1040.5120565493357[0m
[37m[1m[2023-07-10 21:50:00,912][227910] Min Reward on eval: -1246.980292275129[0m
[37m[1m[2023-07-10 21:50:00,912][227910] Mean Reward across all agents: 188.24556710333306[0m
[37m[1m[2023-07-10 21:50:00,912][227910] Average Trajectory Length: 997.605[0m
[36m[2023-07-10 21:50:00,919][227910] mean_value=-308.6514520827436, max_value=1195.5796090159274[0m
[37m[1m[2023-07-10 21:50:00,922][227910] New mean coefficients: [[ 3.449885   -0.28225288  3.1420193   1.5347592   1.1959715 ]][0m
[37m[1m[2023-07-10 21:50:00,923][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:50:10,621][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 21:50:10,621][227910] FPS: 396023.33[0m
[36m[2023-07-10 21:50:10,623][227910] itr=1434, itrs=2000, Progress: 71.70%[0m
[36m[2023-07-10 21:50:22,350][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 21:50:22,351][227910] FPS: 328066.90[0m
[36m[2023-07-10 21:50:27,244][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:50:27,244][227910] Reward + Measures: [[1043.72585009    0.65272629    0.6184383     0.61584103    0.63641399]][0m
[37m[1m[2023-07-10 21:50:27,244][227910] Max Reward on eval: 1043.7258500894466[0m
[37m[1m[2023-07-10 21:50:27,245][227910] Min Reward on eval: 1043.7258500894466[0m
[37m[1m[2023-07-10 21:50:27,245][227910] Mean Reward across all agents: 1043.7258500894466[0m
[37m[1m[2023-07-10 21:50:27,245][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:50:32,787][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:50:32,787][227910] Reward + Measures: [[ 474.05004739    0.68510002    0.2323        0.68849999    0.61199999]
 [ 117.61187575    0.83050007    0.40489998    0.80389994    0.1772    ]
 [-388.45571625    0.55849999    0.70060009    0.53740001    0.60820001]
 ...
 [-150.02121106    0.18789999    0.7604        0.56459999    0.73750007]
 [ 430.66940504    0.46090004    0.83409995    0.30130002    0.83640003]
 [ 310.98256605    0.84350008    0.26300001    0.87309998    0.46430001]][0m
[37m[1m[2023-07-10 21:50:32,788][227910] Max Reward on eval: 984.8595294401515[0m
[37m[1m[2023-07-10 21:50:32,788][227910] Min Reward on eval: -1130.7261683462536[0m
[37m[1m[2023-07-10 21:50:32,788][227910] Mean Reward across all agents: 32.94968677100411[0m
[37m[1m[2023-07-10 21:50:32,788][227910] Average Trajectory Length: 995.044[0m
[36m[2023-07-10 21:50:32,794][227910] mean_value=-442.80568462521825, max_value=1069.9525871613762[0m
[37m[1m[2023-07-10 21:50:32,797][227910] New mean coefficients: [[ 3.2258396  -0.6735271   2.624022    0.6319851   0.98615575]][0m
[37m[1m[2023-07-10 21:50:32,798][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:50:42,613][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 21:50:42,614][227910] FPS: 391296.71[0m
[36m[2023-07-10 21:50:42,616][227910] itr=1435, itrs=2000, Progress: 71.75%[0m
[36m[2023-07-10 21:50:54,192][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 21:50:54,192][227910] FPS: 332335.09[0m
[36m[2023-07-10 21:50:58,898][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:50:58,899][227910] Reward + Measures: [[1154.30717684    0.55832767    0.64825135    0.59555531    0.60868204]][0m
[37m[1m[2023-07-10 21:50:58,899][227910] Max Reward on eval: 1154.3071768365392[0m
[37m[1m[2023-07-10 21:50:58,899][227910] Min Reward on eval: 1154.3071768365392[0m
[37m[1m[2023-07-10 21:50:58,900][227910] Mean Reward across all agents: 1154.3071768365392[0m
[37m[1m[2023-07-10 21:50:58,900][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:51:04,469][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:51:04,470][227910] Reward + Measures: [[848.14007635   0.54279995   0.71580011   0.53119999   0.72470003]
 [864.57874771   0.20710002   0.8118       0.64620006   0.72480005]
 [296.79703042   0.51630008   0.77100003   0.26480001   0.81450003]
 ...
 [869.27615919   0.1578       0.88980007   0.61110002   0.81949997]
 [985.11666711   0.60620004   0.61910003   0.4982       0.64090002]
 [490.97642308   0.19820002   0.76990002   0.50349998   0.67449999]][0m
[37m[1m[2023-07-10 21:51:04,470][227910] Max Reward on eval: 1210.638282470405[0m
[37m[1m[2023-07-10 21:51:04,470][227910] Min Reward on eval: -615.7508226507925[0m
[37m[1m[2023-07-10 21:51:04,470][227910] Mean Reward across all agents: 733.007667437981[0m
[37m[1m[2023-07-10 21:51:04,471][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:51:04,480][227910] mean_value=402.70608958894974, max_value=1593.5590084047988[0m
[37m[1m[2023-07-10 21:51:04,483][227910] New mean coefficients: [[ 3.533991   -0.6976817   2.830814    1.8902261   0.91533107]][0m
[37m[1m[2023-07-10 21:51:04,484][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:51:14,146][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 21:51:14,147][227910] FPS: 397504.50[0m
[36m[2023-07-10 21:51:14,149][227910] itr=1436, itrs=2000, Progress: 71.80%[0m
[36m[2023-07-10 21:51:25,582][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 21:51:25,582][227910] FPS: 336419.42[0m
[36m[2023-07-10 21:51:30,429][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:51:30,429][227910] Reward + Measures: [[1276.79240621    0.37610865    0.68670833    0.57011366    0.5783357 ]][0m
[37m[1m[2023-07-10 21:51:30,429][227910] Max Reward on eval: 1276.7924062143754[0m
[37m[1m[2023-07-10 21:51:30,429][227910] Min Reward on eval: 1276.7924062143754[0m
[37m[1m[2023-07-10 21:51:30,430][227910] Mean Reward across all agents: 1276.7924062143754[0m
[37m[1m[2023-07-10 21:51:30,430][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:51:35,813][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:51:35,814][227910] Reward + Measures: [[  456.36311387     0.53319997     0.70060003     0.73369998
      0.64660007]
 [-1270.56661042     0.62019998     0.76179999     0.3529
      0.73740005]
 [  819.08522612     0.87019998     0.3281         0.84969997
      0.39739999]
 ...
 [  423.48243714     0.4434         0.72909999     0.52209997
      0.77689999]
 [  377.93872205     0.32269999     0.54729998     0.34130001
      0.6013    ]
 [  905.40751046     0.43790004     0.56750005     0.35569999
      0.59299999]][0m
[37m[1m[2023-07-10 21:51:35,814][227910] Max Reward on eval: 1209.5020699402316[0m
[37m[1m[2023-07-10 21:51:35,814][227910] Min Reward on eval: -1454.488922648481[0m
[37m[1m[2023-07-10 21:51:35,815][227910] Mean Reward across all agents: 233.62532833412817[0m
[37m[1m[2023-07-10 21:51:35,815][227910] Average Trajectory Length: 998.1403333333333[0m
[36m[2023-07-10 21:51:35,822][227910] mean_value=-236.96646715460093, max_value=1400.6744785354938[0m
[37m[1m[2023-07-10 21:51:35,825][227910] New mean coefficients: [[ 3.0191069  -0.00917029  1.8181096   0.25806153  1.2603769 ]][0m
[37m[1m[2023-07-10 21:51:35,826][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:51:45,475][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 21:51:45,475][227910] FPS: 398025.27[0m
[36m[2023-07-10 21:51:45,477][227910] itr=1437, itrs=2000, Progress: 71.85%[0m
[36m[2023-07-10 21:51:56,907][227910] train() took 11.41 seconds to complete[0m
[36m[2023-07-10 21:51:56,907][227910] FPS: 336510.78[0m
[36m[2023-07-10 21:52:01,646][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:52:01,646][227910] Reward + Measures: [[1474.71317549    0.29020432    0.69473958    0.55026263    0.50984466]][0m
[37m[1m[2023-07-10 21:52:01,647][227910] Max Reward on eval: 1474.713175488513[0m
[37m[1m[2023-07-10 21:52:01,647][227910] Min Reward on eval: 1474.713175488513[0m
[37m[1m[2023-07-10 21:52:01,647][227910] Mean Reward across all agents: 1474.713175488513[0m
[37m[1m[2023-07-10 21:52:01,647][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:52:07,092][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:52:07,093][227910] Reward + Measures: [[ 252.57343631    0.89629996    0.82050002    0.92740005    0.85229999]
 [ 810.57674773    0.1777        0.86660004    0.38030002    0.79940003]
 [ 769.20229281    0.98970002    0.97180003    0.98680001    0.9677    ]
 ...
 [ 202.19589018    0.52560008    0.63990003    0.53860003    0.27560002]
 [-488.06015861    0.93349999    0.93850005    0.92430001    0.86730003]
 [  93.78911581    0.98450005    0.98099995    0.98610002    0.96810001]][0m
[37m[1m[2023-07-10 21:52:07,093][227910] Max Reward on eval: 1276.3582260172814[0m
[37m[1m[2023-07-10 21:52:07,093][227910] Min Reward on eval: -893.3008813864901[0m
[37m[1m[2023-07-10 21:52:07,094][227910] Mean Reward across all agents: 526.8579033182647[0m
[37m[1m[2023-07-10 21:52:07,094][227910] Average Trajectory Length: 998.8526666666667[0m
[36m[2023-07-10 21:52:07,105][227910] mean_value=119.81508480776323, max_value=1230.1828675594413[0m
[37m[1m[2023-07-10 21:52:07,107][227910] New mean coefficients: [[ 2.824788    0.27768376  1.9245265  -0.00098935  0.6343349 ]][0m
[37m[1m[2023-07-10 21:52:07,109][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:52:16,873][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 21:52:16,873][227910] FPS: 393345.22[0m
[36m[2023-07-10 21:52:16,875][227910] itr=1438, itrs=2000, Progress: 71.90%[0m
[36m[2023-07-10 21:52:28,381][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 21:52:28,381][227910] FPS: 334407.34[0m
[36m[2023-07-10 21:52:33,105][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:52:33,105][227910] Reward + Measures: [[1862.88548676    0.25853434    0.64382035    0.43434894    0.37528336]][0m
[37m[1m[2023-07-10 21:52:33,105][227910] Max Reward on eval: 1862.8854867603152[0m
[37m[1m[2023-07-10 21:52:33,106][227910] Min Reward on eval: 1862.8854867603152[0m
[37m[1m[2023-07-10 21:52:33,106][227910] Mean Reward across all agents: 1862.8854867603152[0m
[37m[1m[2023-07-10 21:52:33,106][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:52:38,477][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:52:38,478][227910] Reward + Measures: [[ 854.02285755    0.65759999    0.57800001    0.59960002    0.65540004]
 [1187.13517208    0.57840008    0.44819999    0.55370003    0.43920001]
 [ 699.81766468    0.64799994    0.4021        0.66790003    0.4982    ]
 ...
 [-778.44229887    0.0803        0.87489998    0.5029        0.86210006]
 [ 525.75888119    0.51450002    0.45629999    0.51069999    0.4975    ]
 [ 273.48268874    0.79619998    0.2633        0.77670002    0.81010002]][0m
[37m[1m[2023-07-10 21:52:38,478][227910] Max Reward on eval: 1881.9097369211727[0m
[37m[1m[2023-07-10 21:52:38,478][227910] Min Reward on eval: -1016.1643155919504[0m
[37m[1m[2023-07-10 21:52:38,478][227910] Mean Reward across all agents: 633.3779296228992[0m
[37m[1m[2023-07-10 21:52:38,479][227910] Average Trajectory Length: 999.1186666666666[0m
[36m[2023-07-10 21:52:38,487][227910] mean_value=-24.432811181576664, max_value=1202.2715914819519[0m
[37m[1m[2023-07-10 21:52:38,490][227910] New mean coefficients: [[2.9447467  0.30556053 1.7230622  0.11410986 0.48743594]][0m
[37m[1m[2023-07-10 21:52:38,491][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:52:48,152][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 21:52:48,152][227910] FPS: 397549.26[0m
[36m[2023-07-10 21:52:48,154][227910] itr=1439, itrs=2000, Progress: 71.95%[0m
[36m[2023-07-10 21:52:59,632][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 21:52:59,632][227910] FPS: 335113.69[0m
[36m[2023-07-10 21:53:04,384][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:53:04,384][227910] Reward + Measures: [[2198.84881635    0.25670031    0.62562066    0.36120063    0.30537567]][0m
[37m[1m[2023-07-10 21:53:04,384][227910] Max Reward on eval: 2198.8488163546467[0m
[37m[1m[2023-07-10 21:53:04,385][227910] Min Reward on eval: 2198.8488163546467[0m
[37m[1m[2023-07-10 21:53:04,385][227910] Mean Reward across all agents: 2198.8488163546467[0m
[37m[1m[2023-07-10 21:53:04,385][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:53:09,810][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:53:09,811][227910] Reward + Measures: [[1307.53878403    0.27160001    0.54969996    0.4316        0.3457    ]
 [1144.96229866    0.52939999    0.56639999    0.59070003    0.55430001]
 [ 692.92968259    0.46869999    0.48979998    0.45649996    0.39330003]
 ...
 [1037.43086346    0.28280002    0.54430002    0.45910001    0.47420001]
 [-328.07439839    0.59860003    0.63770002    0.65869999    0.62589997]
 [ -77.19664193    0.17209999    0.72139996    0.44800001    0.64289999]][0m
[37m[1m[2023-07-10 21:53:09,811][227910] Max Reward on eval: 1827.4669626381249[0m
[37m[1m[2023-07-10 21:53:09,812][227910] Min Reward on eval: -1413.3895655371016[0m
[37m[1m[2023-07-10 21:53:09,812][227910] Mean Reward across all agents: 386.9253801738475[0m
[37m[1m[2023-07-10 21:53:09,812][227910] Average Trajectory Length: 996.6116666666667[0m
[36m[2023-07-10 21:53:09,819][227910] mean_value=-357.4630494444014, max_value=1458.0215722703153[0m
[37m[1m[2023-07-10 21:53:09,822][227910] New mean coefficients: [[ 2.8113117  -0.00188196  1.5936521   0.67445904  0.6297674 ]][0m
[37m[1m[2023-07-10 21:53:09,823][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:53:19,484][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 21:53:19,484][227910] FPS: 397540.56[0m
[36m[2023-07-10 21:53:19,487][227910] itr=1440, itrs=2000, Progress: 72.00%[0m
[37m[1m[2023-07-10 21:53:23,578][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001420[0m
[36m[2023-07-10 21:53:35,381][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 21:53:35,382][227910] FPS: 333104.36[0m
[36m[2023-07-10 21:53:40,238][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:53:40,238][227910] Reward + Measures: [[2517.94948763    0.26770732    0.59205365    0.31180069    0.24582067]][0m
[37m[1m[2023-07-10 21:53:40,238][227910] Max Reward on eval: 2517.9494876288754[0m
[37m[1m[2023-07-10 21:53:40,239][227910] Min Reward on eval: 2517.9494876288754[0m
[37m[1m[2023-07-10 21:53:40,239][227910] Mean Reward across all agents: 2517.9494876288754[0m
[37m[1m[2023-07-10 21:53:40,239][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:53:45,734][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:53:45,734][227910] Reward + Measures: [[461.13985267   0.43710002   0.92500001   0.37799999   0.93419999]
 [696.53520621   0.47750002   0.48289999   0.31830001   0.40560004]
 [537.41690213   0.42989999   0.92189997   0.37970001   0.90920001]
 ...
 [761.17510337   0.1357       0.74690002   0.38939998   0.64300007]
 [755.05466038   0.0661       0.89990008   0.44959998   0.85100001]
 [404.240221     0.16400002   0.87050003   0.22910002   0.85470003]][0m
[37m[1m[2023-07-10 21:53:45,735][227910] Max Reward on eval: 1964.874004382058[0m
[37m[1m[2023-07-10 21:53:45,735][227910] Min Reward on eval: -913.107145580498[0m
[37m[1m[2023-07-10 21:53:45,735][227910] Mean Reward across all agents: 514.9207995651769[0m
[37m[1m[2023-07-10 21:53:45,735][227910] Average Trajectory Length: 995.1853333333333[0m
[36m[2023-07-10 21:53:45,741][227910] mean_value=-500.45100653212927, max_value=843.0367052422405[0m
[37m[1m[2023-07-10 21:53:45,744][227910] New mean coefficients: [[ 2.950062   -0.42239365  1.2463815   0.8281877  -0.25773937]][0m
[37m[1m[2023-07-10 21:53:45,745][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:53:55,444][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 21:53:55,444][227910] FPS: 395966.00[0m
[36m[2023-07-10 21:53:55,447][227910] itr=1441, itrs=2000, Progress: 72.05%[0m
[36m[2023-07-10 21:54:07,027][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 21:54:07,027][227910] FPS: 332199.46[0m
[36m[2023-07-10 21:54:11,786][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:54:11,787][227910] Reward + Measures: [[2739.68911442    0.26138303    0.58666635    0.29668066    0.22846733]][0m
[37m[1m[2023-07-10 21:54:11,787][227910] Max Reward on eval: 2739.68911441501[0m
[37m[1m[2023-07-10 21:54:11,787][227910] Min Reward on eval: 2739.68911441501[0m
[37m[1m[2023-07-10 21:54:11,787][227910] Mean Reward across all agents: 2739.68911441501[0m
[37m[1m[2023-07-10 21:54:11,788][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:54:17,505][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:54:17,505][227910] Reward + Measures: [[ 325.78998978    0.72349995    0.9321        0.0644        0.89720005]
 [ 667.1417257     0.1087        0.81690007    0.40739998    0.79899997]
 [ 857.19652741    0.2631        0.67370003    0.51450002    0.69700003]
 ...
 [-482.34743388    0.16298118    0.22561918    0.17423539    0.27903667]
 [ -89.24218524    0.66420001    0.87080002    0.0447        0.81920004]
 [1071.08931608    0.49689999    0.73559999    0.42379999    0.68940002]][0m
[37m[1m[2023-07-10 21:54:17,506][227910] Max Reward on eval: 1770.531407677429[0m
[37m[1m[2023-07-10 21:54:17,506][227910] Min Reward on eval: -1194.7492098221671[0m
[37m[1m[2023-07-10 21:54:17,506][227910] Mean Reward across all agents: 357.32649974625025[0m
[37m[1m[2023-07-10 21:54:17,506][227910] Average Trajectory Length: 977.9716666666666[0m
[36m[2023-07-10 21:54:17,512][227910] mean_value=-639.648573758465, max_value=1390.6999867879028[0m
[37m[1m[2023-07-10 21:54:17,514][227910] New mean coefficients: [[ 2.9703922  -0.11543319  0.800365   -0.17817008  0.05827168]][0m
[37m[1m[2023-07-10 21:54:17,515][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:54:27,321][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 21:54:27,321][227910] FPS: 391659.77[0m
[36m[2023-07-10 21:54:27,323][227910] itr=1442, itrs=2000, Progress: 72.10%[0m
[36m[2023-07-10 21:54:38,908][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 21:54:38,909][227910] FPS: 332052.02[0m
[36m[2023-07-10 21:54:43,678][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:54:43,678][227910] Reward + Measures: [[2922.36311783    0.258991      0.56300366    0.29609331    0.22252899]][0m
[37m[1m[2023-07-10 21:54:43,678][227910] Max Reward on eval: 2922.363117830491[0m
[37m[1m[2023-07-10 21:54:43,679][227910] Min Reward on eval: 2922.363117830491[0m
[37m[1m[2023-07-10 21:54:43,679][227910] Mean Reward across all agents: 2922.363117830491[0m
[37m[1m[2023-07-10 21:54:43,679][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:54:49,080][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:54:49,081][227910] Reward + Measures: [[ 755.36691016    0.2414        0.71090001    0.64520001    0.60050005]
 [ 672.61822739    0.2352        0.69260001    0.28830001    0.77100003]
 [ 352.55102113    0.36396924    0.49735719    0.44181871    0.39017364]
 ...
 [ 766.95647299    0.60540003    0.64829999    0.44589996    0.68780005]
 [1592.34277272    0.235         0.68469995    0.43500003    0.38409999]
 [ 902.36849811    0.2739        0.76029998    0.33430001    0.71800005]][0m
[37m[1m[2023-07-10 21:54:49,081][227910] Max Reward on eval: 2557.310727804527[0m
[37m[1m[2023-07-10 21:54:49,081][227910] Min Reward on eval: -553.4782629862718[0m
[37m[1m[2023-07-10 21:54:49,081][227910] Mean Reward across all agents: 729.6817564113268[0m
[37m[1m[2023-07-10 21:54:49,082][227910] Average Trajectory Length: 995.5963333333333[0m
[36m[2023-07-10 21:54:49,090][227910] mean_value=-68.58929397891023, max_value=1494.2956251749215[0m
[37m[1m[2023-07-10 21:54:49,092][227910] New mean coefficients: [[ 3.1333003  -0.37763768  0.77630013  0.39878047 -0.00316752]][0m
[37m[1m[2023-07-10 21:54:49,094][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:54:58,928][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 21:54:58,928][227910] FPS: 390520.72[0m
[36m[2023-07-10 21:54:58,931][227910] itr=1443, itrs=2000, Progress: 72.15%[0m
[36m[2023-07-10 21:55:10,515][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 21:55:10,515][227910] FPS: 332084.32[0m
[36m[2023-07-10 21:55:15,430][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:55:15,435][227910] Reward + Measures: [[3092.85619773    0.25796333    0.54964995    0.28786266    0.21450934]][0m
[37m[1m[2023-07-10 21:55:15,436][227910] Max Reward on eval: 3092.8561977263416[0m
[37m[1m[2023-07-10 21:55:15,436][227910] Min Reward on eval: 3092.8561977263416[0m
[37m[1m[2023-07-10 21:55:15,436][227910] Mean Reward across all agents: 3092.8561977263416[0m
[37m[1m[2023-07-10 21:55:15,436][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:55:20,856][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:55:20,857][227910] Reward + Measures: [[1692.58574058    0.1832        0.61129999    0.51260006    0.46360001]
 [ 770.75139054    0.40180001    0.74489999    0.39630005    0.73649997]
 [ 641.33726938    0.83719999    0.0638        0.84280008    0.81129998]
 ...
 [ -75.10251076    0.60289997    0.60579997    0.36180001    0.41229996]
 [-789.96008984    0.1565        0.32360002    0.26299998    0.329     ]
 [ 234.50911876    0.39120001    0.85970002    0.37359998    0.83359998]][0m
[37m[1m[2023-07-10 21:55:20,857][227910] Max Reward on eval: 2822.9650964684784[0m
[37m[1m[2023-07-10 21:55:20,858][227910] Min Reward on eval: -789.9600898426376[0m
[37m[1m[2023-07-10 21:55:20,858][227910] Mean Reward across all agents: 620.9973556989579[0m
[37m[1m[2023-07-10 21:55:20,858][227910] Average Trajectory Length: 999.0243333333333[0m
[36m[2023-07-10 21:55:20,868][227910] mean_value=54.765219393321146, max_value=1408.2826598209795[0m
[37m[1m[2023-07-10 21:55:20,871][227910] New mean coefficients: [[ 3.0419102  -0.07928512  0.6629066   0.3496998   0.18262686]][0m
[37m[1m[2023-07-10 21:55:20,872][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:55:30,565][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 21:55:30,565][227910] FPS: 396219.32[0m
[36m[2023-07-10 21:55:30,567][227910] itr=1444, itrs=2000, Progress: 72.20%[0m
[36m[2023-07-10 21:55:42,138][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 21:55:42,139][227910] FPS: 332513.33[0m
[36m[2023-07-10 21:55:46,922][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:55:46,923][227910] Reward + Measures: [[3254.48109132    0.25975636    0.54586232    0.283474      0.21232967]][0m
[37m[1m[2023-07-10 21:55:46,923][227910] Max Reward on eval: 3254.4810913227584[0m
[37m[1m[2023-07-10 21:55:46,923][227910] Min Reward on eval: 3254.4810913227584[0m
[37m[1m[2023-07-10 21:55:46,923][227910] Mean Reward across all agents: 3254.4810913227584[0m
[37m[1m[2023-07-10 21:55:46,923][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:55:52,346][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:55:52,346][227910] Reward + Measures: [[ 990.4611808     0.6372        0.70409995    0.6759001     0.63140005]
 [-560.39189185    0.35641143    0.37029836    0.22270496    0.30465397]
 [ 820.24288283    0.40120003    0.29840001    0.4639        0.35800004]
 ...
 [ 328.91918277    0.53200006    0.33839998    0.49530002    0.3876    ]
 [ 864.96989018    0.4506        0.3159        0.51590008    0.34710002]
 [ 947.8175604     0.7568        0.17690001    0.78420001    0.78310001]][0m
[37m[1m[2023-07-10 21:55:52,347][227910] Max Reward on eval: 3053.073461997509[0m
[37m[1m[2023-07-10 21:55:52,347][227910] Min Reward on eval: -2303.981534975313[0m
[37m[1m[2023-07-10 21:55:52,347][227910] Mean Reward across all agents: 628.6835137809828[0m
[37m[1m[2023-07-10 21:55:52,347][227910] Average Trajectory Length: 998.194[0m
[36m[2023-07-10 21:55:52,355][227910] mean_value=-285.0491116405982, max_value=1090.857247962348[0m
[37m[1m[2023-07-10 21:55:52,357][227910] New mean coefficients: [[3.0429971  0.3848922  1.1298379  0.24456179 0.05608636]][0m
[37m[1m[2023-07-10 21:55:52,358][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:56:02,135][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 21:56:02,135][227910] FPS: 392826.87[0m
[36m[2023-07-10 21:56:02,138][227910] itr=1445, itrs=2000, Progress: 72.25%[0m
[36m[2023-07-10 21:56:13,615][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 21:56:13,615][227910] FPS: 335148.47[0m
[36m[2023-07-10 21:56:18,287][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:56:18,288][227910] Reward + Measures: [[1657.51998099    0.47073999    0.34142464    0.30074435    0.294229  ]][0m
[37m[1m[2023-07-10 21:56:18,288][227910] Max Reward on eval: 1657.519980992028[0m
[37m[1m[2023-07-10 21:56:18,288][227910] Min Reward on eval: 1657.519980992028[0m
[37m[1m[2023-07-10 21:56:18,288][227910] Mean Reward across all agents: 1657.519980992028[0m
[37m[1m[2023-07-10 21:56:18,289][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:56:23,717][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:56:23,717][227910] Reward + Measures: [[107.11786234   0.5323       0.36460003   0.52880001   0.0984    ]
 [-43.59498134   0.29819998   0.34220001   0.14570001   0.30489999]
 [526.07732168   0.6674       0.6038       0.53029996   0.4262    ]
 ...
 [893.9972944    0.35770002   0.54629999   0.25909999   0.49980003]
 [248.21624144   0.77940005   0.34300002   0.54000002   0.44080001]
 [140.62381395   0.3082       0.3484       0.40979996   0.2665    ]][0m
[37m[1m[2023-07-10 21:56:23,718][227910] Max Reward on eval: 1816.8700311633293[0m
[37m[1m[2023-07-10 21:56:23,718][227910] Min Reward on eval: -1852.859269835637[0m
[37m[1m[2023-07-10 21:56:23,718][227910] Mean Reward across all agents: 432.74182888267666[0m
[37m[1m[2023-07-10 21:56:23,718][227910] Average Trajectory Length: 994.5649999999999[0m
[36m[2023-07-10 21:56:23,723][227910] mean_value=-778.9720251772586, max_value=959.6365307152039[0m
[37m[1m[2023-07-10 21:56:23,725][227910] New mean coefficients: [[ 3.2740426  -0.00588956  1.1342188   0.6104783  -0.07457632]][0m
[37m[1m[2023-07-10 21:56:23,726][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:56:33,457][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 21:56:33,457][227910] FPS: 394704.36[0m
[36m[2023-07-10 21:56:33,459][227910] itr=1446, itrs=2000, Progress: 72.30%[0m
[36m[2023-07-10 21:56:45,022][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 21:56:45,022][227910] FPS: 332745.08[0m
[36m[2023-07-10 21:56:49,870][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:56:49,871][227910] Reward + Measures: [[2004.32052601    0.40737361    0.3219108     0.30665383    0.24607243]][0m
[37m[1m[2023-07-10 21:56:49,871][227910] Max Reward on eval: 2004.320526010317[0m
[37m[1m[2023-07-10 21:56:49,871][227910] Min Reward on eval: 2004.320526010317[0m
[37m[1m[2023-07-10 21:56:49,871][227910] Mean Reward across all agents: 2004.320526010317[0m
[37m[1m[2023-07-10 21:56:49,872][227910] Average Trajectory Length: 999.7223333333333[0m
[36m[2023-07-10 21:56:55,477][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:56:55,478][227910] Reward + Measures: [[ -82.86740817    0.19240001    0.36080003    0.21620002    0.27410004]
 [ 923.4228787     0.63800001    0.31959999    0.49489999    0.38680002]
 [ 662.38973795    0.257         0.3946        0.23849998    0.27860001]
 ...
 [ 673.98898817    0.28706884    0.27284098    0.33805081    0.27193281]
 [1325.54078437    0.33820006    0.46399999    0.32710001    0.28310001]
 [ 427.77908178    0.29340002    0.5546        0.46599999    0.71360004]][0m
[37m[1m[2023-07-10 21:56:55,478][227910] Max Reward on eval: 2093.7732032748404[0m
[37m[1m[2023-07-10 21:56:55,479][227910] Min Reward on eval: -1223.841973367799[0m
[37m[1m[2023-07-10 21:56:55,479][227910] Mean Reward across all agents: 719.5824554754723[0m
[37m[1m[2023-07-10 21:56:55,479][227910] Average Trajectory Length: 980.2746666666667[0m
[36m[2023-07-10 21:56:55,482][227910] mean_value=-1086.0304956851194, max_value=952.4430673369404[0m
[37m[1m[2023-07-10 21:56:55,485][227910] New mean coefficients: [[ 3.0605989   0.12109764  1.7645807  -0.32595026 -0.3816188 ]][0m
[37m[1m[2023-07-10 21:56:55,486][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:57:05,199][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 21:57:05,199][227910] FPS: 395414.93[0m
[36m[2023-07-10 21:57:05,202][227910] itr=1447, itrs=2000, Progress: 72.35%[0m
[36m[2023-07-10 21:57:16,827][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 21:57:16,827][227910] FPS: 330870.59[0m
[36m[2023-07-10 21:57:21,732][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:57:21,733][227910] Reward + Measures: [[899.5712665    0.07111766   0.89844531   0.37517068   0.84683526]][0m
[37m[1m[2023-07-10 21:57:21,733][227910] Max Reward on eval: 899.5712664958597[0m
[37m[1m[2023-07-10 21:57:21,733][227910] Min Reward on eval: 899.5712664958597[0m
[37m[1m[2023-07-10 21:57:21,733][227910] Mean Reward across all agents: 899.5712664958597[0m
[37m[1m[2023-07-10 21:57:21,733][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:57:27,360][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:57:27,360][227910] Reward + Measures: [[ 849.21160707    0.29499999    0.6099        0.47329998    0.53590006]
 [-112.46179064    0.5068        0.31060001    0.43260002    0.22679999]
 [ 854.39225395    0.2246        0.62320006    0.3897        0.52109998]
 ...
 [1181.0945029     0.56210005    0.5104        0.50640005    0.31670001]
 [ 877.24442188    0.30060002    0.49680004    0.34600002    0.5       ]
 [ 659.76860775    0.45439997    0.3682        0.3804        0.48599997]][0m
[37m[1m[2023-07-10 21:57:27,360][227910] Max Reward on eval: 1552.508375314236[0m
[37m[1m[2023-07-10 21:57:27,361][227910] Min Reward on eval: -495.7745981982036[0m
[37m[1m[2023-07-10 21:57:27,361][227910] Mean Reward across all agents: 666.4955584281295[0m
[37m[1m[2023-07-10 21:57:27,361][227910] Average Trajectory Length: 999.3786666666666[0m
[36m[2023-07-10 21:57:27,369][227910] mean_value=-37.72068200645673, max_value=1169.8093672510654[0m
[37m[1m[2023-07-10 21:57:27,372][227910] New mean coefficients: [[ 3.1471121   0.04068722  1.9459182  -0.25072062 -0.48671418]][0m
[37m[1m[2023-07-10 21:57:27,373][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:57:37,218][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 21:57:37,218][227910] FPS: 390113.48[0m
[36m[2023-07-10 21:57:37,220][227910] itr=1448, itrs=2000, Progress: 72.40%[0m
[36m[2023-07-10 21:57:48,820][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 21:57:48,821][227910] FPS: 331672.83[0m
[36m[2023-07-10 21:57:53,568][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:57:53,568][227910] Reward + Measures: [[1339.52247315    0.16581543    0.74039853    0.34499422    0.61791027]][0m
[37m[1m[2023-07-10 21:57:53,568][227910] Max Reward on eval: 1339.522473153121[0m
[37m[1m[2023-07-10 21:57:53,569][227910] Min Reward on eval: 1339.522473153121[0m
[37m[1m[2023-07-10 21:57:53,569][227910] Mean Reward across all agents: 1339.522473153121[0m
[37m[1m[2023-07-10 21:57:53,569][227910] Average Trajectory Length: 999.0676666666666[0m
[36m[2023-07-10 21:57:59,007][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:57:59,007][227910] Reward + Measures: [[ 656.03981986    0.42990002    0.6505        0.4601        0.57050002]
 [  70.08707402    0.71509999    0.75839996    0.75639999    0.7967    ]
 [1092.79437469    0.16240001    0.62190002    0.33150002    0.48990002]
 ...
 [1381.95653788    0.27700001    0.61420006    0.33770004    0.42490003]
 [ 967.86620996    0.30700001    0.39390001    0.45970002    0.35929999]
 [ 680.5809087     0.34060001    0.44340006    0.61720008    0.63300002]][0m
[37m[1m[2023-07-10 21:57:59,008][227910] Max Reward on eval: 1871.2648090050322[0m
[37m[1m[2023-07-10 21:57:59,008][227910] Min Reward on eval: -620.1835840449669[0m
[37m[1m[2023-07-10 21:57:59,008][227910] Mean Reward across all agents: 759.5644762294465[0m
[37m[1m[2023-07-10 21:57:59,008][227910] Average Trajectory Length: 999.0253333333333[0m
[36m[2023-07-10 21:57:59,015][227910] mean_value=-8.265528796735598, max_value=1541.2931636960293[0m
[37m[1m[2023-07-10 21:57:59,017][227910] New mean coefficients: [[ 3.5975547  -0.04059415  2.2715771   0.41348183 -0.51345855]][0m
[37m[1m[2023-07-10 21:57:59,018][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:58:08,839][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 21:58:08,839][227910] FPS: 391097.35[0m
[36m[2023-07-10 21:58:08,841][227910] itr=1449, itrs=2000, Progress: 72.45%[0m
[36m[2023-07-10 21:58:20,319][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 21:58:20,320][227910] FPS: 335145.20[0m
[36m[2023-07-10 21:58:24,986][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:58:24,987][227910] Reward + Measures: [[1998.80901281    0.22967866    0.51999664    0.32808915    0.33597267]][0m
[37m[1m[2023-07-10 21:58:24,987][227910] Max Reward on eval: 1998.809012809307[0m
[37m[1m[2023-07-10 21:58:24,987][227910] Min Reward on eval: 1998.809012809307[0m
[37m[1m[2023-07-10 21:58:24,988][227910] Mean Reward across all agents: 1998.809012809307[0m
[37m[1m[2023-07-10 21:58:24,988][227910] Average Trajectory Length: 996.5889999999999[0m
[36m[2023-07-10 21:58:30,271][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:58:30,277][227910] Reward + Measures: [[ 904.27034156    0.45660001    0.50650001    0.73089999    0.47659999]
 [1937.38933741    0.25920001    0.3847        0.32070002    0.1591    ]
 [ 729.18612665    0.35549998    0.82270002    0.71210003    0.73089999]
 ...
 [1558.12611918    0.27200001    0.38349998    0.35059997    0.1963    ]
 [-148.36876072    0.25890002    0.34989998    0.3558        0.32979998]
 [ 505.74532457    0.30559999    0.36430001    0.48280001    0.37359998]][0m
[37m[1m[2023-07-10 21:58:30,277][227910] Max Reward on eval: 2949.463233180344[0m
[37m[1m[2023-07-10 21:58:30,277][227910] Min Reward on eval: -761.3921706620953[0m
[37m[1m[2023-07-10 21:58:30,277][227910] Mean Reward across all agents: 663.8486756719817[0m
[37m[1m[2023-07-10 21:58:30,278][227910] Average Trajectory Length: 995.9846666666666[0m
[36m[2023-07-10 21:58:30,281][227910] mean_value=-953.4804810089211, max_value=1334.0289406265042[0m
[37m[1m[2023-07-10 21:58:30,284][227910] New mean coefficients: [[ 3.0709393  -0.16010684  1.2613591  -0.3423769  -0.9160947 ]][0m
[37m[1m[2023-07-10 21:58:30,285][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:58:40,092][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 21:58:40,093][227910] FPS: 391603.91[0m
[36m[2023-07-10 21:58:40,095][227910] itr=1450, itrs=2000, Progress: 72.50%[0m
[37m[1m[2023-07-10 21:58:44,222][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001430[0m
[36m[2023-07-10 21:58:56,048][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 21:58:56,054][227910] FPS: 332386.39[0m
[36m[2023-07-10 21:59:00,845][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:59:00,846][227910] Reward + Measures: [[2465.88696179    0.2419129     0.39979792    0.31317216    0.20515478]][0m
[37m[1m[2023-07-10 21:59:00,846][227910] Max Reward on eval: 2465.8869617925666[0m
[37m[1m[2023-07-10 21:59:00,846][227910] Min Reward on eval: 2465.8869617925666[0m
[37m[1m[2023-07-10 21:59:00,846][227910] Mean Reward across all agents: 2465.8869617925666[0m
[37m[1m[2023-07-10 21:59:00,847][227910] Average Trajectory Length: 996.685[0m
[36m[2023-07-10 21:59:06,399][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:59:06,399][227910] Reward + Measures: [[316.4157425    0.73620003   0.34999999   0.78710002   0.38960001]
 [344.56373268   0.48370001   0.50519997   0.7155       0.78119993]
 [314.68017976   0.45310003   0.73780006   0.80419999   0.6523    ]
 ...
 [-97.39045007   0.23819999   0.70139998   0.62760001   0.68230003]
 [118.85876886   0.76410002   0.74639994   0.3723       0.77440006]
 [252.70930956   0.41589999   0.8405       0.84320003   0.82010001]][0m
[37m[1m[2023-07-10 21:59:06,400][227910] Max Reward on eval: 2233.4191566638183[0m
[37m[1m[2023-07-10 21:59:06,400][227910] Min Reward on eval: -830.662053511769[0m
[37m[1m[2023-07-10 21:59:06,400][227910] Mean Reward across all agents: 412.52562199436915[0m
[37m[1m[2023-07-10 21:59:06,400][227910] Average Trajectory Length: 998.9753333333333[0m
[36m[2023-07-10 21:59:06,407][227910] mean_value=-297.16425098344104, max_value=1119.5275086038623[0m
[37m[1m[2023-07-10 21:59:06,410][227910] New mean coefficients: [[ 3.1441083  -0.37355393  1.1418008   0.10676771 -1.0046495 ]][0m
[37m[1m[2023-07-10 21:59:06,411][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:59:16,326][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 21:59:16,326][227910] FPS: 387347.71[0m
[36m[2023-07-10 21:59:16,328][227910] itr=1451, itrs=2000, Progress: 72.55%[0m
[36m[2023-07-10 21:59:27,951][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 21:59:27,951][227910] FPS: 331041.12[0m
[36m[2023-07-10 21:59:32,720][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:59:32,721][227910] Reward + Measures: [[1592.7575581     0.28213632    0.45957869    0.33924133    0.29153165]][0m
[37m[1m[2023-07-10 21:59:32,721][227910] Max Reward on eval: 1592.7575580961022[0m
[37m[1m[2023-07-10 21:59:32,721][227910] Min Reward on eval: 1592.7575580961022[0m
[37m[1m[2023-07-10 21:59:32,721][227910] Mean Reward across all agents: 1592.7575580961022[0m
[37m[1m[2023-07-10 21:59:32,721][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 21:59:38,058][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 21:59:38,059][227910] Reward + Measures: [[1257.36197838    0.24510002    0.48559999    0.35409999    0.33249998]
 [1150.61472541    0.26120001    0.47799999    0.3881        0.36450002]
 [ 686.55935526    0.2349        0.68549997    0.38180003    0.51950002]
 ...
 [1079.736508      0.25750002    0.64720005    0.41570002    0.43260002]
 [1077.938684      0.28839999    0.43870002    0.35880002    0.31130001]
 [-186.01818809    0.1892014     0.40933472    0.35050496    0.38012546]][0m
[37m[1m[2023-07-10 21:59:38,059][227910] Max Reward on eval: 1890.0349478615449[0m
[37m[1m[2023-07-10 21:59:38,059][227910] Min Reward on eval: -186.018188091254[0m
[37m[1m[2023-07-10 21:59:38,060][227910] Mean Reward across all agents: 861.7788884982589[0m
[37m[1m[2023-07-10 21:59:38,060][227910] Average Trajectory Length: 992.0603333333333[0m
[36m[2023-07-10 21:59:38,063][227910] mean_value=-562.8617503520395, max_value=1095.5190114402726[0m
[37m[1m[2023-07-10 21:59:38,065][227910] New mean coefficients: [[ 3.632598   -0.99125326  1.5915672   0.36772656 -1.4266644 ]][0m
[37m[1m[2023-07-10 21:59:38,066][227910] Moving the mean solution point...[0m
[36m[2023-07-10 21:59:47,935][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 21:59:47,935][227910] FPS: 389169.47[0m
[36m[2023-07-10 21:59:47,937][227910] itr=1452, itrs=2000, Progress: 72.60%[0m
[36m[2023-07-10 21:59:59,663][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 21:59:59,664][227910] FPS: 328105.81[0m
[36m[2023-07-10 22:00:04,494][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:00:04,494][227910] Reward + Measures: [[2153.31604085    0.22845493    0.58844525    0.30355382    0.24257386]][0m
[37m[1m[2023-07-10 22:00:04,494][227910] Max Reward on eval: 2153.316040852461[0m
[37m[1m[2023-07-10 22:00:04,495][227910] Min Reward on eval: 2153.316040852461[0m
[37m[1m[2023-07-10 22:00:04,495][227910] Mean Reward across all agents: 2153.316040852461[0m
[37m[1m[2023-07-10 22:00:04,495][227910] Average Trajectory Length: 998.9226666666666[0m
[36m[2023-07-10 22:00:10,181][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:00:10,181][227910] Reward + Measures: [[-217.24314635    0.1806        0.63789999    0.42589998    0.57580006]
 [2052.96650569    0.2167        0.54910004    0.329         0.2418    ]
 [1604.27794746    0.2525        0.63809997    0.32500002    0.33140001]
 ...
 [ 341.38608275    0.509         0.5844        0.54550004    0.43450004]
 [-496.73969733    0.74309999    0.7579        0.80760002    0.63090003]
 [ 231.28278283    0.24531177    0.34909412    0.20282941    0.19819412]][0m
[37m[1m[2023-07-10 22:00:10,181][227910] Max Reward on eval: 2052.966505693842[0m
[37m[1m[2023-07-10 22:00:10,182][227910] Min Reward on eval: -814.3062241256587[0m
[37m[1m[2023-07-10 22:00:10,182][227910] Mean Reward across all agents: 757.7687646188784[0m
[37m[1m[2023-07-10 22:00:10,182][227910] Average Trajectory Length: 989.0963333333333[0m
[36m[2023-07-10 22:00:10,185][227910] mean_value=-1804.7164425594717, max_value=925.7881694394397[0m
[37m[1m[2023-07-10 22:00:10,187][227910] New mean coefficients: [[ 3.802741   -0.33424526  1.9656887   0.9938559  -2.1162744 ]][0m
[37m[1m[2023-07-10 22:00:10,188][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:00:19,897][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 22:00:19,897][227910] FPS: 395601.98[0m
[36m[2023-07-10 22:00:19,899][227910] itr=1453, itrs=2000, Progress: 72.65%[0m
[36m[2023-07-10 22:00:31,420][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 22:00:31,421][227910] FPS: 333842.76[0m
[36m[2023-07-10 22:00:36,305][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:00:36,305][227910] Reward + Measures: [[2011.46194513    0.26032457    0.46011677    0.33120263    0.13021986]][0m
[37m[1m[2023-07-10 22:00:36,305][227910] Max Reward on eval: 2011.4619451339981[0m
[37m[1m[2023-07-10 22:00:36,305][227910] Min Reward on eval: 2011.4619451339981[0m
[37m[1m[2023-07-10 22:00:36,306][227910] Mean Reward across all agents: 2011.4619451339981[0m
[37m[1m[2023-07-10 22:00:36,306][227910] Average Trajectory Length: 999.6709999999999[0m
[36m[2023-07-10 22:00:41,779][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:00:41,779][227910] Reward + Measures: [[ 991.13160767    0.24089999    0.35340002    0.27809998    0.1973    ]
 [ 769.99342995    0.2274396     0.31337291    0.24174166    0.17794792]
 [1482.86596747    0.2789        0.3364        0.27010003    0.1724    ]
 ...
 [ 877.41059459    0.24425831    0.30980274    0.26040646    0.18780299]
 [ 105.16877273    0.22790001    0.32440001    0.22000001    0.19590001]
 [ 237.55817006    0.23930001    0.32099998    0.21900001    0.20969999]][0m
[37m[1m[2023-07-10 22:00:41,779][227910] Max Reward on eval: 2255.890246342914[0m
[37m[1m[2023-07-10 22:00:41,780][227910] Min Reward on eval: -1116.5647896829062[0m
[37m[1m[2023-07-10 22:00:41,780][227910] Mean Reward across all agents: 508.13858132000496[0m
[37m[1m[2023-07-10 22:00:41,780][227910] Average Trajectory Length: 955.794[0m
[36m[2023-07-10 22:00:41,782][227910] mean_value=-3515.743557954796, max_value=713.1497089542457[0m
[37m[1m[2023-07-10 22:00:41,784][227910] New mean coefficients: [[ 3.2977893   0.14689323  1.0998896   0.54406226 -1.5991566 ]][0m
[37m[1m[2023-07-10 22:00:41,785][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:00:51,536][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 22:00:51,536][227910] FPS: 393871.17[0m
[36m[2023-07-10 22:00:51,539][227910] itr=1454, itrs=2000, Progress: 72.70%[0m
[36m[2023-07-10 22:01:03,164][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 22:01:03,165][227910] FPS: 330838.96[0m
[36m[2023-07-10 22:01:07,903][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:01:07,904][227910] Reward + Measures: [[2308.01576165    0.2439132     0.43739071    0.32401586    0.13404998]][0m
[37m[1m[2023-07-10 22:01:07,904][227910] Max Reward on eval: 2308.0157616536735[0m
[37m[1m[2023-07-10 22:01:07,904][227910] Min Reward on eval: 2308.0157616536735[0m
[37m[1m[2023-07-10 22:01:07,904][227910] Mean Reward across all agents: 2308.0157616536735[0m
[37m[1m[2023-07-10 22:01:07,905][227910] Average Trajectory Length: 999.8233333333333[0m
[36m[2023-07-10 22:01:13,322][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:01:13,322][227910] Reward + Measures: [[1148.51955796    0.23100002    0.52789998    0.3105        0.29569998]
 [-577.11203773    0.41868064    0.48838496    0.45725375    0.06510538]
 [1345.68657507    0.27649999    0.37540004    0.257         0.16950002]
 ...
 [ 567.10237649    0.21885906    0.37041038    0.30419895    0.25630397]
 [ 688.27157754    0.25675833    0.39933023    0.29546249    0.19535938]
 [ -45.00933516    0.15753607    0.38433981    0.25531694    0.26884437]][0m
[37m[1m[2023-07-10 22:01:13,322][227910] Max Reward on eval: 2181.2056764794515[0m
[37m[1m[2023-07-10 22:01:13,323][227910] Min Reward on eval: -577.1120377321262[0m
[37m[1m[2023-07-10 22:01:13,323][227910] Mean Reward across all agents: 604.170513095918[0m
[37m[1m[2023-07-10 22:01:13,323][227910] Average Trajectory Length: 956.0313333333334[0m
[36m[2023-07-10 22:01:13,326][227910] mean_value=-2187.8818283910746, max_value=1243.8108492318052[0m
[37m[1m[2023-07-10 22:01:13,329][227910] New mean coefficients: [[ 3.138857   -0.3253732   1.3925401  -0.03064513 -1.3497704 ]][0m
[37m[1m[2023-07-10 22:01:13,330][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:01:22,962][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 22:01:22,962][227910] FPS: 398732.77[0m
[36m[2023-07-10 22:01:22,964][227910] itr=1455, itrs=2000, Progress: 72.75%[0m
[36m[2023-07-10 22:01:34,446][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 22:01:34,447][227910] FPS: 334974.48[0m
[36m[2023-07-10 22:01:39,225][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:01:39,225][227910] Reward + Measures: [[2492.2615536     0.23851399    0.42624494    0.31856263    0.13567153]][0m
[37m[1m[2023-07-10 22:01:39,226][227910] Max Reward on eval: 2492.2615536026265[0m
[37m[1m[2023-07-10 22:01:39,226][227910] Min Reward on eval: 2492.2615536026265[0m
[37m[1m[2023-07-10 22:01:39,226][227910] Mean Reward across all agents: 2492.2615536026265[0m
[37m[1m[2023-07-10 22:01:39,226][227910] Average Trajectory Length: 999.4433333333333[0m
[36m[2023-07-10 22:01:44,646][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:01:44,647][227910] Reward + Measures: [[ 405.41322451    0.36900002    0.66650003    0.27260002    0.68400007]
 [2094.98318478    0.2264        0.32430002    0.33600003    0.1309    ]
 [ 515.74361473    0.20109999    0.82380009    0.28080001    0.70810002]
 ...
 [ 353.14731498    0.2464        0.86920005    0.1204        0.80590004]
 [ 472.90100099    0.31168002    0.23589841    0.29073897    0.17267488]
 [ 706.13718344    0.39640003    0.54529995    0.32930002    0.42869997]][0m
[37m[1m[2023-07-10 22:01:44,647][227910] Max Reward on eval: 2412.400274639949[0m
[37m[1m[2023-07-10 22:01:44,647][227910] Min Reward on eval: -1388.3240928093205[0m
[37m[1m[2023-07-10 22:01:44,648][227910] Mean Reward across all agents: 696.9632929419565[0m
[37m[1m[2023-07-10 22:01:44,648][227910] Average Trajectory Length: 985.0866666666666[0m
[36m[2023-07-10 22:01:44,650][227910] mean_value=-1338.560033993332, max_value=318.4579976917173[0m
[37m[1m[2023-07-10 22:01:44,652][227910] New mean coefficients: [[ 2.9619336  -0.17181718  2.0108132   0.39085606 -1.3994447 ]][0m
[37m[1m[2023-07-10 22:01:44,653][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:01:54,284][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 22:01:54,284][227910] FPS: 398771.01[0m
[36m[2023-07-10 22:01:54,287][227910] itr=1456, itrs=2000, Progress: 72.80%[0m
[36m[2023-07-10 22:02:05,917][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 22:02:05,917][227910] FPS: 330745.70[0m
[36m[2023-07-10 22:02:10,778][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:02:10,779][227910] Reward + Measures: [[2769.392128      0.22438566    0.40757009    0.31697485    0.14070071]][0m
[37m[1m[2023-07-10 22:02:10,779][227910] Max Reward on eval: 2769.3921280018562[0m
[37m[1m[2023-07-10 22:02:10,779][227910] Min Reward on eval: 2769.3921280018562[0m
[37m[1m[2023-07-10 22:02:10,779][227910] Mean Reward across all agents: 2769.3921280018562[0m
[37m[1m[2023-07-10 22:02:10,780][227910] Average Trajectory Length: 999.4636666666667[0m
[36m[2023-07-10 22:02:16,319][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:02:16,320][227910] Reward + Measures: [[-786.29854093    0.36520001    0.39160001    0.28340003    0.33270001]
 [1749.46227874    0.21040002    0.56770003    0.3087        0.25489998]
 [ 339.47062131    0.3664        0.2674        0.2577        0.1815    ]
 ...
 [ 839.2321115     0.32699999    0.35440001    0.24489999    0.0911    ]
 [-310.88766035    0.32385013    0.18614446    0.18734467    0.20667098]
 [  53.96621457    0.31290001    0.28040001    0.24159999    0.21219997]][0m
[37m[1m[2023-07-10 22:02:16,320][227910] Max Reward on eval: 2709.621303134132[0m
[37m[1m[2023-07-10 22:02:16,321][227910] Min Reward on eval: -1220.573045887379[0m
[37m[1m[2023-07-10 22:02:16,321][227910] Mean Reward across all agents: 544.1379662983006[0m
[37m[1m[2023-07-10 22:02:16,321][227910] Average Trajectory Length: 972.1413333333333[0m
[36m[2023-07-10 22:02:16,323][227910] mean_value=-3242.619110409251, max_value=774.2958260388225[0m
[37m[1m[2023-07-10 22:02:16,326][227910] New mean coefficients: [[ 2.794661   -0.3058131   0.4243996   0.14340107 -1.3460507 ]][0m
[37m[1m[2023-07-10 22:02:16,326][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:02:26,119][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 22:02:26,119][227910] FPS: 392196.86[0m
[36m[2023-07-10 22:02:26,122][227910] itr=1457, itrs=2000, Progress: 72.85%[0m
[36m[2023-07-10 22:02:37,806][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 22:02:37,807][227910] FPS: 329273.82[0m
[36m[2023-07-10 22:02:42,601][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:02:42,601][227910] Reward + Measures: [[2925.85336141    0.22057328    0.41012618    0.32128224    0.14195393]][0m
[37m[1m[2023-07-10 22:02:42,601][227910] Max Reward on eval: 2925.853361407067[0m
[37m[1m[2023-07-10 22:02:42,602][227910] Min Reward on eval: 2925.853361407067[0m
[37m[1m[2023-07-10 22:02:42,602][227910] Mean Reward across all agents: 2925.853361407067[0m
[37m[1m[2023-07-10 22:02:42,602][227910] Average Trajectory Length: 998.0923333333333[0m
[36m[2023-07-10 22:02:48,247][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:02:48,248][227910] Reward + Measures: [[2500.25409839    0.21039999    0.47760001    0.31660002    0.1455    ]
 [1153.40052488    0.34122053    0.39805147    0.45568076    0.26633349]
 [1003.84056375    0.1970132     0.54604524    0.30959055    0.26671323]
 ...
 [1602.2829899     0.2343        0.42570001    0.40149999    0.2343    ]
 [ 884.76000634    0.25760001    0.45029998    0.45650002    0.31070003]
 [ 771.83475539    0.18550001    0.42939997    0.32040003    0.175     ]][0m
[37m[1m[2023-07-10 22:02:48,248][227910] Max Reward on eval: 2662.659114628611[0m
[37m[1m[2023-07-10 22:02:48,248][227910] Min Reward on eval: -1116.807144133479[0m
[37m[1m[2023-07-10 22:02:48,248][227910] Mean Reward across all agents: 842.704508724187[0m
[37m[1m[2023-07-10 22:02:48,249][227910] Average Trajectory Length: 977.6156666666666[0m
[36m[2023-07-10 22:02:48,252][227910] mean_value=-1282.7153977701694, max_value=1152.58555572147[0m
[37m[1m[2023-07-10 22:02:48,255][227910] New mean coefficients: [[ 2.7572937  -0.62431717 -0.15269744  0.2348343  -1.7912972 ]][0m
[37m[1m[2023-07-10 22:02:48,256][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:02:58,124][227910] train() took 9.87 seconds to complete[0m
[36m[2023-07-10 22:02:58,124][227910] FPS: 389193.92[0m
[36m[2023-07-10 22:02:58,126][227910] itr=1458, itrs=2000, Progress: 72.90%[0m
[36m[2023-07-10 22:03:09,691][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 22:03:09,691][227910] FPS: 332690.65[0m
[36m[2023-07-10 22:03:14,457][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:03:14,458][227910] Reward + Measures: [[3061.56158002    0.21868837    0.41465396    0.3267369     0.1439954 ]][0m
[37m[1m[2023-07-10 22:03:14,458][227910] Max Reward on eval: 3061.561580016937[0m
[37m[1m[2023-07-10 22:03:14,458][227910] Min Reward on eval: 3061.561580016937[0m
[37m[1m[2023-07-10 22:03:14,458][227910] Mean Reward across all agents: 3061.561580016937[0m
[37m[1m[2023-07-10 22:03:14,458][227910] Average Trajectory Length: 999.567[0m
[36m[2023-07-10 22:03:19,870][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:03:19,876][227910] Reward + Measures: [[ 698.01317971    0.231557      0.33907786    0.29896384    0.04091379]
 [  46.96537936    0.43524286    0.50531429    0.57385719    0.16712858]
 [ 243.47452305    0.15301691    0.56563926    0.48621836    0.39582992]
 ...
 [-491.45814181    0.46630001    0.70180005    0.28470001    0.67979997]
 [ 955.84392184    0.2287        0.49120003    0.3529        0.2158    ]
 [ 825.49892718    0.28029999    0.3646        0.27919999    0.15439999]][0m
[37m[1m[2023-07-10 22:03:19,876][227910] Max Reward on eval: 2659.8355676291512[0m
[37m[1m[2023-07-10 22:03:19,877][227910] Min Reward on eval: -1143.4485842493827[0m
[37m[1m[2023-07-10 22:03:19,877][227910] Mean Reward across all agents: 815.7873526475214[0m
[37m[1m[2023-07-10 22:03:19,877][227910] Average Trajectory Length: 946.3353333333333[0m
[36m[2023-07-10 22:03:19,879][227910] mean_value=-2336.1079331060396, max_value=337.5605214133885[0m
[37m[1m[2023-07-10 22:03:19,882][227910] New mean coefficients: [[ 2.7757678  -0.28425002 -0.5140699   0.1529189  -1.4834348 ]][0m
[37m[1m[2023-07-10 22:03:19,883][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:03:29,585][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 22:03:29,585][227910] FPS: 395857.62[0m
[36m[2023-07-10 22:03:29,587][227910] itr=1459, itrs=2000, Progress: 72.95%[0m
[36m[2023-07-10 22:03:41,107][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 22:03:41,107][227910] FPS: 334000.10[0m
[36m[2023-07-10 22:03:45,862][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:03:45,863][227910] Reward + Measures: [[3221.58006488    0.21671437    0.42416447    0.32959673    0.14635882]][0m
[37m[1m[2023-07-10 22:03:45,863][227910] Max Reward on eval: 3221.5800648823174[0m
[37m[1m[2023-07-10 22:03:45,863][227910] Min Reward on eval: 3221.5800648823174[0m
[37m[1m[2023-07-10 22:03:45,863][227910] Mean Reward across all agents: 3221.5800648823174[0m
[37m[1m[2023-07-10 22:03:45,863][227910] Average Trajectory Length: 999.0936666666666[0m
[36m[2023-07-10 22:03:51,206][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:03:51,206][227910] Reward + Measures: [[ 146.06147098    0.2211        0.3673        0.22019999    0.14750002]
 [1534.65240707    0.2448        0.55180001    0.40420005    0.1893    ]
 [1179.91209621    0.2201        0.31          0.29319999    0.1366    ]
 ...
 [-386.16601859    0.1605        0.65310001    0.37040001    0.61270005]
 [ 961.77094487    0.30650002    0.4377        0.27000001    0.17260002]
 [ 397.04969976    0.35330001    0.41770002    0.32300001    0.17739999]][0m
[37m[1m[2023-07-10 22:03:51,206][227910] Max Reward on eval: 2820.716568647325[0m
[37m[1m[2023-07-10 22:03:51,207][227910] Min Reward on eval: -781.2923583981581[0m
[37m[1m[2023-07-10 22:03:51,207][227910] Mean Reward across all agents: 814.6959368711221[0m
[37m[1m[2023-07-10 22:03:51,207][227910] Average Trajectory Length: 977.2813333333334[0m
[36m[2023-07-10 22:03:51,209][227910] mean_value=-2692.0872461178556, max_value=965.323997234317[0m
[37m[1m[2023-07-10 22:03:51,211][227910] New mean coefficients: [[ 2.73471     0.13708562  0.708526    0.5739111  -1.2596008 ]][0m
[37m[1m[2023-07-10 22:03:51,212][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:04:00,888][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 22:04:00,888][227910] FPS: 396937.86[0m
[36m[2023-07-10 22:04:00,891][227910] itr=1460, itrs=2000, Progress: 73.00%[0m
[37m[1m[2023-07-10 22:04:04,983][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001440[0m
[36m[2023-07-10 22:04:16,897][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 22:04:16,903][227910] FPS: 329841.47[0m
[36m[2023-07-10 22:04:21,796][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:04:21,797][227910] Reward + Measures: [[3376.07194573    0.21594363    0.4281083     0.33370468    0.14832933]][0m
[37m[1m[2023-07-10 22:04:21,797][227910] Max Reward on eval: 3376.0719457295536[0m
[37m[1m[2023-07-10 22:04:21,797][227910] Min Reward on eval: 3376.0719457295536[0m
[37m[1m[2023-07-10 22:04:21,797][227910] Mean Reward across all agents: 3376.0719457295536[0m
[37m[1m[2023-07-10 22:04:21,798][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:04:27,157][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:04:27,158][227910] Reward + Measures: [[ 912.94034993    0.66230005    0.32810003    0.708         0.64639997]
 [-869.71371888    0.89349997    0.1585        0.72010005    0.80839998]
 [ 753.24218419    0.35209998    0.41929999    0.36110002    0.1904    ]
 ...
 [ 334.24789361    0.41170001    0.35210001    0.46329999    0.366     ]
 [2846.97647986    0.22049999    0.40500003    0.36300001    0.16730002]
 [1370.73558362    0.33890003    0.40480003    0.37780005    0.15550001]][0m
[37m[1m[2023-07-10 22:04:27,158][227910] Max Reward on eval: 3310.5589615908452[0m
[37m[1m[2023-07-10 22:04:27,158][227910] Min Reward on eval: -1101.1972715534969[0m
[37m[1m[2023-07-10 22:04:27,159][227910] Mean Reward across all agents: 1206.662160989436[0m
[37m[1m[2023-07-10 22:04:27,159][227910] Average Trajectory Length: 993.1463333333332[0m
[36m[2023-07-10 22:04:27,161][227910] mean_value=-1449.0873968935211, max_value=882.5775663623454[0m
[37m[1m[2023-07-10 22:04:27,163][227910] New mean coefficients: [[ 2.9235227  -0.04956912  1.4016201   0.06228995 -1.28063   ]][0m
[37m[1m[2023-07-10 22:04:27,164][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:04:36,878][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 22:04:36,878][227910] FPS: 395392.10[0m
[36m[2023-07-10 22:04:36,880][227910] itr=1461, itrs=2000, Progress: 73.05%[0m
[36m[2023-07-10 22:04:48,388][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 22:04:48,388][227910] FPS: 334266.53[0m
[36m[2023-07-10 22:04:53,215][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:04:53,215][227910] Reward + Measures: [[2996.77609149    0.24555367    0.49565092    0.35268661    0.17243306]][0m
[37m[1m[2023-07-10 22:04:53,215][227910] Max Reward on eval: 2996.776091494693[0m
[37m[1m[2023-07-10 22:04:53,216][227910] Min Reward on eval: 2996.776091494693[0m
[37m[1m[2023-07-10 22:04:53,216][227910] Mean Reward across all agents: 2996.776091494693[0m
[37m[1m[2023-07-10 22:04:53,216][227910] Average Trajectory Length: 999.737[0m
[36m[2023-07-10 22:04:58,699][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:04:58,699][227910] Reward + Measures: [[2988.12751349    0.24559999    0.50209999    0.35889998    0.164     ]
 [ 792.82767746    0.35260001    0.5485        0.46149999    0.43470001]
 [ 233.46413495    0.3303        0.61450005    0.38270003    0.54750001]
 ...
 [ 464.71170717    0.35089999    0.5535        0.54879999    0.48260003]
 [1020.06686902    0.34549999    0.42999998    0.52509993    0.3039    ]
 [-189.70528606    0.138         0.76139998    0.29140002    0.72769994]][0m
[37m[1m[2023-07-10 22:04:58,700][227910] Max Reward on eval: 3113.1593680544756[0m
[37m[1m[2023-07-10 22:04:58,700][227910] Min Reward on eval: -274.2830505105318[0m
[37m[1m[2023-07-10 22:04:58,700][227910] Mean Reward across all agents: 1377.1073754365175[0m
[37m[1m[2023-07-10 22:04:58,700][227910] Average Trajectory Length: 998.7293333333333[0m
[36m[2023-07-10 22:04:58,703][227910] mean_value=-586.3619899154479, max_value=949.175338446162[0m
[37m[1m[2023-07-10 22:04:58,705][227910] New mean coefficients: [[ 2.7531977 -0.156304   1.3297421 -0.5965793 -1.2395767]][0m
[37m[1m[2023-07-10 22:04:58,706][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:05:08,420][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 22:05:08,420][227910] FPS: 395388.39[0m
[36m[2023-07-10 22:05:08,422][227910] itr=1462, itrs=2000, Progress: 73.10%[0m
[36m[2023-07-10 22:05:20,088][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 22:05:20,088][227910] FPS: 329823.78[0m
[36m[2023-07-10 22:05:24,778][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:05:24,779][227910] Reward + Measures: [[3312.36112485    0.24024862    0.49485633    0.34296948    0.17521305]][0m
[37m[1m[2023-07-10 22:05:24,779][227910] Max Reward on eval: 3312.361124845602[0m
[37m[1m[2023-07-10 22:05:24,779][227910] Min Reward on eval: 3312.361124845602[0m
[37m[1m[2023-07-10 22:05:24,779][227910] Mean Reward across all agents: 3312.361124845602[0m
[37m[1m[2023-07-10 22:05:24,780][227910] Average Trajectory Length: 999.6859999999999[0m
[36m[2023-07-10 22:05:30,233][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:05:30,233][227910] Reward + Measures: [[ 226.59844324    0.29969999    0.69699997    0.71850002    0.59020001]
 [ 258.26135505    0.0132        0.95310003    0.95909995    0.97749996]
 [1121.82522248    0.3556        0.59849995    0.34200001    0.35339999]
 ...
 [2823.34383771    0.25270003    0.4233        0.3373        0.18479998]
 [1609.31050713    0.28124157    0.32371494    0.36049876    0.14610676]
 [-146.23800868    0.78150004    0.73770005    0.89340001    0.67979997]][0m
[37m[1m[2023-07-10 22:05:30,233][227910] Max Reward on eval: 3145.51100345226[0m
[37m[1m[2023-07-10 22:05:30,234][227910] Min Reward on eval: -1148.5339358322672[0m
[37m[1m[2023-07-10 22:05:30,234][227910] Mean Reward across all agents: 802.0368150277404[0m
[37m[1m[2023-07-10 22:05:30,234][227910] Average Trajectory Length: 992.1836666666667[0m
[36m[2023-07-10 22:05:30,239][227910] mean_value=-553.7175928921995, max_value=1163.6493525675608[0m
[37m[1m[2023-07-10 22:05:30,242][227910] New mean coefficients: [[ 2.5861814   0.47424215  1.3047647  -0.28105035 -1.2024263 ]][0m
[37m[1m[2023-07-10 22:05:30,243][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:05:40,092][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 22:05:40,093][227910] FPS: 389934.95[0m
[36m[2023-07-10 22:05:40,095][227910] itr=1463, itrs=2000, Progress: 73.15%[0m
[36m[2023-07-10 22:05:51,730][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 22:05:51,730][227910] FPS: 330618.52[0m
[36m[2023-07-10 22:05:56,508][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:05:56,508][227910] Reward + Measures: [[3537.83874415    0.21214867    0.46317133    0.29149926    0.16943733]][0m
[37m[1m[2023-07-10 22:05:56,509][227910] Max Reward on eval: 3537.838744149834[0m
[37m[1m[2023-07-10 22:05:56,509][227910] Min Reward on eval: 3537.838744149834[0m
[37m[1m[2023-07-10 22:05:56,509][227910] Mean Reward across all agents: 3537.838744149834[0m
[37m[1m[2023-07-10 22:05:56,509][227910] Average Trajectory Length: 999.1383333333333[0m
[36m[2023-07-10 22:06:01,964][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:06:01,965][227910] Reward + Measures: [[1018.53709372    0.24240001    0.44369999    0.28590003    0.37310001]
 [ 453.26114128    0.87880003    0.78370005    0.88119996    0.53560001]
 [ -80.49508316    0.56820005    0.39009997    0.57020009    0.30400002]
 ...
 [ 666.34148875    0.59549999    0.40190002    0.62910002    0.3319    ]
 [ 538.60403351    0.68850005    0.29840001    0.7215001     0.31380001]
 [ 710.65540844    0.81209993    0.6681        0.86180001    0.35640001]][0m
[37m[1m[2023-07-10 22:06:01,965][227910] Max Reward on eval: 3306.965790291922[0m
[37m[1m[2023-07-10 22:06:01,965][227910] Min Reward on eval: -715.3747623691568[0m
[37m[1m[2023-07-10 22:06:01,966][227910] Mean Reward across all agents: 726.7312304418332[0m
[37m[1m[2023-07-10 22:06:01,966][227910] Average Trajectory Length: 993.1783333333333[0m
[36m[2023-07-10 22:06:01,971][227910] mean_value=-696.5504778962835, max_value=1141.6761623056625[0m
[37m[1m[2023-07-10 22:06:01,974][227910] New mean coefficients: [[ 2.972709   0.6025369  1.8567405  0.6307137 -1.1784594]][0m
[37m[1m[2023-07-10 22:06:01,975][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:06:11,653][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 22:06:11,653][227910] FPS: 396828.48[0m
[36m[2023-07-10 22:06:11,656][227910] itr=1464, itrs=2000, Progress: 73.20%[0m
[36m[2023-07-10 22:06:23,111][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 22:06:23,111][227910] FPS: 335870.54[0m
[36m[2023-07-10 22:06:27,978][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:06:27,979][227910] Reward + Measures: [[2195.03276056    0.26159799    0.48527598    0.34933564    0.25239   ]][0m
[37m[1m[2023-07-10 22:06:27,979][227910] Max Reward on eval: 2195.0327605564385[0m
[37m[1m[2023-07-10 22:06:27,979][227910] Min Reward on eval: 2195.0327605564385[0m
[37m[1m[2023-07-10 22:06:27,979][227910] Mean Reward across all agents: 2195.0327605564385[0m
[37m[1m[2023-07-10 22:06:27,979][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:06:33,575][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:06:33,575][227910] Reward + Measures: [[ 802.6131245     0.30750003    0.40349999    0.33319998    0.13270001]
 [1328.41336204    0.21700001    0.3477        0.22430001    0.1972    ]
 [1188.77861816    0.28550002    0.39110002    0.30930001    0.14600001]
 ...
 [ 540.61964888    0.21599999    0.38649997    0.25620002    0.2325    ]
 [1093.12848417    0.31440002    0.40990001    0.31650001    0.19680001]
 [ 406.55561086    0.3856        0.41409999    0.35170001    0.21280001]][0m
[37m[1m[2023-07-10 22:06:33,576][227910] Max Reward on eval: 2485.6876936890185[0m
[37m[1m[2023-07-10 22:06:33,576][227910] Min Reward on eval: -240.6513250624179[0m
[37m[1m[2023-07-10 22:06:33,576][227910] Mean Reward across all agents: 1120.7604199388038[0m
[37m[1m[2023-07-10 22:06:33,576][227910] Average Trajectory Length: 989.2986666666666[0m
[36m[2023-07-10 22:06:33,579][227910] mean_value=-1964.7175696955842, max_value=723.2188993802431[0m
[37m[1m[2023-07-10 22:06:33,581][227910] New mean coefficients: [[ 3.1924586   0.23780438  0.7633593   1.0691705  -0.8919723 ]][0m
[37m[1m[2023-07-10 22:06:33,582][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:06:43,399][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 22:06:43,400][227910] FPS: 391207.19[0m
[36m[2023-07-10 22:06:43,402][227910] itr=1465, itrs=2000, Progress: 73.25%[0m
[36m[2023-07-10 22:06:54,854][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 22:06:54,854][227910] FPS: 335964.21[0m
[36m[2023-07-10 22:06:59,746][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:06:59,746][227910] Reward + Measures: [[2416.14754833    0.25485131    0.48173544    0.33826238    0.23191115]][0m
[37m[1m[2023-07-10 22:06:59,746][227910] Max Reward on eval: 2416.1475483332692[0m
[37m[1m[2023-07-10 22:06:59,747][227910] Min Reward on eval: 2416.1475483332692[0m
[37m[1m[2023-07-10 22:06:59,747][227910] Mean Reward across all agents: 2416.1475483332692[0m
[37m[1m[2023-07-10 22:06:59,747][227910] Average Trajectory Length: 999.5706666666666[0m
[36m[2023-07-10 22:07:05,244][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:07:05,245][227910] Reward + Measures: [[1247.05402706    0.4434        0.52450001    0.50060004    0.45090005]
 [  33.78902537    0.55830002    0.7446        0.62109995    0.7051    ]
 [1295.86581413    0.40260002    0.51030004    0.48390004    0.47490001]
 ...
 [ 784.56485087    0.42030001    0.5388        0.35349998    0.48499998]
 [ 955.46573611    0.53299999    0.50260001    0.38289997    0.27250001]
 [1343.20706469    0.24010001    0.48520002    0.35410002    0.33450001]][0m
[37m[1m[2023-07-10 22:07:05,245][227910] Max Reward on eval: 2423.10503359586[0m
[37m[1m[2023-07-10 22:07:05,245][227910] Min Reward on eval: -148.29720197979478[0m
[37m[1m[2023-07-10 22:07:05,245][227910] Mean Reward across all agents: 1232.282009751773[0m
[37m[1m[2023-07-10 22:07:05,246][227910] Average Trajectory Length: 998.7756666666667[0m
[36m[2023-07-10 22:07:05,250][227910] mean_value=-559.1206097704471, max_value=1204.4030163357602[0m
[37m[1m[2023-07-10 22:07:05,253][227910] New mean coefficients: [[ 3.6594028  -0.16903168  1.2630076   1.4473128  -1.1801755 ]][0m
[37m[1m[2023-07-10 22:07:05,254][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:07:14,910][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 22:07:14,910][227910] FPS: 397741.32[0m
[36m[2023-07-10 22:07:14,912][227910] itr=1466, itrs=2000, Progress: 73.30%[0m
[36m[2023-07-10 22:07:26,424][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 22:07:26,425][227910] FPS: 334102.38[0m
[36m[2023-07-10 22:07:31,273][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:07:31,274][227910] Reward + Measures: [[2686.64691781    0.24737735    0.48468676    0.32248852    0.20701382]][0m
[37m[1m[2023-07-10 22:07:31,274][227910] Max Reward on eval: 2686.64691780604[0m
[37m[1m[2023-07-10 22:07:31,274][227910] Min Reward on eval: 2686.64691780604[0m
[37m[1m[2023-07-10 22:07:31,274][227910] Mean Reward across all agents: 2686.64691780604[0m
[37m[1m[2023-07-10 22:07:31,275][227910] Average Trajectory Length: 999.8639999999999[0m
[36m[2023-07-10 22:07:36,800][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:07:36,801][227910] Reward + Measures: [[-1102.97081964     0.0728         0.72410005     0.56360006
      0.68400002]
 [ 1838.3156443      0.33930001     0.49429998     0.37880003
      0.30420002]
 [ -194.4580792      0.0382         0.9149         0.80010003
      0.87590009]
 ...
 [  721.93207758     0.24150001     0.59040004     0.40839997
      0.5194    ]
 [  168.39372724     0.1523         0.92320007     0.8235001
      0.87470001]
 [  557.52423571     0.3288528      0.38830066     0.35491785
      0.32630783]][0m
[37m[1m[2023-07-10 22:07:36,801][227910] Max Reward on eval: 2577.5225319911724[0m
[37m[1m[2023-07-10 22:07:36,801][227910] Min Reward on eval: -1145.7263477052097[0m
[37m[1m[2023-07-10 22:07:36,801][227910] Mean Reward across all agents: 666.1534615514626[0m
[37m[1m[2023-07-10 22:07:36,802][227910] Average Trajectory Length: 985.478[0m
[36m[2023-07-10 22:07:36,805][227910] mean_value=-789.0213684901679, max_value=1033.8810698701245[0m
[37m[1m[2023-07-10 22:07:36,807][227910] New mean coefficients: [[ 3.742902   -0.17688207  0.7743064   1.6068286  -0.38105804]][0m
[37m[1m[2023-07-10 22:07:36,808][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:07:46,531][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 22:07:46,531][227910] FPS: 395021.22[0m
[36m[2023-07-10 22:07:46,533][227910] itr=1467, itrs=2000, Progress: 73.35%[0m
[36m[2023-07-10 22:07:58,013][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 22:07:58,014][227910] FPS: 335050.76[0m
[36m[2023-07-10 22:08:02,774][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:08:02,774][227910] Reward + Measures: [[3009.82643792    0.24440606    0.49322957    0.31273159    0.18239848]][0m
[37m[1m[2023-07-10 22:08:02,775][227910] Max Reward on eval: 3009.826437920579[0m
[37m[1m[2023-07-10 22:08:02,775][227910] Min Reward on eval: 3009.826437920579[0m
[37m[1m[2023-07-10 22:08:02,775][227910] Mean Reward across all agents: 3009.826437920579[0m
[37m[1m[2023-07-10 22:08:02,775][227910] Average Trajectory Length: 998.8923333333333[0m
[36m[2023-07-10 22:08:08,262][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:08:08,263][227910] Reward + Measures: [[1929.9852004     0.24000001    0.42300001    0.36219999    0.255     ]
 [1502.92388333    0.2052        0.41330004    0.33849999    0.28439999]
 [-358.87308005    0.22570001    0.42030001    0.29320002    0.39850003]
 ...
 [-308.13599272    0.20170002    0.70959997    0.24510002    0.67080003]
 [2265.87186441    0.24730001    0.34689999    0.32220003    0.2057    ]
 [1459.90237331    0.2638        0.50640005    0.35640001    0.345     ]][0m
[37m[1m[2023-07-10 22:08:08,263][227910] Max Reward on eval: 3281.802555137593[0m
[37m[1m[2023-07-10 22:08:08,263][227910] Min Reward on eval: -975.1846426417236[0m
[37m[1m[2023-07-10 22:08:08,263][227910] Mean Reward across all agents: 1045.0863310196617[0m
[37m[1m[2023-07-10 22:08:08,264][227910] Average Trajectory Length: 992.8056666666666[0m
[36m[2023-07-10 22:08:08,266][227910] mean_value=-1442.4366159063643, max_value=756.3572183966016[0m
[37m[1m[2023-07-10 22:08:08,268][227910] New mean coefficients: [[ 3.534637   -0.17725381 -0.40931916  1.1655344   0.18729103]][0m
[37m[1m[2023-07-10 22:08:08,269][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:08:17,997][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 22:08:17,997][227910] FPS: 394838.09[0m
[36m[2023-07-10 22:08:17,999][227910] itr=1468, itrs=2000, Progress: 73.40%[0m
[36m[2023-07-10 22:08:29,612][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 22:08:29,612][227910] FPS: 331211.46[0m
[36m[2023-07-10 22:08:34,439][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:08:34,440][227910] Reward + Measures: [[3255.42708697    0.24105634    0.48423159    0.3053098     0.16993493]][0m
[37m[1m[2023-07-10 22:08:34,440][227910] Max Reward on eval: 3255.4270869662837[0m
[37m[1m[2023-07-10 22:08:34,440][227910] Min Reward on eval: 3255.4270869662837[0m
[37m[1m[2023-07-10 22:08:34,440][227910] Mean Reward across all agents: 3255.4270869662837[0m
[37m[1m[2023-07-10 22:08:34,441][227910] Average Trajectory Length: 999.3266666666666[0m
[36m[2023-07-10 22:08:39,914][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:08:39,915][227910] Reward + Measures: [[1615.5654735     0.41060001    0.46259999    0.41870004    0.26799998]
 [1149.3112558     0.27269998    0.34220001    0.37059999    0.30150002]
 [1296.10864377    0.19069999    0.4948        0.24889998    0.31290001]
 ...
 [1035.47475947    0.36020002    0.53139997    0.41279998    0.33449998]
 [ 911.57745445    0.58570004    0.46240002    0.54719996    0.61230004]
 [1068.43379616    0.32500002    0.4903        0.35460001    0.28929999]][0m
[37m[1m[2023-07-10 22:08:39,915][227910] Max Reward on eval: 2976.1274640589954[0m
[37m[1m[2023-07-10 22:08:39,916][227910] Min Reward on eval: -269.0457251366694[0m
[37m[1m[2023-07-10 22:08:39,916][227910] Mean Reward across all agents: 1179.6469417252335[0m
[37m[1m[2023-07-10 22:08:39,916][227910] Average Trajectory Length: 981.7916666666666[0m
[36m[2023-07-10 22:08:39,919][227910] mean_value=-1055.96196839042, max_value=808.5589688571573[0m
[37m[1m[2023-07-10 22:08:39,922][227910] New mean coefficients: [[ 3.5144715  -0.56148493 -0.83778274  0.6807381   0.18570761]][0m
[37m[1m[2023-07-10 22:08:39,923][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:08:49,753][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 22:08:49,753][227910] FPS: 390701.39[0m
[36m[2023-07-10 22:08:49,756][227910] itr=1469, itrs=2000, Progress: 73.45%[0m
[36m[2023-07-10 22:09:01,444][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 22:09:01,445][227910] FPS: 329048.42[0m
[36m[2023-07-10 22:09:06,152][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:09:06,153][227910] Reward + Measures: [[3515.64144543    0.23568352    0.47275847    0.30237225    0.15334016]][0m
[37m[1m[2023-07-10 22:09:06,153][227910] Max Reward on eval: 3515.641445431706[0m
[37m[1m[2023-07-10 22:09:06,153][227910] Min Reward on eval: 3515.641445431706[0m
[37m[1m[2023-07-10 22:09:06,153][227910] Mean Reward across all agents: 3515.641445431706[0m
[37m[1m[2023-07-10 22:09:06,153][227910] Average Trajectory Length: 996.2623333333333[0m
[36m[2023-07-10 22:09:11,837][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:09:11,842][227910] Reward + Measures: [[2000.87179764    0.20939998    0.55890006    0.38510001    0.1762    ]
 [1722.91505007    0.27630001    0.53470004    0.37470001    0.28210002]
 [-153.21606876    0.45360002    0.78529996    0.44800001    0.53619999]
 ...
 [ 879.27255696    0.4499        0.57800001    0.2994        0.398     ]
 [2276.67923207    0.2773        0.51739997    0.32589999    0.22620001]
 [ 561.03837428    0.25890002    0.61440003    0.57279998    0.49059996]][0m
[37m[1m[2023-07-10 22:09:11,843][227910] Max Reward on eval: 3264.055429716408[0m
[37m[1m[2023-07-10 22:09:11,844][227910] Min Reward on eval: -1098.6083242059917[0m
[37m[1m[2023-07-10 22:09:11,844][227910] Mean Reward across all agents: 902.0605620140389[0m
[37m[1m[2023-07-10 22:09:11,845][227910] Average Trajectory Length: 976.4649999999999[0m
[36m[2023-07-10 22:09:11,852][227910] mean_value=-1011.3642886721481, max_value=847.004685444443[0m
[37m[1m[2023-07-10 22:09:11,857][227910] New mean coefficients: [[ 3.5771096  -0.69179964 -0.45554957  0.988917    0.24647366]][0m
[37m[1m[2023-07-10 22:09:11,859][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:09:21,619][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 22:09:21,619][227910] FPS: 393533.50[0m
[36m[2023-07-10 22:09:21,621][227910] itr=1470, itrs=2000, Progress: 73.50%[0m
[37m[1m[2023-07-10 22:09:25,610][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001450[0m
[36m[2023-07-10 22:09:37,448][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 22:09:37,449][227910] FPS: 332157.62[0m
[36m[2023-07-10 22:09:42,171][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:09:42,171][227910] Reward + Measures: [[3756.0636817     0.2331591     0.47598535    0.29924074    0.14532995]][0m
[37m[1m[2023-07-10 22:09:42,172][227910] Max Reward on eval: 3756.0636817035534[0m
[37m[1m[2023-07-10 22:09:42,172][227910] Min Reward on eval: 3756.0636817035534[0m
[37m[1m[2023-07-10 22:09:42,172][227910] Mean Reward across all agents: 3756.0636817035534[0m
[37m[1m[2023-07-10 22:09:42,172][227910] Average Trajectory Length: 995.6726666666666[0m
[36m[2023-07-10 22:09:47,847][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:09:47,847][227910] Reward + Measures: [[1707.51822754    0.2818        0.46469998    0.45460001    0.24609999]
 [1264.87246011    0.38130003    0.41770002    0.4183        0.37630001]
 [1238.89145713    0.28080001    0.55100006    0.252         0.3626    ]
 ...
 [2128.53279654    0.30379999    0.51440001    0.36470002    0.30490002]
 [2393.63143122    0.2757        0.45720002    0.32350001    0.25619999]
 [1277.949248      0.28239998    0.3712        0.30089998    0.19660001]][0m
[37m[1m[2023-07-10 22:09:47,847][227910] Max Reward on eval: 3510.0215417365544[0m
[37m[1m[2023-07-10 22:09:47,848][227910] Min Reward on eval: -1049.6187138809096[0m
[37m[1m[2023-07-10 22:09:47,848][227910] Mean Reward across all agents: 1243.0968912127075[0m
[37m[1m[2023-07-10 22:09:47,848][227910] Average Trajectory Length: 994.0949999999999[0m
[36m[2023-07-10 22:09:47,852][227910] mean_value=-827.4124817819794, max_value=1152.9829844593883[0m
[37m[1m[2023-07-10 22:09:47,855][227910] New mean coefficients: [[ 3.693036   -0.9018896  -0.03026757  1.1809685   0.50729793]][0m
[37m[1m[2023-07-10 22:09:47,856][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:09:57,467][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 22:09:57,468][227910] FPS: 399575.44[0m
[36m[2023-07-10 22:09:57,470][227910] itr=1471, itrs=2000, Progress: 73.55%[0m
[36m[2023-07-10 22:10:09,028][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 22:10:09,029][227910] FPS: 332798.56[0m
[36m[2023-07-10 22:10:13,793][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:10:13,793][227910] Reward + Measures: [[2936.74179749    0.25933534    0.47879365    0.32046664    0.21890666]][0m
[37m[1m[2023-07-10 22:10:13,793][227910] Max Reward on eval: 2936.7417974942773[0m
[37m[1m[2023-07-10 22:10:13,794][227910] Min Reward on eval: 2936.7417974942773[0m
[37m[1m[2023-07-10 22:10:13,794][227910] Mean Reward across all agents: 2936.7417974942773[0m
[37m[1m[2023-07-10 22:10:13,794][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:10:19,260][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:10:19,261][227910] Reward + Measures: [[  36.69637547    0.77450001    0.57929999    0.78399998    0.75929999]
 [-675.25909339    0.815         0.81599998    0.88450003    0.63600004]
 [ 365.9995955     0.22557889    0.28843427    0.31120878    0.21950439]
 ...
 [2071.36938424    0.2335        0.45510003    0.35279998    0.19450001]
 [ 633.47308845    0.58626729    0.27210304    0.68826699    0.2820271 ]
 [ 419.12879696    0.5183        0.53720003    0.5381        0.56889999]][0m
[37m[1m[2023-07-10 22:10:19,261][227910] Max Reward on eval: 2874.522775416076[0m
[37m[1m[2023-07-10 22:10:19,262][227910] Min Reward on eval: -1608.2910545089048[0m
[37m[1m[2023-07-10 22:10:19,262][227910] Mean Reward across all agents: 935.5523104370277[0m
[37m[1m[2023-07-10 22:10:19,262][227910] Average Trajectory Length: 994.9796666666666[0m
[36m[2023-07-10 22:10:19,267][227910] mean_value=-566.5435428166325, max_value=1157.9653185305197[0m
[37m[1m[2023-07-10 22:10:19,269][227910] New mean coefficients: [[ 3.6025524  -0.58320534  0.6856892   1.8061671   0.62665975]][0m
[37m[1m[2023-07-10 22:10:19,270][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:10:29,153][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 22:10:29,154][227910] FPS: 388613.67[0m
[36m[2023-07-10 22:10:29,156][227910] itr=1472, itrs=2000, Progress: 73.60%[0m
[36m[2023-07-10 22:10:40,808][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 22:10:40,808][227910] FPS: 330136.99[0m
[36m[2023-07-10 22:10:45,668][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:10:45,668][227910] Reward + Measures: [[3269.26866253    0.26256967    0.47813135    0.33459964    0.21810535]][0m
[37m[1m[2023-07-10 22:10:45,668][227910] Max Reward on eval: 3269.2686625332367[0m
[37m[1m[2023-07-10 22:10:45,668][227910] Min Reward on eval: 3269.2686625332367[0m
[37m[1m[2023-07-10 22:10:45,669][227910] Mean Reward across all agents: 3269.2686625332367[0m
[37m[1m[2023-07-10 22:10:45,669][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:10:51,126][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:10:51,127][227910] Reward + Measures: [[ 461.52174254    0.36820003    0.45860001    0.4127        0.31790003]
 [ 474.97947157    0.27059999    0.43190002    0.36560002    0.3231    ]
 [ 695.08151739    0.31070003    0.43509999    0.4192        0.2658    ]
 ...
 [ 512.56094215    0.28979999    0.5704        0.38890001    0.41099998]
 [-142.75639229    0.0855        0.77280003    0.66110003    0.7323001 ]
 [ 174.67496994    0.53730512    0.51130515    0.43600512    0.52448463]][0m
[37m[1m[2023-07-10 22:10:51,127][227910] Max Reward on eval: 3267.1625694826243[0m
[37m[1m[2023-07-10 22:10:51,127][227910] Min Reward on eval: -583.3686971988994[0m
[37m[1m[2023-07-10 22:10:51,127][227910] Mean Reward across all agents: 713.5221804747466[0m
[37m[1m[2023-07-10 22:10:51,128][227910] Average Trajectory Length: 984.8236666666667[0m
[36m[2023-07-10 22:10:51,130][227910] mean_value=-1482.4724854939784, max_value=855.7256495606305[0m
[37m[1m[2023-07-10 22:10:51,132][227910] New mean coefficients: [[ 3.2404165  -0.5486133  -0.30792284  0.95960575  0.5717484 ]][0m
[37m[1m[2023-07-10 22:10:51,133][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:11:00,916][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 22:11:00,916][227910] FPS: 392584.84[0m
[36m[2023-07-10 22:11:00,918][227910] itr=1473, itrs=2000, Progress: 73.65%[0m
[36m[2023-07-10 22:11:12,491][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 22:11:12,491][227910] FPS: 332365.34[0m
[36m[2023-07-10 22:11:17,369][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:11:17,369][227910] Reward + Measures: [[3521.53889795    0.26072866    0.47239533    0.34550667    0.21426432]][0m
[37m[1m[2023-07-10 22:11:17,370][227910] Max Reward on eval: 3521.538897954983[0m
[37m[1m[2023-07-10 22:11:17,370][227910] Min Reward on eval: 3521.538897954983[0m
[37m[1m[2023-07-10 22:11:17,370][227910] Mean Reward across all agents: 3521.538897954983[0m
[37m[1m[2023-07-10 22:11:17,370][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:11:22,841][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:11:22,847][227910] Reward + Measures: [[-282.67238259    0.22556369    0.22240102    0.22169474    0.20537849]
 [1780.32141245    0.21659999    0.66079998    0.2818        0.28000003]
 [2252.61731256    0.22930001    0.45320001    0.28840002    0.24609999]
 ...
 [2513.24667132    0.2419        0.38840005    0.2649        0.2167    ]
 [1028.59038739    0.64890003    0.46739998    0.65070003    0.31149998]
 [1161.64258504    0.22230001    0.63239998    0.24780002    0.45299998]][0m
[37m[1m[2023-07-10 22:11:22,847][227910] Max Reward on eval: 3682.4574889373034[0m
[37m[1m[2023-07-10 22:11:22,847][227910] Min Reward on eval: -867.246251748252[0m
[37m[1m[2023-07-10 22:11:22,848][227910] Mean Reward across all agents: 893.3632452513522[0m
[37m[1m[2023-07-10 22:11:22,848][227910] Average Trajectory Length: 980.058[0m
[36m[2023-07-10 22:11:22,851][227910] mean_value=-1145.6757167891922, max_value=1125.460928142852[0m
[37m[1m[2023-07-10 22:11:22,854][227910] New mean coefficients: [[ 2.6302116  -0.3128947  -0.33684844  0.28813112  0.69667506]][0m
[37m[1m[2023-07-10 22:11:22,855][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:11:32,794][227910] train() took 9.94 seconds to complete[0m
[36m[2023-07-10 22:11:32,794][227910] FPS: 386415.61[0m
[36m[2023-07-10 22:11:32,797][227910] itr=1474, itrs=2000, Progress: 73.70%[0m
[36m[2023-07-10 22:11:44,351][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 22:11:44,351][227910] FPS: 332996.56[0m
[36m[2023-07-10 22:11:49,121][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:11:49,122][227910] Reward + Measures: [[3754.67818149    0.26552486    0.4560467     0.34809178    0.21299869]][0m
[37m[1m[2023-07-10 22:11:49,122][227910] Max Reward on eval: 3754.6781814895594[0m
[37m[1m[2023-07-10 22:11:49,122][227910] Min Reward on eval: 3754.6781814895594[0m
[37m[1m[2023-07-10 22:11:49,122][227910] Mean Reward across all agents: 3754.6781814895594[0m
[37m[1m[2023-07-10 22:11:49,123][227910] Average Trajectory Length: 999.8643333333333[0m
[36m[2023-07-10 22:11:54,798][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:11:54,799][227910] Reward + Measures: [[1053.62529739    0.4086        0.58050007    0.51569998    0.3924    ]
 [ 498.55287821    0.33029997    0.46149999    0.25869998    0.48719999]
 [ 433.59815451    0.56870002    0.4104        0.41910002    0.57590002]
 ...
 [1271.99731467    0.21739998    0.45120001    0.40920001    0.17920001]
 [-104.8283857     0.60050005    0.41560006    0.43129998    0.7554    ]
 [1437.88958298    0.33580002    0.61869997    0.3994        0.389     ]][0m
[37m[1m[2023-07-10 22:11:54,799][227910] Max Reward on eval: 3427.6762083292006[0m
[37m[1m[2023-07-10 22:11:54,799][227910] Min Reward on eval: -1000.413205230725[0m
[37m[1m[2023-07-10 22:11:54,799][227910] Mean Reward across all agents: 853.6399872558194[0m
[37m[1m[2023-07-10 22:11:54,800][227910] Average Trajectory Length: 991.322[0m
[36m[2023-07-10 22:11:54,804][227910] mean_value=-850.4238868282797, max_value=1137.6268540264084[0m
[37m[1m[2023-07-10 22:11:54,806][227910] New mean coefficients: [[ 2.4507885  -0.4219808  -0.93838865  0.08208773  0.59820324]][0m
[37m[1m[2023-07-10 22:11:54,807][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:12:04,523][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 22:12:04,523][227910] FPS: 395304.99[0m
[36m[2023-07-10 22:12:04,526][227910] itr=1475, itrs=2000, Progress: 73.75%[0m
[36m[2023-07-10 22:12:16,023][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 22:12:16,023][227910] FPS: 334643.89[0m
[36m[2023-07-10 22:12:20,769][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:12:20,769][227910] Reward + Measures: [[3926.97435954    0.2629568     0.44906241    0.35195893    0.21177514]][0m
[37m[1m[2023-07-10 22:12:20,769][227910] Max Reward on eval: 3926.9743595387167[0m
[37m[1m[2023-07-10 22:12:20,770][227910] Min Reward on eval: 3926.9743595387167[0m
[37m[1m[2023-07-10 22:12:20,770][227910] Mean Reward across all agents: 3926.9743595387167[0m
[37m[1m[2023-07-10 22:12:20,770][227910] Average Trajectory Length: 999.5413333333333[0m
[36m[2023-07-10 22:12:26,290][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:12:26,290][227910] Reward + Measures: [[ 341.14905262    0.2719        0.76450002    0.3263        0.61510003]
 [ -24.66618308    0.94150001    0.92360002    0.8847        0.93540001]
 [-142.87550296    0.30720001    0.66429996    0.38679999    0.59440005]
 ...
 [ 727.7174112     0.2955111     0.34025559    0.30796668    0.26046667]
 [ 167.79300413    0.5363        0.42720005    0.53839999    0.34059998]
 [ 328.41132948    0.87729996    0.89939994    0.87519997    0.87740004]][0m
[37m[1m[2023-07-10 22:12:26,291][227910] Max Reward on eval: 3510.1983019417153[0m
[37m[1m[2023-07-10 22:12:26,291][227910] Min Reward on eval: -1091.1011356732809[0m
[37m[1m[2023-07-10 22:12:26,291][227910] Mean Reward across all agents: 728.858301726591[0m
[37m[1m[2023-07-10 22:12:26,291][227910] Average Trajectory Length: 993.184[0m
[36m[2023-07-10 22:12:26,295][227910] mean_value=-861.6462898298388, max_value=1654.786245352686[0m
[37m[1m[2023-07-10 22:12:26,298][227910] New mean coefficients: [[ 2.5772078  -0.4126056   0.15609878  0.36833516  0.63021624]][0m
[37m[1m[2023-07-10 22:12:26,299][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:12:36,099][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 22:12:36,099][227910] FPS: 391890.52[0m
[36m[2023-07-10 22:12:36,102][227910] itr=1476, itrs=2000, Progress: 73.80%[0m
[36m[2023-07-10 22:12:47,760][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 22:12:47,760][227910] FPS: 329924.14[0m
[36m[2023-07-10 22:12:52,445][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:12:52,445][227910] Reward + Measures: [[4041.78503316    0.26360163    0.447777      0.35830668    0.20444272]][0m
[37m[1m[2023-07-10 22:12:52,446][227910] Max Reward on eval: 4041.7850331581035[0m
[37m[1m[2023-07-10 22:12:52,446][227910] Min Reward on eval: 4041.7850331581035[0m
[37m[1m[2023-07-10 22:12:52,446][227910] Mean Reward across all agents: 4041.7850331581035[0m
[37m[1m[2023-07-10 22:12:52,446][227910] Average Trajectory Length: 999.2503333333333[0m
[36m[2023-07-10 22:12:57,801][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:12:57,802][227910] Reward + Measures: [[ 475.72512286    0.60110003    0.6225        0.43800002    0.2841    ]
 [ 458.11559313    0.4337        0.61560005    0.43099999    0.36229998]
 [ 781.31045747    0.48909998    0.69329995    0.39939997    0.31279999]
 ...
 [ 302.82077961    0.57930005    0.75139999    0.31090003    0.47929999]
 [1802.42457027    0.3152        0.50610006    0.3125        0.24159999]
 [2226.58960264    0.29269999    0.4797        0.36750004    0.2139    ]][0m
[37m[1m[2023-07-10 22:12:57,802][227910] Max Reward on eval: 3707.7547487312927[0m
[37m[1m[2023-07-10 22:12:57,802][227910] Min Reward on eval: -275.5089088640176[0m
[37m[1m[2023-07-10 22:12:57,802][227910] Mean Reward across all agents: 1556.2655853075119[0m
[37m[1m[2023-07-10 22:12:57,803][227910] Average Trajectory Length: 994.804[0m
[36m[2023-07-10 22:12:57,808][227910] mean_value=-512.3312447147244, max_value=1451.4816993802915[0m
[37m[1m[2023-07-10 22:12:57,810][227910] New mean coefficients: [[ 3.0845952  -0.56838375  0.3865833   1.322294    0.8859055 ]][0m
[37m[1m[2023-07-10 22:12:57,811][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:13:07,550][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 22:13:07,550][227910] FPS: 394381.75[0m
[36m[2023-07-10 22:13:07,552][227910] itr=1477, itrs=2000, Progress: 73.85%[0m
[36m[2023-07-10 22:13:19,006][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 22:13:19,006][227910] FPS: 335811.86[0m
[36m[2023-07-10 22:13:23,741][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:13:23,741][227910] Reward + Measures: [[1015.95096153    0.27485734    0.63194966    0.24483167    0.37083733]][0m
[37m[1m[2023-07-10 22:13:23,742][227910] Max Reward on eval: 1015.9509615289207[0m
[37m[1m[2023-07-10 22:13:23,742][227910] Min Reward on eval: 1015.9509615289207[0m
[37m[1m[2023-07-10 22:13:23,742][227910] Mean Reward across all agents: 1015.9509615289207[0m
[37m[1m[2023-07-10 22:13:23,742][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:13:29,114][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:13:29,114][227910] Reward + Measures: [[ 591.99291585    0.354         0.40060002    0.19930001    0.26630002]
 [ 664.83947349    0.28350002    0.57179999    0.32750002    0.31280002]
 [ 409.3350656     0.41328344    0.26673761    0.3787415     0.3615981 ]
 ...
 [1047.50630758    0.2669        0.52590001    0.27449998    0.32949996]
 [ 506.22141322    0.48660001    0.2402        0.53760004    0.44570002]
 [1178.17286922    0.228         0.42490003    0.18420002    0.37849998]][0m
[37m[1m[2023-07-10 22:13:29,114][227910] Max Reward on eval: 1588.8656296896283[0m
[37m[1m[2023-07-10 22:13:29,115][227910] Min Reward on eval: -706.828561979899[0m
[37m[1m[2023-07-10 22:13:29,115][227910] Mean Reward across all agents: 545.0845790205132[0m
[37m[1m[2023-07-10 22:13:29,115][227910] Average Trajectory Length: 978.3103333333333[0m
[36m[2023-07-10 22:13:29,118][227910] mean_value=-1464.441846676883, max_value=804.4222317496344[0m
[37m[1m[2023-07-10 22:13:29,120][227910] New mean coefficients: [[ 2.8393216  -0.7899335  -0.09396726  0.71806514  0.7049102 ]][0m
[37m[1m[2023-07-10 22:13:29,121][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:13:38,929][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 22:13:38,929][227910] FPS: 391578.80[0m
[36m[2023-07-10 22:13:38,932][227910] itr=1478, itrs=2000, Progress: 73.90%[0m
[36m[2023-07-10 22:13:50,607][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 22:13:50,608][227910] FPS: 329415.82[0m
[36m[2023-07-10 22:13:55,447][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:13:55,447][227910] Reward + Measures: [[1236.4449546     0.24921867    0.60614496    0.20704634    0.36281136]][0m
[37m[1m[2023-07-10 22:13:55,447][227910] Max Reward on eval: 1236.4449545979255[0m
[37m[1m[2023-07-10 22:13:55,448][227910] Min Reward on eval: 1236.4449545979255[0m
[37m[1m[2023-07-10 22:13:55,448][227910] Mean Reward across all agents: 1236.4449545979255[0m
[37m[1m[2023-07-10 22:13:55,448][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:14:00,957][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:14:00,958][227910] Reward + Measures: [[ 574.76570494    0.1734        0.37219998    0.19059999    0.25420001]
 [1036.30760866    0.36999997    0.37739998    0.27160001    0.3089    ]
 [ 157.01732772    0.26320001    0.40500003    0.32030001    0.29960001]
 ...
 [1172.89456496    0.31580001    0.49680001    0.24020003    0.3152    ]
 [1038.40321341    0.22360002    0.41693917    0.17422174    0.20905216]
 [1601.5018375     0.30520001    0.53900003    0.23559999    0.28670001]][0m
[37m[1m[2023-07-10 22:14:00,958][227910] Max Reward on eval: 1761.7600581556442[0m
[37m[1m[2023-07-10 22:14:00,958][227910] Min Reward on eval: -610.0159467588295[0m
[37m[1m[2023-07-10 22:14:00,958][227910] Mean Reward across all agents: 727.6052157055408[0m
[37m[1m[2023-07-10 22:14:00,958][227910] Average Trajectory Length: 989.6443333333333[0m
[36m[2023-07-10 22:14:00,960][227910] mean_value=-1686.8026868723002, max_value=682.8138058739876[0m
[37m[1m[2023-07-10 22:14:00,962][227910] New mean coefficients: [[ 2.6014712  -0.45701826 -0.38356233  0.05539691  0.5234984 ]][0m
[37m[1m[2023-07-10 22:14:00,963][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:14:10,791][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 22:14:10,792][227910] FPS: 390779.57[0m
[36m[2023-07-10 22:14:10,794][227910] itr=1479, itrs=2000, Progress: 73.95%[0m
[36m[2023-07-10 22:14:22,424][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 22:14:22,424][227910] FPS: 330709.97[0m
[36m[2023-07-10 22:14:27,247][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:14:27,248][227910] Reward + Measures: [[1566.47961865    0.25297961    0.58410138    0.16702096    0.34847474]][0m
[37m[1m[2023-07-10 22:14:27,248][227910] Max Reward on eval: 1566.4796186502906[0m
[37m[1m[2023-07-10 22:14:27,248][227910] Min Reward on eval: 1566.4796186502906[0m
[37m[1m[2023-07-10 22:14:27,249][227910] Mean Reward across all agents: 1566.4796186502906[0m
[37m[1m[2023-07-10 22:14:27,249][227910] Average Trajectory Length: 999.5503333333334[0m
[36m[2023-07-10 22:14:32,944][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:14:32,945][227910] Reward + Measures: [[ 606.42606252    0.1859        0.67679995    0.21689999    0.40890002]
 [ 306.65232779    0.27689552    0.52777946    0.32336578    0.35509902]
 [ 302.37581103    0.19089697    0.4750061     0.22016363    0.25847575]
 ...
 [ 633.760949      0.10750001    0.79710001    0.44390002    0.77530003]
 [  11.49123281    0.27424464    0.43316007    0.31385493    0.17220767]
 [1413.05446312    0.2376        0.52150005    0.182         0.38789999]][0m
[37m[1m[2023-07-10 22:14:32,945][227910] Max Reward on eval: 1750.7691398476716[0m
[37m[1m[2023-07-10 22:14:32,945][227910] Min Reward on eval: -640.2711555817863[0m
[37m[1m[2023-07-10 22:14:32,945][227910] Mean Reward across all agents: 688.7139270579161[0m
[37m[1m[2023-07-10 22:14:32,946][227910] Average Trajectory Length: 961.6229999999999[0m
[36m[2023-07-10 22:14:32,948][227910] mean_value=-1288.130457249976, max_value=1707.379609283805[0m
[37m[1m[2023-07-10 22:14:32,951][227910] New mean coefficients: [[ 2.8198376   0.28478038 -0.28222752  0.39307988  0.55551034]][0m
[37m[1m[2023-07-10 22:14:32,952][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:14:42,833][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 22:14:42,833][227910] FPS: 388680.79[0m
[36m[2023-07-10 22:14:42,835][227910] itr=1480, itrs=2000, Progress: 74.00%[0m
[37m[1m[2023-07-10 22:14:46,851][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001460[0m
[36m[2023-07-10 22:14:58,808][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 22:14:58,809][227910] FPS: 328725.20[0m
[36m[2023-07-10 22:15:03,554][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:15:03,555][227910] Reward + Measures: [[1720.90655881    0.2558699     0.58947539    0.16993333    0.33461043]][0m
[37m[1m[2023-07-10 22:15:03,555][227910] Max Reward on eval: 1720.906558811782[0m
[37m[1m[2023-07-10 22:15:03,555][227910] Min Reward on eval: 1720.906558811782[0m
[37m[1m[2023-07-10 22:15:03,556][227910] Mean Reward across all agents: 1720.906558811782[0m
[37m[1m[2023-07-10 22:15:03,556][227910] Average Trajectory Length: 999.7666666666667[0m
[36m[2023-07-10 22:15:09,024][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:15:09,024][227910] Reward + Measures: [[1350.14999295    0.21329999    0.58700001    0.2545        0.27900001]
 [1325.00307498    0.3407        0.50649995    0.2369        0.3601    ]
 [ 210.99359782    0.24751583    0.38378856    0.30784774    0.29507676]
 ...
 [1121.51721968    0.3962        0.5255        0.28980002    0.40620002]
 [ 210.49113046    0.41910002    0.32769999    0.34810001    0.41420004]
 [1113.69987097    0.31500003    0.3242        0.32179999    0.26470003]][0m
[37m[1m[2023-07-10 22:15:09,025][227910] Max Reward on eval: 1975.5670967462472[0m
[37m[1m[2023-07-10 22:15:09,025][227910] Min Reward on eval: -551.4310457658139[0m
[37m[1m[2023-07-10 22:15:09,025][227910] Mean Reward across all agents: 971.8982861666693[0m
[37m[1m[2023-07-10 22:15:09,025][227910] Average Trajectory Length: 993.1519999999999[0m
[36m[2023-07-10 22:15:09,027][227910] mean_value=-1295.5823099408067, max_value=1044.964700735197[0m
[37m[1m[2023-07-10 22:15:09,030][227910] New mean coefficients: [[ 2.6811962   0.04691264 -0.19787914 -0.11745924  0.60917366]][0m
[37m[1m[2023-07-10 22:15:09,031][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:15:18,995][227910] train() took 9.96 seconds to complete[0m
[36m[2023-07-10 22:15:18,996][227910] FPS: 385425.19[0m
[36m[2023-07-10 22:15:18,998][227910] itr=1481, itrs=2000, Progress: 74.05%[0m
[36m[2023-07-10 22:15:30,951][227910] train() took 11.76 seconds to complete[0m
[36m[2023-07-10 22:15:30,951][227910] FPS: 326500.70[0m
[36m[2023-07-10 22:15:35,705][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:15:35,706][227910] Reward + Measures: [[1855.31602585    0.25475499    0.59567702    0.17453466    0.32228231]][0m
[37m[1m[2023-07-10 22:15:35,706][227910] Max Reward on eval: 1855.316025853139[0m
[37m[1m[2023-07-10 22:15:35,706][227910] Min Reward on eval: 1855.316025853139[0m
[37m[1m[2023-07-10 22:15:35,706][227910] Mean Reward across all agents: 1855.316025853139[0m
[37m[1m[2023-07-10 22:15:35,706][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:15:41,175][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:15:41,176][227910] Reward + Measures: [[1514.41570272    0.21469998    0.61519998    0.19380002    0.2744    ]
 [ 228.90782816    0.1775054     0.32265407    0.21177296    0.22282162]
 [1634.37664112    0.2156        0.58579999    0.21250001    0.28529999]
 ...
 [1803.86246097    0.2079        0.59759998    0.19050001    0.28959998]
 [1454.92419266    0.2175        0.49270001    0.21139999    0.29890001]
 [ 801.85942702    0.43710002    0.75780004    0.45950004    0.75640005]][0m
[37m[1m[2023-07-10 22:15:41,176][227910] Max Reward on eval: 1881.098483610805[0m
[37m[1m[2023-07-10 22:15:41,177][227910] Min Reward on eval: -791.4193713022366[0m
[37m[1m[2023-07-10 22:15:41,177][227910] Mean Reward across all agents: 879.4870892410372[0m
[37m[1m[2023-07-10 22:15:41,177][227910] Average Trajectory Length: 996.1183333333333[0m
[36m[2023-07-10 22:15:41,182][227910] mean_value=-545.5791091098404, max_value=1833.7564296806931[0m
[37m[1m[2023-07-10 22:15:41,184][227910] New mean coefficients: [[ 2.2693243  0.6063913 -0.0730908  0.2411361  0.8006879]][0m
[37m[1m[2023-07-10 22:15:41,185][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:15:50,936][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 22:15:50,936][227910] FPS: 393901.07[0m
[36m[2023-07-10 22:15:50,938][227910] itr=1482, itrs=2000, Progress: 74.10%[0m
[36m[2023-07-10 22:16:02,404][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 22:16:02,404][227910] FPS: 335460.66[0m
[36m[2023-07-10 22:16:07,216][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:16:07,216][227910] Reward + Measures: [[2031.8186793     0.24501733    0.61027199    0.18132402    0.30742136]][0m
[37m[1m[2023-07-10 22:16:07,216][227910] Max Reward on eval: 2031.818679295012[0m
[37m[1m[2023-07-10 22:16:07,217][227910] Min Reward on eval: 2031.818679295012[0m
[37m[1m[2023-07-10 22:16:07,217][227910] Mean Reward across all agents: 2031.818679295012[0m
[37m[1m[2023-07-10 22:16:07,217][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:16:12,572][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:16:12,572][227910] Reward + Measures: [[ 292.85937966    0.4192        0.91289997    0.10089999    0.86199999]
 [ 275.2373796     0.41890001    0.83850002    0.1171        0.78470004]
 [ 475.2329314     0.35476866    0.32458207    0.20041792    0.26704478]
 ...
 [1347.11238736    0.352         0.6807        0.35129997    0.34829998]
 [ 566.10865979    0.38479999    0.33119997    0.25320002    0.2316    ]
 [-357.03678726    0.57490003    0.33470002    0.76939994    0.68489999]][0m
[37m[1m[2023-07-10 22:16:12,573][227910] Max Reward on eval: 2078.3773329358783[0m
[37m[1m[2023-07-10 22:16:12,573][227910] Min Reward on eval: -481.53247032118963[0m
[37m[1m[2023-07-10 22:16:12,573][227910] Mean Reward across all agents: 734.2970762005277[0m
[37m[1m[2023-07-10 22:16:12,573][227910] Average Trajectory Length: 997.9066666666666[0m
[36m[2023-07-10 22:16:12,578][227910] mean_value=-525.6220366219135, max_value=1254.7518133595972[0m
[37m[1m[2023-07-10 22:16:12,580][227910] New mean coefficients: [[2.7299094  0.47520873 0.28543    0.47786194 0.8150291 ]][0m
[37m[1m[2023-07-10 22:16:12,581][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:16:22,252][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 22:16:22,252][227910] FPS: 397156.69[0m
[36m[2023-07-10 22:16:22,254][227910] itr=1483, itrs=2000, Progress: 74.15%[0m
[36m[2023-07-10 22:16:33,723][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 22:16:33,724][227910] FPS: 335366.86[0m
[36m[2023-07-10 22:16:38,451][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:16:38,451][227910] Reward + Measures: [[2195.22142078    0.23797336    0.62936693    0.18769364    0.29460993]][0m
[37m[1m[2023-07-10 22:16:38,451][227910] Max Reward on eval: 2195.2214207831944[0m
[37m[1m[2023-07-10 22:16:38,451][227910] Min Reward on eval: 2195.2214207831944[0m
[37m[1m[2023-07-10 22:16:38,452][227910] Mean Reward across all agents: 2195.2214207831944[0m
[37m[1m[2023-07-10 22:16:38,452][227910] Average Trajectory Length: 999.92[0m
[36m[2023-07-10 22:16:43,897][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:16:43,898][227910] Reward + Measures: [[ 170.29575642    0.21328115    0.27196166    0.1034879     0.22390313]
 [1536.77267925    0.30279142    0.59808284    0.23508762    0.30875325]
 [1550.09078515    0.29910001    0.61190003    0.22330001    0.31020001]
 ...
 [ 516.75494019    0.2762        0.42960006    0.2234        0.2428    ]
 [ 295.7050485     0.2402        0.38690001    0.20179999    0.2053    ]
 [ 218.64185173    0.28340003    0.41219997    0.2561        0.24039999]][0m
[37m[1m[2023-07-10 22:16:43,898][227910] Max Reward on eval: 2227.491781285126[0m
[37m[1m[2023-07-10 22:16:43,899][227910] Min Reward on eval: -418.92707743373467[0m
[37m[1m[2023-07-10 22:16:43,899][227910] Mean Reward across all agents: 938.6347686282056[0m
[37m[1m[2023-07-10 22:16:43,899][227910] Average Trajectory Length: 994.569[0m
[36m[2023-07-10 22:16:43,901][227910] mean_value=-1253.631549260857, max_value=1354.83445112784[0m
[37m[1m[2023-07-10 22:16:43,904][227910] New mean coefficients: [[ 2.1937857   0.31924134  0.45149177 -0.21796912  0.5913583 ]][0m
[37m[1m[2023-07-10 22:16:43,905][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:16:53,880][227910] train() took 9.97 seconds to complete[0m
[36m[2023-07-10 22:16:53,880][227910] FPS: 385033.50[0m
[36m[2023-07-10 22:16:53,882][227910] itr=1484, itrs=2000, Progress: 74.20%[0m
[36m[2023-07-10 22:17:05,655][227910] train() took 11.75 seconds to complete[0m
[36m[2023-07-10 22:17:05,656][227910] FPS: 326792.83[0m
[36m[2023-07-10 22:17:10,457][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:17:10,458][227910] Reward + Measures: [[2332.63399488    0.23327366    0.64827567    0.194333      0.28551766]][0m
[37m[1m[2023-07-10 22:17:10,458][227910] Max Reward on eval: 2332.6339948762156[0m
[37m[1m[2023-07-10 22:17:10,458][227910] Min Reward on eval: 2332.6339948762156[0m
[37m[1m[2023-07-10 22:17:10,459][227910] Mean Reward across all agents: 2332.6339948762156[0m
[37m[1m[2023-07-10 22:17:10,459][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:17:15,962][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:17:15,963][227910] Reward + Measures: [[ 627.74567653    0.41420004    0.50260001    0.27200001    0.58560002]
 [1495.19466186    0.17580001    0.6947        0.18540001    0.27040002]
 [1495.02206788    0.2309        0.42179999    0.21140002    0.28749999]
 ...
 [ 547.65292346    0.53940004    0.48249999    0.27599999    0.42179996]
 [1434.50247474    0.25009999    0.45450002    0.24419999    0.29860002]
 [1167.27537511    0.21089999    0.49149999    0.227         0.28909999]][0m
[37m[1m[2023-07-10 22:17:15,963][227910] Max Reward on eval: 2313.3175241608174[0m
[37m[1m[2023-07-10 22:17:15,963][227910] Min Reward on eval: -346.3284711439279[0m
[37m[1m[2023-07-10 22:17:15,963][227910] Mean Reward across all agents: 1256.1791186046316[0m
[37m[1m[2023-07-10 22:17:15,964][227910] Average Trajectory Length: 995.7873333333333[0m
[36m[2023-07-10 22:17:15,966][227910] mean_value=-1129.7730140424885, max_value=889.5596761705401[0m
[37m[1m[2023-07-10 22:17:15,969][227910] New mean coefficients: [[ 2.3597527   0.00339657  0.47132894 -0.5554265  -0.08823442]][0m
[37m[1m[2023-07-10 22:17:15,970][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:17:25,881][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 22:17:25,881][227910] FPS: 387514.33[0m
[36m[2023-07-10 22:17:25,883][227910] itr=1485, itrs=2000, Progress: 74.25%[0m
[36m[2023-07-10 22:17:37,582][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 22:17:37,583][227910] FPS: 328866.13[0m
[36m[2023-07-10 22:17:42,298][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:17:42,299][227910] Reward + Measures: [[2455.81689075    0.23252848    0.65014571    0.19743921    0.27418888]][0m
[37m[1m[2023-07-10 22:17:42,299][227910] Max Reward on eval: 2455.816890745809[0m
[37m[1m[2023-07-10 22:17:42,299][227910] Min Reward on eval: 2455.816890745809[0m
[37m[1m[2023-07-10 22:17:42,299][227910] Mean Reward across all agents: 2455.816890745809[0m
[37m[1m[2023-07-10 22:17:42,300][227910] Average Trajectory Length: 999.4713333333333[0m
[36m[2023-07-10 22:17:47,992][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:17:47,993][227910] Reward + Measures: [[1351.54335187    0.37990001    0.56010002    0.19489999    0.4073    ]
 [1838.5441726     0.2764        0.53170007    0.22579999    0.30669999]
 [1406.73913044    0.38870001    0.55849999    0.2103        0.35560003]
 ...
 [ 109.19544306    0.27495441    0.3096725     0.18355718    0.21409746]
 [1251.30275379    0.19520001    0.58280003    0.24319999    0.2586    ]
 [1691.75381107    0.19030002    0.63770002    0.17220001    0.32860002]][0m
[37m[1m[2023-07-10 22:17:47,993][227910] Max Reward on eval: 2424.1753921439404[0m
[37m[1m[2023-07-10 22:17:47,993][227910] Min Reward on eval: -156.3769143148558[0m
[37m[1m[2023-07-10 22:17:47,994][227910] Mean Reward across all agents: 1281.6700484899188[0m
[37m[1m[2023-07-10 22:17:47,994][227910] Average Trajectory Length: 997.1096666666666[0m
[36m[2023-07-10 22:17:47,998][227910] mean_value=-893.8260395744918, max_value=1899.2304257888045[0m
[37m[1m[2023-07-10 22:17:48,000][227910] New mean coefficients: [[ 2.2910888   0.5999148   0.76955146 -0.17311049 -0.05443604]][0m
[37m[1m[2023-07-10 22:17:48,001][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:17:57,654][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 22:17:57,654][227910] FPS: 397893.05[0m
[36m[2023-07-10 22:17:57,656][227910] itr=1486, itrs=2000, Progress: 74.30%[0m
[36m[2023-07-10 22:18:09,129][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 22:18:09,129][227910] FPS: 335306.17[0m
[36m[2023-07-10 22:18:13,984][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:18:13,985][227910] Reward + Measures: [[2600.93117768    0.22785237    0.66080791    0.20504522    0.26566818]][0m
[37m[1m[2023-07-10 22:18:13,985][227910] Max Reward on eval: 2600.9311776819914[0m
[37m[1m[2023-07-10 22:18:13,985][227910] Min Reward on eval: 2600.9311776819914[0m
[37m[1m[2023-07-10 22:18:13,985][227910] Mean Reward across all agents: 2600.9311776819914[0m
[37m[1m[2023-07-10 22:18:13,986][227910] Average Trajectory Length: 999.81[0m
[36m[2023-07-10 22:18:19,512][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:18:19,513][227910] Reward + Measures: [[2042.44624038    0.245         0.65200007    0.2124        0.28660002]
 [2280.59052173    0.2422        0.5675        0.25190002    0.23940001]
 [1098.24998011    0.27710003    0.49670002    0.1875        0.30689999]
 ...
 [2551.90016474    0.22040001    0.64729995    0.2181        0.24730001]
 [ 993.12498072    0.29080003    0.57349998    0.21010001    0.3662    ]
 [1989.49599339    0.28620002    0.64639997    0.27590001    0.26300001]][0m
[37m[1m[2023-07-10 22:18:19,513][227910] Max Reward on eval: 2801.043229094334[0m
[37m[1m[2023-07-10 22:18:19,513][227910] Min Reward on eval: -23.77431429326898[0m
[37m[1m[2023-07-10 22:18:19,513][227910] Mean Reward across all agents: 1393.5205949484368[0m
[37m[1m[2023-07-10 22:18:19,514][227910] Average Trajectory Length: 999.127[0m
[36m[2023-07-10 22:18:19,516][227910] mean_value=-621.2022887462805, max_value=1601.9540098791217[0m
[37m[1m[2023-07-10 22:18:19,519][227910] New mean coefficients: [[ 2.1828854   0.12432966  0.86478704 -0.5064176  -0.18106484]][0m
[37m[1m[2023-07-10 22:18:19,520][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:18:29,108][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 22:18:29,108][227910] FPS: 400550.21[0m
[36m[2023-07-10 22:18:29,111][227910] itr=1487, itrs=2000, Progress: 74.35%[0m
[36m[2023-07-10 22:18:40,763][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 22:18:40,763][227910] FPS: 330072.66[0m
[36m[2023-07-10 22:18:45,509][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:18:45,509][227910] Reward + Measures: [[2764.3612379     0.22487974    0.67841458    0.20764907    0.25690842]][0m
[37m[1m[2023-07-10 22:18:45,509][227910] Max Reward on eval: 2764.3612378972325[0m
[37m[1m[2023-07-10 22:18:45,510][227910] Min Reward on eval: 2764.3612378972325[0m
[37m[1m[2023-07-10 22:18:45,510][227910] Mean Reward across all agents: 2764.3612378972325[0m
[37m[1m[2023-07-10 22:18:45,510][227910] Average Trajectory Length: 999.9996666666666[0m
[36m[2023-07-10 22:18:51,060][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:18:51,060][227910] Reward + Measures: [[1664.43586345    0.2836        0.53290004    0.20009999    0.28980002]
 [1022.73892416    0.22680001    0.42550001    0.15109999    0.28719997]
 [1265.38710001    0.229         0.41729999    0.19160001    0.27270001]
 ...
 [1801.61804789    0.28480002    0.64120001    0.3231        0.34240001]
 [1091.04222949    0.3064        0.3784        0.20009999    0.23730002]
 [1833.24111367    0.2462        0.55559999    0.2332        0.2735    ]][0m
[37m[1m[2023-07-10 22:18:51,061][227910] Max Reward on eval: 2586.131890145317[0m
[37m[1m[2023-07-10 22:18:51,061][227910] Min Reward on eval: -81.1191240040207[0m
[37m[1m[2023-07-10 22:18:51,061][227910] Mean Reward across all agents: 1366.0011767474086[0m
[37m[1m[2023-07-10 22:18:51,062][227910] Average Trajectory Length: 998.3483333333332[0m
[36m[2023-07-10 22:18:51,065][227910] mean_value=-775.0551479238912, max_value=2704.744545684522[0m
[37m[1m[2023-07-10 22:18:51,067][227910] New mean coefficients: [[ 2.4315982  -0.18554181  0.6306999  -0.5212621   0.02572243]][0m
[37m[1m[2023-07-10 22:18:51,068][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:19:00,900][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 22:19:00,901][227910] FPS: 390607.93[0m
[36m[2023-07-10 22:19:00,903][227910] itr=1488, itrs=2000, Progress: 74.40%[0m
[36m[2023-07-10 22:19:12,626][227910] train() took 11.70 seconds to complete[0m
[36m[2023-07-10 22:19:12,626][227910] FPS: 328118.67[0m
[36m[2023-07-10 22:19:17,375][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:19:17,381][227910] Reward + Measures: [[2915.07487224    0.21859835    0.68759429    0.21398498    0.24798299]][0m
[37m[1m[2023-07-10 22:19:17,381][227910] Max Reward on eval: 2915.0748722413196[0m
[37m[1m[2023-07-10 22:19:17,381][227910] Min Reward on eval: 2915.0748722413196[0m
[37m[1m[2023-07-10 22:19:17,382][227910] Mean Reward across all agents: 2915.0748722413196[0m
[37m[1m[2023-07-10 22:19:17,382][227910] Average Trajectory Length: 999.8776666666666[0m
[36m[2023-07-10 22:19:22,886][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:19:22,891][227910] Reward + Measures: [[1639.25010583    0.29210001    0.53579998    0.35040003    0.2263    ]
 [1092.75527434    0.42359996    0.39970002    0.59860003    0.3116    ]
 [2608.07411341    0.2208        0.69329995    0.22199999    0.28920001]
 ...
 [1495.60477615    0.32360002    0.50139999    0.53260005    0.23720001]
 [1778.51748844    0.2701        0.53360003    0.38100004    0.2386    ]
 [1420.73434654    0.26900002    0.42050001    0.21690002    0.2714    ]][0m
[37m[1m[2023-07-10 22:19:22,892][227910] Max Reward on eval: 2923.960381705314[0m
[37m[1m[2023-07-10 22:19:22,892][227910] Min Reward on eval: 398.2389943883871[0m
[37m[1m[2023-07-10 22:19:22,892][227910] Mean Reward across all agents: 1780.359202307321[0m
[37m[1m[2023-07-10 22:19:22,892][227910] Average Trajectory Length: 998.658[0m
[36m[2023-07-10 22:19:22,896][227910] mean_value=-466.6843265075126, max_value=1856.2775914609774[0m
[37m[1m[2023-07-10 22:19:22,898][227910] New mean coefficients: [[ 2.4168584  -0.36486185  0.49041927 -0.59948784  0.15870342]][0m
[37m[1m[2023-07-10 22:19:22,900][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:19:32,609][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 22:19:32,610][227910] FPS: 395539.14[0m
[36m[2023-07-10 22:19:32,612][227910] itr=1489, itrs=2000, Progress: 74.45%[0m
[36m[2023-07-10 22:19:44,144][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 22:19:44,144][227910] FPS: 333646.00[0m
[36m[2023-07-10 22:19:49,033][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:19:49,033][227910] Reward + Measures: [[3008.53982652    0.21889775    0.69501197    0.21211249    0.24625914]][0m
[37m[1m[2023-07-10 22:19:49,033][227910] Max Reward on eval: 3008.5398265189688[0m
[37m[1m[2023-07-10 22:19:49,034][227910] Min Reward on eval: 3008.5398265189688[0m
[37m[1m[2023-07-10 22:19:49,034][227910] Mean Reward across all agents: 3008.5398265189688[0m
[37m[1m[2023-07-10 22:19:49,034][227910] Average Trajectory Length: 999.9336666666667[0m
[36m[2023-07-10 22:19:54,585][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:19:54,586][227910] Reward + Measures: [[2567.64352386    0.25900003    0.73890007    0.2198        0.26830003]
 [1447.89356861    0.1761        0.47160003    0.19059999    0.24790001]
 [1775.18168448    0.23720001    0.5485        0.24249999    0.23730002]
 ...
 [2446.01218129    0.21820001    0.64130002    0.2234        0.27410001]
 [1445.37227516    0.1971        0.53460002    0.19880001    0.30130002]
 [1679.69103882    0.27470002    0.59680003    0.2572        0.3788    ]][0m
[37m[1m[2023-07-10 22:19:54,586][227910] Max Reward on eval: 3021.0104606400246[0m
[37m[1m[2023-07-10 22:19:54,586][227910] Min Reward on eval: -336.8331530489057[0m
[37m[1m[2023-07-10 22:19:54,586][227910] Mean Reward across all agents: 1806.9795166626732[0m
[37m[1m[2023-07-10 22:19:54,587][227910] Average Trajectory Length: 996.4596666666666[0m
[36m[2023-07-10 22:19:54,590][227910] mean_value=-955.9594347350135, max_value=2797.685475104549[0m
[37m[1m[2023-07-10 22:19:54,593][227910] New mean coefficients: [[ 2.0751555   0.27661955  0.6457745  -0.09286135  0.4329689 ]][0m
[37m[1m[2023-07-10 22:19:54,594][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:20:04,451][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 22:20:04,451][227910] FPS: 389616.87[0m
[36m[2023-07-10 22:20:04,454][227910] itr=1490, itrs=2000, Progress: 74.50%[0m
[37m[1m[2023-07-10 22:20:08,634][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001470[0m
[36m[2023-07-10 22:20:20,482][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 22:20:20,483][227910] FPS: 331869.19[0m
[36m[2023-07-10 22:20:25,171][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:20:25,171][227910] Reward + Measures: [[3146.55307541    0.21729597    0.69837469    0.21546398    0.24072933]][0m
[37m[1m[2023-07-10 22:20:25,172][227910] Max Reward on eval: 3146.5530754146353[0m
[37m[1m[2023-07-10 22:20:25,172][227910] Min Reward on eval: 3146.5530754146353[0m
[37m[1m[2023-07-10 22:20:25,172][227910] Mean Reward across all agents: 3146.5530754146353[0m
[37m[1m[2023-07-10 22:20:25,172][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:20:30,657][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:20:30,663][227910] Reward + Measures: [[2213.94698477    0.23290001    0.58820003    0.22570001    0.28890002]
 [2682.26539166    0.21350002    0.65340006    0.21759999    0.2595    ]
 [ 412.15816085    0.21140002    0.58210003    0.2376        0.22830001]
 ...
 [1357.00301624    0.2299        0.47989997    0.2146        0.24010001]
 [1748.76578019    0.20640002    0.62600005    0.28570002    0.33059999]
 [2165.64395916    0.24370001    0.53500003    0.30399999    0.2349    ]][0m
[37m[1m[2023-07-10 22:20:30,663][227910] Max Reward on eval: 3137.2415413819253[0m
[37m[1m[2023-07-10 22:20:30,664][227910] Min Reward on eval: -331.1470739641416[0m
[37m[1m[2023-07-10 22:20:30,664][227910] Mean Reward across all agents: 1431.17277641677[0m
[37m[1m[2023-07-10 22:20:30,664][227910] Average Trajectory Length: 997.89[0m
[36m[2023-07-10 22:20:30,667][227910] mean_value=-862.605830959436, max_value=1729.510571019954[0m
[37m[1m[2023-07-10 22:20:30,669][227910] New mean coefficients: [[ 2.3452492   0.28929716  0.39399093 -0.11446764  0.27183032]][0m
[37m[1m[2023-07-10 22:20:30,670][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:20:40,428][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 22:20:40,429][227910] FPS: 393582.52[0m
[36m[2023-07-10 22:20:40,431][227910] itr=1491, itrs=2000, Progress: 74.55%[0m
[36m[2023-07-10 22:20:51,924][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 22:20:51,924][227910] FPS: 334646.78[0m
[36m[2023-07-10 22:20:56,568][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:20:56,569][227910] Reward + Measures: [[3263.0825662     0.21349499    0.6944297     0.22091699    0.23296334]][0m
[37m[1m[2023-07-10 22:20:56,569][227910] Max Reward on eval: 3263.082566202837[0m
[37m[1m[2023-07-10 22:20:56,569][227910] Min Reward on eval: 3263.082566202837[0m
[37m[1m[2023-07-10 22:20:56,569][227910] Mean Reward across all agents: 3263.082566202837[0m
[37m[1m[2023-07-10 22:20:56,569][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:21:02,128][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:21:02,128][227910] Reward + Measures: [[ 595.74879119    0.2414        0.27879998    0.13770001    0.22879998]
 [ 920.76420701    0.23990002    0.47609997    0.1779        0.39709997]
 [2199.57869047    0.26610002    0.60690004    0.19860001    0.23449998]
 ...
 [ 594.8537088     0.50810003    0.30520001    0.42030001    0.30939999]
 [1176.81982281    0.26030001    0.44619998    0.16019998    0.33040002]
 [2395.70444799    0.23729999    0.63120002    0.27989998    0.27449998]][0m
[37m[1m[2023-07-10 22:21:02,129][227910] Max Reward on eval: 3110.5729755312204[0m
[37m[1m[2023-07-10 22:21:02,129][227910] Min Reward on eval: -862.9677523143124[0m
[37m[1m[2023-07-10 22:21:02,129][227910] Mean Reward across all agents: 1408.012409020396[0m
[37m[1m[2023-07-10 22:21:02,129][227910] Average Trajectory Length: 994.9866666666667[0m
[36m[2023-07-10 22:21:02,132][227910] mean_value=-952.9356103260284, max_value=1407.237288015129[0m
[37m[1m[2023-07-10 22:21:02,134][227910] New mean coefficients: [[ 2.35035     0.15438889  0.57869434 -0.02495265  0.04685372]][0m
[37m[1m[2023-07-10 22:21:02,135][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:21:11,837][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 22:21:11,838][227910] FPS: 395841.10[0m
[36m[2023-07-10 22:21:11,840][227910] itr=1492, itrs=2000, Progress: 74.60%[0m
[36m[2023-07-10 22:21:23,337][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 22:21:23,338][227910] FPS: 334527.29[0m
[36m[2023-07-10 22:21:28,102][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:21:28,103][227910] Reward + Measures: [[3354.78008391    0.21026534    0.69083601    0.22831266    0.22720934]][0m
[37m[1m[2023-07-10 22:21:28,103][227910] Max Reward on eval: 3354.780083910899[0m
[37m[1m[2023-07-10 22:21:28,103][227910] Min Reward on eval: 3354.780083910899[0m
[37m[1m[2023-07-10 22:21:28,103][227910] Mean Reward across all agents: 3354.780083910899[0m
[37m[1m[2023-07-10 22:21:28,103][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:21:33,528][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:21:33,529][227910] Reward + Measures: [[2680.6758191     0.23050001    0.62070006    0.25819999    0.2642    ]
 [2441.03722162    0.20200001    0.57690001    0.23220001    0.21430002]
 [2237.01070676    0.19160001    0.74299997    0.1974        0.25999999]
 ...
 [2064.14546198    0.2253        0.56549996    0.19930001    0.20119999]
 [1607.04065999    0.2985        0.67180002    0.2273        0.34699997]
 [1640.72493944    0.25209999    0.57530004    0.39089999    0.2863    ]][0m
[37m[1m[2023-07-10 22:21:33,529][227910] Max Reward on eval: 3305.116310710274[0m
[37m[1m[2023-07-10 22:21:33,529][227910] Min Reward on eval: -135.09816791031045[0m
[37m[1m[2023-07-10 22:21:33,529][227910] Mean Reward across all agents: 1639.1454422377408[0m
[37m[1m[2023-07-10 22:21:33,530][227910] Average Trajectory Length: 970.704[0m
[36m[2023-07-10 22:21:33,532][227910] mean_value=-1157.4985158010236, max_value=2216.541083735246[0m
[37m[1m[2023-07-10 22:21:33,535][227910] New mean coefficients: [[ 2.1073468  -0.02410127 -0.07858163 -0.46333697  0.07538128]][0m
[37m[1m[2023-07-10 22:21:33,536][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:21:43,399][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 22:21:43,399][227910] FPS: 389405.55[0m
[36m[2023-07-10 22:21:43,402][227910] itr=1493, itrs=2000, Progress: 74.65%[0m
[36m[2023-07-10 22:21:55,036][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 22:21:55,037][227910] FPS: 330655.47[0m
[36m[2023-07-10 22:21:59,859][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:21:59,859][227910] Reward + Measures: [[3456.04506023    0.21040267    0.69575852    0.23306435    0.22626413]][0m
[37m[1m[2023-07-10 22:21:59,860][227910] Max Reward on eval: 3456.0450602297437[0m
[37m[1m[2023-07-10 22:21:59,860][227910] Min Reward on eval: 3456.0450602297437[0m
[37m[1m[2023-07-10 22:21:59,860][227910] Mean Reward across all agents: 3456.0450602297437[0m
[37m[1m[2023-07-10 22:21:59,860][227910] Average Trajectory Length: 999.7663333333333[0m
[36m[2023-07-10 22:22:05,344][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:22:05,345][227910] Reward + Measures: [[2465.64036103    0.20420001    0.58919996    0.24060002    0.25480002]
 [ 456.01000705    0.2213145     0.55315411    0.28870097    0.47498307]
 [1934.4703962     0.18479998    0.59980005    0.249         0.31060001]
 ...
 [1033.18530035    0.41420004    0.56420004    0.3389        0.31560001]
 [2539.8587492     0.20219998    0.61480004    0.20209999    0.24270001]
 [1481.57805004    0.20560001    0.54449999    0.29270002    0.24950002]][0m
[37m[1m[2023-07-10 22:22:05,345][227910] Max Reward on eval: 3446.630820154166[0m
[37m[1m[2023-07-10 22:22:05,345][227910] Min Reward on eval: -167.06651598762838[0m
[37m[1m[2023-07-10 22:22:05,346][227910] Mean Reward across all agents: 1757.2924192380704[0m
[37m[1m[2023-07-10 22:22:05,346][227910] Average Trajectory Length: 999.0459999999999[0m
[36m[2023-07-10 22:22:05,348][227910] mean_value=-426.71177045445387, max_value=1291.2914531302602[0m
[37m[1m[2023-07-10 22:22:05,351][227910] New mean coefficients: [[ 1.6992564   0.22058387 -0.17512849 -0.42687294  0.22392875]][0m
[37m[1m[2023-07-10 22:22:05,352][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:22:15,010][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 22:22:15,010][227910] FPS: 397653.50[0m
[36m[2023-07-10 22:22:15,013][227910] itr=1494, itrs=2000, Progress: 74.70%[0m
[36m[2023-07-10 22:22:26,518][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 22:22:26,518][227910] FPS: 334339.50[0m
[36m[2023-07-10 22:22:31,327][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:22:31,328][227910] Reward + Measures: [[3586.05718594    0.20819688    0.69021791    0.2361204     0.22034693]][0m
[37m[1m[2023-07-10 22:22:31,328][227910] Max Reward on eval: 3586.0571859370434[0m
[37m[1m[2023-07-10 22:22:31,328][227910] Min Reward on eval: 3586.0571859370434[0m
[37m[1m[2023-07-10 22:22:31,328][227910] Mean Reward across all agents: 3586.0571859370434[0m
[37m[1m[2023-07-10 22:22:31,329][227910] Average Trajectory Length: 999.779[0m
[36m[2023-07-10 22:22:36,744][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:22:36,744][227910] Reward + Measures: [[2739.19848168    0.20250002    0.60089999    0.2376        0.2131    ]
 [1822.54099437    0.23457885    0.54676789    0.16424842    0.19053762]
 [3241.12882852    0.21870001    0.64589995    0.19630001    0.19810002]
 ...
 [3023.60568198    0.1944        0.65350002    0.29830003    0.24530001]
 [2644.47951247    0.22780001    0.64399999    0.31200001    0.2595    ]
 [2534.04329743    0.199         0.60510004    0.31080002    0.27480003]][0m
[37m[1m[2023-07-10 22:22:36,744][227910] Max Reward on eval: 3587.929150334746[0m
[37m[1m[2023-07-10 22:22:36,745][227910] Min Reward on eval: -75.72573718713247[0m
[37m[1m[2023-07-10 22:22:36,745][227910] Mean Reward across all agents: 1943.4893149890374[0m
[37m[1m[2023-07-10 22:22:36,745][227910] Average Trajectory Length: 977.6086666666666[0m
[36m[2023-07-10 22:22:36,748][227910] mean_value=-1045.3122365756117, max_value=2118.6177698645556[0m
[37m[1m[2023-07-10 22:22:36,751][227910] New mean coefficients: [[ 1.4395254   0.2134006   0.22544914 -0.20981047  0.29931155]][0m
[37m[1m[2023-07-10 22:22:36,752][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:22:46,619][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 22:22:46,619][227910] FPS: 389267.02[0m
[36m[2023-07-10 22:22:46,621][227910] itr=1495, itrs=2000, Progress: 74.75%[0m
[36m[2023-07-10 22:22:58,258][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 22:22:58,258][227910] FPS: 330516.86[0m
[36m[2023-07-10 22:23:03,051][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:23:03,056][227910] Reward + Measures: [[3713.65312093    0.20480967    0.68379587    0.24040566    0.21059233]][0m
[37m[1m[2023-07-10 22:23:03,056][227910] Max Reward on eval: 3713.6531209258083[0m
[37m[1m[2023-07-10 22:23:03,057][227910] Min Reward on eval: 3713.6531209258083[0m
[37m[1m[2023-07-10 22:23:03,057][227910] Mean Reward across all agents: 3713.6531209258083[0m
[37m[1m[2023-07-10 22:23:03,057][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:23:08,595][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:23:08,601][227910] Reward + Measures: [[3158.0279864     0.22379999    0.65780002    0.25930002    0.22669999]
 [1075.59607946    0.39320001    0.59470004    0.1337        0.46250007]
 [2280.07799619    0.2419        0.51679999    0.2735        0.22160001]
 ...
 [2271.14434841    0.2418        0.60089999    0.35279998    0.30070001]
 [1117.69521371    0.37560001    0.52600002    0.09949999    0.38580003]
 [2216.14154279    0.2149        0.50450003    0.31539997    0.25080001]][0m
[37m[1m[2023-07-10 22:23:08,601][227910] Max Reward on eval: 3613.7628275059164[0m
[37m[1m[2023-07-10 22:23:08,601][227910] Min Reward on eval: 22.215931436850223[0m
[37m[1m[2023-07-10 22:23:08,602][227910] Mean Reward across all agents: 1874.957828779623[0m
[37m[1m[2023-07-10 22:23:08,602][227910] Average Trajectory Length: 994.9686666666666[0m
[36m[2023-07-10 22:23:08,606][227910] mean_value=-617.4972675071285, max_value=1538.7785398198437[0m
[37m[1m[2023-07-10 22:23:08,608][227910] New mean coefficients: [[ 1.5838683  -0.04939115 -0.40230745 -0.00753213  0.3627498 ]][0m
[37m[1m[2023-07-10 22:23:08,610][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:23:18,288][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 22:23:18,288][227910] FPS: 396822.27[0m
[36m[2023-07-10 22:23:18,291][227910] itr=1496, itrs=2000, Progress: 74.80%[0m
[36m[2023-07-10 22:23:29,896][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 22:23:29,896][227910] FPS: 331454.97[0m
[36m[2023-07-10 22:23:34,596][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:23:34,597][227910] Reward + Measures: [[3794.73066017    0.20271374    0.67774874    0.24597733    0.20583734]][0m
[37m[1m[2023-07-10 22:23:34,597][227910] Max Reward on eval: 3794.730660171844[0m
[37m[1m[2023-07-10 22:23:34,597][227910] Min Reward on eval: 3794.730660171844[0m
[37m[1m[2023-07-10 22:23:34,597][227910] Mean Reward across all agents: 3794.730660171844[0m
[37m[1m[2023-07-10 22:23:34,597][227910] Average Trajectory Length: 999.3176666666666[0m
[36m[2023-07-10 22:23:40,231][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:23:40,232][227910] Reward + Measures: [[1297.83780004    0.28659996    0.42640001    0.15480001    0.28570002]
 [ 113.92923862    0.49740002    0.28529999    0.54549998    0.35840002]
 [1730.38943178    0.22778396    0.35976672    0.19347474    0.19717474]
 ...
 [2112.87254736    0.2033        0.52450001    0.27559999    0.3603    ]
 [ 737.39148645    0.28470001    0.43759999    0.36069998    0.39680001]
 [ 819.18546339    0.21070002    0.47440001    0.29799998    0.30400002]][0m
[37m[1m[2023-07-10 22:23:40,232][227910] Max Reward on eval: 3723.7341192606837[0m
[37m[1m[2023-07-10 22:23:40,233][227910] Min Reward on eval: -63.33956094258465[0m
[37m[1m[2023-07-10 22:23:40,233][227910] Mean Reward across all agents: 1690.5524523558029[0m
[37m[1m[2023-07-10 22:23:40,233][227910] Average Trajectory Length: 996.5369999999999[0m
[36m[2023-07-10 22:23:40,236][227910] mean_value=-887.399299035276, max_value=2157.063005658095[0m
[37m[1m[2023-07-10 22:23:40,239][227910] New mean coefficients: [[1.2424318  0.19108585 0.05223411 0.1580245  0.28809905]][0m
[37m[1m[2023-07-10 22:23:40,240][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:23:49,987][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 22:23:49,987][227910] FPS: 394030.92[0m
[36m[2023-07-10 22:23:49,989][227910] itr=1497, itrs=2000, Progress: 74.85%[0m
[36m[2023-07-10 22:24:01,670][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 22:24:01,670][227910] FPS: 329333.74[0m
[36m[2023-07-10 22:24:06,514][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:24:06,514][227910] Reward + Measures: [[3806.78369878    0.19984296    0.65183663    0.26012594    0.19217098]][0m
[37m[1m[2023-07-10 22:24:06,515][227910] Max Reward on eval: 3806.783698780273[0m
[37m[1m[2023-07-10 22:24:06,515][227910] Min Reward on eval: 3806.783698780273[0m
[37m[1m[2023-07-10 22:24:06,515][227910] Mean Reward across all agents: 3806.783698780273[0m
[37m[1m[2023-07-10 22:24:06,515][227910] Average Trajectory Length: 999.669[0m
[36m[2023-07-10 22:24:12,038][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:24:12,039][227910] Reward + Measures: [[1818.09399067    0.25049999    0.50310004    0.2339        0.1858    ]
 [1373.91613114    0.28119999    0.39579999    0.199         0.23120002]
 [3053.9663141     0.226         0.68000001    0.2132        0.20370002]
 ...
 [1750.19377676    0.2175        0.43850002    0.221         0.16940001]
 [ 740.71889641    0.20833011    0.35457635    0.19288279    0.14925592]
 [2695.89927859    0.18610001    0.5715        0.28430003    0.20070003]][0m
[37m[1m[2023-07-10 22:24:12,039][227910] Max Reward on eval: 3725.096134253964[0m
[37m[1m[2023-07-10 22:24:12,039][227910] Min Reward on eval: -60.62740257775003[0m
[37m[1m[2023-07-10 22:24:12,039][227910] Mean Reward across all agents: 1796.1302904745417[0m
[37m[1m[2023-07-10 22:24:12,040][227910] Average Trajectory Length: 992.87[0m
[36m[2023-07-10 22:24:12,041][227910] mean_value=-1292.6361201503782, max_value=1041.9200300873536[0m
[37m[1m[2023-07-10 22:24:12,044][227910] New mean coefficients: [[ 1.201629    0.03517227 -0.35400292  0.09566268  0.01708463]][0m
[37m[1m[2023-07-10 22:24:12,045][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:24:21,821][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 22:24:21,822][227910] FPS: 392856.30[0m
[36m[2023-07-10 22:24:21,824][227910] itr=1498, itrs=2000, Progress: 74.90%[0m
[36m[2023-07-10 22:24:33,442][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 22:24:33,443][227910] FPS: 331040.90[0m
[36m[2023-07-10 22:24:38,199][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:24:38,200][227910] Reward + Measures: [[3884.03119688    0.1979318     0.64538896    0.26568389    0.18950664]][0m
[37m[1m[2023-07-10 22:24:38,200][227910] Max Reward on eval: 3884.0311968821898[0m
[37m[1m[2023-07-10 22:24:38,200][227910] Min Reward on eval: 3884.0311968821898[0m
[37m[1m[2023-07-10 22:24:38,200][227910] Mean Reward across all agents: 3884.0311968821898[0m
[37m[1m[2023-07-10 22:24:38,200][227910] Average Trajectory Length: 999.3396666666666[0m
[36m[2023-07-10 22:24:43,627][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:24:43,627][227910] Reward + Measures: [[2657.61885353    0.25850001    0.53280002    0.26210001    0.2217    ]
 [2476.51545875    0.29840001    0.57739997    0.29879999    0.26550001]
 [2087.04595931    0.26179999    0.46399999    0.23559999    0.1806    ]
 ...
 [2578.00780815    0.23000002    0.65239996    0.27160001    0.21360002]
 [2640.79620029    0.22059999    0.54370004    0.26290002    0.2333    ]
 [2255.81109323    0.18550001    0.51389998    0.24650002    0.20609999]][0m
[37m[1m[2023-07-10 22:24:43,628][227910] Max Reward on eval: 3828.7240375077354[0m
[37m[1m[2023-07-10 22:24:43,628][227910] Min Reward on eval: -162.11318903927457[0m
[37m[1m[2023-07-10 22:24:43,628][227910] Mean Reward across all agents: 2074.9957140283527[0m
[37m[1m[2023-07-10 22:24:43,628][227910] Average Trajectory Length: 994.3863333333333[0m
[36m[2023-07-10 22:24:43,630][227910] mean_value=-1285.8707030449702, max_value=3813.8914143305274[0m
[37m[1m[2023-07-10 22:24:43,633][227910] New mean coefficients: [[0.7084609  0.34914094 0.03796813 0.23764    0.17861041]][0m
[37m[1m[2023-07-10 22:24:43,634][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:24:53,370][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 22:24:53,371][227910] FPS: 394477.39[0m
[36m[2023-07-10 22:24:53,373][227910] itr=1499, itrs=2000, Progress: 74.95%[0m
[36m[2023-07-10 22:25:04,889][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 22:25:04,889][227910] FPS: 333974.36[0m
[36m[2023-07-10 22:25:09,693][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:25:09,694][227910] Reward + Measures: [[3968.93264902    0.19939469    0.64782524    0.27631557    0.18980306]][0m
[37m[1m[2023-07-10 22:25:09,694][227910] Max Reward on eval: 3968.9326490226026[0m
[37m[1m[2023-07-10 22:25:09,694][227910] Min Reward on eval: 3968.9326490226026[0m
[37m[1m[2023-07-10 22:25:09,694][227910] Mean Reward across all agents: 3968.9326490226026[0m
[37m[1m[2023-07-10 22:25:09,695][227910] Average Trajectory Length: 999.7476666666666[0m
[36m[2023-07-10 22:25:15,209][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:25:15,210][227910] Reward + Measures: [[2365.647352      0.25932774    0.50063038    0.22756238    0.20660691]
 [2854.1624627     0.21900001    0.53530008    0.24970002    0.2089    ]
 [1062.19620873    0.20250002    0.42430001    0.34419999    0.26370001]
 ...
 [3580.42333555    0.2277        0.59069997    0.27620003    0.21030001]
 [2493.39905284    0.21920002    0.5632        0.28239998    0.21440001]
 [1693.03959928    0.23239999    0.47390005    0.2383        0.2554    ]][0m
[37m[1m[2023-07-10 22:25:15,210][227910] Max Reward on eval: 3664.2398910842835[0m
[37m[1m[2023-07-10 22:25:15,210][227910] Min Reward on eval: 520.054089345222[0m
[37m[1m[2023-07-10 22:25:15,210][227910] Mean Reward across all agents: 2308.176460394945[0m
[37m[1m[2023-07-10 22:25:15,211][227910] Average Trajectory Length: 992.3893333333333[0m
[36m[2023-07-10 22:25:15,213][227910] mean_value=-1490.0303125484186, max_value=3693.9444929671286[0m
[37m[1m[2023-07-10 22:25:15,215][227910] New mean coefficients: [[ 0.6169423  -0.04305634  0.07010013 -0.16660085 -0.0671384 ]][0m
[37m[1m[2023-07-10 22:25:15,216][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:25:25,059][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 22:25:25,059][227910] FPS: 390228.31[0m
[36m[2023-07-10 22:25:25,061][227910] itr=1500, itrs=2000, Progress: 75.00%[0m
[37m[1m[2023-07-10 22:25:29,192][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001480[0m
[36m[2023-07-10 22:25:41,073][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 22:25:41,073][227910] FPS: 330880.80[0m
[36m[2023-07-10 22:25:46,015][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:25:46,015][227910] Reward + Measures: [[4058.63416222    0.19592407    0.64666486    0.2705138     0.18503483]][0m
[37m[1m[2023-07-10 22:25:46,016][227910] Max Reward on eval: 4058.6341622219957[0m
[37m[1m[2023-07-10 22:25:46,016][227910] Min Reward on eval: 4058.6341622219957[0m
[37m[1m[2023-07-10 22:25:46,016][227910] Mean Reward across all agents: 4058.6341622219957[0m
[37m[1m[2023-07-10 22:25:46,016][227910] Average Trajectory Length: 999.2566666666667[0m
[36m[2023-07-10 22:25:51,556][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:25:51,557][227910] Reward + Measures: [[2209.12721935    0.33320004    0.59849995    0.3143        0.3073    ]
 [3040.96291081    0.21390001    0.64059997    0.30450001    0.2194    ]
 [3283.66255858    0.1962        0.64280003    0.2244        0.19029999]
 ...
 [1676.1433541     0.20460001    0.51630002    0.2897        0.21950002]
 [2483.43223745    0.2362        0.60090005    0.28260002    0.2177    ]
 [2000.57225374    0.37820002    0.55299997    0.22449999    0.20689999]][0m
[37m[1m[2023-07-10 22:25:51,557][227910] Max Reward on eval: 3953.428941011708[0m
[37m[1m[2023-07-10 22:25:51,557][227910] Min Reward on eval: 209.9440437499783[0m
[37m[1m[2023-07-10 22:25:51,557][227910] Mean Reward across all agents: 2317.2129866069727[0m
[37m[1m[2023-07-10 22:25:51,558][227910] Average Trajectory Length: 996.3876666666666[0m
[36m[2023-07-10 22:25:51,560][227910] mean_value=-986.9110281628623, max_value=1524.3164328072949[0m
[37m[1m[2023-07-10 22:25:51,562][227910] New mean coefficients: [[ 0.66167396 -0.02857459 -0.23132953 -0.23730397 -0.21229622]][0m
[37m[1m[2023-07-10 22:25:51,563][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:26:01,243][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 22:26:01,243][227910] FPS: 396788.00[0m
[36m[2023-07-10 22:26:01,245][227910] itr=1501, itrs=2000, Progress: 75.05%[0m
[36m[2023-07-10 22:26:12,758][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 22:26:12,758][227910] FPS: 334076.37[0m
[36m[2023-07-10 22:26:17,441][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:26:17,441][227910] Reward + Measures: [[2461.32728485    0.25100538    0.65544671    0.22754321    0.2820783 ]][0m
[37m[1m[2023-07-10 22:26:17,441][227910] Max Reward on eval: 2461.3272848537295[0m
[37m[1m[2023-07-10 22:26:17,441][227910] Min Reward on eval: 2461.3272848537295[0m
[37m[1m[2023-07-10 22:26:17,442][227910] Mean Reward across all agents: 2461.3272848537295[0m
[37m[1m[2023-07-10 22:26:17,442][227910] Average Trajectory Length: 999.0133333333333[0m
[36m[2023-07-10 22:26:22,842][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:26:22,848][227910] Reward + Measures: [[2101.76305331    0.26150003    0.56020004    0.1663        0.2563    ]
 [1986.25603752    0.28890002    0.66889995    0.2342        0.25      ]
 [1179.94557416    0.2507        0.58020002    0.2138        0.43660003]
 ...
 [2078.50895451    0.24650002    0.59950006    0.19359998    0.25009999]
 [1865.09977983    0.27449998    0.62409997    0.24879999    0.37189999]
 [1053.79549549    0.23480001    0.5966        0.1584        0.23530002]][0m
[37m[1m[2023-07-10 22:26:22,848][227910] Max Reward on eval: 2867.421862237295[0m
[37m[1m[2023-07-10 22:26:22,849][227910] Min Reward on eval: -192.06777073967677[0m
[37m[1m[2023-07-10 22:26:22,849][227910] Mean Reward across all agents: 1522.5433888555608[0m
[37m[1m[2023-07-10 22:26:22,849][227910] Average Trajectory Length: 998.4549999999999[0m
[36m[2023-07-10 22:26:22,852][227910] mean_value=-699.0018127462224, max_value=1832.3628273927343[0m
[37m[1m[2023-07-10 22:26:22,855][227910] New mean coefficients: [[ 0.514446    0.14012732 -0.12288761 -0.39143097 -0.04853787]][0m
[37m[1m[2023-07-10 22:26:22,856][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:26:32,540][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 22:26:32,540][227910] FPS: 396610.67[0m
[36m[2023-07-10 22:26:32,542][227910] itr=1502, itrs=2000, Progress: 75.10%[0m
[36m[2023-07-10 22:26:44,007][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 22:26:44,007][227910] FPS: 335488.52[0m
[36m[2023-07-10 22:26:48,782][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:26:48,783][227910] Reward + Measures: [[1967.4647992     0.22695354    0.5722084     0.15934046    0.2806524 ]][0m
[37m[1m[2023-07-10 22:26:48,783][227910] Max Reward on eval: 1967.464799203225[0m
[37m[1m[2023-07-10 22:26:48,783][227910] Min Reward on eval: 1967.464799203225[0m
[37m[1m[2023-07-10 22:26:48,783][227910] Mean Reward across all agents: 1967.464799203225[0m
[37m[1m[2023-07-10 22:26:48,783][227910] Average Trajectory Length: 997.7903333333333[0m
[36m[2023-07-10 22:26:54,000][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:26:54,001][227910] Reward + Measures: [[2242.63464915    0.2375        0.6045        0.17550001    0.27440003]
 [ 826.35036781    0.34880003    0.41279998    0.13160001    0.37200001]
 [ 730.06120437    0.35170001    0.70979995    0.52720004    0.68500006]
 ...
 [1366.42531442    0.28290001    0.66769999    0.36849999    0.51030004]
 [1474.29436122    0.23050001    0.5097        0.156         0.30030003]
 [1584.91448319    0.29809999    0.51800007    0.1786        0.32230002]][0m
[37m[1m[2023-07-10 22:26:54,001][227910] Max Reward on eval: 2352.3038760231225[0m
[37m[1m[2023-07-10 22:26:54,001][227910] Min Reward on eval: -301.64640528397865[0m
[37m[1m[2023-07-10 22:26:54,002][227910] Mean Reward across all agents: 1408.7608742755617[0m
[37m[1m[2023-07-10 22:26:54,002][227910] Average Trajectory Length: 998.2923333333333[0m
[36m[2023-07-10 22:26:54,006][227910] mean_value=-533.2368497760389, max_value=1046.947810323353[0m
[37m[1m[2023-07-10 22:26:54,008][227910] New mean coefficients: [[ 0.42504755  0.05119787 -0.01431449 -0.44291994  0.19410618]][0m
[37m[1m[2023-07-10 22:26:54,009][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:27:03,668][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 22:27:03,669][227910] FPS: 397623.80[0m
[36m[2023-07-10 22:27:03,671][227910] itr=1503, itrs=2000, Progress: 75.15%[0m
[36m[2023-07-10 22:27:15,123][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 22:27:15,123][227910] FPS: 335860.70[0m
[36m[2023-07-10 22:27:19,834][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:27:19,834][227910] Reward + Measures: [[2348.95926005    0.21316935    0.56889397    0.19492525    0.23983279]][0m
[37m[1m[2023-07-10 22:27:19,834][227910] Max Reward on eval: 2348.9592600522296[0m
[37m[1m[2023-07-10 22:27:19,835][227910] Min Reward on eval: 2348.9592600522296[0m
[37m[1m[2023-07-10 22:27:19,835][227910] Mean Reward across all agents: 2348.9592600522296[0m
[37m[1m[2023-07-10 22:27:19,835][227910] Average Trajectory Length: 998.7136666666667[0m
[36m[2023-07-10 22:27:25,474][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:27:25,475][227910] Reward + Measures: [[ 810.32688898    0.4456        0.56269997    0.4842        0.27350003]
 [ 538.70286013    0.37400004    0.43790004    0.3409        0.41809997]
 [2294.69440311    0.21970001    0.57100004    0.19330001    0.2483    ]
 ...
 [1451.13015415    0.25369999    0.54980004    0.23220001    0.36960003]
 [1496.02258474    0.26339999    0.55269998    0.18169999    0.26760003]
 [ 559.34306621    0.62000006    0.56470001    0.66570008    0.1999    ]][0m
[37m[1m[2023-07-10 22:27:25,475][227910] Max Reward on eval: 2760.4654793855734[0m
[37m[1m[2023-07-10 22:27:25,476][227910] Min Reward on eval: -61.92926764637232[0m
[37m[1m[2023-07-10 22:27:25,476][227910] Mean Reward across all agents: 1503.6161788994466[0m
[37m[1m[2023-07-10 22:27:25,476][227910] Average Trajectory Length: 998.1303333333333[0m
[36m[2023-07-10 22:27:25,478][227910] mean_value=-831.796321163501, max_value=769.1680617656087[0m
[37m[1m[2023-07-10 22:27:25,481][227910] New mean coefficients: [[ 0.070375    0.24779376 -0.34026057 -0.43005392  0.17994592]][0m
[37m[1m[2023-07-10 22:27:25,482][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:27:35,204][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 22:27:35,205][227910] FPS: 395028.77[0m
[36m[2023-07-10 22:27:35,207][227910] itr=1504, itrs=2000, Progress: 75.20%[0m
[36m[2023-07-10 22:27:46,798][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 22:27:46,799][227910] FPS: 331812.38[0m
[36m[2023-07-10 22:27:51,683][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:27:51,684][227910] Reward + Measures: [[2427.69015109    0.21152234    0.56930697    0.19527699    0.23749332]][0m
[37m[1m[2023-07-10 22:27:51,684][227910] Max Reward on eval: 2427.690151090355[0m
[37m[1m[2023-07-10 22:27:51,684][227910] Min Reward on eval: 2427.690151090355[0m
[37m[1m[2023-07-10 22:27:51,684][227910] Mean Reward across all agents: 2427.690151090355[0m
[37m[1m[2023-07-10 22:27:51,685][227910] Average Trajectory Length: 998.7423333333332[0m
[36m[2023-07-10 22:27:57,207][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:27:57,208][227910] Reward + Measures: [[2061.29773824    0.21599999    0.60570002    0.18429999    0.2816    ]
 [1942.10183493    0.21420002    0.50800002    0.2316        0.19420001]
 [2352.17568171    0.2182        0.58590001    0.2165        0.22760001]
 ...
 [2361.7917376     0.21339999    0.54420006    0.19570002    0.23029999]
 [2296.61680837    0.22270003    0.57970005    0.1971        0.22309999]
 [1901.15193996    0.25369999    0.54000002    0.15989999    0.3249    ]][0m
[37m[1m[2023-07-10 22:27:57,208][227910] Max Reward on eval: 2543.0695605332266[0m
[37m[1m[2023-07-10 22:27:57,209][227910] Min Reward on eval: -549.886088273325[0m
[37m[1m[2023-07-10 22:27:57,209][227910] Mean Reward across all agents: 1440.9947881404858[0m
[37m[1m[2023-07-10 22:27:57,209][227910] Average Trajectory Length: 995.7383333333333[0m
[36m[2023-07-10 22:27:57,211][227910] mean_value=-1197.0228141363825, max_value=873.5804796291369[0m
[37m[1m[2023-07-10 22:27:57,214][227910] New mean coefficients: [[-0.02808712  0.13318789 -0.60197794 -0.23263548  0.31281608]][0m
[37m[1m[2023-07-10 22:27:57,215][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:28:07,014][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 22:28:07,015][227910] FPS: 391911.78[0m
[36m[2023-07-10 22:28:07,017][227910] itr=1505, itrs=2000, Progress: 75.25%[0m
[36m[2023-07-10 22:28:18,525][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 22:28:18,525][227910] FPS: 334245.73[0m
[36m[2023-07-10 22:28:23,277][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:28:23,277][227910] Reward + Measures: [[2276.53675334    0.21772783    0.54313576    0.18629318    0.25367361]][0m
[37m[1m[2023-07-10 22:28:23,278][227910] Max Reward on eval: 2276.536753335273[0m
[37m[1m[2023-07-10 22:28:23,278][227910] Min Reward on eval: 2276.536753335273[0m
[37m[1m[2023-07-10 22:28:23,278][227910] Mean Reward across all agents: 2276.536753335273[0m
[37m[1m[2023-07-10 22:28:23,278][227910] Average Trajectory Length: 998.5989999999999[0m
[36m[2023-07-10 22:28:28,733][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:28:28,733][227910] Reward + Measures: [[1712.32975878    0.2667        0.5273        0.14890002    0.31420001]
 [1085.46009016    0.24890001    0.5848        0.23099999    0.2694    ]
 [ 706.03665938    0.40349999    0.35169998    0.51499999    0.4271    ]
 ...
 [1510.90314692    0.2967        0.46779999    0.27620003    0.27420002]
 [1583.78855447    0.22729997    0.5104        0.22790001    0.29359999]
 [1446.53107512    0.25370002    0.48649999    0.18110001    0.34260002]][0m
[37m[1m[2023-07-10 22:28:28,734][227910] Max Reward on eval: 2580.3378196290696[0m
[37m[1m[2023-07-10 22:28:28,734][227910] Min Reward on eval: -114.25526750241406[0m
[37m[1m[2023-07-10 22:28:28,734][227910] Mean Reward across all agents: 1338.3893241308006[0m
[37m[1m[2023-07-10 22:28:28,734][227910] Average Trajectory Length: 995.259[0m
[36m[2023-07-10 22:28:28,737][227910] mean_value=-906.719906938882, max_value=832.961528222319[0m
[37m[1m[2023-07-10 22:28:28,739][227910] New mean coefficients: [[-0.18129657  0.17322746 -0.60802156 -0.2964295   0.17327555]][0m
[37m[1m[2023-07-10 22:28:28,740][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:28:38,569][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 22:28:38,569][227910] FPS: 390763.79[0m
[36m[2023-07-10 22:28:38,571][227910] itr=1506, itrs=2000, Progress: 75.30%[0m
[36m[2023-07-10 22:28:50,111][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 22:28:50,111][227910] FPS: 333300.15[0m
[36m[2023-07-10 22:28:54,952][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:28:54,953][227910] Reward + Measures: [[2003.58516421    0.22841772    0.53854507    0.16514398    0.28080732]][0m
[37m[1m[2023-07-10 22:28:54,953][227910] Max Reward on eval: 2003.5851642148489[0m
[37m[1m[2023-07-10 22:28:54,953][227910] Min Reward on eval: 2003.5851642148489[0m
[37m[1m[2023-07-10 22:28:54,953][227910] Mean Reward across all agents: 2003.5851642148489[0m
[37m[1m[2023-07-10 22:28:54,954][227910] Average Trajectory Length: 995.935[0m
[36m[2023-07-10 22:29:00,416][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:29:00,417][227910] Reward + Measures: [[1351.21612801    0.1911        0.52240002    0.23580001    0.25940001]
 [1738.0832811     0.25439999    0.58530003    0.15800001    0.29159999]
 [ 286.10883347    0.32860002    0.4725        0.39860001    0.48600003]
 ...
 [ 766.35405079    0.2832        0.4061        0.1639        0.347     ]
 [1032.80749906    0.27130002    0.44140002    0.14390001    0.2687    ]
 [1528.62848088    0.2414        0.40880004    0.18130001    0.3073    ]][0m
[37m[1m[2023-07-10 22:29:00,417][227910] Max Reward on eval: 2162.943401404261[0m
[37m[1m[2023-07-10 22:29:00,417][227910] Min Reward on eval: -639.3203330292483[0m
[37m[1m[2023-07-10 22:29:00,418][227910] Mean Reward across all agents: 1158.3070838217548[0m
[37m[1m[2023-07-10 22:29:00,418][227910] Average Trajectory Length: 995.598[0m
[36m[2023-07-10 22:29:00,420][227910] mean_value=-800.5713176185736, max_value=1464.7265877932118[0m
[37m[1m[2023-07-10 22:29:00,423][227910] New mean coefficients: [[-0.41396418  0.4700152  -0.1332806  -0.1540399   0.3282218 ]][0m
[37m[1m[2023-07-10 22:29:00,424][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:29:10,166][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 22:29:10,166][227910] FPS: 394219.05[0m
[36m[2023-07-10 22:29:10,168][227910] itr=1507, itrs=2000, Progress: 75.35%[0m
[36m[2023-07-10 22:29:21,742][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 22:29:21,742][227910] FPS: 332325.59[0m
[36m[2023-07-10 22:29:26,599][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:29:26,599][227910] Reward + Measures: [[1597.02743396    0.25459242    0.53529197    0.13191356    0.33942792]][0m
[37m[1m[2023-07-10 22:29:26,600][227910] Max Reward on eval: 1597.0274339554608[0m
[37m[1m[2023-07-10 22:29:26,600][227910] Min Reward on eval: 1597.0274339554608[0m
[37m[1m[2023-07-10 22:29:26,600][227910] Mean Reward across all agents: 1597.0274339554608[0m
[37m[1m[2023-07-10 22:29:26,600][227910] Average Trajectory Length: 991.929[0m
[36m[2023-07-10 22:29:32,016][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:29:32,017][227910] Reward + Measures: [[2218.00489386    0.25640002    0.56490004    0.21180001    0.2313    ]
 [2201.35837053    0.27150002    0.52249998    0.19860001    0.25630003]
 [1791.75087934    0.24190001    0.46420002    0.20060001    0.2739    ]
 ...
 [ 665.81434206    0.41560003    0.61710006    0.44300005    0.55109996]
 [1901.12606484    0.2581        0.5449        0.1763        0.36569998]
 [ 947.66105091    0.24779999    0.44120002    0.19060001    0.41170001]][0m
[37m[1m[2023-07-10 22:29:32,017][227910] Max Reward on eval: 2314.054178156075[0m
[37m[1m[2023-07-10 22:29:32,018][227910] Min Reward on eval: -28.30160879617906[0m
[37m[1m[2023-07-10 22:29:32,018][227910] Mean Reward across all agents: 1286.3952721130581[0m
[37m[1m[2023-07-10 22:29:32,018][227910] Average Trajectory Length: 989.677[0m
[36m[2023-07-10 22:29:32,020][227910] mean_value=-596.5712513592672, max_value=1948.708025345765[0m
[37m[1m[2023-07-10 22:29:32,023][227910] New mean coefficients: [[ 0.02288616  0.37457913 -0.04308698 -0.2322743   0.28424332]][0m
[37m[1m[2023-07-10 22:29:32,024][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:29:41,805][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 22:29:41,806][227910] FPS: 392635.56[0m
[36m[2023-07-10 22:29:41,808][227910] itr=1508, itrs=2000, Progress: 75.40%[0m
[36m[2023-07-10 22:29:53,379][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 22:29:53,379][227910] FPS: 332505.41[0m
[36m[2023-07-10 22:29:58,078][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:29:58,078][227910] Reward + Measures: [[1370.26658842    0.27838969    0.53371102    0.11411799    0.38723648]][0m
[37m[1m[2023-07-10 22:29:58,078][227910] Max Reward on eval: 1370.2665884231108[0m
[37m[1m[2023-07-10 22:29:58,079][227910] Min Reward on eval: 1370.2665884231108[0m
[37m[1m[2023-07-10 22:29:58,079][227910] Mean Reward across all agents: 1370.2665884231108[0m
[37m[1m[2023-07-10 22:29:58,079][227910] Average Trajectory Length: 991.0683333333333[0m
[36m[2023-07-10 22:30:03,652][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:30:03,652][227910] Reward + Measures: [[1458.22741236    0.24130002    0.62480003    0.1514        0.331     ]
 [ 642.32232997    0.36069998    0.48379999    0.35980001    0.2915    ]
 [1278.04451257    0.24150001    0.53360003    0.14570001    0.38240001]
 ...
 [1018.76601505    0.30070001    0.45720002    0.1463        0.39430001]
 [ 970.57962363    0.27599999    0.53219998    0.13560002    0.57670003]
 [1677.94393072    0.3048        0.52350003    0.20460001    0.3416    ]][0m
[37m[1m[2023-07-10 22:30:03,652][227910] Max Reward on eval: 1826.361411045771[0m
[37m[1m[2023-07-10 22:30:03,653][227910] Min Reward on eval: 217.48383409614908[0m
[37m[1m[2023-07-10 22:30:03,653][227910] Mean Reward across all agents: 1034.7308720044466[0m
[37m[1m[2023-07-10 22:30:03,653][227910] Average Trajectory Length: 997.0103333333333[0m
[36m[2023-07-10 22:30:03,657][227910] mean_value=-255.78936790751328, max_value=1327.5676528656652[0m
[37m[1m[2023-07-10 22:30:03,660][227910] New mean coefficients: [[-0.04906414  0.3653028  -0.2198381  -0.30598786  0.3662888 ]][0m
[37m[1m[2023-07-10 22:30:03,661][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:30:13,259][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 22:30:13,259][227910] FPS: 400155.34[0m
[36m[2023-07-10 22:30:13,262][227910] itr=1509, itrs=2000, Progress: 75.45%[0m
[36m[2023-07-10 22:30:24,733][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 22:30:24,733][227910] FPS: 335287.59[0m
[36m[2023-07-10 22:30:29,564][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:30:29,564][227910] Reward + Measures: [[1107.24493371    0.31278014    0.51947194    0.09588632    0.47218859]][0m
[37m[1m[2023-07-10 22:30:29,564][227910] Max Reward on eval: 1107.2449337077926[0m
[37m[1m[2023-07-10 22:30:29,565][227910] Min Reward on eval: 1107.2449337077926[0m
[37m[1m[2023-07-10 22:30:29,565][227910] Mean Reward across all agents: 1107.2449337077926[0m
[37m[1m[2023-07-10 22:30:29,565][227910] Average Trajectory Length: 988.971[0m
[36m[2023-07-10 22:30:35,061][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:30:35,062][227910] Reward + Measures: [[ 949.44957003    0.2723        0.44759998    0.1279        0.61240005]
 [ 476.64977183    0.29169998    0.43290001    0.36390001    0.58599997]
 [ 736.03447287    0.41974425    0.48650166    0.08471804    0.42939836]
 ...
 [ 966.53107874    0.45520002    0.5212        0.1061        0.43770003]
 [1292.04876986    0.27540001    0.52720004    0.20349999    0.4298    ]
 [ 998.10525972    0.30899999    0.4862        0.115         0.53210002]][0m
[37m[1m[2023-07-10 22:30:35,062][227910] Max Reward on eval: 1746.8963952715742[0m
[37m[1m[2023-07-10 22:30:35,062][227910] Min Reward on eval: -120.85541065724101[0m
[37m[1m[2023-07-10 22:30:35,063][227910] Mean Reward across all agents: 819.5837408782847[0m
[37m[1m[2023-07-10 22:30:35,063][227910] Average Trajectory Length: 993.1706666666666[0m
[36m[2023-07-10 22:30:35,068][227910] mean_value=49.05003002132585, max_value=1681.61004553081[0m
[37m[1m[2023-07-10 22:30:35,071][227910] New mean coefficients: [[ 0.23269236  0.27446944 -0.20980158 -0.4005661   0.28019956]][0m
[37m[1m[2023-07-10 22:30:35,072][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:30:44,769][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 22:30:44,769][227910] FPS: 396066.85[0m
[36m[2023-07-10 22:30:44,772][227910] itr=1510, itrs=2000, Progress: 75.50%[0m
[37m[1m[2023-07-10 22:30:48,695][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001490[0m
[36m[2023-07-10 22:31:00,506][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 22:31:00,506][227910] FPS: 332891.65[0m
[36m[2023-07-10 22:31:05,203][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:31:05,203][227910] Reward + Measures: [[1313.56741864    0.29798591    0.52937257    0.10038783    0.39799601]][0m
[37m[1m[2023-07-10 22:31:05,203][227910] Max Reward on eval: 1313.5674186399176[0m
[37m[1m[2023-07-10 22:31:05,204][227910] Min Reward on eval: 1313.5674186399176[0m
[37m[1m[2023-07-10 22:31:05,204][227910] Mean Reward across all agents: 1313.5674186399176[0m
[37m[1m[2023-07-10 22:31:05,204][227910] Average Trajectory Length: 981.3303333333333[0m
[36m[2023-07-10 22:31:10,733][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:31:10,734][227910] Reward + Measures: [[ 873.40408581    0.3495        0.52579999    0.0466        0.51120007]
 [ 842.01866982    0.33150002    0.5262        0.23400001    0.43499994]
 [ 716.19792632    0.32940003    0.4316        0.34900001    0.52530003]
 ...
 [1020.84538658    0.2309        0.43660003    0.2448        0.4039    ]
 [1226.98958954    0.30690002    0.49659997    0.1339        0.37240002]
 [1147.48485085    0.30440003    0.51330006    0.0929        0.41930005]][0m
[37m[1m[2023-07-10 22:31:10,734][227910] Max Reward on eval: 2198.3687914096517[0m
[37m[1m[2023-07-10 22:31:10,734][227910] Min Reward on eval: -345.2853862422984[0m
[37m[1m[2023-07-10 22:31:10,734][227910] Mean Reward across all agents: 991.0227939579315[0m
[37m[1m[2023-07-10 22:31:10,734][227910] Average Trajectory Length: 992.6873333333333[0m
[36m[2023-07-10 22:31:10,738][227910] mean_value=-376.23711274843674, max_value=1356.7639797134743[0m
[37m[1m[2023-07-10 22:31:10,741][227910] New mean coefficients: [[ 0.5325538   0.13349432 -0.13340218 -0.4341164   0.33644462]][0m
[37m[1m[2023-07-10 22:31:10,742][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:31:20,566][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 22:31:20,566][227910] FPS: 390954.62[0m
[36m[2023-07-10 22:31:20,568][227910] itr=1511, itrs=2000, Progress: 75.55%[0m
[36m[2023-07-10 22:31:32,168][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 22:31:32,168][227910] FPS: 331590.33[0m
[36m[2023-07-10 22:31:36,991][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:31:36,991][227910] Reward + Measures: [[1511.53741626    0.27983224    0.53406453    0.11290468    0.3651118 ]][0m
[37m[1m[2023-07-10 22:31:36,991][227910] Max Reward on eval: 1511.537416264279[0m
[37m[1m[2023-07-10 22:31:36,992][227910] Min Reward on eval: 1511.537416264279[0m
[37m[1m[2023-07-10 22:31:36,992][227910] Mean Reward across all agents: 1511.537416264279[0m
[37m[1m[2023-07-10 22:31:36,992][227910] Average Trajectory Length: 981.1206666666666[0m
[36m[2023-07-10 22:31:42,505][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:31:42,506][227910] Reward + Measures: [[1419.83419554    0.27729997    0.51200002    0.134         0.347     ]
 [ 911.9489801     0.30089998    0.59390002    0.1037        0.51410002]
 [ 832.41922132    0.26229998    0.54290003    0.11260001    0.4975    ]
 ...
 [1350.0102886     0.29450002    0.5535        0.1024        0.4289    ]
 [ 850.55380152    0.22190002    0.4506        0.134         0.33180001]
 [1132.03815206    0.29200003    0.38910002    0.2023        0.31440002]][0m
[37m[1m[2023-07-10 22:31:42,506][227910] Max Reward on eval: 1802.931569392979[0m
[37m[1m[2023-07-10 22:31:42,507][227910] Min Reward on eval: 426.90833639620104[0m
[37m[1m[2023-07-10 22:31:42,507][227910] Mean Reward across all agents: 1107.347113250137[0m
[37m[1m[2023-07-10 22:31:42,507][227910] Average Trajectory Length: 995.7686666666666[0m
[36m[2023-07-10 22:31:42,509][227910] mean_value=-374.9679678630337, max_value=1474.182830831526[0m
[37m[1m[2023-07-10 22:31:42,512][227910] New mean coefficients: [[ 1.037096    0.14961463 -0.12282354 -0.4796915   0.38191473]][0m
[37m[1m[2023-07-10 22:31:42,513][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:31:52,204][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 22:31:52,204][227910] FPS: 396289.90[0m
[36m[2023-07-10 22:31:52,207][227910] itr=1512, itrs=2000, Progress: 75.60%[0m
[36m[2023-07-10 22:32:03,826][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 22:32:03,826][227910] FPS: 331131.41[0m
[36m[2023-07-10 22:32:08,708][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:32:08,708][227910] Reward + Measures: [[1770.27305192    0.2612789     0.53717381    0.13438599    0.32521996]][0m
[37m[1m[2023-07-10 22:32:08,708][227910] Max Reward on eval: 1770.2730519158654[0m
[37m[1m[2023-07-10 22:32:08,709][227910] Min Reward on eval: 1770.2730519158654[0m
[37m[1m[2023-07-10 22:32:08,709][227910] Mean Reward across all agents: 1770.2730519158654[0m
[37m[1m[2023-07-10 22:32:08,709][227910] Average Trajectory Length: 985.351[0m
[36m[2023-07-10 22:32:14,308][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:32:14,308][227910] Reward + Measures: [[1221.07275296    0.3389        0.58890003    0.2218        0.4382    ]
 [ 791.06078797    0.3339        0.64429998    0.1955        0.39439997]
 [ 987.42278798    0.27710003    0.52020001    0.33630002    0.40410003]
 ...
 [ 990.43817925    0.36240003    0.55110002    0.24510002    0.38770002]
 [1557.61136632    0.30289999    0.50839996    0.1096        0.3348    ]
 [1490.82727693    0.2913        0.46150002    0.13020001    0.3466    ]][0m
[37m[1m[2023-07-10 22:32:14,308][227910] Max Reward on eval: 1972.0681748270756[0m
[37m[1m[2023-07-10 22:32:14,309][227910] Min Reward on eval: 191.6068694710557[0m
[37m[1m[2023-07-10 22:32:14,309][227910] Mean Reward across all agents: 1324.9166363339764[0m
[37m[1m[2023-07-10 22:32:14,309][227910] Average Trajectory Length: 990.1923333333333[0m
[36m[2023-07-10 22:32:14,312][227910] mean_value=-396.71514599186565, max_value=1251.742600393778[0m
[37m[1m[2023-07-10 22:32:14,314][227910] New mean coefficients: [[ 1.1670085   0.01467563  0.09426928 -0.63200563  0.18732059]][0m
[37m[1m[2023-07-10 22:32:14,315][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:32:24,259][227910] train() took 9.94 seconds to complete[0m
[36m[2023-07-10 22:32:24,259][227910] FPS: 386254.36[0m
[36m[2023-07-10 22:32:24,261][227910] itr=1513, itrs=2000, Progress: 75.65%[0m
[36m[2023-07-10 22:32:35,785][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 22:32:35,785][227910] FPS: 333883.31[0m
[36m[2023-07-10 22:32:40,586][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:32:40,587][227910] Reward + Measures: [[2138.74007247    0.23962195    0.55043888    0.16463423    0.28151947]][0m
[37m[1m[2023-07-10 22:32:40,587][227910] Max Reward on eval: 2138.7400724722957[0m
[37m[1m[2023-07-10 22:32:40,587][227910] Min Reward on eval: 2138.7400724722957[0m
[37m[1m[2023-07-10 22:32:40,587][227910] Mean Reward across all agents: 2138.7400724722957[0m
[37m[1m[2023-07-10 22:32:40,588][227910] Average Trajectory Length: 993.5513333333333[0m
[36m[2023-07-10 22:32:46,018][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:32:46,019][227910] Reward + Measures: [[1898.98344509    0.22579999    0.52960002    0.16660002    0.30599999]
 [1964.19162354    0.24750002    0.54049999    0.1497        0.27980003]
 [2170.66951868    0.22500001    0.50620002    0.1786        0.24389999]
 ...
 [1558.50452457    0.26120001    0.55489999    0.13399999    0.39030001]
 [2004.44314262    0.24730001    0.54030001    0.18080001    0.26159999]
 [ 731.68993097    0.34979999    0.59549999    0.4571        0.53439999]][0m
[37m[1m[2023-07-10 22:32:46,019][227910] Max Reward on eval: 2308.478366319393[0m
[37m[1m[2023-07-10 22:32:46,019][227910] Min Reward on eval: 132.09108691763248[0m
[37m[1m[2023-07-10 22:32:46,020][227910] Mean Reward across all agents: 1580.7216796495645[0m
[37m[1m[2023-07-10 22:32:46,020][227910] Average Trajectory Length: 994.526[0m
[36m[2023-07-10 22:32:46,023][227910] mean_value=-418.68883081523364, max_value=1111.9686594708737[0m
[37m[1m[2023-07-10 22:32:46,025][227910] New mean coefficients: [[ 1.2268441  -0.15407434  0.43664333 -0.9339763   0.24017552]][0m
[37m[1m[2023-07-10 22:32:46,027][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:32:55,805][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 22:32:55,805][227910] FPS: 392774.65[0m
[36m[2023-07-10 22:32:55,808][227910] itr=1514, itrs=2000, Progress: 75.70%[0m
[36m[2023-07-10 22:33:07,521][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 22:33:07,521][227910] FPS: 328382.87[0m
[36m[2023-07-10 22:33:12,223][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:33:12,224][227910] Reward + Measures: [[2454.81444956    0.22443475    0.55428559    0.18555009    0.2454768 ]][0m
[37m[1m[2023-07-10 22:33:12,224][227910] Max Reward on eval: 2454.814449564124[0m
[37m[1m[2023-07-10 22:33:12,224][227910] Min Reward on eval: 2454.814449564124[0m
[37m[1m[2023-07-10 22:33:12,224][227910] Mean Reward across all agents: 2454.814449564124[0m
[37m[1m[2023-07-10 22:33:12,225][227910] Average Trajectory Length: 996.8973333333333[0m
[36m[2023-07-10 22:33:17,671][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:33:17,672][227910] Reward + Measures: [[1016.54640141    0.33950001    0.57880002    0.2184        0.44630003]
 [2530.02817161    0.23169999    0.5844        0.2057        0.22649999]
 [2182.59934737    0.24249999    0.55340004    0.24700001    0.19750001]
 ...
 [ 280.24907613    0.58920002    0.77030003    0.1547        0.76820004]
 [2150.69537645    0.22390001    0.53960007    0.14680001    0.24360001]
 [1250.5728744     0.28150001    0.46510002    0.2069        0.29899999]][0m
[37m[1m[2023-07-10 22:33:17,672][227910] Max Reward on eval: 2619.667630324443[0m
[37m[1m[2023-07-10 22:33:17,672][227910] Min Reward on eval: 142.33122780476697[0m
[37m[1m[2023-07-10 22:33:17,673][227910] Mean Reward across all agents: 1716.0595325662475[0m
[37m[1m[2023-07-10 22:33:17,673][227910] Average Trajectory Length: 998.0683333333333[0m
[36m[2023-07-10 22:33:17,675][227910] mean_value=-976.7664189695511, max_value=1137.7606224481624[0m
[37m[1m[2023-07-10 22:33:17,677][227910] New mean coefficients: [[ 0.92831826 -0.00170414  0.33413067 -0.6278814   0.33294868]][0m
[37m[1m[2023-07-10 22:33:17,678][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:33:27,330][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 22:33:27,331][227910] FPS: 397904.95[0m
[36m[2023-07-10 22:33:27,333][227910] itr=1515, itrs=2000, Progress: 75.75%[0m
[36m[2023-07-10 22:33:38,817][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 22:33:38,818][227910] FPS: 334910.32[0m
[36m[2023-07-10 22:33:43,493][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:33:43,493][227910] Reward + Measures: [[2733.17143938    0.21335375    0.55902255    0.20446749    0.21894978]][0m
[37m[1m[2023-07-10 22:33:43,493][227910] Max Reward on eval: 2733.171439380536[0m
[37m[1m[2023-07-10 22:33:43,494][227910] Min Reward on eval: 2733.171439380536[0m
[37m[1m[2023-07-10 22:33:43,494][227910] Mean Reward across all agents: 2733.171439380536[0m
[37m[1m[2023-07-10 22:33:43,494][227910] Average Trajectory Length: 998.458[0m
[36m[2023-07-10 22:33:49,181][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:33:49,186][227910] Reward + Measures: [[1585.39625537    0.22760001    0.49070001    0.28410003    0.40150005]
 [1867.01405983    0.206         0.58520001    0.27379999    0.39480004]
 [1523.55408469    0.30400002    0.49219999    0.3055        0.36110002]
 ...
 [2158.74229013    0.21769999    0.53560001    0.19760001    0.26809999]
 [1633.77689267    0.23445283    0.43402758    0.17345627    0.26942724]
 [1116.2196814     0.22503464    0.38905594    0.15785985    0.25503466]][0m
[37m[1m[2023-07-10 22:33:49,186][227910] Max Reward on eval: 2979.2942824734373[0m
[37m[1m[2023-07-10 22:33:49,187][227910] Min Reward on eval: 271.7061889566132[0m
[37m[1m[2023-07-10 22:33:49,187][227910] Mean Reward across all agents: 1890.3852549588657[0m
[37m[1m[2023-07-10 22:33:49,187][227910] Average Trajectory Length: 985.0413333333333[0m
[36m[2023-07-10 22:33:49,189][227910] mean_value=-1342.705950902425, max_value=507.90343732025804[0m
[37m[1m[2023-07-10 22:33:49,192][227910] New mean coefficients: [[ 0.9145922  -0.12794723  0.5101334  -0.6613973   0.06493506]][0m
[37m[1m[2023-07-10 22:33:49,193][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:33:58,885][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 22:33:58,886][227910] FPS: 396249.01[0m
[36m[2023-07-10 22:33:58,888][227910] itr=1516, itrs=2000, Progress: 75.80%[0m
[36m[2023-07-10 22:34:10,351][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 22:34:10,352][227910] FPS: 335516.28[0m
[36m[2023-07-10 22:34:15,100][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:34:15,100][227910] Reward + Measures: [[2975.23387369    0.20440859    0.57055175    0.21897498    0.20167626]][0m
[37m[1m[2023-07-10 22:34:15,101][227910] Max Reward on eval: 2975.233873685278[0m
[37m[1m[2023-07-10 22:34:15,101][227910] Min Reward on eval: 2975.233873685278[0m
[37m[1m[2023-07-10 22:34:15,101][227910] Mean Reward across all agents: 2975.233873685278[0m
[37m[1m[2023-07-10 22:34:15,101][227910] Average Trajectory Length: 998.8846666666666[0m
[36m[2023-07-10 22:34:20,596][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:34:20,601][227910] Reward + Measures: [[1486.85736479    0.2366        0.53660005    0.15360001    0.33129999]
 [ 801.66601484    0.15448418    0.61624467    0.15310057    0.44968644]
 [ 539.24589963    0.24890251    0.56468648    0.32067338    0.29419324]
 ...
 [ 492.76143819    0.1496        0.49430004    0.1716        0.31990001]
 [1925.19674901    0.2472        0.53689998    0.1559        0.35870001]
 [1917.38026666    0.2045        0.5266        0.22819999    0.26589999]][0m
[37m[1m[2023-07-10 22:34:20,601][227910] Max Reward on eval: 3029.576188589656[0m
[37m[1m[2023-07-10 22:34:20,602][227910] Min Reward on eval: -167.95609477491817[0m
[37m[1m[2023-07-10 22:34:20,602][227910] Mean Reward across all agents: 1524.9785180228635[0m
[37m[1m[2023-07-10 22:34:20,602][227910] Average Trajectory Length: 995.8736666666666[0m
[36m[2023-07-10 22:34:20,605][227910] mean_value=-497.07933494891466, max_value=1297.101660911991[0m
[37m[1m[2023-07-10 22:34:20,608][227910] New mean coefficients: [[ 0.76698375 -0.2530914   0.50804037 -0.8831905  -0.15356173]][0m
[37m[1m[2023-07-10 22:34:20,609][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:34:30,419][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 22:34:30,420][227910] FPS: 391491.83[0m
[36m[2023-07-10 22:34:30,422][227910] itr=1517, itrs=2000, Progress: 75.85%[0m
[36m[2023-07-10 22:34:41,920][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 22:34:41,920][227910] FPS: 334613.24[0m
[36m[2023-07-10 22:34:46,632][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:34:46,633][227910] Reward + Measures: [[3100.49218215    0.20304814    0.58473217    0.22228861    0.19683915]][0m
[37m[1m[2023-07-10 22:34:46,633][227910] Max Reward on eval: 3100.4921821501293[0m
[37m[1m[2023-07-10 22:34:46,633][227910] Min Reward on eval: 3100.4921821501293[0m
[37m[1m[2023-07-10 22:34:46,633][227910] Mean Reward across all agents: 3100.4921821501293[0m
[37m[1m[2023-07-10 22:34:46,634][227910] Average Trajectory Length: 998.583[0m
[36m[2023-07-10 22:34:52,075][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:34:52,075][227910] Reward + Measures: [[-132.5826378     0.26045999    0.48034       0.23374       0.24907999]
 [1524.60932412    0.259         0.58929998    0.15629999    0.24419999]
 [1796.36506818    0.24159999    0.67839998    0.1673        0.23670001]
 ...
 [1496.1774779     0.2739        0.53570002    0.2379        0.29809999]
 [1948.6708933     0.23269999    0.58249998    0.1926        0.27880001]
 [1901.56990846    0.23269999    0.68050003    0.18670002    0.234     ]][0m
[37m[1m[2023-07-10 22:34:52,076][227910] Max Reward on eval: 3090.0327113987178[0m
[37m[1m[2023-07-10 22:34:52,076][227910] Min Reward on eval: -736.6700289514381[0m
[37m[1m[2023-07-10 22:34:52,076][227910] Mean Reward across all agents: 1614.6234555494998[0m
[37m[1m[2023-07-10 22:34:52,076][227910] Average Trajectory Length: 990.809[0m
[36m[2023-07-10 22:34:52,078][227910] mean_value=-1133.414733469127, max_value=1546.9637920286418[0m
[37m[1m[2023-07-10 22:34:52,081][227910] New mean coefficients: [[ 0.95681536 -0.0966574   0.35964885 -0.81759906 -0.01209795]][0m
[37m[1m[2023-07-10 22:34:52,082][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:35:01,842][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 22:35:01,843][227910] FPS: 393475.51[0m
[36m[2023-07-10 22:35:01,845][227910] itr=1518, itrs=2000, Progress: 75.90%[0m
[36m[2023-07-10 22:35:13,402][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 22:35:13,402][227910] FPS: 332808.78[0m
[36m[2023-07-10 22:35:18,095][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:35:18,095][227910] Reward + Measures: [[3242.1430227     0.20003322    0.58632386    0.23053604    0.19175114]][0m
[37m[1m[2023-07-10 22:35:18,096][227910] Max Reward on eval: 3242.143022702704[0m
[37m[1m[2023-07-10 22:35:18,096][227910] Min Reward on eval: 3242.143022702704[0m
[37m[1m[2023-07-10 22:35:18,096][227910] Mean Reward across all agents: 3242.143022702704[0m
[37m[1m[2023-07-10 22:35:18,096][227910] Average Trajectory Length: 999.9723333333333[0m
[36m[2023-07-10 22:35:23,669][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:35:23,669][227910] Reward + Measures: [[2385.00528037    0.27200004    0.55549997    0.22019999    0.25340003]
 [1475.41636842    0.31311449    0.46718168    0.16392061    0.33323815]
 [1692.68655555    0.3224        0.49170002    0.24530001    0.2834    ]
 ...
 [2468.94570952    0.22810002    0.52890003    0.2067        0.21890001]
 [1616.14973576    0.27620003    0.50009996    0.15310001    0.40619999]
 [2163.85488198    0.27580002    0.49540001    0.20089999    0.25439999]][0m
[37m[1m[2023-07-10 22:35:23,669][227910] Max Reward on eval: 3161.9774573321456[0m
[37m[1m[2023-07-10 22:35:23,670][227910] Min Reward on eval: 579.1893086204539[0m
[37m[1m[2023-07-10 22:35:23,670][227910] Mean Reward across all agents: 2007.908475753603[0m
[37m[1m[2023-07-10 22:35:23,670][227910] Average Trajectory Length: 974.3006666666666[0m
[36m[2023-07-10 22:35:23,672][227910] mean_value=-1485.8694798743325, max_value=1129.8545047638952[0m
[37m[1m[2023-07-10 22:35:23,675][227910] New mean coefficients: [[ 0.63407755 -0.01811656 -0.3232409  -0.7699497  -0.09618556]][0m
[37m[1m[2023-07-10 22:35:23,676][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:35:33,514][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 22:35:33,515][227910] FPS: 390353.63[0m
[36m[2023-07-10 22:35:33,517][227910] itr=1519, itrs=2000, Progress: 75.95%[0m
[36m[2023-07-10 22:35:45,116][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 22:35:45,116][227910] FPS: 331604.88[0m
[36m[2023-07-10 22:35:49,835][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:35:49,841][227910] Reward + Measures: [[3352.68904459    0.19916084    0.5891977     0.23357114    0.18574986]][0m
[37m[1m[2023-07-10 22:35:49,842][227910] Max Reward on eval: 3352.68904459487[0m
[37m[1m[2023-07-10 22:35:49,842][227910] Min Reward on eval: 3352.68904459487[0m
[37m[1m[2023-07-10 22:35:49,843][227910] Mean Reward across all agents: 3352.68904459487[0m
[37m[1m[2023-07-10 22:35:49,843][227910] Average Trajectory Length: 999.9643333333333[0m
[36m[2023-07-10 22:35:55,611][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:35:55,612][227910] Reward + Measures: [[1520.57205047    0.27270001    0.58020008    0.24949999    0.3574    ]
 [1500.19453412    0.25670001    0.45140001    0.24609999    0.2552    ]
 [1638.22861689    0.21329999    0.50770003    0.20050001    0.25580001]
 ...
 [3013.07141232    0.23749998    0.62670004    0.24790001    0.20999999]
 [1615.0963217     0.28075382    0.55611867    0.22970584    0.32779533]
 [1836.55020351    0.2667        0.55980003    0.21399999    0.29609999]][0m
[37m[1m[2023-07-10 22:35:55,612][227910] Max Reward on eval: 3375.923272285494[0m
[37m[1m[2023-07-10 22:35:55,612][227910] Min Reward on eval: 281.7418741546513[0m
[37m[1m[2023-07-10 22:35:55,612][227910] Mean Reward across all agents: 2047.7304167812526[0m
[37m[1m[2023-07-10 22:35:55,613][227910] Average Trajectory Length: 994.91[0m
[36m[2023-07-10 22:35:55,614][227910] mean_value=-893.0301441847248, max_value=280.58808271330327[0m
[37m[1m[2023-07-10 22:35:55,617][227910] New mean coefficients: [[ 0.08684039  0.19846946 -0.19071037 -0.70997155 -0.00762501]][0m
[37m[1m[2023-07-10 22:35:55,618][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:36:05,438][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 22:36:05,438][227910] FPS: 391094.20[0m
[36m[2023-07-10 22:36:05,441][227910] itr=1520, itrs=2000, Progress: 76.00%[0m
[37m[1m[2023-07-10 22:36:09,428][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001500[0m
[36m[2023-07-10 22:36:21,334][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 22:36:21,335][227910] FPS: 330390.48[0m
[36m[2023-07-10 22:36:26,035][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:36:26,036][227910] Reward + Measures: [[3376.73663452    0.20005068    0.58907682    0.23040594    0.1860009 ]][0m
[37m[1m[2023-07-10 22:36:26,036][227910] Max Reward on eval: 3376.736634518154[0m
[37m[1m[2023-07-10 22:36:26,036][227910] Min Reward on eval: 3376.736634518154[0m
[37m[1m[2023-07-10 22:36:26,036][227910] Mean Reward across all agents: 3376.736634518154[0m
[37m[1m[2023-07-10 22:36:26,037][227910] Average Trajectory Length: 998.837[0m
[36m[2023-07-10 22:36:31,558][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:36:31,559][227910] Reward + Measures: [[1789.49185395    0.2836        0.51170003    0.2421        0.23559999]
 [2282.96710129    0.25770003    0.53640002    0.2175        0.22650002]
 [1409.78801281    0.292         0.51780003    0.23740001    0.29099998]
 ...
 [ 602.62504235    0.23339999    0.56339997    0.3777        0.3942    ]
 [2528.41107756    0.23580001    0.5905        0.17410001    0.24129999]
 [2280.16105507    0.22090001    0.59580004    0.2119        0.25120002]][0m
[37m[1m[2023-07-10 22:36:31,559][227910] Max Reward on eval: 3272.992077226215[0m
[37m[1m[2023-07-10 22:36:31,560][227910] Min Reward on eval: 364.2202704277588[0m
[37m[1m[2023-07-10 22:36:31,560][227910] Mean Reward across all agents: 1993.7127893946024[0m
[37m[1m[2023-07-10 22:36:31,560][227910] Average Trajectory Length: 997.4056666666667[0m
[36m[2023-07-10 22:36:31,563][227910] mean_value=-862.2172112601432, max_value=835.5678876760326[0m
[37m[1m[2023-07-10 22:36:31,565][227910] New mean coefficients: [[ 0.19915468 -0.05248427  0.18150431 -0.8344084   0.07073114]][0m
[37m[1m[2023-07-10 22:36:31,566][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:36:41,279][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 22:36:41,279][227910] FPS: 395421.85[0m
[36m[2023-07-10 22:36:41,281][227910] itr=1521, itrs=2000, Progress: 76.05%[0m
[36m[2023-07-10 22:36:52,826][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 22:36:52,826][227910] FPS: 333203.75[0m
[36m[2023-07-10 22:36:57,624][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:36:57,624][227910] Reward + Measures: [[3454.83415248    0.19971864    0.59782839    0.22403377    0.18451114]][0m
[37m[1m[2023-07-10 22:36:57,624][227910] Max Reward on eval: 3454.834152479806[0m
[37m[1m[2023-07-10 22:36:57,625][227910] Min Reward on eval: 3454.834152479806[0m
[37m[1m[2023-07-10 22:36:57,625][227910] Mean Reward across all agents: 3454.834152479806[0m
[37m[1m[2023-07-10 22:36:57,625][227910] Average Trajectory Length: 999.4093333333333[0m
[36m[2023-07-10 22:37:03,062][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:37:03,062][227910] Reward + Measures: [[2293.54036233    0.25959998    0.60429996    0.1892        0.2014    ]
 [2515.53574199    0.25130001    0.64890003    0.19500001    0.2185    ]
 [1672.60523899    0.37090001    0.49359998    0.34459999    0.24700001]
 ...
 [2723.90413014    0.1851        0.59350002    0.1811        0.29440004]
 [1538.85044346    0.2288876     0.57624334    0.16934779    0.21097079]
 [2285.96996884    0.23840001    0.5873        0.17569999    0.191     ]][0m
[37m[1m[2023-07-10 22:37:03,062][227910] Max Reward on eval: 3514.1728800106794[0m
[37m[1m[2023-07-10 22:37:03,063][227910] Min Reward on eval: 329.4132244111912[0m
[37m[1m[2023-07-10 22:37:03,063][227910] Mean Reward across all agents: 1920.8986673143336[0m
[37m[1m[2023-07-10 22:37:03,063][227910] Average Trajectory Length: 995.0076666666666[0m
[36m[2023-07-10 22:37:03,066][227910] mean_value=-667.499609513474, max_value=2280.8138492706803[0m
[37m[1m[2023-07-10 22:37:03,069][227910] New mean coefficients: [[ 0.3861351  -0.00848545  0.39005542 -0.81303996  0.41542685]][0m
[37m[1m[2023-07-10 22:37:03,070][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:37:12,928][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 22:37:12,928][227910] FPS: 389597.70[0m
[36m[2023-07-10 22:37:12,931][227910] itr=1522, itrs=2000, Progress: 76.10%[0m
[36m[2023-07-10 22:37:24,486][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 22:37:24,487][227910] FPS: 332891.84[0m
[36m[2023-07-10 22:37:29,215][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:37:29,215][227910] Reward + Measures: [[1291.70384282    0.24708693    0.48043379    0.15189609    0.34019476]][0m
[37m[1m[2023-07-10 22:37:29,216][227910] Max Reward on eval: 1291.7038428203011[0m
[37m[1m[2023-07-10 22:37:29,216][227910] Min Reward on eval: 1291.7038428203011[0m
[37m[1m[2023-07-10 22:37:29,216][227910] Mean Reward across all agents: 1291.7038428203011[0m
[37m[1m[2023-07-10 22:37:29,216][227910] Average Trajectory Length: 995.6536666666666[0m
[36m[2023-07-10 22:37:34,726][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:37:34,727][227910] Reward + Measures: [[1449.31421412    0.2841        0.52959996    0.20639999    0.23819999]
 [ 922.81698394    0.25670001    0.45310003    0.2362        0.4032    ]
 [ 953.39791644    0.25250003    0.4585        0.23640001    0.3628    ]
 ...
 [ 964.41609624    0.37450001    0.56100005    0.20369999    0.42649999]
 [ 256.25027877    0.24562339    0.31016597    0.14131916    0.33461487]
 [ 578.00255763    0.25600001    0.42910001    0.17620002    0.37939999]][0m
[37m[1m[2023-07-10 22:37:34,727][227910] Max Reward on eval: 1679.575231467164[0m
[37m[1m[2023-07-10 22:37:34,727][227910] Min Reward on eval: 212.63775107966794[0m
[37m[1m[2023-07-10 22:37:34,727][227910] Mean Reward across all agents: 965.9573146843328[0m
[37m[1m[2023-07-10 22:37:34,728][227910] Average Trajectory Length: 997.4876666666667[0m
[36m[2023-07-10 22:37:34,729][227910] mean_value=-1016.8976576751849, max_value=677.5925195343978[0m
[37m[1m[2023-07-10 22:37:34,732][227910] New mean coefficients: [[ 0.19413747  0.1541367   0.29409963 -0.62453693  0.37465063]][0m
[37m[1m[2023-07-10 22:37:34,733][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:37:44,583][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 22:37:44,583][227910] FPS: 389913.00[0m
[36m[2023-07-10 22:37:44,585][227910] itr=1523, itrs=2000, Progress: 76.15%[0m
[36m[2023-07-10 22:37:56,228][227910] train() took 11.62 seconds to complete[0m
[36m[2023-07-10 22:37:56,228][227910] FPS: 330368.07[0m
[36m[2023-07-10 22:38:01,025][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:38:01,025][227910] Reward + Measures: [[1479.9537324     0.23462494    0.50773191    0.14609995    0.31143028]][0m
[37m[1m[2023-07-10 22:38:01,025][227910] Max Reward on eval: 1479.9537324028984[0m
[37m[1m[2023-07-10 22:38:01,026][227910] Min Reward on eval: 1479.9537324028984[0m
[37m[1m[2023-07-10 22:38:01,026][227910] Mean Reward across all agents: 1479.9537324028984[0m
[37m[1m[2023-07-10 22:38:01,026][227910] Average Trajectory Length: 994.0753333333333[0m
[36m[2023-07-10 22:38:06,496][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:38:06,496][227910] Reward + Measures: [[ 902.95127258    0.2454        0.4729        0.12400001    0.42820001]
 [ 613.86605398    0.38480002    0.49460003    0.3414        0.4779    ]
 [ 798.74274878    0.25409999    0.41799998    0.09980001    0.36330003]
 ...
 [1416.64956903    0.2375        0.491         0.14560001    0.32459998]
 [1074.23595511    0.2757        0.48909998    0.20729999    0.36160001]
 [ 303.38283325    0.49470001    0.72490007    0.64439994    0.67549998]][0m
[37m[1m[2023-07-10 22:38:06,497][227910] Max Reward on eval: 1815.2184839712922[0m
[37m[1m[2023-07-10 22:38:06,497][227910] Min Reward on eval: 87.24910418236978[0m
[37m[1m[2023-07-10 22:38:06,497][227910] Mean Reward across all agents: 840.182679653154[0m
[37m[1m[2023-07-10 22:38:06,497][227910] Average Trajectory Length: 994.4726666666667[0m
[36m[2023-07-10 22:38:06,499][227910] mean_value=-693.1646028423972, max_value=1559.2534529005177[0m
[37m[1m[2023-07-10 22:38:06,502][227910] New mean coefficients: [[ 0.7513173   0.16247752  0.27809823 -0.6695334   0.21829689]][0m
[37m[1m[2023-07-10 22:38:06,503][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:38:16,227][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 22:38:16,227][227910] FPS: 394969.24[0m
[36m[2023-07-10 22:38:16,230][227910] itr=1524, itrs=2000, Progress: 76.20%[0m
[36m[2023-07-10 22:38:27,738][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 22:38:27,738][227910] FPS: 334207.00[0m
[36m[2023-07-10 22:38:32,541][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:38:32,541][227910] Reward + Measures: [[1740.11854648    0.21793021    0.50323594    0.16307043    0.27270153]][0m
[37m[1m[2023-07-10 22:38:32,541][227910] Max Reward on eval: 1740.1185464846744[0m
[37m[1m[2023-07-10 22:38:32,542][227910] Min Reward on eval: 1740.1185464846744[0m
[37m[1m[2023-07-10 22:38:32,542][227910] Mean Reward across all agents: 1740.1185464846744[0m
[37m[1m[2023-07-10 22:38:32,542][227910] Average Trajectory Length: 992.9553333333333[0m
[36m[2023-07-10 22:38:38,040][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:38:38,041][227910] Reward + Measures: [[1569.50863542    0.21500002    0.5499        0.15460001    0.29840001]
 [ 949.23313012    0.3026        0.47180006    0.15799999    0.41340002]
 [ 426.69297761    0.59680003    0.68199998    0.30850002    0.60170001]
 ...
 [1478.92239309    0.2105        0.45660001    0.18090001    0.31029996]
 [1208.92744575    0.23240001    0.52720004    0.18249999    0.30230001]
 [1425.52943991    0.2313        0.46149999    0.18080001    0.28910002]][0m
[37m[1m[2023-07-10 22:38:38,041][227910] Max Reward on eval: 1817.3439828777687[0m
[37m[1m[2023-07-10 22:38:38,041][227910] Min Reward on eval: -11.967452894768211[0m
[37m[1m[2023-07-10 22:38:38,042][227910] Mean Reward across all agents: 913.1093417892256[0m
[37m[1m[2023-07-10 22:38:38,042][227910] Average Trajectory Length: 996.0353333333333[0m
[36m[2023-07-10 22:38:38,044][227910] mean_value=-970.3200275572451, max_value=781.5279666960427[0m
[37m[1m[2023-07-10 22:38:38,047][227910] New mean coefficients: [[ 0.20195866  0.0953204   0.71383923 -0.80053127  0.56938857]][0m
[37m[1m[2023-07-10 22:38:38,048][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:38:47,726][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 22:38:47,726][227910] FPS: 396847.37[0m
[36m[2023-07-10 22:38:47,729][227910] itr=1525, itrs=2000, Progress: 76.25%[0m
[36m[2023-07-10 22:38:59,208][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 22:38:59,208][227910] FPS: 335055.67[0m
[36m[2023-07-10 22:39:03,894][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:39:03,894][227910] Reward + Measures: [[1864.18894456    0.21147874    0.50747699    0.16691574    0.25840351]][0m
[37m[1m[2023-07-10 22:39:03,895][227910] Max Reward on eval: 1864.1889445594052[0m
[37m[1m[2023-07-10 22:39:03,895][227910] Min Reward on eval: 1864.1889445594052[0m
[37m[1m[2023-07-10 22:39:03,895][227910] Mean Reward across all agents: 1864.1889445594052[0m
[37m[1m[2023-07-10 22:39:03,895][227910] Average Trajectory Length: 991.0356666666667[0m
[36m[2023-07-10 22:39:09,490][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:39:09,491][227910] Reward + Measures: [[1183.93967657    0.25068167    0.47698259    0.19760092    0.32625142]
 [1131.03024482    0.27620003    0.50150001    0.2007        0.3163    ]
 [ 326.47510116    0.56690001    0.57249999    0.57880002    0.57579994]
 ...
 [ 947.93262466    0.2333        0.51340002    0.29229999    0.50800008]
 [1676.41459818    0.19640002    0.59400004    0.1551        0.28400001]
 [1594.58598129    0.20290001    0.50650001    0.1569        0.2841    ]][0m
[37m[1m[2023-07-10 22:39:09,491][227910] Max Reward on eval: 1952.0085142214782[0m
[37m[1m[2023-07-10 22:39:09,492][227910] Min Reward on eval: -273.78496631876044[0m
[37m[1m[2023-07-10 22:39:09,492][227910] Mean Reward across all agents: 1132.385604333654[0m
[37m[1m[2023-07-10 22:39:09,492][227910] Average Trajectory Length: 978.808[0m
[36m[2023-07-10 22:39:09,494][227910] mean_value=-729.2318018385981, max_value=908.5006975996685[0m
[37m[1m[2023-07-10 22:39:09,496][227910] New mean coefficients: [[ 0.31936386  0.27654326  0.4865753  -0.7657466   0.6088026 ]][0m
[37m[1m[2023-07-10 22:39:09,497][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:39:19,207][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 22:39:19,207][227910] FPS: 395540.10[0m
[36m[2023-07-10 22:39:19,210][227910] itr=1526, itrs=2000, Progress: 76.30%[0m
[36m[2023-07-10 22:39:30,880][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 22:39:30,880][227910] FPS: 329666.48[0m
[36m[2023-07-10 22:39:35,666][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:39:35,666][227910] Reward + Measures: [[2017.65668551    0.20705371    0.51724863    0.17284594    0.24807283]][0m
[37m[1m[2023-07-10 22:39:35,667][227910] Max Reward on eval: 2017.656685505428[0m
[37m[1m[2023-07-10 22:39:35,667][227910] Min Reward on eval: 2017.656685505428[0m
[37m[1m[2023-07-10 22:39:35,667][227910] Mean Reward across all agents: 2017.656685505428[0m
[37m[1m[2023-07-10 22:39:35,668][227910] Average Trajectory Length: 991.6913333333333[0m
[36m[2023-07-10 22:39:41,121][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:39:41,121][227910] Reward + Measures: [[1684.06592005    0.20870002    0.6372        0.1847        0.31640002]
 [1352.62641931    0.24888277    0.58249998    0.12759656    0.3303276 ]
 [1506.71139935    0.25572208    0.50999576    0.14193876    0.2781454 ]
 ...
 [1277.39524984    0.2105        0.59930003    0.13310002    0.3723    ]
 [1384.45143909    0.23720001    0.55050004    0.16989999    0.30299997]
 [1447.76312832    0.21619999    0.51929998    0.13510001    0.27630001]][0m
[37m[1m[2023-07-10 22:39:41,122][227910] Max Reward on eval: 2597.0588816508885[0m
[37m[1m[2023-07-10 22:39:41,122][227910] Min Reward on eval: 93.52638452774845[0m
[37m[1m[2023-07-10 22:39:41,122][227910] Mean Reward across all agents: 1405.2863383590172[0m
[37m[1m[2023-07-10 22:39:41,122][227910] Average Trajectory Length: 988.9893333333333[0m
[36m[2023-07-10 22:39:41,124][227910] mean_value=-764.3202891204312, max_value=850.6756800710696[0m
[37m[1m[2023-07-10 22:39:41,127][227910] New mean coefficients: [[-0.7061353   0.6818968   0.7084292  -0.8442377   0.93855065]][0m
[37m[1m[2023-07-10 22:39:41,128][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:39:50,952][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 22:39:50,952][227910] FPS: 390940.77[0m
[36m[2023-07-10 22:39:50,955][227910] itr=1527, itrs=2000, Progress: 76.35%[0m
[36m[2023-07-10 22:40:02,587][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 22:40:02,587][227910] FPS: 330664.13[0m
[36m[2023-07-10 22:40:07,371][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:40:07,371][227910] Reward + Measures: [[1878.61621508    0.21448569    0.53207225    0.16216791    0.26915812]][0m
[37m[1m[2023-07-10 22:40:07,371][227910] Max Reward on eval: 1878.6162150797222[0m
[37m[1m[2023-07-10 22:40:07,372][227910] Min Reward on eval: 1878.6162150797222[0m
[37m[1m[2023-07-10 22:40:07,372][227910] Mean Reward across all agents: 1878.6162150797222[0m
[37m[1m[2023-07-10 22:40:07,372][227910] Average Trajectory Length: 994.6089999999999[0m
[36m[2023-07-10 22:40:12,907][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:40:12,908][227910] Reward + Measures: [[1385.3562533     0.20729999    0.4111        0.133         0.2471    ]
 [ 671.64472699    0.29480001    0.40529996    0.22019999    0.21610001]
 [1761.48050962    0.22413655    0.51024544    0.17229059    0.27158478]
 ...
 [ 695.08181518    0.2044435     0.45009699    0.1649832     0.37322518]
 [1312.97981471    0.19939768    0.4200165     0.14626072    0.25475571]
 [1323.29388523    0.21760002    0.49650002    0.162         0.24000001]][0m
[37m[1m[2023-07-10 22:40:12,908][227910] Max Reward on eval: 1980.203715722752[0m
[37m[1m[2023-07-10 22:40:12,908][227910] Min Reward on eval: -280.1531484753126[0m
[37m[1m[2023-07-10 22:40:12,908][227910] Mean Reward across all agents: 914.6342351908589[0m
[37m[1m[2023-07-10 22:40:12,909][227910] Average Trajectory Length: 985.8156666666666[0m
[36m[2023-07-10 22:40:12,910][227910] mean_value=-1128.6855324630892, max_value=897.7860116935818[0m
[37m[1m[2023-07-10 22:40:12,913][227910] New mean coefficients: [[-1.5508087   1.1044981   0.76119137 -0.7354321   1.1618688 ]][0m
[37m[1m[2023-07-10 22:40:12,914][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:40:22,617][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 22:40:22,617][227910] FPS: 395848.53[0m
[36m[2023-07-10 22:40:22,619][227910] itr=1528, itrs=2000, Progress: 76.40%[0m
[36m[2023-07-10 22:40:34,282][227910] train() took 11.64 seconds to complete[0m
[36m[2023-07-10 22:40:34,283][227910] FPS: 329780.40[0m
[36m[2023-07-10 22:40:38,936][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:40:38,936][227910] Reward + Measures: [[1518.76550793    0.23328419    0.53453106    0.13942143    0.32233533]][0m
[37m[1m[2023-07-10 22:40:38,936][227910] Max Reward on eval: 1518.7655079302301[0m
[37m[1m[2023-07-10 22:40:38,937][227910] Min Reward on eval: 1518.7655079302301[0m
[37m[1m[2023-07-10 22:40:38,937][227910] Mean Reward across all agents: 1518.7655079302301[0m
[37m[1m[2023-07-10 22:40:38,937][227910] Average Trajectory Length: 996.9646666666666[0m
[36m[2023-07-10 22:40:44,402][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:40:44,403][227910] Reward + Measures: [[ -28.93365415    0.54098547    0.57967663    0.55307192    0.42851168]
 [1159.96940382    0.2807        0.47329998    0.22330001    0.39120001]
 [   3.82709159    0.47540003    0.4226        0.45569998    0.31800002]
 ...
 [ -46.10128728    0.28887498    0.74914998    0.58167505    0.65934998]
 [-140.94325548    0.35489997    0.70390004    0.3558        0.74700004]
 [-401.80272013    0.44536781    0.57992935    0.32577458    0.52614802]][0m
[37m[1m[2023-07-10 22:40:44,403][227910] Max Reward on eval: 1522.3210674572504[0m
[37m[1m[2023-07-10 22:40:44,403][227910] Min Reward on eval: -690.6501539654855[0m
[37m[1m[2023-07-10 22:40:44,403][227910] Mean Reward across all agents: -2.991089208780248[0m
[37m[1m[2023-07-10 22:40:44,404][227910] Average Trajectory Length: 966.9053333333333[0m
[36m[2023-07-10 22:40:44,405][227910] mean_value=-1116.9783208618458, max_value=450.64103831116904[0m
[37m[1m[2023-07-10 22:40:44,408][227910] New mean coefficients: [[-0.6040175   0.7854316   0.39471823 -0.99365234  0.63468504]][0m
[37m[1m[2023-07-10 22:40:44,409][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:40:54,246][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 22:40:54,246][227910] FPS: 390433.78[0m
[36m[2023-07-10 22:40:54,249][227910] itr=1529, itrs=2000, Progress: 76.45%[0m
[36m[2023-07-10 22:41:05,833][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 22:41:05,833][227910] FPS: 332133.06[0m
[36m[2023-07-10 22:41:10,572][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:41:10,573][227910] Reward + Measures: [[1250.95314193    0.25131848    0.51523858    0.1233715     0.37966779]][0m
[37m[1m[2023-07-10 22:41:10,573][227910] Max Reward on eval: 1250.953141929963[0m
[37m[1m[2023-07-10 22:41:10,573][227910] Min Reward on eval: 1250.953141929963[0m
[37m[1m[2023-07-10 22:41:10,573][227910] Mean Reward across all agents: 1250.953141929963[0m
[37m[1m[2023-07-10 22:41:10,574][227910] Average Trajectory Length: 997.7646666666666[0m
[36m[2023-07-10 22:41:16,014][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:41:16,015][227910] Reward + Measures: [[1079.6163969     0.2527        0.46550003    0.20490001    0.33980003]
 [ 875.34699313    0.29120001    0.54870003    0.26990005    0.42350003]
 [ 456.40319765    0.33830002    0.54139996    0.20179999    0.54439998]
 ...
 [ 699.86552635    0.2863        0.48890001    0.26719999    0.551     ]
 [1074.5079512     0.278         0.53079998    0.2132        0.36579999]
 [ 477.92940468    0.3493        0.61040002    0.34720001    0.61070001]][0m
[37m[1m[2023-07-10 22:41:16,015][227910] Max Reward on eval: 1400.4949395676376[0m
[37m[1m[2023-07-10 22:41:16,015][227910] Min Reward on eval: -52.244975726446135[0m
[37m[1m[2023-07-10 22:41:16,016][227910] Mean Reward across all agents: 575.0527148015208[0m
[37m[1m[2023-07-10 22:41:16,016][227910] Average Trajectory Length: 997.6446666666666[0m
[36m[2023-07-10 22:41:16,017][227910] mean_value=-624.5757454393868, max_value=472.15812917212867[0m
[37m[1m[2023-07-10 22:41:16,020][227910] New mean coefficients: [[-1.3635006   1.1471804   0.21721907 -0.7767085   0.9376335 ]][0m
[37m[1m[2023-07-10 22:41:16,021][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:41:25,747][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 22:41:25,747][227910] FPS: 394886.46[0m
[36m[2023-07-10 22:41:25,750][227910] itr=1530, itrs=2000, Progress: 76.50%[0m
[37m[1m[2023-07-10 22:41:29,842][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001510[0m
[36m[2023-07-10 22:41:41,627][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 22:41:41,627][227910] FPS: 333666.75[0m
[36m[2023-07-10 22:41:46,437][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:41:46,437][227910] Reward + Measures: [[953.71720195   0.2656295    0.48501366   0.13571255   0.4517456 ]][0m
[37m[1m[2023-07-10 22:41:46,438][227910] Max Reward on eval: 953.7172019453272[0m
[37m[1m[2023-07-10 22:41:46,438][227910] Min Reward on eval: 953.7172019453272[0m
[37m[1m[2023-07-10 22:41:46,438][227910] Mean Reward across all agents: 953.7172019453272[0m
[37m[1m[2023-07-10 22:41:46,438][227910] Average Trajectory Length: 998.8716666666667[0m
[36m[2023-07-10 22:41:51,986][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:41:51,986][227910] Reward + Measures: [[369.21445167   0.27700001   0.45320007   0.24589999   0.5255    ]
 [870.05301466   0.3486       0.42519999   0.2414       0.4797    ]
 [324.71464029   0.35960001   0.3242       0.3294       0.43789998]
 ...
 [441.57006079   0.38129997   0.4127       0.264        0.59240001]
 [519.11166674   0.2141       0.39039999   0.21430002   0.42469999]
 [555.33679245   0.3249       0.38329998   0.1717       0.44970003]][0m
[37m[1m[2023-07-10 22:41:51,986][227910] Max Reward on eval: 1216.8781829935033[0m
[37m[1m[2023-07-10 22:41:51,987][227910] Min Reward on eval: -116.22879041832638[0m
[37m[1m[2023-07-10 22:41:51,987][227910] Mean Reward across all agents: 390.7648865080474[0m
[37m[1m[2023-07-10 22:41:51,987][227910] Average Trajectory Length: 997.1526666666666[0m
[36m[2023-07-10 22:41:51,990][227910] mean_value=-523.9877073909951, max_value=660.5676556546118[0m
[37m[1m[2023-07-10 22:41:51,992][227910] New mean coefficients: [[-1.3844498   1.1190326   0.13332453 -1.1300268   0.8979157 ]][0m
[37m[1m[2023-07-10 22:41:51,993][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:42:01,758][227910] train() took 9.76 seconds to complete[0m
[36m[2023-07-10 22:42:01,758][227910] FPS: 393332.19[0m
[36m[2023-07-10 22:42:01,760][227910] itr=1531, itrs=2000, Progress: 76.55%[0m
[36m[2023-07-10 22:42:13,334][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 22:42:13,334][227910] FPS: 332442.81[0m
[36m[2023-07-10 22:42:18,070][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:42:18,070][227910] Reward + Measures: [[641.60657761   0.32340097   0.49572173   0.24905685   0.53816599]][0m
[37m[1m[2023-07-10 22:42:18,070][227910] Max Reward on eval: 641.6065776149766[0m
[37m[1m[2023-07-10 22:42:18,071][227910] Min Reward on eval: 641.6065776149766[0m
[37m[1m[2023-07-10 22:42:18,071][227910] Mean Reward across all agents: 641.6065776149766[0m
[37m[1m[2023-07-10 22:42:18,071][227910] Average Trajectory Length: 999.8076666666666[0m
[36m[2023-07-10 22:42:23,664][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:42:23,665][227910] Reward + Measures: [[-170.94000678    0.1728        0.31729999    0.1866        0.30570003]
 [-313.93861269    0.12229999    0.18740001    0.1124        0.15260001]
 [ 801.30438106    0.26040003    0.43560001    0.1158        0.36930001]
 ...
 [-265.91455547    0.1719        0.24679999    0.09670001    0.23149998]
 [ 531.4461697     0.22319999    0.42390004    0.12290001    0.27779999]
 [ 267.09631923    0.25880003    0.62910002    0.1148        0.63029999]][0m
[37m[1m[2023-07-10 22:42:23,665][227910] Max Reward on eval: 1010.11718566661[0m
[37m[1m[2023-07-10 22:42:23,665][227910] Min Reward on eval: -1113.7290540338495[0m
[37m[1m[2023-07-10 22:42:23,665][227910] Mean Reward across all agents: 176.6343215670079[0m
[37m[1m[2023-07-10 22:42:23,666][227910] Average Trajectory Length: 996.6756666666666[0m
[36m[2023-07-10 22:42:23,667][227910] mean_value=-1537.3776486177915, max_value=288.5735319191549[0m
[37m[1m[2023-07-10 22:42:23,669][227910] New mean coefficients: [[-0.7005317   0.97668433  0.105406   -0.83772975  0.6201507 ]][0m
[37m[1m[2023-07-10 22:42:23,670][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:42:33,367][227910] train() took 9.69 seconds to complete[0m
[36m[2023-07-10 22:42:33,367][227910] FPS: 396083.69[0m
[36m[2023-07-10 22:42:33,369][227910] itr=1532, itrs=2000, Progress: 76.60%[0m
[36m[2023-07-10 22:42:44,939][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 22:42:44,939][227910] FPS: 332422.86[0m
[36m[2023-07-10 22:42:49,809][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:42:49,810][227910] Reward + Measures: [[356.61648678   0.41634765   0.61218232   0.45057634   0.66838962]][0m
[37m[1m[2023-07-10 22:42:49,810][227910] Max Reward on eval: 356.6164867791532[0m
[37m[1m[2023-07-10 22:42:49,810][227910] Min Reward on eval: 356.6164867791532[0m
[37m[1m[2023-07-10 22:42:49,810][227910] Mean Reward across all agents: 356.6164867791532[0m
[37m[1m[2023-07-10 22:42:49,810][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:42:55,337][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:42:55,338][227910] Reward + Measures: [[682.63444073   0.22979999   0.57960004   0.0813       0.45930001]
 [791.80698858   0.22160001   0.57709998   0.1054       0.4348    ]
 [504.02515467   0.73139995   0.68219995   0.83840001   0.6742    ]
 ...
 [589.02856246   0.28220001   0.46310002   0.13219999   0.53619999]
 [475.34615991   0.35800001   0.52240002   0.27180001   0.51969999]
 [650.3022253    0.18269999   0.61490005   0.12920001   0.43689996]][0m
[37m[1m[2023-07-10 22:42:55,338][227910] Max Reward on eval: 919.8577624577331[0m
[37m[1m[2023-07-10 22:42:55,338][227910] Min Reward on eval: 135.68384012152674[0m
[37m[1m[2023-07-10 22:42:55,339][227910] Mean Reward across all agents: 674.5810033631523[0m
[37m[1m[2023-07-10 22:42:55,339][227910] Average Trajectory Length: 999.0276666666666[0m
[36m[2023-07-10 22:42:55,340][227910] mean_value=-507.50225294726977, max_value=602.8287443769165[0m
[37m[1m[2023-07-10 22:42:55,343][227910] New mean coefficients: [[-0.4047014   0.86845756 -0.14514212 -0.8364823   0.4215714 ]][0m
[37m[1m[2023-07-10 22:42:55,344][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:43:05,159][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 22:43:05,160][227910] FPS: 391273.23[0m
[36m[2023-07-10 22:43:05,162][227910] itr=1533, itrs=2000, Progress: 76.65%[0m
[36m[2023-07-10 22:43:16,659][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 22:43:16,659][227910] FPS: 334651.80[0m
[36m[2023-07-10 22:43:21,392][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:43:21,392][227910] Reward + Measures: [[292.85496885   0.56433368   0.68599135   0.59991902   0.76954639]][0m
[37m[1m[2023-07-10 22:43:21,392][227910] Max Reward on eval: 292.8549688506919[0m
[37m[1m[2023-07-10 22:43:21,393][227910] Min Reward on eval: 292.8549688506919[0m
[37m[1m[2023-07-10 22:43:21,393][227910] Mean Reward across all agents: 292.8549688506919[0m
[37m[1m[2023-07-10 22:43:21,393][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:43:26,719][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:43:26,719][227910] Reward + Measures: [[465.85125529   0.23239999   0.41030008   0.1259       0.27720001]
 [369.69643652   0.39230004   0.68320006   0.47639999   0.73900002]
 [381.51351393   0.29650003   0.354        0.1568       0.3664    ]
 ...
 [379.67607867   0.54479998   0.46989998   0.47890002   0.32350001]
 [394.03103403   0.74040002   0.71049994   0.7349       0.55120003]
 [544.60101543   0.2911       0.43829998   0.11849999   0.35839999]][0m
[37m[1m[2023-07-10 22:43:26,719][227910] Max Reward on eval: 733.5961355399456[0m
[37m[1m[2023-07-10 22:43:26,720][227910] Min Reward on eval: 20.602472172945273[0m
[37m[1m[2023-07-10 22:43:26,720][227910] Mean Reward across all agents: 437.3050101679223[0m
[37m[1m[2023-07-10 22:43:26,720][227910] Average Trajectory Length: 996.577[0m
[36m[2023-07-10 22:43:26,722][227910] mean_value=-1534.8176042121058, max_value=587.97103682222[0m
[37m[1m[2023-07-10 22:43:26,725][227910] New mean coefficients: [[ 1.066232    0.29281628 -0.03133281 -0.90514576  0.2605899 ]][0m
[37m[1m[2023-07-10 22:43:26,726][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:43:36,674][227910] train() took 9.95 seconds to complete[0m
[36m[2023-07-10 22:43:36,674][227910] FPS: 386054.89[0m
[36m[2023-07-10 22:43:36,677][227910] itr=1534, itrs=2000, Progress: 76.70%[0m
[36m[2023-07-10 22:43:48,420][227910] train() took 11.72 seconds to complete[0m
[36m[2023-07-10 22:43:48,421][227910] FPS: 327656.98[0m
[36m[2023-07-10 22:43:53,249][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:43:53,250][227910] Reward + Measures: [[411.14089574   0.45672697   0.61526817   0.39869547   0.67830622]][0m
[37m[1m[2023-07-10 22:43:53,250][227910] Max Reward on eval: 411.1408957358796[0m
[37m[1m[2023-07-10 22:43:53,250][227910] Min Reward on eval: 411.1408957358796[0m
[37m[1m[2023-07-10 22:43:53,251][227910] Mean Reward across all agents: 411.1408957358796[0m
[37m[1m[2023-07-10 22:43:53,251][227910] Average Trajectory Length: 999.867[0m
[36m[2023-07-10 22:43:58,610][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:43:58,611][227910] Reward + Measures: [[391.0742138    0.4844       0.76230001   0.55900002   0.70610005]
 [260.92491065   0.49060002   0.59189999   0.57690001   0.56689996]
 [224.22405457   0.41260001   0.77080005   0.60400003   0.79940003]
 ...
 [298.98637731   0.54799998   0.73050004   0.60529995   0.72750002]
 [495.38424045   0.38159999   0.53509998   0.31720001   0.56599998]
 [367.50280375   0.4894       0.76179999   0.55369997   0.74150002]][0m
[37m[1m[2023-07-10 22:43:58,611][227910] Max Reward on eval: 958.6909153115819[0m
[37m[1m[2023-07-10 22:43:58,611][227910] Min Reward on eval: 97.93201541600283[0m
[37m[1m[2023-07-10 22:43:58,611][227910] Mean Reward across all agents: 353.6918041303697[0m
[37m[1m[2023-07-10 22:43:58,611][227910] Average Trajectory Length: 999.8923333333333[0m
[36m[2023-07-10 22:43:58,614][227910] mean_value=-266.45602664092866, max_value=458.5580852689177[0m
[37m[1m[2023-07-10 22:43:58,617][227910] New mean coefficients: [[ 2.032466    0.19291091  0.07880267 -0.9046736   0.45765716]][0m
[37m[1m[2023-07-10 22:43:58,617][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:44:08,234][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 22:44:08,234][227910] FPS: 399378.62[0m
[36m[2023-07-10 22:44:08,237][227910] itr=1535, itrs=2000, Progress: 76.75%[0m
[36m[2023-07-10 22:44:19,734][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 22:44:19,735][227910] FPS: 334519.74[0m
[36m[2023-07-10 22:44:24,435][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:44:24,435][227910] Reward + Measures: [[609.15359353   0.35929906   0.53649324   0.20984632   0.6074568 ]][0m
[37m[1m[2023-07-10 22:44:24,435][227910] Max Reward on eval: 609.1535935281657[0m
[37m[1m[2023-07-10 22:44:24,436][227910] Min Reward on eval: 609.1535935281657[0m
[37m[1m[2023-07-10 22:44:24,436][227910] Mean Reward across all agents: 609.1535935281657[0m
[37m[1m[2023-07-10 22:44:24,436][227910] Average Trajectory Length: 999.7303333333333[0m
[36m[2023-07-10 22:44:29,942][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:44:29,943][227910] Reward + Measures: [[ 57.61892421   0.43959999   0.3211       0.19560002   0.46680003]
 [ 80.9268501    0.19860001   0.1866       0.0544       0.22640002]
 [541.81151348   0.23639999   0.70740002   0.28120002   0.75220007]
 ...
 [576.00929646   0.28660002   0.5869       0.23959999   0.66580003]
 [304.20561242   0.3506       0.31560001   0.17029999   0.37650001]
 [538.52476285   0.3369       0.48059997   0.16600001   0.5783    ]][0m
[37m[1m[2023-07-10 22:44:29,943][227910] Max Reward on eval: 1199.8202529782197[0m
[37m[1m[2023-07-10 22:44:29,944][227910] Min Reward on eval: -388.75360871827286[0m
[37m[1m[2023-07-10 22:44:29,944][227910] Mean Reward across all agents: 527.68714200479[0m
[37m[1m[2023-07-10 22:44:29,944][227910] Average Trajectory Length: 999.7406666666666[0m
[36m[2023-07-10 22:44:29,949][227910] mean_value=-113.34913051500332, max_value=1047.4206498921096[0m
[37m[1m[2023-07-10 22:44:29,951][227910] New mean coefficients: [[ 1.9038215   0.31331766  0.3994953  -0.8041382   0.8270099 ]][0m
[37m[1m[2023-07-10 22:44:29,952][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:44:39,738][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 22:44:39,738][227910] FPS: 392492.79[0m
[36m[2023-07-10 22:44:39,740][227910] itr=1536, itrs=2000, Progress: 76.80%[0m
[36m[2023-07-10 22:44:51,233][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 22:44:51,233][227910] FPS: 334668.60[0m
[36m[2023-07-10 22:44:56,054][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:44:56,054][227910] Reward + Measures: [[841.37492592   0.30698067   0.494955     0.10638534   0.54766667]][0m
[37m[1m[2023-07-10 22:44:56,055][227910] Max Reward on eval: 841.374925916749[0m
[37m[1m[2023-07-10 22:44:56,055][227910] Min Reward on eval: 841.374925916749[0m
[37m[1m[2023-07-10 22:44:56,055][227910] Mean Reward across all agents: 841.374925916749[0m
[37m[1m[2023-07-10 22:44:56,055][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:45:01,715][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:45:01,716][227910] Reward + Measures: [[1036.15925935    0.2782        0.5176        0.0851        0.49869996]
 [ 848.57652108    0.31380001    0.53560001    0.1645        0.59740001]
 [1010.5173814     0.2674        0.53310001    0.0944        0.51060003]
 ...
 [1002.29220682    0.2793        0.49950001    0.0852        0.51499999]
 [ 938.63709528    0.30590001    0.49829999    0.08350001    0.51209998]
 [1135.82589852    0.27110001    0.51500005    0.0938        0.46689996]][0m
[37m[1m[2023-07-10 22:45:01,716][227910] Max Reward on eval: 1459.5131532016908[0m
[37m[1m[2023-07-10 22:45:01,716][227910] Min Reward on eval: 410.97065558455654[0m
[37m[1m[2023-07-10 22:45:01,716][227910] Mean Reward across all agents: 925.6255080002205[0m
[37m[1m[2023-07-10 22:45:01,717][227910] Average Trajectory Length: 999.5369999999999[0m
[36m[2023-07-10 22:45:01,719][227910] mean_value=-58.92728469295778, max_value=1166.0356616888018[0m
[37m[1m[2023-07-10 22:45:01,722][227910] New mean coefficients: [[ 2.034313    0.25763392  0.67372215 -0.82763267  0.96183217]][0m
[37m[1m[2023-07-10 22:45:01,723][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:45:11,565][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 22:45:11,565][227910] FPS: 390224.25[0m
[36m[2023-07-10 22:45:11,567][227910] itr=1537, itrs=2000, Progress: 76.85%[0m
[36m[2023-07-10 22:45:23,041][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 22:45:23,041][227910] FPS: 335327.64[0m
[36m[2023-07-10 22:45:27,877][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:45:27,877][227910] Reward + Measures: [[1070.98653171    0.28746065    0.52710861    0.09122766    0.47648332]][0m
[37m[1m[2023-07-10 22:45:27,878][227910] Max Reward on eval: 1070.9865317136794[0m
[37m[1m[2023-07-10 22:45:27,878][227910] Min Reward on eval: 1070.9865317136794[0m
[37m[1m[2023-07-10 22:45:27,878][227910] Mean Reward across all agents: 1070.9865317136794[0m
[37m[1m[2023-07-10 22:45:27,878][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:45:33,356][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:45:33,356][227910] Reward + Measures: [[964.12774873   0.28509998   0.47890005   0.0842       0.47340003]
 [949.16741662   0.31150001   0.49989995   0.10469999   0.58919996]
 [993.09268246   0.26949999   0.51540005   0.10339999   0.57970005]
 ...
 [996.54545677   0.27379999   0.514        0.1127       0.58490002]
 [999.43288698   0.26639998   0.50120002   0.11800001   0.62379998]
 [957.73346546   0.25929999   0.52610004   0.1019       0.57210004]][0m
[37m[1m[2023-07-10 22:45:33,357][227910] Max Reward on eval: 1159.02008484588[0m
[37m[1m[2023-07-10 22:45:33,357][227910] Min Reward on eval: 757.2504587323172[0m
[37m[1m[2023-07-10 22:45:33,357][227910] Mean Reward across all agents: 985.1636079711471[0m
[37m[1m[2023-07-10 22:45:33,357][227910] Average Trajectory Length: 999.608[0m
[36m[2023-07-10 22:45:33,361][227910] mean_value=119.50941801117166, max_value=1265.5463567594095[0m
[37m[1m[2023-07-10 22:45:33,364][227910] New mean coefficients: [[ 2.5234962   0.20474866  1.2625449  -0.8707824   1.1536804 ]][0m
[37m[1m[2023-07-10 22:45:33,365][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:45:43,088][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 22:45:43,088][227910] FPS: 395024.19[0m
[36m[2023-07-10 22:45:43,090][227910] itr=1538, itrs=2000, Progress: 76.90%[0m
[36m[2023-07-10 22:45:54,578][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 22:45:54,578][227910] FPS: 334917.89[0m
[36m[2023-07-10 22:45:59,306][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:45:59,307][227910] Reward + Measures: [[1325.98505201    0.26462653    0.56563687    0.10035811    0.38983425]][0m
[37m[1m[2023-07-10 22:45:59,307][227910] Max Reward on eval: 1325.9850520085104[0m
[37m[1m[2023-07-10 22:45:59,307][227910] Min Reward on eval: 1325.9850520085104[0m
[37m[1m[2023-07-10 22:45:59,307][227910] Mean Reward across all agents: 1325.9850520085104[0m
[37m[1m[2023-07-10 22:45:59,307][227910] Average Trajectory Length: 996.55[0m
[36m[2023-07-10 22:46:04,684][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:46:04,685][227910] Reward + Measures: [[ 885.79047611    0.2809        0.43540001    0.1612        0.45559999]
 [ 883.80317364    0.29270002    0.46129999    0.0983        0.52030003]
 [1105.48734138    0.2739        0.51430005    0.075         0.46510002]
 ...
 [1158.6871115     0.22600003    0.4973        0.0949        0.38000003]
 [ 955.7835086     0.27959999    0.50349998    0.0669        0.49109998]
 [ 645.29227175    0.264         0.46290001    0.0902        0.49959999]][0m
[37m[1m[2023-07-10 22:46:04,685][227910] Max Reward on eval: 1558.132094600983[0m
[37m[1m[2023-07-10 22:46:04,685][227910] Min Reward on eval: 462.6309669127921[0m
[37m[1m[2023-07-10 22:46:04,685][227910] Mean Reward across all agents: 1113.2031006475706[0m
[37m[1m[2023-07-10 22:46:04,686][227910] Average Trajectory Length: 999.41[0m
[36m[2023-07-10 22:46:04,688][227910] mean_value=-115.42141378117904, max_value=1104.0320230105076[0m
[37m[1m[2023-07-10 22:46:04,690][227910] New mean coefficients: [[ 2.3379054   0.46216574  1.2299486  -0.7450337   1.3582981 ]][0m
[37m[1m[2023-07-10 22:46:04,691][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:46:14,354][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 22:46:14,355][227910] FPS: 397448.99[0m
[36m[2023-07-10 22:46:14,357][227910] itr=1539, itrs=2000, Progress: 76.95%[0m
[36m[2023-07-10 22:46:26,107][227910] train() took 11.73 seconds to complete[0m
[36m[2023-07-10 22:46:26,107][227910] FPS: 327333.15[0m
[36m[2023-07-10 22:46:30,919][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:46:30,919][227910] Reward + Measures: [[1678.70743138    0.23590663    0.57670498    0.13078074    0.32001588]][0m
[37m[1m[2023-07-10 22:46:30,919][227910] Max Reward on eval: 1678.7074313761486[0m
[37m[1m[2023-07-10 22:46:30,919][227910] Min Reward on eval: 1678.7074313761486[0m
[37m[1m[2023-07-10 22:46:30,920][227910] Mean Reward across all agents: 1678.7074313761486[0m
[37m[1m[2023-07-10 22:46:30,920][227910] Average Trajectory Length: 998.0743333333334[0m
[36m[2023-07-10 22:46:36,348][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:46:36,349][227910] Reward + Measures: [[ 883.35673669    0.35570002    0.54470009    0.1969        0.46799999]
 [ 572.08463723    0.39050001    0.4698        0.19250001    0.5722    ]
 [ 970.38727082    0.32819998    0.6243        0.1505        0.40459999]
 ...
 [1267.46077137    0.32729998    0.61980003    0.1066        0.47710004]
 [1369.11753471    0.27620003    0.58890003    0.1362        0.39040002]
 [1615.36533444    0.2859        0.65530008    0.1053        0.34999999]][0m
[37m[1m[2023-07-10 22:46:36,349][227910] Max Reward on eval: 1931.526803748752[0m
[37m[1m[2023-07-10 22:46:36,349][227910] Min Reward on eval: 423.51748515113724[0m
[37m[1m[2023-07-10 22:46:36,350][227910] Mean Reward across all agents: 1374.8493448266333[0m
[37m[1m[2023-07-10 22:46:36,350][227910] Average Trajectory Length: 995.1893333333333[0m
[36m[2023-07-10 22:46:36,352][227910] mean_value=-117.15690029062335, max_value=2261.6710148637185[0m
[37m[1m[2023-07-10 22:46:36,355][227910] New mean coefficients: [[ 2.141154    0.55988765  1.7030351  -0.63524324  1.5693705 ]][0m
[37m[1m[2023-07-10 22:46:36,356][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:46:46,133][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 22:46:46,133][227910] FPS: 392815.01[0m
[36m[2023-07-10 22:46:46,135][227910] itr=1540, itrs=2000, Progress: 77.00%[0m
[37m[1m[2023-07-10 22:46:50,310][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001520[0m
[36m[2023-07-10 22:47:02,301][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 22:47:02,301][227910] FPS: 327931.80[0m
[36m[2023-07-10 22:47:07,102][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:47:07,103][227910] Reward + Measures: [[1909.63956062    0.22112736    0.57669419    0.14799723    0.2854574 ]][0m
[37m[1m[2023-07-10 22:47:07,103][227910] Max Reward on eval: 1909.6395606234744[0m
[37m[1m[2023-07-10 22:47:07,103][227910] Min Reward on eval: 1909.6395606234744[0m
[37m[1m[2023-07-10 22:47:07,103][227910] Mean Reward across all agents: 1909.6395606234744[0m
[37m[1m[2023-07-10 22:47:07,104][227910] Average Trajectory Length: 997.135[0m
[36m[2023-07-10 22:47:12,731][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:47:12,732][227910] Reward + Measures: [[ 462.67710749    0.37310001    0.26440001    0.45470005    0.35279998]
 [ 777.96845835    0.384         0.52500004    0.38229999    0.59179997]
 [1258.46990124    0.30848628    0.49589053    0.28726664    0.45437917]
 ...
 [1174.63635587    0.2588        0.3766        0.2911        0.35190001]
 [1034.96255361    0.34119996    0.35360003    0.39209998    0.41589999]
 [ 445.93419415    0.38869998    0.31140003    0.4571        0.40820003]][0m
[37m[1m[2023-07-10 22:47:12,732][227910] Max Reward on eval: 1810.9594823126215[0m
[37m[1m[2023-07-10 22:47:12,732][227910] Min Reward on eval: -434.89193714329156[0m
[37m[1m[2023-07-10 22:47:12,732][227910] Mean Reward across all agents: 670.7046086913563[0m
[37m[1m[2023-07-10 22:47:12,733][227910] Average Trajectory Length: 996.0649999999999[0m
[36m[2023-07-10 22:47:12,735][227910] mean_value=-635.1503409944075, max_value=684.1201916552669[0m
[37m[1m[2023-07-10 22:47:12,737][227910] New mean coefficients: [[ 2.300427    0.59508777  1.9144715  -0.6230289   1.5927619 ]][0m
[37m[1m[2023-07-10 22:47:12,738][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:47:22,652][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 22:47:22,652][227910] FPS: 387411.67[0m
[36m[2023-07-10 22:47:22,654][227910] itr=1541, itrs=2000, Progress: 77.05%[0m
[36m[2023-07-10 22:47:34,359][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 22:47:34,359][227910] FPS: 328602.40[0m
[36m[2023-07-10 22:47:39,242][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:47:39,243][227910] Reward + Measures: [[2207.84825636    0.2052221     0.56696564    0.17194983    0.25078276]][0m
[37m[1m[2023-07-10 22:47:39,243][227910] Max Reward on eval: 2207.848256362482[0m
[37m[1m[2023-07-10 22:47:39,243][227910] Min Reward on eval: 2207.848256362482[0m
[37m[1m[2023-07-10 22:47:39,243][227910] Mean Reward across all agents: 2207.848256362482[0m
[37m[1m[2023-07-10 22:47:39,244][227910] Average Trajectory Length: 993.2013333333333[0m
[36m[2023-07-10 22:47:44,771][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:47:44,772][227910] Reward + Measures: [[ 699.81945486    0.2789        0.49370003    0.2554        0.43700001]
 [ 580.19600353    0.28380001    0.46020004    0.25530002    0.40699998]
 [ 635.76091528    0.29699907    0.50815374    0.26968428    0.46951666]
 ...
 [1044.69984505    0.23509999    0.50700003    0.2089        0.37100002]
 [ 391.80287939    0.33890003    0.49090001    0.30490002    0.45059997]
 [ 422.51946333    0.31659999    0.47790003    0.28369999    0.45199999]][0m
[37m[1m[2023-07-10 22:47:44,772][227910] Max Reward on eval: 2376.431477712607[0m
[37m[1m[2023-07-10 22:47:44,773][227910] Min Reward on eval: 15.360136700945441[0m
[37m[1m[2023-07-10 22:47:44,773][227910] Mean Reward across all agents: 1016.6751650796645[0m
[37m[1m[2023-07-10 22:47:44,773][227910] Average Trajectory Length: 994.1213333333333[0m
[36m[2023-07-10 22:47:44,774][227910] mean_value=-959.9492014039622, max_value=132.795291979161[0m
[37m[1m[2023-07-10 22:47:44,777][227910] New mean coefficients: [[ 2.4053206   0.5783101   1.5222415  -0.63432413  1.4130003 ]][0m
[37m[1m[2023-07-10 22:47:44,778][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:47:54,637][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 22:47:54,637][227910] FPS: 389532.84[0m
[36m[2023-07-10 22:47:54,640][227910] itr=1542, itrs=2000, Progress: 77.10%[0m
[36m[2023-07-10 22:48:06,346][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 22:48:06,347][227910] FPS: 328552.20[0m
[36m[2023-07-10 22:48:11,104][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:48:11,104][227910] Reward + Measures: [[2495.11568096    0.19115466    0.55580312    0.1912128     0.22317633]][0m
[37m[1m[2023-07-10 22:48:11,104][227910] Max Reward on eval: 2495.115680960383[0m
[37m[1m[2023-07-10 22:48:11,105][227910] Min Reward on eval: 2495.115680960383[0m
[37m[1m[2023-07-10 22:48:11,105][227910] Mean Reward across all agents: 2495.115680960383[0m
[37m[1m[2023-07-10 22:48:11,105][227910] Average Trajectory Length: 990.0296666666667[0m
[36m[2023-07-10 22:48:16,599][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:48:16,600][227910] Reward + Measures: [[ 601.08329209    0.24089999    0.3698        0.11489999    0.30500004]
 [1590.75159539    0.22390001    0.565         0.1411        0.27610001]
 [1228.41529611    0.23839998    0.51940006    0.1372        0.30090001]
 ...
 [1853.10491167    0.24080001    0.64300001    0.1725        0.31310001]
 [1496.85096074    0.24643464    0.61830652    0.15270352    0.32060298]
 [1939.78851597    0.2342        0.62889999    0.17110001    0.29590002]][0m
[37m[1m[2023-07-10 22:48:16,600][227910] Max Reward on eval: 2498.830771560222[0m
[37m[1m[2023-07-10 22:48:16,600][227910] Min Reward on eval: 63.25318543255853[0m
[37m[1m[2023-07-10 22:48:16,600][227910] Mean Reward across all agents: 1381.0543912030912[0m
[37m[1m[2023-07-10 22:48:16,601][227910] Average Trajectory Length: 995.1586666666666[0m
[36m[2023-07-10 22:48:16,602][227910] mean_value=-715.592593125383, max_value=212.86664495999594[0m
[37m[1m[2023-07-10 22:48:16,604][227910] New mean coefficients: [[ 2.0409331  0.6219483  0.9811222 -0.5475944  1.3547412]][0m
[37m[1m[2023-07-10 22:48:16,605][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:48:26,535][227910] train() took 9.93 seconds to complete[0m
[36m[2023-07-10 22:48:26,535][227910] FPS: 386796.54[0m
[36m[2023-07-10 22:48:26,537][227910] itr=1543, itrs=2000, Progress: 77.15%[0m
[36m[2023-07-10 22:48:38,322][227910] train() took 11.77 seconds to complete[0m
[36m[2023-07-10 22:48:38,322][227910] FPS: 326368.85[0m
[36m[2023-07-10 22:48:43,182][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:48:43,183][227910] Reward + Measures: [[2673.01584353    0.18468592    0.55408496    0.19961344    0.2102713 ]][0m
[37m[1m[2023-07-10 22:48:43,183][227910] Max Reward on eval: 2673.0158435327326[0m
[37m[1m[2023-07-10 22:48:43,183][227910] Min Reward on eval: 2673.0158435327326[0m
[37m[1m[2023-07-10 22:48:43,183][227910] Mean Reward across all agents: 2673.0158435327326[0m
[37m[1m[2023-07-10 22:48:43,183][227910] Average Trajectory Length: 989.836[0m
[36m[2023-07-10 22:48:48,706][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:48:48,712][227910] Reward + Measures: [[2667.82402086    0.18730001    0.57440001    0.2139        0.2271    ]
 [2489.07780311    0.19050001    0.52710003    0.19950001    0.2335    ]
 [1822.5374771     0.1916889     0.44352221    0.192921      0.23577285]
 ...
 [1955.82566066    0.1875        0.47540003    0.21539998    0.26720002]
 [2609.92993439    0.1829        0.55409998    0.20089999    0.20710002]
 [1659.33177957    0.15841769    0.40967211    0.1708177     0.21469998]][0m
[37m[1m[2023-07-10 22:48:48,713][227910] Max Reward on eval: 2849.0123734749855[0m
[37m[1m[2023-07-10 22:48:48,714][227910] Min Reward on eval: -3.804216649569571[0m
[37m[1m[2023-07-10 22:48:48,714][227910] Mean Reward across all agents: 1978.190506400407[0m
[37m[1m[2023-07-10 22:48:48,715][227910] Average Trajectory Length: 941.3059999999999[0m
[36m[2023-07-10 22:48:48,718][227910] mean_value=-1975.1501804240486, max_value=332.1234823107793[0m
[37m[1m[2023-07-10 22:48:48,723][227910] New mean coefficients: [[ 2.3456304   0.20121565  1.1112174  -0.65819526  1.3180553 ]][0m
[37m[1m[2023-07-10 22:48:48,725][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:48:58,589][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 22:48:58,589][227910] FPS: 389363.02[0m
[36m[2023-07-10 22:48:58,592][227910] itr=1544, itrs=2000, Progress: 77.20%[0m
[36m[2023-07-10 22:49:10,297][227910] train() took 11.69 seconds to complete[0m
[36m[2023-07-10 22:49:10,297][227910] FPS: 328622.58[0m
[36m[2023-07-10 22:49:15,113][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:49:15,118][227910] Reward + Measures: [[2836.74917612    0.18110918    0.55390549    0.20957434    0.20089579]][0m
[37m[1m[2023-07-10 22:49:15,119][227910] Max Reward on eval: 2836.7491761212304[0m
[37m[1m[2023-07-10 22:49:15,119][227910] Min Reward on eval: 2836.7491761212304[0m
[37m[1m[2023-07-10 22:49:15,119][227910] Mean Reward across all agents: 2836.7491761212304[0m
[37m[1m[2023-07-10 22:49:15,119][227910] Average Trajectory Length: 985.1173333333332[0m
[36m[2023-07-10 22:49:20,516][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:49:20,522][227910] Reward + Measures: [[2834.42454629    0.1816        0.60750002    0.1948        0.21610001]
 [2834.93500515    0.18639998    0.60319996    0.2014        0.20460001]
 [2490.86467275    0.1761        0.50400001    0.1833        0.1948    ]
 ...
 [2467.34222402    0.19360001    0.648         0.17400001    0.23220001]
 [2681.95199829    0.18630001    0.62160003    0.1926        0.21950002]
 [2714.03411447    0.18520001    0.62219995    0.18509999    0.2119    ]][0m
[37m[1m[2023-07-10 22:49:20,522][227910] Max Reward on eval: 2963.193561856123[0m
[37m[1m[2023-07-10 22:49:20,522][227910] Min Reward on eval: 2290.569955200079[0m
[37m[1m[2023-07-10 22:49:20,523][227910] Mean Reward across all agents: 2740.140977173135[0m
[37m[1m[2023-07-10 22:49:20,523][227910] Average Trajectory Length: 993.4523333333333[0m
[36m[2023-07-10 22:49:20,525][227910] mean_value=-278.7686774626387, max_value=2034.0294313777827[0m
[37m[1m[2023-07-10 22:49:20,528][227910] New mean coefficients: [[ 1.8271918   0.17873865  1.5952027  -0.6173972   1.586672  ]][0m
[37m[1m[2023-07-10 22:49:20,529][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:49:30,335][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 22:49:30,336][227910] FPS: 391647.88[0m
[36m[2023-07-10 22:49:30,338][227910] itr=1545, itrs=2000, Progress: 77.25%[0m
[36m[2023-07-10 22:49:42,133][227910] train() took 11.77 seconds to complete[0m
[36m[2023-07-10 22:49:42,133][227910] FPS: 326178.73[0m
[36m[2023-07-10 22:49:46,890][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:49:46,896][227910] Reward + Measures: [[2915.16603328    0.17948814    0.5601806     0.21253045    0.19521655]][0m
[37m[1m[2023-07-10 22:49:46,897][227910] Max Reward on eval: 2915.1660332752044[0m
[37m[1m[2023-07-10 22:49:46,897][227910] Min Reward on eval: 2915.1660332752044[0m
[37m[1m[2023-07-10 22:49:46,898][227910] Mean Reward across all agents: 2915.1660332752044[0m
[37m[1m[2023-07-10 22:49:46,898][227910] Average Trajectory Length: 982.0709999999999[0m
[36m[2023-07-10 22:49:52,370][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:49:52,370][227910] Reward + Measures: [[2948.83318703    0.1869        0.59990001    0.20570002    0.21360002]
 [2832.85150693    0.1894        0.5873        0.2086        0.21280001]
 [2942.1897411     0.17819999    0.57569999    0.2287        0.20200001]
 ...
 [3035.18265148    0.17510001    0.56829995    0.2119        0.20479999]
 [2909.66609159    0.18020001    0.57680005    0.2096        0.20629998]
 [2701.71475463    0.18771574    0.59003592    0.21961911    0.20243934]][0m
[37m[1m[2023-07-10 22:49:52,370][227910] Max Reward on eval: 3200.138489310816[0m
[37m[1m[2023-07-10 22:49:52,371][227910] Min Reward on eval: 2086.4905130153757[0m
[37m[1m[2023-07-10 22:49:52,371][227910] Mean Reward across all agents: 2841.1120075807617[0m
[37m[1m[2023-07-10 22:49:52,371][227910] Average Trajectory Length: 990.7483333333333[0m
[36m[2023-07-10 22:49:52,372][227910] mean_value=-472.20272827393995, max_value=235.19312208483916[0m
[37m[1m[2023-07-10 22:49:52,375][227910] New mean coefficients: [[ 1.2783089   0.30153742  1.5359545  -0.69895464  1.582869  ]][0m
[37m[1m[2023-07-10 22:49:52,376][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:50:02,077][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 22:50:02,077][227910] FPS: 395898.66[0m
[36m[2023-07-10 22:50:02,079][227910] itr=1546, itrs=2000, Progress: 77.30%[0m
[36m[2023-07-10 22:50:13,622][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 22:50:13,622][227910] FPS: 333354.59[0m
[36m[2023-07-10 22:50:18,322][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:50:18,323][227910] Reward + Measures: [[3081.43668722    0.17706811    0.57035357    0.21749471    0.18918355]][0m
[37m[1m[2023-07-10 22:50:18,323][227910] Max Reward on eval: 3081.436687217366[0m
[37m[1m[2023-07-10 22:50:18,323][227910] Min Reward on eval: 3081.436687217366[0m
[37m[1m[2023-07-10 22:50:18,323][227910] Mean Reward across all agents: 3081.436687217366[0m
[37m[1m[2023-07-10 22:50:18,323][227910] Average Trajectory Length: 991.56[0m
[36m[2023-07-10 22:50:23,628][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:50:23,629][227910] Reward + Measures: [[1291.49062571    0.27169999    0.44350001    0.26700002    0.3019    ]
 [ 749.88964527    0.3423        0.41990003    0.27469999    0.31160003]
 [1674.160499      0.23940001    0.58179998    0.21300001    0.2868    ]
 ...
 [ 779.93592778    0.32100001    0.4745        0.25580001    0.31010002]
 [1737.66422315    0.23580001    0.52270001    0.24679999    0.29499999]
 [1210.06847406    0.27130002    0.54619998    0.22299998    0.3053    ]][0m
[37m[1m[2023-07-10 22:50:23,629][227910] Max Reward on eval: 3091.1528607361483[0m
[37m[1m[2023-07-10 22:50:23,629][227910] Min Reward on eval: -38.56707684255671[0m
[37m[1m[2023-07-10 22:50:23,629][227910] Mean Reward across all agents: 1411.8480905695951[0m
[37m[1m[2023-07-10 22:50:23,630][227910] Average Trajectory Length: 997.3363333333333[0m
[36m[2023-07-10 22:50:23,631][227910] mean_value=-1252.1986755356681, max_value=-106.05062128255759[0m
[36m[2023-07-10 22:50:23,633][227910] XNES is restarting with a new solution whose measures are [0.24730001 0.63129997 0.3477     0.86210006] and objective is 525.1267356651136[0m
[36m[2023-07-10 22:50:23,634][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 22:50:23,636][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 22:50:23,637][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:50:33,248][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 22:50:33,248][227910] FPS: 399613.27[0m
[36m[2023-07-10 22:50:33,250][227910] itr=1547, itrs=2000, Progress: 77.35%[0m
[36m[2023-07-10 22:50:44,931][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 22:50:44,931][227910] FPS: 329310.74[0m
[36m[2023-07-10 22:50:49,772][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:50:49,772][227910] Reward + Measures: [[466.29662527   0.21253568   0.61370736   0.33465934   0.85643435]][0m
[37m[1m[2023-07-10 22:50:49,772][227910] Max Reward on eval: 466.2966252651625[0m
[37m[1m[2023-07-10 22:50:49,773][227910] Min Reward on eval: 466.2966252651625[0m
[37m[1m[2023-07-10 22:50:49,773][227910] Mean Reward across all agents: 466.2966252651625[0m
[37m[1m[2023-07-10 22:50:49,773][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:50:55,306][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:50:55,307][227910] Reward + Measures: [[ -272.00038202     0.4258         0.57700002     0.32090002
      0.54070002]
 [ -803.89180359     0.33020002     0.66300005     0.48890001
      0.51419997]
 [ -929.25265141     0.28670001     0.36199999     0.2999
      0.42010003]
 ...
 [   30.57892578     0.23239999     0.44280002     0.28240001
      0.65920001]
 [-1045.67436022     0.48429996     0.4851         0.60009998
      0.2561    ]
 [ -440.79994727     0.22187768     0.55746502     0.31052589
      0.63877755]][0m
[37m[1m[2023-07-10 22:50:55,307][227910] Max Reward on eval: 567.2146097221412[0m
[37m[1m[2023-07-10 22:50:55,307][227910] Min Reward on eval: -1729.5386629476677[0m
[37m[1m[2023-07-10 22:50:55,307][227910] Mean Reward across all agents: -569.8804163139323[0m
[37m[1m[2023-07-10 22:50:55,308][227910] Average Trajectory Length: 989.2136666666667[0m
[36m[2023-07-10 22:50:55,309][227910] mean_value=-1356.1066497303254, max_value=611.5597001713678[0m
[37m[1m[2023-07-10 22:50:55,312][227910] New mean coefficients: [[ 0.32117862 -0.02437651 -0.56087124 -0.78082585 -1.5742667 ]][0m
[37m[1m[2023-07-10 22:50:55,313][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:51:05,134][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 22:51:05,135][227910] FPS: 391043.68[0m
[36m[2023-07-10 22:51:05,137][227910] itr=1548, itrs=2000, Progress: 77.40%[0m
[36m[2023-07-10 22:51:16,822][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 22:51:16,822][227910] FPS: 329253.39[0m
[36m[2023-07-10 22:51:21,666][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:51:21,667][227910] Reward + Measures: [[401.37028032   0.27765235   0.48240733   0.29166666   0.79570705]][0m
[37m[1m[2023-07-10 22:51:21,667][227910] Max Reward on eval: 401.37028031853805[0m
[37m[1m[2023-07-10 22:51:21,667][227910] Min Reward on eval: 401.37028031853805[0m
[37m[1m[2023-07-10 22:51:21,667][227910] Mean Reward across all agents: 401.37028031853805[0m
[37m[1m[2023-07-10 22:51:21,668][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:51:27,196][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:51:27,197][227910] Reward + Measures: [[ -289.8050938      0.66139996     0.50740004     0.4937
      0.28210005]
 [ -577.38974582     0.57710004     0.1436         0.53920001
      0.44350001]
 [ -939.83833203     0.18343493     0.27410874     0.2536867
      0.37350523]
 ...
 [-1080.63187828     0.86219996     0.88370001     0.84380001
      0.86920005]
 [-1075.45898847     0.38120005     0.1091         0.31019998
      0.33059999]
 [ -134.83895037     0.5110001      0.36490002     0.43780002
      0.31689999]][0m
[37m[1m[2023-07-10 22:51:27,197][227910] Max Reward on eval: 373.82659198525363[0m
[37m[1m[2023-07-10 22:51:27,197][227910] Min Reward on eval: -1762.4829851748887[0m
[37m[1m[2023-07-10 22:51:27,198][227910] Mean Reward across all agents: -518.8547351599337[0m
[37m[1m[2023-07-10 22:51:27,198][227910] Average Trajectory Length: 973.386[0m
[36m[2023-07-10 22:51:27,200][227910] mean_value=-1304.4195811451123, max_value=664.1791895406109[0m
[37m[1m[2023-07-10 22:51:27,203][227910] New mean coefficients: [[ 0.04831976  1.2513131  -0.46514753 -0.3988856  -1.4212241 ]][0m
[37m[1m[2023-07-10 22:51:27,204][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:51:37,105][227910] train() took 9.90 seconds to complete[0m
[36m[2023-07-10 22:51:37,106][227910] FPS: 387885.00[0m
[36m[2023-07-10 22:51:37,108][227910] itr=1549, itrs=2000, Progress: 77.45%[0m
[36m[2023-07-10 22:51:48,732][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 22:51:48,732][227910] FPS: 330897.36[0m
[36m[2023-07-10 22:51:53,579][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:51:53,579][227910] Reward + Measures: [[339.63768136   0.28288466   0.41822734   0.27441499   0.71517664]][0m
[37m[1m[2023-07-10 22:51:53,580][227910] Max Reward on eval: 339.63768135895407[0m
[37m[1m[2023-07-10 22:51:53,580][227910] Min Reward on eval: 339.63768135895407[0m
[37m[1m[2023-07-10 22:51:53,580][227910] Mean Reward across all agents: 339.63768135895407[0m
[37m[1m[2023-07-10 22:51:53,580][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:51:59,057][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:51:59,058][227910] Reward + Measures: [[ -392.39781851     0.52829999     0.55360001     0.62029999
      0.7044    ]
 [ -607.14973289     0.50410002     0.3788         0.46940002
      0.4061    ]
 [ -589.62146946     0.45539999     0.67249995     0.3888
      0.79510003]
 ...
 [-1185.15724754     0.57969999     0.47020003     0.49990001
      0.31819999]
 [ -662.04486425     0.19555913     0.25517765     0.1900702
      0.19979671]
 [ -151.01836648     0.36430001     0.78000003     0.28260002
      0.85830003]][0m
[37m[1m[2023-07-10 22:51:59,058][227910] Max Reward on eval: 441.32463938206786[0m
[37m[1m[2023-07-10 22:51:59,058][227910] Min Reward on eval: -1993.5147727991803[0m
[37m[1m[2023-07-10 22:51:59,059][227910] Mean Reward across all agents: -546.4604159964562[0m
[37m[1m[2023-07-10 22:51:59,059][227910] Average Trajectory Length: 979.867[0m
[36m[2023-07-10 22:51:59,060][227910] mean_value=-1273.8788645761724, max_value=614.9358912282889[0m
[37m[1m[2023-07-10 22:51:59,063][227910] New mean coefficients: [[-0.40185365  0.58130234  0.02530262 -0.26736805 -0.8636204 ]][0m
[37m[1m[2023-07-10 22:51:59,064][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:52:08,791][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 22:52:08,792][227910] FPS: 394808.63[0m
[36m[2023-07-10 22:52:08,794][227910] itr=1550, itrs=2000, Progress: 77.50%[0m
[37m[1m[2023-07-10 22:52:12,898][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001530[0m
[36m[2023-07-10 22:52:24,831][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 22:52:24,832][227910] FPS: 329531.82[0m
[36m[2023-07-10 22:52:29,664][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:52:29,664][227910] Reward + Measures: [[87.81260209  0.35366935  0.41316968  0.12091966  0.66343302]][0m
[37m[1m[2023-07-10 22:52:29,665][227910] Max Reward on eval: 87.81260208931981[0m
[37m[1m[2023-07-10 22:52:29,665][227910] Min Reward on eval: 87.81260208931981[0m
[37m[1m[2023-07-10 22:52:29,665][227910] Mean Reward across all agents: 87.81260208931981[0m
[37m[1m[2023-07-10 22:52:29,666][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:52:35,192][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:52:35,193][227910] Reward + Measures: [[-2060.43723789     0.69670004     0.83470005     0.0156
      0.85180008]
 [ -434.41499143     0.73110002     0.75209999     0.75310004
      0.17990001]
 [-1672.4856647      0.37360001     0.43930003     0.54040003
      0.60740006]
 ...
 [  -91.11306124     0.47350001     0.48249999     0.30129999
      0.5711    ]
 [    9.74657864     0.29250002     0.31750003     0.18470001
      0.5927    ]
 [ -131.46488819     0.21960001     0.69070005     0.28940001
      0.67250007]][0m
[37m[1m[2023-07-10 22:52:35,193][227910] Max Reward on eval: 369.41817554178414[0m
[37m[1m[2023-07-10 22:52:35,193][227910] Min Reward on eval: -2271.792270174227[0m
[37m[1m[2023-07-10 22:52:35,193][227910] Mean Reward across all agents: -552.49693657452[0m
[37m[1m[2023-07-10 22:52:35,194][227910] Average Trajectory Length: 957.4889999999999[0m
[36m[2023-07-10 22:52:35,196][227910] mean_value=-1451.809308590876, max_value=516.2925042145206[0m
[37m[1m[2023-07-10 22:52:35,199][227910] New mean coefficients: [[-0.21373676  0.00934196  0.02737825  0.02565613 -0.5445403 ]][0m
[37m[1m[2023-07-10 22:52:35,200][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:52:44,983][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 22:52:44,983][227910] FPS: 392569.81[0m
[36m[2023-07-10 22:52:44,986][227910] itr=1551, itrs=2000, Progress: 77.55%[0m
[36m[2023-07-10 22:52:56,538][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 22:52:56,538][227910] FPS: 332974.54[0m
[36m[2023-07-10 22:53:01,305][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:53:01,306][227910] Reward + Measures: [[-71.88493402   0.50469708   0.5053798    0.06448132   0.69694954]][0m
[37m[1m[2023-07-10 22:53:01,306][227910] Max Reward on eval: -71.88493401599156[0m
[37m[1m[2023-07-10 22:53:01,306][227910] Min Reward on eval: -71.88493401599156[0m
[37m[1m[2023-07-10 22:53:01,306][227910] Mean Reward across all agents: -71.88493401599156[0m
[37m[1m[2023-07-10 22:53:01,306][227910] Average Trajectory Length: 999.783[0m
[36m[2023-07-10 22:53:06,702][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:53:06,703][227910] Reward + Measures: [[-145.44444134    0.2832        0.2665        0.0949        0.43169999]
 [-201.35122076    0.32070002    0.32430002    0.3457        0.30250001]
 [-292.18215482    0.5025        0.49870005    0.42540002    0.33310002]
 ...
 [  55.19503716    0.55599999    0.58990002    0.07120001    0.68079996]
 [   7.00417336    0.36920002    0.55109996    0.1099        0.62599999]
 [-142.2087451     0.29440001    0.48789999    0.15580001    0.47040001]][0m
[37m[1m[2023-07-10 22:53:06,703][227910] Max Reward on eval: 149.7646837622451[0m
[37m[1m[2023-07-10 22:53:06,703][227910] Min Reward on eval: -1502.6530156806227[0m
[37m[1m[2023-07-10 22:53:06,703][227910] Mean Reward across all agents: -278.47047172222636[0m
[37m[1m[2023-07-10 22:53:06,704][227910] Average Trajectory Length: 998.1229999999999[0m
[36m[2023-07-10 22:53:06,706][227910] mean_value=-1145.031990458119, max_value=538.9953820017022[0m
[37m[1m[2023-07-10 22:53:06,709][227910] New mean coefficients: [[ 0.72088444 -0.15528615 -0.54769856  0.9361278  -0.01262617]][0m
[37m[1m[2023-07-10 22:53:06,710][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:53:16,448][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 22:53:16,448][227910] FPS: 394386.30[0m
[36m[2023-07-10 22:53:16,451][227910] itr=1552, itrs=2000, Progress: 77.60%[0m
[36m[2023-07-10 22:53:28,237][227910] train() took 11.77 seconds to complete[0m
[36m[2023-07-10 22:53:28,237][227910] FPS: 326321.59[0m
[36m[2023-07-10 22:53:32,984][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:53:32,984][227910] Reward + Measures: [[-54.43123602   0.36027598   0.42963201   0.11082701   0.66676432]][0m
[37m[1m[2023-07-10 22:53:32,985][227910] Max Reward on eval: -54.43123602286017[0m
[37m[1m[2023-07-10 22:53:32,985][227910] Min Reward on eval: -54.43123602286017[0m
[37m[1m[2023-07-10 22:53:32,985][227910] Mean Reward across all agents: -54.43123602286017[0m
[37m[1m[2023-07-10 22:53:32,985][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:53:38,409][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:53:38,409][227910] Reward + Measures: [[ -423.89173494     0.4172         0.54870003     0.22369997
      0.47609997]
 [ -220.45033265     0.31959999     0.41640002     0.3513
      0.46559998]
 [-1040.88729534     0.44310004     0.1523         0.43309999
      0.27110001]
 ...
 [ -767.31773984     0.47399998     0.41409999     0.66949999
      0.68769997]
 [-1357.96294718     0.67510003     0.46490002     0.56529999
      0.5007    ]
 [ -575.38192957     0.4923         0.42139998     0.58380002
      0.61790001]][0m
[37m[1m[2023-07-10 22:53:38,409][227910] Max Reward on eval: 81.32723280385544[0m
[37m[1m[2023-07-10 22:53:38,410][227910] Min Reward on eval: -2346.822975652013[0m
[37m[1m[2023-07-10 22:53:38,410][227910] Mean Reward across all agents: -655.2532467817196[0m
[37m[1m[2023-07-10 22:53:38,410][227910] Average Trajectory Length: 994.3306666666666[0m
[36m[2023-07-10 22:53:38,412][227910] mean_value=-1232.1199838272653, max_value=330.3345147945633[0m
[37m[1m[2023-07-10 22:53:38,414][227910] New mean coefficients: [[-0.11224037  0.31676656 -0.17719412  0.54474545  0.52247816]][0m
[37m[1m[2023-07-10 22:53:38,415][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:53:48,032][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 22:53:48,032][227910] FPS: 399394.76[0m
[36m[2023-07-10 22:53:48,034][227910] itr=1553, itrs=2000, Progress: 77.65%[0m
[36m[2023-07-10 22:53:59,585][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 22:53:59,585][227910] FPS: 333092.67[0m
[36m[2023-07-10 22:54:04,405][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:54:04,405][227910] Reward + Measures: [[-63.21189388   0.35759634   0.42129564   0.11259001   0.66266936]][0m
[37m[1m[2023-07-10 22:54:04,406][227910] Max Reward on eval: -63.21189387691252[0m
[37m[1m[2023-07-10 22:54:04,406][227910] Min Reward on eval: -63.21189387691252[0m
[37m[1m[2023-07-10 22:54:04,406][227910] Mean Reward across all agents: -63.21189387691252[0m
[37m[1m[2023-07-10 22:54:04,406][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:54:09,927][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:54:09,933][227910] Reward + Measures: [[  95.14596956    0.37890002    0.51630002    0.1628        0.74949998]
 [ -74.01712902    0.38690001    0.45030004    0.11870001    0.73359996]
 [-204.00558413    0.3752        0.39180002    0.09599999    0.67089999]
 ...
 [-165.41887802    0.39320001    0.4851        0.1211        0.77640003]
 [-469.37313496    0.36900002    0.43269998    0.0951        0.6049    ]
 [-306.85195571    0.26850003    0.28819999    0.10399999    0.45339999]][0m
[37m[1m[2023-07-10 22:54:09,933][227910] Max Reward on eval: 129.32211010768953[0m
[37m[1m[2023-07-10 22:54:09,934][227910] Min Reward on eval: -1098.6790836963337[0m
[37m[1m[2023-07-10 22:54:09,934][227910] Mean Reward across all agents: -273.9741006029088[0m
[37m[1m[2023-07-10 22:54:09,934][227910] Average Trajectory Length: 996.1943333333332[0m
[36m[2023-07-10 22:54:09,937][227910] mean_value=-434.23062905753517, max_value=382.03207302444105[0m
[37m[1m[2023-07-10 22:54:09,940][227910] New mean coefficients: [[-0.01707988  0.8134068  -0.2620495   0.18270701  0.58465415]][0m
[37m[1m[2023-07-10 22:54:09,941][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:54:19,608][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 22:54:19,608][227910] FPS: 397290.02[0m
[36m[2023-07-10 22:54:19,610][227910] itr=1554, itrs=2000, Progress: 77.70%[0m
[36m[2023-07-10 22:54:31,222][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 22:54:31,223][227910] FPS: 331218.35[0m
[36m[2023-07-10 22:54:36,007][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:54:36,007][227910] Reward + Measures: [[-86.65890015   0.38133365   0.42950797   0.109708     0.69187969]][0m
[37m[1m[2023-07-10 22:54:36,008][227910] Max Reward on eval: -86.65890014705496[0m
[37m[1m[2023-07-10 22:54:36,008][227910] Min Reward on eval: -86.65890014705496[0m
[37m[1m[2023-07-10 22:54:36,008][227910] Mean Reward across all agents: -86.65890014705496[0m
[37m[1m[2023-07-10 22:54:36,008][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:54:41,692][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:54:41,693][227910] Reward + Measures: [[-212.35545032    0.40470001    0.46700001    0.1195        0.77069998]
 [-241.27119166    0.3233        0.53829998    0.16970001    0.76480007]
 [-247.90022225    0.30130002    0.49920002    0.18419999    0.73610002]
 ...
 [ -48.67642303    0.32300001    0.4962        0.161         0.73340005]
 [-196.99258384    0.25750002    0.52150005    0.1944        0.70349997]
 [-124.9377363     0.40760002    0.4673        0.10919999    0.73789996]][0m
[37m[1m[2023-07-10 22:54:41,693][227910] Max Reward on eval: 101.23019026069669[0m
[37m[1m[2023-07-10 22:54:41,693][227910] Min Reward on eval: -1463.4498729053187[0m
[37m[1m[2023-07-10 22:54:41,694][227910] Mean Reward across all agents: -193.84215579535183[0m
[37m[1m[2023-07-10 22:54:41,694][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:54:41,697][227910] mean_value=-273.0712127160719, max_value=556.4700210332189[0m
[37m[1m[2023-07-10 22:54:41,700][227910] New mean coefficients: [[-0.7140556   1.6296072   0.24886936  0.00177328  0.5628672 ]][0m
[37m[1m[2023-07-10 22:54:41,701][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:54:51,523][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 22:54:51,523][227910] FPS: 391014.61[0m
[36m[2023-07-10 22:54:51,526][227910] itr=1555, itrs=2000, Progress: 77.75%[0m
[36m[2023-07-10 22:55:03,205][227910] train() took 11.66 seconds to complete[0m
[36m[2023-07-10 22:55:03,205][227910] FPS: 329374.81[0m
[36m[2023-07-10 22:55:07,936][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:55:07,936][227910] Reward + Measures: [[-141.99458626    0.40371335    0.45223367    0.10772733    0.72442603]][0m
[37m[1m[2023-07-10 22:55:07,937][227910] Max Reward on eval: -141.99458625968737[0m
[37m[1m[2023-07-10 22:55:07,937][227910] Min Reward on eval: -141.99458625968737[0m
[37m[1m[2023-07-10 22:55:07,937][227910] Mean Reward across all agents: -141.99458625968737[0m
[37m[1m[2023-07-10 22:55:07,937][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:55:13,312][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:55:13,313][227910] Reward + Measures: [[-146.30515141    0.39049998    0.3872        0.10150001    0.63490003]
 [ -14.66892721    0.34460002    0.39019999    0.13          0.65950006]
 [-219.25462473    0.45250002    0.4711        0.09940001    0.75640005]
 ...
 [-130.67786644    0.345         0.34639999    0.09429999    0.55410004]
 [-242.29152124    0.4578        0.48269996    0.1035        0.76989996]
 [-236.38098599    0.42470002    0.42340001    0.0981        0.71130008]][0m
[37m[1m[2023-07-10 22:55:13,313][227910] Max Reward on eval: 107.09311905226204[0m
[37m[1m[2023-07-10 22:55:13,313][227910] Min Reward on eval: -385.4563014911837[0m
[37m[1m[2023-07-10 22:55:13,313][227910] Mean Reward across all agents: -145.76796003115967[0m
[37m[1m[2023-07-10 22:55:13,314][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:55:13,317][227910] mean_value=-32.8320825777022, max_value=483.0786791902859[0m
[37m[1m[2023-07-10 22:55:13,320][227910] New mean coefficients: [[-0.56877065  2.686847    0.5060693  -0.20262322 -0.44024736]][0m
[37m[1m[2023-07-10 22:55:13,321][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:55:22,919][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 22:55:22,920][227910] FPS: 400151.41[0m
[36m[2023-07-10 22:55:22,922][227910] itr=1556, itrs=2000, Progress: 77.80%[0m
[36m[2023-07-10 22:55:34,374][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 22:55:34,374][227910] FPS: 335961.11[0m
[36m[2023-07-10 22:55:39,089][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:55:39,090][227910] Reward + Measures: [[-190.27554007    0.43612832    0.47690764    0.09867899    0.74460638]][0m
[37m[1m[2023-07-10 22:55:39,090][227910] Max Reward on eval: -190.2755400689018[0m
[37m[1m[2023-07-10 22:55:39,090][227910] Min Reward on eval: -190.2755400689018[0m
[37m[1m[2023-07-10 22:55:39,091][227910] Mean Reward across all agents: -190.2755400689018[0m
[37m[1m[2023-07-10 22:55:39,091][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:55:44,499][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:55:44,500][227910] Reward + Measures: [[-260.09822737    0.46240002    0.49130002    0.0962        0.77530003]
 [-268.17982943    0.44000003    0.44370005    0.1067        0.71069998]
 [-149.99227905    0.40060002    0.43490002    0.0993        0.64779997]
 ...
 [-333.57226741    0.4777        0.50009996    0.103         0.7913    ]
 [-339.97102204    0.47479996    0.47880003    0.08220001    0.72930002]
 [-356.81519316    0.48100001    0.58050001    0.1416        0.84839994]][0m
[37m[1m[2023-07-10 22:55:44,500][227910] Max Reward on eval: 37.679520672775105[0m
[37m[1m[2023-07-10 22:55:44,500][227910] Min Reward on eval: -468.2648355579295[0m
[37m[1m[2023-07-10 22:55:44,501][227910] Mean Reward across all agents: -228.22118836958995[0m
[37m[1m[2023-07-10 22:55:44,501][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:55:44,503][227910] mean_value=-163.5485077739215, max_value=342.6506813763059[0m
[37m[1m[2023-07-10 22:55:44,506][227910] New mean coefficients: [[-0.20240885  2.7993495   0.11639577  0.4025096  -1.9086514 ]][0m
[37m[1m[2023-07-10 22:55:44,507][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:55:54,225][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 22:55:54,225][227910] FPS: 395220.38[0m
[36m[2023-07-10 22:55:54,227][227910] itr=1557, itrs=2000, Progress: 77.85%[0m
[36m[2023-07-10 22:56:05,717][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 22:56:05,718][227910] FPS: 334744.04[0m
[36m[2023-07-10 22:56:10,458][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:56:10,459][227910] Reward + Measures: [[-273.4300114     0.78680795    0.81101722    0.66816932    0.92306703]][0m
[37m[1m[2023-07-10 22:56:10,459][227910] Max Reward on eval: -273.43001139903055[0m
[37m[1m[2023-07-10 22:56:10,459][227910] Min Reward on eval: -273.43001139903055[0m
[37m[1m[2023-07-10 22:56:10,459][227910] Mean Reward across all agents: -273.43001139903055[0m
[37m[1m[2023-07-10 22:56:10,460][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:56:15,869][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:56:15,875][227910] Reward + Measures: [[-249.85864694    0.77319998    0.78180003    0.66170001    0.90610009]
 [-460.55255116    0.70130002    0.74519998    0.47010002    0.90430003]
 [-296.8193306     0.75220007    0.778         0.55100006    0.91250002]
 ...
 [-281.14979815    0.75089997    0.8247        0.48519999    0.93400002]
 [-484.45263377    0.61390001    0.69329995    0.31820002    0.91480011]
 [-424.46572346    0.73219997    0.76330006    0.52679998    0.91660005]][0m
[37m[1m[2023-07-10 22:56:15,876][227910] Max Reward on eval: -88.29553554345038[0m
[37m[1m[2023-07-10 22:56:15,877][227910] Min Reward on eval: -591.6761163298739[0m
[37m[1m[2023-07-10 22:56:15,877][227910] Mean Reward across all agents: -342.9784256285332[0m
[37m[1m[2023-07-10 22:56:15,878][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:56:15,881][227910] mean_value=-751.3952795441963, max_value=32.43315275354587[0m
[37m[1m[2023-07-10 22:56:15,886][227910] New mean coefficients: [[-0.1233937   2.258615   -0.04731777  0.54555917 -3.5967658 ]][0m
[37m[1m[2023-07-10 22:56:15,887][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:56:25,493][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 22:56:25,494][227910] FPS: 399844.75[0m
[36m[2023-07-10 22:56:25,496][227910] itr=1558, itrs=2000, Progress: 77.90%[0m
[36m[2023-07-10 22:56:37,199][227910] train() took 11.68 seconds to complete[0m
[36m[2023-07-10 22:56:37,199][227910] FPS: 328675.92[0m
[36m[2023-07-10 22:56:41,978][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:56:41,979][227910] Reward + Measures: [[-238.40462122    0.81521267    0.80037767    0.68968862    0.91698736]][0m
[37m[1m[2023-07-10 22:56:41,979][227910] Max Reward on eval: -238.40462122418856[0m
[37m[1m[2023-07-10 22:56:41,979][227910] Min Reward on eval: -238.40462122418856[0m
[37m[1m[2023-07-10 22:56:41,980][227910] Mean Reward across all agents: -238.40462122418856[0m
[37m[1m[2023-07-10 22:56:41,980][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:56:47,507][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:56:47,508][227910] Reward + Measures: [[-179.91202145    0.86910003    0.83530009    0.78479999    0.92070007]
 [-249.61220671    0.72910005    0.6573        0.6103        0.84159994]
 [-140.47197718    0.72200006    0.76770002    0.54089999    0.90679997]
 ...
 [-291.52668605    0.86129999    0.80100006    0.78290004    0.91640007]
 [-243.44456028    0.80050004    0.85810006    0.68020004    0.9424001 ]
 [-217.9289898     0.83190006    0.76880008    0.73590004    0.89880002]][0m
[37m[1m[2023-07-10 22:56:47,508][227910] Max Reward on eval: -61.97778251136187[0m
[37m[1m[2023-07-10 22:56:47,508][227910] Min Reward on eval: -541.5585198974353[0m
[37m[1m[2023-07-10 22:56:47,508][227910] Mean Reward across all agents: -229.5869409421381[0m
[37m[1m[2023-07-10 22:56:47,508][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:56:47,509][227910] mean_value=-741.8145132917849, max_value=-131.789401656662[0m
[36m[2023-07-10 22:56:47,512][227910] XNES is restarting with a new solution whose measures are [0.52060002 0.20780002 0.79190004 0.50770003] and objective is -226.66833262318977[0m
[36m[2023-07-10 22:56:47,513][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 22:56:47,515][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 22:56:47,516][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:56:57,504][227910] train() took 9.99 seconds to complete[0m
[36m[2023-07-10 22:56:57,504][227910] FPS: 384510.93[0m
[36m[2023-07-10 22:56:57,506][227910] itr=1559, itrs=2000, Progress: 77.95%[0m
[36m[2023-07-10 22:57:09,139][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 22:57:09,139][227910] FPS: 330675.07[0m
[36m[2023-07-10 22:57:13,961][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:57:13,961][227910] Reward + Measures: [[-400.04207179    0.47825089    0.27176163    0.64688408    0.45179704]][0m
[37m[1m[2023-07-10 22:57:13,961][227910] Max Reward on eval: -400.0420717854317[0m
[37m[1m[2023-07-10 22:57:13,962][227910] Min Reward on eval: -400.0420717854317[0m
[37m[1m[2023-07-10 22:57:13,962][227910] Mean Reward across all agents: -400.0420717854317[0m
[37m[1m[2023-07-10 22:57:13,962][227910] Average Trajectory Length: 989.2253333333333[0m
[36m[2023-07-10 22:57:19,558][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:57:19,559][227910] Reward + Measures: [[ -975.1727472      0.41570002     0.30379999     0.62919998
      0.4887    ]
 [-2085.94727971     0.72000003     0.57960004     0.71240002
      0.59610003]
 [-1036.03642021     0.16979507     0.25108176     0.34394893
      0.22329538]
 ...
 [-1799.97326303     0.72099996     0.72390002     0.75509995
      0.71270001]
 [-1809.87762564     0.76835376     0.13002837     0.78922093
      0.67617315]
 [-1209.56381709     0.26044336     0.42657456     0.46992597
      0.52269948]][0m
[37m[1m[2023-07-10 22:57:19,559][227910] Max Reward on eval: -142.6431992806145[0m
[37m[1m[2023-07-10 22:57:19,559][227910] Min Reward on eval: -2271.330730471737[0m
[37m[1m[2023-07-10 22:57:19,560][227910] Mean Reward across all agents: -1198.0829604440403[0m
[37m[1m[2023-07-10 22:57:19,560][227910] Average Trajectory Length: 901.1569999999999[0m
[36m[2023-07-10 22:57:19,561][227910] mean_value=-2568.616316026356, max_value=-284.3889849161154[0m
[36m[2023-07-10 22:57:19,563][227910] XNES is restarting with a new solution whose measures are [0.3348     0.676      0.51160002 0.31090003] and objective is 671.9367170543992[0m
[36m[2023-07-10 22:57:19,565][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 22:57:19,567][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 22:57:19,568][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:57:29,285][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 22:57:29,285][227910] FPS: 395248.96[0m
[36m[2023-07-10 22:57:29,287][227910] itr=1560, itrs=2000, Progress: 78.00%[0m
[37m[1m[2023-07-10 22:57:33,206][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001540[0m
[36m[2023-07-10 22:57:45,060][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 22:57:45,061][227910] FPS: 331646.24[0m
[36m[2023-07-10 22:57:49,861][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:57:49,862][227910] Reward + Measures: [[593.31918504   0.28186134   0.70691097   0.14527901   0.44525763]][0m
[37m[1m[2023-07-10 22:57:49,862][227910] Max Reward on eval: 593.3191850429964[0m
[37m[1m[2023-07-10 22:57:49,862][227910] Min Reward on eval: 593.3191850429964[0m
[37m[1m[2023-07-10 22:57:49,862][227910] Mean Reward across all agents: 593.3191850429964[0m
[37m[1m[2023-07-10 22:57:49,863][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:57:55,574][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:57:55,575][227910] Reward + Measures: [[  535.52970926     0.1142         0.36660001     0.38750002
      0.33680001]
 [ -585.35973358     0.0267         0.78929996     0.68430007
      0.71359998]
 [ -887.24600337     0.63373512     0.62437147     0.26043376
      0.68395066]
 ...
 [ -414.31545248     0.25733009     0.27045149     0.25607187
      0.35426798]
 [-1068.03527911     0.29370001     0.35350001     0.46900001
      0.2182    ]
 [-1008.88539712     0.3744677      0.48254901     0.12060835
      0.52608442]][0m
[37m[1m[2023-07-10 22:57:55,575][227910] Max Reward on eval: 1592.0106145799741[0m
[37m[1m[2023-07-10 22:57:55,575][227910] Min Reward on eval: -1763.469715240551[0m
[37m[1m[2023-07-10 22:57:55,575][227910] Mean Reward across all agents: -684.5849593861582[0m
[37m[1m[2023-07-10 22:57:55,576][227910] Average Trajectory Length: 883.785[0m
[36m[2023-07-10 22:57:55,577][227910] mean_value=-2162.7398623628847, max_value=563.6555508529212[0m
[37m[1m[2023-07-10 22:57:55,580][227910] New mean coefficients: [[ 0.75381625 -0.26702467 -1.1062018  -1.864047   -0.6950517 ]][0m
[37m[1m[2023-07-10 22:57:55,581][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:58:05,386][227910] train() took 9.80 seconds to complete[0m
[36m[2023-07-10 22:58:05,386][227910] FPS: 391682.25[0m
[36m[2023-07-10 22:58:05,389][227910] itr=1561, itrs=2000, Progress: 78.05%[0m
[36m[2023-07-10 22:58:17,010][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 22:58:17,010][227910] FPS: 331024.25[0m
[36m[2023-07-10 22:58:21,687][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:58:21,687][227910] Reward + Measures: [[845.0513707    0.169966     0.52961099   0.26704699   0.33364132]][0m
[37m[1m[2023-07-10 22:58:21,687][227910] Max Reward on eval: 845.051370699985[0m
[37m[1m[2023-07-10 22:58:21,688][227910] Min Reward on eval: 845.051370699985[0m
[37m[1m[2023-07-10 22:58:21,688][227910] Mean Reward across all agents: 845.051370699985[0m
[37m[1m[2023-07-10 22:58:21,688][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:58:27,161][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:58:27,162][227910] Reward + Measures: [[-1164.6762916      0.4368135      0.28912166     0.48599768
      0.17956786]
 [ -611.43133731     0.2416722      0.52771235     0.43565607
      0.51483405]
 [ -690.10163049     0.6354         0.2026         0.73030001
      0.71139997]
 ...
 [-1253.38329285     0.4081566      0.32671648     0.43749666
      0.31315729]
 [-1578.86546489     0.36621928     0.50583214     0.38492188
      0.36096948]
 [  128.23613685     0.4003         0.389          0.47569999
      0.35089999]][0m
[37m[1m[2023-07-10 22:58:27,162][227910] Max Reward on eval: 1078.0615669863182[0m
[37m[1m[2023-07-10 22:58:27,162][227910] Min Reward on eval: -1972.3776373500004[0m
[37m[1m[2023-07-10 22:58:27,162][227910] Mean Reward across all agents: -732.8249202498083[0m
[37m[1m[2023-07-10 22:58:27,163][227910] Average Trajectory Length: 899.6126666666667[0m
[36m[2023-07-10 22:58:27,164][227910] mean_value=-2377.4640030222804, max_value=513.5223236712698[0m
[37m[1m[2023-07-10 22:58:27,166][227910] New mean coefficients: [[ 1.2672081  -0.47822914 -0.69679105 -1.1514469  -1.3716371 ]][0m
[37m[1m[2023-07-10 22:58:27,167][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:58:36,852][227910] train() took 9.68 seconds to complete[0m
[36m[2023-07-10 22:58:36,852][227910] FPS: 396574.31[0m
[36m[2023-07-10 22:58:36,854][227910] itr=1562, itrs=2000, Progress: 78.10%[0m
[36m[2023-07-10 22:58:48,458][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 22:58:48,458][227910] FPS: 331463.96[0m
[36m[2023-07-10 22:58:53,186][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:58:53,187][227910] Reward + Measures: [[1262.59748325    0.11913       0.62450397    0.21209967    0.30997366]][0m
[37m[1m[2023-07-10 22:58:53,187][227910] Max Reward on eval: 1262.597483249623[0m
[37m[1m[2023-07-10 22:58:53,187][227910] Min Reward on eval: 1262.597483249623[0m
[37m[1m[2023-07-10 22:58:53,187][227910] Mean Reward across all agents: 1262.597483249623[0m
[37m[1m[2023-07-10 22:58:53,188][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:58:58,644][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:58:58,645][227910] Reward + Measures: [[  -81.87091434     0.32390001     0.77010006     0.57120001
      0.57429999]
 [ -360.46037251     0.24969998     0.7464         0.2696
      0.70630008]
 [   96.22864645     0.41319999     0.59919995     0.20030001
      0.55809999]
 ...
 [-1195.93909927     0.74299997     0.78729999     0.74710006
      0.82420009]
 [ -243.81181274     0.4664         0.7902         0.63200009
      0.64020002]
 [ -794.01268678     0.17957516     0.33507791     0.23625858
      0.25177336]][0m
[37m[1m[2023-07-10 22:58:58,645][227910] Max Reward on eval: 1448.5082531337393[0m
[37m[1m[2023-07-10 22:58:58,645][227910] Min Reward on eval: -2396.454115607962[0m
[37m[1m[2023-07-10 22:58:58,645][227910] Mean Reward across all agents: -433.8193946135082[0m
[37m[1m[2023-07-10 22:58:58,646][227910] Average Trajectory Length: 927.4193333333333[0m
[36m[2023-07-10 22:58:58,647][227910] mean_value=-2221.9943460706877, max_value=538.828217290106[0m
[37m[1m[2023-07-10 22:58:58,649][227910] New mean coefficients: [[ 1.4372928  -0.6271365  -0.34446916 -0.31304133 -0.830325  ]][0m
[37m[1m[2023-07-10 22:58:58,650][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:59:08,327][227910] train() took 9.67 seconds to complete[0m
[36m[2023-07-10 22:59:08,327][227910] FPS: 396914.13[0m
[36m[2023-07-10 22:59:08,329][227910] itr=1563, itrs=2000, Progress: 78.15%[0m
[36m[2023-07-10 22:59:19,858][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 22:59:19,859][227910] FPS: 333600.16[0m
[36m[2023-07-10 22:59:24,654][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:59:24,654][227910] Reward + Measures: [[1406.28742241    0.14830567    0.54322666    0.45622632    0.30917066]][0m
[37m[1m[2023-07-10 22:59:24,654][227910] Max Reward on eval: 1406.2874224112456[0m
[37m[1m[2023-07-10 22:59:24,655][227910] Min Reward on eval: 1406.2874224112456[0m
[37m[1m[2023-07-10 22:59:24,655][227910] Mean Reward across all agents: 1406.2874224112456[0m
[37m[1m[2023-07-10 22:59:24,655][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 22:59:30,045][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:59:30,046][227910] Reward + Measures: [[ -435.77179374     0.30779311     0.20398621     0.36981726
      0.21516208]
 [  293.96247798     0.1365         0.25750002     0.22739999
      0.2024    ]
 [-1308.79866153     0.57407868     0.2870073      0.60222369
      0.47502947]
 ...
 [-1040.06307522     0.22124425     0.28940663     0.25019953
      0.19374414]
 [ -267.50377031     0.25750002     0.3348         0.32370004
      0.30520001]
 [ -341.08294395     0.23173332     0.30063334     0.37983337
      0.22266667]][0m
[37m[1m[2023-07-10 22:59:30,046][227910] Max Reward on eval: 1356.8975351636298[0m
[37m[1m[2023-07-10 22:59:30,046][227910] Min Reward on eval: -1916.2406518482603[0m
[37m[1m[2023-07-10 22:59:30,046][227910] Mean Reward across all agents: -351.78380412125625[0m
[37m[1m[2023-07-10 22:59:30,047][227910] Average Trajectory Length: 977.2876666666666[0m
[36m[2023-07-10 22:59:30,048][227910] mean_value=-2025.4985337248872, max_value=516.4275686417968[0m
[37m[1m[2023-07-10 22:59:30,051][227910] New mean coefficients: [[ 1.360172   -1.1915252   0.0168106  -0.10553646  0.01639354]][0m
[37m[1m[2023-07-10 22:59:30,052][227910] Moving the mean solution point...[0m
[36m[2023-07-10 22:59:39,756][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 22:59:39,756][227910] FPS: 395759.79[0m
[36m[2023-07-10 22:59:39,758][227910] itr=1564, itrs=2000, Progress: 78.20%[0m
[36m[2023-07-10 22:59:51,276][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 22:59:51,276][227910] FPS: 333989.51[0m
[36m[2023-07-10 22:59:56,014][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 22:59:56,014][227910] Reward + Measures: [[1166.95742227    0.10797307    0.38463339    0.46769217    0.30391875]][0m
[37m[1m[2023-07-10 22:59:56,015][227910] Max Reward on eval: 1166.9574222685728[0m
[37m[1m[2023-07-10 22:59:56,015][227910] Min Reward on eval: 1166.9574222685728[0m
[37m[1m[2023-07-10 22:59:56,015][227910] Mean Reward across all agents: 1166.9574222685728[0m
[37m[1m[2023-07-10 22:59:56,015][227910] Average Trajectory Length: 980.2073333333333[0m
[36m[2023-07-10 23:00:01,412][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:00:01,412][227910] Reward + Measures: [[  269.92708805     0.3048         0.3881         0.35630003
      0.26359999]
 [   47.49967618     0.1814         0.27420002     0.30699998
      0.2669    ]
 [ -251.27209252     0.1532         0.244          0.26139998
      0.18030001]
 ...
 [  596.52032189     0.12547298     0.38715407     0.39704326
      0.29156756]
 [ -893.23630546     0.19721922     0.22381194     0.22426346
      0.12715761]
 [-1116.72447247     0.1813         0.18070002     0.2148
      0.15109999]][0m
[37m[1m[2023-07-10 23:00:01,412][227910] Max Reward on eval: 1055.210574406886[0m
[37m[1m[2023-07-10 23:00:01,413][227910] Min Reward on eval: -1558.5797819875122[0m
[37m[1m[2023-07-10 23:00:01,413][227910] Mean Reward across all agents: -283.24969440175414[0m
[37m[1m[2023-07-10 23:00:01,413][227910] Average Trajectory Length: 966.6323333333333[0m
[36m[2023-07-10 23:00:01,415][227910] mean_value=-2187.582045944256, max_value=828.5481937985776[0m
[37m[1m[2023-07-10 23:00:01,417][227910] New mean coefficients: [[ 1.5279534  -0.698274    0.8638811   0.85464966  0.8925503 ]][0m
[37m[1m[2023-07-10 23:00:01,418][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:00:11,236][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 23:00:11,236][227910] FPS: 391181.74[0m
[36m[2023-07-10 23:00:11,239][227910] itr=1565, itrs=2000, Progress: 78.25%[0m
[36m[2023-07-10 23:00:22,980][227910] train() took 11.72 seconds to complete[0m
[36m[2023-07-10 23:00:22,981][227910] FPS: 327571.44[0m
[36m[2023-07-10 23:00:27,789][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:00:27,789][227910] Reward + Measures: [[1266.95932685    0.10181995    0.37113866    0.4658868     0.29174492]][0m
[37m[1m[2023-07-10 23:00:27,789][227910] Max Reward on eval: 1266.9593268494516[0m
[37m[1m[2023-07-10 23:00:27,789][227910] Min Reward on eval: 1266.9593268494516[0m
[37m[1m[2023-07-10 23:00:27,789][227910] Mean Reward across all agents: 1266.9593268494516[0m
[37m[1m[2023-07-10 23:00:27,790][227910] Average Trajectory Length: 982.2703333333333[0m
[36m[2023-07-10 23:00:33,543][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:00:33,544][227910] Reward + Measures: [[ -811.91651784     0.33669445     0.33853889     0.29920003
      0.31360558]
 [  444.71655749     0.2084         0.45000002     0.41009998
      0.34770003]
 [-1144.14356189     0.69960004     0.75340003     0.69270009
      0.73859996]
 ...
 [ -437.31629135     0.43729997     0.65350002     0.30970001
      0.565     ]
 [ -872.51726419     0.22656631     0.17172192     0.1775258
      0.1622411 ]
 [ -455.82087743     0.61549997     0.63079995     0.64130002
      0.29010001]][0m
[37m[1m[2023-07-10 23:00:33,544][227910] Max Reward on eval: 1167.1412045805366[0m
[37m[1m[2023-07-10 23:00:33,544][227910] Min Reward on eval: -1938.8851811909583[0m
[37m[1m[2023-07-10 23:00:33,544][227910] Mean Reward across all agents: -352.8836569431118[0m
[37m[1m[2023-07-10 23:00:33,545][227910] Average Trajectory Length: 966.5106666666667[0m
[36m[2023-07-10 23:00:33,547][227910] mean_value=-1732.378611804237, max_value=1530.9111886994197[0m
[37m[1m[2023-07-10 23:00:33,549][227910] New mean coefficients: [[ 1.6119344   0.24604487 -0.03508246  0.7305958   0.64304405]][0m
[37m[1m[2023-07-10 23:00:33,550][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:00:43,484][227910] train() took 9.93 seconds to complete[0m
[36m[2023-07-10 23:00:43,484][227910] FPS: 386638.22[0m
[36m[2023-07-10 23:00:43,486][227910] itr=1566, itrs=2000, Progress: 78.30%[0m
[36m[2023-07-10 23:00:55,276][227910] train() took 11.77 seconds to complete[0m
[36m[2023-07-10 23:00:55,276][227910] FPS: 326228.56[0m
[36m[2023-07-10 23:01:00,023][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:01:00,023][227910] Reward + Measures: [[529.92735597   0.38403574   0.4137356    0.51818675   0.28619644]][0m
[37m[1m[2023-07-10 23:01:00,023][227910] Max Reward on eval: 529.9273559686123[0m
[37m[1m[2023-07-10 23:01:00,023][227910] Min Reward on eval: 529.9273559686123[0m
[37m[1m[2023-07-10 23:01:00,024][227910] Mean Reward across all agents: 529.9273559686123[0m
[37m[1m[2023-07-10 23:01:00,024][227910] Average Trajectory Length: 998.6573333333333[0m
[36m[2023-07-10 23:01:05,432][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:01:05,433][227910] Reward + Measures: [[-981.0804518     0.60979998    0.57989997    0.42829999    0.54089999]
 [-249.76040668    0.35549998    0.3705        0.36690003    0.3179    ]
 [ 121.11816455    0.40790001    0.40450001    0.47089997    0.38510004]
 ...
 [  94.05284742    0.56159997    0.32950002    0.52280003    0.3944    ]
 [-779.35420674    0.25470003    0.5733        0.4436        0.52250004]
 [ -66.04203848    0.41680002    0.32479998    0.42319998    0.35310003]][0m
[37m[1m[2023-07-10 23:01:05,433][227910] Max Reward on eval: 443.51569575558534[0m
[37m[1m[2023-07-10 23:01:05,433][227910] Min Reward on eval: -1670.445196594065[0m
[37m[1m[2023-07-10 23:01:05,434][227910] Mean Reward across all agents: -389.635137415434[0m
[37m[1m[2023-07-10 23:01:05,434][227910] Average Trajectory Length: 953.257[0m
[36m[2023-07-10 23:01:05,435][227910] mean_value=-2138.3258858546765, max_value=142.72854427777142[0m
[37m[1m[2023-07-10 23:01:05,438][227910] New mean coefficients: [[-0.18053484  1.3546482   0.0613011   0.87325835  0.96943915]][0m
[37m[1m[2023-07-10 23:01:05,439][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:01:15,194][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 23:01:15,194][227910] FPS: 393716.05[0m
[36m[2023-07-10 23:01:15,196][227910] itr=1567, itrs=2000, Progress: 78.35%[0m
[36m[2023-07-10 23:01:26,932][227910] train() took 11.72 seconds to complete[0m
[36m[2023-07-10 23:01:26,932][227910] FPS: 327727.17[0m
[36m[2023-07-10 23:01:31,757][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:01:31,758][227910] Reward + Measures: [[464.46484596   0.33104694   0.40645263   0.43201643   0.28353846]][0m
[37m[1m[2023-07-10 23:01:31,758][227910] Max Reward on eval: 464.4648459607843[0m
[37m[1m[2023-07-10 23:01:31,758][227910] Min Reward on eval: 464.4648459607843[0m
[37m[1m[2023-07-10 23:01:31,759][227910] Mean Reward across all agents: 464.4648459607843[0m
[37m[1m[2023-07-10 23:01:31,759][227910] Average Trajectory Length: 997.3086666666667[0m
[36m[2023-07-10 23:01:37,347][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:01:37,348][227910] Reward + Measures: [[ 360.13358125    0.27729997    0.37420002    0.35279998    0.2764    ]
 [ 316.95981189    0.28548238    0.42002377    0.37269163    0.22454801]
 [-685.40030691    0.12228382    0.18750313    0.13570185    0.11515345]
 ...
 [-416.1942939     0.34059998    0.53249997    0.49439999    0.57069999]
 [-333.75507222    0.18486507    0.2804516     0.31898496    0.25226939]
 [ -51.12733658    0.55419999    0.65250003    0.64710003    0.63970006]][0m
[37m[1m[2023-07-10 23:01:37,348][227910] Max Reward on eval: 641.5608930360875[0m
[37m[1m[2023-07-10 23:01:37,348][227910] Min Reward on eval: -1596.4522721042972[0m
[37m[1m[2023-07-10 23:01:37,349][227910] Mean Reward across all agents: -410.2985319539933[0m
[37m[1m[2023-07-10 23:01:37,349][227910] Average Trajectory Length: 929.2093333333333[0m
[36m[2023-07-10 23:01:37,350][227910] mean_value=-2482.4887859052633, max_value=-145.23623099770163[0m
[36m[2023-07-10 23:01:37,352][227910] XNES is restarting with a new solution whose measures are [0.82240003 0.8276     0.92089999 0.0055    ] and objective is -281.94232798804876[0m
[36m[2023-07-10 23:01:37,353][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 23:01:37,356][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 23:01:37,357][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:01:47,329][227910] train() took 9.97 seconds to complete[0m
[36m[2023-07-10 23:01:47,329][227910] FPS: 385123.03[0m
[36m[2023-07-10 23:01:47,331][227910] itr=1568, itrs=2000, Progress: 78.40%[0m
[36m[2023-07-10 23:01:59,023][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 23:01:59,024][227910] FPS: 328956.39[0m
[36m[2023-07-10 23:02:03,914][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:02:03,914][227910] Reward + Measures: [[-281.71017557    0.31553409    0.34705266    0.44299108    0.13274129]][0m
[37m[1m[2023-07-10 23:02:03,914][227910] Max Reward on eval: -281.7101755676723[0m
[37m[1m[2023-07-10 23:02:03,915][227910] Min Reward on eval: -281.7101755676723[0m
[37m[1m[2023-07-10 23:02:03,915][227910] Mean Reward across all agents: -281.7101755676723[0m
[37m[1m[2023-07-10 23:02:03,915][227910] Average Trajectory Length: 997.7216666666666[0m
[36m[2023-07-10 23:02:09,398][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:02:09,399][227910] Reward + Measures: [[ -496.86132973     0.44250003     0.454          0.60500002
      0.09850001]
 [-1377.71399217     0.1797         0.6706         0.50080001
      0.67400002]
 [ -493.67352428     0.31810001     0.44589996     0.27700004
      0.42460003]
 ...
 [ -884.91557363     0.56860006     0.1425         0.65289998
      0.361     ]
 [-1501.77072404     0.39480001     0.41780001     0.4804
      0.52579999]
 [-1139.32401876     0.74430007     0.51560003     0.65210003
      0.0704    ]][0m
[37m[1m[2023-07-10 23:02:09,399][227910] Max Reward on eval: -30.235603245417586[0m
[37m[1m[2023-07-10 23:02:09,399][227910] Min Reward on eval: -1923.1570641672122[0m
[37m[1m[2023-07-10 23:02:09,400][227910] Mean Reward across all agents: -762.8021058004123[0m
[37m[1m[2023-07-10 23:02:09,400][227910] Average Trajectory Length: 976.4576666666667[0m
[36m[2023-07-10 23:02:09,401][227910] mean_value=-2162.0481384711443, max_value=-298.11863269486696[0m
[36m[2023-07-10 23:02:09,403][227910] XNES is restarting with a new solution whose measures are [0.30449998 0.79619998 0.38649997 0.2888    ] and objective is 1499.2917918310966[0m
[36m[2023-07-10 23:02:09,404][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 23:02:09,407][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 23:02:09,407][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:02:19,189][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 23:02:19,189][227910] FPS: 392632.77[0m
[36m[2023-07-10 23:02:19,192][227910] itr=1569, itrs=2000, Progress: 78.45%[0m
[36m[2023-07-10 23:02:30,739][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 23:02:30,739][227910] FPS: 333185.95[0m
[36m[2023-07-10 23:02:35,596][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:02:35,601][227910] Reward + Measures: [[370.61462838   0.56370068   0.60983902   0.75520134   0.224389  ]][0m
[37m[1m[2023-07-10 23:02:35,601][227910] Max Reward on eval: 370.61462837523453[0m
[37m[1m[2023-07-10 23:02:35,602][227910] Min Reward on eval: 370.61462837523453[0m
[37m[1m[2023-07-10 23:02:35,602][227910] Mean Reward across all agents: 370.61462837523453[0m
[37m[1m[2023-07-10 23:02:35,602][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:02:41,086][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:02:41,087][227910] Reward + Measures: [[ -560.0799595      0.5456         0.31310001     0.67550004
      0.46490002]
 [-2026.10787942     0.64579999     0.47020003     0.51010001
      0.15750001]
 [-1576.96505484     0.55164665     0.50107968     0.31113553
      0.392939  ]
 ...
 [-2174.09120156     0.65570003     0.51179999     0.6415
      0.0878    ]
 [-1866.07582021     0.39232877     0.52465749     0.51015753
      0.21793699]
 [-2305.92803927     0.58090007     0.41599998     0.74770004
      0.41529998]][0m
[37m[1m[2023-07-10 23:02:41,087][227910] Max Reward on eval: 243.66750602239046[0m
[37m[1m[2023-07-10 23:02:41,087][227910] Min Reward on eval: -3333.402189528197[0m
[37m[1m[2023-07-10 23:02:41,087][227910] Mean Reward across all agents: -1506.366635141895[0m
[37m[1m[2023-07-10 23:02:41,088][227910] Average Trajectory Length: 962.055[0m
[36m[2023-07-10 23:02:41,089][227910] mean_value=-2658.189884662313, max_value=135.58018849019766[0m
[37m[1m[2023-07-10 23:02:41,092][227910] New mean coefficients: [[ 0.00192574  0.06650454 -0.1881218  -1.2937243  -1.4022617 ]][0m
[37m[1m[2023-07-10 23:02:41,092][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:02:50,797][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 23:02:50,797][227910] FPS: 395777.31[0m
[36m[2023-07-10 23:02:50,799][227910] itr=1570, itrs=2000, Progress: 78.50%[0m
[37m[1m[2023-07-10 23:02:54,875][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001550[0m
[36m[2023-07-10 23:03:06,682][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 23:03:06,687][227910] FPS: 332947.96[0m
[36m[2023-07-10 23:03:11,516][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:03:11,516][227910] Reward + Measures: [[-18.52531613   0.45454165   0.92884064   0.01376      0.89375633]][0m
[37m[1m[2023-07-10 23:03:11,517][227910] Max Reward on eval: -18.525316129918284[0m
[37m[1m[2023-07-10 23:03:11,517][227910] Min Reward on eval: -18.525316129918284[0m
[37m[1m[2023-07-10 23:03:11,517][227910] Mean Reward across all agents: -18.525316129918284[0m
[37m[1m[2023-07-10 23:03:11,517][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:03:17,003][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:03:17,004][227910] Reward + Measures: [[-1424.65021778     0.3265         0.45950004     0.30389997
      0.45040002]
 [-1358.41004066     0.79960006     0.39940003     0.75340003
      0.15350001]
 [-1629.96217499     0.47399998     0.30490002     0.47610003
      0.28200001]
 ...
 [ -633.69462742     0.2342         0.40910003     0.24340001
      0.29800004]
 [-1983.39754048     0.50640005     0.72469997     0.75920004
      0.35170004]
 [-1027.08885891     0.21159999     0.76310003     0.21900001
      0.8217001 ]][0m
[37m[1m[2023-07-10 23:03:17,004][227910] Max Reward on eval: 1103.6416979541682[0m
[37m[1m[2023-07-10 23:03:17,005][227910] Min Reward on eval: -2430.9600112169983[0m
[37m[1m[2023-07-10 23:03:17,005][227910] Mean Reward across all agents: -975.7263433524097[0m
[37m[1m[2023-07-10 23:03:17,005][227910] Average Trajectory Length: 990.3256666666666[0m
[36m[2023-07-10 23:03:17,007][227910] mean_value=-1980.693531055204, max_value=350.9737644526068[0m
[37m[1m[2023-07-10 23:03:17,009][227910] New mean coefficients: [[ 0.09509927  0.5874777  -0.22819953 -0.49140817 -0.14988613]][0m
[37m[1m[2023-07-10 23:03:17,010][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:03:26,890][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 23:03:26,890][227910] FPS: 388726.06[0m
[36m[2023-07-10 23:03:26,893][227910] itr=1571, itrs=2000, Progress: 78.55%[0m
[36m[2023-07-10 23:03:38,477][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 23:03:38,477][227910] FPS: 332179.96[0m
[36m[2023-07-10 23:03:43,243][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:03:43,243][227910] Reward + Measures: [[-118.39730759    0.47804597    0.82272702    0.05417667    0.79880726]][0m
[37m[1m[2023-07-10 23:03:43,243][227910] Max Reward on eval: -118.39730759207505[0m
[37m[1m[2023-07-10 23:03:43,244][227910] Min Reward on eval: -118.39730759207505[0m
[37m[1m[2023-07-10 23:03:43,244][227910] Mean Reward across all agents: -118.39730759207505[0m
[37m[1m[2023-07-10 23:03:43,244][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:03:48,779][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:03:48,780][227910] Reward + Measures: [[ -719.00017213     0.75260001     0.75520003     0.57130003
      0.81560004]
 [ -942.86703083     0.24024971     0.30689868     0.30035862
      0.36190706]
 [-1095.40608611     0.0803         0.69379997     0.33810002
      0.75739998]
 ...
 [-1560.65659596     0.67000002     0.84679997     0.30090004
      0.88560003]
 [  346.09693375     0.36469999     0.4822         0.37
      0.479     ]
 [  -85.5844206      0.34199998     0.54440004     0.29350001
      0.58680004]][0m
[37m[1m[2023-07-10 23:03:48,780][227910] Max Reward on eval: 621.5755062796176[0m
[37m[1m[2023-07-10 23:03:48,780][227910] Min Reward on eval: -2100.729523456469[0m
[37m[1m[2023-07-10 23:03:48,781][227910] Mean Reward across all agents: -668.0900339406292[0m
[37m[1m[2023-07-10 23:03:48,781][227910] Average Trajectory Length: 983.7683333333333[0m
[36m[2023-07-10 23:03:48,782][227910] mean_value=-1910.3974022558389, max_value=191.9589443499106[0m
[37m[1m[2023-07-10 23:03:48,785][227910] New mean coefficients: [[ 0.62781686  0.41099826  0.6957878  -0.36041713 -0.9699139 ]][0m
[37m[1m[2023-07-10 23:03:48,786][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:03:58,429][227910] train() took 9.64 seconds to complete[0m
[36m[2023-07-10 23:03:58,429][227910] FPS: 398278.80[0m
[36m[2023-07-10 23:03:58,431][227910] itr=1572, itrs=2000, Progress: 78.60%[0m
[36m[2023-07-10 23:04:10,063][227910] train() took 11.61 seconds to complete[0m
[36m[2023-07-10 23:04:10,063][227910] FPS: 330687.75[0m
[36m[2023-07-10 23:04:14,826][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:04:14,827][227910] Reward + Measures: [[234.6438267    0.40020004   0.59671468   0.34144965   0.486837  ]][0m
[37m[1m[2023-07-10 23:04:14,827][227910] Max Reward on eval: 234.64382669747786[0m
[37m[1m[2023-07-10 23:04:14,827][227910] Min Reward on eval: 234.64382669747786[0m
[37m[1m[2023-07-10 23:04:14,827][227910] Mean Reward across all agents: 234.64382669747786[0m
[37m[1m[2023-07-10 23:04:14,828][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:04:20,319][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:04:20,320][227910] Reward + Measures: [[-1126.16385851     0.2316         0.4052         0.34510002
      0.44960004]
 [  -16.81032897     0.32780001     0.41780004     0.32520002
      0.37380001]
 [-1486.34215631     0.57054394     0.61466527     0.13793398
      0.47321516]
 ...
 [-1534.92958032     0.27554414     0.15901129     0.32665715
      0.19047165]
 [ -216.35997886     0.15176356     0.43858981     0.1715851
      0.29832196]
 [-1177.16263364     0.40220004     0.16620001     0.44590002
      0.23720001]][0m
[37m[1m[2023-07-10 23:04:20,320][227910] Max Reward on eval: 647.8000394125818[0m
[37m[1m[2023-07-10 23:04:20,320][227910] Min Reward on eval: -2028.048961937707[0m
[37m[1m[2023-07-10 23:04:20,321][227910] Mean Reward across all agents: -816.2205136267733[0m
[37m[1m[2023-07-10 23:04:20,321][227910] Average Trajectory Length: 949.6996666666666[0m
[36m[2023-07-10 23:04:20,323][227910] mean_value=-2134.9100924724107, max_value=431.5545437260065[0m
[37m[1m[2023-07-10 23:04:20,325][227910] New mean coefficients: [[ 0.59704936  0.23217425  0.91841334 -0.09711105 -0.6943929 ]][0m
[37m[1m[2023-07-10 23:04:20,326][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:04:30,042][227910] train() took 9.71 seconds to complete[0m
[36m[2023-07-10 23:04:30,043][227910] FPS: 395281.50[0m
[36m[2023-07-10 23:04:30,045][227910] itr=1573, itrs=2000, Progress: 78.65%[0m
[36m[2023-07-10 23:04:41,691][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 23:04:41,691][227910] FPS: 330311.48[0m
[36m[2023-07-10 23:04:46,417][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:04:46,423][227910] Reward + Measures: [[612.46948676   0.174612     0.52655071   0.31062999   0.46996897]][0m
[37m[1m[2023-07-10 23:04:46,423][227910] Max Reward on eval: 612.4694867622907[0m
[37m[1m[2023-07-10 23:04:46,423][227910] Min Reward on eval: 612.4694867622907[0m
[37m[1m[2023-07-10 23:04:46,423][227910] Mean Reward across all agents: 612.4694867622907[0m
[37m[1m[2023-07-10 23:04:46,424][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:04:51,786][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:04:51,792][227910] Reward + Measures: [[ -556.07733387     0.47269997     0.44940001     0.49939999
      0.22650002]
 [-1290.91845561     0.33071706     0.52750844     0.30784315
      0.52796888]
 [ -778.72208308     0.34889999     0.38960001     0.44759998
      0.2357    ]
 ...
 [  218.55875579     0.42910001     0.57580006     0.48850003
      0.40580001]
 [-1440.469544       0.16743147     0.21141045     0.21633868
      0.15891626]
 [-1100.22076093     0.1441         0.2184         0.24230002
      0.24450003]][0m
[37m[1m[2023-07-10 23:04:51,792][227910] Max Reward on eval: 736.4541419895365[0m
[37m[1m[2023-07-10 23:04:51,793][227910] Min Reward on eval: -2295.2462039338193[0m
[37m[1m[2023-07-10 23:04:51,793][227910] Mean Reward across all agents: -747.2126570920595[0m
[37m[1m[2023-07-10 23:04:51,793][227910] Average Trajectory Length: 966.6946666666666[0m
[36m[2023-07-10 23:04:51,794][227910] mean_value=-2112.7769573587125, max_value=-37.62923249771512[0m
[36m[2023-07-10 23:04:51,797][227910] XNES is restarting with a new solution whose measures are [0.2256     0.82800007 0.56610006 0.84790003] and objective is 657.2556279312121[0m
[36m[2023-07-10 23:04:51,798][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 23:04:51,800][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 23:04:51,801][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:05:01,460][227910] train() took 9.66 seconds to complete[0m
[36m[2023-07-10 23:05:01,460][227910] FPS: 397610.06[0m
[36m[2023-07-10 23:05:01,463][227910] itr=1574, itrs=2000, Progress: 78.70%[0m
[36m[2023-07-10 23:05:13,088][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 23:05:13,088][227910] FPS: 330923.69[0m
[36m[2023-07-10 23:05:17,936][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:05:17,936][227910] Reward + Measures: [[270.10042444   0.72361034   0.25178164   0.78522706   0.63348132]][0m
[37m[1m[2023-07-10 23:05:17,936][227910] Max Reward on eval: 270.1004244396013[0m
[37m[1m[2023-07-10 23:05:17,937][227910] Min Reward on eval: 270.1004244396013[0m
[37m[1m[2023-07-10 23:05:17,937][227910] Mean Reward across all agents: 270.1004244396013[0m
[37m[1m[2023-07-10 23:05:17,937][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:05:23,519][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:05:23,520][227910] Reward + Measures: [[ -173.08539043     0.64188331     0.41261664     0.77189165
      0.16794167]
 [-1307.47304546     0.46790001     0.48950002     0.4298
      0.42610002]
 [ -767.31530988     0.3428041      0.25685915     0.33175308
      0.39259386]
 ...
 [  185.13511514     0.73100001     0.73520005     0.78239995
      0.3874    ]
 [-1469.87818117     0.32066539     0.32580766     0.26498955
      0.27328953]
 [-1860.62476493     0.57171541     0.39453462     0.71192312
      0.68038076]][0m
[37m[1m[2023-07-10 23:05:23,520][227910] Max Reward on eval: 694.51008094243[0m
[37m[1m[2023-07-10 23:05:23,520][227910] Min Reward on eval: -2505.0863011349925[0m
[37m[1m[2023-07-10 23:05:23,521][227910] Mean Reward across all agents: -697.537823368989[0m
[37m[1m[2023-07-10 23:05:23,521][227910] Average Trajectory Length: 972.4306666666666[0m
[36m[2023-07-10 23:05:23,523][227910] mean_value=-1843.2179611636263, max_value=485.8273674617611[0m
[37m[1m[2023-07-10 23:05:23,525][227910] New mean coefficients: [[ 0.8165395  -0.37903905 -1.2188916  -0.4537443  -1.1903675 ]][0m
[37m[1m[2023-07-10 23:05:23,526][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:05:33,390][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 23:05:33,391][227910] FPS: 389350.47[0m
[36m[2023-07-10 23:05:33,393][227910] itr=1575, itrs=2000, Progress: 78.75%[0m
[36m[2023-07-10 23:05:45,016][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 23:05:45,016][227910] FPS: 331030.84[0m
[36m[2023-07-10 23:05:49,808][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:05:49,809][227910] Reward + Measures: [[283.84135114   0.70498407   0.28319398   0.77099413   0.45431739]][0m
[37m[1m[2023-07-10 23:05:49,809][227910] Max Reward on eval: 283.84135114433656[0m
[37m[1m[2023-07-10 23:05:49,809][227910] Min Reward on eval: 283.84135114433656[0m
[37m[1m[2023-07-10 23:05:49,809][227910] Mean Reward across all agents: 283.84135114433656[0m
[37m[1m[2023-07-10 23:05:49,809][227910] Average Trajectory Length: 999.755[0m
[36m[2023-07-10 23:05:55,287][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:05:55,287][227910] Reward + Measures: [[ -402.92634086     0.82980007     0.90780002     0.05
      0.88430005]
 [-1577.15545337     0.55360001     0.13250001     0.68110001
      0.58270001]
 [ -917.56538749     0.20939998     0.1516         0.1578
      0.2105    ]
 ...
 [ -877.39761643     0.30720001     0.47539997     0.19820002
      0.47930002]
 [ -698.72997458     0.19492565     0.20345023     0.20968449
      0.17762253]
 [  -93.21513995     0.22790001     0.39320001     0.15609999
      0.43129998]][0m
[37m[1m[2023-07-10 23:05:55,287][227910] Max Reward on eval: 1115.2565464366228[0m
[37m[1m[2023-07-10 23:05:55,288][227910] Min Reward on eval: -2797.8315702249297[0m
[37m[1m[2023-07-10 23:05:55,288][227910] Mean Reward across all agents: -860.6985345455115[0m
[37m[1m[2023-07-10 23:05:55,288][227910] Average Trajectory Length: 985.4773333333333[0m
[36m[2023-07-10 23:05:55,290][227910] mean_value=-1646.9774811610152, max_value=469.9067052346308[0m
[37m[1m[2023-07-10 23:05:55,292][227910] New mean coefficients: [[ 1.4225667  -0.11048278 -0.4899811   0.44464523 -0.4943545 ]][0m
[37m[1m[2023-07-10 23:05:55,293][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:06:05,046][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 23:06:05,046][227910] FPS: 393813.58[0m
[36m[2023-07-10 23:06:05,048][227910] itr=1576, itrs=2000, Progress: 78.80%[0m
[36m[2023-07-10 23:06:16,526][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 23:06:16,526][227910] FPS: 335124.46[0m
[36m[2023-07-10 23:06:21,306][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:06:21,312][227910] Reward + Measures: [[309.62050349   0.72130632   0.22027999   0.7840026    0.41290098]][0m
[37m[1m[2023-07-10 23:06:21,312][227910] Max Reward on eval: 309.6205034919685[0m
[37m[1m[2023-07-10 23:06:21,312][227910] Min Reward on eval: 309.6205034919685[0m
[37m[1m[2023-07-10 23:06:21,312][227910] Mean Reward across all agents: 309.6205034919685[0m
[37m[1m[2023-07-10 23:06:21,313][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:06:27,000][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:06:27,006][227910] Reward + Measures: [[  69.53916275    0.56089997    0.61140001    0.41800004    0.64560002]
 [-261.49596175    0.21430002    0.84810001    0.38430002    0.84120005]
 [ 541.01145452    0.2282        0.71130002    0.2431        0.47860003]
 ...
 [-732.69049059    0.44159999    0.50749999    0.3238        0.34580001]
 [-958.37415705    0.26350001    0.77450001    0.0612        0.58520001]
 [ 386.0112283     0.2599        0.64729995    0.19670001    0.49260002]][0m
[37m[1m[2023-07-10 23:06:27,006][227910] Max Reward on eval: 541.011454520101[0m
[37m[1m[2023-07-10 23:06:27,007][227910] Min Reward on eval: -1973.3491040624679[0m
[37m[1m[2023-07-10 23:06:27,007][227910] Mean Reward across all agents: -291.53608374473805[0m
[37m[1m[2023-07-10 23:06:27,007][227910] Average Trajectory Length: 999.7743333333333[0m
[36m[2023-07-10 23:06:27,009][227910] mean_value=-727.4031383960469, max_value=611.5278260524894[0m
[37m[1m[2023-07-10 23:06:27,012][227910] New mean coefficients: [[ 1.1605551  -0.50930905 -0.20332259  0.51016027 -0.03847733]][0m
[37m[1m[2023-07-10 23:06:27,013][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:06:36,872][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 23:06:36,872][227910] FPS: 389551.81[0m
[36m[2023-07-10 23:06:36,875][227910] itr=1577, itrs=2000, Progress: 78.85%[0m
[36m[2023-07-10 23:06:48,415][227910] train() took 11.52 seconds to complete[0m
[36m[2023-07-10 23:06:48,415][227910] FPS: 333385.55[0m
[36m[2023-07-10 23:06:53,163][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:06:53,163][227910] Reward + Measures: [[356.34640058   0.69905704   0.21711434   0.76664597   0.38865098]][0m
[37m[1m[2023-07-10 23:06:53,164][227910] Max Reward on eval: 356.34640058370167[0m
[37m[1m[2023-07-10 23:06:53,164][227910] Min Reward on eval: 356.34640058370167[0m
[37m[1m[2023-07-10 23:06:53,164][227910] Mean Reward across all agents: 356.34640058370167[0m
[37m[1m[2023-07-10 23:06:53,164][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:06:58,684][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:06:58,684][227910] Reward + Measures: [[ -254.57843361     0.72570002     0.89589995     0.1147
      0.89830011]
 [   80.41376009     0.51319999     0.28280002     0.46740004
      0.23830001]
 [ -234.28059098     0.37390003     0.52340001     0.74970001
      0.73449999]
 ...
 [  151.02370573     0.45269999     0.35600004     0.40690002
      0.21300001]
 [ -385.3766486      0.34980002     0.32969999     0.42750001
      0.28749999]
 [-1389.84451466     0.55250001     0.77860004     0.16919999
      0.73260003]][0m
[37m[1m[2023-07-10 23:06:58,685][227910] Max Reward on eval: 603.6500651337789[0m
[37m[1m[2023-07-10 23:06:58,685][227910] Min Reward on eval: -1892.523230368551[0m
[37m[1m[2023-07-10 23:06:58,685][227910] Mean Reward across all agents: -413.82795884869887[0m
[37m[1m[2023-07-10 23:06:58,685][227910] Average Trajectory Length: 991.947[0m
[36m[2023-07-10 23:06:58,687][227910] mean_value=-1401.0525945533345, max_value=444.74726699104536[0m
[37m[1m[2023-07-10 23:06:58,690][227910] New mean coefficients: [[ 1.3079844  -0.63012457  0.21470201  0.5851103   0.5960444 ]][0m
[37m[1m[2023-07-10 23:06:58,691][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:07:08,345][227910] train() took 9.65 seconds to complete[0m
[36m[2023-07-10 23:07:08,345][227910] FPS: 397834.14[0m
[36m[2023-07-10 23:07:08,347][227910] itr=1578, itrs=2000, Progress: 78.90%[0m
[36m[2023-07-10 23:07:19,849][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 23:07:19,850][227910] FPS: 334423.33[0m
[36m[2023-07-10 23:07:24,640][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:07:24,641][227910] Reward + Measures: [[407.0814992    0.68909031   0.22097266   0.75108933   0.46633935]][0m
[37m[1m[2023-07-10 23:07:24,641][227910] Max Reward on eval: 407.0814992034607[0m
[37m[1m[2023-07-10 23:07:24,641][227910] Min Reward on eval: 407.0814992034607[0m
[37m[1m[2023-07-10 23:07:24,642][227910] Mean Reward across all agents: 407.0814992034607[0m
[37m[1m[2023-07-10 23:07:24,642][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:07:30,159][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:07:30,160][227910] Reward + Measures: [[   18.36900751     0.15930001     0.75489998     0.1497
      0.78560001]
 [-1222.76145486     0.92000008     0.43170005     0.86479998
      0.46760002]
 [ -146.3335982      0.38709998     0.72429997     0.0608
      0.78210002]
 ...
 [  624.75527915     0.31060001     0.78010005     0.47510001
      0.61379999]
 [ -967.72762292     0.32339999     0.33329996     0.35409999
      0.4693    ]
 [ -413.6972021      0.25910002     0.41759998     0.30169997
      0.46970001]][0m
[37m[1m[2023-07-10 23:07:30,160][227910] Max Reward on eval: 1149.1744333198992[0m
[37m[1m[2023-07-10 23:07:30,161][227910] Min Reward on eval: -2946.394508387707[0m
[37m[1m[2023-07-10 23:07:30,161][227910] Mean Reward across all agents: -285.38456144795003[0m
[37m[1m[2023-07-10 23:07:30,161][227910] Average Trajectory Length: 997.9876666666667[0m
[36m[2023-07-10 23:07:30,163][227910] mean_value=-900.9197534050944, max_value=986.0279166207434[0m
[37m[1m[2023-07-10 23:07:30,166][227910] New mean coefficients: [[ 1.496425   -1.0932472  -0.0812633   0.84691715  0.5657111 ]][0m
[37m[1m[2023-07-10 23:07:30,167][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:07:39,954][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 23:07:39,954][227910] FPS: 392430.91[0m
[36m[2023-07-10 23:07:39,956][227910] itr=1579, itrs=2000, Progress: 78.95%[0m
[36m[2023-07-10 23:07:51,629][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 23:07:51,629][227910] FPS: 329533.53[0m
[36m[2023-07-10 23:07:56,438][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:07:56,439][227910] Reward + Measures: [[453.87521506   0.61753798   0.30529201   0.67475832   0.56986338]][0m
[37m[1m[2023-07-10 23:07:56,439][227910] Max Reward on eval: 453.8752150614344[0m
[37m[1m[2023-07-10 23:07:56,439][227910] Min Reward on eval: 453.8752150614344[0m
[37m[1m[2023-07-10 23:07:56,440][227910] Mean Reward across all agents: 453.8752150614344[0m
[37m[1m[2023-07-10 23:07:56,440][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:08:01,942][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:08:01,942][227910] Reward + Measures: [[ -753.35556706     0.29910001     0.5399         0.25580001
      0.41370001]
 [  141.90911819     0.78509998     0.57679999     0.65560001
      0.542     ]
 [  -28.82703623     0.58879995     0.64490002     0.3461
      0.57090002]
 ...
 [  611.33611796     0.42290002     0.57960004     0.51000005
      0.49579999]
 [-1549.39149986     0.26139998     0.565          0.16039999
      0.59790003]
 [  608.9023141      0.26279998     0.73210001     0.345
      0.67989999]][0m
[37m[1m[2023-07-10 23:08:01,942][227910] Max Reward on eval: 780.5826834189705[0m
[37m[1m[2023-07-10 23:08:01,943][227910] Min Reward on eval: -1674.731705868477[0m
[37m[1m[2023-07-10 23:08:01,943][227910] Mean Reward across all agents: -300.95490573402367[0m
[37m[1m[2023-07-10 23:08:01,943][227910] Average Trajectory Length: 998.7366666666667[0m
[36m[2023-07-10 23:08:01,946][227910] mean_value=-911.0638946490116, max_value=717.760609018047[0m
[37m[1m[2023-07-10 23:08:01,948][227910] New mean coefficients: [[ 0.52028555 -0.3129478  -0.86453736  0.76329666  0.72058535]][0m
[37m[1m[2023-07-10 23:08:01,949][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:08:11,789][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 23:08:11,789][227910] FPS: 390316.12[0m
[36m[2023-07-10 23:08:11,792][227910] itr=1580, itrs=2000, Progress: 79.00%[0m
[37m[1m[2023-07-10 23:08:15,996][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001560[0m
[36m[2023-07-10 23:08:28,142][227910] train() took 11.87 seconds to complete[0m
[36m[2023-07-10 23:08:28,142][227910] FPS: 323584.26[0m
[36m[2023-07-10 23:08:32,879][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:08:32,880][227910] Reward + Measures: [[522.34163744   0.58421201   0.31015798   0.64758497   0.60523999]][0m
[37m[1m[2023-07-10 23:08:32,880][227910] Max Reward on eval: 522.3416374377067[0m
[37m[1m[2023-07-10 23:08:32,880][227910] Min Reward on eval: 522.3416374377067[0m
[37m[1m[2023-07-10 23:08:32,881][227910] Mean Reward across all agents: 522.3416374377067[0m
[37m[1m[2023-07-10 23:08:32,881][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:08:38,388][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:08:38,388][227910] Reward + Measures: [[ 751.11147686    0.47910005    0.32360002    0.4639        0.3125    ]
 [-463.87013153    0.72180003    0.1415        0.75099999    0.77530003]
 [-431.72137476    0.68699998    0.24600001    0.70469999    0.80179995]
 ...
 [ 512.84815987    0.39019999    0.70960003    0.1846        0.65879995]
 [-840.06111192    0.71169996    0.18880001    0.71720004    0.77310002]
 [ 563.96047225    0.25120002    0.81099999    0.29800001    0.76449996]][0m
[37m[1m[2023-07-10 23:08:38,389][227910] Max Reward on eval: 938.9749031158747[0m
[37m[1m[2023-07-10 23:08:38,389][227910] Min Reward on eval: -1646.2321914870874[0m
[37m[1m[2023-07-10 23:08:38,389][227910] Mean Reward across all agents: -64.8793373742851[0m
[37m[1m[2023-07-10 23:08:38,389][227910] Average Trajectory Length: 996.4486666666667[0m
[36m[2023-07-10 23:08:38,392][227910] mean_value=-702.7462846686311, max_value=759.8709411825403[0m
[37m[1m[2023-07-10 23:08:38,395][227910] New mean coefficients: [[ 0.62408656 -0.13415934 -0.38259107  1.0729502   1.4799883 ]][0m
[37m[1m[2023-07-10 23:08:38,396][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:08:48,127][227910] train() took 9.73 seconds to complete[0m
[36m[2023-07-10 23:08:48,127][227910] FPS: 394660.79[0m
[36m[2023-07-10 23:08:48,130][227910] itr=1581, itrs=2000, Progress: 79.05%[0m
[36m[2023-07-10 23:08:59,824][227910] train() took 11.67 seconds to complete[0m
[36m[2023-07-10 23:08:59,824][227910] FPS: 329006.97[0m
[36m[2023-07-10 23:09:04,599][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:09:04,604][227910] Reward + Measures: [[612.38919902   0.51341969   0.36566299   0.59064698   0.61863536]][0m
[37m[1m[2023-07-10 23:09:04,604][227910] Max Reward on eval: 612.3891990244005[0m
[37m[1m[2023-07-10 23:09:04,605][227910] Min Reward on eval: 612.3891990244005[0m
[37m[1m[2023-07-10 23:09:04,605][227910] Mean Reward across all agents: 612.3891990244005[0m
[37m[1m[2023-07-10 23:09:04,605][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:09:09,967][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:09:09,972][227910] Reward + Measures: [[169.61760027   0.58899999   0.52270001   0.73579997   0.14579999]
 [205.65987259   0.54150003   0.78960001   0.57520002   0.76560003]
 [367.19226618   0.30950001   0.29270002   0.45839998   0.22760001]
 ...
 [208.6826719    0.70590001   0.80470002   0.30770001   0.80960006]
 [-60.20427788   0.3348       0.70639998   0.51500005   0.61000001]
 [  2.47269883   0.63630003   0.78470004   0.69640005   0.77079999]][0m
[37m[1m[2023-07-10 23:09:09,973][227910] Max Reward on eval: 1067.922327205597[0m
[37m[1m[2023-07-10 23:09:09,973][227910] Min Reward on eval: -1418.0610279546236[0m
[37m[1m[2023-07-10 23:09:09,973][227910] Mean Reward across all agents: 251.35361529073379[0m
[37m[1m[2023-07-10 23:09:09,973][227910] Average Trajectory Length: 996.7909999999999[0m
[36m[2023-07-10 23:09:09,979][227910] mean_value=-308.94384716551957, max_value=819.2203071014742[0m
[37m[1m[2023-07-10 23:09:09,982][227910] New mean coefficients: [[ 0.527674    0.16979827 -0.7861556   1.2571266   2.3604486 ]][0m
[37m[1m[2023-07-10 23:09:09,982][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:09:19,615][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 23:09:19,615][227910] FPS: 398709.71[0m
[36m[2023-07-10 23:09:19,618][227910] itr=1582, itrs=2000, Progress: 79.10%[0m
[36m[2023-07-10 23:09:31,039][227910] train() took 11.40 seconds to complete[0m
[36m[2023-07-10 23:09:31,039][227910] FPS: 336884.95[0m
[36m[2023-07-10 23:09:35,747][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:09:35,747][227910] Reward + Measures: [[625.52403057   0.490394     0.38638166   0.57521069   0.62397599]][0m
[37m[1m[2023-07-10 23:09:35,747][227910] Max Reward on eval: 625.524030567537[0m
[37m[1m[2023-07-10 23:09:35,748][227910] Min Reward on eval: 625.524030567537[0m
[37m[1m[2023-07-10 23:09:35,748][227910] Mean Reward across all agents: 625.524030567537[0m
[37m[1m[2023-07-10 23:09:35,748][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:09:41,420][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:09:41,421][227910] Reward + Measures: [[ 549.82115008    0.55150002    0.41350004    0.67979997    0.78270006]
 [-558.59028773    0.60230005    0.28799999    0.78109998    0.56400001]
 [ 392.39371335    0.34290001    0.62379998    0.43780002    0.62670004]
 ...
 [ 908.81570812    0.22259998    0.57010001    0.33459997    0.5018    ]
 [ 114.98150101    0.1699        0.82310003    0.55050004    0.7913    ]
 [ 508.28148368    0.3831        0.70069999    0.47399998    0.67870009]][0m
[37m[1m[2023-07-10 23:09:41,421][227910] Max Reward on eval: 1042.3239406128298[0m
[37m[1m[2023-07-10 23:09:41,421][227910] Min Reward on eval: -1678.5501721436042[0m
[37m[1m[2023-07-10 23:09:41,421][227910] Mean Reward across all agents: 110.45568607166268[0m
[37m[1m[2023-07-10 23:09:41,422][227910] Average Trajectory Length: 990.2326666666667[0m
[36m[2023-07-10 23:09:41,425][227910] mean_value=-644.6182380912102, max_value=987.5490662188619[0m
[37m[1m[2023-07-10 23:09:41,428][227910] New mean coefficients: [[ 0.12610507 -0.29417795  0.17400682  1.5698525   2.4806037 ]][0m
[37m[1m[2023-07-10 23:09:41,429][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:09:51,167][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 23:09:51,167][227910] FPS: 394396.53[0m
[36m[2023-07-10 23:09:51,169][227910] itr=1583, itrs=2000, Progress: 79.15%[0m
[36m[2023-07-10 23:10:02,666][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 23:10:02,667][227910] FPS: 334655.71[0m
[36m[2023-07-10 23:10:07,385][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:10:07,386][227910] Reward + Measures: [[615.8026396    0.41443065   0.48353899   0.53532827   0.68055665]][0m
[37m[1m[2023-07-10 23:10:07,386][227910] Max Reward on eval: 615.8026396023879[0m
[37m[1m[2023-07-10 23:10:07,386][227910] Min Reward on eval: 615.8026396023879[0m
[37m[1m[2023-07-10 23:10:07,386][227910] Mean Reward across all agents: 615.8026396023879[0m
[37m[1m[2023-07-10 23:10:07,387][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:10:12,808][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:10:12,809][227910] Reward + Measures: [[ 593.56796167    0.19219999    0.87709999    0.22810002    0.8688001 ]
 [ 687.58180126    0.68309999    0.41060001    0.80790007    0.14740001]
 [ 671.23913446    0.57459998    0.6613        0.68400002    0.42020002]
 ...
 [ 318.83397308    0.62910002    0.43710002    0.80900002    0.0355    ]
 [-390.53860722    0.39309999    0.69169998    0.40880004    0.59820002]
 [ -76.66828005    0.2378        0.2194        0.4041        0.1524    ]][0m
[37m[1m[2023-07-10 23:10:12,809][227910] Max Reward on eval: 957.0996179421898[0m
[37m[1m[2023-07-10 23:10:12,809][227910] Min Reward on eval: -1404.815044343192[0m
[37m[1m[2023-07-10 23:10:12,809][227910] Mean Reward across all agents: 18.202558456413936[0m
[37m[1m[2023-07-10 23:10:12,810][227910] Average Trajectory Length: 998.562[0m
[36m[2023-07-10 23:10:12,813][227910] mean_value=-644.2448673904375, max_value=818.8339730793784[0m
[37m[1m[2023-07-10 23:10:12,816][227910] New mean coefficients: [[0.39761645 0.4801035  0.12286304 1.2263824  3.037054  ]][0m
[37m[1m[2023-07-10 23:10:12,817][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:10:22,591][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 23:10:22,591][227910] FPS: 392952.46[0m
[36m[2023-07-10 23:10:22,593][227910] itr=1584, itrs=2000, Progress: 79.20%[0m
[36m[2023-07-10 23:10:34,083][227910] train() took 11.47 seconds to complete[0m
[36m[2023-07-10 23:10:34,083][227910] FPS: 334748.73[0m
[36m[2023-07-10 23:10:38,809][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:10:38,814][227910] Reward + Measures: [[511.68897542   0.44933829   0.70780534   0.61934072   0.81599301]][0m
[37m[1m[2023-07-10 23:10:38,815][227910] Max Reward on eval: 511.68897541980044[0m
[37m[1m[2023-07-10 23:10:38,815][227910] Min Reward on eval: 511.68897541980044[0m
[37m[1m[2023-07-10 23:10:38,815][227910] Mean Reward across all agents: 511.68897541980044[0m
[37m[1m[2023-07-10 23:10:38,815][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:10:44,219][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:10:44,220][227910] Reward + Measures: [[ 454.57037216    0.22159998    0.63280004    0.3326        0.63560003]
 [ -72.90681444    0.4413        0.71180004    0.51270002    0.73240006]
 [  -3.15054835    0.70480007    0.22830001    0.65640002    0.29139999]
 ...
 [   8.79868039    0.28020003    0.41960001    0.39030001    0.39969999]
 [ 153.39036709    0.60880005    0.75879997    0.2624        0.66159999]
 [-178.00334124    0.4535        0.58380002    0.25549999    0.61129999]][0m
[37m[1m[2023-07-10 23:10:44,220][227910] Max Reward on eval: 729.8012013735249[0m
[37m[1m[2023-07-10 23:10:44,220][227910] Min Reward on eval: -1122.1887844766025[0m
[37m[1m[2023-07-10 23:10:44,221][227910] Mean Reward across all agents: 251.63263791612863[0m
[37m[1m[2023-07-10 23:10:44,221][227910] Average Trajectory Length: 999.8639999999999[0m
[36m[2023-07-10 23:10:44,225][227910] mean_value=-386.3909707667214, max_value=708.0604649866048[0m
[37m[1m[2023-07-10 23:10:44,228][227910] New mean coefficients: [[0.8516208  0.8164823  0.17442729 2.3210015  3.105417  ]][0m
[37m[1m[2023-07-10 23:10:44,229][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:10:54,069][227910] train() took 9.84 seconds to complete[0m
[36m[2023-07-10 23:10:54,070][227910] FPS: 390286.46[0m
[36m[2023-07-10 23:10:54,072][227910] itr=1585, itrs=2000, Progress: 79.25%[0m
[36m[2023-07-10 23:11:05,695][227910] train() took 11.60 seconds to complete[0m
[36m[2023-07-10 23:11:05,695][227910] FPS: 330950.91[0m
[36m[2023-07-10 23:11:10,648][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:11:10,649][227910] Reward + Measures: [[416.83133623   0.52675897   0.89992666   0.7640776    0.92777324]][0m
[37m[1m[2023-07-10 23:11:10,649][227910] Max Reward on eval: 416.8313362283206[0m
[37m[1m[2023-07-10 23:11:10,649][227910] Min Reward on eval: 416.8313362283206[0m
[37m[1m[2023-07-10 23:11:10,649][227910] Mean Reward across all agents: 416.8313362283206[0m
[37m[1m[2023-07-10 23:11:10,650][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:11:16,087][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:11:16,093][227910] Reward + Measures: [[ 323.11110818    0.33050001    0.82130003    0.69590008    0.78410006]
 [-599.10660766    0.61829996    0.76220006    0.79100007    0.76539999]
 [ 265.46662274    0.2676        0.54469997    0.45450002    0.40790001]
 ...
 [  21.5857314     0.32660002    0.51060003    0.3461        0.52759999]
 [ 551.42254291    0.35010001    0.41270003    0.55220002    0.33989999]
 [ 485.14551585    0.36110002    0.74559999    0.45049998    0.65059996]][0m
[37m[1m[2023-07-10 23:11:16,093][227910] Max Reward on eval: 1001.934602324455[0m
[37m[1m[2023-07-10 23:11:16,093][227910] Min Reward on eval: -1566.8751064946177[0m
[37m[1m[2023-07-10 23:11:16,094][227910] Mean Reward across all agents: 43.58130828007326[0m
[37m[1m[2023-07-10 23:11:16,094][227910] Average Trajectory Length: 995.5493333333333[0m
[36m[2023-07-10 23:11:16,097][227910] mean_value=-741.9119434875528, max_value=831.6869475915412[0m
[37m[1m[2023-07-10 23:11:16,100][227910] New mean coefficients: [[0.72083676 0.41974157 0.2727964  0.630144   3.493851  ]][0m
[37m[1m[2023-07-10 23:11:16,100][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:11:25,825][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 23:11:25,825][227910] FPS: 394941.10[0m
[36m[2023-07-10 23:11:25,828][227910] itr=1586, itrs=2000, Progress: 79.30%[0m
[36m[2023-07-10 23:11:37,500][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 23:11:37,500][227910] FPS: 329520.51[0m
[36m[2023-07-10 23:11:42,163][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:11:42,163][227910] Reward + Measures: [[842.7762519    0.35370165   0.66587234   0.44713166   0.60220498]][0m
[37m[1m[2023-07-10 23:11:42,164][227910] Max Reward on eval: 842.7762519029724[0m
[37m[1m[2023-07-10 23:11:42,164][227910] Min Reward on eval: 842.7762519029724[0m
[37m[1m[2023-07-10 23:11:42,164][227910] Mean Reward across all agents: 842.7762519029724[0m
[37m[1m[2023-07-10 23:11:42,164][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:11:47,572][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:11:47,572][227910] Reward + Measures: [[ 558.93514085    0.46110001    0.84219998    0.3973        0.84829998]
 [ -59.81604063    0.30310002    0.36209998    0.36050001    0.26300001]
 [ 305.13203174    0.3804        0.84880012    0.53090006    0.80699998]
 ...
 [ -23.28314132    0.30250001    0.68339998    0.46250001    0.56080002]
 [-820.53641916    0.74559999    0.1665        0.84549999    0.79439998]
 [-234.92530368    0.1568        0.4276        0.38060004    0.18969999]][0m
[37m[1m[2023-07-10 23:11:47,572][227910] Max Reward on eval: 893.7382902886718[0m
[37m[1m[2023-07-10 23:11:47,573][227910] Min Reward on eval: -1740.2559794118163[0m
[37m[1m[2023-07-10 23:11:47,573][227910] Mean Reward across all agents: -159.19879054163675[0m
[37m[1m[2023-07-10 23:11:47,573][227910] Average Trajectory Length: 991.8956666666667[0m
[36m[2023-07-10 23:11:47,575][227910] mean_value=-981.1393569961011, max_value=568.3081244540077[0m
[37m[1m[2023-07-10 23:11:47,578][227910] New mean coefficients: [[ 0.39475223 -0.5356636   0.3057368   0.05498695  3.553823  ]][0m
[37m[1m[2023-07-10 23:11:47,578][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:11:57,211][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 23:11:57,211][227910] FPS: 398724.60[0m
[36m[2023-07-10 23:11:57,213][227910] itr=1587, itrs=2000, Progress: 79.35%[0m
[36m[2023-07-10 23:12:08,781][227910] train() took 11.55 seconds to complete[0m
[36m[2023-07-10 23:12:08,782][227910] FPS: 332495.32[0m
[36m[2023-07-10 23:12:13,589][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:12:13,589][227910] Reward + Measures: [[619.54854921   0.326507     0.76350766   0.36884868   0.80207366]][0m
[37m[1m[2023-07-10 23:12:13,590][227910] Max Reward on eval: 619.5485492130242[0m
[37m[1m[2023-07-10 23:12:13,590][227910] Min Reward on eval: 619.5485492130242[0m
[37m[1m[2023-07-10 23:12:13,590][227910] Mean Reward across all agents: 619.5485492130242[0m
[37m[1m[2023-07-10 23:12:13,590][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:12:19,260][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:12:19,260][227910] Reward + Measures: [[ 60.24525942   0.62849998   0.87190002   0.10749999   0.71100003]
 [109.15452304   0.43350002   0.48030001   0.69550002   0.21710001]
 [102.13209885   0.75230002   0.65630001   0.80459994   0.16320001]
 ...
 [103.31642543   0.3576       0.40559998   0.50880003   0.29780003]
 [528.36695992   0.48339996   0.76279998   0.32600003   0.69670004]
 [354.44824444   0.59160006   0.52430004   0.65690005   0.3653    ]][0m
[37m[1m[2023-07-10 23:12:19,261][227910] Max Reward on eval: 700.4634399172734[0m
[37m[1m[2023-07-10 23:12:19,261][227910] Min Reward on eval: -1403.087096242234[0m
[37m[1m[2023-07-10 23:12:19,261][227910] Mean Reward across all agents: 88.17777329888473[0m
[37m[1m[2023-07-10 23:12:19,261][227910] Average Trajectory Length: 997.6946666666666[0m
[36m[2023-07-10 23:12:19,267][227910] mean_value=-384.25449365694004, max_value=930.3014665799169[0m
[37m[1m[2023-07-10 23:12:19,270][227910] New mean coefficients: [[-0.2134653  -0.54110265  0.3664897   0.04164957  4.0699043 ]][0m
[37m[1m[2023-07-10 23:12:19,271][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:12:29,013][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 23:12:29,014][227910] FPS: 394202.59[0m
[36m[2023-07-10 23:12:29,016][227910] itr=1588, itrs=2000, Progress: 79.40%[0m
[36m[2023-07-10 23:12:40,607][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 23:12:40,607][227910] FPS: 331859.96[0m
[36m[2023-07-10 23:12:45,459][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:12:45,459][227910] Reward + Measures: [[533.99998036   0.26425165   0.8646493    0.25828132   0.90916872]][0m
[37m[1m[2023-07-10 23:12:45,459][227910] Max Reward on eval: 533.999980361673[0m
[37m[1m[2023-07-10 23:12:45,460][227910] Min Reward on eval: 533.999980361673[0m
[37m[1m[2023-07-10 23:12:45,460][227910] Mean Reward across all agents: 533.999980361673[0m
[37m[1m[2023-07-10 23:12:45,460][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:12:51,013][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:12:51,019][227910] Reward + Measures: [[ 574.83662768    0.36060002    0.72440004    0.44369999    0.70410007]
 [-416.5310557     0.1293        0.84910005    0.81199998    0.71789998]
 [-207.6710312     0.46049997    0.89319992    0.80980009    0.86180013]
 ...
 [ 557.83695933    0.25850001    0.83689994    0.2703        0.86040002]
 [ 208.3330855     0.32969999    0.36490002    0.6117        0.296     ]
 [ 669.20111979    0.59540004    0.57429999    0.78720003    0.26089999]][0m
[37m[1m[2023-07-10 23:12:51,020][227910] Max Reward on eval: 1000.3736151068704[0m
[37m[1m[2023-07-10 23:12:51,021][227910] Min Reward on eval: -1055.7460301551268[0m
[37m[1m[2023-07-10 23:12:51,021][227910] Mean Reward across all agents: 395.70205935253625[0m
[37m[1m[2023-07-10 23:12:51,022][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:12:51,031][227910] mean_value=-177.82430546994854, max_value=1094.359443966563[0m
[37m[1m[2023-07-10 23:12:51,035][227910] New mean coefficients: [[-0.48536697 -0.03673512  0.47254902  0.6263543   5.0114822 ]][0m
[37m[1m[2023-07-10 23:12:51,036][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:13:00,920][227910] train() took 9.88 seconds to complete[0m
[36m[2023-07-10 23:13:00,921][227910] FPS: 388587.57[0m
[36m[2023-07-10 23:13:00,923][227910] itr=1589, itrs=2000, Progress: 79.45%[0m
[36m[2023-07-10 23:13:12,458][227910] train() took 11.51 seconds to complete[0m
[36m[2023-07-10 23:13:12,458][227910] FPS: 333558.36[0m
[36m[2023-07-10 23:13:17,164][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:13:17,164][227910] Reward + Measures: [[514.62444895   0.218986     0.94818467   0.15737766   0.96040499]][0m
[37m[1m[2023-07-10 23:13:17,164][227910] Max Reward on eval: 514.624448945693[0m
[37m[1m[2023-07-10 23:13:17,165][227910] Min Reward on eval: 514.624448945693[0m
[37m[1m[2023-07-10 23:13:17,165][227910] Mean Reward across all agents: 514.624448945693[0m
[37m[1m[2023-07-10 23:13:17,165][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:13:22,565][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:13:22,570][227910] Reward + Measures: [[-359.62522615    0.46709999    0.4533        0.54580003    0.88010007]
 [ 624.39344997    0.38870001    0.60580003    0.60720009    0.67259997]
 [ 294.99135909    0.60729998    0.5151        0.62470001    0.67530006]
 ...
 [  86.76932805    0.52630001    0.29600003    0.509         0.44390002]
 [ 305.20707237    0.52540004    0.69929999    0.5693        0.71500003]
 [ 430.71275971    0.0697        0.94229996    0.61840004    0.9526    ]][0m
[37m[1m[2023-07-10 23:13:22,571][227910] Max Reward on eval: 688.8934354767902[0m
[37m[1m[2023-07-10 23:13:22,571][227910] Min Reward on eval: -1201.3201758691343[0m
[37m[1m[2023-07-10 23:13:22,571][227910] Mean Reward across all agents: 253.68752637783473[0m
[37m[1m[2023-07-10 23:13:22,572][227910] Average Trajectory Length: 999.785[0m
[36m[2023-07-10 23:13:22,577][227910] mean_value=-219.9320275416422, max_value=857.0950119893554[0m
[37m[1m[2023-07-10 23:13:22,579][227910] New mean coefficients: [[-0.25189215 -0.13641575  0.46437153  0.5327286   5.192075  ]][0m
[37m[1m[2023-07-10 23:13:22,580][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:13:32,184][227910] train() took 9.60 seconds to complete[0m
[36m[2023-07-10 23:13:32,184][227910] FPS: 399929.12[0m
[36m[2023-07-10 23:13:32,186][227910] itr=1590, itrs=2000, Progress: 79.50%[0m
[37m[1m[2023-07-10 23:13:36,341][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001570[0m
[36m[2023-07-10 23:13:48,123][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 23:13:48,123][227910] FPS: 333934.84[0m
[36m[2023-07-10 23:13:52,816][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:13:52,816][227910] Reward + Measures: [[536.06500947   0.22024068   0.96148634   0.14556967   0.97304392]][0m
[37m[1m[2023-07-10 23:13:52,816][227910] Max Reward on eval: 536.0650094663401[0m
[37m[1m[2023-07-10 23:13:52,816][227910] Min Reward on eval: 536.0650094663401[0m
[37m[1m[2023-07-10 23:13:52,817][227910] Mean Reward across all agents: 536.0650094663401[0m
[37m[1m[2023-07-10 23:13:52,817][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:13:58,187][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:13:58,187][227910] Reward + Measures: [[242.48915085   0.19670001   0.72980005   0.35099998   0.78299999]
 [-70.02931768   0.0432       0.93420011   0.70960003   0.96539992]
 [717.50356099   0.27340001   0.55849999   0.43850002   0.44160005]
 ...
 [828.92938563   0.42930004   0.66679996   0.4474       0.65369999]
 [ 57.63705955   0.2189       0.69730002   0.18980001   0.63490003]
 [229.52307764   0.2304       0.70600003   0.19750001   0.62099999]][0m
[37m[1m[2023-07-10 23:13:58,188][227910] Max Reward on eval: 1185.4254650153453[0m
[37m[1m[2023-07-10 23:13:58,188][227910] Min Reward on eval: -1318.2773284910713[0m
[37m[1m[2023-07-10 23:13:58,188][227910] Mean Reward across all agents: 307.7904762826081[0m
[37m[1m[2023-07-10 23:13:58,188][227910] Average Trajectory Length: 999.6986666666667[0m
[36m[2023-07-10 23:13:58,193][227910] mean_value=-324.7293266475103, max_value=997.4207222539844[0m
[37m[1m[2023-07-10 23:13:58,196][227910] New mean coefficients: [[ 0.78904253 -0.18069986  0.4618247   0.6948153   5.6062627 ]][0m
[37m[1m[2023-07-10 23:13:58,196][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:14:07,828][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 23:14:07,828][227910] FPS: 398755.74[0m
[36m[2023-07-10 23:14:07,831][227910] itr=1591, itrs=2000, Progress: 79.55%[0m
[36m[2023-07-10 23:14:19,331][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 23:14:19,332][227910] FPS: 334542.80[0m
[36m[2023-07-10 23:14:24,047][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:14:24,047][227910] Reward + Measures: [[597.04371655   0.26398534   0.97140032   0.12896466   0.97849494]][0m
[37m[1m[2023-07-10 23:14:24,048][227910] Max Reward on eval: 597.0437165489103[0m
[37m[1m[2023-07-10 23:14:24,048][227910] Min Reward on eval: 597.0437165489103[0m
[37m[1m[2023-07-10 23:14:24,048][227910] Mean Reward across all agents: 597.0437165489103[0m
[37m[1m[2023-07-10 23:14:24,048][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:14:29,542][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:14:29,543][227910] Reward + Measures: [[627.37514421   0.49160001   0.93599999   0.09080001   0.95230007]
 [600.04607074   0.38200003   0.95749998   0.0785       0.97600001]
 [555.42874043   0.43719998   0.95249999   0.0875       0.9594    ]
 ...
 [541.09112172   0.22150002   0.93600005   0.229        0.93979996]
 [315.60298572   0.39990002   0.72820002   0.39770004   0.7766    ]
 [538.38380175   0.31529999   0.96660006   0.08180001   0.97580004]][0m
[37m[1m[2023-07-10 23:14:29,543][227910] Max Reward on eval: 878.8002167191356[0m
[37m[1m[2023-07-10 23:14:29,544][227910] Min Reward on eval: -208.9701144097722[0m
[37m[1m[2023-07-10 23:14:29,544][227910] Mean Reward across all agents: 517.145871833879[0m
[37m[1m[2023-07-10 23:14:29,544][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:14:29,549][227910] mean_value=130.09494833472755, max_value=1128.9793244705354[0m
[37m[1m[2023-07-10 23:14:29,552][227910] New mean coefficients: [[ 1.2625402  -0.1429467   0.42401057 -0.6623838   5.003828  ]][0m
[37m[1m[2023-07-10 23:14:29,553][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:14:39,333][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 23:14:39,333][227910] FPS: 392694.66[0m
[36m[2023-07-10 23:14:39,335][227910] itr=1592, itrs=2000, Progress: 79.60%[0m
[36m[2023-07-10 23:14:50,897][227910] train() took 11.54 seconds to complete[0m
[36m[2023-07-10 23:14:50,897][227910] FPS: 332680.92[0m
[36m[2023-07-10 23:14:55,707][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:14:55,707][227910] Reward + Measures: [[739.98576387   0.52663732   0.85493433   0.314592     0.92712027]][0m
[37m[1m[2023-07-10 23:14:55,707][227910] Max Reward on eval: 739.9857638737027[0m
[37m[1m[2023-07-10 23:14:55,708][227910] Min Reward on eval: 739.9857638737027[0m
[37m[1m[2023-07-10 23:14:55,708][227910] Mean Reward across all agents: 739.9857638737027[0m
[37m[1m[2023-07-10 23:14:55,708][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:15:01,063][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:15:01,063][227910] Reward + Measures: [[ 671.9120336     0.47690001    0.87340003    0.39569998    0.84400004]
 [ 719.13356782    0.68019998    0.79430002    0.5309        0.83809996]
 [ 679.33792358    0.7313        0.77720004    0.52330005    0.83750004]
 ...
 [ 518.52234078    0.19700001    0.94950002    0.1531        0.96610004]
 [-110.44766277    0.43059999    0.75830001    0.80289996    0.77470005]
 [ 219.48027768    0.25190002    0.85190004    0.50040001    0.83290005]][0m
[37m[1m[2023-07-10 23:15:01,064][227910] Max Reward on eval: 922.4345895190141[0m
[37m[1m[2023-07-10 23:15:01,064][227910] Min Reward on eval: -1261.823533731635[0m
[37m[1m[2023-07-10 23:15:01,064][227910] Mean Reward across all agents: 379.48599763096104[0m
[37m[1m[2023-07-10 23:15:01,064][227910] Average Trajectory Length: 999.7246666666666[0m
[36m[2023-07-10 23:15:01,072][227910] mean_value=-119.43797013542348, max_value=1177.3702168923528[0m
[37m[1m[2023-07-10 23:15:01,074][227910] New mean coefficients: [[ 0.74333125 -0.32311112  0.7668543  -0.6443878   4.9331646 ]][0m
[37m[1m[2023-07-10 23:15:01,075][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:15:10,827][227910] train() took 9.75 seconds to complete[0m
[36m[2023-07-10 23:15:10,827][227910] FPS: 393838.94[0m
[36m[2023-07-10 23:15:10,830][227910] itr=1593, itrs=2000, Progress: 79.65%[0m
[36m[2023-07-10 23:15:22,315][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 23:15:22,315][227910] FPS: 335019.43[0m
[36m[2023-07-10 23:15:27,123][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:15:27,123][227910] Reward + Measures: [[660.67821358   0.33540136   0.98234731   0.02503733   0.99570334]][0m
[37m[1m[2023-07-10 23:15:27,123][227910] Max Reward on eval: 660.6782135815679[0m
[37m[1m[2023-07-10 23:15:27,124][227910] Min Reward on eval: 660.6782135815679[0m
[37m[1m[2023-07-10 23:15:27,124][227910] Mean Reward across all agents: 660.6782135815679[0m
[37m[1m[2023-07-10 23:15:27,124][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:15:32,537][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:15:32,543][227910] Reward + Measures: [[ 659.62238741    0.31410003    0.9896        0.0256        0.99720001]
 [  85.50849379    0.70710003    0.67650002    0.69210005    0.64880002]
 [ 461.96318286    0.52930003    0.64830005    0.50920004    0.67390001]
 ...
 [ 591.66032103    0.5632        0.66600001    0.2577        0.63310003]
 [-374.49442421    0.0148        0.90869999    0.65819997    0.99049997]
 [ 588.97725288    0.59280002    0.79500002    0.52440006    0.76589996]][0m
[37m[1m[2023-07-10 23:15:32,543][227910] Max Reward on eval: 763.3556658639573[0m
[37m[1m[2023-07-10 23:15:32,543][227910] Min Reward on eval: -696.1792613015743[0m
[37m[1m[2023-07-10 23:15:32,544][227910] Mean Reward across all agents: 482.34620214683986[0m
[37m[1m[2023-07-10 23:15:32,544][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:15:32,550][227910] mean_value=23.70010033277761, max_value=1149.191262923344[0m
[37m[1m[2023-07-10 23:15:32,553][227910] New mean coefficients: [[ 0.9052664  -0.0848444   0.8832159   0.46725947  4.7793527 ]][0m
[37m[1m[2023-07-10 23:15:32,554][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:15:42,363][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 23:15:42,364][227910] FPS: 391508.84[0m
[36m[2023-07-10 23:15:42,366][227910] itr=1594, itrs=2000, Progress: 79.70%[0m
[36m[2023-07-10 23:15:53,821][227910] train() took 11.43 seconds to complete[0m
[36m[2023-07-10 23:15:53,822][227910] FPS: 335887.48[0m
[36m[2023-07-10 23:15:58,606][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:15:58,607][227910] Reward + Measures: [[225.17000572   0.002664     0.97371036   0.885405     0.99432224]][0m
[37m[1m[2023-07-10 23:15:58,607][227910] Max Reward on eval: 225.17000572103197[0m
[37m[1m[2023-07-10 23:15:58,607][227910] Min Reward on eval: 225.17000572103197[0m
[37m[1m[2023-07-10 23:15:58,607][227910] Mean Reward across all agents: 225.17000572103197[0m
[37m[1m[2023-07-10 23:15:58,607][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:16:04,249][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:16:04,250][227910] Reward + Measures: [[-100.10614913    0.0012        0.99300003    0.97469997    0.99699992]
 [ 443.22792112    0.0963        0.85709995    0.2326        0.92740005]
 [-388.30093429    0.0032        0.95440006    0.90179998    0.98659992]
 ...
 [ 318.17401232    0.1664        0.85530007    0.22059999    0.93810004]
 [ 466.48749913    0.168         0.72320002    0.2024        0.81229991]
 [ 435.30824752    0.123         0.77080005    0.24489999    0.88669997]][0m
[37m[1m[2023-07-10 23:16:04,250][227910] Max Reward on eval: 524.2863321959973[0m
[37m[1m[2023-07-10 23:16:04,250][227910] Min Reward on eval: -409.7104062619328[0m
[37m[1m[2023-07-10 23:16:04,250][227910] Mean Reward across all agents: 311.0431953343223[0m
[37m[1m[2023-07-10 23:16:04,251][227910] Average Trajectory Length: 999.079[0m
[36m[2023-07-10 23:16:04,254][227910] mean_value=-37.80499784748521, max_value=636.5733696149729[0m
[37m[1m[2023-07-10 23:16:04,256][227910] New mean coefficients: [[ 1.5424473  -0.26877576  0.7761206   0.44304982  4.8672094 ]][0m
[37m[1m[2023-07-10 23:16:04,257][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:16:14,052][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 23:16:14,052][227910] FPS: 392114.36[0m
[36m[2023-07-10 23:16:14,055][227910] itr=1595, itrs=2000, Progress: 79.75%[0m
[36m[2023-07-10 23:16:25,666][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 23:16:25,666][227910] FPS: 331300.27[0m
[36m[2023-07-10 23:16:30,494][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:16:30,495][227910] Reward + Measures: [[135.73237113   0.00351933   0.9862166    0.70875335   0.99197966]][0m
[37m[1m[2023-07-10 23:16:30,495][227910] Max Reward on eval: 135.73237113337177[0m
[37m[1m[2023-07-10 23:16:30,495][227910] Min Reward on eval: 135.73237113337177[0m
[37m[1m[2023-07-10 23:16:30,495][227910] Mean Reward across all agents: 135.73237113337177[0m
[37m[1m[2023-07-10 23:16:30,495][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:16:35,973][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:16:35,973][227910] Reward + Measures: [[141.84591947   0.0038       0.99089998   0.71090001   0.99419993]
 [197.2044152    0.11329999   0.89469999   0.45220003   0.95889997]
 [596.72457691   0.30590001   0.37090001   0.4436       0.3565    ]
 ...
 [526.14241697   0.37490001   0.89609998   0.61540002   0.96540004]
 [191.13218532   0.0184       0.98310006   0.41920003   0.99169999]
 [335.45531042   0.0462       0.95710003   0.36900002   0.97299999]][0m
[37m[1m[2023-07-10 23:16:35,974][227910] Max Reward on eval: 831.6330721667676[0m
[37m[1m[2023-07-10 23:16:35,974][227910] Min Reward on eval: -705.3740112130181[0m
[37m[1m[2023-07-10 23:16:35,974][227910] Mean Reward across all agents: 286.09533549435247[0m
[37m[1m[2023-07-10 23:16:35,974][227910] Average Trajectory Length: 999.8323333333333[0m
[36m[2023-07-10 23:16:35,979][227910] mean_value=-101.32990811868551, max_value=1020.170884819678[0m
[37m[1m[2023-07-10 23:16:35,981][227910] New mean coefficients: [[ 1.5135183  -0.90629035  0.628893    0.7686926   5.1768765 ]][0m
[37m[1m[2023-07-10 23:16:35,982][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:16:45,806][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 23:16:45,806][227910] FPS: 390958.15[0m
[36m[2023-07-10 23:16:45,808][227910] itr=1596, itrs=2000, Progress: 79.80%[0m
[36m[2023-07-10 23:16:57,278][227910] train() took 11.45 seconds to complete[0m
[36m[2023-07-10 23:16:57,278][227910] FPS: 335335.75[0m
[36m[2023-07-10 23:17:02,111][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:17:02,112][227910] Reward + Measures: [[438.26087074   0.12446266   0.93494725   0.391543     0.952627  ]][0m
[37m[1m[2023-07-10 23:17:02,112][227910] Max Reward on eval: 438.26087074449225[0m
[37m[1m[2023-07-10 23:17:02,112][227910] Min Reward on eval: 438.26087074449225[0m
[37m[1m[2023-07-10 23:17:02,112][227910] Mean Reward across all agents: 438.26087074449225[0m
[37m[1m[2023-07-10 23:17:02,112][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:17:07,621][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:17:07,622][227910] Reward + Measures: [[-780.7783374     0.57609999    0.39089999    0.70429993    0.5697    ]
 [-570.98925705    0.64669997    0.62629998    0.67290002    0.65350002]
 [ 583.34232645    0.41920003    0.95850003    0.41          0.96320003]
 ...
 [-303.41028414    0.005         0.90500003    0.80810004    0.97419995]
 [ 347.60846214    0.59150004    0.34640002    0.45460001    0.56910002]
 [-199.68134231    0.80179995    0.10079999    0.91800004    0.73280001]][0m
[37m[1m[2023-07-10 23:17:07,622][227910] Max Reward on eval: 719.9791802078311[0m
[37m[1m[2023-07-10 23:17:07,622][227910] Min Reward on eval: -1016.8570638241945[0m
[37m[1m[2023-07-10 23:17:07,622][227910] Mean Reward across all agents: 30.642983620715828[0m
[37m[1m[2023-07-10 23:17:07,622][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:17:07,629][227910] mean_value=-264.62982579883436, max_value=1040.7781457655299[0m
[37m[1m[2023-07-10 23:17:07,631][227910] New mean coefficients: [[ 1.6109072  -0.47574076  0.62855303  0.8734219   5.1847124 ]][0m
[37m[1m[2023-07-10 23:17:07,633][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:17:17,428][227910] train() took 9.79 seconds to complete[0m
[36m[2023-07-10 23:17:17,428][227910] FPS: 392098.99[0m
[36m[2023-07-10 23:17:17,430][227910] itr=1597, itrs=2000, Progress: 79.85%[0m
[36m[2023-07-10 23:17:29,346][227910] train() took 11.90 seconds to complete[0m
[36m[2023-07-10 23:17:29,346][227910] FPS: 322818.03[0m
[36m[2023-07-10 23:17:34,092][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:17:34,093][227910] Reward + Measures: [[608.61660976   0.41639432   0.93028498   0.04382      0.92575061]][0m
[37m[1m[2023-07-10 23:17:34,093][227910] Max Reward on eval: 608.6166097563988[0m
[37m[1m[2023-07-10 23:17:34,093][227910] Min Reward on eval: 608.6166097563988[0m
[37m[1m[2023-07-10 23:17:34,094][227910] Mean Reward across all agents: 608.6166097563988[0m
[37m[1m[2023-07-10 23:17:34,094][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:17:39,515][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:17:39,516][227910] Reward + Measures: [[390.89743313   0.83670008   0.91530001   0.27239999   0.86440003]
 [242.22031851   0.7881       0.86479998   0.40540001   0.84189999]
 [367.43278441   0.80540001   0.95450002   0.078        0.91209996]
 ...
 [526.23315666   0.52590001   0.82770008   0.3132       0.80840009]
 [449.08674026   0.79100001   0.87419999   0.0886       0.95769995]
 [-64.01277243   0.91140002   0.33050004   0.90240002   0.51800007]][0m
[37m[1m[2023-07-10 23:17:39,516][227910] Max Reward on eval: 724.0250196787529[0m
[37m[1m[2023-07-10 23:17:39,516][227910] Min Reward on eval: -750.3042262058473[0m
[37m[1m[2023-07-10 23:17:39,517][227910] Mean Reward across all agents: 228.4311567157685[0m
[37m[1m[2023-07-10 23:17:39,517][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:17:39,522][227910] mean_value=-156.62462644915342, max_value=908.8859952119994[0m
[37m[1m[2023-07-10 23:17:39,525][227910] New mean coefficients: [[ 1.5076921  -0.68128085  0.97161627  0.5554425   5.045906  ]][0m
[37m[1m[2023-07-10 23:17:39,526][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:17:49,441][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 23:17:49,441][227910] FPS: 387349.56[0m
[36m[2023-07-10 23:17:49,443][227910] itr=1598, itrs=2000, Progress: 79.90%[0m
[36m[2023-07-10 23:18:01,023][227910] train() took 11.56 seconds to complete[0m
[36m[2023-07-10 23:18:01,023][227910] FPS: 332195.97[0m
[36m[2023-07-10 23:18:05,818][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:18:05,818][227910] Reward + Measures: [[616.95827726   0.31471968   0.93053174   0.06894333   0.94155937]][0m
[37m[1m[2023-07-10 23:18:05,818][227910] Max Reward on eval: 616.9582772629403[0m
[37m[1m[2023-07-10 23:18:05,818][227910] Min Reward on eval: 616.9582772629403[0m
[37m[1m[2023-07-10 23:18:05,819][227910] Mean Reward across all agents: 616.9582772629403[0m
[37m[1m[2023-07-10 23:18:05,819][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:18:11,411][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:18:11,412][227910] Reward + Measures: [[374.55903175   0.52719992   0.96320003   0.30509996   0.98380005]
 [220.73296304   0.7209       0.77630001   0.69890004   0.83879995]
 [399.64370444   0.60100001   0.89449996   0.59850001   0.94640011]
 ...
 [160.48351817   0.17379999   0.94230002   0.61969995   0.96740007]
 [563.41863424   0.42640001   0.94390005   0.08130001   0.93790001]
 [428.96501737   0.48549995   0.94340003   0.14170001   0.94280005]][0m
[37m[1m[2023-07-10 23:18:11,412][227910] Max Reward on eval: 635.9604206350865[0m
[37m[1m[2023-07-10 23:18:11,412][227910] Min Reward on eval: -379.70565519288647[0m
[37m[1m[2023-07-10 23:18:11,413][227910] Mean Reward across all agents: 414.73032595075927[0m
[37m[1m[2023-07-10 23:18:11,413][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:18:11,417][227910] mean_value=133.58076238836037, max_value=926.3987255655054[0m
[37m[1m[2023-07-10 23:18:11,420][227910] New mean coefficients: [[ 0.9160798  -0.9061097   1.1599634   0.19526288  5.4602613 ]][0m
[37m[1m[2023-07-10 23:18:11,421][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:18:21,056][227910] train() took 9.63 seconds to complete[0m
[36m[2023-07-10 23:18:21,057][227910] FPS: 398609.70[0m
[36m[2023-07-10 23:18:21,059][227910] itr=1599, itrs=2000, Progress: 79.95%[0m
[36m[2023-07-10 23:18:32,730][227910] train() took 11.65 seconds to complete[0m
[36m[2023-07-10 23:18:32,731][227910] FPS: 329567.77[0m
[36m[2023-07-10 23:18:37,487][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:18:37,487][227910] Reward + Measures: [[627.78902368   0.31442699   0.94517761   0.05204801   0.95668429]][0m
[37m[1m[2023-07-10 23:18:37,488][227910] Max Reward on eval: 627.7890236803015[0m
[37m[1m[2023-07-10 23:18:37,488][227910] Min Reward on eval: 627.7890236803015[0m
[37m[1m[2023-07-10 23:18:37,488][227910] Mean Reward across all agents: 627.7890236803015[0m
[37m[1m[2023-07-10 23:18:37,488][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:18:42,981][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:18:42,987][227910] Reward + Measures: [[542.29512609   0.71900004   0.89989996   0.58500004   0.91870004]
 [682.67459844   0.0352       0.91989994   0.47410002   0.97180003]
 [459.46796611   0.80610001   0.66460001   0.73769999   0.70740002]
 ...
 [578.34437301   0.69929999   0.83430004   0.66150004   0.86149997]
 [515.73938896   0.2494       0.86549997   0.5248       0.9788    ]
 [ 80.48154512   0.70819998   0.2676       0.69029999   0.37990004]][0m
[37m[1m[2023-07-10 23:18:42,987][227910] Max Reward on eval: 715.1789371037041[0m
[37m[1m[2023-07-10 23:18:42,988][227910] Min Reward on eval: -113.60966512268642[0m
[37m[1m[2023-07-10 23:18:42,988][227910] Mean Reward across all agents: 420.358225468497[0m
[37m[1m[2023-07-10 23:18:42,988][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:18:42,994][227910] mean_value=29.12983783709094, max_value=1112.0498399478615[0m
[37m[1m[2023-07-10 23:18:42,997][227910] New mean coefficients: [[ 1.5353054  -0.24823916  1.3874125   0.35833314  4.976331  ]][0m
[37m[1m[2023-07-10 23:18:42,998][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:18:52,814][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 23:18:52,814][227910] FPS: 391285.22[0m
[36m[2023-07-10 23:18:52,816][227910] itr=1600, itrs=2000, Progress: 80.00%[0m
[37m[1m[2023-07-10 23:18:56,962][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001580[0m
[36m[2023-07-10 23:19:08,706][227910] train() took 11.46 seconds to complete[0m
[36m[2023-07-10 23:19:08,706][227910] FPS: 334944.25[0m
[36m[2023-07-10 23:19:13,526][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:19:13,526][227910] Reward + Measures: [[467.58966446   0.62928432   0.97532666   0.30116832   0.95743632]][0m
[37m[1m[2023-07-10 23:19:13,526][227910] Max Reward on eval: 467.58966446353526[0m
[37m[1m[2023-07-10 23:19:13,526][227910] Min Reward on eval: 467.58966446353526[0m
[37m[1m[2023-07-10 23:19:13,527][227910] Mean Reward across all agents: 467.58966446353526[0m
[37m[1m[2023-07-10 23:19:13,527][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:19:19,060][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:19:19,060][227910] Reward + Measures: [[490.89761952   0.82709998   0.93330002   0.5226       0.91590005]
 [234.65488773   0.81379998   0.91040003   0.7967       0.93419999]
 [513.6918109    0.44350001   0.55019999   0.52179998   0.74760002]
 ...
 [647.36004637   0.63770002   0.76530004   0.19600001   0.92049998]
 [420.03539291   0.50500005   0.45430002   0.63990003   0.71940005]
 [324.93295717   0.36220002   0.54360002   0.62010002   0.57539999]][0m
[37m[1m[2023-07-10 23:19:19,060][227910] Max Reward on eval: 725.961564318696[0m
[37m[1m[2023-07-10 23:19:19,061][227910] Min Reward on eval: -12.160368166596163[0m
[37m[1m[2023-07-10 23:19:19,061][227910] Mean Reward across all agents: 505.21284409624167[0m
[37m[1m[2023-07-10 23:19:19,061][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:19:19,070][227910] mean_value=291.7657659302827, max_value=972.8907198984916[0m
[37m[1m[2023-07-10 23:19:19,072][227910] New mean coefficients: [[ 1.5867412  -0.0917947   0.99543333  0.5017841   4.3053417 ]][0m
[37m[1m[2023-07-10 23:19:19,073][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:19:28,933][227910] train() took 9.86 seconds to complete[0m
[36m[2023-07-10 23:19:28,934][227910] FPS: 389514.55[0m
[36m[2023-07-10 23:19:28,936][227910] itr=1601, itrs=2000, Progress: 80.05%[0m
[36m[2023-07-10 23:19:40,532][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 23:19:40,532][227910] FPS: 331684.66[0m
[36m[2023-07-10 23:19:45,300][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:19:45,301][227910] Reward + Measures: [[37.47709859  0.014124    0.88942164  0.73829573  0.95214295]][0m
[37m[1m[2023-07-10 23:19:45,301][227910] Max Reward on eval: 37.47709858832785[0m
[37m[1m[2023-07-10 23:19:45,301][227910] Min Reward on eval: 37.47709858832785[0m
[37m[1m[2023-07-10 23:19:45,301][227910] Mean Reward across all agents: 37.47709858832785[0m
[37m[1m[2023-07-10 23:19:45,301][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:19:50,721][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:19:50,721][227910] Reward + Measures: [[-200.86911198    0.65979999    0.41169998    0.88870001    0.80330002]
 [-550.38499508    0.71020001    0.3644        0.55890006    0.8549    ]
 [ 181.45000655    0.68149996    0.77520001    0.2861        0.92659998]
 ...
 [-178.00294653    0.66420001    0.45769998    0.89890003    0.84869999]
 [-633.45399787    0.97560006    0.0143        0.98760003    0.87599993]
 [ 209.14518823    0.40039998    0.87550002    0.33290002    0.93870002]][0m
[37m[1m[2023-07-10 23:19:50,722][227910] Max Reward on eval: 360.980381241662[0m
[37m[1m[2023-07-10 23:19:50,722][227910] Min Reward on eval: -1558.9530755118467[0m
[37m[1m[2023-07-10 23:19:50,722][227910] Mean Reward across all agents: -111.87104709773175[0m
[37m[1m[2023-07-10 23:19:50,722][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:19:50,726][227910] mean_value=-325.25420974609915, max_value=511.459171164484[0m
[37m[1m[2023-07-10 23:19:50,729][227910] New mean coefficients: [[1.2514929  0.39569575 1.1975698  0.6993153  4.7292304 ]][0m
[37m[1m[2023-07-10 23:19:50,730][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:20:00,544][227910] train() took 9.81 seconds to complete[0m
[36m[2023-07-10 23:20:00,544][227910] FPS: 391362.58[0m
[36m[2023-07-10 23:20:00,546][227910] itr=1602, itrs=2000, Progress: 80.10%[0m
[36m[2023-07-10 23:20:12,150][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 23:20:12,151][227910] FPS: 331475.80[0m
[36m[2023-07-10 23:20:16,952][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:20:16,953][227910] Reward + Measures: [[211.63992072   0.36753231   0.75526637   0.14291233   0.8780936 ]][0m
[37m[1m[2023-07-10 23:20:16,953][227910] Max Reward on eval: 211.6399207158129[0m
[37m[1m[2023-07-10 23:20:16,953][227910] Min Reward on eval: 211.6399207158129[0m
[37m[1m[2023-07-10 23:20:16,954][227910] Mean Reward across all agents: 211.6399207158129[0m
[37m[1m[2023-07-10 23:20:16,954][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:20:22,412][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:20:22,413][227910] Reward + Measures: [[195.21372327   0.33419999   0.73019999   0.15949999   0.88309997]
 [153.19970446   0.38680002   0.68970007   0.1556       0.86040002]
 [225.0660132    0.37970001   0.77930003   0.12630001   0.88319999]
 ...
 [238.18195875   0.40099999   0.7899       0.1237       0.89529991]
 [130.70090105   0.39070001   0.69969994   0.14839999   0.88829994]
 [179.119844     0.36120003   0.7263       0.1481       0.87620002]][0m
[37m[1m[2023-07-10 23:20:22,413][227910] Max Reward on eval: 244.10703820312628[0m
[37m[1m[2023-07-10 23:20:22,413][227910] Min Reward on eval: 71.04515739741328[0m
[37m[1m[2023-07-10 23:20:22,414][227910] Mean Reward across all agents: 165.52092700545944[0m
[37m[1m[2023-07-10 23:20:22,414][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:20:22,417][227910] mean_value=-24.692751307946796, max_value=68.87428154024434[0m
[37m[1m[2023-07-10 23:20:22,420][227910] New mean coefficients: [[1.296715   0.33852726 1.1947701  0.13854033 4.2250686 ]][0m
[37m[1m[2023-07-10 23:20:22,421][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:20:32,142][227910] train() took 9.72 seconds to complete[0m
[36m[2023-07-10 23:20:32,142][227910] FPS: 395074.92[0m
[36m[2023-07-10 23:20:32,145][227910] itr=1603, itrs=2000, Progress: 80.15%[0m
[36m[2023-07-10 23:20:43,696][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 23:20:43,697][227910] FPS: 332998.64[0m
[36m[2023-07-10 23:20:48,411][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:20:48,412][227910] Reward + Measures: [[-235.17614765    0.42889732    0.79886764    0.17183769    0.95897734]][0m
[37m[1m[2023-07-10 23:20:48,412][227910] Max Reward on eval: -235.17614764534778[0m
[37m[1m[2023-07-10 23:20:48,412][227910] Min Reward on eval: -235.17614764534778[0m
[37m[1m[2023-07-10 23:20:48,412][227910] Mean Reward across all agents: -235.17614764534778[0m
[37m[1m[2023-07-10 23:20:48,413][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:20:53,773][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:20:53,774][227910] Reward + Measures: [[ -73.7539681     0.36760002    0.72970003    0.20990001    0.90620005]
 [ -95.39510832    0.44260001    0.82770008    0.21169999    0.94869995]
 [-184.28440388    0.44380003    0.77209997    0.2026        0.93689996]
 ...
 [ 142.80441941    0.34629998    0.8318001     0.36409998    0.92189997]
 [-102.06058815    0.39129996    0.78570002    0.2476        0.93600005]
 [ -69.51523979    0.3822        0.79409999    0.20990001    0.93290007]][0m
[37m[1m[2023-07-10 23:20:53,774][227910] Max Reward on eval: 259.0777157693519[0m
[37m[1m[2023-07-10 23:20:53,774][227910] Min Reward on eval: -454.454758968344[0m
[37m[1m[2023-07-10 23:20:53,774][227910] Mean Reward across all agents: -67.81453549731901[0m
[37m[1m[2023-07-10 23:20:53,774][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:20:53,777][227910] mean_value=-272.3576608690285, max_value=423.90533482461[0m
[37m[1m[2023-07-10 23:20:53,779][227910] New mean coefficients: [[ 1.2834111   0.31540552  1.3488771  -0.21650258  3.8278482 ]][0m
[37m[1m[2023-07-10 23:20:53,780][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:21:03,561][227910] train() took 9.78 seconds to complete[0m
[36m[2023-07-10 23:21:03,562][227910] FPS: 392647.05[0m
[36m[2023-07-10 23:21:03,564][227910] itr=1604, itrs=2000, Progress: 80.20%[0m
[36m[2023-07-10 23:21:15,170][227910] train() took 11.59 seconds to complete[0m
[36m[2023-07-10 23:21:15,171][227910] FPS: 331415.87[0m
[36m[2023-07-10 23:21:19,930][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:21:19,931][227910] Reward + Measures: [[-400.76138396    0.03858966    0.90003198    0.49460331    0.97509062]][0m
[37m[1m[2023-07-10 23:21:19,931][227910] Max Reward on eval: -400.76138395641874[0m
[37m[1m[2023-07-10 23:21:19,931][227910] Min Reward on eval: -400.76138395641874[0m
[37m[1m[2023-07-10 23:21:19,932][227910] Mean Reward across all agents: -400.76138395641874[0m
[37m[1m[2023-07-10 23:21:19,932][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:21:25,447][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:21:25,448][227910] Reward + Measures: [[ -84.83232724    0.0583        0.88980001    0.35180002    0.94519997]
 [ -70.89298623    0.0734        0.8811        0.31420001    0.93689996]
 [-348.64795462    0.0114        0.89170009    0.43779999    0.96990007]
 ...
 [-433.77294324    0.0457        0.85880011    0.43319997    0.91350001]
 [-141.06734763    0.07129999    0.87320006    0.33419999    0.9271    ]
 [-272.62834486    0.0279        0.92620003    0.38840002    0.97690004]][0m
[37m[1m[2023-07-10 23:21:25,448][227910] Max Reward on eval: 78.24486965058604[0m
[37m[1m[2023-07-10 23:21:25,448][227910] Min Reward on eval: -1058.4653310330352[0m
[37m[1m[2023-07-10 23:21:25,448][227910] Mean Reward across all agents: -224.42295063506617[0m
[37m[1m[2023-07-10 23:21:25,449][227910] Average Trajectory Length: 999.803[0m
[36m[2023-07-10 23:21:25,450][227910] mean_value=-673.3955995061747, max_value=391.76826713500776[0m
[37m[1m[2023-07-10 23:21:25,453][227910] New mean coefficients: [[ 1.6376389   1.0119612   1.4937174  -0.10491215  3.9212012 ]][0m
[37m[1m[2023-07-10 23:21:25,454][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:21:35,369][227910] train() took 9.91 seconds to complete[0m
[36m[2023-07-10 23:21:35,369][227910] FPS: 387338.37[0m
[36m[2023-07-10 23:21:35,372][227910] itr=1605, itrs=2000, Progress: 80.25%[0m
[36m[2023-07-10 23:21:46,969][227910] train() took 11.58 seconds to complete[0m
[36m[2023-07-10 23:21:46,969][227910] FPS: 331712.45[0m
[36m[2023-07-10 23:21:51,843][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:21:51,844][227910] Reward + Measures: [[-885.79655511    0.18574765    0.62457234    0.42879       0.83350164]][0m
[37m[1m[2023-07-10 23:21:51,844][227910] Max Reward on eval: -885.7965551103354[0m
[37m[1m[2023-07-10 23:21:51,844][227910] Min Reward on eval: -885.7965551103354[0m
[37m[1m[2023-07-10 23:21:51,844][227910] Mean Reward across all agents: -885.7965551103354[0m
[37m[1m[2023-07-10 23:21:51,845][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:21:57,579][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:21:57,580][227910] Reward + Measures: [[-524.42516337    0.1689        0.86139995    0.55809999    0.88430005]
 [-574.54805199    0.2084        0.8427        0.58929998    0.87370008]
 [-640.36523503    0.29030004    0.75570005    0.5941        0.85840005]
 ...
 [-273.26527074    0.3118        0.75400001    0.5582        0.80380005]
 [-643.11088055    0.32090002    0.74330002    0.57669997    0.81199998]
 [-573.10146702    0.19460002    0.80680001    0.57450002    0.88929999]][0m
[37m[1m[2023-07-10 23:21:57,580][227910] Max Reward on eval: -182.1919784651487[0m
[37m[1m[2023-07-10 23:21:57,580][227910] Min Reward on eval: -883.6807484225021[0m
[37m[1m[2023-07-10 23:21:57,581][227910] Mean Reward across all agents: -554.4857669998593[0m
[37m[1m[2023-07-10 23:21:57,581][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:21:57,582][227910] mean_value=-810.3451254437305, max_value=197.13848121630724[0m
[37m[1m[2023-07-10 23:21:57,585][227910] New mean coefficients: [[1.8945761  1.5567932  1.7400973  0.47283143 3.852017  ]][0m
[37m[1m[2023-07-10 23:21:57,586][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:22:07,412][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 23:22:07,413][227910] FPS: 390841.33[0m
[36m[2023-07-10 23:22:07,415][227910] itr=1606, itrs=2000, Progress: 80.30%[0m
[36m[2023-07-10 23:22:19,007][227910] train() took 11.57 seconds to complete[0m
[36m[2023-07-10 23:22:19,008][227910] FPS: 331815.98[0m
[36m[2023-07-10 23:22:23,705][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:22:23,705][227910] Reward + Measures: [[-525.37374432    0.06119267    0.92844969    0.52835834    0.95421767]][0m
[37m[1m[2023-07-10 23:22:23,706][227910] Max Reward on eval: -525.3737443167304[0m
[37m[1m[2023-07-10 23:22:23,706][227910] Min Reward on eval: -525.3737443167304[0m
[37m[1m[2023-07-10 23:22:23,706][227910] Mean Reward across all agents: -525.3737443167304[0m
[37m[1m[2023-07-10 23:22:23,706][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:22:29,204][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:22:29,205][227910] Reward + Measures: [[-1365.71488588     0.1141         0.85080004     0.31189999
      0.8768    ]
 [-1147.83418114     0.1693         0.82390004     0.30610001
      0.83819991]
 [ -495.98210713     0.13710001     0.86830008     0.51440001
      0.91749996]
 ...
 [ -896.64797451     0.46439996     0.6027         0.57499999
      0.74730003]
 [-1207.25139997     0.34090003     0.60249996     0.37310001
      0.72180003]
 [-1420.37359325     0.58400005     0.82720006     0.62440002
      0.85599995]][0m
[37m[1m[2023-07-10 23:22:29,205][227910] Max Reward on eval: -270.0066229544929[0m
[37m[1m[2023-07-10 23:22:29,205][227910] Min Reward on eval: -1788.9901921069365[0m
[37m[1m[2023-07-10 23:22:29,205][227910] Mean Reward across all agents: -1046.2493967963817[0m
[37m[1m[2023-07-10 23:22:29,205][227910] Average Trajectory Length: 999.6506666666667[0m
[36m[2023-07-10 23:22:29,207][227910] mean_value=-1642.6454107870734, max_value=-418.14513106343065[0m
[36m[2023-07-10 23:22:29,209][227910] XNES is restarting with a new solution whose measures are [0.49689999 0.82889998 0.4842     0.48810002] and objective is -22.02641905026976[0m
[36m[2023-07-10 23:22:29,210][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 23:22:29,212][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 23:22:29,213][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:22:38,917][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 23:22:38,917][227910] FPS: 395782.53[0m
[36m[2023-07-10 23:22:38,919][227910] itr=1607, itrs=2000, Progress: 80.35%[0m
[36m[2023-07-10 23:22:50,574][227910] train() took 11.63 seconds to complete[0m
[36m[2023-07-10 23:22:50,574][227910] FPS: 330118.11[0m
[36m[2023-07-10 23:22:55,376][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:22:55,376][227910] Reward + Measures: [[-38.87368465   0.52161056   0.57626343   0.59720027   0.14197509]][0m
[37m[1m[2023-07-10 23:22:55,377][227910] Max Reward on eval: -38.87368464975958[0m
[37m[1m[2023-07-10 23:22:55,377][227910] Min Reward on eval: -38.87368464975958[0m
[37m[1m[2023-07-10 23:22:55,377][227910] Mean Reward across all agents: -38.87368464975958[0m
[37m[1m[2023-07-10 23:22:55,377][227910] Average Trajectory Length: 962.3893333333333[0m
[36m[2023-07-10 23:23:00,854][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:23:00,906][227910] Reward + Measures: [[  -80.12492454     0.33768579     0.53066581     0.28734118
      0.24614592]
 [-1265.95652729     0.46259999     0.60850006     0.25349998
      0.45900002]
 [ -525.03408421     0.56629556     0.14122091     0.60737318
      0.43537316]
 ...
 [-1294.59578086     0.79840004     0.0257         0.79589999
      0.68090004]
 [-1368.65040677     0.3888         0.52649999     0.1714
      0.44060001]
 [ -986.55905476     0.45359659     0.29424867     0.48712206
      0.30369493]][0m
[37m[1m[2023-07-10 23:23:00,907][227910] Max Reward on eval: 472.99793659269346[0m
[37m[1m[2023-07-10 23:23:00,907][227910] Min Reward on eval: -1846.9252062932355[0m
[37m[1m[2023-07-10 23:23:00,907][227910] Mean Reward across all agents: -785.2086328400244[0m
[37m[1m[2023-07-10 23:23:00,907][227910] Average Trajectory Length: 888.722[0m
[36m[2023-07-10 23:23:00,909][227910] mean_value=-2215.850188987156, max_value=54.98506230700029[0m
[37m[1m[2023-07-10 23:23:00,911][227910] New mean coefficients: [[ 0.4770137  -0.45718062 -0.6277687  -1.5233362  -1.647652  ]][0m
[37m[1m[2023-07-10 23:23:00,912][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:23:10,738][227910] train() took 9.82 seconds to complete[0m
[36m[2023-07-10 23:23:10,738][227910] FPS: 390878.84[0m
[36m[2023-07-10 23:23:10,740][227910] itr=1608, itrs=2000, Progress: 80.40%[0m
[36m[2023-07-10 23:23:22,293][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 23:23:22,293][227910] FPS: 332940.65[0m
[36m[2023-07-10 23:23:27,066][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:23:27,066][227910] Reward + Measures: [[-347.93933702    0.57299322    0.56670117    0.66672873    0.10238941]][0m
[37m[1m[2023-07-10 23:23:27,067][227910] Max Reward on eval: -347.9393370247474[0m
[37m[1m[2023-07-10 23:23:27,067][227910] Min Reward on eval: -347.9393370247474[0m
[37m[1m[2023-07-10 23:23:27,067][227910] Mean Reward across all agents: -347.9393370247474[0m
[37m[1m[2023-07-10 23:23:27,067][227910] Average Trajectory Length: 926.78[0m
[36m[2023-07-10 23:23:32,559][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:23:32,560][227910] Reward + Measures: [[ -687.3788468      0.34481904     0.41851908     0.27389365
      0.45127144]
 [ -782.76983167     0.34472728     0.41020909     0.30436364
      0.16598788]
 [-1802.19290968     0.70200008     0.59780002     0.6584
      0.73689997]
 ...
 [-2196.96669077     0.78442591     0.77813703     0.7872259
      0.78942966]
 [ -951.91070048     0.48828107     0.37637684     0.59299994
      0.43203232]
 [-1782.89898091     0.81040001     0.79040003     0.83430004
      0.82310003]][0m
[37m[1m[2023-07-10 23:23:32,560][227910] Max Reward on eval: 540.4469906347192[0m
[37m[1m[2023-07-10 23:23:32,560][227910] Min Reward on eval: -2432.3870084029622[0m
[37m[1m[2023-07-10 23:23:32,561][227910] Mean Reward across all agents: -1006.868130479169[0m
[37m[1m[2023-07-10 23:23:32,561][227910] Average Trajectory Length: 918.5773333333333[0m
[36m[2023-07-10 23:23:32,562][227910] mean_value=-1992.639745862862, max_value=-292.2186288280699[0m
[36m[2023-07-10 23:23:32,564][227910] XNES is restarting with a new solution whose measures are [0.81899995 0.92140001 0.7676     0.93120003] and objective is 617.7502607754316[0m
[36m[2023-07-10 23:23:32,565][227910] Emitter restarted. Changing the mean agent...[0m
[37m[1m[2023-07-10 23:23:32,568][227910] New mean coefficients: [[ 0.2738619 -0.6621878 -0.8221464 -1.9655045 -1.5956032]][0m
[37m[1m[2023-07-10 23:23:32,569][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:23:42,424][227910] train() took 9.85 seconds to complete[0m
[36m[2023-07-10 23:23:42,425][227910] FPS: 389676.80[0m
[36m[2023-07-10 23:23:42,427][227910] itr=1609, itrs=2000, Progress: 80.45%[0m
[36m[2023-07-10 23:23:53,974][227910] train() took 11.53 seconds to complete[0m
[36m[2023-07-10 23:23:53,975][227910] FPS: 333072.33[0m
[36m[2023-07-10 23:23:58,712][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:23:58,712][227910] Reward + Measures: [[207.71217976   0.83053303   0.79806393   0.80916566   0.80900633]][0m
[37m[1m[2023-07-10 23:23:58,712][227910] Max Reward on eval: 207.712179757681[0m
[37m[1m[2023-07-10 23:23:58,713][227910] Min Reward on eval: 207.712179757681[0m
[37m[1m[2023-07-10 23:23:58,713][227910] Mean Reward across all agents: 207.712179757681[0m
[37m[1m[2023-07-10 23:23:58,713][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:24:04,310][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:24:04,311][227910] Reward + Measures: [[152.86436741   0.82110006   0.77669996   0.80089992   0.76610005]
 [156.79572289   0.85839999   0.62540001   0.80800003   0.75269997]
 [180.30867955   0.8337       0.8933       0.79329997   0.86910003]
 ...
 [101.29819098   0.86269999   0.79840004   0.81099999   0.77750003]
 [102.05526126   0.8714       0.8278001    0.87389994   0.86490005]
 [ 58.52348547   0.75940007   0.7561       0.75340003   0.67549998]][0m
[37m[1m[2023-07-10 23:24:04,311][227910] Max Reward on eval: 478.7203442412254[0m
[37m[1m[2023-07-10 23:24:04,311][227910] Min Reward on eval: -52.07233881272841[0m
[37m[1m[2023-07-10 23:24:04,312][227910] Mean Reward across all agents: 190.32594870512685[0m
[37m[1m[2023-07-10 23:24:04,312][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:24:04,313][227910] mean_value=-361.55381521997714, max_value=400.82784076549484[0m
[37m[1m[2023-07-10 23:24:04,316][227910] New mean coefficients: [[-0.07901749 -1.1093687  -1.775853   -2.6876822  -1.2758709 ]][0m
[37m[1m[2023-07-10 23:24:04,317][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:24:14,055][227910] train() took 9.74 seconds to complete[0m
[36m[2023-07-10 23:24:14,056][227910] FPS: 394376.66[0m
[36m[2023-07-10 23:24:14,058][227910] itr=1610, itrs=2000, Progress: 80.50%[0m
[37m[1m[2023-07-10 23:24:18,063][227910] Removing checkpoint experiments/paper_ppga_ant/1111/checkpoints/cp_00001590[0m
[36m[2023-07-10 23:24:30,058][227910] train() took 11.71 seconds to complete[0m
[36m[2023-07-10 23:24:30,058][227910] FPS: 327942.14[0m
[36m[2023-07-10 23:24:34,737][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:24:34,737][227910] Reward + Measures: [[184.57122812   0.77145326   0.75527161   0.75111133   0.75581038]][0m
[37m[1m[2023-07-10 23:24:34,738][227910] Max Reward on eval: 184.57122812141296[0m
[37m[1m[2023-07-10 23:24:34,738][227910] Min Reward on eval: 184.57122812141296[0m
[37m[1m[2023-07-10 23:24:34,738][227910] Mean Reward across all agents: 184.57122812141296[0m
[37m[1m[2023-07-10 23:24:34,738][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:24:40,317][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:24:40,318][227910] Reward + Measures: [[ 118.79730037    0.71630001    0.77109998    0.72020006    0.74119997]
 [ 193.86798623    0.69510001    0.45950004    0.73280001    0.35889998]
 [-197.2061113     0.59170002    0.76780003    0.49689999    0.74940002]
 ...
 [-274.45932281    0.63869995    0.77829999    0.27169999    0.81440002]
 [-239.12679989    0.27669999    0.45180002    0.45039997    0.25490001]
 [  42.77367864    0.76849997    0.76710004    0.76620001    0.71980006]][0m
[37m[1m[2023-07-10 23:24:40,318][227910] Max Reward on eval: 843.7221144386101[0m
[37m[1m[2023-07-10 23:24:40,318][227910] Min Reward on eval: -1169.2024621155113[0m
[37m[1m[2023-07-10 23:24:40,319][227910] Mean Reward across all agents: -34.19587666403361[0m
[37m[1m[2023-07-10 23:24:40,319][227910] Average Trajectory Length: 997.9143333333333[0m
[36m[2023-07-10 23:24:40,320][227910] mean_value=-677.0265022506647, max_value=261.9612358269309[0m
[37m[1m[2023-07-10 23:24:40,323][227910] New mean coefficients: [[-0.01492524  1.3133088  -1.7009368  -2.5844665  -1.631106  ]][0m
[37m[1m[2023-07-10 23:24:40,324][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:24:49,911][227910] train() took 9.59 seconds to complete[0m
[36m[2023-07-10 23:24:49,911][227910] FPS: 400616.42[0m
[36m[2023-07-10 23:24:49,913][227910] itr=1611, itrs=2000, Progress: 80.55%[0m
[36m[2023-07-10 23:25:01,430][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 23:25:01,431][227910] FPS: 334063.73[0m
[36m[2023-07-10 23:25:06,056][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:25:06,057][227910] Reward + Measures: [[232.36742764   0.69964731   0.70599663   0.67010069   0.70424169]][0m
[37m[1m[2023-07-10 23:25:06,057][227910] Max Reward on eval: 232.36742763971986[0m
[37m[1m[2023-07-10 23:25:06,057][227910] Min Reward on eval: 232.36742763971986[0m
[37m[1m[2023-07-10 23:25:06,058][227910] Mean Reward across all agents: 232.36742763971986[0m
[37m[1m[2023-07-10 23:25:06,058][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:25:11,353][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:25:11,354][227910] Reward + Measures: [[-590.65091793    0.69800001    0.51410002    0.68410003    0.53900003]
 [ -47.45509052    0.79089999    0.4474        0.75029999    0.75209999]
 [-546.92018717    0.7493        0.5334        0.75080001    0.67989999]
 ...
 [  19.98437858    0.64899999    0.90859997    0.43899998    0.88870013]
 [-121.38533409    0.59509999    0.4664        0.68559998    0.3096    ]
 [-152.78318688    0.55590004    0.66130006    0.56620002    0.71460003]][0m
[37m[1m[2023-07-10 23:25:11,354][227910] Max Reward on eval: 527.3007946425932[0m
[37m[1m[2023-07-10 23:25:11,354][227910] Min Reward on eval: -1081.5434292499208[0m
[37m[1m[2023-07-10 23:25:11,355][227910] Mean Reward across all agents: -58.96816536819124[0m
[37m[1m[2023-07-10 23:25:11,355][227910] Average Trajectory Length: 996.9166666666666[0m
[36m[2023-07-10 23:25:11,357][227910] mean_value=-846.7193605922724, max_value=446.4864408917344[0m
[37m[1m[2023-07-10 23:25:11,359][227910] New mean coefficients: [[ 2.3217113  1.0828972 -1.1061559 -2.9080124 -1.9771166]][0m
[37m[1m[2023-07-10 23:25:11,360][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:25:20,986][227910] train() took 9.62 seconds to complete[0m
[36m[2023-07-10 23:25:20,986][227910] FPS: 399010.69[0m
[36m[2023-07-10 23:25:20,988][227910] itr=1612, itrs=2000, Progress: 80.60%[0m
[36m[2023-07-10 23:25:32,499][227910] train() took 11.49 seconds to complete[0m
[36m[2023-07-10 23:25:32,500][227910] FPS: 334155.54[0m
[36m[2023-07-10 23:25:37,166][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:25:37,166][227910] Reward + Measures: [[297.15392202   0.63449866   0.67959732   0.60500336   0.66106629]][0m
[37m[1m[2023-07-10 23:25:37,166][227910] Max Reward on eval: 297.1539220179242[0m
[37m[1m[2023-07-10 23:25:37,167][227910] Min Reward on eval: 297.1539220179242[0m
[37m[1m[2023-07-10 23:25:37,167][227910] Mean Reward across all agents: 297.1539220179242[0m
[37m[1m[2023-07-10 23:25:37,167][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:25:42,585][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:25:42,590][227910] Reward + Measures: [[ 233.08850991    0.71530002    0.49119997    0.75440001    0.56829995]
 [ 259.0334562     0.21659999    0.21200001    0.29949999    0.33260003]
 [-767.09813608    0.77279997    0.47300002    0.77590001    0.14649999]
 ...
 [ 415.59601726    0.52100003    0.5424        0.53380001    0.5722    ]
 [ 414.37477368    0.51070005    0.52039999    0.54180002    0.5284    ]
 [  35.91095477    0.5226        0.33440003    0.57060003    0.28819999]][0m
[37m[1m[2023-07-10 23:25:42,591][227910] Max Reward on eval: 592.2765675853123[0m
[37m[1m[2023-07-10 23:25:42,591][227910] Min Reward on eval: -1147.1903925960419[0m
[37m[1m[2023-07-10 23:25:42,591][227910] Mean Reward across all agents: -15.943771545036155[0m
[37m[1m[2023-07-10 23:25:42,591][227910] Average Trajectory Length: 989.8299999999999[0m
[36m[2023-07-10 23:25:42,593][227910] mean_value=-1147.371841171904, max_value=673.5002134789015[0m
[37m[1m[2023-07-10 23:25:42,596][227910] New mean coefficients: [[ 1.3396837  -0.40908265 -0.85958827 -2.453539   -1.7397164 ]][0m
[37m[1m[2023-07-10 23:25:42,597][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:25:52,210][227910] train() took 9.61 seconds to complete[0m
[36m[2023-07-10 23:25:52,211][227910] FPS: 399503.90[0m
[36m[2023-07-10 23:25:52,213][227910] itr=1613, itrs=2000, Progress: 80.65%[0m
[36m[2023-07-10 23:26:03,674][227910] train() took 11.44 seconds to complete[0m
[36m[2023-07-10 23:26:03,674][227910] FPS: 335637.49[0m
[36m[2023-07-10 23:26:08,453][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:26:08,454][227910] Reward + Measures: [[380.02053044   0.57094961   0.71034527   0.50198901   0.67069769]][0m
[37m[1m[2023-07-10 23:26:08,454][227910] Max Reward on eval: 380.0205304447931[0m
[37m[1m[2023-07-10 23:26:08,454][227910] Min Reward on eval: 380.0205304447931[0m
[37m[1m[2023-07-10 23:26:08,455][227910] Mean Reward across all agents: 380.0205304447931[0m
[37m[1m[2023-07-10 23:26:08,455][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:26:13,994][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:26:13,994][227910] Reward + Measures: [[196.52014918   0.62700003   0.59420007   0.65230006   0.5521    ]
 [385.78461072   0.61070007   0.77689999   0.50340003   0.73590004]
 [418.73845641   0.49560004   0.68419999   0.31580001   0.62619996]
 ...
 [260.3807312    0.61190003   0.64539999   0.6013       0.59310001]
 [399.31399296   0.53470004   0.67559999   0.48859999   0.64589995]
 [261.5410253    0.62970001   0.68269998   0.52120006   0.62029999]][0m
[37m[1m[2023-07-10 23:26:13,995][227910] Max Reward on eval: 609.6241352448648[0m
[37m[1m[2023-07-10 23:26:13,995][227910] Min Reward on eval: 19.991989290143827[0m
[37m[1m[2023-07-10 23:26:13,995][227910] Mean Reward across all agents: 307.4150677506142[0m
[37m[1m[2023-07-10 23:26:13,995][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:26:13,997][227910] mean_value=-379.07486728708943, max_value=700.0215742661735[0m
[37m[1m[2023-07-10 23:26:14,000][227910] New mean coefficients: [[ 1.1911174 -1.6432334  1.4683801 -2.161381  -1.2894506]][0m
[37m[1m[2023-07-10 23:26:14,001][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:26:23,777][227910] train() took 9.77 seconds to complete[0m
[36m[2023-07-10 23:26:23,777][227910] FPS: 392868.80[0m
[36m[2023-07-10 23:26:23,780][227910] itr=1614, itrs=2000, Progress: 80.70%[0m
[36m[2023-07-10 23:26:35,307][227910] train() took 11.50 seconds to complete[0m
[36m[2023-07-10 23:26:35,307][227910] FPS: 333795.89[0m
[36m[2023-07-10 23:26:40,122][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:26:40,123][227910] Reward + Measures: [[487.64685236   0.48497298   0.75176007   0.35445234   0.70053971]][0m
[37m[1m[2023-07-10 23:26:40,123][227910] Max Reward on eval: 487.6468523588423[0m
[37m[1m[2023-07-10 23:26:40,123][227910] Min Reward on eval: 487.6468523588423[0m
[37m[1m[2023-07-10 23:26:40,123][227910] Mean Reward across all agents: 487.6468523588423[0m
[37m[1m[2023-07-10 23:26:40,123][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:26:45,572][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:26:45,573][227910] Reward + Measures: [[427.15193873   0.59650004   0.6652       0.63339996   0.6469    ]
 [495.16236505   0.47159997   0.602        0.53140002   0.5607    ]
 [ 84.76062221   0.72670001   0.77179998   0.67720002   0.73390001]
 ...
 [440.41155573   0.60810006   0.81259996   0.27420002   0.7669    ]
 [435.93862007   0.55520004   0.60410005   0.5503       0.542     ]
 [302.81339931   0.59700006   0.69010001   0.48259997   0.64370006]][0m
[37m[1m[2023-07-10 23:26:45,573][227910] Max Reward on eval: 793.8560719103843[0m
[37m[1m[2023-07-10 23:26:45,573][227910] Min Reward on eval: -237.40240938400385[0m
[37m[1m[2023-07-10 23:26:45,574][227910] Mean Reward across all agents: 448.0445450658194[0m
[37m[1m[2023-07-10 23:26:45,574][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:26:45,576][227910] mean_value=-367.3690319076452, max_value=842.7658979310277[0m
[37m[1m[2023-07-10 23:26:45,579][227910] New mean coefficients: [[ 1.5072498  0.7728678  2.0596933 -1.5678031 -1.1183492]][0m
[37m[1m[2023-07-10 23:26:45,580][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:26:55,411][227910] train() took 9.83 seconds to complete[0m
[36m[2023-07-10 23:26:55,411][227910] FPS: 390668.12[0m
[36m[2023-07-10 23:26:55,413][227910] itr=1615, itrs=2000, Progress: 80.75%[0m
[36m[2023-07-10 23:27:06,909][227910] train() took 11.48 seconds to complete[0m
[36m[2023-07-10 23:27:06,909][227910] FPS: 334613.53[0m
[36m[2023-07-10 23:27:11,726][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:27:11,726][227910] Reward + Measures: [[575.19308492   0.46818998   0.84782469   0.14597166   0.78905934]][0m
[37m[1m[2023-07-10 23:27:11,727][227910] Max Reward on eval: 575.1930849199957[0m
[37m[1m[2023-07-10 23:27:11,727][227910] Min Reward on eval: 575.1930849199957[0m
[37m[1m[2023-07-10 23:27:11,727][227910] Mean Reward across all agents: 575.1930849199957[0m
[37m[1m[2023-07-10 23:27:11,727][227910] Average Trajectory Length: 1000.0[0m
[36m[2023-07-10 23:27:17,399][227910] Finished Evaluation Step[0m
[37m[1m[2023-07-10 23:27:17,400][227910] Reward + Measures: [[-294.14320776    0.58600003    0.72749996    0.25920001    0.72799999]
 [-241.46145028    0.39302865    0.37145945    0.41876575    0.32392317]
 [-491.02868275    0.45532194    0.50940138    0.22003654    0.56709272]
 ...
 [-390.0658111     0.59149998    0.70209998    0.3285        0.71990007]
 [-631.74021333    0.28427881    0.34811345    0.19909146    0.38322771]
 [ 128.82313701    0.85210001    0.64899999    0.81529999    0.73119992]][0m
[37m[1m[2023-07-10 23:27:17,400][227910] Max Reward on eval: 664.203327357478[0m
[37m[1m[2023-07-10 23:27:17,401][227910] Min Reward on eval: -1028.463716713246[0m
[37m[1m[2023-07-10 23:27:17,401][227910] Mean Reward across all agents: 35.30332965455374[0m
[37m[1m[2023-07-10 23:27:17,401][227910] Average Trajectory Length: 984.745[0m
[36m[2023-07-10 23:27:17,403][227910] mean_value=-810.6684102631557, max_value=312.53181157809865[0m
[37m[1m[2023-07-10 23:27:17,406][227910] New mean coefficients: [[ 0.5927163   1.1883527   2.2325532   0.06612706 -0.93904686]][0m
[37m[1m[2023-07-10 23:27:17,407][227910] Moving the mean solution point...[0m
[36m[2023-07-10 23:27:27,107][227910] train() took 9.70 seconds to complete[0m
[36m[2023-07-10 23:27:27,107][227910] FPS: 395941.11[0m
[36m[2023-07-10 23:27:27,109][227910] itr=1616, itrs=2000, Progress: 80.80%[0m
